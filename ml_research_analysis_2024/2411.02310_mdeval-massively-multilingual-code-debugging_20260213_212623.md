---
ver: rpa2
title: 'MdEval: Massively Multilingual Code Debugging'
arxiv_id: '2411.02310'
source_url: https://arxiv.org/abs/2411.02310
tags:
- code
- arxiv
- debugging
- language
- program
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MDEVAL, the first massively multilingual
  debugging benchmark covering 20 programming languages with 3.9K samples across three
  tasks: automated program repair (APR), bug localization (BL), and bug identification
  (BI). The authors create MDEVAL-INSTRUCT by injecting bugs into correct multilingual
  queries and solutions using three strategies: modifying queries, injecting bugs
  into solutions, or using round-trip code translation.'
---

# MdEval: Massively Multilingual Code Debugging

## Quick Facts
- arXiv ID: 2411.02310
- Source URL: https://arxiv.org/abs/2411.02310
- Reference count: 17
- Key outcome: First massively multilingual debugging benchmark covering 20 programming languages with 3.9K samples across three tasks

## Executive Summary
This paper introduces MDEVAL, the first massively multilingual debugging benchmark spanning 20 programming languages with 3.9K samples across automated program repair, bug localization, and bug identification tasks. The authors create MDEVAL-INSTRUCT by injecting bugs into correct multilingual code using three strategies, then train xDebugCoder as a multilingual debugging baseline. Experiments reveal a substantial performance gap between closed-source models (GPT-4o, Claude-3.5) and open-source alternatives, with xDebugCoder achieving competitive results despite limited training data.

## Method Summary
The method constructs MDEVAL with 3.9K multilingual debugging samples across 20 programming languages, then generates MDEVAL-INSTRUCT by injecting bugs into correct code using xDebugGen's three strategies. xDebugCoder is fine-tuned on this instruction corpus using Qwen2.5-Coder-7B base model for 3 epochs with AdamW optimizer. The model is evaluated on MDEVAL's three tasks (APR, BL, BI) using Pass@1 for APR and accuracy for BL/BI metrics.

## Key Results
- MDEVAL covers 20 programming languages with 3.9K samples across three debugging tasks
- xDebugCoder achieves competitive results despite being trained on only 16K instruction samples
- Closed-source models (GPT-4o, Claude-3.5) significantly outperform open-source alternatives in multilingual debugging
- Bug localization proves notably more challenging than automated program repair

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark effectively captures multilingual debugging capabilities by providing a diverse set of tasks and languages.
- Mechanism: MDEVAL covers 20 programming languages and includes three tasks (APR, BL, BI), allowing for a comprehensive evaluation of LLM debugging capabilities across different language-specific error types.
- Core assumption: The diversity of languages and tasks in MDEVAL reflects the real-world challenges of multilingual debugging.
- Evidence anchors:
  - [abstract] "MDEVAL, the first massively multilingual debugging benchmark covering 20 programming languages with 3.9K samples across three tasks: automated program repair (APR), bug localization (BL), and bug identification (BI)."
  - [section] "MDEVAL consists of 3.9K problems... We design 3 multilingual debugging-related tasks: Automated Program Repair, Bug Localization, and Bug Identification."
- Break condition: If the tasks or languages included in MDEVAL do not represent the actual challenges faced in multilingual debugging scenarios.

### Mechanism 2
- Claim: xDebugGen effectively creates a multilingual debugging instruction corpus by injecting bugs into correct code snippets.
- Mechanism: xDebugGen uses three strategies to inject bugs into correct multilingual queries and solutions, creating pairs of buggy code and correct code for instruction tuning.
- Core assumption: Injecting bugs into correct code snippets using the proposed strategies can create realistic debugging scenarios for training LLMs.
- Evidence anchors:
  - [abstract] "We introduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs into correct multilingual queries and solutions (xDebugGen)."
  - [section] "We propose xDebugGen comprised of the following three strategies to create the code debugging instruction corpora MDEVAL-INSTRUCT..."
- Break condition: If the injected bugs do not accurately represent real-world debugging scenarios or if the strategies fail to generate diverse and challenging debugging tasks.

### Mechanism 3
- Claim: xDebugCoder, trained on MDEVAL-INSTRUCT, effectively handles bugs across a wide range of programming languages.
- Mechanism: xDebugCoder is fine-tuned on the MDEVAL-INSTRUCT corpus, which includes multilingual debugging instructions, enabling it to address language-specific bugs and improve debugging performance.
- Core assumption: Fine-tuning a model on a diverse set of multilingual debugging instructions can improve its ability to handle bugs in various programming languages.
- Evidence anchors:
  - [abstract] "Further, a multilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong baseline specifically to handle bugs of a wide range of programming languages (e.g. 'Missing Mut' in language Rust and 'Misused Macro Definition' in language C)."
  - [section] "Leveraging MDEVAL-INSTRUCT, we develop xDebugCoder as a strong baseline, assessing the transferability of LLMs in multilingual debugging tasks."
- Break condition: If the fine-tuning process does not lead to improved debugging performance across different programming languages or if the model fails to generalize to unseen language-specific bugs.

## Foundational Learning

- Concept: Understanding the differences between programming languages and their specific error types.
  - Why needed here: Multilingual debugging requires knowledge of language-specific bugs and how to handle them effectively.
  - Quick check question: Can you identify the differences between a syntax error in Python and a syntax error in Rust?

- Concept: Familiarity with automated program repair (APR), bug localization (BL), and bug identification (BI) tasks.
  - Why needed here: These tasks form the core of the MDEVAL benchmark and are essential for evaluating LLM debugging capabilities.
  - Quick check question: How would you approach fixing a bug in a given code snippet, and how does this differ from localizing the bug or identifying its type?

- Concept: Knowledge of instruction tuning and its role in improving LLM performance on specific tasks.
  - Why needed here: xDebugCoder is fine-tuned on MDEVAL-INSTRUCT to enhance its multilingual debugging capabilities.
  - Quick check question: How does instruction tuning differ from traditional pre-training, and why is it important for adapting LLMs to specific tasks like debugging?

## Architecture Onboarding

- Component map:
  MDEVAL dataset -> xDebugGen bug injection -> MDEVAL-INSTRUCT instruction corpus -> xDebugCoder fine-tuning -> Model evaluation on MDEVAL tasks

- Critical path:
  1. Construct MDEVAL with diverse programming languages and tasks.
  2. Generate MDEVAL-INSTRUCT using xDebugGen to inject bugs into correct code snippets.
  3. Fine-tune xDebugCoder on MDEVAL-INSTRUCT to create a strong multilingual debugging baseline.
  4. Evaluate xDebugCoder and other models on MDEVAL to assess their debugging capabilities.

- Design tradeoffs:
  - Language coverage vs. task diversity: Including more languages may limit the number of tasks or samples per language.
  - Bug injection strategies vs. realism: Injecting bugs may not fully capture the complexity of real-world debugging scenarios.
  - Model size vs. performance: Larger models may perform better but are more resource-intensive to train and deploy.

- Failure signatures:
  - Poor performance on language-specific bugs: Indicates insufficient coverage of language-specific error types in MDEVAL or MDEVAL-INSTRUCT.
  - Inability to handle complex debugging scenarios: Suggests that the injected bugs or tasks in MDEVAL are not challenging enough.
  - Limited transferability across languages: Implies that the fine-tuning process or the instruction corpus does not effectively capture multilingual debugging patterns.

- First 3 experiments:
  1. Evaluate xDebugCoder on a subset of MDEVAL tasks and languages to assess its initial performance and identify areas for improvement.
  2. Analyze the performance of xDebugCoder on language-specific bugs to determine if the fine-tuning process effectively addresses these challenges.
  3. Compare the performance of xDebugCoder with other multilingual models on MDEVAL to validate its effectiveness as a strong baseline.

## Open Questions the Paper Calls Out

- How to evaluate LLMs on real-world multilingual debugging scenarios, as MDEVAL aims to simulate realistic debugging scenarios but may not fully capture the complexity and variability of real-world software development.

- How to improve the quality and diversity of the instruction data, as the quality and diversity of the generated data could be further improved and alternative methods for generating high-quality instruction data could be explored.

- How to optimize the trade-off between model size and performance, as larger models may perform better but are more resource-intensive to train and deploy.

## Limitations

- The benchmark relies on synthetic bug injection rather than real-world debugging data, which may not accurately represent natural debugging scenarios.
- The relatively small dataset size (3.9K samples) compared to single-language benchmarks may limit the robustness of findings, particularly for less-represented languages.
- Performance gap between closed-source and open-source models raises questions about fair representation of current multilingual debugging capabilities.

## Confidence

- High Confidence: The benchmark construction methodology and evaluation framework are well-defined and reproducible. The performance gap between closed-source and open-source models is consistently observed across multiple tasks and languages.
- Medium Confidence: The effectiveness of xDebugCoder as a multilingual debugging baseline, while demonstrated, is based on limited training data and may not fully capture the complexity of real-world multilingual debugging scenarios.
- Low Confidence: The claim that MDEVAL "comprehensively captures" multilingual debugging capabilities is somewhat overstated given the synthetic nature of the data and potential limitations in representing natural debugging scenarios across all 20 languages.

## Next Checks

1. Evaluate xDebugCoder on a subset of real-world debugging data from multilingual code repositories to assess whether performance on synthetic bugs translates to natural debugging scenarios.

2. Conduct controlled experiments measuring how debugging performance in one language transfers to another, particularly for languages with similar syntax (e.g., C and C++, Python and JavaScript) versus dissimilar ones (e.g., Python and Rust).

3. Systematically analyze performance variance across the 20 languages, focusing on under-represented languages in the dataset, to identify whether certain language families require targeted data augmentation strategies.