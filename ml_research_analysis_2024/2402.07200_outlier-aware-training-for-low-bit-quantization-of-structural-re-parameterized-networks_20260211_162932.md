---
ver: rpa2
title: Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized
  Networks
arxiv_id: '2402.07200'
source_url: https://arxiv.org/abs/2402.07200
tags:
- quantization
- oabn
- repvgg
- weights
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors identify a quantization bottleneck in structural re-parameterized
  (SR) networks like RepVGG, caused by outliers introduced during the merging of training
  and inference structures. They propose Outlier-Aware Batch Normalization (OABN),
  which clips batch normalization parameters during training to suppress these outliers,
  making the network compatible with post-training quantization (PTQ) and quantization-aware
  training (QAT).
---

# Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks

## Quick Facts
- arXiv ID: 2402.07200
- Source URL: https://arxiv.org/abs/2402.07200
- Reference count: 1
- The authors propose Outlier-Aware Batch Normalization (OABN) and ClusterQAT to address quantization bottlenecks in RepVGG networks caused by outliers from identity batch normalization branches, achieving near-full-precision accuracy at 3-6 bits.

## Executive Summary
This paper addresses a fundamental quantization bottleneck in structural re-parameterized (SR) networks like RepVGG, where the merging of training and inference structures introduces outliers that severely degrade low-bit quantization performance. The authors propose Outlier-Aware Batch Normalization (OABN) to suppress these outliers during training by clipping batch normalization parameters, and ClusterQAT, a clustering-based non-uniform quantization framework that dynamically adjusts quantization intervals. Together, these methods enable RepVGG to achieve near-full-precision accuracy even at 3-6 bits, where conventional methods fail.

## Method Summary
The authors identify that outliers in RepVGG weights originate from identity batch normalization branches during the training-to-inference structure merging process. They propose OABN, which clips the γ parameter in batch normalization layers based on running variance to suppress outlier generation during training. For quantization-aware training, they introduce ClusterQAT, a clustering-based non-uniform quantization approach that dynamically adjusts quantization intervals during training to better preserve weight distributions. The method involves training with OABN to create outlier-free weights, then fine-tuning with ClusterQAT for low-bit quantization, achieving significant accuracy improvements over uniform quantization methods.

## Key Results
- OABN with k=0.5 enables effective post-training quantization (PTQ) for RepVGG, where standard methods fail
- ClusterQAT combined with OABN achieves 4-bit quantized accuracy close to full-precision models on CIFAR-10 and ImageNet
- The approach successfully quantizes RepVGG to 3-6 bits with minimal accuracy loss, compared to >10% drops with conventional methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Outliers in RepVGG weights are introduced by the merging of training and inference structures, particularly from identity batch normalization (BN) branches.
- Mechanism: During the conversion of a 3-branch training structure to a single-branch inference structure, identity BN blocks contribute γ/√σ values that become outliers in the merged weights. These outliers disrupt uniform quantization by dominating the dynamic range.
- Core assumption: The merging rules in RepVGG amplify certain weight values disproportionately, and these amplified values originate from identity BN branches rather than the main convolutional branches.
- Evidence anchors:
  - [abstract] "the merging process in SR networks introduces outliers into weights, making their distribution distinct from conventional networks and thus heightening difficulties in quantization."
  - [section] "According to the merging rules in [Ding, Honghao Chen, et al. 2022], the identity branches are calculated as (4)." and "Most of the large weights are located in the position of Class A, instead of Class B and Class C."

### Mechanism 2
- Claim: Outlier-Aware Batch Normalization (OABN) suppresses outliers by clipping the γ parameter during training based on running variance.
- Mechanism: By limiting γ to the range [-kσ, kσ] during forward propagation, OABN prevents the identity BN branches from injecting large weight values into the merged kernel. This keeps the weight distribution compact and quantization-friendly.
- Core assumption: Clipping γ does not harm the representational power of the network if k is chosen properly, because the BN normalization already scales activations to a reasonable range.
- Evidence anchors:
  - [section] "We clip γ according to the running variance σ during training. The training process using OABN is given in Algorithm 1."
  - [section] "Specifically, for small values of k (e.g. k = 0.5), all PTQ methods mentioned above become effective, while they are less effective for larger values of k."

### Mechanism 3
- Claim: ClusterQAT dynamically adjusts quantization intervals during training to preserve weight distribution under low-bit constraints.
- Mechanism: Clustering-based non-uniform quantization partitions the weight space into optimal intervals per layer, then updates these intervals as training progresses. This preserves the structure of the weight distribution better than fixed uniform quantization.
- Core assumption: The weight distribution learned during OABN training is stable enough that dynamic clustering can track and adapt to it without destabilizing convergence.
- Evidence anchors:
  - [abstract] "ClusterQAT, a clustering-based non-uniform quantization framework for Quantization-Aware Training (QAT) named ClusterQAT."
  - [section] "The key to the quality of quantization lies in whether the quantization method can preserve the original distribution of the full-precision weights."

## Foundational Learning

- Concept: Batch Normalization mechanics (γ, β, µ, σ) and how they merge into convolutional kernels.
  - Why needed here: Understanding BN merging rules is essential to see how identity branches create outliers in RepVGG.
  - Quick check question: In RepVGG merging, what term from the identity BN branch becomes an additive outlier in the merged 3×3 kernel?

- Concept: Post-training quantization (PTQ) vs quantization-aware training (QAT).
  - Why needed here: The paper improves both PTQ and QAT; knowing their differences explains why OABN helps PTQ and ClusterQAT helps QAT.
  - Quick check question: Why does RepVGG's outlier issue block PTQ but not QAT in conventional designs?

- Concept: Non-uniform vs uniform quantization.
  - Why needed here: ClusterQAT's core improvement is switching to non-uniform quantization; understanding this helps evaluate its benefits.
  - Quick check question: What is the main advantage of non-uniform quantization when weight distributions are skewed or contain outliers?

## Architecture Onboarding

- Component map:
  - RepVGG block: 3-branch training (3×3 conv, 1×1 conv, identity) → single-branch inference (VGG-like 3×3 conv)
  - OABN layer: Modified BN that clips γ based on σ during training
  - ClusterQAT module: Clustering-based non-uniform quantizer that updates intervals during QAT

- Critical path:
  1. Train with OABN to suppress outliers → clean weight distribution
  2. Apply uniform PTQ for 8-bit accuracy
  3. Fine-tune with ClusterQAT for lower bits (4-6)

- Design tradeoffs:
  - OABN k hyperparameter: small k → better quantization, lower FP32 accuracy; large k → better FP32 accuracy, worse quantization
  - ClusterQAT bit-width: lower bits → more aggressive compression but higher quantization error; must balance with clustering granularity
  - Memory overhead: non-uniform quantization stores per-layer interval tables; affects deployment

- Failure signatures:
  - High variance in merged weights → PTQ fails (large accuracy drop)
  - Low FP32 accuracy after OABN → indicates k too aggressive
  - ClusterQAT not improving → suggests pre-trained model not trained with OABN or k too large

- First 3 experiments:
  1. Train RepVGG-A0 with k=1 vs k=10; measure FP32 and INT8 accuracy to confirm OABN effect
  2. Apply uniform PTQ to OABN-trained vs standard BN-trained RepVGG; compare state size and accuracy
  3. Fine-tune OABN-trained model with ClusterQAT at 4-bit; compare against uniform QAT baseline

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- OABN's effectiveness depends heavily on the k hyperparameter, with a narrow effective range that may vary across architectures
- ClusterQAT's computational overhead during training and memory requirements for storing non-uniform intervals are not thoroughly analyzed
- The method is only validated on RepVGG architecture, limiting generalizability to other SR networks

## Confidence
- **High confidence**: The identification of outliers from identity BN branches as the quantization bottleneck in RepVGG networks, supported by weight distribution analysis and ablation studies on k values
- **Medium confidence**: The OABN mechanism's effectiveness across different bit-widths, as results are primarily shown for 8-bit and lower (4-6 bit) cases, with limited validation at intermediate bit-widths
- **Medium confidence**: ClusterQAT's generalization to other network architectures beyond RepVGG, as the method is only evaluated on the specific RepVGG-A0 architecture

## Next Checks
1. Conduct sensitivity analysis on the k hyperparameter across multiple network depths and datasets to establish guidelines for k selection
2. Evaluate ClusterQAT's performance when applied to standard convolutional networks without structural re-parameterization to test its broader applicability
3. Measure the computational overhead and memory requirements of ClusterQAT during training and inference to assess practical deployment implications