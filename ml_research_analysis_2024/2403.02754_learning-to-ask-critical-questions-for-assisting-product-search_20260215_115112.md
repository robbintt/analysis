---
ver: rpa2
title: Learning to Ask Critical Questions for Assisting Product Search
arxiv_id: '2403.02754'
source_url: https://arxiv.org/abs/2403.02754
tags:
- user
- item
- search
- questions
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of product search in e-commerce
  where traditional query-based methods often fail due to vague user input. The authors
  propose a dual-learning model that combines session-aware recommendation with proactive
  question asking.
---

# Learning to Ask Critical Questions for Assisting Product Search

## Quick Facts
- arXiv ID: 2403.02754
- Source URL: https://arxiv.org/abs/2403.02754
- Authors: Zixuan Li; Lizi Liao; Tat-Seng Chua
- Reference count: 40
- Primary result: Dual-learning model combining session-aware recommendation with proactive question asking achieves up to 45.96% improvement in MRR over baselines

## Executive Summary
This paper addresses the challenge of product search in e-commerce where traditional query-based methods often fail due to vague user input. The authors propose a novel dual-learning framework that combines session-aware recommendation with proactive question asking to clarify user preferences. Their approach uses a utility score to identify the most informative items in a user's browsing session and leverages this to select critical questions for the user. Extensive experiments on a public dataset demonstrate significant performance improvements over state-of-the-art baselines, showing the effectiveness of combining implicit session feedback with explicit question answering for improving product search accuracy.

## Method Summary
The authors develop a dual-learning model that operates in two phases: first identifying informative items in a user's browsing session through a utility score mechanism, then using these insights to ask clarifying questions. The Selection Net calculates contextualized utility scores using a Transformer-based encoder to capture complex item relationships, while the dual Ranking Net predicts target items based on user feedback. The system generates synthetic items to facilitate question selection and uses attribute-based questions to gather explicit user preferences. The approach is trained on session click data and validated through extensive experiments on the Diginetica dataset.

## Key Results
- Achieves up to 45.96% improvement in MRR compared to existing methods
- DualSI-randomQ (random question selection) achieves 35.54% improvement in MRR
- DualSI-randomA (random question answering) achieves 28.83% improvement in MRR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The utility score identifies which clicked items provide the most useful information for finding the target product.
- Mechanism: The model calculates attention scores between the search query and contextualized item embeddings, where higher scores indicate items that carry more critical information leading to the target.
- Core assumption: Items that are more frequently clicked and share attributes with the target product carry more utility for finding the target.
- Evidence anchors:
  - [abstract] "We first establish a novel utility score to measure whether a clicked item provides useful information for finding the target."
  - [section 3.1.1] "We explicitly measure it by introducing the utility score α. Intuitively, higher utility score implies that the item carries more critical information related to the target product within its search context."
  - [corpus] Weak evidence - corpus neighbors discuss question-based retrieval but don't specifically validate the utility score mechanism.
- Break condition: If the assumption that frequently clicked attributes indicate target relevance fails, the utility score would not effectively identify critical items.

### Mechanism 2
- Claim: The dual-learning framework effectively combines implicit session feedback with explicit question answering to improve product search accuracy.
- Mechanism: The Selection Net uses session click data to calculate utility scores and select critical questions, while the dual Ranking Net uses user responses to these questions to rank target items more accurately.
- Core assumption: Users develop clearer preferences after browsing, making question asking more effective than early-stage questioning.
- Evidence anchors:
  - [abstract] "Our approach uses a novel utility score to identify the most informative items in a user's browsing session and leverages this to select critical questions for the user."
  - [section 3] "This design leverages both user's implicit feedback and explicit question answering, which is inspired by the fact that shoppers usually gets clearer on the exact item to purchase after some initial browsing."
  - [corpus] Moderate evidence - neighbors discuss interactive search but don't validate the specific dual-learning approach.
- Break condition: If users don't develop clearer preferences after browsing, the timing assumption breaks and question asking becomes less effective.

### Mechanism 3
- Claim: The Transformer-based Selection Net captures complex transition patterns beyond monotonic chains in session data.
- Mechanism: The Selection Net encodes item sequences with static dynamic embeddings and self-attention blocks, allowing it to capture complex context information among clicked items rather than just sequential patterns.
- Core assumption: Complex item relationships in browsing sessions contain more information than simple sequential patterns for identifying target items.
- Evidence anchors:
  - [section 3.1.2] "We adopt the transformer module because of its capability in capturing the complex context information among the clicked items."
  - [section 2.1] "SR-GNN is one of the state-of-the-art model for session-aware recommendation. It constructs the clicked items into graph to capture the complex transitions"
  - [corpus] Weak evidence - corpus neighbors discuss complex patterns but don't specifically validate Transformer-based approaches for this problem.
- Break condition: If simple sequential patterns are sufficient for identifying target items, the complexity of Transformer-based encoding becomes unnecessary overhead.

## Foundational Learning

- Concept: Session-aware recommendation
  - Why needed here: The paper builds on session-aware recommendation to understand user behavior during browsing sessions, which is essential for identifying when to ask questions.
  - Quick check question: How does session-aware recommendation differ from traditional user-based recommendation methods?

- Concept: Interactive search and question asking
  - Why needed here: The paper introduces question asking to clarify user preferences, requiring understanding of how interactive systems gather explicit feedback.
  - Quick check question: What are the key challenges in designing effective clarification questions for product search?

- Concept: Transformer attention mechanisms
  - Why needed here: The Selection Net uses Transformer architecture to capture complex item relationships, requiring understanding of multi-head attention and positional encoding.
  - Quick check question: How does self-attention in Transformers differ from traditional RNN-based sequence modeling?

## Architecture Onboarding

- Component map: Clicked items → Static Dynamic Embedding → Transformer → Selection Net (utility scores) → Question selection → User response → Dual Ranking Net → Final ranking

- Critical path: Clicked items → Static Dynamic Embedding → Transformer → Selection Net (utility scores) → Question selection → User response → Dual Ranking Net → Final ranking

- Design tradeoffs:
  - Complexity vs. performance: Transformer-based approach provides better context capture but increases computational overhead
  - Question timing: Asking questions after browsing improves effectiveness but may lengthen sessions
  - Attribute selection: Using product attributes for questions requires good attribute extraction but provides targeted clarification

- Failure signatures:
  - Low utility score differentiation: Model fails to distinguish between informative and non-informative items
  - Poor question selection: Selected questions don't correlate with target item attributes
  - Ranking performance drops: Dual Ranking Net doesn't effectively incorporate user responses

- First 3 experiments:
  1. Test utility score calculation with synthetic data to verify it correctly identifies informative items
  2. Validate question selection mechanism by checking if selected questions appear in target items
  3. Compare performance with and without user responses to quantify the benefit of explicit feedback

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the utility score calculation handle sessions with highly diverse clicked items where no clear attribute patterns emerge?
- Basis in paper: [inferred] The paper mentions utility scores are based on attribute occurrence patterns but doesn't address scenarios with diverse, unrelated clicks.
- Why unresolved: The methodology assumes some level of attribute consistency in clicked items, but real user sessions may contain unrelated browsing behavior.
- What evidence would resolve it: Experiments showing performance degradation or adaptation mechanisms when utility scores are calculated from diverse, patternless click sequences.

### Open Question 2
- Question: What is the impact of synthetic item generation complexity on the Selection Net's ability to identify critical questions when the attribute space is extremely large?
- Basis in paper: [explicit] The paper mentions synthetic items are generated using different attribute combinations but doesn't discuss computational limits or performance with large attribute spaces.
- Why unresolved: The combinatorial explosion of possible attribute combinations could make synthetic item generation computationally prohibitive in real-world scenarios.
- What evidence would resolve it: Empirical results showing Selection Net performance degradation or computational time increases as the number of possible attributes grows beyond the tested range.

### Open Question 3
- Question: How does the model handle cases where user answers contradict the implicit feedback patterns established from click-stream data?
- Basis in paper: [inferred] The dual ranking mechanism combines explicit answers with implicit feedback, but no discussion exists on handling contradictions between them.
- Why unresolved: The paper doesn't address scenarios where user answers directly conflict with what the implicit feedback would suggest about their preferences.
- What evidence would resolve it: Analysis of model performance when explicit answers and implicit feedback patterns are intentionally made contradictory, and how the model weights each source.

## Limitations
- The evaluation relies entirely on offline simulation using the Diginetica dataset without real user testing
- Synthetic question answering assumes users answer questions perfectly, which is unrealistic in practice
- Performance gains come from a specific experimental setup with controlled question selection and perfect user responses that may not generalize to real-world scenarios

## Confidence
**High Confidence**: The core architectural design combining session-aware recommendation with question asking is well-specified and technically sound.
**Medium Confidence**: The claimed performance improvements (45.96% MRR gain) are based on synthetic evaluation that may overstate real-world effectiveness.
**Low Confidence**: The paper does not address how the system would perform with imperfect user responses, diverse question types, or in production environments with noisy user behavior.

## Next Checks
1. **Real User Study**: Conduct an A/B test comparing the dual-learning approach against baselines with actual users, measuring not just ranking metrics but also user satisfaction, session duration, and conversion rates.
2. **Noise Tolerance Testing**: Evaluate system performance when user responses to questions are imperfect (50-80% accuracy) rather than perfect, to assess robustness to realistic user behavior.
3. **Cross-Dataset Validation**: Test the approach on multiple e-commerce datasets with different product domains and user demographics to verify generalizability beyond the Diginetica dataset.