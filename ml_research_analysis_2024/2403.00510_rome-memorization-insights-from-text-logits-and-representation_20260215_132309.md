---
ver: rpa2
title: 'ROME: Memorization Insights from Text, Logits and Representation'
arxiv_id: '2403.00510'
source_url: https://arxiv.org/abs/2403.00510
tags:
- memorization
- memorized
- non-memorized
- hidden
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ROME, a novel approach to probe memorization
  in large language models (LLMs) without accessing training data. Instead of comparing
  model outputs with training corpora, ROME categorizes samples into memorized and
  non-memorized groups using extra datasets with ground-truth labels, and analyzes
  the disparities between these groups from text, probability, and hidden state perspectives.
---

# ROME: Memorization Insights from Text, Logits and Representation

## Quick Facts
- arXiv ID: 2403.00510
- Source URL: https://arxiv.org/abs/2403.00510
- Reference count: 17
- Key outcome: Proposes ROME to probe LLM memorization without training data access by comparing memorized vs non-memorized groups using curated datasets

## Executive Summary
This paper introduces ROME, a novel approach to probe memorization in large language models without accessing training data. Instead of comparing model outputs with training corpora, ROME categorizes samples into memorized and non-memorized groups using extra datasets with ground-truth labels, and analyzes the disparities between these groups from text, probability, and hidden state perspectives. The method reveals several findings about memorization patterns, including the effects of prompt length, word length, and part-of-speech on memorization likelihood.

## Method Summary
ROME probes memorization by categorizing samples into memorized and non-memorized groups using curated datasets with ground-truth labels. The method analyzes disparities between these groups from three perspectives: text features (word length, POS, prompt length), probability distributions (mean and variance of token probabilities), and hidden state representations (mean and variance of hidden states). The approach uses datasets like IDIOMIM and CelebrityParent with known answers to avoid direct training data access while still identifying memorization patterns.

## Key Results
- Longer prompts are more likely to be memorized, while longer words are less memorized
- Memorized samples exhibit higher mean and lower variance in probability distributions
- Memorized groups show smaller means and variances in hidden state representations
- Nouns show moderate memorization compared to other POS categories
- Model version sensitivity affects memorization patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ROME probes memorization by comparing features between memorized and non-memorized groups without accessing training data.
- Mechanism: The model uses datasets with ground-truth labels to categorize samples, then analyzes disparities in text, probability, and hidden state representations between the two groups.
- Core assumption: Memorization can be reliably detected by comparing model behavior on curated datasets with known answers rather than training data comparison.
- Evidence anchors:
  - [abstract] "ROME categorizes samples into memorized and non-memorized groups using extra datasets with ground-truth labels"
  - [section] "Instead of comparing model outputs with training corpora, ROME categorizes samples into memorized and non-memorized groups using extra datasets with ground-truth labels"
- Break condition: If the curated datasets don't capture the true memorization behavior or if the ground-truth labeling is unreliable, the comparisons would be meaningless.

### Mechanism 2
- Claim: Longer prompts increase memorization likelihood while longer words decrease it.
- Mechanism: The model's ability to recall information is influenced by prompt length (more context aids recall) and word complexity (longer words are harder to memorize).
- Core assumption: The relationship between prompt length/word length and memorization is causal rather than correlational.
- Evidence anchors:
  - [section] "longer prompt are more possible to be memorized or tend to help model recall more memorized knowledge"
  - [section] "as the length of the last word increases, the ratio of non-memorized samples increase sharply"
- Break condition: If the observed relationship is actually driven by other factors (like word frequency or semantic complexity) rather than length itself.

### Mechanism 3
- Claim: Memorized samples exhibit higher mean and lower variance in probability distributions, and smaller means and variances in hidden states.
- Mechanism: The model's confidence (probability) and representation (hidden state) patterns differ systematically between memorized and non-memorized content.
- Core assumption: These statistical differences in probability and hidden state distributions are reliable indicators of memorization.
- Evidence anchors:
  - [section] "memorized samples typically exhibit higher mean and decreased variance scores" for probabilities
  - [section] "memorized groups generally exhibiting lower mean and variance values" for hidden states
- Break condition: If these patterns are actually artifacts of the generation process or dataset characteristics rather than memorization itself.

## Foundational Learning

- Concept: Ground-truth labeling and dataset curation
  - Why needed here: ROME relies on curated datasets with known answers to categorize memorized vs non-memorized samples without accessing training data
  - Quick check question: How would you construct ground-truth labels for a dataset where the "correct" answer depends on training data memorization?

- Concept: Statistical analysis of model outputs
  - Why needed here: The method requires understanding how to analyze and compare probability distributions and hidden state representations between groups
  - Quick check question: What statistical tests would you use to determine if differences in mean/variance between memorized and non-memorized groups are significant?

- Concept: Token-level probability and hidden state extraction
  - Why needed here: The analysis requires accessing and comparing probability scores and hidden states at the token level during generation
  - Quick check question: How would you modify a generation loop to capture probability and hidden state information for each token?

## Architecture Onboarding

- Component map: Dataset curation module -> Model inference pipeline -> Feature extraction layer -> Comparative analysis engine

- Critical path:
  1. Dataset selection and ground-truth construction
  2. Sample categorization (memorized vs non-memorized)
  3. Feature extraction (text, probability, hidden state)
  4. Statistical comparison and visualization

- Design tradeoffs:
  - Using curated datasets vs direct training data comparison (privacy vs accuracy)
  - Token-level vs sequence-level analysis (granularity vs computational cost)
  - Manual vs automated ground-truth labeling (control vs scalability)

- Failure signatures:
  - No significant differences between memorized and non-memorized groups
  - Inconsistent categorization results across different prompt versions
  - Statistical patterns that don't align with expected memorization behavior

- First 3 experiments:
  1. Replicate the IDIOMIM dataset analysis to verify the prompt length effect
  2. Test different prompt versions on CelebrityParent to observe the model version effect
  3. Compare probability and hidden state distributions using different similarity metrics (cosine, Euclidean, inclination)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model versions (e.g., base vs. chat) influence the patterns of memorization observed in the study?
- Basis in paper: [explicit] The authors note that the base version of LLaMA 2 may exhibit enhanced responsiveness to few-shot prompts but diminished comprehension of prefix instructions compared to the chat version.
- Why unresolved: The study primarily uses the base version and does not explore the chat version's impact on memorization patterns, leaving a gap in understanding how model versions affect memorization.
- What evidence would resolve it: Conducting experiments with both base and chat versions of LLaMA 2 using the same datasets and prompts to compare their memorization patterns and responses.

### Open Question 2
- Question: Does the length of words in idioms have a consistent impact on memorization across different datasets and models?
- Basis in paper: [explicit] The study finds that longer words are less likely to be memorized, but this contradicts existing findings that atypical samples (possibly longer words) tend to receive higher memorization scores.
- Why unresolved: The study's findings on word length and memorization are specific to the IDIOMIM dataset and may not generalize to other datasets or models.
- What evidence would resolve it: Testing the relationship between word length and memorization across various datasets and models to determine if the pattern holds consistently.

### Open Question 3
- Question: How does word frequency influence memorization in models trained on deduplicated datasets?
- Basis in paper: [explicit] The study observes that identical names in reversal relations exhibit different memorization tendencies, challenging the notion that higher word frequency leads to enhanced memorization.
- Why unresolved: The study does not explore the impact of deduplication on word frequency's influence on memorization, leaving questions about how deduplication affects this relationship.
- What evidence would resolve it: Analyzing memorization patterns in models trained on both duplicated and deduplicated datasets to assess the impact of word frequency on memorization.

### Open Question 4
- Question: How do different decoding strategies (e.g., greedy, beam search, top-k sampling) affect the memorization characteristics of large language models?
- Basis in paper: [explicit] The study uses greedy decoding to obtain probabilities and hidden states, but acknowledges that other decoding strategies exist and may influence memorization.
- Why unresolved: The study does not compare memorization patterns across different decoding strategies, leaving uncertainty about how these strategies impact memorization.
- What evidence would resolve it: Experimenting with various decoding strategies on the same datasets to compare their effects on memorization patterns and characteristics.

## Limitations

- Ground-truth labeling reliability is critical but not validated
- Method may be sensitive to prompt engineering rather than capturing fundamental memorization
- Limited evidence challenges established findings about word frequency and memorization

## Confidence

**High Confidence**: Memorized samples show higher mean and lower variance in probability distributions - well-supported by data analysis methodology

**Medium Confidence**: Prompt length affects memorization likelihood - moderately supported but could be confounded by other factors

**Low Confidence**: Word frequency doesn't enhance memorization - based on limited evidence from single model and contradicts established research

## Next Checks

1. Conduct ground-truth validation study with human annotators to verify label reliability and identify systematic biases in the ground-truth construction process

2. Test the same methodology on at least two additional LLMs (e.g., GPT-3.5, Mistral) to determine whether observed patterns are model-specific artifacts or general memorization phenomena

3. Systematically vary prompt templates, lengths, and formats beyond just v1 and v2 to quantify how sensitive the memorization detection is to prompt engineering