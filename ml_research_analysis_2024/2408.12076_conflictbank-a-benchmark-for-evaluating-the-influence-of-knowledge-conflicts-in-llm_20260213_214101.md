---
ver: rpa2
title: 'ConflictBank: A Benchmark for Evaluating the Influence of Knowledge Conflicts
  in LLM'
arxiv_id: '2408.12076'
source_url: https://arxiv.org/abs/2408.12076
tags:
- subject
- conflict
- evidence
- knowledge
- conflicts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ConflictBank is a benchmark dataset for evaluating how large language
  models handle knowledge conflicts. It introduces 7.4M claim-evidence pairs and 553K
  QA pairs covering three conflict causes: misinformation, temporal discrepancies,
  and semantic divergences.'
---

# ConflictBank: A Benchmark for Evaluating the Influence of Knowledge Conflicts in LLM

## Quick Facts
- arXiv ID: 2408.12076
- Source URL: https://arxiv.org/abs/2408.12076
- Authors: Zhaochen Su; Jun Zhang; Xiaoye Qu; Tong Zhu; Yanshu Li; Jiashuo Sun; Juntao Li; Min Zhang; Yu Cheng
- Reference count: 40
- One-line primary result: Introduces ConflictBank, a benchmark dataset with 7.4M claim-evidence pairs and 553K QA pairs to evaluate how LLMs handle knowledge conflicts

## Executive Summary
This paper introduces ConflictBank, a comprehensive benchmark for evaluating how large language models handle knowledge conflicts arising from misinformation, temporal discrepancies, and semantic divergences. The dataset is constructed from 2.86M Wikidata claims, generating 7.45M claim-evidence pairs and 553K QA pairs with rigorous quality controls. Experiments across 12 models from four families show that models are highly receptive to external evidence, more sensitive to temporal and semantic conflicts than explicit misinformation, and that larger models are more susceptible to conflicting knowledge. The work demonstrates that models rely more on retrieved knowledge when internal conflicts exist, and that detailed descriptions in questions improve faithfulness to external evidence.

## Method Summary
ConflictBank is constructed through a multi-step pipeline: extracting structured facts from Wikidata, creating conflict claims via entity substitution (misinformation), adding future timestamps (temporal conflicts), or modifying descriptions (semantic conflicts), generating diverse textual evidence using LLMs with different text styles, and applying quality controls including entailment checking and conflict confirmation. The dataset enables evaluation of three conflict scenarios: conflicts in retrieved knowledge, conflicts within embedded knowledge, and their interplay. Models are pre-trained with varying proportions of conflict data and evaluated using metrics including Memorization Ratio (MR), Original Answer Ratio (OAR), and Counter Answer Ratio (CAR).

## Key Results
- Models are highly receptive to external evidence, with detailed question descriptions significantly improving faithfulness
- Temporal and semantic conflicts have greater impact on model behavior than explicit misinformation
- Larger models show higher susceptibility to conflicting knowledge
- Models rely more on retrieved knowledge when internal conflicts exist
- The interplay between embedded and retrieved conflicts reveals complex model behavior patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark dataset construction successfully simulates realistic knowledge conflicts by combining factual extraction from Wikidata with controlled conflict generation.
- Mechanism: The dataset leverages a multi-step process: extracting structured facts from Wikidata, creating conflict claims via entity substitution, generating diverse textual evidence with LLMs, and applying quality controls (entailment checking, conflict confirmation).
- Core assumption: Wikidata facts are reliable, diverse, and representative of real-world knowledge; generative models can produce coherent and stylistically varied evidence.
- Evidence anchors:
  - [abstract] states the benchmark is built from 2.86M Wikidata claims, generating 7.45M claim-evidence pairs and 553K QA pairs.
  - [section] describes the pipeline: fact extraction → conflict claim construction → evidence generation → quality control.
  - [corpus] shows the dataset is positioned alongside other conflict detection and resolution works, indicating relevance and alignment with current research needs.
- Break condition: If Wikidata contains systematic biases or errors, or if generative models produce low-quality or non-diverse evidence, the dataset's realism and usefulness degrade.

### Mechanism 2
- Claim: The dataset effectively captures three distinct conflict causes (misinformation, temporal, semantic) and enables their systematic evaluation.
- Mechanism: By substituting entities for misinformation, adding future timestamps for temporal conflicts, and modifying descriptions for semantic conflicts, the dataset isolates and represents each conflict type distinctly.
- Core assumption: These three conflict types cover the main real-world scenarios where knowledge conflicts arise.
- Evidence anchors:
  - [abstract] identifies the three conflict causes and their simulation methods.
  - [section] explains how each conflict type is constructed and why it matters for model evaluation.
  - [corpus] lists related works that also focus on conflict detection, showing that this multi-type approach is novel and necessary.
- Break condition: If other important conflict types (e.g., logical inconsistency, source credibility) are omitted, the benchmark may miss critical model weaknesses.

### Mechanism 3
- Claim: The benchmark allows evaluation of both retrieved and embedded knowledge conflicts and their interplay, providing comprehensive insights into model behavior.
- Mechanism: The dataset structure supports experiments where models face conflicts only in external evidence, only in their parametric memory (via continual pre-training), or both simultaneously, enabling controlled isolation of effects.
- Core assumption: Models can be reliably fine-tuned to embed conflicts and their behavior in each scenario can be meaningfully compared.
- Evidence anchors:
  - [abstract] explicitly states the benchmark covers conflicts in retrieved knowledge, embedded knowledge, and their interplay.
  - [section] describes the experimental setups for each scenario and the metrics used (OAR, CAR, MR).
  - [corpus] shows the work is situated among studies of retrieval-augmented generation and knowledge conflict resolution, supporting the comprehensiveness claim.
- Break condition: If continual pre-training does not effectively embed conflicts or if model responses are too variable, the interplay experiments may not yield clear insights.

## Foundational Learning

- Concept: Entailment checking between claims and evidence.
  - Why needed here: Ensures that generated evidence actually supports the corresponding claim, maintaining dataset quality and preventing spurious conflicts.
  - Quick check question: If a claim is "Anne Hathaway received the Hugo Award" and the evidence says she won an Emmy, does the entailment model flag this as a mismatch?

- Concept: Conflict confirmation between pieces of evidence.
  - Why needed here: Guarantees that default and conflict evidence truly contradict each other, which is essential for valid conflict experiments.
  - Quick check question: If default evidence says "X is from France" and conflict evidence says "X is from Germany", does the conflict classifier correctly identify them as conflicting?

- Concept: Memorization ratio (MR) as a metric for conflict handling.
  - Why needed here: MR quantifies how often models rely on their internal knowledge versus external evidence when conflicts are present, directly measuring susceptibility to conflicts.
  - Quick check question: If a model answers with its internal knowledge despite contradictory evidence, does MR increase or decrease?

## Architecture Onboarding

- Component map: Wikidata fact extraction → Claim construction module → LLM evidence generation → Quality control pipeline (entailment check + conflict confirmation) → QA pair creation → Dataset release + evaluation tools.
- Critical path: 1. Extract facts from Wikidata. 2. Generate conflict claims and corresponding evidence. 3. Apply entailment checking to filter non-supporting evidence. 4. Apply conflict confirmation to ensure true contradictions. 5. Construct QA pairs and release dataset with evaluation scripts.
- Design tradeoffs: Using Wikidata ensures factual grounding but may introduce biases or gaps. Generative evidence creation is efficient and scalable but may produce hallucinated or inconsistent content if not carefully controlled. Manual quality checks are accurate but time-consuming; automated checks speed up processing but may miss subtle issues.
- Failure signatures: High rejection rate during entailment checking indicates evidence-generation models are not producing relevant or coherent evidence. Low conflict confirmation accuracy suggests the conflict classifier is not reliably detecting contradictions, undermining experimental validity. Low memorization ratios across all models might indicate the conflicts are not challenging enough or evidence is too dominant.
- First 3 experiments: 1. Evaluate model memorization ratio (MR) when given only conflicting external evidence to measure susceptibility. 2. Evaluate MR when given both default and conflicting evidence to measure confirmation bias and evidence order effects. 3. Pre-train models with embedded conflicts and measure how this affects reliance on external evidence when conflicts are presented.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model architectures (e.g., decoder-only vs encoder-decoder) handle knowledge conflicts differently?
- Basis in paper: [inferred] The paper tests four model families (GEMMA, LLaMA2, LLaMA3, QWEN) but doesn't analyze architectural differences within these results
- Why unresolved: The experiments group results by model family but don't specifically isolate architectural effects from other factors like scale or training data
- What evidence would resolve it: Direct comparison of decoder-only vs encoder-decoder models of similar scale trained on similar data, showing differential performance on conflict types

### Open Question 2
- Question: What is the minimum amount of conflicting data needed to measurably impact model performance?
- Basis in paper: [explicit] The paper tests ratios of 2:1, 1:1, and 2:3 (default:conflict) but doesn't explore finer-grained ratios or find the threshold
- Why unresolved: The paper shows performance degrades with more conflicts but doesn't identify the critical point where degradation begins
- What evidence would resolve it: Systematic testing of incremental ratios (e.g., 10:1, 5:1, 3:1, 2:1) to find the tipping point where OAR begins to decline

### Open Question 3
- Question: How do knowledge conflicts affect model calibration and confidence in answers?
- Basis in paper: [inferred] The paper uses OAR and CAR metrics but doesn't examine whether models become over/under-confident when facing conflicts
- Why unresolved: The paper focuses on answer accuracy but doesn't analyze probability distributions or calibration curves under conflict conditions
- What evidence would resolve it: Temperature-scaled probability distributions showing how confidence changes with different conflict types and evidence orders

## Limitations
- The benchmark relies on Wikidata as a factual foundation, which may introduce biases or coverage gaps
- Quality of generated evidence depends heavily on LLM performance and may contain subtle hallucinations
- Manual verification process for conflict confirmation limits scalability and may introduce human bias
- The dataset may not capture all real-world conflict types (e.g., logical inconsistency, source credibility)

## Confidence
- **High Confidence**: The three conflict types (misinformation, temporal, semantic) are clearly defined and their construction methods are explicitly described and reproducible.
- **Medium Confidence**: The claim that larger models are more susceptible to conflicting knowledge is supported by experiments, but could be influenced by confounding factors like training data distribution.
- **Medium Confidence**: The assertion that detailed question descriptions improve faithfulness to external evidence is demonstrated, but the effect size and generalizability across different domains need further validation.

## Next Checks
1. **Cross-dataset validation**: Test the same conflict evaluation methodology on an independent knowledge source (e.g., Freebase or Wikipedia-derived facts) to verify that results generalize beyond Wikidata.
2. **Temporal conflict edge cases**: Create controlled test cases with ambiguous or borderline temporal conflicts (e.g., events spanning multiple years) to evaluate model robustness and identify potential failure modes.
3. **Human evaluation of evidence quality**: Conduct a small-scale human study to rate the coherence, relevance, and diversity of the generated evidence across different text styles, validating the automated quality control metrics.