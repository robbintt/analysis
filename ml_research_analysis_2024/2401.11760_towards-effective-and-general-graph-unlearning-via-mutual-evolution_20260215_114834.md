---
ver: rpa2
title: Towards Effective and General Graph Unlearning via Mutual Evolution
arxiv_id: '2401.11760'
source_url: https://arxiv.org/abs/2401.11760
tags:
- unlearning
- graph
- megu
- performance
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes MEGU, a mutual evolution framework for graph
  unlearning that simultaneously evolves predictive and unlearning capabilities. The
  key idea is to leverage two modules: a predictive module that adjusts the original
  model and an unlearning module that generates predictions while offering forgetting
  capacity.'
---

# Towards Effective and General Graph Unlearning via Mutual Evolution

## Quick Facts
- arXiv ID: 2401.11760
- Source URL: https://arxiv.org/abs/2401.11760
- Reference count: 13
- Primary result: MEGU achieves 2.7%, 2.5%, and 3.2% average performance improvements over state-of-the-art baselines across feature, node, and edge unlearning tasks respectively

## Executive Summary
This paper proposes MEGU, a mutual evolution framework for graph unlearning that simultaneously evolves predictive and unlearning capabilities. The key idea is to leverage two modules: a predictive module that adjusts the original model and an unlearning module that generates predictions while offering forgetting capacity. Experiments on 9 benchmark datasets demonstrate MEGU achieves significant performance improvements over state-of-the-art baselines while reducing training time and space overhead by an average of 159.8x and 9.6x compared to retraining GNN from scratch.

## Method Summary
MEGU employs a mutual evolution framework where a predictive module adjusts the original model to retain reasoning capability while an unlearning module generates predictions for non-unlearning entities with forgetting capacity. The framework includes two key mechanisms: Adaptive High-Influence Neighborhood Selection (Ada. HIN) that identifies nodes highly influenced by unlearning entities using forward and inverse feature propagation, and Topology-Aware Unlearning Propagation (Topo. UP) that leverages the predictive module's reasoning capability and graph topological structure to generate reliable predictions. The unified optimization framework aligns with both prediction and unlearning requirements through a carefully designed loss function.

## Key Results
- Achieves 2.7%, 2.5%, and 3.2% average performance improvements over state-of-the-art baselines across feature, node, and edge unlearning tasks respectively
- Reduces training time and space overhead by an average of 159.8x and 9.6x compared to retraining GNN from scratch
- Demonstrates effectiveness across 9 benchmark datasets including Cora, CiteSeer, PubMed, and Amazon datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MEGU's mutual evolution design ensures complementary optimization between predictive and unlearning modules, leading to improved performance and efficiency.
- **Mechanism**: The predictive module adjusts the original model to retain reasoning capability, while the unlearning module generates predictions for non-unlearning entities with forgetting capacity. These modules interact through a unified optimization framework, allowing them to mutually enhance each other.
- **Core assumption**: The effectiveness of the predictive module in eliminating unlearning entity influence relies on the forgetting capability of the unlearning module, and vice versa.
- **Evidence anchors**:
  - [abstract]: "MEGU ensures complementary optimization in a unified training framework that aligns with the prediction and unlearning requirements."
  - [section]: "From the mutual evolution perspective, the effectiveness of the predictive module in eliminating the influence of unlearning entities relies on the forgetting capability of the unlearning module, and the reasoning capability of the predictive module is essential for the unlearning module to generate reliable predictions."
- **Break condition**: If the interaction between the predictive and unlearning modules is disrupted, the mutual evolution process may fail, leading to suboptimal performance.

### Mechanism 2
- **Claim**: MEGU's adaptive high-influence neighborhood selection (Ada. HIN) effectively identifies nodes highly influenced by unlearning entities, enabling targeted optimization.
- **Mechanism**: Ada. HIN uses forward and inverse feature propagation to obtain smoothed features from two perspectives. It then quantifies the difference using influence distance, which serves as a measure to adaptively select high-influence nodes.
- **Core assumption**: Nodes with significant changes in smoothed features when unlearning entities are reversed are highly influenced by unlearning entities.
- **Evidence anchors**:
  - [section]: "Due to the rich interactions in the GNNs, we need to identify the nodes that are highly influenced by unlearning entities. This is pivotal in forming an optimization objective that preserves predictive accuracy while reducing unlearning entity impacts."
  - [section]: "By considering the unique structural properties of different entities, our approach effectively mitigates the bias that arises from treating all nodes within a fixed neighborhood equally."
- **Break condition**: If the influence distance calculation is inaccurate or the threshold for selecting high-influence nodes is not well-tuned, the Ada. HIN mechanism may fail to identify the correct nodes.

### Mechanism 3
- **Claim**: MEGU's topology-aware unlearning propagation (Topo. UP) leverages the predictive module's reasoning capability and the graph's topological structure to generate reliable predictions for non-unlearning entities.
- **Mechanism**: Topo. UP combines the predictive module's output with self-supervised information and encourages smoothness over the distribution of labels. It uses a personalized PageRank approximation to propagate information across the graph.
- **Core assumption**: Connected nodes in a graph exhibit similar labels, aligning with the network's inherent homophily or assortative characteristics.
- **Evidence anchors**:
  - [section]: "This strategy considers both the topological structure and the self-supervised information L from the predictive module, which effectively integrates the predictive and unlearning modules while upholding the homophily assumption to improve predictions."
  - [section]: "The aforementioned process can be regarded as the materialization of the unlearning module leveraging the reasoning capacity of the predictive module to generate reliable predictions."
- **Break condition**: If the graph's homophily assumption is violated or the self-supervised information is not reliable, the Topo. UP mechanism may fail to generate accurate predictions.

## Foundational Learning

- **Concept**: Graph Neural Networks (GNNs)
  - **Why needed here**: GNNs are the backbone models used in MEGU to learn node representations and make predictions. Understanding GNNs is crucial for grasping the overall architecture and the challenges addressed by MEGU.
  - **Quick check question**: What are the key components of a typical GNN architecture, and how do they contribute to the learning process?

- **Concept**: Graph Unlearning (GU)
  - **Why needed here**: GU is the main problem that MEGU aims to solve. It involves removing the influence of specific nodes, edges, or features from a trained GNN model. Understanding the GU problem is essential for appreciating the motivation behind MEGU's design.
  - **Quick check question**: What are the main challenges in graph unlearning compared to traditional machine unlearning in other domains, such as computer vision?

- **Concept**: Mutual Evolution
  - **Why needed here**: Mutual evolution is the core concept behind MEGU's design. It refers to the idea that the predictive and unlearning modules can benefit from each other's capabilities, leading to improved performance and efficiency. Understanding mutual evolution is key to grasping MEGU's unique approach.
  - **Quick check question**: How does the mutual evolution mechanism in MEGU differ from traditional approaches that treat the predictive and unlearning modules separately?

## Architecture Onboarding

- **Component map**: Predictive Module -> Unlearning Module -> Ada. HIN -> Topo. UP
- **Critical path**: The predictive module and unlearning module interact through the mutual evolution framework. The Ada. HIN mechanism provides targeted optimization, while the Topo. UP mechanism ensures reliable predictions.
- **Design tradeoffs**: MEGU trades off some predictive accuracy for improved unlearning performance and efficiency. The mutual evolution design requires careful tuning of the loss function to balance the contributions of the predictive and unlearning modules.
- **Failure signatures**: If the mutual evolution process fails, the model may exhibit poor performance on non-unlearning entities or fail to effectively remove the influence of unlearning entities. If the Ada. HIN mechanism fails, the model may not target the correct nodes for optimization. If the Topo. UP mechanism fails, the model may generate unreliable predictions.
- **First 3 experiments**:
  1. Train MEGU on a small dataset with a simple GNN backbone (e.g., GCN) and a single unlearning request (e.g., node-level unlearning). Evaluate the performance on non-unlearning entities and the effectiveness of removing unlearning entity influence.
  2. Compare MEGU with a baseline approach (e.g., retraining from scratch) on a larger dataset with multiple unlearning requests. Measure the training time and memory usage of both approaches.
  3. Perform an ablation study to assess the individual contributions of the Ada. HIN and Topo. UP mechanisms to MEGU's overall performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of MEGU scale with increasingly large graph sizes and edge/node/attribute unlearning requests?
- **Basis in paper**: [explicit] The paper mentions evaluating on 9 benchmark datasets but doesn't explore scalability to larger graphs or higher percentages of unlearning entities beyond 10%.
- **Why unresolved**: The paper only tests up to 10% unlearning ratios and does not discuss performance on graphs significantly larger than the tested datasets.
- **What evidence would resolve it**: Experiments testing MEGU on graphs orders of magnitude larger than current benchmarks, and with unlearning ratios approaching 50% or higher.

### Open Question 2
- **Question**: How does MEGU perform on heterogeneous graphs with multiple node/edge types compared to homogeneous graphs?
- **Basis in paper**: [inferred] The paper focuses on homogeneous graph benchmarks and doesn't discuss how the mutual evolution framework would adapt to heterogeneous graph structures.
- **Why unresolved**: The current formulation assumes uniform node/edge types, and the adaptive neighborhood selection may need modification for multi-type graphs.
- **What evidence would resolve it**: Implementation and evaluation of MEGU on benchmark heterogeneous graphs like OGB-HET, showing comparative performance to homogeneous graph results.

### Open Question 3
- **Question**: What is the theoretical bound on unlearning effectiveness versus predictive performance in MEGU's mutual evolution framework?
- **Basis in paper**: [explicit] The paper demonstrates empirical trade-offs through hyperparameter κ but doesn't provide theoretical guarantees on the Pareto frontier between unlearning and prediction.
- **Why unresolved**: The paper shows empirical results but lacks formal analysis of the fundamental limits of balancing these two objectives.
- **What evidence would resolve it**: A theoretical framework proving bounds on the achievable trade-off between unlearning effectiveness and predictive accuracy, potentially using information-theoretic or statistical learning theory approaches.

## Limitations
- The mutual evolution framework's effectiveness depends heavily on the interaction between predictive and unlearning modules, with implementation details remaining unclear
- The adaptive high-influence neighborhood selection mechanism relies on assumptions about feature changes that may not hold in all graph structures, particularly those with low homophily
- The topology-aware unlearning propagation mechanism's reliance on the homophily assumption could be problematic for heterophilic graphs

## Confidence
- **High Confidence**: The claim that MEGU achieves 2.7%, 2.5%, and 3.2% average performance improvements over state-of-the-art baselines across feature, node, and edge unlearning tasks respectively
- **Medium Confidence**: The claim that MEGU reduces training time and space overhead by an average of 159.8x and 9.6x compared to retraining GNN from scratch
- **Medium Confidence**: The claim that the mutual evolution design ensures complementary optimization between predictive and unlearning modules

## Next Checks
1. **Ablation Study on Module Interaction**: Conduct an ablation study to isolate the contribution of the mutual evolution mechanism by training the predictive and unlearning modules separately, then together, to quantify the exact performance gain from their interaction.
2. **Cross-Dataset Homophily Testing**: Test MEGU on datasets with varying levels of homophily (including heterophilic graphs) to validate whether the topology-aware unlearning propagation mechanism's reliance on homophily assumptions holds across different graph types.
3. **Hyperparameter Sensitivity Analysis**: Perform a comprehensive sensitivity analysis on the key hyperparameters (κ, α values) across different dataset sizes and graph structures to determine the robustness of MEGU's performance to hyperparameter tuning.