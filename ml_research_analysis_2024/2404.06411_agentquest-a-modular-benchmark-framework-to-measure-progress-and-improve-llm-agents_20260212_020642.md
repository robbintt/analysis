---
ver: rpa2
title: 'AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve
  LLM Agents'
arxiv_id: '2404.06411'
source_url: https://arxiv.org/abs/2404.06411
tags:
- agent
- progress
- action
- rate
- agentquest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces AgentQuest, a modular benchmark framework
  for evaluating and improving LLM agents. AgentQuest addresses the limitations of
  existing benchmarks by providing a unified interface for connecting diverse agent
  architectures with various benchmarks, and by introducing two new evaluation metrics:
  progress rate and repetition rate.'
---

# AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents

## Quick Facts
- arXiv ID: 2404.06411
- Source URL: https://arxiv.org/abs/2404.06411
- Authors: Luca Gioacchini; Giuseppe Siracusano; Davide Sanvito; Kiril Gashteovski; David Friede; Roberto Bifulco; Carolin Lawrence
- Reference count: 9
- Primary result: AgentQuest introduces modular benchmark framework with progress rate and repetition rate metrics to improve LLM agent evaluation

## Executive Summary
AgentQuest is a novel benchmark framework designed to address the fragmentation in LLM agent evaluation by providing a unified interface that connects diverse agent architectures with various benchmarks. The framework introduces two new evaluation metrics—progress rate and repetition rate—that offer deeper insights into agent behavior beyond traditional success rate measurements. Through demonstrations on benchmarks like Mastermind, Lateral Thinking Puzzles, ALFWorld, and Sudoku, the authors show how these metrics can reveal behavioral patterns and guide architectural improvements, such as adding memory components to reduce repetition and improve success rates.

## Method Summary
The AgentQuest framework implements a modular Driver class that serves as the primary interface between agents and benchmarks, with standard reset and step methods that abstract environment interactions. Agents communicate through Observation and Action classes, while custom benchmarks implement driver interfaces and metric computation functions. The framework includes two novel metrics: progress rate tracks advancement toward solution milestones through scoring functions, and repetition rate quantifies action similarity to detect stuck behavior using similarity functions like Levenshtein distance or BERTScore. The system is designed for extensibility, allowing researchers to add new benchmarks and metrics without modifying existing agent implementations.

## Key Results
- AgentQuest framework successfully connects diverse agent architectures with multiple benchmarks through a unified interface
- Progress rate and repetition rate metrics provide actionable insights into agent behavior beyond traditional success rate
- Adding memory components to agent architecture improved Mastermind success rate by 13 percentage points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The modular driver interface enables compatibility between arbitrary agent architectures and diverse benchmarks with minimal programming effort.
- Mechanism: By abstracting the environment interaction through a unified `Driver` class with standard `reset` and `step` methods, any agent can interface with any benchmark without rewriting agent code for each new benchmark.
- Core assumption: All benchmarks can be modeled as environments with hidden states, observations, and actions conforming to the interface.
- Evidence anchors:
  - [abstract] "a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs"
  - [section] "Instead, AgentQuest exposes a single unified Python interface, i.e. the Driver and two classes reflecting the agent-environment interaction components (i.e. Observation, Action)."
  - [corpus] Weak evidence; corpus contains unrelated benchmarks but no direct mention of AgentQuest's modular driver approach.
- Break condition: If a benchmark cannot be modeled with hidden states and discrete observations/actions, or requires fundamentally different interaction patterns not expressible through the Driver interface.

### Mechanism 2
- Claim: Progress rate and repetition rate metrics provide actionable insights into agent behavior that traditional success rate metrics cannot capture.
- Mechanism: Progress rate tracks advancement toward solution milestones through scoring functions, while repetition rate quantifies action similarity to detect stuck behavior. Together they reveal whether failures stem from lack of progress or inefficient exploration.
- Core assumption: Agent behavior can be meaningfully decomposed into progress toward goal and repetition of actions, and these dimensions are orthogonal.
- Evidence anchors:
  - [abstract] "we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task"
  - [section] "we define the progress rate PRt : S → [0, 1] dependant of such scoring function, as an indication of how far the agent is from the goal" and "At execution step t, we consider the set of unique actions taken by the agent up to t − 1"
  - [corpus] No direct evidence in corpus about progress/repetition metrics specifically.
- Break condition: If agent behavior cannot be meaningfully measured by milestones or action similarity, or if the chosen similarity function poorly captures meaningful repetitions.

### Mechanism 3
- Claim: The framework's extensibility allows researchers to add new benchmarks and metrics without modifying existing agent implementations.
- Mechanism: By providing template functions and a clear interface specification, new benchmarks only need to implement the driver and metric computation functions, leaving agent code untouched.
- Core assumption: New benchmarks can conform to the existing interface design without requiring fundamental changes to the framework architecture.
- Evidence anchors:
  - [abstract] "we provide the Driver class with two mandatory methods: (i) the reset method initialises a new instance of the environment and returns the first observation; (ii) the step method performs one single execution step"
  - [section] "Depending on the benchmark, the progress rate might also decrease during the execution. Milestones can either be manually annotated, or internally computed."
  - [corpus] No corpus evidence about extensibility or benchmark addition process.
- Break condition: If new benchmark types require fundamentally different interaction patterns or metric computations that cannot be expressed through the existing template system.

## Foundational Learning

- Concept: Agent-environment interaction in reinforcement learning terminology (environment, state, observation, action)
  - Why needed here: The paper explicitly builds on RL terminology to frame how agents interact with benchmarks, making this foundational understanding necessary to grasp the interface design.
  - Quick check question: What is the difference between an environment's hidden state and an observation in the RL framework?

- Concept: Metric design and evaluation in AI benchmarking
  - Why needed here: Understanding how traditional metrics like success rate work and their limitations is crucial to appreciate why progress rate and repetition rate are valuable additions.
  - Quick check question: Why is success rate alone insufficient for debugging agent architectures?

- Concept: Similarity metrics and string comparison (Levenshtein distance, BLEU, ROUGE, BERTScore)
  - Why needed here: The repetition rate metric relies on computing action similarity, requiring understanding of available similarity measures and their appropriate use cases.
  - Quick check question: When would you choose Levenshtein similarity over BERTScore for comparing agent actions?

## Architecture Onboarding

- Component map: Driver class -> Observation class -> Action class -> Metric functions -> Custom benchmark implementations
- Critical path: Agent → Action → Driver.step() → Environment → Observation → Agent
  - The driver mediates all communication between agent and environment
- Design tradeoffs:
  - Flexibility vs. simplicity: The unified interface enables broad compatibility but may not capture all benchmark-specific nuances
  - Granularity vs. usability: Progress rate requires defining meaningful milestones, which may be subjective
  - Computation cost vs. insight: Tracking action history for repetition rate adds overhead but provides debugging value
- Failure signatures:
  - Agent fails to interface: Check if agent outputs conform to Action class expectations
  - Metrics produce NaN or errors: Verify milestone definitions and similarity function implementations
  - Driver.step() hangs: Ensure benchmark implementation properly handles all possible actions
- First 3 experiments:
  1. Implement a simple benchmark (like Mastermind) and verify basic agent-driver interaction works
  2. Add progress rate computation with manually defined milestones and validate against known solutions
  3. Implement repetition rate with a simple similarity function and test on benchmarks where repetition behavior is expected vs. unexpected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the progress rate metric behave in more complex environments with a larger state space or longer solution paths?
- Basis in paper: [inferred] The paper demonstrates the progress rate on relatively simple benchmarks like Mastermind and Sudoku, but does not explore its behavior in more complex environments.
- Why unresolved: The current evaluation focuses on a limited set of benchmarks, and it is unclear how the progress rate metric scales to more complex tasks.
- What evidence would resolve it: Evaluating the progress rate on a diverse set of complex environments with varying state spaces and solution lengths would provide insights into its scalability and applicability.

### Open Question 2
- Question: Can the repetition rate metric be adapted to capture more nuanced forms of repetition, such as repeating similar but not identical actions?
- Basis in paper: [explicit] The paper mentions that the repetition rate currently uses a similarity function to classify repeated actions, but does not explore more advanced methods for capturing nuanced repetition.
- Why unresolved: The current implementation of the repetition rate metric may not capture all forms of repetition that could be relevant for understanding agent behavior.
- What evidence would resolve it: Exploring alternative similarity functions or developing new methods for capturing nuanced repetition would help improve the metric's ability to identify repetitive patterns in agent actions.

### Open Question 3
- Question: How do the progress and repetition rates interact with other evaluation metrics, such as success rate and time to success, in providing a comprehensive understanding of agent performance?
- Basis in paper: [explicit] The paper introduces the progress and repetition rates as complementary metrics to success rate, but does not extensively explore their interactions with other evaluation metrics.
- Why unresolved: While the progress and repetition rates provide valuable insights into agent behavior, their interactions with other metrics are not fully understood.
- What evidence would resolve it: Conducting a comprehensive evaluation that includes the progress and repetition rates alongside other metrics, such as success rate and time to success, would help elucidate their relationships and provide a more holistic view of agent performance.

## Limitations
- The Driver interface may not accommodate benchmarks requiring continuous action spaces or non-sequential interactions
- Progress rate computation depends on manually defined milestones, which may be subjective and domain-specific
- The framework's effectiveness across diverse agent architectures beyond the demonstrated LangChain agent with GPT-4 remains unproven

## Confidence
- Modular Driver Interface: Medium-High - Well-grounded in RL frameworks but universal applicability unproven
- Progress/Repetition Rate Metrics: Medium-High - Theoretically sound but practical effectiveness across diverse architectures needs validation
- Framework Extensibility: Medium - Clear design but real-world extensibility challenges may emerge

## Next Checks
1. Test AgentQuest's Driver interface with a benchmark requiring continuous action spaces (e.g., MuJoCo control tasks) to verify interface limitations.
2. Apply progress rate computation to a domain with ambiguous solution paths (e.g., creative writing tasks) to evaluate milestone definition challenges.
3. Compare repetition rate results using different similarity functions (Levenshtein, BERTScore, ROUGE) on the same benchmark to determine sensitivity to choice of metric.