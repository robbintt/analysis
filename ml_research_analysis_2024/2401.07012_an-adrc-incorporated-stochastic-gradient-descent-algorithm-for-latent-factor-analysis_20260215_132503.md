---
ver: rpa2
title: An ADRC-Incorporated Stochastic Gradient Descent Algorithm for Latent Factor
  Analysis
arxiv_id: '2401.07012'
source_url: https://arxiv.org/abs/2401.07012
tags:
- ieee
- learning
- latent
- error
- factor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an ADRC-incorporated SGD (ADS) algorithm to
  improve the convergence rate of latent factor analysis (LFA) on high-dimensional
  and incomplete (HDI) matrices. The key idea is to refine the instance learning error
  by considering historical and future states following the principle of an ADRC controller.
---

# An ADRC-Incorporated Stochastic Gradient Descent Algorithm for Latent Factor Analysis

## Quick Facts
- arXiv ID: 2401.07012
- Source URL: https://arxiv.org/abs/2401.07012
- Authors: Jinli Li; Ye Yuan
- Reference count: 40
- Key outcome: ADS-based LFA achieves up to 70% lower computational time and up to 0.26% lower RMSE compared to baseline methods

## Executive Summary
This paper proposes an Active Disturbance Rejection Control (ADRC)-incorporated Stochastic Gradient Descent (ADS) algorithm to improve the convergence rate of Latent Factor Analysis (LFA) on high-dimensional and incomplete (HDI) matrices. The key innovation is refining the instance learning error by considering historical and future states following ADRC principles, which includes tracking differentiator (TD), extended state observer (ESO), and error compensator (EC) components. Experiments on MovieLens 10M and Douban datasets demonstrate significant improvements in both computational efficiency and prediction accuracy compared to state-of-the-art LFA models.

## Method Summary
The proposed ADS algorithm integrates ADRC principles into standard SGD for LFA by refining the instance learning error using historical and future state information. The method employs a tracking differentiator to extract input values and their derivatives, an extended state observer to predict and cancel disturbances, and an error compensator to adjust the learning process. Each instance in the HDI matrix is controlled by an independent ADRC controller, with parameters tuned through a combination of theoretical derivation and empirical optimization. The algorithm is tested on two real-world datasets with 70% training, 20% testing, and 10% validation splits, using 5-fold cross-validation and stopping when RMSE difference falls below 10^-5 or 1000 iterations are reached.

## Key Results
- ADS-based LFA achieves up to 70% lower computational time compared to baseline methods
- Prediction accuracy improves with up to 0.26% lower RMSE on tested datasets
- The algorithm demonstrates robust performance across different matrix densities (0.22-0.31%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADS improves convergence by refining instance learning errors using both historical and future state information
- Mechanism: ADRC components (TD, ESO, EC) track and compensate for disturbances in the learning process, creating a more stable gradient update
- Core assumption: Learning error dynamics can be modeled as a control system with predictable disturbances
- Evidence anchors:
  - [abstract] "refine the instance learning error by considering historical and future states following the principle of an ADRC controller"
  - [section] "ADRC utilizes historical information to compute the proportion, integral, and derivation terms, making the current updates become more comprehensive and reasonable"
- Break condition: If learning error does not follow predictable patterns, ADRC compensation may introduce instability rather than improvement

### Mechanism 2
- Claim: ADRC's disturbance compensation directly reduces the variance in gradient estimates during SGD
- Mechanism: ESO observes and predicts disturbances in the learning process, allowing the algorithm to cancel them out before they affect updates
- Core assumption: Disturbances in SGD are systematic enough to be observed and compensated
- Evidence anchors:
  - [section] "ESO observes the disturbance present in the model and cancels it out in advance"
  - [section] "ADRC provides a more precise way to acquire the differential value of instant learning error"
- Break condition: If disturbances are too chaotic or unpredictable, ESO compensation may add noise rather than reduce it

### Mechanism 3
- Claim: ADS achieves faster convergence by incorporating temporal information into each update
- Mechanism: TD tracks the true value trajectory over iterations, allowing updates to be more informed about the learning process direction
- Core assumption: Learning trajectory has smooth temporal characteristics that can be tracked
- Evidence anchors:
  - [section] "TD is to extract the input value and its differential value"
  - [section] "We substitute the time point in ADRC with the training iterations in an SGD-based LFA model"
- Break condition: If learning trajectory is too erratic, TD tracking may lag and cause suboptimal updates

## Foundational Learning

- Concept: ADRC (Active Disturbance Rejection Control)
  - Why needed here: Provides the theoretical framework for handling disturbances and incorporating temporal information in SGD
  - Quick check question: What are the three main components of ADRC and their roles?

- Concept: Latent Factor Analysis (LFA)
  - Why needed here: The target application that benefits from improved SGD convergence
  - Quick check question: How does LFA model high-dimensional sparse data using low-rank matrix factorization?

- Concept: Stochastic Gradient Descent (SGD)
  - Why needed here: The optimization algorithm being enhanced with ADRC principles
  - Quick check question: What is the fundamental limitation of standard SGD that ADRC addresses?

## Architecture Onboarding

- Component map: HDI matrix -> LFA model -> ADS algorithm -> convergence check -> prediction
- Critical path: HDI matrix → LFA model → ADS algorithm → convergence check → prediction
- Design tradeoffs:
  - ADRC adds computational overhead but improves convergence rate
  - More ADRC parameters (b0, b1, b2, β1, β2, β3) increase tuning complexity
  - Tradeoff between disturbance compensation accuracy and update stability
- Failure signatures:
  - Divergence in training indicates ESO parameters too aggressive
  - No convergence improvement suggests ADRC parameters not well-tuned
  - Increased training time with no accuracy gain indicates overhead not justified
- First 3 experiments:
  1. Compare ADS vs standard SGD on convergence rate using ML10M dataset
  2. Test sensitivity to ADRC parameter tuning (b0, b1, b2 values)
  3. Evaluate disturbance compensation effectiveness by measuring gradient variance reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ADRC-based learning error refinement scale with the size of the known entry set |Λ| in terms of computational complexity?
- Basis in paper: [explicit] The paper states that each instance is controlled by an independent ADRC controller, implying a per-instance computation
- Why unresolved: The paper does not analyze the computational complexity of the ADS algorithm in relation to the number of known entries |Λ|
- What evidence would resolve it: Detailed computational complexity analysis comparing ADS with standard SGD in terms of iterations and per-iteration cost

### Open Question 2
- Question: What is the impact of ADRC controller parameter tuning (b0, b1, b2, β1, β2, β3) on the convergence behavior and prediction accuracy of the ADSL model?
- Basis in paper: [explicit] The paper mentions these parameters but does not provide a systematic study of their impact
- Why unresolved: The experimental results use fixed parameter values without exploring sensitivity or optimization of these parameters
- What evidence would resolve it: A parameter sensitivity analysis showing how different parameter settings affect convergence rate and RMSE

### Open Question 3
- Question: How does the ADSL model perform on extremely sparse matrices where the density is below 0.1% compared to the tested datasets with 0.22-0.31% density?
- Basis in paper: [inferred] The paper tests on datasets with relatively low density but does not explore the performance at extreme sparsity levels
- Why unresolved: The paper does not include experiments on matrices with lower density than the tested cases
- What evidence would resolve it: Experimental results on datasets with varying levels of sparsity, particularly below 0.1% density, comparing ADSL with baseline methods

## Limitations
- Limited comparison with recent state-of-the-art methods like neural collaborative filtering
- Computational efficiency gains demonstrated only on two specific datasets
- Limited sensitivity analysis for ADRC parameter tuning
- No ablation studies to isolate contributions of individual ADRC components

## Confidence

**Confidence Labels:**
- High confidence: The theoretical framework of ADRC components and their mathematical formulation
- Medium confidence: The experimental results showing RMSE improvements on tested datasets
- Low confidence: The generalizability of computational time savings across different problem domains

## Next Checks

1. Implement ablation studies to measure individual contributions of TD, ESO, and EC components to convergence improvement
2. Test the algorithm on additional matrix completion datasets with varying sparsity levels and dimensions
3. Compare against recent deep learning-based matrix factorization methods to establish state-of-the-art positioning