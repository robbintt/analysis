---
ver: rpa2
title: Improve Academic Query Resolution through BERT-based Question Extraction from
  Images
arxiv_id: '2405.01587'
source_url: https://arxiv.org/abs/2405.01587
tags:
- question
- questions
- images
- text
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to improve academic query resolution
  in Edtech organizations by extracting questions from text or images using a BERT-based
  deep learning model. Traditional approaches like rule-based methods have proven
  ineffective in accurately extracting questions from complex academic queries, often
  resulting in low accuracy and user dissatisfaction.
---

# Improve Academic Query Resolution through BERT-based Question Extraction from Images

## Quick Facts
- arXiv ID: 2405.01587
- Source URL: https://arxiv.org/abs/2405.01587
- Authors: Nidhi Kamal; Saurabh Yadav; Jorawar Singh; Aditi Avasthi
- Reference count: 0
- Primary result: BERT-based NER model achieves 96% precision and 83% recall for question extraction from academic images

## Executive Summary
This paper addresses the challenge of extracting questions from academic images in Edtech platforms, where traditional rule-based methods fail due to complex formatting and noisy content. The authors propose a BERT-based Named Entity Recognition approach that identifies question spans within OCR-extracted text using token classification with BIO tags. The method outperforms both rule-based and LayoutLM-based approaches in accuracy while being significantly smaller and faster. The model is fine-tuned on augmented data (~22k samples) created by adding noise text and random questions to the original dataset.

## Method Summary
The approach uses OCR to extract raw text from academic query images, then applies a BERT-based NER model to identify question spans using BIO tagging (B-Question, I-Question, O). The BERT model is fine-tuned on augmented data created by appending noise text and random questions to the original ~300-image dataset. The system is compared against rule-based and LayoutLM approaches using precision, recall, model size, and inference time as metrics. The BERT model is trained for 20 epochs with SGD optimizer, CrossEntropy loss, and 2e-5 learning rate.

## Key Results
- BERT-based model achieves 96% precision and 83% recall on validation set
- Model size: 107M parameters, inference time: 205ms per query
- Outperforms LayoutLM (133M parameters, 526ms) in both accuracy and efficiency
- Robust to noise through data augmentation with ~22k training samples

## Why This Works (Mechanism)

### Mechanism 1
The BERT-based NER model captures contextual dependencies between tokens, enabling accurate identification of question spans even when surrounded by noisy or non-question text. The model learns to assign BIO tags to tokens based on learned contextual embeddings, distinguishing question spans more effectively than deterministic rules or purely visual-textual models.

### Mechanism 2
Data augmentation with random noise and additional questions improves the model's robustness to real-world academic image inputs containing extraneous text or multiple questions. The synthetic expansion to ~22k samples teaches the model to filter out irrelevant content and focus on legitimate question spans.

### Mechanism 3
The combination of OCR text extraction followed by BERT-based token classification provides computational efficiency compared to multimodal models like LayoutLM. This avoids the need for both text and image features, reducing model size and inference time while maintaining accuracy.

## Foundational Learning

- **Named Entity Recognition (NER) with BIO tagging**: The task requires identifying and classifying contiguous spans of text (questions) within raw OCR output. Quick check: What do the BIO tags B-Question, I-Question, and O represent in the context of question extraction?

- **Transformer-based contextual embeddings**: BERT's pre-training on large text corpora allows it to generate embeddings that capture semantic and syntactic relationships between words, crucial for distinguishing questions from surrounding noise. Quick check: How does BERT's bidirectional attention mechanism improve its ability to understand the context of a word compared to unidirectional models?

- **Data augmentation techniques for NLP**: The original dataset is small (~300 images), so augmentation with synthetic noise and questions is necessary to create a robust model that generalizes to diverse real-world query formats. Quick check: Why is it important to include both noise text and additional questions in the augmentation process for this specific task?

## Architecture Onboarding

- **Component map**: Image → OCR → Text preprocessing → BERT tokenization → NER inference → BIO tag sequence → Question span reconstruction

- **Critical path**: Image → OCR → Text preprocessing → BERT tokenization → NER inference → BIO tag sequence → Question span reconstruction

- **Design tradeoffs**: Speed vs accuracy (BERT faster than LayoutLM but may be less effective on visually complex layouts); OCR dependency (pipeline quality depends on OCR output); augmentation vs overfitting (extensive augmentation helps with limited data but may introduce synthetic patterns)

- **Failure signatures**: High precision but low recall (model misses questions but doesn't generate false positives); low precision but high recall (model over-generates questions); consistently poor performance on certain image types (OCR may be failing on specific fonts, handwriting, or layouts)

- **First 3 experiments**: 1) Baseline evaluation: Run OCR on diverse academic images and measure raw text extraction accuracy; 2) Model comparison: Evaluate BERT NER on OCR-extracted text and compare precision/recall to rule-based and LayoutLM approaches; 3) Augmentation ablation: Train BERT with and without augmented data and measure impact on performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several remain unresolved:

- How does the performance of the proposed BERT-based question extraction model compare to human annotators in terms of precision and recall?

- How does the proposed method handle complex academic queries that involve mathematical equations, tables, or diagrams?

- How does the proposed method perform on queries in languages other than English, such as Indic languages?

- How does the proposed method handle queries with multiple questions that are semantically related or dependent on each other?

## Limitations

- Heavy reliance on OCR accuracy, which is not evaluated or reported in the paper

- Augmentation strategy is underspecified and lacks validation that synthetic data distribution matches real-world academic queries

- Evaluation metrics focus on question extraction precision and recall but don't measure impact on downstream answer accuracy or user satisfaction

## Confidence

**High Confidence**: BERT-based NER approach using BIO tagging is technically sound and well-supported by literature on transformer models for sequence labeling tasks.

**Medium Confidence**: Claim that augmentation improves robustness is plausible but lacks direct validation comparing performance with and without augmentation.

**Low Confidence**: Overall effectiveness for improving "academic query resolution" in Edtech organizations is not directly measured - the paper demonstrates question extraction capability but doesn't validate translation to better query handling or user outcomes.

## Next Checks

1. **OCR Quality Assessment**: Run complete pipeline on diverse academic images and measure OCR accuracy separately from question extraction, calculating character error rates and correlating OCR failures with question extraction failures.

2. **Augmentation Strategy Validation**: Train two BERT models - one with proposed augmentation and one without - and compare their performance on held-out test set of real academic images to quantify actual benefit of synthetic data generation.

3. **End-to-End Resolution Impact**: Measure how extracted questions affect downstream answer accuracy in Edtech system by comparing answer quality when using automatically extracted questions versus manually identified questions or user-provided questions.