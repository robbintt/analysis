---
ver: rpa2
title: 'GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation'
arxiv_id: '2404.03746'
source_url: https://arxiv.org/abs/2404.03746
tags: []
core_contribution: This paper proposes GenQREnsemble, a zero-shot ensemble prompting
  approach for query reformulation that improves retrieval effectiveness by generating
  multiple reformulations from paraphrased instructions. The method uses N paraphrases
  of a base instruction to prompt an LLM and generate diverse keyword expansions,
  which are appended to the original query.
---

# GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation

## Quick Facts
- **arXiv ID:** 2404.03746
- **Source URL:** https://arxiv.org/abs/2404.03746
- **Reference count:** 39
- **Key outcome:** Zero-shot ensemble prompting approach achieving up to 18% nDCG@10 and 24% MAP improvements over previous state-of-the-art methods

## Executive Summary
GenQREnsemble introduces a novel zero-shot ensemble prompting technique for query reformulation that generates multiple diverse reformulations by prompting large language models (LLMs) with N paraphrased versions of a base instruction. The approach constructs an ensemble by generating keyword expansions from each paraphrased instruction and appending them to the original query, effectively leveraging the LLM's generative capabilities without requiring fine-tuning or training data. An extension, GenQREnsembleRF, incorporates relevance feedback by including top-ranked documents as context during the reformulation process. The method demonstrates substantial improvements across four IR benchmarks, with relative gains of up to 18% in nDCG@10 and 24% in MAP over previous zero-shot approaches, while the relevance feedback variant achieves 5% MRR improvement with pseudo-relevance feedback and 9% nDCG@10 with relevant feedback documents.

## Method Summary
The GenQREnsemble approach works by first creating N paraphrased versions of a base instruction for query reformulation, then prompting an LLM separately with each paraphrased instruction to generate diverse keyword expansions. These expansions are systematically appended to the original query to create an ensemble of reformulated queries that are subsequently used for retrieval. The relevance feedback extension (GenQREnsembleRF) enhances this process by incorporating top-ranked documents as additional context in the LLM prompts, allowing the model to refine reformulations based on document content. The ensemble methodology aims to capture diverse reformulation perspectives while maintaining semantic equivalence across paraphrases, with the LLM serving as a zero-shot generator that doesn't require task-specific training.

## Key Results
- Achieved relative nDCG@10 improvements up to 18% compared to previous state-of-the-art zero-shot query reformulation methods
- Demonstrated relative MAP improvements up to 24% on four IR benchmarks, showing consistent effectiveness across datasets
- Relevance feedback extension (GenQREnsembleRF) showed 5% MRR improvement using pseudo-relevance feedback and 9% nDCG@10 improvement using relevant feedback documents

## Why This Works (Mechanism)
The approach leverages the diversity of paraphrased instructions to prompt the LLM into generating varied yet semantically related keyword expansions, effectively creating an ensemble that captures multiple reformulation perspectives. By using N different instruction variants, the method reduces the risk of the LLM fixating on particular reformulation patterns and instead encourages exploration of the semantic space around the original query. The ensemble aggregation ensures that high-quality expansions from any single paraphrase are preserved while the diversity helps cover different aspects of the information need. The relevance feedback extension further improves performance by providing the LLM with document context, enabling it to refine reformulations based on actual retrieved content rather than relying solely on the original query formulation.

## Foundational Learning
- **Query reformulation fundamentals** - Understanding how query expansion and modification techniques improve retrieval effectiveness by addressing vocabulary mismatch and underspecified queries; quick check: can you explain why simple term addition often improves precision without sacrificing recall?
- **Zero-shot prompting strategies** - Knowledge of how instruction-based prompting enables LLMs to perform tasks without fine-tuning; quick check: can you articulate the difference between zero-shot and few-shot prompting in LLM applications?
- **Ensemble methods in IR** - Understanding how combining multiple retrieval strategies or query variants typically improves robustness and effectiveness; quick check: can you identify scenarios where ensemble methods might underperform single strong models?
- **Relevance feedback mechanisms** - Knowledge of how incorporating user or system feedback about document relevance can iteratively improve query formulations; quick check: can you distinguish between explicit and implicit relevance feedback approaches?
- **Paraphrase generation techniques** - Understanding methods for creating semantically equivalent but lexically varied expressions of instructions or queries; quick check: can you evaluate whether a given paraphrase maintains semantic equivalence across different domains?
- **LLM inference cost modeling** - Ability to estimate computational costs associated with multiple LLM calls per query; quick check: can you calculate the cost implications of generating N reformulations for a production-scale query workload?

## Architecture Onboarding

**Component Map:** Base instruction -> Paraphrase generator -> N paraphrase variants -> LLM prompt generator -> LLM expansion generator -> Query ensemble builder -> Retrieval system -> (Optional) Relevance feedback processor -> Refined ensemble

**Critical Path:** Original query → Paraphrase generation → N LLM prompts → N expansion generations → Ensemble construction → Retrieval execution → Ranking → (Optional) Feedback incorporation → Final ranking

**Design Tradeoffs:** The approach trades computational cost (N LLM calls per query) for improved retrieval effectiveness through ensemble diversity; this represents a shift from traditional query expansion methods that typically use single expansion strategies or require training data for supervised approaches.

**Failure Signatures:** Poor paraphrase quality leading to semantically divergent reformulations; LLM generation failures producing irrelevant or redundant expansions; ensemble aggregation overwhelming the original query semantics; relevance feedback incorporating misleading document content.

**First Experiments:**
1. Test individual paraphrase variants independently to identify which instruction formulations produce the most effective expansions
2. Evaluate ensemble performance with varying N values to determine the optimal number of paraphrases for balancing effectiveness and cost
3. Compare pseudo-relevance feedback versus relevant feedback document incorporation to quantify the value of feedback quality on ensemble refinement

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The approach's scalability and computational cost for production deployment are not thoroughly explored, raising concerns about practicality for high-volume applications
- Evaluation is limited to four IR benchmarks without cross-domain generalization testing, making it unclear whether the paraphrased instruction approach maintains effectiveness across different query characteristics
- The method shows modest improvements with pseudo-relevance feedback (5% MRR) compared to relevant feedback (9% nDCG@10), suggesting sensitivity to feedback quality that may limit real-world applicability

## Confidence
- **Zero-shot effectiveness claims:** Medium - substantial improvements shown but measured only against previous zero-shot approaches, not supervised methods
- **Ensemble prompting mechanism:** Medium - novel combination of established concepts but lacks deep exploration of cost-benefit tradeoffs
- **Relevance feedback extension:** Medium - promising results with relevant feedback but modest gains with pseudo-relevance suggest limitations
- **Cross-domain generalization:** Low - no experiments conducted outside the four benchmark datasets to validate approach robustness

## Next Checks
1. Conduct cross-domain experiments to assess whether the paraphrased instruction approach maintains effectiveness when applied to domains with different query characteristics and information needs
2. Perform ablation studies to quantify the contribution of each paraphrase variant to the ensemble performance, and test whether fewer, carefully selected paraphrases could achieve similar results at lower computational cost
3. Evaluate the cost-effectiveness ratio by measuring both inference costs and retrieval performance improvements, comparing against both zero-shot and supervised baselines under realistic deployment constraints