---
ver: rpa2
title: 'ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies'
arxiv_id: '2403.01139'
source_url: https://arxiv.org/abs/2403.01139
tags:
- paragraph
- analogy
- analogies
- relations
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a pipeline for generating complex, paragraph-level
  analogies using large language models (LLMs), addressing the lack of scalable analogy
  datasets beyond simple word analogies. The method leverages GPT-3.5 and GPT-4 to
  generate analogy candidates, filter them via human annotation and auto-labeling,
  and create challenging distractors by altering causal order.
---

# ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies

## Quick Facts
- arXiv ID: 2403.01139
- Source URL: https://arxiv.org/abs/2403.01139
- Reference count: 36
- Primary result: Human-annotated analogy datasets beyond simple word analogies are scarce, limiting analogy-based research

## Executive Summary
The paper introduces ParallelPARC, a pipeline that leverages LLMs to generate complex, paragraph-level analogies at scale. By decomposing analogy generation into subject-relation identification and paragraph creation, the method produces high-quality analogies from the ProPara dataset of scientific processes. The pipeline combines human annotation with auto-labeling to create gold and silver sets, and generates challenging distractors by reversing event order. Experiments show humans outperform models by ~13% on analogy recognition tasks, with distractors significantly reducing LLM accuracy while remaining identifiable by humans.

## Method Summary
The ParallelPARC pipeline generates paragraph-level analogies by first identifying analogous subjects and their relations using GPT-3.5, then converting these relations into natural-language paragraphs. Human annotators validate a random sample of candidates, which serves as few-shot examples for GPT-4 to auto-label the full candidate pool. High-confidence predictions form the gold-set, while the remaining candidates create an automatic silver-set. Distractors are generated by reversing the order of dependent events in target paragraphs, creating challenging alternatives that confuse models but not humans.

## Key Results
- Humans outperform best models by ~13% on binary and multiple-choice analogy recognition tasks after minimal supervision
- Auto-labeling model achieves 85.1% accuracy and 79.5% precision on high-likelihood predictions
- 89% of generated distractors were correct and maintained logical coherence
- Silver-set enables rapid dataset expansion while gold-set ensures quality through human validation

## Why This Works (Mechanism)

### Mechanism 1
LLMs generate high-quality complex analogies when guided by structured relational prompts rather than full paragraph generation. By decomposing analogy generation into two steps—first identifying analogous subject and similar relations, then converting these relations into natural-language paragraphs—the model avoids repetitive or artificially sounding text.

### Mechanism 2
Human-annotated data trains an auto-labeling model that filters high-confidence analogy candidates, enabling scalable dataset creation. A small set of human-labeled analogy candidates serves as few-shot examples for GPT-4, which then labels the full candidate pool. High-confidence predictions are retained for the gold-set.

### Mechanism 3
Distractors created by reversing dependent event order in target paragraphs effectively challenge models while remaining identifiable by humans. For analogous paragraphs, two dependent events are identified and swapped, creating a paragraph that preserves first-order relations but alters higher-order causal structure.

## Foundational Learning

- Concept: Relational similarity vs. object attribute similarity
  - Why needed here: Understanding the distinction is critical for designing prompts that generate valid analogies and for evaluating whether candidate pairs are truly analogous.
  - Quick check question: In the analogy "electrical circuit : water pump :: electrons : water", what is the basis of similarity—object attributes or relations?

- Concept: In-context learning with LLMs
  - Why needed here: The pipeline relies on GPT-4's ability to learn from a few labeled examples without full fine-tuning, enabling rapid scaling of data generation.
  - Quick check question: What happens to GPT-4's accuracy on analogy classification if you double the number of few-shot examples from 5 to 10?

- Concept: Prompt decomposition
  - Why needed here: Breaking complex generation tasks into smaller, more focused prompts improves output quality and reduces repetition.
  - Quick check question: Why might asking GPT-3.5 to generate both base and target paragraphs in one prompt lead to lower diversity than separate prompts?

## Architecture Onboarding

- Component map: ProPara paragraphs (titles, domains, text) -> GPT-3.5 generates analogy candidates (subject + relations) -> Human annotation of random sample -> GPT-4 auto-labels candidates using human annotations as few-shot -> Human validation of high-confidence candidates (gold-set) + automatic silver-set -> GPT-4 generates distractors by event reordering -> ProPara-Logy dataset (gold + silver sets with positives, simple negatives, challenging distractors)

- Critical path: Input -> Step 1 -> Step 2 -> Step 3 -> Step 4 -> Output
  (Steps 1-3 are fully automated; Steps 2-4 require human involvement)

- Design tradeoffs:
  - Using GPT-3.5 for initial generation keeps costs low but may produce noisier relations
  - Human validation ensures gold-set quality but limits scalability
  - Automatic silver-set generation enables rapid expansion but with unknown accuracy
  - Event-reordering for distractors is simple to implement but may not cover all distractor types

- Failure signatures:
  - Low human annotation agreement indicates ambiguous analogy boundaries
  - Auto-labeling precision <80% suggests few-shot examples are unrepresentative
  - Distractor coherence <80% means event dependencies are not being correctly identified
  - Model performance on distractors close to random indicates distractors are not challenging enough

- First 3 experiments:
  1. Generate 100 analogy candidates with one-prompt approach vs. two-prompt approach; measure relation quality and paragraph diversity
  2. Test GPT-4 auto-labeling with 5, 10, and 20 few-shot examples; measure precision on high-confidence predictions
  3. Create distractors for 50 analogous pairs; have 3 annotators evaluate coherence and difficulty; calculate success rate

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- The pipeline's reliance on human validation for gold-set quality creates a fundamental scalability bottleneck
- Auto-labeling accuracy claims lack comparison to alternative approaches or statistical significance testing
- Event-reordering distractor strategy may not generalize beyond scientific processes with clear event dependencies

## Confidence
- High confidence: Pipeline architecture reproducibility; 13% human advantage in analogy recognition; distractor generation methodology
- Medium confidence: Auto-labeling model performance metrics; scalability claims for silver-set generation; generalizability beyond ProPara's scientific domain
- Low confidence: Quality of automatically generated analogies without human validation; effectiveness of event-reordering as universal distractor strategy; performance on analogy types beyond scientific processes

## Next Checks
1. Compare auto-labeling precision with and without domain-specific few-shot examples to determine if ProPara's scientific focus artificially inflates accuracy
2. Test distractor effectiveness on a held-out set of human-annotated analogies to verify the 89% coherence rate is not inflated by positive selection bias
3. Generate analogies using the pipeline on a non-scientific dataset (e.g., historical events or literary themes) and compare quality metrics to ProPara results