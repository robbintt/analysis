---
ver: rpa2
title: Training Spiking Neural Networks via Augmented Direct Feedback Alignment
arxiv_id: '2409.07776'
source_url: https://arxiv.org/abs/2409.07776
tags:
- function
- neural
- snns
- networks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training spiking neural networks
  (SNNs) by proposing an augmented direct feedback alignment (aDFA) method. This approach
  is biologically plausible and easy to implement physically, as it breaks the backpropagation
  chain rule and uses imprecise information.
---

# Training Spiking Neural Networks via Augmented Direct Feedback Alignment

## Quick Facts
- **arXiv ID**: 2409.07776
- **Source URL**: https://arxiv.org/abs/2409.07776
- **Reference count**: 40
- **Primary result**: Achieved 97.91% accuracy on MNIST and 87.20% on Fashion-MNIST using biologically plausible aDFA method

## Executive Summary
This paper proposes an augmented direct feedback alignment (aDFA) method for training spiking neural networks (SNNs), addressing the challenge of non-differentiable LIF neuron dynamics that make standard backpropagation difficult. The approach breaks the backpropagation chain rule by using fixed random feedback weights and arbitrary nonlinear backward functions instead of computing precise derivatives. Through systematic validation on MNIST and Fashion-MNIST tasks, the method demonstrates competitive performance while being more biologically plausible and hardware-friendly than traditional approaches.

## Method Summary
The aDFA method trains SNNs by injecting global error signals directly into each layer through fixed random matrices, replacing the derivative calculation with arbitrary positive nonlinear functions. The core innovation involves using random Fourier series (RFS) shifted to positive values (PRFS) as backward functions, which avoids interference with LIF neuron spike generation. A genetic algorithm optimizes PRFS parameters to discover effective backward function shapes, typically converging to bell-curve forms near the spike threshold. The method requires no accurate prior knowledge about the system and can be implemented without computing complex derivatives.

## Key Results
- Achieved 97.91% test accuracy on MNIST and 87.20% on Fashion-MNIST tasks
- Demonstrated competitive performance compared to backpropagation while being more biologically plausible
- Showed that arbitrary positive nonlinear functions can effectively replace derivatives in SNN training
- Validated the effectiveness of genetic algorithm optimization for discovering optimal backward functions

## Why This Works (Mechanism)

### Mechanism 1
aDFA breaks the backpropagation chain rule by using random fixed feedback weights and arbitrary nonlinear backward functions, enabling gradient-free training of SNNs. Instead of computing precise transposed weight matrices and derivatives, aDFA injects the global error directly into each layer through fixed random matrices. It replaces the derivative f' with arbitrary nonlinear functions g, removing the need for differentiable neuron dynamics. Random fixed feedback weights can carry error signals effectively, and arbitrary positive nonlinear functions that are not excessively negatively correlated with the derivative of the LIF neuron can serve as backward functions. If the backward function g is excessively negatively correlated with f' (η < -0.2), or if the random feedback weights are not fixed and properly initialized, the scheme fails.

### Mechanism 2
Using positive nonlinear functions (PRFS) instead of negative-valued functions prevents interference with LIF neuron spike generation. Negative values in backward functions can reverse weight update directions, affecting membrane potential accumulation and spike timing. Shifting PRFS to positive values avoids this interference. LIF neurons rely on positive membrane potential accumulation; negative backward signals disrupt the learning process. If backward functions are not constrained to positive values, or if the negative parts are not negligible compared to positive parts, training fails.

### Mechanism 3
Genetic algorithm (GA) optimization of PRFS parameters can discover backward functions with favorable shapes (e.g., "bell curve" near the threshold) that yield good performance. GA evolves random Fourier series parameters to maximize test accuracy, converging toward functions that resemble the derivative shape near the spike threshold. There exists a class of nonlinear functions whose shape near the threshold is more important than exact derivative matching for effective learning. If the GA population size or generation count is too small to explore the function space adequately, optimal functions may not be discovered.

## Foundational Learning

- **Concept**: Leaky Integrate-and-Fire (LIF) neuron dynamics
  - **Why needed here**: Understanding how membrane potential accumulates, spikes, and resets is essential to grasp why nondifferentiable dynamics make BP hard and why aDFA can bypass this.
  - **Quick check question**: In a LIF neuron, what happens to the membrane potential after a spike is generated?

- **Concept**: Direct Feedback Alignment (DFA) and its extension to aDFA
  - **Why needed here**: aDFA builds on DFA by replacing the derivative with arbitrary functions; knowing DFA's mechanism clarifies what aDFA changes.
  - **Quick check question**: How does DFA differ from standard backpropagation in terms of error propagation?

- **Concept**: Random Fourier Series (RFS) and correlation coefficient
  - **Why needed here**: RFS is used to generate candidate backward functions; correlation coefficient measures similarity to the derivative, guiding function selection.
  - **Quick check question**: What does a correlation coefficient of 0 between g and f' imply about their relationship?

## Architecture Onboarding

- **Component map**: Input -> LIF Forward Path -> Spike Generation -> Global Error Computation -> Fixed Random Feedback Matrices B -> Nonlinear Function g -> Weight Update
- **Critical path**: Forward pass → spike generation → compute global error → inject via B → modulate with g → update weights
- **Design tradeoffs**:
  - Using fixed random B simplifies hardware but may reduce convergence speed compared to BP
  - Arbitrary g functions ease biological plausibility but require careful shaping (e.g., positive, bell-like) to avoid training failure
  - GA optimization improves performance but adds computational overhead during hyperparameter search
- **Failure signatures**:
  - If backward functions contain negative values, membrane potential accumulation is disrupted
  - If g is too dissimilar to f' (low or highly negative correlation), learning stalls or diverges
  - If random matrices B are not properly scaled or initialized, error signals may be too weak or too strong
- **First 3 experiments**:
  1. Implement aDFA with a simple fixed g (e.g., ReLU or Gaussian) on a small SNN (784-100-10) on MNIST; verify learning occurs without derivative calculation
  2. Replace g with negative-valued RFS; observe training failure, confirming the need for positive functions
  3. Run GA to evolve PRFS parameters; compare performance against fixed g and standard BP

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of backward function g impact the generalization performance of aDFA-trained SNNs on tasks outside of MNIST and Fashion-MNIST? The study focuses on two specific image classification tasks and does not investigate the framework's performance on a wider range of problems or datasets. Testing the aDFA-SNNs framework on various datasets and tasks, such as object detection, speech recognition, or reinforcement learning, and comparing its performance to other training methods would resolve this.

### Open Question 2
What is the optimal balance between the complexity of the backward function g and the performance of the aDFA-SNNs scheme? While the paper demonstrates that different forms of g can achieve good performance, it does not provide a clear understanding of how the complexity of g affects the overall performance of the aDFA-SNNs scheme. Conducting a comprehensive study that varies the complexity of g and measures its impact on performance across multiple tasks and datasets would resolve this.

### Open Question 3
How does the aDFA-SNNs framework perform when combined with other biologically plausible learning rules, such as spike-timing-dependent plasticity (STDP)? The study investigates the aDFA method in isolation and does not examine its potential synergies or conflicts with other biologically plausible learning rules that are commonly used in SNNs. Implementing and testing the aDFA-SNNs framework in combination with STDP or other biologically plausible learning rules would resolve this.

## Limitations
- Performance validation limited to two image classification datasets (MNIST and Fashion-MNIST)
- Genetic algorithm optimization adds computational overhead during hyperparameter search
- Fixed random feedback weights may reduce convergence speed compared to backpropagation

## Confidence
- **High Confidence**: The mechanism by which aDFA breaks the backpropagation chain rule using fixed random feedback weights and arbitrary nonlinear functions is well-established in the literature and properly demonstrated here
- **Medium Confidence**: The requirement for positive backward functions is supported by experimental evidence, but the exact threshold for acceptable negative correlation (-0.2) needs further theoretical justification
- **Medium Confidence**: The GA optimization approach shows empirical success, but the generalizability of evolved function shapes to other tasks remains unproven

## Next Checks
1. Test aDFA with PRFS on deeper SNN architectures (beyond three layers) and on datasets like CIFAR-10 to verify scalability and robustness
2. Conduct ablation studies systematically varying the correlation threshold η to determine the precise range where training remains stable and effective
3. Implement aDFA without GA optimization using simple positive functions (ReLU, Gaussian) to verify if the evolved function shapes are truly necessary or if simpler alternatives suffice