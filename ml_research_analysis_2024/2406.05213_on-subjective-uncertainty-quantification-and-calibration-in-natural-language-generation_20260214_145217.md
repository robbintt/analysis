---
ver: rpa2
title: On Subjective Uncertainty Quantification and Calibration in Natural Language
  Generation
arxiv_id: '2406.05213'
source_url: https://arxiv.org/abs/2406.05213
tags:
- uncertainty
- language
- epistemic
- subjective
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses uncertainty quantification for free-form natural
  language generation, where defining task-specific uncertainty is challenging due
  to irrelevant variation in responses. The authors propose a Bayesian decision-theoretic
  framework that quantifies subjective uncertainty through expected utility of generated
  responses, calibrated using reliability diagrams and generalized expected calibration
  error.
---

# On Subjective Uncertainty Quantification and Calibration in Natural Language Generation

## Quick Facts
- arXiv ID: 2406.05213
- Source URL: https://arxiv.org/abs/2406.05213
- Authors: Ziyu Wang; Chris Holmes
- Reference count: 14
- Key outcome: This paper addresses uncertainty quantification for free-form natural language generation, where defining task-specific uncertainty is challenging due to irrelevant variation in responses. The authors propose a Bayesian decision-theoretic framework that quantifies subjective uncertainty through expected utility of generated responses, calibrated using reliability diagrams and generalized expected calibration error. They derive a principled epistemic uncertainty measure for in-context learning by connecting it to excess risk via a missing data perspective. Experiments on question answering and machine translation show the methods extract meaningful uncertainty estimates from GPT and Gemini models, reveal calibration differences across tasks, and demonstrate that epistemic uncertainty captures task-specific uncertainty levels (e.g., higher for low-resource languages).

## Executive Summary
This paper introduces a principled framework for quantifying and calibrating uncertainty in natural language generation (NLG) tasks using Bayesian decision theory. The key insight is that subjective uncertainty can be measured through expected utility of generated responses, where utility is characterized by a task-specific similarity measure. The framework provides both a way to quantify uncertainty and evaluate calibration using reliability diagrams and generalized expected calibration error. A novel epistemic uncertainty measure is derived by connecting in-context learning to excess risk through a missing data perspective. Experiments demonstrate the framework's effectiveness on question answering and machine translation tasks using GPT and Gemini models, showing that it can capture meaningful uncertainty patterns and calibration differences across tasks.

## Method Summary
The method uses Bayesian decision theory to quantify subjective uncertainty in NLG by defining utility through a similarity measure S(y', y; I) between generated and true responses. Expected utility is computed via Monte Carlo sampling from the model's predictive distribution, and subjective uncertainty is measured as the Bayes risk (minimum achievable risk). Calibration is evaluated by comparing expected utility with observed utility using reliability diagrams and generalized expected calibration error (gECE). Epistemic uncertainty is quantified through a missing data perspective that measures excess risk reducible with additional in-context learning samples. The framework is applied to question answering and machine translation tasks using large language models like GPT and Gemini with in-context learning.

## Key Results
- The methods successfully extract meaningful uncertainty estimates from GPT and Gemini models on question answering and machine translation tasks
- Reliability diagrams and gECE reveal calibration differences across tasks, with calibration improving as sample size increases
- Epistemic uncertainty captures task-specific uncertainty levels, showing higher uncertainty for low-resource languages like Tigrinya compared to high-resource languages like French
- The framework demonstrates that LLMs can be calibrated in a task-specific sense when using appropriate utility functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian decision theory allows defining task-specific uncertainty by connecting it to expected utility via a similarity measure S(y', y; I).
- Mechanism: The similarity measure S(y', y; I) serves as a utility function in a decision-theoretic framework. By maximizing expected utility under the model's predictive distribution p_M, we obtain a principled measure of subjective uncertainty as the Bayes risk (minimum achievable risk).
- Core assumption: The similarity measure S(y', y; I) can be cheaply evaluated and accurately reflects task-specific utility.
- Evidence anchors:
  - [abstract]: "starting from the assumption that our utility is characterized by a similarity measure that compares a generated response with a hypothetical true response"
  - [section 2.1]: "we have a NLG task: given prompt I ∈ I, generate a response y' ∈ Y where Y denotes the space of natural language responses; our utility can be measured through the similarity measure S(y', y; I) ∈ R"
  - [corpus]: Weak - corpus contains related works on uncertainty quantification but doesn't directly validate the S(y', y; I) utility assumption.
- Break condition: If the similarity measure S(y', y; I) cannot be accurately evaluated or doesn't reflect true task utility, the entire framework fails.

### Mechanism 2
- Claim: Calibration can be evaluated by comparing expected subjective utility with actual observed utility using reliability diagrams and generalized expected calibration error.
- Mechanism: The model is calibrated if the expected utility of its actions (based on p_M) matches the actual utility observed from the true data distribution. This is quantified through gECE which generalizes ECE to natural language generation tasks.
- Core assumption: The true data distribution p_0 can be approximated through Monte Carlo sampling from the model and observations.
- Evidence anchors:
  - [abstract]: "We discuss how this assumption enables principled quantification of the model's subjective uncertainty and its calibration"
  - [section 2.3]: "we find that LMs are calibrated in the sense of (3') only if gECE(p_M) := E_s|f_M(s) - s| = 0"
  - [corpus]: Weak - corpus mentions calibration works but doesn't specifically validate the reliability diagram approach for NLG.
- Break condition: If the approximation of p_0 is poor or the grouping function G(I) doesn't capture relevant task variations, calibration evaluation becomes unreliable.

### Mechanism 3
- Claim: Epistemic uncertainty can be quantified through a missing data perspective that measures excess risk reducible with additional in-context learning samples.
- Mechanism: The epistemic uncertainty measure compares the minimum risk achievable with current information versus the risk achievable with additional samples. This captures uncertainty about the model's knowledge that could be reduced with more data.
- Core assumption: The large language model approximates Bayesian inference and the likelihood function is identifiable given sufficient samples.
- Evidence anchors:
  - [abstract]: "We further derive a measure for epistemic uncertainty, based on a missing data perspective and its characterization as an excess risk"
  - [section 2.4]: "The proposed measure of epistemic uncertainty inspired by Fong et al. (2024); Xu and Raginsky (2022)"
  - [corpus]: Moderate - contains works on epistemic uncertainty but doesn't validate the specific missing data approach for black-box LMs.
- Break condition: If the model doesn't approximate Bayesian inference or the likelihood isn't identifiable, the epistemic uncertainty measure loses its theoretical justification.

## Foundational Learning

- Concept: Bayesian decision theory
  - Why needed here: Provides the mathematical framework for defining utility, risk, and uncertainty in a principled way that generalizes across tasks.
  - Quick check question: How does the Bayes risk relate to the variance of a predictive distribution in a simple regression task?

- Concept: Monte Carlo approximation
  - Why needed here: Enables practical computation of expected utility and risk measures when dealing with continuous or high-dimensional response spaces.
  - Quick check question: What happens to the Monte Carlo estimate of expected utility as the number of samples approaches infinity?

- Concept: Reliability diagrams and calibration
  - Why needed here: Provides the evaluation framework to verify that the model's uncertainty estimates are well-calibrated and trustworthy.
  - Quick check question: In a reliability diagram, what does it mean if the function f_M(s) consistently lies below the diagonal line?

## Architecture Onboarding

- Component map: Similarity/utility function S(y', y; I) -> Large language model p_M providing predictive distributions -> Monte Carlo sampling infrastructure -> Reliability diagram and gECE computation -> Epistemic uncertainty computation using additional ICL samples
- Critical path: For uncertainty quantification, the critical path is: prompt I → p_M(y|I) sampling → utility evaluation S(y', y; I) → expected utility calculation → risk/uncertainty measure computation.
- Design tradeoffs: The choice between Gibbs predictor (single sample) and MBR decoding (multiple samples) involves a tradeoff between computational efficiency and uncertainty quality. More samples improve uncertainty estimates but increase computational cost.
- Failure signatures: Poor calibration indicated by large gECE values, unrealistic uncertainty estimates (e.g., too high or too low relative to task difficulty), or epistemic uncertainty not decreasing with more in-context examples.
- First 3 experiments:
  1. Implement S(y', y; I) as simple exact match similarity and verify Bayes risk computation on a toy dataset
  2. Test calibration evaluation on a classification task with known ground truth to validate reliability diagram implementation
  3. Compare epistemic uncertainty estimates across tasks with known difficulty levels (e.g., high-resource vs low-resource languages)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the subjective uncertainty measure (2) compare to semantic uncertainty measures in terms of downstream task performance?
- Basis in paper: [explicit] The paper discusses how previous methods for "semantic uncertainty" can be adapted to a broader range of scenarios using the decision-theoretic perspective, but notes that being a measure of subjective uncertainty, it is not guaranteed to always improve on downstream tasks compared with heuristic alternatives, especially if the LM is less calibrated.
- Why unresolved: The paper mentions that subjective uncertainty measures may not always outperform heuristic alternatives, but does not provide empirical comparisons to validate this claim.
- What evidence would resolve it: Conducting experiments comparing the performance of subjective uncertainty measures to semantic uncertainty measures on various downstream tasks would provide insights into their relative effectiveness.

### Open Question 2
- Question: What is the impact of different choices of action spaces Y′_I on the quantification of subjective uncertainty and epistemic uncertainty?
- Basis in paper: [explicit] The paper discusses that different choices of predictors (Y′_I) should be matched with different uncertainty measures and that the choice of Y′_I can affect the quantification of epistemic uncertainty.
- Why unresolved: The paper does not provide a detailed analysis of how varying the action space Y′_I influences the results of uncertainty quantification.
- What evidence would resolve it: Experiments varying the size and composition of Y′_I across different tasks and observing the changes in uncertainty estimates would clarify the impact of this design choice.

### Open Question 3
- Question: How does the calibration of large language models differ across tasks with varying levels of resource availability?
- Basis in paper: [inferred] The paper observes that calibration improves as the sample size n increases and that the LM demonstrates higher epistemic uncertainty on lower-resource languages, suggesting a relationship between resource availability and model calibration.
- Why unresolved: While the paper hints at differences in calibration across tasks, it does not provide a comprehensive analysis of how resource availability affects model calibration.
- What evidence would resolve it: Conducting a systematic study comparing the calibration of models across tasks with different resource levels (e.g., high-resource vs. low-resource languages) would provide insights into this relationship.

## Limitations

- The framework's effectiveness depends heavily on the quality of the similarity measure S(y', y; I) as a proxy for task utility, which may not capture all aspects of utility for complex NLG tasks
- Monte Carlo approximations with relatively small sample sizes (20 for QA, 35 for MT) may introduce sampling bias and affect the reliability of uncertainty estimates
- The epistemic uncertainty measure's theoretical foundation assumes the LLM approximates Bayesian inference and that the likelihood is identifiable, which may not hold for all models or tasks

## Confidence

- **High confidence** in the theoretical framework connecting Bayesian decision theory to subjective uncertainty quantification and calibration. The mathematical derivations and decision-theoretic setup are sound.
- **Medium confidence** in the empirical validation. While the experiments demonstrate the methods can extract meaningful uncertainty estimates, the sample sizes are relatively small, and the results could benefit from more extensive testing across diverse tasks and model architectures.
- **Medium confidence** in the practical utility of the epistemic uncertainty measure. The missing data perspective is theoretically motivated, but its behavior in real-world scenarios with non-ideal model approximations needs further investigation.

## Next Checks

1. **Cross-model validation**: Apply the framework to multiple LLM architectures (including smaller models) to test robustness and identify whether the uncertainty estimates generalize across model families or are specific to the tested GPT and Gemini models.

2. **Ablation study on sample size**: Systematically vary the number of Monte Carlo samples used for utility estimation and epistemic uncertainty computation to quantify the impact on estimate quality and identify minimum viable sample sizes for reliable uncertainty quantification.

3. **Human evaluation of uncertainty calibration**: Conduct human studies where annotators assess whether the model's uncertainty estimates align with their own subjective uncertainty about generated responses, providing ground truth for calibration quality beyond the gECE metric.