---
ver: rpa2
title: Efficient Language Model Architectures for Differentially Private Federated
  Learning
arxiv_id: '2403.08100'
source_url: https://arxiv.org/abs/2403.08100
tags:
- learning
- federated
- cifg
- client
- scale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Scale-Invariant Coupled Input Forget
  Gate (SI-CIFG) recurrent neural network architecture for federated learning (FL)
  of language models on edge devices. By modifying the sigmoid and tanh activation
  functions to be scale-invariant, the proposed SI-CIFG model converges faster and
  achieves better utility than standard CIFG models in large-scale FL experiments.
---

# Efficient Language Model Architectures for Differentially Private Federated Learning

## Quick Facts
- arXiv ID: 2403.08100
- Source URL: https://arxiv.org/abs/2403.08100
- Reference count: 22
- Primary result: SI-CIFG model achieves better utility and faster convergence than standard CIFG in FL experiments

## Executive Summary
This paper introduces a scale-invariant Coupled Input Forget Gate (SI-CIFG) architecture for federated learning of language models on edge devices. By modifying the sigmoid and tanh activations to be scale-invariant using MaxN normalization, the proposed SI-CIFG model demonstrates faster convergence and better perplexity compared to standard CIFG models in large-scale FL experiments. The scale-invariant approach is also shown to benefit larger Transformer models and improve privacy-utility trade-offs in FL with differential privacy.

## Method Summary
The paper proposes SI-CIFG, a recurrent neural network architecture for federated learning that modifies standard CIFG cells with scale-invariant activations. The SI-σ and SI-tanh activations use ReLU followed by MaxN normalization to ensure scale invariance. This approach is also extended to Transformers by replacing softmax with scale-invariant attention. Experiments are conducted using FedAdam optimizer with DP-FTRL for privacy, training on Stack Overflow and live English virtual keyboard datasets.

## Key Results
- SI-CIFG achieves the best final perplexity and fastest convergence speed in federated learning experiments
- Standard CIFG models diverge at 2K rounds while SI-CIFG trains smoothly
- Scale-invariant modifications improve privacy-utility trade-offs in DP-FTRL experiments

## Why This Works (Mechanism)

### Mechanism 1
Scale-invariant activations stabilize training by normalizing intermediate gate values in CIFG cells. The SI-σ and SI-tanh activations use MaxN normalization, which divides each gate output by the maximum absolute value in the hidden dimension. This prevents the internal gate signals from growing or shrinking arbitrarily, keeping them in a stable range.

### Mechanism 2
Scale-invariant attention in Transformers reduces gradient variance by decoupling attention scores from weight magnitudes. Replacing softmax with ReLU + row-wise normalization removes the exponential dependence on dot-product scale, so large weight magnitudes do not dominate attention probabilities.

### Mechanism 3
Scale-invariant models are robust to heterogeneous client updates in federated learning. By normalizing gate activations and attention scores, the model's parameter updates become less sensitive to the absolute scale of gradients from different clients, reducing the impact of out-sized client updates.

## Foundational Learning

- **Federated Learning (FL)**: Distributed machine learning where clients train locally and share model updates. Why needed: The paper compares scale-invariant models in a cross-device FL setting. Quick check: In cross-device FL, what optimizer is typically used on clients due to memory constraints? (Answer: SGD)

- **Differential Privacy (DP)**: Framework for protecting individual data privacy during model training. Why needed: The experiments include DP-FTRL algorithm to provide formal privacy guarantees. Quick check: What is the purpose of clipping and adding noise to client updates in DP-FL? (Answer: To bound sensitivity and provide privacy guarantees)

- **Recurrent Neural Networks (RNNs) and LSTM/CIFG cells**: Neural architectures for sequential data. Why needed: The proposed SI-CIFG architecture modifies the standard CIFG cell. Quick check: What is the main advantage of CIFG over vanilla LSTM? (Answer: Fewer parameters due to coupled input and forget gates)

## Architecture Onboarding

- **Component map**: SI-CIFG cell (CIFG gates + SI-σ and SI-tanh) -> SI-Transformer (Transformer + SI-Attention + ReLU feedforward) -> DP-FTRL server (clipped, noisy client updates)

- **Critical path**: Local client training (SGD) -> Model update aggregation (DP-FTRL or FedAdam) -> Server model update -> Next round

- **Design tradeoffs**: Scale invariance vs. expressiveness (ReLU may reduce non-linearity), MaxN normalization (computationally cheap but can be unstable with small hidden dimensions), SI-Transformer (requires replacing softmax, may affect interpretability)

- **Failure signatures**: NaN or inf values in gate activations (MaxN denominator is zero), training divergence despite scale invariance (extreme client heterogeneity), slow convergence if ReLU activations saturate too early

- **First 3 experiments**:
  1. Implement SI-σ and SI-tanh for a small CIFG model; verify scale invariance by scaling input and checking that output is unchanged
  2. Replace softmax with SI-Attention in a Transformer; compare attention distributions before/after scaling weights
  3. Train SI-CIFG and base CIFG on a small federated dataset (e.g., Shakespeare); compare convergence speed and final perplexity

## Open Questions the Paper Calls Out

### Open Question 1
How does the scale-invariant modification impact the performance of other neural network architectures beyond CIFG and Transformer models? The paper demonstrates effectiveness for specific architectures but doesn't explore generalizability to other neural network types.

### Open Question 2
What are the trade-offs between model size and the benefits of scale-invariant modifications in federated learning? The paper discusses use in both small and large models but doesn't provide detailed analysis of how model size impacts benefits.

### Open Question 3
How does the scale-invariant approach affect the robustness of federated learning models to non-IID data distributions? The paper demonstrates improved convergence but doesn't specifically address robustness to non-IID data.

### Open Question 4
Can the scale-invariant approach be extended to other optimization algorithms beyond SGD and Adam in federated learning? The paper mentions compatibility with non-adaptive algorithms but doesn't explore other optimization methods.

## Limitations
- Limited testing on diverse datasets beyond Stack Overflow and virtual keyboard data
- Scale-invariant Transformer modifications proposed but not extensively evaluated in FL setting
- MaxN normalization's numerical stability across different hardware platforms not discussed

## Confidence

- **High confidence**: Core claim that scale-invariant activations improve federated language model training (supported by convergence results in both Stack Overflow and live device experiments)
- **Medium confidence**: Exact mechanism of how scale invariance interacts with differential privacy in FL (DP-FTRL experiments show trade-offs but don't thoroughly analyze impact on privacy guarantees)
- **Low confidence**: Performance generalization to other neural network architectures beyond CIFG and Transformer

## Next Checks

1. Reproduce SI-CIFG implementation with careful attention to MaxN normalization stability
2. Extend experiments to test SI-Transformer in cross-device FL setting
3. Analyze interaction between scale invariance and DP-FTRL clipping parameters