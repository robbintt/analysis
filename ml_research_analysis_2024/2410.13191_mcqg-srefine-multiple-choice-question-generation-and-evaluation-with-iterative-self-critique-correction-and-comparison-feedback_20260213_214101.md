---
ver: rpa2
title: 'MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative
  Self-Critique, Correction, and Comparison Feedback'
arxiv_id: '2410.13191'
source_url: https://arxiv.org/abs/2410.13191
tags:
- question
- context
- answer
- correct
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MCQG-SRefine improves large language model ability to generate
  high-quality, USMLE-style multiple-choice questions by integrating expert-driven
  prompts with iterative self-refinement (critique and correction). The framework
  significantly enhances question quality and difficulty, achieving a 72.5% human
  expert preference win rate and a 79.8% win rate against GPT-4 in automated evaluation.
---

# MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback

## Quick Facts
- arXiv ID: 2410.13191
- Source URL: https://arxiv.org/abs/2410.13191
- Reference count: 40
- Primary result: Improves LLM-generated USMLE-style MCQs with iterative self-critique, achieving 72.5% human expert preference win rate

## Executive Summary
MCQG-SRefine is a framework that enhances large language models' ability to generate high-quality, USMLE-style multiple-choice questions through expert-driven prompts and iterative self-refinement. The system uses a ColBERT retriever to gather relevant topics and test points, then generates and refines questions through critique-feedback cycles. Evaluation shows significant improvements in question quality and difficulty, with human experts preferring MCQG-SRefine outputs 72.5% of the time and the system achieving a 79.8% win rate against GPT-4 in automated evaluation.

## Method Summary
The MCQG-SRefine framework generates USMLE-style multiple-choice questions from medical cases using an iterative pipeline. It begins with expert-driven prompts to generate question components (context, question, answer, distractors, reasoning), then applies iterative self-critique and correction cycles up to 4 times or until a score threshold is met. The system uses a detailed rubric to evaluate each component, with the LLM revising based on feedback. Topics and test points from the USMLE Content Outline guide question generation, with expert-provided inputs producing higher quality and more difficult questions than machine-generated ones. An LLM-as-Judge metric with 10 key criteria provides scalable evaluation, achieving moderate reliability (Cohen's kappa 0.539) compared to expert assessment.

## Key Results
- Human expert preference win rate of 72.5% for MCQG-SRefine-generated questions
- Reduction in easy questions by 80% and fourfold increase in hard questions when using expert-provided topics and test points
- LLM-as-Judge metric achieves moderate reliability (Cohen's kappa 0.539) compared to expert evaluation
- 79.8% win rate against GPT-4 in automated evaluation

## Why This Works (Mechanism)

### Mechanism 1
Iterative self-critique and correction cycles improve question quality and difficulty. The system generates initial MCQs, then uses the same LLM to critique each component against a rubric. Based on the critique, the LLM revises the components, repeating up to 4 times or until a score threshold is met. The LLM provides meaningful, actionable feedback on its own output when guided by a detailed rubric.

### Mechanism 2
Using expert-identified topics and test points produces higher quality and more difficult questions than machine-generated ones. Topics and test points retrieved from official USMLE content outlines guide the LLM to focus on specific concepts. Human-annotated topics/keypoints provide clearer direction than model-identified ones.

### Mechanism 3
LLM-as-Judge with filtered criteria achieves moderate reliability (kappa 0.539) compared to expert evaluation, offering a scalable alternative. Initial critique criteria are filtered via correlation analysis with expert preferences, selecting 10 key aspects. LLM evaluates both rating and comparison modes, serving as a proxy for expert judgments.

## Foundational Learning

- **USMLE-style MCQ structure (stem, correct answer, distractors)**: The framework must generate questions matching the official format and difficulty progression used in medical licensing exams. Quick check: What are the five core components of a USMLE-style MCQ?

- **Large language model self-correction via critique-feedback loops**: Enables iterative refinement without costly human input, leveraging the model's reasoning for quality improvement. Quick check: How many rounds of critique-correction does MCQG-SRefine perform before stopping?

- **Cohen's kappa as a reliability metric for inter-rater agreement**: Used to quantify how closely the LLM-as-Judge aligns with human expert evaluations. Quick check: What kappa value range is considered "moderate" reliability?

## Architecture Onboarding

- **Component map**: Input layer (medical case, topic list, test points) -> Retriever (ColBERT) -> INIT pipeline (context → question → answer → distractors) -> QA step (answer with reasoning) -> CRITIQUE step (score components) -> CORRECTION step (revise components) -> LLM-as-Judge (evaluate outputs) -> Output (final MCQ)

- **Critical path**: 1. INIT generates all components in sequence, 2. QA provides additional feedback, 3. CRITIQUE scores each component, 4. CORRECTION refines components, 5. Repeat CRITIQUE-CORRECTION until threshold or max rounds, 6. LLM-as-Judge evaluates final outputs

- **Design tradeoffs**: More rounds → higher quality but diminishing returns and risk of over-critique; rubric complexity → better coverage but harder for LLM to interpret consistently; human vs. machine topics → higher quality but requires expert effort vs. scalable but noisier

- **Failure signatures**: Stagnation (scores plateau early, no meaningful improvement after round 2), Drift (components diverge from USMLE style or become incoherent), Bias (LLM-as-Judge shows high position bias or inconsistent preferences)

- **First 3 experiments**: 1. Run INIT alone on 10 cases, compare output quality to GPT-4 baseline, 2. Add CRITIQUE only (no correction), measure rubric score improvement, 3. Full MCQG-SRefine pipeline, evaluate human preference and difficulty shift

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of iterations for MCQG-SRefine to maximize question quality without over-self-critiquing? The paper shows performance varies by component and input length, but doesn't establish a universal stopping criterion. Systematic evaluation across different clinical note lengths and topic/keypoint quality levels would determine optimal iteration count for each scenario.

### Open Question 2
How does MCQG-SRefine perform on medical domains outside USMLE-style questions? The framework was only tested on USMLE-style questions from medical cases, limiting understanding of broader applicability. Comparative studies applying MCQG-SRefine to other medical education domains and non-medical educational contexts would address this.

### Open Question 3
What mechanisms can improve the reliability of LLM-as-Judge evaluation in specialized domains? Current filtering methods improved correlation but didn't achieve high reliability, and position bias remains a challenge. Development and validation of domain-specific evaluation protocols that combine multiple LLM models, external validation sets, and systematic bias mitigation strategies would resolve this.

## Limitations
- The framework's effectiveness depends heavily on the quality of expert-driven prompts and the rubric used for self-critique, with key specifications missing
- The study lacks a comprehensive analysis of potential biases in the LLM's self-critique process
- Generalizability beyond USMLE-style questions remains unproven due to narrow testing scope

## Confidence

- **High Confidence**: The iterative self-critique and correction mechanism is well-supported by the reported 72.5% human expert preference win rate
- **Medium Confidence**: The use of expert-identified topics and test points is supported by the reported 80% reduction in easy questions, but exact methodology is not fully specified
- **Low Confidence**: The LLM-as-Judge metric's reliability is based on moderate Cohen's kappa of 0.539, but filtering criteria and correlation methodology are not fully detailed

## Next Checks

1. Validate rubric effectiveness by testing with different LLMs or human evaluators to ensure consistent quality improvement
2. Assess ColBERT retriever performance across diverse medical cases to ensure scalability and relevance
3. Benchmark LLM-as-Judge reliability against multiple human expert panels to confirm moderate reliability and identify potential biases