---
ver: rpa2
title: 'FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation'
arxiv_id: '2405.07682'
source_url: https://arxiv.org/abs/2405.07682
tags: []
core_contribution: This paper introduces FastSAG, a diffusion-based method for fast,
  coherent, and high-quality singing accompaniment generation. Unlike previous autoregressive
  (AR) approaches, FastSAG employs a non-AR framework that directly generates the
  Mel spectrogram of the accompaniment using a diffusion model conditioned on input
  vocals.
---

# FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation

## Quick Facts
- arXiv ID: 2405.07682
- Source URL: https://arxiv.org/abs/2405.07682
- Authors: Jianyi Chen; Wei Xue; Xu Tan; Zhen Ye; Qifeng Liu; Yike Guo
- Reference count: 6
- One-line primary result: FastSAG generates high-quality, coherent singing accompaniments 30x faster than the state-of-the-art, achieving real-time generation with an RTF below 1.

## Executive Summary
FastSAG introduces a non-autoregressive diffusion-based framework for fast and high-quality singing accompaniment generation. Unlike previous autoregressive methods, it directly generates the Mel spectrogram of the accompaniment conditioned on input vocals using a carefully designed diffusion model. The framework employs a semantic projection block for high-level semantic alignment and a prior projection block for frame-level control, ensuring both semantic and rhythmic coherence. Experiments show that FastSAG outperforms the state-of-the-art SingSong method in both objective metrics (FAD) and subjective evaluations (MOS) while achieving over 30x speedup in generation.

## Method Summary
FastSAG is a non-autoregressive diffusion-based method that generates singing accompaniments by directly modeling the Mel spectrogram conditioned on input vocals. The model consists of a semantic projection block that maps vocal features to high-level semantic embeddings of the expected accompaniment, and a prior projection block that resamples and refines these embeddings into frame-level Mel-spectrogram priors. A conditional diffusion model (EDM) then generates the final accompaniment Mel spectrogram using these priors as conditioning. The model is trained with three loss functions: semantic loss, prior loss, and diffusion loss, weighted to balance semantic alignment and acoustic quality. Source-separated vocal-accompaniment pairs are used for training, and BigvGAN vocoder converts the generated Mel spectrograms to audio waveforms.

## Key Results
- FastSAG achieves FAD scores of 6.27, 8.11, and 7.38 (VGGish, MERT, CLAP-MUSIC) on MUSDB18, outperforming SingSong (FAD 10.32, 9.24, 8.41).
- Subjective MOS evaluations show FastSAG scores 4.03 on harmony and 3.91 on coherence, compared to SingSong's 3.82 and 3.69.
- FastSAG generates accompaniments in 0.45 seconds for 10-second audio, achieving a real-time factor below 1, which is over 30x faster than SingSong's 13.9 seconds.

## Why This Works (Mechanism)
### Mechanism 1
- Claim: The diffusion model can directly generate a coherent accompaniment by modeling the Mel spectrogram conditioned on vocal features.
- Mechanism: The model learns a probability flow ODE that transforms a noise sample into a Mel spectrogram aligned with the vocal input through a learned denoiser conditioned on high-level semantic features and frame-level priors.
- Core assumption: The Mel spectrogram is a sufficient continuous representation for modeling the accompaniment-audio relationship without autoregression.
- Evidence anchors:
  - [abstract] "A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly."
  - [section 4.1] "we design a diffusion-based non-AR framework here. In the continuous Mel spectrogram space, the EDM introduced in Section 3 takes the conditions containing the semantic and rhythmic information to generate the Mel spectrogram of the accompaniment."
  - [corpus] "Found 25 related papers" (weak signal for diffusion in singing accompaniment, but strong in general audio/music generation)
- Break condition: If the semantic projection or prior projection blocks fail to produce meaningful alignment cues, the diffusion model cannot recover coherent accompaniment structure.

### Mechanism 2
- Claim: Semantic projection followed by prior projection ensures both high-level and frame-level coherence between vocal and accompaniment.
- Mechanism: The semantic projection block maps vocal features to a high-level semantic embedding of the expected accompaniment. The prior projection block resamples and refines this semantic embedding to produce a Mel-spectrogram-like prior aligned in time with the vocal. This prior conditions the diffusion model.
- Core assumption: Modeling high-level semantic alignment first, then refining to frame-level, is easier than direct frame-level mapping.
- Evidence anchors:
  - [section 4.2] "we propose a semantic projection block for semantic alignment, and a prior projection block to enhance the frame-level alignment and control."
  - [section 4.2.2] "Considering that the shape (time resolution and feature dimension) of the semantic feature may differ from that of the desired frame-level prior, we design two kinds of resampling modules."
  - [corpus] Weak; no explicit evidence in related papers for this two-stage projection design.
- Break condition: If the resampling module fails to align temporal resolution, the prior becomes misaligned and the diffusion model produces incoherent accompaniment.

### Mechanism 3
- Claim: Combining semantic loss, prior loss, and diffusion loss yields better semantic and rhythmic alignment than using only diffusion loss.
- Mechanism: The semantic loss enforces the predicted accompaniment semantic to match the ground-truth accompaniment semantic. The prior loss enforces the predicted Mel-spectrogram prior to match the ground-truth Mel-spectrogram. The diffusion loss refines the final output conditioned on these two cues.
- Core assumption: Multi-task losses enforce alignment at multiple abstraction levels, improving overall coherence.
- Evidence anchors:
  - [section 4.4] "To make the model generate coherent and harmonious accompaniments... we design three different loss functions, including the semantic loss, prior loss, and diffusion loss."
  - [section 4.4] "L = Lsemantic × λs + Lprior × λp + Ldif f usion × λd"
  - [corpus] Weak; no explicit evidence in related papers for this specific loss combination in singing accompaniment generation.
- Break condition: If the weighting of losses is imbalanced, the model may overfit to one aspect (semantic or acoustic) and underperform in the other.

## Foundational Learning
- Concept: Diffusion models and stochastic differential equations (SDEs).
  - Why needed here: FastSAG uses an EDM diffusion model to sample accompaniment Mel spectrograms; understanding SDEs and probability flow ODEs is essential to implement and debug the sampling process.
  - Quick check question: In an EDM diffusion model, what is the role of the score function and how is it related to the denoising function?
- Concept: Conditional audio generation and representation alignment.
  - Why needed here: The method conditions on vocal-derived features and must align semantic and frame-level representations; understanding feature extraction, alignment, and conditioning mechanisms is critical.
  - Quick check question: How does the prior projection block transform high-level semantic features into frame-level priors, and why is resampling necessary?
- Concept: Loss function design for multi-objective training.
  - Why needed here: FastSAG uses three loss terms with different abstraction levels; understanding how to balance and combine such losses is necessary for effective training.
  - Quick check question: What is the effect of weighting λs, λp, and λd differently, and how might this impact semantic vs. acoustic coherence?

## Architecture Onboarding
- Component map: Source separation -> Semantic projection -> Prior projection -> Diffusion sampling -> Vocoder -> Output audio
- Critical path: Source separation → Semantic projection → Prior projection → Diffusion sampling → Vocoder → Output audio
- Design tradeoffs:
  - Non-autoregressive vs autoregressive: Non-AR diffusion is faster but may require more complex conditioning and careful alignment to maintain coherence.
  - Continuous Mel spectrogram vs discrete tokens: Mel spectrogram is easier to condition but may lose fine-grained structure compared to token-based methods.
  - Semantic vs acoustic conditioning: Semantic features are easier to align but may be too abstract for fine control.
- Failure signatures:
  - Poor FAD scores: Likely indicates misalignment or insufficient conditioning.
  - Slow generation: May indicate inefficient diffusion sampling or overly complex conditioning pipeline.
  - Audio quality issues: Could stem from source separation artifacts, vocoder limitations, or inadequate training data.
- First 3 experiments:
  1. Train and evaluate FastSAG with only semantic loss (no prior loss, no diffusion loss) to check semantic alignment baseline.
  2. Replace PerceiverIO with bilinear interpolation in prior projection block and compare FAD and MOS.
  3. Vary λs, λp, λd weights systematically and evaluate impact on FAD and subjective coherence scores.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The effectiveness of the semantic and prior projection blocks in maintaining alignment across diverse musical styles remains unverified beyond the specific test sets used.
- The speedup claim of 30x is based on generation speed alone and does not account for preprocessing, postprocessing, or system overhead.
- The claim of "real-time" generation with RTF < 1 is hardware-dependent and may not hold across different deployment environments.

## Confidence
- High confidence: The non-autoregressive diffusion framework is technically sound and the core components are well-defined and implementable.
- Medium confidence: The reported improvements in FAD and subjective scores are likely valid within the tested datasets, but may not generalize to broader musical contexts.
- Low confidence: The real-time generation claim is hardware-dependent and may not be consistent across different systems.

## Next Checks
1. Perform a genre-transfer evaluation: Train FastSAG on a subset of genres and test on held-out genres to assess robustness and generalization.
2. Isolate the contribution of the two-stage conditioning: Train a variant that skips the prior projection and directly conditions the diffusion model on semantic features; compare FAD and MOS to FastSAG.
3. Benchmark RTF on multiple hardware platforms (CPU, GPU, edge devices) to confirm the 30x speedup claim and assess deployment feasibility.