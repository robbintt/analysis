---
ver: rpa2
title: Prioritize Alignment in Dataset Distillation
arxiv_id: '2408.03360'
source_url: https://arxiv.org/abs/2408.03360
tags:
- information
- dataset
- distillation
- data
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of misaligned information in dataset
  distillation, which can compromise the quality of synthetic datasets. The authors
  propose Prioritize Alignment in Dataset Distillation (PAD), a two-step strategy
  to filter out misaligned information during both information extraction and embedding
  stages.
---

# Prioritize Alignment in Dataset Distillation

## Quick Facts
- arXiv ID: 2408.03360
- Source URL: https://arxiv.org/abs/2408.03360
- Authors: Zekai Li; Ziyao Guo; Wangbo Zhao; Tianle Zhang; Zhi-Qi Cheng; Samir Khaki; Kaipeng Zhang; Ahmad Sajedi; Konstantinos N Plataniotis; Kai Wang; Yang You
- Reference count: 40
- One-line primary result: State-of-the-art dataset distillation performance achieved through two-stage filtering approach

## Executive Summary
This paper addresses the problem of misaligned information in dataset distillation, which can compromise the quality of synthetic datasets. The authors propose Prioritize Alignment in Dataset Distillation (PAD), a two-step strategy to filter out misaligned information during both information extraction and embedding stages. For information extraction, PAD uses a data scheduler to prune the target dataset based on sample difficulty, ensuring only aligned information is extracted by the agent model. For information embedding, PAD masks out shallow-layer parameters to avoid injecting low-level basic information into the synthetic data. Experimental results demonstrate that PAD achieves state-of-the-art performance on various benchmarks, outperforming previous matching-based methods.

## Method Summary
PAD addresses dataset distillation misalignment through a two-module approach. First, a data scheduler filters information extraction by pruning the target dataset based on sample difficulty, using Error L2-Norm (EL2N) to score samples and adjusting the training set composition during trajectory matching. Second, a parameter selection module filters information embedding by masking out shallow-layer parameters during metric matching, ordering parameters from shallow to deep and discarding a threshold ratio α. These modules integrate with trajectory matching frameworks like DATM, where the agent model is trained on filtered real data and synthetic data is optimized using only selected deep-layer parameters.

## Key Results
- State-of-the-art performance on CIFAR-10, CIFAR-100, and Tiny ImageNet benchmarks
- Significant improvements in cross-architecture generalization compared to baseline methods
- Effective performance across various compression ratios (IPCs) with consistent gains

## Why This Works (Mechanism)

### Mechanism 1: Data Difficulty Alignment
- Claim: Aligning the difficulty of samples used for distillation with the desired compression ratio improves synthetic dataset quality.
- Mechanism: Samples are scored by difficulty using Error L2-Norm (EL2N). During trajectory matching, easy samples are used for small IPCs while hard samples are used for large IPCs. The data scheduler gradually adjusts the training set composition.
- Core assumption: Different compression ratios require different levels of sample difficulty for optimal distillation.
- Evidence anchors:
  - [abstract] "we find that existing methods introduce misaligned information in both information extraction and embedding stages"
  - [section 2.1] "information related to easy samples is only needed when the compression ratio is high"
  - [corpus] Weak - no direct corpus evidence supporting difficulty-alignment claim
- Break condition: If sample difficulty scoring becomes unreliable or if difficulty requirements change with model architecture.

### Mechanism 2: Deep Layer Parameter Selection
- Claim: Using only deep layer parameters for trajectory matching filters out low-level redundant information.
- Mechanism: Parameters are ordered from shallow to deep. A threshold ratio α determines how many shallow-layer parameters to discard during matching.
- Core assumption: Shallow layers capture basic data distributions while deep layers capture semantic information relevant for classification.
- Evidence anchors:
  - [abstract] "use only deep layers of the agent model to perform the distillation to avoid excessively introducing low-level information"
  - [section 2.2] "shallow layer parameters of the model can only provide low-quality, basic signals, which are redundant for dataset distillation"
  - [section 5.2] "matching shallow layers primarily conveys low-level information that is readily captured by the synthetic data and quickly saturated"
- Break condition: If task requires more low-level feature information or if deep layers don't capture sufficient semantic information.

### Mechanism 3: Two-Stage Filtering Integration
- Claim: Combining data selection (FIEX) and parameter selection (FIEM) modules provides multiplicative improvement over either alone.
- Mechanism: FIEX filters misaligned samples based on difficulty; FIEM filters misaligned parameters based on layer depth. Both modules work independently but complementarily.
- Core assumption: Misalignment occurs in both data selection and parameter usage stages, and addressing both simultaneously yields better results.
- Evidence anchors:
  - [abstract] "aligns information from the following two perspectives"
  - [section 3] "To alleviate the information misalignment issue, based on trajectory matching (TM) [ 1, 11], we propose Prioritizing Alignment in Dataset Distillation (PAD)"
  - [corpus] Weak - no direct corpus evidence supporting combined filtering approach
- Break condition: If one filtering stage becomes unnecessary or if filtering introduces harmful bias.

## Foundational Learning

- Concept: Trajectory Matching
  - Why needed here: PAD builds upon trajectory matching as its base method for dataset distillation
  - Quick check question: What is the core difference between trajectory matching and gradient matching in dataset distillation?

- Concept: Error L2-Norm (EL2N) scoring
  - Why needed here: Used to quantify sample difficulty for the data selection module
  - Quick check question: How does EL2N score relate to sample difficulty in the context of dataset distillation?

- Concept: Parameter depth ordering
  - Why needed here: Enables systematic removal of shallow-layer parameters during matching
  - Quick check question: Why are parameters ordered from shallow to deep in the parameter selection module?

## Architecture Onboarding

- Component map: Data Scheduler -> EL2N Scoring -> Parameter Selection -> Trajectory Matching -> Synthetic Data Optimization

- Critical path:
  1. Score samples using EL2N
  2. Initialize data scheduler with easy samples
  3. During trajectory training, gradually add hard samples
  4. For matching, discard shallow-layer parameters based on IPC
  5. Optimize synthetic data

- Design tradeoffs:
  - More aggressive filtering → better alignment but potential information loss
  - Deeper parameter selection → cleaner semantic information but possible loss of low-level cues
  - Faster data scheduler → quicker adaptation but risk of missing useful samples

- Failure signatures:
  - Performance drops when IPC ratio changes significantly
  - Distorted synthetic images with excessive smoothing
  - Overfitting to specific difficulty ranges

- First 3 experiments:
  1. Validate EL2N scoring correlates with actual sample difficulty on CIFAR-10
  2. Test parameter selection threshold (α) impact on synthetic image quality
  3. Compare single-module vs dual-module filtering on trajectory matching baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PAD vary when using alternative difficulty scoring functions beyond EL2N, such as those based on prediction loss from pre-trained models or uncertainty scores?
- Basis in paper: [explicit] The paper mentions that EL2N is used as the default difficulty scoring function but acknowledges that other metrics can be chosen, and compares EL2N with prediction loss and uncertainty score in Table 4(a).
- Why unresolved: The paper only provides a limited comparison of difficulty scoring functions, and the relative effectiveness of other metrics like prediction loss or uncertainty scores across different IPC settings and datasets remains unexplored.
- What evidence would resolve it: Comprehensive experiments comparing PAD's performance using various difficulty scoring functions (e.g., prediction loss, uncertainty scores) across multiple datasets, IPC settings, and compression ratios, along with ablation studies isolating the impact of the scoring function choice.

### Open Question 2
- Question: What is the impact of the initial ratio (IR) and end epoch of hard sample addition (AEE) hyperparameters on PAD's performance, and how sensitive is the method to their values?
- Basis in paper: [explicit] The paper discusses the role of IR and AEE in controlling the data scheduler and mentions that larger IR and faster addition bring better performances, but provides limited ablation results in Table 3(c).
- Why unresolved: The paper only shows results for a few combinations of IR and AEE values, and the sensitivity of PAD's performance to these hyperparameters across different datasets, IPC settings, and compression ratios is not fully characterized.
- What evidence would resolve it: Extensive ablation studies varying IR and AEE across a wide range of values for multiple datasets, IPC settings, and compression ratios, along with sensitivity analyses quantifying the impact of these hyperparameters on PAD's performance.

### Open Question 3
- Question: How does the ratio of shallow-layer parameters discarded (α) in the parameter selection module affect PAD's performance, and is there an optimal α that balances the removal of low-level information with the preservation of essential features?
- Basis in paper: [explicit] The paper discusses the rationale for discarding shallow-layer parameters and shows results for different α values in Table 4(b), but the optimal α and its sensitivity across datasets and IPC settings are not fully explored.
- Why unresolved: The paper only provides results for a limited set of α values and does not thoroughly investigate the trade-off between removing low-level information and preserving essential features, nor does it explore the sensitivity of PAD's performance to α across different datasets and IPC settings.
- What evidence would resolve it: Comprehensive experiments varying α across a wide range of values for multiple datasets, IPC settings, and compression ratios, along with analyses quantifying the impact of α on the quality of distilled datasets and the trade-off between removing low-level information and preserving essential features.

## Limitations
- The assumption that sample difficulty correlates directly with information value for distillation lacks theoretical grounding
- The mechanism by which shallow-layer parameter masking improves distillation quality is demonstrated empirically but not fully explained mechanistically
- Limited testing on non-vision domains raises questions about cross-domain generalization

## Confidence
- High confidence in the empirical effectiveness of the two-module filtering approach (achieved state-of-the-art results)
- Medium confidence in the theoretical justification for parameter depth ordering (based on intuition rather than rigorous analysis)
- Medium confidence in the scalability claims (limited testing on non-vision domains)

## Next Checks
1. **Ablation of difficulty scoring**: Replace EL2N with alternative difficulty metrics (e.g., entropy, confidence margin) to verify that the specific choice of scoring method is not critical to performance gains

2. **Layer-wise parameter analysis**: Systematically vary the parameter selection threshold α and measure the impact on synthetic data quality and downstream performance to identify optimal selection ranges

3. **Cross-domain generalization**: Apply PAD to non-image domains (e.g., tabular data, time series) to validate whether the alignment principles generalize beyond vision tasks