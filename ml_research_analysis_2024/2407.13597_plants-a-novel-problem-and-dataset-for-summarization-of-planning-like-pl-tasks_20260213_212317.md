---
ver: rpa2
title: 'PLANTS: A Novel Problem and Dataset for Summarization of Planning-Like (PL)
  Tasks'
arxiv_id: '2407.13597'
source_url: https://arxiv.org/abs/2407.13597
tags:
- summarization
- plans
- planning
- each
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel problem of summarizing planning-like
  (PL) tasks, which involve sequences of actions aimed at achieving specific goals.
  The authors propose a new dataset, PLANTS, consisting of automated plans, recipes,
  and travel routes, with 10 problems per domain and multiple plans per problem.
---

# PLANTS: A Novel Problem and Dataset for Summarization of Planning-Like (PL) Tasks

## Quick Facts
- arXiv ID: 2407.13597
- Source URL: https://arxiv.org/abs/2407.13597
- Reference count: 13
- One-line primary result: GPT-4o produces the most information-rich summaries with highest lexical density and ease of understanding for planning-like tasks

## Executive Summary
This paper introduces PLANTS, a novel dataset for summarizing planning-like tasks that involve sequences of actions aimed at achieving specific goals. The dataset includes automated plans, recipes, and travel routes, with 10 problems per domain and multiple plans per problem. The authors propose a frequency-based baseline method for extractive summarization and compare it with TextRank and GPT-4o using quantitative metrics and user studies. Results show that GPT-4o generates the most information-rich summaries with the highest lexical density and ease of understanding, while the baseline method performs second-best for automated plans.

## Method Summary
The paper introduces a new problem of summarizing planning-like (PL) tasks and presents the PLANTS dataset consisting of automated plans, recipes, and travel routes. A frequency-based baseline method for extractive summarization is proposed, which analyzes plans to extract common actions and sequences. This baseline is compared with TextRank and GPT-4o using quantitative metrics (lexical density) and qualitative user studies measuring ease of understanding and user preferences.

## Key Results
- GPT-4o produced the most information-rich summaries with the highest lexical density across all PL tasks
- The baseline method ranked second for automated plans but showed limitations for recipes and travel routes
- User study results showed GPT-4o received highest scores for ease of understanding (scale of 1-5) across all PL tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The frequency-based baseline method improves summarization by extracting common actions and sequences from multiple plans
- Mechanism: Parses each plan to extract actions, then analyzes text and plan views to identify common individual actions, n-grams, action sequences, and shortest plans
- Core assumption: Common actions and sequences across multiple plans represent essential steps
- Evidence anchors:
  - [abstract]: "We propose a new dataset, PLANTS, consisting of automated plans, recipes, and travel routes, with 10 problems per domain and multiple plans per problem. A frequency-based baseline method for extractive summarization is presented."
  - [section]: "Algorithm 1 outlines our baseline method, which involves parsing the plans to extract actions and creating a structured representation of the data."
- Break condition: If plans are too diverse with little action overlap, method may produce overly generic summaries

### Mechanism 2
- Claim: GPT-4o generates more information-rich summaries with higher lexical density
- Mechanism: As a large language model, GPT-4o understands context and generates abstractive summaries with more content words relative to total words
- Core assumption: Large language models can generate more informative and detailed summaries than extractive methods
- Evidence anchors:
  - [abstract]: "GPT-4o produced the most information-rich summaries with the highest lexical density and ease of understanding"
  - [section]: "Experiment 2: Comparing the information-richness of the summaries. In this experiment, we measure the lexical density of summaries generated by baseline, TextRank, and GPT-4o"
- Break condition: If GPT-4o includes hallucinations or information not grounded in source plans

### Mechanism 3
- Claim: User preference and ease of understanding are highest for GPT-4o-generated summaries
- Mechanism: GPT-4o's ability to generate fluent and coherent summaries makes them easier for users to understand and act upon
- Core assumption: Users prefer summaries that are easy to understand and can be acted upon
- Evidence anchors:
  - [abstract]: "GPT-4o produced the most information-rich summaries with the highest lexical density and ease of understanding"
  - [section]: "Experiment 3: Comparing the ease of understanding of the summaries. From the user studies, we obtained results on how easy it is to understand a summary to take an action"
- Break condition: If users need exact steps or ingredients, abstractive summaries might not be preferred

## Foundational Learning

- Concept: Planning-like tasks and their characteristics
  - Why needed here: Understanding PL task nature is crucial for designing effective summarization methods
  - Quick check question: What are the key differences between automated plans, recipes, and travel routes in terms of their structure and summarization requirements?

- Concept: Text summarization techniques (extractive and abstractive)
  - Why needed here: Paper compares different summarization methods, understanding their approaches is essential
  - Quick check question: How do extractive and abstractive summarization methods differ in their approach to generating summaries?

- Concept: Evaluation metrics for summarization
  - Why needed here: Paper uses various metrics (lexical density, user study scores) to evaluate summaries
  - Quick check question: What are the advantages and limitations of using lexical density and user study scores as evaluation metrics?

## Architecture Onboarding

- Component map: Dataset creation -> Summarization methods -> Evaluation -> Analysis
- Critical path:
  1. Parse and structure plans to extract actions
  2. Analyze structured data in text and plan views to identify common elements
  3. Generate summaries using frequency-based baseline, TextRank, or GPT-4o
  4. Evaluate summaries using quantitative metrics and user studies
  5. Compare results to identify most effective summarization method
- Design tradeoffs:
  - Extractive vs. abstractive methods: Extractive methods are more faithful but may lack coherence; abstractive methods are more fluent but risk hallucination
  - Dataset size and diversity: Limited size (10 problems per domain) may not capture real-world variability
  - Evaluation metrics: User studies provide qualitative insights but are subjective and may not capture executional semantics
- Failure signatures:
  - Baseline produces overly generic summaries due to lack of common actions
  - GPT-4o generates summaries with hallucinations or ungrounded information
  - User studies show low inter-annotator agreement
  - Quantitative metrics don't correlate with user preferences or ease of understanding
- First 3 experiments:
  1. Evaluate frequency-based baseline on small PLANTS subset to assess ability to capture common actions
  2. Compare baseline and TextRank on larger subset to identify strengths and weaknesses of each extractive method
  3. Test GPT-4o on representative PL tasks to assess abstractive summary quality and faithfulness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do abstractive methods handle hallucination when generating summaries for PL tasks, and what strategies can mitigate this issue?
- Basis in paper: [explicit] The paper mentions hallucination in abstractive methods remains a significant challenge
- Why unresolved: While acknowledging the problem, the paper doesn't provide specific strategies to address hallucination in PL tasks
- What evidence would resolve it: Empirical studies comparing different methods to reduce hallucination, such as using additional constraints or fine-tuning on domain-specific data

### Open Question 2
- Question: What factors influence user preferences for summaries in different PL task domains, and how can these preferences be quantified and incorporated into summarization models?
- Basis in paper: [inferred] The paper conducts user studies but doesn't analyze specific factors driving preferences across domains
- Why unresolved: Study identifies preferences but lacks detailed analysis of underlying factors
- What evidence would resolve it: Detailed surveys or interviews to identify specific factors, followed by experiments to quantify their impact

### Open Question 3
- Question: How can evaluation metrics be developed to better assess executional semantics of PL task summaries, ensuring they maintain logical flow and executability?
- Basis in paper: [explicit] The paper highlights need for evaluation metrics specifically tailored for PL task summaries
- Why unresolved: Current metrics focus on subjective measures that don't fully capture executional semantics
- What evidence would resolve it: Developing and validating new metrics that assess logical flow and executability through expert evaluations or automated testing frameworks

## Limitations
- Dataset size limited to 10 problems per domain may not capture full diversity of real-world PL tasks
- User study methodology lacks detailed specifications for annotator instructions and inter-annotator agreement measurements
- Evaluation metrics don't fully capture executional semantics, particularly important for automated plans requiring precise action sequences

## Confidence
- High: GPT-4o's superior performance in lexical density and user preference scores
- Medium: The frequency-based baseline method's effectiveness for automated plans
- Medium: The overall approach to PL task summarization and its potential benefits

## Next Checks
1. Conduct a larger-scale user study with more diverse participants and explicit inter-annotator agreement measurements to validate qualitative findings
2. Test summarization methods on a more extensive and diverse set of PL tasks beyond the current 10 problems per domain
3. Develop and implement objective evaluation metrics specific to PL task summaries that can measure executional semantics and plan fidelity