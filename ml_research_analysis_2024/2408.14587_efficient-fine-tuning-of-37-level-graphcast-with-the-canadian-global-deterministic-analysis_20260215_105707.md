---
ver: rpa2
title: Efficient fine-tuning of 37-level GraphCast with the Canadian global deterministic
  analysis
arxiv_id: '2408.14587'
source_url: https://arxiv.org/abs/2408.14587
tags:
- training
- graphcast
- forecast
- level
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first detailed account of fine-tuning the
  full 37-level GraphCast model to a different operational analysis system (ECCC's
  GDPS). The fine-tuning adapts GraphCast's weights to better predict GDPS analysis
  fields using only two years of training data and 37 GPU-days of computation.
---

# Efficient fine-tuning of 37-level GraphCast with the Canadian global deterministic analysis

## Quick Facts
- arXiv ID: 2408.14587
- Source URL: https://arxiv.org/abs/2408.14587
- Reference count: 8
- This work presents the first detailed account of fine-tuning the full 37-level GraphCast model to a different operational analysis system (ECCC's GDPS).

## Executive Summary
This paper demonstrates the successful fine-tuning of DeepMind's GraphCast model to predict the Canadian Global Deterministic Prediction System (GDPS) analysis fields instead of its original ERA5 training data. The fine-tuning process adapts the model's weights using only two years of GDPS data and 37 GPU-days of computation, significantly improving forecast skill over both the unmodified GraphCast and ECCC's operational forecasts. The work provides a practical guideline for national meteorological centers to adapt GraphCast to their local operational configurations, including key innovations like sensitivity-based error weighting and split-horizon training to manage memory constraints.

## Method Summary
The fine-tuning process involved re-normalizing GraphCast's inputs to match GDPS statistics, replacing pressure-based error weights with sensitivity-based weights derived from perturbation analysis, and using a truncated training curriculum with split-horizon training for 3-day forecasts. The training proceeded through five stages: 10240 batches at high learning rate for normalization, 81920 batches with sensitivity weights, and sequential training over 2, 4, 8, and 12-step forecasts with progressively lower learning rates. The 12-step forecast used a split-horizon approach (4+8 steps) to manage memory constraints while computing gradients effectively.

## Key Results
- Fine-tuned GraphCast significantly outperforms both the unmodified GraphCast and operational forecasts in the troposphere over 1-10 day lead times
- The sensitivity-based error weighting improved stratospheric performance compared to pressure-based weights
- Split-horizon training enabled effective 3-day forecast training within memory constraints while maintaining forecast skill
- The control run with ERA5 data showed some overfitting, validating the need for GDPS-specific training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GraphCast can be fine-tuned to predict a different analysis system (GDPS) with significantly improved performance over both the unmodified GraphCast and the operational forecast.
- Mechanism: The fine-tuning process adapts GraphCast's weights to better predict GDPS analysis fields using a shorter, optimized training curriculum that consolidates autoregressive stages and splits 3-day forecasts into two sub-steps to conserve memory.
- Core assumption: The systematic differences between GDPS and ERA5 can be learned by GraphCast given sufficient, but not excessive, training data and computational resources.
- Evidence anchors:
  - [abstract]: "Using two years of training data (July 2019 -- December 2021) and 37 GPU-days of computation to tune the 37-level, quarter-degree version of GraphCast, the resulting model significantly outperforms both the unmodified GraphCast and operational forecast..."
  - [section 3]: "This work elaborates on the fourth option, discussing the steps required to fine-tune GraphCast for the operational Canadian GDPS analysis using a relatively short training period taken from the operational system..."
  - [corpus]: No direct evidence; corpus neighbors focus on general AI weather model performance rather than fine-tuning specific analysis systems.
- Break condition: If the systematic differences between the target analysis system and the original training data are too large or too complex to be captured by the model's capacity within the available training data and computational budget.

### Mechanism 2
- Claim: Replacing the pressure-based error weights with sensitivity-based weights improves the fine-tuned model's performance, especially in the stratosphere.
- Mechanism: The sensitivity analysis measures the change in 5-day forecast output with respect to model forecast error at each pressure level, providing a more accurate weighting of errors that reflects the model's actual impact on forecast skill.
- Core assumption: The sensitivity of the forecast to errors at different pressure levels is a better indicator of their importance for overall forecast accuracy than a simple pressure-based weighting.
- Evidence anchors:
  - [section 3c]: "To address these factors, this work derives new level-based error weights through a sensitivity analysis, described in detail in appendix 5..."
  - [section 4b]: "The resulting fine-tuned model improves upon ECCC's operational forecast at 1/4° resolution, although like the unmodified GraphCast model it smooths fine-scale structures at longer lead times..."
  - [corpus]: No direct evidence; corpus neighbors do not discuss sensitivity-based error weighting.
- Break condition: If the sensitivity analysis does not accurately capture the relative importance of errors at different pressure levels, or if the computational cost of the sensitivity analysis outweighs its benefits.

### Mechanism 3
- Claim: Split-horizon training allows effective computation of gradients over 3-day forecasts despite memory limitations, enabling the fine-tuning process to proceed.
- Mechanism: By splitting the 12-step (3-day) forecast into two stages of 4 and 8 steps, the memory required for gradient computation is reduced, and the data-loading task can be parallelized, allowing the fine-tuning to complete within the available computational resources.
- Core assumption: The gradients computed over the split horizons are sufficiently similar to those computed over the full forecast horizon to allow effective training.
- Evidence anchors:
  - [section 3e]: "The final training stage, over 12-step (3-day) forecasts required adjustment... This work takes inspiration from the replay buffer... Rather than freeze initial conditions to perform training over a single step, however, the 12-step forecast is split into just two stages of 4 and 8 steps respectively..."
  - [section 4]: "The resulting fine-tuned model improves upon ECCC's operational forecast at 1/4° resolution, although like the unmodified GraphCast model it smooths fine-scale structures at longer lead times..."
  - [corpus]: No direct evidence; corpus neighbors do not discuss split-horizon training.
- Break condition: If the gradients computed over the split horizons diverge significantly from those computed over the full forecast horizon, leading to suboptimal model performance.

## Foundational Learning

- Concept: GraphCast architecture and its normalization scheme
  - Why needed here: Understanding the model's architecture and how it processes input data is crucial for designing an effective fine-tuning strategy and interpreting the results.
  - Quick check question: What are the key components of the GraphCast architecture, and how does the normalization scheme affect the model's input and output?

- Concept: Sensitivity analysis and its application to error weighting
  - Why needed here: The sensitivity analysis is a key innovation in this work, and understanding how it is performed and how it affects the model's training is essential for replicating or extending this research.
  - Quick check question: How is the sensitivity analysis performed, and how are the resulting sensitivities used to compute the new error weights?

- Concept: Split-horizon training and its impact on gradient computation
  - Why needed here: Split-horizon training is a crucial technique for enabling the fine-tuning process to proceed within the available computational resources, and understanding its mechanics and trade-offs is important for applying it to other models or training scenarios.
  - Quick check question: How does split-horizon training work, and what are the potential benefits and drawbacks compared to training over the full forecast horizon?

## Architecture Onboarding

- Component map: Input normalization -> GraphCast encoding stage -> Multi-resolution icosahedral grid processing -> Decoding stage -> Output normalization
- Critical path: Compute GDPS normalization factors and sensitivity weights → Train 10240 batches at high learning rate → Train 81920 batches with sensitivity weights → Sequentially train 2, 4, 8, and 12-step forecasts with decreasing learning rates → Apply split-horizon for 12-step training
- Design tradeoffs: Computational efficiency through split-horizon training and truncated curriculum versus potential performance gains from full-horizon training; sensitivity-based weights improve stratospheric performance but add computational complexity
- Failure signatures: Memory overflow during backpropagation over 12-step forecasts; model divergence due to learning rate too high; overfitting to training period; suboptimal performance from gradient approximation in split-horizon approach
- First 3 experiments:
  1. Validate the normalization factors and error weights computed for the GDPS dataset by comparing them to those used for the ERA5 dataset
  2. Test the impact of different learning rates and batch sizes on the fine-tuning process using a small subset of the training data
  3. Evaluate the performance of the fine-tuned model on a held-out validation dataset at each stage of the training process to monitor for overfitting and assess the effectiveness of the split-horizon approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much of the improved forecast skill in the fine-tuned model is due to better-constrained initial conditions versus actual model improvements?
- Basis in paper: [inferred] The authors discuss that recent analyses may be better-constrained by observational data, potentially exposing more predictability that data-driven models can learn from. They also note that the fine-tuned model shows improvement even for times before the training period, suggesting some model improvement beyond just better initial conditions.
- Why unresolved: The paper does not provide a clear separation of these two effects. It mentions both as possibilities but does not quantify their relative contributions to the improved forecast skill.
- What evidence would resolve it: A detailed analysis comparing forecast skill using both fine-tuned and unmodified models with both GDPS and ERA5 initial conditions across different time periods (especially pre-training period) would help quantify the contributions of better initial conditions versus model improvements.

### Open Question 2
- Question: What is the optimal training schedule for fine-tuning GraphCast to balance computational efficiency and forecast accuracy?
- Basis in paper: [explicit] The authors discuss abbreviating DeepMind's original training curriculum, consolidating autoregressive stages, and using larger learning rates. They also mention a computational budget constraint and provide a specific training schedule used in their study.
- Why unresolved: While the authors present a specific training schedule, they acknowledge that selecting optimal learning rates is an art and that their choice was based on heuristic search. The paper does not explore alternative training schedules or provide a systematic method for determining the optimal approach.
- What evidence would resolve it: A comprehensive study comparing different training schedules, including variations in learning rates, batch sizes, forecast lengths, and number of training examples, would help determine the optimal approach for fine-tuning GraphCast.

### Open Question 3
- Question: How can the smoothing of fine scales at longer lead times in data-driven forecast models like GraphCast be mitigated without sacrificing accuracy?
- Basis in paper: [explicit] The authors discuss the known issue of deterministic data-driven forecast models trained to minimize mean squared errors suffering from smoothing of fine scales at longer lead times. They mention potential solutions like alternative loss calculations or low-rank adaptation but do not explore these in detail.
- Why unresolved: While the paper acknowledges the smoothing problem and suggests potential solutions, it does not implement or evaluate these approaches. The authors note that this is an area for future work but do not provide concrete results or recommendations.
- What evidence would resolve it: Implementation and comparison of different approaches to mitigate smoothing, such as alternative loss functions, model architectures, or training techniques, evaluated against both accuracy metrics and spectral characteristics of the forecasts, would provide insights into effective solutions.

## Limitations

- Training period (July 2019 - December 2021) may not capture full seasonal variability and includes a strong El Niño period
- Split-horizon training introduces approximation errors in gradient computation that are not quantified
- Sensitivity-based error weighting relies on perturbations to 5-day forecasts that may not represent shorter-range prediction errors

## Confidence

- **High confidence**: The core claim that GraphCast can be successfully fine-tuned to predict GDPS analysis with improved performance over both the unmodified model and operational forecasts
- **Medium confidence**: The assertion that sensitivity-based error weighting provides meaningful improvements, particularly in the stratosphere
- **Medium confidence**: The practical utility of the fine-tuned model for operational forecasting (demonstrated on historical data but not validated in real-time operational settings)

## Next Checks

1. Conduct a longer-term validation (2+ years) to assess whether the model maintains performance across different climate regimes and extreme weather events not represented in the training period.

2. Implement a full-horizon training run on a subset of the data to quantify the approximation error introduced by the split-horizon approach and determine optimal split points.

3. Test the fine-tuned model in a quasi-operational setting where forecasts are generated in real-time and compared against operational forecasts as they verify, rather than retrospectively on archived data.