---
ver: rpa2
title: When Can Memorization Improve Fairness?
arxiv_id: '2412.09254'
source_url: https://arxiv.org/abs/2412.09254
tags:
- fairness
- bias
- odds
- classifier
- memorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how memorization of a subset of the data can
  be used to improve fairness metrics in classification tasks. The authors develop
  explicit expressions for how the bias of a classifier that memorizes part of the
  data relates to the bias of the base classifier and the composition of the memorized
  subset.
---

# When Can Memorization Improve Fairness?

## Quick Facts
- arXiv ID: 2412.09254
- Source URL: https://arxiv.org/abs/2412.09254
- Reference count: 40
- This paper analyzes how memorization of a subset of the data can be used to improve fairness metrics in classification tasks.

## Executive Summary
This paper investigates how memorizing a subset of the data can be used to improve fairness metrics in classification tasks. The authors develop explicit expressions for how the bias of a classifier that memorizes part of the data relates to the base classifier's bias and the composition of the memorized subset. They characterize exactly which compositions of the memorized dataset lead to zero bias for statistical parity, equal opportunity, and equalized odds metrics. The paper also provides upper and lower bounds on the probability mass that needs to be memorized to completely eliminate these biases.

## Method Summary
The authors model memorization as a random variable and express classifier behavior in terms of conditional probabilities. They formulate the problem as a linear programming system to characterize when bias can be eliminated through memorization. The framework assumes knowledge of base classifier statistics (confusion matrices, base rates) and population distributions, then derives conditions on the memorized subset composition needed to achieve zero bias for various fairness metrics.

## Key Results
- Memorization can theoretically eliminate statistical parity, equal opportunity, or equalized odds bias through specific subset compositions
- Significant memorization is required - bounds show the probability mass needed depends on group imbalance and classifier performance
- Equalized odds is much harder to eliminate than other metrics due to restrictive conditions on misclassification rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memorization of a carefully chosen subset of the data can completely eliminate statistical parity, equal opportunity, or equalized odds bias.
- Mechanism: The paper derives explicit expressions showing how the bias of a classifier that memorizes a subset relates to the bias of the base classifier and the composition of the memorized subset. By solving systems of linear equations, specific compositions can be found that drive bias to zero.
- Core assumption: The base classifier's bias and the population composition are known exactly, and the memorized subset can be chosen with precise control over its label and group membership distribution.
- Evidence anchors:
  - [abstract] "We also characterize the memorized datasets that eliminate the bias for all three metrics considered."
  - [section 3] "we show how the fairness gap of the classifier with memorization varies with the composition of the memorized dataset in terms of subpopulations with different labels and sensitive attribute values."
- Break condition: If the base classifier's bias depends on the choice of memorized dataset (e.g., through data leakage), the linear expressions may not hold. Also, if the required memorized proportions cannot be achieved due to data constraints, zero bias is impossible.

### Mechanism 2
- Claim: Memorizing a significant portion of the data is necessary to eliminate bias, and this proportion can be bounded.
- Mechanism: The paper provides upper and lower bounds on the probability mass that needs to be memorized to completely eliminate statistical parity or equal opportunity biases. These bounds depend on the imbalance between groups and the classifier's performance on each group.
- Evidence anchors:
  - [abstract] "Finally we provide upper and lower bounds on the total probability mass in the memorized dataset that is necessary for the complete elimination of these biases."
  - [section 3] "Our results in Corollaries 11 and 14 provide lower bounds on which fraction of the total population the specialized model would have to cover in order to hide a certain level of intrinsic bias."
- Break condition: If the bounds are not tight, or if practical constraints prevent memorizing the required fraction, the method cannot fully eliminate bias.

### Mechanism 3
- Claim: Equalized odds is much harder to eliminate through memorization than statistical parity or equal opportunity.
- Mechanism: The paper proves that equalized odds can only be eliminated if a specific ratio of misclassification rates is independent of the predicted label, which is a very restrictive condition unlikely to occur in practice.
- Evidence anchors:
  - [section 3] "The following result shows that the corresponding bias can be completely eliminated by memorization only in very specific situations. Even if this is unlikely to occur in practical applications, we expect that the result can still be useful in combination with perturbation methods."
  - [section 3] "Equalized odds is a very strong constraint on the classifier. The following result shows that the corresponding bias can be completely eliminated by memorization only in very specific situations."
- Break condition: If the ratio of misclassification rates is not constant across labels, equalized odds bias cannot be fully eliminated by memorization alone.

## Foundational Learning

- Concept: Linear programming and systems of linear inequalities
  - Why needed here: The core results characterize bias-eliminating memorized subsets as solutions to linear programs. Understanding how to formulate and solve these is essential to apply the results.
  - Quick check question: Can you write the system of inequalities that characterizes when statistical parity bias can be eliminated? (See Theorem 9 and Corollaries 10, 11)

- Concept: Fairness metrics in machine learning (statistical parity, equal opportunity, equalized odds)
  - Why needed here: The paper is entirely focused on how memorization affects these specific fairness metrics. Knowing their definitions and differences is critical.
  - Quick check question: What is the key difference between statistical parity and equal opportunity?

- Concept: Probabilistic modeling of classification and memorization
  - Why needed here: The framework models memorization as a random variable and expresses classifier behavior in terms of conditional probabilities. This probabilistic lens is essential for understanding the derivations.
  - Quick check question: What does the random variable D = 1 represent in this framework?

## Architecture Onboarding

- Component map:
  - Input: Population distribution (p+, py, p+_y), base classifier confusion matrix (φ_y, C_y,ŷ), desired fairness metric
  - Process: Formulate and solve linear program for memorized subset composition (qy, q+_y) that eliminates bias
  - Output: Characterization of whether bias can be eliminated, and if so, the required memorized proportion pD

- Critical path: The path from known population and classifier statistics to bias elimination is through the linear expressions in Theorem 8, then solving the systems in Theorems 9, 12, or 15. The most common path is through Theorems 9 and 12.

- Design tradeoffs: Memorizing more data increases the chance of bias elimination but also increases the risk of overfitting and data inefficiency. The paper shows this is necessary but does not provide a method to balance these concerns.

- Failure signatures: If the linear program for bias elimination is infeasible, it means the desired fairness cannot be achieved through memorization alone. This is characterized by the bounds in Corollaries 10, 11, 13, 14.

- First 3 experiments:
  1. Verify Theorem 8 by constructing a synthetic population and base classifier, then computing the bias of a memorizing classifier with a chosen memorized subset.
  2. Test Theorem 9 by generating random base classifier statistics and population distributions, then checking if the linear program for eliminating statistical parity bias is feasible.
  3. Validate Corollary 10 by checking if the bound on pD correctly predicts when the linear program for statistical parity is feasible.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions can memorization-based fairness improvements be generalized beyond the specific mathematical framework presented?
- Basis in paper: [explicit] The authors acknowledge their framework doesn't directly yield a method for eliminating bias without extra assumptions
- Why unresolved: The paper focuses on theoretical characterizations but doesn't provide concrete algorithmic approaches for practical implementation
- What evidence would resolve it: Empirical studies demonstrating successful application of these theoretical results in real-world classification tasks

### Open Question 2
- Question: How do the theoretical bounds on memorization probability translate to practical dataset sizes in real applications?
- Basis in paper: [explicit] The authors provide upper and lower bounds on the total probability mass that needs to be memorized
- Why unresolved: The paper doesn't connect these abstract probability bounds to concrete dataset sizes or storage requirements
- What evidence would resolve it: Case studies showing actual memorization requirements for specific dataset sizes and application domains

### Open Question 3
- Question: What are the computational implications of solving the linear programming systems required to achieve zero bias through memorization?
- Basis in paper: [inferred] The results are framed as systems of linear equality and inequality constraints that can be numerically solved
- Why unresolved: The paper doesn't analyze the computational complexity or scalability of these solutions
- What evidence would resolve it: Computational experiments measuring runtime and resource requirements for different problem sizes and dataset characteristics

## Limitations
- The framework requires precise knowledge of base classifier statistics and population distributions, which are rarely available in practice
- Memorizing specific proportions of each subgroup presents significant practical challenges
- Equalized odds bias elimination conditions are so restrictive that the mechanism is likely impractical for real-world applications

## Confidence

**High Confidence**: The linear algebraic framework for characterizing bias elimination through memorization (Theorem 8 and related corollaries) is mathematically rigorous and well-founded.

**Medium Confidence**: The practical feasibility of achieving the required memorized subset compositions in real datasets, given sampling constraints and finite data.

**Low Confidence**: The practical applicability of equalized odds elimination through memorization, given the extremely restrictive conditions required.

## Next Checks

1. **Empirical feasibility test**: Implement the linear programming framework on a real dataset with a pre-trained classifier to verify whether the theoretically required memorized proportions are achievable given sampling constraints.

2. **Robustness to parameter estimation error**: Systematically vary the assumed base classifier parameters and population distributions to quantify how sensitive the bias elimination results are to estimation error.

3. **Comparison with alternative bias mitigation**: Benchmark the memorization approach against standard bias mitigation techniques (reweighting, adversarial debiasing) on the same tasks to assess practical utility.