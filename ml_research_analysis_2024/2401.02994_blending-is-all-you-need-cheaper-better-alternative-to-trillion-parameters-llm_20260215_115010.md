---
ver: rpa2
title: 'Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters
  LLM'
arxiv_id: '2401.02994'
source_url: https://arxiv.org/abs/2401.02994
tags:
- chat
- blended
- user
- systems
- retention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Blended, a method for combining multiple
  smaller language models (6-13B parameters) to achieve performance comparable to
  or better than much larger models like ChatGPT (175B+ parameters). The core idea
  is to stochastically select responses from a set of diverse base chat AI systems,
  allowing them to collaboratively generate more engaging and diverse conversations.
---

# Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM

## Quick Facts
- arXiv ID: 2401.02994
- Source URL: https://arxiv.org/abs/2401.02994
- Reference count: 19
- Primary result: Blending 6-13B parameter models outperforms ChatGPT in user engagement while using 1/30th the parameters

## Executive Summary
This paper demonstrates that combining multiple smaller language models through stochastic selection can achieve superior performance to massive trillion-parameter models for conversational AI. The Blended approach randomly selects responses from a diverse set of base chat AI systems, enabling implicit knowledge transfer and response diversity. Large-scale A/B tests on the Chai research platform show Blended significantly outperforms both individual smaller models and ChatGPT in user engagement and retention metrics, while requiring only a fraction of the computational resources.

## Method Summary
The Blended algorithm works by randomly selecting which base chat AI model generates each response in a conversation, with each response conditioned on the full conversational history. This allows characteristics from different systems to influence the overall conversation flow. The approach was tested by deploying a Blended model alongside three base models (Pygmalion 6B, Chai Model 6B, Vicuna 13B) and ChatGPT on the Chai Research platform, measuring user retention and engagement over 30 days with at least 10,000 users per group.

## Key Results
- Blended achieved 40% higher user retention than Vicuna and 20% higher than ChatGPT over 30 days
- User engagement time was 25% longer for Blended compared to individual base models
- Blended required only 1/30th the parameters of ChatGPT while achieving superior performance
- Inference cost remained equivalent to a single 6B/13B model since only one model is executed per response

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stochastic selection enables implicit knowledge transfer across conversation turns
- Mechanism: Each randomly selected response becomes part of the conversational history, allowing subsequent models to build upon responses generated by others
- Core assumption: The conversational history is sufficiently rich for each component model to meaningfully condition its responses
- Evidence anchors:
  - [abstract] "by conditioning a response on the conversational history, a single model with particular properties learns abilities from other systems"
  - [section 3.3] "the response generated by a specific chat AI is conditional on all previous responses generated by the previously selected chat AIs"

### Mechanism 2
- Claim: Model blending achieves diversity in response generation that a single large model cannot match
- Mechanism: Different base chat AIs have unique strengths from different training approaches, and random selection draws from multiple expertise areas
- Core assumption: Component models have sufficiently diverse capabilities that their combination provides broader coverage
- Evidence anchors:
  - [section 3.1] "different recipes and training seeds may lead to highly diverse systems that each demonstrate unique strengths and characteristics"
  - [section 5.2] "Vicuna has a significantly lower initial retention ratio, ∆ζ, demonstrating that Vicuna would require an extended period of time to reach Blended's retention score"

### Mechanism 3
- Claim: Computational efficiency is maintained because inference only runs on one model per response
- Mechanism: Unlike traditional ensemble methods, Blended only executes inference on a single randomly selected component model for each response
- Core assumption: The random selection process doesn't require additional computational overhead
- Evidence anchors:
  - [section 5.2] "since responses for Blended are each sampled from a single component chat AI, the inference cost is equivalent to that of a single 6B/13B system"
  - [section 5.2] "Blended only requiring a fraction of the inference cost and memory overhead"

## Foundational Learning

- Concept: Ensemble methods and diversity in machine learning
  - Why needed here: Understanding why combining diverse models can outperform individual models is fundamental to grasping the Blended approach
  - Quick check question: Why might averaging predictions from diverse models often outperform any single model, even if that single model is individually strong?

- Concept: Conditional probability and Markov processes
  - Why needed here: The paper relies on the idea that each response conditions on the full conversational history, which is a Markov-like property
  - Quick check question: How does conditioning each response on the full conversational history differ from treating each response independently?

- Concept: A/B testing methodology and statistical significance
  - Why needed here: The evaluation framework uses A/B testing to compare user engagement metrics, which requires understanding experimental design
  - Quick check question: What are the key considerations when designing an A/B test to compare user engagement between different AI systems?

## Architecture Onboarding

- Component map:
  - Base chat AI models (6B/13B parameter models like Pygmalion, ChaiLLM, Vicuna)
  - Selection mechanism (uniform random selection)
  - Inference pipeline (single model execution per response)
  - User interaction layer (handles user messages and displays responses)
  - Evaluation system (tracks retention and engagement metrics)

- Critical path:
  1. User sends message
  2. System randomly selects one of N base models
  3. Selected model generates response conditioned on full conversation history
  4. Response is sent to user
  5. Engagement/retention metrics are tracked

- Design tradeoffs:
  - Uniform random selection vs. learned selection distribution (section 6 suggests this could improve performance)
  - Number of component models vs. complexity of coordination
  - Conversation history length vs. computational overhead

- Failure signatures:
  - Poor user engagement despite having large models (indicates selection mechanism issues)
  - Inconsistent conversation quality (indicates base models are too dissimilar)
  - High computational cost (indicates selection mechanism overhead)

- First 3 experiments:
  1. Deploy a Blended system with just two base models and compare engagement to each individual model
  2. Test different selection distributions (uniform vs. learned) with the same base models
  3. Gradually increase the number of base models to find the point of diminishing returns on engagement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of models to include in a blended ensemble for conversational AI?
- Basis in paper: [explicit] The paper suggests exploring "Selection set scaling" as future work, noting that increasing the number of component systems has no computational cost and may increase conversation richness.
- Why unresolved: The paper only tested with three models and did not explore how performance scales with different numbers of models.
- What evidence would resolve it: Empirical results comparing blended ensembles with varying numbers of component models (e.g., 2, 3, 4, 5+) across multiple metrics (engagement, retention, inference speed) would determine the optimal number.

### Open Question 2
- Question: How can we develop an optimal selection distribution for choosing which model generates each response in a blended system?
- Basis in paper: [explicit] The paper proposes using a classifier trained on human feedback to predict the probability distribution over the chat AI selection set, but notes this as future work.
- Why unresolved: The paper used a simple uniform distribution for model selection and did not implement or test more sophisticated selection strategies.
- What evidence would resolve it: Development and evaluation of different selection strategies (e.g., classifier-based, reinforcement learning, or other approaches) compared to uniform selection across various conversation scenarios.

### Open Question 3
- Question: Can blended models be effectively combined with existing methods like RLHF or instruction tuning?
- Basis in paper: [inferred] The paper focuses on blending already-trained models but does not explore combining this approach with other training methodologies commonly used in conversational AI.
- Why unresolved: The paper demonstrates the effectiveness of blending as a standalone approach but does not investigate how it might interact with or complement other established techniques.
- What evidence would resolve it: Experiments comparing blended models trained with different methodologies (e.g., RLHF, instruction tuning, or other alignment techniques) against blended models trained with standard fine-tuning alone.

## Limitations
- The specific conversational datasets and reward model training procedures used to create the base chat AI systems are not detailed
- The exact implementation details of the user engagement metric calculation and how it aligns with retention measurement are not specified
- The generalizability of results to other conversational AI applications or user demographics is unclear

## Confidence

**High Confidence:**
- The computational efficiency claim that Blended maintains inference costs equivalent to a single 6B/13B model is well-supported by the methodology described
- The mechanism by which stochastic selection enables knowledge transfer across conversation turns is clearly explained and theoretically sound

**Medium Confidence:**
- The claim that Blended significantly outperforms ChatGPT in user engagement and retention is supported by the experimental results, but the methodology details are limited
- The assertion that blending diverse models provides broader coverage than individual models is plausible but depends on the specific characteristics of the component models

**Low Confidence:**
- The generalizability of results to other conversational AI applications or user demographics is uncertain due to limited information about the experimental setup
- The long-term stability and scalability of the Blended approach with increasing numbers of component models is not addressed

## Next Checks
1. **Component Model Diversity Analysis**: Conduct a detailed analysis of the capabilities and characteristics of each base chat AI model to quantify their diversity
2. **Controlled Ablation Study**: Perform an ablation study where you systematically remove individual component models from the Blended system to determine the marginal contribution of each model
3. **Cross-Platform Replication**: Attempt to replicate the core findings on a different conversational AI platform or with a different user demographic