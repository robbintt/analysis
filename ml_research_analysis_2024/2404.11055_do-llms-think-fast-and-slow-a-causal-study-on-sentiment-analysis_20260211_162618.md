---
ver: rpa2
title: Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis
arxiv_id: '2404.11055'
source_url: https://arxiv.org/abs/2404.11055
tags:
- causal
- review
- sentiment
- prompt
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a causal framework for sentiment analysis
  (SA) that distinguishes between two underlying causal processes: (1) review-driven
  sentiment (C1) where the review causes the sentiment, and (2) sentiment-driven review
  (C2) where the sentiment causes the review. The authors ground these processes in
  psychological theories of fast and slow thinking, using the peak-end rule to classify
  samples into C1 or C2 based on how well their overall sentiment aligns with either
  the average of all sentence-level sentiments or the average of peak and end sentiments.'
---

# Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis

## Quick Facts
- arXiv ID: 2404.11055
- Source URL: https://arxiv.org/abs/2404.11055
- Reference count: 40
- Primary result: Up to 32.13 F1 points improvement on zero-shot five-class SA using causal prompts

## Executive Summary
This paper introduces a causal framework for sentiment analysis that distinguishes between two underlying causal processes: review-driven sentiment (C1) where the review causes the sentiment, and sentiment-driven review (C2) where the sentiment causes the review. The authors ground these processes in psychological theories of fast and slow thinking, using the peak-end rule to classify samples into C1 or C2 based on how well their overall sentiment aligns with either the average of all sentence-level sentiments or the average of peak and end sentiments. For the prediction task, they propose causal prompts that align with the underlying causal direction, achieving up to 32.13 F1 points improvement on zero-shot five-class SA.

## Method Summary
The authors develop a two-step framework for sentiment analysis that first classifies whether a review follows a C1 or C2 causal process using the peak-end rule, then applies corresponding causal prompts for prediction. They construct a new dataset by labeling Yelp reviews according to this rule, where C1 samples have overall sentiment scores that align with the average of all sentence-level sentiments, while C2 samples align with the average of peak and end sentiments. The causal prompts explicitly guide LLMs to reason in the appropriate causal direction - either inferring sentiment from the review content (C1) or inferring review content from the sentiment (C2). The framework is tested across multiple LLMs including GPT-3.5, GPT-4, and LLaMA-2-7B-Chat using both in-context learning and few-shot settings.

## Key Results
- LLMs perform significantly better on C2 data under standard prompts compared to C1 data
- Causal prompts improve F1 scores by up to 32.13 points compared to standard prompts in zero-shot settings
- The framework shows consistent improvements across multiple LLMs and prompt settings
- Mechanistic interpretability analysis reveals models still struggle to fully capture expected causal patterns despite improved performance

## Why This Works (Mechanism)
The framework works by aligning the model's reasoning process with the underlying causal structure of the text. When a review follows the C1 process (review causes sentiment), the model needs to aggregate information across all sentences to infer sentiment. When following C2 (sentiment causes review), the model should focus on how the peak and end experiences shape the overall sentiment. By explicitly prompting for these different reasoning patterns, the framework helps models overcome their tendency to apply uniform processing regardless of the text's causal structure.

## Foundational Learning
- Peak-end rule: psychological principle that people judge experiences based on peak moments and endings rather than the average of all moments
  - Why needed: Provides theoretical grounding for distinguishing between C1 and C2 processes
  - Quick check: Test whether human annotators can reliably distinguish C1 vs C2 samples using this rule

- Causal inference framework: formal approach to reasoning about cause-effect relationships
  - Why needed: Enables proper formulation of the two distinct sentiment analysis processes
  - Quick check: Verify that the proposed causal graphs correctly represent the assumed relationships

- Zero-shot learning: training-free approach where models make predictions without parameter updates
  - Why needed: Demonstrates the framework's applicability without requiring labeled data
  - Quick check: Compare performance against few-shot and fine-tuned baselines

## Architecture Onboarding

Component Map:
Causal Classification (Peak-end rule) -> Causal Prompt Selection -> LLM Prediction

Critical Path:
The critical path involves first classifying the review's causal structure using the peak-end rule, then selecting the appropriate causal prompt, and finally generating the sentiment prediction using the LLM. The classification step must complete before prompt selection can occur, and prompt selection must complete before prediction.

Design Tradeoffs:
The framework trades off complexity for performance - adding a classification step and requiring different prompts for different samples increases overall system complexity but yields substantial performance gains. The reliance on the peak-end rule as a proxy for causal structure introduces potential errors if the rule doesn't accurately capture the underlying causal mechanisms.

Failure Signatures:
- Poor classification accuracy in the peak-end rule step will cascade to poor prompt selection
- LLMs may fail to follow causal prompts even when correctly selected
- The framework may not generalize to domains where the peak-end rule is less applicable

Three First Experiments:
1. Evaluate classification accuracy of the peak-end rule on held-out data
2. Test causal prompt performance on synthetically generated C1 and C2 samples
3. Compare performance against a baseline that uses uniform processing for all samples

## Open Questions the Paper Calls Out
None

## Limitations
- The causal framework relies heavily on the peak-end rule as a proxy for distinguishing between C1 and C2 processes, which hasn't been rigorously validated for text-based sentiment analysis
- The claim that "sentiment causes review" (C2) conflates linguistic expression patterns with genuine causal mechanisms
- The manual annotation process introduces potential subjectivity and scalability concerns

## Confidence

High confidence: The empirical observation that LLMs perform differently on C1 vs C2 samples under standard prompts

Medium confidence: The peak-end rule classification methodology and its correlation with model performance

Low confidence: The theoretical framing of sentiment-driven review (C2) as a true causal process rather than a linguistic pattern

## Next Checks

1. Conduct ablation studies removing the peak-end rule classification to determine if performance improvements persist when using random or alternative classification schemes

2. Test the causal prompts on multiple diverse datasets beyond Yelp reviews to assess generalizability of the causal framework

3. Implement controlled experiments where human annotators create synthetic C1 and C2 samples to isolate whether the peak-end rule truly captures the proposed causal mechanisms or merely reflects writing style variations