---
ver: rpa2
title: 'System-2 Recommenders: Disentangling Utility and Engagement in Recommendation
  Systems via Temporal Point-Processes'
arxiv_id: '2406.01611'
source_url: https://arxiv.org/abs/2406.01611
tags:
- utility
- user
- where
- engagement
- system-1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of inferring user utility in
  recommender systems, which is complicated by the confounding effects of impulsive
  (System-1) and utility-driven (System-2) decision processes. The authors propose
  a novel approach using a Hawkes process model that disentangles these two components
  by analyzing user return probabilities to the platform.
---

# System-2 Recommenders: Disentangling Utility and Engagement in Recommendation Systems via Temporal Point-Processes

## Quick Facts
- arXiv ID: 2406.01611
- Source URL: https://arxiv.org/abs/2406.01611
- Authors: Arpit Agarwal; Nicolas Usunier; Alessandro Lazaric; Maximilian Nickel
- Reference count: 12
- Primary result: Proposed model disentangles System-1 and System-2 behaviors in recommendation systems, enabling utility optimization over engagement-based methods

## Executive Summary
This paper addresses the challenge of inferring user utility in recommendation systems, where impulsive (System-1) and utility-driven (System-2) decision processes confound each other. The authors propose a Hawkes process model that disentangles these components by analyzing user return probabilities to the platform. By modeling two trigger intensities with different decay rates—one fast-decaying for System-1 and one slow-decaying for System-2—the model captures how only utility-driven interactions create lasting increases in return probability. Under certain conditions, the model parameters are identifiable, allowing accurate disentanglement of System-1 and System-2 behaviors and enabling utility maximization that outperforms engagement-based methods.

## Method Summary
The method models user session arrivals as a Hawkes process with dual trigger intensities representing System-1 (impulse-driven) and System-2 (utility-driven) behaviors. Each user is characterized by two embeddings, and each session is represented by an item embedding. The intensity function combines a base rate with contributions from past sessions weighted by exponential decay terms with different rates (β1 for System-1, β2 for System-2). The infectivity rates are determined by dot products between user and item embeddings, transformed through a link function. Maximum likelihood estimation is used to learn the user embeddings and Hawkes process parameters from observed session sequences. The model assumes known item embeddings and focuses on identifying user-specific parameters.

## Key Results
- Under conditions where decay rates differ significantly (β1 >> β2) and session content vectors span the embedding space, model parameters are identifiable
- Maximum likelihood estimation of the Hawkes process parameters is consistent, converging to true values with sufficient data
- Synthetic experiments demonstrate successful recovery of model parameters and show that utility maximization using estimated System-2 embeddings significantly outperforms engagement-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Hawkes process with dual trigger intensities allows identification of System-1 (impulse) and System-2 (utility) components from return patterns.
- Mechanism: The model uses two exponential decay terms in the intensity function—one fast-decaying for System-1 and one slow-decaying for System-2. Over time, the contribution of System-1 vanishes while System-2 persists, creating observable differences in return patterns.
- Core assumption: The decay rates β1 and β2 are sufficiently different (β1 >> β2) and the user's session content vectors span the full embedding space.
- Evidence anchors:
  - [abstract]: "under certain conditions, the model parameters are identifiable, allowing for accurate disentanglement of System-1 and System-2 behaviors"
  - [section 3]: "We show analytically that given samples it is possible to disentangle System-1 and System-2"
  - [corpus]: Weak evidence - no direct citations, but related work on dual systems exists
- Break condition: If β1 ≈ β2, the two decay processes become indistinguishable; if session vectors don't span the embedding space, parameter recovery fails.

### Mechanism 2
- Claim: Return probability captures long-term utility better than engagement signals because impulsive interactions have short-lived effects on return likelihood.
- Mechanism: Engagement signals like clicks and watch time can be driven by System-1 impulses, but only utility-driven interactions create lasting increases in return probability. The model uses return events as the primary observable rather than engagement metrics.
- Core assumption: Users who derive utility from a platform are more likely to return over time, while purely impulsive users may stop returning once novelty wears off.
- Evidence anchors:
  - [abstract]: "users tend to return to a platform in the long run if it creates utility for them, while pure engagement-driven interactions... may affect user return in the short term but will not have a lasting effect"
  - [section 2.3]: "utility-driven sessions drive sessions in the long-term... moreishness-driven sessions influences sessions only in the short-term"
  - [corpus]: Weak evidence - no direct citations, but aligns with psychological literature
- Break condition: If users develop addictive behaviors where they continue returning despite low utility, or if platform design creates artificial return incentives.

### Mechanism 3
- Claim: Maximum likelihood estimation of the Hawkes process parameters is consistent, enabling reliable disentanglement of System-1 and System-2 behavior.
- Mechanism: The paper proves that under technical conditions (decay rates differ, embeddings are bounded, session vectors span space), the MLE of the Hawkes process parameters converges to true values as sample size increases.
- Core assumption: The Hawkes process is stationary and ergodic, and the nuisance parameters (μ, β1, β2) are identifiable.
- Evidence anchors:
  - [section 3]: "we show analytically that given samples it is possible to disentangle System-1 and System-2 decision-processes"
  - [section 3]: "the MLE of our Hawkes process recommender model is consistent"
  - [corpus]: Weak evidence - no direct citations, but consistency results for Hawkes processes exist in literature
- Break condition: If the underlying assumptions are violated (e.g., non-stationarity, insufficient sample size, or parameter space constraints), consistency may fail.

## Foundational Learning

- Concept: Temporal Point Processes and Hawkes processes
  - Why needed here: The entire model framework relies on modeling user arrival times as self-exciting point processes where past events influence future intensity
  - Quick check question: Can you explain the difference between a standard Poisson process and a Hawkes process, and why the self-exciting property matters for modeling user returns?

- Concept: Identifiability in statistical models
  - Why needed here: The core theoretical contribution is proving that the model parameters are identifiable from observed data, which is necessary for the approach to work
  - Quick check question: What does it mean for a statistical model to be identifiable, and why is this property crucial for parameter estimation?

- Concept: Maximum Likelihood Estimation and consistency
  - Why needed here: The paper uses MLE to estimate parameters and proves consistency, showing that estimates converge to true values with enough data
  - Quick check question: What is the difference between consistency and unbiasedness in parameter estimation, and why is consistency more important for large-scale applications?

## Architecture Onboarding

- Component map: Synthetic data generation -> Hawkes process likelihood computation -> MLE optimization -> Disentangled embeddings -> Content ranking
- Critical path: Session logs → Hawkes process likelihood computation → MLE optimization → Disentangled embeddings → Content ranking
- Design tradeoffs:
  - Using return times vs engagement signals: More aligned with utility but requires longer observation windows
  - Two separate embeddings vs joint embedding: Allows explicit disentanglement but doubles parameter space
  - Exponential decay vs other kernels: Simpler computation but may not capture all temporal dynamics
- Failure signatures:
  - Poor parameter recovery: Indicates violation of identifiability conditions (similar decay rates, insufficient data)
  - Disentanglement fails: Suggests System-1 and System-2 behaviors are too similar in the data
  - Utility optimization doesn't improve: Could indicate weak correlation between return patterns and actual utility
- First 3 experiments:
  1. Synthetic data generation with known parameters: Verify the algorithm can recover parameters across different signal-to-noise ratios
  2. Parameter identifiability test: Fix all parameters except one and verify the likelihood surface has a clear global optimum
  3. Utility maximization evaluation: Compare recommendation quality using estimated System-2 embeddings vs using engagement-based methods on synthetic data with controlled utility-engagement alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Hawkes process model be extended to handle non-stationary user arrival rates due to seasonality, policy changes, or evolving user preferences?
- Basis in paper: [explicit] The authors acknowledge the potential for non-stationarity in user arrival rates and suggest that some effects like seasonality can be incorporated by adding a time-dependent base intensity to the Hawkes process model.
- Why unresolved: The paper focuses on the core model of disentangling System-1 and System-2 behaviors, and a detailed exploration of handling non-stationarity is beyond the scope of the current work.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of the extended model in handling non-stationary user arrival rates in real-world datasets would provide strong evidence.

### Open Question 2
- Question: How can the model be adapted to handle item embeddings that are not known a priori and need to be learned jointly with user embeddings?
- Basis in paper: [explicit] The authors assume known item embeddings due to the abundance of item-level data. They acknowledge that joint learning of item and user features is possible but would make the problem of joint identifiability and learning more challenging.
- Why unresolved: The paper focuses on the case where item embeddings are known, and the joint learning scenario is left for future work.
- What evidence would resolve it: Theoretical analysis establishing the conditions for joint identifiability and experimental results demonstrating the performance of the model with jointly learned item and user embeddings would provide strong evidence.

### Open Question 3
- Question: How can the model be extended to handle sessions where the length is correlated with System-2 utility, introducing entanglement between observed session lengths and arrival times?
- Basis in paper: [explicit] The authors mention this as a limitation and an interesting direction for future work, noting that it would be interesting to strengthen the theoretical results by providing an analysis for this case.
- Why unresolved: The current model assumes that sessions are generated independently of previous arrival times, and handling correlated session lengths is beyond the scope of the current work.
- What evidence would resolve it: Theoretical analysis establishing the conditions for identifiability and consistency in the presence of correlated session lengths, along with experimental results demonstrating the model's performance in such scenarios, would provide strong evidence.

## Limitations

- The model requires strong assumptions including significantly different decay rates (β1 >> β2) and spanning session content vectors, which may not hold in real-world data
- Experimental validation is limited to synthetic data, leaving questions about real-world applicability and robustness to violations of model assumptions
- The approach relies on return probabilities as proxies for utility, assuming a stable relationship that may not persist across different platform designs or user demographics

## Confidence

**High Confidence**: The mathematical framework for the Hawkes process model and the identifiability proof under stated conditions. The synthetic data experiments successfully demonstrate parameter recovery in controlled settings.

**Medium Confidence**: The assumption that return probabilities are reliable indicators of utility, and that System-1 and System-2 behaviors can be cleanly separated in practice. The claim that engagement optimization leads to poor utility outcomes.

**Low Confidence**: The generalizability of results to real-world recommendation systems, the robustness to violations of model assumptions, and the practical benefits of disentanglement in terms of recommendation quality.

## Next Checks

1. **Real Data Validation**: Apply the model to logged user interaction data from an existing platform, comparing return-based utility optimization against engagement-based methods on held-out test sets.

2. **Assumption Stress Testing**: Systematically relax key assumptions (β1 ≈ β2, incomplete span of session vectors) in synthetic experiments to quantify the impact on identifiability and parameter recovery.

3. **Alternative Proxy Validation**: Compare return probabilities against multiple utility proxies (user satisfaction surveys, retention after extended periods, revenue) to validate the assumed relationship between returns and utility.