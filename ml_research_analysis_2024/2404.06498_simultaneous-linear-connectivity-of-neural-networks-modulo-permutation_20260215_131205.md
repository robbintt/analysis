---
ver: rpa2
title: Simultaneous linear connectivity of neural networks modulo permutation
arxiv_id: '2404.06498'
source_url: https://arxiv.org/abs/2404.06498
tags:
- networks
- barrier
- permutation
- loss
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the hypothesis that permutation symmetries
  are the only source of non-convexity in neural network loss landscapes, implying
  that networks can be linearly connected if appropriately permuted. The authors formalize
  three distinct claims of increasing strength regarding linear mode connectivity
  modulo permutation, distinguishing between weak and strong connectivity.
---

# Simultaneous linear connectivity of neural networks modulo permutation

## Quick Facts
- arXiv ID: 2404.06498
- Source URL: https://arxiv.org/abs/2404.06498
- Reference count: 40
- Primary result: A single permutation can simultaneously align two networks along both training trajectories and iterative pruning sequences

## Executive Summary
This paper investigates whether permutation symmetries are the only source of non-convexity in neural network loss landscapes, which would imply networks can be linearly connected if appropriately permuted. The authors formalize three distinct claims of increasing strength regarding linear mode connectivity modulo permutation: weak, strong, and simultaneous weak connectivity. Through empirical studies on VGG-16 and ResNet-20 architectures trained on CIFAR datasets, they demonstrate that a single permutation aligning two dense trained networks also aligns corresponding partially trained networks throughout training and matching subnetworks found via iterative magnitude pruning. The work provides preliminary evidence that strong linear connectivity may emerge with increased network width.

## Method Summary
The authors train two independent neural networks with identical architectures on the same dataset but different initializations and minibatch orders. They then apply permutation finding algorithms (weight matching and activation matching) to align the networks by finding permutations that map neurons between corresponding layers. The linear mode connectivity is evaluated by measuring loss and error barriers along linear interpolation paths between the aligned networks. The study extends to examining connectivity across training trajectories and sparse subnetworks obtained through iterative magnitude pruning, as well as investigating how error barriers change with network width when considering three-network alignments.

## Key Results
- A single permutation can simultaneously align two networks along both training trajectories and iterative pruning sequences
- Networks exhibit low loss barriers at each step of optimization and sparsification trajectories when appropriately permuted
- Error barriers between three networks decrease with increasing network width when aligned relative to a reference network
- The same permutation aligning dense trained networks also linearly connects sequences of sparse networks from iterative magnitude pruning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Permutation symmetries are the only source of non-convexity in neural network loss landscapes.
- Mechanism: By applying an appropriate permutation to one of two independently trained networks, the networks can be linearly connected without encountering a high loss barrier.
- Core assumption: The loss landscape is convex after accounting for permutation symmetries.
- Evidence anchors:
  - [abstract] "Recent work has argued that permutation symmetries are the only sources of non-convexity, meaning there are essentially no such barriers between trained networks if they are permuted appropriately."
  - [section 4] "In this work, we introduce an intermediate claimâ€”that for certain sequences of networks, there exists one permutation that simultaneously aligns matching pairs of networks from these sequences."
- Break condition: If the loss landscape contains non-convexities that are not due to permutation symmetries, then this mechanism would fail.

### Mechanism 2
- Claim: Simultaneous weak linear connectivity modulo permutation holds for sequences of iteratively trained networks.
- Mechanism: A single permutation that aligns two dense trained networks also aligns corresponding partially trained networks throughout training and matching subnetworks found via iterative magnitude pruning.
- Core assumption: The permutation aligning two networks at the end of training remains effective throughout the entire training trajectory and for all iterations of iterative pruning.
- Evidence anchors:
  - [section 5.1] "We discover that a single permutation aligns sequences of iteratively trained as well as iteratively pruned networks, meaning that two networks exhibit low loss barriers at each step of their optimization and sparsification trajectories respectively."
  - [section 5.2] "We show that a permutation aligning two dense trained networks also linearly connects the sequences of sparse networks obtained from the same dense networks via IMP."
- Break condition: If the permutation found at the end of training does not align the networks throughout the training process or for all iterations of pruning, this mechanism would fail.

### Mechanism 3
- Claim: Strong linear connectivity modulo permutation may be possible under certain conditions, such as increased network width.
- Mechanism: As network width increases, the error barriers between three networks decrease when aligned relative to a reference network, suggesting that sufficiently wide networks exhibit strong connectivity.
- Core assumption: Increased network width reduces the error barriers between networks aligned relative to a reference network.
- Evidence anchors:
  - [abstract] "Finally, we provide the first evidence that strong linear connectivity may be possible under certain conditions, by showing that barriers decrease with increasing network width when interpolating among three networks."
  - [section 5.3] "We provide the first evidence towards strong linear connectivity modulo permutation, by showing that the barrier between two independently trained networks can be reduced by alignment with a third, unrelated network. As this effect scales with width, we conjecture that sufficiently wide networks exhibit strong connectivity."
- Break condition: If increasing network width does not reduce the error barriers between aligned networks, this mechanism would fail.

## Foundational Learning

- Concept: Permutation symmetries in neural networks
  - Why needed here: Understanding permutation symmetries is crucial for grasping how networks can be linearly connected modulo permutation.
  - Quick check question: What are permutation symmetries in the context of neural networks?

- Concept: Linear mode connectivity
  - Why needed here: Linear mode connectivity is the property that allows networks to be linearly connected without encountering high loss barriers.
  - Quick check question: What is linear mode connectivity, and why is it important for understanding the loss landscape of neural networks?

- Concept: Iterative magnitude pruning (IMP)
  - Why needed here: IMP is a procedure used to find sparse subnetworks ("winning tickets") that are matching, meaning they have the same accuracy as their dense counterpart.
  - Quick check question: What is iterative magnitude pruning, and how does it relate to the concept of "winning tickets" in neural networks?

## Architecture Onboarding

- Component map:
  Network Training -> Permutation Finding (Weight Matching/Activation Matching) -> Barrier Computation -> Connectivity Analysis

- Critical path:
  1. Train two networks independently.
  2. Apply permutation finding algorithms to align the networks.
  3. Compute the loss barrier between the aligned networks.
  4. If the loss barrier is low, the networks are linearly connected modulo permutation.

- Design tradeoffs:
  - Weight matching vs. activation matching: Weight matching is more dependent on large-magnitude weights, while activation matching is generally more robust across different settings.
  - Direct alignment vs. indirect alignment: Direct alignment aligns two networks directly, while indirect alignment aligns them relative to a reference network. Indirect alignment may be more effective for finding strong linear connectivity.

- Failure signatures:
  - High loss barriers between aligned networks indicate that the permutation finding algorithms have failed to find an effective permutation.
  - Failure to find a permutation that aligns networks throughout the entire training trajectory or for all iterations of pruning indicates that the assumption of simultaneous weak linear connectivity modulo permutation may not hold.

- First 3 experiments:
  1. Train two VGG-16 networks on CIFAR-10 with different initializations and minibatch orders. Apply weight matching and activation matching to align the networks, and compute the loss barrier between the aligned networks.
  2. Apply iterative magnitude pruning to the trained networks from experiment 1, and apply the same permutation that aligned the dense networks to the sparse subnetworks. Compute the loss barrier between the aligned sparse subnetworks.
  3. Train three VGG-16 networks on CIFAR-10 with different initializations and minibatch orders. Align two of the networks relative to the third using both weight matching and activation matching, and compute the loss barrier between the indirectly aligned networks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact mathematical conditions under which strong linear connectivity modulo permutation (SLC mod P) emerges in neural networks, particularly in relation to width, depth, and architecture?
- Basis in paper: [explicit] The paper provides empirical evidence that barriers between three networks decrease with increasing network width, suggesting SLC mod P may emerge with sufficient width, but does not establish precise mathematical conditions.
- Why unresolved: The paper only provides preliminary evidence for SLC mod P in wide networks and conjectures about its existence without proving general conditions. The relationship between width and barrier reduction is empirical but not theoretically explained.
- What evidence would resolve it: Formal proofs showing the relationship between network width, depth, architecture, and barrier behavior under permutation alignment. Mathematical characterization of when a single permutation can simultaneously align multiple networks.

### Open Question 2
- Question: How do different permutation-finding algorithms (weight matching vs. activation matching) compare in their ability to discover permutations that minimize loss barriers, particularly across different training stages and architectures?
- Basis in paper: [explicit] The paper identifies specific limitations of weight matching algorithms - they are equivalent to activation matching only on trained networks, rely on larger magnitude weights appearing later in training, and align lower layers earlier in training.
- Why unresolved: The paper observes algorithmic limitations but does not provide a comprehensive theoretical understanding of why these differences exist or how to improve the algorithms systematically.
- What evidence would resolve it: Comparative studies of permutation-finding algorithms across diverse architectures, training stages, and optimization objectives. Development of unified theoretical framework explaining when each algorithm succeeds or fails.

### Open Question 3
- Question: What is the precise relationship between network stability to SGD noise and the existence of permutation symmetries that enable linear connectivity?
- Basis in paper: [explicit] The paper shows that network stability at initialization is neither necessary nor sufficient for finding permutation symmetries that enable linear connectivity, challenging previous assumptions about this relationship.
- Why unresolved: The paper demonstrates this through counterexamples but does not provide a complete theoretical framework for understanding when stability and permutation alignment are related.
- What evidence would resolve it: Theoretical analysis of the relationship between SGD noise stability, loss landscape geometry, and permutation symmetries. Experimental studies across different training regimes and architectures to characterize conditions for both properties.

### Open Question 4
- Question: How does the existence of simultaneous weak linear connectivity modulo permutation (simultaneous WLC mod P) impact practical applications like federated learning, model merging, and lottery ticket hypothesis research?
- Basis in paper: [explicit] The paper demonstrates that a single permutation can align training trajectories and sparse subnetworks simultaneously, suggesting potential applications, but does not explore practical implications.
- Why unresolved: The paper establishes the theoretical possibility but does not investigate concrete applications or quantify the benefits in real-world scenarios.
- What evidence would resolve it: Case studies applying simultaneous WLC mod P in federated learning systems, model merging protocols, and lottery ticket research. Performance comparisons between using single permutations versus pairwise alignments in these applications.

## Limitations
- Empirical evidence for strong linear connectivity remains preliminary and conditional on network width
- The claim that permutation symmetries are the only source of non-convexity is theoretical and not definitively proven
- Results focus primarily on VGG-16 and ResNet-20 architectures on standard image classification datasets

## Confidence
- **High confidence**: Empirical demonstration of simultaneous weak linear connectivity modulo permutation (Section 5.1-5.2)
- **Medium confidence**: Evidence that error barriers decrease with width for strong connectivity (Section 5.3)
- **Low confidence**: Theoretical claim that permutation symmetries are the only source of non-convexity

## Next Checks
1. **Width sensitivity validation**: Systematically vary network width across a broader range (not just doubling) to establish the precise relationship between width and error barriers for strong connectivity claims.

2. **Cross-architecture generalization**: Test simultaneous connectivity claims on architectures beyond VGG-16 and ResNet-20 (e.g., Transformers, MLPs) to assess whether permutation-based alignment generalizes across different network families.

3. **Alternative dataset validation**: Evaluate the simultaneous connectivity phenomenon on non-image datasets (text, audio, tabular) to determine if the observed patterns are specific to image classification or represent a more general property of neural network loss landscapes.