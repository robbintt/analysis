---
ver: rpa2
title: A Sample Efficient Alternating Minimization-based Algorithm For Robust Phase
  Retrieval
arxiv_id: '2409.04733'
source_url: https://arxiv.org/abs/2409.04733
tags: []
core_contribution: This work addresses robust phase retrieval with arbitrary corruptions,
  aiming to recover a signal from magnitude-only measurements when a fraction of responses
  are arbitrarily corrupted. The authors propose an alternating minimization algorithm
  that combines data preprocessing with an oracle solver for a nonconvex optimization
  problem.
---

# A Sample Efficient Alternating Minimization-based Algorithm For Robust Phase Retrieval

## Quick Facts
- **arXiv ID:** 2409.04733
- **Source URL:** https://arxiv.org/abs/2409.04733
- **Reference count:** 40
- **Primary result:** Nearly linear sample complexity O(d polylog(d)) for robust phase retrieval with sparse arbitrary corruptions

## Executive Summary
This work presents a sample-efficient alternating minimization algorithm for robust phase retrieval in the presence of arbitrary corruptions. The algorithm combines data preprocessing with a simple gradient descent oracle to recover signals from magnitude-only measurements when a fraction of responses are corrupted. Under a sparse corruption model where an adversary can corrupt up to k out of n measurements, the method achieves O(√ε) error with only O(d polylog(d)) measurements when k = O(n^{1-p}) for p ∈ (0,1]. The approach avoids computationally intensive spectral initialization by using random initialization with gradient descent, providing competitive performance compared to existing methods like Median RWF and PhaseLift.

## Method Summary
The algorithm employs an alternating minimization framework that alternates between filtering corrupted measurements and solving a nonconvex least squares problem. First, it preprocesses the data by discarding negative responses and trimming the largest values to form a clean measurement set. Then it iteratively refines the signal estimate by solving a nonconvex LSQ problem on subsets of the filtered measurements using gradient descent with random initialization. The method achieves convergence in O(Py_i^2 / 4(n-k)ε^2) iterations and provides explicit polynomial dependence of convergence rate on the corruption fraction. The theoretical analysis shows that under the assumption that corruptions are independent of covariates, the loss landscape maintains benign geometry with all critical points being either strict saddles or global minima.

## Key Results
- Achieves d(θ̂, θ*) ≤ O(√ε) error with O(d polylog(d)) measurements
- Avoids computationally intensive spectral initialization using simple gradient descent with random initialization
- Converges in O(Py_i^2 / 4(n-k)ε^2) iterations with explicit polynomial dependence on corruption fraction
- Demonstrates competitive performance compared to Median RWF and PhaseLift with better runtime scaling for larger dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating minimization converges to a stationary point that is close to the true signal despite arbitrary corruptions
- Mechanism: The algorithm alternates between filtering measurements to remove obviously corrupted data, and solving a nonconvex LSQ problem on the filtered set. This process iteratively refines both the parameter estimate and the set of reliable measurements.
- Core assumption: The preprocessing step successfully removes measurements with corruption ηᵢ = Ω(log n) with high probability, leaving a set with bounded corruption
- Break condition: If corruption proportion ϵ does not vanish (i.e., k/n = constant), exact recovery is impossible with constant probability

### Mechanism 2
- Claim: The loss landscape geometry remains tractable for efficient optimization even with sparse arbitrary outliers
- Mechanism: Under Assumption 1 (corruption ηᵢ independent of covariates xi), the expected loss function FU(θ) maintains benign geometric properties - all critical points are either strict saddles or global minima
- Core assumption: Corruption is independent of the covariates (sparse arbitrary outliers model)
- Break condition: If corruption depends on covariates, the benign geometry may not be preserved

### Mechanism 3
- Claim: Sample complexity remains nearly linear in dimension despite corruption
- Mechanism: The algorithm achieves d(θ̂, θ*) ≤ O(√ε) error with only O(d polylog(d)) measurements when k = O(n^(1-p)) for p ∈ (0,1]
- Core assumption: Corruption proportion ϵ vanishes with n (i.e., ϵ ∈ K where k/n → 0 as n → ∞)
- Break condition: When ϵ is constant, exact recovery is impossible

## Foundational Learning

- **Concept: Phase retrieval problem formulation**
  - Why needed here: Understanding that yi = ⟨xi, θ*⟩² represents magnitude-only measurements where phase information is lost
  - Quick check question: Why can we only recover θ* up to sign in the uncorrupted case?

- **Concept: Alternating minimization framework**
  - Why needed here: The algorithm alternates between estimating θ given a set of measurements, and selecting which measurements to use
  - Quick check question: What are the two subproblems being solved in each iteration of ALT-MIN-PHASE?

- **Concept: Benign nonconvex geometry**
  - Why needed here: The key insight that allows efficient optimization is that all local minima are global minima and all saddles are strict
  - Quick check question: What geometric property of the loss function allows random initialization to succeed in phase retrieval?

## Architecture Onboarding

- **Component map:** Preprocessing -> Alternating minimization loop -> LSQ-PHASE-ORACLE -> Convergence check

- **Critical path:**
  1. Preprocessing creates filtered set with |~S| = n - k measurements
  2. ALT-MIN-PHASE iterates: select U ⊂ ~S with |U| = n - 2k, solve for θ using LSQ-PHASE-ORACLE
  3. Convergence when objective decrease < β = ε²
  4. Output θ̂ and final measurement set Û

- **Design tradeoffs:**
  - Preprocessing removes some good measurements but eliminates obviously corrupted ones
  - Using n - 2k measurements balances noise reduction with sample efficiency
  - Gradient descent vs spectral initialization trades computational efficiency for potentially slower convergence

- **Failure signatures:**
  - High relative error despite many iterations suggests corruption proportion too large (ε not vanishing)
  - Slow convergence suggests need to tune step size or check gradient computation
  - Poor initialization performance suggests corruption too dependent on covariates

- **First 3 experiments:**
  1. Verify preprocessing removes ~k measurements and that remaining yi values are bounded
  2. Test LSQ-PHASE-ORACLE on clean data to confirm convergence to true θ*
  3. Run full algorithm with k = √n corruption on synthetic data and verify relative error decreases with more measurements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the algorithm be extended to handle constant corruption proportions under Assumption 1?
- Basis in paper: The paper states "Future research could explore extending our analysis to regimes with constant corruption proportions under Assumption 1."
- Why unresolved: The current theoretical analysis only works for vanishing corruption proportions (k = O(n^{1-p}) for p ∈ (0,1]). The paper acknowledges this limitation but does not provide a solution.
- What evidence would resolve it: A proof showing convergence guarantees for the algorithm when k/n is constant, along with experimental validation on corrupted datasets with constant corruption ratios.

### Open Question 2
- Question: How would the algorithm perform with non-Gaussian covariate distributions?
- Basis in paper: The current analysis assumes Gaussian design where each entry x_ij is drawn i.i.d. from N(0,1). The paper does not discuss performance under other distributions.
- Why unresolved: The Gaussian assumption is crucial for many of the concentration inequalities used in the analysis, and it's unclear if the same guarantees hold for other distributions.
- What evidence would resolve it: Experimental results comparing the algorithm's performance on datasets with different covariate distributions (e.g., uniform, exponential) and theoretical analysis extending the convergence guarantees to non-Gaussian settings.

### Open Question 3
- Question: Can we develop oracles that do not require Assumption 1?
- Basis in paper: The paper states "It would also be interesting to investigate oracles that do not require Assumption 1."
- Why unresolved: The current efficient oracle construction relies on the assumption that corruptions are independent of covariates. This is a strong assumption that may not hold in many practical scenarios.
- What evidence would resolve it: Development of an oracle construction that works under the original strong corruption model without requiring independence between corruptions and covariates, accompanied by theoretical analysis and experimental validation.

## Limitations
- The theoretical guarantees only hold for vanishing corruption proportions (k/n → 0), not for constant corruption ratios
- Requires corruption to be independent of covariates (Assumption 1), which may not hold in many practical scenarios
- Exact implementation details of the gradient descent oracle with corruption handling are referenced but not fully specified

## Confidence

- **High confidence**: The alternating minimization framework and preprocessing steps are well-established techniques that should work as described for sparse corruption models
- **Medium confidence**: The sample complexity claims and convergence rates are theoretically sound but may be conservative in practice
- **Low confidence**: The specific numerical constants and implementation details for the oracle subroutine are not fully specified

## Next Checks

1. **Validate corruption regime assumptions**: Test the algorithm with various corruption levels ε to empirically verify when the theoretical conditions break down and relative error starts degrading

2. **Benchmark against oracle assumptions**: Compare the simple gradient descent oracle performance against the ideal LSQ-PHASE-ORACLE to quantify the impact of using an efficient but approximate solver

3. **Runtime scaling verification**: Systematically measure runtime scaling as d increases to confirm the claimed advantages over Median RWF and PhaseLift in the large-dimension regime