---
ver: rpa2
title: 'Towards scientific discovery with dictionary learning: Extracting biological
  concepts from microscopy foundation models'
arxiv_id: '2412.16247'
source_url: https://arxiv.org/abs/2412.16247
tags:
- features
- learning
- feature
- figure
- cells
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that dictionary learning can extract biologically
  meaningful concepts from microscopy foundation models, opening a path to scientific
  discovery via mechanistic interpretability. The authors propose a new algorithm,
  Iterative Codebook Feature Learning (ICFL), combined with PCA whitening on control
  data, to improve feature selectivity.
---

# Towards scientific discovery with dictionary learning: Extracting biological concepts from microscopy foundation models

## Quick Facts
- arXiv ID: 2412.16247
- Source URL: https://arxiv.org/abs/2412.16247
- Reference count: 25
- Key outcome: Dictionary learning with ICFL and PCA whitening extracts biologically meaningful concepts from microscopy foundation models

## Executive Summary
This work demonstrates that dictionary learning can extract biologically meaningful concepts from microscopy foundation models, opening a path to scientific discovery via mechanistic interpretability. The authors propose a new algorithm, Iterative Codebook Feature Learning (ICFL), combined with PCA whitening on control data, to improve feature selectivity. Experiments on masked autoencoders trained on cell microscopy images show that ICFL features achieve higher selectivity scores than TopK sparse autoencoders for biological concepts like cell types and genetic perturbations. The approach preserves significant linear probing signals, and features correlate with specific cellular morphologies, validated through expert analysis of single-cell resolution.

## Method Summary
The method combines a pre-trained masked autoencoder with Iterative Codebook Feature Learning (ICFL) and PCA whitening on control data. The MAE generates intermediate representations from cell microscopy images, which are then whitened using PCA derived from unperturbed control cells. ICFL extracts sparse features by iteratively selecting feature directions aligned with residuals, solving for sparse reconstruction, and updating the residual. These features are evaluated using linear probing on classification tasks and selectivity scores for biological concepts.

## Key Results
- ICFL features achieve higher selectivity scores than TopK SAEs for biological concepts like cell types and genetic perturbations
- The learned features preserve significant linear probing signals, maintaining biologically-meaningful information
- Features correlate with specific cellular morphologies, validated through expert analysis at single-cell resolution
- ICFL features almost match the selectivity scores of human-designed features (CellProfiler) with Pearson correlation 0.71

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PCA whitening on a control dataset acts as weak supervision that improves feature selectivity by downweighting dominant directions that do not correspond to biological perturbations.
- Mechanism: The PCA transform learns to whiten the control data (unperturbed cells), effectively removing variance due to batch effects and other non-perturbation-related sources. When applied to the full dataset, this pre-processing step reduces the influence of these dominant directions during dictionary learning, allowing the algorithm to focus on perturbations of interest.
- Core assumption: Dominant directions in control data do not correspond to meaningful biological concepts (perturbations), so downweighting them improves concept extraction.
- Evidence anchors:
  - [abstract] "we propose a novel combination of a sparse DL algorithm... with a PCA whitening pre-processing step derived from control data"
  - [section] "we use a dataset of control samples as a form of weak supervision, downweighting dominant directions in this control dataset as we know they do not correspond to the biological perturbations of interest"
  - [corpus] Weak - no direct mention of PCA whitening in corpus papers, though related concepts exist

### Mechanism 2
- Claim: Iterative Codebook Feature Learning (ICFL) reduces "dead features" by updating sparse features using residual-based orthogonal matching pursuit rather than a fixed encoder.
- Mechanism: ICFL iteratively selects top-k feature directions aligned with the current residual, solves for sparse reconstruction using only those directions, then updates the residual. This process allows later iterations to capture features not correlated with dominant directions, naturally avoiding features that would otherwise be "dead."
- Core assumption: Feature directions that are not correlated with dominant patterns in the data are still meaningful and can be captured through residual-based selection.
- Evidence anchors:
  - [section] "The key idea of ICFL is that early iterations subtract dominant feature directions from x, allowing the algorithm in later iterations to select a broader set of feature directions that are not as correlated with the main features in x"
  - [section] "Consequently, z is at most J k-sparse. We call the columns of Wdec the corresponding feature directions of z"
  - [corpus] Weak - corpus papers mention SAEs and dictionary learning but don't specifically discuss ICFL's residual-based approach

### Mechanism 3
- Claim: Linear probing signals are preserved when dictionary features are extracted, indicating that sparse representations retain biologically-relevant information.
- Mechanism: The sparse dictionary features learned from intermediate model representations maintain sufficient information to achieve high accuracy on classification tasks (cell types, perturbations, gene groups), suggesting that the compressed sparse representation still contains the essential biological signals.
- Core assumption: The linear probing performance is a valid proxy for whether the extracted features contain biologically-relevant information.
- Evidence anchors:
  - [abstract] "we show that the learned features preserve significant amounts of biologically-meaningful information"
  - [section] "Figure 2a shows that almost the entire signal is preserved for simple concepts such as cell types (1), batch effects (2) and perturbations with strong morphological changes (4)"
  - [corpus] Moderate - corpus papers mention similar probing techniques but don't specifically address preservation of biological signals

## Foundational Learning

- Concept: Dictionary Learning and Sparse Coding
  - Why needed here: The paper relies on extracting sparse features from model representations, which is fundamentally a dictionary learning problem where the goal is to find a sparse representation of data using an overcomplete dictionary.
  - Quick check question: What is the main difference between a standard autoencoder and a sparse autoencoder in the context of dictionary learning?

- Concept: Linear Probing for Feature Evaluation
  - Why needed here: The paper uses linear probing to assess whether extracted features preserve biologically-relevant information, requiring understanding of how linear classifiers can evaluate representation quality.
  - Quick check question: How does linear probing help determine if extracted features contain meaningful information about biological concepts?

- Concept: Control Datasets and Weak Supervision
  - Why needed here: The PCA whitening approach uses control (unperturbed) data as weak supervision to guide feature extraction, requiring understanding of how control datasets can inform learning in unsupervised settings.
  - Quick check question: Why would using unperturbed cell images as control data help improve feature extraction for perturbed cells?

## Architecture Onboarding

- Component map: input image -> MAE representation -> PCA whitening -> ICFL feature extraction -> linear probe evaluation
- Critical path: The critical path is: input image → MAE representation → PCA whitening → ICFL feature extraction → linear probe evaluation. Each step must work correctly for the system to extract meaningful biological concepts.
- Design tradeoffs: The tradeoff between reconstruction quality and feature selectivity - more aggressive sparsity (fewer non-zero elements) may improve interpretability but reduce reconstruction accuracy and potentially lose biological information. Another tradeoff is between model complexity (larger MAEs) and computational cost versus feature quality.
- Failure signatures: Low selectivity scores across all features (indicating no meaningful biological concepts were extracted), poor linear probing performance on reconstructions (indicating loss of biological information), or high reconstruction error (indicating the sparse representation is too lossy).
- First 3 experiments:
  1. Run ICFL with and without PCA whitening on a small subset of data to observe the effect on feature selectivity scores.
  2. Compare linear probing accuracy on original representations versus reconstructions from both ICFL and TopK SAEs.
  3. Visualize the top correlated and anti-correlated crops for selected features to qualitatively assess biological interpretability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between sparsity level and feature selectivity for biological concepts in microscopy foundation models?
- Basis in paper: [explicit] The paper shows Figure 2b comparing selectivity scores across different sparsity levels for classification Task 5, noting that "increasing the number of non-zeros improves the accuracy, the effect is limited compared to PCA whitening."
- Why unresolved: While the paper demonstrates that selectivity improves with increased sparsity up to a point, it doesn't identify an optimal sparsity level or explain the diminishing returns observed. The relationship appears non-linear and task-dependent.
- What evidence would resolve it: Systematic experiments varying sparsity levels across multiple biological concepts while measuring both selectivity scores and reconstruction quality would clarify the optimal trade-off point.

### Open Question 2
- Question: Can dictionary learning extract linear features for more subtle morphological changes that current methods fail to capture?
- Basis in paper: [explicit] The paper states "we see significant drops in their linear-probing performance on tasks that involve more subtle changes in morphology" and questions "whether these more subtle changes are simply not represented linearly in embedding space."
- Why unresolved: The paper demonstrates success with coarse-grained biological concepts but shows limitations with subtle morphological changes, raising fundamental questions about the nature of feature representation in these models.
- What evidence would resolve it: Developing new dictionary learning algorithms or combining them with non-linear feature extraction methods, then testing on datasets with known subtle morphological variations, would determine if this limitation is methodological or fundamental.

### Open Question 3
- Question: How do learned features from foundation models compare to human-designed features in terms of capturing biologically meaningful information?
- Basis in paper: [explicit] The paper directly compares ICFL features to CellProfiler features in Figure 3e-3g, showing "features extracted by ICFL almost match the selectivity scores of the handcrafted, human-designed features" with a Pearson correlation coefficient of 0.71.
- Why unresolved: While the paper shows comparable performance, it doesn't establish which approach captures more biologically relevant information or under what conditions one outperforms the other.
- What evidence would resolve it: Comprehensive benchmarking using expert biological validation across diverse perturbation types, measuring both selectivity scores and biological interpretability, would reveal the relative strengths of each approach.

## Limitations
- The paper provides limited ablation studies comparing ICFL's unique mechanisms against alternatives, making it unclear which components are essential for success
- Selectivity scores and linear probing results are primarily evaluated on a single dataset (RxRx1) and may not generalize to other biological domains
- The claim that ICFL's specific iterative mechanism is superior to other sparse coding approaches lacks direct empirical support through ablation studies

## Confidence
- High: The basic methodology of using dictionary learning with PCA whitening on control data is sound and technically implementable
- Medium: The selectivity scores and linear probing results suggest ICFL features capture biological concepts, though the magnitude of improvement over baselines could be context-dependent
- Low: The claim that ICFL's specific iterative mechanism is superior to other sparse coding approaches lacks direct empirical support through ablation studies

## Next Checks
1. **Ablation study**: Compare ICFL against standard dictionary learning with different initialization strategies (random, K-means, and residual-based) to isolate the contribution of the iterative mechanism versus other design choices
2. **Generalization test**: Apply the same methodology to a different biological dataset (e.g., single-cell RNA-seq or a different microscopy modality) to assess whether the approach generalizes beyond Cell Painting images
3. **Feature interpretability validation**: Conduct a blinded expert review where domain biologists evaluate feature visualizations without knowing which method (ICFL vs. TopK SAE) generated them, to assess whether ICFL truly produces more interpretable biological concepts