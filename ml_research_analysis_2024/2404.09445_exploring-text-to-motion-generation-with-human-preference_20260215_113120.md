---
ver: rpa2
title: Exploring Text-to-Motion Generation with Human Preference
arxiv_id: '2404.09445'
source_url: https://arxiv.org/abs/2404.09445
tags:
- preference
- motion
- learning
- data
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores preference learning for text-to-motion generation,
  a task where current methods rely on expensive expert labelers with motion capture
  systems. Instead, the authors propose using human preference data, where labelers
  simply compare two generated motions without needing expertise.
---

# Exploring Text-to-Motion Generation with Human Preference

## Quick Facts
- **arXiv ID**: 2404.09445
- **Source URL**: https://arxiv.org/abs/2404.09445
- **Reference count**: 40
- **Primary result**: Preference learning using human judgments significantly improves text-to-motion generation, with Direct Preference Optimization (DPO) outperforming Reinforcement Learning from Human Feedback (RLHF).

## Executive Summary
This paper pioneers the use of human preference data for improving text-to-motion generation, addressing the limitation of current methods that rely on expensive expert labelers with motion capture systems. The authors annotate 3,528 preference pairs generated by MotionGPT and systematically investigate various algorithms for learning from this preference data. Their experimental results demonstrate that preference learning can greatly enhance current text-to-motion generative models, with DPO showing superior performance over RLHF due to reduced overfitting on the limited dataset.

## Method Summary
The authors collect human preference data by having annotators compare pairs of generated motions without requiring motion expertise. They explore two main approaches: Reinforcement Learning from Human Feedback (RLHF) with a reward model trained via Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO) which trains directly on preference pairs using a supervised loss. The DPO approach uses the IPO loss variant with LoRA regularization (rank=8, alpha=16) and beta=0.1. Models are evaluated on alignment metrics (R-precision, MM Dist, MModality), quality metrics (FID, Diversity), and human preference win rates on the HumanML3D test set.

## Key Results
- DPO outperforms RLHF on text-to-motion generation, with RLHF suffering from reward model overfitting on limited data
- Preference data with strong degrees ("Much better" labels) significantly contributes to performance gains in R-precision
- Human evaluators prefer DPO outputs over baseline MotionGPT and RLHF outputs
- R-precision and FID metrics show poor correlation with human evaluation, suggesting preference learning could serve as a better evaluation metric

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO performs better than RLHF because RLHF overfits on limited data and its reward model fails to generalize
- Mechanism: DPO trains directly on preference pairs using a simple supervised loss without an intermediate reward model, avoiding the generalization gap and reward hacking that occurs in RLHF
- Core assumption: The Bradley-Terry model is a good proxy for human preference with limited training data
- Evidence anchors: Abstract finding that DPO outperforms RLHF due to overfitting susceptibility; section discussion on avoiding reward hacking

### Mechanism 2
- Claim: Preference data with pronounced degrees significantly enhances R-precision
- Mechanism: Strong preference labels provide clearer optimization signals with larger gradients during training
- Core assumption: Degree strength correlates with quality distinctions and provides better training signals
- Evidence anchors: Abstract observation about pronounced preference contributions; finding that "Much better" pairs provide most performance gains

### Mechanism 3
- Claim: Scarcity of large-scale text-motion pairs causes reward model overfitting in RLHF
- Mechanism: Limited training data causes the reward model to memorize specific patterns rather than learning generalizable reward functions
- Core assumption: Reward model performance degrades significantly on small datasets, directly impacting RLHF effectiveness
- Evidence anchors: Abstract noting propensity for reward model overfitting; section findings on scarcity leading to overfitting

## Foundational Learning

- **Autoregressive sequence modeling**: MotionGPT uses autoregressive Transformers to generate motion tokens conditioned on text prompts, requiring understanding of sequence-to-sequence modeling and tokenization
- **Preference learning and the Bradley-Terry model**: The paper uses preference learning algorithms that rely on the Bradley-Terry model to infer scalar rewards from pairwise comparisons
- **Reinforcement Learning and Policy Gradient methods**: RLHF uses reinforcement learning to fine-tune the policy against a learned reward model, requiring understanding of policy gradient methods and training challenges with learned rewards

## Architecture Onboarding

- **Component map**: MotionGPT (pretrained autoregressive model) -> preference dataset (3,528 pairs with degrees) -> reward model (RLHF) -> DPO loss function -> evaluation metrics (R-precision, FID, diversity)
- **Critical path**: Collect preference pairs → Train reward model (RLHF) or apply DPO directly → Fine-tune MotionGPT → Evaluate on alignment and quality metrics
- **Design tradeoffs**: RLHF vs DPO (complexity vs simplicity, exploration vs exploitation), dataset size vs overfitting, preference degree granularity vs labeling effort
- **Failure signatures**: Reward model overfitting (validation loss increases while training loss decreases), KL divergence spikes during RLHF training, poor generalization to unseen prompts
- **First 3 experiments**:
  1. Train DPO on full preference dataset with IPO loss and LoRA regularization, evaluate on HumanML3D test set
  2. Train RLHF with separate value and policy networks, compare performance to DPO
  3. Train DPO on subsets of preference data (e.g., only "Much better" pairs) to analyze preference degree impact

## Open Questions the Paper Calls Out

- **Generalizability across models**: How well does preference data generalize across different text-to-motion models beyond MotionGPT? The study only tested on one specific model, leaving open whether benefits would be seen with other architectures.
- **Incorporating skipped and unsure samples**: Can skipped samples (where both generations failed) and "unsure" samples be effectively incorporated into preference learning? The study excluded these samples, leaving unknown whether incorporating them would improve results.
- **Preference learning as evaluation metric**: Can preference learning serve as an effective evaluation metric for text-to-motion generation, replacing or supplementing R-precision and FID? While human labelers preferred DPO outputs, the authors haven't tested whether a reward model could serve as a reliable automatic evaluation metric.

## Limitations

- The dataset size (3,528 preference pairs) is relatively small for both training a reward model and evaluating generalization
- RLHF implementation details are sparse, with critical hyperparameters like KL coefficient schedules and reward model architecture specifics not fully specified
- The preference data collection process relied on gpt-3.5-turbo-0125 for prompt generation, but the exact methodology for creating similar prompts remains unclear

## Confidence

**High Confidence**: The observation that DPO performs better than RLHF on the tested dataset is well-supported by experimental results, with alignment metrics (R-precision) and quality metrics (FID) consistently favoring DPO.

**Medium Confidence**: The explanation that RLHF's poor performance is primarily due to reward model overfitting on limited data is plausible but not definitively proven, lacking direct evidence of reward model performance degradation.

**Low Confidence**: The finding that preference data with strong degrees significantly improves R-precision may be influenced by labeler consistency issues, as the paper doesn't provide inter-annotator agreement metrics or analyze correlation between degree strength and actual quality differences.

## Next Checks

1. **Dataset Size Sensitivity Analysis**: Systematically vary the size of the preference dataset (25%, 50%, 75%, 100%) to quantify how dataset size affects RLHF vs DPO performance differences, directly testing the overfitting hypothesis.

2. **Reward Model Generalization Testing**: Train the RLHF reward model on the full dataset, then evaluate its performance on held-out motion samples and cross-validation splits to measure generalization capabilities and identify overfitting patterns.

3. **Preference Degree Consistency Study**: Conduct a controlled experiment where the same preference pairs are labeled by multiple annotators, measuring inter-annotator agreement and analyzing whether stronger preference degrees correlate with higher agreement and better downstream performance.