---
ver: rpa2
title: Heterogeneous Relationships of Subjects and Shapelets for Semi-supervised Multivariate
  Series Classification
arxiv_id: '2411.18043'
source_url: https://arxiv.org/abs/2411.18043
tags:
- time
- graph
- series
- shapelets
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multivariate time series
  (MTS) classification with limited labeled data. It proposes a heterogeneous graph
  representation learning (HGRL) method that integrates subject features and shapelet
  information into a heterogeneous graph structure.
---

# Heterogeneous Relationships of Subjects and Shapelets for Semi-supervised Multivariate Series Classification

## Quick Facts
- arXiv ID: 2411.18043
- Source URL: https://arxiv.org/abs/2411.18043
- Reference count: 40
- Primary result: HGRL outperforms current state-of-the-art methods in MTS classification tasks, achieving best accuracy on 4 out of 10 UEA datasets and best accuracy on HAR and ISRUC-3 datasets.

## Executive Summary
This paper addresses the challenge of multivariate time series (MTS) classification with limited labeled data by proposing a heterogeneous graph representation learning (HGRL) method. The approach integrates subject features and shapelet information into a heterogeneous graph structure, using a contrast temporal self-attention module to obtain sparse MTS representations, learning subject-specific shapelets, and employing a dual-level graph attention network to capture relationships between different types of information. The method demonstrates superior performance across multiple datasets, validating its effectiveness for semi-supervised MTS classification.

## Method Summary
The HGRL method involves preprocessing MTS data, applying a contrast temporal self-attention (CTSA) module with triplet loss to obtain sparse MTS representations, computing a similarity graph using soft DTW on these representations, and learning subject-specific shapelets using a multi-task loss. Subject features are embedded using T5 (or one-hot encoding), and a heterogeneous graph is constructed with MTS, subject, and shapelet nodes. Finally, a dual-level GAT is trained for classification, integrating heterogeneous information through type-level and node-level attention mechanisms.

## Key Results
- HGRL achieves the best accuracy on 4 out of 10 UEA datasets.
- HGRL achieves the best accuracy on Human Activity Recognition (HAR) and ISRUC-3 datasets.
- The method outperforms current state-of-the-art methods in MTS classification tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Contrast Temporal Self-Attention (CTSA) module captures sparse, discriminative representations by leveraging principal component dimensions and contrastive learning.
- Mechanism: CTSA first partitions the multivariate time series into overlapping tokens, then computes attention only within each dimension while masking out highly overlapping subsequences to reduce redundancy. The contrastive loss uses anchor, positive, and negative samples from principal components to refine embeddings without labels.
- Core assumption: Principal component dimensions preserve the most variance and thus the most informative features for classification.
- Evidence anchors:
  - [abstract] "we first utilize a contrast temporal self-attention module to obtain sparse MTS representations"
  - [section] "We use a contrast temporal self-attention (CTSA) module with contrast temporal loss to obtain MTS sparse representations"
  - [corpus] No direct evidence found in related papers; this appears to be a novel contribution.
- Break condition: If the principal component analysis fails to identify meaningful dimensions (e.g., in datasets with low variance structure), the contrastive sampling strategy may not yield informative representations.

### Mechanism 2
- Claim: Soft Dynamic Time Warping (soft DTW) models similarity between MTS representations while preserving temporal alignment and differentiability.
- Mechanism: After CTSA produces embeddings, soft DTW computes a smooth minimum over alignment costs using dynamic programming, allowing gradient-based optimization. The result is normalized and transformed into a similarity matrix for graph construction.
- Core assumption: Aligning sequences via DTW captures their true similarity better than Euclidean distance, and the soft variant allows end-to-end training.
- Evidence anchors:
  - [abstract] "we use soft DTW to model the similarity between MTS representations, constructing a similarity graph"
  - [section] "we employ the CTSA module to obtain a robust MTS representations T rep i . This enables us to use DTW to achieve a more efficient and effective similarity representation"
  - [corpus] Limited direct support; corpus neighbors focus on graph representations but not soft DTW for MTS similarity.
- Break condition: If time series are very long or high-dimensional, soft DTW computation may become prohibitive, or the smoothing parameter may cause loss of discriminative power.

### Mechanism 3
- Claim: Dual-level Graph Attention Network (GAT) integrates heterogeneous node types by learning both type-level and node-level attention, capturing richer relational structure than homogeneous GAT.
- Mechanism: Type-level attention weights contributions from different node categories (MTS, subjects, shapelets), while node-level attention refines importance among nodes within the same type. This is combined into a layer-wise propagation that updates embeddings.
- Core assumption: The heterogeneity of information (shapelets, subjects, MTS) carries complementary signals that must be jointly modeled rather than flattened into a single node type.
- Evidence anchors:
  - [abstract] "Finally, we use a dual level graph attention network to get prediction"
  - [section] "We employ a dual level GAT to learn representations for heterogeneous graphs, consisting of type-level and node-level attention mechanisms"
  - [corpus] Some support from related work on heterogeneous GNNs (e.g., Moon, Periodic Graph-Enhanced MTS Anomaly Detector), though HGRLâ€™s specific dual-level design is novel.
- Break condition: If the attention weights collapse to uniform or if node types are not well-differentiated, the dual attention may not add value over a simpler GAT.

## Foundational Learning

- Concept: Contrastive representation learning
  - Why needed here: Enables learning discriminative embeddings from unlabeled data, crucial for semi-supervised MTS classification where labels are scarce.
  - Quick check question: In contrastive learning, what is the role of negative samples in shaping the embedding space?

- Concept: Graph Neural Networks for heterogeneous data
  - Why needed here: Captures complex relationships between multiple node types (MTS, subjects, shapelets) that are not reducible to homogeneous graphs.
  - Quick check question: How does node-level attention differ from type-level attention in a heterogeneous GAT?

- Concept: Shapelet mining and interpretability
  - Why needed here: Provides interpretable local patterns that explain classification decisions and improve generalization via subject-specific shapelets.
  - Quick check question: Why might subject-specific shapelets outperform global shapelets in MTS classification?

## Architecture Onboarding

- Component map: Input MTS data -> CTSA module -> sparse MTS embeddings -> soft DTW -> similarity graph -> shapelet learner -> subject features (T5) -> heterogeneous graph (MTS, subjects, shapelets) -> dual-level GAT -> node embeddings -> softmax classifier -> predicted labels
- Critical path: CTSA -> Soft DTW -> Graph construction -> Dual GAT -> Classification
- Design tradeoffs:
  - CTSA vs full self-attention: computational efficiency vs global context
  - Principal components vs random dimensions: informativeness vs robustness
  - Soft DTW vs Euclidean: alignment accuracy vs simplicity
  - Dual GAT vs single GAT: expressivity vs parameter overhead
- Failure signatures:
  - Poor accuracy: check CTSA overlap mask, soft DTW smoothing, attention weight distributions
  - Overfitting: too many shapelets, high GAT depth, insufficient regularization
  - Slow training: DTW computation, attention matrix sparsity, GPU memory
- First 3 experiments:
  1. Ablation: Replace CTSA with full self-attention; measure impact on accuracy and runtime.
  2. Sensitivity: Vary the number of shapelets (32, 64, 128, 256) and observe overfitting trends.
  3. Graph structure: Remove subject or shapelet nodes; assess contribution of heterogeneous information.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of shapelets impact classification performance across different datasets, and what is the optimal number of shapelets for maximizing accuracy?
- Basis in paper: [explicit] The paper discusses the impact of varying the number of shapelets (32, 64, 128, 256, 512) on classification accuracy for HAR, HMD, and LIB datasets, noting that performance improves with increasing shapelets up to a point before potentially decreasing.
- Why unresolved: The paper does not provide a clear explanation for why performance decreases after a certain number of shapelets, nor does it offer a general method for determining the optimal number of shapelets for any given dataset.
- What evidence would resolve it: Experimental results showing the classification accuracy for a wider range of datasets and shapelet numbers, along with an analysis of the trade-offs between accuracy and computational efficiency at different shapelet counts.

### Open Question 2
- Question: What are the limitations of using principal component analysis (PCA) for selecting dimensions in the contrast temporal self-attention module, and are there alternative methods that could improve the selection process?
- Basis in paper: [explicit] The paper mentions using PCA to select principal component dimensions for the contrast temporal self-attention module but does not discuss the limitations of this approach or explore alternative methods for dimension selection.
- Why unresolved: The paper does not provide a comparison of PCA with other dimension selection methods or discuss the potential drawbacks of relying solely on PCA for this task.
- What evidence would resolve it: Comparative studies evaluating the performance of the contrast temporal self-attention module using different dimension selection methods, such as random selection, variance-based selection, or other dimensionality reduction techniques.

### Open Question 3
- Question: How does the dual-level graph attention network (GAT) compare to other graph neural network architectures, such as Graph Convolutional Networks (GCN), in terms of capturing complex relationships in multivariate time series data?
- Basis in paper: [explicit] The paper introduces a dual-level GAT for heterogeneous graph representation learning and compares its performance to GCN in an ablation study, showing that GAT generally outperforms GCN across different datasets.
- Why unresolved: The paper does not provide a detailed analysis of why GAT performs better than GCN or explore the specific advantages of the dual-level attention mechanism in capturing complex relationships in multivariate time series data.
- What evidence would resolve it: In-depth studies comparing the performance of GAT and GCN on a wider range of datasets, along with an analysis of the attention weights and graph structures learned by each architecture to understand their strengths and weaknesses in capturing complex relationships.

## Limitations
- The novelty of the Contrast Temporal Self-Attention module and the specific dual-level GAT architecture is not well-supported by external evidence, relying primarily on ablation studies within the paper.
- The generalizability of the method across different types of MTS data is uncertain, as the evaluation focuses on specific datasets with limited variation in characteristics.
- The computational complexity of the method, particularly for soft DTW computation and the dual-level GAT, is not well-analyzed, making it unclear how the method scales with very large datasets or high-dimensional time series.

## Confidence
- High Confidence: The overall methodology is sound and the evaluation on multiple datasets demonstrates strong performance. The use of heterogeneous graph representation learning for MTS classification is a valid and promising approach.
- Medium Confidence: The specific implementations of the CTSA module and the dual-level GAT are likely effective, but the lack of direct evidence in the corpus or related work introduces some uncertainty. The paper's claims about the superiority of these mechanisms are based primarily on ablation studies within its own experiments.
- Low Confidence: The paper's claims about the interpretability of the learned shapelets and the robustness of the method to different types of noise or missing data are not well-supported by the provided evidence.

## Next Checks
1. Replicate the ablation studies presented in the paper, specifically comparing the performance of HGRL with and without the CTSA module, and with different configurations of the dual-level GAT (e.g., varying the number of layers and attention heads).
2. Evaluate HGRL on additional MTS datasets that were not used in the original paper, particularly those with different characteristics (e.g., longer sequences, more variables, different domains). This will help assess the generalizability of the method.
3. Conduct a detailed analysis of the computational complexity of HGRL, particularly for the soft DTW computation and the dual-level GAT. This will help understand how the method scales with very large datasets or high-dimensional time series.