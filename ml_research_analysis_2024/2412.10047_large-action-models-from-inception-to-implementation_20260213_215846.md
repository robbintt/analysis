---
ver: rpa2
title: 'Large Action Models: From Inception to Implementation'
arxiv_id: '2412.10047'
source_url: https://arxiv.org/abs/2412.10047
tags:
- task
- data
- action
- control
- lams
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Action Models (LAMs) are a new class of AI models that extend
  Large Language Models (LLMs) by generating executable actions in digital and physical
  environments. LAMs interpret user intentions, generate action sequences, dynamically
  plan and adapt, and are specialized for specific domains.
---

# Large Action Models: From Inception to Implementation

## Quick Facts
- arXiv ID: 2412.10047
- Source URL: https://arxiv.org/abs/2412.10047
- Reference count: 40
- Key outcome: LAM4 achieves 81.2% task success rate, outperforming GPT-4o's 67.2% in offline evaluations

## Executive Summary
Large Action Models (LAMs) extend Large Language Models to generate executable actions in digital and physical environments by interpreting user intentions and producing action sequences. The paper introduces a four-phase training pipeline that first teaches LAMs to generate coherent plans, then execute actions aligned with those plans, followed by self-boosting exploration and reward-based optimization. Demonstrated using a Windows OS GUI agent (UFO), LAMs achieve 81.2% task success rate offline and 71.0% online while maintaining significantly lower latency than GPT-4o.

## Method Summary
The methodology employs a four-phase training pipeline: Phase 1 trains task-plan generation using cross-entropy loss on plan sequences; Phase 2 fine-tunes action execution using expert-labeled trajectories from GPT-4o; Phase 3 enables self-boosting exploration to tackle previously failed tasks and generate new success cases; Phase 4 optimizes decision-making using offline PPO with a reward model that evaluates both successful and failed trajectories. The approach is demonstrated using the UFO agent framework with textual-only inputs processed through UI Automation API, avoiding visual data to reduce computational overhead.

## Key Results
- LAM4 achieves 81.2% task success rate in offline evaluation, outperforming GPT-4o's 67.2%
- Online evaluation shows LAM achieving 71.0% task success rate with significantly lower latency than GPT-4o
- Text-only processing strategy reduces computational overhead while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LAMs achieve better action accuracy by leveraging specialized task-plan pretraining before learning from expert demonstrations
- Mechanism: The training pipeline first teaches the model to generate coherent step-by-step plans (Phase 1), then teaches it to execute actions aligned with those plans (Phase 2), creating a foundation that improves action execution accuracy
- Core assumption: Planning capability learned in Phase 1 transfers to and improves action execution in Phase 2
- Evidence anchors:
  - [abstract] "LAMs are fine-tuned for executing specialized sequences of actions within specific environments [8]. By focusing on particular domains, LAMs achieve a high degree of accuracy and adaptability, outperforming general-purpose LLMs in targeted applications"
  - [section] "While LAM1 can produce structured plans, it lacks the ability to execute them. In Phase 2, we introduce expert-labeled task-action trajectories from GPT-4o (Section 3.2) to teach the model how to perform actions"
  - [corpus] Weak evidence - no direct mention of task-plan pretraining improving action accuracy
- Break condition: If plan generation quality does not correlate with action execution quality, or if the model cannot transfer planning knowledge to execution

### Mechanism 2
- Claim: LAMs outperform GPT-4o by learning from both successful and failed trajectories through self-boosting exploration and reward-based optimization
- Mechanism: Phase 3 allows LAM to attempt previously failed tasks and generate new successful trajectories, while Phase 4 uses reinforcement learning on both successes and failures to refine decision-making
- Core assumption: Learning from failures provides unique insights that improve model robustness beyond what expert-only training provides
- Evidence anchors:
  - [abstract] "Phase 3: Self-Boosting Exploration encourages the model to tackle tasks that even GPT-4o failed to solve, autonomously generating new success cases and evolving into LAM3"
  - [section] "Despite the improvements, Phases 1–3 focus on successes or expert-like behavior. They offer limited insights into intermediate decision quality and fail to exploit learning opportunities presented by failed attempts"
  - [corpus] Weak evidence - no direct comparison of performance gains from learning from failures
- Break condition: If failure trajectories do not provide meaningful learning signals, or if reinforcement learning on failures degrades performance

### Mechanism 3
- Claim: LAMs achieve superior efficiency by processing only textual inputs rather than visual data
- Mechanism: By excluding screenshots and focusing on textual descriptions of UI states, LAMs reduce computational overhead and latency while maintaining competitive accuracy
- Core assumption: Textual descriptions contain sufficient information for accurate action selection compared to visual inputs
- Evidence anchors:
  - [abstract] "Online evaluations demonstrate LAM achieves a task success rate of 71.0% with significantly lower latency compared to GPT-4o"
  - [section] "LAM processes only textual inputs, excluding screenshots, while the baseline models were evaluated using both textual and visual modalities"
  - [corpus] Weak evidence - no explanation of why textual inputs are sufficient for GUI interaction
- Break condition: If textual descriptions lack critical visual information needed for accurate control selection, or if accuracy degrades significantly without visual inputs

## Foundational Learning

- Concept: Cross-entropy loss for sequence prediction
  - Why needed here: Used in supervised fine-tuning phases to train LAM to predict correct plan sequences and action sequences
  - Quick check question: What is the difference between standard cross-entropy loss and masked cross-entropy loss used in LAM training?

- Concept: Reinforcement learning with Proximal Policy Optimization (PPO)
  - Why needed here: Used in Phase 4 to fine-tune LAM4 by learning from both successful and failed trajectories using reward signals
  - Quick check question: How does offline PPO differ from online PPO, and why is offline PPO appropriate for LAM training?

- Concept: Reward modeling for intermediate action evaluation
  - Why needed here: The reward model provides scalar values representing action quality, enabling the LAM to learn from partial successes and failures
  - Quick check question: What are the advantages and disadvantages of using binary rewards (+1/-1) versus continuous rewards for LAM training?

## Architecture Onboarding

- Component map: User request → LAM core model → Action grounding → Environment execution → Feedback → Next inference cycle
- Critical path: User request → LAM inference → Action grounding → Environment execution → Feedback → Next inference cycle
- Design tradeoffs:
  - Text-only vs. multimodal inputs (efficiency vs. potential accuracy)
  - Specialized vs. general-purpose model (performance vs. flexibility)
  - Expert-only vs. self-exploration learning (data quality vs. robustness)
  - Real-time vs. offline evaluation (practicality vs. controlled testing)
- Failure signatures:
  - High object accuracy but low operation accuracy → UI control identification works but action mapping is incorrect
  - Low step success rate but high task success rate → Model finds alternative paths to complete tasks
  - High latency in online evaluation → Model inference or action execution bottlenecks
  - GPT-4o outperforms LAM on planning but LAM outperforms on execution → Planning learned but not properly transferred to action execution
- First 3 experiments:
  1. Test LAM1 planning accuracy on held-out task-plan pairs to verify Phase 1 effectiveness
  2. Compare LAM2 action accuracy on expert-labeled trajectories vs. GPT-4o baseline
  3. Evaluate LAM3 self-exploration success rate on previously failed tasks to measure Phase 3 improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the four-phase training pipeline for LAMs compare in terms of efficiency and effectiveness to other existing approaches for training action-oriented models, such as those using reinforcement learning from scratch or combining LLMs with specialized action modules?
- Basis in paper: [explicit] The paper presents a four-phase training pipeline for LAMs, including task-plan pretraining, learning from expert demonstrations, self-boosting exploration, and learning from a reward model. It claims this approach outperforms GPT-4o in offline and online evaluations.
- Why unresolved: The paper does not provide a direct comparison with other training methodologies, such as those using RL from scratch or LLM-action module combinations. It only benchmarks against GPT-4o, which is not specifically designed for action generation.
- What evidence would resolve it: A comparative study evaluating LAMs trained with the proposed pipeline against models trained using alternative approaches on the same benchmarks and tasks.

### Open Question 2
- Question: What are the specific safety mechanisms and validation strategies that should be implemented to ensure LAMs can be deployed in real-world environments without causing harm or unintended consequences?
- Basis in paper: [explicit] The paper acknowledges safety risks as a limitation of LAMs, noting that errors in inference or execution can lead to unintended or harmful consequences. It mentions the need for safety mechanisms such as formal verification, action validation, and fallback strategies.
- Why unresolved: The paper does not provide specific details on what safety mechanisms or validation strategies should be implemented. It only mentions the need for them without elaborating on their design or effectiveness.
- What evidence would resolve it: A detailed analysis of safety mechanisms implemented in LAM systems, including case studies of their effectiveness in preventing or mitigating harmful actions.

### Open Question 3
- Question: How can LAMs be effectively scaled and adapted to new environments and applications with minimal data collection and retraining efforts, while maintaining high performance and reliability?
- Basis in paper: [explicit] The paper identifies scalability, generalizability, and adaptability as limitations of LAMs, noting that they are often tailored to specific environments and that scaling to new contexts is challenging due to the high cost of data collection.
- Why unresolved: The paper does not provide concrete solutions for scaling LAMs to new environments. It only mentions the need for techniques like transfer learning, multi-task learning, and few-shot learning without detailing their implementation or effectiveness.
- What evidence would resolve it: Experimental results demonstrating the successful application of transfer learning, multi-task learning, or few-shot learning techniques to scale LAMs to new environments with minimal data and retraining.

## Limitations
- Offline evaluation shows strong performance (81.2% vs 67.2%) but online evaluation uses a significantly smaller test set (18 tasks vs 50 tasks)
- Claims about efficiency gains from text-only processing lack quantitative latency measurements and comparison with multimodal approaches
- Paper does not address potential safety risks or ethical concerns associated with autonomous GUI control

## Confidence

- High confidence: The four-phase training methodology is well-specified and the overall framework for LAM development is clearly articulated
- Medium confidence: The offline evaluation results showing LAM4 outperforming GPT-4o on task success rate, though the smaller online evaluation sample size reduces confidence
- Low confidence: Claims about efficiency gains from text-only processing and the scalability of the approach to more complex environments

## Next Checks

1. Re-run online evaluations with larger task sets (minimum 50 tasks) to establish statistically significant latency and accuracy comparisons between LAM and GPT-4o

2. Evaluate LAM performance on non-Windows environments (macOS, web browsers) to assess generalization beyond the Windows OS case study

3. Conduct systematic safety audits to identify potential failure modes where LAM could cause unintended system modifications or data loss