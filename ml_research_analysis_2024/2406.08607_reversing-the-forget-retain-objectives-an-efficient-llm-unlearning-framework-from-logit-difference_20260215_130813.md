---
ver: rpa2
title: 'Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework
  from Logit Difference'
arxiv_id: '2406.08607'
source_url: https://arxiv.org/abs/2406.08607
tags: []
core_contribution: The paper addresses the challenge of LLM unlearning, which involves
  making an LLM forget specific knowledge while retaining its overall capabilities.
  The authors propose a novel framework called Unlearning from Logit Difference (ULD)
  that uses an assistant LLM to remember the forget documents and forget the retain
  knowledge, and then derives the unlearned LLM by computing the logit difference
  between the target and assistant LLMs.
---

# Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference

## Quick Facts
- arXiv ID: 2406.08607
- Source URL: https://arxiv.org/abs/2406.08607
- Reference count: 40
- Key outcome: ULD achieves superior forget performance while preserving model utility, reducing training time by more than threefold compared to baseline methods

## Executive Summary
This paper addresses the challenge of making large language models (LLMs) forget specific knowledge while retaining their overall capabilities. The authors propose a novel framework called Unlearning from Logit Difference (ULD) that uses an assistant LLM to remember forget documents and forget retain knowledge, then derives the unlearned LLM by computing logit differences between the target and assistant models. This approach naturally resolves the challenges of degenerated output and catastrophic forgetting faced by conventional unlearning methods.

The framework demonstrates significant improvements over existing methods, achieving 0% utility loss on the TOFU benchmark while effectively removing targeted knowledge. ULD's efficiency gains are particularly noteworthy, requiring more than three times less training time than baseline approaches while maintaining or improving unlearning performance.

## Method Summary
ULD operates by training an assistant LLM to simultaneously remember forget documents and forget retain documents, then computing the logit difference between this assistant model and the target LLM. The assistant LLM is constructed with 8 layers of the original model plus LoRA adapters, resulting in approximately 20M trainable parameters. This reverse training objective allows the framework to naturally avoid the degenerated outputs and catastrophic forgetting problems that plague traditional unlearning approaches.

## Key Results
- ULD achieves 0% model utility loss on the TOFU benchmark while effectively unlearning targeted knowledge
- Training time is reduced by more than threefold compared to baseline methods
- ULD outperforms competitive baselines that sacrifice an average of 17% utility to achieve comparable forget quality

## Why This Works (Mechanism)
ULD works by reversing the traditional unlearning objective: instead of directly training the target LLM to forget specific knowledge, it trains an assistant model to remember forget documents and forget retain documents. The logit difference between these two models naturally extracts the desired unlearning behavior. This approach avoids direct optimization on the target model, which often leads to catastrophic forgetting and output degeneration. By leveraging the assistant model as an intermediary, ULD can precisely control what knowledge is retained versus forgotten without compromising overall model performance.

## Foundational Learning
- **Logit difference computation**: Needed to extract the specific knowledge differences between models; quick check: verify that logit differences produce meaningful semantic differences
- **Assistant LLM training**: Required for the dual objective of remembering and forgetting; quick check: confirm assistant can simultaneously handle both objectives
- **LoRA adapters**: Enable efficient fine-tuning with fewer parameters; quick check: measure parameter efficiency vs full fine-tuning
- **Catastrophic forgetting prevention**: Critical for maintaining overall model capabilities; quick check: test model performance on general tasks post-unlearning
- **Output degeneration detection**: Important for ensuring quality unlearned outputs; quick check: evaluate output coherence and relevance
- **Knowledge representation separation**: Fundamental to distinguishing forget vs retain content; quick check: validate separation through controlled experiments

## Architecture Onboarding

**Component Map:** Input documents → Assistant LLM training → Logit difference computation → Unlearned LLM

**Critical Path:** Forget documents + Retain documents → Assistant model training → Logit difference calculation → Final unlearned model

**Design Tradeoffs:** Assistant model size (8 layers chosen) vs training efficiency, LoRA adapter usage vs parameter count, simultaneous vs sequential training objectives

**Failure Signatures:** Degenerated outputs when logit differences are too extreme, incomplete forgetting when assistant fails to learn forget documents, catastrophic forgetting when retain knowledge is overly suppressed

**First Experiments:**
1. Verify assistant model can simultaneously remember forget documents and forget retain documents
2. Test logit difference computation produces meaningful knowledge differences
3. Validate unlearning effectiveness on simple, controlled document sets before scaling to full datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ULD compare to other unlearning methods when using a larger or smaller assistant LLM?
- Basis in paper: [inferred] The paper states that ULD uses an assistant LLM with 8 layers of the original LLM and adds LoRA layers, resulting in 20M trainable parameters. It would be interesting to see how the performance changes with different sizes of assistant LLMs.
- Why unresolved: The paper only experiments with one configuration of the assistant LLM size. It is unclear how the performance scales with larger or smaller assistant LLMs.
- What evidence would resolve it: Experiments comparing the performance of ULD with assistant LLMs of varying sizes (e.g., 4, 12, 16 layers) on the same datasets would provide insights into the impact of assistant LLM size on unlearning performance.

### Open Question 2
- Question: Can ULD be extended to handle unlearning tasks where the forget and retain documents are not clearly defined or labeled?
- Basis in paper: [inferred] The paper assumes access to forget and retain documents for the unlearning task. In practice, it may not always be possible to obtain such labeled data. Exploring how ULD can be adapted for unsupervised or weakly supervised unlearning scenarios would be valuable.
- Why unresolved: The paper does not discuss the applicability of ULD to unlearning tasks without explicit forget and retain document labels.
- What evidence would resolve it: Experiments demonstrating the performance of ULD on unlearning tasks with unlabeled or weakly labeled data would provide insights into its potential for handling more realistic unlearning scenarios.

### Open Question 3
- Question: How does the performance of ULD vary with different types of forget and retain documents, such as documents with varying levels of complexity or domain specificity?
- Basis in paper: [explicit] The paper evaluates ULD on the TOFU and HarryPotter datasets, which involve forgetting knowledge about fictional writers and copyright content, respectively. It would be interesting to see how ULD performs on unlearning tasks with different types of forget and retain documents.
- Why unresolved: The paper only considers two specific unlearning tasks. It is unclear how ULD generalizes to other types of forget and retain documents.
- What evidence would resolve it: Experiments evaluating ULD on a diverse set of unlearning tasks with different types of forget and retain documents (e.g., scientific articles, legal documents, social media posts) would provide insights into its robustness and generalizability.

## Limitations
- Relies on an assistant LLM that must accurately balance dual objectives, introducing potential failure modes
- Claims about avoiding catastrophic forgetting are based on limited experimental scope with only two dataset pairs
- Computational efficiency gains may depend heavily on specific architecture and model scale tested

## Confidence

**High confidence:** The core methodology of using logit differences between target and assistant LLMs is technically sound and well-explained

**Medium confidence:** The claimed efficiency improvements (3x reduction in training time) appear robust but may not generalize across different model scales

**Medium confidence:** The preservation of model utility (0% loss on TOFU) is impressive but based on limited experimental scope

**Low confidence:** The framework's robustness to various types of knowledge overlap between forget and retain sets remains unverified

## Next Checks

1. Test ULD across multiple diverse dataset pairs beyond TOFU and HarryPotter to assess generalizability
2. Conduct ablation studies removing the assistant LLM component to quantify its contribution to performance
3. Evaluate the framework's behavior when forget and retain document sets have significant semantic overlap to test robustness claims