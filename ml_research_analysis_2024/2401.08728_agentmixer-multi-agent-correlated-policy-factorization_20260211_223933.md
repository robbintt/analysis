---
ver: rpa2
title: 'AgentMixer: Multi-Agent Correlated Policy Factorization'
arxiv_id: '2401.08728'
source_url: https://arxiv.org/abs/2401.08728
tags:
- policy
- joint
- learning
- agentmixer
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of achieving coordinated policies
  in multi-agent reinforcement learning under partial observability. The authors propose
  AgentMixer, a framework that combines decentralized partially observable policies
  into a correlated joint fully observable policy using a Policy Modifier (PM) module.
---

# AgentMixer: Multi-Agent Correlated Policy Factorization

## Quick Facts
- arXiv ID: 2401.08728
- Source URL: https://arxiv.org/abs/2401.08728
- Reference count: 40
- Authors: Zhiyuan Li; Wenshuai Zhao; Lijun Wu; Joni Pajarinen
- One-line primary result: AgentMixer achieves state-of-the-art performance on Multi-Agent MuJoCo, SMAC-v2, Matrix Game, and Predator-Prey benchmarks by combining decentralized partially observable policies into a correlated joint policy.

## Executive Summary
AgentMixer addresses the challenge of achieving coordinated policies in multi-agent reinforcement learning under partial observability. The framework combines decentralized partially observable policies into a correlated joint fully observable policy using a Policy Modifier (PM) module. To prevent asymmetric learning failure where decentralized policies fail to imitate a centralized expert, AgentMixer introduces Individual-Global-Consistency (IGC) to maintain mode consistency between individual and joint policies. The method is theoretically grounded with convergence guarantees to an ϵ-approximate Correlated Equilibrium.

## Method Summary
AgentMixer operates by first constructing a joint policy from individual decentralized policies using a Policy Modifier module. This PM module employs interleaved agent-mixing and channel-mixing MLPs to combine policies in a non-linear fashion, enabling complex correlations among agents. The Individual-Global-Consistency mechanism ensures that the most frequently occurring actions (modes) in the joint policy and individual policies remain equivalent, preventing asymmetric learning failure. During training, a single-agent RL algorithm optimizes the joint policy, while IGC guarantees that the resulting decentralized policies maintain consistency with this joint policy. The framework can be combined with various single-agent algorithms like PPO or SAC.

## Key Results
- Outperforms or matches state-of-the-art methods on Multi-Agent MuJoCo, SMAC-v2, Matrix Game, and Predator-Prey benchmarks
- Achieves higher episode returns and win rates compared to MAPPO, HAPPO, MA VEN, ARMAPPO, MAT, and MAT-Dec
- Demonstrates strong performance particularly in scenarios with partial observability, such as the Ant-v2 task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AgentMixer uses a Policy Modifier (PM) to combine individual partially observable policies into a joint fully observable policy in a non-linear fashion, introducing coordination among agents.
- Mechanism: The PM module uses interleaved agent-mixing and channel-mixing MLPs. Agent-mixing MLPs allow inter-agent communication by operating on each channel of the feature independently, while channel-mixing MLPs allow intra-agent information fusion by operating on each agent independently.
- Core assumption: The Policy Modifier can effectively learn to combine individual policies into a coordinated joint policy that is better than the independent Cartesian product of individual policies.
- Evidence anchors:
  - [abstract]: "AgentMixer combines individual partially observable policies into a joint fully observable policy non-linearly."
  - [section]: "Specifically, agent- and channel-mixing can be written as follows: Hagent = Hinput + W(2) agentσ(W(1) agentLayerNorm(Hinput)), Hchannel = Hagent + σ(W(1) channelLayerNorm(Hagent))W(2) channel"
  - [corpus]: Weak. The corpus papers focus on value decomposition or attention-based methods, not on this specific MLP-based mixing architecture.

### Mechanism 2
- Claim: Individual-Global-Consistency (IGC) maintains mode consistency between individual policies and the joint policy while allowing correlated exploration, preventing asymmetric learning failure.
- Mechanism: IGC enforces that the most frequently occurring actions (modes) in the joint policy and individual policies are equivalent. For continuous actions, this means setting the mean of the joint policy equal to the collection of individual policy means. For discrete actions, it uses Gumbel-Softmax to disentangle exploration and mode, ensuring the mode of the joint policy matches the individual policies.
- Core assumption: Maintaining mode consistency is sufficient to ensure the individual policies can effectively imitate the joint policy without suffering from asymmetric learning failure due to partial observability.
- Evidence anchors:
  - [abstract]: "To enable decentralized execution, we introduce Individual-Global-Consistency to guarantee mode consistency during joint training of the centralized and decentralized policies"
  - [section]: "IGC enables the actions that occur most frequently in the joint policy and the product policy to be equivalent. Crucially, IGC minimizes the divergence between the two policies while allowing correlated exploration in the joint policy."
  - [corpus]: Missing. No direct evidence in corpus about mode consistency mechanisms.

### Mechanism 3
- Claim: AgentMixer converges to an ϵ-approximate Correlated Equilibrium by interleaving the learning of the correlated joint policy and the individual policies under IGC.
- Mechanism: The joint policy is learned as a single-agent RL problem using the Policy Modifier and IGC constraint. The monotonic improvement property from TRPO guarantees that the sequence of joint policies converges to a local optimum. IGC then ensures that the individual policies are consistent with this joint policy, making the product of individual policies an ϵ-CE.
- Core assumption: The single-agent RL problem defined by the Policy Modifier and IGC is well-behaved and converges to a good solution, and IGC is sufficient to translate this into a CE for the product policy.
- Evidence anchors:
  - [abstract]: "prove that AgentMixer converges to an ϵ-approximate Correlated Equilibrium"
  - [section]: "Theorem 2 (Convergence of AgentMixer). The product partially observable policy generated by AgentMixer is a ϵ-CE."
  - [corpus]: Weak. The corpus papers focus on other MARL methods but don't directly address convergence to CE.

## Foundational Learning

- Concept: Correlated Equilibrium (CE) in game theory
  - Why needed here: AgentMixer aims to achieve a CE among agents, which is a solution concept where agents' strategies are correlated but not necessarily independent, allowing for better coordination than a Nash Equilibrium.
  - Quick check question: What is the key difference between a Correlated Equilibrium and a Nash Equilibrium in terms of the set of achievable joint policies?

- Concept: Centralized Training with Decentralized Execution (CTDE)
  - Why needed here: AgentMixer uses CTDE, where agents have access to global information during training but only local information during execution. This is crucial for learning coordinated policies under partial observability.
  - Quick check question: How does CTDE address the challenge of partial observability in multi-agent reinforcement learning?

- Concept: Asymmetric learning failure
  - Why needed here: AgentMixer is designed to mitigate asymmetric learning failure, which occurs when a centralized policy (with full observability) is used to train decentralized policies (with partial observability), leading to suboptimal individual policies.
  - Quick check question: In the bridge crossing task example, why would naively learning from the omniscient expert policy lead to agents jamming on the same passage?

## Architecture Onboarding

- Component map: Individual Policies -> Policy Modifier (PM) -> Joint Policy -> Single-Agent RL Algorithm -> Individual Policies (via IGC)
- Critical path:
  1. Initialize individual policies and PM.
  2. Construct joint policy using PM and IGC.
  3. Run single-agent RL algorithm on joint policy.
  4. Extract individual policies for execution.

- Design tradeoffs:
  - Deeper vs. shallower PM: Deeper PM can model more complex correlations but is harder to train and may overfit.
  - Strict vs. relaxed IGC: Strict IGC ensures better consistency but may limit exploration; relaxed IGC allows more exploration but may lead to asymmetric learning failure.

- Failure signatures:
  - Individual policies perform well in training but poorly in execution: Likely asymmetric learning failure, IGC not working.
  - Joint policy learns but individual policies don't improve: PM not effectively combining policies, or IGC too restrictive.
  - Both joint and individual policies learn slowly: Single-agent RL algorithm hyperparameters may need tuning.

- First 3 experiments:
  1. Implement a simple version of AgentMixer with a linear PM (no mixing MLPs) and test on a small matrix game to verify basic functionality.
  2. Add agent-mixing MLPs to the PM and test on a partially observable gridworld to see if coordination improves.
  3. Implement IGC with continuous actions and test on a multi-agent MuJoCo task (e.g., Ant-v2) to verify it prevents asymmetric learning failure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AgentMixer change when using different underlying single-agent reinforcement learning algorithms?
- Basis in paper: [explicit] The paper states that AgentMixer can benefit from various single-agent algorithms and provides experimental results using PPO and SAC.
- Why unresolved: The paper only demonstrates performance with PPO and SAC, but does not explore other potential single-agent algorithms that could be used with AgentMixer.
- What evidence would resolve it: Experiments comparing AgentMixer's performance when using different single-agent algorithms (e.g., A2C, DQN, TD3) on the same benchmarks would provide insight into the algorithm's versatility and optimal configuration.

### Open Question 2
- Question: What is the impact of the Policy Modifier (PM) module's architecture on AgentMixer's performance?
- Basis in paper: [explicit] The paper mentions that the PM module consists of agent-mixing and channel-mixing MLPs, and provides an ablation study comparing different PM architectures on SMACv2.
- Why unresolved: While the paper shows that MLP-based PM performs best among the tested architectures, it does not explore other potential architectures or the impact of different hyperparameter settings within the MLP structure.
- What evidence would resolve it: Experiments testing various PM architectures (e.g., different MLP configurations, attention mechanisms, or other neural network structures) and their hyperparameter settings on multiple benchmarks would clarify the optimal PM design for AgentMixer.

### Open Question 3
- Question: How does AgentMixer perform in environments with more than two agents or with heterogeneous agent capabilities?
- Basis in paper: [inferred] The paper evaluates AgentMixer on tasks with up to 17 agents (Humanoid-v2) and mentions that it outperforms or matches state-of-the-art methods, but does not specifically address heterogeneous agent capabilities.
- Why unresolved: The paper focuses on homogeneous agent environments and does not explore how AgentMixer handles heterogeneous agent capabilities or scales to environments with a larger number of agents.
- What evidence would resolve it: Experiments testing AgentMixer's performance in environments with heterogeneous agents (e.g., different action spaces, observation spaces, or reward structures) and in larger-scale multi-agent scenarios would demonstrate its applicability to a wider range of real-world problems.

## Limitations

- The theoretical convergence proof relies on TRPO-style updates, but empirical implementation uses PPO/SAC which lack monotonic improvement guarantees
- The mode-consistency constraint in IGC is formally defined only for discrete actions, with continuous actions using a heuristic mean-matching approach
- The paper does not provide quantitative analysis of how often asymmetric learning failure occurs without IGC

## Confidence

- **High**: The core architectural contributions (PM module design, IGC mechanism) are clearly specified and reproducible
- **Medium**: Empirical performance claims are supported by benchmark results, though ablation studies on IGC are limited
- **Low**: Theoretical convergence guarantees, as the practical algorithm deviates from the assumptions in the proof

## Next Checks

1. Implement a minimal version of AgentMixer with linear policy mixing (no MLP layers) and verify that asymmetric learning failure occurs without IGC on a simple matrix coordination game.
2. Conduct an ablation study comparing performance with and without IGC across different levels of partial observability (varying observation dimensions) on Multi-Agent MuJoCo tasks.
3. Test the sensitivity of the PM module depth and width on both coordination performance and training stability across the benchmark tasks.