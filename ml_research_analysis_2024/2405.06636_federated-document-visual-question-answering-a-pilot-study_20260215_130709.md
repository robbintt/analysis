---
ver: rpa2
title: 'Federated Document Visual Question Answering: A Pilot Study'
arxiv_id: '2405.06636'
source_url: https://arxiv.org/abs/2405.06636
tags:
- data
- document
- training
- docvqa
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a federated learning framework for Document
  Visual Question Answering (DocVQA) to address privacy concerns with copyrighted
  or sensitive document data. The method uses self-pretraining with federated optimization
  to train a multimodal model (T5 + visual features) across heterogeneous document
  datasets without centralizing data.
---

# Federated Document Visual Question Answering: A Pilot Study

## Quick Facts
- arXiv ID: 2405.06636
- Source URL: https://arxiv.org/abs/2405.06636
- Authors: Khanh Nguyen; Dimosthenis Karatzas
- Reference count: 40
- Primary result: 45.2% accuracy on FeDocVQA benchmark with federated training across 10 clients

## Executive Summary
This paper presents a federated learning framework for Document Visual Question Answering (DocVQA) that addresses privacy concerns with copyrighted or sensitive document data. The method uses self-pretraining with federated optimization to train a multimodal model (T5 + visual features) across heterogeneous document datasets without centralizing data. Three existing DocVQA datasets (DocVQA, WikiTableQuestions, TabFact) are distributed across clients to simulate realistic non-IID data scenarios. The proposed approach combines self-pretraining with server-side adaptive optimization (FedAdam), achieving 45.2% accuracy on the FeDocVQA benchmark with 10 clients and 0.7 sampling probability, outperforming the FedAvg baseline (43.3%) and approaching centralized training performance (53.16%). The work demonstrates that federated DocVQA is viable and effective, with self-pretraining and proper hyperparameter tuning being essential for optimal performance.

## Method Summary
The federated DocVQA framework uses T5-base as the backbone model with multimodal input combining document images, OCR text, and layout information. Three DocVQA datasets are distributed across clients to simulate non-IID data scenarios. The method employs self-pretraining with Text Modeling, Layout Modeling, and Text-Layout Modeling objectives before fine-tuning with supervised labels. Federated training uses FedAdam optimizer with adaptive server-side optimization. The model architecture includes a Layout-Induced Vision-Text Embedding that fuses visual patch features from MAE ViT with OCR tokens. Training involves client sampling with probability C and aggregation of model updates at the server.

## Key Results
- Federated self-pretraining achieves 45.2% accuracy on FeDocVQA benchmark
- FedAdam optimizer outperforms FedAvg baseline (43.3% accuracy)
- Self-pretraining provides 2.6 percentage points improvement over baseline
- Performance approaches centralized training (53.16% accuracy) with proper hyperparameter tuning
- Client sampling probability C=0.7 provides optimal balance between privacy and performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Federated self-pretraining (FSP) enables domain adaptation without centralizing private document data
- Mechanism: By pretraining on unlabeled documents within each client's local data using self-supervised objectives, the model learns domain-specific representations that improve downstream DocVQA performance while preserving privacy
- Core assumption: Documents from different DocVQA datasets share underlying structural features (layout, reading order) that can be jointly learned across heterogeneous sources
- Evidence anchors:
  - [abstract]: "We explore the self-pretraining technique in this multi-modal setting, where the same data is used for both pretraining and finetuning, making it relevant for privacy preservation"
  - [section 3.3]: "Self-Pretraining refers to pretraining a model on the unlabeled training data for a given task before finetuning using the labels and the task-specific loss"
  - [corpus]: Weak evidence - corpus contains no direct studies of federated self-pretraining for DocVQA
- Break condition: If document heterogeneity exceeds the shared structural features, FSP would fail to create useful cross-domain representations

### Mechanism 2
- Claim: Adaptive server optimization (FedAdam) outperforms FedAvg in federated DocVQA training
- Mechanism: By incorporating second-moment gradient information at the server level, FedAdam adapts learning rates based on gradient variance across clients, leading to faster convergence and better handling of non-IID data
- Core assumption: Pre-trained models with federated training require server-side adaptive optimization more than client-side heterogeneity solutions
- Evidence anchors:
  - [abstract]: "We further propose combining self-pretraining with a Federated DocVQA training method using centralized adaptive optimization that outperforms the FedAvg baseline"
  - [section 3.2]: "The square of the gradient is integrated to effectively control its variance in each update, thus accelerating convergence"
  - [corpus]: Weak evidence - corpus contains no direct comparisons of FedAdam vs FedAvg for multimodal federated learning
- Break condition: If gradient variance is low across clients, the adaptive benefits of FedAdam would be minimal

### Mechanism 3
- Claim: Higher client participation probability (C) improves federated DocVQA performance
- Mechanism: Increasing the fraction of clients participating in each round (C approaching 1.0) provides better gradient estimates and reduces the impact of non-IID data distribution across clients
- Core assumption: Document understanding capabilities are homogeneous across diverse DocVQA datasets, making aggregation more effective with broader participation
- Evidence anchors:
  - [abstract]: "We show that our pretraining strategies can effectively learn and scale up under federated training with diverse DocVQA datasets and tuning hyperparameters is essential"
  - [section 5.1]: "For most cases of K, we observe that significant improvements occur around high values of C, particularly with K = 3"
  - [corpus]: Weak evidence - corpus contains no direct studies of client participation effects in federated multimodal learning
- Break condition: If client data is highly specialized and dissimilar, broader participation could introduce noise rather than improve performance

## Foundational Learning

- Concept: Federated Learning fundamentals
  - Why needed here: Understanding how model aggregation works across decentralized clients is essential for implementing the federated training loop
  - Quick check question: What is the key difference between FedAvg and FedAdam in terms of how they aggregate client updates?

- Concept: Self-supervised pretraining objectives
  - Why needed here: The text modeling, layout modeling, and text-layout modeling tasks are critical for adapting the model to document-specific features without labeled data
  - Quick check question: How does the Text-Layout Modeling objective differ from Text Modeling in terms of what information is masked?

- Concept: Multimodal document representation
  - Why needed here: Understanding how to combine visual features from document images with text and layout information is crucial for the model architecture
  - Quick check question: What role does the Layout-Induced Vision-Text Embedding play in combining OCR tokens with visual patch features?

## Architecture Onboarding

- Component map: Document image -> Patch embedding -> Layout-induced fusion -> T5 encoder -> Self-supervised pretraining/finetuning -> Answer generation
- Critical path: Document image -> Patch embedding -> Layout-induced fusion -> T5 encoder -> Self-supervised pretraining/finetuning -> Answer generation
- Design tradeoffs: Using T5-base instead of T5-large reduces parameters (220M vs 770M) but may sacrifice some performance; federated training trades centralization benefits for privacy
- Failure signatures: Training instability (high variance in validation metrics), catastrophic forgetting (performance on one dataset drops while training on another), poor convergence with high client counts
- First 3 experiments:
  1. Run FedAvg baseline with K=3, C=0.35 to observe catastrophic forgetting behavior
  2. Add FSP with all three pretraining objectives to measure improvement from domain adaptation
  3. Switch to FedAdam optimizer to test if adaptive server optimization improves convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does federated learning performance scale with increasing numbers of clients in DocVQA tasks, and what is the optimal client-to-dataset ratio for maximizing accuracy while maintaining privacy?
- Basis in paper: [explicit] The paper shows performance drops when increasing clients from 10 to 30 due to less local computation and more frequent synchronization, but doesn't explore intermediate values or determine optimal ratios.
- Why unresolved: The paper only tests three specific client counts (3, 10, 30) and doesn't systematically explore the relationship between client count, dataset distribution, and performance.
- What evidence would resolve it: Systematic experiments varying client counts between 10-30 with controlled dataset distributions, measuring both accuracy and communication efficiency.

### Open Question 2
- Question: What is the long-term effectiveness of self-pretraining in federated settings when applied to diverse document domains, and does it maintain advantages as document heterogeneity increases?
- Basis in paper: [explicit] The paper shows self-pretraining improves performance by 2.6 points but only tests on three specific datasets with limited heterogeneity.
- Why unresolved: The paper demonstrates effectiveness on current datasets but doesn't explore whether benefits persist with more diverse or domain-specific document collections.
- What evidence would resolve it: Experiments testing self-pretraining across 10+ diverse document domains with varying levels of domain similarity, tracking performance degradation as heterogeneity increases.

### Open Question 3
- Question: How do different federated optimization strategies (FedAvg, FedAdam, FedAvgM) perform when scaling to larger, more heterogeneous document datasets with complex layouts like tables and infographics?
- Basis in paper: [explicit] The paper compares optimization strategies but only on three relatively small datasets, and doesn't explore performance on more complex document types.
- Why unresolved: Current results may not generalize to larger-scale implementations or more complex document structures that require advanced reasoning capabilities.
- What evidence would resolve it: Large-scale experiments (100+ clients) using complex document types (infographics, technical documents) comparing optimization strategy performance across different levels of data heterogeneity.

## Limitations

- Performance gap of 7.96 percentage points between federated (45.2%) and centralized training (53.16%) suggests substantial efficiency loss
- Exact data partitioning strategy between clients remains unspecified beyond average document counts
- No error analysis is provided to understand failure modes across different document types

## Confidence

- Federated self-pretraining effectiveness: Medium - The claim is supported by the 45.2% accuracy result, but lacks ablation studies showing the individual contribution of each pretraining objective
- FedAdam superiority over FedAvg: Low - The paper claims FedAdam outperforms FedAvg but provides no comparative results or convergence analysis
- Client participation probability impact: Medium - The claim about C values improving performance is partially supported, but the relationship appears non-linear and dataset-dependent

## Next Checks

1. Implement an ablation study isolating the contribution of each self-pretraining objective (Text Modeling, Layout Modeling, Text-Layout Modeling) to quantify their individual impact on federated performance
2. Conduct experiments comparing FedAdam against FedAvg across multiple runs with statistical significance testing to validate the claimed superiority
3. Test the approach with 20-50 clients to identify scalability bottlenecks and measure the trade-off between privacy (more clients) and performance (fewer clients)