---
ver: rpa2
title: Investigating Symbolic Capabilities of Large Language Models
arxiv_id: '2405.13209'
source_url: https://arxiv.org/abs/2405.13209
tags: []
core_contribution: This study evaluates eight large language models (LLMs) on symbolic
  tasks such as addition, multiplication, modulus arithmetic, decimal arithmetic,
  and symbolic counting. The evaluation framework uses Chomsky's Hierarchy to classify
  task complexity and employs zero-shot Chain-of-Thought prompting.
---

# Investigating Symbolic Capabilities of Large Language Models

## Quick Facts
- **arXiv ID**: 2405.13209
- **Source URL**: https://arxiv.org/abs/2405.13209
- **Reference count**: 31
- **Primary result**: LLMs show significant performance degradation on increasingly complex symbolic tasks, suggesting reliance on knowledge-tuple encoding rather than learned symbolic manipulation rules.

## Executive Summary
This study evaluates eight large language models on symbolic tasks including addition, multiplication, modulus arithmetic, decimal arithmetic, and symbolic counting. Using Chomsky's Hierarchy to classify task complexity and zero-shot Chain-of-Thought prompting, the research demonstrates that LLMs struggle with symbolic reasoning as task complexity increases. Enterprise-grade models marginally outperform open-source alternatives, but fine-tuning shows little improvement in generalization. The findings suggest that current LLMs encode mathematical relationships as knowledge tuples rather than learning the underlying symbolic manipulation rules, indicating a need for specialized architectures to improve symbol-based reasoning capabilities.

## Method Summary
The study employs a systematic evaluation framework using Chomsky's Hierarchy to classify symbolic tasks by complexity. Researchers prepared 100 samples for each difficulty level across five symbolic tasks, utilizing zero-shot Chain-of-Thought prompting with temperature=0 and max tokens=4096. The evaluation compared four enterprise LLMs (GPT-3.5, GPT-4, and two others) against four open-source models on tasks including addition, multiplication, modulus arithmetic, decimal arithmetic, and symbolic counting using Hindu-Arabic numerals and English alphabets. Performance was measured by comparing model outputs to correct answers across varying task complexities.

## Key Results
- LLM performance degrades significantly as symbolic complexity increases, with enterprise models showing only marginal improvements over open-source alternatives
- Fine-tuning on symbolic tasks demonstrates minimal impact on generalization across unseen inputs
- The study reveals that LLMs appear to rely on knowledge-tuple encoding rather than learning symbolic manipulation rules, requiring exponentially more parameters for complex tasks

## Why This Works (Mechanism)

### Mechanism 1
LLMs fail at symbolic tasks because they encode mathematical relationships as knowledge tuples rather than simulating rule-based symbolic machines. Instead of learning step-by-step symbolic manipulation rules (e.g., multi-digit addition), LLMs store specific input-output pairs. This tuple-based encoding is less efficient for generalization and scales poorly with input complexity.

### Mechanism 2
The number of parameters required to encode symbolic tasks as knowledge tuples grows exponentially with the number of symbols, exceeding practical LLM sizes. Each unique symbolic expression must be stored as a tuple, and as digits increase, the number of possible tuples grows exponentially, quickly surpassing even large LLM storage capacity.

### Mechanism 3
Fine-tuning on symbolic tasks does not improve LLM generalization because models do not learn underlying symbolic manipulation rules. When fine-tuned, LLMs memorize training examples but fail to acquire the ability to apply symbolic rules to unseen inputs, with performance on larger or more complex inputs remaining poor.

## Foundational Learning

- **Concept**: Chomsky Hierarchy and its relation to computational complexity
  - Why needed here: Understanding where symbolic tasks fit in the Chomsky Hierarchy (e.g., addition as context-free, multiplication as context-sensitive) is crucial for analyzing LLM performance and designing appropriate models
  - Quick check question: Which type of formal language (regular, context-free, context-sensitive, recursively enumerable) is required to describe multi-digit multiplication, and what kind of computational model is needed to recognize it?

- **Concept**: Knowledge tuple encoding vs. symbolic machine encoding
  - Why needed here: Distinguishing between these two approaches to representing symbolic knowledge is essential for understanding why LLMs struggle with generalization and how to design better models
  - Quick check question: What is the key difference between encoding symbolic addition as a knowledge tuple versus as a finite state machine, and how does this affect the number of parameters required?

- **Concept**: Bit complexity and parameter estimation for neural networks
  - Why needed here: Estimating the number of bits and parameters required to encode symbolic tasks is crucial for understanding the limitations of current LLMs and the potential of future models
  - Quick check question: How does the number of bits required to encode a knowledge tuple for multiplication scale with the number of digits in the operands, and why does this quickly become infeasible for large inputs?

## Architecture Onboarding

- **Component map**: Input processing -> Tokenization and embedding -> Core model (Transformer-based LLM) -> Output generation -> Evaluation

- **Critical path**: 1. Prepare symbolic task input (e.g., "12 + 34") 2. Tokenize and embed the input 3. Pass the embedded input through the LLM 4. Generate the model's output 5. Evaluate the output against the correct answer

- **Design tradeoffs**: Model size vs. performance (larger models perform better but require more resources); Fine-tuning vs. zero-shot (fine-tuning may improve specific inputs but not generalization); Knowledge tuple encoding vs. symbolic machine encoding (tuples are more parameter-efficient for small inputs but scale poorly, while machines require fewer parameters overall but are more complex)

- **Failure signatures**: Degradation in performance as input complexity increases; Inability to generalize to unseen symbolic expressions; Over-reliance on memorized input-output pairs rather than learned rules

- **First 3 experiments**:
  1. Evaluate an LLM on addition tasks with increasing numbers of digits (e.g., 2-digit, 4-digit, 6-digit) to observe performance degradation
  2. Compare the performance of an LLM fine-tuned on symbolic tasks to a vanilla LLM on both seen and unseen inputs to assess generalization
  3. Implement a simple symbolic rule-based system (e.g., a finite state machine for addition) and compare its parameter efficiency and performance to an LLM on symbolic tasks

## Open Questions the Paper Calls Out

### Open Question 1
How can we design LLM architectures that effectively learn symbolic manipulation rules rather than relying on knowledge-tuple encoding? The paper concludes that LLMs rely on knowledge-tuple encoding rather than learning symbolic manipulation rules, indicating a need for specialized architectures to improve symbol-based reasoning. This remains unresolved because current LLMs, even with fine-tuning, show limited generalization on symbolic tasks. Development and testing of new LLM architectures specifically designed for symbolic reasoning, with empirical results showing improved performance on symbolic tasks compared to current models, would resolve this question.

### Open Question 2
What is the theoretical minimum number of parameters required for an LLM to efficiently encode symbolic tasks as automata versus knowledge tuples? The paper discusses the bit complexity of encoding symbolic tasks as an automaton versus a set of knowledge tuples, showing that encoding as automata requires considerably fewer parameters. While the paper provides estimates for parameter requirements, it does not determine the exact theoretical minimum for efficient encoding of symbolic tasks in LLMs. Rigorous mathematical proofs establishing the lower bounds on parameters needed for LLM architectures to encode symbolic tasks as automata, compared to knowledge tuple encoding, would resolve this question.

### Open Question 3
How does the performance of LLMs on symbolic tasks scale with model size, and is there a point of diminishing returns? The paper compares performance of enterprise and open-source LLMs, noting that enterprise models (with more parameters) generally perform better but all models show similar performance trends. The paper does not explore performance scaling beyond the models tested or identify if there's a point where increasing model size no longer significantly improves symbolic reasoning capabilities. Systematic scaling studies of LLM performance on symbolic tasks across a wide range of model sizes, identifying performance plateaus and optimal model sizes for symbolic reasoning, would resolve this question.

## Limitations

- The study relies on zero-shot Chain-of-Thought prompting without exploring alternative prompting strategies or model architectures that could better handle symbolic reasoning
- The assertion about knowledge-tuple encoding versus symbolic machine encoding is based on theoretical parameter estimates rather than empirical measurements from actual LLM architectures
- The comparison between enterprise and open-source models does not account for potential improvements from recent advances in sparse attention mechanisms or specialized neural architectures designed for symbolic computation

## Confidence

**High Confidence**: The empirical observation that LLM performance degrades significantly as symbolic complexity increases is well-supported by experimental results. The comparison between enterprise and open-source models also shows consistent patterns across multiple tasks.

**Medium Confidence**: The theoretical framework connecting Chomsky Hierarchy to symbolic task complexity and the parameter estimation for knowledge tuple versus symbolic machine encoding are mathematically sound but rely on simplifying assumptions about LLM architectures. The conclusion about fine-tuning ineffectiveness is supported by experimental data but may not generalize to all fine-tuning approaches.

**Low Confidence**: The claim that LLMs inherently cannot learn symbolic manipulation rules and must rely on knowledge-tuple encoding is an extrapolation from current evidence. This conclusion would require more direct evidence from model internals or systematic architectural comparisons to be fully validated.

## Next Checks

1. **Direct Parameter Efficiency Test**: Implement a minimal symbolic machine (e.g., a finite state automaton for addition) and measure its actual parameter count and performance on symbolic tasks, then compare this to an LLM of equivalent parameter count on the same tasks. This would provide empirical validation of the theoretical parameter efficiency claims.

2. **Architecture Ablation Study**: Design experiments comparing standard transformer architectures to modified versions with explicit symbolic reasoning components (e.g., neural Turing machines or models with external memory) on the same symbolic tasks, controlling for parameter count and training data.

3. **Internal Representation Analysis**: Use mechanistic interpretability techniques to examine the internal representations of LLMs during symbolic task processing, specifically looking for evidence of rule-based computation versus pattern matching. This would directly test whether models are simulating symbolic machines or storing knowledge tuples.