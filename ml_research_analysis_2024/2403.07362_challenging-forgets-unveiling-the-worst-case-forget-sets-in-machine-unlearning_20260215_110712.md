---
ver: rpa2
title: 'Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning'
arxiv_id: '2403.07362'
source_url: https://arxiv.org/abs/2403.07362
tags: []
core_contribution: "This work addresses the problem of reliable machine unlearning\
  \ (MU) evaluation by identifying worst-case forget sets\u2014data subsets that are\
  \ hardest to unlearn. The authors formulate this as a bilevel optimization (BLO)\
  \ problem, where the upper level optimizes the selection of data points to maximize\
  \ unlearning difficulty, and the lower level performs standard MU training."
---

# Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning

## Quick Facts
- **arXiv ID**: 2403.07362
- **Source URL**: https://arxiv.org/abs/2403.07362
- **Reference count**: 40
- **Primary result**: Worst-case forget sets significantly increase unlearning difficulty and reveal weaknesses in relabeling-based methods.

## Executive Summary
This work introduces a method to identify worst-case forget sets—subsets of data that are hardest to unlearn—by formulating the problem as a bilevel optimization. The approach uses sign-based stochastic gradient descent (signSGD) in the lower level to simplify implicit gradient computation, enabling first-order methods to solve the BLO efficiently. Experiments across multiple datasets show that these worst-case sets increase unlearning difficulty for both exact and approximate methods, expose weaknesses in relabeling-based approaches, and suggest their complements can serve as high-quality coresets.

## Method Summary
The method frames the identification of worst-case forget sets as a bilevel optimization: the upper level selects data points to maximize unlearning difficulty, while the lower level performs standard machine unlearning training. To make this tractable, signSGD is used in the lower level, simplifying implicit gradient computation and allowing the BLO to be solved with first-order methods. The approach generalizes to class-wise and prompt-wise forgetting scenarios.

## Key Results
- Worst-case forget sets significantly increase unlearning difficulty for both exact (Retrain) and approximate methods.
- Relabeling-based unlearning methods perform poorly under worst-case conditions.
- The complement of worst-case forget sets can act as a high-quality coreset.

## Why This Works (Mechanism)
The method works by strategically selecting data subsets that maximize unlearning difficulty through bilevel optimization. SignSGD in the lower level simplifies gradient computation, enabling efficient optimization of the forget set selection. This adversarial selection process reveals weaknesses in existing unlearning methods, particularly relabeling-based approaches, and provides a more rigorous evaluation framework.

## Foundational Learning

**Bilevel Optimization (BLO)**: Why needed? To jointly optimize forget set selection and unlearning performance. Quick check: Verify the upper-level objective aligns with worst-case forget set criteria.

**Sign-based SGD (signSGD)**: Why needed? Simplifies implicit gradient computation in the lower-level unlearning step. Quick check: Confirm signSGD maintains convergence and accuracy in the lower-level optimization.

**Implicit Gradient Computation**: Why needed? Enables efficient gradient-based optimization in the bilevel setting. Quick check: Validate that the simplified gradients approximate true gradients adequately.

**Coreset Selection**: Why needed? The complement of worst-case sets can serve as high-quality coresets. Quick check: Compare coreset quality against standard coreset selection methods.

## Architecture Onboarding

**Component Map**: Forget Set Selection (Upper Level) -> Bilevel Optimization -> signSGD-based Unlearning (Lower Level) -> Worst-Case Forget Sets

**Critical Path**: Upper-level optimization selects data points → Lower-level signSGD unlearning trains model → Implicit gradients guide next iteration → Converges to worst-case forget set

**Design Tradeoffs**: Computational cost of BLO vs. effectiveness of worst-case sets; signSGD simplicity vs. potential loss of gradient information.

**Failure Signatures**: BLO convergence issues; signSGD instability; poor worst-case set identification due to hyperparameter misalignment.

**First Experiments**:
1. Validate BLO convergence on a small dataset with known forget sets.
2. Compare worst-case set identification against random selection.
3. Test signSGD lower-level optimization stability across learning rates.

## Open Questions the Paper Calls Out
None

## Limitations
- Computational scalability of bilevel optimization for large datasets and models.
- Reliance on gradient-based methods may not generalize to non-differentiable unlearning objectives.
- Limited exploration of hyperparameter sensitivity and real-world data distributions.

## Confidence
- **High**: Worst-case forget sets significantly increase unlearning difficulty (supported by experiments).
- **Medium**: Relabeling-based methods perform poorly under worst-case conditions (limited method comparisons).
- **Low**: Complement of worst-case sets as high-quality coresets (minimal validation).

## Next Checks
1. Test scalability and performance on larger datasets (e.g., ImageNet-22K) and deeper architectures (e.g., Vision Transformers).
2. Evaluate robustness of worst-case forget sets to adversarial perturbations or noisy labels.
3. Compare against non-gradient-based unlearning methods (e.g., influence-based or data-tracing techniques).