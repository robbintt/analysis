---
ver: rpa2
title: When a language model is optimized for reasoning, does it still show embers
  of autoregression? An analysis of OpenAI o1
arxiv_id: '2410.01792'
source_url: https://arxiv.org/abs/2410.01792
tags:
- task
- rare
- common
- probability
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: o1, a new LLM optimized for reasoning, still shows probability
  sensitivity similar to standard LLMs. Across multiple tasks, o1 achieves higher
  accuracy and uses fewer tokens for high-probability outputs than low-probability
  ones.
---

# When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1

## Quick Facts
- arXiv ID: 2410.01792
- Source URL: https://arxiv.org/abs/2410.01792
- Reference count: 5
- Key outcome: o1 shows probability sensitivity similar to standard LLMs, achieving higher accuracy and using fewer tokens for high-probability outputs

## Executive Summary
This study investigates whether OpenAI's o1 model, specifically optimized for reasoning tasks, still exhibits behaviors characteristic of standard autoregressive language models. Despite being trained to excel at reasoning, o1 continues to show probability sensitivity: it achieves higher accuracy and uses fewer tokens for high-probability outputs compared to low-probability ones. The model also tends to use more tokens when solving rare task variants than common ones. While o1 outperforms previous models—particularly on rare task variants—it still demonstrates behavioral patterns rooted in its next-word prediction origins. These findings suggest that optimizing for reasoning can reduce but not eliminate the limitations inherent in probabilistic training objectives.

## Method Summary
The authors analyzed OpenAI o1 across three benchmark tasks: GSM8K (mathematical reasoning), BIG-Bench (diverse reasoning challenges), and SimpleLocalize (localization tasks). They measured accuracy, token usage, and output probability distributions, comparing o1's performance on high-probability versus low-probability outputs and common versus rare task variants. The analysis focused on identifying correlations between output probability and model behavior, including accuracy rates and computational effort (measured by token count).

## Key Results
- o1 achieves higher accuracy and uses fewer tokens for high-probability outputs compared to low-probability ones
- The model uses more tokens for rare task variants than common ones
- o1 outperforms previous models on rare task variants but still shows probability-sensitive behavioral patterns

## Why This Works (Mechanism)
The persistence of autoregressive "embers" in o1 likely stems from the fundamental architecture and training objectives shared with standard LLMs. Even when fine-tuned for reasoning, the model retains its core next-token prediction mechanism and probability distributions learned during pretraining. The chain-of-thought reasoning process may amplify rather than eliminate these probabilistic dependencies, as the model still needs to sample tokens at each reasoning step. The optimization for reasoning may have shifted the model's capabilities but not fundamentally altered its probabilistic nature.

## Foundational Learning
- **Autoregressive generation**: Why needed - core mechanism for producing sequential outputs; Quick check - observe token-by-token generation patterns
- **Probability distributions**: Why needed - determines likelihood of different outputs; Quick check - analyze output entropy across tasks
- **Chain-of-thought reasoning**: Why needed - enables step-by-step problem solving; Quick check - examine intermediate reasoning steps
- **Token economy**: Why needed - measures computational effort and efficiency; Quick check - compare token counts across task variants
- **Output probability sensitivity**: Why needed - reveals model's confidence and biases; Quick check - correlate accuracy with output probabilities
- **Task variant frequency**: Why needed - distinguishes common from rare problem types; Quick check - analyze performance differences

## Architecture Onboarding
**Component map**: Input text -> Embedding layer -> Transformer blocks -> Probability distribution -> Output tokens -> Chain-of-thought reasoning steps -> Final answer

**Critical path**: Input embedding → Multi-head attention → Feed-forward networks → Probability prediction → Token sampling → Reasoning chain construction → Answer generation

**Design tradeoffs**: The model balances reasoning depth (more tokens, potentially better accuracy) against computational efficiency (fewer tokens, faster response). The retention of probabilistic sensitivity represents a tradeoff between leveraging learned patterns and maintaining flexibility for novel reasoning.

**Failure signatures**: 
- Over-reliance on high-probability patterns leading to incorrect but common answers
- Excessive token usage on rare variants without corresponding accuracy gains
- Chain-of-thought reasoning that reinforces initial probabilistic biases

**Three first experiments**:
1. Vary temperature settings to observe how sampling affects probability sensitivity
2. Compare o1 with standard LLMs on identical reasoning tasks to quantify optimization gains
3. Analyze intermediate reasoning steps to identify where probability sensitivity most strongly influences decisions

## Open Questions the Paper Calls Out
The study does not explicitly identify open questions, focusing instead on demonstrating the persistence of autoregressive behaviors in reasoning-optimized models.

## Limitations
- Analysis limited to three specific benchmark tasks, which may not capture full reasoning capabilities
- Focus on aggregate patterns rather than systematic investigation of individual reasoning steps or failure cases
- Unclear whether observed patterns reflect fundamental constraints or training artifacts

## Confidence
- High: The observed correlations between output probability and both accuracy/token usage are empirically robust within the tested tasks
- Medium: The interpretation that these patterns represent "embers of autoregression" is plausible but not definitively established
- Medium: The claim that reasoning optimization "mitigates but not fully eliminates" probability sensitivity is supported but could benefit from broader task coverage

## Next Checks
1. Test o1 on additional reasoning benchmarks (particularly those involving non-linguistic reasoning or multi-modal inputs) to assess whether probability sensitivity generalizes beyond the current task set
2. Conduct ablation studies varying temperature and other sampling parameters to determine whether the observed patterns are artifacts of generation settings or reflect deeper architectural constraints
3. Analyze intermediate reasoning steps in o1's chain-of-thought to identify whether probability sensitivity manifests at specific stages of the reasoning process or is distributed throughout