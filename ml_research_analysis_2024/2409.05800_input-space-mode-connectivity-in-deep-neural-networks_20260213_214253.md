---
ver: rpa2
title: Input Space Mode Connectivity in Deep Neural Networks
arxiv_id: '2409.05800'
source_url: https://arxiv.org/abs/2409.05800
tags:
- loss
- connectivity
- input
- mode
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the concept of mode connectivity from parameter
  space to input space in deep neural networks. It demonstrates that different input
  images with similar predictions are generally connected by low-loss paths, even
  in untrained models.
---

# Input Space Mode Connectivity in Deep Neural Networks

## Quick Facts
- arXiv ID: 2409.05800
- Source URL: https://arxiv.org/abs/2409.05800
- Reference count: 40
- Key outcome: Demonstrates that different input images with similar predictions are generally connected by low-loss paths in deep neural networks, with applications to adversarial detection

## Executive Summary
This paper extends the concept of mode connectivity from parameter space to input space in deep neural networks. The authors show that images with similar predictions are typically connected by low-loss paths, even in untrained models, with trained models exhibiting approximately linear connectivity. This geometric property enables a novel adversarial detection method that outperforms existing techniques by analyzing the barrier heights between real and adversarial image pairs. The work provides both theoretical justification using percolation theory and extensive empirical validation across different model architectures and datasets.

## Method Summary
The authors demonstrate input space mode connectivity by linearly interpolating between image pairs and identifying maximum loss barriers, then using constrained optimization to bypass these barriers and find low-loss paths. For adversarial detection, they compare barrier heights between real-real and real-adversarial image pairs, using these differences as features for a KNN classifier. The method was evaluated on GoogLeNet trained on ImageNet and ResNet18 trained on CIFAR-10, with synthetic optimal inputs generated using feature visualization by optimization (FVO) technique.

## Key Results
- Input space mode connectivity exists in both trained and untrained models, explained by high-dimensional geometry and percolation theory
- Trained models exhibit approximately linear connectivity with small deviations, potentially due to implicit regularization
- The method achieves 93.7% accuracy on CIFAR-10 for Carlini-Wagner attacks, outperforming baselines
- Real-adversarial pairs show significantly different barrier characteristics compared to real-real pairs, enabling effective adversarial detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Mode connectivity in input space exists due to high-dimensional geometry and Lipschitz continuity of neural networks.
- **Mechanism**: Small changes in input space lead to small changes in network output because the network is Lipschitz continuous. This allows the input space to be divided into hypercubes where all points have similar outputs, creating a structure where connected paths can be found through percolation theory.
- **Core assumption**: Neural networks at initialization are Lipschitz continuous and their outputs for nearby inputs are correlated.
- **Evidence anchors**:
  - [abstract]: "We conjecture that input space mode connectivity in high-dimensional spaces is a geometric effect that takes place even in untrained models and can be explained through percolation theory."
  - [section 5.2]: "We present the concept of geometric mode connectivity, suggesting that almost all inputs on which a neural network makes similar predictions tend to be connected, as dX grows to infinity."
  - [corpus]: Weak evidence - no direct mention of percolation theory in neighbor papers, but "Exploring Neural Network Landscapes: Star-Shaped and Geodesic Connectivity" might be related.
- **Break condition**: If the network is not Lipschitz continuous or if the correlation between nearby inputs is negative or too weak.

### Mechanism 2
- **Claim**: Trained models exhibit approximately linear mode connectivity due to implicit regularization.
- **Mechanism**: Deep learning models prefer simpler hypotheses, including not changing drastically between similar inputs. This, combined with geometric considerations, leads to approximately linear paths between connected modes with only small barriers.
- **Core assumption**: Implicit regularization in deep learning includes a preference for linear-like behavior between similar inputs.
- **Evidence anchors**:
  - [abstract]: "for trained models, the path tends to be simple, with only a small deviation from being a linear path."
  - [section 5.3]: "We speculate that this phenomenon results from implicit regularization... one kind of simplicity preferred by deep learning is generally not to change between similar inputs if possible."
  - [corpus]: Weak evidence - no direct mention of implicit regularization in neighbor papers.
- **Break condition**: If the model is not overparameterized or if explicit regularization is applied that contradicts this preference.

### Mechanism 3
- **Claim**: The difference in barrier heights between real-real and real-adversarial pairs can be used for adversarial detection.
- **Mechanism**: Adversarial examples, even when visually similar to real images, have higher and more complex loss barriers when connected to real images. This difference can be quantified and used as a feature for classification.
- **Core assumption**: The barrier height and complexity are significantly different between real-real and real-adversarial pairs and can be reliably measured.
- **Evidence anchors**:
  - [abstract]: "We observe fundamental differences in connections between loss-minimizing inputs in various setups (real vs. real images, real vs. adversarial attacks, synthetic images) that allow us to distinguish between them."
  - [section 4.2]: "We find that adversarial examples, even when significantly different in terms of human visual perception, are mode-connected to 'true' real images... However, after a single round of this procedure, non-negligible secondary barriers emerge in paths A→B' and B'→K'..."
  - [corpus]: Weak evidence - no direct mention of adversarial detection in neighbor papers.
- **Break condition**: If the adversarial attack is designed to minimize the barrier height or if the model is too robust to have significant differences.

## Foundational Learning

- **Concept: Percolation Theory**
  - Why needed here: Provides the theoretical framework for understanding how connected paths can exist in high-dimensional spaces.
  - Quick check question: What is the critical probability in percolation theory, and how does it relate to the existence of connected paths?

- **Concept: Lipschitz Continuity**
  - Why needed here: Ensures that small changes in input lead to small changes in output, which is fundamental to the argument about mode connectivity.
  - Quick check question: How does the Lipschitz constant of a neural network relate to its sensitivity to input perturbations?

- **Concept: Implicit Regularization**
  - Why needed here: Explains why trained models exhibit approximately linear mode connectivity, which is key to the adversarial detection application.
  - Quick check question: What are some examples of implicit regularization in deep learning, and how do they affect the loss landscape?

## Architecture Onboarding

- **Component map**: Image pairs → Linear interpolation → Loss barrier detection → Optimization to bypass barrier → Low-loss path → Repeat for adversarial pairs → Compare barrier heights → Adversarial detection
- **Critical path**: Real/real image pairs → Linear interpolation → Loss barrier detection → Optimization to bypass barrier → Low-loss path → Repeat for adversarial pairs → Compare barrier heights → Adversarial detection
- **Design tradeoffs**: Using cross-entropy loss vs. other loss functions, balancing the complexity of the path with the accuracy of the detection, computational cost of the optimization process
- **Failure signatures**: Failure to find a low-loss path, similar barrier heights for real-real and real-adversarial pairs, high computational cost making the method impractical
- **First 3 experiments**:
  1. Verify mode connectivity for real image pairs in a trained model by computing the loss curve along the linear interpolation and optimizing to bypass the barrier
  2. Generate synthetic optimal inputs for a class using FVO and verify their connectivity
  3. Compare the barrier heights for real-real and real-adversarial pairs and train a simple classifier to detect adversarial examples based on this feature

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact theoretical conditions under which input space mode connectivity holds in untrained networks?
- Basis in paper: [explicit] The paper conjectures geometric mode connectivity exists in untrained networks and provides a proof sketch based on percolation theory
- Why unresolved: The proof is only a sketch and relies on assumptions about independence between input cubes that don't hold in practice due to correlations in neural networks
- What evidence would resolve it: A complete mathematical proof showing exact conditions under which connectivity emerges, accounting for correlations between nearby inputs in neural networks

### Open Question 2
- Question: What is the mechanism behind the observed approximate linear mode connectivity in trained networks?
- Basis in paper: [explicit] The paper speculates this could be an aspect of implicit regularization but acknowledges this is speculative
- Why unresolved: While the paper provides some theoretical ideas (NTK limit, gradient descent behavior), these are not validated and the exact mechanism remains unclear
- What evidence would resolve it: Rigorous mathematical analysis connecting implicit regularization to linear connectivity, or experimental evidence showing how specific regularization mechanisms affect connectivity patterns

### Open Question 3
- Question: How does input space mode connectivity generalize to non-vision domains like language models?
- Basis in paper: [inferred] The paper only tests connectivity in vision models and mentions technical adjustments would be needed for language models using tokenization
- Why unresolved: The experiments are limited to continuous input spaces in vision models; language models have discrete token spaces that require different analysis
- What evidence would resolve it: Empirical studies showing connectivity patterns in language models, or theoretical analysis of how connectivity properties transfer between continuous and discrete input spaces

## Limitations
- The theoretical explanation via percolation theory remains unproven and is presented as a conjecture
- The effectiveness of adversarial detection may degrade against attacks specifically designed to minimize barrier heights
- The method requires significant computational resources for the optimization process to bypass loss barriers

## Confidence

- **High confidence**: The empirical demonstration of input space mode connectivity in trained models (Mechanism 3) is well-supported by the presented experiments.
- **Medium confidence**: The observation that untrained models also exhibit connectivity (Mechanism 1) is empirically valid but the theoretical explanation via percolation theory remains unproven.
- **Low confidence**: The claim that implicit regularization causes linear paths in trained models (Mechanism 2) is speculative and lacks direct evidence.

## Next Checks

1. **Theoretical validation**: Rigorously test whether the connectivity in high-dimensional spaces can be mathematically explained by percolation theory, particularly examining the critical probability threshold for connectivity.

2. **Robustness evaluation**: Test the adversarial detection method against gradient-based attacks specifically designed to minimize barrier heights, measuring performance degradation.

3. **Cross-architecture verification**: Evaluate mode connectivity and adversarial detection performance across different network architectures (CNNs, transformers, vision transformers) to determine if the phenomenon is architecture-dependent.