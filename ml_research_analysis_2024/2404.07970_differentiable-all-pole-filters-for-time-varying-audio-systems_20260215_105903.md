---
ver: rpa2
title: Differentiable All-pole Filters for Time-varying Audio Systems
arxiv_id: '2404.07970'
source_url: https://arxiv.org/abs/2404.07970
tags:
- filter
- audio
- time-varying
- which
- filters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training time-varying audio
  systems containing recursive IIR filters using automatic differentiation. The authors
  propose an efficient backpropagation algorithm for all-pole filters, enabling end-to-end
  training without approximations or frame-based processing.
---

# Differentiable All-pole Filters for Time-varying Audio Systems

## Quick Facts
- arXiv ID: 2404.07970
- Source URL: https://arxiv.org/abs/2404.07970
- Reference count: 0
- Primary result: Proposed time-domain backpropagation algorithm for all-pole filters enables efficient training of time-varying audio systems with significant speedups (2-30x) over frequency-sampling methods

## Executive Summary
This paper introduces a novel backpropagation algorithm for training time-varying audio systems containing recursive IIR all-pole filters. The method computes exact gradients through time-varying filters in the time domain, avoiding the approximations and artifacts associated with frequency-sampling approaches. By re-expressing the all-pole filter structure, the algorithm enables end-to-end training without frame-based processing or spectral transformations. The approach is demonstrated on three audio systems: a phaser, a subtractive synthesizer, and a compressor, showing significant improvements in both accuracy and training efficiency compared to existing methods.

## Method Summary
The method re-expresses time-varying all-pole filters to enable exact gradient computation through backpropagation. Instead of converting to frequency domain with STFT or FFT, gradients are computed directly in the time domain using recursive filter structures. The key insight is expressing the time-varying all-pole filter as a sequence of operations that can be backpropagated through itself using the same filter structure. This is achieved through closed-form expressions for gradients that allow reuse of the original filter implementation for both forward and backward passes. The algorithm is implemented using custom PyTorch operators and Numba acceleration, enabling efficient execution without the artifacts introduced by frequency sampling approaches.

## Key Results
- Phaser model achieves lower error-to-signal ratios compared to frequency-sampling methods
- Subtractive synthesizer model outperforms frequency-sampling approaches in both accuracy and speed, with 2-30x speedup
- Compressor model shows improved performance and faster training times
- Time-domain implementation avoids artifacts from windowing, frame boundaries, and circular convolution inherent in frequency sampling

## Why This Works (Mechanism)

### Mechanism 1
The method achieves exact gradient computation through time-varying all-pole filters by re-expressing the filter as a sequence of operations that can be backpropagated through itself using the same filter structure. This leverages closed-form expressions for gradients that allow reuse of the original filter implementation for both forward and backward passes, avoiding the deep computational graphs created by standard automatic differentiation frameworks.

### Mechanism 2
The approach enables efficient gradient computation by avoiding frame-based processing and frequency sampling approximations. Instead of converting to frequency domain with STFT or FFT, gradients are computed directly in the time domain using the recursive filter structure. This eliminates artifacts from windowing, frame boundaries, and circular convolution inherent in frequency sampling approaches.

### Mechanism 3
The method provides significant speedups by leveraging highly optimized implementations and avoiding redundant computations. Gradients are computed with a single pass through the filter plus simple multiplications, rather than requiring separate forward and backward passes through different implementations. The use of Numba and custom PyTorch operators enables efficient execution.

## Foundational Learning

- Concept: Automatic differentiation through recursive structures
  - Why needed here: Standard automatic differentiation frameworks struggle with recursive filters because each time step creates a new node in the computational graph, leading to deep graphs and slow training
  - Quick check question: Why can't standard PyTorch handle recursive IIR filters efficiently?

- Concept: Frequency sampling vs time-domain processing
  - Why needed here: Understanding the trade-offs between frequency-domain approximations (faster but less accurate) and time-domain implementations (slower but exact)
  - Quick check question: What are the main artifacts introduced by frequency sampling that the time-domain approach avoids?

- Concept: All-pole filter stability and pole placement
  - Why needed here: The method requires careful handling of filter poles to maintain stability during both forward and backward passes
  - Quick check question: How does the method ensure stability when computing gradients through potentially unstable filter configurations?

## Architecture Onboarding

- Component map: Differentiable all-pole filter -> Filter decomposition system (phaser, synthesizer, compressor) -> Control signal generation (LFOs, neural networks) -> Training loop with gradient descent -> Evaluation and benchmarking utilities
- Critical path: Filter → Gradient computation → Parameter update → Stability check
- Design tradeoffs: Time-domain accuracy vs frequency-domain speed, single precision vs double precision for numerical stability, custom implementation complexity vs framework integration
- Failure signatures: Numerical instability when poles approach unit circle, slow convergence when learning rates are too high, accuracy degradation when control signals change too rapidly
- First 3 experiments:
  1. Implement the basic all-pole filter with forward and backward passes using the FLIP operation
  2. Test gradient computation on a simple time-varying filter with known analytical gradients
  3. Benchmark speed and accuracy against frequency sampling on a basic phaser model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of numerical precision on the stability of the proposed time-domain all-pole filter implementation?
- Basis in paper: The authors mention observing instability during early experiments with the TD implementation, which was resolved by switching from single-precision to double-precision.
- Why unresolved: The paper does not provide a detailed analysis of how different numerical precisions affect the stability and performance of the algorithm.
- What evidence would resolve it: A comprehensive study comparing the performance and stability of the TD implementation across various numerical precisions (e.g., single, double, half) on different hardware platforms.

### Open Question 2
- Question: How does the proposed time-domain method compare to frequency-sampling in terms of computational efficiency for very long audio signals?
- Basis in paper: The paper demonstrates that the TD method is faster than FS for the tested audio systems, but it does not explicitly address the performance for very long audio signals.
- Why unresolved: The experiments conducted in the paper use relatively short audio signals, and the computational efficiency for longer signals is not explored.
- What evidence would resolve it: Benchmarking the TD and FS methods on audio signals of varying lengths, particularly focusing on very long signals, to compare their computational efficiency and scalability.

### Open Question 3
- Question: Can the proposed backpropagation algorithm be extended to handle non-linear systems or systems with feedback loops beyond simple all-pole filters?
- Basis in paper: The authors mention that their method is specifically designed for all-pole filters and demonstrate its application on three audio systems with recursive structures.
- Why unresolved: The paper does not explore the potential extension of the algorithm to more complex non-linear systems or those with more intricate feedback structures.
- What evidence would resolve it: Developing and testing an extended version of the backpropagation algorithm that can handle non-linear systems or more complex feedback structures, and comparing its performance to existing methods.

## Limitations
- Numerical stability concerns when filter poles approach the unit circle, potentially requiring double precision arithmetic
- Scalability limitations for very large audio systems with high-order filters and complex control signal architectures
- Method is specifically designed for all-pole filters and may not generalize to other recursive filter types without modification

## Confidence

**High Confidence** (Mechanistic Claims):
- The derivation of the time-domain backpropagation algorithm for all-pole filters is mathematically sound and well-supported by the equations provided
- The basic implementation approach using FLIP operations and recursive filter reuse is valid and can be implemented as described

**Medium Confidence** (Performance Claims):
- The reported speed improvements (2-30x) are likely accurate for the specific implementations used but may vary significantly with different hardware, frameworks, or optimization levels
- The accuracy improvements over frequency sampling methods are demonstrated but may depend heavily on the specific audio content and filter configurations tested

**Low Confidence** (Generalization Claims):
- The assertion that this approach will work equally well for all types of time-varying audio systems and filter configurations is not fully supported by the experimental evidence
- The claim about seamless integration with neural network-based control signal generation, while plausible, is not thoroughly validated across different network architectures

## Next Checks
1. **Stability Analysis**: Systematically test the method's numerical stability across a wide range of filter pole locations, control signal frequencies, and filter orders. Measure the maximum stable learning rates and identify conditions where double precision becomes necessary.

2. **Scalability Benchmark**: Implement and test the method on larger audio processing systems with higher-order filters (8th order and above) and more complex control signal architectures. Compare computational complexity and memory usage against frequency sampling approaches at scale.

3. **Cross-Platform Validation**: Reimplement the core algorithm in a different framework (e.g., TensorFlow or JAX) without relying on custom operators or Numba. Validate that the same accuracy and speed improvements can be achieved with standard framework capabilities.