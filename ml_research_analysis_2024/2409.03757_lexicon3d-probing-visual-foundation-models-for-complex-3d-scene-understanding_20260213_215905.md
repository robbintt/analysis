---
ver: rpa2
title: 'Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding'
arxiv_id: '2409.03757'
source_url: https://arxiv.org/abs/2409.03757
tags:
- scene
- understanding
- foundation
- tasks
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first comprehensive study that systematically\
  \ probes various visual foundation models for 3D scene understanding, identifying\
  \ the strengths and limitations of each model across different scenarios. We evaluate\
  \ seven vision foundation encoders\u2014image-based, video-based, and 3D foundation\
  \ models\u2014on four diverse tasks: Vision-Language Scene Reasoning, Visual Grounding,\
  \ Segmentation, and Registration."
---

# Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding

## Quick Facts
- arXiv ID: 2409.03757
- Source URL: https://arxiv.org/abs/2409.03757
- Authors: Yunze Man; Shuhong Zheng; Zhipeng Bao; Martial Hebert; Liang-Yan Gui; Yu-Xiong Wang
- Reference count: 40
- Key outcome: First comprehensive study systematically probing various visual foundation models for 3D scene understanding across diverse tasks

## Executive Summary
This paper presents Lexicon3D, the first comprehensive study that systematically probes various visual foundation models for 3D scene understanding. The research evaluates seven vision foundation encoders—image-based, video-based, and 3D foundation models—on four diverse tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration. Through a unified probing framework, the study identifies DINOv2 as demonstrating superior overall performance, while revealing unexpected limitations in language-pretrained models on language-related tasks and highlighting how pretraining objectives influence task-specific capabilities.

## Method Summary
The study employs a unified probing framework that extracts features from different visual foundation models, constructs 3D feature embeddings as scene representations, and evaluates them on multiple downstream tasks. The framework freezes encoder parameters and tunes only linear or shallow probing heads. Evaluation uses datasets including ScanNet, ScanQA, SQA3D, ScanRefer, and 3D-Language-Data across four tasks: vision-language reasoning (BLEU, ROUGE, METEOR, CIDEr, EM-1), visual grounding (accuracy metrics), semantic segmentation (mIoU), and partial scene registration (RR, RTE metrics).

## Key Results
- DINOv2 demonstrates superior overall performance across vision-language reasoning and visual grounding tasks
- Video models excel in object-level tasks due to temporally continuous input frames providing instance-aware multi-view consistent guidance
- Diffusion models benefit geometric tasks because their generative pretraining objective requires understanding spatial relationships and structure
- Language-pretrained models show unexpected limitations in language-related tasks despite their linguistic training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DINOv2 demonstrates superior performance across vision-language reasoning and visual grounding tasks due to its self-supervised training on massive datasets with a combination of masked-image modeling and invariance-based self-distillation.
- **Mechanism:** The pretraining strategy enables DINOv2 to learn rich, generalizable visual representations that transfer well to diverse 3D scene understanding tasks without task-specific fine-tuning.
- **Core assumption:** Large-scale pretraining with contrastive objectives and denoising can produce robust features that generalize across different modalities and tasks.
- **Evidence anchors:**
  - [abstract] "DINOv2 demonstrates superior overall performance; video models excel in object-level tasks; diffusion models benefit geometric tasks"
  - [section 2] "DINOv2 [61], combining a masked-image modeling loss and an invariance-based self-distillation loss, has become one of the most scalable and competitive self-supervised learning architectures"
  - [corpus] Weak - no direct comparison of DINOv2's pretraining approach to alternatives
- **Break condition:** If the task domain significantly differs from natural image distribution, or if the task requires specific domain knowledge not captured in pretraining.

### Mechanism 2
- **Claim:** Video models excel in object-level tasks due to temporally continuous input frames providing instance-aware multi-view consistent guidance.
- **Mechanism:** Temporal continuity in video data allows models to learn object boundaries and distinguish between multiple instances of the same semantic category across frames.
- **Core assumption:** Temporal information in video data provides geometric and instance-level cues that static images cannot capture.
- **Evidence anchors:**
  - [section 3.3] "video encoding models demonstrate significant advantages over image and 3D encoders... primarily lies in the multiple category, indicating that these models excel at discriminating the correct object among multiple objects of the same semantic category"
  - [section 3.3] "This capability largely stems from the temporally continuous input frames, which provide instance-aware multi-view consistent guidance"
  - [corpus] Weak - limited corpus evidence directly comparing video vs image models on instance discrimination
- **Break condition:** If the video data lacks temporal coherence or contains significant motion blur, or if the scene understanding task is purely global without object-level discrimination requirements.

### Mechanism 3
- **Claim:** Diffusion models benefit geometric tasks because their generative pretraining objective requires understanding spatial relationships and structure.
- **Mechanism:** The denoising diffusion process during training forces the model to learn geometric correspondence and spatial reasoning to reconstruct coherent images.
- **Core assumption:** Generative pretraining with diffusion processes inherently captures geometric understanding beyond semantic features.
- **Evidence anchors:**
  - [section 3.5] "StableDiffusion and StableVideoDiffusion showcase superior geometric capability in our partial scene registration task. It demonstrates that the pretraining objective of generation empowers the foundation models to have a decent capability of finding geometric correspondences in 3D scenes"
  - [section 3.5] "video encoders generally perform better than image encoders. The reason is that video foundation models have a better understanding of object shapes and geometry within the scenes from the multi-view input frames"
  - [corpus] Weak - no direct evidence linking diffusion model architecture to geometric understanding capability
- **Break condition:** If the geometric task requires precise metric understanding rather than relative correspondence, or if the scene lacks sufficient texture for diffusion models to capture geometry.

## Foundational Learning

- **Concept:** Vision foundation models and their pretraining paradigms (self-supervised, language-guided, generative)
  - Why needed here: Understanding the different training approaches helps explain why certain models perform better on specific tasks and guides selection for new applications.
  - Quick check question: What are the three main categories of vision foundation model pretraining approaches evaluated in Lexicon3D, and which model belongs to each category?

- **Concept:** 3D scene representation using multi-view images and point clouds
  - Why needed here: The unified probing framework relies on projecting 2D features into 3D space, requiring understanding of camera poses and point cloud representations.
  - Quick check question: How does the multi-view 3D projection module convert image features into point cloud features for 3D scene understanding?

- **Concept:** Evaluation metrics for vision-language tasks (BLEU, ROUGE, METEOR, CIDEr)
  - Why needed here: These metrics are used to evaluate the vision-language reasoning task, and understanding them is crucial for interpreting results.
  - Quick check question: Which metric would be most appropriate for evaluating the factual accuracy of generated answers in vision-language reasoning tasks?

## Architecture Onboarding

- **Component map:**
  Visual foundation model encoders (frozen) → Multi-view 3D projection module → Task-specific probing heads
  For each task: different probing heads (VQA with Q-Former, visual grounding with attention fusion, semantic segmentation with linear classifier, registration with transformer cross-encoder)
  Data pipeline: multi-view images and point clouds → feature extraction → 3D feature field construction → task evaluation

- **Critical path:**
  1. Load pretrained visual foundation model checkpoint
  2. Extract features from multi-view images or videos
  3. Project features into 3D space using camera poses
  4. Aggregate point cloud features with mean pooling
  5. Pass features through task-specific probing head
  6. Compute evaluation metrics

- **Design tradeoffs:**
  - Frozen encoder vs. fine-tuned: Frozen encoders provide consistent comparison but may miss task-specific adaptation; fine-tuning improves performance but obscures model capabilities.
  - Linear probing vs. deeper heads: Linear probing isolates encoder quality but may be too restrictive; deeper heads may learn task-specific features but conflate encoder and head performance.
  - Memory vs. performance: Processing full-resolution videos provides better features but requires more memory; downsampling saves resources but may lose information.

- **Failure signatures:**
  - Poor performance across all tasks: Likely indicates issues with feature projection or task head implementation
  - Good performance on some tasks but not others: Suggests encoder-task mismatch or dataset bias
  - Memory errors during feature extraction: Indicates insufficient GPU memory for full-resolution processing
  - Slow processing times: May indicate inefficient feature aggregation or unnecessary computation

- **First 3 experiments:**
  1. Verify feature extraction pipeline by visualizing projected 3D feature fields for a simple scene using DINOv2
  2. Test vision-language reasoning pipeline end-to-end on a small subset of ScanQA with a single encoder
  3. Compare performance of frozen vs. fine-tuned Q-Former on vision-language reasoning to understand adaptation benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do language-pretrained models like CLIP or LSeg actually underperform on language tasks because of domain shift, or is it due to their specific pretraining objectives not aligning with the evaluation tasks?
- Basis in paper: [explicit] The paper notes that language-pretrained models like CLIP and LSeg, despite their language guidance during pretraining, show limitations in language-related tasks such as vision-language reasoning.
- Why unresolved: The paper suggests this is an unexpected finding but doesn't definitively explain whether it's due to domain shift (training vs. evaluation data) or the mismatch between pretraining objectives and the specific evaluation tasks.
- What evidence would resolve it: A controlled study comparing these models' performance when evaluated on tasks similar to their pretraining objectives versus the current evaluation tasks, or ablation studies isolating the effect of pretraining objectives vs. domain shift.

### Open Question 2
- Question: Can the observed performance gap between object-centric and scene-level pretrained models (Uni3D vs. Swin3D) be bridged by a hybrid pretraining approach that combines both perspectives?
- Basis in paper: [explicit] The paper compares Uni3D (object-centric) and Swin3D (scene-level), showing distinct strengths and weaknesses for each, particularly in scene-level vs. object-centric tasks.
- Why unresolved: While the paper demonstrates the differences, it doesn't explore whether a hybrid pretraining strategy could leverage the strengths of both approaches.
- What evidence would resolve it: Developing and evaluating a model pretrained on both object-centric and scene-level data, measuring performance improvements across all evaluated tasks.

### Open Question 3
- Question: How much does the performance of visual foundation models on 3D scene understanding tasks depend on the quality and quantity of 3D data used during pretraining?
- Basis in paper: [inferred] The paper notes that 3D foundation models like Swin3D underperform compared to 2D models, attributing this to "limited training data" and "domain shift."
- Why unresolved: The paper doesn't quantify the relationship between pretraining data characteristics and downstream task performance for 3D understanding.
- What evidence would resolve it: Systematic experiments varying the amount and quality of 3D pretraining data, or comparing models pretrained on synthetic vs. real 3D data.

## Limitations
- None of the tested models can reliably generalize to tasks requiring spatial understanding beyond their pretraining distribution
- The study only evaluates seven specific models, potentially missing emergent capabilities from newer architectures
- Frozen encoder approach may underestimate the true potential of models that could benefit from task-specific fine-tuning

## Confidence
- **High confidence**: DINOv2's superior overall performance and video models' advantage in object-level tasks
- **Medium confidence**: Diffusion models benefit geometric tasks (requires more direct evidence linking diffusion training objectives to geometric understanding)
- **Low confidence**: Unexpected limitations of language-pretrained models in language tasks (lacks sufficient explanation)

## Next Checks
1. Test the same probing framework on additional foundation models released after the study to verify whether observed patterns hold across a broader model population
2. Conduct ablation studies comparing frozen vs. fine-tuned encoders on the same tasks to quantify the impact of adaptation on performance
3. Evaluate models on synthetic 3D scenes with controlled properties to systematically identify which scene characteristics most impact model performance across different task types