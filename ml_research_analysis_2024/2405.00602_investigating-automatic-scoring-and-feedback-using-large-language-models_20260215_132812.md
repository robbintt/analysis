---
ver: rpa2
title: Investigating Automatic Scoring and Feedback using Large Language Models
arxiv_id: '2405.00602'
source_url: https://arxiv.org/abs/2405.00602
tags: []
core_contribution: This paper investigates the use of large language models (LLMs)
  for automatic grading and feedback generation in educational settings. The authors
  explore parameter-efficient fine-tuning (PEFT) methods like LoRA and QLoRA to address
  the computational challenges of fine-tuning LLMs.
---

# Investigating Automatic Scoring and Feedback using Large Language Models

## Quick Facts
- **arXiv ID:** 2405.00602
- **Source URL:** https://arxiv.org/abs/2405.00602
- **Reference count:** 0
- **Primary result:** Fine-tuned 4-bit quantized LLaMA-2 models achieve <3% error in grade prediction and outperform base models in feedback generation quality

## Executive Summary
This paper explores the application of large language models for automatic grading and feedback generation in educational contexts. The authors address computational constraints by employing parameter-efficient fine-tuning methods including LoRA and QLoRA on 4-bit quantized LLaMA-2 models. Through experiments on both proprietary and open-source datasets, the study demonstrates that fine-tuned models can accurately predict grades with minimal error while generating feedback that closely resembles expert feedback as measured by standard text similarity metrics.

## Method Summary
The researchers fine-tune 4-bit quantized LLaMA-2 models using parameter-efficient techniques (LoRA and QLoRA) for two distinct tasks: regression-based grade prediction and generative feedback generation. The fine-tuning process targets both proprietary and open-source educational datasets, with model performance evaluated through accuracy metrics for grade prediction and BLEU/ROUGE scores for feedback quality assessment. The quantization approach enables cost-effective model deployment while maintaining competitive performance against full-precision base models.

## Key Results
- Fine-tuned LLMs achieve less than 3% average error in grade prediction accuracy
- 4-bit quantized LLaMA-2 13B models outperform competitive base models in feedback generation
- Generated feedback demonstrates high similarity to expert feedback with strong BLEU and ROUGE scores
- Quantization approaches enable lower computational costs and reduced latency for model deployment

## Why This Works (Mechanism)
The combination of parameter-efficient fine-tuning (LoRA/QLoRA) and 4-bit quantization effectively balances computational efficiency with model performance. LoRA reduces the number of trainable parameters by learning low-rank updates to the model weights, while QLoRA extends this by applying quantization to the pre-trained model weights. This dual approach allows fine-tuning on resource-constrained hardware without significant performance degradation. The fine-tuned models can adapt to specific educational domains while maintaining the general language understanding capabilities of the base LLaMA-2 architecture.

## Foundational Learning
- **Parameter-efficient fine-tuning (PEFT):** Why needed - reduces computational cost of adapting large models to specific tasks; Quick check - verify training time reduction compared to full fine-tuning
- **4-bit quantization:** Why needed - enables model deployment on resource-constrained hardware; Quick check - confirm model size reduction and inference speed improvements
- **BLEU and ROUGE metrics:** Why needed - provide quantitative measures of text similarity for feedback evaluation; Quick check - ensure scores correlate with human assessment of feedback quality
- **Regression vs generative tasks:** Why needed - different evaluation approaches required for numeric prediction versus text generation; Quick check - validate appropriate metric selection for each task type

## Architecture Onboarding

**Component Map:** Input data → Preprocessing → PEFT (LoRA/QLoRA) → Fine-tuned Model → Grade Prediction/Feedback Generation → Evaluation Metrics

**Critical Path:** Training data preparation → Model fine-tuning → Inference → Performance evaluation

**Design Tradeoffs:** Computational efficiency vs. model performance (quantization reduces cost but may impact quality), accuracy vs. latency in real-time grading applications

**Failure Signatures:** Overfitting to training data (poor generalization), metric optimization without pedagogical value (high BLEU/ROUGE but low feedback utility), computational bottlenecks during inference

**First Experiments:**
1. Baseline grade prediction accuracy using full-precision LLaMA-2 models
2. Comparative analysis of LoRA vs QLoRA fine-tuning effectiveness
3. Human evaluation study of feedback quality beyond automated metrics

## Open Questions the Paper Calls Out
- How do different educational domains and subject matters affect model performance in grade prediction and feedback generation?
- What are the long-term impacts of using AI-generated feedback on student learning outcomes and writing development?
- How can models be adapted to handle diverse assessment formats beyond traditional essay-style assignments?

## Limitations
- Heavy reliance on BLEU and ROUGE metrics may not capture pedagogical effectiveness of feedback
- Proprietary datasets limit independent validation and replication of results
- Focus on LLaMA-2 models restricts assessment of other architectures or quantization methods
- Potential biases in grading and feedback generation from training data composition
- Limited evaluation of feedback quality beyond text similarity metrics to expert responses

## Confidence

**High confidence:** Grade prediction accuracy (<3% error) and computational advantages of quantization
**Medium confidence:** Feedback quality comparisons between fine-tuned and base models based on BLEU/ROUGE scores
**Medium confidence:** Generalizability to different educational domains and assessment types

## Next Checks

1. Conduct blind human expert evaluation studies comparing pedagogical quality and actionability of AI-generated feedback versus human feedback across multiple subjects
2. Perform bias audits on fine-tuned models using diverse student populations to identify systematic grading disparities
3. Test model robustness through cross-dataset validation using public educational datasets to assess generalizability beyond proprietary data sources
4. Evaluate long-term learning impacts through controlled studies measuring student performance when receiving AI-generated feedback