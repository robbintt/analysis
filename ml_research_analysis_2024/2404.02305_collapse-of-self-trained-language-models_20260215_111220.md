---
ver: rpa2
title: Collapse of Self-trained Language Models
arxiv_id: '2404.02305'
source_url: https://arxiv.org/abs/2404.02305
tags:
- language
- final
- self-training
- gpt-2
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential and limitations of self-training
  language models on their own outputs. The authors explore whether models can evolve
  and learn from their own generated data, similar to how humans build on previous
  thoughts.
---

# Collapse of Self-trained Language Models

## Quick Facts
- arXiv ID: 2404.02305
- Source URL: https://arxiv.org/abs/2404.02305
- Reference count: 14
- Models collapse into repetitive output when self-trained on their own generated data

## Executive Summary
This study investigates the potential and limitations of self-training language models on their own outputs. Using GPT-2 as a testbed, the authors conduct experiments where the model is trained on its own generated sequences in a loop. The primary finding is that extended self-training leads to severe performance degradation - the model collapses into generating repetitive and biased token sequences. This collapse occurs more rapidly with higher learning rates, and while training loss approaches zero on generated data, validation loss increases significantly. The authors also find that larger models tend to collapse faster, raising concerns about future training data contamination as more AI-generated content appears online.

## Method Summary
The authors use GPT-2 as their testbed for self-training experiments. They create a loop where the model generates sequences that are then used as training data for subsequent training iterations. The experiments vary learning rates and model sizes to understand how these factors affect the self-training dynamics. Training and validation losses are monitored throughout the process, and the quality and diversity of generated outputs are analyzed. The study focuses on unconditional generation without task-specific objectives, creating a controlled environment to observe the fundamental limitations of self-training.

## Key Results
- Extended self-training causes language models to collapse into generating repetitive and biased token sequences
- Higher learning rates accelerate the collapse process
- Larger models tend to collapse faster than smaller ones
- Training loss approaches zero on generated data while validation loss increases significantly

## Why This Works (Mechanism)
The collapse occurs because self-training creates a positive feedback loop where the model reinforces its own biases and errors. As the model generates data, any initial biases or artifacts become embedded in the training corpus. Subsequent training iterations amplify these patterns, causing the model to converge on a narrow distribution of tokens and sequences. The training loss decreases because the model learns to perfectly reproduce its own limited output distribution, but this comes at the cost of generalization ability and diversity. The phenomenon is exacerbated by larger models' greater capacity to memorize and overfit to the increasingly homogeneous training data.

## Foundational Learning
- **Language Model Training**: Understanding how language models learn from sequential data through next-token prediction is essential for grasping why self-training creates problematic feedback loops. Quick check: Verify you can explain the difference between training loss and validation loss in the context of language modeling.
- **Positive Feedback Loops in Machine Learning**: Recognizing how models can amplify their own errors when trained on their outputs helps contextualize the collapse phenomenon. Quick check: Identify other scenarios in ML where positive feedback loops can cause problems.
- **Token Distribution Collapse**: Understanding how probability distributions over tokens can narrow over successive training iterations explains the loss of diversity. Quick check: Demonstrate how a uniform token distribution can become peaked through repeated sampling and retraining.

## Architecture Onboarding

**Component Map**: Data Generator -> Training Loop -> Model Parameters -> Data Generator

**Critical Path**: The critical path is the continuous loop where generated data becomes training data, which updates model parameters, which generates new data. This closed loop is what creates the positive feedback that leads to collapse.

**Design Tradeoffs**: The study sacrifices task-specific performance for generality by focusing on unconditional generation. This allows observation of fundamental limitations but may not reflect practical deployment scenarios where models are fine-tuned for specific tasks.

**Failure Signatures**: Early warning signs include decreasing validation loss while training loss continues to decrease, increasing repetition in generated sequences, and narrowing of the token distribution as measured by entropy.

**First Experiments**: 
1. Replicate the self-training loop with GPT-2 at different learning rates to verify the relationship between learning rate and collapse speed
2. Test whether introducing a small amount of fresh, human-written data at each iteration can prevent or delay collapse
3. Measure vocabulary diversity metrics (unique tokens, n-gram diversity) throughout self-training to quantify when diversity degradation begins

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments are limited to GPT-2 architecture, limiting generalizability to other model families
- Study focuses on unconditional generation without task-specific objectives
- Mechanism explaining why larger models collapse faster remains speculative
- Practical implications are limited by the narrow experimental scope

## Confidence
- Model collapse observation: High
- Larger models collapse faster: Medium
- Data contamination concerns: Low-Medium

## Next Checks
1. Test self-training dynamics across multiple model architectures (GPT-3, LLaMA, Mistral) to verify if collapse patterns are architecture-independent
2. Investigate whether task-specific fine-tuning during self-training can mitigate collapse by adding supervised objectives to the training loop
3. Measure diversity metrics (e.g., vocabulary coverage, n-gram uniqueness) during self-training to quantify when and how diversity degradation begins