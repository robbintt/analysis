---
ver: rpa2
title: Large Language Models for Expansion of Spoken Language Understanding Systems
  to New Languages
arxiv_id: '2404.02588'
source_url: https://arxiv.org/abs/2404.02588
tags:
- translation
- language
- slot
- languages
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using LLM-based machine translation to expand
  Spoken Language Understanding (SLU) systems to new languages, addressing the challenge
  of low-resource languages lacking extensive labeled datasets. The core idea is to
  fine-tune a large language model (BigTranslate) for machine translation of slot-annotated
  SLU training data, leveraging the EasyProject method for slot annotation with HTML-like
  tags.
---

# Large Language Models for Expansion of Spoken Language Understanding Systems to New Languages

## Quick Facts
- arXiv ID: 2404.02588
- Source URL: https://arxiv.org/abs/2404.02588
- Authors: Jakub Hoscilowicz; Pawel Pawlowski; Marcin Skorupa; Marcin Sowa≈Ñski; Artur Janicki
- Reference count: 0
- One-line primary result: LLM-based machine translation of slot-annotated SLU data improves Overall Accuracy from 53% to 62.18% on MultiATIS++ benchmark.

## Executive Summary
This paper proposes using LLM-based machine translation to expand Spoken Language Understanding (SLU) systems to new languages, addressing the challenge of low-resource languages lacking extensive labeled datasets. The core idea is to fine-tune a large language model (BigTranslate) for machine translation of slot-annotated SLU training data, leveraging the EasyProject method for slot annotation with HTML-like tags. This approach improves the Overall Accuracy metric on the MultiATIS++ benchmark from 53% to 62.18% compared to the state-of-the-art method, Fine and Coarse-grained Multi-Task Learning Framework (FC-MTLF), in the cloud scenario using an mBERT model. In the on-device scenario, the method improves Overall Accuracy from 5.31% to 22.06% over the baseline Global-Local Contrastive Learning Framework (GL-CLeF) method. The LLM-based machine translation does not require changes in the production architecture of SLU and is slot-type independent, making it a promising solution for real-world applications.

## Method Summary
The proposed method involves fine-tuning the BigTranslate LLM on the MASSIVE dataset with HTML-like tagged named entities, then using it to translate English SLU training data into 8 target languages. The translated datasets are used to train SLU models (mBERT for cloud, small on-device models for mobile) without modifying their architecture. The approach leverages an iterative feedback loop to ensure consistency in slot annotations between source and target sentences, and is evaluated on the MultiATIS++ benchmark.

## Key Results
- Improved Overall Accuracy on MultiATIS++ benchmark from 53% to 62.18% compared to FC-MTLF in cloud scenario.
- Improved Overall Accuracy from 5.31% to 22.06% over GL-CLeF in on-device scenario.
- Method is slot-type independent and does not require changes to existing SLU production architecture.
- HTML-like tags for slot annotation effectively preserve slot integrity during translation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based machine translation can effectively preserve and translate slot annotations across languages by leveraging HTML-like tags for structured entity marking.
- Mechanism: The BigTranslate LLM is fine-tuned on the MASSIVE dataset where named entities are explicitly annotated with HTML-like tags (e.g., `<a>`, `<b>`). During translation, the model learns to maintain the semantic integrity of these annotated entities, ensuring that slot values (e.g., "John", "London") remain intact and correctly positioned in the target language output.
- Core assumption: The LLM can generalize from HTML-tagged slot annotations to correctly transfer and place slot values in translated sentences without explicit slot definitions or examples.
- Evidence anchors:
  - [abstract] "Our approach improved on the MultiATIS++ benchmark... utilizing Large Language Models (LLMs) that we fine-tune for machine translation of slot-annotated SLU training data."
  - [section 3.2] "Central to our methodology is employing the MASSIVE parallel open-source dataset... We modified annotations within the dataset with HTML-like tags, using the idea from EasyProject."
  - [corpus] Weak evidence: No direct corpus support for HTML-tag effectiveness in slot preservation; based on experimental results.
- Break condition: If the LLM fails to generalize slot annotation transfer or if HTML-tag usage introduces parsing errors in the target language.

### Mechanism 2
- Claim: The proposed method achieves state-of-the-art performance without requiring changes to the existing SLU production architecture.
- Mechanism: By focusing on data transformation (translating and annotating slot data) rather than model architecture changes, the method integrates seamlessly into existing SLU pipelines. The translated datasets are used to train or fine-tune SLU models (e.g., mBERT, BiLSTM) that are already in production.
- Core assumption: Existing SLU models can effectively learn from translated slot-annotated data without architectural modifications.
- Evidence anchors:
  - [abstract] "Contrary to both FC-MTLF and GL-CLeF, our LLM-based machine translation does not require changes in the production architecture of SLU."
  - [section 3.3] "We utilized only the English training set from the MultiATIS++. This dataset... served as a foundational resource. To demonstrate the efficacy of LLM in translating SLU datasets, we subsequently translated the English train set into several other languages."
  - [corpus] No direct corpus support for architectural integration benefits.
- Break condition: If the existing SLU architecture cannot effectively utilize the translated data or if integration introduces performance degradation.

### Mechanism 3
- Claim: The method is slot-type independent and does not require predefined slot definitions or examples, making it broadly applicable.
- Mechanism: The LLM-based translation does not rely on understanding specific slot semantics; it only needs to recognize and transfer HTML-tagged entities. This allows the method to be applied to various SLU domains without additional slot annotation efforts.
- Core assumption: HTML-like tags are sufficient to guide the LLM in slot transfer without semantic understanding of slot types.
- Evidence anchors:
  - [abstract] "Additionally, our pipeline is slot-type independent: it does not require any slot definitions or examples."
  - [section 3.1] "The Slot Transfer Task is the core challenge for a cross-lingual SLU. It involves accurately annotating named entities during the translation process."
  - [corpus] No direct corpus support for slot-type independence effectiveness.
- Break condition: If the absence of slot definitions leads to inaccurate translations or if the method fails in domains with complex slot structures.

## Foundational Learning

- Concept: Machine Translation and Slot Annotation
  - Why needed here: Understanding how machine translation can be adapted to preserve structured information (slots) is crucial for expanding SLU systems to new languages.
  - Quick check question: How do HTML-like tags help in maintaining slot integrity during machine translation?
- Concept: Multilingual BERT (mBERT) and Cross-Lingual Transfer
  - Why needed here: mBERT is used as the base model for SLU; understanding its multilingual capabilities is essential for leveraging translated data.
  - Quick check question: What are the advantages of using mBERT for cross-lingual SLU tasks?
- Concept: Slot Transfer Task and Named Entity Recognition
  - Why needed here: Slot transfer is central to maintaining the functionality of SLU across languages; understanding how named entities are identified and transferred is key.
  - Quick check question: Why is accurate slot transfer critical for the performance of cross-lingual SLU systems?

## Architecture Onboarding

- Component map:
  - Data Preparation: English SLU dataset with slot annotations.
  - Machine Translation: BigTranslate LLM fine-tuned on MASSIVE dataset with HTML-tagged entities.
  - Translation Pipeline: Iterative feedback loop to ensure tag consistency.
  - SLU Model Training: mBERT or BiLSTM models trained on translated datasets.
  - Evaluation: MultiATIS++ benchmark for assessing Overall Accuracy.
- Critical path:
  1. Fine-tune BigTranslate on MASSIVE dataset with HTML-tagged slots.
  2. Translate English SLU training data into target languages.
  3. Train SLU models (mBERT/BiLSTM) on translated datasets.
  4. Evaluate performance on MultiATIS++ test sets.
- Design tradeoffs:
  - Using a large LLM (BigTranslate) offers high translation quality but increases computational requirements.
  - HTML-tag annotation simplifies slot transfer but may introduce parsing complexities.
  - The method avoids architectural changes, enhancing compatibility but potentially limiting optimization for specific languages.
- Failure signatures:
  - Significant drop in Overall Accuracy on target languages.
  - Mismatch between slot counts in source and target sentences.
  - SLU models fail to learn from translated data effectively.
- First 3 experiments:
  1. Fine-tune BigTranslate on a subset of MASSIVE dataset and evaluate slot preservation accuracy.
  2. Translate a small English SLU dataset and train a simple SLU model to assess translation quality.
  3. Conduct ablation studies by removing HTML-tags to measure their impact on slot transfer accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the LLM-based MT approach compare to RLHF-enhanced MT in terms of SLU performance?
- Basis in paper: [inferred] The authors suggest that including RLHF could improve the method by addressing nuanced language complexities.
- Why unresolved: The paper does not implement RLHF and does not provide empirical comparisons.
- What evidence would resolve it: Empirical studies comparing LLM-based MT with and without RLHF on SLU datasets.

### Open Question 2
- Question: What are the limitations of the current slot annotation approach in handling complex linguistic phenomena like idiomatic expressions or multi-word entities?
- Basis in paper: [inferred] The authors mention mismatches in Portuguese translations, indicating potential issues with complex linguistic structures.
- Why unresolved: The paper does not explore the handling of complex linguistic phenomena in detail.
- What evidence would resolve it: Detailed analysis of translation errors and their impact on SLU performance for complex linguistic structures.

### Open Question 3
- Question: How does the performance of the LLM-based MT approach vary across different domains beyond air travel?
- Basis in paper: [inferred] The authors mention the potential for broader application but do not provide domain-specific results.
- Why unresolved: The paper focuses on the air travel domain and does not explore other domains.
- What evidence would resolve it: Experimental results comparing the approach's performance across multiple SLU domains.

## Limitations
- HTML-tag based slot annotation lacks direct corpus support for effectiveness in slot preservation across languages.
- Method's reliance on a single large LLM (BigTranslate) introduces potential scalability and computational challenges.
- Absence of slot-type independence validation across diverse SLU domains leaves generalizability questions.

## Confidence
**High Confidence:**
- The LLM-based machine translation approach improves Overall Accuracy on the MultiATIS++ benchmark compared to existing methods.
- The method successfully translates slot-annotated data without requiring changes to existing SLU production architecture.
- The HTML-tag based slot annotation provides a viable mechanism for slot preservation during translation.

**Medium Confidence:**
- The approach is truly slot-type independent and can be applied to various SLU domains without additional slot annotation efforts.
- The iterative feedback loop effectively ensures tag consistency between source and target sentences.
- The method scales effectively to production environments with computational and latency constraints.

**Low Confidence:**
- The proposed approach outperforms all existing methods across all tested languages and scenarios.
- The HTML-tag based slot annotation is the optimal mechanism for slot preservation in LLM-based machine translation.
- The method's performance generalizes to real-world SLU systems beyond the MultiATIS++ benchmark.

## Next Checks
1. **Ablation Study on HTML-Tag Effectiveness:** Remove HTML-tags from the slot annotation process and evaluate the impact on slot preservation accuracy and Overall Accuracy in the MultiATIS++ benchmark. This will validate the necessity and effectiveness of HTML-tags in the proposed approach.

2. **Cross-Domain Generalization Test:** Apply the proposed method to a different SLU domain (e.g., automotive or smart home) with complex slot structures. Evaluate slot transfer accuracy and Overall Accuracy to assess the method's generalizability beyond the tested domain.

3. **Scalability and Computational Efficiency Analysis:** Measure the computational resources and latency introduced by the LLM-based machine translation pipeline in a production-like environment. Compare these metrics against the performance gains to assess the method's practical viability for real-world deployment.