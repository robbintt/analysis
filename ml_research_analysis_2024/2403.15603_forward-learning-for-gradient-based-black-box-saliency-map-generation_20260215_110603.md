---
ver: rpa2
title: Forward Learning for Gradient-based Black-box Saliency Map Generation
arxiv_id: '2403.15603'
source_url: https://arxiv.org/abs/2403.15603
tags:
- gradient
- saliency
- black-box
- gradients
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for generating saliency maps to explain
  decisions of black-box deep learning models. The key idea is to use the likelihood
  ratio method to estimate gradients with respect to the inputs, without requiring
  access to the model's internal parameters or architecture.
---

# Forward Learning for Gradient-based Black-box Saliency Map Generation

## Quick Facts
- arXiv ID: 2403.15603
- Source URL: https://arxiv.org/abs/2403.15603
- Authors: Zeliang Zhang; Mingqian Feng; Jinyang Jiang; Rongyi Zhu; Yijie Peng; Chenliang Xu
- Reference count: 35
- One-line primary result: Introduces a likelihood ratio method for black-box saliency map generation that outperforms existing techniques on ImageNet and scales to GPT-Vision

## Executive Summary
This paper addresses the challenge of generating saliency maps for black-box deep learning models without access to internal parameters. The proposed method uses the likelihood ratio technique to estimate gradients with respect to inputs by injecting noise and computing proxy gradients. To handle high-dimensional inputs, a blockwise computation approach is introduced where noise is injected into randomly selected image regions rather than the entire image. The method demonstrates superior performance on ImageNet benchmarks and shows promise for explaining decisions of large vision models like GPT-Vision.

## Method Summary
The method injects Gaussian noise into input images and computes proxy gradients using the likelihood ratio estimator, which allows gradient estimation without backpropagation through the black-box model. To reduce variance in high-dimensional spaces, noise is injected only into randomly selected rectangular blocks of the image, and results are averaged across multiple blocks and samples. The estimated gradients are then used to generate saliency maps that highlight important features for the model's decisions. The approach is evaluated on ImageNet using deletion/insertion metrics and tested for adversarial attack transferability.

## Key Results
- Outperforms existing techniques on ImageNet for saliency map generation quality
- Demonstrates improved adversarial attack transferability rates compared to baselines
- Successfully scales to explain decisions of large black-box models like GPT-Vision
- Shows significant variance reduction through blockwise computation compared to standard methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The likelihood ratio method can estimate gradients of black-box models without access to internal parameters.
- Mechanism: By injecting noise into inputs and computing proxy gradients using the likelihood ratio, the method approximates true gradients while bypassing the need for backpropagation through the model.
- Core assumption: The injected noise has zero mean and small variance, allowing the expectation of the proxy gradient to converge to the true gradient.
- Evidence anchors: [abstract] "We employ the likelihood ratio method to estimate output-to-input gradients and utilize them for saliency map generation." [section 3.2] "We propose our likelihood ratio gradient estimator for black-box models..."

### Mechanism 2
- Claim: Blockwise computation reduces variance in gradient estimation for high-dimensional inputs.
- Mechanism: By injecting noise only into randomly selected rectangular blocks of the image and averaging over multiple blocks, the variance of the gradient estimator is reduced compared to injecting noise into the entire image.
- Core assumption: The noise is independent between dimensions, and the number of times each input dimension is perturbed is maintained across standard and blockwise methods.
- Evidence anchors: [section 3.3] "We propose the blockwise computation pattern to enhance estimation accuracy... this approach involves initially selecting a rectangular segment... we inject noise exclusively into the block area."

### Mechanism 3
- Claim: The method can generate saliency maps that are transferable for adversarial attacks on black-box models.
- Mechanism: By using the estimated gradients to generate saliency maps, the method can identify the most important features for the model's decision, which can be perturbed to fool the model in adversarial attacks.
- Core assumption: The saliency maps capture robust features shared between different models, making the adversarial examples more transferable.
- Evidence anchors: [abstract] "Experiments on ImageNet demonstrate that the proposed method outperforms existing techniques in generating saliency maps that better capture model decisions and are more transferable to adversarial attacks."

## Foundational Learning

- Concept: Likelihood ratio method for gradient estimation
  - Why needed here: To estimate gradients of black-box models without access to internal parameters.
  - Quick check question: How does the likelihood ratio method compute proxy gradients using injected noise?

- Concept: Blockwise computation for variance reduction
  - Why needed here: To reduce the variance of gradient estimation for high-dimensional inputs.
  - Quick check question: How does injecting noise into randomly selected blocks of the image reduce variance compared to injecting noise into the entire image?

- Concept: Saliency maps for model interpretability
  - Why needed here: To generate visual explanations of the model's decision-making process.
  - Quick check question: How do saliency maps highlight the most important features for the model's decision?

## Architecture Onboarding

- Component map: Input -> Noise injection -> Black-box model -> Proxy gradient computation -> Gradient estimation -> Saliency map generation
- Critical path:
  1. Inject noise into the input
  2. Forward pass through the black-box model
  3. Compute proxy gradients using the likelihood ratio method
  4. Average proxy gradients to estimate true gradients
  5. Generate saliency maps using estimated gradients
- Design tradeoffs:
  - Noise injection: Tradeoff between noise magnitude and gradient estimation accuracy
  - Blockwise computation: Tradeoff between block size and variance reduction
  - Number of copies: Tradeoff between gradient estimation accuracy and computational cost
- Failure signatures:
  - Poor gradient estimation accuracy: Insufficient noise injection or inappropriate block size
  - High variance in gradient estimation: Inadequate number of copies or inappropriate block size
  - Inaccurate saliency maps: Poor gradient estimation or inappropriate saliency map generation method
- First 3 experiments:
  1. Evaluate gradient estimation accuracy on a simple black-box model with known gradients
  2. Compare variance reduction of blockwise computation vs. standard computation on high-dimensional inputs
  3. Assess saliency map quality and transferability for adversarial attacks on a black-box model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the blockwise computation technique scale with the dimensionality of the input data, and are there diminishing returns or limitations as the input size increases?
- Basis in paper: [inferred] The paper mentions the blockwise computation technique is introduced to address the challenge of high variance in gradient estimation for high-dimensional inputs. It presents experimental results showing improved performance with smaller block sizes, but does not explore the scalability limits.
- Why unresolved: The paper does not provide a comprehensive analysis of how the blockwise technique performs as the input dimensionality increases, leaving open questions about its scalability and potential limitations.
- What evidence would resolve it: Experimental results comparing the performance of the blockwise technique across a range of input dimensionalities, including very high-dimensional data, would provide insights into its scalability and limitations.

### Open Question 2
- Question: How does the choice of the target class (correct vs. incorrect) for gradient estimation impact the quality and interpretability of the generated saliency maps, and are there optimal strategies for selecting the target class in different scenarios?
- Basis in paper: [explicit] The paper discusses the selection of the class activation for gradient computation, including the use of nearest path distance and semantic distance for text-based outputs. It presents examples of explaining decisions for both correct and incorrect target classes, but does not provide a comprehensive analysis of the impact of this choice.
- Why unresolved: The paper does not explore the impact of the target class selection on the quality and interpretability of the saliency maps, leaving open questions about the optimal strategies for different scenarios.
- What evidence would resolve it: A systematic evaluation of the saliency maps generated for different target classes, including both correct and incorrect ones, would provide insights into the impact of this choice and help identify optimal strategies for different scenarios.

### Open Question 3
- Question: How does the proposed method perform on other types of black-box models, such as those with continuous outputs or multi-modal inputs, and are there any modifications or extensions required to adapt the method to these cases?
- Basis in paper: [explicit] The paper focuses on the application of the proposed method to classification models with discrete outputs, and provides a brief discussion on adapting the method to text-based outputs. It does not explore the performance on other types of black-box models.
- Why unresolved: The paper does not provide a comprehensive analysis of the performance of the proposed method on other types of black-box models, leaving open questions about its applicability and potential modifications required for different scenarios.
- What evidence would resolve it: Experimental results evaluating the performance of the proposed method on a diverse range of black-box models, including those with continuous outputs and multi-modal inputs, would provide insights into its applicability and potential modifications required for different scenarios.

## Limitations

- Likelihood ratio method's convergence guarantees for high-dimensional inputs are not rigorously proven
- Blockwise computation introduces sampling variance that may affect gradient estimation accuracy in certain regions
- Transferability claims assume saliency maps capture robust features across models without systematic exploration

## Confidence

- Mechanism 1 (likelihood ratio gradient estimation): Medium confidence - well-established in reinforcement learning but requires careful noise calibration for saliency maps
- Mechanism 2 (blockwise variance reduction): High confidence - mathematically sound principle with empirical validation
- Mechanism 3 (adversarial transferability): Low confidence - demonstrated but not mechanistically explained

## Next Checks

1. Verify gradient estimation accuracy on synthetic black-box functions with known gradients to isolate whether performance gaps stem from the likelihood ratio estimator versus implementation details
2. Systematically vary block sizes and sampling patterns to quantify the variance reduction trade-off and identify optimal configurations for different input resolutions
3. Test saliency map transferability across model pairs with varying architectural similarity to understand when explanations generalize versus when they fail