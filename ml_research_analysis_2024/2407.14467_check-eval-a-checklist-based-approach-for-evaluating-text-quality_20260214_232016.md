---
ver: rpa2
title: 'Check-Eval: A Checklist-based Approach for Evaluating Text Quality'
arxiv_id: '2407.14467'
source_url: https://arxiv.org/abs/2407.14467
tags:
- text
- evaluation
- checklist
- check-ev
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Check-Eval, a novel framework for evaluating
  text quality using large language models (LLMs) with a checklist-based approach.
  The method addresses the challenge of traditional metrics failing to align with
  human judgments in text generation tasks.
---

# Check-Eval: A Checklist-based Approach for Evaluating Text Quality

## Quick Facts
- arXiv ID: 2407.14467
- Source URL: https://arxiv.org/abs/2407.14467
- Reference count: 10
- Higher correlation with human judgments compared to existing metrics (G-Eval, GPTScore)

## Executive Summary
This paper introduces Check-Eval, a novel framework for evaluating text quality using large language models (LLMs) with a checklist-based approach. The method addresses the challenge of traditional metrics failing to align with human judgments in text generation tasks. Check-Eval works by generating a checklist of key points from a source text and then evaluating candidate texts against this checklist, offering both reference-free and reference-dependent evaluation methods. The framework was validated on two benchmark datasets: Portuguese Legal Semantic Textual Similarity and SummEval, showing significant improvements over existing metrics.

## Method Summary
Check-Eval is a two-stage framework that uses LLMs to evaluate text quality through checklist generation and evaluation. First, an LLM generates a checklist of key points from a source/reference text. Then, another LLM evaluates the candidate text against this checklist to assess presence/absence of each point. The framework offers three variations: reference-guided (evaluates recall of source content), candidate-guided (evaluates precision of generated content), and criterion-guided (reference-free evaluation using evaluation criteria). GPT-4-turbo and GPT-4 were used for experiments on Portuguese Legal Semantic Textual Similarity and SummEval datasets respectively.

## Key Results
- Check-Eval achieved a Spearman correlation of 0.623 for consistency on SummEval, significantly outperforming G-Eval (0.507) and GPTScore (0.449)
- The framework demonstrated higher correlations with human judgments across multiple dimensions including consistency, relevance, coherence, and fluency
- Reference-guided and candidate-guided variations provided both recall and precision evaluation perspectives, with F1 scores calculated to balance these metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Check-Eval improves alignment with human judgments by providing structured, checklist-based evaluation instead of probabilistic scoring
- Mechanism: The LLM generates a checklist of key content points from the source text, and the candidate text is evaluated against this checklist to assess presence/absence of each point
- Core assumption: Human evaluators implicitly use a checklist-like approach when judging text quality, focusing on key content points rather than probabilistic text generation likelihood
- Evidence anchors:
  - [abstract] "Our results demonstrate that Check-Eval achieves higher correlations with human judgments compared to existing metrics"
  - [section] "Unlike GPTScore, which relies on the probabilistic output of LLMs, Check-Eval generates a checklist of key points derived from the source text"
  - [corpus] Weak - no direct corpus evidence comparing human judgment patterns to checklist-based evaluation
- Break condition: If the checklist generation fails to capture the essential elements that humans consider important, or if humans evaluate based on different criteria than content presence

### Mechanism 2
- Claim: Reference-guided and candidate-guided variations provide both recall and precision evaluation perspectives
- Mechanism: Reference-guided checklist (from reference text) evaluates candidate recall of source content; candidate-guided checklist (from candidate text) evaluates reference precision coverage
- Core assumption: Text quality assessment requires both recall (coverage of source content) and precision (relevance of generated content) metrics
- Evidence anchors:
  - [section] "Reference-Guided and Candidate-Guided Checklist Generation... These two variations function as recall and precision evaluations, respectively"
  - [section] "As we consider the reference-guided variation as the recall evaluation and the candidate-guided variation as the precision evaluation, we also computed the F1 score"
  - [corpus] Weak - no corpus evidence on the importance of recall/precision balance in human text evaluation
- Break condition: If the source document contains information that is not relevant to the task, recall metrics may incorrectly penalize concise, focused summaries

### Mechanism 3
- Claim: Criterion-guided variation enables reference-free evaluation by using evaluation criteria instead of source documents
- Mechanism: LLM generates checklist based on evaluation criteria (consistency, relevance, coherence, fluency) without needing reference text, enabling evaluation of text generation tasks where references don't exist
- Core assumption: Text quality can be evaluated based on intrinsic criteria rather than comparison to reference texts, particularly for creative or open-ended generation tasks
- Evidence anchors:
  - [section] "The Criterion-Guided variation uses specific evaluation criteria to generate the checklist, which is then used to evaluate the candidate text against the reference text. It is useful when no reference text is available"
  - [section] "We also evaluated Check-Eval in a reference-free evaluation scenario using the SummEval dataset"
  - [corpus] Weak - no corpus evidence on criterion-guided evaluation effectiveness in human studies
- Break condition: If the evaluation criteria are not well-defined or don't capture the aspects humans value, the criterion-guided approach may produce unreliable results

## Foundational Learning

- Concept: Checklist-based evaluation methodology
  - Why needed here: Understanding how structured checklists capture human judgment patterns and provide interpretable evaluation results
  - Quick check question: How does a checklist-based approach differ from probabilistic scoring methods in terms of interpretability and bias?

- Concept: Recall vs precision evaluation in text generation
  - Why needed here: Reference-guided and candidate-guided variations require understanding of how recall (coverage) and precision (relevance) relate to text quality assessment
  - Quick check question: In what scenarios would you prefer reference-guided evaluation over candidate-guided evaluation, and vice versa?

- Concept: Reference-free vs reference-based evaluation metrics
  - Why needed here: Criterion-guided variation requires understanding when references are unavailable and how to evaluate text quality without them
  - Quick check question: What types of text generation tasks cannot use reference-based evaluation, and why?

## Architecture Onboarding

- Component map: Checklist Generation (LLM prompt processing) → Checklist Storage → Checklist Evaluation (LLM comparison) → Scoring → Results Aggregation
- Critical path: Source text → Checklist generation → Candidate text evaluation → Final score calculation
- Design tradeoffs: Structured checklist evaluation vs. flexibility of LLM-as-judge; computational cost of multiple LLM calls vs. accuracy; bias toward source content vs. summary quality
- Failure signatures: Low correlation with human judgments despite checklist-based approach; checklist generation produces irrelevant or incomplete items; evaluation scores don't vary meaningfully across text quality differences
- First 3 experiments:
  1. Test checklist generation quality by having humans rate generated checklists against source texts for completeness and relevance
  2. Compare reference-guided vs candidate-guided scores on the same text pairs to verify recall/precision behavior
  3. Evaluate criterion-guided variation on text summarization tasks with no reference texts to verify reference-free capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Check-Eval's performance scale with different types of text generation tasks beyond summarization and legal text similarity?
- Basis in paper: [inferred] The paper mentions that future work should explore Check-Eval's application to a broader range of NLG tasks, suggesting that its performance in other domains is currently unknown
- Why unresolved: The experiments were limited to summarization and legal text similarity datasets, leaving performance in other NLG tasks unverified
- What evidence would resolve it: Testing Check-Eval on diverse NLG tasks such as dialogue generation, story writing, or code generation would demonstrate its generalizability and limitations across different text types

### Open Question 2
- Question: What is the optimal size and granularity of checklists for maximizing evaluation accuracy without introducing excessive computational overhead?
- Basis in paper: [inferred] The paper mentions that checklist generation and evaluation require significant computational resources, and future work should optimize efficiency, but doesn't specify the relationship between checklist complexity and performance
- Why unresolved: The paper doesn't provide systematic analysis of how checklist size affects evaluation quality or computational requirements
- What evidence would resolve it: Controlled experiments varying checklist length and detail across multiple datasets would reveal the trade-off between evaluation accuracy and computational efficiency

### Open Question 3
- Question: How does Check-Eval handle cases where the source text contains implicit information that should be conveyed in the summary but isn't explicitly stated?
- Basis in paper: [explicit] The paper notes that Check-Eval evaluates presence/absence of key points from the source text, but doesn't address how it handles implicit information or inferences
- Why unresolved: The methodology focuses on explicit checklist items without discussing strategies for evaluating implicit content or common-sense reasoning
- What evidence would resolve it: Testing Check-Eval on summaries containing inferred information and comparing its performance to human judgment on such cases would reveal its limitations with implicit content

### Open Question 4
- Question: To what extent does Check-Eval's performance depend on the specific LLM used for checklist generation and evaluation?
- Basis in paper: [explicit] The paper acknowledges that Check-Eval's performance is tied to the capabilities of the underlying LLM, but doesn't provide comparative analysis across different LLM models
- Why unresolved: Experiments used GPT-4, but the paper doesn't explore how different LLM architectures or sizes might affect evaluation quality
- What evidence would resolve it: Systematic testing of Check-Eval with various LLM models (different sizes, architectures, training datasets) would quantify the dependency on LLM choice and identify optimal configurations

## Limitations

- Framework's performance depends heavily on the quality of checklist generation, which itself relies on LLM performance and prompt engineering
- Computational cost of multiple LLM calls per evaluation could limit scalability for large-scale applications
- Paper doesn't address potential biases in checklist generation or how the approach handles nuanced cases where source documents contain irrelevant information

## Confidence

- **High confidence**: The core mechanism of using structured checklists for evaluation works as described, given the significant improvements over baseline metrics (e.g., 0.623 vs 0.507 Spearman correlation for consistency on SummEval)
- **Medium confidence**: The framework's effectiveness across diverse text generation tasks beyond the two tested datasets, as the paper provides limited evidence of generalization
- **Low confidence**: The approach's robustness to different types of source document quality and relevance, as this critical aspect is not systematically tested

## Next Checks

1. **Cross-domain validation**: Test Check-Eval on diverse text generation tasks (creative writing, technical documentation, dialogue generation) to verify the framework's generalizability beyond legal text and news summarization

2. **Checklist quality assessment**: Conduct human evaluation of generated checklists to measure their completeness, relevance, and alignment with human judgment patterns, addressing the weak corpus evidence for Mechanism 1

3. **Bias and error analysis**: Systematically evaluate how Check-Eval handles cases where source documents contain irrelevant information or where candidate texts deliberately omit non-essential content, testing the framework's precision-recall balance assumptions