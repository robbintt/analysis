---
ver: rpa2
title: Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing
arxiv_id: '2402.02097'
source_url: https://arxiv.org/abs/2402.02097
tags:
- agent
- reward
- mace
- agents
- novelty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of enabling effective coordinated
  exploration in decentralized multi-agent reinforcement learning, particularly for
  sparse-reward tasks where only local observations are available and global state
  novelty cannot be directly measured. The proposed Multi-Agent Coordinated Exploration
  (MACE) method introduces two intrinsic rewards to guide exploration: (1) a novelty-based
  intrinsic reward that approximates global novelty by aggregating local novelties
  shared among agents through limited communication, and (2) a hindsight-based intrinsic
  reward that encourages agents to take actions that significantly influence other
  agents'' future novelty accumulation.'
---

# Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing

## Quick Facts
- **arXiv ID**: 2402.02097
- **Source URL**: https://arxiv.org/abs/2402.02097
- **Authors**: Haobin Jiang; Ziluo Ding; Zongqing Lu
- **Reference count**: 36
- **Primary result**: Novel MACE method outperforms baselines in sparse-reward decentralized multi-agent exploration tasks

## Executive Summary
This paper addresses the challenge of enabling effective coordinated exploration in decentralized multi-agent reinforcement learning where agents have only local observations and cannot directly measure global state novelty. The proposed Multi-Agent Coordinated Exploration (MACE) method introduces two intrinsic rewards: a novelty-based reward that approximates global novelty through limited communication among agents, and a hindsight-based reward that encourages actions influencing other agents' future novelty accumulation. Empirical results on GridWorld, Overcooked, and SMAC environments demonstrate that MACE outperforms baseline methods including local novelty, approximated global novelty, and standard mutual information-based rewards, successfully solving tasks requiring coordinated exploration among 2-3 agents.

## Method Summary
MACE tackles decentralized multi-agent exploration by combining two intrinsic reward mechanisms. The novelty-based intrinsic reward approximates global novelty by having agents share their local novelty measurements through limited communication channels, creating a collective estimate of exploration progress. The hindsight-based intrinsic reward encourages agents to take actions that significantly influence other agents' future novelty accumulation, calculated using weighted mutual information between an agent's action and the accumulated novelty of other agents. This dual-reward system addresses the challenge of sparse rewards in multi-agent settings by providing intrinsic motivation for coordinated exploration, where agents are rewarded not just for discovering novel states individually, but for contributing to the collective discovery process through their actions.

## Key Results
- MACE successfully solves GridWorld tasks requiring coordinated exploration among 2-3 agents, outperforming baselines including IPPO with local novelty and approximated global novelty
- In sparse-reward Overcooked environments, MACE achieves superior performance compared to standard mutual information-based rewards and other exploration methods
- Across all three tested environments (GridWorld, Overcooked, SMAC), MACE demonstrates consistent improvement over baselines, validating both the global novelty approximation approach and the weighted mutual information-based hindsight reward mechanism

## Why This Works (Mechanism)
The effectiveness of MACE stems from its dual approach to addressing the exploration-exploitation dilemma in decentralized multi-agent settings. By approximating global novelty through local communication, agents can coordinate their exploration efforts despite having only partial observability of the environment. The hindsight reward mechanism creates a feedback loop where agents learn to take actions that have downstream effects on collective exploration progress, effectively encouraging strategic behavior that considers the impact on other agents' learning trajectories. This combination allows the system to discover coordinated policies that would be difficult to learn through independent exploration or simple reward sharing mechanisms.

## Foundational Learning
- **Novelty-based intrinsic rewards**: Essential for encouraging exploration in sparse-reward environments; quick check: verify novelty measures are properly normalized across different state spaces
- **Mutual information in multi-agent RL**: Critical for understanding action influence between agents; quick check: ensure mutual information calculations handle the high-dimensional joint action spaces correctly
- **Limited communication protocols**: Necessary for practical decentralized coordination; quick check: test communication efficiency under varying bandwidth constraints
- **Hindsight credit assignment**: Important for long-term coordination; quick check: verify that hindsight rewards correctly propagate through time steps
- **Global vs. local state representations**: Fundamental challenge in multi-agent exploration; quick check: compare exploration efficiency with varying levels of state aggregation
- **Sparse reward problem in MARL**: Core motivation for intrinsic motivation methods; quick check: measure baseline performance without intrinsic rewards to establish the exploration gap

## Architecture Onboarding
**Component Map**: Agent sensors -> Local novelty calculator -> Communication module -> Global novelty approximator -> Hindsight reward calculator -> Joint policy network -> Environment

**Critical Path**: Local observation → Novelty computation → Communication → Global approximation → Hindsight reward → Policy update

**Design Tradeoffs**: The method trades off between communication overhead (limited messages for novelty sharing) and exploration efficiency (better global coordination). The weighted mutual information calculation balances computational complexity against the accuracy of hindsight reward estimation.

**Failure Signatures**: Poor performance may indicate: 1) communication bottlenecks preventing effective novelty sharing, 2) inaccurate novelty approximations leading to misguided exploration, 3) hindsight rewards not properly capturing action influence, or 4) policy network unable to effectively combine intrinsic and extrinsic rewards.

**First Experiments**: 1) Test novelty sharing with 2 agents in simple GridWorld to verify basic coordination, 2) Evaluate hindsight rewards in a controlled environment where action influence is easily measurable, 3) Measure communication efficiency by varying message sizes and observing impact on exploration success rate.

## Open Questions the Paper Calls Out
The paper acknowledges several limitations and areas for future work, including the need for systematic analysis of scalability as agent count increases, the impact of communication bandwidth constraints on performance, and the method's effectiveness in environments with more complex state spaces and continuous action domains. The authors also note that while the weighted mutual information approach shows promise, its assumptions about the relationship between novelty magnitude and action importance may not hold in all scenarios.

## Limitations
- Scalability concerns remain untested for scenarios with more than 3 agents, raising questions about performance in larger multi-agent systems
- The communication protocol's efficiency under bandwidth constraints is not systematically analyzed, leaving uncertainty about practical deployment
- The method's effectiveness in environments with continuous state spaces and non-grid-based navigation has not been evaluated
- The assumption that novelty magnitude directly correlates with action importance may not generalize to all multi-agent scenarios

## Confidence
- **High Confidence**: The core theoretical framework for novelty sharing and hindsight rewards is sound and well-justified
- **Medium Confidence**: Empirical results on the three tested environments are robust, but generalizability to other domains is uncertain
- **Medium Confidence**: The communication protocol efficiency claims are reasonable but lack comprehensive analysis

## Next Checks
1. **Scalability Test**: Systematically evaluate MACE performance across varying agent counts (2, 4, 8, 16) in GridWorld to identify communication bottlenecks and performance degradation patterns
2. **Communication Constraint Analysis**: Test MACE under different communication bandwidth limitations (1-bit, 2-bit, 4-bit messages) to quantify the trade-off between communication cost and exploration efficiency
3. **Environment Transferability**: Apply MACE to a diverse set of environments beyond the three tested, particularly focusing on continuous state spaces and non-grid-based navigation tasks to assess generalization capability