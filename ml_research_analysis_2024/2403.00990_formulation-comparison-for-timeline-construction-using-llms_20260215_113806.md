---
ver: rpa2
title: Formulation Comparison for Timeline Construction using LLMs
arxiv_id: '2403.00990'
source_url: https://arxiv.org/abs/2403.00990
tags:
- events
- temporal
- event
- timeline
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TimeSET, a new evaluation dataset for timeline
  construction with context-based temporal order annotation. The dataset features
  saliency-based event selection and partial ordering to reduce annotation cost while
  maintaining practical utility.
---

# Formulation Comparison for Timeline Construction using LLMs

## Quick Facts
- arXiv ID: 2403.00990
- Source URL: https://arxiv.org/abs/2403.00990
- Reference count: 40
- Authors: Kimihiro Hasegawa; Nikhil Kandukuri; Susan Holm; Yukari Yamakawa; Teruko Mitamura
- Key outcome: TimeSET dataset enables cross-model and cross-formulation comparisons for timeline construction using four formulations (NLI, Pairwise, MRC, Timeline)

## Executive Summary
This paper introduces TimeSET, a novel evaluation dataset for timeline construction with context-based temporal order annotation. The authors propose a comprehensive evaluation framework that casts timeline construction into four formulations and benchmark open LLMs (Llama 2 and Flan-T5) against fine-tuned models. The results demonstrate that NLI formulation with Flan-T5 performs strongly, while timeline construction remains challenging for few-shot LLMs. Fine-tuned models significantly outperform LLMs in few-shot settings, highlighting the limitations of current few-shot learning approaches for temporal reasoning tasks.

## Method Summary
The authors create TimeSET by retrieving Wikinews articles, annotating events, creating temporal links between events, and annotating arguments. They cast the timeline construction task into four formulations (NLI, Pairwise, MRC, and Timeline) with corresponding prompt templates. The evaluation uses in-context learning with Llama 2 and Flan-T5 models, testing 0-2 demonstrations per template. Performance is measured using document-wise pairwise F1 score for TimeSET and various metrics for existing temporal ordering datasets. The framework enables systematic comparison across models, formulations, and dataset characteristics.

## Key Results
- NLI formulation with Flan-T5 demonstrates the strongest performance among the four formulations tested
- Larger models (13B) consistently outperform smaller models (7B) across all formulations
- Fine-tuned models significantly outperform LLMs in few-shot settings, with fine-tuned models achieving 0.74 F1 versus 0.47 F1 for LLMs
- Performance degrades with more events and longer documents, especially for Timeline formulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NLI formulation with Flan-T5 performs strongly due to alignment with pretraining objectives
- Mechanism: Flan-T5 was instruction-tuned on diverse datasets including NLI datasets, making it particularly well-suited for NLI formulation which involves judging entailment between event relations
- Core assumption: Performance advantage comes from architectural alignment between formulation and model's training history
- Evidence anchors: Abstract mentions strong NLI performance; Section 4.4 discusses Flan-T5's instruction-tuning data containing NLI datasets

### Mechanism 2
- Claim: Larger models perform better due to greater capacity for representing complex event orderings
- Mechanism: As model size increases, models can better represent hierarchical and contextual dependencies needed for accurate timeline construction
- Core assumption: Performance scales predictably with parameter count for this task
- Evidence anchors: Section 4.4 shows larger models perform better regardless of formulations; this tendency is more evident for Flan-T5

### Mechanism 3
- Claim: Partial ordering annotation reduces cost while maintaining practical utility
- Mechanism: By selecting salient events and connecting each event to only one preceding and one following event, the dataset achieves practical annotation workload while preserving enough temporal information
- Core assumption: Salient events capture core temporal structure needed for timeline construction
- Evidence anchors: Section 3 describes saliency-based event selection and partial-order annotation inspired by script annotation

## Foundational Learning

- Concept: Event temporal ordering as core subtask
  - Why needed: Timeline construction fundamentally depends on correctly identifying chronological relationships between events
  - Quick check: If two events A and B occur, and context states "A happened, then B happened," what temporal relation holds between them?

- Concept: Task formulation alignment
  - Why needed: Different formulations decompose the same underlying task differently, affecting performance
  - Quick check: How many prompts needed to fully annotate a document with 5 events using pairwise versus timeline formulation?

- Concept: Few-shot learning limitations
  - Why needed: Paper benchmarks LLMs in few-shot settings and finds they underperform fine-tuned models
  - Quick check: If 7B model performs at 0.47 F1 and fine-tuned model at 0.74 F1, what does this suggest about few-shot learning capabilities?

## Architecture Onboarding

- Component map: Wikinews article retrieval → event annotation → temporal link annotation → argument annotation → prompt template generation → model inference → evaluation metric calculation
- Critical path: Document collection → event identification → temporal link annotation → partial order creation → prompt template application → model inference → evaluation
- Design tradeoffs:
  - Complete vs partial ordering: Complete provides full temporal information but is expensive; partial reduces cost but may miss relationships
  - Salient vs all events: All events provide comprehensive coverage but increase complexity; salient events simplify task but may miss details
  - Single vs multiple formulations: Single simplifies evaluation but may miss model strengths; multiple provide comprehensive assessment but increase complexity
- Failure signatures:
  - Low temporal awareness scores indicate annotation quality issues
  - Performance variance across prompt templates suggests sensitivity to prompt engineering
  - Document length correlation with performance indicates context window limitations
  - Formulation-specific performance drops suggest task decomposition issues
- First 3 experiments:
  1. Run baseline evaluation on TimeSET with all four formulations using Llama 2 7B with zero-shot setting
  2. Test different numbers of demonstrations (0, 1, 2) for each formulation to understand demonstration sensitivity
  3. Evaluate on a subset of documents split by creation date to test potential data contamination effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do performance characteristics of LLMs differ when handling documents with varying numbers of events, and at what point does performance degradation become significant?
- Basis: Paper discusses performance getting worse with more events, especially for Timeline formulation
- Why unresolved: Paper only provides general observation about degradation but doesn't specify at what point it becomes significant
- What evidence would resolve it: Detailed analysis showing relationship between number of events and performance metrics for each formulation with clear threshold where performance significantly drops

### Open Question 2
- Question: What specific characteristics of document content or structure influence LLM performance beyond length and number of events?
- Basis: Paper briefly mentions investigating document characteristics including DCT and document length but doesn't explore other factors
- Why unresolved: Paper only scratches surface of document characteristics, leaving many potential factors unexplored
- What evidence would resolve it: Comprehensive study examining various document features (topic complexity, event density, narrative structure, temporal complexity) and their correlation with LLM performance

### Open Question 3
- Question: How do different evaluation strategies (likelihood-based vs string-based) affect measured performance of LLMs on event temporal ordering tasks?
- Basis: Paper discusses using string match over likelihood-based evaluation in Section C.3
- Why unresolved: Paper justifies string-based choice but doesn't empirically compare effects of different evaluation strategies
- What evidence would resolve it: Comparative study applying both evaluation approaches to same LLM outputs across different formulations, quantifying differences in performance metrics

## Limitations
- Evaluation framework relies heavily on in-context learning, which is highly sensitive to prompt engineering choices
- TimeSET dataset may not fully capture real-world timeline construction complexity due to focus on salient events only
- Comparison between LLMs and fine-tuned models constrained by few-shot setting, which may not reflect true potential of either approach

## Confidence
- **High Confidence**: NLI formulation with Flan-T5 performs strongly; larger models generally perform better across formulations
- **Medium Confidence**: Partial ordering annotation maintains practical utility while reducing annotation cost; timeline construction remains challenging for few-shot LLMs
- **Low Confidence**: Salient event selection captures core temporal structure adequately; Flan-T5 superiority for NLI due to instruction-tuning data alignment

## Next Checks
1. Systematically vary prompt templates for each formulation to quantify sensitivity to prompt engineering
2. Evaluate same models and formulations on MATRES dataset to determine if TimeSET-specific findings generalize
3. Test intermediate model sizes (e.g., 3B, 10B) between 7B and 13B models to better understand scaling relationship