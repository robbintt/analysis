---
ver: rpa2
title: Evolve Cost-aware Acquisition Functions Using Large Language Models
arxiv_id: '2404.16906'
source_url: https://arxiv.org/abs/2404.16906
tags:
- budget
- cost-aware
- cost
- test
- acquisition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EvolCAF, a novel framework that integrates
  large language models (LLMs) with evolutionary computation (EC) to automatically
  design cost-aware acquisition functions (AFs) for Bayesian optimization. EvolCAF
  enables crossover and mutation in the algorithm space to iteratively search for
  elite AFs, significantly reducing reliance on domain expertise and model training.
---

# Evolve Cost-aware Acquisition Functions Using Large Language Models

## Quick Facts
- arXiv ID: 2404.16906
- Source URL: https://arxiv.org/abs/2404.16906
- Reference count: 32
- Outperforms well-known EIpu and EI-cool methods designed by human experts

## Executive Summary
This paper introduces EvolCAF, a novel framework that integrates large language models (LLMs) with evolutionary computation (EC) to automatically design cost-aware acquisition functions (AFs) for Bayesian optimization. EvolCAF enables crossover and mutation in the algorithm space to iteratively search for elite AFs, significantly reducing reliance on domain expertise and model training. The designed cost-aware AF maximizes utilization of available information from historical data, surrogate models, and budget details.

## Method Summary
EvolCAF integrates LLMs with evolutionary computation to automatically design cost-aware acquisition functions for Bayesian optimization. The framework uses evolutionary algorithms with crossover and mutation operators in the AF algorithm space, where LLMs generate new AF designs based on structured prompts. Each AF is evaluated through cost-aware Bayesian optimization loops on synthetic problems, with fitness measured as optimal gap between true optimum and found solution. The evolutionary process iteratively refines AF designs over multiple generations to discover high-performing, interpretable AFs that generalize well across different optimization tasks.

## Key Results
- EvolCAF outperforms well-known EIpu and EI-cool methods on 12 synthetic problems and 3 real-world hyperparameter tuning test sets
- The evolved AF demonstrates remarkable efficiency and generalization across various tasks
- Framework significantly reduces reliance on domain expertise and model training compared to traditional approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The evolutionary framework enables systematic exploration of acquisition function designs beyond human-designed heuristics by treating AF design as an algorithmic search problem.
- Mechanism: EvolCAF applies crossover and mutation operators in the space of AF algorithm definitions, iteratively refining candidate functions based on fitness evaluated across synthetic problems.
- Core assumption: The fitness landscape of AF designs is sufficiently smooth that evolutionary operators can progressively improve solutions over generations.
- Evidence anchors:
  - [abstract] "Leveraging the crossover and mutation in the algorithmic space, EvolCAF offers a novel design paradigm"
  - [section 3.1] "The general format of prompt engineering used to inform LLMs consists of four parts: (1) a general description of the task, (2) code instructions for implementing algorithms..."
- Break condition: If the fitness evaluation is noisy or the AF design space is too discontinuous, evolutionary search may converge to suboptimal solutions or fail to progress.

### Mechanism 2
- Claim: LLMs provide the generative capability to create novel AF algorithm descriptions and code implementations that are syntactically valid and semantically meaningful.
- Mechanism: Prompts guide LLMs to generate AFs using historical data, model predictions, and budget constraints as inputs, creating explicit mathematical formulations rather than black-box parameterizations.
- Core assumption: LLMs can reliably generate executable Python code from algorithmic descriptions when provided with structured prompts and input interpretations.
- Evidence anchors:
  - [section 3.3] "Following the general format, in initialization, we instruct LLMs to create a completely new AF to promote population diversity"
  - [section 4.2] "Fig. 4 shows the optimal AF with the minimum fitness value, including a general description of the algorithmic idea in defining the AF and a detailed code implementation"
- Break condition: If LLM generations frequently produce invalid code or fail to incorporate specified inputs meaningfully, the evolutionary process cannot progress.

### Mechanism 3
- Claim: The fitness function based on optimal gap across multiple synthetic problems drives evolution toward AFs that generalize well beyond the training instances.
- Mechanism: Each AF is evaluated on 2D Ackley and 2D Rastrigin functions with 10 random seeds (20 total instances), with fitness defined as the average optimal gap between true optimum and found solution.
- Core assumption: Synthetic problems used during evolution represent a sufficiently diverse subset of real-world optimization challenges to enable generalization.
- Evidence anchors:
  - [section 4.1] "To calculate the fitness value, we evaluate each evolved AF on 2D Ackley and 2D Rastigin functions with 10 different random seeds in the experimental design, resulting in a total of 20 instances"
  - [section 4.3] "Synthetic Problems In this subsection, we evaluate the optimal AF on 12 different synthetic instances with different landscapes and input dimensions"
- Break condition: If evolved AFs perform well on synthetic problems but fail to generalize to real-world HPO tasks, the synthetic evaluation metric may not capture relevant performance characteristics.

## Foundational Learning

- Concept: Gaussian Process regression as surrogate modeling
  - Why needed here: The AFs depend on GP posterior mean and variance predictions (µ(x), σ(x)) to quantify uncertainty and guide exploration
  - Quick check question: What are the formulas for GP posterior mean and variance given historical data and a test point?

- Concept: Acquisition function design principles
  - Why needed here: Understanding how traditional AFs like EI, EIpu, and EI-cool balance exploration vs exploitation and cost considerations is essential for interpreting the evolved AF's behavior
  - Quick check question: How does EIpu differ from standard EI in handling evaluation costs?

- Concept: Evolutionary algorithm mechanics
  - Why needed here: The framework uses crossover and mutation operators to explore AF design space, requiring understanding of how these operators preserve and combine algorithmic components
  - Quick check question: What is the difference between crossover and mutation in the context of evolving algorithm descriptions?

## Architecture Onboarding

- Component map:
  - LLM prompt generator -> LLM interface -> AF evaluator -> Population manager -> GP model trainer

- Critical path:
  1. Generate initial population via LLM initialization prompts
  2. Evaluate each AF's fitness by running BO on synthetic problems
  3. Select parents based on fitness, apply crossover/mutation via LLM
  4. Replace worst performers with new offspring
  5. Repeat until convergence or generation limit

- Design tradeoffs:
  - Fitness evaluation cost vs evolutionary progress: More synthetic problems improve generalization but increase computational burden
  - LLM model size vs generation quality: Larger models may produce better AFs but increase API costs
  - Population size vs diversity: Larger populations explore more broadly but require more evaluations

- Failure signatures:
  - Stagnant fitness values across generations indicate premature convergence or inadequate exploration
  - High variance in fitness evaluations suggests noisy objective or insufficient random seeds
  - Invalid code generations from LLM indicate prompt engineering issues

- First 3 experiments:
  1. Run EvolCAF with only initialization (no crossover/mutation) to verify LLM can generate valid AFs from scratch
  2. Test fitness evaluation pipeline on known synthetic functions to ensure correct BO implementation
  3. Run a small evolutionary run (3-5 generations) with simplified prompts to validate the full evolutionary loop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the EvolCAF framework be effectively adapted to other Bayesian optimization settings such as high-dimensional BO, batch BO, and multi-objective BO?
- Basis in paper: [explicit] The paper mentions that future work will explore adapting EvolCAF to other popular BO settings like high-dimensional BO, batch BO, and multi-objective BO.
- Why unresolved: The paper only demonstrates the effectiveness of EvolCAF on cost-aware BO problems, and does not explore its performance on other BO settings.
- What evidence would resolve it: Experimental results showing the performance of EvolCAF on high-dimensional BO, batch BO, and multi-objective BO problems compared to state-of-the-art methods for these settings.

### Open Question 2
- Question: How does the performance of the designed acquisition function change when using different types of cost functions in the evolutionary process?
- Basis in paper: [explicit] The paper suggests exploring the integration of different types of cost functions into the evolutionary process to enhance the robustness of the designed AF in future work.
- Why unresolved: The paper only uses a specific cost function design (most expensive at the global optimum) in the experiments, and does not investigate the impact of different cost function designs on the evolved AF's performance.
- What evidence would resolve it: Comparative experiments showing the performance of the evolved AF when using different cost function designs, and analysis of how the cost function design affects the AF's robustness and generalization.

### Open Question 3
- Question: How does the interpretability of the designed acquisition function compare to that of other interpretable AFs in the literature?
- Basis in paper: [explicit] The paper emphasizes that the designed AF allows for clear interpretations to provide insights into its behavior and decision-making process, and contrasts this with model-based methods that have poor interpretability due to being represented by network parameters.
- Why unresolved: While the paper claims the designed AF has good interpretability, it does not provide a quantitative or qualitative comparison with other interpretable AFs in the literature.
- What evidence would resolve it: A detailed comparison of the interpretability of the designed AF with other interpretable AFs, potentially using metrics like the number of human-understandable terms or the ability to explain specific decisions made by the AF.

## Limitations
- Evolutionary search relies heavily on synthetic problem generalization, but 12 synthetic instances may not capture full diversity of real-world optimization landscapes
- LLM-generated AFs show promising performance on benchmark tasks, but behavior on truly novel problems remains uncertain
- Computational cost of repeated fitness evaluations across populations and generations presents scalability concerns

## Confidence
Medium: The framework shows promise but has key limitations including synthetic problem generalization concerns, uncertain behavior on novel problems, and computational scalability issues.

## Next Checks
1. Test evolved AFs on held-out synthetic functions not seen during evolution to assess true generalization capability
2. Implement statistical significance tests comparing EvolCAF against baselines across multiple random seeds and problem instances
3. Profile computational overhead of the evolutionary loop to identify bottlenecks and optimize population/generation parameters for practical deployment