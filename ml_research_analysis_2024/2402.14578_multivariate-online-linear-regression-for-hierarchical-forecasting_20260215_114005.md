---
ver: rpa2
title: Multivariate Online Linear Regression for Hierarchical Forecasting
arxiv_id: '2402.14578'
source_url: https://arxiv.org/abs/2402.14578
tags:
- online
- linear
- regression
- multiv
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies multivariate online linear regression, where
  responses can be vectors rather than scalars, and proposes MultiVAW, an algorithm
  that extends the Vovk-Azoury-Warmuth (VAW) method to this setting. MultiVAW enjoys
  a logarithmic regret bound, generalizing the O(d log(T)) rate of VAW to multivariate
  responses.
---

# Multivariate Online Linear Regression for Hierarchical Forecasting

## Quick Facts
- arXiv ID: 2402.14578
- Source URL: https://arxiv.org/abs/2402.14578
- Reference count: 37
- Key outcome: MultiVAW extends VAW to multivariate responses with logarithmic regret bounds, applied to online hierarchical forecasting

## Executive Summary
This paper introduces MultiVAW, a novel algorithm that extends the Vovk-Azoury-Warmuth (VAW) method to multivariate online linear regression where responses are vectors rather than scalars. The authors prove that MultiVAW achieves logarithmic regret bounds that generalize the O(d log(T)) rate of VAW to the multivariate setting. The method is particularly applied to online hierarchical forecasting (OHF), where predictions must satisfy linear aggregation constraints, and the paper shows that MetaVAW is a special case of MultiVAW under specific conditions.

## Method Summary
The paper proposes MultiVAW as an extension of VAW to multivariate online linear regression, allowing vector-valued responses. The algorithm incorporates time-varying regularization matrices and uses a modified loss function to handle multivariate responses. For online hierarchical forecasting, the authors reduce the problem to multivariate online linear regression by constructing appropriate feature matrices using Kronecker products and vectorization. They provide theoretical regret bounds and show that MetaVAW is a special case of their framework under certain conditions.

## Key Results
- MultiVAW extends VAW to multivariate responses with regret bound O(∥θ∥²_ΛT + dȳ² Σ log(λ_i(AT)/λ_i(Λ₁)))
- The algorithm recovers MetaVAW as a special case under specific conditions
- Numerical experiments show MetaVAW performs best, followed closely by MultiVAW-OHF, with regret decreasing over time
- The method handles online hierarchical forecasting problems with coherent predictions under aggregation constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MultiVAW extends the Vovk-Azoury-Warmuth (VAW) algorithm to multivariate online linear regression, achieving logarithmic regret rates.
- Mechanism: MultiVAW generalizes the VAW update by incorporating regularization matrices Λt that can be time-varying and positive definite, and extending the loss function to handle vector-valued responses.
- Core assumption: The sequence of regularization matrices is non-decreasing (Λ1 ⪯ Λ2 ⪯ … ⪯ ΛT) and the features and responses are bounded.
- Evidence anchors:
  - [abstract] "MultiVAW, a method that extends the well-known Vovk-Azoury-Warmuth algorithm to the multivariate setting, and show that it also enjoys logarithmic regret in time."
  - [section 3.1] Definition of MultiVAW with time-varying regularization matrices.

### Mechanism 2
- Claim: The regret bound for MultiVAW is O(∥θ∥²_ΛT + dȳ² Σ log(λ_i(AT)/λ_i(Λ₁))), which generalizes the O(d log(T)) rate of VAW.
- Mechanism: The regret bound is derived using techniques from the analysis of VAW and Online Newton Step (ONS), incorporating the trace of the inverse of the covariance matrix and the eigenvalues of the accumulated features.
- Core assumption: The features and responses are bounded (there exist constants ¯x, ¯y > 0 such that ∥xt∥2 ≤ ¯x and ∥yt∥2 ≤ ¯y for all t).
- Evidence anchors:
  - [section 3.3] Theorem 3.1 stating the regret bound for MultiVAW.
  - [section 3.3] Corollary 3.2 deriving a simpler bound for time-invariant regularization.

### Mechanism 3
- Claim: MultiVAW-OHF can be applied to online hierarchical forecasting (OHF) by reducing the problem to multivariate online linear regression.
- Mechanism: The OHF problem is transformed by setting Xt = x⊤_t ⊗ St and θt = vec(Θt), where St is the summing matrix and xt is the feature vector. MultiVAW-OHF then learns the parameters Θt to make coherent predictions.
- Core assumption: The summing matrices St and feature vectors xt are revealed sequentially, and the responses yt are arbitrary vectors in Rnt.
- Evidence anchors:
  - [section 4.2] Description of the OHF problem and its reduction to multivariate online linear regression.
  - [section 4.4] Algorithm 1 for MultiVAW-OHF.

## Foundational Learning

- Concept: Online Convex Optimization (OCO)
  - Why needed here: Understanding the differences between OCO and multivariate online linear regression is crucial for appreciating the novelty of MultiVAW.
  - Quick check question: What are the key differences between the losses in OCO and those in multivariate online linear regression, and why do standard OCO algorithms like OGD and FTRL not achieve logarithmic regret bounds in the latter setting?

- Concept: Kronecker Product and Vectorization
  - Why needed here: The reduction of OHF to multivariate online linear regression involves the Kronecker product and vectorization of matrices, which are essential for the correct formulation of the problem.
  - Quick check question: Given a feature vector xt ∈ Rm and a summing matrix St ∈ Rnt×d, how do you construct the matrix Xt ∈ Rnt×dm used in the reduction, and what is the resulting vector θt ∈ Rdm?

- Concept: Regularization in Online Learning
  - Why needed here: MultiVAW allows for time-varying regularization matrices, which is a generalization of the standard time-invariant regularization used in VAW. Understanding the role of regularization is key to appreciating the flexibility and power of MultiVAW.
  - Quick check question: What are the benefits of using time-varying regularization matrices in MultiVAW compared to the standard time-invariant regularization, and how does this affect the regret bounds?

## Architecture Onboarding

- Component map:
  MultiVAW -> MultiVAW-OHF -> OHF Problem -> Reduction using Kronecker product and vectorization

- Critical path:
  1. Receive summing matrix St and feature vector xt at time step t.
  2. Construct Xt = x⊤_t ⊗ St and update regularization matrix Λt.
  3. Compute θt = A−1_t bt−1 using the recursive formulas for At and bt.
  4. Make prediction ˆyt = Xtθt.
  5. Receive response yt and update bt = bt−1 + X⊤_t yt.

- Design tradeoffs:
  - Time-varying vs. time-invariant regularization: Time-varying regularization allows for more flexibility but may be more computationally expensive.
  - Complexity of reduction: The reduction of OHF to multivariate online linear regression involves the Kronecker product and vectorization, which may increase the dimensionality of the problem.

- Failure signatures:
  - Non-positive definite regularization matrices: May lead to undefined or incorrect updates.
  - Unbounded features or responses: May violate the assumptions of the regret bounds.
  - Incorrect reduction of OHF: May lead to suboptimal or invalid predictions.

- First 3 experiments:
  1. Implement MultiVAW with time-invariant regularization on a simple multivariate online linear regression problem with known regret bounds.
  2. Apply MultiVAW-OHF to a small OHF problem with a known summing matrix and feature vectors, and verify that the predictions are coherent.
  3. Compare the performance of MultiVAW-OHF and MetaVAW on a larger OHF problem, varying the regularization parameter and observing the effect on regret and forecast quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the specific structure of typical summing matrices impact the regret bounds in online hierarchical forecasting?
- Basis in paper: Explicit - The authors mention that the regret bounds obtained for OHF are agnostic to the particular structure that usual summing matrices exhibit, and suggest it would be interesting to know how that structure can impact the rates.
- Why unresolved: The current regret bounds (e.g., Corollary 4.4) do not exploit the special properties of summing matrices used in hierarchical forecasting.
- What evidence would resolve it: Analyzing regret bounds that explicitly incorporate the structure of summing matrices, potentially leading to tighter or more interpretable bounds.

### Open Question 2
- Question: How do the regret bounds for MultiVAW-OHF compare to those of MetaVAW in practical scenarios involving multiple hierarchies?
- Basis in paper: Explicit - The authors provide a comparison of regret bounds for MultiVAW-OHF and MetaVAW, noting that in most use cases involving a single hierarchy, the bound for MultiVAW-OHF is smaller.
- Why unresolved: The comparison is based on theoretical bounds, which may not accurately reflect performance in practical scenarios with multiple hierarchies.
- What evidence would resolve it: Empirical studies comparing the regret bounds of MultiVAW-OHF and MetaVAW in scenarios with multiple hierarchies.

### Open Question 3
- Question: What is the impact of time-varying summing matrices on the performance of MultiVAW-OHF compared to MetaVAW?
- Basis in paper: Explicit - The authors state that MetaVAW is no longer applicable when the summing matrices are time-varying, making MultiVAW-OHF a viable algorithm.
- Why unresolved: The paper does not provide a direct comparison of the performance of MultiVAW-OHF and MetaVAW in scenarios with time-varying summing matrices.
- What evidence would resolve it: Experimental results comparing the performance of MultiVAW-OHF and MetaVAW in scenarios with time-varying summing matrices.

## Limitations

- The analysis relies on strong boundedness assumptions for features and responses, which may not hold in many practical forecasting applications.
- The reduction from OHF to multivariate online linear regression involves a dimensionality expansion through the Kronecker product, potentially increasing computational complexity significantly.
- The numerical experiments only compare against MetaVAW and do not benchmark against other hierarchical forecasting approaches or traditional time series methods.

## Confidence

- **High confidence**: The theoretical framework for MultiVAW as an extension of VAW to multivariate settings. The regret bound derivation follows established techniques in online learning theory, and the reduction of OHF to multivariate regression is mathematically sound.
- **Medium confidence**: The practical performance claims based on numerical experiments. The experiments show MetaVAW performs best among tested methods, but the comparison is limited in scope and doesn't explore various data-generating processes or hierarchical structures.
- **Medium confidence**: The claim that MultiVAW recovers MetaVAW as a special case. While the mathematical relationship appears correct, the practical implications and differences in implementation details could affect real-world performance.

## Next Checks

1. **Robustness to unbounded data**: Test MultiVAW on datasets with outliers or heavy-tailed distributions to assess sensitivity to the boundedness assumptions. Measure performance degradation and compare with robust alternatives.

2. **Scalability analysis**: Implement MultiVAW-OHF on hierarchies of increasing depth and width to empirically verify the O(dmn) complexity claim. Benchmark against naive implementations and measure memory and time requirements.

3. **Cross-method comparison**: Conduct a comprehensive benchmark comparing MultiVAW-OHF and MetaVAW against established hierarchical forecasting methods (bottom-up, top-down, optimal combination) on multiple real-world hierarchical datasets, evaluating both point forecasts and coherence violations.