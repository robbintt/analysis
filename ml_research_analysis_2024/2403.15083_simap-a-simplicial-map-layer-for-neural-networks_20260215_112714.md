---
ver: rpa2
title: 'SIMAP: A simplicial-map layer for neural networks'
arxiv_id: '2403.15083'
source_url: https://arxiv.org/abs/2403.15083
tags: []
core_contribution: This paper introduces SIMAP, a novel simplicial-map layer for neural
  networks designed to enhance interpretability in deep learning models. Unlike traditional
  dense layers, SIMAP layers are based on simplicial maps and barycentric subdivisions,
  avoiding the computational complexity of Delaunay triangulations.
---

# SIMAP: A simplicial-map layer for neural networks

## Quick Facts
- arXiv ID: 2403.15083
- Source URL: https://arxiv.org/abs/2403.15083
- Authors: Rocio Gonzalez-Diaz; Miguel A. Gutiérrez-Naranjo; Eduardo Paluzo-Hidalgo
- Reference count: 14
- Key outcome: Introduces SIMAP, a simplicial-map layer based on barycentric subdivisions that avoids Delaunay triangulation complexity while maintaining interpretability in neural networks

## Executive Summary
This paper presents SIMAP, a novel neural network layer that leverages simplicial maps and barycentric subdivisions to create interpretable deep learning models. Unlike traditional dense layers that rely on opaque weight matrices, SIMAP computes barycentric coordinates with respect to fixed maximal simplices, enabling transparent geometric decision-making. The approach avoids the computational complexity of Delaunay triangulations while maintaining high accuracy on both synthetic datasets and MNIST classification tasks.

## Method Summary
SIMAP layers replace traditional dense layers with simplicial map computations based on barycentric subdivisions of fixed maximal simplices. The method computes barycentric coordinates through efficient matrix multiplications rather than geometric constructions, enabling scalable implementation. Model capacity increases through iterative barycentric subdivisions, with weight matrices inherited and updated across subdivision levels using matrix transformations. This architecture allows for transparent decision-making where classification can be explained by geometric relationships within simplices.

## Key Results
- SIMAP layers achieve high accuracy on synthetic datasets and MNIST while maintaining interpretability
- One or two barycentric subdivisions typically provide sufficient model capacity for good performance
- SIMAP layers integrate seamlessly with convolutional networks as final classification layers
- The method avoids computational complexity of Delaunay triangulations while preserving geometric interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SIMAP layers avoid the computational complexity of Delaunay triangulation by using fixed maximal simplices with barycentric subdivisions
- Mechanism: Instead of computing a Delaunay triangulation over the entire dataset (which becomes intractable in high dimensions), SIMAP uses a fixed n-simplex that contains all data points. Barycentric subdivisions are computed using efficient matrix multiplications rather than geometric constructions
- Core assumption: The fixed simplex approach preserves sufficient geometric relationships between data points for accurate classification
- Evidence anchors:
  - [abstract]: "Unlike traditional dense layers, SIMAP layers are based on simplicial maps and barycentric subdivisions, avoiding the computational complexity of Delaunay triangulations"
  - [section]: "Unlike the previous definitions of simplicial-map neural networks [10], the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set"
  - [corpus]: No direct corpus evidence found
- Break condition: If the data distribution is highly non-convex or has complex boundaries that cannot be approximated by successive barycentric subdivisions of a single simplex

### Mechanism 2
- Claim: SIMAP layers maintain interpretability by using transparent barycentric coordinate computations instead of opaque weight matrices
- Mechanism: The layer computes barycentric coordinates of input data with respect to simplices, and classification decisions can be explained by which simplex contains the point and the corresponding weights. This is more interpretable than traditional dense layers where weights have no geometric meaning
- Core assumption: Human-understandable geometric relationships (which simplex contains a point) are sufficient for meaningful model interpretation
- Evidence anchors:
  - [abstract]: "The work contributes to the development of explainable AI by providing a transparent and understandable approach to neural network decision-making"
  - [section]: "we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex. We also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. In this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans."
  - [corpus]: No direct corpus evidence found
- Break condition: If the geometric relationships become too complex after multiple subdivisions to remain interpretable by humans

### Mechanism 3
- Claim: SIMAP layers achieve transfer learning by inheriting and updating weight matrices across barycentric subdivisions
- Mechanism: When performing a barycentric subdivision, the barycentric coordinates of points with respect to the subdivided simplex can be computed from the original coordinates using matrix multiplication. This allows the weight matrix from the previous subdivision to be transformed and used as initialization for the next, enabling efficient training
- Core assumption: The transformation of barycentric coordinates preserves the classification-relevant information needed for the next subdivision level
- Evidence anchors:
  - [section]: "The idea is to train another perceptron N^1 from it, emulating a barycentric subdivision. This new perceptron N^1 is initialized as N^1(x) = softmax(ξSd σ(x) · Ω1) where Ω1 is a (2n+1 − 1) × (k + 1) matrix whose rows are in one-to-one correspondence with the value of φ applied to the vertices of Sd σ and ξSd σ(x) is computed as explained in subsection 3.2. Specifically, by construction, we have ξSd σ(x) · Ω1 = ξSd σ(x) · Q · Ω = b(x) · Ω"
  - [section]: "The process can be iterated until a given error is reached"
  - [corpus]: No direct corpus evidence found
- Break condition: If the transformation introduces numerical instability or if the inherited weights become suboptimal for the subdivided geometry

## Foundational Learning

- Concept: Barycentric coordinates and their geometric interpretation
  - Why needed here: SIMAP layers fundamentally rely on computing and manipulating barycentric coordinates to classify points within simplices
  - Quick check question: Given a triangle with vertices A(0,0), B(1,0), C(0,1), what are the barycentric coordinates of point P(0.2, 0.3) with respect to this triangle?

- Concept: Simplicial complexes and their subdivisions
  - Why needed here: Understanding how barycentric subdivisions create finer simplicial complexes is crucial for grasping how SIMAP layers increase model capacity
  - Quick check question: If a 2-simplex is subdivided once via barycentric subdivision, how many 2-simplices are created in the resulting complex?

- Concept: VC dimension and its relationship to model capacity
  - Why needed here: The paper explicitly relates the number of barycentric subdivisions to VC dimension, which determines the model's ability to shatter datasets
  - Quick check question: What is the VC dimension of a perceptron with 3 inputs and no bias term?

## Architecture Onboarding

- Component map: Input layer → Barycentric coordinate computation → SIMAP layer (perceptron) → Output layer. Multiple SIMAP layers can be chained with barycentric subdivisions between them
- Critical path: Data preprocessing → Barycentric coordinate calculation → Initial perceptron training → Barycentric subdivision → Weight inheritance → Subsequent perceptron training
- Design tradeoffs: Fixed simplex vs. data-driven triangulation (simplicity and efficiency vs. potential geometric misalignment), number of subdivisions (model capacity vs. overfitting and interpretability loss)
- Failure signatures: Training loss plateaus despite subdivisions, accuracy degrades after certain subdivision depth, barycentric coordinates become numerically unstable
- First 3 experiments:
  1. Binary classification on a simple XOR-like dataset (2D) with 0, 1, and 2 subdivisions to observe decision boundary complexity
  2. Multi-class classification on MNIST using SIMAP as final layer after CNN feature extraction, comparing with and without subdivisions
  3. Synthetic datasets with varying dimensionality (2D, 3D, 4D) to test scalability and the impact of the curse of dimensionality

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity increases with dimensionality despite avoiding Delaunay triangulation, as multiple matrix multiplications are still required
- The fixed maximal simplex assumption may not hold for complex, non-convex data distributions with intricate decision boundaries
- Interpretability claims rely on geometric intuition that may break down after multiple subdivisions, potentially making decisions difficult to explain

## Confidence

**High Confidence**: The core mathematical framework of barycentric subdivisions and coordinate computation is well-established and correctly implemented. The avoidance of Delaunay triangulation complexity is clearly demonstrated.

**Medium Confidence**: The empirical results on synthetic datasets and MNIST are promising, but the sample size and variety of datasets are limited. The claim that one or two subdivisions are "often sufficient" needs broader validation across diverse real-world problems.

**Low Confidence**: The interpretability claims depend heavily on geometric intuition that may not scale to complex, high-dimensional problems. The paper lacks quantitative measures of interpretability and does not address how the model performs on datasets with highly non-convex decision boundaries.

## Next Checks
1. **Scalability Testing**: Evaluate SIMAP performance on high-dimensional datasets (e.g., CIFAR-10, 3D point clouds) to quantify the curse of dimensionality effects and validate the computational complexity claims beyond synthetic 2D and 3D examples.

2. **Interpretability Benchmarking**: Develop quantitative metrics for interpretability (e.g., explanation fidelity, human evaluation studies) to empirically validate that SIMAP layers are more interpretable than traditional dense layers, particularly after multiple barycentric subdivisions.

3. **Robustness Analysis**: Test SIMAP performance on datasets with complex, non-convex decision boundaries (e.g., concentric circles, spirals) and compare against traditional neural networks to identify failure modes where the fixed simplex assumption breaks down.