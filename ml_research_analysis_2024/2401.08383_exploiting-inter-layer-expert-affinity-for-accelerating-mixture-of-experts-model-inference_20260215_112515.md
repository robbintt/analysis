---
ver: rpa2
title: Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts
  Model Inference
arxiv_id: '2401.08383'
source_url: https://arxiv.org/abs/2401.08383
tags:
- expert
- experts
- affinity
- layer
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high communication overhead in Mixture-of-Experts
  (MoE) model inference due to extensive Alltoall operations. The authors introduce
  ExFlow, a novel optimization technique that exploits inter-layer expert affinity
  to reduce communication.
---

# Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference

## Quick Facts
- **arXiv ID**: 2401.08383
- **Source URL**: https://arxiv.org/abs/2401.08383
- **Reference count**: 40
- **Primary result**: Up to 67% reduction in cross-GPU routing latency and up to 2.2x improvement in inference throughput for GPT MoE models

## Executive Summary
This paper introduces ExFlow, a novel optimization technique for accelerating inference in Mixture-of-Experts (MoE) models by exploiting inter-layer expert affinity. The key insight is that pre-trained GPT MoE models exhibit strong conditional probability patterns in expert routing decisions across layers. By proposing context-coherent expert parallelism and designing an integer programming model to optimize expert placement based on these affinity patterns, ExFlow significantly reduces the communication overhead inherent in MoE architectures without requiring fine-tuning or accuracy degradation.

## Method Summary
ExFlow addresses the high communication overhead in MoE model inference by exploiting inter-layer expert affinity. The method involves profiling expert routing decisions on pre-trained models to capture conditional probability patterns, formulating an integer programming problem to optimize expert placement on GPUs, and implementing context-coherent expert parallelism to eliminate one of the two Alltoall communications required in traditional MoE layers. This approach achieves substantial reductions in cross-GPU routing latency and inference throughput improvements while maintaining model accuracy.

## Key Results
- Up to 67% reduction in cross-GPU routing latency
- Up to 2.2x improvement in inference throughput
- Eliminates need for fine-tuning or accuracy degradation
- Adaptable to various hardware configurations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pre-trained GPT MoE models exhibit strong inter-layer expert affinity
- **Mechanism**: Gating functions in pre-trained models create conditional probability patterns where tokens routed to specific experts at one layer have high likelihood of being routed to particular experts at subsequent layers
- **Core assumption**: Expert affinity is an inherent property of pre-trained models, not dependent on input data
- **Evidence anchors**: [abstract] proves pre-trained GPT MoE models exhibit strong inter-layer expert affinity; [section] defines expert affinity as conditional probability across layers
- **Break condition**: If models are not pre-trained or have been fine-tuned on different data distributions

### Mechanism 2
- **Claim**: Context-coherent expert parallelism eliminates the need for second Alltoall communication
- **Mechanism**: Initial Allgather ensures all GPUs have coherent context of all tokens, enabling in-place attention computation regardless of token location
- **Core assumption**: Data parallelism constraints can be relaxed when context coherence is maintained
- **Evidence anchors**: [abstract] context-coherent expert parallelism uses only one Alltoall; [section] tokens can perform in-place attention computation anywhere
- **Break condition**: If token generation patterns change significantly

### Mechanism 3
- **Claim**: Integer programming optimization of expert placement reduces cross-GPU routing latency
- **Mechanism**: ILP formulation maximizes combined conditional probability while minimizing token re-routing costs to derive optimal placement strategy
- **Core assumption**: Expert affinity patterns are stable enough to be captured through profiling
- **Evidence anchors**: [abstract] ILP model captures features and reduces up to 67% cross-GPU routing latency; [section] composite objective function ensures high propensity for subsequent routing
- **Break condition**: If expert affinity patterns change significantly during inference

## Foundational Learning

- **Concept**: Mixture-of-Experts (MoE) architecture
  - **Why needed here**: Understanding how MoE layers work and why they require Alltoall communications is fundamental to grasping the optimization opportunity
  - **Quick check question**: What are the two Alltoall communications required in each MoE layer and why are they necessary?

- **Concept**: Expert parallelism and data parallelism
  - **Why needed here**: The paper leverages context coherence to overcome data locality constraints inherent in expert parallelism
  - **Quick check question**: How does combining expert parallelism with data parallelism create the need for two Alltoall operations?

- **Concept**: Conditional probability and routing patterns
  - **Why needed here**: Expert affinity is defined as the conditional probability of expert selection across layers, which is the core insight exploited by the optimization
  - **Quick check question**: How is expert affinity mathematically defined in terms of conditional probability?

## Architecture Onboarding

- **Component map**: GPT MoE model with multiple layers -> Distributed system with G GPUs -> Communication fabric (NVLINK/InfiniBand) -> Context coherence mechanism (Allgather) -> Expert affinity profiler -> Integer programming solver

- **Critical path**: 
  1. Profile expert affinity on pre-trained model using sample tokens
  2. Solve ILP problem to determine optimal expert placement
  3. Load model onto GPUs following placement strategy
  4. During inference: use context coherence to eliminate second Alltoall
  5. Leverage expert affinity to minimize remaining Alltoall communication

- **Design tradeoffs**:
  - Context coherence vs. memory overhead (each GPU stores all contexts)
  - Profiling accuracy vs. profiling time (more tokens give better affinity estimates)
  - ILP solution optimality vs. computation time (larger models take longer to solve)
  - Expert affinity stability vs. deployment flexibility (affinity may vary across datasets)

- **Failure signatures**:
  - Unexpected performance degradation could indicate broken expert affinity patterns
  - Memory issues might suggest context coherence mechanism isn't working properly
  - Incorrect expert placement could lead to increased cross-GPU communication

- **First 3 experiments**:
  1. Profile expert affinity on a small pre-trained MoE model and visualize routing patterns
  2. Implement context-coherent expert parallelism and measure reduction in Alltoall operations
  3. Solve ILP for expert placement on a small model and benchmark communication reduction

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does expert affinity evolve during the training process, and can it be predicted or controlled to optimize model performance?
- **Basis in paper**: The paper investigates how expert affinity evolves and stabilizes during training
- **Why unresolved**: Only provides insights into early training stages, not throughout entire training or predictability
- **What evidence would resolve it**: Track expert affinity across all training stages and develop models to predict or control its evolution based on training parameters

### Open Question 2
- **Question**: What is the impact of varying the number of tokens used to capture expert affinity on the accuracy and efficiency of the ExFlow optimization technique?
- **Basis in paper**: Mentions more tokens provide better approximation but doesn't explore optimal number for different model sizes
- **Why unresolved**: Only briefly touches on relationship between tokens and accuracy without comprehensive analysis
- **What evidence would resolve it**: Experiments with varying numbers of tokens and evaluating trade-off between accuracy and efficiency in different model sizes and hardware configurations

### Open Question 3
- **Question**: How does the expert affinity property generalize across different types of MoE models and datasets, and what are the implications for its application in real-world scenarios?
- **Basis in paper**: Briefly mentions testing on out-of-distribution datasets but doesn't provide comprehensive analysis
- **Why unresolved**: Only provides limited exploration focusing on single MoE model and few datasets
- **What evidence would resolve it**: Experiments with various MoE models, architectures, and datasets to assess generalizability and implications for real-world deployment

## Limitations
- Expert affinity patterns may change significantly when models are fine-tuned on different datasets
- Memory overhead of context coherence mechanism scales with number of GPUs, potentially prohibitive for very large clusters
- Offline profiling and optimization requirements may not be practical for dynamic environments

## Confidence

**High Confidence**: Core insight about inter-layer expert affinity in pre-trained models is well-supported by theoretical framework and experimental results. Context-coherent expert parallelism mechanism is technically sound.

**Medium Confidence**: ILP optimization approach shows promising results but generalizability across different model architectures and hardware topologies requires further validation. Performance improvements are based on specific model sizes and hardware configurations.

**Low Confidence**: Long-term stability of expert affinity patterns and effectiveness on fine-tuned or domain-adapted models have not been thoroughly evaluated. Memory overhead implications for large-scale deployments are not fully characterized.

## Next Checks

1. **Affinity Stability Test**: Profile expert affinity on the same pre-trained model using multiple distinct datasets (Pile, C4, domain-specific corpora) and measure variance in affinity patterns to validate robustness to input distribution changes.

2. **Memory Overhead Characterization**: Implement context-coherence mechanism and measure actual memory overhead per GPU as function of number of GPUs in cluster, benchmarking trade-off between reduced communication and increased memory usage across different cluster sizes.

3. **Fine-tuning Impact Analysis**: Take pre-trained MoE model, fine-tune it on target domain, and re-profile expert affinity to quantify how much affinity patterns change, determining whether optimization needs recomputation after fine-tuning or remains effective.