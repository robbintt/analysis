---
ver: rpa2
title: 'Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled
  Factors'
arxiv_id: '2403.07366'
source_url: https://arxiv.org/abs/2403.07366
tags:
- deyo
- plpd
- entropy
- conference
- factors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of entropy-based confidence
  metrics in test-time adaptation (TTA), particularly under biased and wild distribution
  shifts. The authors propose a novel TTA method, Destroy Your Object (DeYO), which
  introduces a new confidence metric, Pseudo-Label Probability Difference (PLPD),
  to quantify the influence of object shape (a key CPR factor) on model predictions.
---

# Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors

## Quick Facts
- arXiv ID: 2403.07366
- Source URL: https://arxiv.org/abs/2403.07366
- Reference count: 40
- One-line primary result: Introduces DeYO, a test-time adaptation method that combines entropy and PLPD metrics to outperform baseline methods across various distribution shifts.

## Executive Summary
This paper addresses the fundamental limitation of entropy-based confidence metrics in test-time adaptation (TTA) when spurious correlations shift between training and test distributions. The authors propose DeYO (Destroy Your Object), a novel TTA method that introduces PLPD (Pseudo-Label Probability Difference) as a confidence metric that quantifies the influence of object shape on model predictions. By combining entropy with PLPD for both sample selection and weighting, DeYO achieves state-of-the-art performance on multiple benchmarks including ImageNet-C, ColoredMNIST, and Waterbirds, with particularly significant improvements on biased datasets where worst-group accuracy was previously near random guessing.

## Method Summary
DeYO is a test-time adaptation method that addresses the limitations of entropy-based confidence metrics under spurious correlation shifts. The method introduces PLPD, which measures how much prediction confidence drops when object shape information is destroyed through patch-shuffling transformation. DeYO uses both entropy and PLPD for sample selection (filtering low-entropy and high-PLPD samples) and sample weighting (giving higher weights to samples that are both confident and shape-driven). The method is compatible with any pre-trained DNN and employs online updates using weighted entropy loss.

## Key Results
- DeYO consistently outperforms baseline methods across mild and wild distribution shifts
- On ColoredMNIST, DeYO is the first TTA method to exceed random guessing for worst-group accuracy
- Significant improvements on biased benchmarks like Waterbirds where traditional methods fail
- Performance gains maintained across different architectures (ResNet, ViT) and corruption types

## Why This Works (Mechanism)

### Mechanism 1
Entropy alone cannot distinguish between predictions driven by CPR factors versus TRAP factors. Low entropy only indicates high confidence, not whether that confidence stems from stable (CPR) or spurious (TRAP) correlations. When distribution shifts occur, TRAP factors become unreliable, leading to harmful adaptation.

### Mechanism 2
PLPD effectively measures CPR factor influence by quantifying prediction confidence drop when object shape is destroyed. High PLPD indicates the prediction was shape-driven and thus more likely reliable under distribution shifts, as shape is assumed to be a CPR factor.

### Mechanism 3
Combining entropy and PLPD in both sample selection and weighting creates robust adaptation. Entropy filters out uncertain samples while PLPD ensures remaining samples are shape-driven rather than spurious-correlation-driven. The weighting function prioritizes samples that are both low-entropy and high-PLPD.

## Foundational Learning

- **Latent disentangled factors in data**: The framework assumes input data can be decomposed into CPR (correlated positively with labels in both train and test) and TRAP (correlated positively in train but negatively or uncorrelated in test) factors. Why needed: The entire theoretical framework relies on this decomposition. Quick check: Can you explain the difference between CPR factors (e.g., shape) and TRAP factors (e.g., background) and why this distinction matters for test-time adaptation?

- **Distribution shift types**: Different types of distribution shifts affect CPR and TRAP factors differently. Why needed: Understanding these differences is crucial for why entropy alone fails in certain scenarios. Quick check: What happens to a model's reliance on background features (TRAP) when the background distribution changes between training and test data?

- **Confidence metrics and their limitations**: Entropy is a common confidence metric, but its limitations must be understood. Why needed: Understanding what confidence metrics measure and their assumptions is key to grasping why PLPD is needed. Quick check: Why does low entropy not necessarily mean a sample is reliable for adaptation, especially under spurious correlation shifts?

## Architecture Onboarding

- **Component map**: Input pipeline -> Pre-trained model -> Object-destructive transformation module -> Entropy calculator -> PLPD calculator -> Sample selector -> Weight calculator -> Adaptation loop
- **Critical path**: Forward pass → Entropy & PLPD computation → Sample selection → Weight calculation → Backward pass with weighted loss
- **Design tradeoffs**: Transformation choice (patch-shuffling vs alternatives) balances computational cost and effectiveness; threshold selection balances adaptation coverage and reliability; weighting function design balances entropy-based and PLPD-based weighting
- **Failure signatures**: No samples selected (thresholds too restrictive) → model doesn't adapt; performance worse than baseline (harmful samples not filtered) → PLPD not effective or thresholds incorrect; slow adaptation (computational overhead) → transformation too complex or too many forward passes
- **First 3 experiments**: 1) Run DeYO with only entropy filtering (τPLPD = 0) on Waterbirds to confirm baseline performance; 2) Run DeYO with only PLPD filtering (τEnt = ∞) on same benchmark to isolate PLPD's contribution; 3) Run DeYO with both filters on ImageNet-C severity level 5 to verify improvement over baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
What is the impact of incorporating additional CPR factors beyond object shape into the PLPD metric? The paper focuses on object shape but acknowledges other CPR factors exist and suggests exploring intermediate feature maps for other CPR factors.

### Open Question 2
How does the choice of object-destructive transformation affect DeYO's performance? The paper compares three transformations and finds patch-shuffling best, but doesn't explore other potential transformations or analyze performance differences.

### Open Question 3
How does using PLPD during pre-training affect DeYO's performance? The paper mentions pre-training with PLPD-based filtering improves performance but doesn't provide detailed results or analysis.

## Limitations
- Effectiveness heavily depends on shape being a reliable CPR factor across all distribution shifts
- Computational overhead from multiple forward passes may be prohibitive for resource-constrained deployment
- Assumes clean disentanglement between CPR and TRAP factors, which may not hold for complex real-world data

## Confidence

**High Confidence**: Entropy alone is insufficient for TTA under spurious correlation shifts, supported by strong empirical evidence across multiple benchmarks.

**Medium Confidence**: PLPD effectiveness as a confidence metric is reasonably supported, though specific transformation choice could be questioned.

**Low Confidence**: The assumption that shape is universally a reliable CPR factor across all distribution shifts is the weakest claim.

## Next Checks
1. Test DeYO on non-image datasets (text classification, speech recognition) to verify cross-modal generalization
2. Replace patch-shuffling with alternative object-destructive transformations to determine if performance gains are transformation-specific
3. Measure computational overhead and implement optimizations to assess practical deployment viability