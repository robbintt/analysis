---
ver: rpa2
title: 'Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language
  Models'
arxiv_id: '2402.07865'
source_url: https://arxiv.org/abs/2402.07865
tags:
- training
- language
- visual
- vlms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic investigation into the design
  space of visually-conditioned language models (VLMs). The authors identify key design
  decisions around image preprocessing, architecture, and optimization that impact
  model performance.
---

# Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models

## Quick Facts
- arXiv ID: 2402.07865
- Source URL: https://arxiv.org/abs/2402.07865
- Reference count: 40
- Models achieve state-of-the-art performance across 12 benchmarks while saving 30% training compute

## Executive Summary
This paper presents a systematic investigation into the design space of visually-conditioned language models (VLMs). The authors identify key design decisions around image preprocessing, architecture, and optimization that impact model performance. To enable this analysis, they develop a standardized evaluation suite spanning visual question answering, object localization, and challenge tasks. They also create an optimized and flexible codebase for training VLMs with various components and optimization procedures. Through targeted experiments, they explore four key design axes and distill insights for training future VLMs.

## Method Summary
The authors systematically investigate VLMs through controlled experiments across four key design axes: visual backbone selection, image preprocessing strategies, language model choices, and optimization procedures. They develop a standardized evaluation suite spanning visual question answering (VQAv2, GQA, VizWiz, TextVQA), object localization (RefCOCO, RefCOCO+, RefCOCOg, OCID-Ref), and challenge tasks (VSR, POPE, TallyQA, AI2D). Using a single-stage training approach with frozen visual backbones, CLIP/SigLIP DINOv2-fused features, naive image resizing, and base LLaMA-2 LMs, they train PRISM models at 7-13B scale that outperform state-of-the-art open VLMs while achieving 30% compute savings through efficient training.

## Key Results
- PRISM models outperform InstructBLIP and LLaVA v1.5 across 12 diverse benchmarks
- Models achieve state-of-the-art performance on visual question answering, object localization, and challenge tasks
- Training efficiency improves by 30% through optimized two-epoch training compared to baseline three-epoch approaches

## Why This Works (Mechanism)
The performance gains stem from systematically exploring and optimizing four critical design axes: selecting appropriate visual backbones (CLIP vs SigLIP DINOv2), implementing effective image preprocessing (naive resizing vs complex letterboxing), choosing suitable language models (base vs instruction-tuned), and optimizing training procedures (single-stage with frozen visual encoders). The combination of these optimized components, along with feature fusion between different visual backbones, enables better visual understanding and reasoning capabilities while maintaining training efficiency.

## Foundational Learning
1. **Visually-Conditioned Language Models (VLMs)**: Models that combine visual and textual information for multimodal understanding. Needed to understand the core problem being addressed. Quick check: Can describe how VLMs differ from standard language models in processing visual inputs.

2. **Single-Stage Training**: Training approach where visual and language components are trained together in one phase. Needed to understand the efficiency claims. Quick check: Can explain why single-stage training is more efficient than two-stage approaches.

3. **Feature Fusion**: Combining features from multiple visual backbones (CLIP + SigLIP DINOv2). Needed to understand the architectural innovations. Quick check: Can describe different methods for combining visual features from multiple sources.

## Architecture Onboarding
**Component Map**: Image -> Visual Backbone (ViT-L/14) -> MLP Projector -> Language Model (LLaMA-2) -> Output

**Critical Path**: The key architectural decision is freezing the visual backbone and using a single linear projector layer to adapt visual features for the language model, enabling efficient training while maintaining visual understanding.

**Design Tradeoffs**: Visual backbone selection (CLIP vs SigLIP DINOv2) affects feature quality and computational cost; image preprocessing impacts input quality and computational efficiency; LM choice (base vs instruction-tuned) affects alignment with multimodal tasks; optimization procedure impacts training speed and final performance.

**Failure Signatures**: Poor performance may result from incorrect image preprocessing (wrong resize dimensions causing input length mismatches), unfrozen visual backbones causing training instability, or inadequate feature fusion leading to suboptimal visual representations.

**Three First Experiments**:
1. Compare CLIP vs SigLIP DINOv2 visual backbones with frozen parameters on a subset of benchmarks
2. Test naive image resizing vs letterboxing preprocessing on input quality and model performance
3. Evaluate base LLaMA-2 vs instruction-tuned LLaVA language models on multimodal understanding tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Missing implementation details for DINOv2 + CLIP/SigLIP feature fusion method (concatenation approach, projector dimension adjustment)
- Incomplete specification of trigger prompts for LLaVA v1.5 dataset components
- Lack of complete evaluation pipeline details including resolution settings and decoding strategies

## Confidence
- **High Confidence**: Systematic investigation methodology and identification of design space dimensions; performance improvements over baselines
- **Medium Confidence**: 30% training compute savings claim based on epoch count differences without accounting for batch size or learning rate variations
- **Low Confidence**: Exact reproduction requires unspecified implementation details for feature fusion and evaluation procedures

## Next Checks
1. Implement and compare different feature fusion strategies (simple concatenation vs. learned weighted combination) for the DINOv2 + CLIP/SigLIP features to determine if fusion method impacts performance

2. Cross-validate the LLaVA v1.5 dataset preparation against described components to ensure all image-caption pairs and multimodal instruct examples are correctly loaded with appropriate trigger prompts

3. Verify that evaluation is performed at 336px resolution (not the 224px used in pretraining) and that the same greedy decoding strategy is applied across all benchmarks to ensure fair comparison with reported results