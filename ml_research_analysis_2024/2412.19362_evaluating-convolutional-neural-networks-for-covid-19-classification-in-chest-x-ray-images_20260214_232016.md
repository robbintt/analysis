---
ver: rpa2
title: Evaluating Convolutional Neural Networks for COVID-19 classification in chest
  X-ray images
arxiv_id: '2412.19362'
source_url: https://arxiv.org/abs/2412.19362
tags:
- covid-19
- images
- x-ray
- chest
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates four convolutional neural networks\u2014\
  AlexNet, VGG-11, SqueezeNet, and DenseNet-121\u2014for automatic detection of COVID-19\
  \ from chest X-ray images. To address the challenge of limited labeled data, the\
  \ approach combines shallow fine-tuning with data augmentation strategies including\
  \ flips and rotations."
---

# Evaluating Convolutional Neural Networks for COVID-19 classification in chest X-ray images

## Quick Facts
- arXiv ID: 2412.19362
- Source URL: https://arxiv.org/abs/2412.19362
- Reference count: 40
- Primary result: SqueezeNet achieved 99.20% accuracy for COVID-19 detection in chest X-ray images using shallow fine-tuning and data augmentation

## Executive Summary
This study evaluates four convolutional neural networks—AlexNet, VGG-11, SqueezeNet, and DenseNet-121—for automatic detection of COVID-19 from chest X-ray images. To address the challenge of limited labeled data, the approach combines shallow fine-tuning with data augmentation strategies including flips and rotations. A 10-fold cross-validation was used to ensure reliable performance estimates. SqueezeNet achieved the highest accuracy of 99.20% with a precision of 99.40% and F1-score of 99.10%, outperforming other models and existing state-of-the-art methods. The results indicate that deep CNNs, when fine-tuned and augmented appropriately, can serve as a promising tool for assisting in the early diagnosis of COVID-19 from chest X-ray images.

## Method Summary
The study uses 108 COVID-19 positive chest X-ray images and 299 negative pneumonia images, applying 10-fold stratified cross-validation with shallow fine-tuning of pre-trained CNN models (AlexNet, VGG-11, SqueezeNet, DenseNet-121). Images are resized to 224x224 pixels, augmented with flips and ±10° rotations, and normalized before training only the final fully connected layer for 30 epochs using SGD with learning rate 0.001, momentum 0.9, and batch size 8. Performance is evaluated using accuracy, precision, recall, F1-score, and AUC metrics.

## Key Results
- SqueezeNet achieved the highest accuracy of 99.20% with precision of 99.40% and F1-score of 99.10%
- DenseNet-121 followed with accuracy of 99.10%, precision of 99.30%, and F1-score of 99.00%
- AlexNet and VGG-11 showed slightly lower but still high performance metrics
- All models demonstrated strong performance using shallow fine-tuning and data augmentation strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shallow fine-tuning (SFT) allows pre-trained CNNs to adapt to COVID-19 detection without overfitting on small datasets.
- Mechanism: By initializing weights from ImageNet pre-trained models and only updating the final fully connected layer, the network preserves low-level feature detectors (edges, textures) while adapting higher-level decision boundaries to X-ray patterns.
- Core assumption: Pre-trained CNNs contain generalizable low-level features relevant to medical image domains.
- Evidence anchors:
  - [abstract] "We validate our experiments using a ten-fold cross-validation procedure over the training and test sets. Our findings include the shallow fine-tuning and data augmentation strategies..."
  - [section] "The Shallow Fine-Tuning (SFT) [31] [32] is a training strategy based on the concept of transfer learning and is suitable for small data sets."
- Break condition: If medical images have fundamentally different low-level structures than ImageNet photos (e.g., grayscale vs. color textures), SFT performance degrades.

### Mechanism 2
- Claim: Data augmentation with flips and rotations compensates for class imbalance and limited COVID-19 image availability.
- Mechanism: Random transformations create synthetic training samples that expose the CNN to diverse geometric and spatial variations of lung pathologies without introducing labeling costs.
- Core assumption: The augmented transformations preserve diagnostically relevant features while expanding the effective dataset size.
- Evidence anchors:
  - [abstract] "Our findings include the shallow fine-tuning and data augmentation strategies that can assist in dealing with the low number of positive COVID-19 images publicly available."
  - [section] "To overcome this issue, we applied data augmentation strategies aim to increase the classification rate..."
- Break condition: If augmented views distort clinically significant pathology patterns (e.g., unusual rotation hides consolidation edges), model generalization suffers.

### Mechanism 3
- Claim: 10-fold cross-validation ensures robust performance estimates despite small dataset size.
- Mechanism: By rotating test sets and averaging metrics across folds, the evaluation captures model stability and reduces variance from random train-test splits.
- Core assumption: Each fold preserves class distribution and model performance is consistent across different subsets.
- Evidence anchors:
  - [abstract] "We validate our experiments using a ten-fold cross-validation procedure over the training and test sets."
  - [section] "The training and testing procedures are repeated k times, alternating the testing folds. Thus, we guarantee that each image will participate in the training process ( k − 1 times) and will also be part of the test group (1 time)."
- Break condition: If the dataset is too small, some folds may lack sufficient positive samples, inflating variance in performance estimates.

## Foundational Learning

- Concept: Transfer learning with pre-trained CNNs
  - Why needed here: Avoids training from scratch, which would require vastly more COVID-19 images than are publicly available.
  - Quick check question: Why is it safe to freeze lower CNN layers when adapting to a medical image task?

- Concept: Data augmentation strategies (flips, rotations)
  - Why needed here: Artificially increases dataset size to mitigate overfitting while preserving class balance.
  - Quick check question: Which augmentations risk removing diagnostically relevant pathology features?

- Concept: Stratified k-fold cross-validation
  - Why needed here: Provides unbiased performance estimates without holding out validation data that would reduce training samples.
  - Quick check question: How does stratification prevent bias when one class is underrepresented?

## Architecture Onboarding

- Component map: Image loading → resize 224x224 → augmentation (flips, ±10° rotation) → normalization → CNN model → loss calculation → backprop on FC layer → optimizer step
- Critical path: Data loading → augmentation → model forward pass → loss (cross-entropy) → backprop on FC layer only → optimizer step → repeat for 30 epochs → fold evaluation
- Design tradeoffs:
  - SFT vs full fine-tuning: SFT faster, less data-hungry, but may underfit if task-specific features differ greatly
  - Augmentation range: ±10° rotation balances realistic variation vs. pathology distortion
  - Fold count: 10-fold maximizes test coverage without splitting dataset too thin
- Failure signatures:
  - High variance across folds → overfitting or insufficient data
  - Low recall but high precision → model conservative, missing positives
  - Very low AUC → poor threshold calibration or weak feature discrimination
- First 3 experiments:
  1. Run baseline SqueezeNet SFT with no augmentation, record 10-fold CV metrics.
  2. Add ±10° rotation and flips, compare mean and std dev of F1 across folds.
  3. Replace SFT with full fine-tuning on final block, observe training stability and performance changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed models perform with a larger, more balanced dataset containing a greater number of positive COVID-19 cases?
- Basis in paper: [explicit] The authors note that the lack of positive COVID-19 images is a challenge and that "it is necessary to wait for more images of positive COVID-19 to be available in order to train more reliable models."
- Why unresolved: The current study is limited by the small number of positive COVID-19 images available in the public repository, which may affect the generalizability of the results.
- What evidence would resolve it: Testing the models on a larger dataset with more balanced class distribution and comparing the performance metrics to those reported in the current study.

### Open Question 2
- Question: Would hyperparameter optimization and early stopping strategies improve the performance of the CNN models?
- Basis in paper: [inferred] The authors mention that they did not perform hyperparameter optimization or early stopping due to the small dataset size, as these procedures require a validation set, which further reduces the number of images available for testing.
- Why unresolved: The study avoided these strategies to prevent further reduction of the dataset, but their potential benefits remain untested.
- What evidence would resolve it: Implementing hyperparameter optimization and early stopping on a larger dataset and comparing the performance metrics to those achieved without these strategies.

### Open Question 3
- Question: How effective are other data augmentation strategies beyond flips and rotations in improving the classification performance?
- Basis in paper: [explicit] The authors applied data augmentation using flips and rotations to address the low number of positive COVID-19 images, but they suggest evaluating more data augmentation strategies as a future work.
- Why unresolved: The study only explored flips and rotations, leaving the effectiveness of other augmentation techniques untested.
- What evidence would resolve it: Experimenting with various data augmentation techniques (e.g., scaling, brightness adjustments) and comparing their impact on the model's performance metrics.

## Limitations
- Extremely small dataset size (108 COVID-19 positive images) limits generalizability of results
- Narrow augmentation range (±10° rotation) may not capture full variability of clinical presentations
- No external validation on independent datasets to verify true performance

## Confidence
- High Confidence: The mechanism of shallow fine-tuning preserving pre-trained features while adapting classification layers is theoretically sound and well-supported.
- Medium Confidence: The specific augmentation strategy and 10-fold CV implementation likely contribute positively but haven't been validated against alternative approaches.
- Low Confidence: The absolute performance metrics (99.20% accuracy, 99.40% precision) are likely inflated due to the small dataset size and lack of external validation.

## Next Checks
1. External Validation: Test the trained models on an independent, larger COVID-19 chest X-ray dataset (e.g., COVID-19 Radiography Database) to assess true generalization performance.
2. Ablation Study: Compare SFT against full fine-tuning and evaluate whether the final layer contains sufficient capacity to learn COVID-19-specific features from this limited dataset.
3. Clinical Feature Analysis: Use Grad-CAM or similar visualization techniques to verify that the model focuses on clinically relevant regions (lung opacities, consolidation patterns) rather than background or irrelevant features.