---
ver: rpa2
title: 'Routers in Vision Mixture of Experts: An Empirical Study'
arxiv_id: '2401.15969'
source_url: https://arxiv.org/abs/2401.15969
tags:
- choice
- expert
- token
- softmax
- routers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a comprehensive empirical study of routers in
  vision Mixture of Experts (MoE) models, introducing a unified formulation that subsumes
  different MoEs through parametric routing tensors. It conducts head-to-head experiments
  comparing six different routers, including existing ones from prior work and new
  ones introduced in this study.
---

# Routers in Vision Mixture of Experts: An Empirical Study

## Quick Facts
- arXiv ID: 2401.15969
- Source URL: https://arxiv.org/abs/2401.15969
- Reference count: 9
- Primary result: Comprehensive empirical study of routers in vision MoE models showing that soft MoEs generally outperform sparse MoEs, Expert Choice routers outperform Token Choice routers, and language modeling routers adapt well to vision tasks

## Executive Summary
This paper presents a comprehensive empirical study of routers in vision Mixture of Experts (MoE) models, introducing a unified formulation that subsumes different MoEs through parametric routing tensors. The study conducts head-to-head experiments comparing six different routers, including existing ones from prior work and new ones introduced in this study. Through extensive experiments, the paper demonstrates that many routers originally developed for language modeling can be adapted to perform strongly in vision tasks, Expert Choice routers generally outperform Token Choice routers in sparse MoEs, and soft MoEs generally outperform sparse MoEs with a fixed compute budget.

## Method Summary
The paper introduces a unified formulation for vision MoE models through parametric routing tensors that can subsume different MoE variants. Six different routers are compared head-to-head, including both existing routers from prior work and new routers introduced in this study. The experiments systematically evaluate these routers across various vision tasks, with particular focus on classification benchmarks. The study also investigates the trade-offs between sparse and soft MoE architectures under fixed compute budgets, providing insights into router design choices for vision models.

## Key Results
- Many routers originally developed for language modeling can be adapted to perform strongly in vision tasks
- In sparse MoEs, Expert Choice routers generally outperform Token Choice routers
- Soft MoEs generally outperform sparse MoEs with a fixed compute budget

## Why This Works (Mechanism)
The paper's unified formulation through parametric routing tensors provides a flexible framework that captures the essential characteristics of different MoE variants while allowing for systematic comparison. This mathematical framework enables fair head-to-head comparisons by standardizing how routing decisions are made and evaluated across different router types. The parametric approach allows for both discrete (sparse) and continuous (soft) routing strategies to be expressed within the same formulation, making it possible to directly compare their performance under identical conditions.

## Foundational Learning
1. Mixture of Experts (MoE) architecture: Why needed - MoEs enable conditional computation by activating different subsets of parameters per input; Quick check - Verify understanding of expert selection mechanism and gating function
2. Sparse versus soft routing: Why needed - Different routing strategies have distinct computational and performance trade-offs; Quick check - Compare computational complexity and representational capacity of each approach
3. Vision-specific adaptations: Why needed - Vision tasks have unique characteristics requiring specialized router designs; Quick check - Identify key differences between vision and language MoE requirements
4. Unified formulation: Why needed - Provides theoretical framework for comparing diverse MoE variants; Quick check - Verify ability to map different routers to the unified formulation
5. Expert Choice vs Token Choice: Why needed - Different allocation strategies impact model capacity and specialization; Quick check - Understand the fundamental difference in how experts are assigned
6. Parametric routing tensors: Why needed - Enables flexible representation of routing decisions across different MoE variants; Quick check - Verify understanding of tensor dimensionality and semantics

## Architecture Onboarding

Component map: Input features -> Router -> Routing tensor -> Expert selection -> Output aggregation

Critical path: The router computes routing decisions based on input features, which are used to select appropriate experts and aggregate their outputs. The routing tensor serves as the central data structure mediating between input representations and expert selection.

Design tradeoffs: The main tradeoff is between sparse routing (computational efficiency, reduced interference) and soft routing (better gradient flow, higher representational capacity). Expert Choice routers allocate experts to tokens, while Token Choice routers assign tokens to experts, affecting specialization and capacity utilization.

Failure signatures: Poor routing decisions lead to underutilization of experts, capacity bottlenecks, or degraded performance due to improper expert selection. Soft routing may suffer from gradient vanishing or exploding, while sparse routing may face optimization challenges due to non-differentiable decisions.

First experiments: 1) Baseline comparison of router performance on standard vision classification tasks; 2) Ablation study of routing tensor dimensions and parameterizations; 3) Scaling analysis of expert count and model capacity

## Open Questions the Paper Calls Out
None

## Limitations
- Relies entirely on empirical comparisons without theoretical grounding for why certain router types perform better
- Unified formulation may oversimplify complex interactions between routing strategies and vision-specific architectural considerations
- Findings focused on classification tasks, limiting conclusions about router performance in other vision domains
- Claim that soft MoEs generally outperform sparse MoEs depends heavily on implementation details and evaluation metrics

## Confidence
- High confidence: The unified formulation subsuming different MoEs through parametric routing tensors is well-supported by the mathematical framework presented
- Medium confidence: The empirical finding that many language modeling routers adapt well to vision tasks, though this may be dataset and architecture dependent
- Medium confidence: The comparative performance between Expert Choice and Token Choice routers, though external validation would strengthen this claim
- Medium confidence: The soft MoE versus sparse MoE performance comparison, as this is sensitive to implementation details and evaluation methodology

## Next Checks
1. Replicate the Expert Choice versus Token Choice router comparison across diverse vision tasks beyond classification, including object detection and segmentation
2. Conduct ablation studies isolating the impact of routing tensor parameterization from other architectural differences
3. Perform scaling analysis to test whether the observed performance patterns hold when increasing model depth, width, and expert count beyond the tested configurations