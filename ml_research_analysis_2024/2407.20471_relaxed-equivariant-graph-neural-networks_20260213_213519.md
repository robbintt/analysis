---
ver: rpa2
title: Relaxed Equivariant Graph Neural Networks
arxiv_id: '2407.20471'
source_url: https://arxiv.org/abs/2407.20471
tags:
- relaxed
- symmetry
- weights
- equivariant
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for relaxed E(3) graph equivariant
  neural networks that can learn and represent symmetry breaking within continuous
  groups. The authors extend the e3nn framework by proposing the use of relaxed weights
  to allow for controlled symmetry breaking while maintaining equivariance.
---

# Relaxed Equivariant Graph Neural Networks

## Quick Facts
- arXiv ID: 2407.20471
- Source URL: https://arxiv.org/abs/2407.20471
- Reference count: 18
- Introduces framework for relaxed E(3) graph equivariant neural networks that can learn and represent symmetry breaking within continuous groups

## Executive Summary
This paper proposes a framework for relaxed E(3) graph equivariant neural networks that can learn and represent symmetry breaking within continuous groups. The authors extend the e3nn framework by introducing relaxed weights that allow for controlled symmetry breaking while maintaining equivariance. Through experiments on shape deformations and charged particle trajectories in electromagnetic fields, they demonstrate that the model can discover symmetry-breaking factors and learn physical relationships. The relaxed weights are interpretable as spherical harmonic signals, providing insights into the learned symmetry breaking.

## Method Summary
The authors propose relaxed E(3)NNs that use relaxed weights to parameterize tensor products between node features and spherical harmonic projections of relative vectors. These relaxed weights are initialized to preserve equivariance (only scalar irreps active) and then trained with regularization to learn controlled symmetry breaking. The framework extends standard E(3)NNs by allowing non-scalar irreps to activate during training, with relaxed weights shared across channels to limit parameter growth. The method is validated through experiments on shape deformations and a charged particle in an electromagnetic field, showing that the model can discover symmetry-breaking factors and learn physical relationships.

## Key Results
- Relaxed E(3)NN learns symmetry breaking factors through trained relaxed weights, demonstrated on shape deformations (cube to rectangular prism)
- The model discovers physical relationships by learning electric and magnetic field vectors from charged particle trajectories
- Relaxed weights are interpretable as spherical harmonic signals, revealing the form of symmetry breaking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The relaxed E(3)NN learns symmetry-breaking factors through trained relaxed weights.
- Mechanism: Relaxed weights are initialized to preserve equivariance (only scalar irreps active). During training, gradients allow non-scalar irreps to activate, breaking symmetry in a controlled way. The relaxed weights are constrained to be shared across channels, limiting parameter growth while enabling symmetry discovery.
- Core assumption: Symmetry breaking is represented by activating non-scalar irreps in the relaxed weight tensor.
- Evidence anchors:
  - [abstract] "propose the use of relaxed weights to allow for controlled symmetry breaking"
  - [section 4.1] "We initialize the relaxed weights ˜θ such that the initial model remains equivariant to ensure the model learns the minimal amount of symmetry breaking."
  - [corpus] Weak - corpus papers discuss symmetry breaking but do not anchor to relaxed weights in this framework.
- Break condition: If the relaxed weights are not initialized equivariantly or if training does not allow non-scalar irreps to be activated.

### Mechanism 2
- Claim: The relaxed weights are interpretable as spherical harmonic signals representing symmetry breaking.
- Mechanism: Non-scalar irreps correspond to spherical harmonics with specific parity and angular frequency. The scalar signal computed from relaxed weights reveals the form of symmetry breaking (e.g., x² = y² ≠ z²).
- Core assumption: Spherical harmonics form a basis for functions on the sphere, so irreps can be visualized as scalar or pseudoscalar signals.
- Evidence anchors:
  - [section 4.2] "As spherical harmonics form the basis for functions on a sphere, irreps representing symmetry breaking can be visualized as scalar or pseudoscalar signals"
  - [section 5.1] "We can calculate the learned scalar signals from the relaxed weights using Equation 8"
  - [corpus] Weak - corpus papers discuss spherical harmonics but do not anchor to this specific interpretation of relaxed weights.
- Break condition: If the relaxed weights do not correspond to spherical harmonic projections or if the visualization method is incorrect.

### Mechanism 3
- Claim: The relaxed E(3)NN can discover physical relationships through symmetry breaking.
- Mechanism: By relaxing equivariance, the model can learn vector and pseudovector forms of physical fields (e.g., electric and magnetic fields). This is demonstrated in the charged particle experiment where the model learns E⃗ and B⃗ fields from particle trajectories.
- Core assumption: Physical symmetry breaking corresponds to non-scalar irreps in the relaxed weight tensor.
- Evidence anchors:
  - [section 5.2] "The relaxed E(3)NN learns the correct ⃗E and ⃗B fields, with error of approximately 0.001-0.01 in the y and z components after normalization"
  - [section 5.2] "An equivariant neural network with equivalent architecture without relaxed weights cannot accomplish this task"
  - [corpus] Weak - corpus papers discuss physical symmetry breaking but do not anchor to this specific physical relationship discovery.
- Break condition: If the physical fields do not correspond to vector and pseudovector irreps or if the model cannot learn the correct magnitudes and directions.

## Foundational Learning

- Concept: Group representation theory and irreducible representations
  - Why needed here: E(3)NNs rely on group representations to maintain equivariance. Understanding irreps is crucial for interpreting relaxed weights and symmetry breaking.
  - Quick check question: What is the difference between a scalar and a pseudovector under inversion in O(3)?

- Concept: Spherical harmonics and their relationship to O(3) irreps
  - Why needed here: Spherical harmonics form the basis for equivariant convolutions. Relaxed weights are interpreted as spherical harmonic signals.
  - Quick check question: How many spherical harmonics are there for a given angular frequency l?

- Concept: Character tables and symmetry analysis
  - Why needed here: Character tables are used to analyze which irreps break symmetry between different groups. This is essential for interpreting the learned symmetry breaking in shape deformation experiments.
  - Quick check question: What is the character of a representation for elements in the same class?

## Architecture Onboarding

- Component map: Input -> Relaxed weights -> Convolution -> Output
- Critical path: Initialize relaxed weights equivariantly → Train with MSE loss → Analyze relaxed weights for symmetry breaking
- Design tradeoffs:
  - lrelaxed vs model expressiveness: Higher lrelaxed allows more complex symmetry breaking but increases parameters
  - Equivariance regularization: λ controls how much symmetry breaking is allowed
  - Input/output irreps: Choice affects model capacity and task suitability
- Failure signatures:
  - Relaxed weights remain zero: Model stuck in equivariant configuration, cannot break symmetry
  - All relaxed weights active: Model breaks too much symmetry, may not generalize
  - Incorrect symmetry breaking: Relaxed weights do not correspond to correct irreps for the task
- First 3 experiments:
  1. Cube to rectangular prism: Verify relaxed weights learn x² = y² ≠ z² relationship
  2. Charged particle in electromagnetic field: Verify model learns correct E⃗ and B⃗ fields
  3. Different lrelaxed values: Study effect on symmetry breaking and model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different choices of lmax and lrelaxed impact the learned symmetry breaking factors and model performance?
- Basis in paper: [inferred] The authors note that "both the choice of the network lmax and lrelaxed is a hyperparameter" and choose lrelaxed > lmax to illustrate different learned symmetry breaking factors.
- Why unresolved: The paper does not systematically explore how varying these hyperparameters affects the learned symmetry breaking or model performance across different tasks.
- What evidence would resolve it: A systematic study comparing model performance and learned symmetry breaking across a range of lmax and lrelaxed values on multiple tasks.

### Open Question 2
- Question: What are the optimization dynamics of relaxed E(3)NNs compared to standard E(3)NNs?
- Basis in paper: [inferred] The authors mention they "aim to further characterize the mathematical properties of relaxed E(3)NNs" and "study their optimization dynamics" in the future.
- Why unresolved: The paper does not provide any analysis of how the introduction of relaxed weights affects training dynamics, convergence, or stability.
- What evidence would resolve it: Comparative analysis of training curves, loss landscapes, and convergence rates between relaxed and standard E(3)NNs on various tasks.

### Open Question 3
- Question: How does the performance of relaxed E(3)NNs scale with the number of symmetry breaking factors in a system?
- Basis in paper: [explicit] The authors demonstrate the method on systems with one (rectangular prism) and many (random shape) symmetry breaking factors, noting that "when transforming the cube to a random shape, all relaxed weights are non-zero."
- Why unresolved: The paper does not explore how model performance changes as the number of symmetry breaking factors increases, or if there are limits to the method's effectiveness.
- What evidence would resolve it: Systematic experiments varying the number and complexity of symmetry breaking factors in a system and measuring model performance and interpretability.

## Limitations

- Experimental validation is primarily limited to synthetic datasets (shape deformations, single physical system)
- Claim that relaxed weights learn "correct" amount of symmetry breaking relies on synthetic ground truth
- Physical relationship discovery capability needs broader validation beyond 2D electromagnetic field configuration

## Confidence

- Mechanism of symmetry breaking through relaxed weights: Medium
- Interpretability of relaxed weights as spherical harmonic signals: High
- Physical relationship discovery capability: Low

## Next Checks

1. Test on real-world molecular datasets where symmetry breaking is known (e.g., Jahn-Teller distortions) to validate generalizability beyond synthetic data
2. Compare convergence speed and final performance against non-equivariant baselines on standard benchmarks to quantify the tradeoff of relaxed equivariance
3. Perform ablation studies varying the regularization coefficient λ across multiple orders of magnitude to characterize its effect on learned symmetry breaking