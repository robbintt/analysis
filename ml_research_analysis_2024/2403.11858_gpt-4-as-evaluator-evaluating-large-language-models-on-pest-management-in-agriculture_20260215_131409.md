---
ver: rpa2
title: 'GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in
  Agriculture'
arxiv_id: '2403.11858'
source_url: https://arxiv.org/abs/2403.11858
tags:
- pest
- language
- prompting
- management
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the performance of large language models (LLMs)
  for generating pest management advice in agriculture. GPT-3.5 and GPT-4 were tested
  using zero-shot, few-shot, instruction-based, and self-consistency prompting techniques.
---

# GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture

## Quick Facts
- arXiv ID: 2403.11858
- Source URL: https://arxiv.org/abs/2403.11858
- Reference count: 0
- Large language models evaluated for generating pest management advice in agriculture

## Executive Summary
This study evaluates the performance of large language models (LLMs) for generating pest management advice in agriculture. GPT-3.5 and GPT-4 were tested using zero-shot, few-shot, instruction-based, and self-consistency prompting techniques. An expert system provided ground truth labels for pest scenarios. GPT-4 served as an evaluator to assess response quality across six dimensions: Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and Exhaustiveness. Results showed GPT-3.5 and GPT-4 outperformed FLAN models on all linguistic dimensions. Instruction-based prompting achieved the highest accuracy (72%) in correctly classifying pest management scenarios. Self-consistency prompting performed worst due to its tendency to classify negative samples as positive. The findings demonstrate the potential of LLMs as agricultural decision support tools, particularly when enhanced with domain-specific knowledge.

## Method Summary
The study evaluates LLMs (GPT-3.5, GPT-4, FLAN-T5) on pest management advice generation using 50 labeled pest scenarios from an expert system. Four prompting techniques (zero-shot, few-shot, instruction-based, self-consistency) are applied to generate responses, which are then evaluated by GPT-4 across six linguistic dimensions plus factual accuracy. Final scores combine linguistic quality (60% weight) and accuracy (40% weight), with additional metrics including Precision, Recall, F1, and confusion matrix counts.

## Key Results
- GPT-3.5 and GPT-4 outperformed FLAN models on all linguistic dimensions, with GPT-4 achieving perfect fluency scores
- Instruction-based prompting with domain-specific knowledge achieved 72% accuracy, the highest among all methods
- Self-consistency prompting performed worst, showing high false positive rates by classifying negative samples as positive
- GPT-3.5 demonstrated better sensitivity to negative pest management scenarios compared to GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 evaluator provides a scalable and consistent method for assessing linguistic quality of LLM-generated pest management advice.
- Mechanism: GPT-4 is prompted to evaluate responses across six dimensions (Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, Exhaustiveness) using a structured CoT framework, enabling automated quality assessment without human expert review.
- Core assumption: GPT-4's evaluation is sufficiently aligned with human expert judgment and can reliably assess linguistic quality across multiple dimensions.
- Evidence anchors:
  - [abstract] "We proposed an innovative approach, using GPT-4 as an evaluator, to score the generated content on Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and Exhaustiveness."
  - [section 3.4] "These responses are then evaluated by GPT-4... regarding the accuracy and the linguistic quality of the generated pest management suggestions."
- Break condition: If GPT-4's evaluation systematically diverges from human expert assessments, or if the CoT framework fails to capture important quality dimensions.

### Mechanism 2
- Claim: Instruction-based prompting with domain-specific knowledge significantly improves LLM accuracy in pest management classification.
- Mechanism: Prompts that include specific crop thresholds and affected crops guide LLMs to make more informed decisions, achieving 72% accuracy compared to lower performance with zero-shot and few-shot methods.
- Core assumption: Including domain-specific information in prompts provides sufficient context for LLMs to accurately classify pest scenarios.
- Evidence anchors:
  - [abstract] "Furthermore, the use of instruction-based prompting containing domain-specific knowledge proved the feasibility of LLMs as an effective tool in agriculture, with an accuracy rate of 72%."
  - [section 4] "The instruction-based method with GPT-3.5 demonstrates the best performance in accuracy, precision, recall, and F1 scores."
- Break condition: If LLMs fail to properly utilize the domain-specific information in prompts, or if the provided thresholds are insufficient for accurate classification.

### Mechanism 3
- Claim: GPT-3.5 and GPT-4 outperform FLAN models in both linguistic quality and task accuracy for agricultural pest management.
- Mechanism: GPT models leverage their larger parameter counts and broader pretraining to better understand and generate context-appropriate pest management advice.
- Core assumption: Model architecture and training data significantly impact performance on domain-specific tasks.
- Evidence anchors:
  - [abstract] "The results showed that GPT-3.4 and GPT-4 outperform the FLAN models in most evaluation categories."
  - [section 4] "GPT-3.5 and GPT-4 models scored significantly higher than the FLAN model on all dimensions, especially GPT-4, which achieved a perfect score of 10 on fluency."
- Break condition: If FLAN models can be fine-tuned or prompted to achieve comparable performance, or if task-specific evaluation reveals limitations in GPT models.

## Foundational Learning

- Concept: Prompt engineering techniques (zero-shot, few-shot, instruction-based, self-consistency)
  - Why needed here: Different prompting methods significantly impact LLM performance on pest management tasks, requiring understanding of when to apply each technique.
  - Quick check question: Which prompting method achieved the highest accuracy in this study, and what key element distinguished it from others?

- Concept: Evaluation metrics for NLP tasks (accuracy, precision, recall, F1-score, linguistic quality dimensions)
  - Why needed here: Proper evaluation of LLM performance requires understanding both task-specific metrics and linguistic quality assessment.
  - Quick check question: What were the six dimensions used to evaluate linguistic quality, and why is this multi-dimensional approach important?

- Concept: Expert systems and domain knowledge integration
  - Why needed here: The study uses an expert system as ground truth for factual accuracy, highlighting the importance of domain knowledge in LLM evaluation.
  - Quick check question: What role did the expert system play in this study, and how was it used to generate ground truth labels?

## Architecture Onboarding

- Component map:
  - AHDB data files -> Expert system -> Labeled pest scenarios
  - Labeled scenarios -> LLMs (GPT-3.5, GPT-4, FLAN-T5) -> Generated responses
  - Generated responses -> GPT-4 evaluator -> Linguistic quality and accuracy scores
  - Scores -> Aggregation -> Final performance metrics

- Critical path:
  1. Generate pest scenario samples from expert system data
  2. Apply prompting techniques to LLMs to generate responses
  3. Use GPT-4 to evaluate response accuracy and linguistic quality
  4. Aggregate scores and calculate final performance metrics

- Design tradeoffs:
  - Using GPT-4 as evaluator provides scalability but may introduce evaluator bias
  - Instruction-based prompting requires more detailed prompt construction but yields better results
  - Self-consistency prompting improves performance but increases computational cost

- Failure signatures:
  - Low accuracy scores indicate poor task understanding
  - Inconsistent linguistic quality scores suggest evaluator bias
  - High false positive rates indicate model sensitivity issues

- First 3 experiments:
  1. Compare zero-shot prompting across all three LLM models using identical pest scenarios
  2. Test instruction-based prompting with varying levels of domain-specific detail
  3. Evaluate self-consistency prompting effectiveness by comparing aggregated responses to individual method outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do GPT-3.5 and GPT-4 compare in sensitivity to negative pest management scenarios, and what specific model differences account for this?
- Basis in paper: [explicit] The paper notes that GPT-3.5 adheres strictly to thresholds and correctly identifies more negative samples, while GPT-4 occasionally makes incorrect judgments even when thresholds aren't met.
- Why unresolved: While the paper identifies this difference, it doesn't deeply investigate the underlying architectural or training differences between GPT-3.5 and GPT-4 that cause this sensitivity variation.
- What evidence would resolve it: Comparative analysis of model architecture, training data composition, and fine-tuning procedures between GPT-3.5 and GPT-4 specifically for domain-specific reasoning tasks.

### Open Question 2
- Question: What is the optimal balance between instruction-based prompting and other prompting techniques for pest management applications?
- Basis in paper: [explicit] The paper shows instruction-based prompting achieves highest accuracy (72%) but doesn't explore hybrid approaches or optimal parameter tuning.
- Why unresolved: The study compares prompting methods in isolation but doesn't investigate whether combining techniques or optimizing prompt parameters could yield better results.
- What evidence would resolve it: Systematic experiments testing hybrid prompting approaches, parameter sensitivity analysis, and optimization of instruction-based prompts for different agricultural scenarios.

### Open Question 3
- Question: How does the performance of GPT-4 as an evaluator compare to human expert evaluation in agricultural pest management contexts?
- Basis in paper: [inferred] The study relies entirely on GPT-4 for evaluation across six dimensions but doesn't validate these assessments against human expert judgments.
- Why unresolved: Without ground truth from human experts, it's unclear whether GPT-4's evaluation accurately reflects real-world agricultural expertise.
- What evidence would resolve it: Direct comparison studies where GPT-4 evaluations are validated against assessments from experienced agricultural specialists across the same datasets.

## Limitations
- Reliance on GPT-4 as evaluator without human expert validation creates potential circularity and uncertainty about linguistic quality scores
- Limited sample size of 50 pest scenarios may not represent real-world agricultural diversity
- Lack of detailed information about expert system construction and threshold definitions

## Confidence
- High Confidence: Comparative performance between GPT-3.5, GPT-4, and FLAN models; instruction-based prompting effectiveness
- Medium Confidence: Linguistic quality assessment framework; accuracy rates and F1 scores for relative comparisons
- Low Confidence: Absolute linguistic quality scores; final weighted scores combining linguistic and accuracy metrics

## Next Checks
1. Conduct human expert validation of a subset of responses to verify GPT-4's linguistic quality assessments
2. Test prompting techniques and model performance using a larger dataset (200-300 scenarios) representing diverse agricultural contexts
3. Implement evaluation using a different LLM as evaluator to assess consistency and potential model-specific biases