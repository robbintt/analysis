---
ver: rpa2
title: The Role of Depth, Width, and Tree Size in Expressiveness of Deep Forest
arxiv_id: '2407.05108'
source_url: https://arxiv.org/abs/2407.05108
tags:
- deep
- tree
- trees
- decision
- forests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work analyzes the expressiveness of deep forests, a non-differentiable
  deep learning model based on decision trees, focusing on three hyperparameters:
  depth, width, and tree size. The authors prove that depth exponentially enhances
  the expressiveness of deep forests compared to width and tree size when approximating
  generalized parity functions.'
---

# The Role of Depth, Width, and Tree Size in Expressiveness of Deep Forest

## Quick Facts
- arXiv ID: 2407.05108
- Source URL: https://arxiv.org/abs/2407.05108
- Reference count: 40
- Primary result: Depth exponentially enhances expressiveness of deep forests compared to width and tree size when approximating parity functions

## Executive Summary
This work analyzes the expressiveness of deep forests, a non-differentiable deep learning model based on decision trees, focusing on three hyperparameters: depth, width, and tree size. The authors prove that depth exponentially enhances the expressiveness of deep forests compared to width and tree size when approximating generalized parity functions. Specifically, they show that depth can achieve the same approximation accuracy using exponentially fewer leaves than width or tree size. Theoretical findings are verified through numerical simulations and real-world experiments, demonstrating that deeper forests consistently outperform shallower ones with equivalent total numbers of trees.

## Method Summary
The authors employ theoretical analysis to prove upper and lower bounds on the approximation complexity of deep forests concerning depth, width, and tree size. They construct a product distribution to efficiently learn parity functions and verify their theoretical findings through numerical simulations and real-world experiments. The methodology compares deep forests with random forests and individual decision trees across various configurations.

## Key Results
- Depth exponentially enhances expressiveness compared to width and tree size for parity functions
- Deep forests achieve same accuracy with exponentially fewer leaves than shallow alternatives
- Theoretical results validated through numerical simulations and real-world experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Depth exponentially enhances expressiveness compared to width and tree size for parity functions
- Mechanism: Each layer performs feature transformation, enabling efficient expression of functions requiring exponential complexity in shallow models
- Core assumption: Parity functions require exponential complexity for shallow models
- Evidence anchors:
  - Abstract confirms depth exponentially enhances expressiveness
  - Section proves advantage of depth over tree size via separation results
  - Corpus papers focus on pruning/optimization but not depth-width expressiveness tradeoffs
- Break condition: Target function not as complex as parity function or low input dimension

### Mechanism 2
- Claim: Deep trees can express decision trees leaf-by-leaf with linear overhead
- Mechanism: Cascade structure replicates decision boundaries leaf-by-leaf and combines them
- Core assumption: Decision boundaries can be decomposed into simpler boundaries for successive layers
- Evidence anchors:
  - Section describes leaf-by-leaf representation proof
  - Corpus papers don't address leaf-by-leaf decomposition
- Break condition: Complex decision trees with interacting splits make linear overhead prohibitive

### Mechanism 3
- Claim: Product distribution enables efficient parity learning via symmetric decision paths
- Mechanism: Symmetric structure created by midpoint splits and same-feature splits
- Core assumption: Symmetry in decision paths essential for efficient parity learning
- Evidence anchors:
  - Section describes distribution construction requirements
  - Corpus papers don't discuss distribution impact on expressiveness
- Break condition: Non-symmetric data distributions diminish depth advantage

## Foundational Learning

- Concept: Parity functions and their complexity
  - Why needed here: Benchmark for demonstrating exponential depth advantage
  - Quick check question: Why are parity functions hard to approximate with polynomial leaves?

- Concept: Approximation complexity and expressiveness
  - Why needed here: Framework for comparing model expressiveness
  - Quick check question: How does approximation complexity relate to parameters needed for accuracy?

- Concept: Label-connected sets
  - Why needed here: Characterizes parity function complexity
  - Quick check question: How do label-connected sets relate to piecewise constant nature of trees?

## Architecture Onboarding

- Component map: Input layer → Forest layers (D layers) → Output aggregation
- Critical path: Input features → Forest layer 1 → ... → Forest layer D → Output aggregation
- Design tradeoffs:
  - Depth vs. Width: Deeper forests express complex functions but harder to train; wider forests easier to train but need more trees
  - Tree size: Larger trees capture complex patterns but may overfit; smaller trees more robust but may underfit
- Failure signatures:
  - Underfitting: High bias, low variance, model too simple
  - Overfitting: Low bias, high variance, model too complex
- First 3 experiments:
  1. Verify theoretical results on synthetic dataset with known parity functions
  2. Compare deep forests with different depths on real-world dataset
  3. Analyze impact of tree size on deep forest performance

## Open Questions the Paper Calls Out

- Question: How does depth advantage scale for function classes beyond parity functions?
  - Basis: Paper mentions investigating learnability advantage for other function classes
  - Why unresolved: Theoretical analysis focuses on parity functions as benchmark
  - What evidence would resolve it: Empirical studies on diverse datasets with varying concept complexities

- Question: How does depth advantage change with noisy or adversarial data?
  - Basis: Paper mentions analyzing robustness to noisy data
  - Why unresolved: Theoretical bounds assume clean data and perfect approximation
  - What evidence would resolve it: Performance measurements under increasing noise levels and adversarial attacks

- Question: What is optimal tradeoff between depth, width, and tree size?
  - Basis: Paper shows depth advantages but also demonstrates tree size impacts practical performance
  - Why unresolved: Complex interaction between hyperparameters needs optimization
  - What evidence would resolve it: Systematic hyperparameter optimization across diverse datasets

## Limitations
- Focus on parity functions may not capture real-world dataset complexity
- Assumes discrete input spaces, limiting continuous data applicability
- Theoretical bounds may be conservative in practice

## Confidence
- High confidence: Mathematical rigor of theoretical proofs for depth advantage
- Medium confidence: Experimental results showing deeper forests outperform shallower ones
- Medium confidence: Claim that depth achieves sufficient complexity with fewer parameters

## Next Checks
1. Test depth advantage on other hard functions beyond parity (DNF formulas, threshold functions)
2. Extend theoretical analysis to continuous input spaces
3. Evaluate deep forest performance under different data distributions beyond the product distribution used in analysis