---
ver: rpa2
title: Sampling from Boltzmann densities with physics informed low-rank formats
arxiv_id: '2412.07637'
source_url: https://arxiv.org/abs/2412.07637
tags:
- flow
- steps
- stochastic
- which
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for sampling from Boltzmann densities
  using a physics-informed low-rank tensor train (TT) format to solve the continuity
  equation along an annealing path. The approach alternates between deterministic
  TT-based time steps and stochastic steps (Langevin and resampling) to improve expressiveness
  and address the "teleportation issue" near the target distribution.
---

# Sampling from Boltzmann densities with physics informed low-rank formats

## Quick Facts
- arXiv ID: 2412.07637
- Source URL: https://arxiv.org/abs/2412.07637
- Reference count: 38
- Primary result: Physics-informed low-rank TT format for sampling from Boltzmann densities achieves superior performance on multi-modal distributions compared to pure stochastic methods

## Executive Summary
This paper introduces a novel sampling method for Boltzmann densities that combines deterministic physics-informed low-rank tensor train (TT) representations with stochastic components. The approach solves the continuity equation along an annealing path by alternating between TT-based deterministic time steps and stochastic Langevin/resampling steps. This hybrid strategy addresses the "teleportation issue" that occurs when deterministic flows fail to properly mix modes near the target distribution, while maintaining computational efficiency through low-rank tensor compression.

## Method Summary
The method reformulates sampling as solving the continuity equation for a velocity field that transports an initial distribution to the target Boltzmann density. The velocity field is represented in a low-rank TT format using H² orthonormal Fourier basis functions. An adaptive rank adjustment scheme using exponential moving average enables the representation to grow as needed during training. The algorithm alternates between deterministic ODE solves using the TT velocity field and stochastic steps (Langevin dynamics and resampling) to ensure proper mode coverage and correct local distribution mismatches. Optimization is performed using an alternating linear scheme with empirical risk minimization.

## Key Results
- The TT flow significantly outperforms pure stochastic sampling on 2D Gaussian mixture problems, with energy distance of 0.076 vs 0.14 for the 40-mode case
- The method successfully handles high-dimensional "many well problems" with up to 16 dimensions
- Best performance is achieved when combining deterministic and stochastic steps, with deterministic steps being more efficient for large-scale transport
- The adaptive rank scheme effectively manages representation complexity without requiring predetermined rank bounds

## Why This Works (Mechanism)

### Mechanism 1
Alternating deterministic TT flow steps with stochastic Langevin and resampling steps improves sampling expressiveness and mitigates the "teleportation issue" near the target distribution. The deterministic TT flow provides efficient large-scale transport along the annealing path, while stochastic steps correct local distribution mismatches and ensure proper mode weighting. This assumes the continuity equation solution exists and can be approximated by low-rank TT representations for the annealing path.

### Mechanism 2
Using H² orthonormal Fourier basis functions instead of polynomials improves PDE solving stability and accuracy. H² orthogonalization provides better-conditioned basis functions that avoid the ill-conditioning issues associated with high-degree polynomials in tensor train decompositions. This assumes the H² norm provides an appropriate measure of function regularity for the continuity equation solution space.

### Mechanism 3
Exponential moving average (EMA) of successive TT approximations enables adaptive rank adjustment without requiring predetermined rank bounds. By blending old and new TT cores using EMA weights, the method avoids drastic rank changes between iterations while still allowing the representation to grow as needed. This assumes the TT representation of the velocity field changes smoothly between iterations as more training samples become available.

## Foundational Learning

- Concept: Tensor Train (TT) format and low-rank tensor decompositions
  - Why needed here: The high-dimensional nature of the velocity field representation requires dimensionality reduction to make the problem computationally tractable
  - Quick check question: How does the TT format represent a high-dimensional tensor using a chain of 3D core tensors?

- Concept: Continuity equation and optimal transport
  - Why needed here: The sampling problem is reformulated as finding a velocity field that satisfies the continuity equation along an annealing path
  - Quick check question: What is the relationship between the continuity equation and the transport map ϕ that pushes forward the initial distribution?

- Concept: Annealing paths and sequential Monte Carlo
  - Why needed here: The method uses a linear interpolation in energy space and alternates between deterministic and stochastic steps inspired by SMC
  - Quick check question: How does the annealing parameter t ∈ [0,1] interpolate between the initial and target distributions?

## Architecture Onboarding

- Component map: Sample from p₀ → Apply ODE solve with learned velocity field → Apply stochastic steps → Repeat until t=1
- Critical path: Sample from p₀ → Apply ODE solve with learned velocity field → Apply stochastic steps → Repeat until t=1
- Design tradeoffs: Deterministic TT flow vs stochastic steps (accuracy vs expressiveness), rank adaptivity vs computational cost, Fourier basis vs other basis choices
- Failure signatures: Poor mode coverage (teleportation issue), slow convergence (rank too low), numerical instability (basis ill-conditioning), mode collapse (insufficient stochastic steps)
- First 3 experiments:
  1. Test on 2D Gaussian mixture with 2 modes - verify basic functionality and compare with stochastic-only sampling
  2. Test on 2D Gaussian mixture with 40 modes - evaluate mode mixing and teleportation issue mitigation
  3. Test on many well problem in moderate dimensions (d=4) - assess scalability and rank adaptivity performance

## Open Questions the Paper Calls Out

### Open Question 1
How do different choices of basis functions (beyond H² orthonormal Fourier basis) affect the performance and stability of the FTT architecture for solving the continuity equation? The paper mentions that common basis functions like polynomials perform poorly, attributed to lack of Lipschitz continuity, but does not explore other alternatives systematically. Systematic comparison of FTT performance with different basis functions (splines, wavelets, Chebyshev polynomials, etc.) on benchmark sampling problems would show which bases provide optimal stability and accuracy.

### Open Question 2
Can non-sample-based techniques be developed to choose the optimal TT architecture during training, rather than relying on adaptive rank adjustment through EMA? The paper mentions this as future work: "non-sample-based techniques could be developed to choose the best architecture during training, e.g., based on the residual of the PDE." Development and validation of criterion-based methods (PDE residual, condition number monitoring, etc.) that can automatically determine optimal TT ranks without ground truth samples would resolve this question.

### Open Question 3
How does the proposed method scale to extremely high-dimensional problems (d > 100) compared to other sampling methods like normalizing flows or diffusion models? The paper only tests up to moderate dimensions (the "many well problem" with d=4,8,16) and does not compare computational complexity or sampling quality with competing methods in very high dimensions. Benchmarking the method against state-of-the-art normalizing flows and diffusion models on problems with d > 100, measuring both sampling quality and computational resources required, would provide answers.

## Limitations

- The H² orthonormal Fourier basis claim lacks comprehensive comparison with alternative basis functions in the literature
- The adaptive rank scheme has no theoretical guarantees for convergence or stability in very high dimensions
- The "teleportation issue" explanation, while intuitive, lacks rigorous mathematical characterization

## Confidence

- **High confidence**: The alternating deterministic-stochastic framework is technically sound and the basic TT representation approach for velocity fields is well-established in the tensor literature
- **Medium confidence**: The empirical results showing improved performance over pure stochastic sampling are convincing, though limited to specific test cases
- **Low confidence**: Claims about H² basis superiority, rank adaptation benefits, and teleportation issue mitigation lack strong empirical or theoretical support

## Next Checks

1. **Ablation study on basis functions**: Systematically compare H² Fourier basis with standard polynomial bases and other orthogonal bases (Legendre, Chebyshev) across different distribution types to quantify the claimed stability improvements

2. **Rank growth analysis**: Track TT rank evolution during training on progressively higher-dimensional problems to determine if the adaptive scheme maintains computational tractability and identify rank saturation points

3. **Mode mixing characterization**: Design experiments that specifically stress-test the teleportation issue claim by creating distributions with sharply separated modes and varying energy gaps to measure the method's ability to maintain proper mode weights