---
ver: rpa2
title: Calibrating Large Language Models Using Their Generations Only
arxiv_id: '2403.05973'
source_url: https://arxiv.org/abs/2403.05973
tags:
- confidence
- calibration
- uncertainty
- arxiv
- auxiliary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces APRICOT, a method to calibrate large language
  models (LLMs) using only their generated text. The core idea is to train an auxiliary
  model to predict the LLM's confidence in its answers based on the input question
  and output text alone.
---

# Calibrating Large Language Models Using Their Generations Only

## Quick Facts
- arXiv ID: 2403.05973
- Source URL: https://arxiv.org/abs/2403.05973
- Reference count: 30
- Primary result: Introduces APRICOT, a method to calibrate LLM confidence using only model generations without ground truth labels

## Executive Summary
This paper introduces APRICOT, a novel approach for calibrating large language models' confidence estimates using only the model's generated text. The method trains an auxiliary model to predict the LLM's confidence by clustering questions with similar embeddings and computing accuracy rates within each cluster. Experiments demonstrate that APRICOT outperforms baseline methods in detecting incorrect LLM answers while achieving competitive calibration error metrics, and it works for both white-box and black-box LLMs requiring only black-box access to the target model.

## Method Summary
APRICOT calibrates LLMs by training an auxiliary model to predict confidence scores based on input questions and output text. The method works by first clustering questions with similar embeddings, then computing accuracy rates within each cluster to establish calibration targets. The auxiliary model is trained to predict these cluster-level accuracy scores, effectively learning to estimate the LLM's confidence without requiring ground truth labels or access to internal model parameters. This approach enables calibration for both white-box and black-box scenarios while only requiring access to the LLM's outputs.

## Key Results
- APRICOT outperforms baseline methods in detecting incorrect LLM answers (measured by AUROC)
- Achieves competitive Expected Calibration Error (ECE) and Maximum Calibration Error (MCE) metrics
- Works effectively for both white-box and black-box LLMs requiring only black-box access

## Why This Works (Mechanism)
APRICOT leverages the observation that questions with similar semantic content tend to have similar accuracy rates when answered by an LLM. By clustering questions and computing cluster-level accuracy statistics, the method creates reliable calibration targets without requiring ground truth labels. The auxiliary model learns to map input-output pairs to these accuracy-based confidence scores, effectively capturing the LLM's reliability patterns across different question types.

## Foundational Learning
- **Question clustering**: Grouping semantically similar questions is essential for creating reliable accuracy estimates within clusters. Quick check: Verify cluster coherence using intrinsic evaluation metrics.
- **Embedding models**: Quality embeddings are crucial for meaningful question similarity. Quick check: Evaluate embedding quality using nearest neighbor retrieval tasks.
- **Calibration metrics**: Understanding ECE, MCE, and AUROC is necessary for proper evaluation. Quick check: Compare multiple calibration metrics to ensure robust assessment.
- **Confidence estimation**: The relationship between model outputs and prediction reliability needs to be learned. Quick check: Validate confidence predictions against held-out accuracy data.

## Architecture Onboarding

Component map: Question/Answer pairs -> Embedding Model -> Clustering -> Cluster Accuracy Computation -> Auxiliary Confidence Model

Critical path: The most important components are the clustering mechanism and the auxiliary model training. The embedding model quality directly impacts clustering effectiveness, which in turn affects the reliability of cluster accuracy targets used for training the auxiliary model.

Design tradeoffs: The method trades computational overhead of clustering and training an auxiliary model against the benefit of label-free calibration. More granular clustering provides better accuracy estimation but requires more data and computation.

Failure signatures: Poor calibration performance typically indicates issues with embedding quality, inappropriate clustering granularity, or insufficient training data for the auxiliary model. Very low AUROC scores suggest the auxiliary model fails to capture meaningful confidence patterns.

First experiments:
1. Test clustering quality by manually inspecting cluster coherence
2. Validate auxiliary model predictions on held-out data
3. Compare calibration performance across different embedding models

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness depends heavily on clustering quality and the assumption that questions within clusters have similar accuracy rates
- Experiments primarily focus on open-ended question answering, leaving unclear how well the method generalizes to other LLM applications
- The paper does not thoroughly investigate sensitivity to different clustering approaches or embedding models

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| APRICOT can calibrate LLM confidence without ground truth labels | High |
| APRICOT works equally well for white-box and black-box LLMs | Medium |
| APRICOT generalizes across different LLM architectures and tasks | Low |

## Next Checks
1. Test APRICOT's performance across multiple diverse tasks (e.g., code generation, commonsense reasoning, fact verification) to evaluate generalization beyond open-ended question answering.

2. Conduct ablation studies on different clustering approaches and embedding models to quantify their impact on calibration performance.

3. Evaluate the method's robustness when the auxiliary model is trained on limited data or when question distributions shift between training and test sets.