---
ver: rpa2
title: Random Scaling and Momentum for Non-smooth Non-convex Optimization
arxiv_id: '2405.09742'
source_url: https://arxiv.org/abs/2405.09742
tags:
- optimization
- stationary
- algorithm
- point
- non-convex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles non-smooth non-convex optimization, a key challenge
  in training neural networks with architectures like ReLU and max pooling. Classical
  analysis of stochastic gradient descent with momentum (SGDM) fails here because
  it assumes either convexity or smoothness.
---

# Random Scaling and Momentum for Non-smooth Non-convex Optimization

## Quick Facts
- arXiv ID: 2405.09742
- Source URL: https://arxiv.org/abs/2405.09742
- Reference count: 40
- Introduces exponential random scaling for non-smooth non-convex optimization

## Executive Summary
This paper addresses the fundamental challenge of applying stochastic gradient descent with momentum (SGDM) to non-smooth non-convex optimization problems common in neural network training. The authors introduce a simple yet powerful modification: scaling each update by an exponentially distributed random scalar. This modification enables SGDM to find (c, ε)-stationary points at the optimal rate of O(c^{1/2}ε^{-7/2}) without requiring Taylor approximations. The approach maintains the familiar structure of SGDM while providing strong theoretical guarantees, and empirical validation on CIFAR-10 with ResNet-18 confirms comparable performance to standard SGDM.

## Method Summary
The authors propose Exponentiated O2NC, which modifies SGDM by multiplying each update by an exponential random variable before applying it. This random scaling creates a linear relationship between objective progress and expected gradient, bypassing the need for Taylor approximations in non-smooth settings. The algorithm converts the non-convex optimization problem into an online convex optimization (OCO) problem where regret minimization guarantees convergence. A variance-regularizing regularizer controls the norm of updates, and the momentum parameter β is carefully tuned based on the smoothness properties of the objective function.

## Key Results
- Achieves optimal rate of O(c^{1/2}ε^{-7/2}) for finding (c, ε)-stationary points in non-smooth non-convex optimization
- Maintains standard SGDM structure with only an additional random scaling factor
- Empirically validated on CIFAR-10 with ResNet-18, showing comparable performance to standard SGDM
- Provides the first optimal convergence guarantees for non-smooth non-convex problems with momentum

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exponential random scaling enables linear relationship between objective progress and expected gradient
- Mechanism: Scaling updates by an exponentially distributed random scalar removes the need for Taylor approximations in non-smooth settings
- Core assumption: The expectation over the exponential scaling factor can be computed analytically
- Evidence anchors:
  - [abstract] "scaling each update by an exponentially distributed random scalar"
  - [section] "Lemma 3.1. Let s ∼ Exp(λ) for some λ > 0, then Es[F (x + s∆) − F (x)] = Es[⟨∇F (x + s∆), ∆⟩]/λ"
  - [corpus] Weak - no corpus evidence found
- Break condition: If the exponential distribution assumption fails or if F is not Lipschitz continuous.

### Mechanism 2
- Claim: Exponentiated gradient weighting reduces the problem to minimizing regret in online convex optimization
- Mechanism: By upweighting gradients with factor β^-n, the algorithm can bound expected gradients through weighted regret minimization
- Core assumption: The OCO algorithm can minimize the weighted regret effectively
- Evidence anchors:
  - [section] "The most significant feature of Exponentiated O2NC... is the loss function: ℓn(∆) = ⟨β−ngn, ∆⟩ + Rn(∆)"
  - [section] "we can estimate the 'training progress' F (xn) − F (xn−1) by directly computing the stochastic gradient at iterate xn"
  - [corpus] Weak - no corpus evidence found
- Break condition: If the OCO algorithm fails to minimize regret or if the weighting becomes too extreme (β too small).

### Mechanism 3
- Claim: Variance regularization through regularizer Rn(∆) controls the variance of the random iterate distribution
- Mechanism: The regularizer ensures that the norm of updates remains bounded, which in turn bounds the variance of the random variable yn
- Core assumption: The regularizer is properly tuned to control variance without hindering convergence
- Evidence anchors:
  - [section] "We impose the regularizer Rn(∆) = µn/2 ∥∆∥2 to control the variance E ∥yn − xn∥2"
  - [section] "Lemma 3.2. For any β ∈ (0, 1), Es PN n=1 Eyn ∥yn − xn∥2 ≤ PN n=1 12/(1 − β)2 ∥∆n∥2"
  - [corpus] Weak - no corpus evidence found
- Break condition: If the regularizer is too strong (overly conservative updates) or too weak (uncontrolled variance).

## Foundational Learning

- Concept: Exponential distribution and its properties
  - Why needed here: The algorithm relies on exponential random scaling to establish linear relationships without Taylor approximations
  - Quick check question: What is the expectation and variance of an Exp(λ) random variable?

- Concept: Online convex optimization and regret minimization
  - Why needed here: The algorithm converts the non-convex optimization problem into an OCO problem where regret minimization guarantees convergence
  - Quick check question: How does regret minimization in OCO relate to convergence in non-convex optimization?

- Concept: Bregman divergences and mirror descent
  - Why needed here: The unconstrained OGD variant uses Bregman divergences for the update rule
  - Quick check question: What is the three-point identity for Bregman divergences and how is it used in OMD analysis?

## Architecture Onboarding

- Component map: Exponential random scaler -> OCO algorithm -> Regularizer -> Momentum accumulator -> Parameter updater

- Critical path:
  1. Receive gradient gt
  2. Compute regularized loss ℓn(∆) = ⟨β−ngn, ∆⟩ + Rn(∆)
  3. Send loss to OCO algorithm
  4. Receive update ∆n from OCO
  5. Apply exponential scaling: sn+1 ~ Exp(1)
  6. Update parameter: xt+1 = xt - sn+1 · ηmt+1
  7. Update momentum: mt+1 = βmt + (1-β)gt

- Design tradeoffs:
  - Random scaling vs deterministic updates: Random scaling enables convergence guarantees but adds stochasticity
  - Regularizer strength vs convergence speed: Stronger regularization ensures stability but may slow convergence
  - Momentum parameter β vs learning rate η: These must be balanced for optimal performance

- Failure signatures:
  - Exploding parameter values (suggests inadequate regularization)
  - Very small updates (suggests over-regularization)
  - Non-convergence (suggests incorrect β/η tuning)
  - High variance in training loss (suggests insufficient regularization)

- First 3 experiments:
  1. Run with standard SGDM (no random scaling) on a smooth convex problem to establish baseline performance
  2. Add exponential random scaling with β=0.9 and monitor convergence behavior
  3. Vary the momentum parameter β while keeping other parameters fixed to find optimal setting for specific problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the algorithm perform with adaptive online learning algorithms like AdaGrad or Adam as the OCO subroutine?
- Basis in paper: [explicit] The authors discuss that using AdaGrad as the OCO subroutine could potentially recover Adam's update mechanism, but note that convergence analysis is complex and requires nuanced approach.
- Why unresolved: The authors mention this is an interesting open problem but do not provide analysis or experimental results for adaptive algorithms.
- What evidence would resolve it: Implementation and analysis of Exponentiated O2NC with AdaGrad/Adam as OCO subroutine, including convergence guarantees and empirical performance comparison.

### Open Question 2
- Question: What is the optimal choice of the momentum parameter β in different smoothness regimes?
- Basis in paper: [explicit] The authors derive specific values for β in smooth and second-order smooth cases but don't provide general guidance or explore the full parameter space.
- Why unresolved: The analysis provides specific values for extreme cases but doesn't establish a general theory for choosing β across different problem characteristics.
- What evidence would resolve it: Systematic empirical study varying β across problems with different smoothness properties, or theoretical analysis establishing optimal β as a function of problem parameters.

### Open Question 3
- Question: How does the random scaling affect the algorithm's performance in practice compared to its theoretical benefits?
- Basis in paper: [explicit] The authors implement and test the algorithm with random scaling on CIFAR-10 with ResNet-18, finding performance similar to standard SGDM, but don't deeply analyze the scaling's practical impact.
- Why unresolved: The empirical evaluation shows similar performance but doesn't investigate when or why the random scaling might help or hurt, or explore different scaling distributions.
- What evidence would resolve it: Extensive empirical studies comparing different scaling distributions, analyzing cases where random scaling helps/hurts, and investigating the relationship between problem characteristics and scaling effectiveness.

## Limitations

- The exponential random scaling assumption may not generalize well to all problem structures, particularly with heavy-tailed gradient distributions
- Performance benefits in practice are not clearly established - empirical results show comparable performance to standard SGDM
- The approach requires careful tuning of the momentum parameter β based on problem smoothness properties

## Confidence

- High confidence: The linear relationship between objective progress and expected gradient under exponential scaling is well-established theoretically
- Medium confidence: The conversion to OCO and regret minimization approach works in theory but may have practical implementation challenges
- Medium confidence: Variance regularization effectiveness depends heavily on proper hyperparameter tuning

## Next Checks

1. Test the algorithm on non-smooth non-convex problems with varying degrees of Lipschitz continuity to verify the theoretical bounds empirically
2. Evaluate sensitivity to the exponential distribution parameter by varying λ and measuring impact on convergence rate and stability
3. Compare against alternative variance reduction techniques for non-smooth optimization to establish relative performance advantages