---
ver: rpa2
title: 'Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word Embeddings'
arxiv_id: '2411.16527'
source_url: https://arxiv.org/abs/2411.16527
tags:
- bias
- stereotype
- terms
- dimensions
- names
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces bias profiles for LLMs using stereotype dimensions
  from social psychology. It projects contextual embeddings onto interpretable spaces
  based on warmth and competence dimensions, as well as finer-grained aspects like
  sociability and morality.
---

# Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word Embeddings

## Quick Facts
- arXiv ID: 2411.16527
- Source URL: https://arxiv.org/abs/2411.16527
- Reference count: 19
- Authors: Carolin M. Schuster; Maria-Alexandra Dinisor; Shashwat Ghatiwala; Georg Groh
- Primary result: LLMs exhibit significant gender bias, associating female names with higher warmth and male names with higher competence

## Executive Summary
This paper introduces a novel method for profiling bias in large language models using stereotype dimensions from social psychology. The approach transforms contextual word embeddings into interpretable stereotype spaces based on warmth and competence dimensions, enabling intuitive visualization and quantification of bias. The method is validated across 12 different LLMs and demonstrates consistent gender bias patterns that align with human stereotypes.

## Method Summary
The method transforms contextual embeddings into interpretable stereotype dimensions using a change of basis matrix derived from word lists representing high and low poles of each dimension. The authors compile dictionaries for seven stereotype dimensions (sociability, morality, ability, agency, status, politics, religion) and generate context examples for each term. They extract contextual embeddings from multiple layers of 12 different LLMs, project these embeddings onto the stereotype space, and statistically analyze differences between gender-associated groups. The approach enables both 2D warmth-competence profiles and 7D multidimensional bias assessments.

## Key Results
- LLMs show significant gender bias, associating female names with higher warmth and male names with higher competence
- The bias is robust across 12 different models, context types, and network layers
- Prediction accuracy for additional terms on stereotype dimensions validates the transformation approach

## Why This Works (Mechanism)

### Mechanism 1
The bias profiles work by transforming high-dimensional contextual embeddings into interpretable stereotype dimensions using a change of basis matrix derived from word lists representing high and low poles of each dimension. The method computes sense embeddings for each word by averaging contextual embeddings across examples, then calculates pole embeddings by averaging sense embeddings for words labeled as high or low on each dimension. The difference between these pole embeddings defines the change of basis matrix that maps the original embedding space to the interpretable stereotype space.

### Mechanism 2
The method effectively detects gender bias by comparing the projected values of gender-associated names and terms on the warmth and competence dimensions, revealing systematic differences consistent with human stereotypes. After transforming embeddings to the stereotype space, the method standardizes projected values for names and terms separately, then uses statistical tests to determine if differences between female-associated and male-associated groups are significant.

### Mechanism 3
The method works across different LLMs and contexts because the stereotype dimensions are theoretically grounded and the transformation is applied consistently regardless of model architecture or context type. The method uses the same dictionary-based transformation for all models, applying it to contextual embeddings from different layers and with different context types (generated, dictionary, Reddit examples).

## Foundational Learning

- **Social psychology research on stereotype content model**: The method is grounded in the stereotype content model, which provides the theoretical basis for the dimensions used to profile bias. *Quick check: What are the two primary dimensions of the stereotype content model and what social concepts do they represent?*

- **Word embedding transformation and projection techniques**: The method relies on transforming high-dimensional embeddings into interpretable spaces using change of basis matrices. *Quick check: How does the method compute the change of basis matrix for projecting embeddings to stereotype dimensions?*

- **Statistical testing for significance of differences**: The method uses statistical tests to determine if observed differences in projected values between gender groups are significant. *Quick check: What statistical test does the method use to assess the significance of gender differences in projected stereotype values?*

## Architecture Onboarding

- **Component map**: Dictionary processing -> Context example generation -> Embedding extraction -> Projection transformation -> Statistical analysis -> Visualization

- **Critical path**: 
  1. Load dictionary and generate/retrieve context examples
  2. Extract contextual embeddings for dictionary words and target terms
  3. Compute change of basis matrix from dictionary word embeddings
  4. Project target term embeddings to stereotype space
  5. Standardize and statistically analyze differences
  6. Visualize results

- **Design tradeoffs**:
  - Using generated vs. natural context examples: Generated examples avoid introducing additional bias but may lack real-world diversity; natural examples are more realistic but may contain confounding biases
  - Seed vs. extended dictionaries: Seed dictionaries are theoretically grounded but smaller; extended dictionaries include more terms but may include noise
  - Layer selection: Using all layers captures overall model behavior but may obscure layer-specific patterns; analyzing individual layers provides more detail but increases complexity

- **Failure signatures**:
  - Low prediction accuracy for direction of additional terms suggests poor representation of stereotype dimensions
  - Inconsistent results across context types or layers suggests sensitivity to contextual factors
  - Non-significant results despite known bias in literature suggests insufficient statistical power or poor transformation

- **First 3 experiments**:
  1. Test prediction accuracy on additional terms from extended dictionary using generated context examples to validate dimension representation
  2. Compare 2D warmth-competence profiles for gender-associated names across all 12 models to establish baseline gender bias patterns
  3. Analyze layerwise bias patterns for a single model to identify if bias is concentrated in specific network layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of context examples affect the statistical significance and robustness of stereotype dimension measurements?
- Basis in paper: [explicit] The paper discusses increasing context examples as a potential solution for analyzing smaller vocabulary populations and mentions preliminary findings about example properties.
- Why unresolved: The current study uses a fixed number of 5 context examples per term, and the authors acknowledge that this may limit statistical power for smaller groups.
- What evidence would resolve it: Systematic experiments varying the number of context examples (e.g., 1, 3, 5, 10, 20) while measuring stereotype dimension stability and statistical significance across different vocabulary populations.

### Open Question 2
- Question: How do stereotype dimensions in LLMs transfer across different languages and cultural contexts?
- Basis in paper: [inferred] The authors acknowledge their analysis is limited to English contextual embeddings and suggest extending to multilingual settings as future work.
- Why unresolved: The paper only examines English-language models, leaving open whether the same stereotype patterns emerge in other languages with different cultural contexts.
- What evidence would resolve it: Replicating the stereotype dimension analysis on multilingual models (e.g., mBERT, XLM-R) and comparing results across languages, particularly those from different cultural backgrounds.

### Open Question 3
- Question: What is the relationship between embedding-based bias measurements and downstream task performance for different social groups?
- Basis in paper: [explicit] The authors acknowledge that embedding-based methods have been critiqued for their remoteness from downstream applications and are no substitute for task-specific investigations.
- Why unresolved: While the paper demonstrates bias in embeddings, it doesn't empirically connect these findings to actual performance disparities in real-world tasks.
- What evidence would resolve it: Controlled experiments measuring task performance (e.g., sentiment analysis, resume screening, medical diagnosis) across different social groups while correlating with embedding-based bias scores.

## Limitations
- The transformation relies heavily on the quality and representativeness of word lists used to define each dimension
- The averaging process for computing sense embeddings may not adequately disambiguate all word senses
- The statistical analysis assumes normal distributions and equal variances, which may not be valid for all projected values

## Confidence
- **High confidence**: The general finding that LLMs exhibit gender bias consistent with human stereotypes
- **Medium confidence**: The specific quantification of bias magnitudes and the robustness of results across different context types and model layers
- **Medium confidence**: The effectiveness of the polar projection framework in transforming embeddings to interpretable spaces

## Next Checks
1. Test the sensitivity of bias profiles to different word list sizes and composition by systematically varying the number of terms in each dimension and comparing resulting bias patterns
2. Validate the statistical assumptions underlying the t-tests by examining the distribution of projected values and testing for normality and equal variance across groups
3. Conduct ablation studies on context example quality by comparing bias profiles generated from high-quality vs. low-quality context examples to assess robustness to contextual noise