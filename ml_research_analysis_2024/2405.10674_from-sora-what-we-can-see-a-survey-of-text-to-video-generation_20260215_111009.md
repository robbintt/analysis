---
ver: rpa2
title: 'From Sora What We Can See: A Survey of Text-to-Video Generation'
arxiv_id: '2405.10674'
source_url: https://arxiv.org/abs/2405.10674
tags:
- video
- arxiv
- generation
- videos
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively reviews text-to-video (T2V) generation
  literature, focusing on insights from OpenAI''s Sora. It categorizes T2V methods
  into three evolutionary generator frameworks: GAN/VAE-based, Diffusion-based, and
  Autoregressive-based.'
---

# From Sora What We Can See: A Survey of Text-to-Video Generation

## Quick Facts
- **arXiv ID**: 2405.10674
- **Source URL**: https://arxiv.org/abs/2405.10674
- **Reference count**: 40
- **Key outcome**: Comprehensive review of text-to-video generation literature with focus on OpenAI's Sora, categorizing methods into three generator frameworks and identifying key quality metrics and challenges.

## Executive Summary
This survey provides a comprehensive overview of text-to-video generation methods, particularly focusing on insights derived from OpenAI's Sora model. The authors systematically categorize T2V approaches into three evolutionary frameworks: GAN/VAE-based, Diffusion-based, and Autoregressive-based generators. The paper identifies three critical quality metrics for evaluating T2V outputs: extended duration, superior resolution, and seamless quality. Additionally, it proposes a "realistic panorama" approach that addresses dynamic motion, complex scenes, multiple objects, and rational layouts as key challenges in the field.

## Method Summary
The survey employs a comprehensive literature review methodology, analyzing existing T2V generation approaches through the lens of insights gained from OpenAI's Sora announcement. The authors categorize methods into three distinct generator frameworks based on their architectural foundations and evolution patterns. They identify and analyze key quality metrics that define excellent video generation outputs, while also examining the challenges faced by current T2V systems including unrealistic motion patterns, object appearance inconsistencies, and limited understanding of object characteristics. The survey also reviews available T2V datasets and evaluation metrics, providing a structured framework for understanding the current state of the field.

## Key Results
- Categorization of T2V methods into GAN/VAE-based, Diffusion-based, and Autoregressive-based frameworks
- Identification of three key quality metrics: extended duration, superior resolution, and seamless quality
- Proposal of a realistic panorama approach addressing dynamic motion, complex scenes, multiple objects, and rational layouts

## Why This Works (Mechanism)
The survey's effectiveness stems from its systematic approach to categorizing T2V methods based on their underlying generative frameworks. By identifying the evolutionary progression from GAN/VAE to Diffusion to Autoregressive approaches, the authors provide a clear understanding of how different architectural choices impact video generation quality. The identification of key quality metrics aligns with fundamental video characteristics that human observers naturally evaluate. The realistic panorama approach effectively captures the multi-faceted challenges in T2V generation by breaking down the problem into manageable components that can be individually addressed while maintaining overall coherence.

## Foundational Learning

**GAN/VAE-based generation**: These methods learn to map latent spaces to video outputs through adversarial training or variational inference. Why needed: Provides foundation for understanding early T2V approaches and their limitations. Quick check: Evaluate generated video quality and training stability across different loss functions.

**Diffusion models**: These models learn to denoise random noise iteratively to generate high-quality outputs. Why needed: Represents the current state-of-the-art approach that powers models like Sora. Quick check: Measure sample quality improvements with increased denoising steps.

**Autoregressive generation**: These models generate content sequentially, conditioning each output on previous elements. Why needed: Enables precise control over generation but faces challenges with long-range coherence. Quick check: Assess temporal consistency across generated video frames.

## Architecture Onboarding

**Component map**: Data preprocessing -> Generator framework selection -> Quality metric evaluation -> Challenge assessment
**Critical path**: Input text encoding → Latent space representation → Frame generation → Temporal coherence enforcement → Output video assembly
**Design tradeoffs**: Resolution vs. generation speed, duration vs. quality, complexity vs. controllability
**Failure signatures**: Unrealistic motion patterns, object appearance inconsistencies, temporal discontinuities
**First experiments**:
1. Generate short videos with varying text prompts to test basic text-video alignment
2. Evaluate generation quality across different resolution settings
3. Test temporal coherence by generating videos with complex motion sequences

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on insights from OpenAI's Sora, which was announced but not publicly released at the time of writing
- May oversimplify nuanced technical requirements in the proposed "realistic panorama" approach
- Discussion of future directions may not accurately predict the most fruitful research areas

## Confidence
**High confidence**: The categorization of T2V methods into GAN/VAE-based, Diffusion-based, and Autoregressive-based frameworks is well-established in the literature and accurately reflects the current state of the field.

**Medium confidence**: The proposed "realistic panorama" approach and its four components represent reasonable synthesis of current challenges but may oversimplify the nuanced technical requirements for each component.

**Low confidence**: Claims about Sora's specific technical capabilities and limitations are largely speculative given the lack of public access to the model.

## Next Checks
1. Direct technical validation of Sora's capabilities once publicly available, comparing claimed performance against actual model outputs across the identified quality metrics.
2. Empirical testing of the proposed "realistic panorama" framework by implementing and evaluating representative methods for each of the four identified components to verify their practical importance and interdependencies.
3. Systematic comparison of current T2V evaluation metrics against human perceptual studies to identify potential gaps between quantitative measurements and qualitative video quality assessments.