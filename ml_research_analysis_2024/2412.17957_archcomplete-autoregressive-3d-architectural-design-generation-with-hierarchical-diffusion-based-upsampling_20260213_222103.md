---
ver: rpa2
title: 'ArchComplete: Autoregressive 3D Architectural Design Generation with Hierarchical
  Diffusion-Based Upsampling'
arxiv_id: '2412.17957'
source_url: https://arxiv.org/abs/2412.17957
tags:
- voxel
- shape
- archcomplete
- design
- patches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ArchComplete, a two-stage voxel-based 3D
  generative pipeline for architectural design. The method addresses the challenge
  of generating complex architectural geometries and topologies with fine geometric
  details at high resolutions.
---

# ArchComplete: Autoregressive 3D Architectural Design Generation with Hierarchical Diffusion-Based Upsampling

## Quick Facts
- arXiv ID: 2412.17957
- Source URL: https://arxiv.org/abs/2412.17957
- Authors: S. Rasoulzadeh; M. Bank; I. Kovacic; K. Schinegger; S. Rutzinger; M. Wimmer
- Reference count: 11
- Key outcome: Introduces a two-stage voxel-based 3D generative pipeline for architectural design that achieves state-of-the-art performance in quality, diversity, and computational efficiency, generating models at resolutions up to 512³ with voxel sizes as small as 9cm

## Executive Summary
ArchComplete presents a novel two-stage pipeline for generating complex 3D architectural designs with fine geometric details. The method learns a contextually rich codebook of local patch embeddings using a 3D Voxel VQGAN with a novel 2.5D perceptual loss, then employs hierarchical diffusion models trained on cropped local volumetric patches for high-resolution generation. The pipeline achieves state-of-the-art performance across multiple architectural generation tasks while maintaining computational efficiency through local patch processing and progressive refinement.

## Method Summary
ArchComplete is a two-stage voxel-based 3D generative pipeline that addresses architectural design generation through a combination of vector quantization and hierarchical diffusion models. The first stage uses a 3D Voxel VQGAN with a 2.5D perceptual loss to learn discrete codebook embeddings of local volumetric patches from coarse voxelized models (64³). An autoregressive transformer then models the composition of these codebook indices to generate coarse architectural shapes. The second stage employs a cascade of 3D conditional DDPMs trained on hierarchies of local volumetric patches (8³, 16³, 32³, 64³) to progressively upsample to high resolution (512³). The approach reframes upsampling as local patch processing, significantly reducing computational requirements while maintaining detail quality through parallel processing and a voxel clean-up method to remove artifacts.

## Key Results
- Achieves state-of-the-art performance in quality, diversity, and computational efficiency for 3D architectural generation
- Generates high-resolution models up to 512³ with voxel sizes as small as 9cm
- Solves multiple tasks including shape completion, geometric detailisation, and unconditional synthesis
- Outperforms prior methods in reconstruction quality and diversity metrics (COV, MMD, 1-NNA, UHD, TMD)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Patch-wise quantisation with a 2.5D perceptual loss enables learning of coherent global spatial relationships in architectural models
- Mechanism: The 3D Voxel VQGAN encodes voxelised models into discrete codebook vectors for local volumetric patches, optimised alongside a 2.5D perceptual loss that captures global spatial correspondence through projections onto three orthogonal planes. This combination ensures the codebook vectors reflect both local geometric details and global structural relationships.
- Core assumption: Local patch embeddings can capture architectural semantics when trained with global spatial constraints
- Evidence anchors:
  - [abstract] "Key to our pipeline is (i) learning a contextually rich codebook of local patch embeddings, optimised alongside a 2.5D perceptual loss that captures global spatial correspondence of projections onto three axis-aligned orthogonal planes"
  - [section] "we propose a 2.5D perceptual loss L2.5P(v, ˆv) = ∑i∈{xy,yz,xz} ∑ j λ j∥φ j( fi(v)) − φ j( fi( ˆv))∥3, where φ j represents the individual layers of a pre-trained visual perception network (in our case, VGG-16 [SZ14])"
- Break condition: If the architectural models have highly irregular topologies that cannot be adequately represented by local patch embeddings, or if the 2.5D projections fail to capture essential 3D spatial relationships

### Mechanism 2
- Claim: Hierarchical diffusion models trained on coarse-to-fine local patches enable high-resolution generation with reduced computational cost
- Mechanism: The second stage uses a cascade of 3D conditional DDPMs trained on a hierarchy of local volumetric patches (8³, 16³, 32³, 64³) where each finer patch is contained within its coarser counterpart. This local processing approach drastically reduces memory requirements while maintaining detail quality through progressive refinement.
- Core assumption: Local volumetric patches contain sufficient information for global shape reconstruction when processed hierarchically
- Evidence anchors:
  - [abstract] "redefining upsampling as a set of conditional diffusion models learning from a hierarchy of randomly cropped coarse-to-fine local volumetric patches"
  - [section] "we define a set of 3D conditional Denoising Diffusion Probabilistic Models (3D c-DDPMs) [HJA20] that train on a hierarchy of local volumetric patches instead of entire 3D voxelised models"
- Break condition: If the hierarchical patch boundaries create visible seams or discontinuities in the final generated model, or if the local patches lack sufficient context for coherent global structure

### Mechanism 3
- Claim: Autoregressive transformer modeling of discrete codebook sequences enables flexible shape manipulation operations
- Mechanism: The transformer learns statistical correlations between discrete codebook indices, enabling autoregressive generation of patch sequences. This discrete representation reframes shape interpolation and variation as sequence manipulation problems using genetic algorithm operators.
- Core assumption: Discrete codebook representations preserve architectural semantics necessary for meaningful shape manipulation
- Evidence anchors:
  - [abstract] "whose composition is modelled with an autoregressive transformer for generating coarse shapes"
  - [section] "Benefiting from a compact composition and tractable order of discrete representation, the correlation between discrete codes corresponding to local patches can be absorbedly learned, thereby effectively exploring priors of shape composition"
- Break condition: If the discrete codebook loses critical architectural information during quantisation, making shape manipulation operations produce unrealistic results

## Foundational Learning

- Concept: Vector Quantisation and Discrete Representation Learning
  - Why needed here: Enables compact, tractable representation of 3D architectural shapes that can be efficiently manipulated by transformers
  - Quick check question: How does vector quantisation differ from continuous latent space representations in terms of learning complexity and generation quality?

- Concept: Diffusion Probabilistic Models and Conditional Generation
  - Why needed here: Provides the framework for high-resolution upsampling while maintaining coherence with coarse shape priors
  - Quick check question: What is the key difference between unconditional and conditional diffusion models in terms of training objective and generation process?

- Concept: Perceptual Loss and Multi-view Representation
  - Why needed here: Ensures global spatial coherence in generated models by capturing relationships across multiple views
  - Quick check question: Why is a 2.5D perceptual loss (using three orthogonal projections) more effective than voxel-wise reconstruction loss for architectural models?

## Architecture Onboarding

- Component map:
  Data Preparation -> 3D Voxel VQGAN (Encoder, Decoder, PatchGAN, 2.5D Perceptual Loss) + Transformer -> Hierarchical 3D c-DDPMs (U-Net variants at multiple resolutions) -> Post-processing (Voxel clean-up)

- Critical path: Data Preparation → 3D Voxel VQGAN Training → Transformer Training → Hierarchical Upsampling Models Training → Inference Pipeline

- Design tradeoffs:
  - Dense voxel representation vs sparse representations: Dense provides direct correspondence to 3D space but requires more memory
  - Local patch processing vs global processing: Local reduces memory but may lose global context
  - Discrete codebook vs continuous latent space: Discrete enables efficient manipulation but may lose fine details

- Failure signatures:
  - Noisy voxel grids with floating or sticking voxels (indicates clean-up method issues)
  - Visible seams between patches in final output (indicates hierarchical upsampling problems)
  - Loss of architectural details during quantisation (indicates codebook capacity issues)
  - Degraded generation quality at higher resolutions (indicates diffusion model training issues)

- First 3 experiments:
  1. Test 3D Voxel VQGAN reconstruction quality on simple geometric shapes before moving to complex architectural models
  2. Validate hierarchical upsampling on synthetic multi-resolution shapes to check for seam artifacts
  3. Test shape completion capability on partially masked models to verify transformer's conditional generation ability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed voxel clean-up method scale with different architectural styles and design complexities?
- Basis in paper: [explicit] The paper describes a voxel clean-up method that removes noise artefacts like sticking and floating voxels, but does not evaluate its performance across different architectural styles or design complexities.
- Why unresolved: The method is designed specifically for the dataset of 3D house models used in the paper, and its effectiveness for other architectural styles or more complex designs remains unexplored.
- What evidence would resolve it: Testing the clean-up method on diverse architectural datasets with varying styles and complexities, and evaluating its performance in terms of noise reduction and preservation of design details.

### Open Question 2
- Question: Can the hierarchical diffusion models be further improved to reduce error accumulation across levels?
- Basis in paper: [explicit] The paper acknowledges that error accumulation is inherent in the hierarchical upsampling process, where higher-resolution grids cannot easily fix artefacts from preceding levels.
- Why unresolved: The current implementation does not address this issue, and potential solutions like adding refinement networks to each level are left for future work.
- What evidence would resolve it: Implementing and evaluating refinement networks at each level of the hierarchy, and comparing the results with the current approach in terms of artefact reduction and detail preservation.

### Open Question 3
- Question: How does the performance of ArchComplete compare to other 3D generative models when trained on larger and more diverse architectural datasets?
- Basis in paper: [inferred] The paper mentions that the current dataset is limited in size and diversity, and expanding it to include a broader range of architectural typologies could improve the pipeline's generalization.
- Why unresolved: The paper does not provide any quantitative or qualitative comparisons with other 3D generative models on larger and more diverse datasets.
- What evidence would resolve it: Training ArchComplete and other state-of-the-art 3D generative models on a larger and more diverse architectural dataset, and comparing their performance in terms of shape quality, diversity, and computational efficiency.

## Limitations

- Lack of detailed architectural specifications for the 3D VQGAN and transformer components, making exact reproduction challenging
- High computational requirements for training at 512³ resolution may be prohibitive for many research groups
- Voxel clean-up method is only briefly described without clear criteria for what constitutes "noisy" voxels

## Confidence

- **High Confidence**: The hierarchical diffusion-based upsampling approach with local patch processing is technically sound and addresses real computational challenges
- **Medium Confidence**: The 2.5D perceptual loss effectively captures global spatial relationships - while the concept is reasonable, empirical validation is limited
- **Medium Confidence**: The two-stage pipeline achieves state-of-the-art performance - claims are based on reported metrics but lack extensive comparative analysis

## Next Checks

1. **Architecture Verification**: Implement a simplified version of the 3D VQGAN with the 2.5D perceptual loss on synthetic 3D shapes (cubes, spheres, basic structures) to verify that the loss function effectively captures global spatial relationships before scaling to architectural models

2. **Memory Efficiency Test**: Profile the hierarchical diffusion model training at different resolutions (32³ → 64³, 64³ → 128³) to quantify the claimed computational efficiency gains and identify memory bottlenecks

3. **Cross-Dataset Generalization**: Test the pretrained model on a held-out subset of the architectural dataset or a different 3D model collection to evaluate whether the learned codebook and diffusion models generalize beyond the training distribution