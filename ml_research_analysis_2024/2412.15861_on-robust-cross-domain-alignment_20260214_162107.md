---
ver: rpa2
title: On Robust Cross Domain Alignment
arxiv_id: '2412.15861'
source_url: https://arxiv.org/abs/2412.15861
tags:
- robust
- such
- given
- where
- also
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robust cross-domain alignment
  using the Gromov-Wasserstein (GW) distance, which is known to be vulnerable to contamination
  in the underlying measures. The authors propose three novel techniques to robustify
  GW and its variants, drawing inspiration from robust statistics rather than traditional
  optimal transport approaches.
---

# On Robust Cross Domain Alignment

## Quick Facts
- arXiv ID: 2412.15861
- Source URL: https://arxiv.org/abs/2412.15861
- Authors: Anish Chakrabarty; Arkaprabha Basu; Swagatam Das
- Reference count: 17
- Key outcome: Three novel robust GW variants using loss functions, truncated metrics, and proxy regularization, validated on shape matching and image translation under contamination

## Executive Summary
This paper addresses the vulnerability of Gromov-Wasserstein (GW) distances to contamination in underlying measures by proposing three novel robustification techniques. Drawing from robust statistics rather than traditional optimal transport approaches, the authors introduce methods that penalize large distortions, deploy relaxed metrics, and regularize optimization with proxy distributions. The proposed methods demonstrate superior resilience to outliers compared to state-of-the-art approaches across various tasks including shape matching and image-to-image translation.

## Method Summary
The paper introduces three distinct approaches to robustify GW distances against contamination. The first method embeds Tukey and Huber loss functions into the GW framework, replacing the standard Lp norm to limit the influence of large pairwise distortions. The second approach uses truncated metrics that preserve topology, creating a lower bound to GW that limits extreme pairwise distances at the metric level. The third method regularizes the optimization over couplings using 'clean' proxy distributions, achieving robust measure-preserving maps. These methods are implemented using entropic regularization and Sinkhorn scaling, with empirical validation on shape matching and image-to-image translation tasks under various contamination models.

## Key Results
- Huber GW (HGW) with data-driven threshold selection shows significant improvement in shape matching under Cauchy and Gaussian outliers
- Robust GcGAN/UNIT architectures using RSGW loss exhibit superior denoising capacity in image-to-image translation under pixel contamination
- The proposed methods demonstrate better resilience to contamination compared to GW, FGW, PGW, and UGW baselines across multiple tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tukey and Huber loss functions can be embedded into the GW framework to create robust distance metrics that limit the influence of large pairwise distortions.
- Mechanism: By replacing the Lp norm in the GW cost with polynomially bounded loss functions (e.g., Tp or H), extreme pairwise distances are truncated or smoothed, preventing outlier observations from dominating the alignment.
- Core assumption: The threshold τ is set appropriately to balance robustness and preservation of geometric information.
- Evidence anchors:
  - [abstract] "The first method introduces penalization to large distortions while calculating GW, using Tukey and Huber loss functions."
  - [section 4.1] Proposition 3 gives robustness guarantees for the Tukey GW distance under Huber contamination.
  - [corpus] Weak corpus alignment; only "Robust Alignment via Partial Gromov-Wasserstein Distances" appears relevant.
- Break condition: If τ is set too low, valid geometric information is lost; if too high, robustness gains diminish.

### Mechanism 2
- Claim: Local robustification using truncated metrics lλ(dX) and lλ(dY) creates a lower bound to GW that limits extreme pairwise distances at the metric level.
- Mechanism: Replacing the original metrics dX and dY with their truncated versions lλ ensures that any pairwise distance above λ is clamped, preventing outliers from skewing the GW cost. This yields the locally robust GW (LRGW) distance.
- Core assumption: The threshold λ can be tuned to the expected support of the clean data, allowing distances above λ to be treated as outliers.
- Evidence anchors:
  - [section 4.2] Lemma 9 and Proposition 11 show that LRGW is a lower bound to TGW and can be expressed via a trimmed OT problem.
  - [abstract] "Offering a finer control over extreme pairwise distances from either space, the second method rather deploys relaxed metrics that preserve topology."
  - [corpus] Weak corpus evidence; "Metric properties of partial and robust Gromov-Wasserstein distances" may be relevant.
- Break condition: If λ is set above the true diameter of the clean data, the truncation has no effect; if too low, legitimate distances are overly suppressed.

### Mechanism 3
- Claim: Regularizing the optimization over couplings with proxy distributions yields robust measure-preserving maps that are resilient to contamination.
- Mechanism: By optimizing over a restricted set of couplings Πϵ that only transport a fraction (1−ϵ) of mass from each space, outliers are excluded from the alignment. This is formalized as the robust reversible Gromov-Monge (RRGM) distance.
- Core assumption: There exists a set of "clean" proxy distributions that can approximate the inlier structure even under contamination.
- Evidence anchors:
  - [abstract] "The third approach regularizes the optimization based on 'clean' proxy distributions to achieve robust measure-preserving maps."
  - [section 4.3] Lemma 17 connects this regularization to ROT and Proposition 16 shows RRGM's robustness to outliers.
  - [corpus] No strong corpus evidence for this specific regularization approach.
- Break condition: If the clean proxy distributions are poorly estimated, the resulting maps may still be corrupted.

## Foundational Learning

- Concept: Gromov-Wasserstein distance as a measure of structural alignment between distributions on different spaces.
  - Why needed here: Understanding GW is essential to see why contamination is problematic and how the three robustification methods modify it.
  - Quick check question: What distinguishes GW from OT in terms of the cost it optimizes?

- Concept: Huber contamination model and its implications for robust statistics.
  - Why needed here: The robustness guarantees hinge on assumptions about how data is contaminated (e.g., fraction ϵ replaced by outliers).
  - Quick check question: In Huber contamination, what is the probability that a draw comes from the outlier distribution?

- Concept: Metric properties (non-negativity, symmetry, triangle inequality) and their preservation under robustification.
  - Why needed here: The proposed robust GW variants must retain these properties to be useful in practice.
  - Quick check question: Does the triangle inequality for TGW follow from that of the Tukey norm?

## Architecture Onboarding

- Component map: Raw pairwise distance matrices → one of three robustification strategies (Tukey/Huber loss, truncated metrics, or proxy regularization) → optimization via entropic regularization → robust distance or robust map
- Critical path: For Tukey/Huber: distance matrix → loss embedding → Sinkhorn scaling → robust distance. For LRGW: distance matrix → truncation → OT solver → lower bound distance. For RRGM: data → proxy selection → partial coupling optimization → robust map
- Design tradeoffs: Tukey/Huber balances robustness and information retention but requires careful τ tuning; LRGW offers finer control but may sacrifice non-degeneracy; RRGM ensures robust maps but at the cost of more complex optimization
- Failure signatures: Loss of geometric fidelity (τ or λ too low), instability under contamination (ε too small), or poor proxy estimation (proxy regularization fails)
- First 3 experiments:
  1. Shape matching with Gaussian and Cauchy outliers using HGW; measure FID against baseline GW
  2. Image-to-image translation (Apples-Oranges) with pixel noise using robust GcGAN; compare FID to vanilla GcGAN
  3. MNIST↔USPS translation with random bright pixels using RRGM; compare sample quality and denoising to CycleGAN

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact statistical relationship between the empirical HGW distance and its population counterpart under Huber contamination, and what are the corresponding concentration bounds?
- Basis in paper: [explicit] The paper mentions that HGW serves as a robust estimate of GW but does not provide detailed statistical analysis or concentration inequalities for the empirical HGW.
- Why unresolved: While the paper establishes theoretical properties of HGW, it lacks rigorous statistical analysis of the empirical version, particularly concentration bounds under contamination.
- What evidence would resolve it: Proofs of concentration inequalities for empirical HGW under various contamination regimes, similar to those provided for other methods in the paper.

### Open Question 2
- Question: How does the sample complexity of RRGM compare to existing methods when learning measure-preserving maps under Huber contamination?
- Basis in paper: [inferred] The paper proposes RRGM and discusses its robustness properties but does not provide detailed sample complexity analysis or comparison with existing methods.
- Why unresolved: While the paper establishes RRGM's theoretical properties and demonstrates empirical success, it lacks rigorous sample complexity analysis, particularly under contamination.
- What evidence would resolve it: Formal sample complexity bounds for RRGM under various contamination models, with comparison to existing methods.

### Open Question 3
- Question: What are the computational trade-offs between different robust GW formulations (TGW, HGW, LRGW) in terms of accuracy and efficiency?
- Basis in paper: [explicit] The paper introduces multiple robust GW formulations but does not provide comprehensive comparative analysis of their computational properties.
- Why unresolved: While each method is studied individually, the paper does not systematically compare their computational trade-offs or provide guidelines for choosing between them.
- What evidence would resolve it: Detailed computational complexity analysis and empirical comparisons of all proposed methods across various tasks and contamination levels.

## Limitations

- The robustness claims hinge critically on the assumed contamination model (Huber contamination with known fraction ϵ) and the ability to tune thresholds in a data-driven way
- The novel RRGM method lacks strong corpus validation, and the exact neural architectures for robust I2I translation are underspecified
- The guarantees in Proposition 3 for TGW require τ > 1, which may not always be achievable in real data

## Confidence

- **High confidence**: The Huber GW method's robustness mechanism and its connection to existing robust statistics (Tukey loss, Huber contamination)
- **Medium confidence**: The LRGW lower bound and its connection to trimmed OT; the empirical validation of denoising in robust I2I models
- **Low confidence**: The RRGM regularization approach and its practical implementation details for measure-preserving maps

## Next Checks

1. **Huber GW Sensitivity**: Systematically vary τ across multiple orders of magnitude in the shape matching experiment and report the trade-off between robustness (outlier resistance) and fidelity (clean data alignment quality)

2. **LRGW Practical Utility**: Compare the empirical performance of LRGW (as a lower bound) against the upper bound TGW in downstream tasks (e.g., classification or clustering) to assess if the lower bound is tight enough for practical use

3. **RRGM Proxy Estimation**: Implement and evaluate the impact of different proxy distribution estimation strategies (e.g., k-NN inliers vs. EM-based outlier detection) on the robustness of the resulting measure-preserving maps