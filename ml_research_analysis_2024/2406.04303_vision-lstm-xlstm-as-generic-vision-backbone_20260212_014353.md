---
ver: rpa2
title: 'Vision-LSTM: xLSTM as Generic Vision Backbone'
arxiv_id: '2406.04303'
source_url: https://arxiv.org/abs/2406.04303
tags:
- vision
- learning
- computer
- mlstm
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Vision-LSTM (ViL), a new vision backbone\
  \ that adapts the xLSTM architecture for computer vision tasks. ViL processes image\
  \ patches in alternating directions\u2014odd blocks traverse top-to-bottom, even\
  \ blocks bottom-to-top\u2014using mLSTM blocks with exponential gating and matrix\
  \ memory for efficient, linear-complexity computation."
---

# Vision-LSTM: xLSTM as Generic Vision Backbone

## Quick Facts
- arXiv ID: 2406.04303
- Source URL: https://arxiv.org/abs/2406.04303
- Authors: Benedikt Alkin; Maximilian Beck; Korbinian Pöppel; Sepp Hochreiter; Johannes Brandstetter
- Reference count: 40
- Primary result: Vision-LSTM (ViL) achieves 78.3% ImageNet-1K accuracy for tiny model, outperforming transformers and Mamba-based models

## Executive Summary
This paper introduces Vision-LSTM (ViL), a new vision backbone that adapts the xLSTM architecture for computer vision tasks. ViL processes image patches in alternating directions—odd blocks traverse top-to-bottom, even blocks bottom-to-top—using mLSTM blocks with exponential gating and matrix memory for efficient, linear-complexity computation. The model is pre-trained on ImageNet-1K and evaluated on ImageNet-1K classification, ADE20K semantic segmentation, and VTAB-1K transfer learning. ViL achieves strong performance: 78.3% ImageNet-1K accuracy for the tiny model, 46.3% mIoU on ADE20K, and 68.3% average accuracy on VTAB-1K, outperforming state-of-the-art transformers and Mamba-based models despite less hyperparameter tuning. Ablation studies show the importance of bidirectional traversal, 2D convolutions for local context, and positional embeddings. The main limitation is the lack of optimized hardware for mLSTM, but the authors argue ViL's linear complexity makes it especially promising for high-resolution tasks like segmentation.

## Method Summary
ViL adapts xLSTM to vision by treating image patches as a sequence with spatial traversal patterns. The architecture uses mLSTM blocks with exponential gating and matrix memory, processing patches in alternating directions (odd blocks top-to-bottom, even blocks bottom-to-top) to capture bidirectional context. Local spatial relationships are preserved through 2D convolutions within each block, while positional embeddings encode spatial information. The model achieves linear computational complexity through efficient recurrence, avoiding the quadratic complexity of attention mechanisms. ViL uses standard classification aggregation methods (concatenating first and last patches) and semantic segmentation approaches (averaging all patches). The architecture is pre-trained on ImageNet-1K and evaluated across multiple vision tasks without extensive hyperparameter tuning.

## Key Results
- ViL tiny achieves 78.3% top-1 accuracy on ImageNet-1K, outperforming state-of-the-art transformers and Mamba-based models
- On ADE20K semantic segmentation, ViL tiny achieves 46.3% mIoU, demonstrating strong performance on dense prediction tasks
- ViL achieves 68.3% average accuracy on VTAB-1K transfer learning benchmark, showing good generalization to diverse tasks
- Ablation studies confirm bidirectional traversal, 2D convolutions, and positional embeddings are critical for performance
- ViL is 45-69% faster than Vim implementation despite lacking optimized hardware for mLSTM

## Why This Works (Mechanism)
ViL works by combining the efficiency of recurrent neural networks with spatial processing capabilities. The alternating traversal direction allows each patch to accumulate context from both previous and subsequent patches in the sequence, similar to bidirectional attention but with linear complexity. The mLSTM blocks maintain memory efficiently through exponential gating, while 2D convolutions preserve local spatial relationships that pure recurrence might lose. Positional embeddings provide absolute spatial information that recurrence alone cannot capture. This combination allows ViL to model long-range dependencies efficiently while maintaining strong local feature extraction capabilities.

## Foundational Learning
- **mLSTM (matrix LSTM)**: An extension of LSTM that uses matrix operations for memory and gating, enabling more efficient sequence processing - needed for handling image patches as sequences while maintaining computational efficiency
- **Exponential gating**: A gating mechanism using exponential functions rather than sigmoids, providing sharper activation boundaries and better gradient flow - needed for stable training of deep recurrent networks
- **Spatial traversal patterns**: Processing image patches in specific directional patterns (top-to-bottom, bottom-to-top) rather than random order - needed to capture meaningful spatial context in images
- **Positional embeddings in vision**: Adding spatial position information to patch representations - needed because recurrence alone doesn't preserve absolute spatial locations
- **Linear complexity architectures**: Models with O(n) computational complexity rather than O(n²) or O(n³) - needed for scalability to high-resolution images
- **Classification aggregation methods**: Techniques for converting sequence outputs to single classification predictions - needed to adapt sequence models to standard image classification tasks

## Architecture Onboarding

Component Map:
Input Image -> Patch Extraction -> Linear Projection -> mLSTM Blocks (Alternating Traversal) -> 2D Convolutions -> Classification/Segmentation Head

Critical Path:
Patch extraction → Linear projection → mLSTM blocks with alternating traversal → 2D convolutions → Output head

Design Tradeoffs:
- Linear complexity vs. full attention: ViL sacrifices some global context modeling for O(n) complexity, making it more scalable
- Recurrence vs. convolution: Combines both to get long-range modeling from recurrence and local feature extraction from convolution
- Fixed traversal vs. learned order: Uses fixed alternating pattern rather than learning optimal patch ordering

Failure Signatures:
- Poor performance on tasks requiring complex global reasoning due to limited context modeling compared to full attention
- Potential loss of fine-grained local details if 2D convolution parameters are not properly tuned
- Sensitivity to patch size selection, as too large patches lose detail and too small patches increase sequence length

First Experiments:
1. Ablation study removing alternating traversal to test importance of bidirectional context
2. Comparison with pure convolutional baseline using same parameter count
3. Runtime benchmark on high-resolution images to verify linear complexity benefits

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the optimal traversal direction configuration for ViL on different vision tasks?
- Basis in paper: [explicit] The paper shows that quad-directional traversal improves segmentation performance over bi-directional, but uses bi-directional due to technical limitations with torch.compile
- Why unresolved: The technical limitation preventing quad-directional implementation is not a methodological constraint but rather an implementation issue with current mLSTM optimizations
- What evidence would resolve it: A full comparison of uni-directional, bi-directional, quad-directional, and oct-directional configurations on multiple vision tasks (classification, segmentation, transfer learning) using an optimized mLSTM implementation

### Open Question 2
- Question: How would an optimized hardware implementation of mLSTM affect ViL's runtime performance compared to ViT?
- Basis in paper: [explicit] The paper notes that ViL is already 45-69% faster than Vim despite lacking optimized hardware implementation, and suggests ViL could be much faster than ViT on high-resolution tasks once optimized
- Why unresolved: Current mLSTM implementations rely on generic speed optimization (torch.compile) rather than custom CUDA kernels, making runtime comparisons difficult
- What evidence would resolve it: Head-to-head runtime benchmarks of ViL versus ViT on high-resolution vision tasks using an optimized mLSTM implementation (custom CUDA kernels or Triton)

### Open Question 3
- Question: What is the optimal classification aggregation method for ViL across different vision tasks?
- Basis in paper: [explicit] The paper shows that concatenating first and last patches performs best for classification, while averaging all patches performs best for segmentation, but does not explore all possible configurations
- Why unresolved: The paper only explores a limited set of aggregation methods and suggests that using [AVG] for segmentation could further improve performance
- What evidence would resolve it: Systematic evaluation of all possible patch-based and [CLS]-based aggregation methods on multiple vision tasks with consistent architecture and training protocol

### Open Question 4
- Question: How does ViL perform with different patch sizes and hierarchical architectures?
- Basis in paper: [inferred] The paper uses fixed 16x16 patches and a non-hierarchical design, but mentions hierarchical architectures as a promising future direction
- Why unresolved: The paper does not explore different patch sizes or hierarchical designs, which could significantly impact performance and computational efficiency
- What evidence would resolve it: Comparative evaluation of ViL with various patch sizes (e.g., 8x8, 16x16, 32x32) and hierarchical architectures versus non-hierarchical baselines on multiple vision tasks

## Limitations
- Limited evaluation scope focusing primarily on classification and segmentation, with no testing on object detection or video understanding tasks
- Comparison with state-of-the-art methods is not exhaustive, missing many recent vision backbones from the literature
- Ablation studies are limited in scope, particularly regarding different positional encoding schemes and traversal configurations
- Performance claims relative to other architectures may be influenced by differences in hyperparameter tuning and training protocols

## Confidence
- **High confidence**: ViL's architecture design and its adaptation of xLSTM for vision tasks is technically sound and well-documented
- **Medium confidence**: Performance claims relative to state-of-the-art transformers and Mamba-based models, as the comparison is not comprehensive and lacks extensive hyperparameter tuning
- **Medium confidence**: The assertion that linear complexity makes ViL particularly suitable for high-resolution tasks, as this is theoretical rather than empirically demonstrated

## Next Checks
1. Evaluate ViL on additional vision tasks such as object detection (COCO) and video understanding (Kinetics) to assess its generalizability
2. Conduct runtime and memory efficiency benchmarks on actual hardware to verify the practical benefits of linear complexity
3. Perform ablation studies on different positional encoding methods and unidirectional vs. bidirectional traversal to isolate their contributions to performance