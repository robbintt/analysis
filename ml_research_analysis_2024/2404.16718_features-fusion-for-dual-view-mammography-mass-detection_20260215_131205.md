---
ver: rpa2
title: Features Fusion for Dual-View Mammography Mass Detection
arxiv_id: '2404.16718'
source_url: https://arxiv.org/abs/2404.16718
tags:
- fusion
- decoder
- feature
- object
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MAMM-Net, a novel approach for dual-view mammography
  mass detection that fuses information on both object and feature levels. The key
  innovation is the Fusion Layer, which uses deformable attention to combine features
  from craniocaudal (CC) and mediolateral oblique (MLO) views.
---

# Features Fusion for Dual-View Mammography Mass Detection

## Quick Facts
- arXiv ID: 2404.16718
- Source URL: https://arxiv.org/abs/2404.16718
- Authors: Arina Varlamova; Valery Belotsky; Grigory Novikov; Anton Konushin; Evgeny Sidorov
- Reference count: 0
- Key outcome: Proposed MAMM-Net achieves 81.6%, 87.9%, and 90.6% recall at 0.25, 0.5, and 1.0 false positives per image respectively on DDSM dataset

## Executive Summary
This paper introduces MAMM-Net, a novel approach for dual-view mammography mass detection that fuses information on both object and feature levels. The key innovation is the Fusion Layer, which uses deformable attention to combine features from craniocaudal (CC) and mediolateral oblique (MLO) views. MAMM-Net consists of a shared backbone, Fusion Pixel Decoder with Fusion Layers, and a View-Interactive Transformer Decoder that processes fused features. Experiments on the DDSM dataset show superior performance compared to previous state-of-the-art methods, achieving 81.6%, 87.9%, and 90.6% recall at 0.25, 0.5, and 1.0 false positives per image respectively. The approach also introduces lesion annotation on pixel-level and malignancy classification.

## Method Summary
MAMM-Net processes paired mammography images from CC and MLO views through a shared EfficientNet-b3 backbone to extract features. A Fusion Pixel Decoder progressively fuses features at multiple scales using Fusion Layers based on deformable attention, while maintaining high-resolution feature maps. The View-Interactive Transformer Decoder then processes these fused features with self-attention within each view and inter-attention between views to capture relationships between detected objects. The model outputs object class, malignancy score, and segmentation masks, trained with detection, linker, and malignancy losses using L2 regularization and data augmentations.

## Key Results
- Achieves 81.6%, 87.9%, and 90.6% recall at 0.25, 0.5, and 1.0 false positives per image respectively
- Outperforms previous state-of-the-art methods on DDSM dataset
- Introduces pixel-level lesion annotation and malignancy classification
- Demonstrates effective feature-level fusion between CC and MLO views

## Why This Works (Mechanism)

### Mechanism 1
The Fusion Layer uses deformable attention to selectively sample features from corresponding regions across CC and MLO views, improving detection precision without sacrificing recall. Deformable attention allows the model to focus on relevant spatial positions in the reference feature map by predicting offsets from predefined uniform points. This enables efficient cross-view feature fusion by attending only to a limited subset of spatial positions rather than the entire feature map. The core assumption is that corresponding regions across views contain complementary information that can improve detection when properly aligned and fused.

### Mechanism 2
The View-Interactive Transformer Decoder (VITD) with inter-attention layers captures relationships between objects detected in different views, improving lesion correspondence. The VITD processes fused features through self-attention within each view and inter-attention between views. The inter-attention layer uses query vectors from one view as keys and values for the other view, allowing the model to learn correspondence between detected objects across projections. The core assumption is that there are consistent relationships between objects detected in CC and MLO views that can be learned through attention mechanisms.

### Mechanism 3
The Fusion Pixel Decoder progressively fuses features at multiple scales, providing both high-resolution masks and multi-scale fused features for the VITD. The Fusion Pixel Decoder uses Fusion Layers at lower resolutions to combine CC and MLO features, then applies FPN-style element-wise addition to merge feature maps. At higher resolutions, independent 1×1 convolutions are used instead of Fusion Layers. The core assumption is that feature fusion is more beneficial at lower resolutions where spatial correspondence is easier to establish, while high-resolution features are better processed independently.

## Foundational Learning

- Concept: Deformable attention mechanism
  - Why needed here: Traditional cross-attention between high-resolution feature maps is computationally expensive. Deformable attention allows efficient sampling of relevant spatial positions across views without computing attention scores for all positions.
  - Quick check question: How does deformable attention differ from standard cross-attention in terms of computational complexity and the number of spatial positions considered?

- Concept: Transformer decoder architecture with masked attention
  - Why needed here: Masked attention allows the model to focus on spatial features restricted to the foreground area of predicted masks, which is crucial for accurate lesion segmentation in mammography images where lesions occupy small regions.
  - Quick check question: What is the purpose of the mask in masked attention, and how does it affect the attention computation?

- Concept: Multi-view medical imaging principles
  - Why needed here: Understanding the clinical rationale for dual-view mammography (CC and MLO projections) and how radiologists use both views together for diagnosis is essential for designing appropriate fusion mechanisms.
  - Quick check question: Why are both craniocaudal (CC) and mediolateral oblique (MLO) views necessary for mammography interpretation, and what complementary information does each provide?

## Architecture Onboarding

- Component map: Shared backbone (EfficientNet-b3) -> Fusion Pixel Decoder (Fusion Layers) -> View-Interactive Transformer Decoder (VITD) -> Lesion Linker -> Loss computation
- Critical path: Shared backbone → Fusion Pixel Decoder → VITD → Lesion Linker → Loss computation
- Design tradeoffs:
  - Using Fusion Layers only at lower resolutions balances computational efficiency with fusion benefits
  - Two separate object query branches allow the model to detect objects independently in each view while still enabling cross-view interaction
  - The choice of EfficientNet-b3 as backbone prioritizes speed and efficiency over potentially higher accuracy from larger backbones
- Failure signatures:
  - Degraded performance on single-view input indicates over-reliance on cross-view fusion
  - High false positive rates suggest fusion is not effectively filtering irrelevant detections
  - Poor correspondence between CC and MLO detections indicates inter-attention is not learning meaningful relationships
- First 3 experiments:
  1. Compare performance with Fusion Layers disabled (using only independent convolutions) to quantify the contribution of feature-level fusion
  2. Evaluate ablation of inter-attention in VITD to determine its impact on cross-view correspondence
  3. Test different numbers of Fusion Layers and their placement at various resolutions to optimize the fusion strategy

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but the following areas remain unexplored based on the content:

### Open Question 1
How does MAMM-Net's performance scale with different backbone architectures (e.g., EfficientNet variants) and how does this impact computational efficiency? The paper uses EfficientNet-b3 as backbone without exploring alternatives systematically. Systematic experiments comparing MAMM-Net with different backbone architectures showing performance-accuracy tradeoffs and computational complexity measurements would resolve this.

### Open Question 2
How does the Fusion Layer's performance change with different numbers of reference points and deformable attention parameters? The paper presents specific parameter choices without exploring their sensitivity or optimal values. Ablation studies varying the number of reference points, attention heads, and other Fusion Layer parameters while measuring impact on recall and false positive rates would resolve this.

### Open Question 3
How does MAMM-Net generalize to other medical imaging datasets beyond DDSM, particularly for different types of lesions or modalities? The paper only evaluates on DDSM dataset without testing generalization to other medical imaging tasks or datasets. Experiments applying MAMM-Net to other medical imaging datasets with different lesion types or imaging modalities would resolve this.

## Limitations
- Limited ablation studies isolating specific contributions of fusion mechanisms
- Reliance on dated DDSM dataset raises generalization concerns to modern mammography
- Insufficient exploration of parameter sensitivity for deformable attention and Fusion Layers

## Confidence
- High confidence: Overall architecture design and superiority over baseline methods (shown by improved recall metrics)
- Medium confidence: Specific contribution of deformable attention for cross-view feature fusion
- Low confidence: Clinical relevance of learned correspondences between CC and MLO views and whether these align with radiologist interpretation patterns

## Next Checks
1. Conduct controlled ablation studies removing Fusion Layers at different resolutions to quantify their individual contributions to performance gains
2. Evaluate model performance on a modern mammography dataset with higher image resolution and more diverse patient demographics
3. Perform interpretability analysis on the inter-attention weights to verify they capture clinically meaningful relationships between views rather than statistical artifacts