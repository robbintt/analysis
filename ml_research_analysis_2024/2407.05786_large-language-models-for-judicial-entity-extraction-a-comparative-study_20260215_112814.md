---
ver: rpa2
title: 'Large Language Models for Judicial Entity Extraction: A Comparative Study'
arxiv_id: '2407.05786'
source_url: https://arxiv.org/abs/2407.05786
tags:
- legal
- language
- entity
- gemma
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates four large language models (LLaMA 3, Mistral,
  Gemma, and Phi 3) for extracting judicial entities from Indian legal texts. It uses
  a zero-shot prompt-based approach without task-specific training.
---

# Large Language Models for Judicial Entity Extraction: A Comparative Study

## Quick Facts
- arXiv ID: 2407.05786
- Source URL: https://arxiv.org/abs/2407.05786
- Reference count: 1
- Four LLMs (LLaMA 3, Mistral, Gemma, Phi 3) evaluated for judicial entity extraction from Indian legal texts

## Executive Summary
This paper evaluates four large language models for extracting judicial entities from Indian legal texts using a zero-shot prompt-based approach. The study uses the InLegalNER dataset containing 14 entity types such as court, judge, lawyer, and petitioner. Mistral and Gemma achieved the highest F1 scores (0.6376 and 0.6353 respectively), demonstrating that LLMs can effectively handle specialized legal terminology without task-specific training.

## Method Summary
The study employed a zero-shot prompt-based approach where four LLMs (LLaMA 3, Mistral, Gemma, and Phi 3) were evaluated for named entity recognition in Indian legal documents. The InLegalNER dataset served as the evaluation benchmark, containing 14 different entity types. Models were tested without any task-specific fine-tuning, relying solely on their pre-trained capabilities to identify and extract judicial entities from legal text.

## Key Results
- Mistral achieved the highest F1 score of 0.6376 among all evaluated models
- Gemma followed closely with an F1 score of 0.6353
- LLaMA 3 and Phi 3 showed significantly lower performance compared to Mistral and Gemma
- All models demonstrated capability to handle specialized legal terminology in a zero-shot setting

## Why This Works (Mechanism)
The zero-shot capability of large language models allows them to leverage their pre-trained understanding of language patterns and contextual relationships. When presented with legal text, these models can identify entity boundaries and classify them based on their learned representations of how different entity types typically appear in text. The success with specialized legal terminology suggests that the models' pre-training captured sufficient domain-agnostic linguistic patterns to generalize to this specialized context without additional training.

## Foundational Learning
- Named Entity Recognition (NER): The task of identifying and classifying named entities in text; needed to understand the specific problem being solved, quick check: can identify basic entities like names and locations in news articles
- Zero-shot learning: Model's ability to perform tasks without task-specific training; needed to understand the experimental approach, quick check: can classify unseen categories in image datasets
- Legal terminology: Specialized vocabulary and language patterns in legal documents; needed to appreciate why this domain presents unique challenges, quick check: can distinguish between common words and their legal-specific meanings
- F1 score: Harmonic mean of precision and recall; needed to interpret model performance metrics, quick check: understand that 0.63 represents moderate performance
- Entity types: Specific categories of information to extract (court, judge, lawyer, etc.); needed to understand the task complexity, quick check: can list and differentiate between various entity categories

## Architecture Onboarding
**Component map:** Text input -> LLM inference -> Entity extraction -> Evaluation metrics
**Critical path:** Legal text document → Prompt formulation → LLM processing → Entity recognition output → Performance evaluation
**Design tradeoffs:** Zero-shot approach vs. fine-tuning (simplicity vs. potential performance), computational cost of large models vs. accuracy gains
**Failure signatures:** Inconsistent entity boundary detection, misclassification of similar entity types, inability to recognize domain-specific abbreviations or terminology
**3 first experiments:** 1) Test with simplified legal text containing only basic entities, 2) Evaluate on non-legal text to establish baseline performance, 3) Compare with a small fine-tuned model on the same dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-shot approach without fine-tuning limits performance ceiling
- Modest F1 scores (around 0.63-0.64) indicate room for significant improvement
- Evaluation restricted to Indian legal texts, limiting generalizability

## Confidence
- Claim: Mistral and Gemma outperform other LLMs in judicial entity extraction - High
- Claim: LLMs can effectively handle specialized legal terminology - Medium
- Claim: Zero-shot approach is sufficient for legal entity extraction - Low

## Next Checks
1. Conduct head-to-head comparison between zero-shot LLM performance and fine-tuned traditional NER models on the same InLegalNER dataset
2. Evaluate model performance across multiple legal domains (criminal, civil, corporate law) within the Indian legal system
3. Test model generalization by evaluating performance on legal texts from different jurisdictions (US, UK, or EU legal documents)