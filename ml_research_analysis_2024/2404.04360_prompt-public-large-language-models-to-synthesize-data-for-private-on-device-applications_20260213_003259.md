---
ver: rpa2
title: Prompt Public Large Language Models to Synthesize Data for Private On-device
  Applications
arxiv_id: '2404.04360'
source_url: https://arxiv.org/abs/2404.04360
tags:
- data
- arxiv
- private
- training
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using large language models (LLMs) to generate
  synthetic data that better matches the distribution of private user typing data
  on mobile devices, aiming to improve pre-training for on-device language models
  in a privacy-preserving way. The method involves carefully designed LLM prompts
  to filter and transform existing public data (C4), and to directly generate diverse
  chat-like data, all intended to resemble the target private domain.
---

# Prompt Public Large Language Models to Synthesize Data for Private On-device Applications

## Quick Facts
- arXiv ID: 2404.04360
- Source URL: https://arxiv.org/abs/2404.04360
- Reference count: 40
- Primary result: LLM-generated synthetic data improves on-device LM pre-training accuracy by 19-22% over C4 baseline

## Executive Summary
This paper addresses the challenge of improving on-device language models for mobile keyboards when only private user typing data is available for training. The authors propose using large language models to generate synthetic data that better matches the distribution of private mobile typing data, enabling effective server-side pre-training before privacy-preserving fine-tuning. By carefully designing LLM prompts to filter, transform, and generate data, they create a synthetic dataset that significantly reduces the distribution gap between public and private data. The approach is evaluated on real user data from US and India populations, demonstrating substantial improvements in next-word prediction accuracy and reduced privacy budget requirements during federated learning.

## Method Summary
The method involves a two-step process: first, using LLMs to generate synthetic data by filtering public C4 data, generating new chat-like data via chain-of-thought prompting, and transforming C4 articles into conversational formats. These synthetic datasets are then combined to create a proxy training set that better represents mobile typing patterns. Second, an on-device language model is pre-trained on this synthetic data using standard cross-entropy loss and Adam optimization, followed by differentially private federated learning fine-tuning on real user data using production FL systems with privacy-preserving techniques like clipping and noise addition.

## Key Results
- Pre-trained LM on synthetic data achieves 19.0% and 22.8% relative improvement in NWP accuracy for US and India populations respectively compared to C4 baseline
- The LLM-pretrained model outperforms baseline in DP FL fine-tuning rounds, requiring fewer rounds to reach target accuracy
- Production A/B testing shows the LLM-pretrained model maintains accuracy gains in real-world deployment
- Combining filtered C4 and synthetic chat data yields better results than either component alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM prompts can effectively filter and transform public data to better match private mobile typing distributions.
- Mechanism: Targeted prompts identify mobile-relevant content in public data and convert formal text to conversational formats.
- Core assumption: LLMs trained on public data understand mobile typing contexts.
- Evidence anchors: [abstract] carefully designed LLM prompts to filter and transform existing public data; [section] Table 1 shows LLM makes reasonable filtering decisions.
- Break condition: If LLM lacks domain knowledge about mobile chat contexts, filtering/transforming will not improve distribution alignment.

### Mechanism 2
- Claim: Directly generating synthetic chat data via chain-of-thought prompting increases diversity and resembles private data.
- Mechanism: Structured prompts with variables (age, gender, time, day, chat-app) and chain-of-thought generation create varied conversational examples.
- Core assumption: Generated conversations will cover vocabulary and patterns present in real mobile typing.
- Evidence anchors: [abstract] generate diverse chat data by chain-of-thought style prompting; [section] key challenge is ensuring generated data is diverse.
- Break condition: If generation lacks diversity or produces unnatural conversations, it won't match real typing patterns.

### Mechanism 3
- Claim: Combining filtered public data and synthetic chat data provides better pre-training distribution coverage than either alone.
- Mechanism: Filtered C4 retains public domain knowledge while synthetic chat adds mobile-specific patterns, creating a more representative proxy dataset.
- Core assumption: The target distribution requires both formal and conversational elements.
- Evidence anchors: [abstract] Combining the filtered C4 and the synthetic chat data gives the best pre-training dataset 'LLM-mix-166G'; [section] combine 19GB generated data and 10GB transformed C4 to obtain 29GB chat dataset.
- Break condition: If one component dominates the mix, the combined dataset may inherit its limitations.

## Foundational Learning

- Concept: Differentially Private Federated Learning
  - Why needed here: The final model must be trained on private user data while providing mathematical privacy guarantees.
  - Quick check question: What are the two main operations used to achieve DP in FL?

- Concept: Prompt Engineering for LLMs
  - Why needed here: Careful prompt design is essential for guiding LLMs to generate or filter data that matches the target distribution.
  - Quick check question: How does chain-of-thought prompting differ from direct prompting in this context?

- Concept: Distribution Matching in Pre-training
  - Why needed here: The closer the pre-training data distribution is to the target private data, the more efficiently privacy budget can be used during fine-tuning.
  - Quick check question: Why does distribution alignment reduce privacy budget requirements?

## Architecture Onboarding

- Component map: Public C4 → LLM filtering/transforming → Synthetic chat generation → Combined proxy dataset → Server-side pre-training → DP FL fine-tuning → On-device deployment
- Critical path: LLM prompt design → Data synthesis → Pre-training evaluation → DP FL fine-tuning → A/B testing
- Design tradeoffs:
  - Diversity vs. data quality in synthetic generation
  - Data size vs. privacy guarantees
  - Prompt complexity vs. generation consistency
- Failure signatures:
  - Low NWP accuracy after pre-training (poor distribution match)
  - Minimal improvement during DP FL fine-tuning (insufficient domain adaptation)
  - Negative A/B testing results (model doesn't improve user experience)
- First 3 experiments:
  1. Run federated evaluation comparing NWP accuracy for C4 vs. LLM-filtered C4
  2. Generate synthetic chat data and measure vocabulary coverage against real typing data
  3. Train LM on combined dataset and evaluate on holdout mobile typing data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop practical privacy-preserving methods to measure the distribution similarity between server-side data and private on-device data?
- Basis in paper: [explicit] Important future work direction in Section 4.1, noting that evaluating generated data quality is challenging when target domain is private.
- Why unresolved: Current methods like FreD require careful privacy budget allocation in federated learning, complicating production implementation.
- What evidence would resolve it: Development and validation of new privacy-preserving metric that accurately measures distribution similarity without accessing private data.

### Open Question 2
- Question: Does incorporating country/region information into LLM prompts improve the quality of synthesized data for different populations?
- Basis in paper: [inferred] Section 4.2 notes LLM synthetic data was more effective for en-US than en-IN, suggesting potential improvements by including location-specific information.
- Why unresolved: Only experimented with English language data without testing prompts with country/region variables.
- What evidence would resolve it: Comparative experiments showing improved NWP accuracy when country-specific prompts are used for different populations.

### Open Question 3
- Question: What sampling methods beyond top-k sampling with fixed temperature could improve the diversity of synthetic chat data?
- Basis in paper: [explicit] Section 3.2 mentions due to resource limitations, fixed temperature was chosen and exploration of other sampling parameters or methods is left as future work.
- Why unresolved: Paper only used basic top-k sampling with temperature 0.2, leaving potential improvements unexplored.
- What evidence would resolve it: Experiments comparing different sampling strategies (nucleus sampling, temperature scheduling, etc.) showing improved vocabulary coverage and evaluation accuracy.

## Limitations

- Evaluation primarily conducted in controlled research environment rather than real-world deployment conditions
- Privacy budget consumed during DP FL fine-tuning phase is not explicitly reported
- Scalability to other languages and domains beyond English mobile typing is not explored

## Confidence

- **High Confidence**: Core claim that LLM-generated synthetic data improves on-device LM pre-training is well-supported by experimental results
- **Medium Confidence**: Effectiveness of specific prompt engineering techniques is supported by qualitative examples and ablation studies
- **Low Confidence**: Long-term stability and generalization of models trained on synthetic data is not thoroughly evaluated

## Next Checks

1. Conduct detailed analysis of privacy budget consumed during entire pipeline (LLM generation, pre-training, and DP FL fine-tuning)
2. Systematically vary LLM prompts and measure impact on quality and diversity of generated synthetic data
3. Track performance of models trained on synthetic data over extended periods to assess adaptation to changing user typing patterns