---
ver: rpa2
title: Partially Recentralization Softmax Loss for Vision-Language Models Robustness
arxiv_id: '2402.03627'
source_url: https://arxiv.org/abs/2402.03627
tags:
- arxiv
- adversarial
- robustness
- language
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of vision-language models
  (VLMs) to adversarial attacks. The proposed method modifies the loss function to
  restrict top-K softmax outputs during fine-tuning, thereby improving model robustness.
---

# Partially Recentralization Softmax Loss for Vision-Language Models Robustness

## Quick Facts
- **arXiv ID**: 2402.03627
- **Source URL**: https://arxiv.org/abs/2402.03627
- **Reference count**: 40
- **Primary result**: PRSL improves VLM robustness against BIM attacks while maintaining baseline performance (BERTScore ~0.935-0.945)

## Executive Summary
This paper addresses the vulnerability of vision-language models (VLMs) to adversarial attacks by proposing a novel loss function modification. The Partially Recentralization Softmax Loss (PRSL) restricts top-K softmax outputs during fine-tuning to improve model robustness while maintaining performance on clean data. Experiments on ViT-GPT2 (image captioning) and Blip2-OPT (VQA) models demonstrate that PRSL significantly reduces performance degradation under BIM adversarial attacks across multiple evaluation metrics including BERTScore, CIDEr, ROUGE, BLEU, and METEOR.

## Method Summary
The proposed method modifies the standard cross-entropy loss by incorporating a mechanism that restricts top-K softmax outputs during training. This approach, termed Partially Recentralization Softmax Loss (PRSL), introduces an additional hyperparameter K that controls the number of top softmax outputs to be recentralized. During fine-tuning, PRSL adjusts the probability distribution of the top-K predicted tokens to prevent overconfident predictions that make models vulnerable to adversarial attacks. The loss function balances between standard cross-entropy for overall prediction accuracy and the recentralization term that specifically targets the most confident predictions.

## Key Results
- PRSL significantly improves robustness against BIM adversarial attacks on both image captioning (ViT-GPT2) and VQA (Blip2-OPT) tasks
- Models fine-tuned with PRSL show consistent score degradation reduction across all evaluation metrics (BERTScore, CIDEr, ROUGE, BLEU, METEOR)
- Performance on clean data remains similar to baseline models (BERTScore ~0.935-0.945)
- The improvement in robustness is achieved without significant trade-off in standard performance metrics

## Why This Works (Mechanism)
The PRSL mechanism works by preventing the model from becoming overconfident in its top predictions during training. By restricting the top-K softmax outputs, the model learns to maintain more balanced probability distributions rather than concentrating probability mass on single predictions. This makes the model less sensitive to small input perturbations that adversarial attacks exploit, as the model is trained to avoid extreme confidence values that are easily shifted by noise.

## Foundational Learning
**Vision-Language Models (VLMs)**: Multimodal models that process both visual and textual inputs, typically using separate encoders for images and text combined with a decoder or multimodal transformer. Why needed: Understanding VLM architecture is crucial as PRSL is specifically designed for these multimodal models. Quick check: Verify the model uses separate image and text encoders with a multimodal fusion component.

**Adversarial Attacks**: Techniques that generate small, imperceptible perturbations to input data to cause model misclassification or degraded performance. Why needed: PRSL is designed specifically to defend against these attacks, particularly BIM (Basic Iterative Method). Quick check: Confirm attack method uses iterative gradient-based perturbations.

**BIM (Basic Iterative Method)**: An iterative adversarial attack that applies small perturbations in the direction of the gradient multiple times. Why needed: The paper's experiments focus specifically on BIM attacks, so understanding this attack type is essential. Quick check: Verify the attack uses epsilon-stepped gradient ascent over multiple iterations.

**Softmax Function**: Converts raw model outputs (logits) into probability distributions that sum to 1. Why needed: PRSL modifies the softmax outputs during training, making understanding this function critical. Quick check: Confirm the model uses standard softmax on final layer logits before loss computation.

**Cross-Entropy Loss**: Standard loss function for classification that measures the difference between predicted probability distributions and true labels. Why needed: PRSL builds upon cross-entropy by adding a recentralization term. Quick check: Verify the base loss is cross-entropy before PRSL modifications.

## Architecture Onboarding

**Component Map**: Image Encoder -> Text Encoder -> Multimodal Fusion -> Decoder -> PRSL Loss (instead of Cross-Entropy Loss)

**Critical Path**: During training, the critical path involves: forward pass through both encoders, multimodal fusion, decoder output generation, softmax computation, PRSL loss calculation with top-K recentralization, and backpropagation through all components.

**Design Tradeoffs**: The main tradeoff is between robustness and model confidence. PRSL reduces overconfident predictions which improves adversarial robustness but may slightly reduce model certainty on clean data. The top-K hyperparameter controls this tradeoff - higher K values maintain more confidence but may reduce robustness benefits.

**Failure Signatures**: Models trained with PRSL may show slightly more uniform probability distributions across predictions, potentially leading to less sharp outputs. The top-K parameter must be carefully tuned - too small values may overly suppress confident predictions, while too large values may not provide sufficient robustness benefits.

**3 First Experiments**:
1. Train baseline VLM with standard cross-entropy loss and measure performance on clean and BIM-attacked data
2. Train VLM with PRSL using different top-K values (K=1, K=3, K=5) and compare robustness across all values
3. Evaluate PRSL-trained models on clean data to confirm baseline performance is maintained while robustness improves

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on BIM adversarial attacks and does not evaluate robustness against other attack types like PGD or FGSM
- Evaluation is limited to two specific VLM architectures (ViT-GPT2 and Blip2-OPT) and two tasks, which may not generalize to other multimodal architectures
- The proposed PRSL introduces an additional hyperparameter (top-K) that requires careful tuning without systematic sensitivity analysis
- The improvement is measured only through fixed epsilon values for perturbations without exploring the trade-off between perturbation magnitude and robustness

## Confidence
- **High confidence**: PRSL improves robustness against BIM attacks - well-supported by experimental results showing consistent score degradation reduction
- **Medium confidence**: Performance on clean data remains similar to baseline - supported but would benefit from statistical significance testing
- **Medium confidence**: General effectiveness of top-K softmax restriction for robustness - plausible but needs validation on diverse attack types and architectures

## Next Checks
1. Evaluate PRSL's effectiveness against multiple adversarial attack types (PGD, FGSM, optimization-based attacks) to establish broader robustness claims beyond BIM attacks
2. Conduct systematic hyperparameter sensitivity analysis for the top-K parameter across different model architectures and tasks to determine optimal configurations and generalization patterns
3. Test PRSL on additional VLM architectures (e.g., LLaVA, BLIP-2 with different backbones) and diverse downstream tasks (e.g., visual reasoning, image-text retrieval) to assess generalizability