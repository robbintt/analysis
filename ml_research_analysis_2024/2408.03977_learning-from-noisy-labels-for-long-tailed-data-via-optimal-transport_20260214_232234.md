---
ver: rpa2
title: Learning from Noisy Labels for Long-tailed Data via Optimal Transport
arxiv_id: '2408.03977'
source_url: https://arxiv.org/abs/2408.03977
tags:
- noisy
- long-tailed
- learning
- labels
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning from noisy labels
  in long-tailed data distributions, a common challenge in real-world scenarios. The
  authors propose a novel approach called OTLNL (Optimal Transport Learning from Noisy
  Labels) that combines dynamic loss-distance cross-selection and optimal transport
  strategies.
---

# Learning from Noisy Labels for Long-tailed Data via Optimal Transport

## Quick Facts
- arXiv ID: 2408.03977
- Source URL: https://arxiv.org/abs/2408.03977
- Reference count: 35
- Primary result: OTLNL method outperforms state-of-the-art by up to 6.53% accuracy on CIFAR-10 and 0.87% on WebVision under high noise and imbalance conditions

## Executive Summary
This paper addresses the challenging problem of learning from noisy labels in long-tailed data distributions, a common scenario in real-world applications. The authors propose OTLNL (Optimal Transport Learning from Noisy Labels), a novel method that combines dynamic loss-distance cross-selection with optimal transport strategies. The approach first filters clean samples using a class-specific threshold mechanism that integrates class predictions and feature distributions, then employs optimal transport to generate high-quality pseudo-labels for the noisy set in a semi-supervised training manner. Extensive experiments on synthetic and real-world datasets demonstrate significant performance improvements over existing methods, particularly in high noise and imbalance scenarios.

## Method Summary
The OTLNL method consists of two main components: a dynamic loss-distance cross-selection module and an optimal transport-based pseudo-label generation strategy. The cross-selection module filters clean samples by computing class-specific thresholds based on average prediction probabilities, dynamically adjusting them per epoch using a layback ratio λ. This reduces the impact of long-tailed distributions on sample selection. The optimal transport strategy then generates pseudo-labels for the noisy set by aligning the distribution of pseudo-labeled samples with the labeled clean set, using class centroids to mitigate sample scarcity in tail classes. The method is trained using semi-supervised learning with contrastive loss.

## Key Results
- Achieves up to 6.53% improvement in test accuracy on CIFAR-10 compared to state-of-the-art methods
- Demonstrates 0.87% improvement on WebVision dataset under high noise and imbalance ratios
- Shows consistent performance gains across multiple noise levels (40% to 80%) and imbalance ratios (50 to 100)

## Why This Works (Mechanism)

### Mechanism 1
Dynamic class-specific thresholds in the loss-distance cross-selection module reduce overlap between clean tail-class and noisy head-class samples by computing thresholds based on average prediction probabilities of each class, dynamically adjusted per epoch using a layback ratio λ.

### Mechanism 2
Optimal transport strategies improve pseudo-label quality for the noisy set in the semi-supervised phase by aligning the distribution of pseudo-labeled samples with the labeled clean set using class centroids to mitigate sample scarcity in tail classes.

## Foundational Learning

### Long-tailed Distribution
- Why needed: Real-world datasets often have highly imbalanced class distributions where head classes have many more samples than tail classes
- Quick check: Verify dataset statistics show imbalance ratio > 50

### Noisy Label Learning
- Why needed: Labels in real-world datasets frequently contain errors that can degrade model performance if not properly handled
- Quick check: Confirm noise injection mechanism creates class-dependent label corruption

### Optimal Transport
- Why needed: Provides a principled way to align distributions when sample scarcity exists, particularly useful for tail classes
- Quick check: Validate that transport cost matrix properly accounts for class relationships

## Architecture Onboarding

### Component Map
Feature Extractor -> Loss-Distance Cross-Selection -> Clean Sample Filter -> Optimal Transport Module -> Pseudo-Label Generator -> Semi-Supervised Trainer

### Critical Path
Cross-selection filtering → Clean set formation → Optimal transport pseudo-labeling → Semi-supervised training with contrastive loss

### Design Tradeoffs
- Dynamic thresholds vs. fixed thresholds: Dynamic adaptation improves robustness to distribution shifts but adds computational overhead
- Class centroids vs. individual samples: Using centroids reduces computational complexity but may lose fine-grained sample information

### Failure Signatures
- Low tail class F1-scores indicate incorrect implementation of dynamic threshold or distance calculation
- Degraded semi-supervised learning performance suggests issues with optimal transport plan calculation or consistency loss implementation

### First Experiments
1. Verify cross-selection module correctly separates clean samples from noisy ones at different noise levels
2. Test optimal transport alignment quality by measuring distribution divergence between clean and pseudo-labeled sets
3. Evaluate semi-supervised training convergence with contrastive loss on filtered dataset

## Open Questions the Paper Calls Out
- How does the optimal transport strategy for pseudo-label generation scale with the number of classes in extremely large-scale datasets?
- How robust is the method to variations in the imbalance ratio within individual classes?
- What is the impact of different noise types (e.g., class-dependent vs. instance-dependent) on the method's performance?
- How does the method perform when applied to other tasks beyond image classification?

## Limitations
- Computational complexity of optimal transport component not discussed, potentially limiting scalability
- Limited evaluation on extremely long-tailed distributions with imbalance ratios > 100
- Lack of detailed ablation studies on individual component contributions

## Confidence

**High Confidence:** Core claim of outperforming state-of-the-art methods on benchmark datasets is well-supported by experimental results across multiple datasets and noise levels.

**Medium Confidence:** Dynamic class-specific thresholds effectively reducing overlap between clean tail-class and noisy head-class samples is theoretically sound but lacks comprehensive empirical validation.

**Low Confidence:** Optimal transport strategies significantly improving pseudo-label quality for tail classes needs more rigorous validation, as limited evidence is provided on how the mechanism specifically addresses sample scarcity in extreme long-tailed scenarios.

## Next Checks

1. Conduct detailed ablation study to isolate individual contributions of dynamic loss-distance cross-selection and optimal transport strategy to overall performance improvements.

2. Evaluate computational complexity and memory requirements of optimal transport component when scaling to larger datasets with more classes.

3. Test method's robustness on datasets with more severe long-tailed distributions (imbalance ratios > 100) to determine limits of the proposed approach.