---
ver: rpa2
title: 'DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging'
arxiv_id: '2407.01470'
source_url: https://arxiv.org/abs/2407.01470
tags:
- reward
- merging
- math
- code
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DogeRM, a framework that integrates domain-specific
  knowledge into reward models (RMs) through model merging. The method merges a general
  RM trained on preference data with a domain-specific supervised fine-tuned (SFT)
  language model, eliminating the need for costly domain-specific preference data.
---

# DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging

## Quick Facts
- arXiv ID: 2407.01470
- Source URL: https://arxiv.org/abs/2407.01470
- Authors: Tzu-Han Lin; Chen-An Li; Hung-yi Lee; Yun-Nung Chen
- Reference count: 40
- Key outcome: Achieves up to 17% accuracy improvement in math domains and 6% in coding domains by merging general reward models with domain-specific SFT models

## Executive Summary
DogeRM introduces a novel approach to enhance reward models with domain-specific knowledge through model merging, eliminating the need for expensive domain-specific preference data. By combining a general reward model trained on preference data with a domain-specific supervised fine-tuned language model, DogeRM achieves significant performance improvements in specialized domains like math and coding. The framework demonstrates that domain knowledge can be effectively transferred to reward models without requiring additional preference data collection.

## Method Summary
DogeRM employs a model merging technique that combines a general reward model (trained on preference data) with a domain-specific supervised fine-tuned (SFT) language model. The merging process leverages low-rank adaptation (LoRA) components from the SFT model to augment the general reward model's capabilities in specific domains. This approach allows the reward model to benefit from domain-specific knowledge encoded in the SFT model without requiring additional preference data collection for the target domain. The framework is designed to be architecture-agnostic and can be applied to various reward model and SFT model combinations.

## Key Results
- Achieves up to 17% accuracy improvement on math domains compared to general reward models
- Demonstrates 6% accuracy gains in coding domains on benchmark tests
- Shows significant improvements in Best-of-N sampling performance on GSM8K and MBPP datasets
- Validated across multiple model architectures, confirming generalizability

## Why This Works (Mechanism)
The effectiveness of DogeRM stems from leveraging the complementary strengths of general reward models and domain-specific SFT models. General reward models capture broad preference patterns across diverse tasks, while SFT models encode specialized domain knowledge. By merging these models, DogeRM combines the preference discrimination capabilities of reward models with the domain expertise of SFT models. The use of LoRA components enables efficient knowledge transfer without requiring full model fine-tuning, making the approach computationally practical while preserving the general reward model's ability to discriminate between preferred and non-preferred outputs.

## Foundational Learning

**Reward Models**: ML models that predict human preferences between text pairs; needed because they guide RLHF training; quick check: can distinguish preferred from dispreferred responses

**Supervised Fine-Tuning (SFT)**: Training language models on high-quality instruction-response pairs; needed because it encodes domain expertise; quick check: generates accurate domain-specific responses

**LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning method using low-rank matrices; needed because it enables efficient knowledge transfer; quick check: can merge LoRA components between models

**Model Merging**: Combining different models' weights or adapters; needed because it transfers knowledge without full retraining; quick check: merged model retains both sources' capabilities

**RLHF (Reinforcement Learning from Human Feedback)**: Training method using reward models; needed because it produces aligned language models; quick check: improves response quality based on human preferences

## Architecture Onboarding

**Component Map**: General RM -> LoRA Merger -> DogeRM (merged) -> Evaluation -> Benchmark Performance

**Critical Path**: Domain-specific SFT model → LoRA component extraction → General RM merging → Reward model fine-tuning → Evaluation

**Design Tradeoffs**: Computational efficiency vs. knowledge completeness; general capability preservation vs. domain specialization; merging complexity vs. performance gains

**Failure Signatures**: Degraded general performance after merging; domain knowledge conflicts; merging instability; preference discrimination loss

**First 3 Experiments**:
1. Merge math SFT with general reward model and evaluate on math benchmarks
2. Test merging stability across multiple random seeds
3. Compare DogeRM performance against full fine-tuning approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focused on math and coding domains, with limited assessment of other specialized domains
- Does not address potential knowledge conflicts when merging models with incompatible domain-specific knowledge
- Long-term stability and performance degradation during extended RLHF training are not investigated

## Confidence

**High confidence**: The core merging methodology and its effectiveness in improving reward model performance for math and coding tasks

**Medium confidence**: Generalizability claims across different model architectures and domains beyond the tested scenarios

**Medium confidence**: The assertion that no domain-specific preference data is needed, as some degree of preference data may still be beneficial for optimal performance

## Next Checks
1. Evaluate DogeRM on additional domains (e.g., legal reasoning, medical diagnosis, creative writing) to test generalizability
2. Conduct ablation studies to determine the minimum quality and quantity requirements for the domain-specific SFT model
3. Test the merged reward model's performance after multiple rounds of RLHF training to assess stability and potential knowledge drift