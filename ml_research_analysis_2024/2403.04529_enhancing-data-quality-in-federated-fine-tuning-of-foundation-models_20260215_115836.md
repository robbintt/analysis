---
ver: rpa2
title: Enhancing Data Quality in Federated Fine-Tuning of Foundation Models
arxiv_id: '2403.04529'
source_url: https://arxiv.org/abs/2403.04529
tags:
- data
- quality
- training
- federated
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of data quality control in federated
  fine-tuning of large language models (LLMs), particularly when relying on private
  domain data sources. The authors propose a data quality control pipeline that computes
  scores reflecting the quality of training data and determines a global threshold
  for a unified standard.
---

# Enhancing Data Quality in Federated Fine-Tuning of Foundation Models

## Quick Facts
- **arXiv ID**: 2403.04529
- **Source URL**: https://arxiv.org/abs/2403.04529
- **Reference count**: 40
- **Primary result**: Data quality control pipeline using scoring functions and anchor-based thresholds improves federated LLM fine-tuning performance

## Executive Summary
This paper addresses the challenge of data quality control in federated fine-tuning of large language models when relying on private domain data sources. The authors propose a pipeline that computes quality scores for training samples using data valuation algorithms and establishes a unified global threshold using anchor data. Experiments on medical question-answering tasks demonstrate that this approach leads to better performance compared to models trained on low-quality datasets, with the global model outperforming oracle models in both centralized and federated settings, including Non-IID scenarios.

## Method Summary
The proposed method consists of two phases: Phase I (Pre-training) involves computing sample quality scores using scoring functions (perplexity, conditional probability, influence functions) and determining a global threshold using a minimal set of anchor data. Phase II (Federated Learning) filters out low-quality data based on the threshold and conducts federated training on high-quality data. The pipeline is evaluated using PMC-LLama and Medalpaca-flashcards datasets with LLaMA2-7b base model, comparing performance against models trained on low-quality datasets and oracle models trained only on high-quality data.

## Key Results
- Proposed quality control pipeline leads to better performance compared to models trained on low-quality datasets
- Using conditional probability and in-context learning as scoring methods, the global model outperforms models trained with oracle set in both centralized and federated settings
- Quality control pipeline demonstrates effectiveness in Non-IID federated learning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Data valuation algorithms score individual training samples to enable fine-grained quality control
- **Core assumption**: Scoring function accurately reflects true utility of samples to global model
- **Evidence**: Scoring methods (perplexity, conditional probability, influence functions) enable evaluation of individual sample quality
- **Break condition**: Misclassification of high-quality samples as low-quality degrades model performance

### Mechanism 2
- **Claim**: Anchor data establishes unified quality standard across heterogeneous clients
- **Core assumption**: Anchor data is representative enough to set fair global standard
- **Evidence**: Server selects minimal anchor set and broadcasts average score as quality threshold
- **Break condition**: Non-representative anchor data leads to inappropriate thresholds

### Mechanism 3
- **Claim**: Filtering low-quality data before federated training improves global model performance
- **Core assumption**: Filtered dataset retains sufficient diversity and quantity for effective learning
- **Evidence**: Models trained with quality control outperform those trained on low-quality data
- **Break condition**: Excessive filtering results in dataset that's too small or unrepresentative

## Foundational Learning

- **Data valuation and influence functions**: Needed to quantify contribution of individual training samples to model performance. *Quick check*: What's the difference between perplexity and influence functions as scoring methods?
- **Federated learning with heterogeneous data**: Needed to understand challenges of data quality variation across clients. *Quick check*: How does Non-IID data distribution affect federated learning performance?
- **Anchor data selection and threshold setting**: Needed to create unified quality standard across diverse clients. *Quick check*: Why is using minimal anchor set more practical than aggregating all client scores?

## Architecture Onboarding

- **Component map**: Client-side (data scoring, local filtering) → Server-side (anchor selection, threshold computation, aggregation)
- **Critical path**: Scoring → Threshold setting → Data filtering → Federated training
- **Design tradeoffs**: More anchor data = more accurate threshold but higher communication cost; stricter threshold = higher quality but less diversity; complex scoring = better assessment but higher computational cost
- **Failure signatures**: Performance drops despite filtering (scoring misclassifies); some clients show poor performance (threshold unsuitable); communication overhead increases (too many anchor samples)
- **First 3 experiments**:
  1. Test scoring functions on small dataset to verify they identify low-quality samples correctly
  2. Run with different numbers of anchor data samples to find optimal balance between accuracy and cost
  3. Compare performance with and without data filtering in federated setting with controlled low-quality data

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does performance of federated fine-tuning with data quality control compare to centralized fine-tuning with oracle high-quality data?
- **Basis**: Paper compares proposed pipeline against models trained on low-quality datasets and oracle high-quality data
- **Unresolved**: Comprehensive comparison across all settings and metrics not provided
- **Evidence needed**: Detailed comparison of performance metrics across centralized, federated IID, and federated Non-IID settings

### Open Question 2
- **Question**: How do different data valuation algorithms perform in data quality control for federated fine-tuning?
- **Basis**: Paper mentions using Perplexity, Conditional Probability, Influence Functions as scoring methods
- **Unresolved**: Comprehensive analysis of relative performance and effectiveness not provided
- **Evidence needed**: Detailed analysis of each algorithm's impact on model performance, efficiency, and privacy preservation

### Open Question 3
- **Question**: How does pipeline perform in complex federated learning scenarios with Non-IID distributions and varying client participation rates?
- **Basis**: Paper mentions experiments in federated IID and Non-IID settings
- **Unresolved**: Pipeline's performance under challenging conditions not extensively explored
- **Evidence needed**: Experiments with highly skewed distributions, varying participation rates, and different heterogeneity levels

## Limitations

- **Scoring function dependence**: Methods may not perfectly align with downstream task performance and sensitivity to domain shifts unclear
- **Anchor-based threshold assumptions**: Small representative set may not fairly standardize quality across significantly heterogeneous clients
- **Computational overhead**: Data valuation algorithms like influence functions could be prohibitive for very large datasets

## Confidence

- **High confidence**: Core finding that data quality filtering improves federated learning performance is well-supported by experimental results
- **Medium confidence**: Effectiveness of specific scoring functions demonstrated but generalizability requires further validation
- **Medium confidence**: Anchor-based threshold mechanism provides practical solution but robustness to extreme heterogeneity untested

## Next Checks

1. **Cross-domain validation**: Test pipeline on non-medical datasets to assess generalizability of scoring functions and threshold mechanisms
2. **Scalability assessment**: Evaluate computational overhead of scoring methods on datasets with 10x more samples
3. **Threshold sensitivity analysis**: Systematically vary anchor set size and composition to quantify impact on performance across different heterogeneity levels