---
ver: rpa2
title: Machine Unlearning in Large Language Models
arxiv_id: '2405.15152'
source_url: https://arxiv.org/abs/2405.15152
tags:
- unlearning
- language
- dataset
- harmful
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a gradient ascent-based methodology for machine
  unlearning in large language models (LLMs), addressing the challenge of selectively
  forgetting harmful or copyrighted content while preserving general knowledge. The
  approach targets harmful responses by applying gradient ascent on the PKU dataset,
  achieving a 75% reduction in harmful outputs for OPT1.3b and OPT2.7b models, while
  maintaining knowledge retention via the TruthfulQA dataset.
---

# Machine Unlearning in Large Language Models

## Quick Facts
- arXiv ID: 2405.15152
- Source URL: https://arxiv.org/abs/2405.15152
- Authors: Saaketh Koundinya Gundavarapu; Shreya Agarwal; Arushi Arora; Chandana Thimmalapura Jagadeeshaiah
- Reference count: 6
- Primary result: Achieves 75% reduction in harmful responses while preserving general knowledge through gradient ascent unlearning

## Executive Summary
This paper introduces a gradient ascent-based methodology for machine unlearning in large language models (LLMs), addressing the challenge of selectively forgetting harmful or copyrighted content while preserving general knowledge. The approach targets harmful responses by applying gradient ascent on the PKU dataset, achieving a 75% reduction in harmful outputs for OPT1.3b and OPT2.7b models, while maintaining knowledge retention via the TruthfulQA dataset. For copyrighted content, the authors construct a Lord of the Rings-based dataset, fine-tune models using LoRA, and apply gradient ascent to effectively unlearn the copyrighted material, reducing its presence significantly. A novel evaluation technique is proposed, using a trained classifier to quantitatively measure unlearning effectiveness. The experiments demonstrate that the unlearning mechanism successfully mitigates the impact of harmful and copyrighted prompts while preserving model performance on benign inputs.

## Method Summary
The method employs gradient ascent optimization to selectively unlearn harmful or copyrighted content from LLMs. The approach uses a weighted combination of losses: gradient ascent loss (Lfgt) on forget dataset samples, random output loss (Lrdn) to reinforce forgetting, and KL divergence loss (Lnor) to preserve normal behavior from the normal dataset. For copyrighted content, LoRA fine-tuning is first applied to align models with the target content before unlearning. A trained classifier evaluates unlearning effectiveness by measuring the reduction in classification accuracy on unlearned model outputs compared to original outputs.

## Key Results
- Achieves 75% reduction in harmful responses for OPT1.3b and OPT2.7b models
- Successfully reduces copyrighted content presence using Lord of the Rings corpus
- Maintains general knowledge retention through TruthfulQA and Book Corpus datasets
- Introduces novel classifier-based evaluation method for quantitative unlearning assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient ascent selectively reduces harmful and copyrighted content by increasing loss on targeted samples while preserving general knowledge.
- Mechanism: The unlearning algorithm updates model parameters using a weighted combination of losses: gradient ascent loss (Lfgt) on forget dataset samples, random output loss (Lrdn) to reinforce forgetting, and KL divergence loss (Lnor) to preserve normal behavior from the normal dataset.
- Core assumption: The forget dataset contains representative examples of harmful/copyrighted content that the model needs to unlearn, and the normal dataset contains sufficient benign content to maintain general knowledge.
- Evidence anchors:
  - [abstract] "achieving a 75% reduction in harmful responses for Open Pre-trained Transformer Language Models (OPT1.3b and OPT2.7b) Zhang et al. [2022] while retaining previous knowledge using the TruthfulQA dataset Lin et al. [2021]."
  - [section 3.1] "The update formula is expressed as follows: θt+1 ← θt − ϵ1 · ∇θt Lfgt − ϵ2 · ∇θt Lrdn − ϵ3 · ∇θt Lnor"
- Break condition: If the forget dataset is incomplete or biased, the unlearning may fail to target all harmful/copyrighted content effectively.

### Mechanism 2
- Claim: LoRA fine-tuning enables efficient adaptation of large language models for unlearning specific content without full fine-tuning.
- Mechanism: LoRA introduces low-rank matrices to approximate weight updates during fine-tuning, reducing computational cost while allowing the model to learn new behaviors (like copyright alignment) and then unlearn them.
- Core assumption: The low-rank approximation maintains sufficient model capacity to learn and unlearn specific content without degrading overall performance.
- Evidence anchors:
  - [abstract] "For handling copyrighted content, we constructed a custom dataset based on the Lord of the Rings corpus and aligned LLMs (OPT1.3b and OPT2.7b) Zhang et al. [2022] through LoRA: Low-Rank Adaptation of Large Language Models Hu et al. [2021] finetuning."
  - [section 3.1] "Let Yrdn be a set of random (non-harmful) responses unrelated to unlearned prompts xfgt, constructed by gathering irrelevant responses from the normal dataset."
- Break condition: If the low-rank approximation is too restrictive, the model may not learn the targeted content sufficiently before unlearning.

### Mechanism 3
- Claim: The novel evaluation method using a trained classifier provides quantitative measurement of unlearning effectiveness by comparing classifier accuracy on original vs. unlearned model outputs.
- Mechanism: A text classifier is trained on the forget dataset, then applied to evaluate model outputs before and after unlearning. The reduction in classification accuracy indicates successful unlearning.
- Core assumption: The classifier accurately captures the relevant features of harmful/copyrighted content and generalizes to model outputs.
- Evidence anchors:
  - [abstract] "Additionally, we propose a new evaluation technique for assessing the effectiveness of harmful unlearning. Initially, we train a classifier to determine if a given text is harmful."
  - [section 3.2] "The effectiveness of our unlearning process can be quantified using the reduction in the classifier's accuracy when applied to the unlearned model's responses."
- Break condition: If the classifier is not representative of the true distribution of harmful/copyrighted content, the evaluation metric may be misleading.

## Foundational Learning

- Concept: Gradient descent and ascent optimization
  - Why needed here: Understanding how gradient ascent differs from gradient descent is crucial for implementing the unlearning algorithm that increases loss on targeted content.
  - Quick check question: How does gradient ascent differ from gradient descent in terms of the direction of parameter updates?

- Concept: KL divergence and its role in preserving knowledge
  - Why needed here: KL divergence loss (Lnor) is used to maintain the model's normal behavior during unlearning, preventing catastrophic forgetting.
  - Quick check question: What does KL divergence measure between two probability distributions, and how does it help preserve knowledge?

- Concept: Low-rank adaptation (LoRA) principles
  - Why needed here: LoRA is used for efficient fine-tuning before unlearning, so understanding its mechanism helps in implementing and debugging the approach.
  - Quick check question: How does LoRA reduce computational cost compared to full fine-tuning, and what are its limitations?

## Architecture Onboarding

- Component map:
  - Datasets: PKU (harmful), Lord of the Rings (copyrighted), TruthfulQA (normal), Book Corpus (diverse knowledge)
  - Models: OPT-1.3b, OPT-2.7b, BERT classifier
  - Training pipeline: Fine-tuning → Unlearning → Evaluation
  - Evaluation: Classifier accuracy reduction, BLEU score for copyright

- Critical path:
  1. Prepare forget and normal datasets
  2. Fine-tune model with LoRA on copyright dataset (if applicable)
  3. Apply gradient ascent unlearning algorithm
  4. Evaluate using classifier and BLEU metrics

- Design tradeoffs:
  - LoRA vs. full fine-tuning: LoRA is more efficient but may limit learning capacity
  - Classifier-based evaluation: Provides quantitative measure but depends on classifier quality
  - Random output loss (Lrdn): Reinforces forgetting but may introduce noise

- Failure signatures:
  - Classifier accuracy doesn't decrease after unlearning: Unlearning not effective
  - BLEU score remains high after unlearning: Copyright content still present
  - Normal dataset loss increases significantly: Unlearning affecting general knowledge

- First 3 experiments:
  1. Test gradient ascent unlearning on a small subset of harmful data with a simple classifier
  2. Evaluate LoRA fine-tuning effectiveness on copyright data before unlearning
  3. Measure impact of unlearning on normal dataset performance to check for catastrophic forgetting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of gradient ascent compare to other unlearning techniques like RLHF in terms of computational efficiency and alignment performance?
- Basis in paper: [explicit] The paper mentions that RLHF requires resources and time equivalent to training an LLM, while gradient ascent only requires around 2% of the computing power.
- Why unresolved: The paper does not provide a direct comparison of the effectiveness of gradient ascent versus RLHF in terms of alignment performance or computational efficiency.
- What evidence would resolve it: Conducting a comparative study between gradient ascent and RLHF on the same datasets and metrics would provide insights into their relative effectiveness.

### Open Question 2
- Question: What is the impact of using different optimizers (e.g., Adam, Adagrad) on the unlearning process and its outcomes?
- Basis in paper: [inferred] The paper mentions that they used 8-bit Adam and AdamW but found no significant differences. However, they did not explore other optimizers.
- Why unresolved: The paper does not investigate the effects of using different optimizers on the unlearning process and its outcomes.
- What evidence would resolve it: Experimenting with various optimizers and comparing their performance in the unlearning process would provide insights into their impact.

### Open Question 3
- Question: How does the unlearning of a specific concept affect the model's performance on other related concepts?
- Basis in paper: [inferred] The paper mentions that the unlearning mechanism shows promise in mitigating the impact of harmful and copyrighted prompts while preserving the model's performance on benign inputs. However, it does not explore the impact on other related concepts.
- Why unresolved: The paper does not provide a detailed analysis of how unlearning a specific concept affects the model's performance on other related concepts.
- What evidence would resolve it: Conducting a comprehensive study on the impact of unlearning on related concepts would provide insights into the broader implications of the unlearning process.

## Limitations
- Dataset Representativeness: The effectiveness depends heavily on the quality and representativeness of the forget datasets, which may contain biases or gaps.
- Classifier Reliance: The evaluation method relies on a trained classifier whose accuracy and robustness are not extensively validated.
- Catastrophic Forgetting: Limited evidence demonstrates that unlearning doesn't cause catastrophic forgetting of important information beyond targeted content.

## Confidence
- High Confidence: The mathematical formulation of the gradient ascent unlearning algorithm is clearly defined and theoretically sound.
- Medium Confidence: Experimental results showing 75% reduction in harmful outputs are promising but limited by classifier validation and dataset concerns.
- Low Confidence: Claims about generalization to other harmful/copyrighted content without modifications are not well-supported by the paper's focused scope.

## Next Checks
1. Conduct thorough analysis of classifier's performance on diverse subsets of forget datasets, including edge cases and adversarial examples.
2. Perform in-depth analysis of forget datasets (PKU and Lord of the Rings) to identify potential biases or gaps, and consider augmenting with additional examples.
3. Design experiments to explicitly measure the impact of unlearning on model's performance on diverse, unrelated tasks to quantify potential catastrophic forgetting.