---
ver: rpa2
title: 'DPPA: Pruning Method for Large Language Model to Model Merging'
arxiv_id: '2403.02799'
source_url: https://arxiv.org/abs/2403.02799
tags: []
core_contribution: 'This paper introduces DPPA, a dual-stage method to address parameter
  conflicts when merging complex fine-tuned models from multiple domains. The approach
  consists of two components: Dynamically Pruning (DP), which adaptively adjusts pruning
  rates at the linear layer level to retain more important parameters at high pruning
  rates, and Dynamically Partition Amplification (DPA), which rescales parameter partitions
  based on their significance.'
---

# DPPA: Pruning Method for Large Language Model to Model Merging
## Quick Facts
- arXiv ID: 2403.02799
- Source URL: https://arxiv.org/abs/2403.02799
- Authors: Yaochen Zhu; Rui Xia; Jiajun Zhang
- Reference count: 40
- One-line primary result: Achieves comparable performance to methods preserving 90% of parameters while only keeping 20% through dual-stage pruning and amplification

## Executive Summary
DPPA is a dual-stage method addressing parameter conflicts when merging complex fine-tuned models from multiple domains. It combines Dynamically Pruning (DP), which adaptively adjusts pruning rates at the linear layer level to retain more important parameters at high pruning rates, and Dynamically Partition Amplification (DPA), which rescales parameter partitions based on their significance. DPPA achieves comparable performance to methods that retain up to 90% of domain-specific parameters while only keeping 20%, demonstrating a 20% improvement in model merging performance.

## Method Summary
DPPA addresses parameter conflicts in multi-domain model merging through a two-stage approach. First, Dynamically Pruning (DP) uses linear layers as the minimum unit for pruning rate adjustment, calculating parameter significance based on accumulated magnitudes and adjusting pruning rates inversely proportional to significance. Second, Dynamically Partition Amplification (DPA) segments parameters at different pruning rates and applies different amplification factors to each partition, prioritizing partitions with higher pruning rates as more important. The pruned and amplified delta parameters are then merged using AdaMerging and added to the base model to produce the final merged model.

## Key Results
- Maintains 20% of domain-specific parameters while achieving comparable performance to methods preserving 90%
- Demonstrates 20% improvement in model merging performance over baseline methods
- Validated on LLaMA 2 across mathematics, finance, and law domains with significant parameter divergence from base model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DP improves performance at higher pruning rates by adaptively adjusting pruning rates at the linear layer level based on parameter significance.
- Mechanism: Uses linear layers as minimum unit for pruning rate adjustment, calculating significance as accumulated magnitudes exceeding average by N-fold, then adjusting pruning rates inversely proportional to significance.
- Core assumption: Parameter importance varies across linear layers, and preserving more important parameters at high pruning rates maintains model capability.
- Evidence anchors: Abstract mentions DP as improved magnitude pruning approach; paper proposes linear layers as minimum unit; no corpus evidence found.
- Break condition: If parameter importance doesn't vary significantly across linear layers or significance metric fails to capture true importance.

### Mechanism 2
- Claim: DPA enhances performance by dynamically amplifying parameter partitions based on their significance levels.
- Mechanism: Segments parameters at different pruning rates and applies different amplification factors to each partition, prioritizing partitions with higher pruning rates.
- Core assumption: Parameters showing significant deviations from baseline during fine-tuning are more important for domain-specific capabilities.
- Evidence anchors: Abstract mentions DPA as rescaling strategy; paper surmises parameters with significant deviations are most important; no corpus evidence found.
- Break condition: If amplification rate is not properly calibrated, it could under- or over-amplify parameters, degrading performance.

### Mechanism 3
- Claim: Combination of DP and DPA achieves comparable performance to methods preserving 90% of parameters while only keeping 20%.
- Mechanism: DP reduces parameter conflicts by pruning less important parameters, while DPA restores capability by amplifying remaining important parameters. Two-stage approach balances sparsity and performance.
- Core assumption: Interaction between pruning and amplification can preserve domain-specific capabilities while significantly reducing parameter count.
- Evidence anchors: Abstract states method maintains 20% of parameters with comparable performance to 90% methods; experimental results show this performance; no corpus evidence for this specific combination.
- Break condition: If pruned model loses too much information during DP, DPA cannot fully recover original capabilities.

## Foundational Learning

- Concept: Magnitude-based pruning and its limitations
  - Why needed here: Understanding why traditional magnitude pruning fails for delta parameters in model merging
  - Quick check question: Why does magnitude pruning often underperform when applied to delta parameters rather than original model parameters?

- Concept: Parameter conflicts in model merging
  - Why needed here: Understanding the core problem DPPA addresses - conflicting parameter updates from different domains
  - Quick check question: What causes parameter conflicts when merging fine-tuned models, and how do delta parameters help characterize these conflicts?

- Concept: Linear layer architecture in transformers
  - Why needed here: Understanding granularity of DP's pruning adjustments at linear layer level
  - Quick check question: What are the different linear layers within a transformer block (attention heads, MLP), and how do their parameter counts typically compare?

## Architecture Onboarding

- Component map: Fine-tuned models from multiple domains -> Delta parameter calculation -> Dynamically Pruning (DP) -> Dynamically Partition Amplification (DPA) -> Merged model with reduced parameters but preserved capability

- Critical path: 1. Calculate delta parameters for each fine-tuned model 2. Apply DP to obtain pruned delta parameters 3. Apply DPA to amplify important parameter partitions 4. Merge pruned and amplified delta parameters using AdaMerging 5. Add merged deltas to base model to get final model

- Design tradeoffs: Sparsity vs. performance (higher pruning rates save more parameters but risk capability loss), Granularity vs. complexity (linear layer-level pruning is more precise but computationally complex), Amplification calibration (finding optimal amplification rates for different partitions)

- Failure signatures: Performance degradation beyond expected levels at given pruning rates, Instability in amplification rates during DPA, Inconsistent results across different domains or model sizes

- First 3 experiments: 1. Implement DP on single fine-tuned model at various pruning rates (10%, 50%, 90%) and compare to baseline magnitude pruning 2. Apply DPA to DP output and evaluate performance recovery at each pruning rate 3. Test DPPA on two-domain merging (math + finance) at 80% pruning rate and compare to DARE baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical explanation for why DPPA's performance improvement decreases as pruning rates exceed 80%?
- Basis in paper: Inferred - paper shows DPPA's effectiveness at 80-90% pruning rates but doesn't explain diminishing returns at higher rates
- Why unresolved: Paper demonstrates effectiveness but lacks theoretical analysis of why performance gains plateau at extreme pruning rates
- What evidence would resolve it: Theoretical analysis of parameter space preservation and performance degradation patterns at various pruning thresholds

### Open Question 2
- Question: How does DPPA's effectiveness scale with model size beyond LLaMA 2 (7B and 13B parameters)?
- Basis in paper: Explicit - paper only tests on LLaMA 2 models and states "we will present more comprehensive results" for larger models
- Why unresolved: Experiments limited to LLaMA 2 models without testing on larger or smaller model variants
- What evidence would resolve it: Comparative experiments across different model sizes (1B, 33B, 70B parameters) using same methodology

### Open Question 3
- Question: What is the relationship between parameter offset magnitude and optimal pruning strategy selection?
- Basis in paper: Explicit - "When DARE falls below 90% performance at a pruning rate of 90%, our method can serve as a viable alternative"
- Why unresolved: Paper identifies threshold for when DPPA outperforms DARE but doesn't establish systematic relationship between offset magnitude and optimal pruning strategy
- What evidence would resolve it: Empirical study mapping parameter offset ranges to optimal pruning strategy selection across multiple domains

## Limitations

- Limited empirical validation scope: Evaluation focuses exclusively on LLaMA 2 across three domains using specific datasets, without testing generalizability to other model architectures or diverse domain combinations
- Parameter significance metric opacity: Significance calculation uses "accumulated magnitudes exceeding average by N-fold" without detailed justification for threshold choice or comparison to alternative metrics
- Computational overhead considerations: Paper does not report training or inference time impacts of dual-stage pruning and amplification process, which is critical for practical deployment

## Confidence

- High confidence: Core problem identification and general two-stage approach structure are well-established; experimental methodology using standard datasets and evaluation metrics is sound
- Medium confidence: Mechanism claims for DP and DPA are logically coherent but rely on novel formulations without extensive theoretical grounding or empirical comparison to alternatives
- Low confidence: Claim that DPPA achieves "comparable performance to methods preserving 90% of parameters while only keeping 20%" requires more extensive validation across different model sizes and domain combinations

## Next Checks

1. Implement and evaluate DP and DPA independently on the same datasets to quantify their individual contributions to the 20% improvement and clarify whether both stages are necessary

2. Test DPPA on alternative LLM architectures (e.g., OPT, Falcon) and smaller model variants to assess generalizability beyond LLaMA 2 and establish whether linear-layer-based pruning strategy transfers across architectures

3. Implement DPPA using Fisher information-based or correlation-based parameter importance metrics instead of the magnitude-based approach, and compare performance to establish whether the proposed significance metric is optimal