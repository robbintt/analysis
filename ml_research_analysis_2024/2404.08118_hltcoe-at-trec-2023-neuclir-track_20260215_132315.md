---
ver: rpa2
title: HLTCOE at TREC 2023 NeuCLIR Track
arxiv_id: '2404.08118'
source_url: https://arxiv.org/abs/2404.08118
tags:
- plaid
- retrieval
- colbert-x
- language
- runs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The HLTCOE team experimented with PLAID, an mT5 reranker, and document
  translation for the TREC 2023 NeuCLIR track. They explored various training techniques
  including translate-train (TT), translate-distill (TD), and multilingual translate-train
  (MTT).
---

# HLTCOE at TREC 2023 NeuCLIR Track

## Quick Facts
- arXiv ID: 2404.08118
- Source URL: https://arxiv.org/abs/2404.08118
- Reference count: 27
- HLTCOE explored PLAID, mT5 reranking, and document translation for TREC 2023 NeuCLIR tasks

## Executive Summary
The HLTCOE team participated in the TREC 2023 NeuCLIR track, focusing on cross-lingual information retrieval across English, Persian, and Chinese. They developed and evaluated multiple approaches including translate-train (TT), translate-distill (TD), and multilingual translate-train (MTT) techniques for training ColBERT models. Their experiments combined document translation with advanced reranking using mT5, ultimately achieving strong performance across all three language pairs. The team submitted runs for CLIR, MLIR news, and technical document tasks, demonstrating the effectiveness of their distillation and stacking approaches.

## Method Summary
The team employed a multi-faceted approach to cross-lingual retrieval. They implemented translate-train by training ColBERT models with English queries paired with translated passages in target languages. The multilingual translate-train variant mixed translated documents from all three languages in each training batch. For distillation, they used mT5 scores over translated document pairs to teach query-document scoring to student models. They also explored document translation as a baseline approach and implemented model stacking by combining teacher mT5 rerankers with student ColBERT-X models. The evaluation covered all NeuCLIR tasks: CLIR, MLIR news, and technical documents, with experiments conducted across English-Persian, English-Chinese, and Chinese-Persian language pairs.

## Key Results
- mT5-distilled models outperformed ColBERT-X models trained with TT across all three NeuCLIR languages
- Student ColBERT-X models performed on par with their mT5 reranker teacher while being more efficient
- Stacking teacher reranker on top of student model produced the most effective submission across all three languages

## Why This Works (Mechanism)
The translate-train approach works by exposing the retrieval model to aligned query-passage pairs across languages during training, allowing it to learn cross-lingual semantic relationships. The distillation mechanism transfers knowledge from the strong mT5 reranker to the more efficient student model through soft label supervision, capturing nuanced scoring patterns. Multilingual translate-train enhances generalization by exposing the model to diverse linguistic patterns across all three languages simultaneously. The stacking approach combines the complementary strengths of the teacher's sophisticated scoring with the student's efficiency and task-specific adaptation.

## Foundational Learning

**Cross-lingual Retrieval**: The task of finding relevant documents across language boundaries using queries in a different language. Needed because most information exists in multiple languages but users typically search in only one. Quick check: Verify alignment between query language and document language in test collections.

**Knowledge Distillation**: Transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) through soft labels. Needed to achieve both performance and efficiency in production systems. Quick check: Compare student performance against teacher on held-out validation data.

**Translate-Train**: Training models with translated data to handle cross-lingual tasks. Needed when parallel corpora are limited but machine translation is available. Quick check: Ensure translation quality scores meet minimum thresholds before training.

**Document Translation**: Translating documents to a common language before retrieval. Needed as a strong baseline for cross-lingual retrieval. Quick check: Verify translation coverage and quality across all document collections.

## Architecture Onboarding

**Component Map**: Document Translation -> ColBERT Retrieval -> mT5 Reranking -> Knowledge Distillation -> Stacked Model

**Critical Path**: The most important sequence is Document Translation -> ColBERT Retrieval -> mT5 Reranking, as this forms the foundation for all downstream techniques including distillation and stacking.

**Design Tradeoffs**: The team balanced between model complexity (mT5 reranker) and efficiency (ColBERT-X student), ultimately finding that stacking provided the best of both worlds. Translation quality versus computational cost was another key tradeoff, with document translation providing strong baselines but requiring significant processing.

**Failure Signatures**: Poor translation quality would manifest as degraded retrieval performance, particularly for low-resource language pairs. Overfitting to translated data could occur if training and test domains differ significantly. The distillation process could fail if teacher model confidence is too low or too high.

**First Experiments**:
1. Baseline document translation with standard retriever
2. mT5 reranking on translated document pairs
3. Translate-train ColBERT models with English queries and translated passages

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Efficiency claims lack quantitative runtime metrics and computational requirements
- Stacking approach needs detailed ablation studies to isolate component contributions
- Results are validated only on TREC NeuCLIR 2023 data without testing on additional cross-lingual retrieval datasets

## Confidence

**High confidence**: mT5-distilled models outperform TT-trained ColBERT-X models
**Medium confidence**: Student models are more efficient than mT5 rerankers (lacks quantitative support)
**High confidence**: Stacking produces the most effective results across all languages

## Next Checks
1. Conduct runtime efficiency measurements comparing student models against mT5 rerankers across different hardware configurations
2. Perform ablation studies to quantify the individual contributions of TT, TD, and stacking components
3. Test the trained models on additional cross-lingual retrieval datasets (e.g., CLEF collections) to assess generalizability beyond TREC NeuCLIR