---
ver: rpa2
title: 'Bag of Lies: Robustness in Continuous Pre-training BERT'
arxiv_id: '2406.09967'
source_url: https://arxiv.org/abs/2406.09967
tags:
- data
- bert
- knowledge
- pre-training
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates continuous pre-training (CPT) of BERT for
  entity knowledge acquisition, using the COVID-19 pandemic as a case study. Since
  COVID-19 emerged after BERT's pre-training, the model lacks specific entity knowledge
  about it.
---

# Bag of Lies: Robustness in Continuous Pre-training BERT
## Quick Facts
- arXiv ID: 2406.09967
- Source URL: https://arxiv.org/abs/2406.09967
- Authors: Ine Gevers; Walter Daelemans
- Reference count: 27
- Continuous pre-training on misinformation and shuffled text sometimes improves fact-checking performance

## Executive Summary
This study investigates continuous pre-training (CPT) of BERT for acquiring entity knowledge about COVID-19, a topic that emerged after BERT's original pre-training. The authors evaluate CPT variants on a fact-checking benchmark, including adversarial approaches like training on misinformation and shuffling word order. Surprisingly, these manipulations do not degrade and sometimes even improve downstream performance, suggesting CPT is robust against misinformation. The paper also introduces a new dataset pairing original COVID-19 academic texts with AI-generated false counterparts.

## Method Summary
The authors extracted texts from the LitCovid repository and Reddit comments about COVID-19, then generated AI-generated misinformation and paraphrased versions using GPT-4. They pre-trained BERT-base and BERT-large models using Masked Language Modeling (MLM) on various COVID-19 text sources with adversarial transformations (misinformation and word order shuffling). The CPT models were then fine-tuned on the Check-COVID fact-checking benchmark using 5-fold cross-validation with 5 random seeds, comparing performance metrics with baseline models.

## Key Results
- CPT on domain-specific COVID-19 texts improves fact-checking performance compared to BERT-base baseline
- Pre-training on AI-generated misinformation sometimes outperforms pre-training on original correct information
- Shuffling word order during CPT has no negative effect and can improve performance when original CPT is ineffective

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Continuous pre-training on entity knowledge enhances fact verification accuracy compared to base BERT model
- **Mechanism:** CPT exposes the model to domain-specific texts, allowing it to update internal representations with new entity knowledge not present in original pre-training data
- **Core assumption:** The model can integrate new factual information without catastrophically forgetting previous knowledge
- **Evidence anchors:**
  - "Using continuous pre-training, we control what entity knowledge is available to the model."
  - "CPT generally helps performance on this downstream task, which confirms prior literature."
- **Break condition:** If CPT data is too small or not aligned with the task, performance gain may not be significant

### Mechanism 2
- **Claim:** Misinformation in CPT data does not degrade and sometimes improves downstream fact-checking performance
- **Mechanism:** The model may learn general linguistic patterns or document-level associations during CPT that are useful for the task, regardless of factual accuracy
- **Core assumption:** Performance is more influenced by language style or document-level patterns than by factual correctness during CPT
- **Evidence anchors:**
  - "these methods do not degrade, and sometimes even improve, the model's downstream performance."
  - "BERT-base models CPT on AI-generated data perform better than BERT-base models CPT on original correct information."
- **Break condition:** If misinformation is too subtle or complex, it might confuse the model and lead to degraded performance

### Mechanism 3
- **Claim:** Shuffling word order during CPT does not degrade performance and can improve it when original CPT is ineffective
- **Mechanism:** CPT focuses on learning document-level associations and hierarchical structures rather than exact word order
- **Core assumption:** The model learns primarily from distributional information and document-level context, not from specific word sequences
- **Evidence anchors:**
  - "shuffling the word order until the input becomes nonsensical."
  - "shuffling word order has no effect [Chiang and Lee, 2020, Krishna et al., 2021, Sinha et al., 2021]."
- **Break condition:** If word order is too scrambled, input may become incomprehensible, preventing learning useful patterns

## Foundational Learning

- **Concept:** Masked Language Modeling (MLM)
  - **Why needed here:** MLM is the objective function used during BERT's pre-training and CPT
  - **Quick check question:** In MLM, what percentage of tokens are typically masked during pre-training?

- **Concept:** Catastrophic Forgetting
  - **Why needed here:** CPT aims to update the model's knowledge without losing previously learned information
  - **Quick check question:** What is one technique to mitigate catastrophic forgetting during fine-tuning?

- **Concept:** Task-Adaptive Pre-Training
  - **Why needed here:** The paper uses task-adaptive pre-training by using unlabeled data from the fact-checking benchmark during CPT
  - **Quick check question:** What is the main difference between task-adaptive pre-training and in-domain pre-training?

## Architecture Onboarding

- **Component map:** LitCovid/Reddit texts → Continuous Pre-Training (MLM objective) → Fine-tuning (fact verification task) → Evaluation
- **Critical path:** Continuous Pre-Training → Fine-tuning → Evaluation
- **Design tradeoffs:** Larger CPT datasets may improve performance but increase computational cost; using misinformation improves performance but raises ethical concerns
- **Failure signatures:** If CPT does not improve performance, it could be due to insufficient data, misaligned language style, or the model not retaining CPT information
- **First 3 experiments:**
  1. Compare BERT-base with BERT-base CPT on academic texts (LitCovid) on fact-checking task
  2. Compare BERT-base CPT on misinformation with BERT-base CPT on original texts on fact-checking task
  3. Compare BERT-base CPT on shuffled word order with BERT-base CPT on original word order on fact-checking task

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does BERT's performance on fact-checking change when pre-trained on misinformation from sources beyond COVID-19?
- **Basis in paper:** The study demonstrates CPT on COVID-19 misinformation improves performance, suggesting this might generalize to other domains
- **Why unresolved:** The study only tests this on COVID-19 misinformation, so it's unclear if the effect holds for other topics
- **What evidence would resolve it:** Testing CPT on misinformation from other domains (e.g., politics, science) and measuring downstream performance on fact-checking benchmarks for those domains

### Open Question 2
- **Question:** What is the mechanism behind BERT's improved performance when pre-trained on misinformation or social media data?
- **Basis in paper:** The authors hypothesize that AI-generated texts have more "standard" language, but they don't conclusively determine the underlying cause
- **Why unresolved:** The paper observes the effect but doesn't investigate internal mechanisms of how the model processes and benefits from this data
- **What evidence would resolve it:** Analyzing attention patterns, probing tasks, or ablation studies to understand how the model utilizes information from different data sources

### Open Question 3
- **Question:** Does the size of the pre-training dataset affect the robustness of BERT against adversarial attacks like misinformation?
- **Basis in paper:** The authors test with 200 and 10,000 texts but don't explore intermediate sizes or larger datasets
- **Why unresolved:** The study only compares two dataset sizes, so it's unclear if the effect is consistent across a wider range of sizes
- **What evidence would resolve it:** Experimenting with a range of dataset sizes (e.g., 500, 1,000, 5,000) and measuring performance on adversarial attacks

### Open Question 4
- **Question:** How does BERT's robustness to misinformation compare to other language models like RoBERTa or GPT?
- **Basis in paper:** The study focuses on BERT, so it's unclear if this robustness is a general property of language models or specific to BERT
- **Why unresolved:** The authors don't test other models, so it's difficult to generalize the findings
- **What evidence would resolve it:** Repeating the experiments with other language models and comparing their performance on misinformation and adversarial attacks

## Limitations
- Performance gains from adversarial CPT may be specific to the Check-COVID benchmark's characteristics rather than demonstrating general robustness
- Study lacks ablation studies isolating whether improvements stem from linguistic patterns, document structure, or other factors in adversarial data
- The dataset pairing original texts with AI-generated false counterparts may not capture the full complexity of real-world misinformation

## Confidence
- **High Confidence**: That CPT on domain-specific texts (LitCovid) improves fact-checking performance compared to BERT-base baseline
- **Medium Confidence**: That adversarial CPT methods (misinformation and shuffling) do not degrade and sometimes improve performance
- **Low Confidence**: That these robustness findings generalize to other domains and tasks beyond COVID-19 fact-checking

## Next Checks
1. **Ablation study on misinformation sources**: Test CPT with misinformation from different sources (not just GPT-4) and with varying degrees of semantic distortion to isolate what aspects of adversarial data drive performance changes

2. **Cross-domain transfer validation**: Apply the most successful CPT variants (including adversarial approaches) to a different fact-checking dataset or domain to test generalizability of the robustness findings

3. **Mechanistic probing experiments**: Use attention visualization and feature attribution methods to understand how the model processes original vs. adversarial CPT data, particularly focusing on whether it's learning factual knowledge or document-level patterns