---
ver: rpa2
title: 'ACR: A Benchmark for Automatic Cohort Retrieval'
arxiv_id: '2406.14780'
source_url: https://arxiv.org/abs/2406.14780
tags:
- patients
- queries
- query
- find
- cohort
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Automatic Cohort Retrieval (ACR), a new task
  focused on retrieving patient cohorts from longitudinal Electronic Medical Records
  (EMRs) at scale. The authors propose a benchmark including a query dataset of 113
  oncology use cases, an EMR dataset of 1436 patients with 115,865 documents, and
  an evaluation framework.
---

# ACR: A Benchmark for Automatic Cohort Retrieval

## Quick Facts
- arXiv ID: 2406.14780
- Source URL: https://arxiv.org/abs/2406.14780
- Authors: Dung Ngoc Thai; Victor Ardulov; Jose Ulises Mena; Simran Tiwari; Gleb Erofeev; Ramy Eskander; Karim Tarabishy; Ravi B Parikh; Wael Salloum
- Reference count: 40
- One-line primary result: Neuro-symbolic approach (Hypercube) outperforms LLM-only baselines on Automatic Cohort Retrieval task

## Executive Summary
This paper introduces Automatic Cohort Retrieval (ACR), a new task focused on retrieving patient cohorts from longitudinal Electronic Medical Records (EMRs) at scale. The authors propose a benchmark including a query dataset of 113 oncology use cases, an EMR dataset of 1436 patients with 115,865 documents, and an evaluation framework. Three baseline approaches are evaluated: a retriever-only method using text embeddings, a retrieve-then-read method combining retrieval with an LLM reader, and a neuro-symbolic approach (Hypercube) that integrates symbolic reasoning with language models. The neuro-symbolic approach outperforms the LLM-only baselines in F1-score across all query categories, demonstrating the value of domain-specific knowledge and offline longitudinal reasoning.

## Method Summary
The authors propose a benchmark for Automatic Cohort Retrieval (ACR) that includes a query dataset, an EMR dataset, and an evaluation framework. They evaluate three baseline approaches: a retriever-only method using dense retrieval with text embeddings, a retrieve-then-read method combining retrieval with an LLM reader, and a neuro-symbolic approach (Hypercube) that integrates symbolic reasoning with language models. The neuro-symbolic approach performs knowledge acquisition offline through text-based and longitudinal reasoning, then uses large-scale reasoning to answer clinical queries. Evaluation metrics include F1-score, hallucination ratio, and set-theoretic consistency.

## Key Results
- Neuro-symbolic approach (Hypercube) outperforms retriever-only and retrieve-then-read baselines in F1-score across all query categories
- LLM-only approaches show higher hallucination tendencies compared to neuro-symbolic method
- All approaches struggle with set-theoretic consistency, but neuro-symbolic approach shows better consistency
- Performance degrades as patient record length increases for both Hypercube and ada+GPT4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neuro-symbolic approaches outperform pure LLM baselines on ACR tasks due to their ability to integrate domain-specific medical ontologies with symbolic reasoning
- Mechanism: Hypercube's architecture combines large language models with symbolic reasoning and medical ontologies to create a knowledge base offline. This allows for efficient and effective reasoning across millions of patients by leveraging pre-defined medical hierarchies and set operations
- Core assumption: The medical ontologies and symbolic reasoning components accurately capture the relevant medical knowledge and relationships needed for cohort retrieval
- Evidence anchors:
  - [abstract]: "The neuro-symbolic approach outperforms the LLM-only baselines in F1-score across all query categories, demonstrating the value of domain-specific knowledge and offline longitudinal reasoning."
  - [section]: "Hypercube pairs LLMs with symbolic reasoning [Shekhar et al., 2023] in the following components... Longitudinal reasoning: Hypercube utilizes a Dynamic Symbolic Memory as it examines all KR artifacts extracted from each report and cross-references them against the evolving journey in the dynamic memory."
  - [corpus]: Weak. The corpus does not provide direct evidence about the effectiveness of the symbolic reasoning component
- Break condition: If the medical ontologies are incomplete or inaccurate, or if the symbolic reasoning engine cannot handle complex or ambiguous medical scenarios

### Mechanism 2
- Claim: Offline longitudinal reasoning significantly improves the quality of cohort retrieval by consolidating patient information and resolving contradictions across multiple documents
- Mechanism: During the knowledge acquisition phase, Hypercube performs text-based reasoning to convert medical text into a knowledge representation (KR) for each document. Then, it performs longitudinal reasoning to consolidate these KRs into a comprehensive patient model, resolving any contradictions and maintaining coherence
- Core assumption: The patient's medical journey can be accurately represented as a consistent and coherent narrative, even when faced with contradictory information from different documents
- Evidence anchors:
  - [abstract]: "The neuro-symbolic approach outperforms the LLM-only baselines in F1-score across all query categories, demonstrating the value of domain-specific knowledge and offline longitudinal reasoning."
  - [section]: "Longitudinal reasoning is the capacity to comprehend and retain knowledge from an extended narrative that develops over time. It entails tracking the progression of events and their facts, resolving any contradictions, and piecing together evolving but compatible elements into a consistent and comprehensive journey while maintaining coherence and explainability."
  - [corpus]: Weak. The corpus does not provide direct evidence about the effectiveness of the longitudinal reasoning process
- Break condition: If the patient's medical journey is too complex or ambiguous to be represented as a coherent narrative, or if the longitudinal reasoning engine cannot handle all types of contradictions

### Mechanism 3
- Claim: Efficient large-scale reasoning is essential for practical ACR systems, as it allows for fast and accurate cohort retrieval across millions of patients
- Mechanism: Hypercube employs a proprietary event-rooted knowledge base and a large-scale reasoning engine that can efficiently and effectively reason across millions of patients to answer clinical queries. This allows for fast retrieval of cohorts while maintaining high precision and recall
- Core assumption: The knowledge base and reasoning engine can scale efficiently to handle millions of patients without significant performance degradation
- Evidence anchors:
  - [abstract]: "The neuro-symbolic approach outperforms the LLM-only baselines in F1-score across all query categories, demonstrating the value of domain-specific knowledge and offline longitudinal reasoning."
  - [section]: "Large-scale reasoning: A proprietary event-rooted knowledge base is developed to allow efficient and effective reasoning across millions of patients to answer clinical queries."
  - [corpus]: Weak. The corpus does not provide direct evidence about the efficiency of the large-scale reasoning component
- Break condition: If the knowledge base becomes too large to manage efficiently, or if the reasoning engine cannot scale to handle the complexity of millions of patients

## Foundational Learning

- Concept: Longitudinal reasoning
  - Why needed here: Cohort retrieval requires understanding a patient's medical journey across multiple documents and time periods, which involves resolving contradictions and maintaining coherence
  - Quick check question: How would you handle a patient record that contains conflicting information about a diagnosis across different documents?

- Concept: Set-theoretic consistency
  - Why needed here: Ensuring that the system retrieves consistent cohorts for different but related queries is crucial for the reliability and trustworthiness of the ACR system
  - Quick check question: What are the potential consequences of a system that retrieves inconsistent cohorts for paraphrased or intersection queries?

- Concept: Hallucination tendency
  - Why needed here: Understanding and mitigating the tendency of ACR systems to retrieve false positives (hallucinations) is essential for maintaining the quality and reliability of the system
  - Quick check question: How would you measure and compare the hallucination tendencies of different ACR systems?

## Architecture Onboarding

- Component map: Knowledge Acquisition (KA) -> Question Answering (QA)
- Critical path: The critical path for ACR is the time it takes to perform large-scale reasoning over the knowledge base to retrieve cohorts for a given query
- Design tradeoffs: The tradeoff between effectiveness and efficiency is a key consideration in ACR system design. While more complex reasoning may improve accuracy, it can also increase computational costs and reduce scalability
- Failure signatures: Common failure modes include retrieving inconsistent cohorts for related queries, hallucinating false positives, and failing to handle complex or ambiguous medical scenarios
- First 3 experiments:
  1. Evaluate the system's performance on a small set of queries and patients to ensure basic functionality
  2. Test the system's ability to handle complex queries and longitudinal data by using a diverse set of queries and patient records
  3. Measure the system's hallucination tendency by comparing its retrieved cohorts against ground truth labels for a set of queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms by which Hypercube's symbolic reasoning outperforms LLM-only approaches in handling longitudinal EMR data?
- Basis in paper: [explicit] The paper states that Hypercube uses symbolic reasoning combined with medical ontologies and outperforms LLM-only baselines in F1-score across all query categories
- Why unresolved: The paper does not provide detailed insights into the specific mechanisms or algorithms that enable Hypercube's superior performance in longitudinal reasoning
- What evidence would resolve it: Detailed analysis of Hypercube's internal algorithms, comparison of its reasoning steps with those of LLM-only approaches on specific cases, and ablation studies isolating the contributions of different components

### Open Question 2
- Question: How does the performance of ACR systems degrade as the length and complexity of patient records increase, and what are the specific factors contributing to this degradation?
- Basis in paper: [explicit] The paper mentions that Hypercube and ada+GPT4 show quality degradation as patient records get longer, but the specific factors and extent of degradation are not fully explored
- Why unresolved: The paper provides initial observations but does not conduct a comprehensive analysis of the relationship between record length, complexity, and system performance
- What evidence would resolve it: Systematic experiments varying record length and complexity, detailed analysis of error patterns at different record lengths, and identification of specific bottlenecks in the reasoning process

### Open Question 3
- Question: What are the key challenges in maintaining set-theoretic consistency in ACR systems, and how can these challenges be addressed to improve system reliability?
- Basis in paper: [explicit] The paper introduces the concept of set-theoretic consistency and identifies inconsistencies in LLM-only approaches, but does not provide solutions to address these challenges
- Why unresolved: While the paper highlights the importance of set-theoretic consistency and identifies inconsistencies, it does not explore potential solutions or methods to improve consistency in ACR systems
- What evidence would resolve it: Development and evaluation of techniques to improve set-theoretic consistency, such as enhanced query parsing, improved reasoning algorithms, or the incorporation of symbolic knowledge bases

## Limitations

- The neuro-symbolic approach's superior performance may depend heavily on Hypercube's proprietary knowledge base and reasoning engine, which are not fully specified
- The effectiveness of the longitudinal reasoning component is difficult to evaluate without detailed error analysis of how contradictions across documents are resolved
- The hallucination metric introduced may be overly simplistic and not capture more nuanced forms of hallucination

## Confidence

- **High Confidence:** The benchmark dataset creation and the three baseline approaches are well-specified and reproducible
- **Medium Confidence:** The general claim that neuro-symbolic approaches outperform pure LLM baselines in cohort retrieval, based on the F1-score results presented
- **Low Confidence:** The specific mechanisms by which Hypercube's longitudinal reasoning and large-scale reasoning components achieve their performance gains, due to limited implementation details

## Next Checks

1. **Cross-validation on independent datasets:** Test the ACR benchmark's baselines on a separate cohort dataset from a different medical institution to assess generalizability
2. **Ablation study on knowledge base components:** Systematically remove or modify components of the medical ontologies and symbolic reasoning rules to quantify their individual contributions to performance
3. **Error analysis of hallucination cases:** Conduct a detailed qualitative analysis of false positives to understand whether they represent true hallucinations or legitimate edge cases in the query definitions