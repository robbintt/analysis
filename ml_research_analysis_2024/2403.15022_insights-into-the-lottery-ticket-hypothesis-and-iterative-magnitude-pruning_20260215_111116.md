---
ver: rpa2
title: Insights into the Lottery Ticket Hypothesis and Iterative Magnitude Pruning
arxiv_id: '2403.15022'
source_url: https://arxiv.org/abs/2403.15022
tags:
- loss
- pruning
- network
- training
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the iterative magnitude pruning (IMP) and lottery
  ticket hypothesis through the lens of loss landscape geometry and volume characteristics.
  The authors analyze the behavior of solutions at various IMP levels using a ResNet-20
  model on CIFAR-10.
---

# Insights into the Lottery Ticket Hypothesis and Iterative Magnitude Pruning

## Quick Facts
- arXiv ID: 2403.15022
- Source URL: https://arxiv.org/abs/2403.15022
- Authors: Tausifa Jan Saleem; Ramanjit Ahuja; Surendra Prasad; Brejesh Lall
- Reference count: 40
- Primary result: IMP solutions exist with good generalization but small volume in original parameter space

## Executive Summary
This paper investigates the lottery ticket hypothesis and iterative magnitude pruning (IMP) through the lens of loss landscape geometry and volume characteristics. Using a ResNet-20 model on CIFAR-10, the authors demonstrate that special solutions exist in the loss landscape with good generalization performance but very small volume in the original parameter space. These solutions are exposed by IMP through pruning away problematic dimensions with steep curvature. The study reveals that IMP solutions obtained via rewinding lie within the same loss sublevel set, while random initialization or pruning typically takes SGD outside this sublevel set, converging to inferior solutions. The research provides insights into why magnitude-based pruning works, why iterative pruning outperforms one-shot pruning, and why fine-tuning underperforms compared to rewinding.

## Method Summary
The study trains ResNet-20 on CIFAR-10 for 160 epochs using SGD with learning rate decay at epochs 80 and 120. IMP is then applied with weight rewinding for 10 iterations, pruning 20% of smallest magnitude weights each iteration and rewinding unpruned weights to values at the 2000th training step. For each IMP level, training loss and test accuracy are computed, and Hessian eigenvalues are calculated on 1/5th of training data to estimate basin volumes. The approach is compared against one-shot pruning, fine-tuning, random initialization, and random pruning baselines.

## Key Results
- Special solutions exist in the loss landscape with good generalization performance but very small volume in the original space
- IMP solutions obtained via rewinding lie within the same loss sublevel set, while random initialization takes SGD outside this sublevel set
- Iterative pruning outperforms one-shot pruning because it incrementally increases the loss baseline while maintaining solution quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IMP solutions exist that have small volume in original parameter space but good generalization performance.
- Mechanism: Pruning removes dimensions with steep curvature and small weights, exposing otherwise hidden solutions with flatter profiles in remaining dimensions.
- Core assumption: Neural network loss landscapes contain special minima with very narrow profiles along certain dimensions and flatter profiles along others.
- Evidence anchors:
  - [abstract] The primary finding is that special solutions exist in the loss landscape with good generalization performance but very small volume in the original space.
  - [section IV-A] We demonstrate that there also exist another kind of solutions which have good generalization performance but have a (relatively) small volume.
- Break condition: If the narrow dimensions contain important weights for generalization, pruning them would degrade performance rather than expose better solutions.

### Mechanism 2
- Claim: Specific initialization proposed by lottery ticket hypothesis works because it keeps SGD within the same loss sublevel set as the original dense network.
- Mechanism: Rewinding to initialization values after pruning maintains the starting point within the same connected loss sublevel set, allowing SGD to converge to better minima in the pruned space.
- Core assumption: The initialization point matters for finding good solutions in sparse subspaces, unlike in dense spaces where good minima are dense.
- Evidence anchors:
  - [abstract] IMP solutions obtained via rewinding lie within the same loss sublevel set, while random initialization or pruning typically takes SGD outside this sublevel set.
  - [section IV-B] The importance of initialization proposed by the lottery ticket hypothesis, therefore, becomes quite apparent.
- Break condition: If the rewound initialization happens to be at a bad point within the sublevel set, SGD may still converge to inferior solutions.

### Mechanism 3
- Claim: Iterative pruning works better than one-shot pruning because it incrementally increases loss baseline while maintaining solution quality.
- Mechanism: Each pruning step increases training loss and decreases volume of the current minimum, creating a baseline that SGD avoids by finding a better minimum in the next iteration.
- Core assumption: Removing too many weights at once creates a baseline minimum that is too degraded for SGD to recover from effectively.
- Evidence anchors:
  - [abstract] The study demonstrates why iterative pruning outperforms one-shot pruning.
  - [section IV-C] One-shot pruning removes too many weights and therefore, has inferior performance.
- Break condition: If the pruning fraction per iteration is too small, SGD may simply return to the same minimum without finding better solutions.

## Foundational Learning

- Concept: Loss landscape geometry and volume measures
  - Why needed here: Understanding how volume and curvature relate to generalization performance is central to interpreting IMP results
  - Quick check question: How does the product of top Hessian eigenvalues relate to basin volume?

- Concept: Sublevel sets and connected regions in parameter space
  - Why needed here: Determining whether rewound solutions stay within the same sublevel set is key to understanding initialization importance
  - Quick check question: What distinguishes two points being in the same sublevel set versus different sublevel sets?

- Concept: Linear mode connectivity and barrier detection
  - Why needed here: Testing whether IMP solutions at successive levels are linearly connected reveals fundamental properties of the loss landscape
  - Quick check question: How would you detect a barrier between two minima using interpolation?

## Architecture Onboarding

- Component map: Dense network training → iterative pruning with weight rewinding → solution analysis via loss landscape and volume metrics
- Critical path: Train ResNet-20 on CIFAR-10 → Apply IMP with 10 iterations, pruning 20% weights each iteration → Analyze solutions using Hessian eigenvalues and interpolation
- Design tradeoffs: Between pruning fraction per iteration (too large degrades baseline, too small doesn't progress) and number of iterations
- Failure signatures: Solutions falling outside the original loss sublevel set, or showing degraded generalization despite high sparsity
- First 3 experiments:
  1. Compare training loss and test accuracy across IMP levels to establish baseline behavior
  2. Calculate Hessian eigenvalues at successive IMP solutions to quantify volume changes
  3. Perform linear interpolation between successive IMP solutions to detect barriers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are there fundamental differences in the nature of minima discovered by IMP in the loss landscape versus the test error landscape, and how do these differences impact generalization?
- Basis in paper: [explicit] The paper contrasts findings with Paul et al. [17] who studied the error landscape, finding linearly connected IMP solutions, while this paper shows barriers in the loss landscape.
- Why unresolved: The relationship between loss landscape geometry and test error landscape characteristics remains unclear, particularly regarding the impact of landscape barriers on generalization performance.
- What evidence would resolve it: Direct comparison of IMP solutions in both loss and test error landscapes across multiple architectures and datasets, showing correlation between landscape barriers and generalization gaps.

### Open Question 2
- Question: What specific properties of the "special solutions" with good generalization but small volume make them discoverable only through IMP, and can these properties be characterized mathematically?
- Basis in paper: [explicit] The paper identifies solutions with good generalization but small volume that are only exposed through pruning, suggesting unique geometric properties.
- Why unresolved: The paper identifies these solutions but doesn't fully characterize their mathematical properties or explain why they remain hidden in the original space.
- What evidence would resolve it: Mathematical characterization of these solutions' Hessian structure, explaining why pruning exposes them, and whether similar solutions exist in other optimization problems.

### Open Question 3
- Question: Can the pruning strategy be optimized beyond magnitude-based pruning to more efficiently discover good sparse solutions while maintaining generalization?
- Basis in paper: [explicit] The paper shows magnitude-based pruning outperforms random pruning and discusses why pruning smaller weights is beneficial, but doesn't explore alternative pruning strategies.
- Why unresolved: The paper demonstrates magnitude-based pruning works but doesn't investigate whether other criteria could be more effective or efficient.
- What evidence would resolve it: Comparative study of various pruning strategies (e.g., gradient-based, second-order methods, learned pruning masks) showing which methods most efficiently discover high-quality sparse solutions.

## Limitations
- Theoretical claims about loss landscape geometry rely heavily on Hessian eigenvalue approximations and basin volume estimates
- Study focuses primarily on ResNet-20 on CIFAR-10 with limited validation on other architectures
- Geometric interpretation of why specific initializations work remains largely qualitative

## Confidence

### Major Uncertainties
The study's theoretical claims about loss landscape geometry rely heavily on approximations through Hessian eigenvalue analysis and basin volume estimates. The work focuses primarily on ResNet-20 on CIFAR-10, with limited validation on other architectures (VGG-16 in appendix). The geometric interpretation of why specific initializations work remains largely qualitative, with the quantitative connection between volume measures and generalization performance needing further exploration.

### Confidence Labels
- **High confidence**: The empirical observations about IMP performance improvements and the basic mechanics of iterative vs one-shot pruning are well-established through direct experimentation.
- **Medium confidence**: The geometric interpretation of solutions having small volume but good generalization is supported by Hessian analysis but requires more rigorous theoretical grounding.
- **Medium confidence**: The claims about rewinding keeping solutions within the same loss sublevel set are supported by interpolation experiments but the broader implications for initialization strategies need further validation.

## Next Validation Checks
1. Test the geometric hypotheses on additional architectures (e.g., Vision Transformers, EfficientNet) to verify generalizability beyond ResNet-20.
2. Conduct ablation studies varying the rewind point (beyond just the 2000th step) to quantify sensitivity to initialization timing.
3. Implement alternative volume estimation methods (beyond Hessian eigenvalues) to verify the robustness of basin volume calculations across different IMP levels.