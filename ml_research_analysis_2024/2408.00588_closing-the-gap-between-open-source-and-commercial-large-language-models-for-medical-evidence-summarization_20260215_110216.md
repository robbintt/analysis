---
ver: rpa2
title: Closing the gap between open-source and commercial large language models for
  medical evidence summarization
arxiv_id: '2408.00588'
source_url: https://arxiv.org/abs/2408.00588
tags: []
core_contribution: This study evaluates fine-tuning open-source LLMs for medical evidence
  summarization, addressing the limitations of proprietary models in transparency
  and customization. Using the MedReview dataset of 8,161 systematic review pairs,
  the authors fine-tune PRIMERA, LongT5, and Llama-2 using LoRA parameter-efficient
  adaptation.
---

# Closing the gap between open-source and commercial large language models for medical evidence summarization

## Quick Facts
- arXiv ID: 2408.00588
- Source URL: https://arxiv.org/abs/2408.00588
- Authors: Gongbo Zhang, Qiao Jin, Yiliang Zhou, Song Wang, Betina R. Idnay, Yiming Luo, Elizabeth Park, Jordan G. Nestor, Matthew E. Spotnitz, Ali Soroush, Thomas Campion, Zhiyong Lu, Chunhua Weng, Yifan Peng
- Reference count: 40
- Key outcome: Fine-tuned open-source LLMs (PRIMERA, LongT5, Llama-2) on MedReview dataset show 9.89-15.82% improvements in summarization metrics and outperform larger zero-shot models

## Executive Summary
This study demonstrates that fine-tuning open-source large language models using parameter-efficient LoRA adaptation can significantly close the performance gap with commercial models for medical evidence summarization. Using the MedReview dataset of 8,161 systematic review pairs, the authors show that fine-tuned models achieve substantial improvements in automatic metrics (ROUGE-L +9.89%, METEOR +13.21%, CHRF +15.82%) and human-evaluated summary quality. Notably, smaller fine-tuned models like LongT5-base outperform larger zero-shot models such as LongT5-xl, highlighting the effectiveness of domain adaptation over model scaling.

## Method Summary
The authors fine-tune three open-source LLMs (PRIMERA, LongT5, and Llama-2) using Low-Rank Adaptation (LoRA) on the MedReview dataset containing 8,161 pairs of systematic review abstracts and summaries. Models are trained with LoRA rank=8, learning rate=1e-3 for 1 epoch, then evaluated using automatic metrics (ROUGE-L, METEOR, CHRF) and human/GPT-4 evaluations on summary quality dimensions (consistency, comprehensiveness, specificity, readability).

## Key Results
- Fine-tuned models show 9.89% increase in ROUGE-L, 13.21% in METEOR, and 15.82% in CHRF scores
- Smaller fine-tuned models (LongT5-base) outperform larger zero-shot models (LongT5-xl)
- Human and GPT-4 evaluations confirm quality improvements across consistency, comprehensiveness, specificity, and readability
- Fine-tuned models align more closely with ground-truth summaries than zero-shot approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA fine-tuning enables smaller models to outperform larger zero-shot models on domain-specific tasks.
- Mechanism: By freezing most parameters and updating only low-rank matrices, LoRA reduces catastrophic forgetting and computational overhead, allowing efficient adaptation of smaller architectures to specialized medical summarization.
- Core assumption: Medical evidence summarization requires deep domain knowledge that can be captured through targeted parameter updates rather than scaling model size.
- Evidence anchors: [abstract] "smaller fine-tuned models like LongT5-base outperform larger zero-shot models such as LongT5-xl"; [section] "We employed Low-Rank Adaptation (LoRA), which is a parameter-efficient fine-tuning method focusing on updating only a minimal amount of model parameters during the fine-tuning process."
- Break condition: If domain knowledge is shallow or task is generic, full fine-tuning may be needed to achieve comparable performance.

### Mechanism 2
- Claim: Fine-tuning aligns open-source models with ground truth summary structures, overcoming zero-shot positional embedding bias.
- Mechanism: Zero-shot models extract leading sentences due to pre-training positional embedding biases, while fine-tuning on MedReview teaches models to prioritize semantic relevance over positional information.
- Core assumption: Ground truth summaries follow specific structural patterns that differ from general summarization tasks.
- Evidence anchors: [abstract] "fine-tuned models align more closely with ground-truth summaries"; [section] "zero-shot models tend to present a detailed background of the summarized studies but do not provide any findings or conclusions... This indicates that positional embeddings significantly impacted the summary more than word embeddings in zero-shot open-source models."
- Break condition: If ground truth summaries follow no consistent structure, positional bias may not be a significant factor.

### Mechanism 3
- Claim: GPT-4 can simulate expert evaluation with reasonable accuracy, enabling scalable assessment of fine-tuned models.
- Mechanism: GPT-4's strong language understanding allows it to evaluate summary quality across multiple dimensions (consistency, comprehensiveness, specificity, readability) with accuracy comparable to human experts.
- Core assumption: GPT-4's evaluation capabilities transfer to specialized domains like medical evidence summarization.
- Evidence anchors: [abstract] "257 out of 378 simulated evaluation results concord with the judgment of human experts (68% accuracy)"; [section] "We also analyzed the percentage of questions that human judgments agree with the GPT-4 evaluation."
- Break condition: If evaluation criteria become too specialized or nuanced, GPT-4's accuracy may drop below useful thresholds.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (LoRA)
  - Why needed here: Reduces computational cost while maintaining performance gains, critical for medical domain adaptation
  - Quick check question: How does LoRA's rank hyperparameter affect the number of trainable parameters?

- Concept: Medical evidence summarization task structure
  - Why needed here: Understanding the PICO framework and systematic review format is essential for effective fine-tuning
  - Quick check question: What are the key differences between medical evidence summaries and general document summaries?

- Concept: Evaluation metric limitations and correlations
  - Why needed here: Automatic metrics alone are insufficient; understanding their correlations helps interpret results
  - Quick check question: Why do PICO metrics show weaker correlation with traditional NLG metrics in this study?

## Architecture Onboarding

- Component map: Data preprocessing -> LoRA adapter injection -> Model fine-tuning -> Automatic evaluation (ROUGE-L, METEOR, CHRF) -> Human/GPT-4 evaluation -> Quality assessment
- Critical path: Fine-tuning → Evaluation → Expert feedback loop
- Design tradeoffs: Parameter efficiency vs. full fine-tuning capability, open-source transparency vs. proprietary performance
- Failure signatures: Overfitting on training data, catastrophic forgetting of general language capabilities, evaluation metric misalignment
- First 3 experiments:
  1. Compare LoRA rank=1 vs rank=8 on PRIMERA to find optimal parameter efficiency
  2. Test few-shot learning with 10 vs 100 samples to establish minimum effective training data
  3. Run ablation study on which sections of systematic reviews provide most useful training signal

## Open Questions the Paper Calls Out

- Question: How does the performance of fine-tuned open-source LLMs compare to GPT-4 in medical evidence summarization tasks?
  - Basis in paper: [explicit] The authors note that fine-tuned models like LongT5 achieve similar results to GPT-3.5-turbo, but they did not extensively explore fine-tuning models like GPT-4.
  - Why unresolved: The study primarily focused on open-source LLMs and did not fine-tune proprietary models like GPT-4.
  - What evidence would resolve it: Conducting a study where GPT-4 is fine-tuned on the same MedReview dataset and comparing its performance to fine-tuned open-source models would provide a direct comparison.

- Question: Can fine-tuned LLMs effectively summarize clinical trial publications directly, rather than relying on manually curated "main results" sections?
  - Basis in paper: [inferred] The authors mention that LLMs were not fine-tuned to summarize clinical trial publications but rather the manually curated "main results" of review abstracts. They suggest that a future direction is to deploy LLMs to directly synthesize information from clinical trials.
  - Why unresolved: The current study used a simplified dataset to expedite development and testing, which may not fully represent the complexity of summarizing raw clinical trial publications.
  - What evidence would resolve it: Conducting experiments where LLMs are fine-tuned on raw clinical trial publications and evaluating their performance compared to manually curated summaries would provide insights into their effectiveness.

- Question: How can the trustworthiness and accuracy of LLM-generated summaries be improved through fine-tuning?
  - Basis in paper: [explicit] The authors acknowledge that fine-tuning does not guarantee truthful and accurate summaries, as noted in previous evaluations of zero-shot GPT models.
  - Why unresolved: Despite improvements in performance metrics, the study highlights that ensuring the factual correctness and reliability of generated summaries remains a challenge.
  - What evidence would resolve it: Developing and testing additional fine-tuning techniques, such as incorporating fact-checking mechanisms or using more diverse training data, and evaluating their impact on summary accuracy would help address this issue.

## Limitations
- Results may not generalize to other medical literature types beyond systematic reviews from Cochrane Library
- Human evaluation sample size (9 pairs) is relatively small for establishing robust quality assessments
- GPT-4 evaluation agreement with humans (68%) may not capture all nuanced aspects of medical evidence summarization quality

## Confidence
- High Confidence: Technical implementation of LoRA fine-tuning and automatic metric improvements are well-documented and reproducible
- Medium Confidence: Human and GPT-4 evaluation results are promising but based on limited samples
- Low Confidence: Generalizability to other medical domains and optimal LoRA configuration parameters remain uncertain

## Next Checks
1. Apply fine-tuned models to different types of medical literature (clinical trial reports, case studies) to assess generalization
2. Conduct human evaluations on larger sample size (50-100 summary pairs) across multiple medical domains
3. Systematically vary LoRA rank values and training configurations to determine optimal parameter settings for different model sizes and medical summarization tasks