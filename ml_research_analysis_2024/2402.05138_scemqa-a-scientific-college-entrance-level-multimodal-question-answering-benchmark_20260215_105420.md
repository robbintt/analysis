---
ver: rpa2
title: 'SceMQA: A Scientific College Entrance Level Multimodal Question Answering
  Benchmark'
arxiv_id: '2402.05138'
source_url: https://arxiv.org/abs/2402.05138
tags:
- arxiv
- math
- physics
- scemqa
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SceMQA, a novel benchmark for scientific multimodal
  question answering at the college entrance level. It addresses a critical educational
  phase often overlooked in existing benchmarks, spanning high school to pre-college
  levels.
---

# SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark

## Quick Facts
- arXiv ID: 2402.05138
- Source URL: https://arxiv.org/abs/2402.05138
- Reference count: 13
- Primary result: SceMQA achieves 50-60% accuracy with strongest models, highlighting need for more capable MLLMs

## Executive Summary
SceMQA introduces a novel benchmark for scientific multimodal question answering at the college entrance level, addressing a critical educational phase often overlooked in existing benchmarks. The benchmark spans high school to pre-college levels across four core science subjects: Mathematics, Physics, Chemistry, and Biology. Featuring a blend of multiple-choice and free-response formats with identical contexts but varied questions, SceMQA provides comprehensive evaluation of AI models' reasoning capabilities while including detailed explanations and knowledge point classifications for each problem.

## Method Summary
The benchmark was created by collecting problems from public online materials for college entrance tests across four science subjects. The dataset includes 1,045 problems with essential visual components. Evaluation uses exact-match accuracy across multiple experimental settings including zero-shot, few-shot, and text-only conditions. The benchmark uniquely features problems with identical contexts but different questions to test deep understanding, and includes detailed human-verified explanations for over 90% of problems along with specific knowledge point classifications.

## Key Results
- Strongest models achieve only 50-60% accuracy, indicating significant room for improvement
- GPT-4 shows intermediate performance between primary and college-level benchmarks, confirming appropriate difficulty calibration
- Error analysis reveals distinct failure modes including reasoning errors, image perception errors, and knowledge gaps
- Subject-specific performance varies significantly, with better results in conceptual chemistry/biology versus calculation-heavy math/physics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark's difficulty is positioned between primary and college levels, making it a meaningful step in curriculum learning progression.
- Mechanism: By targeting the "college entrance" level, SceMQA fills a critical gap in the existing benchmark landscape. The paper shows GPT-4 achieves intermediate accuracy on SceMQA compared to its performance on primary and college benchmarks, confirming the appropriate difficulty calibration.
- Core assumption: Models improve more effectively when training data follows a difficulty progression rather than jumping from simple to highly complex problems.
- Evidence anchors:
  - [abstract] "This leaves a significant educational phase in human learning – the high school, or college entrance level – relatively unaddressed."
  - [section] "Figure 3 demonstrates the moderate difficulty level of our benchmark, positioning between the existing benchmark on primary and college levels."
- Break condition: If models show no improvement when trained on progressively difficult benchmarks, or if the difficulty calibration is inaccurate.

### Mechanism 2
- Claim: The combination of multiple question types per context improves evaluation of deep semantic understanding versus shallow pattern matching.
- Mechanism: By presenting multiple questions with identical contexts but different queries, the benchmark forces models to demonstrate genuine comprehension rather than memorizing superficial patterns. This design is explicitly informed by prior research showing models can exploit single-question-per-context setups.
- Core assumption: Models can be evaluated more accurately when they cannot rely on context-specific heuristics or pattern matching.
- Evidence anchors:
  - [section] "SceMQA uniquely features problems with the same context but different questions... this one-context multiple-questions setting can not only test the depth of understanding and reasoning capabilities"
  - [section] "without diverse question types for each narrative context, models might resort to learning shallow heuristics or patterns rather than developing a deep, semantic understanding"
- Break condition: If models still perform well by exploiting context-level patterns despite having multiple questions per context.

### Mechanism 3
- Claim: High annotation granularity with detailed explanations and knowledge point classification enables more precise model evaluation and targeted improvement.
- Mechanism: The benchmark includes detailed, human-verified explanations for >90% of problems and classifies each problem under specific knowledge components. This allows researchers to identify exactly where models fail and provides data for supervised fine-tuning.
- Core assumption: Detailed annotations improve both evaluation quality and enable more effective model training approaches.
- Evidence anchors:
  - [abstract] "our benchmark provides specific knowledge points for each problem and detailed explanations for each answer"
  - [section] "SceMQA includes detailed, human-verified explanations for each answer... each problem is classified into a specific knowledge component"
  - [section] "These explanations are useful for indentifying errors in model predictions and could also be instrumental in future supervised fine-tuning (SFT)"
- Break condition: If the additional annotation effort does not translate to better evaluation quality or training outcomes.

## Foundational Learning

- Concept: Multimodal reasoning combining visual understanding with scientific domain knowledge
  - Why needed here: SceMQA requires models to interpret diagrams, charts, and images while applying subject-specific knowledge in math, physics, chemistry, and biology
  - Quick check question: Can you explain how a model would need to integrate visual information from a chemistry diagram with chemical bonding knowledge to answer a question correctly?

- Concept: Curriculum learning and progressive difficulty scaling
  - Why needed here: The paper positions SceMQA as filling a gap between primary and college-level benchmarks, suggesting models benefit from intermediate difficulty progression
  - Quick check question: What would be the expected performance difference if a model trained only on primary-level benchmarks versus one that follows a primary→SceMQA→college progression?

- Concept: Error analysis and failure mode identification
  - Why needed here: The paper conducts detailed error analysis categorizing failures into reasoning errors, image perception errors, and knowledge gaps, which is essential for targeted model improvement
  - Quick check question: How would you distinguish between a reasoning error and an image perception error when a model fails to solve a physics problem involving a diagram?

## Architecture Onboarding

- Component map: Data collection → Problem annotation → Model evaluation → Error analysis → Insight generation
- Critical path: Data collection → Problem annotation → Model evaluation → Error analysis → Insight generation
- Design tradeoffs: The benchmark prioritizes annotation quality and educational relevance over size, choosing depth over breadth. This means fewer total problems but more comprehensive evaluation per problem.
- Failure signatures: Models failing on image interpretation tasks (physics optics, math continuity), struggling with calculation-heavy problems (integration, kinematics), and showing subject-specific weaknesses (better in conceptual chemistry/biology than calculation-heavy math/physics)
- First 3 experiments:
  1. Run baseline models on SceMQA to establish performance floor and identify subject-specific weaknesses
  2. Conduct ablation study removing either visual or textual components to quantify modality importance
  3. Test curriculum learning approach using SceMQA as intermediate step between primary and college benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of detailed explanations for each problem in SceMQA affect the learning and evaluation of Multimodal Large Language Models (MLLMs) compared to benchmarks without such explanations?
- Basis in paper: [explicit] The paper highlights that SceMQA includes detailed, human-verified explanations for each answer, which are useful for identifying errors in model predictions and could be instrumental in future supervised fine-tuning (SFT) and few-shot prompting methodologies.
- Why unresolved: The paper mentions the potential benefits of these explanations but does not empirically demonstrate their impact on MLLM learning and evaluation.
- What evidence would resolve it: Comparative studies showing the performance of MLLMs on SceMQA with and without access to the detailed explanations, measuring differences in accuracy and learning efficiency.

### Open Question 2
- Question: What specific challenges do MLLMs face when solving problems that involve identical contexts but varied questions, as presented in SceMQA?
- Basis in paper: [explicit] The paper states that SceMQA features problems with identical contexts but different questions to test the depth of understanding and reasoning capabilities of AI models.
- Why unresolved: The paper indicates that this format is challenging for AI models but does not provide detailed insights into the specific difficulties encountered by MLLMs.
- What evidence would resolve it: Analysis of error patterns and reasoning processes of MLLMs when solving these varied questions, identifying common pitfalls and areas where models struggle.

### Open Question 3
- Question: How does the performance of MLLMs on SceMQA compare to their performance on other multimodal benchmarks, particularly in terms of reasoning and visual understanding?
- Basis in paper: [explicit] The paper compares the performance of GPT-4 on SceMQA with other benchmarks, showing that SceMQA poses a moderate challenge, positioning between primary and college-level benchmarks.
- Why unresolved: While the paper provides a general comparison, it does not delve into detailed performance metrics across different types of reasoning and visual understanding tasks.
- What evidence would resolve it: Comprehensive performance analysis of MLLMs across multiple benchmarks, focusing on specific reasoning and visual understanding tasks, to identify strengths and weaknesses relative to SceMQA.

## Limitations
- Data provenance and licensing constraints limit reproducibility and independent verification of benchmark construction
- Evaluation methodology lacks transparency regarding human evaluator training, calibration, and inter-rater reliability
- Error analysis methodology is not fully specified, raising concerns about subjective bias in failure categorization

## Confidence

**Medium Confidence**: The claim that SceMQA fills a critical gap in the educational progression from primary to college-level benchmarks is moderately supported by the comparison of GPT-4 performance across different benchmarks. However, the evidence could be strengthened by including more extensive curriculum learning experiments and additional baseline comparisons.

**Medium Confidence**: The assertion that the one-context multiple-questions design improves evaluation of deep semantic understanding is conceptually sound and informed by prior research, but lacks direct empirical validation within the paper. The claim would benefit from ablation studies comparing single-question versus multiple-question contexts.

**Low Confidence**: The claim that detailed annotations will enable more effective model training through supervised fine-tuning is currently speculative, as the paper presents no experiments demonstrating improved model performance when using these annotations for training. This represents a forward-looking hypothesis rather than an established finding.

## Next Checks

1. **Ablation study on context-question design**: Run experiments comparing model performance on SceMQA's multiple-questions-per-context format versus a modified version with only single questions per context. This would provide direct empirical evidence for whether the multiple-questions design actually improves evaluation quality.

2. **Cross-cultural benchmark validation**: Evaluate whether the SceMQA benchmark maintains its difficulty calibration and relevance when applied to educational systems in different countries. This would test the generalizability of the "college entrance level" positioning beyond the Chinese context implied by the data sources.

3. **Annotation utility validation**: Conduct experiments using the detailed explanations and knowledge point annotations for supervised fine-tuning on a subset of the benchmark, then measure whether models trained with these annotations show improved performance compared to models trained without them. This would validate the claimed utility of the annotation work.