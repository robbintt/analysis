---
ver: rpa2
title: Convergent Privacy Loss of Noisy-SGD without Convexity and Smoothness
arxiv_id: '2410.01068'
source_url: https://arxiv.org/abs/2410.01068
tags:
- privacy
- lemma
- gradient
- bound
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves convergent R\'enyi differential privacy (RDP)
  bounds for noisy stochastic gradient descent (Noisy-SGD) without requiring convexity
  or smoothness assumptions. The key contribution is showing that having H\"older
  continuous gradients with order $\lambda \in (0,1]$ is sufficient for convergent
  privacy loss.
---

# Convergent Privacy Loss of Noisy-SGD without Convexity and Smoothness

## Quick Facts
- arXiv ID: 2410.01068
- Source URL: https://arxiv.org/abs/2410.01068
- Authors: Eli Chien; Pan Li
- Reference count: 40
- Primary result: Proves convergent RDP bounds for Noisy-SGD with H"older continuous gradients (λ ∈ (0,1]) without convexity or smoothness

## Executive Summary
This paper establishes convergent Rényi differential privacy (RDP) bounds for noisy stochastic gradient descent (Noisy-SGD) under significantly weaker assumptions than previous work. The key insight is that H"older continuous gradients with order λ ∈ (0,1] are sufficient to guarantee convergent privacy loss, extending beyond the traditional requirements of smoothness and convexity. The authors provide both general privacy bounds for non-smooth cases and strictly tighter bounds for smooth strongly convex losses. The analysis covers both subsampling without replacement and shuffled cyclic mini-batch settings, offering practical relevance for federated learning and differentially private optimization.

## Method Summary
The authors develop a unified framework for analyzing privacy loss in Noisy-SGD by leveraging shifted Rényi divergence techniques and careful tracking of Wasserstein distances between parameter distributions. The core mechanism tracks how gradient noise accumulates across iterations while accounting for the geometry of the loss landscape through H"older continuity parameters. The privacy analysis decouples the effects of noise addition from the optimization dynamics, allowing convergence guarantees even when traditional smoothness assumptions fail. The framework supports both independent sampling and shuffled mini-batch variants through careful handling of gradient computation dependencies.

## Key Results
- Proves convergent RDP bounds for Noisy-SGD with H"older continuous gradients (λ ∈ (0,1]) without requiring convexity or smoothness
- Provides strictly tighter privacy bounds for smooth strongly convex losses compared to prior work
- Supports both subsampling without replacement and shuffled cyclic mini-batch settings
- Demonstrates that privacy loss remains bounded under reasonable assumptions on gradient noise accumulation

## Why This Works (Mechanism)
The mechanism relies on the geometric properties of H"older continuous gradients, which control how loss surfaces vary across the parameter space. By introducing Gaussian noise to gradient updates, the method creates sufficient randomness to mask individual training examples while the H"older continuity ensures that optimization trajectories remain well-behaved. The shifted Rényi divergence technique allows tracking privacy loss accumulation by measuring how noise transforms the distribution of model parameters across iterations. The Wasserstein distance analysis captures how gradients from different data points spread through the parameter space, ensuring that privacy guarantees hold even when gradients vary significantly.

## Foundational Learning

1. **Rényi Differential Privacy (RDP)**
   - Why needed: Provides a unified framework for analyzing privacy loss accumulation across iterations
   - Quick check: Verify RDP composition properties align with expected (ε,δ)-DP guarantees

2. **H"older Continuity**
   - Why needed: Generalizes gradient smoothness assumptions to allow for non-differentiable or irregular loss surfaces
   - Quick check: Confirm λ ∈ (0,1] parameter matches observed gradient behavior in practice

3. **Wasserstein Distance**
   - Why needed: Measures distance between probability distributions of model parameters
   - Quick check: Validate that gradient noise spreads sufficiently to satisfy Wasserstein bounds

4. **Shifted Rényi Divergence**
   - Why needed: Tracks privacy loss accumulation when distributions evolve over iterations
   - Quick check: Ensure shifted divergence properly accounts for parameter distribution changes

5. **Subsampling without Replacement**
   - Why needed: More realistic than i.i.d. sampling for practical implementations
   - Quick check: Verify sampling probabilities match theoretical assumptions

6. **Shuffled Mini-Batches**
   - Why needed: Provides stronger privacy guarantees through gradient averaging
   - Quick check: Confirm batch size and shuffling frequency align with theoretical requirements

## Architecture Onboarding

**Component Map:**
Data → Gradient Computation → Gaussian Noise Addition → Parameter Update → Loss Evaluation → Convergence Check

**Critical Path:**
Gradient computation → Noise addition → Parameter update → Privacy tracking (via RDP accounting)

**Design Tradeoffs:**
- H"older continuity vs. smoothness: More general but potentially weaker convergence
- Noise scale vs. privacy: Higher noise provides better privacy but slower convergence
- Batch size vs. privacy accounting: Larger batches improve utility but complicate privacy analysis

**Failure Signatures:**
- Privacy loss fails to converge: Check H"older continuity assumptions or gradient bounds
- Optimization diverges: Verify noise scale and learning rate compatibility
- Tightness bounds not achieved: Reassess gradient variance assumptions

**First Experiments:**
1. Implement Noisy-SGD with synthetic H"older continuous loss to verify privacy convergence
2. Compare RDP bounds against existing implementations on MNIST with logistic regression
3. Test shuffled mini-batch privacy guarantees with varying batch sizes on CIFAR-10

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations

- Bounded gradient and loss assumptions lack explicit constants for practical implementations
- H"older continuity verification may be challenging for complex neural network architectures
- Analysis framework may not generalize beyond Gaussian noise mechanisms
- High-probability convergence bounds are not established
- Combining subsampling and shuffling techniques is left for future work
- Utility analysis assumes access to optimal solutions that may not be available in practice

## Confidence

- Privacy bounds for H"older continuous gradients: High
- Improved bounds for smooth strongly convex cases: High
- Convergence analysis framework: Medium
- Utility bounds applicability: Low

## Next Checks

1. Verify H"older continuity parameter λ for common deep learning architectures and assess practical impact on privacy bounds
2. Implement the privacy accountant from the paper and compare against existing implementations (Opacus, TensorFlow Privacy) on benchmark datasets
3. Test the bounds with non-Gaussian noise mechanisms to evaluate the generality of the analysis framework