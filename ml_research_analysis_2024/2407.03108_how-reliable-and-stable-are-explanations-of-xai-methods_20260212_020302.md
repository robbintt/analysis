---
ver: rpa2
title: How Reliable and Stable are Explanations of XAI Methods?
arxiv_id: '2407.03108'
source_url: https://arxiv.org/abs/2407.03108
tags:
- methods
- perturbations
- reliable
- feature
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the reliability and stability of XAI methods
  by testing their feature relevance rankings under data perturbations. Using the
  diabetes dataset and four ML models (LGBM, MLP, DT, KNN), the study generates explanations
  via six XAI methods: eXirt, Dalex, Eli5, Lofo, SHAP, and Skater.'
---

# How Reliable and Stable are Explanations of XAI Methods?

## Quick Facts
- arXiv ID: 2407.03108
- Source URL: https://arxiv.org/abs/2407.03108
- Reference count: 32
- This paper evaluates the reliability and stability of XAI methods by testing their feature relevance rankings under data perturbations.

## Executive Summary
This study investigates the reliability and stability of XAI methods by analyzing how their feature relevance rankings change under data perturbations. Using the diabetes dataset and four ML models (LGBM, MLP, DT, KNN), the research applies six XAI methods (eXirt, Dalex, Eli5, Lofo, SHAP, Skater) and measures their stability through Spearman correlation of feature rankings across perturbed test data (4%, 6%, 10% noise). The novel eXirt method, based on Item Response Theory, uniquely identifies model reliability through discrimination, difficulty, and guessing parameters, detecting unstable models via negative discrimination curves. Results show SHAP as the most stable method with perfect correlation across models, while most other methods show sensitivity to perturbations, highlighting limitations in real-world applications.

## Method Summary
The study evaluates XAI method stability through a systematic pipeline: first training four ML models on standardized diabetes dataset, then generating feature relevance rankings using six XAI methods on both unperturbed and perturbed test data (4%, 6%, 10% noise). Stability is measured via Spearman correlation between rankings across perturbation levels, while eXirt additionally assesses model reliability through IRT parameters (discrimination, difficulty, guessing) and visualizes these via ICC curves. The analysis reveals that while SHAP demonstrates perfect stability across all models, other methods show varying degrees of sensitivity to data perturbations, with eXirt providing unique insights into model reliability beyond standard ranking comparisons.

## Key Results
- SHAP demonstrates perfect Spearman correlation (1.0) across all models and perturbation levels, indicating maximum stability
- Most XAI methods show sensitivity to perturbations, with Skater being highly stable except for KNN model
- eXirt uniquely identifies model reliability through IRT parameters, detecting unstable models via negative discrimination curves
- Discrimination parameter analysis reveals specific instances where models show low prediction confidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IRT-based XAI methods (eXirt) can quantify model reliability through discrimination, difficulty, and guessing parameters.
- Mechanism: IRT models the probability of correct predictions as a function of latent model ability (Î¸) and item characteristics (discrimination, difficulty, guessing). By fitting these parameters, eXirt identifies stable models with high discrimination, low difficulty, and low guessing.
- Core assumption: The prediction process can be mapped to an IRT test-taking scenario where instances are items, features are commands, and predictions are answers.
- Evidence anchors:
  - [abstract] "eXirt, based on Item Response Theory, uniquely identifies model reliability through discrimination, difficulty, and guessing parameters, detecting unstable models via negative discrimination curves."
  - [section] "IRT seeks to evaluate unobservable latent characteristics of an individual, relating the probability of a correct answer to their latent traits... IRT consists of mathematical models that represent the probability of an individual getting an item correct, considering item parameters and the respondent's ability."
  - [corpus] Weak - no direct IRT-based XAI papers found in neighbors; this appears to be a novel contribution.
- Break condition: If the mapping between model predictions and IRT test-taking breaks down (e.g., non-binary outputs, highly correlated features), the IRT parameters lose interpretability.

### Mechanism 2
- Claim: Spearman correlation between feature relevance ranks across perturbations measures XAI stability.
- Mechanism: When perturbations are applied, stable XAI methods produce feature rankings that remain highly correlated with the unperturbed ranking. Perfect correlation indicates no sensitivity to input changes.
- Core assumption: Feature relevance rankings are comparable across perturbation levels and can be meaningfully correlated.
- Evidence anchors:
  - [abstract] "Using the diabetes dataset and four ML models... compares feature rankings using Spearman correlation and Bump charts."
  - [section] "Each column references different models... Spearman Correlation values existing between the ranks generated by models with the presence of perturbations in relation to the ranks generated by models without perturbations are also displayed."
  - [corpus] Weak - no direct Spearman correlation usage in XAI stability found in neighbors.
- Break condition: If feature rankings become incomparable due to extreme perturbations or if the correlation metric fails to capture meaningful changes in ranking structure.

### Mechanism 3
- Claim: Negative discrimination curves in ICC plots indicate model unreliability for specific instances.
- Mechanism: In IRT, negative discrimination means the model is less likely to predict correctly as instance difficulty increases, signaling prediction problems for those instances. eXirt visualizes these through ICC curves.
- Core assumption: Discrimination parameter sign reliably indicates instance-level prediction confidence.
- Evidence anchors:
  - [abstract] "eXirt... detecting unstable models via negative discrimination curves."
  - [section] "Discrimination: consists in how much a specific item i is able to differentiate between highly and poorly skilled respondents... Negative discrimination means a problem in the analyzed instance, indicating low confidence in the prediction."
  - [corpus] Weak - no direct ICC or discrimination curve analysis found in neighbors.
- Break condition: If negative discrimination occurs due to data distribution issues rather than model unreliability, or if the visualization becomes uninformative with many negative curves.

## Foundational Learning

- Concept: Item Response Theory (IRT) fundamentals
  - Why needed here: eXirt's entire reliability assessment mechanism depends on correctly interpreting IRT parameters (discrimination, difficulty, guessing).
  - Quick check question: If a model has high discrimination but also high guessing, is it more or less reliable than a model with moderate discrimination and low guessing?

- Concept: Spearman rank correlation
  - Why needed here: The stability assessment across XAI methods relies on comparing ranked feature lists using Spearman correlation.
  - Quick check question: What Spearman correlation value would indicate perfect stability across perturbations?

- Concept: Feature importance vs. feature relevance distinction
  - Why needed here: The paper explicitly distinguishes these concepts, with relevance based on model output and importance based on ground truth labels.
  - Quick check question: If two features have similar ground truth importance but different model relevance, which ranking should an XAI method prioritize?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model training -> Perturbation generator -> XAI explanation pipeline -> Evaluation metrics -> Statistical testing

- Critical path:
  1. Train models on clean data
  2. Generate unperturbed explanations
  3. Apply perturbations to test set
  4. Generate perturbed explanations
  5. Compute stability metrics
  6. Generate IRT reliability assessments

- Design tradeoffs:
  - Single dataset vs. multiple datasets: Chosen for controlled perturbation analysis but limits generalizability
  - Binary classification vs. multi-class: Simplifies analysis but may not capture complexity of other problem types
  - Four perturbation levels: Balances granularity with computational cost

- Failure signatures:
  - Low Spearman correlations across all methods suggest dataset sensitivity issues
  - Negative discrimination curves appearing even in unperturbed data suggest model architecture problems
  - Statistical tests showing no performance differences despite visual differences suggest sample size issues

- First 3 experiments:
  1. Run all XAI methods on unperturbed data only to establish baseline rankings
  2. Apply 4% perturbation and compare all methods' stability via Spearman correlation
  3. Generate ICC plots for each model to identify negative discrimination instances

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the discrimination, difficulty, and guessing properties from eXirt be combined into a single score for easier model reliability interpretation?
- Basis in paper: [explicit] The paper mentions this as a future work, highlighting the need for a unified metric to simplify model reliability assessment.
- Why unresolved: The current use of three separate IRT properties makes it harder to quickly interpret model reliability compared to a single score.
- What evidence would resolve it: Development and validation of a mathematical equation or formula that effectively combines the three IRT properties into a single reliability score.

### Open Question 2
- Question: How do XAI methods perform under different types of perturbations, such as adversarial attacks or data corruption, beyond random noise?
- Basis in paper: [inferred] The paper tested stability against random noise but did not explore other perturbation types, suggesting a gap in understanding robustness.
- Why unresolved: The study focused on controlled random noise perturbations, leaving the behavior under more complex or realistic perturbations unexplored.
- What evidence would resolve it: Testing XAI methods under diverse perturbation scenarios, such as adversarial attacks or structured data corruption, and comparing their stability.

### Open Question 3
- Question: How do XAI methods behave with regression and multiclass classification tasks, beyond the binary classification used in this study?
- Basis in paper: [explicit] The paper suggests testing eXirt with other prediction problems like regression and multiclass classification as future work.
- Why unresolved: The current evaluation is limited to binary classification, so the generalizability of XAI methods to other task types is unclear.
- What evidence would resolve it: Applying and evaluating the XAI methods on regression and multiclass classification datasets to assess their stability and reliability.

## Limitations

- The study uses a single dataset (diabetes), limiting generalizability to other domains and problem types
- Analysis is restricted to binary classification, leaving performance on regression and multiclass tasks unexplored
- Only random noise perturbations are tested, without evaluating more complex perturbation types like adversarial attacks

## Confidence

The study demonstrates Medium confidence in its core findings about XAI stability. The novel IRT-based eXirt approach shows promise but lacks extensive validation against established IRT benchmarks. The perturbation analysis reveals method sensitivities but is constrained by using a single dataset, limiting generalizability. SHAP's perfect stability across models appears exceptional and warrants deeper investigation into whether this reflects true robustness or potential overfitting to the diabetes dataset structure. The negative discrimination detection mechanism is theoretically sound but requires empirical validation on models with known reliability issues.

## Next Checks

1. Replicate the stability analysis on at least three additional diverse datasets (classification and regression) to test generalizability of the XAI method rankings.
2. Conduct ablation studies removing SHAP from the comparison to determine if its perfect correlation score significantly influences overall method rankings.
3. Implement ground truth feature importance based on domain expertise or causal analysis to validate the distinction between feature relevance and importance across all XAI methods.