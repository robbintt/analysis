---
ver: rpa2
title: Learning-to-Rank with Nested Feedback
arxiv_id: '2401.04053'
source_url: https://arxiv.org/abs/2401.04053
tags:
- feed
- learning
- user
- ranking
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of learning-to-rank in nested feed
  interfaces where positive feedback on lower-level items should be attributed to
  higher-level items. The authors propose a method to incorporate 2nd-level feedback
  into 1st-level ranking models by creating combined relevance signals.
---

# Learning-to-Rank with Nested Feedback

## Quick Facts
- arXiv ID: 2401.04053
- Source URL: https://arxiv.org/abs/2401.04053
- Reference count: 40
- The paper proposes a method to incorporate 2nd-level feedback into 1st-level ranking models by creating combined relevance signals, showing improved offline DCG metrics and online engagement metrics in experiments on a short-video recommendation system.

## Executive Summary
This paper addresses the challenge of learning-to-rank in nested feed interfaces where user interactions with lower-level items should be attributed to higher-level items. The authors propose a method to incorporate 2nd-level feedback into 1st-level ranking models by creating combined relevance signals. Theoretical analysis under a Position-Based Model shows that summing L2 feedback is optimal for maximizing overall ranking quality. Experiments on a large-scale short-video recommendation system demonstrate that incorporating L2 feedback improves both offline DCG metrics and online engagement metrics, including increased likes, shares, favorites, and user retention.

## Method Summary
The method involves collecting user interaction logs at both L1 and L2 levels, engineering features and combining L2 feedback into L1 relevance labels using summation (not discounting), and training standard LTR models (e.g., YetiRank with Catboost) on the transformed dataset. The key insight is that under a Position-Based Model assumption, summing L2 feedback into L1 relevance labels creates an unbiased estimator of total user satisfaction, allowing standard LTR algorithms to optimize for the combined signal.

## Key Results
- Theoretical analysis shows that summing L2 feedback (without discounting) is optimal for maximizing overall ranking quality under a Position-Based Model
- Experiments demonstrate improved offline DCG metrics (DCG@3, DCG@5, DCG@10) when using combined L1+L2 feedback labels
- Online A/B testing shows increased user engagement metrics including likes, shares, favorites, and retention when incorporating L2 feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The L2 feedback signal should be summed into L1 relevance labels to maximize overall ranking quality.
- Mechanism: By assuming the Position-Based Model (PBM), the authors show that treating the combined relevance signal as `RA(u,ai) + Σ RB(u,bij)` creates an unbiased estimator of the total user satisfaction metric Q.
- Core assumption: PBM holds, meaning user viewing probability at position i is `1/log2(1+i)`, and the L2 ranking is fixed (non-optimized).
- Evidence anchors:
  - [abstract] "Theoretical analysis under a Position-Based Model shows that summing L2 feedback is optimal for maximizing overall ranking quality."
  - [section] "From the very definition of Q it is seen that we should define eR(u, ai) = RA(u, ai) + Σ RB(u, bij) as the new relevance signal for the item ai."
- Break condition: If the L2 ranking is not fixed, or if PBM does not hold, the sum may overcount or undercount L2 feedback.

### Mechanism 2
- Claim: The L2 relevance signal should not be discounted by position in the L1 feed when incorporated.
- Mechanism: Since the L2 feed ranking is fixed, the observed L2 relevance signals are already positionally biased. Applying DCG-style discounting would incorrectly cancel out this bias.
- Core assumption: L2 ranking is fixed, so debiasing L2 feedback with position-based discounting would double-count the bias.
- Evidence anchors:
  - [section] "To see that, we can formally write: ... those debiased labels have the same denominator as the positional bias and, thus, it cancels out."
- Break condition: If L2 ranking is optimized or varies per L1 item, then position-based discounting of L2 feedback may become necessary.

### Mechanism 3
- Claim: The transformed dataset with combined relevance labels can be fed into standard LTR algorithms.
- Mechanism: By relabeling each L1 item with `Σ RB(u,bij)`, the problem reduces to standard LTR with one relevance label per L1 item, allowing use of LambdaRank, YetiRank, etc.
- Core assumption: Standard LTR methods can optimize for the new composite relevance signal without modification.
- Evidence anchors:
  - [section] "The dataset {(u, A,eY)} now resembles a classic LTR dataset and, therefore, can serve as input to well-established LTR methods."
- Break condition: If standard LTR methods assume single-level feedback, they may not fully exploit the hierarchical structure.

## Foundational Learning

- Concept: Position-Based Model (PBM)
  - Why needed here: PBM is used to justify why summing L2 feedback (without discounting) yields an unbiased estimator of total user satisfaction.
  - Quick check question: What is the viewing probability at position i under PBM, and how does it relate to the discounting factor in DCG?
- Concept: Counterfactual inference for bias correction
  - Why needed here: The paper mentions debiasing L2 feedback, which requires understanding how to correct for positional bias in implicit feedback.
  - Quick check question: Why does debiasing L2 feedback with position-based discounting cancel out under a fixed L2 ranking?
- Concept: Learning-to-Rank (LTR) algorithms (e.g., LambdaRank, YetiRank)
  - Why needed here: The combined relevance labels are fed into standard LTR algorithms, so understanding their objectives and training procedures is essential.
  - Quick check question: How does YetiRank optimize for DCG, and why is it suitable for the transformed dataset?

## Architecture Onboarding

- Component map: Data collection -> Feature engineering -> Label transformation -> Model training -> Evaluation
- Critical path:
  1. Collect user interaction logs at both L1 and L2 levels.
  2. Engineer features and combine L2 feedback into L1 relevance labels.
  3. Train LTR model (e.g., YetiRank) on transformed dataset.
  4. Evaluate offline DCG and deploy for online A/B testing.
  5. Monitor engagement and retention metrics.
- Design tradeoffs:
  - Summation vs. discounting of L2 feedback: Summation is theoretically justified under fixed L2 ranking, but may overcount if L2 ranking is optimized.
  - Fixed L2 ranking vs. joint optimization: Keeping L2 ranking fixed simplifies the problem but may miss potential gains from end-to-end optimization.
  - Feature complexity: More features may improve model performance but increase training time and risk of overfitting.
- Failure signatures:
  - Online metrics do not improve despite offline DCG gains: Indicates a mismatch between offline evaluation and online user behavior.
  - L2 feedback is not properly attributed: May result from incorrect label transformation or feature engineering errors.
  - Model overfits to L2 feedback: Can occur if L2 feedback is sparse or noisy.
- First 3 experiments:
  1. Compare DCG@3, DCG@5, DCG@10 when training on S1 (L1 only), S2 (L1 + discounted L2), and S3 (L1 + summed L2) labels.
  2. A/B test S1 vs. S3 online, measuring engagement (likes, shares, favorites), dwell time, and user retention.
  3. Analyze the distribution of posts suggested by each variant, focusing on personalized vs. non-personalized candidate generators.

## Open Questions the Paper Calls Out

- Question: How does the performance of the proposed method change when the ranking policies for both L1 and L2 feeds are jointly optimized rather than keeping the L2 ranking fixed?
- Question: How does the optimal method for combining L2 feedback (sum vs. discounted) vary across different types of platforms or content domains?
- Question: What is the optimal weight for balancing L1 and L2 feedback signals when they have different distributions or correlation structures?

## Limitations

- The theoretical analysis assumes a fixed L2 ranking and relies on PBM assumptions that may not hold in practice.
- The method's performance may vary across different types of nested feeds (e.g., news feeds vs. video recommendations).
- The approach depends heavily on the quality and volume of L2 feedback signals, which may be sparse or noisy in some applications.

## Confidence

- High confidence: The theoretical framework under PBM assumptions and the basic methodology of combining L2 feedback through summation
- Medium confidence: The offline experimental results showing DCG improvements, as they are based on proprietary data without full reproducibility details
- Medium confidence: The online experimental results showing engagement improvements, though sample size and duration are not specified

## Next Checks

1. Test the method on datasets where L2 ranking is optimized rather than fixed to validate robustness beyond the theoretical assumptions
2. Conduct ablation studies to determine the optimal weighting scheme for combining different types of L2 feedback (likes, shares, favorites)
3. Evaluate the method across different platform types (news, video, e-commerce) to assess generalizability beyond the short-video recommendation domain