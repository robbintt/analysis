---
ver: rpa2
title: 'MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale'
arxiv_id: '2409.00134'
source_url: https://arxiv.org/abs/2409.00134
tags:
- mapf
- agents
- learning
- maps
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAPF-GPT, a foundation model for multi-agent
  pathfinding (MAPF) that uses imitation learning to train a transformer-based neural
  network on a dataset of 1 billion expert trajectories. MAPF-GPT predicts agent actions
  based on local observations without requiring additional planning or communication
  mechanisms.
---

# MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale

## Quick Facts
- arXiv ID: 2409.00134
- Source URL: https://arxiv.org/abs/2409.00134
- Authors: Anton Andreychuk; Konstantin Yakovlev; Aleksandr Panov; Alexey Skrynnik
- Reference count: 32
- Primary result: MAPF-GPT achieves up to 99% success rate in multi-agent pathfinding without requiring additional planning or communication mechanisms

## Executive Summary
MAPF-GPT introduces a foundation model for multi-agent pathfinding that uses imitation learning to train a transformer-based neural network on a dataset of 1 billion expert trajectories. The model predicts agent actions based on local observations, eliminating the need for additional planning or communication mechanisms. Evaluated on diverse MAPF benchmarks, MAPF-GPT-85M demonstrates superior performance compared to existing learnable MAPF solvers like DCC and SCRIMP, particularly on out-of-distribution tasks.

## Method Summary
MAPF-GPT employs a transformer-based architecture trained through imitation learning on expert trajectories. The model takes local observations as input and directly predicts agent actions, operating in a decentralized manner without requiring inter-agent communication. The training dataset consists of 1 billion expert trajectories, enabling the model to learn effective pathfinding strategies. Multiple model sizes are presented (85M, 6M, and 2M parameters), allowing for trade-offs between performance and computational efficiency.

## Key Results
- MAPF-GPT-85M achieves up to 99% success rate on diverse MAPF benchmarks
- Outperforms existing learnable MAPF solvers like DCC and SCRIMP, especially on out-of-distribution tasks
- Smaller variants (6M, 2M parameters) demonstrate strong performance with significantly faster inference times
- Ablation studies confirm the importance of cost-to-go and greedy action features
- Demonstrates zero-shot learning capability in lifelong MAPF tasks and benefits from fine-tuning

## Why This Works (Mechanism)
MAPF-GPT's success stems from its ability to learn complex pathfinding strategies directly from expert demonstrations at scale. The transformer architecture effectively captures long-range dependencies and spatial relationships in the environment, while the massive training dataset enables robust generalization across diverse scenarios. By predicting actions based on local observations without requiring communication, the model achieves decentralized operation that scales efficiently with the number of agents.

## Foundational Learning
- **Imitation Learning**: Learning from expert demonstrations to replicate successful strategies
  - Why needed: Provides high-quality training signals without requiring reward engineering
  - Quick check: Verify that expert trajectories cover diverse scenarios and edge cases

- **Transformer Architecture**: Neural network design capable of handling sequential and spatial data
  - Why needed: Captures complex dependencies between agents and environmental features
  - Quick check: Confirm attention mechanisms effectively model agent interactions

- **Large-Scale Training**: Utilizing 1 billion expert trajectories for comprehensive learning
  - Why needed: Ensures robust generalization across diverse MAPF scenarios
  - Quick check: Validate dataset diversity and balance across different task types

- **Decentralized Decision-Making**: Each agent makes decisions based on local observations
  - Why needed: Enables scalable operation without communication overhead
  - Quick check: Test performance with varying observation radii and agent densities

## Architecture Onboarding

**Component Map**: Input Observations -> Transformer Encoder -> Action Prediction -> Output Actions

**Critical Path**: The model processes local observations through the transformer encoder, which generates contextual embeddings that inform action predictions for each agent independently.

**Design Tradeoffs**: The choice of transformer architecture enables powerful modeling capabilities but increases computational requirements. Multiple model sizes (85M, 6M, 2M parameters) provide flexibility for different deployment scenarios.

**Failure Signatures**: Performance degradation may occur in highly congested scenarios or when agents have limited local observations. The model may struggle with tasks requiring long-term strategic planning beyond its observation window.

**First Experiments**:
1. Validate core functionality by testing on simple grid environments with known optimal solutions
2. Compare performance across different model sizes on standard benchmark tasks
3. Evaluate generalization by testing on out-of-distribution map sizes and agent densities

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on success rate metrics, with limited characterization of path optimality and computational efficiency
- 99% success rate claim requires scrutiny regarding evaluation protocol definitions
- Real-world deployment scenarios and scalability beyond tested map sizes remain incompletely characterized

## Confidence

- **High confidence**: Technical implementation details of transformer architecture and training methodology are well-documented and reproducible
- **Medium confidence**: Comparative performance claims against existing solvers may not fully capture real-world deployment scenarios
- **Medium confidence**: Zero-shot learning capabilities claim needs additional validation across broader task distributions

## Next Checks

1. Conduct comprehensive runtime efficiency analysis comparing MAPF-GPT's inference latency against traditional search-based solvers across different hardware configurations
2. Perform extensive out-of-distribution testing with map sizes and agent densities beyond the training distribution to validate generalization claims
3. Implement systematic comparison of solution quality (path lengths, makespan) between MAPF-GPT and optimal solvers to quantify optimality trade-offs