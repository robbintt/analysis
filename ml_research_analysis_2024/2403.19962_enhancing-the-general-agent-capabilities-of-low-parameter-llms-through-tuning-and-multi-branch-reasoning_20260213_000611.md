---
ver: rpa2
title: Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning
  and Multi-Branch Reasoning
arxiv_id: '2403.19962'
source_url: https://arxiv.org/abs/2403.19962
tags:
- llms
- agent
- tasks
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of enhancing the agent capabilities
  of low-parameter open-source LLMs (7B and 13B), which currently underperform compared
  to commercial models like ChatGPT and GPT-4 on agent tasks. The core method involves
  two main strategies: (1) supervised fine-tuning (SFT) using agent-specific data
  constructed with GPT-4, and (2) multi-path reasoning enhanced by task decomposition
  and backtracking.'
---

# Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning

## Quick Facts
- arXiv ID: 2403.19962
- Source URL: https://arxiv.org/abs/2403.19962
- Reference count: 3
- One-line primary result: Low-parameter LLMs (7B, 13B) achieve up to 5-6% improvement on household tasks and 1% on web shopping tasks through supervised fine-tuning with agent-specific data and multi-path reasoning

## Executive Summary
This paper addresses the challenge of enhancing agent capabilities in low-parameter open-source LLMs (7B and 13B), which currently underperform compared to commercial models like ChatGPT and GPT-4 on agent tasks. The authors propose a two-pronged approach: supervised fine-tuning (SFT) using agent-specific data constructed with GPT-4, and multi-path reasoning enhanced by task decomposition and backtracking. The agent-specific data construction simulates multi-turn dialogues with different roles, while task decomposition breaks down complex tasks into manageable subtasks, and backtracking explores multiple reasoning paths. Experimental results on AgentBench tasks show that the proposed approach improves performance compared to standard methods.

## Method Summary
The proposed method combines supervised fine-tuning with multi-path reasoning enhanced by task decomposition and backtracking. Agent-specific data is constructed using GPT-4 to simulate multi-turn dialogues representing realistic agent-environment interactions. The LLMs are then fine-tuned using this data mixed with general instruction tuning data via Low-Rank Adaptation (LORA). For complex tasks, the approach decomposes them into smaller subtasks and explores multiple reasoning paths with backtracking to avoid suboptimal solutions. The method is evaluated on 7B and 13B LLMs across various agent tasks including household activities, web shopping, and database operations.

## Key Results
- Supervised fine-tuning with agent-specific data significantly reduces hallucination outputs and formatting errors in agent tasks
- Task decomposition improves performance on planning-intensive tasks by breaking complex multi-step tasks into manageable subtasks
- Multi-path reasoning with backtracking increases the likelihood of finding optimal solutions for complex agent tasks with vast search spaces
- 13B models show specific improvements of up to 5-6% on household tasks and 1% on web shopping tasks compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning (SFT) with agent-specific data reduces hallucination outputs and formatting errors in 7B and 13B LLMs during agent tasks.
- Mechanism: The fine-tuning process exposes the model to curated multi-turn dialogue trajectories that simulate realistic agent-environment interactions, training the model to follow correct formatting and logical reasoning patterns.
- Core assumption: Agent-specific data construction captures the full range of interactive behaviors needed for effective agent performance.
- Evidence anchors:
  - [abstract] "supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks."
  - [section 3.2] "We propose to use GPT-4 to construct data...captures the interaction between different roles."
  - [corpus] Found 25 related papers with average neighbor FMR=0.515; however, no direct citations to this specific claim in the corpus.

### Mechanism 2
- Claim: Task decomposition breaks down complex multi-step tasks into manageable subtasks, reducing problem complexity and improving agent performance.
- Mechanism: By decomposing a complex task into smaller, simpler subtasks, the model can focus on completing each subtask sequentially, leveraging its existing reasoning capabilities more effectively.
- Core assumption: LLMs with smaller parameter sizes have limited long-term memory capacity, making it difficult to handle complex tasks without decomposition.
- Evidence anchors:
  - [section 3.3] "Task decomposition leverages the task planning capability of the LLMs to decompose complex and lengthy tasks into several smaller subtasks."
  - [section 4.2] "We find that task decomposition is more effective for agent tasks that emphasize planning abilities."
  - [corpus] No direct corpus evidence supporting this specific claim; assumption based on general LLM limitations.

### Mechanism 3
- Claim: Multi-path reasoning with backtracking allows the model to explore alternative reasoning paths, increasing the likelihood of finding optimal solutions for complex agent tasks.
- Mechanism: The model generates multiple available actions at each reasoning step, and a judge model selects one action to continue. If a reasoning path yields a suboptimal output, the model backtracks and explores alternative paths.
- Core assumption: A single inference path is unlikely to yield the optimal solution for complex tasks with vast search spaces.
- Evidence anchors:
  - [section 3.3] "Agent tasks in the real world are often complex and one single reasoning path may not yield the optimal answer...we propose to take multi-path reasoning with LLMs."
  - [section 4.2] "We conduct experiments of 'num path' shown in Tab. 3 left. It can be seen that appropriately increasing 'num path' can improve performance."
  - [corpus] No direct corpus evidence supporting this specific claim; assumption based on general search space complexity.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT is used to fundamentally improve the agent capabilities of low-parameter LLMs by exposing them to agent-specific data.
  - Quick check question: What is the purpose of mixing general instruction tuning data with agent-specific data during SFT?

- Concept: Task Decomposition
  - Why needed here: Task decomposition is used to break down complex multi-step tasks into smaller, more manageable subtasks, reducing problem complexity for LLMs with limited memory capacity.
  - Quick check question: How does the planning module (Mp) generate a list of subtasks for a given complex task?

- Concept: Backtracking
  - Why needed here: Backtracking is used to explore alternative reasoning paths when a particular path yields a suboptimal output, increasing the likelihood of finding optimal solutions for complex agent tasks.
  - Quick check question: What is the purpose of the backtracking prompt, and how does it guide the model to avoid previously deduced reasoning paths?

## Architecture Onboarding

- Component map: GPT-4 for data construction -> SFT with LORA -> Planning module (Mp) -> Judge module (Mjdg) -> Multi-path reasoning with backtracking
- Critical path: Data construction → SFT with LORA → Task decomposition → Multi-path reasoning with backtracking → Agent task execution
- Design tradeoffs:
  - More agent-specific data vs. general instruction data in SFT (λ parameter)
  - Number of subtasks in task decomposition (k parameter)
  - Number of reasoning paths explored (num path parameter)
  - Number of nodes expanded at each reasoning step (num branch parameter)
- Failure signatures:
  - High hallucination rates or formatting errors during agent tasks (SFT failure)
  - Inability to complete complex tasks (task decomposition failure)
  - Suboptimal task completion or getting stuck in infinite loops (multi-path reasoning failure)
- First 3 experiments:
  1. Fine-tune a 7B LLM with a small amount of agent-specific data and evaluate its performance on a simple agent task (e.g., ALFWorld).
  2. Implement task decomposition on a 7B LLM and evaluate its performance on a complex agent task (e.g., WebShop) compared to a baseline without decomposition.
  3. Implement multi-path reasoning with backtracking on a 7B LLM and evaluate its performance on a complex agent task (e.g., Operating System) compared to a baseline without backtracking.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal mixing ratio λ of general instruction data to agent-specific data vary across different agent task domains and model sizes?
- Basis in paper: [explicit] The paper states "We observe that deterioration of the general ability of LLMs will also decrease the agent ability, so we set a small value for λ" and provides experimental results showing λ values of 0.1, 0.3, 0.5, and 0.8.
- Why unresolved: The paper only tests a limited range of λ values and doesn't systematically explore how the optimal ratio varies across different task types (planning vs. API usage) or model sizes.
- What evidence would resolve it: A comprehensive study varying λ across all task types and model sizes, showing performance curves and identifying optimal ratios for each combination.

### Open Question 2
- Question: How does the effectiveness of task decomposition versus backtracking vary across different types of agent tasks (planning-intensive vs. API-intensive)?
- Basis in paper: [explicit] The paper states "We find that task decomposition is more effective for agent tasks that emphasize planning abilities, while backtracking is more effective for agent tasks that emphasize API invocation capabilities."
- Why unresolved: The paper provides qualitative observations but lacks quantitative analysis comparing the two techniques across different task categories or investigating potential synergies between them.
- What evidence would resolve it: A systematic comparison of decomposition vs. backtracking across all task types, with statistical analysis of performance differences and investigation of hybrid approaches.

### Open Question 3
- Question: What is the relationship between model parameter size and the effectiveness of multi-path reasoning techniques like backtracking?
- Basis in paper: [inferred] The paper focuses on 7B and 13B models and mentions that "For LLMs with small parameter sizes, due to their limited long-term memory capacity, it is challenging for them to handle complex long dialogue tasks."
- Why unresolved: The experiments are limited to 7B and 13B models, and the paper doesn't explore how reasoning techniques scale with model size or whether larger models benefit differently from these approaches.
- What evidence would resolve it: Experiments comparing reasoning technique effectiveness across a wider range of model sizes (e.g., 1B, 7B, 13B, 30B, 70B) with statistical analysis of performance trends.

## Limitations

- The data construction process using GPT-4 lacks detailed methodology and validation, making it difficult to assess whether improvements stem from genuine capability enhancement or overfitting to specific dialogue patterns
- The interactions between task decomposition and multi-path reasoning are not fully characterized, with unclear complementarity or redundancy in certain task domains
- Claims about significant reduction in hallucination outputs are asserted but not empirically validated with specific hallucination metrics or error rate measurements

## Confidence

- **High confidence:** The general effectiveness of supervised fine-tuning with task-specific data to improve LLM agent performance
- **Medium confidence:** The specific mechanisms of task decomposition and backtracking for complex agent tasks, with relatively modest improvements (1-6%)
- **Low confidence:** The claim that SFT "significantly reduces hallucination outputs and formatting errors" without empirical validation

## Next Checks

1. Conduct ablation studies isolating the impact of task decomposition vs. multi-path reasoning on the same task suite, with quantitative measurements of reasoning steps and computational overhead for each approach

2. Implement a blind evaluation where human judges assess whether outputs from fine-tuned models demonstrate genuinely improved reasoning versus memorized patterns from the training data

3. Test the approach's generalization by evaluating on agent tasks from domains not represented in the AgentBench training data, measuring performance degradation as an indicator of overfitting