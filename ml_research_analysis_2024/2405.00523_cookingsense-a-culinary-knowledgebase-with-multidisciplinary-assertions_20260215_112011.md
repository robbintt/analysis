---
ver: rpa2
title: 'CookingSense: A Culinary Knowledgebase with Multidisciplinary Assertions'
arxiv_id: '2405.00523'
source_url: https://arxiv.org/abs/2405.00523
tags: []
core_contribution: This paper introduces CookingSense, a culinary knowledgebase built
  from diverse sources including web data, scientific papers, and recipes. CookingSense
  is constructed using dictionary-based and language model-based filtering techniques,
  resulting in rich multidisciplinary food-related assertions.
---

# CookingSense: A Culinary Knowledgebase with Multidisciplinary Assertions

## Quick Facts
- arXiv ID: 2405.00523
- Source URL: https://arxiv.org/abs/2405.00523
- Reference count: 0
- One-line primary result: CookingSense is a large-scale culinary knowledgebase with 54 million assertions across multiple culinary facets, improving retrieval-augmented language model performance on culinary tasks.

## Executive Summary
CookingSense is a comprehensive culinary knowledgebase constructed from diverse sources including web data, scientific papers, and recipes. The knowledgebase contains 54 million assertions covering various culinary facets such as common sense, culinary arts, health & nutrition, culinary culture, and food management & safety. Through dictionary-based and language model-based filtering techniques, CookingSense captures multidisciplinary food-related assertions. The authors introduce FoodBench, a novel benchmark for evaluating culinary decision support systems, and demonstrate that CookingSense improves the performance of retrieval-augmented language models on these tasks.

## Method Summary
CookingSense is constructed through a multi-stage pipeline starting with data collection from three sources: C4 web corpus, Semantic Scholar API for scientific papers, and Recipe1M+ recipes. The pipeline applies 27 handcrafted filtering rules from GenericsKB to remove non-generic assertions, followed by dictionary-based filtering using culinary term dictionaries to eliminate non-culinary content. Assertions are then categorized into six types using a BERT classifier trained on a balanced dataset. The final knowledgebase is evaluated using retrieval-augmented generation with Okapi-BM25 retriever and Flan-T5 language model on the FoodBench benchmark covering question answering, flavor prediction, and cultural perspective tasks.

## Key Results
- CookingSense contains 54 million assertions across six culinary facets, with distribution varying by source type
- Retrieval-augmented language models using CookingSense show improved performance on FoodBench tasks compared to baseline knowledgebases
- Qualitative analysis reveals CookingSense provides diverse information including common sense, cultural perspectives, and expert knowledge from different sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CookingSense improves RAG performance because it provides richer, more diverse knowledge assertions than baseline KBs
- Mechanism: Combining three diverse data sources (web text, scientific papers, recipes) with filtering removes noise and captures common sense, cultural, and expert knowledge, enabling RAG models to retrieve more contextually relevant information
- Core assumption: Diverse sources + filtering → higher quality assertions → better RAG performance
- Evidence anchors: Paper claims filtering produces "rich knowledgebase of multidisciplinary food-related assertions" and shows experimental improvement
- Break condition: If filtering fails to remove noise or assertions are too generic, performance gain may not materialize

### Mechanism 2
- Claim: Large scale (54 million assertions) enables coverage across multiple culinary facets, improving task performance
- Mechanism: A large KB with assertions categorized into facets ensures retrieval systems can find relevant knowledge for various tasks, improving recall and accuracy
- Core assumption: Larger, faceted KB → better coverage → improved downstream performance
- Evidence anchors: KB statistics show distribution across culinary facets and sources
- Break condition: If most assertions are low quality or irrelevant, scale alone won't improve performance

### Mechanism 3
- Claim: Unsupervised semantic categorization using BERT improves usability by allowing targeted retrieval based on culinary facets
- Mechanism: Training BERT classifier on labeled assertions enables retrieval of context-specific knowledge aligned with task needs
- Core assumption: Fine-grained categorization → better task alignment → better performance
- Evidence anchors: Classifier achieved 0.76 accuracy on test split
- Break condition: If classifier mislabels assertions or categories are too coarse, retrieval specificity is lost

## Foundational Learning

- Concept: Knowledge graph construction and filtering pipelines
  - Why needed here: CookingSense's construction relies on filtering noisy, irrelevant data from multiple sources to create high-quality assertions
  - Quick check question: What are the two main types of filtering used in CookingSense's construction?

- Concept: Retrieval-augmented generation (RAG) with BM25
  - Why needed here: RAG experiments use Okapi-BM25 as retriever, which relies on term and document frequency for ranking
  - Quick check question: Why was BM25 chosen over more complex retrievers?

- Concept: Multilingual and multi-domain data integration
  - Why needed here: CookingSense combines web data, scientific papers, and recipes with different linguistic characteristics and knowledge types
  - Quick check question: How does the source of data affect the type of knowledge assertions generated?

## Architecture Onboarding

- Component map: Data ingestion → Text splitting → Generic filtering → Irrelevant filtering → Semantic categorization → KB storage → RAG evaluation
- Critical path: Data ingestion → filtering → categorization → KB usage in RAG
- Design tradeoffs: Larger KB size vs. quality, unsupervised categorization vs. manual annotation, simple BM25 vs. learned retrievers
- Failure signatures:
  - Low RAG performance → filtering may be ineffective or assertions too generic
  - Poor categorization → BERT classifier accuracy issues or imbalanced categories
  - Missing knowledge in retrieval → insufficient source coverage or filtering too aggressive
- First 3 experiments:
  1. Run RAG on FoodBench with and without CookingSense; compare accuracy
  2. Test filtering pipeline on a small sample of assertions; check noise removal rate
  3. Evaluate BERT classifier on held-out validation set; measure category accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CookingSense compare to domain-specific culinary knowledge bases when used in retrieval-augmented language models?
- Basis in paper: Paper mentions CookingSense outperforms baseline KBs but notes FooDB performed best for FORK benchmark
- Why unresolved: No comprehensive comparison against other culinary-specific knowledge bases across all FoodBench tasks
- What evidence would resolve it: Detailed experimental comparison of CookingSense with other culinary KBs across all FoodBench tasks

### Open Question 2
- Question: What is the impact of incorporating multilingual data sources on the performance and coverage of CookingSense?
- Basis in paper: Paper mentions current data sources are in English, limiting cultural nuance collection
- Why unresolved: Paper does not explore benefits of including multilingual data sources
- What evidence would resolve it: Experimental study comparing CookingSense performance with and without multilingual data sources

### Open Question 3
- Question: How does the quality of assertions in CookingSense vary across different sources and types?
- Basis in paper: Qualitative analysis highlights diverse and complementary information from different sources and types
- Why unresolved: No quantitative assessment of assertion quality across sources and types
- What evidence would resolve it: Comprehensive evaluation of assertion quality using human annotation or automated metrics stratified by source and type

## Limitations

- Limited evidence for filtering pipeline effectiveness - no quantitative quality metrics or false positive/negative rates provided
- Narrow experimental evaluation covering only three task types on FoodBench benchmark
- 0.76 accuracy for BERT categorization suggests potential misclassifications affecting downstream performance

## Confidence

**High Confidence**: Core construction methodology clearly described and reproducible; knowledgebase size and source distribution statistics are verifiable

**Medium Confidence**: Claim that CookingSense improves RAG performance on FoodBench tasks supported by experiments, but evaluation limited to single RAG setup and narrow task types

**Low Confidence**: Quality and diversity of 54 million assertions cannot be independently verified; claims about capturing common sense, cultural, and expert knowledge are qualitative assertions without systematic validation

## Next Checks

1. Sample 100 assertions from each source category (post-filtering) and manually evaluate filtering pipeline precision in removing irrelevant/non-generic content

2. Perform ablation studies by removing categories with lowest classifier confidence scores and measure impact on downstream task performance

3. Compare CookingSense's performance against alternative culinary KBs using same RAG architecture and FoodBench tasks to isolate KB quality contribution