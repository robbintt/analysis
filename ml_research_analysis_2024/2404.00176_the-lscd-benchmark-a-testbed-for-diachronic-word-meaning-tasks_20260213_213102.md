---
ver: rpa2
title: 'The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks'
arxiv_id: '2404.00176'
source_url: https://arxiv.org/abs/2404.00176
tags:
- word
- lscd
- task
- schlechtweg
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of a standardized evaluation setup
  for the complex task of Lexical Semantic Change Detection (LSCD), which combines
  Word-in-Context (WiC) and Word Sense Induction (WSI). The authors present a benchmark
  repository that implements evaluation procedures for these tasks across multiple
  high-quality, human-annotated datasets in various languages.
---

# The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks

## Quick Facts
- arXiv ID: 2404.00176
- Source URL: https://arxiv.org/abs/2404.00176
- Authors: Dominik Schlechtweg; Sachin Yadav; Nikolay Arefyev
- Reference count: 0
- One-line primary result: New SOTA on graded change and COMPARE tasks using XL-DURel with APD

## Executive Summary
This work addresses the lack of standardized evaluation for Lexical Semantic Change Detection (LSCD) by presenting a benchmark repository that implements evaluation procedures for WiC, WSI, and LSCD tasks across multiple high-quality, human-annotated datasets in various languages. The benchmark reflects the modularity of LSCD, allowing evaluation of individual subtasks and their combinations, which enables systematic comparison of state-of-the-art models. Using this benchmark, the authors establish new SOTA results, finding that XL-DURel with Average Pairwise Distance (APD) performs best, with discretization slightly improving results.

## Method Summary
The benchmark implements token-based LSCD models using contextualized embeddings (BERT/XLM-R variants) with three key components: WiC evaluation (computing pairwise similarities between usages), WSI through correlation clustering, and LSCD aggregation using measures like APD, COS, and DiaSense. The system supports multiple languages (German, English, Swedish, Spanish, Italian, Norwegian, Chinese, Russian) and provides standardized evaluation procedures that exploit the sequential nature of LSCD tasks. The implementation includes state-of-the-art models (XL-DURel, XL-LEXEME, DeepMistake) and allows for systematic comparison across different configurations, dataset versions, and preprocessing choices.

## Key Results
- XL-DURel with Average Pairwise Distance (APD) achieves new SOTA on graded change and COMPARE tasks
- Discretizing WiC predictions slightly improves LSCD performance
- WiC performance strongly influences LSCD performance, but exceptions exist where this correlation breaks down
- Raw text with minimal preprocessing outperforms historical spelling normalization for German datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LSCD benchmark works by reflecting the modularity of lexical semantic change detection, allowing separate evaluation of Word-in-Context (WiC), Word Sense Induction (WSI), and LSCD tasks on the same datasets.
- Mechanism: The benchmark implements standardized evaluation procedures that exploit the sequential nature of LSCD (WiC → WSI → LSCD), enabling researchers to evaluate increasingly complex model components and transfer knowledge between related tasks.
- Core assumption: Performance on subtasks (WiC and WSI) directly determines performance on the meta task (LSCD).
- Evidence anchors:
  - [abstract] "The benchmark exploits the modularity of the meta task LSCD by allowing for evaluation of the subtasks WiC and WSI on the same datasets."
  - [section 3] "The three tasks are reflected in the human... as well as the computational process... of measuring lexical semantic change."
- Break condition: If the performance on subtasks does not strongly correlate with LSCD performance, or if the sequential dependency between tasks is not as strict as assumed.

### Mechanism 2
- Claim: The benchmark improves reproducibility and comparability of LSCD models by standardizing evaluation across multiple languages and dataset versions.
- Mechanism: By implementing state-of-the-art models on high-quality, human-annotated evaluation data from multiple languages and providing standard splits, the benchmark creates a common evaluation setup that makes results easily reproducible and allows different components to be freely combined.
- Core assumption: Heterogeneity in modeling options, task definitions, dataset versions, preprocessing options, and evaluation metrics makes it difficult to evaluate models under comparable conditions.
- Evidence anchors:
  - [abstract] "This heterogeneity makes it difficult to evaluate models under comparable conditions, to choose optimal model combinations or to reproduce results."
  - [section 1] "Hence, we provide a benchmark repository standardizing LSCD evaluation. Through transparent implementation results become easily reproducible and by standardization different components can be freely combined."
- Break condition: If different dataset versions or preprocessing choices lead to inconsistent model rankings that cannot be resolved through standardization.

### Mechanism 3
- Claim: The benchmark enables systematic improvement of state-of-the-art models by providing a testbed for comparing and analyzing different model components and configurations.
- Mechanism: Through experiments comparing state-of-the-art models (XL-DURel, XL-LEXEME, DeepMistake) and analyzing factors like WiC prediction discretization, dataset versions, and preprocessing, the benchmark identifies optimal model combinations and configurations that improve performance.
- Core assumption: Careful evaluation of increasingly complex model components provides new ways of model optimization.
- Evidence anchors:
  - [abstract] "We use the implemented benchmark to conduct a number of experiments with recent models and systematically improve the state-of-the-art."
  - [section 7] "We now use the benchmark the perform a number of experiments... setting a new state-of-the-art in LSCD and providing a better general understanding on LSCD model evaluation and improvement."
- Break condition: If experiments fail to identify clear patterns or improvements, or if the benchmark becomes outdated as new models and datasets emerge.

## Foundational Learning

- Concept: Diachronic Lexical Semantic Change Detection (LSCD)
  - Why needed here: LSCD is the central task being addressed, and understanding its complexity and modular nature is crucial for understanding why a benchmark is needed.
  - Quick check question: What are the three sequential tasks that typically comprise LSCD, and why are they performed in this order?

- Concept: Word-in-Context (WiC) task
  - Why needed here: WiC is the first subtask in the LSCD pipeline, and the benchmark evaluates WiC models as part of the overall LSCD evaluation.
  - Quick check question: How does the WiC task differ between binary classification and graded formulations, and what evaluation metrics are used for each?

- Concept: Word Sense Induction (WSI) task
  - Why needed here: WSI is the second subtask in the LSCD pipeline, and the benchmark evaluates WSI methods as part of the overall LSCD evaluation.
  - Quick check question: What is the difference between hard clustering and soft clustering in the context of WSI, and when would each be appropriate?

## Architecture Onboarding

- Component map: WiC evaluation → WSI clustering → LSCD aggregation
- Critical path: 1) Select dataset and model configuration, 2) Run WiC evaluation, 3) Perform WSI clustering, 4) Compute LSCD scores, 5) Analyze results
- Design tradeoffs: The benchmark trades off comprehensiveness (supporting many languages and datasets) with ease of use (requiring complex setup and understanding of the modular LSCD pipeline)
- Failure signatures: 1) Poor WiC performance leading to poor LSCD results, 2) Inconsistent performance across dataset versions, 3) Models not generalizing across languages or time periods
- First 3 experiments:
  1. Run XL-DURel with APD on CoMeDi test split for Graded Change detection to establish baseline
  2. Compare XL-DURel, XL-LEXEME, and DeepMistake on WiC task to understand relative strengths
  3. Test impact of WiC prediction discretization on LSCD performance by comparing thresholded and non-thresholded versions of XL-DURel

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the benchmarking approach be extended to evaluate and compare cluster-based LSCD models, which have been shown to have high potential but are not currently included in the state-of-the-art evaluation?
- Basis in paper: [explicit] The paper mentions that cluster-based models have not been evaluated in the current benchmarking approach, despite their potential for high performance, and suggests this as future work.
- Why unresolved: The paper explicitly states that cluster-based models were not included in the current evaluation, focusing instead on aggregate measures that skip the clustering step, and defers this comparison to future work.
- What evidence would resolve it: Implementing cluster-based models within the benchmarking framework and conducting systematic comparisons against aggregate measure-based models across multiple datasets and languages.

### Open Question 2
- Question: What is the optimal balance between WiC performance and LSCD performance, and how can we identify when improving WiC ranking does not guarantee better LSCD performance?
- Basis in paper: [inferred] The paper shows that WiC performance strongly influences LSCD performance, but also identifies notable exceptions where WiC dominance does not translate to LSCD dominance, suggesting that score distribution and other factors may play a role.
- Why unresolved: While the paper demonstrates a correlation between WiC and LSCD performance, it also highlights cases where this correlation breaks down, indicating that additional factors beyond WiC ranking influence LSCD outcomes.
- What evidence would resolve it: Systematic analysis of score distributions and their impact on LSCD performance across different WiC models, combined with theoretical work on how WiC predictions should be aggregated for optimal LSCD results.

### Open Question 3
- Question: How does historical spelling variation impact the performance of current state-of-the-art LSCD models, and what preprocessing strategies are most effective?
- Basis in paper: [explicit] The paper tests the influence of spelling normalization and lemmatization on German datasets with historical spelling variants and finds that raw text (with minimal preprocessing) gives top performance for all models tested.
- Why unresolved: The experiments show that current base embedders are robust to spelling variation, but the paper notes this is based on limited testing and suggests further investigation into preprocessing strategies is needed.
- What evidence would resolve it: Comprehensive testing across multiple languages with varying degrees of historical spelling variation, including evaluation of different preprocessing strategies and their impact on model performance.

## Limitations
- The benchmark focuses primarily on token-based approaches with contextualized embeddings, potentially overlooking other promising methodologies
- Limited empirical evidence is provided for the strength of the correlation between WiC/WSI performance and LSCD performance
- The study doesn't address computational efficiency considerations or long-term maintenance of the benchmark as new datasets and models emerge

## Confidence
- Claims about benchmark mechanism (Modularity improves evaluation): Medium
- Claims about reproducibility improvements: Medium
- Claims about state-of-the-art improvements: High (based on demonstrated results)

## Next Checks
1. Replicate the WiC-to-LSCD performance correlation analysis across all supported datasets to quantify the strength of the sequential dependency assumption
2. Test model performance consistency across different dataset versions and preprocessing configurations to validate standardization benefits
3. Evaluate the benchmark's effectiveness with models beyond the token-based contextualized embedding approaches used in the study