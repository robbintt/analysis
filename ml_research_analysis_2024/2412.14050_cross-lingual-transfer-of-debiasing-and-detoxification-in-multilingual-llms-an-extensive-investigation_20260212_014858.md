---
ver: rpa2
title: 'Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs:
  An Extensive Investigation'
arxiv_id: '2412.14050'
source_url: https://arxiv.org/abs/2412.14050
tags:
- language
- bias
- toxicity
- english
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates cross-lingual transfer of bias and toxicity
  mitigation in multilingual LLMs by finetuning on English datasets using supervised
  fine-tuning and direct preference optimization. Debiasing is most effective with
  supervised fine-tuning on curated text, while toxicity mitigation succeeds only
  with direct preference optimization.
---

# Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation

## Quick Facts
- **arXiv ID**: 2412.14050
- **Source URL**: https://arxiv.org/abs/2412.14050
- **Reference count**: 40
- **Primary result**: Cross-lingual transfer of debiasing and toxicity mitigation from English to other languages works but often degrades generation quality, especially in lower-resource languages

## Executive Summary
This study systematically investigates how debiasing and detoxification capabilities transfer from English to other languages in multilingual LLMs through supervised fine-tuning (SFT) and direct preference optimization (DPO). The research reveals that debiasing transfers most effectively through SFT on curated text datasets, while toxicity mitigation requires DPO with preference pairs. Both methods successfully transfer mitigation capabilities from English to target languages, but this transfer frequently degrades generation quality, particularly for lower-resource languages. The effectiveness of transfer correlates with the amount of pretraining data available in the target language.

## Method Summary
The authors evaluate cross-lingual transfer of bias and toxicity mitigation in multilingual LLMs by finetuning on English datasets using two distinct approaches: supervised fine-tuning (SFT) for debiasing tasks using curated text, and direct preference optimization (DPO) for toxicity mitigation using preference pairs. They assess both the effectiveness of the mitigation and its impact on generation quality across multiple target languages, examining how pretraining data volume in each language influences transfer success.

## Key Results
- Debiasing is most effective with supervised fine-tuning on curated text datasets
- Toxicity mitigation succeeds only with direct preference optimization using preference pairs
- Both methods successfully transfer from English to other languages but often degrade generation quality, particularly for lower-resource languages
- Transfer effectiveness correlates with the amount of pretraining data in the target language

## Why This Works (Mechanism)
The differential effectiveness of SFT versus DPO for debiasing versus toxicity mitigation reflects fundamental differences in how these optimization methods handle different types of learning objectives. SFT works well for debiasing because it can directly teach models to generate less biased text through exposure to curated examples, while DPO excels at toxicity mitigation because it can effectively learn preferences between toxic and non-toxic outputs through pairwise comparisons. The transfer success correlates with pretraining data because models with more exposure to a target language develop better linguistic representations that can be more effectively adapted for mitigation tasks.

## Foundational Learning

**Multilingual LLM Pretraining**: Training models on diverse language corpora builds shared representations across languages, enabling transfer of capabilities. This is needed because it creates the foundation for cross-lingual generalization. Quick check: Verify pretraining data statistics across target languages.

**Supervised Fine-Tuning (SFT)**: Direct training on labeled examples for specific tasks. This is needed because it provides explicit guidance for generating desired outputs. Quick check: Compare loss curves during SFT across languages.

**Direct Preference Optimization (DPO)**: Learning from pairwise comparisons between preferred and non-preferred outputs. This is needed because it captures relative quality judgments rather than absolute labels. Quick check: Analyze preference distribution shifts during training.

**Cross-Lingual Transfer**: Applying knowledge learned in one language to improve performance in another. This is needed because it enables resource-efficient multilingual capability development. Quick check: Measure zero-shot performance across languages.

## Architecture Onboarding

**Component Map**: English datasets (SFT/DPO) -> Multilingual LLM finetuning -> Cross-lingual transfer evaluation -> Generation quality assessment

**Critical Path**: Dataset preparation → Model finetuning → Cross-lingual evaluation → Quality degradation analysis

**Design Tradeoffs**: SFT provides direct instruction but requires curated data; DPO learns preferences but needs pairwise comparisons; transfer exploits pretraining but risks quality degradation

**Failure Signatures**: SFT fails on toxicity when preferences matter; DPO underperforms on debiasing without sufficient examples; both degrade quality more severely in low-resource languages

**First Experiments**: 1) Compare SFT vs DPO effectiveness on debiasing in high-resource vs low-resource languages; 2) Measure generation quality degradation correlation with pretraining data volume; 3) Test zero-shot transfer to languages not in pretraining

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses exclusively on bias and toxicity mitigation objectives, limiting generalizability to other harmful content types
- Results depend on automated evaluation metrics that may not capture nuanced cultural and linguistic aspects across languages
- Correlation between pretraining data volume and transfer success is observed but not causally established

## Confidence
- **High confidence**: SFT vs DPO effectiveness patterns align with established preference learning literature
- **Medium confidence**: Generation quality degradation findings depend on automatic evaluation metrics
- **Medium confidence**: Pretraining data correlation lacks causal mechanism exploration

## Next Checks
1. Human evaluation studies across multiple languages to validate automated metrics for both debiasing effectiveness and generation quality degradation
2. Ablation studies isolating pretraining data volume impact from other factors like tokenization quality
3. Experiments testing alternative mitigation objectives and optimization methods to test generalizability of SFT/DPO patterns