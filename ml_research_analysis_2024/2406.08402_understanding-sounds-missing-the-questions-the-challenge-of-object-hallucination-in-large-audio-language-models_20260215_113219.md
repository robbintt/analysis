---
ver: rpa2
title: 'Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination
  in Large Audio-Language Models'
arxiv_id: '2406.08402'
source_url: https://arxiv.org/abs/2406.08402
tags:
- audio
- lalms
- tasks
- object
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines object hallucination in large audio-language
  models (LALMs), a previously unexplored issue. The authors design discriminative
  tasks to probe whether LALMs can correctly identify the presence or absence of specific
  object sounds in audio clips, and generative tasks to measure object hallucination
  in audio captioning.
---

# Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models

## Quick Facts
- **arXiv ID**: 2406.08402
- **Source URL**: https://arxiv.org/abs/2406.08402
- **Authors**: Chun-Yi Kuan; Wei-Ping Huang; Hung-yi Lee
- **Reference count**: 0
- **Primary result**: LALMs struggle with yes/no questions about audio content despite strong generative capabilities, with object hallucination being a significant issue.

## Executive Summary
This work investigates object hallucination in large audio-language models (LALMs), a previously unexplored issue in audio understanding. The authors design discriminative tasks to test whether LALMs can correctly identify the presence or absence of specific object sounds, and generative tasks to measure object hallucination in audio captioning. They find that while LALMs perform comparably to specialized models in audio captioning, they struggle significantly with discriminative questions, tending to answer "yes" too often. The study also explores prompt engineering as a potential solution, finding that specific instructions can substantially improve discriminative task performance.

## Method Summary
The authors evaluate LALMs using two types of tasks: discriminative (yes/no questions about audio content) and generative (audio captioning). They use the AudioCaps dataset for audio captioning and CHIME-6 for noisy speech recognition. For discriminative tasks, they create balanced datasets with 15,110 positive and 15,110 negative samples using random, popular, and adversarial sampling strategies. Performance is measured using F1 score, accuracy, precision, and recall. For generative tasks, they evaluate using ECHO and Cover metrics. The study tests various prompt engineering techniques to improve discriminative task performance.

## Key Results
- LALMs achieve low F1 scores (46.1-53.1%) on discriminative tasks, indicating struggle with yes/no questions
- Models show systematic affirmative bias, with recall consistently higher than precision
- On generative audio captioning, LALMs perform comparably to specialized models (ECHO scores 30-40%)
- Prompt engineering significantly improves discriminative performance when instructing models to "listen closely" or "focus on the question"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LALMs struggle with discriminative tasks due to a fundamental mismatch between audio understanding and precise yes/no discrimination.
- Mechanism: While LALMs can generate descriptive captions and understand general audio content, they have difficulty extracting specific information required for discriminative questions. The models tend to answer "yes" too often when asked about the presence of specific sounds, suggesting they struggle to accurately identify when sounds are absent.
- Core assumption: The models have learned general audio understanding but have not been specifically trained or fine-tuned for precise binary discrimination tasks.
- Evidence anchors:
  - [abstract] states "LALMs are comparable to specialized audio captioning models in their understanding of audio content, but struggle to answer discriminative questions"
  - [section] shows "recall scores of all LALMs are significantly lower than their precision scores" and "LALMs tend to provide affirmative answers"
  - [corpus] shows related work on reducing object hallucination in LALMs, suggesting this is a recognized issue
- Break condition: If the models were explicitly trained on large-scale discriminative tasks or if prompt engineering could completely eliminate the bias toward affirmative answers.

### Mechanism 2
- Claim: Prompt engineering can significantly improve discriminative task performance by directing the model's attention.
- Mechanism: By adding prefix prompts that instruct the model to "listen closely," "focus on the question," or "carefully consider the answer," performance on discriminative tasks improves. This suggests that the models have the underlying capability but need explicit guidance to apply it correctly to discriminative queries.
- Core assumption: The models can understand discriminative questions when properly prompted, but default to a more general comprehension mode without explicit instructions.
- Evidence anchors:
  - [abstract] states "we explore the potential of prompt engineering to enhance LALMs' performance on discriminative questions"
  - [section] shows "instructing LALMs to carefully consider the question and to both attentively listen to the audio and carefully consider the question yielded the most significant improvement in performance"
  - [corpus] evidence is weak - no direct corpus support for this specific mechanism
- Break condition: If the improvement from prompt engineering is inconsistent across different models or if the prompts need to be excessively long to achieve meaningful improvement.

### Mechanism 3
- Claim: The level of object hallucination in LALMs is comparable to specialized audio captioning models.
- Mechanism: When performing generative audio captioning tasks, LALMs produce results with similar ECHO scores (around 30-40%) to specialized models, indicating they understand audio content at a similar level but still exhibit object hallucination.
- Core assumption: The models have learned sufficient audio understanding to perform captioning tasks, but object hallucination is an inherent limitation of current audio-language model architectures.
- Evidence anchors:
  - [abstract] states "LALMs rival specialized models in audio captioning tasks, demonstrating their ability to comprehend audio information"
  - [section] shows "the performance of LALMs on the ECHO and Cover metrics is comparable to that of Whisper-based caption model"
  - [corpus] shows related work on object hallucination evaluation, supporting this as a recognized phenomenon
- Break condition: If future models show significantly lower hallucination rates or if the hallucination is shown to be primarily due to specific architectural choices rather than inherent limitations.

## Foundational Learning

- Concept: Binary classification vs. generative tasks
  - Why needed here: Understanding the difference between yes/no discrimination tasks and open-ended generation helps explain why LALMs perform differently on these task types
  - Quick check question: Why do models that excel at generating descriptive captions struggle with simple yes/no questions about audio content?

- Concept: Object hallucination in multimodal models
  - Why needed here: Recognizing that hallucination is a known issue in vision-language models helps contextualize the findings for audio-language models
  - Quick check question: How does object hallucination in LALMs compare to similar phenomena in large vision-language models?

- Concept: Prompt engineering effectiveness
  - Why needed here: Understanding when and why prompt engineering works helps explain the proposed improvement methods
  - Quick check question: What types of prompts are most effective for improving discriminative task performance in LALMs?

## Architecture Onboarding

- Component map: Audio encoder -> Text encoder -> Multimodal fusion layer -> Language model decoder
- Critical path: Audio input -> Feature extraction -> Multimodal fusion -> Text generation/decision
- Design tradeoffs: Balance between audio understanding capability and precise discrimination ability; tradeoff between general comprehension and task-specific performance
- Failure signatures: High affirmative bias in discriminative tasks; object hallucination in generative tasks; inconsistent performance across different sampling strategies
- First 3 experiments:
  1. Test LALM performance on simple yes/no audio questions with different prompt variations to measure the impact of prompt engineering
  2. Compare LALM performance on discriminative tasks vs. generative tasks using the same audio clips to quantify the discrimination gap
  3. Implement a cascade approach (audio captioning model + LLM) and compare its performance to native LALMs on both task types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can prompt engineering be systematically optimized to reduce object hallucination in LALMs beyond the current ad-hoc approaches?
- Basis in paper: [explicit] The authors demonstrate that prompt engineering can significantly improve discriminative task performance but only explore a limited set of prompts.
- Why unresolved: The paper tests only eight specific prompts without exploring optimization strategies or systematic prompt design frameworks.
- What evidence would resolve it: A comprehensive study comparing different prompt engineering approaches (e.g., automated prompt optimization, few-shot learning, chain-of-thought prompting) on both discriminative and generative tasks.

### Open Question 2
- Question: What are the fundamental architectural differences between LALMs and cascade pipelines that lead to such disparate performance on discriminative tasks?
- Basis in paper: [inferred] The authors note a "substantial gap" between LALMs and cascade pipelines but do not analyze the architectural reasons for this difference.
- Why unresolved: The paper focuses on performance evaluation rather than architectural analysis or comparative studies of internal representations.
- What evidence would resolve it: Comparative studies of attention patterns, feature representations, and decision boundaries between LALMs and cascade models during discriminative tasks.

### Open Question 3
- Question: How does object hallucination in LALMs generalize across different domains beyond audio captioning and ASR, such as environmental sound classification or music understanding?
- Basis in paper: [explicit] The authors evaluate only audio captioning and noisy speech recognition tasks, leaving generalization to other audio domains unexplored.
- Why unresolved: The study is limited to specific task types without investigating whether the observed hallucination patterns persist across diverse audio domains.
- What evidence would resolve it: Systematic evaluation of LALMs on environmental sound classification, music tagging, and other audio understanding tasks using similar hallucination metrics.

## Limitations

- LALMs exhibit systematic affirmative bias in discriminative tasks, indicating a fundamental limitation in precise discrimination
- The effectiveness of prompt engineering varies across models and may require extensive prompt engineering
- Comparison with specialized models is limited by dataset-specific fine-tuning and may not reflect broader audio understanding capabilities

## Confidence

- **High confidence**: The observation that LALMs struggle with discriminative yes/no questions is well-supported by consistent performance metrics across multiple models and tasks. The systematic affirmative bias is a clear and reproducible finding.
- **Medium confidence**: The effectiveness of prompt engineering is demonstrated but may not generalize across all LALM architectures or task types. The specific prompts that work best may vary depending on the model.
- **Low confidence**: The claim that LALMs are "comparable to specialized models" in generative tasks is based on limited metrics (ECHO and Cover scores) and may not capture the full picture of audio understanding capabilities.

## Next Checks

1. Test the prompt engineering techniques across a broader range of LALM architectures (beyond the 10 models evaluated) to assess generalizability of the improvements.

2. Design more challenging discriminative tasks that specifically target the affirmative bias, such as using semantically similar but acoustically distinct sounds to test precise discrimination capabilities.

3. Conduct ablation studies to identify which components of LALMs (audio encoder, fusion layer, language model) contribute most to object hallucination, potentially guiding architectural improvements.