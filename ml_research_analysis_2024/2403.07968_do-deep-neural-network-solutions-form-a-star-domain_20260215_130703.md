---
ver: rpa2
title: Do Deep Neural Network Solutions Form a Star Domain?
arxiv_id: '2403.07968'
source_url: https://arxiv.org/abs/2403.07968
tags:
- star
- loss
- solutions
- domain
- conjecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conjectures that deep neural network solution sets form
  a star domain rather than a convex set, relaxing Entezari et al.'s convexity conjecture.
  The proposed Starlight algorithm finds a "star model" linearly connected to other
  solutions via low-loss paths modulo permutations.
---

# Do Deep Neural Network Solutions Form a Star Domain?

## Quick Facts
- **arXiv ID:** 2403.07968
- **Source URL:** https://arxiv.org/abs/2403.07968
- **Reference count:** 32
- **Primary result:** Star domain conjecture proposed; Starlight algorithm finds models linearly connected to other solutions, reducing loss barriers from 0.32 to 0.03 and improving uncertainty estimates over deep ensembles.

## Executive Summary
This paper investigates the geometric structure of deep neural network solution sets, challenging the prevailing notion that they form convex regions. Instead, the authors conjecture that these solution sets form star domains - sets where all points can be connected to a central "star model" via low-loss linear paths (modulo permutation symmetries). The proposed Starlight algorithm iteratively optimizes a model to minimize expected loss along linear interpolations to a set of independently trained source models. Experiments with ResNet18 on CIFAR-10/100 demonstrate that star models achieve near-zero loss barriers with heldout models (0.03 vs 0.32 for regular-to-regular pairs), supporting the star domain conjecture. The approach also provides better uncertainty estimates for Bayesian Model Averaging than deep ensembles and offers a computationally efficient alternative to model ensembles.

## Method Summary
The method involves training multiple independent source models on the same task, then using the Starlight algorithm to find a "star model" that minimizes expected loss along linear interpolations to these source models while accounting for permutation symmetries through weight matching. The optimization iteratively samples source models, finds optimal permutations, samples interpolation points, computes gradients on interpolated losses, and updates the star model parameters. The resulting star model is evaluated for loss barriers with heldout models, uncertainty estimation quality, and computational efficiency compared to model ensembles.

## Key Results
- Star models achieve near-zero loss barriers with heldout models (0.03 vs 0.32 for regular-to-regular pairs)
- Incorporating more source models (up to |Z|≤50) further reduces loss barriers, demonstrating increasing "starness"
- Star domain posterior provides better uncertainty estimates (AUROC) than deep ensembles for Bayesian Model Averaging
- Star models maintain comparable accuracy to ensembles while reducing inference costs significantly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The solution set of deep neural networks forms a star domain rather than a convex set, allowing for a "star model" that is linearly connected to all other solutions modulo permutations.
- **Mechanism:** The Starlight algorithm iteratively optimizes a model to minimize the expected loss along linear paths connecting it to a set of independently trained source models, using permutation alignment to factor out symmetry.
- **Core assumption:** Linear mode connectivity can be achieved between a star model and other solutions if the star model is trained to lie in the low-loss valley shared by the source models.
- **Evidence anchors:**
  - [abstract] "The proposed Starlight algorithm finds a 'star model' linearly connected to other solutions via low-loss paths modulo permutations."
  - [section] "Starlight finds a model that is linearly connected with a finite set of independent solutions."
- **Break condition:** If the loss barriers between the star model and held-out models remain high despite increasing the number of source models, or if permutation alignment fails to align solutions meaningfully.

### Mechanism 2
- **Claim:** Incorporating more source models into the Starlight algorithm enhances the "starness" of the resulting model, reducing loss barriers with arbitrary solutions.
- **Mechanism:** By including a larger set of independently trained models as source models, the optimization objective encourages the star model to reside in a region of the parameter space that is linearly connected to a broader set of solutions.
- **Core assumption:** The solution set has a central region (the star domain) that is linearly connected to most other solutions, and increasing the diversity of source models helps locate this region.
- **Evidence anchors:**
  - [section] "Incorporating more source models |Z| enables finding a better star model with a lower loss barrier against an arbitrary solution."
- **Break condition:** If adding more source models does not decrease loss barriers, suggesting the solution set may not have a well-defined star domain structure.

### Mechanism 3
- **Claim:** Star models can serve as efficient substitutes for model ensembles, maintaining comparable accuracy with reduced inference costs.
- **Mechanism:** A star model, being linearly connected to a set of source models, can approximate the ensemble behavior by representing a central point in the solution space, thus reducing the need to store and evaluate multiple models.
- **Core assumption:** The star model captures the essential predictive behavior of the ensemble by lying in a low-loss region shared by the source models.
- **Evidence anchors:**
  - [abstract] "Additionally, star models offer a computationally efficient alternative to model ensembles, maintaining comparable accuracy with reduced inference costs."
- **Break condition:** If the star model's accuracy degrades significantly compared to the ensemble, or if the reduction in inference cost does not justify any loss in performance.

## Foundational Learning

- **Concept:** Linear mode connectivity
  - Why needed here: Understanding whether two solutions can be connected by a low-loss linear path is central to the star domain conjecture.
  - Quick check question: Can you explain the difference between linear mode connectivity and non-linear mode connectivity in neural network solution spaces?

- **Concept:** Permutation invariance in neural networks
  - Why needed here: Factoring out permutation symmetries is essential for defining the solution set and testing the star domain conjecture.
  - Quick check question: Why do permutation symmetries exist in neural networks, and how do they affect the geometry of the solution set?

- **Concept:** Bayesian Model Averaging (BMA)
  - Why needed here: BMA is used to evaluate the uncertainty estimates of the star domain posterior compared to deep ensembles.
  - Quick check question: How does BMA improve uncertainty estimation compared to using a single model, and what role does the posterior distribution play?

## Architecture Onboarding

- **Component map:** Source models → Permutation alignment → Starlight optimization → Star model → Evaluation (loss barriers, uncertainty, efficiency)
- **Critical path:** 1. Train source models independently. 2. Initialize star model and set up permutation alignment. 3. Iteratively optimize star model using linear interpolation loss. 4. Evaluate loss barriers with held-out models. 5. Assess practical benefits (uncertainty estimation, model fusion).
- **Design tradeoffs:** Number of source models vs. computational cost; frequency of permutation alignment updates vs. optimization speed; linear vs. non-linear interpolation for loss estimation; star model accuracy vs. inference efficiency.
- **Failure signatures:** High loss barriers persist despite increasing source models; permutation alignment fails to find meaningful correspondences; star model accuracy degrades compared to individual source models; uncertainty estimates are worse than deep ensemble baselines.
- **First 3 experiments:** 1. Train 2-3 source models and run Starlight to find a star model; evaluate loss barriers. 2. Increase the number of source models (e.g., 5, 10, 20) and observe changes in loss barriers. 3. Compare star model performance (accuracy, uncertainty) against deep ensemble baselines on a held-out test set.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the star domain conjecture hold for larger networks and more complex datasets beyond ResNet18 on CIFAR-10/100 and ImageNet?
- **Basis in paper:** [inferred] The paper acknowledges the need to test the conjecture on different architectures (DenseNet) and datasets (ImageNet) and suggests that more extensive validation is needed.
- **Why unresolved:** The experiments were limited to ResNet18 on CIFAR-10/100 and ImageNet. Testing on a wider range of architectures and datasets would provide stronger empirical support.
- **What evidence would resolve it:** Conducting experiments with various architectures (e.g., VGG, DenseNet, EfficientNet) on diverse datasets (e.g., ImageNet, COCO, large-scale medical imaging datasets) and observing consistent results supporting the star domain conjecture.

### Open Question 2
- **Question:** How does the choice of optimization algorithm (e.g., Adam, SGD with different learning rates) affect the star domain properties of the solution set?
- **Basis in paper:** [explicit] The paper briefly explores the connectivity between SGD solutions, Adam solutions, and SGD-induced star models, noting differences in loss barriers, but does not provide a comprehensive analysis.
- **Why unresolved:** The paper only provides preliminary observations on the connectivity between SGD and Adam solutions. A thorough investigation of the impact of different optimization algorithms on the star domain properties is lacking.
- **What evidence would resolve it:** Systematically comparing the loss barriers and star model properties across various optimization algorithms (e.g., SGD with different learning rates, Adam, RMSprop) and analyzing the differences in the solution sets.

### Open Question 3
- **Question:** What is the theoretical basis for the star domain conjecture, and under what conditions does it hold?
- **Basis in paper:** [inferred] The paper presents the star domain conjecture as a relaxation of the convexity conjecture and provides empirical evidence, but does not offer a theoretical justification or identify the conditions under which the conjecture is valid.
- **Why unresolved:** The paper focuses on empirical validation and does not provide a theoretical framework or proof for the star domain conjecture. Understanding the theoretical underpinnings and conditions for the conjecture would strengthen its validity.
- **What evidence would resolve it:** Developing a theoretical analysis of the star domain conjecture, potentially leveraging tools from optimization theory, geometry, or information theory, and identifying the conditions (e.g., network architecture, dataset properties, optimization algorithm) under which the conjecture holds.

## Limitations

- Empirical validation limited to specific architectures (ResNet18) and datasets (CIFAR-10/100), requiring broader testing for generalization.
- Permutation alignment process relies on heuristic weight matching that may not scale well to larger networks or different architectures.
- Computational overhead of finding permutations and evaluating linear interpolations may limit practical applicability for very large models.

## Confidence

- **Star domain existence:** High - Supported by consistent empirical evidence across multiple experiments showing low loss barriers between star models and heldout solutions.
- **Star model efficiency:** Medium - Demonstrated through reduced inference costs, but long-term stability and generalization to other architectures require further validation.
- **Uncertainty estimation improvements:** Medium - AUROC improvements are statistically significant, but the practical impact on real-world decision-making needs more thorough evaluation.

## Next Checks

1. **Generalization test:** Evaluate star models on diverse architectures (e.g., Vision Transformers, RNNs) and tasks (regression, reinforcement learning) to verify the star domain conjecture beyond image classification.

2. **Permutation robustness analysis:** Systematically study how permutation alignment quality affects loss barriers and star model performance, including sensitivity to initialization and alignment algorithms.

3. **Scalability assessment:** Measure the computational overhead of star model training and inference as network depth and width increase, establishing practical limits for real-world deployment.