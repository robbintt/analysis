---
ver: rpa2
title: 'EKM: An exact, polynomial-time algorithm for the $K$-medoids problem'
arxiv_id: '2405.12237'
source_url: https://arxiv.org/abs/2405.12237
tags:
- algorithm
- problem
- time
- algorithms
- exact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The K-medoids clustering problem was addressed with a new exact
  algorithm (EKM) that achieves polynomial-time worst-case complexity O(N^K+1), where
  N is dataset size and K is the number of clusters. The method uses transformational
  programming and combinatorial generation techniques to derive a provably correct
  algorithm through equational reasoning from an exhaustive search specification.
---

# EKM: An exact, polynomial-time algorithm for the $K$-medoids problem

## Quick Facts
- arXiv ID: 2405.12237
- Source URL: https://arxiv.org/abs/2405.12237
- Reference count: 5
- EKM achieves polynomial-time worst-case complexity O(N^K+1) for K-medoids clustering

## Executive Summary
The K-medoids clustering problem, traditionally considered NP-hard with exponential exact solution methods, is addressed with a new algorithm (EKM) that achieves polynomial-time worst-case complexity O(N^K+1). The method uses transformational programming and combinatorial generation techniques to derive a provably correct algorithm through equational reasoning from an exhaustive search specification. EKM was shown to outperform existing exact methods, including branch-and-bound MIP solvers, on both synthetic and real-world datasets up to N=5000 data points, delivering globally optimal solutions.

## Method Summary
EKM uses transformational programming based on the Bird-Meertens formalism to derive an efficient exact algorithm for K-medoids clustering. Starting from a correct but inefficient exhaustive search specification, the algorithm applies equational reasoning steps including shortcut fusion theorems to transform it into an efficient implementation while preserving correctness by construction. The key innovation is fusing the combinatorial generator with the evaluator and selector functions to avoid explicit enumeration of all possible medoid configurations, achieving polynomial-time complexity while guaranteeing global optimality.

## Key Results
- EKM achieves polynomial worst-case complexity O(N^K+1), outperforming exponential branch-and-bound MIP solvers
- On synthetic data with K=3 clusters, EKM matched predicted O(N^4) complexity while MIP solvers showed exponential scaling
- EKM successfully clustered real-world datasets up to 5000 points, delivering globally optimal solutions
- The algorithm is provably correct by construction through formal equational reasoning from exhaustive search

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EKM achieves polynomial-time complexity by fusing combinatorial generation and evaluation through equational reasoning, avoiding exhaustive enumeration.
- Mechanism: The algorithm uses a recursive combination generator (gen_combs) that incrementally builds medoid sets, fusing the evaluator into the generator via tupling fusion. This eliminates the need to generate and store all possible configurations explicitly.
- Core assumption: The objective function can be evaluated incrementally as partial configurations are built, and the selector can be fused with the evaluator-generator.
- Evidence anchors:
  - [section] "Efficiency is achieved by recognizing that in many combinatorial problems, generator functions can be given as efficient recursions, for instance taking O (N ) steps."
  - [section] "Using this, we fuseeval E inside the combination generator gen combs by defining evalgen E,combs..."
  - [corpus] Weak - no direct citations about polynomial-time fusion techniques in ML clustering, but combinatorial generation theory is well-established.
- Break condition: If the objective function cannot be decomposed into incremental terms or if the selector cannot be fused, the polynomial-time guarantee fails.

### Mechanism 2
- Claim: The algorithm guarantees global optimality by deriving from an exhaustive search specification through provably correct equational transformations.
- Mechanism: Starting from a correct but inefficient exhaustive search (2), the algorithm applies shortcut fusion theorems (e.g., tupling fusion) to transform it into an efficient implementation while preserving correctness by construction.
- Core assumption: The equational reasoning steps are formally correct and preserve the semantics of the original exhaustive search.
- Evidence anchors:
  - [section] "Since the exhaustive search algorithm is correct for the MIP problem, any algorithm derived through correct equational reasoning steps from this specification is also (provably) correct."
  - [abstract] "The derived algorithm is provably correct by construction."
  - [corpus] Weak - no direct citations about formal program derivation in clustering, but Bird-Meertens formalism is established in algorithm theory.
- Break condition: If any transformation step introduces an error or if the original exhaustive search specification is incorrect, the global optimality guarantee fails.

### Mechanism 3
- Claim: EKM outperforms MIP solvers on practical datasets by avoiding the exponential worst-case complexity of branch-and-bound methods.
- Mechanism: MIP solvers like GLPK have exponential worst-case time complexity due to branch-and-bound, while EKM has polynomial worst-case O(N^(K+1)) complexity. Empirical results show EKM matching predicted polynomial scaling while MIP solvers exhibit exponential scaling.
- Core assumption: The worst-case analysis of EKM accurately predicts practical performance, and MIP solvers indeed have exponential worst-case complexity.
- Evidence anchors:
  - [abstract] "We show that the wall-clock run time of our algorithm matches the worst-case time complexity analysis on synthetic datasets, clearly outperforming the exponential time complexity of benchmark branch-and-bound based MIP solvers."
  - [section] "As predicted, the off-the-shelf BnB-based MIP solver (GLPK) exhibits worst-case exponential time complexity."
  - [corpus] Weak - no direct citations comparing EKM to MIP solvers, but NP-hardness of K-medoids and exponential branch-and-bound complexity are well-established.
- Break condition: If the polynomial-time analysis is incorrect or if practical datasets have special structure that benefits MIP solvers, the performance advantage may not hold.

## Foundational Learning

- Concept: Transformational programming and the Bird-Meertens formalism
  - Why needed here: EKM is derived through equational reasoning from an exhaustive search specification using these formal methods, ensuring correctness by construction.
  - Quick check question: What is the key principle behind transformational programming that allows deriving efficient algorithms from specifications?

- Concept: Combinatorial generation and shortcut fusion
  - Why needed here: EKM uses efficient recursive combination generators and fuses them with evaluators and selectors to avoid exponential enumeration, achieving polynomial-time complexity.
  - Quick check question: How does shortcut fusion reduce the computational complexity of exhaustive search algorithms?

- Concept: Mixed-integer programming (MIP) and NP-hardness
  - Why needed here: K-medoids is typically specified as an MIP and is NP-hard, motivating the need for exact polynomial-time algorithms like EKM and explaining why existing methods have exponential complexity.
  - Quick check question: Why does the NP-hardness of K-medoids imply that existing exact methods likely have exponential worst-case complexity?

## Architecture Onboarding

- Component map:
  - Input: Dataset D of N points in D-dimensional space
  - Generator: Recursive combination generator (gen_combs) that builds medoid sets incrementally
  - Evaluator: Fused evaluator-generator (evalgen_E_combs) that computes objective values incrementally
  - Selector: Fused selector (sel_E) that tracks the best configuration found so far
  - Output: Optimal medoid set and corresponding clustering

- Critical path:
  1. Initialize recursive generator with empty set
  2. At each recursive step, extend partial configurations with the next data point
  3. Fuse evaluator to compute objective values incrementally
  4. Fuse selector to track the best configuration
  5. Return the globally optimal medoid set when recursion completes

- Design tradeoffs:
  - Time vs. space: Fusing selector reduces space complexity to O(N^(K-1)) but requires more computation per step
  - Exactness vs. scalability: EKM guarantees global optimality but has polynomial complexity in K, limiting scalability to large K
  - Generality vs. efficiency: EKM works with any distance function but may not exploit problem-specific structure

- Failure signatures:
  - Incorrect results: Likely due to errors in equational reasoning or fusion transformations
  - Memory overflow: Occurs when K is large and O(N^(K-1)) space is insufficient
  - Excessive runtime: May happen if the fused evaluator is not efficient or if K is too large

- First 3 experiments:
  1. Verify correctness on small synthetic datasets (N<=10, K<=3) by comparing to exhaustive enumeration
  2. Validate polynomial-time scaling by running on synthetic datasets with increasing N and K, checking that runtime matches O(N^(K+1))
  3. Compare performance against MIP solvers (GLPK) on medium-sized real-world datasets (N~1000, K~5), verifying that EKM outperforms on wall-clock time while guaranteeing global optimality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum number of medoids (K) beyond which the EKM algorithm becomes computationally intractable in practice, despite its polynomial worst-case complexity?
- Basis in paper: [explicit] The paper states "the main disadvantage of our algorithm is that its space and time complexity is polynomial with respect to the number of medoids, K. Thus for problems that involve a large number of medoids, our algorithm quickly becomes intractable."
- Why unresolved: The paper demonstrates polynomial complexity O(N^K+1) but doesn't provide specific practical limits for K values where the algorithm becomes unusable, nor does it show empirical results for large K values.
- What evidence would resolve it: Empirical testing of EKM on datasets with varying K values (e.g., K=10, 20, 50, 100) showing the point where wall-clock time becomes impractical, along with memory usage measurements.

### Open Question 2
- Question: How does the performance of EKM compare to state-of-the-art approximate algorithms (like PAM, CLARANS) on datasets where exact solutions are computationally feasible?
- Basis in paper: [explicit] The paper mentions EKM "clearly outperforms the exponential time complexity of benchmark branch-and-bound based MIP solvers" but doesn't provide direct comparisons against approximate algorithms in terms of solution quality vs. computational cost.
- Why unresolved: While the paper shows EKM achieves better objective values than approximate methods, it doesn't quantify the trade-off between the marginal improvement in solution quality and the increased computational cost compared to fast approximate algorithms.
- What evidence would resolve it: Benchmark studies comparing EKM's solution quality and runtime against PAM, CLARANS, and other approximate methods on medium-sized datasets (e.g., N=500-2000) where both exact and approximate methods can be run to completion.

### Open Question 3
- Question: Can the semiring lifting technique used in EKM be generalized to other combinatorial optimization problems beyond clustering, and what are the limitations of this approach?
- Basis in paper: [inferred] The paper discusses how "the constraint algebra can be implicitly embedded within a generator semiring" and mentions "we can easily incorporate these types of constraints into our generator using the semiring lifting technique," suggesting potential for broader application.
- Why unresolved: The paper demonstrates the technique for K-medoids but doesn't explore its applicability to other NP-hard problems or discuss the theoretical boundaries of when semiring lifting can achieve polynomial-time solutions.
- What evidence would resolve it: Successful applications of the semiring lifting approach to other combinatorial problems (e.g., traveling salesman, knapsack, graph coloring) along with analysis of which problem characteristics enable or prevent polynomial-time solutions through this method.

## Limitations

- EKM's polynomial complexity O(N^K+1) makes it impractical for datasets requiring large numbers of clusters (high K values)
- The algorithm was primarily tested on synthetic and medium-sized real-world datasets (Nâ‰¤5000), with limited validation on very large datasets
- Performance comparisons were limited to one MIP solver (GLPK) and did not include other exact methods like dynamic programming approaches

## Confidence

- High: Polynomial worst-case complexity O(N^K+1) and its derivation from exhaustive search
- Medium: Global optimality guarantee and outperformance over MIP solvers
- Low: Scalability to very large datasets (N>10000) and performance on datasets with special structure

## Next Checks

1. Verify correctness on larger synthetic datasets (N=100-1000, K=2-10) using independent implementations
2. Test EKM on datasets with special structure (e.g., clustered, uniform, or high-dimensional) to identify potential performance limitations
3. Compare EKM against other exact methods (e.g., dynamic programming, branch-and-cut) on diverse datasets to establish relative performance