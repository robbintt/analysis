---
ver: rpa2
title: How does Your RL Agent Explore? An Optimal Transport Analysis of Occupancy
  Measure Trajectories
arxiv_id: '2402.09113'
source_url: https://arxiv.org/abs/2402.09113
tags:
- policy
- learning
- occupancy
- optimal
- updates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework to quantitatively compare the exploratory
  processes of different reinforcement learning algorithms using the concept of occupancy
  measures. The key idea is to represent the learning process of an RL algorithm as
  a sequence of policies, and then study the trajectory induced in the manifold of
  state-action occupancy measures.
---

# How does Your RL Agent Explore? An Optimal Transport Analysis of Occupancy Measure Trajectories

## Quick Facts
- arXiv ID: 2402.09113
- Source URL: https://arxiv.org/abs/2402.09113
- Authors: Reabetswe M. Nkhumise; Debabrota Basu; Tony J. Prescott; Aditya Gilra
- Reference count: 40
- Key outcome: Introduces ESL and OMR metrics using optimal transport to quantify exploration efficiency in RL agents

## Executive Summary
This paper proposes a framework to quantitatively compare the exploratory processes of different reinforcement learning algorithms using occupancy measures. The authors represent an RL algorithm's learning process as a sequence of policies and study the trajectory induced in the manifold of state-action occupancy measures. Two novel indices, Effort of Sequential Learning (ESL) and Optimal Movement Ratio (OMR), are introduced to measure the efficiency and effectiveness of an RL algorithm's exploration process. ESL quantifies the relative distance traveled compared to the shortest path from initial to optimal policy, while OMR assesses the fraction of policy updates that effectively reduce an analogue of regret.

## Method Summary
The authors derive approximation guarantees to estimate ESL and OMR with finite samples and without access to an optimal policy. Empirical analyses on various environments and algorithms demonstrate that ESL and OMR provide insights into the exploration processes of RL algorithms and the hardness of different tasks. For example, in a 5x5 gridworld with dense rewards, Q-learning with ϵ=1 (always exploring) has an ESL of 6.27±2.22 and OMR of 0.61±0.09, while ϵ=0 (always exploiting) has an ESL of 15.5±5.28 and OMR of 0.53±0.06, indicating that the former is more efficient in its exploration process.

## Key Results
- ESL and OMR metrics provide a quantitative framework to compare the exploration efficiency of RL algorithms
- In a 5x5 gridworld, Q-learning with always exploring (ϵ=1) is more efficient than always exploiting (ϵ=0), with lower ESL and higher OMR
- The metrics are applicable to both model-based and model-free RL algorithms

## Why This Works (Mechanism)
The framework works by representing the learning process of an RL algorithm as a sequence of policies and studying the trajectory induced in the manifold of state-action occupancy measures. By using optimal transport distances to quantify the similarity between consecutive policies, the authors can measure the effort required for an algorithm to reach the optimal policy (ESL) and the effectiveness of its exploration process (OMR).

## Foundational Learning
- **Occupancy measures**: probability distributions over state-action pairs induced by a policy; needed to represent the exploration process of RL algorithms; quick check: verify that the occupancy measure integrates to 1 over the state-action space
- **Optimal transport**: a mathematical framework to compare probability distributions; needed to quantify the similarity between consecutive policies; quick check: ensure that the optimal transport distance satisfies the triangle inequality
- **Manifold of occupancy measures**: a geometric representation of the space of all possible occupancy measures; needed to study the trajectory of policies during learning; quick check: verify that the manifold is connected and has a well-defined metric
- **Regret**: a measure of the suboptimality of a policy compared to the optimal policy; needed to define the OMR metric; quick check: ensure that the regret is non-negative and decreases with better policies

## Architecture Onboarding
- **Component map**: Environment -> Policy -> Occupancy Measure -> Optimal Transport Distance -> ESL/OMR
- **Critical path**: The sequence of policies induced by an RL algorithm during learning, represented as a trajectory in the manifold of occupancy measures
- **Design tradeoffs**: The choice of optimal transport distance (e.g., Wasserstein vs. Kullback-Leibler) and the discretization of the state-action space affect the accuracy and computational cost of ESL and OMR
- **Failure signatures**: ESL and OMR may be sensitive to hyperparameters and implementation details, such as the learning rate and function approximation method
- **3 first experiments**:
  1. Compare ESL and OMR for Q-learning with different exploration rates (ϵ) in a gridworld environment
  2. Evaluate the sensitivity of ESL and OMR to the choice of optimal transport distance
  3. Assess the impact of function approximation on the accuracy of ESL and OMR estimates

## Open Questions the Paper Calls Out
None

## Limitations
- The framework relies on occupancy measures, which can be challenging to estimate accurately in high-dimensional state-action spaces
- The assumption of a fixed policy space may not hold in continuous control tasks
- The computational cost of computing optimal transport distances can be high, especially for large policy spaces

## Confidence
- Theoretical validity: High, as the connections to optimal transport theory are well-established
- Practical utility of ESL and OMR: Medium, as their sensitivity to hyperparameters and implementation details requires further investigation

## Next Checks
1. Evaluate ESL and OMR on more complex environments, such as continuous control tasks from the MuJoCo suite, to assess their applicability beyond gridworlds and discrete action spaces
2. Investigate the sensitivity of ESL and OMR to hyperparameters (e.g., learning rate, exploration rate) and implementation details (e.g., function approximation, optimization algorithm) to determine their robustness and interpretability
3. Compare ESL and OMR with alternative exploration metrics, such as state visitation entropy or learning curves, to establish their unique value proposition and complementarity to existing analysis tools