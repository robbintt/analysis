---
ver: rpa2
title: 'How to build the best medical image segmentation algorithm using foundation
  models: a comprehensive empirical study with Segment Anything Model'
arxiv_id: '2404.09957'
source_url: https://arxiv.org/abs/2404.09957
tags:
- segmentation
- training
- medical
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates fine-tuning strategies for
  the Segment Anything Model (SAM) in medical image segmentation. It examines 18 combinations
  of backbone architectures, model components, and fine-tuning algorithms across 17
  medical datasets.
---

# How to build the best medical image segmentation algorithm using foundation models: a comprehensive empirical study with Segment Anything Model

## Quick Facts
- **arXiv ID**: 2404.09957
- **Source URL**: https://arxiv.org/abs/2404.09957
- **Reference count**: 40
- **Primary result**: Fine-tuning SAM with parameter-efficient learning in both encoder and decoder outperforms traditional UNet methods for medical image segmentation

## Executive Summary
This comprehensive study systematically evaluates 18 fine-tuning strategies for Segment Anything Model (SAM) across 17 medical imaging datasets. The authors investigate three encoder architectures (ViT-B, ViT-H, ViT-T), three fine-tuning methods (vanilla, Adapter, LoRA), and two fine-tuning scopes (En/Decoder vs Decoder-only). Through extensive experimentation, they identify that parameter-efficient fine-tuning (PEFT) with Adapter or LoRA in both encoder and decoder yields superior performance compared to full fine-tuning or decoder-only approaches. The study also examines task-expansive prompt-based learning and self-supervised pre-training, finding limited benefits for the former and modality-specific improvements for the latter. The recommended configuration is ViT-B + En/Decoder + PEFT, which consistently outperforms baseline methods including UNet variants.

## Method Summary
The study employs a three-stage approach: (1) task-specific supervised training using ground truth masks, (2) task-expansive prompt-based learning with generated prompts, and (3) task-agnostic self-supervised learning using MAE pre-training. The authors evaluate 18 combinations across three encoder sizes (ViT-B/H/T), three fine-tuning methods (vanilla, Adapter, LoRA), and two fine-tuning scopes (En/Decoder vs Decoder-only). Experiments are conducted on 17 medical datasets spanning MRI, CT, ultrasound, and X-ray modalities. Performance is measured using Dice Similarity Coefficient (DSC%) as the primary metric, with models compared against traditional UNet, nnUNet, and SwinUNet baselines. The study releases code and MRI-specific fine-tuned weights for reproducibility.

## Key Results
- Fine-tuning SAM with parameter-efficient learning in both encoder and decoder significantly outperforms traditional UNet methods and decoder-only fine-tuning strategies
- ViT-B with En/Decoder + PEFT configuration provides the best balance of performance and efficiency across diverse medical imaging tasks
- Network architecture choice has minimal impact on final performance when using appropriate fine-tuning strategies
- Self-supervised MAE pre-training improves MRI-specific performance but shows limited benefits for other modalities
- Task-expansive prompt-based learning and additional pre-training with limited data prove ineffective for improving segmentation performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter-efficient fine-tuning (PEFT) with Adapter or LoRA outperforms full fine-tuning for medium-to-large backbones (ViT-H, ViT-B) in medical image segmentation
- Mechanism: PEFT reduces catastrophic forgetting and overfitting by updating only a small subset of parameters while preserving the pretrained knowledge of the encoder
- Core assumption: The encoder's pretrained features are transferable to medical images and don't require extensive modification
- Evidence anchors:
  - [abstract] "fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies"
  - [section] "We observe an almost definite (except in the ViT-B + Vanilla setting) improvement in the average performance when changing from only updating the decoder parameters to that of both the encoder and decoder"
  - [corpus] No direct evidence, but corpus neighbors discuss parameter-efficient fine-tuning strategies
- Break condition: When the domain gap between natural and medical images is too large, or when the target task requires significant architectural changes

### Mechanism 2
- Claim: Updating both encoder and decoder (En/Decoder) yields better performance than decoder-only fine-tuning
- Mechanism: Medical images have different visual characteristics than natural images, requiring adaptation of the encoder to extract relevant features
- Core assumption: SAM's encoder, pretrained on natural images, lacks the domain-specific knowledge needed for medical image segmentation
- Evidence anchors:
  - [abstract] "fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies"
  - [section] "We first observe an almost definite (except in the ViT-B + Vanilla setting) improvement in the average performance when changing from only updating the decoder parameters to that of both the encoder and decoder"
  - [corpus] No direct evidence, but related work in the corpus discusses encoder adaptation
- Break condition: When the encoder's pretrained features are highly transferable to the target domain, or when computational resources are severely limited

### Mechanism 3
- Claim: Self-supervised learning (MAE) on medical images improves segmentation performance when the test data is from the same modality
- Mechanism: MAE pretraining on medical images adapts the encoder to the domain-specific visual characteristics without requiring labels
- Core assumption: The encoder can learn useful representations for segmentation tasks through reconstruction of masked patches
- Evidence anchors:
  - [abstract] "further training SAM with self-supervised learning can improve final model performance"
  - [section] "We observe an improvement in the average performance of all MRIs across all architecture combinations besides En/Decoder + LoRA"
  - [corpus] No direct evidence, but corpus neighbors discuss self-supervised learning for medical images
- Break condition: When the target task has significantly different characteristics from the pretraining data, or when labeled data is abundant

## Foundational Learning

- Concept: Catastrophic forgetting in fine-tuning
  - Why needed here: Understanding why PEFT outperforms full fine-tuning is crucial for choosing the right approach
  - Quick check question: What happens to the pretrained knowledge when fine-tuning all parameters of a large model on a small dataset?

- Concept: Domain adaptation
  - Why needed here: Medical images have different visual characteristics than natural images, requiring adaptation of the model
  - Quick check question: How can we measure the domain gap between natural and medical images?

- Concept: Self-supervised learning objectives
  - Why needed here: MAE pretraining is used to adapt the encoder to medical images without labels
  - Quick check question: What is the reconstruction objective in MAE, and how does it help learn useful representations?

## Architecture Onboarding

- Component map:
  - Image encoder (ViT-B, ViT-H, ViT-T) -> Prompt encoder (fixed) -> Mask decoder (transformer decoder) -> Adapter/LoRA blocks (optional)

- Critical path:
  - Load pretrained SAM weights
  - Choose encoder architecture (ViT-B recommended)
  - Decide on PEFT method (Adapter or LoRA)
  - Apply En/Decoder fine-tuning
  - Train on target dataset

- Design tradeoffs:
  - Encoder size: ViT-B vs. ViT-H vs. ViT-T (speed vs. performance)
  - Fine-tuning scope: En/Decoder vs. Decoder only (performance vs. efficiency)
  - PEFT method: Adapter vs. LoRA (implementation complexity vs. performance)

- Failure signatures:
  - Model collapse during training (high validation loss)
  - Overfitting (high training accuracy, low validation accuracy)
  - Underfitting (low training and validation accuracy)

- First 3 experiments:
  1. ViT-B + En/Decoder + Adapter on a single MRI dataset
  2. ViT-B + Decoder only + LoRA on the same dataset
  3. ViT-B + En/Decoder + Vanilla fine-tuning for comparison

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of ViT-T with PEFT compared to larger backbones vary significantly across different medical imaging modalities (e.g., MRI vs. CT vs. ultrasound)?
- Basis in paper: [inferred] The paper states that ViT-T achieves similar performance to larger backbones (ViT-H, ViT-B) across all 17 datasets, but does not analyze performance differences across modalities.
- Why unresolved: The paper's results are aggregated across all modalities, making it impossible to determine if modality-specific performance differences exist.
- What evidence would resolve it: Separate performance analysis of ViT-T vs. larger backbones for each imaging modality, showing potential variations in relative effectiveness.

### Open Question 2
- Question: What is the impact of using different prompt generation strategies (e.g., random sampling vs. error-region sampling) on the performance of task-expansive prompt-based learning?
- Basis in paper: [explicit] The paper mentions that it uses a different prompt generation strategy (random sampling with distance-based point selection) compared to SAM's original strategy (uniform sampling with error-region refinement), but does not evaluate the impact of this choice.
- Why unresolved: The paper only tests one prompt generation strategy and does not compare it to alternatives or to SAM's original approach.
- What evidence would resolve it: Comparative experiments testing different prompt generation strategies (including SAM's original approach) within the task-expansive prompt-based learning framework.

### Open Question 3
- Question: How does the performance of fine-tuned SAM models compare to human expert performance in medical image segmentation tasks?
- Basis in paper: [inferred] The paper extensively evaluates SAM's performance against other automated methods (UNet variants) but does not benchmark against human performance, which is the gold standard in medical image analysis.
- Why unresolved: The study focuses on comparing different algorithmic approaches but lacks a comparison to the actual clinical standard of human expert annotation.
- What evidence would resolve it: Direct comparison studies measuring dice similarity scores of fine-tuned SAM models against human expert segmentations on the same datasets.

## Limitations

- Architecture generalizability: The study focuses on 17 medical datasets but only tests three encoder architectures, limiting generalizability to other backbone designs
- Dataset representativeness: Heavy MRI focus (14 of 17 datasets) may bias findings toward MRI characteristics, with limited validation on CT, ultrasound, and X-ray modalities
- Computational resource implications: The study recommends En/Decoder fine-tuning with PEFT but does not quantify computational trade-offs or inference overhead differences between strategies

## Confidence

- **High confidence**: Superiority of PEFT methods over full fine-tuning for medium-to-large backbones
- **High confidence**: En/Decoder fine-tuning outperforms decoder-only approaches
- **Medium confidence**: ViT-B as the optimal encoder recommendation
- **Low confidence**: Self-supervised MAE pre-training benefits are modality-specific and not robustly demonstrated
- **Low confidence**: Task-expansive prompt-based learning is ineffective based on limited experimentation

## Next Checks

1. **Cross-modal validation**: Test the recommended ViT-B + En/Decoder + LoRA configuration on non-MRI datasets (CT, US, X-ray) to verify whether the performance advantages generalize across medical imaging modalities or are specific to MRI characteristics

2. **Encoder architecture ablation**: Implement and test additional backbone architectures (ConvNeXt, Swin Transformer, ResNet) with the same En/Decoder + PEFT configuration to determine whether the findings are specific to ViT-based encoders or apply more broadly to transformer-based medical image segmentation

3. **Computational overhead analysis**: Measure and compare the inference latency, memory usage, and parameter counts for all 18 fine-tuning strategies to quantify the practical trade-offs between the recommended approach and alternatives, particularly for clinical deployment scenarios with resource constraints