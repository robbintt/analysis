---
ver: rpa2
title: 'Don''t Think It Twice: Exploit Shift Invariance for Efficient Online Streaming
  Inference of CNNs'
arxiv_id: '2408.03223'
source_url: https://arxiv.org/abs/2408.03223
tags:
- inference
- pooling
- streaming
- padding
- window
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes StreamiNNC, a strategy to deploy Convolutional
  Neural Networks (CNNs) for online streaming inference by exploiting shift invariance
  properties of convolution. The method addresses the computational overhead introduced
  by overlapping windows in time-series processing, which is common in deep learning
  applications such as robotics and healthcare.
---

# Don't Think It Twice: Exploit Shift Invariance for Efficient Online Streaming Inference of CNNs

## Quick Facts
- arXiv ID: 2408.03223
- Source URL: https://arxiv.org/abs/2408.03223
- Authors: Christodoulos Kechris; Jonathan Dan; Jose Miranda; David Atienza
- Reference count: 28
- Primary result: Achieves low deviation (2.03 - 3.55% NRMSE) between streaming and normal CNN inference while linearly speeding up execution

## Executive Summary
This paper presents StreamiNNC, a strategy to enable efficient online streaming inference for Convolutional Neural Networks (CNNs) processing time-series data. The method exploits the shift invariance properties of convolution to eliminate the computational overhead typically associated with overlapping input windows in streaming applications. By analyzing the limitations introduced by zero-padding and pooling operations, the authors derive theoretical error bounds and propose signal padding and pooling alignment techniques to maintain inference accuracy while achieving linear speedup.

## Method Summary
StreamiNNC addresses the computational inefficiency of traditional overlapping window approaches by exploiting convolution's shift invariance. The method involves signal padding (replacing zero-padding with previous window values) and pooling alignment (ensuring window step size is a multiple of pooling window length). The approach requires retraining CNNs with extended signal windows when using signal padding. The paper provides theoretical error bounds for pooling misalignment and demonstrates the method across three biomedical signal processing applications: heart rate estimation from PPG signals, seizure detection from EEG signals, and seizure detection from wrist acceleration data.

## Key Results
- Achieved 2.03 - 3.55% NRMSE deviation between streaming and normal inference across three biomedical applications
- Demonstrated linear speedup in inference execution time for streaming mode
- Showed effective heart rate extraction with MAE of 1.76 BPM in streaming mode for PPG data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-padding destroys convolution's shift invariance by introducing positional information, causing deviation in streaming inference.
- Mechanism: Padding with zeros at the boundaries of each input window breaks the natural temporal translation invariance of convolution because the padded zeros introduce artificial boundaries that are not present in the original continuous signal.
- Core assumption: The original CNN was trained with zero-padding, and the convolutional kernels are large enough relative to the input window size that zero-padding effects become significant in deeper layers.
- Evidence anchors:
  - [abstract] "Although convolutions are shift-invariant, zero-padding and pooling operations, widely used in such networks, are not efficient and complicate efficient streaming inference."
  - [section] "Zero Padding Effect" and Figure 5 show that after several layers, more than 50% of activation outputs are affected by zeros in the padding.
  - [corpus] No direct evidence in corpus; the claim is supported by the paper's experimental results and theoretical analysis.
- Break condition: If the convolutional kernels are small relative to the input window size, or if the network architecture uses signal padding during training, the zero-padding effect may be negligible.

### Mechanism 2
- Claim: Pooling operations can be approximately shift invariant if the window step size is a multiple of the pooling window length, but misalignment introduces approximation errors.
- Mechanism: When the window step size (S) is not aligned with the pooling window length (Lp), the pooling operation cannot be exactly shift invariant. However, if the sampling frequency is high enough relative to the signal's bandwidth, the approximation error can be small.
- Core assumption: The input signal is band-limited, and the sampling frequency is sufficiently high compared to the signal's bandwidth.
- Evidence anchors:
  - [abstract] "We derive theoretical error upper bounds for pooling during streaming."
  - [section] "Pooling" section derives error bounds for approximating pooling as shift invariant when S and Lp are misaligned.
  - [corpus] No direct evidence in corpus; the claim is supported by the paper's theoretical derivations.
- Break condition: If the sampling frequency is not high enough relative to the signal's bandwidth, or if the pooling window length is too large, the approximation error can become significant.

### Mechanism 3
- Claim: Signal padding allows exact streaming inference by replacing zero-padding with previous window values, preserving shift invariance.
- Mechanism: Instead of padding with zeros, signal padding uses values from the previous window as padding. This maintains the temporal translation invariance of the convolution operation because the padding values are consistent with the original signal's continuity.
- Core assumption: The original CNN can be retrained with signal padding, or the zero-padding effects are small enough to be negligible.
- Evidence anchors:
  - [abstract] "We address these limitations by proposing signal padding and pooling alignment."
  - [section] "Signal Padding" describes how signal padding replaces zero-padding with previous window values.
  - [corpus] No direct evidence in corpus; the claim is supported by the paper's experimental results.
- Break condition: If the convolutional kernels are too large relative to the input window size, or if the signal padding retraining is not feasible, signal padding may not be practical.

## Foundational Learning

- Concept: Temporal translation invariance
  - Why needed here: Understanding how convolution and pooling operations affect temporal translation invariance is crucial for designing efficient streaming inference strategies.
  - Quick check question: If a convolution operation is applied to a signal and then the signal is shifted in time, will the output also be shifted by the same amount?

- Concept: Zero-padding vs. signal padding
  - Why needed here: Choosing between zero-padding and signal padding affects the shift invariance of the network and the accuracy of streaming inference.
  - Quick check question: What is the difference between zero-padding and signal padding, and how does each affect the shift invariance of a convolutional neural network?

- Concept: Pooling window alignment
  - Why needed here: Ensuring that the window step size is aligned with the pooling window length is crucial for maintaining shift invariance in streaming inference.
  - Quick check question: If the window step size is not a multiple of the pooling window length, what effect does this have on the shift invariance of the pooling operation?

## Architecture Onboarding

- Component map: Feature extractor (h) -> Classifier (g)
- Critical path:
  1. Process new input samples through the feature extractor.
  2. Store intermediate embeddings in the aggregation buffer.
  3. Aggregate embeddings from all sub-windows.
  4. Process the aggregated embedding through the classifier.
  5. Output the final inference result.

- Design tradeoffs:
  - Signal padding vs. zero-padding: Signal padding ensures exact shift invariance but requires retraining and additional memory for padding buffers. Zero-padding is simpler but can introduce errors in streaming inference.
  - Exact streaming vs. approximate streaming: Exact streaming maintains accuracy but requires additional buffers and memory. Approximate streaming is faster but can introduce errors due to zero-padding effects.

- Failure signatures:
  - High NRMSE between streaming and full inference results.
  - Significant deviation in activation outputs between consecutive windows.
  - Memory overflow due to insufficient buffer sizes for signal padding or aggregation.

- First 3 experiments:
  1. Evaluate the effect of zero-padding on the shift invariance of the network using constant input and moving average filter weights.
  2. Test streaming inference with zero-padding on a pre-trained CNN and compare the results to full inference.
  3. Implement signal padding and retrain the CNN, then evaluate streaming inference accuracy and memory usage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal signal padding length (La) to minimize retraining time while ensuring shift invariance?
- Basis in paper: [explicit] The paper mentions that La should be at least equal to the receptive field of the deepest convolutional layer in the feature extractor.
- Why unresolved: The paper does not provide experimental data on how different values of La affect retraining time and shift invariance.
- What evidence would resolve it: Empirical studies comparing retraining times and inference accuracy for different values of La.

### Open Question 2
- Question: How does the choice of pooling method (e.g., average pooling vs. max pooling) affect the shift invariance and accuracy of streaming inference?
- Basis in paper: [explicit] The paper derives theoretical error bounds for pooling during streaming and discusses the effect of pooling misalignment.
- Why unresolved: The paper does not provide experimental comparisons of different pooling methods in terms of shift invariance and accuracy.
- What evidence would resolve it: Experiments comparing the performance of different pooling methods in streaming inference scenarios.

### Open Question 3
- Question: Can the StreamiNNC approach be extended to other types of neural networks, such as recurrent neural networks (RNNs) or transformers, for online streaming inference?
- Basis in paper: [inferred] The paper focuses on convolutional neural networks (CNNs) and their shift invariance properties.
- Why unresolved: The paper does not explore the applicability of StreamiNNC to other neural network architectures.
- What evidence would resolve it: Theoretical analysis and experimental validation of StreamiNNC for RNNs or transformers in streaming inference tasks.

## Limitations

- The approach requires retraining CNNs with signal padding, which may not be feasible for all architectures or when pre-trained models are used
- The strict pooling alignment requirement (S mod Lp = 0) limits architectural flexibility and may not be practical for all applications
- Experimental evaluation is limited to biomedical signal processing applications, limiting generalizability to other time-series domains

## Confidence

- **High Confidence**: The theoretical analysis of zero-padding effects on shift invariance (Section 4.1) is well-supported by mathematical derivations and experimental validation.
- **Medium Confidence**: The signal padding mechanism and retraining approach shows promise but lacks detailed implementation specifications and broader architectural validation.
- **Medium Confidence**: The pooling alignment requirements and error bounds are theoretically sound but may be overly restrictive in practical applications.

## Next Checks

1. **Architectural Generalization Test**: Apply StreamiNNC to diverse CNN architectures beyond the three biomedical signal processing networks used in the paper, including varying kernel sizes and depths.
2. **Pooling Alignment Relaxation**: Investigate the practical impact of relaxing the strict pooling alignment requirement (S mod Lp = 0) through experiments with different step sizes and pooling configurations.
3. **Memory Overhead Analysis**: Conduct comprehensive measurements of memory usage for signal padding buffers across different signal types and window sizes to quantify the practical feasibility of the approach.