---
ver: rpa2
title: 'Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?'
arxiv_id: '2407.14790'
source_url: https://arxiv.org/abs/2407.14790
tags:
- reasoning
- error
- puzzle
- lost
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates LLMs' logical reasoning abilities using a new
  dataset of 274 grid-based puzzles. It develops a fine-grained error taxonomy and
  proposes automated evaluation methods to identify where LLMs fail in reasoning chains.
---

# Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?

## Quick Facts
- **arXiv ID:** 2407.14790
- **Source URL:** https://arxiv.org/abs/2407.14790
- **Reference count:** 40
- **Primary result:** LLMs achieve low accuracy (max 5.1%) on grid puzzles despite most reasoning steps being correct, with dominant errors in wrong reasoning and elimination.

## Executive Summary
This paper evaluates large language models' logical reasoning abilities using a novel dataset of 274 grid-based puzzles. The study develops a fine-grained error taxonomy through manual analysis of 150 reasoning chains and proposes an automated evaluation framework (PuzzleEval) to identify where LLMs fail in their reasoning chains. Despite most individual reasoning steps being error-free, LLMs achieve very low accuracy (maximum 5.1%), with dominant error types including wrong reasoning and elimination. The paper finds that existing prompting methods do not improve performance, highlighting the need for better approaches to enhance LLMs' puzzle-solving abilities.

## Method Summary
The study introduces GridPuzzle, a dataset of 274 grid-based puzzles with varying sizes (3×4, 3×5, 4×4, 4×5, 4×6) and three difficulty levels. LLMs generate reasoning chains using zero-shot Chain-of-Thought prompting, which are then evaluated both manually and automatically. The authors develop a five-category error taxonomy (Wrong Reasoning, Wrong Elimination, Right Reasoning, Right Wrong, No Conclusion) with nine sub-categories through iterative manual analysis of 150 chains. They propose PuzzleEval, an LLM-based reference-free metric that extracts logical conclusions from reasoning chains, identifies pair-wise relations, and validates these against gold solutions. The framework achieves ~86% agreement with manual annotation and enables scalable analysis of error distributions across the full dataset.

## Key Results
- LLMs achieve maximum accuracy of 5.1% on grid puzzles despite most reasoning steps being error-free
- Error analysis reveals that "RR" (correct reasoning) steps occur mainly in initial chain halves while errors occur in latter halves
- Existing prompting methods (Plan-and-Solve, Self-correct, Self-discover, Feedback-Learning) do not improve performance
- Automated evaluation using GPT-4o achieves ~86% agreement with manual annotation
- Dominant error types are Wrong Reasoning and Wrong Elimination across all tested LLM models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evaluating reasoning chains with fine-grained error taxonomy reveals hidden failure modes that final answer accuracy misses
- Mechanism: Manual analysis of 150 reasoning chains identified five broad error categories (WW, WR, RW, RR, NC) and nine sub-categories. Automated evaluation using GPT-4o achieves ~86% agreement with manual annotation, enabling scalable identification of specific error types like "Wrong Reasoning" and "Error Propagation" that explain why models fail despite having many correct reasoning steps
- Core assumption: Error categorization can be automated reliably enough to scale beyond manual analysis while maintaining accuracy
- Evidence anchors:
  - [abstract] "We propose a new error taxonomy derived from manual analysis of reasoning chains from LLMs including GPT-4, Claude-3, Gemini, Mistral, and Llama-2"
  - [section 3.3] "Based on this method, we have carefully filtered and categorized several errors made by LLMs, presenting them as five broad categories and nine sub-categories"
  - [corpus] Weak - no corpus evidence provided for error taxonomy validity

### Mechanism 2
- Claim: PuzzleEval metric provides reference-free evaluation of reasoning chain correctness using only gold solution
- Mechanism: Three-stage pipeline extracts logical conclusions from reasoning chains, identifies pair-wise relations between puzzle elements, and validates these pairs against gold solution to compute correctness score for each step and average correctness score for entire chain
- Core assumption: Logical conclusions extracted from reasoning chains can be accurately mapped to pair-wise relations that are verifiable against final answer
- Evidence anchors:
  - [abstract] "we propose PuzzleEval, an LLM-based framework to evaluate reasoning chains for grid-based puzzles"
  - [section 4.2] "PuzzleEval is a reference-free metric specifically designed for assessing reasoning chains generated for grid-based puzzle tasks"
  - [corpus] Weak - no corpus evidence provided for PuzzleEval effectiveness

### Mechanism 3
- Claim: Disparity between reasoning chain correctness and final answer accuracy reveals timing of errors within chains
- Mechanism: Analysis shows "RR" (correct reasoning) steps occur mainly in initial half of chains while "RW", "WR", "WW" errors occur in latter half, explaining why models with many correct steps still achieve low accuracy
- Core assumption: Error location within reasoning chain correlates with final answer correctness
- Evidence anchors:
  - [section 5.2] "It has been observed that 'RR' category reasoning steps mainly occur in the initial half of the chain, leading to a high overall PuzzleEval score. Conversely, errors in the 'RW', 'WR', and 'WW' categories typically occur in the latter half of the chain"
  - [section 5.1] "Despite most reasoning steps being error-free, LLMs achieve low accuracy (max 5.1%) on the task"
  - [corpus] Weak - no corpus evidence provided for temporal error distribution patterns

## Foundational Learning

- Concept: Error taxonomy development through iterative manual analysis
  - Why needed here: To systematically identify and categorize LLM reasoning failures beyond simple right/wrong classification
  - Quick check question: How many reasoning chains were manually analyzed to develop the error taxonomy before no new error types emerged?

- Concept: Automated evaluation using LLM as judge
  - Why needed here: Manual analysis is tedious and doesn't scale to entire dataset; automated evaluation enables broader insights into error distributions
  - Quick check question: What agreement percentage was achieved between manual and automated evaluation?

- Concept: Reference-free evaluation metrics
  - Why needed here: Traditional reference-based metrics require gold reasoning chains which are expensive to create; reference-free metrics use only final answers
  - Quick check question: What is the range of values for the Average Correctness Score (ACS) produced by PuzzleEval?

## Architecture Onboarding

- Component map: GridPuzzle dataset (274 puzzles) → LLM reasoning generation → Manual evaluation (150 chains) → Error taxonomy development → Automated evaluation (1,370 chains) → PuzzleEval metric → Performance analysis
- Critical path: Error taxonomy development → Automated evaluation implementation → PuzzleEval metric development → Performance analysis and findings
- Design tradeoffs: Manual analysis provides high-quality error categorization but doesn't scale; automated evaluation scales but may miss nuanced errors; reference-free metrics avoid gold reasoning chains but may miss intermediate reasoning quality
- Failure signatures: Low agreement between manual and automated evaluation (>14% discrepancy); PuzzleEval scores that don't correlate with observed reasoning quality; Error taxonomy that doesn't capture new error types in expanded dataset
- First 3 experiments:
  1. Evaluate 20 reasoning chains manually and automatically to measure agreement and refine error taxonomy
  2. Run PuzzleEval on a subset of reasoning chains and compare scores with manual correctness judgments
  3. Test prompting strategies (Plan-and-Solve, Self-correct, Self-discover, Feedback-Learning) on 3x4 grid subset and measure impact on both accuracy and PuzzleEval scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would integrating formal logic frameworks into the error taxonomy significantly improve the precision of error categorization?
- Basis in paper: Inferred
- Why unresolved: The paper suggests that the current error taxonomy, while effective, could be further refined by mapping errors to formal logic to identify more detailed and atomic errors. However, this approach was not implemented or tested.
- What evidence would resolve it: Implementing formal logic mapping and comparing its error categorization precision against the current taxonomy.

### Open Question 2
- Question: How would expanding GridPuzzle to include more complex grid sizes beyond 4x6 and different types of puzzles affect the performance and error patterns of LLMs?
- Basis in paper: Inferred
- Why unresolved: The paper acknowledges that GridPuzzle's complexity could be enhanced by incorporating larger grid sizes and different puzzle types, but this was not explored.
- What evidence would resolve it: Evaluating LLM performance and error distributions on an expanded GridPuzzle dataset with larger grids and diverse puzzle types.

### Open Question 3
- Question: Would using supervised learning methods to automate error identification outperform the current LLM-based auto-evaluator?
- Basis in paper: Inferred
- Why unresolved: The paper suggests exploring other automated methods using smaller-scale supervised learning as a promising future research direction, but does not provide results or comparisons.
- What evidence would resolve it: Developing and testing a supervised learning model for error identification and comparing its performance to the LLM-based auto-evaluator.

### Open Question 4
- Question: How would the performance of LLMs on GridPuzzle change if evaluated in multilingual scenarios?
- Basis in paper: Inferred
- Why unresolved: The paper notes that the research is currently limited to the English language and could be extended to multilingual scenarios, but does not provide any data or analysis.
- What evidence would resolve it: Evaluating LLM performance on GridPuzzle in multiple languages and analyzing any differences in accuracy and error patterns.

## Limitations

- Automated error classification achieves only ~86% agreement with manual annotation, potentially missing or misclassifying 14% of errors
- The error taxonomy and PuzzleEval metric lack corpus evidence for validation across diverse puzzle types
- The study only tests five LLM models with limited prompting strategies, leaving uncertainty about whether different approaches might yield better results
- The GridPuzzle dataset contains only 274 puzzles across five grid sizes, which may not capture the full diversity of logical reasoning challenges

## Confidence

- **High confidence**: The observation that LLMs achieve low accuracy (max 5.1%) on grid puzzles despite most reasoning steps being error-free is well-supported by the experimental results across multiple models
- **Medium confidence**: The proposed error taxonomy and automated evaluation framework are reasonable approaches but lack extensive validation corpus evidence to confirm their robustness and completeness
- **Low confidence**: The claim that existing prompting methods do not improve performance is based on limited testing (only five methods on a subset of puzzles), suggesting the conclusion may be premature without broader experimentation

## Next Checks

1. **Validate automated error classification reliability**: Manually review 50 additional reasoning chains and calculate the agreement rate with automated classification to confirm the 86% consistency holds across expanded samples

2. **Test broader prompting strategies**: Implement and evaluate at least 10 additional prompting techniques (including tree-of-thought, scratchpad, and debate-based approaches) on the full puzzle dataset to verify that existing methods truly do not improve performance

3. **Assess puzzle diversity coverage**: Analyze the GridPuzzle dataset to determine whether the 274 puzzles adequately represent different types of logical reasoning challenges, and test model performance on puzzles from external sources to evaluate generalizability