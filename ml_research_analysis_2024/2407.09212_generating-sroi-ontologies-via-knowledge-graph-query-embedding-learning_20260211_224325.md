---
ver: rpa2
title: Generating $SROI^-$ Ontologies via Knowledge Graph Query Embedding Learning
arxiv_id: '2407.09212'
source_url: https://arxiv.org/abs/2407.09212
tags:
- query
- acone
- cone
- logical
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces AConE, a query embedding method that answers\
  \ complex logical queries over incomplete knowledge graphs by learning SROI\u2212\
  \ description logic axioms. The method embeds SROI\u2212 concepts as cones in complex\
  \ vector space and relations as transformations that rotate and scale these cones,\
  \ enabling a one-to-one mapping between logical and geometric operators."
---

# Generating $SROI^-$ Ontologies via Knowledge Graph Query Embedding Learning

## Quick Facts
- arXiv ID: 2407.09212
- Source URL: https://arxiv.org/abs/2407.09212
- Authors: Yunjie He; Daniel Hernandez; Mojtaba Nayyeri; Bo Xiong; Yuqicheng Zhu; Evgeny Kharlamov; Steffen Staab
- Reference count: 40
- Primary result: AConE achieves 18.35% higher average accuracy than baseline models on WN18RR with significantly fewer parameters

## Executive Summary
This paper introduces AConE, a novel query embedding method that answers complex logical queries over incomplete knowledge graphs by learning SROI− description logic axioms. The method represents SROI− concepts as cones in complex vector space and relations as transformations that rotate and scale these cones, enabling a one-to-one mapping between logical and geometric operators. Unlike existing approaches that rely heavily on neural networks, AConE is more parameter-efficient while maintaining explainability. Theoretical analysis shows that AConE can learn specific SROI− axioms, and empirical results on multiple datasets demonstrate superior performance over state-of-the-art baselines.

## Method Summary
AConE addresses the challenge of answering complex logical queries on incomplete knowledge graphs by embedding SROI− description logic axioms in complex vector space. The method uses cone-based representations where concepts are embedded as cones and relations are represented as transformations that rotate and scale these cones. This geometric approach establishes a direct correspondence between logical operators and geometric transformations, enabling efficient query answering while maintaining explainability. The method is designed to be more parameter-efficient than existing neural network-based approaches while preserving the ability to learn specific SROI− axioms through theoretical guarantees.

## Key Results
- AConE achieves an 18.35% higher average accuracy than baseline models on the WN18RR dataset
- The method demonstrates superior performance over state-of-the-art baselines across multiple datasets
- AConE maintains parameter efficiency while providing explainable query answering capabilities

## Why This Works (Mechanism)
AConE works by leveraging the geometric properties of complex vector spaces to represent logical relationships. By embedding concepts as cones and relations as transformations (rotations and scalings), the method creates a direct mapping between logical operators and geometric operations. This approach allows for efficient computation of query answers while maintaining interpretability. The cone-based representation naturally captures the hierarchical and relational structure inherent in description logic axioms, enabling the model to learn and reason about complex logical relationships with fewer parameters than traditional neural approaches.

## Foundational Learning
- **Description Logic SROI−**: A fragment of description logic that forms the theoretical foundation for the ontology reasoning task; needed to define the scope of axioms that can be learned and reasoned about
- **Complex Vector Space Embeddings**: Mathematical framework for representing concepts and relations as geometric objects; enables the one-to-one mapping between logical and geometric operators
- **Cone Representations**: Geometric objects used to embed concepts; provides natural representation for hierarchical relationships in ontologies
- **Geometric Transformations**: Operations (rotation and scaling) used to represent relations; enables composition of multiple relations in logical queries
- **Parameter Efficiency in Neural Models**: Design principle that reduces model complexity while maintaining performance; critical for practical deployment and interpretability
- **Explainability in Query Answering**: Ability to trace reasoning steps and understand model decisions; essential for trust and debugging in knowledge graph applications

## Architecture Onboarding

**Component Map**: Input KG -> Cone Embedding Layer -> Relation Transformation Layer -> Query Evaluation Layer -> Output

**Critical Path**: Knowledge graph triples are first converted to cone embeddings, then relation transformations are applied sequentially to evaluate complex logical queries, with the final cone representation used to determine query answers

**Design Tradeoffs**: The geometric approach trades some representational flexibility for parameter efficiency and explainability, compared to fully neural approaches. The restriction to SROI− axioms limits applicability but enables theoretical guarantees and simpler geometric interpretations.

**Failure Signatures**: 
- Poor performance on queries involving relations not well-represented by rotations and scalings
- Degradation when query complexity exceeds the expressiveness of cone-based representations
- Potential issues with very large knowledge graphs due to computational complexity of geometric operations

**First Experiments**:
1. Evaluate AConE on simple conjunctive queries to establish baseline performance
2. Test the model on queries involving negation and existential quantification to assess SROI− coverage
3. Compare parameter counts and inference times against baseline neural approaches on the same tasks

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Focus on SROI− axioms restricts applicability to more complex description logics
- Evaluation limited to knowledge graph datasets without testing on real-world ontology reasoning tasks
- Geometric interpretation may not scale well to larger, more complex queries

## Confidence
- Parameter efficiency claims: High
- Explainability benefits: High
- Accuracy improvement on benchmarks: High
- Scalability to complex queries: Medium
- Applicability to general ontology reasoning: Medium

## Next Checks
1. Test AConE on real-world ontologies with varying complexity levels to assess practical applicability
2. Evaluate performance on larger knowledge graphs and more complex logical queries to verify scalability
3. Conduct ablation studies to quantify the contribution of different geometric components to overall performance