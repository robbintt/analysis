---
ver: rpa2
title: 'MacBehaviour: An R package for behavioural experimentation on large language
  models'
arxiv_id: '2405.07495'
source_url: https://arxiv.org/abs/2405.07495
tags: []
core_contribution: MacBehaviour is an R package designed to facilitate behavioral
  experimentation on large language models (LLMs) by providing a standardized interface
  for interacting with over 60 different models including OpenAI GPT family, Claude
  family, Gemini, Llama family, and open-source models. The package offers functions
  for experiment design, stimuli presentation, model behavior manipulation, response
  logging, and token probability collection, supporting both cloud-based LLMs through
  APIs and self-hosted models via FastChat.
---

# MacBehaviour: An R package for behavioural experimentation on large language models

## Quick Facts
- arXiv ID: 2405.07495
- Source URL: https://arxiv.org/abs/2405.07495
- Reference count: 0
- Primary result: R package providing standardized interface for behavioral experimentation on 60+ LLMs

## Executive Summary
MacBehaviour is an R package designed to standardize behavioral experimentation on large language models (LLMs) by providing a unified interface for over 60 different models including OpenAI GPT family, Claude family, Gemini, Llama family, and various open-source models. The package addresses the challenge of inconsistent experimental designs and manual processes in LLM behavioral research by offering functions for experiment design, stimuli presentation, model behavior manipulation, response logging, and token probability collection. It supports both cloud-based LLMs through APIs and self-hosted models via FastChat, making it accessible for researchers with different technical resources.

The package was validated through psycholinguistic experiments replicating sound-gender association phenomena, where novel personal names were used to test gender inference based on phonology. The validation demonstrated that MacBehaviour can effectively reproduce human-like behavioral patterns in LLMs across different experimental paradigms, including multiple-trials-per-run and one-trial-per-run designs. The results showed significant effects across GPT-3.5, Llama-2 7B, and Vicuna-1.5 13B models, highlighting both the package's versatility and the importance of experimental design considerations in LLM behavioral research.

## Method Summary
The MacBehaviour package provides a systematic framework for conducting behavioral experiments on LLMs through a standardized workflow. Researchers prepare experimental stimuli as CSV files or data frames with columns for Run, Item, Condition, and Prompt. The package offers two experimental designs: multiple-trials-per-run (similar to traditional psychological experiments) and one-trial-per-run (each stimulus in a separate conversation). Key functions include loadData() for importing stimuli, experimentDesign() for defining experimental structure, preCheck() for configuring model parameters and verifying token limits, runExperiment() for executing experiments, and saveResults() for storing output. The package supports both cloud-based models through API integration and self-hosted models via FastChat, with functions for checking token limits, managing conversation history, and collecting token probabilities for detailed analysis.

## Key Results
- Successfully replicated sound-gender association effects in LLMs across three experimental paradigms using novel personal names
- Demonstrated consistent behavioral patterns across GPT-3.5, Llama-2 7B, and Vicuna-1.5 13B models with varying but significant effects
- Validated the package's versatility in handling different experimental designs (multiple-trials-per-run and one-trial-per-run)
- Showed that LLMs exhibit human-like tendencies to infer gender from phonology of novel personal names
- Highlighted the importance of experimental design considerations through variation in results across different paradigms

## Why This Works (Mechanism)
The package works by providing a standardized interface that abstracts away the complexities of interacting with different LLM APIs and experimental designs. By implementing consistent functions for experiment design, stimuli presentation, and response collection, MacBehaviour ensures reproducibility and comparability across different models and research studies. The token limit checking functionality prevents experiment failures due to context window constraints, while the dual support for both cloud-based and self-hosted models makes the package accessible to researchers with varying technical resources and privacy requirements.

## Foundational Learning
- Experimental design patterns: Why needed - Different designs (multiple-trials-per-run vs one-trial-per-run) can yield different results; Quick check - Compare results from both designs on same stimuli
- Token limit management: Why needed - LLMs have strict token limits that can break experiments; Quick check - Use preCheck() to verify token counts before running experiments
- API integration patterns: Why needed - Different models require different connection methods and parameters; Quick check - Test setKey() with valid credentials for multiple model types
- Data preparation standards: Why needed - Consistent data format ensures reproducibility; Quick check - Validate CSV structure matches required Run, Item, Condition, Prompt columns
- Response logging mechanisms: Why needed - Structured logging enables statistical analysis; Quick check - Verify output files contain expected response fields
- Probability collection methods: Why needed - Token probabilities provide insight into model reasoning; Quick check - Test probability collection on simple prompts

## Architecture Onboarding

**Component map:** Data preparation -> Experiment design -> Model configuration -> Token checking -> Experiment execution -> Results saving

**Critical path:** The core execution flow involves preparing stimuli data, designing the experiment structure, configuring model parameters and checking token limits, running the experiment, and saving results. This pipeline ensures that experiments are properly structured before execution and that results are systematically collected for analysis.

**Design tradeoffs:** The package trades flexibility for standardization, providing a consistent interface across many models but potentially limiting advanced customization options. The token limit constraint (2048 tokens) represents a significant design limitation that affects what types of experiments can be conducted. The choice to support both cloud-based and self-hosted models adds complexity but increases accessibility for different research contexts.

**Failure signatures:** Common failures include token limit exceeded errors, API connection failures due to incorrect credentials or URLs, and data format mismatches. The package provides diagnostic tools like preCheck() for token verification and clear error messages for connection issues, but researchers must still validate their data structure and configuration parameters.

**First experiments:**
1. Test the package with a simple prompt like "What is 2+2?" across multiple models to verify basic functionality
2. Run a small-scale sound-gender association experiment using provided stimuli to validate the experimental workflow
3. Test token limit checking by creating a prompt that approaches the 2048 token constraint to observe boundary behavior

## Open Questions the Paper Calls Out

**Open Question 1:** What are the specific effects of different experimental designs (multiple-trials-per-run vs. one-trial-per-run) on the reliability and validity of LLM behavioral measurements? The paper demonstrates that different experimental paradigms can yield different results with the same stimuli, but doesn't systematically compare the reliability or validity of these approaches across multiple types of behavioral measurements.

**Open Question 2:** How do different LLM architectures (e.g., transformer-based vs. other approaches) differ in their behavioral responses to the same psychological paradigms? While the package supports over 60 models with different architectures, the validation experiments only used three models, leaving questions about systematic architectural differences in behavioral patterns.

**Open Question 3:** To what extent do context windows and conversation history influence LLM behavioral responses in multi-trial experiments? The paper acknowledges context effects in multiple-trials-per-run designs but doesn't quantify how different amounts of conversational history affect behavioral measurements or determine optimal context management strategies.

## Limitations
- Token limit constraint (2048 tokens) may exclude experiments requiring longer context windows
- Validation relies on psycholinguistic experiments that may not generalize to other behavioral domains
- Package depends on external APIs and services, introducing potential reproducibility and long-term usability concerns
- No systematic investigation of how different experimental designs affect measurement reliability and validity

## Confidence
- Core functionality claims: **High** - The package appears well-designed and addresses a clear research need
- Validation results: **Medium** - Replication of sound-gender associations is internally consistent but generalizability to other phenomena is uncertain
- Model coverage claims: **High** - Package explicitly supports numerous models, though practical access may vary by user

## Next Checks
1. Test the package with a different behavioral experiment domain (e.g., moral reasoning or decision-making tasks) to assess generalizability beyond sound-gender associations
2. Verify the token limit handling by deliberately designing an experiment that approaches the 2048 token constraint to observe how the package manages boundary conditions
3. Test the package's performance with open-source models self-hosted via FastChat compared to cloud-based APIs to evaluate consistency across deployment methods