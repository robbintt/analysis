---
ver: rpa2
title: Understanding the Dataset Practitioners Behind Large Language Model Development
arxiv_id: '2402.16611'
source_url: https://arxiv.org/abs/2402.16611
tags:
- data
- practitioners
- https
- tools
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper defines the role of "dataset practitioners" through
  retrospective analysis and interviews with 10 practitioners working on large language
  model (LLM) development. The researchers found that data quality is the top priority
  for practitioners, but there is no consensus on what constitutes "high quality"
  data or how to evaluate it.
---

# Understanding the Dataset Practitioners Behind Large Language Model Development

## Quick Facts
- arXiv ID: 2402.16611
- Source URL: https://arxiv.org/abs/2402.16611
- Reference count: 40
- Key outcome: Defines "dataset practitioners" role and identifies challenges with data quality evaluation in LLM development

## Executive Summary
This paper defines the role of "dataset practitioners" through retrospective analysis and interviews with 10 practitioners working on large language model (LLM) development. The researchers found that data quality is the top priority for practitioners, but there is no consensus on what constitutes "high quality" data or how to evaluate it. As a result, practitioners either rely on their own intuition or write custom code to evaluate their data. The study discusses potential reasons for this phenomenon and identifies opportunities for alignment in the field.

## Method Summary
The researchers conducted a retrospective analysis of teams contributing to LLM development at Google and performed semi-structured interviews with 10 dataset practitioners across different domains of the model development lifecycle. They used grounded theory methodology to analyze the retrospective data and thematic analysis to synthesize interview findings, identifying common themes, behaviors, and representative quotes related to data quality, exploration patterns, and alignment challenges.

## Key Results
- Data quality is the top priority for practitioners, but there is no consensus on what constitutes "high quality" data
- Practitioners rely on intuition or write custom code to evaluate data quality due to lack of standardized approaches
- There is a gap between low-effort (spreadsheets) and high-effort (custom code) exploration patterns with no intermediate solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Practitioners rely on intuition and custom code because there is no consensus on what constitutes "high quality" data.
- Mechanism: Without a shared framework for evaluating data quality, practitioners default to familiar tools (spreadsheets) and methods (custom code) that they can control and adapt to their specific context.
- Core assumption: Data quality evaluation in LLM development is inherently subjective and context-dependent, preventing standardization.
- Evidence anchors:
  - [abstract]: "there is little consensus around what data quality is and how to evaluate it"
  - [section]: "practitioners either rely on their own intuition or write custom evaluation logic"
  - [corpus]: Weak - corpus shows related papers on data quality but doesn't directly support this mechanism
- Break condition: Emergence of a widely-adopted framework for data quality evaluation in LLM contexts.

### Mechanism 2
- Claim: The lack of intermediate-effort exploration tools creates a gap between low-effort (spreadsheets) and high-effort (custom code) approaches.
- Mechanism: Practitioners must choose between quick visual inspection or time-consuming custom analysis, with no middle ground that balances efficiency and rigor.
- Core assumption: Intermediate-effort tools would reduce the burden of custom code while providing more rigor than spreadsheet inspection.
- Evidence anchors:
  - [abstract]: "There is a lack of consensus across practitioners on what quality is and how to evaluate it"
  - [section]: "There is missing alignment on intermediate-effort exploration patterns"
  - [corpus]: Weak - corpus doesn't provide direct evidence for this specific gap
- Break condition: Development and adoption of widely-used intermediate-effort exploration tools.

### Mechanism 3
- Claim: The field is still emerging, so practitioners stick to familiar tools while waiting for better solutions.
- Mechanism: In the absence of established best practices, practitioners prefer tools they know (spreadsheets) over learning new ones, even when those tools are inadequate.
- Core assumption: The LLM development field is too new for practitioners to have converged on standardized tools and practices.
- Evidence anchors:
  - [abstract]: "we define the role of 'dataset practitioner' by performing a retrospective analysis"
  - [section]: "the world is new" (toolmakers' hypothesis)
  - [corpus]: Weak - corpus shows related work but doesn't strongly support this mechanism
- Break condition: Field maturation and establishment of widely-accepted best practices.

## Foundational Learning

- Concept: Data quality evaluation frameworks
  - Why needed here: The paper identifies lack of consensus on data quality as a key problem
  - Quick check question: What are three common approaches to evaluating data quality in machine learning contexts?

- Concept: LLM development lifecycle
  - Why needed here: Understanding practitioner roles requires knowledge of the broader development context
  - Quick check question: What are the main stages in developing and deploying a large language model?

- Concept: Qualitative research methods
  - Why needed here: The study uses semi-structured interviews and thematic analysis
  - Quick check question: What are the key differences between semi-structured interviews and structured surveys?

## Architecture Onboarding

- Component map: Data practitioner workflow → Data quality evaluation → Tool selection → Custom analysis → Model development
- Critical path: Data exploration → Quality assessment → Decision to use data → Model training → Evaluation
- Design tradeoffs: Customizability vs standardization, speed vs rigor, individual vs collaborative workflows
- Failure signatures: Inconsistent data quality standards, inefficient exploration patterns, reliance on intuition over systematic evaluation
- First 3 experiments:
  1. Conduct interviews with practitioners to identify specific pain points in current workflows
  2. Test intermediate-effort tools with a small group of practitioners and gather feedback
  3. Develop a framework for data quality evaluation and validate it across different LLM development contexts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific frameworks, consensus, and tooling would help dataset practitioners evaluate data quality consistently across different LLM development contexts?
- Basis in paper: [explicit] The paper identifies a lack of consensus around what constitutes "high quality" data and how to evaluate it, with practitioners relying on intuition or custom code instead of adopting existing solutions.
- Why unresolved: Despite active efforts by HCI/VIS researchers to develop relevant sensemaking methods and tools, practitioners are not adopting these solutions. The paper discusses this as an opportunity for alignment but doesn't provide specific solutions.
- What evidence would resolve it: Comparative studies testing different frameworks/tools with practitioners across various LLM development contexts, measuring adoption rates, consistency of quality evaluation, and user satisfaction.

### Open Question 2
- Question: How can the tension between custom requirements for specific use cases and the need for standardized intermediate-effort exploration tools be resolved?
- Basis in paper: [explicit] Practitioners cite custom needs as a reason for not adopting existing tools, yet when asked about their specific requirements, they listed many similar needs including summarizing dataset features, ensuring safety, and evaluating numeric distributions.
- Why unresolved: The paper presents conflicting evidence - practitioners say tools are too specific while also having overlapping needs, suggesting the tension is not fully understood.
- What evidence would resolve it: Systematic analysis of practitioner workflows to identify common patterns versus truly unique requirements, followed by development and testing of flexible tools that balance standardization with customization.

### Open Question 3
- Question: What factors contribute to the persistent use of spreadsheets for low-effort data exploration despite awareness of their limitations and cognitive biases?
- Basis in paper: [explicit] All participants mentioned using spreadsheets for visual inspection despite acknowledging confirmation biases, and cited efficiency, customization, learning curve, and ease of sharing as reasons.
- Why unresolved: The paper identifies this as a pattern but doesn't explain why these factors outweigh the known limitations and why practitioners continue this behavior despite awareness of biases.
- What evidence would resolve it: Longitudinal studies tracking practitioner tool choice decisions, cognitive experiments measuring the impact of spreadsheet-based exploration on bias, and comparative usability studies of alternative tools.

## Limitations

- Study based on small sample (N=10) from single organization (Google), limiting generalizability
- Retrospective analysis methodology not fully specified, making assessment of potential selection biases difficult
- Self-selection of practitioners into study may bias results toward those more engaged with data quality concerns

## Confidence

- **High**: Practitioners face challenges with data quality evaluation and lack standardized approaches (supported by direct interview quotes and observed behaviors)
- **Medium**: The three hypothesized mechanisms explaining these challenges are plausible but not definitively proven (based on researcher interpretation rather than direct evidence)
- **Low**: The claim that intermediate-effort exploration tools are specifically missing from the ecosystem (inferred from practitioners' stated preferences rather than explicit tool evaluations)

## Next Checks

1. Replicate with diverse samples: Conduct the same study with practitioners from multiple organizations and different sizes of LLM development teams to test generalizability of the findings.

2. Validate mechanism hypotheses: Design experiments to test whether the proposed mechanisms (lack of consensus, missing intermediate tools, field immaturity) actually drive the observed behaviors, rather than being merely correlated.

3. Prototype and evaluate intermediate tools: Develop and test specific intermediate-effort exploration tools with practitioners to determine if they address the identified gaps between spreadsheet inspection and custom code analysis.