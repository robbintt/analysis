---
ver: rpa2
title: Merging Multi-Task Models via Weight-Ensembling Mixture of Experts
arxiv_id: '2402.00433'
source_url: https://arxiv.org/abs/2402.00433
tags:
- task
- tasks
- merging
- layer
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of merging multiple task-specific
  Transformer-based models into a single unified model that can handle all tasks concurrently.
  The key issue is mitigating interference between parameters of different models,
  which can substantially deteriorate performance.
---

# Merging Multi-Task Models via Weight-Ensembling Mixture of Experts

## Quick Facts
- arXiv ID: 2402.00433
- Source URL: https://arxiv.org/abs/2402.00433
- Reference count: 40
- Primary result: Achieves 89.4% average accuracy on eight image classification tasks, outperforming traditional multi-task learning baseline by 0.5%

## Executive Summary
This paper addresses the challenge of merging multiple task-specific Transformer-based models into a single unified model capable of handling all tasks concurrently. The key innovation is a weight-ensembling Mixture of Experts (MoE) approach that dynamically integrates shared and task-specific knowledge based on input data. By upscaling the MLP layers of Transformers into MoE modules with gating mechanisms, the method mitigates interference between parameters of different models while maintaining high performance across all tasks.

## Method Summary
The core approach involves transforming the MLP modules of pre-trained Transformer models into weight-ensembling MoE modules. Each MoE module contains multiple experts (task vectors) and a gating network that computes weights for these experts based on the input. During merging, the method first learns a shared representation from all pre-trained models, then identifies task-specific knowledge for each individual task. The MoE router dynamically combines these components based on input data, allowing the unified model to adapt its behavior to specific task requirements. The method requires only 1.25% additional parameters compared to a single pre-trained model and demonstrates rapid convergence within 200 training steps.

## Key Results
- Achieves 89.4% average accuracy on eight image classification tasks using CLIP-ViT-B/32
- Outperforms traditional multi-task learning baseline by 0.5% accuracy
- Requires only 1.25% additional parameters compared to a single pre-trained model
- Converges rapidly, reaching high accuracy in just 200 training steps
- Demonstrates stability across variations in scaling coefficient of task vectors
- Shows strong generalization and robustness to out-of-distribution data

## Why This Works (Mechanism)
The method works by separating shared knowledge from task-specific knowledge and dynamically combining them through an MoE architecture. The gating network learns to assign appropriate weights to different experts based on input characteristics, allowing the model to selectively activate task-relevant components while suppressing interference from unrelated tasks. This selective activation prevents negative transfer between dissimilar tasks while preserving the ability to leverage shared representations where beneficial.

## Foundational Learning
**Mixture of Experts (MoE)**: A neural network architecture that combines multiple specialized "expert" networks with a gating mechanism to dynamically route inputs to appropriate experts. Why needed: Enables selective activation of task-specific components while maintaining a shared backbone. Quick check: Verify that the gating network produces valid probability distributions across experts.

**Weight Ensembling**: Technique for combining multiple model parameters or representations into a single unified model. Why needed: Allows merging of pre-trained models without catastrophic forgetting. Quick check: Confirm that ensembled weights maintain stability during fine-tuning.

**Transformer MLP Layers**: Multi-layer perceptron modules within Transformer architectures that process token representations. Why needed: Primary target for upscaling to MoE modules in this method. Quick check: Ensure upscaled MLPs preserve original functionality before adding MoE complexity.

**Task Vector**: Learnable parameters representing specific task knowledge within the MoE framework. Why needed: Encodes task-specific information that can be dynamically combined with shared knowledge. Quick check: Verify task vectors capture meaningful task distinctions through visualization or analysis.

**Gating Network**: Component that computes weights for expert selection based on input features. Why needed: Controls dynamic routing and prevents interference between incompatible tasks. Quick check: Monitor gating distribution to ensure it's not collapsing to uniform or degenerate values.

## Architecture Onboarding

**Component Map**: Input → Shared Backbone → MLPs → MoE Routers → Task Vectors → Output

**Critical Path**: Input → Shared Backbone → MoE Router → Weighted Sum of Task Vectors → Output

**Design Tradeoffs**: The method balances parameter efficiency (1.25% overhead) against performance gains, choosing to upscale only MLP layers rather than entire model components. This selective upscaling preserves computational efficiency while providing sufficient capacity for task-specific adaptation.

**Failure Signatures**: Poor gating network training leading to uniform expert weights, task vector collapse preventing effective task differentiation, or interference between highly dissimilar tasks overwhelming the shared backbone.

**First 3 Experiments**:
1. Merge two CLIP models trained on different subsets of ImageNet classes to verify basic functionality
2. Test convergence speed by monitoring accuracy over training steps with varying learning rates
3. Evaluate robustness by testing on out-of-distribution data from unseen task categories

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the WEMoE method's performance compare when applied to merging models with different architectures (e.g., CNNs) or from different modalities (e.g., vision and language)?
- Basis in paper: The authors mention future plans to explore the potential of their method in other scenarios, such as merging transformers from different modalities and applying the method to other architectures like CNNs.
- Why unresolved: The current study only focuses on merging Transformer-based vision models, leaving the generalizability of the method to other architectures and modalities untested.
- What evidence would resolve it: Conducting experiments that apply WEMoE to merge models with different architectures or from different modalities, and comparing the performance to the current results.

### Open Question 2
- Question: What is the impact of the number of experts (T) in the WEMoE module on the model's performance and computational efficiency?
- Basis in paper: The paper discusses the upscaling of MLP modules to MoE modules with a router that assigns weights to task vectors. However, the effect of varying the number of experts (T) is not explored.
- Why unresolved: The paper does not provide a detailed analysis of how the number of experts affects the model's performance and efficiency.
- What evidence would resolve it: Conducting experiments with different numbers of experts (T) in the WEMoE module and analyzing the trade-off between performance and computational efficiency.

### Open Question 3
- Question: How does the WEMoE method handle scenarios where the tasks are highly related and positively impact each other, as opposed to the current focus on situations where negative transfer is a prevalent concern?
- Basis in paper: The authors mention that while their method is effective in handling scenarios where negative transfer is a concern, there can be instances where two tasks are sufficiently related to positively impact each other.
- Why unresolved: The paper does not provide an analysis of the method's performance in scenarios where tasks are highly related and positively impact each other.
- What evidence would resolve it: Conducting experiments on a set of highly related tasks and comparing the performance of WEMoE to traditional multi-task learning approaches.

## Limitations
- Method relies primarily on empirical validation without theoretical guarantees for why the MoE-based approach mitigates interference more effectively than alternatives
- Evaluation is limited to image classification tasks using CLIP-ViT-B/32, restricting generalizability to other domains or model architectures
- Does not explore the impact of varying the number of experts on performance and computational efficiency

## Confidence
- **High Confidence**: The empirical results showing 89.4% average accuracy and outperforming baselines by 0.5% on the tested image classification tasks
- **Medium Confidence**: The claims about rapid convergence (200 steps) and stability across scaling coefficient variations, as these were validated under specific experimental conditions
- **Medium Confidence**: The generalization and robustness claims to out-of-distribution data, as the paper does not provide extensive OOD testing or detailed analysis of failure modes

## Next Checks
1. Test the method on non-vision tasks (e.g., NLP, multimodal) to assess cross-domain applicability and identify potential architectural limitations
2. Conduct ablation studies varying the number of experts and the gating mechanism to quantify their impact on performance and parameter efficiency
3. Evaluate the method's behavior when merging models with significant architectural differences or trained on highly divergent data distributions to test robustness limits