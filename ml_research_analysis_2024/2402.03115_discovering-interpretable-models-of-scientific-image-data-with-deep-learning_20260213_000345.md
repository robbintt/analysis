---
ver: rpa2
title: Discovering interpretable models of scientific image data with deep learning
arxiv_id: '2402.03115'
source_url: https://arxiv.org/abs/2402.03115
tags:
- cell
- scheme
- data
- learning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study demonstrates methods for discovering interpretable models\
  \ from complex image data using deep learning. The approach combines disentangled\
  \ representation learning (via \u03B2-TCVAE), sparse neural network training (using\
  \ RigL), and symbolic regression to produce models that are both highly accurate\
  \ and interpretable."
---

# Discovering interpretable models of scientific image data with deep learning

## Quick Facts
- arXiv ID: 2402.03115
- Source URL: https://arxiv.org/abs/2402.03115
- Reference count: 40
- Key outcome: Achieved ~98% accuracy on cell state classification using only 2% of parameters and 0.2% of expression size while producing interpretable mathematical models

## Executive Summary
This paper presents a method for discovering interpretable models from complex scientific image data using deep learning. The approach combines disentangled representation learning via β-TCVAE, sparse neural network training using RigL, and symbolic regression to produce models that are both highly accurate and interpretable. Applied to classifying cell states in microscopy images, the method achieves performance comparable to black-box models while revealing clear scientific explanations about what distinguishes different cell states.

## Method Summary
The method extracts interpretable features from microscopy images using β-TCVAE to learn a disentangled latent space encoding cell morphology, position, and neighborhood information. A sparse neural network trained with RigL identifies the minimal set of relevant features (specifically, latent variables encoding central cell morphology). Symbolic regression using PySR then discovers mathematical expressions that approximate the classification logic, yielding models that are both accurate and interpretable. The complete pipeline is tested on classifying interphase versus metaphase cells from fluorescence microscopy images.

## Key Results
- Achieved ~98% accuracy on test set, comparable to black-box models
- Sparse networks used only 2% of parameters while maintaining performance
- Symbolic expressions achieved 0.2% of the expression size of dense models
- Models correctly identified only cell morphology features as relevant for classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disentangled representation learning via β-TCVAE creates interpretable latent features that map to semantic concepts (cell morphology, position, neighborhood).
- Mechanism: The β-TCVAE loss function explicitly penalizes total correlation between latent dimensions, forcing each latent variable to encode independent factors of variation in the image data.
- Core assumption: The raw microscopy images contain separable factors of variation (cell size, eccentricity, position, neighborhood) that can be mathematically isolated.
- Evidence anchors:
  - [abstract] "The approach combines disentangled representation learning (via β-TCVAE)"
  - [section] "The β-TCVAE is a variant of the VAE that has been designed specifically to produce disentangled latent spaces, where separate latent variables encode separate concepts."
  - [corpus] Weak - no direct evidence found in neighbor papers about β-TCVAE specifically
- Break condition: If the input images contain highly correlated factors that cannot be mathematically separated, or if the image complexity exceeds the model's capacity to disentangle factors.

### Mechanism 2
- Claim: Sparse neural network training using RigL identifies minimal feature sets that prior knowledge would deem relevant.
- Mechanism: Dynamic pruning removes unnecessary connections while re-growth adds connections based on loss gradients, converging on sparse topologies that retain only the most informative pathways.
- Core assumption: The minimum error model for this classification task only requires features encoding central cell morphology (z3, z17, z21, z29).
- Evidence anchors:
  - [abstract] "sparse neural network training (using RigL)"
  - [section] "Strikingly, we find that the model has learnt that the minimal set of required input features corresponds exactly to the latent variables that encode central cell morphology"
  - [corpus] Weak - no direct evidence about RigL found in neighbor papers
- Break condition: If the task requires more complex feature interactions than can be captured by sparse connections, or if pruning removes critical but non-obvious features.

### Mechanism 3
- Claim: Symbolic regression on sparse network outputs produces interpretable mathematical expressions that capture the same classification logic.
- Mechanism: Genetic algorithms search expression trees to minimize loss while penalizing complexity, yielding parsimonious mathematical models that approximate the neural network's behavior.
- Core assumption: The classification boundary can be expressed as a mathematical function of the four relevant latent variables with reasonable accuracy.
- Evidence anchors:
  - [abstract] "symbolic regression to produce models that are both highly accurate and interpretable"
  - [section] "Symbolic regression is a method to identify analytic expressions that approximate the output of an arbitrary function or dataset"
  - [corpus] Weak - neighbor papers discuss symbolic regression but not in the context of neural network outputs
- Break condition: If the underlying function is too complex to be captured by simple mathematical expressions, or if the input space dimensionality is too high for genetic algorithms to search effectively.

## Foundational Learning

- Concept: Variational Autoencoders and their variants
  - Why needed here: Understanding how β-TCVAE differs from standard VAE and why it produces disentangled representations
  - Quick check question: What is the key difference between VAE and β-TCVAE loss functions that enables disentanglement?

- Concept: Dynamic sparse training algorithms
  - Why needed here: Understanding how RigL's pruning and re-growth mechanism identifies optimal sparse topologies
  - Quick check question: How does RigL determine which connections to prune versus re-grow during training?

- Concept: Genetic algorithms for symbolic regression
  - Why needed here: Understanding how PySR searches expression trees and balances accuracy with complexity
  - Quick check question: What role does the parsimony parameter play in symbolic regression, and how does it affect the search process?

## Architecture Onboarding

- Component map: Raw images → β-TCVAE encoder → 32D latent space → Classification (dense/sparse/symbolic) → Output score → Decision boundary
- Critical path: Raw images → β-TCVAE encoding → Classification (dense/sparse/symbolic) → Output score → Decision boundary
- Design tradeoffs:
  - Interpretability vs. accuracy: Scheme 3-4 models sacrifice ~2-3% accuracy for much simpler, interpretable models
  - Model complexity vs. search efficiency: Sparsity reduces parameter count but requires careful hyper-parameter tuning
  - Expression complexity vs. accuracy: Symbolic regression must balance simple expressions against classification performance
- Failure signatures:
  - Poor β-TCVAE reconstructions indicate inadequate disentanglement
  - Sparse models with unexpected feature selection suggest pruning issues
  - Symbolic expressions with poor accuracy or unnecessary complexity indicate regression problems
- First 3 experiments:
  1. Train β-TCVAE on the microscopy dataset and visualize latent traversals to verify disentanglement
  2. Apply RigL to a small dense classifier and compare feature selection with prior knowledge
  3. Run PySR on sparse network outputs and analyze expression forms for interpretability and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do the proposed interpretable models generalize to datasets with significantly more diverse factors of variation, such as varying cell types, imaging conditions, or experimental setups?
- Basis in paper: [inferred] The paper notes that the β-TCVAE was able to extract disentangled features from a relatively consistent dataset with a cell in the center surrounded by neighbors. It suggests that generalizing to more complex datasets may require further development in representation learning.
- Why unresolved: The study only tested the approach on one specific dataset with consistent imaging conditions. No experiments were conducted on datasets with different cell types, imaging modalities, or experimental protocols.
- What evidence would resolve it: Testing the models on multiple benchmark bioimaging datasets with varying cell types (e.g., different tissues, species), imaging conditions (e.g., different microscopes, stains), and experimental protocols (e.g., different cell culture conditions, treatments) would provide evidence of generalizability.

### Open Question 2
- Question: How can we determine whether the discovered interpretable models are capturing true biological phenomena or simply overfitting to the specific training data?
- Basis in paper: [inferred] The paper mentions that in the absence of prior knowledge, it may be difficult to assess whether the latent representation has learned the right features. It also notes that complex behaviors in the models may reflect either legitimate patterns or accidental biases introduced by the model architecture.
- Why unresolved: The study did not include any biological validation of the discovered models. The interpretability of the models was assessed through inspection of their structure and behavior, but not through comparison with known biological mechanisms or experimental validation.
- What evidence would resolve it: Comparing the predictions of the interpretable models with known biological mechanisms, or validating the models through targeted experiments (e.g., perturbing specific cellular features and observing the model's response), would provide evidence of whether the models are capturing true biological phenomena.

### Open Question 3
- Question: How can the proposed interpretable models be extended to handle multi-class classification problems beyond binary classification, such as classifying cells into multiple stages of the cell cycle?
- Basis in paper: [explicit] The paper mentions that symbolic regression is readily applicable to binary classification but may not be valid for multi-class classification. It suggests one potential approach but acknowledges it would be far more complex than the binary case.
- Why unresolved: The study only demonstrated the approach on a binary classification problem (interphase vs. metaphase). No experiments or theoretical analysis were conducted on multi-class classification problems.
- What evidence would resolve it: Testing the models on multi-class classification problems in bioimaging, such as classifying cells into multiple stages of the cell cycle or different cell types, and comparing the performance and interpretability with the binary case would provide evidence of the approach's effectiveness.

## Limitations

- The approach requires careful hyper-parameter tuning for each component (β-TCVAE, RigL, PySR)
- Generalization to datasets with more diverse factors of variation remains untested
- Symbolic regression may struggle with complex multi-class classification problems

## Confidence

- High confidence: The overall methodology framework and its three core components (disentangled learning, sparse training, symbolic regression)
- Medium confidence: The specific claim about identifying "only relevant features" (z3, z17, z21, z29) as these depend on dataset-specific characteristics
- Medium confidence: The robustness to adversarial attacks claim, as this requires external validation

## Next Checks

1. Apply the complete pipeline to a different scientific imaging domain (e.g., protein localization or tissue classification) to test generalizability
2. Perform ablation studies removing each component (β-TCVAE, RigL, PySR) to quantify individual contributions
3. Test model robustness using established adversarial attack benchmarks specific to sparse neural networks