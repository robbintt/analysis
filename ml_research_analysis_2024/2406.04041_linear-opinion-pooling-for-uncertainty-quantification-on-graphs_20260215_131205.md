---
ver: rpa2
title: Linear Opinion Pooling for Uncertainty Quantification on Graphs
arxiv_id: '2406.04041'
source_url: https://arxiv.org/abs/2406.04041
tags:
- uncertainty
- node
- lop-gpn
- information
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses uncertainty quantification (UQ) in semi-supervised
  node classification on graph-structured data, focusing on distinguishing between
  aleatoric and epistemic uncertainty. The authors propose Linear Opinion Pooled Graph
  Posterior Networks (LOP-GPN), which leverage the principle of linear opinion pooling
  to aggregate predictions from neighboring nodes using mixtures of Dirichlet distributions.
---

# Linear Opinion Pooling for Uncertainty Quantification on Graphs

## Quick Facts
- arXiv ID: 2406.04041
- Source URL: https://arxiv.org/abs/2406.04041
- Reference count: 30
- One-line primary result: LOP-GPN achieves strong accuracy and meaningful uncertainty estimates, outperforming GPN in accuracy-rejection curves and OOD detection.

## Executive Summary
This paper addresses uncertainty quantification in semi-supervised node classification on graphs, proposing Linear Opinion Pooled Graph Posterior Networks (LOP-GPN). The method leverages linear opinion pooling to aggregate predictions from neighboring nodes using mixtures of Dirichlet distributions, avoiding the irreducible conflicts assumption present in previous Graph Posterior Networks. Experiments on six graph datasets demonstrate that LOP-GPN achieves strong classification accuracy and meaningful uncertainty estimates, particularly excelling in accuracy-rejection curves and out-of-distribution detection tasks.

## Method Summary
LOP-GPN uses linear opinion pooling with mixtures of Dirichlet distributions to aggregate neighbor predictions, avoiding the irreducible conflicts assumption of GPN. The method propagates uncertainty through sparse personalized PageRank, keeping mixture weights concentrated on close neighbors for scalability. Training uses a second-order loss with entropy regularization bounds, enabling effective separation of aleatoric and epistemic uncertainty.

## Key Results
- LOP-GPN achieves higher test accuracy than GPN in noisy settings
- Strong performance in accuracy-rejection curves and out-of-distribution detection tasks
- Outperforms or matches GPN across six graph datasets
- Particularly effective at detecting nodes with missing or corrupted features

## Why This Works (Mechanism)

### Mechanism 1
LOP-GPN avoids the irreducibility of conflicts assumption by using linear opinion pooling to aggregate Dirichlet mixtures instead of averaging pseudo-counts. In GPN, conflicting neighbor predictions lead to increased aleatoric uncertainty because conflicts are assumed irreducible. LOP-GPN instead treats each node's prediction as a mixture of neighbor Dirichlet distributions weighted by personalized PageRank. Since the aleatoric uncertainty of a mixture is the weighted sum of component uncertainties, aleatoric uncertainty remains unchanged by conflicts while epistemic uncertainty grows to reflect unresolved disagreement. This mechanism assumes network homophily holds and neighborhoods grow unboundedly, making most conflicts reducible rather than irreducible.

### Mechanism 2
Using a mixture of Dirichlet distributions allows more expressive uncertainty modeling than a single Dirichlet, enabling better separation of aleatoric and epistemic uncertainty. Each node's second-order distribution is represented as a weighted sum of neighbor Dirichlet densities, capturing multimodal beliefs without forcing a single consensus distribution. The entropy of the mixture can be bounded using component entropies, enabling tractable training with a differentiable upper bound on the loss. This approach assumes Dirichlet mixtures can approximate the true posterior over label distributions when aggregated over neighbors.

### Mechanism 3
Sparse personalized PageRank propagation limits computational cost while preserving locality of information flow. The L-th power of the teleport-adjusted adjacency matrix is approximated by sparsifying after each multiplication, keeping only entries above a threshold δ. This yields a sparse mixture weight matrix such that mixture coefficients for a node are concentrated on its close neighbors, enabling scalable training on large graphs. The mechanism assumes most useful information comes from nearby neighbors, with distant neighbors contributing negligible mixture weight after sparsification.

## Foundational Learning

- **Dirichlet distribution as conjugate prior**: Essential for understanding how Dirichlet parameters encode beliefs over class probabilities. Quick check: Given a Dirichlet(α₁, α₂, α₃), what is the expected class probability for class 1?

- **Linear opinion pooling properties**: Critical for understanding how mixture aleatoric uncertainty remains the weighted sum of component uncertainties. Quick check: If two Dirichlet distributions have aleatoric uncertainty values 0.2 and 0.3, what is the aleatoric uncertainty of their 0.5-0.5 mixture?

- **Personalized PageRank mechanics**: Important for understanding how teleport probability ε and power L control locality of information flow. Quick check: What effect does increasing ε have on the spread of probability mass in personalized PageRank?

## Architecture Onboarding

- **Component map**: Encoder -> Class-conditional density estimators -> Dirichlet posterior constructor -> Graph propagator -> Loss engine
- **Critical path**: 1) Forward pass through encoder and normalizing flows to get αft, 2) Compute sparse ΠPPR via iterative matrix multiplications with sparsification, 3) Form mixture distribution Qagg = Σ ΠPPR(i,j) Dir(αft,j), 4) Compute upper bound on loss (Eq. 13 with entropy bound), 5) Backpropagate gradients to update encoder and flows
- **Design tradeoffs**: Sparse vs dense PageRank (sparse speeds up training but may lose long-range signals), number of power iterations L (more iterations increase neighborhood reach but cost O(L|E|K)), teleport probability ε (higher ε keeps mixture closer to local predictions; lower ε allows broader influence)
- **Failure signatures**: If L is too small, predictions may ignore useful distant neighbors; if sparsification threshold δ is too high, some nodes may have very few mixture components, reducing uncertainty expressiveness; if ε is too high, the model behaves like a purely feature-based classifier, losing graph signal
- **First 3 experiments**: 1) Train on CoraML with L=5, ε=0.1, δ=1e-5; compare accuracy-rejection curves for total uncertainty, aleatoric uncertainty, and epistemic uncertainty against GPN, 2) Vary L from 3 to 10 on a small graph; measure aleatoric vs epistemic uncertainty separation quality, 3) Test sparsification sensitivity by sweeping δ; measure both runtime and out-of-distribution detection AUC

## Open Questions the Paper Calls Out

- How does the Linear Opinion Pool approach compare to other uncertainty quantification methods (e.g., variational inference, Monte Carlo dropout) in terms of accuracy and computational efficiency? The paper focuses on comparing LOP-GPN with GPN and other baseline models but does not explore a broader range of uncertainty quantification methods.

- How does the choice of teleport probability (ε) in the APPNP aggregation step affect the performance of LOP-GPN? The paper mentions the teleport probability (ε) but does not explore its impact on the performance of LOP-GPN or investigate the sensitivity of LOP-GPN to this important hyperparameter.

- Can the LOP-GPN approach be extended to other graph-related tasks, such as graph classification or link prediction, and how would it perform in those tasks? The paper focuses on node classification and does not discuss the potential application of LOP-GPN to other graph-related tasks or explore its generalizability.

## Limitations

- The claim that conflicts are mostly reducible in practice is plausible but not directly tested; no analysis of conflict types in the datasets is provided.
- The sparse APPNP approximation is not fully specified, potentially hindering exact reproduction.
- No ablation study quantifies the relative contribution of LOP vs. mixture modeling vs. sparse propagation.

## Confidence

- **High**: LOP-GPN achieves competitive accuracy and meaningful uncertainty estimates in experiments.
- **Medium**: The theoretical advantage over GPN (avoiding irreducible conflicts) is well-motivated but unverified in practice.
- **Low**: The generality of the approach to non-homophilic graphs or small neighborhoods is unclear.

## Next Checks

1. Analyze a subset of datasets to quantify the proportion of irreducible vs. reducible conflicts in practice.
2. Perform an ablation study isolating the impact of LOP, Dirichlet mixtures, and sparse propagation on aleatoric/epistemic uncertainty separation.
3. Test LOP-GPN on a non-homophilic graph (e.g., adversarial edges) to assess robustness when the core assumption fails.