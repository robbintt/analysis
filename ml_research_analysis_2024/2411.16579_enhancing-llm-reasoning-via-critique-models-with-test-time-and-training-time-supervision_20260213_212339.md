---
ver: rpa2
title: Enhancing LLM Reasoning via Critique Models with Test-Time and Training-Time
  Supervision
arxiv_id: '2411.16579'
source_url: https://arxiv.org/abs/2411.16579
tags:
- critique
- reasoning
- performance
- actor
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enhancing large language
  models' (LLMs) reasoning capabilities, particularly on complex tasks in domains
  like mathematics, science, and coding. The core method introduces a two-player paradigm
  separating reasoning (actor) and critique models, where the critique model provides
  step-level feedback to supervise the actor during both test-time and training-time.
---

# Enhancing LLM Reasoning via Critique Models with Test-Time and Training-Time Supervision

## Quick Facts
- arXiv ID: 2411.16579
- Source URL: https://arxiv.org/abs/2411.16579
- Reference count: 40
- Primary result: Critique models improve LLM reasoning performance on difficult mathematical tasks through step-level supervision during both test-time and training-time

## Executive Summary
This paper introduces a two-player paradigm that separates reasoning (actor) and critique models to enhance large language models' reasoning capabilities, particularly on complex mathematical tasks. The core innovation is the AutoMathCritique framework, which automatically generates 76,321 high-quality critique samples without human supervision. The approach demonstrates that critique models can provide step-level feedback that significantly improves the actor's performance on difficult queries, especially when scaling inference-time computation. Additionally, incorporating critique supervision into the actor's self-training process leads to better exploration efficiency, solution diversity, and stronger reasoning models.

## Method Summary
The method employs a two-player paradigm where an actor model performs reasoning tasks while a critique model evaluates and provides step-level feedback. The AutoMathCritique framework automatically generates flawed reasoning paths and corresponding critiques using GPT-4/GPT-4o. The critique-in-the-loop self-improvement method incorporates this feedback into the actor's training process, improving exploration efficiency and alleviating the tail-narrowing problem. During test-time, critique supervision is integrated with majority voting to enhance performance on difficult queries. The approach is validated on GSM8K and MATH datasets using various model sizes including Llama3-8B and Qwen-2.5 series models.

## Key Results
- Critique models significantly improve actor performance on difficult mathematical queries during test-time
- Scaling inference-time computation with critique supervision leads to consistent performance gains
- Incorporating critique supervision into training improves exploration efficiency and solution diversity
- AutoMathCritique framework successfully generates 76,321 high-quality critique samples without human supervision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating reasoning (actor) and critique roles improves LLM performance on difficult tasks.
- Mechanism: The critique model provides step-level feedback to the actor during both test-time and training-time, allowing for targeted error identification and correction.
- Core assumption: The critique model can accurately assess reasoning steps and provide useful feedback.
- Evidence anchors:
  - [abstract]: "We delve into a two-player paradigm that separates the roles of reasoning and critique models, where the critique model provides step-level feedback to supervise the reasoning (actor) model during both test-time and training-time."
  - [section]: "In this paper, we delve into a two-player paradigm that separates the roles of reasoning and critique models, where the critique model provides step-level feedback to supervise the reasoning (actor) model during both test-time and train-time."
- Break condition: If the critique model cannot accurately assess reasoning steps, the supervision becomes ineffective and may introduce noise.

### Mechanism 2
- Claim: Scaling inference-time computation with critique supervision consistently improves reasoning performance.
- Mechanism: As more samples are generated and critiqued, the majority voting accuracy increases significantly compared to baseline approaches without critique.
- Core assumption: The critique model can effectively filter and refine responses to improve majority voting outcomes.
- Evidence anchors:
  - [abstract]: "We demonstrate that the critique models consistently improve the actor's performance on difficult queries at test-time, especially when scaling up inference-time computation."
  - [section]: "We find that the critique models are particularly effective in helping the actor achieve better results on difficult queries. Additionally, by scaling inference-time computation, the performance gains brought by the critique models continue to grow."
- Break condition: If the critique model introduces significant latency or fails to provide consistent improvements, the scaling benefits diminish.

### Mechanism 3
- Claim: Incorporating critique supervision into training improves exploration efficiency and solution diversity.
- Mechanism: The critique-in-the-loop self-improvement method uses critique feedback to guide exploration, allocating more computation to difficult problems and reducing the tail-narrowing problem.
- Core assumption: Critique feedback can effectively guide the actor toward diverse and correct solutions during exploration.
- Evidence anchors:
  - [abstract]: "Motivated by these findings, we introduce the critique-based supervision to the actor's self-training process, and propose a critique-in-the-loop self-improvement method."
  - [section]: "With the supervision of critique models and by scaling exploration computation for difficult queries, our method improves the actor's exploration efficiency and solution diversity, alleviating the issue of tail narrowing in reasoning models during iterative exploration and learning."
- Break condition: If the critique model provides misleading feedback or the exploration becomes too biased, the training process may converge to suboptimal solutions.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: The actor model relies on step-by-step reasoning similar to CoT, and the critique model evaluates these reasoning steps.
  - Quick check question: How does step-level feedback differ from end-to-end evaluation in terms of improving reasoning quality?

- Concept: Majority voting for ensemble methods
  - Why needed here: The paper uses majority voting to scale inference-time computation and evaluate the effectiveness of critique supervision.
  - Quick check question: Why does majority voting performance plateau without critique supervision, and how does critique address this limitation?

- Concept: Self-improvement through iterative refinement
  - Why needed here: The critique-in-the-loop method builds on self-improvement techniques but uses critique feedback to guide the exploration process.
  - Quick check question: What is the tail-narrowing problem in self-improvement, and how does critique supervision help mitigate it?

## Architecture Onboarding

- Component map:
  Actor model -> Generate reasoning responses -> Critique model -> Provide step-level feedback -> Refined responses -> Self-improvement pipeline

- Critical path:
  1. Generate reasoning responses from actor model
  2. Apply critique model to evaluate and provide feedback
  3. Refine responses based on critique feedback
  4. (Training) Use refined responses for self-improvement
  5. (Test-time) Use majority voting with critique supervision

- Design tradeoffs:
  - Model size vs. inference efficiency: Larger models may provide better critique but increase latency
  - Data quality vs. quantity: More diverse flawed responses may improve critique model generalization
  - Sequential vs. parallel scaling: Parallel generation offers more diversity, sequential refinement may be more focused

- Failure signatures:
  - Poor critique quality leading to incorrect refinements
  - Tail narrowing in self-improvement causing performance plateau
  - Majority voting performance not improving with scaled computation
  - Self-talk data construction producing incoherent reasoning chains

- First 3 experiments:
  1. Evaluate critique model accuracy on a held-out validation set to ensure quality feedback
  2. Test majority voting performance with and without critique supervision on a small sample
  3. Implement a basic self-talk data construction pipeline and verify step-level coherence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can critique models effectively supervise larger reasoning models beyond their own scale?
- Basis in paper: [explicit] The authors conduct scaling experiments using Qwen-2.5 series models, finding that a 3B critique model can provide effective supervision to actors of various sizes.
- Why unresolved: While the paper demonstrates supervision effectiveness for models up to 14B parameters, it remains unclear whether this relationship holds for significantly larger models (e.g., 70B+).
- What evidence would resolve it: Experiments with critique models supervising reasoning models at least 10× their size would clarify the scaling limits.

### Open Question 2
- Question: What is the optimal strategy for allocating test-time computation between parallel and sequential scaling approaches?
- Basis in paper: [explicit] The authors compare parallel majority voting with sequential approaches, finding that sequential majority voting eventually outperforms parallel methods for challenging problems.
- Why unresolved: The paper identifies a trade-off but doesn't determine the exact conditions (computation budget, problem difficulty) that favor each approach.
- What evidence would resolve it: A systematic study varying problem difficulty, computation budgets, and scaling strategies would identify optimal allocation policies.

### Open Question 3
- Question: How can we develop more advanced test-time scaling techniques that reduce hallucinations in long thinking chains?
- Basis in paper: [inferred] The authors mention the need for improved test-time scaling techniques to address hallucinations, particularly when using critique models for sequential refinement.
- Why unresolved: While the paper demonstrates that critique models can reduce hallucinations by identifying and suppressing incorrect answers, it doesn't explore more sophisticated methods for maintaining chain coherence.
- What evidence would resolve it: Development and evaluation of novel test-time techniques that specifically target hallucination reduction while maintaining solution quality and diversity.

## Limitations
- The evaluation is confined to mathematical reasoning tasks, limiting generalizability to other domains
- The AutoMathCritique framework relies on GPT-4/GPT-4o for synthetic data generation, raising scalability concerns
- The Monte Carlo filtering threshold (τ = 0.3) is set empirically without thorough sensitivity analysis
- Comparison primarily focuses on single-model approaches rather than more recent advanced reasoning methods

## Confidence

**High confidence**: The core two-player paradigm separating actor and critique roles, and the demonstration that critique supervision improves test-time performance on difficult queries are well-supported by experimental results.

**Medium confidence**: The claim that critique supervision improves exploration efficiency and solution diversity during training is supported by experiments but could benefit from more detailed analysis of the solution distribution across difficulty levels.

**Medium confidence**: The effectiveness of scaling inference-time computation with critique supervision is demonstrated, but the analysis could be more comprehensive regarding computational efficiency trade-offs.

## Next Checks
1. Replicate the AutoMathCritique framework with different difficulty levels and problem types to verify that the synthetic data generation process produces diverse and representative flawed reasoning paths across domains
2. Conduct ablation studies on the Monte Carlo filtering threshold (τ) to determine its sensitivity and impact on critique model quality and downstream actor performance
3. Test the two-player paradigm on non-mathematical reasoning tasks (e.g., scientific reasoning or coding problems) to assess generalizability beyond the mathematical domain where the approach was developed