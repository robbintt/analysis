---
ver: rpa2
title: 'ASMR: Activation-sharing Multi-resolution Coordinate Networks For Efficient
  Inference'
arxiv_id: '2405.12398'
source_url: https://arxiv.org/abs/2405.12398
tags:
- asmr
- siren
- inference
- layers
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ASMR, an activation-sharing multi-resolution
  coordinate network that reduces inference cost by decoupling it from network depth.
  ASMR uses multi-resolution coordinate decomposition and hierarchical modulation
  to share activations across data grids, achieving near O(1) inference complexity
  regardless of the number of layers.
---

# ASMR: Activation-sharing Multi-resolution Coordinate Networks For Efficient Inference

## Quick Facts
- arXiv ID: 2405.12398
- Source URL: https://arxiv.org/abs/2405.12398
- Reference count: 40
- Key outcome: ASMR reduces inference cost by up to 500x compared to SIREN while maintaining or improving reconstruction quality through activation-sharing multi-resolution coordinate decomposition

## Executive Summary
ASMR introduces an activation-sharing multi-resolution coordinate network that decouples inference cost from network depth, achieving near O(1) complexity regardless of layer count. By using hierarchical coordinate decomposition and local modulations, ASMR shares activations across data grids, dramatically reducing multiply-accumulate operations while maintaining high reconstruction quality. The method is particularly effective for rasterized data like images and videos, enabling efficient meta-learning through global latent structure learning.

## Method Summary
ASMR employs multi-resolution coordinate decomposition to partition input space into hierarchical grids, where each network layer only processes its resolution level before upsampling and sharing activations. Hierarchical modulators inject position-dependent bias vectors at different resolution levels, improving reconstruction fidelity with minimal parameter increase. The method maintains a purely implicit representation, allowing it to learn global latent structures for dataset-level tasks while achieving significant computational efficiency through activation sharing.

## Key Results
- Reduces MAC operations of vanilla SIREN models by up to 500x while achieving higher reconstruction quality
- Achieves >30dB PSNR on low-resolution images with less than 2K MACs
- Outperforms SIREN on megapixel image fitting tasks with 500x fewer MACs
- Demonstrates effective meta-learning on CIFAR-10 dataset through global latent structure learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ASMR achieves near O(1) inference complexity by decoupling inference cost from network depth through activation sharing.
- **Mechanism**: ASMR uses multi-resolution coordinate decomposition to partition the input space into hierarchical grids. Each layer of the network only needs to infer activations for the grid points at its resolution level, not for all N data points. These activations are then upsampled and shared across the full data grid.
- **Core assumption**: The data exhibits hierarchical and periodic patterns that can be exploited by coordinate decomposition.
- **Evidence anchors**:
  - [abstract] "ASMR uses multi-resolution coordinate decomposition and hierarchical modulation to share activations across grids of the data, achieving near O(1) inference complexity regardless of the number of layers."
  - [section 3.3] "Instead of inferring all N data points on every layer, each hidden layer and modulator only has to infer on Ci data points and Bi grids, respectively."
  - [corpus] Weak - no direct mention of activation sharing in related papers, though multi-resolution approaches are discussed.
- **Break condition**: If the data does not exhibit hierarchical or periodic patterns, the coordinate decomposition will not effectively reduce inference cost.

### Mechanism 2
- **Claim**: Hierarchical modulation improves reconstruction fidelity with minimal parameter increase.
- **Mechanism**: ASMR employs position-dependent modulators at each resolution level (except level-0) to inject coordinate-dependent bias vectors into the network layers. These modulators project the multi-resolution coordinates to the hidden dimension and add them elementwise to the activations.
- **Core assumption**: Local modulations at different resolution levels can capture fine-grained details better than global modulations.
- **Evidence anchors**:
  - [section 3.2] "The output activations from each modulator are then treated as the bias of the corresponding resolution level... This hierarchical modulation technique... improves reconstruction fidelity with only a negligible increase in parameter count."
  - [section 4.3] "ASMR significantly outperforms other baselines, including Instant-NGP, KiloNeRF and its SIREN counterpart, with an absolute gain of over 14dB in PSNR, while also exhibiting the lowest MAC of around 1K."
  - [corpus] Weak - related papers discuss multi-resolution representations but not specifically hierarchical modulation with local bias vectors.
- **Break condition**: If the coordinate decomposition is not aligned with the data structure, the local modulations may not capture meaningful features.

### Mechanism 3
- **Claim**: ASMR enables learning of global latent structures for dataset-level tasks like meta-learning.
- **Mechanism**: ASMR's purely implicit representation allows it to encode an entire dataset with a single INR by learning instance-specific global latent vectors that are injected as modulations. This global latent structure is not possible with explicit or hybrid representations.
- **Core assumption**: A single INR with appropriate modulations can capture shared structures across an entire dataset.
- **Evidence anchors**:
  - [abstract] "ASMR remains purely implicit, allowing it to handle tasks that require global latent structure, such as meta-learning."
  - [section 4.4] "ASMR permits the learning of global latent structure of signals, enabling it to encode an entire dataset with a single INR, which oftentimes is infeasible for methods employing explicit features."
  - [section 4.4] "ASMR... demonstrates that the multi-resolution hierarchical structure of ASMR can be generalized to effectively encode shared structures across images."
  - [corpus] Weak - related papers do not discuss meta-learning with implicit representations.
- **Break condition**: If the dataset lacks shared structures or if the modulations cannot effectively capture instance-specific details, the meta-learning performance will degrade.

## Foundational Learning

- **Concept**: Implicit Neural Representations (INRs)
  - **Why needed here**: ASMR is an INR that encodes signals as continuous functions rather than discrete samples.
  - **Quick check question**: What is the key difference between an INR and a traditional discrete representation like an image array?

- **Concept**: Coordinate Decomposition
  - **Why needed here**: ASMR uses coordinate decomposition to partition the input space into hierarchical grids.
  - **Quick check question**: How does coordinate decomposition relate to the idea of changing the base of a coordinate system?

- **Concept**: Hierarchical Modulation
  - **Why needed here**: ASMR employs hierarchical modulation to inject coordinate-dependent bias vectors at different resolution levels.
  - **Quick check question**: What is the difference between global modulations and the local modulations used in ASMR?

## Architecture Onboarding

- **Component map**:
  - Multi-resolution coordinate decomposition -> Hierarchical modulators -> Activation-sharing inference -> SIREN backbone

- **Critical path**:
  1. Decompose input coordinates into hierarchical levels.
  2. Generate modulations for each level using position-dependent modulators.
  3. Perform activation-sharing inference through upsampling and elementwise addition.
  4. Output the final reconstruction.

- **Design tradeoffs**:
  - Parameter count vs. reconstruction quality: ASMR increases parameters slightly with hierarchical modulation but significantly improves quality.
  - MACs vs. depth: ASMR decouples MACs from depth, allowing deeper networks without increased inference cost.
  - Implicit vs. explicit representation: ASMR remains purely implicit, enabling global latent structure learning but potentially struggling with continuous signals.

- **Failure signatures**:
  - Poor reconstruction quality: Indicates misalignment between coordinate decomposition and data structure.
  - Noisy artifacts with smooth signals: Suggests strong inductive bias towards rasterized data.
  - Excessive memory usage: May indicate inefficient upsampling or too many resolution levels.

- **First 3 experiments**:
  1. Fit a simple low-resolution image (e.g., 64x64) with a 3-layer ASMR and compare MACs and PSNR to SIREN.
  2. Test coordinate decomposition with different base partitions to find optimal trade-off between MACs and quality.
  3. Evaluate meta-learning capability by training ASMR on a small image dataset and measuring reconstruction after few gradient steps.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the theoretical limit of how much ASMR can reduce inference cost while maintaining high reconstruction quality, and how does this depend on the data's inherent multi-resolution structure?
- **Basis in paper**: [inferred] The paper shows ASMR can reduce MACs by up to 500x, but doesn't explore the theoretical bounds or data dependency.
- **Why unresolved**: The experiments demonstrate effectiveness but don't provide a theoretical analysis of the maximum possible reduction or how it varies with data characteristics.
- **What evidence would resolve it**: Theoretical analysis deriving the maximum possible MAC reduction for different data types, and experiments systematically varying data structure to measure impact on ASMR's efficiency.

### Open Question 2
- **Question**: How can ASMR be extended to effectively handle continuous data like signed distance fields (SDFs) without the grid-like symmetry bias causing artifacts?
- **Basis in paper**: [explicit] The paper notes ASMR struggles with smooth signals like SDFs due to grid-like symmetry bias, suggesting this as future work.
- **Why unresolved**: The paper identifies the limitation but doesn't propose solutions for adapting ASMR to continuous data representations.
- **What evidence would resolve it**: Proposed modifications to ASMR architecture or training that enable high-quality SDF reconstruction while maintaining low inference cost.

### Open Question 3
- **Question**: Can ASMR's expressivity be decoupled from network width in addition to depth, and what architectural changes would enable this?
- **Basis in paper**: [explicit] The paper notes that while ASMR decouples expressivity from depth, it still depends on network width, suggesting this as valuable future research.
- **Why unresolved**: The paper achieves one decoupling but identifies the remaining dependency on width as an open problem.
- **What evidence would resolve it**: Novel ASMR variants that achieve high reconstruction quality with minimal width, demonstrating successful decoupling from both depth and width.

## Limitations
- ASMR's strong inductive bias toward rasterized data limits its effectiveness with continuous signals like signed distance fields, where it generates noisy artifacts with sharp edges.
- The method requires careful tuning of coordinate decomposition bases and hierarchical structure for optimal performance.
- While ASMR reduces inference cost significantly, it increases parameter count through hierarchical modulation layers.

## Confidence

**High Confidence**: Claims about MAC reduction (500x on SIREN) are well-supported by experimental results showing clear efficiency gains across multiple datasets.

**Medium Confidence**: Reconstruction quality improvements (>30dB PSNR) are demonstrated but may depend heavily on dataset characteristics and proper hyperparameter tuning.

**Medium Confidence**: Meta-learning capabilities are shown on CIFAR-10 but require further validation on more diverse datasets and tasks.

## Next Checks
1. Test ASMR's performance on continuous signal tasks (e.g., signed distance fields, smooth 3D shapes) to quantify the limitation with non-rasterized data.
2. Conduct ablation studies varying the number of resolution levels and coordinate decomposition bases to identify optimal configurations for different data types.
3. Evaluate generalization across diverse datasets (medical imaging, scientific simulations) to assess robustness beyond the presented Kodak, ShapeNet, and CIFAR-10 datasets.