---
ver: rpa2
title: A Recipe For Building a Compliant Real Estate Chatbot
arxiv_id: '2410.10860'
source_url: https://arxiv.org/abs/2410.10860
tags:
- safety
- real
- estate
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to build a compliant real estate chatbot
  by fine-tuning a llama-3-8B-instruct model on a synthetic dataset that combines
  general real estate instructions with safety-focused data addressing Fair Housing
  Act and Equal Credit Opportunity Act compliance. The authors generate a diverse
  dataset through LLM collaboration and human curation, then prune it to ensure quality.
---

# A Recipe For Building a Compliant Real Estate Chatbot

## Quick Facts
- arXiv ID: 2410.10860
- Source URL: https://arxiv.org/abs/2410.10860
- Reference count: 22
- A fine-tuned llama-3-8B model that outperforms base llama3-8B and matches GPT-4o on helpfulness while significantly improving safety compliance

## Executive Summary
This paper presents a method for building a compliant real estate chatbot by fine-tuning llama-3-8B-instruct on a synthetic dataset that combines general real estate instructions with safety-focused data addressing Fair Housing Act and Equal Credit Opportunity Act compliance. The authors generate a diverse dataset through LLM collaboration and human curation, then prune it to ensure quality. The resulting model demonstrates strong performance, outperforming the base llama3-8B model and matching GPT-4o on helpfulness while significantly improving safety compliance metrics.

## Method Summary
The authors developed a three-stage pipeline to generate a synthetic dataset: first identifying 90 real estate topics through human-LLM collaboration, then generating challenging questions about subtopics, and finally obtaining comprehensive responses. They created three dataset splits - general instructions (20K→16.6K), safety instructions (10K→7.2K), and dialog (2K→1.7K) - using GPT-4o as a generator model. The dataset was pruned using similarity-based approaches with thresholds of 0.9 for general/dialog splits and 0.95 for safety split. They fine-tuned llama-3-8B-instruct with LoRA adapters (rank=128, alpha=256) for 5 epochs on 4 A100 GPUs, achieving significant performance improvements while maintaining computational efficiency.

## Key Results
- The fine-tuned model matches GPT-4o on helpfulness while significantly outperforming base llama3-8B on safety compliance (86% win rate)
- Model wins 65.12% of head-to-head comparisons on helpfulness against base llama3-8B and 72.93% on safety
- Despite having fewer parameters, the model outperforms llama3-70B-Instruct on real estate tasks
- Safety data ablation shows that 25% safety data provides optimal balance between safety compliance and helpfulness

## Why This Works (Mechanism)

### Mechanism 1
The model's enhanced safety compliance stems from targeted fine-tuning on synthetic safety data that explicitly addresses Fair Housing Act and Equal Credit Opportunity Act violations. The authors generated non-compliant queries and designed a response strategy where the model first acknowledges why the query is non-compliant, then provides general compliant information, and finally refers to specialists if needed. This explicit training on problematic scenarios teaches the model to recognize and properly handle compliance issues.

### Mechanism 2
The model achieves strong helpfulness performance through fine-tuning on a diverse, knowledge-intensive real estate instruction dataset. The authors used a three-stage pipeline to generate diverse real estate questions: first identifying 90 real estate topics through human-LLM collaboration, then generating challenging questions about subtopics, and finally obtaining comprehensive responses. This approach ensures the model learns to handle a wide range of real estate expertise scenarios.

### Mechanism 3
The model outperforms larger models (llama3-70b) despite having fewer parameters due to the efficiency of LoRA fine-tuning with carefully curated data. The authors applied LoRA adapters with rank 128 and alpha 256 to the llama3-8B-instruct model, fine-tuning it on their synthetic dataset. This parameter-efficient approach allows the model to adapt to the specific domain and compliance requirements without requiring full fine-tuning.

## Foundational Learning

- **Concept**: Fair Housing Act and Equal Credit Opportunity Act compliance
  - Why needed here: The model must recognize and avoid discriminatory practices like steering and redlining that are prohibited by these laws
  - Quick check question: What are the protected characteristics under the Fair Housing Act that the model must not discriminate against?

- **Concept**: Instruction fine-tuning and alignment techniques
  - Why needed here: The model is fine-tuned on synthetic datasets to improve both its helpfulness and safety compliance, building on techniques like RLHF
  - Quick check question: How does fine-tuning on synthetic safety data differ from traditional alignment approaches like RLHF?

- **Concept**: Synthetic data generation for specialized domains
  - Why needed here: The authors used LLM collaboration and human curation to create domain-specific datasets for both general real estate knowledge and compliance scenarios
  - Quick check question: What are the advantages and limitations of using synthetic data for fine-tuning domain-specific language models?

## Architecture Onboarding

- **Component map**: Base model (llama3-8B-instruct) → LoRA adapters (rank 128, alpha 256) → Synthetic dataset (general, safety, dialog splits) → Evaluation (G-Eval metrics, model-based judges)
- **Critical path**: Dataset generation → LoRA fine-tuning → Evaluation with model-based judges → Deployment
- **Design tradeoffs**: The choice of llama3-8B-instruct over larger models balances computational efficiency with performance; the synthetic dataset approach trades potential data quality issues for control over compliance scenarios
- **Failure signatures**: Poor performance on novel compliance scenarios; inability to handle real-time data; overfitting to synthetic data patterns
- **First 3 experiments**:
  1. Evaluate the base llama3-8B-instruct model on the safety benchmark to establish baseline compliance
  2. Fine-tune the model on only the general instruction dataset and compare performance to measure the impact of domain-specific knowledge
  3. Fine-tune the model on only the safety dataset and compare compliance metrics to measure the impact of safety training

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but based on the limitations and future work implications, several important questions emerge regarding the optimal size of safety data, the impact of dialog data on long-term user satisfaction, generalization to non-US contexts, the relationship between pruning thresholds and model robustness, and safety compliance in low-resource languages.

## Limitations
- Reliance on synthetic data may not fully capture real-world compliance scenario complexity
- Evaluation methodology depends heavily on model-based judges rather than extensive human evaluation
- Single "best-of-3" comparison approach with GPT-4o may not represent typical performance distributions
- Doesn't address handling of real-time market data or rapidly changing compliance regulations

## Confidence
- **High confidence**: The model's technical implementation details (LoRA configuration, dataset generation pipeline, evaluation methodology) are well-specified and reproducible.
- **Medium confidence**: The safety performance improvements are credible given the targeted training approach, but the evaluation methodology introduces some uncertainty.
- **Medium confidence**: The helpfulness performance matching GPT-4o is supported by the results, though the evaluation methodology and comparison approach limit definitive conclusions.

## Next Checks
1. **Human evaluation of compliance scenarios**: Conduct a blind human evaluation where experts rate the model's responses to novel compliance scenarios not present in the training data, focusing on edge cases that might reveal gaps in the safety training.

2. **Long-term stability and drift analysis**: Evaluate the model's performance over extended use periods to assess whether compliance behaviors degrade over time or when exposed to adversarial queries, and measure how quickly the model's knowledge becomes outdated for real estate market trends.

3. **Comparison with alternative fine-tuning approaches**: Replicate the study using full fine-tuning instead of LoRA and compare the safety and helpfulness trade-offs, while also testing different LoRA configurations (rank and alpha values) to optimize the parameter-efficient approach.