---
ver: rpa2
title: 'Seeing Through Their Eyes: Evaluating Visual Perspective Taking in Vision
  Language Models'
arxiv_id: '2409.12969'
source_url: https://arxiv.org/abs/2409.12969
tags:
- object
- performance
- perspective
- answer
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two manually curated datasets, Isle-Bricks
  and Isle-Dots, to evaluate Visual Perspective Taking (VPT) capabilities in Vision
  Language Models (VLMs). VPT is the ability to understand the viewpoint of another
  person, which is essential for human-like interaction and coordination.
---

# Seeing Through Their Eyes: Evaluating Visual Perspective Taking in Vision Language Models

## Quick Facts
- arXiv ID: 2409.12969
- Source URL: https://arxiv.org/abs/2409.12969
- Reference count: 40
- Primary result: VLMs experience 35% average performance drop on VPT tasks compared to object detection, with poor correlation between the two capabilities

## Executive Summary
This paper introduces two manually curated datasets, Isle-Bricks and Isle-Dots, to evaluate Visual Perspective Taking (VPT) capabilities in Vision Language Models (VLMs). The authors test 12 commonly used VLMs, including GPT-4, Claude, Gemini, and open-source models, on these datasets. The primary finding is that VLMs experience a significant performance drop when perspective-taking is required, with accuracy dropping from 83% on object detection to slightly above 54% on VPT tasks. The study reveals that models struggle particularly with scenes involving multiple agents, indicating difficulties in attributing perspective to specific individuals. These results highlight the need for improved VPT capabilities in VLMs and the importance of developing benchmarks specifically designed to test this skill.

## Method Summary
The authors evaluate 12 VLMs on two manually curated datasets (Isle-Bricks and Isle-Dots) using zero-shot and Chain-of-Thought prompting. The datasets contain scenes where models must determine what agents can see from their perspective, contrasting with standard object detection tasks. Performance is measured as accuracy, comparing VPT task results against baseline object detection performance. The evaluation includes both single-agent and multi-agent scenes to assess perspective attribution capabilities.

## Key Results
- VLMs show 35% average performance drop when perspective-taking is required versus standard object detection tasks
- Performance on VPT tasks is poorly correlated with object detection performance (83% vs. 54% accuracy)
- Chain-of-thought prompting offers only slight and inconsistent improvement (average 13% boost, highly model-dependent)
- Models struggle particularly with multi-agent scenes, showing drastic performance drops when scenes contain two people

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VLMs experience significant performance drops on VPT tasks, suggesting they lack true spatial reasoning capabilities
- **Mechanism:** Models can identify objects but fail to understand occlusion and spatial relationships from another viewpoint
- **Core assumption:** Object detection and perspective-taking are separate cognitive capabilities
- **Evidence anchors:** 83% accuracy on object detection vs. 54% on VPT tasks; performance poorly correlated
- **Break condition:** If CoT consistently improved all models by large margins, suggesting limitation is about prompting strategy

### Mechanism 2
- **Claim:** Chain-of-thought prompting provides only slight and inconsistent improvement for VPT tasks
- **Mechanism:** While CoT helps some models break down reasoning, it doesn't address fundamental inability to understand spatial occlusion
- **Core assumption:** VLMs can leverage explicit reasoning steps when available
- **Evidence anchors:** Average 13% improvement with CoT, but highly model-dependent; benefits vary significantly
- **Break condition:** If CoT consistently improved all models by large margins, suggesting limitation is about prompting strategy

### Mechanism 3
- **Claim:** VLMs struggle particularly with scenes involving multiple agents
- **Mechanism:** Models cannot disambiguate which agent's viewpoint should be considered, lacking ability to maintain separate mental models
- **Core assumption:** Presence of multiple agents creates ambiguity the model cannot resolve
- **Evidence anchors:** Performance drops drastically when scenes contain two people; visible drop in multi-person scenes
- **Break condition:** If models performed equally well on single-agent and multi-agent scenes

## Foundational Learning

- **Concept:** Visual perspective-taking (VPT) - understanding what another person can see from their viewpoint
  - Why needed here: Core cognitive skill being evaluated; understanding definition helps interpret experimental results
  - Quick check question: If a person is facing a wall with three colored dots, can they see the dots behind them? Why or why not?

- **Concept:** Object detection vs. spatial reasoning - distinguishing between identifying objects and understanding their spatial relationships
  - Why needed here: Paper shows these are separate capabilities with poor correlation; understanding distinction is crucial
  - Quick check question: If you can identify all objects in a scene, does that mean you can determine what each person in the scene can see? Explain.

- **Concept:** Chain-of-thought prompting - technique where models are prompted to show their reasoning process
  - Why needed here: Paper evaluates whether this technique improves VPT performance; understanding how it works helps interpret mixed results
  - Quick check question: If a model can correctly identify all objects but struggles with VPT, would you expect CoT prompting to help significantly? Why or why not?

## Architecture Onboarding

- **Component map:** Vision encoder → Multimodal transformer → Text decoder
- **Critical path:** Image input → Visual feature extraction → Spatial relationship encoding → Perspective reasoning → Answer generation
- **Design tradeoffs:** Larger models show better consistency but don't necessarily perform better on VPT; CoT helps some models but not others; object detection performance doesn't predict VPT capability
- **Failure signatures:** Random or near-random performance on VPT tasks (54% accuracy); poor performance on multi-agent scenes; inconsistent improvement with CoT; high unknown answer rates with CoT
- **First 3 experiments:**
  1. Test baseline VPT performance on single-agent scenes only to isolate multi-agent effect
  2. Evaluate whether fine-tuning on VPT data improves performance vs. few-shot prompting
  3. Compare spatial reasoning performance on synthetic vs. real images to determine if representation differences affect VPT capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning or k-shot learning affect VLMs' performance on Visual Perspective Taking tasks?
- Basis in paper: The study does not investigate the impact of fine-tuning or k-shot learning on the tested models
- Why unresolved: It's possible that with additional data, the performance of the models would improve
- What evidence would resolve it: Conducting experiments to evaluate VLMs' performance on VPT tasks after fine-tuning or k-shot learning, and comparing the results to the baseline performance

### Open Question 2
- Question: How do VLMs perform on Level 2 Visual Perspective Taking tasks as they become more capable?
- Basis in paper: The task proposed is considered Level 1 in the VPT task hierarchy, meaning it is relatively simple
- Why unresolved: As VLMs get more capable, testing Level 2 capabilities might be needed
- What evidence would resolve it: Developing and evaluating VLMs on Level 2 VPT tasks, which involve more complex scenarios and higher-level reasoning about others' perspectives

### Open Question 3
- Question: How do VLMs handle scenes with a larger number of subjects in Visual Perspective Taking tasks?
- Basis in paper: In the Isle-Dots dataset, there is only one person, and in Isle-Bricks, at most two LEGO figures
- Why unresolved: The study found that performance drops drastically in the presence of multiple people
- What evidence would resolve it: Creating and evaluating VLMs on VPT datasets with a larger number of subjects, and analyzing the performance drop compared to scenes with fewer subjects

## Limitations

- The datasets used are manually curated and may not represent the full diversity of perspective-taking situations encountered in real applications
- The evaluation relies on multiple VLMs with different architectures, but specific implementations and configurations are not fully detailed
- Chain-of-thought prompting showed inconsistent improvements, but underlying reasons for variability across models are not fully explained

## Confidence

- **High confidence:** General finding that VLMs perform significantly worse on VPT tasks compared to object detection tasks (supported by consistent accuracy drops across all tested models)
- **Medium confidence:** Claim that multi-agent scenes are particularly challenging for VLMs (supported by observed performance drops, but could be influenced by dataset design)
- **Low confidence:** Assertion that chain-of-thought prompting offers only slight and inconsistent improvement (highly model-dependent results suggest this may vary significantly with different prompting strategies)

## Next Checks

1. Test the models on additional VPT datasets with varying complexity levels and real-world scenarios to validate whether observed limitations generalize beyond the Isle-Bricks and Isle-Dots datasets
2. Conduct ablation studies to isolate whether performance drops are due to spatial reasoning limitations, perspective attribution issues, or other factors by systematically varying scene complexity and agent configurations
3. Evaluate whether fine-tuning on VPT-specific data or using different prompting strategies (beyond CoT) can substantially improve performance, which would help determine if limitations are fundamental or addressable through training/prompting