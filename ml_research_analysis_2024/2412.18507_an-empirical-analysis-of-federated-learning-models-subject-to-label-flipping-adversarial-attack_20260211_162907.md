---
ver: rpa2
title: An Empirical Analysis of Federated Learning Models Subject to Label-Flipping
  Adversarial Attack
arxiv_id: '2412.18507'
source_url: https://arxiv.org/abs/2412.18507
tags:
- clients
- adversarial
- accuracy
- each
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically analyzes label-flipping attacks on federated
  learning models using MNIST dataset. The study compares seven models (MLR, SVC,
  MLP, CNN, Random Forest, XGBoost, LSTM) across 10 and 100 clients, varying adversarial
  client percentages (10-100%) and label-flipping rates (10-100%).
---

# An Empirical Analysis of Federated Learning Models Subject to Label-Flipping Adversarial Attack

## Quick Facts
- **arXiv ID**: 2412.18507
- **Source URL**: https://arxiv.org/abs/2412.18507
- **Reference count**: 40
- **Primary result**: Empirical analysis of label-flipping attacks on federated learning models using MNIST dataset

## Executive Summary
This paper presents a comprehensive empirical analysis of how different federated learning models respond to label-flipping adversarial attacks. The study evaluates seven distinct models - MLR, SVC, MLP, CNN, Random Forest, XGBoost, and LSTM - across varying client populations (10 and 100 clients) and different attack intensities. The research systematically varies both the percentage of adversarial clients (10-100%) and the proportion of labels they flip (10-100%), providing a detailed mapping of attack effectiveness across different scenarios.

The key finding is that model robustness to label-flipping attacks is not uniform across architectures or attack configurations. Notably, MLR demonstrates the most stable performance across different attack scenarios, while other models show varying sensitivities to specific attack patterns. The study reveals that some models are more vulnerable when few clients flip many labels, while others suffer more when many clients flip fewer labels, suggesting that defense strategies must be tailored to both the model architecture and expected attack patterns.

## Method Summary
The research employs a systematic experimental framework using the MNIST dataset to evaluate seven federated learning models under controlled label-flipping attacks. The methodology involves setting up federated learning scenarios with either 10 or 100 clients, where a specified percentage of clients (ranging from 10% to 100%) are designated as adversarial. These adversarial clients flip a portion of their training labels (10% to 100% of labels) before sending updates to the central server. The study measures model accuracy degradation across all combinations of client count, adversarial percentage, and flipping rate, providing a comprehensive mapping of attack effectiveness for each model architecture.

## Key Results
- Model accuracy consistently decreases as the number of clients increases, with MLR showing the most stable performance across all attack scenarios
- Different models exhibit distinct robustness patterns: some are more resilient when few clients flip many labels, while others maintain better performance when many clients flip few labels
- The choice of federated learning model should be guided by expected attack scenarios, with no single model being universally optimal across all configurations

## Why This Works (Mechanism)
Label-flipping attacks work by corrupting the training data at the client level before model updates are aggregated at the central server. When adversarial clients deliberately mislabel their training data, they introduce systematic bias into the model updates they send. During the federated averaging process, these corrupted updates can significantly shift the global model parameters away from their intended values, particularly when the attack pattern aligns with vulnerabilities in the specific model architecture. The effectiveness of these attacks depends on both the proportion of clients compromised and the intensity of label corruption, with different models showing varying susceptibility to these coordinated data poisoning attempts.

## Foundational Learning
**Federated Learning**: A machine learning paradigm where multiple clients collaboratively train a model under the coordination of a central server while keeping data localized. *Why needed*: Forms the fundamental framework being attacked and analyzed. *Quick check*: Can you explain how federated averaging differs from traditional centralized training?

**Label-Flipping Attacks**: Adversarial strategy where training labels are deliberately corrupted to mislead the learning algorithm. *Why needed*: The specific attack vector being studied that impacts model accuracy. *Quick check*: How does flipping labels systematically affect gradient updates in different model types?

**Model Robustness**: The ability of a learning algorithm to maintain performance under adversarial conditions. *Why needed*: The core metric being evaluated across different architectures. *Quick check*: What factors determine whether a model is more or less vulnerable to data poisoning attacks?

**Gradient Aggregation**: The process of combining model updates from multiple clients during federated learning. *Why needed*: Critical mechanism through which adversarial updates influence the global model. *Quick check*: How does the averaging process amplify or mitigate the impact of corrupted updates?

## Architecture Onboarding

**Component Map**: Data Generation -> Model Training -> Attack Injection -> Aggregation -> Evaluation -> Analysis

**Critical Path**: The experimental pipeline flows from controlled data generation (MNIST) through federated training with injected attacks, followed by aggregation at the central server, then evaluation of accuracy degradation. The analysis phase maps attack configurations to model performance across the seven architectures.

**Design Tradeoffs**: The study balances comprehensive coverage of attack scenarios against computational constraints, limiting experiments to two client populations (10 and 100) while maintaining systematic variation of attack parameters. This design enables thorough analysis within reasonable resource bounds but may miss dynamics at intermediate client counts or more extreme attack configurations.

**Failure Signatures**: Models exhibit distinct failure modes: MLR shows gradual accuracy degradation across all attack types, while deep learning models (MLP, CNN) may initially resist attacks but suffer catastrophic accuracy drops beyond certain thresholds. Random Forest and XGBoost show intermediate patterns, with performance degradation correlating with the percentage of flipped labels rather than the number of adversarial clients.

**3 First Experiments**: 
1. Baseline accuracy test: Run all seven models on clean MNIST data with no attacks to establish performance baselines
2. Single adversarial client test: Deploy one adversarial client flipping 50% of labels to observe initial attack impact
3. Full compromise test: Set all clients as adversarial with 100% label flipping to identify maximum vulnerability points

## Open Questions the Paper Calls Out
None

## Limitations
- The study is limited to the MNIST dataset, which may not generalize to more complex, real-world data distributions and higher-dimensional problems
- Experimental setup assumes static attack patterns and homogeneous client behavior, not reflecting the dynamic nature of real-world federated learning environments
- Analysis covers only seven specific models, leaving uncertainty about how findings would extend to other architectures or more recent federated learning approaches

## Confidence
- Model performance comparisons across attack scenarios: **High** confidence
- Generalizability to non-MNIST datasets: **Low** confidence
- Attack resilience patterns for real-world federated systems: **Medium** confidence

## Next Checks
1. Replicate the experiments on more complex datasets (CIFAR-10/100 or real-world federated datasets) to assess generalizability of the findings
2. Implement and test the models under dynamic attack scenarios where attack patterns change over training rounds
3. Evaluate the models' performance when clients have heterogeneous data distributions (non-IID) in addition to adversarial attacks