---
ver: rpa2
title: Fair Differentiable Neural Network Architecture Search for Long-Tailed Data
  with Self-Supervised Learning
arxiv_id: '2406.16949'
source_url: https://arxiv.org/abs/2406.16949
tags:
- architecture
- search
- learning
- neural
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of neural architecture search
  (NAS) performance degradation on long-tailed datasets, where class imbalance leads
  to biased models favoring head classes. The proposed solution, SSF-NAS, integrates
  FairDARTS and Barlow Twins to improve search and training performance on imbalanced
  data.
---

# Fair Differentiable Neural Network Architecture Search for Long-Tailed Data with Self-Supervised Learning

## Quick Facts
- arXiv ID: 2406.16949
- Source URL: https://arxiv.org/abs/2406.16949
- Authors: Jiaming Yan
- Reference count: 35
- One-line primary result: SSF-NAS achieves 82.64% accuracy on balanced test data, outperforming DARTS (78.26%) and FairDARTS (79.35%)

## Executive Summary
This paper addresses the problem of neural architecture search (NAS) performance degradation on long-tailed datasets, where class imbalance leads to biased models favoring head classes. The proposed solution, SSF-NAS, integrates FairDARTS and Barlow Twins to improve search and training performance on imbalanced data. FairDARTS replaces softmax with sigmoid normalization and adds a zero-one loss to reduce bias in operation selection. Barlow Twins, a self-supervised learning method, aligns representations from differently transformed image pairs, improving performance without labeled data. Experiments on CIFAR10-LT with various long-tailed distributions show that SSF-NAS achieves 82.64% accuracy on balanced test data, outperforming DARTS (78.26%) and FairDARTS (79.35%). The results demonstrate SSF-NAS's effectiveness in handling class imbalance during NAS, aligning with expectations despite not fully matching original results.

## Method Summary
SSF-NAS integrates FairDARTS and Barlow Twins to address NAS performance degradation on long-tailed datasets. The method replaces softmax normalization with sigmoid normalization in FairDARTS to reduce bias toward residual connections, and adds a zero-one loss to push attention weights toward binary values. Barlow Twins provides self-supervised learning by aligning representations from differently transformed image pairs, improving performance without labeled data. The supernet architecture consists of 8 cells (6 normal, 2 reduction) with a DAG structure, and the search space includes 8 operations per edge. Training involves a 40-epoch search phase followed by a 150-epoch retrain phase. Experiments on CIFAR10-LT with imbalance factors of 0.01 and 0.1 show that SSF-NAS outperforms DARTS and FairDARTS in both balanced and long-tailed test accuracy.

## Key Results
- SSF-NAS achieves 82.64% accuracy on balanced test data, outperforming DARTS (78.26%) and FairDARTS (79.35%)
- On long-tailed test data with imbalance factor 0.1, SSF-NAS achieves 52.51% accuracy, surpassing DARTS (46.28%) and FairDARTS (50.43%)
- SSF-NAS demonstrates improved performance on exponential and step distributions compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing softmax with sigmoid normalization in FairDARTS reduces bias toward residual connections.
- Mechanism: In DARTS, softmax normalization across operations amplifies the advantage of residual connections because they can leverage outputs from other operations. Sigmoid normalization allows each operation to be evaluated independently, making the selection fairer.
- Core assumption: Residual connections are inherently more likely to be selected due to their ability to benefit from other operations in softmax normalization.
- Evidence anchors:
  - [section] "FairDARTS [13] proposed to replace the softmax normalization with the sigmoid function...to avoid unfair competition with residual connection"
  - [abstract] "FairDARTS replaces softmax with sigmoid normalization and adds a zero-one loss to reduce bias in operation selection"
- Break condition: If the target dataset does not contain residual connections as candidate operations, or if residual connections are not inherently advantaged in the search space, the benefit of this mechanism diminishes.

### Mechanism 2
- Claim: Barlow Twins self-supervised learning improves representation quality on imbalanced data.
- Mechanism: Barlow Twins generates pairs of transformed images and aligns their representations while decorrelating output dimensions. This forces the model to learn discriminative features without relying on labels, which is beneficial when class distributions are imbalanced.
- Core assumption: Self-supervised learning can extract meaningful representations that generalize better to minority classes in long-tailed datasets.
- Evidence anchors:
  - [abstract] "Barlow Twins, a self-supervised learning method, aligns representations from differently transformed image pairs, improving performance without labeled data"
  - [section] "Barlow Twins can achieve a high accuracy even in the absence of data labels...to enhance the searching and training performance of NAS on the target datasets with long-tailed distributions"
- Break condition: If the dataset is too small or the transformations do not provide sufficient variation, Barlow Twins may fail to learn useful representations.

### Mechanism 3
- Claim: Zero-one loss pushes attention weights toward binary values, reducing selection bias.
- Mechanism: The zero-one loss term encourages attention weights to converge to 0 or 1, mimicking discrete operation selection during the continuous search phase. This reduces the bias introduced when converting continuous weights to one-hot encodings.
- Core assumption: The continuous-to-discrete conversion in DARTS introduces bias that can be mitigated by explicitly optimizing for binary-like weights.
- Evidence anchors:
  - [section] "FairDARTS used a zero-one loss under fair conditions to push the attention weights toward 0 or 1, thereby reducing the bias present when converting continuous encoding to one-hot encoding"
  - [section] "the proposed zero-one loss can be formulated as follows: L0−1 = − 1/N Σ|σ(αi) − 0.5|"
- Break condition: If the search space is already well-balanced or if the zero-one loss hyperparameter is not tuned properly, this mechanism may not provide additional benefit.

## Foundational Learning

- Concept: Neural Architecture Search (NAS)
  - Why needed here: SSF-NAS is built on differentiable NAS methods, so understanding the search process and supernet architecture is essential.
  - Quick check question: In DARTS, what is the role of the supernet and how are candidate operations evaluated during the search phase?

- Concept: Long-tailed data distribution
  - Why needed here: The paper addresses performance degradation on imbalanced datasets, so understanding how class imbalance affects model training is crucial.
  - Quick check question: How does a long-tailed label distribution affect the bias of a neural network during training, and why does this impact NAS performance?

- Concept: Self-supervised learning (Barlow Twins)
  - Why needed here: Barlow Twins is a key component of SSF-NAS for improving representation learning on imbalanced data.
  - Quick check question: What is the objective function of Barlow Twins, and how does it differ from contrastive learning methods like SimCLR?

## Architecture Onboarding

- Component map:
  - Supernet: Stacked cells (normal and reduction) with DAG structure
  - Search space: 8 operations per edge (none, max_pool, avg_pool, skip_connect, sep_conv_3x3, sep_conv_5x5, dil_conv_3x3, dil_conv_5x5)
  - FairDARTS module: Sigmoid normalization + zero-one loss
  - Barlow Twins module: Self-supervised loss function
  - Training pipeline: Search phase (40 epochs) + Retrain phase (150 epochs)

- Critical path:
  1. Initialize supernet with random architecture weights
  2. Search phase: Alternate between updating model weights (supernet training) and architecture weights (FairDARTS + Barlow Twins loss)
  3. Architecture extraction: Select operations with highest attention weights
  4. Retrain phase: Train the final architecture on the target dataset

- Design tradeoffs:
  - Shallow supernet during search reduces computation but may not represent deep model performance
  - Barlow Twins adds self-supervised loss but increases training time
  - Zero-one loss encourages binary weights but may slow convergence if λ0−1 is too high

- Failure signatures:
  - Architecture consistently selects 'none' operation → search space may be too large or operations are not well-suited
  - No improvement in balanced test accuracy → self-supervised loss may not be beneficial for this dataset
  - Training instability → learning rates or weight decay may need adjustment

- First 3 experiments:
  1. Run SSF-NAS on CIFAR10-LT with balanced distribution to verify baseline performance
  2. Test with exponential distribution (µ=0.1) to evaluate long-tailed handling
  3. Compare FairDARTS vs. SSF-NAS to isolate the impact of Barlow Twins

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of Barlow Twins with FairDARTS affect the search performance of NAS on datasets with extremely long-tailed distributions (e.g., imbalance factor µ < 0.01)?
- Basis in paper: [explicit] The paper discusses the use of Barlow Twins to replace supervised loss in SSF-NAS to improve performance on long-tailed datasets, but does not provide detailed results for extremely long-tailed distributions.
- Why unresolved: The paper mentions experiments with imbalance factors of 0.1 and 0.01 but does not delve into the performance specifics for µ < 0.01, leaving a gap in understanding the method's effectiveness under extreme conditions.
- What evidence would resolve it: Conducting experiments with µ < 0.01 and comparing the performance of SSF-NAS against other methods like DARTS and FairDARTS would provide insights into the method's robustness and effectiveness.

### Open Question 2
- Question: What are the computational trade-offs between using a shallow supernet versus a deep model in the search and retraining phases of SSF-NAS?
- Basis in paper: [inferred] The paper mentions the use of a shallow supernet to mitigate computation costs during the search phase, but it does not discuss the impact on the final architecture's performance when transitioning to a deep model.
- Why unresolved: The paper highlights the efficiency of using a shallow supernet but does not explore the potential performance degradation or improvements when retraining the architecture in a deep model.
- What evidence would resolve it: Analyzing the performance differences between architectures searched with shallow supernets and those directly searched with deep models would clarify the trade-offs involved.

### Open Question 3
- Question: How does the zero-one loss function in FairDARTS contribute to reducing bias in the selection of operations, and could alternative loss functions provide better results?
- Basis in paper: [explicit] The paper discusses the use of a zero-one loss function in FairDARTS to push attention weights towards 0 or 1, reducing bias during the transition from continuous to discrete encoding.
- Why unresolved: While the paper explains the purpose of the zero-one loss, it does not compare its effectiveness against other potential loss functions or explore how different formulations might impact the fairness and performance of the architecture search.
- What evidence would resolve it: Experimenting with alternative loss functions and comparing their impact on the fairness and performance of the architecture search would provide insights into the optimal approach for reducing bias.

## Limitations

- The experimental validation focuses primarily on CIFAR10-LT, limiting generalizability to more complex datasets with larger label spaces
- The integration details of Barlow Twins with FairDARTS remain underspecified, particularly regarding how the self-supervised loss is weighted during architecture search
- Claims about the generality of the approach to other NAS methods or more diverse datasets rely on assumptions not fully validated in the paper

## Confidence

- High confidence: The overall performance improvement of SSF-NAS over baseline methods (DARTS and FairDARTS) on CIFAR10-LT
- Medium confidence: The specific contribution of each component (FairDARTS mechanisms and Barlow Twins) to the performance gains
- Low confidence: Claims about the generality of the approach to other NAS methods or more diverse datasets

## Next Checks

1. Implement ablation studies isolating FairDARTS (sigmoid + zero-one loss) from Barlow Twins to quantify each component's individual contribution
2. Test SSF-NAS on CIFAR100-LT or ImageNet-LT to evaluate performance on datasets with larger label spaces and more complex visual patterns
3. Analyze the learned attention weights across different imbalance factors to verify that the zero-one loss effectively pushes weights toward binary values as claimed