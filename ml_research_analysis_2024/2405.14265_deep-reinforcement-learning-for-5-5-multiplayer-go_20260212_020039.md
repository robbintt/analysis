---
ver: rpa2
title: Deep Reinforcement Learning for 5*5 Multiplayer Go
arxiv_id: '2405.14265'
source_url: https://arxiv.org/abs/2405.14265
tags:
- game
- alphazero
- descent
- multiplayer
- been
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work adapts AlphaZero and Descent deep reinforcement learning
  methods to 5x5 multiplayer Go with three players (Black, White, Red). The authors
  train networks to maximize scores under Chinese rules, using self-play with UCT
  agents to warm-start training.
---

# Deep Reinforcement Learning for 5*5 Multiplayer Go

## Quick Facts
- arXiv ID: 2405.14265
- Source URL: https://arxiv.org/abs/2405.14265
- Reference count: 21
- Primary result: AlphaZero and Descent achieve similar performance on 5x5 multiplayer Go, with AlphaZero plateauing after 60 hours while Descent continues improving through 120 hours

## Executive Summary
This paper adapts AlphaZero and Descent deep reinforcement learning methods to 5x5 multiplayer Go with three players (Black, White, Red). The authors train neural networks to maximize scores under Chinese rules using self-play with UCT agents to warm-start training. After 120 hours of training, both methods achieve comparable performance to traditional UCT search, with AlphaZero reaching scores around 16/10/11 points and Descent reaching around 12/12/11 points for Black/White/Red players respectively.

The study demonstrates that deep reinforcement learning can effectively learn 5x5 multiplayer Go, with both methods converging to similar levels. AlphaZero shows more aggressive play as Black compared to UCT baselines, while Descent exhibits more balanced performance across positions. The results suggest that self-play reinforcement learning can discover competitive strategies in this constrained multi-agent environment, though the simplified board size limits generalizability to full-scale Go.

## Method Summary
The authors adapted AlphaZero and Descent deep reinforcement learning frameworks to the 5x5 multiplayer Go setting with three players. Both methods used neural networks to predict move probabilities and position values, trained through self-play against UCT agents that served as warm-start opponents. The training objective was to maximize scores under Chinese rules, with the network learning from game outcomes across multiple training iterations. The AlphaZero variant employed a search-based approach with Monte Carlo Tree Search, while Descent used a different optimization strategy for policy improvement.

## Key Results
- AlphaZero achieves scores of approximately 16/10/11 points for Black/White/Red after 120 hours
- Descent achieves scores of approximately 12/12/11 points for Black/White/Red after 120 hours
- AlphaZero plateaus after 60 hours while Descent continues improving through 120 hours
- Both methods outperform UCT baselines and converge to similar performance levels

## Why This Works (Mechanism)
Deep reinforcement learning methods can effectively learn 5x5 multiplayer Go through self-play and neural network function approximation. The AlphaZero approach combines Monte Carlo Tree Search with neural network policy and value functions, allowing the system to explore the game tree while learning from experience. Descent uses a different optimization strategy that appears to continue improving longer than AlphaZero on this task. Both methods leverage the simplified state space of 5x5 Go to learn competitive strategies within reasonable training timeframes.

## Foundational Learning
- **Multi-agent reinforcement learning**: Why needed - Multiplayer games require coordination and competition among multiple agents; Quick check - Can the methods handle more than two players effectively
- **Self-play training**: Why needed - No human expertise required, can discover novel strategies; Quick check - Does performance improve monotonically with more self-play games
- **Neural network function approximation**: Why needed - Maps board states to move probabilities and values; Quick check - Does network architecture scale with board size
- **UCT search as warm-start**: Why needed - Provides initial strong baseline before RL training; Quick check - Does performance exceed UCT after sufficient training
- **Chinese scoring rules**: Why needed - Defines the reward structure for training; Quick check - Are results consistent across different rule sets
- **Monte Carlo Tree Search**: Why needed - Enables strategic exploration of game tree; Quick check - Does search depth correlate with performance gains

## Architecture Onboarding

**Component Map**: Game environment -> UCT agent -> Self-play games -> Experience buffer -> Neural network -> MCTS/Descent optimization -> Updated policy -> Repeat

**Critical Path**: Game state → Neural network inference → Move selection → Game outcome → Reward signal → Policy/value update → Better network → Improved move selection

**Design Tradeoffs**: The 5x5 board size enables tractable learning but may miss strategic complexities of full Go; Chinese rules simplify scoring but limit cross-comparability; AlphaZero's search-based approach plateaus earlier than Descent's optimization strategy.

**Failure Signatures**: If network overfits to self-play opponents, performance may degrade against UCT; if training diverges, scores may collapse across all players; if exploration is insufficient, the system may get stuck in local optima.

**First Experiments**: 1) Compare performance with and without UCT warm-start to measure its impact on learning efficiency; 2) Test different network architectures (wider vs deeper) to optimize parameter efficiency; 3) Evaluate training stability across multiple random seeds to assess reproducibility.

## Open Questions the Paper Calls Out
None

## Limitations
- 5x5 board size represents a highly simplified version of Go that may not capture essential strategic complexities
- Performance metrics are specific to Chinese scoring rules and may not generalize to other rule sets
- Statistical significance of performance differences (e.g., AlphaZero's "more aggressive" play) is questionable given the small board size and limited sample space
- Long-term learning potential beyond 120 hours is unexplored, particularly for AlphaZero's plateauing behavior

## Confidence
- **High confidence**: Both methods outperform UCT on 5x5 multiplayer Go
- **Medium confidence**: Comparable final performance between AlphaZero and Descent
- **Medium confidence**: AlphaZero shows more aggressive play as Black

## Next Checks
1. Test scalability by evaluating performance on 7x7 and 9x9 multiplayer Go boards
2. Conduct ablation studies to isolate which components (network architecture, search, self-play) drive performance gains
3. Implement statistical significance testing across multiple training runs to quantify performance differences between methods