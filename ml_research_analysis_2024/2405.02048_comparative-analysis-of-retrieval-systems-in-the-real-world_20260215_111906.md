---
ver: rpa2
title: Comparative Analysis of Retrieval Systems in the Real World
arxiv_id: '2405.02048'
source_url: https://arxiv.org/abs/2405.02048
tags:
- retrieval
- search
- pinecone
- robustqa
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares eight retrieval system configurations using
  the RobustQA metric and response time. Methods evaluated include Azure Cognitive
  Search with GPT-4, Pinecone's Canopy framework, Langchain with Pinecone and various
  language models, LlamaIndex with Weaviate Vector Store, Google and Amazon RAG implementations,
  and a novel graph search approach (Writer Retrieval).
---

# Comparative Analysis of Retrieval Systems in the Real World

## Quick Facts
- arXiv ID: 2405.02048
- Source URL: https://arxiv.org/abs/2405.02048
- Authors: Dmytro Mozolevskyi; Waseem AlShikh
- Reference count: 2
- Primary result: Writer Retrieval achieved highest accuracy (86.31 RobustQA score) with fastest response time (<0.6s)

## Executive Summary
This study evaluates eight different retrieval system configurations using the RobustQA benchmark, measuring both accuracy and response time. The systems span various approaches including traditional RAG implementations with vector stores, hybrid search methods, and a novel graph-based retrieval approach. Results demonstrate significant performance differences across methods, with specialized retrieval-aware approaches outperforming standard vector similarity search. The analysis provides practical insights into the tradeoffs between accuracy and efficiency in real-world retrieval systems.

## Method Summary
The study compares eight retrieval system configurations on the RobustQA benchmark, which contains 31,760 test questions across 9 domains with 13.7M+ documents. Methods evaluated include Azure Cognitive Search with GPT-4, Pinecone's Canopy framework, Langchain with Pinecone and various language models, LlamaIndex with Weaviate Vector Store hybrid search, Google and Amazon RAG implementations, and a novel graph search approach called Writer Retrieval. Each system was evaluated for RobustQA average score and response time latency under consistent testing conditions.

## Key Results
- Writer Retrieval achieved highest accuracy (86.31 RobustQA score) with fastest response time (<0.6s)
- Amazon SageMaker's RAG performed worst (32.74 score, <2.0s)
- LlamaIndex with Weaviate Vector Store's hybrid search ranked second in accuracy
- Traditional vector-based RAG approaches showed significant performance gaps compared to graph-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph search algorithms combined with language models achieve higher accuracy than traditional vector-based retrieval
- Mechanism: Graph search explores relational structures and semantic connections more efficiently than nearest-neighbor vector search in high-dimensional space
- Core assumption: The underlying data has meaningful graph structures that represent relationships between concepts
- Evidence anchors:
  - [abstract] "a novel approach called KG-FID Retrieval" and "graph search algorithm with a language model and retrieval awareness"
  - [section] "Graph search algorithm + LLM + Retrieval awareness (Writer Retrieval) This method outperforms all others in accuracy"
  - [corpus] No direct evidence found - weak signal

### Mechanism 2
- Claim: Retrieval-aware language models improve response accuracy by conditioning generation on retrieved context
- Mechanism: Language models trained or fine-tuned to better utilize retrieved documents produce more accurate responses
- Core assumption: The language model can effectively incorporate and reason over retrieved information
- Evidence anchors:
  - [abstract] "combination of a graph search algorithm with a language model and retrieval awareness"
  - [section] "Graph search algorithm + LLM + Retrieval awareness (Writer Retrieval) This method outperforms all others in accuracy"
  - [corpus] Weak evidence - no direct support found in neighbor papers

### Mechanism 3
- Claim: Hybrid search approaches combining vector similarity with keyword matching improve retrieval accuracy
- Mechanism: Multiple retrieval signals (semantic + lexical) capture different aspects of query intent
- Core assumption: Some queries benefit more from keyword matching while others benefit from semantic similarity
- Evidence anchors:
  - [abstract] "LlamaIndex with Weaviate Vector Store's hybrid search"
  - [section] "LlamaIndex + Weaviate Vector Score - Hybrid Search This method is second best in terms of accuracy"
  - [corpus] No direct evidence found - weak signal

## Foundational Learning

- Concept: Vector space embeddings and similarity search
  - Why needed here: All systems rely on representing documents and queries as vectors for similarity comparison
  - Quick check question: How does cosine similarity between vectors relate to semantic similarity between documents?

- Concept: Retrieval-Augmented Generation (RAG) architecture
  - Why needed here: Most systems combine retrieval with language model generation
  - Quick check question: What are the two main stages in a RAG system and how do they interact?

- Concept: Graph data structures and traversal algorithms
  - Why needed here: The top-performing system uses graph search algorithms
  - Quick check question: What advantages do graph traversal algorithms have over vector similarity search for certain types of queries?

## Architecture Onboarding

- Component map:
  Query processor → Retriever → Context selector → Language model → Response generator
  Alternative paths: Graph traversal → Context selector → Language model → Response generator

- Critical path:
  1. Query parsing and preprocessing
  2. Document retrieval (vector similarity, keyword matching, or graph traversal)
  3. Context selection and ranking
  4. Language model inference with retrieved context
  5. Response post-processing and formatting

- Design tradeoffs:
  - Accuracy vs. response time: More complex retrieval methods improve accuracy but increase latency
  - Vector storage vs. graph storage: Different indexing strategies for different retrieval approaches
  - Model size vs. performance: Larger language models may improve quality but increase computational cost

- Failure signatures:
  - Low RobustQA score with fast response time: Retrieval quality issue
  - High RobustQA score with slow response time: Computation or network bottleneck
  - Inconsistent performance across query types: Need for query-specific retrieval strategies

- First 3 experiments:
  1. Implement a baseline vector similarity retriever using FAISS and measure RobustQA score
  2. Add a reranking step with a cross-encoder model and compare performance gains
  3. Test the same query set across all 8 methods to identify which retrieval approach performs best for which query types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural components or design decisions in the Writer Retrieval system contribute to its superior performance compared to traditional RAG approaches?
- Basis in paper: [explicit] The paper identifies Writer Retrieval as achieving the highest accuracy (86.31 RobustQA score) with fastest response time (<0.6s) compared to other methods
- Why unresolved: The paper mentions that Writer Retrieval combines a graph search algorithm with a language model and retrieval awareness, but does not provide detailed technical specifications of its architecture or implementation
- What evidence would resolve it: Detailed architectural breakdown of Writer Retrieval system, including graph algorithm specifications, integration methods with LLM, and retrieval awareness mechanisms

### Open Question 2
- Question: How do different vector indexing strategies and similarity metrics impact the performance of RAG systems across various domains?
- Basis in paper: [inferred] The paper evaluates multiple systems using different vector stores (Pinecone, Weaviate) and search approaches (hybrid search, vector search) but does not isolate the impact of indexing strategies
- Why unresolved: While the paper compares end-to-end system performance, it does not conduct ablation studies on specific indexing or similarity metric components
- What evidence would resolve it: Systematic comparison of RAG systems using identical language models but different vector indexing strategies and similarity metrics

### Open Question 3
- Question: What is the relationship between corpus size, document complexity, and system performance across different retrieval methods?
- Basis in paper: [explicit] Table 1 shows varying corpus sizes across domains (ranging from 105k documents in Finance to 21M passages in Natural Questions) but performance analysis does not explicitly examine this relationship
- Why unresolved: The paper presents performance metrics but does not analyze how system effectiveness scales with corpus size or document complexity
- What evidence would resolve it: Performance analysis stratified by corpus size ranges and document complexity metrics, examining how different retrieval methods scale

## Limitations
- Lack of transparency around the novel Writer Retrieval method prevents independent verification
- Evaluation focuses exclusively on RobustQA as a single accuracy metric
- Response time measurements may not reflect real-world deployment scenarios with network latency and concurrent loads

## Confidence
- High confidence: Comparative ranking of eight methods based on RobustQA scores and response times
- Medium confidence: Mechanism explanations regarding why graph search outperforms vector similarity
- Low confidence: Generalizability of results to other domains or datasets beyond RobustQA

## Next Checks
1. Replicate the Writer Retrieval method using publicly available documentation and verify performance on RobustQA benchmark
2. Extend evaluation to include additional metrics such as faithfulness scores, hallucination detection, and domain-specific accuracy breakdowns
3. Conduct controlled experiments varying concurrent request loads and network conditions to measure performance degradation under realistic production scenarios