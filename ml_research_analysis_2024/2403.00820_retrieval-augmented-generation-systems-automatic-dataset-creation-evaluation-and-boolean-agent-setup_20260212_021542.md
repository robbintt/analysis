---
ver: rpa2
title: 'Retrieval Augmented Generation Systems: Automatic Dataset Creation, Evaluation
  and Boolean Agent Setup'
arxiv_id: '2403.00820'
source_url: https://arxiv.org/abs/2403.00820
tags:
- articles
- dataset
- evaluation
- answer
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a rigorous dataset creation and evaluation
  workflow for quantitatively comparing Retrieval Augmented Generation (RAG) systems.
  The workflow generates datasets from Wikipedia articles with LLM-generated questions,
  ensuring the articles mostly contain information beyond an LLM's knowledge cutoff
  date.
---

# Retrieval Augmented Generation Systems: Automatic Dataset Creation, Evaluation and Boolean Agent Setup

## Quick Facts
- arXiv ID: 2403.00820
- Source URL: https://arxiv.org/abs/2403.00820
- Authors: Tristan Kenneweg; Philip Kenneweg; Barbara Hammer
- Reference count: 14
- The paper introduces a workflow for creating datasets and evaluating RAG systems, and develops a boolean agent RAG setup that decides whether to query a vector database based on user input.

## Executive Summary
This paper presents a rigorous workflow for creating datasets and automatically evaluating Retrieval Augmented Generation (RAG) systems. The workflow generates datasets from Wikipedia articles using LLM-generated questions, ensuring the articles contain information beyond the LLM's knowledge cutoff date. Automatic evaluation is performed using GPT-4 to assess truthfulness and relevance of generated answers. The workflow is used to develop and evaluate a boolean agent RAG setup, where an LLM decides whether to query a vector database for each user input, aiming to save tokens on simple queries. Results show that while naive RAG performs well, the boolean agent approach can save tokens under certain conditions but may slightly decrease answer quality.

## Method Summary
The method involves creating datasets from Wikipedia articles using GPT-4 to generate questions, with articles filtered to be created after GPT-4's knowledge cutoff date (September 2021). Automatic evaluation is performed using GPT-4's function calling API to assess the truthfulness and relevance of generated answers. A boolean agent RAG setup is implemented where GPT-4 decides whether to query the vector database for each user input. The vector database uses OpenAI's Ada-002 embeddings with recursive chunking. The system is evaluated on token usage and answer quality compared to naive RAG.

## Key Results
- Automatic dataset creation workflow successfully generates questions requiring knowledge beyond GPT-4's cutoff date
- Naive RAG performs well on the created dataset, validating the evaluation approach
- Boolean agent RAG can save tokens under certain conditions but may slightly decrease answer quality
- Automatic evaluation using GPT-4 provides quantitative comparison of different RAG strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can be used to generate questions that require knowledge beyond its own training cutoff date.
- Mechanism: The paper uses GPT-4 to generate questions about Wikipedia articles, but specifically filters articles to be created after GPT-4's knowledge cutoff date (September 2021). This ensures the articles contain information GPT-4 doesn't inherently know, making the questions challenging for RAG evaluation.
- Core assumption: GPT-4's knowledge cutoff date is a hard boundary, and it cannot answer questions about events or information after this date without additional context.
- Evidence anchors:
  - [abstract] "The datasets consist of matching questions and articles, wherein the article contains all information that is necessary to answer the questions. The questions are LLM generated. The articles are curated such that they mostly contain information about events post LLM cutoff point."
  - [section] "To solve this problem we propose an automatic dataset creation workflow that can be used to generate datasets from Wikipedia articles and other sources and is suited for arbitrary LLM cutoff points and automatic evaluation."
  - [corpus] Weak evidence - no direct citation, but the approach is consistent with known GPT-4 knowledge cutoff limitations.
- Break condition: If GPT-4's training data somehow includes information from after its stated cutoff date, or if the filtering process fails to exclude articles containing pre-cutoff information.

### Mechanism 2
- Claim: Automatic evaluation using GPT-4 can reliably assess the truthfulness and relevance of RAG-generated answers.
- Mechanism: The paper uses GPT-4's function calling API to generate scores for truthfulness and relevance of answers. This allows for quantitative comparison of different RAG strategies without manual evaluation.
- Core assumption: GPT-4 can accurately judge the quality of its own responses when provided with the correct context, and this judgment correlates with human evaluation.
- Evidence anchors:
  - [abstract] "Automatic evaluation is performed using GPT-4 to assess the truthfulness and relevance of generated answers."
  - [section] "We follow this previous work in spirit, however we decided to switch to the GPT4 function calling API to generate scores, since this increases the output reliability."
  - [corpus] Weak evidence - while automatic evaluation is a known technique, the specific implementation details and reliability claims are not directly supported by the corpus.
- Break condition: If GPT-4's evaluation becomes inconsistent, or if the function calling API introduces unexpected behavior in the scoring process.

### Mechanism 3
- Claim: Boolean Agent RAG can save tokens by deciding whether to query the vector database for each user input.
- Mechanism: The system uses the LLM to decide if additional information is needed for each query. If the LLM determines it can answer without retrieval, it saves the tokens that would have been spent on database querying and context injection.
- Core assumption: The LLM can accurately assess its own knowledge limitations and make cost-effective decisions about when to use the vector database.
- Evidence anchors:
  - [abstract] "The workflow is used to develop and evaluate a boolean agent RAG setup, where an LLM decides whether to query a vector database for each user input, aiming to save tokens on simple queries."
  - [section] "Boolean agent RAG as we term it extends the naive RAG by a boolean decision step. Given the user input query the LLM decides if it needs to query the vector database in order to answer."
  - [corpus] Weak evidence - the corpus doesn't provide specific studies on token savings from Boolean Agent RAG, only general mentions of RAG systems.
- Break condition: If the LLM consistently misjudges its knowledge limitations, leading to either unnecessary queries or failed answers without retrieval.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) systems
  - Why needed here: Understanding RAG is fundamental to grasping the paper's contributions, as it's the core technology being evaluated and improved upon.
  - Quick check question: What are the main components of a RAG system and how do they interact?

- Concept: Vector databases and embeddings
  - Why needed here: The paper relies heavily on vector databases for information retrieval, and understanding how embeddings work is crucial for comprehending the chunking and querying processes.
  - Quick check question: How do vector embeddings enable efficient similarity search in large text corpora?

- Concept: Large Language Model (LLM) knowledge cutoffs
  - Why needed here: The paper's dataset creation method relies on exploiting the knowledge cutoff of GPT-4, so understanding this concept is essential for grasping the evaluation methodology.
  - Quick check question: What is a knowledge cutoff in the context of LLMs, and why does it matter for RAG systems?

## Architecture Onboarding

- Component map:
  Wikipedia article downloader and filter -> GPT-4 question generator -> Vector database (with OpenAI Ada-002 embeddings) -> Recursive chunking module -> LLM answerer (GPT-4-0613) -> Automatic evaluation module (using GPT-4 function calling) -> Boolean Agent RAG decision module

- Critical path:
  1. Download and filter Wikipedia articles based on creation date
  2. Generate questions using GPT-4
  3. Create vector database with embeddings of article chunks
  4. For each query: decide whether to use RAG or not (for Boolean Agent)
  5. Generate answer (with or without retrieval)
  6. Evaluate answer using GPT-4

- Design tradeoffs:
  - Dataset size vs. quality: Larger datasets provide more robust evaluation but may include more irrelevant or noisy data.
  - Chunk size in vector database: Smaller chunks allow for more precise retrieval but may lose context; larger chunks preserve context but may retrieve irrelevant information.
  - Frequency of RAG usage in Boolean Agent: More frequent use ensures better answers but reduces token savings; less frequent use saves more tokens but may compromise answer quality.

- Failure signatures:
  - Poor answer quality despite RAG: Indicates issues with chunking, embedding, or retrieval relevance.
  - Inconsistent automatic evaluation scores: Suggests problems with the evaluation prompt or GPT-4's reliability.
  - Boolean Agent consistently choosing wrong retrieval strategy: Points to issues in the decision-making prompt or LLM's self-assessment capabilities.

- First 3 experiments:
  1. Run baseline tests on the created dataset without any RAG to establish performance without augmentation.
  2. Implement and test naive RAG on the dataset to measure the improvement from basic retrieval augmentation.
  3. Implement and test the advanced Boolean Agent RAG to compare token usage and answer quality against naive RAG.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BARAG change when using a less powerful LLM for the base answering and decision step, compared to using GPT-4-0613 for all steps?
- Basis in paper: [explicit] The paper suggests using a cheaper LLM like GPT-3.5 for the baseline answering and decision step to save cost, but does not evaluate this approach.
- Why unresolved: The paper only evaluates BARAG using GPT-4-0613 for all steps, and does not compare the performance when using a different LLM for the base answering and decision step.
- What evidence would resolve it: Comparing the performance of BARAG when using GPT-4-0613 for all steps versus using a cheaper LLM like GPT-3.5 for the base answering and decision step, in terms of token usage and answer quality.

### Open Question 2
- Question: What is the impact of prompt engineering on the token efficiency of BARAG, while maintaining comparable answer quality?
- Basis in paper: [explicit] The paper mentions that better prompting techniques might increase the token efficiency of BARAG, while keeping comparable answer quality, but does not explore this.
- Why unresolved: The paper does not experiment with different prompting techniques for BARAG, and only uses a simple function call description.
- What evidence would resolve it: Experimenting with different prompting techniques for BARAG, such as adding more context about the LLM's capabilities or using different temperature settings, and comparing the token usage and answer quality with the current approach.

### Open Question 3
- Question: How does the performance of BARAG vary with different vector database configurations, such as chunk size, embedding method, and distance metric?
- Basis in paper: [explicit] The paper uses a specific vector database configuration for the naive RAG system, which is also used for BARAG, but does not explore how different configurations affect BARAG's performance.
- Why unresolved: The paper does not experiment with different vector database configurations for BARAG, and only uses one configuration based on the naive RAG system.
- What evidence would resolve it: Comparing the performance of BARAG with different vector database configurations, such as varying chunk size, embedding method, and distance metric, in terms of token usage and answer quality.

## Limitations

- The reliance on GPT-4 for both dataset creation and automatic evaluation introduces potential circularity and reliability concerns.
- Lack of human evaluation to validate the automatic evaluation results makes it difficult to assess the true quality of the evaluation method.
- The boolean agent RAG approach may not generalize well to domains outside of Wikipedia articles and factual queries.

## Confidence

**High Confidence:**
- The general approach of using LLM-generated questions for dataset creation is sound and has been validated in related work.
- The concept of using vector databases for information retrieval in RAG systems is well-established and reliable.

**Medium Confidence:**
- The specific implementation of the automatic evaluation using GPT-4's function calling API is likely effective, but without human validation, there's uncertainty about its reliability.
- The token savings from the boolean agent RAG approach are plausible based on the described mechanism, but the actual savings may vary depending on the query distribution and LLM's decision-making accuracy.

**Low Confidence:**
- The claim that the dataset effectively contains information beyond GPT-4's knowledge cutoff date is difficult to verify without extensive manual review.
- The generalizability of the boolean agent RAG approach to other domains and query types is uncertain without further testing.

## Next Checks

1. **Human Evaluation Study**: Conduct a small-scale human evaluation of the automatically generated dataset and the evaluation scores to validate the reliability of the GPT-4-based assessment. This would help establish whether the automatic evaluation correlates with human judgment and identify any systematic biases or errors.

2. **Knowledge Cutoff Verification**: Perform a detailed analysis of the dataset to verify that the questions indeed require information beyond GPT-4's knowledge cutoff. This could involve manually reviewing a sample of questions and answers to ensure they cannot be answered using GPT-4's internal knowledge alone.

3. **Domain Generalization Test**: Apply the boolean agent RAG approach to a different domain (e.g., technical documentation or medical literature) to assess its generalizability and identify any domain-specific challenges or limitations. This would help determine the broader applicability of the approach beyond the Wikipedia-based evaluation.