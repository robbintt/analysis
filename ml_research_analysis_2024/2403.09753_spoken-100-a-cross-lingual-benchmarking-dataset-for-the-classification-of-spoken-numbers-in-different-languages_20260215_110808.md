---
ver: rpa2
title: 'SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification of
  Spoken Numbers in Different Languages'
arxiv_id: '2403.09753'
source_url: https://arxiv.org/abs/2403.09753
tags:
- dataset
- neural
- learning
- data
- spoken-100
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SpokeN-100, a novel speech recognition dataset
  for classifying spoken numbers from 0 to 99 in four languages (English, German,
  French, and Mandarin). The dataset was artificially generated using AI models and
  contains 12,800 audio samples from 32 speakers.
---

# SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification of Spoken Numbers in Different Languages

## Quick Facts
- arXiv ID: 2403.09753
- Source URL: https://arxiv.org/abs/2403.09753
- Reference count: 34
- Primary result: 12.6% accuracy on 100-class spoken number classification with microcontroller-deployable EvoNAS models

## Executive Summary
This paper introduces SpokeN-100, a novel speech recognition dataset for classifying spoken numbers from 0 to 99 in four languages (English, German, French, and Mandarin). The dataset was artificially generated using AI models and contains 12,800 audio samples from 32 speakers. To demonstrate the dataset's utility, the authors conducted experiments on two benchmark tasks: language classification and number classification. They trained several state-of-the-art deep neural networks and performed an evolutionary neural architecture search (EvoNAS) to find tiny architectures optimized for the ARM Cortex-M4 nRF52840 microcontroller.

## Method Summary
The authors created SpokeN-100 by artificially generating audio samples using advanced AI models, ensuring controlled diversity across four languages and 32 speakers. They implemented a preprocessing pipeline that converts raw waveforms to mel-spectrograms and conducted experiments using CNN, RNN, and Transformer architectures. For microcontroller deployment, they performed evolutionary neural architecture search optimizing for accuracy, ROM usage, and energy consumption on the ARM Cortex-M4 nRF52840 platform.

## Key Results
- EvoNAS models achieved competitive results while being deployable on microcontroller (81.3% language classification accuracy)
- Fittest EvoNAS model reached 12.6% accuracy for 100-class number classification with 368ms inference time
- Language classification baseline accuracy of 92.7% achieved with CNN models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EvoNAS successfully finds microcontroller-deployable neural architectures for spoken number classification
- Mechanism: EvoNAS optimizes a multi-objective fitness function balancing accuracy, ROM usage, and energy consumption to evolve tiny architectures
- Core assumption: The search space and objective weights allow evolution of compact models that maintain reasonable accuracy
- Evidence anchors:
  - [abstract] "performed an evolutionary neural architecture search to find tiny architectures optimized for the 32-bit ARM Cortex-M4 nRF52840 microcontroller"
  - [section] "We conducted an evolutionary neural architecture search (EvoNAS)... Each model was successfully deployed on the nRF52840 microcontroller"
- Break condition: If the search space is too constrained or the objective weights favor size/energy too heavily, accuracy could drop below useful levels

### Mechanism 2
- Claim: Artificial generation produces diverse and high-quality speech data across languages and speakers
- Mechanism: Using multiple AI models (LLM for text, generative audio for speech) and speaker diversity creates varied audio samples
- Core assumption: AI-generated speech maintains realistic acoustic properties despite being synthetic
- Evidence anchors:
  - [section] "We analyzed the rolling standard deviation of the audio signal to pinpoint the start and end of the numbers" and "The obtained average fundamental frequency values follow the natural range of human language, which is between 50 and 300 Hz"
  - [abstract] "entirely artificially generated using advanced AI models, ensuring a controlled yet realistic variety in the dataset"
- Break condition: If generation models produce unnatural speech patterns or insufficient speaker variation, classification performance would suffer

### Mechanism 3
- Claim: UMAP dimensionality reduction reveals dataset diversity and complexity for classification tasks
- Mechanism: UMAP preserves local structure while reducing high-dimensional audio features to 2D visualization
- Core assumption: The reduced representation maintains meaningful clustering patterns for analysis
- Evidence anchors:
  - [section] "We used UMAP to gain more insight into the diversity of the SpokeN-100 dataset... reveal distinct clustering of data points according to languages"
  - [abstract] "We determine auditory features and use UMAP... to show the diversity and richness of the dataset"
- Break condition: If UMAP fails to preserve critical distinctions between classes, visual analysis could mislead about dataset complexity

## Foundational Learning

- Concept: Audio preprocessing (STFT, Mel-spectrograms)
  - Why needed here: Transforms raw waveforms into frequency-domain features that capture perceptually relevant information
  - Quick check question: What is the purpose of converting audio to mel-spectrograms before classification?

- Concept: Multi-objective optimization in neural architecture search
  - Why needed here: Balances competing requirements of accuracy, memory usage, and energy efficiency for microcontroller deployment
  - Quick check question: How does EvoNAS balance accuracy with deployment constraints in its fitness function?

- Concept: Cross-validation methodology for speaker-independent evaluation
  - Why needed here: Ensures models generalize to unseen speakers rather than memorizing specific voice characteristics
  - Quick check question: Why does the dataset use speaker-stratified splits instead of random splits?

## Architecture Onboarding

- Component map: Input waveform -> STFT layer -> search space of convolutional/depthwise/dense layers -> TFLite Micro deployment -> on-device inference
- Critical path: Input preprocessing (STFT) -> architecture evolution -> model deployment -> on-device inference time measurement
- Design tradeoffs: Search space constraints limit model complexity but ensure deployability; STFT parameters must balance frequency resolution with memory usage
- Failure signatures: Models that exceed memory limits, inference times over 1 second, or accuracy below 10% (random guessing baseline)
- First 3 experiments:
  1. Test EvoNAS with simplified fitness function (accuracy only) to establish baseline performance
  2. Deploy baseline 2D CNN model to measure current state-of-the-art vs EvoNAS performance gap
  3. Vary STFT parameters (n_fft, hop_length) to find optimal preprocessing for microcontroller constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the dataset's performance metrics change if real human speakers were used instead of AI-generated voices?
- Basis in paper: [inferred] The paper notes that the dataset was entirely artificially generated using advanced AI models and compares this to crowd-sourced data which can have uncontrollable bias due to factors such as background noise.
- Why unresolved: The study only used AI-generated voices, so there's no direct comparison with human-recorded speech.
- What evidence would resolve it: Collecting a version of the dataset with real human speakers and comparing the classification accuracy and other performance metrics with the AI-generated version.

### Open Question 2
- Question: What is the impact of using different sampling rates on the model's accuracy and resource consumption?
- Basis in paper: [explicit] The paper mentions that the audio data was downsampled to 11025Hz for the fundamental frequency calculation and to 8000 data points for the EvoNAS model to fit within the microcontroller's memory constraints.
- Why unresolved: The paper doesn't explore the effect of different sampling rates on the final model performance.
- What evidence would resolve it: Experiments with different sampling rates for both the dataset and model training, measuring the trade-off between accuracy and resource usage.

### Open Question 3
- Question: How would the inclusion of more languages or speakers affect the dataset's diversity and the model's ability to generalize?
- Basis in paper: [explicit] The authors plan to enhance the dataset by releasing a second version featuring a greater variety of speakers and additional languages.
- Why unresolved: The current study only includes four languages and 32 speakers, so the effects of expanding this are unknown.
- What evidence would resolve it: Expanding the dataset with more languages and speakers, then retraining and evaluating the models to see how the performance changes.

### Open Question 4
- Question: What is the optimal trade-off between model accuracy and inference time on resource-constrained devices for this specific task?
- Basis in paper: [explicit] The paper discusses the trade-off between accuracy and inference time, noting that the fastest model had significantly lower accuracy than the fittest model.
- Why unresolved: The study presents results for different models but doesn't explore the full range of possible architectures or optimization techniques.
- What evidence would resolve it: Conducting a more extensive search of neural architectures and optimization techniques, specifically targeting the balance between accuracy and inference time on microcontrollers.

## Limitations

- Artificial generation may not capture all nuances of natural speech patterns
- Extremely low accuracy (12.6%) on 100-class number classification task
- No validation with real human speakers to assess generalization gap

## Confidence

- High Confidence: The dataset creation methodology and basic language classification results
- Medium Confidence: The number classification task performance due to the extremely low baseline and limited evaluation
- Low Confidence: Real-world applicability and generalization to natural speech, as no validation against actual spoken numbers was performed

## Next Checks

1. Test EvoNAS models on a small validation set of naturally recorded spoken numbers (even 100-200 samples) to assess generalization gap
2. Evaluate model performance across different recording conditions (noise levels, distances, microphones) to establish robustness
3. Compare AI-generated speech acoustic properties against natural speech databases (e.g., VCTK, LibriSpeech) to quantify generation fidelity