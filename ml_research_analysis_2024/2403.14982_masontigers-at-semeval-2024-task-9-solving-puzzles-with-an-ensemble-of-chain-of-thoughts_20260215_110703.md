---
ver: rpa2
title: 'MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts'
arxiv_id: '2403.14982'
source_url: https://arxiv.org/abs/2403.14982
tags: []
core_contribution: The paper addresses the challenge of solving complex lateral thinking
  puzzles using large language models (LLMs). The core method employs an ensemble
  of chain-of-thought prompts, which break down the reasoning process into step-by-step
  explanations.
---

# MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts

## Quick Facts
- arXiv ID: 2403.14982
- Source URL: https://arxiv.org/abs/2403.14982
- Reference count: 4
- One-line primary result: Ensemble of chain-of-thought prompts achieves 2nd place in word puzzles and 13th in sentence puzzles at SemEval-2024 Task 9

## Executive Summary
This paper addresses the challenge of solving complex lateral thinking puzzles using large language models (LLMs). The core method employs an ensemble of chain-of-thought prompts, which break down the reasoning process into step-by-step explanations. This approach is applied to proprietary models like GPT-4 and Claude 2.1, as well as the open-source Mixtral model. The experiments show that chain-of-thought prompting significantly improves performance compared to zero-shot and few-shot methods. The ensemble approach, which uses majority voting across multiple prompt chains, achieves the best results, ranking 2nd in the word puzzle subtask and 13th in the sentence puzzle subtask. These results demonstrate the effectiveness of structured reasoning prompts in unlocking the latent capabilities of LLMs for complex problem-solving tasks.

## Method Summary
The method employs an ensemble of chain-of-thought prompts to solve lateral thinking puzzles. The approach breaks down the reasoning process into step-by-step explanations, which are then applied to both proprietary models (GPT-4 and Claude 2.1) and the open-source Mixtral model. The ensemble method uses majority voting across multiple prompt chains to arrive at the final answer. This technique significantly outperforms zero-shot and few-shot prompting methods, demonstrating the power of structured reasoning in complex problem-solving tasks.

## Key Results
- Ensemble of chain-of-thought prompts achieves 2nd place in word puzzle subtask
- Same method ranks 13th in sentence puzzle subtask
- Chain-of-thought prompting significantly outperforms zero-shot and few-shot methods

## Why This Works (Mechanism)
The mechanism behind the success of this approach lies in the structured reasoning process enabled by chain-of-thought prompting. By breaking down complex problems into step-by-step explanations, the LLM can better navigate the intricacies of lateral thinking puzzles. The ensemble approach further enhances this by aggregating multiple reasoning paths, reducing the impact of individual errors and leveraging the collective strength of diverse thought processes.

## Foundational Learning
- Chain-of-thought prompting: Why needed - Enables structured reasoning for complex problems; Quick check - Compare performance with and without CoT on puzzle tasks
- Ensemble methods: Why needed - Aggregates multiple reasoning paths to improve accuracy; Quick check - Analyze performance gain from ensemble vs. single model
- Lateral thinking puzzles: Why needed - Test creative problem-solving skills; Quick check - Evaluate model's ability to handle unconventional problem structures
- Large language models: Why needed - Provide the foundational capability for complex reasoning tasks; Quick check - Compare performance across different LLM architectures

## Architecture Onboarding
Component map: Input Puzzle -> Chain-of-Thought Prompting -> Multiple LLM Evaluations -> Majority Voting -> Final Answer

Critical path: The critical path involves generating multiple chain-of-thought reasoning paths, evaluating each through the LLM, and then applying majority voting to determine the final answer. This process ensures that diverse reasoning approaches are considered and aggregated for the best possible outcome.

Design tradeoffs: The main tradeoff is between the increased computational cost of multiple LLM evaluations and the potential for improved accuracy through ensemble voting. This approach sacrifices efficiency for robustness in problem-solving.

Failure signatures: Potential failures may occur when all reasoning paths in the ensemble lead to incorrect conclusions, or when the majority voting mechanism selects a suboptimal answer due to skewed distribution of responses.

Three first experiments:
1. Compare performance of single chain-of-thought prompt vs. ensemble approach on a subset of puzzles
2. Evaluate the impact of different ensemble sizes on overall accuracy
3. Test the method on a diverse set of reasoning tasks beyond lateral thinking puzzles to assess generalizability

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- Proprietary nature of core models (GPT-4 and Claude 2.1) limits reproducibility and transparency
- Evaluation constrained to specific puzzle format of SemEval-2024 Task 9 dataset, raising questions about generalizability
- Ensemble approach effectiveness relies heavily on majority voting, but study lacks thorough analysis of voting failures

## Confidence
- High confidence: Experimental results showing improved performance of chain-of-thought prompting over zero-shot and few-shot methods
- Medium confidence: Generalizability of ensemble approach to other reasoning tasks beyond specific puzzle format
- Low confidence: Claims about "unlocking latent capabilities" of LLMs without comparative analysis to alternative prompt engineering methods

## Next Checks
1. Conduct ablation studies to isolate the contribution of each component (chain-of-thought, ensemble voting, specific model choices) to overall performance
2. Test the method on diverse reasoning tasks beyond lateral thinking puzzles to assess generalizability
3. Compare the ensemble approach against other state-of-the-art reasoning techniques, including fine-tuned models and alternative prompting strategies, to establish relative effectiveness