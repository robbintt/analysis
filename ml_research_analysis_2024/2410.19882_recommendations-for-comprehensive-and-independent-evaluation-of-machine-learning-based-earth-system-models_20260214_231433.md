---
ver: rpa2
title: Recommendations for Comprehensive and Independent Evaluation of Machine Learning-Based
  Earth System Models
arxiv_id: '2410.19882'
source_url: https://arxiv.org/abs/2410.19882
tags:
- climate
- earth
- ml-based
- system
- esms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper outlines recommendations for comprehensive and independent
  evaluation of machine learning-based Earth System Models (ML-based ESMs). The authors
  propose a five-point framework: (1) leverage existing ESM intercomparison frameworks
  and evaluation tools, (2) implement idealized and simplified tests to evaluate basic
  model behavior, (3) understand ML models'' behavior within the broader ecosystem
  of Earth system modeling tools, (4) develop an extensible evaluation framework accepted
  by the scientific community, and (5) identify a trusted independent party to manage
  regular intercomparison between ML-based and physics-based models.'
---

# Recommendations for Comprehensive and Independent Evaluation of Machine Learning-Based Earth System Models

## Quick Facts
- arXiv ID: 2410.19882
- Source URL: https://arxiv.org/abs/2410.19882
- Reference count: 0
- Primary result: A five-point framework for evaluating ML-based ESMs using existing tools, idealized tests, and independent intercomparison

## Executive Summary
This paper presents recommendations for comprehensive evaluation of machine learning-based Earth System Models (ML-based ESMs), addressing the critical need for standardized assessment as these models emerge as potential complements to traditional physics-based approaches. The authors propose a framework that leverages existing ESM evaluation infrastructure while introducing new methodologies specifically designed for ML models. They emphasize the importance of moving beyond historical observation comparison to evaluate models' physical consistency and ability to generalize to out-of-sample conditions. The paper demonstrates that existing benchmarking packages can be readily adapted for ML-based ESM evaluation and provides concrete examples of idealized test cases that reveal fundamental physical behavior.

## Method Summary
The paper proposes a five-point framework for evaluating ML-based ESMs: (1) leverage existing ESM intercomparison frameworks and evaluation tools, (2) implement idealized and simplified tests to evaluate basic model behavior, (3) understand ML models' behavior within the broader ecosystem of Earth system modeling tools, (4) develop an extensible evaluation framework accepted by the scientific community, and (5) identify a trusted independent party to manage regular intercomparison between ML-based and physics-based models. The authors demonstrate adaptation of existing benchmarking packages (PCMDI Metrics Package, ILAMB, IOMB) for ML model evaluation and present results from idealized tests using the Spherical Fourier Neural Operators model, including a moist baroclinic wave test that reveals deviations from physics-based model behavior.

## Key Results
- Existing ESM evaluation tools (PMP, ILAMB, IOMB) can be readily adapted for ML-based ESM evaluation with minimal modification
- Idealized test cases like baroclinic instability tests reveal fundamental physical consistency issues in ML models, even when trained on historical data
- Regular intercomparison managed by independent parties creates accountability and prevents cherry-picking of favorable metrics
- ML models must be evaluated on their ability to generalize beyond training data and represent future coupled states without historical observations
- A concrete evaluation menu is provided to standardize assessment practices across the community

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Existing ESM evaluation tools can be directly adapted for ML-based ESM evaluation
- Mechanism: Many established ESM benchmarking packages already contain standardized metrics, diagnostics, and evaluation frameworks that can be applied to ML models with minimal modification
- Core assumption: ML models can produce outputs in the same format as traditional ESMs, making them compatible with existing evaluation tools
- Evidence anchors:
  - [abstract] "existing benchmarking packages can be readily adapted for ML-based ESM evaluation"
  - [section 2] "Use of these evaluation tools would allow ML model developers to draw on decades of evaluation experience with physics-based ESMs"
  - [corpus] Weak evidence - corpus contains related papers on ML climate models but no direct evidence about adaptation of existing tools
- Break condition: ML models produce outputs in incompatible formats or require fundamentally different evaluation metrics than traditional ESMs

### Mechanism 2
- Claim: Simplified tests reveal fundamental physical consistency issues in ML models
- Mechanism: Idealized test cases (like baroclinic instability tests) expose whether ML models respect conservation laws and produce physically realistic emergent behavior, even when trained on historical data
- Core assumption: Physical consistency can be tested independently of model accuracy against observations
- Evidence anchors:
  - [abstract] "the model must represent the alternate (e.g., future) coupled states of the system for which there are no historical observations"
  - [section 4] "These tests exercise a specific model's ability to simulate physics using initial states that are well outside the model's training dataset"
  - [section 4] "Results from this test are depicted in Figure 2. While the model does produce a Rossby wave, the test immediately reveals an important, albeit expected, deviation"
- Break condition: ML models fail to maintain stability or produce physically meaningful results even in simplified test cases

### Mechanism 3
- Claim: Intercomparison frameworks create accountability and standardization
- Mechanism: Regular intercomparison managed by independent parties ensures all ML models are evaluated using the same metrics, preventing cherry-picking and building community trust
- Core assumption: The scientific community will adopt and maintain standardized evaluation protocols
- Evidence anchors:
  - [abstract] "five recommendations to enhance comprehensive, standardized, and independent evaluation of ML-based ESMs"
  - [section 6] "There is no doubt that the significant successes in ESM development over the past four decades can be attributed to regular intercomparison"
  - [section 6] "The objective would be to provide a buffer against model development groups cherry-picking metrics that show good performance"
- Break condition: Community fails to adopt standardized protocols or independent parties lack credibility/trust

## Foundational Learning

- Concept: Conservation laws in Earth system modeling
  - Why needed here: Understanding how conservation of mass, momentum, and energy should manifest in model outputs is essential for evaluating physical consistency
  - Quick check question: What are the three fundamental conservation laws that should be respected by any Earth system model?

- Concept: Hierarchical model evaluation
  - Why needed here: The paper advocates starting with simplified tests before moving to complex coupled simulations - understanding this hierarchy is crucial for proper evaluation
  - Quick check question: Why is it important to test atmospheric models with moisture disabled before adding complex physics?

- Concept: Intercomparison project methodology
  - Why needed here: The paper recommends building on CMIP and other intercomparison experiences - understanding how these projects work is essential
  - Quick check question: What are the key components of a successful model intercomparison project?

## Architecture Onboarding

- Component map: Existing benchmarking packages (PMP, ILAMB, IOMB, etc.) -> Standardization pipeline -> Evaluation metrics -> Intercomparison database -> Community review
- Critical path: ML model output -> standardization -> evaluation package processing -> metric computation -> intercomparison database -> community review
- Design tradeoffs: Comprehensive evaluation vs. computational cost; standardized metrics vs. model-specific needs; historical comparison vs. physical consistency testing
- Failure signatures: Inconsistent results across evaluation packages; failure of stability in simplified tests; inability to generalize beyond training data; lack of physical consistency in emergent behavior
- First 3 experiments:
  1. Run a pre-trained ML-based ESM through the PCMDI Metrics Package and compare results with traditional ESMs
  2. Implement the baroclinic instability test case from Jablonowski and Williamson (2006) with the ML model
  3. Compare the ML model's output against its parent physics-based model for a simple climate sensitivity experiment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop standardized evaluation metrics that capture both the physical consistency and out-of-sample generalization capabilities of ML-based ESMs without relying on historical observations?
- Basis in paper: [explicit] The paper emphasizes the need for "more creativity in devising evaluation strategies" and highlights that "the majority of the metrics and diagnostics from these packages focus on comparing against historical observations" despite the limitations of historical data for evaluating future climate states.
- Why unresolved: Current benchmarking tools primarily compare models to historical observations, but ML-based ESMs need to demonstrate capabilities beyond historical regimes. The paper identifies this as a core challenge but doesn't provide a definitive solution.
- What evidence would resolve it: Development and validation of evaluation frameworks that successfully assess ML-based ESMs using idealized tests, emergent constraints, and intercomparison with physics-based models, demonstrating predictive skill for out-of-sample conditions.

### Open Question 2
- Question: What organizational structure and governance model would be most effective for managing regular intercomparison of ML-based and physics-based models while ensuring transparency, independence, and community buy-in?
- Basis in paper: [explicit] The authors recommend "identifying a trusted independent party to manage regular intercomparison of ML-based and physics-based models" but acknowledge that "frameworks for intercomparison require agreement and investment from all involved."
- Why unresolved: The paper discusses the need for such an organization but doesn't specify what form it should take or how to establish it, given the different cultures and incentives between the ML and traditional climate modeling communities.
- What evidence would resolve it: A successful implementation of an independent evaluation framework with demonstrated participation from both ML and traditional ESM development groups, showing measurable improvements in model quality and community trust.

### Open Question 3
- Question: To what extent can idealized test cases (like the baroclinic instability test) effectively reveal the physical consistency of ML-based ESMs when these tests are far outside the training distribution?
- Basis in paper: [explicit] The paper presents results from a moist baroclinic wave test using the SFNO model and notes that "the test immediately reveals an important, albeit expected, deviation of SFNO from the behavior of physics-based models" but questions remain about the generalizability of such tests.
- Why unresolved: While the paper demonstrates that idealized tests can reveal differences between ML and physics-based models, it's unclear how predictive these differences are for real-world climate simulation performance and whether alternative idealized tests might be more informative.
- What evidence would resolve it: Systematic comparison of ML-based ESM performance on idealized tests versus their performance on real-world climate simulations, identifying which idealized tests best predict real-world capabilities.

## Limitations

- The scalability of existing ESM evaluation tools to ML-based models remains untested at production scale, with unknown computational overhead
- The selection and definition of "trusted independent parties" for intercomparison management lacks specificity and concrete criteria
- The proposed evaluation framework assumes ML models can produce outputs compatible with existing diagnostic tools, which may not hold for models with different temporal or spatial discretizations

## Confidence

**High Confidence**: The recommendation to leverage existing ESM intercomparison frameworks (Recommendation 1) is well-supported by the demonstrated compatibility of evaluation tools with ML models and the proven track record of CMIP and similar projects.

**Medium Confidence**: The proposed five-point framework represents a reasonable starting point, though its practical effectiveness depends on community adoption and the actual performance of ML models in the suggested test suites.

**Low Confidence**: The identification of specific independent management structures and the scalability of proposed evaluation pipelines to operational use cases remain speculative without concrete implementation examples.

## Next Checks

1. **Tool Adaptation Stress Test**: Apply the full suite of recommended evaluation tools (PMP, ILAMB, IOMB) to three diverse ML-based ESMs with varying architectures and temporal resolutions, documenting computational costs and any adaptation requirements.

2. **Community Adoption Survey**: Conduct a survey of ESM developers and climate scientists to assess willingness to adopt standardized ML evaluation protocols and identify potential barriers to implementation.

3. **Independent Management Prototype**: Design and test a prototype governance structure for ML ESM intercomparison, including conflict resolution procedures and data sharing policies, using a small-scale pilot comparison between two ML models.