---
ver: rpa2
title: Cognitive Evolutionary Learning to Select Feature Interactions for Recommender
  Systems
arxiv_id: '2405.18708'
source_url: https://arxiv.org/abs/2405.18708
tags:
- feature
- interactions
- features
- cell
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a nature-inspired feature interaction selection
  approach named Cognitive Evolutionary Learning (CELL). CELL can adaptively evolve
  into different models for different tasks and data, so as to build the architecture
  under task guidance.
---

# Cognitive Evolutionary Learning to Select Feature Interactions for Recommender Systems

## Quick Facts
- arXiv ID: 2405.18708
- Source URL: https://arxiv.org/abs/2405.18708
- Reference count: 40
- Primary result: CELL significantly outperforms state-of-the-art baselines on four real-world datasets

## Executive Summary
This paper proposes a nature-inspired feature interaction selection approach named Cognitive Evolutionary Learning (CELL) for recommender systems. The framework treats feature interactions as biological genomes and operations as DNA linkages, using evolutionary principles to adaptively select the most relevant features and interactions. CELL employs three stages: DNA search for operation selection via continuous relaxation, genome search for feature/interaction relevance via regularized dual averaging, and model functioning for final MLP-based prediction. Extensive experiments on four real-world datasets demonstrate CELL's superiority over state-of-the-art methods in terms of AUC and Logloss.

## Method Summary
CELL is a three-stage framework for feature interaction selection in recommender systems. First, DNA search uses differentiable architecture search with softmax relaxation to select the best operation (element-wise sum, product, concatenation with feed-forward, or product with feed-forward) for each feature pair. Second, genome search optimizes relevance fitness parameters via regularized dual averaging (RDA) to identify and prune irrelevant features and interactions, with a mutation mechanism to escape local optima. Finally, model functioning trains an MLP on the selected feature interactions using relevance fitness as attention weights. The framework is trained using bilevel optimization, where the upper level optimizes operation selection and relevance fitness, while the lower level trains the MLP parameters.

## Key Results
- CELL significantly outperforms 14 state-of-the-art baselines (LR, FM, AFM, FFM, Wide&Deep, Deep&Cross, NFM, DeepFM, IPNN, OPNN, xDeepFM, AutoInt, AutoGroup, AutoFIS-FM, AutoFIS-IPNN) on four real-world datasets in terms of AUC and Logloss
- Synthetic experiments confirm CELL can consistently discover pre-defined interaction patterns for feature pairs
- CELL demonstrates superior interpretability through visualization of evolution paths, showing how the model selects optimal operations, features, and interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive operation selection improves model fitness by allowing different feature pairs to use task-optimal operations.
- Mechanism: The framework treats feature pairs as "nucleotides" and operations as "linkages," evolving toward the fittest operation per pair via a continuous relaxation of the search space.
- Core assumption: The fitness of a model increases monotonically with the probability that each feature pair is interacted using the best operation.
- Evidence anchors:
  - [abstract] "interactions of feature pairs can be analogous to double-stranded DNA, of which relevant features and interactions can be analogous to genomes"
  - [section] "We adopt a mixed interaction given by a softmax over all possible operations... DNA search can optimize θθθ via the difference between the linkage responses and true labels"
  - [corpus] Weak: No direct citations, but the analogy to evolutionary biology is unique and central to the proposed approach.
- Break condition: If the continuous relaxation no longer reflects discrete operation choice fidelity, or if fitness gradients become unstable across feature pairs.

### Mechanism 2
- Claim: Relevance fitness parameters allow the model to distinguish important features and interactions, suppressing noise.
- Mechanism: During genome search, relevance fitness values ααα and βββ are optimized via RDA, which truncates weights toward zero, effectively removing irrelevant features/interactions.
- Core assumption: The model's predictive loss increases when removing a relevant feature/interaction and decreases when removing an irrelevant one.
- Evidence anchors:
  - [abstract] "useless features and interactions can bring unnecessary noise and complicate the training process"
  - [section] "Let ααα, βββ satisfy the following property: Proposition IV.2... |αi| > |αj| when L(M(fi, w/o(fj))) < L(M(w/o(fi), fj))"
  - [corpus] Weak: No explicit empirical support in the corpus for RDA-based sparsity in feature interaction selection.
- Break condition: If relevance fitness parameters do not converge to sparse solutions, or if the mutation threshold λ is set too high/low.

### Mechanism 3
- Claim: The mutation mechanism introduces exploration to avoid local optima in the operation selection space.
- Mechanism: When the relevance fitness of an interaction drops below threshold λ, the operation mutates to another random operation with probability 1/σ.
- Core assumption: Mutation increases genetic diversity, enabling the search to escape suboptimal local peaks in the fitness landscape.
- Evidence anchors:
  - [abstract] "The mutation mechanism can avoid the search results based on DNA search reaching a suboptimum and benefit genetic diversity"
  - [section] "Mutation probabilistically occurs when the relevance fitness drops to a threshold, resulting in the operation of the interaction mutating into other operations"
  - [corpus] Weak: No empirical mutation rate studies cited in the corpus.
- Break condition: If mutation frequency is too high, causing instability; or too low, causing premature convergence.

## Foundational Learning

- Concept: Gradient-based continuous relaxation of discrete search
  - Why needed here: Enables efficient optimization over a combinatorial space of operation choices without exhaustive enumeration
  - Quick check question: How does the softmax over operations allow differentiable training of the operation selection?

- Concept: Bilevel optimization for architecture search
  - Why needed here: Separates the optimization of operation weights (upper level) from the learning of model parameters (lower level)
  - Quick check question: What is the role of the validation loss in the upper-level optimization of θθθ?

- Concept: Sparsity-inducing optimizers (RDA)
  - Why needed here: Automatically identifies and removes irrelevant features/interactions by driving their relevance fitness toward zero
  - Quick check question: Why does RDA's truncation mechanism favor sparse relevance fitness parameters compared to standard Adam?

## Architecture Onboarding

- Component map:
  - DNA Search -> Genome Search -> Model Functioning
- Critical path:
  1. Initialize continuous fitness θθθ, relevance fitness ααα, βββ
  2. DNA Search → select best operation per feature pair
  3. Genome Search → prune irrelevant features/interactions, mutate operations if needed
  4. Model Functioning → retrain MLP with pruned set
- Design tradeoffs:
  - Continuous relaxation vs. discrete search: efficiency vs. exactness
  - RDA sparsity vs. Adam smoothness: automatic pruning vs. stable convergence
  - Mutation probability vs. exploration: balance stability and diversity
- Failure signatures:
  - Unstable gradients in DNA Search → check learning rate ξ and softmax temperature
  - Relevance fitness not sparse → adjust RDA hyperparameters c, µ
  - Over-mutation → reduce σ or increase λ threshold
- First 3 experiments:
  1. Baseline: Run CELL on a small dataset (e.g., Avazu) with default hyperparameters; verify AUC improvement over FM.
  2. Ablation: Run CELL (-DNA) to confirm operation selection matters; compare to full CELL.
  3. Mutation sensitivity: Vary λ and σ; plot AUC vs. mutation frequency to find sweet spot.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed CELL framework perform when the operation set is expanded beyond the four basic operations (element-wise sum, element-wise product, concatenation with feed-forward layer, and element-wise product with feed-forward layer)?
- Basis in paper: [explicit] The paper discusses the scalability of CELL when the operation set is expanded, stating that the time complexity varies significantly with the number of feature fields but not much with the size of the operation set.
- Why unresolved: The paper does not provide empirical results or a detailed analysis of CELL's performance with an expanded operation set.
- What evidence would resolve it: Conducting experiments with a larger and more diverse operation set, and comparing the performance of CELL with other state-of-the-art models on various datasets.

### Open Question 2
- Question: How does the CELL framework compare to other feature interaction selection methods in terms of interpretability and the ability to provide insights into the model's decision-making process?
- Basis in paper: [explicit] The paper highlights the interpretability of CELL by visualizing the evolution paths of DNA search and genome search, and claims that it can enhance interpretability by revealing the learning abilities of models during training.
- Why unresolved: The paper does not provide a comprehensive comparison of CELL's interpretability with other feature interaction selection methods or quantify the interpretability gains.
- What evidence would resolve it: Conducting a thorough comparison of CELL's interpretability with other methods, including qualitative and quantitative measures, and demonstrating the practical benefits of improved interpretability in real-world applications.

### Open Question 3
- Question: How does the CELL framework perform on other types of data, such as text, image, or graph data, and how can it be adapted to handle different data modalities?
- Basis in paper: [inferred] The paper focuses on feature interaction selection for recommender systems, which typically involve tabular data. However, the CELL framework's underlying principles and techniques could potentially be applied to other data types.
- Why unresolved: The paper does not explore the application of CELL to other data modalities or discuss the necessary modifications to handle different types of data.
- What evidence would resolve it: Extending the CELL framework to handle different data modalities, conducting experiments on various datasets, and comparing the performance of CELL with other methods designed for specific data types.

## Limitations
- The paper's claims about adaptive operation selection, relevance fitness-driven sparsity, and mutation-induced exploration are supported by theoretical analogies to evolutionary biology but lack direct empirical validation in the corpus.
- The softmax relaxation and RDA-based pruning are well-grounded in optimization literature, yet their effectiveness specifically for feature interaction selection in recommender systems remains weakly substantiated.
- The mutation mechanism is described but not empirically tuned or analyzed for its impact on avoiding local optima.

## Confidence
- Claims about continuous relaxation and bilevel optimization: Medium
- Claims about RDA-based sparsity: Medium
- Claims about mutation mechanism: Low
- Claims about interpretability: Low

## Next Checks
1. Conduct ablation studies removing DNA search and genome search components to quantify their individual contributions to model performance.
2. Perform sensitivity analysis on mutation hyperparameters (λ, σ) to determine their impact on convergence and final model fitness.
3. Compare CELL's efficiency and final architecture quality against other differentiable architecture search methods on a held-out validation set.