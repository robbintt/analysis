---
ver: rpa2
title: 'Continual Learning with Pre-Trained Models: A Survey'
arxiv_id: '2401.16386'
source_url: https://arxiv.org/abs/2401.16386
tags:
- learning
- prompt
- continual
- methods
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides the first comprehensive survey of pre-trained
  model-based continual learning (PTM-CIL), categorizing existing methods into three
  groups: prompt-based, representation-based, and model mixture-based approaches.
  The survey analyzes their similarities, differences, and advantages/disadvantages,
  and includes an empirical study comparing representative methods across seven benchmark
  datasets.'
---

# Continual Learning with Pre-Trained Models: A Survey

## Quick Facts
- arXiv ID: 2401.16386
- Source URL: https://arxiv.org/abs/2401.16386
- Reference count: 16
- First comprehensive survey of pre-trained model-based continual learning (PTM-CIL)

## Executive Summary
This survey provides the first comprehensive overview of pre-trained model-based continual learning (PTM-CIL), categorizing existing methods into prompt-based, representation-based, and model mixture-based approaches. The paper identifies a critical fairness issue in current evaluations due to batch-wise prompt selection strategies that give certain methods unfair advantages. Through empirical studies across seven benchmark datasets, the authors find that representation-based methods generally outperform others, and that a simple prototype-based baseline (SimpleCIL) often outperforms more complex prompt-based methods. The survey also outlines important future directions including continual learning with large language models, resource-constrained scenarios, and developing new challenging benchmarks.

## Method Summary
The paper systematically categorizes PTM-CIL methods into three main groups: prompt-based methods that update or design prompts for each task, representation-based methods that learn task-specific representations, and model mixture-based approaches that combine multiple pre-trained models. The survey analyzes the similarities and differences between these approaches, highlighting their respective advantages and disadvantages. An empirical study compares representative methods across seven benchmark datasets, revealing significant performance variations and identifying methodological inconsistencies in evaluation protocols, particularly regarding prompt selection strategies.

## Key Results
- Representation-based methods (like ADAM and RanPAC) generally outperform prompt-based and model mixture-based approaches
- SimpleCIL, a simple prototype-based baseline, often outperforms more complex prompt-based methods
- Critical fairness issue identified in comparisons due to batch-wise prompt selection giving some methods unfair advantages

## Why This Works (Mechanism)
The effectiveness of PTM-CIL methods stems from leveraging pre-trained models that have already learned rich feature representations from large-scale datasets. By building upon these pre-trained foundations rather than learning from scratch, continual learning methods can focus on adapting to new tasks while mitigating catastrophic forgetting. Prompt-based methods work by adjusting the model's behavior through prompt engineering, representation-based methods focus on learning task-specific feature spaces, and model mixture approaches combine strengths of multiple models. The survey reveals that methods maintaining robust representations across tasks (representation-based) tend to be more effective than those relying solely on prompt adaptation.

## Foundational Learning
**Catastrophic Forgetting**: The phenomenon where neural networks forget previously learned tasks when trained on new ones. *Why needed*: Central problem PTM-CIL aims to solve. *Quick check*: Does the method explicitly address forgetting through regularization or memory mechanisms?

**Prompt Engineering**: Techniques for designing or optimizing input prompts to guide model behavior. *Why needed*: Core mechanism for many PTM-CIL methods, especially in vision and language tasks. *Quick check*: Are prompts task-specific or shared across tasks?

**Representation Learning**: Methods for learning task-specific feature representations while preserving general knowledge. *Why needed*: Enables adaptation to new tasks without overwriting existing knowledge. *Quick check*: Does the method maintain a shared representation space or task-specific subspaces?

**Batch-wise Processing**: Training on data in sequential batches rather than as a single stream. *Why needed*: Common in practical continual learning scenarios but creates evaluation fairness issues. *Quick check*: Does the method's evaluation protocol account for batch-level information leakage?

## Architecture Onboarding

**Component Map**: Pre-trained Model -> Prompt/Representation Layer -> Task Head -> Output, or Pre-trained Model -> Model Mixture Module -> Task-specific Heads

**Critical Path**: Pre-trained model features extraction -> Task-specific adaptation (prompt/representation) -> Classification/Prediction

**Design Tradeoffs**: 
- Prompt-based: High flexibility but prone to prompt-level forgetting and fairness issues
- Representation-based: Better stability and performance but may require more computational resources
- Model mixture: Can leverage multiple expertise sources but increases complexity and memory requirements

**Failure Signatures**:
- Performance degradation on earlier tasks (forgetting)
- Suboptimal adaptation due to poor prompt selection
- Computational bottlenecks in resource-constrained scenarios
- Evaluation bias from batch-wise processing advantages

**3 First Experiments**:
1. Compare a prompt-based method against SimpleCIL on a standard vision benchmark with fair evaluation protocol
2. Test representation-based method's performance on out-of-distribution data to assess generalization
3. Evaluate model mixture approach's computational efficiency versus performance gains on resource-constrained hardware

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can continual learning benchmarks be designed to effectively evaluate PTM-based methods on data that is truly novel and outside the pre-training distribution?
- Basis in paper: [explicit] The authors identify this as a key future direction, noting that PTMs rarely encounter unfamiliar information due to extensive pre-training datasets, and that current benchmarks like ImageNet-R/A, ObjectNet, OmniBenchmark, and VTAB may become insufficient as training techniques evolve.
- Why unresolved: Existing benchmarks either have insufficient domain gap from pre-training data or may become saturated as PTMs improve. There is a need for datasets that consistently present truly novel information to PTMs.
- What evidence would resolve it: Development and validation of new benchmark datasets that maintain significant domain gaps from pre-training data while covering diverse real-world scenarios that PTMs have not encountered.

### Open Question 2
- Question: What is the optimal approach to prompt selection in prompt-based continual learning methods to avoid prompt-level forgetting while maintaining computational efficiency?
- Basis in paper: [explicit] The authors identify prompt selection as the "bottleneck in continual learning," noting that hard matching processes like key-query matching can lead to matching-level and prompt-level forgetting, and that current solutions have trade-offs between representation ability and fairness.
- Why unresolved: Current prompt selection methods either suffer from forgetting (hard matching) or have limitations in representation capacity (fixed-size pools). The trade-off between performance, fairness, and efficiency remains unresolved.
- What evidence would resolve it: Comparative studies demonstrating prompt selection methods that achieve high performance without forgetting, while being computationally efficient and fair across different tasks.

### Open Question 3
- Question: How can continual learning methods be designed to work effectively under strict computational resource constraints while maintaining performance comparable to resource-intensive approaches?
- Basis in paper: [explicit] The authors identify this as a future direction, noting that PTM-based CL often incurs significant computational costs, and that edge device deployment requires computationally efficient algorithms.
- Why unresolved: Most current PTM-based CL methods require substantial computational resources for training and inference. There is a gap between theoretical performance and practical deployability in resource-constrained environments.
- What evidence would resolve it: Empirical demonstrations of resource-constrained continual learning methods achieving performance within acceptable margins of state-of-the-art methods, while operating within strict computational budgets.

## Limitations
- Significant heterogeneity in evaluation protocols across different PTM-CIL methods, particularly batch-wise prompt selection creating unfair advantages
- Results based on seven benchmark datasets may not generalize to all real-world scenarios, especially non-vision tasks
- Methodological inconsistencies in comparison protocols may skew reported performance rankings

## Confidence

**Empirical Performance Claims (Medium)**: Based on seven benchmark datasets with sound methodology, but generalizability to other domains requires further validation.

**SimpleCIL Baseline Claims (Medium)**: Compelling observation that may be influenced by specific evaluation protocols; different benchmarks could yield different results.

**Future Direction Claims (Low)**: Predictions about large language models and resource-constrained scenarios are speculative and depend on uncertain technological developments.

## Next Checks

1. Replicate the empirical study using a standardized evaluation protocol across all methods, particularly addressing the batch-wise prompt selection issue to ensure fair comparisons.

2. Extend the empirical evaluation to include non-vision tasks (text, audio, multimodal) and diverse domain shifts to assess the generalizability of the reported performance rankings.

3. Conduct ablation studies to isolate the impact of specific methodological choices (e.g., prompt selection strategy, representation learning techniques) on final performance to better understand the source of observed advantages.