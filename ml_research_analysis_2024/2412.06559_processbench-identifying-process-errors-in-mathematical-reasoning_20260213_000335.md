---
ver: rpa2
title: 'ProcessBench: Identifying Process Errors in Mathematical Reasoning'
arxiv_id: '2412.06559'
source_url: https://arxiv.org/abs/2412.06559
tags:
- process
- qwen2
- circ
- math
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProcessBench, a benchmark for measuring the
  ability to identify erroneous steps in mathematical reasoning. It consists of 3,400
  test cases from competition- and Olympiad-level math problems, with step-by-step
  solutions annotated by human experts to indicate error locations.
---

# ProcessBench: Identifying Process Errors in Mathematical Reasoning

## Quick Facts
- arXiv ID: 2412.06559
- Source URL: https://arxiv.org/abs/2412.06559
- Reference count: 16
- Primary result: Existing process reward models (PRMs) struggle to generalize on challenging math problems and underperform critic models

## Executive Summary
This paper introduces ProcessBench, a benchmark for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases from competition- and Olympiad-level math problems, with step-by-step solutions annotated by human experts to indicate error locations. Models are tasked with identifying the earliest erroneous step or confirming all steps are correct. The evaluation involves two types of models: process reward models (PRMs) and critic models (prompted language models). Results show that existing PRMs struggle to generalize to challenging problems and underperform critic models, raising concerns about their data synthesis methodologies.

## Method Summary
The benchmark uses four datasets (GSM8K, MATH, OlympiadBench, Omni-MATH) with model-generated solutions that are standardized through reformatting. Human experts annotate error locations with strict agreement requirements. Two evaluation approaches are used: PRMs that assess step correctness through scalar scores, and critic models that are prompted to critique each solution step-by-step. Performance is measured using accuracy on erroneous/correct samples and F1 score.

## Key Results
- PRMs significantly underperform critic models on challenging problems
- Within the same model family, error identification performance scales with model size
- QwQ-32B-Preview (open-source) performs competitively with GPT-4o but lags behind o1-mini

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Error identification performance scales with model size within the same family
- Mechanism: Larger models have more capacity to maintain complex reasoning chains and detect subtle errors across multiple steps
- Core assumption: Model capacity directly correlates with ability to track multi-step mathematical reasoning
- Evidence anchors:
  - [abstract] "Within the same model family, the error identification performance favorably scales with increased model sizes"
  - [section] "we observe a consistent performance decline for all the models" when difficulty increases, suggesting larger models handle complexity better
- Break condition: If model scaling hits computational or data quality bottlenecks that prevent further improvement

### Mechanism 2
- Claim: General language models outperform specialized PRMs for error identification due to separate reasoning processes
- Mechanism: When prompted as critics, models can "think" independently about each step rather than being constrained by the solution generation process
- Core assumption: Independent critique allows better error detection than process supervision trained on the same data distribution
- Evidence anchors:
  - [abstract] "critic models... prompt general language models to critique each solution step by step"
  - [section] "Despite recent growing interest, existing PRMs typically underperform the top prompt-driven critic models"
- Break condition: If the independent reasoning introduces noise or if PRMs are retrained on more diverse data

### Mechanism 3
- Claim: Open-source models achieve competitive performance with proprietary models but still lag behind reasoning-specialized models
- Mechanism: General-purpose large models can perform well on error identification tasks but lack the specialized training for mathematical reasoning
- Core assumption: Mathematical reasoning requires specific training patterns not present in general language models
- Evidence anchors:
  - [abstract] "The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini"
  - [section] Performance comparison shows QwQ-32B-Preview at 71.5% vs o1-mini at 87.9% F1 score
- Break condition: If specialized training proves less important than model scale or if open-source models receive reasoning-specific fine-tuning

## Foundational Learning

- Concept: Process supervision vs outcome supervision
  - Why needed here: The paper contrasts PRMs (process supervision) with critic models (outcome-based critique), showing fundamental differences in approach
  - Quick check question: What is the key difference between training a model to evaluate intermediate steps versus having it critique a complete solution?

- Concept: Generalization across difficulty levels
  - Why needed here: All models show performance decline as problems become more challenging, indicating limitations in current approaches
  - Quick check question: How does the performance drop from GSM8K/MATH to OlympiadBench/Omni-MATH help us understand the limitations of current error identification methods?

- Concept: Annotation agreement and data quality
  - Why needed here: The benchmark uses multiple annotators and discards low-agreement samples, directly impacting data reliability
  - Quick check question: What does the increase in required annotators for challenging problems tell us about the inherent difficulty of error identification in mathematical reasoning?

## Architecture Onboarding

- Component map: Problem dataset -> Solution generation -> Reformatting -> Human annotation -> Model evaluation -> Performance analysis
- Critical path: Problem → Solution generation → Reformatting → Human annotation → Model evaluation → Performance analysis
- Design tradeoffs:
  - Solution diversity vs annotation consistency: Using multiple generators creates varied solution styles but requires careful reformatting
  - Expert annotation cost vs data quality: High discard rate ensures quality but reduces dataset size
  - PRM training methodology vs generalization: On-policy training limits PRM effectiveness on challenging problems
- Failure signatures:
  - Low F1 scores indicating inability to balance error detection with correct solution recognition
  - High disagreement among annotators suggesting ambiguous error definitions
  - Performance drops on Olympiad-level problems indicating scaling limitations
  - PRMs underperforming critic models suggesting methodology limitations
- First 3 experiments:
  1. Compare PRM performance using different threshold selection methods (per-subset vs global) to verify if threshold optimization explains performance differences
  2. Test critic models with and without solution reformatting to measure impact of step granularity standardization
  3. Evaluate model scaling within families (7B→14B→32B→72B) to confirm size-performance correlation across all model types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for training process reward models (PRMs) to achieve better generalization on challenging math problems?
- Basis in paper: [explicit] The paper notes that existing PRMs underperform both critic models and a PRM trained on PRM800K, raising concerns about the generalization abilities and scalability of current data synthesis methodologies.
- Why unresolved: The paper suggests that current PRM training methodologies rely on estimating the empirical probability of steps leading to correct final answers, which may not effectively indicate the correctness of reasoning steps generated by different models.
- What evidence would resolve it: Comparative studies of different PRM training approaches on PROCESS BENCH, including variations in data synthesis methods, would help identify optimal training strategies.

### Open Question 2
- Question: How do the critique capabilities of open-source language models scale with model size and architecture?
- Basis in paper: [explicit] The paper shows that within the same model family, error identification performance scales favorably with increased model sizes, and QwQ-32B-Preview performs competitively with GPT-4o.
- Why unresolved: While the paper demonstrates scaling trends within specific model families, it does not provide a comprehensive analysis across different architectures or examine the limits of scaling for critique capabilities.
- What evidence would resolve it: Systematic evaluation of a wider range of open-source models with varying sizes and architectures on PROCESS BENCH would clarify scaling patterns and architectural impacts.

### Open Question 3
- Question: What factors contribute to the gap between open-source models and reasoning-specialized models like o1-mini in identifying erroneous reasoning steps?
- Basis in paper: [explicit] The paper notes that QwQ-32B-Preview, despite competitive performance with GPT-4o, still lags behind o1-mini, suggesting a gap in critique capabilities.
- Why unresolved: The paper does not analyze the specific factors or techniques that enable reasoning-specialized models to outperform general language models in error identification tasks.
- What evidence would resolve it: Detailed comparisons of model architectures, training methodologies, and reasoning processes between open-source and reasoning-specialized models would identify key differentiating factors.

## Limitations

- PRM methodology relies on on-policy data generation that may artificially constrain generalization
- Benchmark focuses on competition-level mathematics, potentially limiting general applicability
- Human annotation shows increasing difficulty and disagreement for challenging problems

## Confidence

- **High confidence**: Performance scaling with model size within families, PRM underperformance compared to critic models, QwQ-32B-Preview competitive performance with GPT-4o
- **Medium confidence**: Claims about PRM methodology limitations and generalization failures, open-source vs proprietary model comparisons
- **Low confidence**: Specific numerical thresholds used in PRM evaluation, exact annotation guidelines and their impact on benchmark quality

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically test different threshold selection methods (per-subset vs global) for PRMs to determine if threshold optimization explains the observed performance differences between PRMs and critic models.

2. **Reformatting Impact Study**: Evaluate critic model performance with and without the solution reformatting pipeline to quantify how step granularity standardization affects error identification accuracy.

3. **Cross-Dataset Generalization**: Test the best-performing models on an independent mathematical reasoning dataset not used in ProcessBench to validate whether observed performance patterns generalize beyond the benchmark's specific problem distribution.