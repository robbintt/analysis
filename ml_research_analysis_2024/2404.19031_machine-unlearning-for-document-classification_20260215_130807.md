---
ver: rpa2
title: Machine Unlearning for Document Classification
arxiv_id: '2404.19031'
source_url: https://arxiv.org/abs/2404.19031
tags: []
core_contribution: This paper introduces machine unlearning methods for document classification,
  aiming to remove sensitive user data from trained models. The authors focus on a
  practical scenario where only a small portion (10%) of the training data is stored
  on the server.
---

# Machine Unlearning for Document Classification

## Quick Facts
- arXiv ID: 2404.19031
- Source URL: https://arxiv.org/abs/2404.19031
- Authors: Lei Kang; Mohamed Ali Souibgui; Fei Yang; Lluis Gomez; Ernest Valveny; Dimosthenis Karatzas
- Reference count: 35
- Primary result: Introduces three machine unlearning methods (RT, FT, RL) for document classification with 10% server-side data storage, achieving effective privacy preservation while maintaining classification accuracy

## Executive Summary
This paper addresses the challenge of removing sensitive user data from document classification models under realistic privacy constraints where only a small fraction of training data can be stored on servers. The authors propose three approximate unlearning techniques and develop a label-guided sample generator to create synthetic data for unlearning when real forget data is unavailable. Their approach is evaluated on the RVL-CDIP dataset, demonstrating that the Random Label method achieves the best balance between unlearning effectiveness and computational efficiency.

## Method Summary
The authors propose three approximate unlearning techniques for document classification models: Retrain (RT), Fine-tune (FT), and Random Label (RL). These methods operate under the constraint that only 10% of the original training data is available on the server. Additionally, they introduce a label-guided sample generator that creates synthetic forget data to enable unlearning when real forget data cannot be accessed. The methods are evaluated on the RVL-CDIP dataset, measuring both the effectiveness of forgetting target categories and the preservation of accuracy on retained data.

## Key Results
- Random Label (RL) method achieves the best trade-off between unlearning effectiveness and computational efficiency
- The proposed methods maintain high classification accuracy on retained data while effectively forgetting target categories
- Label-guided synthetic data generation enables unlearning without requiring access to real forget data
- The approach demonstrates practical viability in privacy-constrained scenarios with limited server-side data storage

## Why This Works (Mechanism)
The unlearning effectiveness stems from the strategic manipulation of model parameters through the proposed methods. Retraining completely rebuilds the model from scratch using available data, ensuring complete removal of forgotten information. Fine-tuning leverages existing model knowledge while adjusting weights to minimize influence from forgotten data. The Random Label method randomly shuffles labels during fine-tuning, creating maximum confusion for the model regarding forgotten categories while preserving knowledge of retained classes. The label-guided sample generator creates synthetic data that statistically resembles the distribution of forgotten data, allowing the model to "unlearn" without accessing real sensitive information.

## Foundational Learning
- **Machine Unlearning**: The process of removing specific data's influence from trained models - needed to ensure privacy compliance when users request data deletion
- **Document Classification**: Categorizing documents into predefined classes - the core task being protected through unlearning mechanisms
- **Synthetic Data Generation**: Creating artificial data samples that mimic real data distributions - required when real forget data cannot be accessed
- **Approximate Unlearning**: Methods that trade perfect unlearning for computational efficiency - necessary due to the high cost of exact unlearning
- **Privacy-Preserving ML**: Techniques that protect sensitive information during model training and deployment - the broader context motivating this work
- **Model Parameter Fine-tuning**: Adjusting model weights based on new objectives - the mechanism through which unlearning is achieved without complete retraining

## Architecture Onboarding

**Component Map:** Input Data -> Feature Extraction -> Classification Model -> Unlearning Method (RT/FT/RL) -> Output Model

**Critical Path:** Data Storage (10%) -> Label-Guided Sample Generation -> Unlearning Execution -> Accuracy Verification

**Design Tradeoffs:** The paper prioritizes computational efficiency over perfect unlearning accuracy, choosing approximate methods that work with limited data storage. The synthetic data approach trades data authenticity for privacy preservation and practicality.

**Failure Signatures:** If unlearning is ineffective, classification accuracy on retained data will decrease significantly. If synthetic data generation is poor, the model will not adequately "forget" the target categories. If the 10% storage constraint is violated, the privacy guarantees are compromised.

**First 3 Experiments:**
1. Compare classification accuracy before and after unlearning on retained data categories
2. Measure computational time and resource requirements for each unlearning method
3. Evaluate the statistical similarity between synthetic forget data and real forget data distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to a single dataset (RVL-CDIP) with a specific 10% data storage constraint, limiting generalizability
- Synthetic data generation approach not validated against real forget data in terms of statistical similarity
- No analysis of potential adversarial attacks that could exploit the unlearning process
- Unclear whether forgotten information is truly removed or merely hidden from the model's decision-making

## Confidence
- **High confidence**: Computational efficiency comparisons between RT, FT, and RL methods
- **Medium confidence**: Unlearning effectiveness on retained data accuracy (dataset-specific)
- **Medium confidence**: Synthetic data generation approach (similarity validation lacking)

## Next Checks
1. Validate the label-guided sample generator by comparing synthetic forget data statistics against real forget data distributions using measures like KL divergence and feature distribution similarity
2. Test the unlearning methods across multiple document classification datasets (e.g., IMDB, Reuters) and varying data storage constraints (5%, 20%, 50%) to assess generalizability
3. Conduct adversarial analysis to verify that the unlearning process prevents model inversion attacks and that forgotten information cannot be recovered through model probing techniques