---
ver: rpa2
title: 'ReFusion: Improving Natural Language Understanding with Computation-Efficient
  Retrieval Representation Fusion'
arxiv_id: '2401.02993'
source_url: https://arxiv.org/abs/2401.02993
tags:
- retrieval
- module
- tasks
- refusion
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ReFusion, a computation-efficient retrieval
  representation fusion framework for non-knowledge-intensive tasks. The method directly
  fuses retrieval representations into transformer models via a retrieval fusion module
  with two ranking schemes and a neural architecture search module to optimize the
  fusion structure.
---

# ReFusion: Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion

## Quick Facts
- arXiv ID: 2401.02993
- Source URL: https://arxiv.org/abs/2401.02993
- Reference count: 17
- This paper proposes ReFusion, a computation-efficient retrieval representation fusion framework for non-knowledge-intensive tasks, achieving average improvements of 2.1% on single-sentence tasks and 3.0% on sentence-pair tasks compared to LM-BFF.

## Executive Summary
ReFusion is a novel framework that improves natural language understanding by directly fusing retrieval representations into transformer models. Unlike previous methods that concatenate retrieved sentences, ReFusion encodes top-k similar sentences and directly adds their averaged representations to the hidden states at each attention module. The framework employs two ranking schemes (reranker-based and ordered-mask-based) to refine retrieval representations and uses neural architecture search to optimize the fusion structure across layers. Experiments on 15 NKI tasks demonstrate ReFusion's superior and robust performance compared to baselines.

## Method Summary
ReFusion is a retrieval representation fusion framework for non-knowledge-intensive tasks. It consists of three modules: retrieval, fusion, and search. The retrieval module retrieves top-k similar sentences using a query encoder and FAISS/ScaNN index. The fusion module directly adds the mean representations of the top-k retrievals to the [CLS] token in each attention module, using either a learnable reranker or an ordered-mask scheme to refine the retrieval representations. The search module employs neural architecture search to find the optimal fusion structure across different layers. The framework is trained using prompt-based few-shot learning with a RoBERTa-large model on 15 NKI tasks.

## Key Results
- ReFusion achieves average improvements of 2.1% on single-sentence tasks and 3.0% on sentence-pair tasks compared to LM-BFF.
- The framework demonstrates consistent effectiveness across varying dataset sizes and is more robust with smaller standard deviation.
- ReFusion outperforms other baselines on various non-knowledge-intensive tasks, including sentiment analysis, opinion polarity, and natural language inference.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Direct fusion of retrieval representations into hidden states avoids truncation and reduces FLOPs compared to concatenation-based augmentations.
- **Mechanism:** ReFusion encodes top-k similar sentences and directly adds their averaged representations to the hidden states (e.g., the [CLS] token) at each attention module, bypassing the max sequence length constraint and reducing the number of tokens processed.
- **Core assumption:** The averaged representation of retrieved sentences adequately captures relevant context and can be effectively integrated into the model's hidden states.
- **Evidence anchors:** [abstract]: "Unlike previous works, ReFusion directly fuses the retrieval representations into the hidden states of models." [section]: "The basic idea is to add the mean representations of k retrievals to the representation of the [CLS] token in each attention module."
- **Break condition:** If the retrieval representations are noisy or irrelevant, adding them to hidden states may degrade performance. If the retrieval encoder is poor, the fused representations will be low quality.

### Mechanism 2
- **Claim:** Learnable reranking and ordered-mask-based ranking schemes improve the quality of fused retrieval representations.
- **Mechanism:** 
  - Reranker: A learnable vector assigns weights to the top-k retrievals, allowing the model to prioritize more relevant ones.
  - Ordered-mask: A chain of Bernoulli variables, modeled with Gumbel-Softmax, masks retrieval representations dimension-wise, effectively learning an importance ranking per dimension.
- **Core assumption:** The retriever's initial ranking is suboptimal, and learning a task-specific ranking improves the relevance and impact of fused representations.
- **Evidence anchors:** [abstract]: "The fusion module has two effective ranking schemes, i.e., the reranker-based scheme and the ordered-mask scheme, to refine the representation of retrievals." [section]: "Therefore, we aim to propose a learnable reranker to learn the ranking distribution tailored to each module in LMs."
- **Break condition:** If the ranking schemes overfit to the training data or if the initial retrieval quality is very high, learned ranking may not improve performance.

### Mechanism 3
- **Claim:** Neural architecture search (NAS) optimizes the placement and choice of ranking schemes across transformer layers, leading to robust performance.
- **Mechanism:** NAS searches over a space where each layer can choose between reranker-based fusion, ordered-mask fusion, or no fusion. The search is continuous via softmax relaxation, and the final architecture selects the best module per layer.
- **Core assumption:** Not all layers benefit equally from retrieval fusion; some may need no fusion or different ranking schemes.
- **Evidence anchors:** [abstract]: "Furthermore, we use Neural Architecture Search (NAS) to seek the optimal fusion structure across different layers." [section]: "It is difficult to tell which ranking scheme is better on each layer in LMs. Therefore, we propose an architecture search module..."
- **Break condition:** If the search space is too constrained or the validation set is not representative, NAS may select suboptimal architectures.

## Foundational Learning

- **Concept:** Prompt-based fine-tuning for few-shot learning
  - Why needed here: ReFusion builds on prompt-based fine-tuning (like LM-BFF) and evaluates on few-shot settings to demonstrate understanding capability.
  - Quick check question: What is the difference between traditional fine-tuning and prompt-based fine-tuning in terms of input format and loss function?

- **Concept:** Retrieval-augmented generation (RAG) and retrieval-based augmentations
  - Why needed here: ReFusion is a retrieval-based augmentation method, but for NKI tasks instead of KI tasks. Understanding how RAG works in KI tasks provides context for ReFusion's approach.
  - Quick check question: How does concatenation-based retrieval augmentation differ from direct representation fusion in terms of computational cost and input length?

- **Concept:** Neural Architecture Search (DARTS-style)
  - Why needed here: ReFusion uses a DARTS-like continuous relaxation for searching the optimal fusion structure across layers.
  - Quick check question: In DARTS, how is the categorical choice of operations relaxed to a continuous space, and how is the final discrete architecture selected?

## Architecture Onboarding

- **Component map:** Retrieval Module -> Fusion Module -> Search Module -> Transformer Backbone
- **Critical path:**
  1. Encode input text â†’ retrieve top-k representations
  2. Apply chosen ranking scheme to retrieval representations
  3. Fuse into hidden states at specified layers
  4. Forward pass through remaining layers
  5. Compute loss and backpropagate (update LM weights, ranking weights, and NAS weights)
- **Design tradeoffs:**
  - Fusion location: Query vs. Key vs. Value vs. all (impacts how retrieval context interacts with attention)
  - k (number of retrievals): More retrievals may help but increase computational cost and risk noise
  - Ranking scheme choice: Reranker is simpler but may not capture dimension-wise importance; Ordered-mask is more expressive but adds complexity
  - NAS complexity: Larger search space increases robustness but also training time and risk of overfitting
- **Failure signatures:**
  - Performance drops with high k: Retrieval representations are noisy or redundant
  - No improvement over baseline: Ranking schemes are ineffective or NAS selects no fusion
  - High variance across runs: NAS instability or ranking scheme sensitivity
  - High computational cost: Inefficient retrieval or overly complex ranking scheme
- **First 3 experiments:**
  1. Ablation: Replace ReFusion's fusion with simple addition of retrieval representations (no ranking, no NAS). Compare performance and FLOPs to baseline LM-BFF.
  2. Ablation: Use only one ranking scheme (reranker OR ordered-mask) across all layers, no NAS. Compare performance to full ReFusion.
  3. Ablation: Fix fusion to a single layer (e.g., first or last) instead of using NAS. Compare performance to full ReFusion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of ranking scheme (reranker-based vs. ordered-mask-based) impact the performance of ReFusion across different layers of the transformer model?
- Basis in paper: [explicit] The paper discusses two ranking schemes (reranker-based and ordered-mask-based) and mentions that the architecture search module aims to select the optimal ranking scheme for different layers.
- Why unresolved: The paper does not provide detailed insights into how the performance of these ranking schemes varies across different layers of the transformer model.
- What evidence would resolve it: A comprehensive analysis showing the performance of each ranking scheme across different layers of the transformer model would resolve this question.

### Open Question 2
- Question: What is the impact of the number of retrieved sentences (k) on the computational efficiency and performance of ReFusion?
- Basis in paper: [explicit] The paper mentions that the number of retrieved sentences (k) is set to 64 and discusses the trade-off between computational efficiency and performance.
- Why unresolved: The paper does not provide a detailed analysis of how varying the number of retrieved sentences (k) affects the computational efficiency and performance of ReFusion.
- What evidence would resolve it: An empirical study varying the number of retrieved sentences (k) and measuring the impact on computational efficiency and performance would resolve this question.

### Open Question 3
- Question: How does the choice of query (input texts vs. hidden states) affect the performance and computational cost of ReFusion?
- Basis in paper: [explicit] The paper discusses two types of queries (input texts and hidden states) and mentions their impact on performance and computational cost.
- Why unresolved: The paper does not provide a detailed comparison of the performance and computational cost of using input texts vs. hidden states as queries.
- What evidence would resolve it: A comprehensive comparison of the performance and computational cost of using input texts vs. hidden states as queries would resolve this question.

### Open Question 4
- Question: How does the performance of ReFusion compare to other retrieval-augmented methods when applied to different types of non-knowledge-intensive tasks?
- Basis in paper: [explicit] The paper mentions that ReFusion outperforms other methods on various non-knowledge-intensive tasks.
- Why unresolved: The paper does not provide a detailed comparison of ReFusion's performance against other retrieval-augmented methods across different types of non-knowledge-intensive tasks.
- What evidence would resolve it: A comprehensive comparison of ReFusion's performance against other retrieval-augmented methods across different types of non-knowledge-intensive tasks would resolve this question.

### Open Question 5
- Question: How does the architecture search module in ReFusion impact the overall performance and robustness of the model?
- Basis in paper: [explicit] The paper discusses the architecture search module and its role in selecting the optimal ranking scheme and fusion structure.
- Why unresolved: The paper does not provide detailed insights into how the architecture search module impacts the overall performance and robustness of ReFusion.
- What evidence would resolve it: An analysis showing the impact of the architecture search module on the overall performance and robustness of ReFusion would resolve this question.

## Limitations

- **Implementation Details**: The paper lacks specific implementation details for the retrieval module, fusion module with ranking schemes, and search module using NAS, making faithful reproduction challenging.
- **Ranking Scheme Effectiveness**: There is no direct empirical comparison to ReFusion's ranking schemes in the provided corpus evidence, leaving the specific effectiveness of the ranking schemes uncertain.
- **NAS Optimization**: The effectiveness of NAS in optimizing the placement and choice of ranking schemes across transformer layers is not directly validated in the corpus evidence.

## Confidence

- **High**: The overall effectiveness of ReFusion in improving NLU performance on NKI tasks compared to baselines.
- **Medium**: The mechanism of direct fusion of retrieval representations into hidden states reducing FLOPs and avoiding truncation.
- **Low**: The specific effectiveness of the ranking schemes and NAS optimization in improving performance.

## Next Checks

1. **Ablation Study**: Conduct an ablation study to isolate the impact of the fusion module's ranking schemes on performance. Compare the performance of ReFusion with and without ranking schemes to quantify their contribution.
2. **NAS Sensitivity Analysis**: Perform a sensitivity analysis on the NAS optimization to understand its impact on performance. Compare the performance of ReFusion with different NAS configurations or without NAS to assess its effectiveness.
3. **Computational Cost Analysis**: Analyze the computational cost of ReFusion compared to baselines like LM-BFF. Measure the FLOPs and training time of ReFusion and compare them to the baselines to validate the claim of computation efficiency.