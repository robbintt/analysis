---
ver: rpa2
title: Online and Offline Evaluation in Search Clarification
arxiv_id: '2403.09180'
source_url: https://arxiv.org/abs/2403.09180
tags:
- cation
- clari
- online
- search
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between online and offline
  evaluations in search clarification, addressing the gap in understanding how these
  evaluations correspond. The study uses user engagement as the ground truth and employs
  various offline labels to assess the similarity of clarification ranked lists.
---

# Online and Offline Evaluation in Search Clarification

## Quick Facts
- arXiv ID: 2403.09180
- Source URL: https://arxiv.org/abs/2403.09180
- Reference count: 40
- One-line primary result: Offline evaluations can complement online evaluations in identifying the most engaging clarification pane, with Overall Quality and Coverage labels performing best.

## Executive Summary
This paper investigates the relationship between online (user engagement-based) and offline (human-annotated) evaluations in search clarification systems. The study uses click-through rate as ground truth and employs various offline labels to assess the similarity of clarification ranked lists. Results show that offline evaluations can identify the most engaging clarification pane with comparable accuracy to online evaluations when uncertainty in online data is low. Large Language Models, particularly GPT-3.5, outperform Learning-to-Rank models and individual offline labels in predicting user engagement.

## Method Summary
The study uses the MIMICS-Duo dataset containing 1,034 query-clarification pairs with both online (Engagement Level, Impression Level) and offline labels (Overall Quality, Coverage, Diversity, Importance Order, List-wise Preference). The methodology involves comparing ranked lists from offline labels to ideal rankings based on Engagement Level, using metrics like P@1, MRR, NDCG@3, RBP, and RBO. The researchers train both Learning-to-Rank models (Mart, RandomForests, RankBoost, AdaRank, SVM-rank) and GPT-3.5 with offline labels as features to predict Engagement Level. They also analyze the impact of query length (1-4 vs. 5-9 words) and Impression Level (high vs. medium-high) on the relationship between online and offline evaluations.

## Key Results
- Offline evaluations can complement online evaluations in identifying the most engaging clarification pane when Impression Level is high
- Overall Quality and Coverage labels perform best among individual offline labels
- GPT-3.5 outperforms Learning-to-Rank models and individual offline labels in predicting user engagement
- Query length has minimal impact on the relationship between online and offline evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Offline evaluations can identify the most engaging clarification pane (MECP) with comparable accuracy to online evaluations when uncertainty in online data is low.
- Mechanism: When Impression Level is high, the Engagement Level based on click-through rate becomes a more reliable proxy for true user engagement. Under these conditions, offline labels such as Overall Quality and Coverage correlate strongly enough with the (now less noisy) online label to allow accurate MECP identification.
- Core assumption: High Impression Level reduces the noise in click-through rate data sufficiently that the ranking of clarification panes by offline labels aligns with the true engagement ranking.
- Evidence anchors:
  - [section] "we learned from Zamani et al. [74] that a clarification pane with high Impression Level was shown to the users more than a clarification pane with low Impression Level. Therefore, the obtained Engagement Level by a clarification pane with a high Impression Level is likely to be more reliable."
  - [abstract] "Results show that offline evaluations can complement online evaluations in identifying the most engaging clarification pane, with Overall Quality and Coverage labels performing best."
- Break condition: If Impression Level remains low or if user behavior is highly heterogeneous across impressions, the noise in Engagement Level may overwhelm the signal from offline labels, causing the correspondence to break down.

### Mechanism 2
- Claim: Large Language Models (LLMs) like GPT-3.5 outperform Learning-to-Rank (LTR) models and individual offline labels in predicting online user engagement for clarification panes.
- Mechanism: GPT-3.5 leverages its deep transformer architecture and unsupervised training on diverse internet text to capture nuanced semantic relationships between query, clarification text, options, and engagement patterns. When provided with offline labels as input features, it can effectively model the mapping from these features to Engagement Level, surpassing LTR models that rely on explicit relevance features and supervised learning.
- Core assumption: The rich, contextual understanding embedded in GPT-3.5's parameters allows it to generalize better from offline labels to predict user engagement than models trained specifically for ranking with limited feature sets.
- Evidence anchors:
  - [abstract] "Large Language Models, particularly GPT-3.5, outperform Learning-to-Rank models and individual offline labels in predicting user engagement."
  - [section] "We observed that when GPT-3.5 was provided with high-quality human-annotated labels of clarification characteristics, it showed better performance compared to the List-wise Preference labelling approach conducted by crowd-source workers."
- Break condition: If offline labels are of low quality or if the model input lacks sufficient contextual information (e.g., when offline labels are omitted), GPT-3.5's performance drops dramatically, indicating the mechanism depends heavily on the quality and presence of these labels.

### Mechanism 3
- Claim: Query length has minimal impact on the relationship between online and offline evaluations in search clarification.
- Mechanism: Both short (1-4 words) and long (5-9 words) queries can be effectively clarified using the same set of offline labels. The evaluation metrics (P@1, MRR) remain in the same order across query lengths, suggesting the engagement prediction task is robust to query length variations.
- Core assumption: The characteristics that make a clarification engaging (quality, coverage, diversity) are independent of query length, and the same offline labels can capture these characteristics for both short and long queries.
- Evidence anchors:
  - [abstract] "The study also finds that query length has minimal impact on the relationship between online and offline evaluations."
  - [section] "We also performed a Tukey HSD test on the calculated P@1 and MRR values for short, long, and all queries. The results indicate that there are no significant differences, suggesting that the length of the query does not have an impact on the relationship between offline evaluations and online evaluations in the context of search clarification."
- Break condition: If query intent becomes significantly more ambiguous in long queries, or if the retrieval quality of documents varies systematically with query length, the minimal impact assumption could break down.

## Foundational Learning

- Concept: Understanding of Information Retrieval evaluation metrics (P@1, MRR, NDCG, RBP, RBO).
  - Why needed here: The study compares online and offline evaluation methodologies using multiple metrics, and understanding their differences is crucial for interpreting results.
  - Quick check question: What is the key difference between P@1 and NDCG@1 in terms of how they treat the Engagement Level of the top-ranked item?

- Concept: Knowledge of click-through rate bias and uncertainty sources in online evaluation.
  - Why needed here: The study explicitly addresses how uncertainty in online labels (due to low Impression Level) affects the correspondence with offline evaluations.
  - Quick check question: Why might a clarification pane with a low Engagement Level still be considered engaging if its Impression Level is low?

- Concept: Familiarity with Learning-to-Rank (LTR) models and Large Language Models (LLMs) architecture and training.
  - Why needed here: The study compares the performance of LTR models (RandomForests, AdaRank, MART, RankBoost, SVM-rank) with GPT-3.5 in predicting engagement, requiring understanding of their strengths and limitations.
  - Quick check question: What is a key architectural difference between GPT-3.5 and traditional LTR models that might explain GPT-3.5's superior performance?

## Architecture Onboarding

- Component map: Data collection (MIMICS-Duo) -> Offline label generation (crowd-sourcing for Quality, Coverage, Diversity, Importance Order, List-wise Preference) -> Online label generation (Engagement Level from click-through rate, Impression Level from impression count) -> Experimental setup (MECP identification, LTR model training, GPT-3.5 prompt engineering) -> Evaluation (P@1, MRR, NDCG, RBP, RBO) -> Analysis (query length grouping, Impression Level filtering)
- Critical path: Collect MIMICS-Duo dataset -> Generate offline labels -> Filter by Impression Level -> Use offline labels to rank clarification panes -> Compare ranked lists to ideal (Engagement Level-based) ranked lists using evaluation metrics -> Analyze impact of query length and uncertainty -> Train GPT-3.5 and LTR models with offline labels -> Compare model performance to individual offline labels
- Design tradeoffs: Using high Impression Level data reduces uncertainty but decreases dataset size; using GPT-3.5 with offline labels improves performance but requires high-quality annotations; focusing on P@1 and MRR prioritizes MECP identification over overall ranking quality
- Failure signatures: If offline labels do not correlate with Engagement Level even at high Impression Level, the MECP identification mechanism fails; if GPT-3.5 performance drops significantly without offline labels, the LLM reliance mechanism fails; if query length significantly impacts metric ordering, the minimal impact assumption fails
- First 3 experiments:
  1. Verify that offline labels (Overall Quality, Coverage) correlate with Engagement Level at high Impression Level by computing Pearson correlation coefficients and checking statistical significance
  2. Test GPT-3.5's ability to predict Engagement Level by prompting it with clarification panes and offline labels, then comparing predicted rankings to the ideal rankings using P@1 and MRR
  3. Evaluate the impact of query length by splitting the dataset into short and long queries, then repeating the MECP identification task with the best-performing offline labels for each group and comparing results using Tukey HSD tests

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of Learning-to-Rank models in search clarification be improved beyond the individual offline labels?
- Basis in paper: [explicit] The paper states that LTR models did not outperform individual offline labels in accurately ranking the most engaging clarification panes.
- Why unresolved: The study did not explore advanced LTR techniques or feature engineering that could potentially enhance model performance.
- What evidence would resolve it: Comparative experiments using state-of-the-art LTR methods (e.g., neural ranking models) with enriched feature sets on larger datasets.

### Open Question 2
- Question: What is the impact of different user demographics on the relationship between online and offline evaluations in search clarification?
- Basis in paper: [inferred] The paper mentions that factors like educational level and familiarity with IR systems impact user satisfaction and click-through data.
- Why unresolved: The study did not analyze the dataset by user demographic segments or conduct controlled user studies.
- What evidence would resolve it: Segmented analysis of user engagement patterns by demographics and controlled experiments with diverse user groups.

### Open Question 3
- Question: Can the integration of additional contextual information (e.g., query intent, SERP quality) improve GPT models' prediction of user engagement?
- Basis in paper: [explicit] The paper discusses the importance of SERP quality and the need to assess both SERP and clarification pane quality.
- Why unresolved: The study only used offline labels as input features for GPT models and did not explore other contextual features.
- What evidence would resolve it: Experiments comparing GPT model performance with and without additional contextual features as input.

## Limitations
- The study's findings are based on a specific dataset (MIMICS-Duo) with 1,034 query-clarification pairs, which may limit generalizability to other search clarification scenarios or domains.
- The reliance on click-through rate as the primary measure of user engagement introduces potential biases and may not capture all aspects of user satisfaction with clarification panes.
- The performance of GPT-3.5 is highly dependent on the quality and presence of offline labels, with significant drops observed when labels are omitted or of low quality.

## Confidence
- High: The core finding that offline evaluations can complement online evaluations in identifying the most engaging clarification pane when Impression Level is high.
- Medium: The superiority of Large Language Models (particularly GPT-3.5) over Learning-to-Rank models in predicting user engagement, given the dependency on high-quality offline labels.
- Low: The generalizability of the query length findings to diverse search contexts and the robustness of the observed relationships across different types of search clarification systems.

## Next Checks
1. **Dataset Diversity Validation**: Test the correspondence between online and offline evaluations using multiple datasets from different domains (e.g., web search, e-commerce, academic search) to assess generalizability of the findings.
2. **Alternative Engagement Metrics**: Validate the results using additional engagement metrics beyond click-through rate, such as dwell time, abandonment rate, or explicit feedback, to ensure the robustness of the observed relationships.
3. **Label Quality Impact Study**: Systematically vary the quality of offline labels (e.g., using crowd-workers vs. experts, different annotation guidelines) and measure the impact on GPT-3.5's performance to quantify the dependency on label quality.