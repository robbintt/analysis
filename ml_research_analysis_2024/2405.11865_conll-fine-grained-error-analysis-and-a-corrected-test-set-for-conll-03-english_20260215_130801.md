---
ver: rpa2
title: 'CoNLL#: Fine-grained Error Analysis and a Corrected Test Set for CoNLL-03
  English'
arxiv_id: '2405.11865'
source_url: https://arxiv.org/abs/2405.11865
tags: []
core_contribution: This paper addresses the stagnation of named entity recognition
  (NER) performance on the CoNLL-03 English dataset by conducting a fine-grained error
  analysis and creating a corrected test set, CoNLL. The authors annotated the test
  set with document-level metadata (domain and format), retrained three state-of-the-art
  NER models, and manually corrected annotation errors by adjudicating among previous
  corrected versions (CoNLL++, ReCoNLL, CoNLL-CODAIT) and fixing systematic errors
  like sentence boundary issues and tokenization problems.
---

# CoNLL#: Fine-grained Error Analysis and a Corrected Test Set for CoNLL-03 English

## Quick Facts
- arXiv ID: 2405.11865
- Source URL: https://arxiv.org/abs/2405.11865
- Reference count: 0
- Primary result: Created CoNLL#, a corrected test set that improves NER model F1 scores by over 2 points and reveals domain-specific weaknesses

## Executive Summary
This paper addresses the stagnation of named entity recognition (NER) performance on the CoNLL-03 English dataset by conducting a fine-grained error analysis and creating a corrected test set, CoNLL#. The authors annotated the test set with document-level metadata (domain and format), retrained three state-of-the-art NER models, and manually corrected annotation errors by adjudicating among previous corrected versions (CoNLL++, ReCoNLL, CoNLL-CODAIT) and fixing systematic errors like sentence boundary issues and tokenization problems. This resulted in CoNLL#, which improved model F1 scores by over 2 points compared to the original test set. The error analysis revealed that models still struggle most with economy documents, ambiguous acronyms, obscure mentions, and irregular capitalization.

## Method Summary
The authors created CoNLL# through a multi-stage process: first annotating the original test set with document-level metadata (domain and format), then running three state-of-the-art NER models to identify systematic errors, and finally manually correcting annotation errors by adjudicating among previous correction attempts. The correction process involved fixing sentence boundary issues, tokenization problems, and inconsistent annotations across the test set. The resulting CoNLL# dataset was then used to rerun the models and conduct a detailed error analysis that categorized errors by type (missed, spurious, boundary, type) and domain.

## Key Results
- Model F1 scores improved by over 2 points on the corrected CoNLL# test set compared to the original
- Economy documents consistently showed the lowest performance across all tested models
- Ambiguous acronyms, obscure mentions, and irregular capitalization remain challenging error categories
- Document-level metadata annotation revealed significant performance variation across domains and formats

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Correcting systematic annotation errors in the test set directly improves measured model performance by removing noise that causes false negatives.
- **Mechanism**: When the test set contains incorrect labels due to tokenization errors, sentence boundary issues, or inconsistent annotations, models are penalized for predictions that would actually be correct on a properly annotated set. Fixing these errors reduces false negatives in evaluation, which increases F1 scores.
- **Core assumption**: The test set annotation errors are systematic and pervasive enough to meaningfully impact evaluation metrics.
- **Evidence anchors**:
  - [abstract] "We introduce CoNLL#, a new corrected version of the test set that addresses its systematic and most prevalent errors, allowing for low-noise, interpretable error analysis."
  - [section 5.2] "Many of the incorrect labels in the CoNLL-03 English test set stem from sentence boundary errors in the original test set."
  - [corpus] Weak evidence - the paper provides counts of corrections but doesn't quantify how many false negatives were due to annotation errors vs model errors.

### Mechanism 2
- **Claim**: Document-level metadata annotation reveals domain-specific model weaknesses that are obscured in aggregate F1 scores.
- **Mechanism**: By annotating documents with domain and format labels, the analysis can identify specific document types where models consistently underperform. This granularity shows that economy documents are a persistent weakness across all models, which aggregate metrics would hide.
- **Core assumption**: Model performance varies significantly across document types and this variation is meaningful for understanding model limitations.
- **Evidence anchors**:
  - [section 4.2] "Table 3 displays results split across document domains and formats... These results show that... the economy domain is in fact the lowest-performing domain."
  - [section 6.1] "All state-of-the-art NER models that we tested performed significantly worse on economy test documents than the others."
  - [corpus] Strong evidence - the paper provides detailed performance breakdowns across domains and formats.

### Mechanism 3
- **Claim**: Adjudicating among previous correction attempts creates a more reliable test set than any single correction effort alone.
- **Mechanism**: Different correction efforts made different decisions about ambiguous cases. By comparing and adjudicating these disagreements, the authors create a test set that benefits from the strengths of each approach while avoiding their individual biases.
- **Core assumption**: The different correction approaches have complementary strengths and weaknesses that can be systematically evaluated.
- **Evidence anchors**:
  - [section 5] "We partly based our corrections on previously-published corrected versions... We decided to perform an adjudication process among all three corrected test sets."
  - [section 5.1] "CoNLL++ was generally more aggressive in its relabeling efforts, but did so with high precision, as 68.95% of the disagreements were judged to be in favor of CoNLL++"
  - [corpus] Moderate evidence - the paper shows disagreement counts but doesn't systematically evaluate which approach was better for which error types.

## Foundational Learning

- **Concept**: Named Entity Recognition (NER) task definition and BIO tagging scheme
  - **Why needed here**: The entire paper revolves around evaluating NER models, so understanding the task and evaluation metrics (precision, recall, F1) is essential for interpreting results.
  - **Quick check question**: What is the difference between a false positive and a false negative in NER evaluation?

- **Concept**: CoNLL-03 dataset structure and historical context
  - **Why needed here**: The paper extensively references the CoNLL-03 dataset and previous correction attempts, so understanding its structure and significance in NER research is crucial.
  - **Quick check question**: How many documents are in the CoNLL-03 English test set and what domains do they cover?

- **Concept**: Error analysis methodology and categorization
  - **Why needed here**: The paper conducts a detailed error analysis beyond simple F1 scores, categorizing errors by type (missed, spurious, boundary, type error) and domain.
  - **Quick check question**: What are the four main error categories used in the paper's analysis?

## Architecture Onboarding

- **Component map**:
  Document annotation pipeline -> Model evaluation framework -> Correction adjudication system -> Error analysis tool -> Dataset generation pipeline

- **Critical path**: 
  1. Annotate test documents with domain/format metadata
  2. Run SOTA models on original test set
  3. Identify annotation errors through error analysis
  4. Adjudicate among previous correction attempts
  5. Create CoNLL# with systematic fixes
  6. Rerun models on corrected test set
  7. Conduct detailed error analysis on corrected data

- **Design tradeoffs**:
  - Annotation granularity vs. annotation effort: More detailed annotations provide better insights but require more manual work
  - Correction scope vs. consistency: Extensive corrections fix more errors but may introduce new inconsistencies
  - Model selection vs. representativeness: Choosing specific SOTA models provides depth but may not represent the full landscape

- **Failure signatures**:
  - Low inter-annotator agreement on domain/format labels
  - Contradictory corrections from different sources that cannot be adjudicated
  - Models show no performance improvement on corrected test set (suggesting errors were not the primary limitation)
  - Error analysis reveals patterns that cannot be addressed with dataset corrections

- **First 3 experiments**:
  1. **Domain performance baseline**: Run models on original test set and compute F1 scores by domain/format to identify worst-performing categories
  2. **Correction impact validation**: Apply a subset of identified corrections and measure performance improvement to validate the correction approach
  3. **Error type distribution analysis**: Categorize all errors from model predictions to understand the relative importance of different error types across domains

## Open Questions the Paper Calls Out
None

## Limitations
- Manual correction process relies heavily on subjective judgments about ambiguous cases
- Limited generalizability due to focus on CoNLL-03 English dataset (news articles from specific time period)
- Does not clearly separate contribution of annotation corrections from other factors

## Confidence
**High Confidence**: Document-level metadata annotation reveals domain-specific model weaknesses (well-supported by detailed performance breakdowns)
**Medium Confidence**: CoNLL# provides more reliable test set than previous corrections (methodologically sound but not systematically evaluated)
**Low Confidence**: Correcting annotation errors directly improves F1 scores by over 2 points (doesn't clearly separate annotation vs model limitations)

## Next Checks
1. **Replication of Correction Impact**: Independently replicate the test set correction process on a subset of documents (e.g., 50 randomly selected) and measure the F1 score difference between the original and corrected versions using the same three SOTA models.

2. **Cross-Domain Generalization Test**: Evaluate the same three SOTA models on the CoNLL# test set and a different NER dataset (e.g., OntoNotes or WNUT) to determine whether the identified weaknesses (economy documents, ambiguous acronyms, etc.) generalize beyond the CoNLL-03 domain.

3. **Error Type Contribution Analysis**: Conduct a systematic error classification on both the original and corrected test sets to quantify the relative contribution of annotation errors versus model limitations to the overall error rate. This would involve manually reviewing a stratified sample of errors to determine their root causes.