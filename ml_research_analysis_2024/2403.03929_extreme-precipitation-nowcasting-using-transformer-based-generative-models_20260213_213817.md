---
ver: rpa2
title: Extreme Precipitation Nowcasting using Transformer-based Generative Models
arxiv_id: '2403.03929'
source_url: https://arxiv.org/abs/2403.03929
tags:
- extreme
- time
- precipitation
- average
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extreme precipitation nowcasting
  using Transformer-based generative models. The authors propose NowcastingGPT-EVL,
  which combines a Vector Quantized-Variational AutoEncoder (VQ-VAE) with an Autoregressive
  Transformer and a novel Extreme Value Loss (EVL) regularization.
---

# Extreme Precipitation Nowcasting using Transformer-based Generative Models

## Quick Facts
- **arXiv ID**: 2403.03929
- **Source URL**: https://arxiv.org/abs/2403.03929
- **Reference count**: 40
- **Primary result**: NowcastingGPT-EVL outperforms existing baselines in predicting extreme precipitation events using a novel dynamic Extreme Value Loss regularization

## Executive Summary
This paper addresses the challenge of extreme precipitation nowcasting by proposing NowcastingGPT-EVL, a Transformer-based generative model that combines a Vector Quantized-Variational AutoEncoder (VQ-VAE) with an Autoregressive Transformer and a novel Extreme Value Loss (EVL) regularization. The key innovation is the dynamic EVL that adapts to changes in the hidden space topology during training, rather than relying on fixed extreme representations. Experiments on the comprehensive KNMI dataset demonstrate superior performance in predicting extreme precipitation events across multiple metrics including MSE, PCC, CSI, FAR, FSS, and AUC.

## Method Summary
The proposed method uses a VQ-VAE to compress radar precipitation maps into discrete tokens, which are then processed by an Autoregressive Transformer to model temporal dynamics. The key innovation is the Extreme Value Loss regularization, which dynamically predicts extreme labels during training without assuming fixed extreme representations. The model is trained on 9-frame sequences and predicts 6 future frames, with extreme events defined as the top 1% of precipitation over three-hour periods. The KNMI radar dataset is used for evaluation, with specific metrics including MSE, PCC, CSI at multiple thresholds, FAR, FSS at different scales, and AUC for extreme event detection.

## Key Results
- NowcastingGPT-EVL achieves the highest Area Under the Curve (AUC) for extreme event detection across multiple thresholds
- The model outperforms baseline models (PySTEPS, TECO, Nuwä-EVL) on majority of metrics including MSE, PCC, and CSI
- Achieves superior temporal consistency in predictions while maintaining computational efficiency with 322.86 seconds generation time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The EVL regularization dynamically adjusts to the evolving hidden space, improving the model's ability to capture extreme precipitation events without relying on fixed representations.
- Mechanism: The EVL loss uses a classifier that dynamically predicts extreme labels during training, allowing the regularization to adapt to changes in the topology of the hidden space as the model learns. This contrasts with fixed-latent approaches that assume extreme features remain constant.
- Core assumption: The hidden space topology changes during training, and these changes affect how extreme events are represented.
- Evidence anchors:
  - [abstract] "We introduce a novel method for computing EVL without assuming fixed extreme representations, addressing the limitations of current models in capturing extreme weather events."
  - [section] "Although Bi et al. (2023) uses an EVL as a regularizer, it assumes a predefined set of representations that should embed the extreme events features, assuming that the extreme features never change during training, which we believe to be a wrong inductive bias, since the topology of the hidden space changes during training."
- Break condition: If the hidden space topology does not change significantly during training, the dynamic EVL would offer no advantage over fixed-latent approaches.

### Mechanism 2
- Claim: The combination of VQ-VAE for discrete token extraction and an Autoregressive Transformer for modeling latent space dynamics enables efficient and accurate nowcasting of precipitation patterns.
- Mechanism: The VQ-VAE compresses precipitation maps into discrete tokens, reducing spatial dimensions while preserving essential features. The Autoregressive Transformer then models the sequence of these tokens, learning the temporal dynamics to predict future precipitation maps.
- Core assumption: The discrete token representation captures the essential features of precipitation patterns necessary for accurate forecasting.
- Evidence anchors:
  - [section] "We define the video prediction backbone of the proposed nowcasting model following the VideoGPT framework, using a VQ-VAE as a feature extractor and an Autoregressive Transformer (Esser et al., 2021) to learn the latent space dynamics and predict the future precipitation maps."
  - [section] "In order to obtain the reconstructed image from the discrete codes, we use a decoder that mirrors the structure of the encoder."
- Break condition: If the VQ-VAE fails to compress the precipitation maps into meaningful discrete tokens, the Autoregressive Transformer would lack the necessary information to model the dynamics accurately.

### Mechanism 3
- Claim: The proposed model achieves superior temporal consistency compared to other baselines, leading to more reliable long-term predictions.
- Mechanism: The model's architecture, particularly the combination of VQ-VAE and Autoregressive Transformer, is designed to capture both spatial and temporal relationships in the precipitation data. The EVL regularization further enhances the model's ability to maintain consistency in predicting extreme events over time.
- Core assumption: Temporal consistency is a critical factor in precipitation nowcasting, especially for predicting extreme events.
- Evidence anchors:
  - [section] "The proposed NowcastingGPT-EVL outperforms the other models on the majority of metrics and close second on the rest."
  - [section] "Furthermore, with a generation time of 322.86 seconds, Nuw¨ a-EVL constitutes a good indicator for the sampling efficiency of autoregressive models."
  - [section] "Remarkably, the graphs presented in Appendix 6.6 demonstrate that TECO achieves results on par with other methods, despite having fewer parameters and a more efficient sampling time, and exhibits superior temporal consistency compared to alternative approaches."
- Break condition: If the dataset does not contain sufficient temporal patterns or if the extreme events are too sporadic, the model's ability to maintain temporal consistency would be limited.

## Foundational Learning

- Concept: Extreme Value Theory (EVT)
  - Why needed here: EVT provides the mathematical foundation for modeling and predicting extreme precipitation events, which are characterized by their rarity and potential for significant impact.
  - Quick check question: What is the key difference between modeling average precipitation and modeling extreme precipitation events?

- Concept: Vector Quantization
  - Why needed here: Vector Quantization is used in the VQ-VAE to compress the precipitation maps into discrete tokens, enabling efficient modeling of the spatial features while reducing computational complexity.
  - Quick check question: How does Vector Quantization differ from traditional continuous feature representation?

- Concept: Autoregressive Models
  - Why needed here: Autoregressive models are used to predict future precipitation maps based on the sequence of past observations, capturing the temporal dependencies in the data.
  - Quick check question: What is the main advantage of using an autoregressive model for precipitation nowcasting compared to a non-autoregressive approach?

## Architecture Onboarding

- Component map: VQ-VAE (Encoder -> Downsampling -> Attention Block -> Codebook) -> Discrete tokens -> Autoregressive Transformer (Input embedding -> Positional embeddings -> Causal attention -> Output head) -> Predicted precipitation maps
- Critical path: VQ-VAE → Discrete tokens → Autoregressive Transformer (with EVL regularization) → Predicted precipitation maps
- Design tradeoffs:
  - VQ-VAE vs. continuous feature representation: Discrete tokens offer efficient compression but may lose some fine-grained information.
  - Autoregressive vs. non-autoregressive: Autoregressive models can capture complex temporal dependencies but are computationally more expensive.
  - EVL regularization: Improves extreme event prediction but adds complexity to the training process.
- Failure signatures:
  - Poor reconstruction quality: Indicates issues with the VQ-VAE encoder or decoder.
  - Inconsistent predictions over time: Suggests problems with the Autoregressive Transformer's ability to model temporal dynamics.
  - Inaccurate extreme event detection: Points to issues with the EVL regularization or the extreme tokens classifier.
- First 3 experiments:
  1. Train the VQ-VAE on the precipitation dataset and evaluate the reconstruction quality using metrics like MSE and PCC.
  2. Train the Autoregressive Transformer on the discrete tokens from the VQ-VAE and assess its ability to predict future precipitation maps using metrics like MAE and CSI.
  3. Incorporate the EVL regularization and evaluate the model's performance in detecting extreme precipitation events using metrics like AUC and FAR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the EVL regularization perform on datasets with different extreme event distributions compared to the KNMI dataset?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of EVL on the KNMI dataset but does not explore its performance on datasets with varying extreme event distributions.
- Why unresolved: The paper's experiments are limited to the KNMI dataset, which may not represent all possible extreme event distributions. Further testing on diverse datasets is needed to validate the generalizability of EVL.
- What evidence would resolve it: Conducting experiments on multiple datasets with different extreme event distributions and comparing the performance of EVL against other regularization techniques.

### Open Question 2
- Question: Can the proposed NowcastingGPT-EVL model be effectively adapted for nowcasting other extreme weather events beyond precipitation, such as hurricanes or tornadoes?
- Basis in paper: [explicit] The paper focuses on extreme precipitation nowcasting but does not explore the model's applicability to other types of extreme weather events.
- Why unresolved: The model's architecture and training process are specifically tailored for precipitation data. Adapting it to other weather events may require significant modifications and validation.
- What evidence would resolve it: Adapting the model for other extreme weather events, retraining on relevant datasets, and comparing its performance against existing models for those events.

### Open Question 3
- Question: What are the limitations of the proposed model in terms of computational resources and how can they be addressed for real-time nowcasting applications?
- Basis in paper: [explicit] The paper mentions the high computational cost of some baseline models but does not provide a detailed analysis of the computational requirements of NowcastingGPT-EVL.
- Why unresolved: The paper does not discuss the model's scalability or its suitability for real-time applications, which are crucial for practical nowcasting systems.
- What evidence would resolve it: Conducting experiments to measure the model's computational requirements, exploring optimization techniques to reduce computational cost, and testing the model's performance in real-time scenarios.

## Limitations

- The dynamic EVL regularization's specific contribution to performance is not fully isolated through ablation studies
- Limited evaluation to the KNMI dataset may restrict generalizability to other geographic regions with different precipitation patterns
- Computational efficiency claims lack context for real-time operational requirements and scaling to larger domains

## Confidence

**High Confidence**:
- The VQ-VAE + Autoregressive Transformer architecture effectively models precipitation patterns and outperforms baselines on standard metrics (MSE, PCC, CSI)
- The model achieves state-of-the-art AUC scores for extreme event detection across multiple thresholds
- TECO's superior temporal consistency despite fewer parameters is well-demonstrated

**Medium Confidence**:
- The dynamic EVL regularization specifically improves extreme event prediction beyond what the base architecture provides
- The assumption about changing hidden space topology is valid and beneficial for capturing extreme events
- The model's computational efficiency is adequate for operational nowcasting

## Next Checks

1. **Ablation study on EVL contribution**: Train NowcastingGPT without the EVL regularization and measure the specific performance drop in extreme event detection metrics (AUC, CSI, FAR) to isolate the EVL's contribution.

2. **Cross-dataset generalization test**: Evaluate the model on precipitation datasets from different geographic regions (e.g., US NEXRAD, Japanese radar data) to assess how well the dynamic EVL adapts to different precipitation patterns.

3. **Hidden space topology analysis**: Visualize and analyze the hidden space representations throughout training to empirically verify that topology changes occur and correlate these changes with improvements in extreme event detection performance.