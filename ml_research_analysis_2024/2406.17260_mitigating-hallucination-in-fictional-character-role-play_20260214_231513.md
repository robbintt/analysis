---
ver: rpa2
title: Mitigating Hallucination in Fictional Character Role-Play
arxiv_id: '2406.17260'
source_url: https://arxiv.org/abs/2406.17260
tags:
- knowledge
- baseline
- hallucination
- rolefact
- character
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucination in fictional
  character role-play, where large language models (LLMs) often make characters act
  out of character or hallucinate about things outside their knowledge. The authors
  propose RoleFact, a method that mitigates hallucination by modulating the influence
  of parametric knowledge using a pre-calibrated confidence threshold.
---

# Mitigating Hallucination in Fictional Character Role-Play

## Quick Facts
- arXiv ID: 2406.17260
- Source URL: https://arxiv.org/abs/2406.17260
- Reference count: 19
- This paper addresses hallucination in fictional character role-play by modulating parametric knowledge influence using confidence thresholds, improving factual precision by 18% for adversarial questions.

## Executive Summary
This paper tackles the challenge of hallucination in fictional character role-play, where large language models often make characters act out of character or hallucinate facts outside their knowledge. The authors propose RoleFact, a method that mitigates hallucination by decomposing responses into atomic facts and verifying them against both retrieved knowledge and parametric LLM knowledge using a pre-calibrated confidence threshold. RoleFact demonstrates significant improvements in factual precision (18% for adversarial questions) and temporal hallucination reduction (44% for time-sensitive interviews). The authors also introduce the SGR dataset with over 2,000 characters and 72,000 interviews, providing a valuable resource for studying various types of hallucinations in character role-play.

## Method Summary
RoleFact is a role-playing method that generates responses using a character profile and retrieved knowledge, then updates responses based on atomic fact verification using both parametric and non-parametric retrieved knowledge. The system decomposes generated responses into atomic facts, checks each fact against retrieved knowledge, and keeps facts supported by retrieved knowledge. Facts only supported by parametric knowledge are kept only if the confidence exceeds a pre-calibrated threshold. The method uses self-reflection to remove unverified atomic facts from the response, effectively eliminating hallucinations. The approach is evaluated on the SGR dataset with 4 diverse tasks: adversarial interview, open-ended interview, dialogue completion, and scene-grounded interview.

## Key Results
- RoleFact improves factual precision by 18% for adversarial questions
- Reduces temporal hallucination by 44% for time-sensitive interviews
- Improves factual precision by 23% for less popular characters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RoleFact improves factual precision by modulating parametric knowledge influence using a calibrated confidence threshold
- Mechanism: The system decomposes generated responses into atomic facts and checks each fact against both retrieved (non-parametric) and parametric knowledge. Facts supported by retrieved knowledge are kept; facts only supported by parametric knowledge are kept only if the confidence exceeds a pre-calibrated threshold
- Core assumption: LLM parametric knowledge is the primary source of hallucination, and its influence can be reliably modulated via confidence thresholds
- Evidence anchors: [abstract] "RoleFact, a role-playing method that mitigates hallucination by modulating the influence of parametric knowledge using a pre-calibrated confidence threshold"; [section] "Facts that are only supported by parametric knowledge of LLMs may remain in the final response if the confidence is above a calibrated threshold"

### Mechanism 2
- Claim: Fact decomposition into atomic facts enables fine-grained factuality evaluation
- Mechanism: The system uses a decomposition function to break responses into atomic facts in "name-only third person" format, which are then individually verified against knowledge sources
- Core assumption: Atomic fact decomposition can capture all relevant factual claims in a response and that this granularity enables precise hallucination detection
- Evidence anchors: [abstract] "Experiments show that the proposed method improves the factual precision of generated responses by 18% for adversarial questions"; [section] "The response z is decomposed into a list of atomic facts Az with a function DEC : Z → Az"

### Mechanism 3
- Claim: Self-reflection updates improve temporal hallucination reduction by removing unsupported facts
- Mechanism: After fact verification, the system performs self-reflection to remove unverified atomic facts from the response, effectively eliminating hallucinations that would otherwise persist
- Core assumption: LLMs can reliably identify and remove their own hallucinated content through self-reflection when guided by fact verification results
- Evidence anchors: [abstract] "reduces temporal hallucination by 44% for time-sensitive interviews"; [section] "The final response y is generated by updating the intermediate response z via self-reflection conditioned on x"

## Foundational Learning

- Concept: Atomic fact decomposition and verification
  - Why needed here: Enables fine-grained evaluation of factuality at the level of individual claims rather than whole responses
  - Quick check question: How does decomposing responses into atomic facts enable more precise hallucination detection compared to evaluating entire responses?

- Concept: Retrieval-augmented generation (RAG) and its limitations
  - Why needed here: Understanding why RAG alone is insufficient for hallucination mitigation in role-play scenarios
  - Quick check question: Why does the paper argue that RAG cannot guarantee most claims are supported by non-parametric knowledge?

- Concept: Confidence calibration and threshold setting
  - Why needed here: The core mechanism relies on setting appropriate confidence thresholds for accepting parametric knowledge claims
  - Quick check question: What happens to informativeness and hallucination rates when the confidence threshold is set too high versus too low?

## Architecture Onboarding

- Component map: Retrieval module → Response generation → Fact decomposition → Fact verification (both retrieval and parametric) → Self-reflection update → Final response
- Critical path: The verification and self-reflection components are critical for hallucination mitigation; if either fails, the system loses its effectiveness
- Design tradeoffs: Higher precision vs. informativeness (tradeoff managed by confidence threshold); more retrieved documents vs. precision (shown to degrade with excess context)
- Failure signatures: Poor retrieval quality directly impacts factual precision; overly conservative thresholds reduce informativeness; insufficient sample size in confidence estimation reduces reliability
- First 3 experiments:
  1. Test different confidence threshold values (t) and sample sizes (m) on validation set to find optimal balance between fact score and SFPR
  2. Compare different retrieval methods (BM25, S-BERT, Contriever) with varying numbers of retrieved documents
  3. Perform ablation study removing retrieval, role profile, or parametric knowledge components to measure individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RoleFact perform on characters from less popular stories compared to characters from well-known franchises like Harry Potter or Star Wars?
- Basis in paper: [explicit] The paper mentions that RoleFact improves factual precision by 23% for less popular characters
- Why unresolved: The paper does not provide specific performance metrics for different levels of character popularity
- What evidence would resolve it: Detailed performance metrics for characters from various popularity levels, including quantitative comparisons

### Open Question 2
- Question: What is the impact of increasing the number of retrieved documents beyond five on the factual precision of RoleFact?
- Basis in paper: [explicit] The paper notes that increasing the number of retrieved documents beyond five may hurt factual precision
- Why unresolved: The paper does not explore the reasons behind this phenomenon or provide detailed analysis of the impact
- What evidence would resolve it: A comprehensive study on the relationship between the number of retrieved documents and factual precision, including potential explanations for the observed trend

### Open Question 3
- Question: How does RoleFact compare to other retrieval-augmented methods in terms of reducing hallucination and improving factual precision?
- Basis in paper: [explicit] The paper compares RoleFact to baselines like KGR and SR, but does not compare it to other retrieval-augmented methods
- Why unresolved: The paper does not explore the performance of RoleFact relative to other state-of-the-art retrieval-augmented methods
- What evidence would resolve it: Comparative analysis of RoleFact with other retrieval-augmented methods on the same tasks and datasets, including quantitative performance metrics

## Limitations

- The pre-calibrated confidence threshold may not generalize well across different domains or LLM architectures
- Atomic fact decomposition may miss complex or implicit factual claims that contribute to hallucination
- The effectiveness of self-reflection depends on the LLM's ability to accurately identify and remove its own hallucinated content

## Confidence

**High Confidence**: The core mechanism of modulating parametric knowledge influence through confidence thresholds is well-supported by the experimental results, showing 18% improvement in factual precision for adversarial questions and 44% reduction in temporal hallucination for time-sensitive interviews.

**Medium Confidence**: The effectiveness of atomic fact decomposition and verification is supported by the improved factuality metrics, but the approach's ability to capture all relevant factual claims remains uncertain. The self-reflection mechanism's reliability in removing hallucinated content also warrants further validation.

**Low Confidence**: The generalizability of the pre-calibrated confidence threshold across different LLM architectures and domains is not thoroughly explored. The paper's claims about the SGR dataset's comprehensiveness and its ability to capture various types of hallucinations also require further validation.

## Next Checks

1. Test RoleFact's effectiveness across different LLM architectures (e.g., GPT-4, Claude, LLaMA) to assess the generalizability of the pre-calibrated confidence threshold and the atomic fact decomposition approach

2. Apply RoleFact to non-fictional domains (e.g., technical documentation, news articles) to evaluate its effectiveness in mitigating hallucination beyond fictional character role-play scenarios

3. Conduct human evaluations to assess the quality and coherence of RoleFact-generated responses, particularly focusing on whether the method's emphasis on factuality compromises the natural flow and informativeness of the responses