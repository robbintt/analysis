---
ver: rpa2
title: 'Quantile Activation: Correcting a Failure Mode of ML Models'
arxiv_id: '2405.11573'
source_url: https://arxiv.org/abs/2405.11573
tags:
- qact
- distribution
- activation
- quantile
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses a fundamental failure mode in machine learning
  where standard models fail when samples come from mixed distributions or under distribution
  shifts. The authors propose Quantile Activation (QAct), an activation function that
  outputs the relative quantile position of neuron activations within their context
  distribution, allowing neurons to adapt to their context.
---

# Quantile Activation: Correcting a Failure Mode of ML Models

## Quick Facts
- arXiv ID: 2405.11573
- Source URL: https://arxiv.org/abs/2405.11573
- Authors: Aditya Challa; Sravan Danda; Laurent Najman; Snehanshu Saha
- Reference count: 21
- Primary result: QAct significantly outperforms ReLU and DINOv2 on CIFAR10C, CIFAR100C, and TinyImagenetC datasets across various architectures, achieving 65.34% accuracy at severity 5 distortions vs 35.86% for ReLU

## Executive Summary
This paper addresses a fundamental failure mode in machine learning where standard models fail when samples come from mixed distributions or under distribution shifts. The authors propose Quantile Activation (QAct), an activation function that outputs the relative quantile position of neuron activations within their context distribution, allowing neurons to adapt to their context. The method involves computing the cumulative distribution function (CDF) of pre-activations at each neuron using the batch distribution, and using this as the activation. They also introduce a "grounding" mechanism where positive and negative values are weighted equally to prevent all neurons from learning the same behavior.

## Method Summary
The proposed Quantile Activation (QAct) addresses distribution shift failures by making neuron activations context-aware. For each neuron, QAct computes the cumulative distribution function (CDF) of its pre-activations using the current batch distribution, then uses this CDF value as the activation output. A grounding mechanism ensures positive and negative values are weighted equally to prevent uniform neuron behavior. For backpropagation, kernel density estimation approximates the density of the weighted distribution. This allows neurons to adapt their activation behavior based on their local distribution context rather than using fixed activation thresholds like ReLU.

## Key Results
- At severity 5 distortions on CIFAR10C, QAct achieves 65.34% accuracy vs 35.86% for ReLU and 30.93% for DINOv2
- Calibration error remains constant across all severity levels for QAct while increasing for other methods
- QAct shows smaller accuracy drops between severity levels compared to baselines
- Works well with different loss functions, with slight improvements using watershed loss

## Why This Works (Mechanism)
Standard activation functions like ReLU use fixed thresholds that don't account for the changing distribution of inputs, especially under distribution shifts or when mixing samples from different distributions. QAct instead makes each neuron's activation relative to its current context by computing the CDF of pre-activations within the batch. This allows neurons to adapt their behavior based on whether inputs fall in typical or atypical regions of their current distribution. The grounding mechanism prevents neurons from collapsing to trivial solutions by ensuring balanced treatment of positive and negative values.

## Foundational Learning

**Kernel Density Estimation**: Non-parametric method to estimate probability density functions from data samples. Needed for approximating the density function required in backpropagation through the CDF computation. Quick check: Verify the KDE bandwidth selection doesn't introduce excessive variance.

**Cumulative Distribution Function (CDF)**: Function giving the probability that a random variable takes a value less than or equal to x. Needed as the core transformation that makes activations context-aware. Quick check: Ensure CDF computation is numerically stable across different batch statistics.

**Batch Normalization Context**: Understanding how batch statistics affect model behavior during training vs inference. Needed to appreciate why context-dependent activations matter. Quick check: Compare QAct behavior under different batch sizes.

## Architecture Onboarding

**Component Map**: Input -> Linear/Conv Layer -> Pre-activation -> Kernel Density Estimation -> CDF Computation -> Grounding Mechanism -> Output Activation

**Critical Path**: The computational bottleneck is the kernel density estimation and CDF computation for each neuron, which must be performed per batch rather than as a fixed operation.

**Design Tradeoffs**: QAct trades computational efficiency for distribution-shift robustness. The grounding mechanism adds hyperparameters but prevents trivial solutions. The method requires careful batch size selection as smaller batches may yield noisier density estimates.

**Failure Signatures**: Poor performance with very small batch sizes due to noisy density estimates, potential overfitting to batch-specific statistics, and computational overhead that scales with model size and batch dimensions.

**First Experiments**:
1. Compare QAct vs ReLU accuracy on CIFAR10C at different severity levels with varying batch sizes
2. Measure computational overhead (FLOPs, memory) of QAct vs ReLU across different architectures
3. Ablate the grounding mechanism to quantify its impact on preventing uniform neuron behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity of CDF computation using kernel density estimation may be prohibitive for large-scale models and datasets
- Grounding mechanism introduces additional hyperparameters requiring careful tuning
- Performance claims heavily rely on synthetic corruption benchmarks with limited real-world applicability demonstration

## Confidence

**Empirical Results on Benchmarks**: High confidence in the strong performance improvements on CIFAR10C, CIFAR100C, and TinyImagenetC datasets

**Scalability and Practical Implementation**: Medium confidence due to lack of detailed discussion on computational overhead and memory requirements

**Real-world Applicability**: Low confidence as the method is primarily validated on synthetic corruption benchmarks rather than natural distribution shifts

## Next Checks

1. Evaluate QAct on large-scale real-world datasets with natural distribution shifts (e.g., domain adaptation tasks, cross-dataset generalization) to assess practical utility beyond synthetic corruptions

2. Benchmark the computational overhead and memory requirements of QAct compared to ReLU across different batch sizes and model architectures to determine scalability limits

3. Conduct ablation studies on the grounding mechanism and kernel density estimation parameters to quantify their impact on performance and identify optimal configurations