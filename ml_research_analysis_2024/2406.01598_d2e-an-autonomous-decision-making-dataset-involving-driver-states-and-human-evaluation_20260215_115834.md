---
ver: rpa2
title: D2E-An Autonomous Decision-making Dataset involving Driver States and Human
  Evaluation
arxiv_id: '2406.01598'
source_url: https://arxiv.org/abs/2406.01598
tags:
- data
- driving
- dataset
- driver
- simulator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces D2E, a dataset for autonomous driving decision-making
  that integrates driver states, vehicle data, environmental information, and human
  evaluation scores. It addresses the limitations of existing datasets, which lack
  comprehensive human-factor data and high-interaction scenarios.
---

# D2E-An Autonomous Decision-making Dataset involving Driver States and Human Evaluation

## Quick Facts
- arXiv ID: 2406.01598
- Source URL: https://arxiv.org/abs/2406.01598
- Reference count: 37
- The paper introduces D2E, a dataset for autonomous driving decision-making that integrates driver states, vehicle data, environmental information, and human evaluation scores.

## Executive Summary
D2E is a novel autonomous driving dataset that addresses critical gaps in existing resources by integrating comprehensive human factor data with vehicle and environmental information. The dataset combines driving simulator data from 80 drivers across 12 high-interaction scenarios with real-world driving data from 7 drivers, enriched with physiological signals (heart rate, EDA, SpO2), eye-tracking data, and first-person videos. Additionally, 40 human volunteers provide subjective evaluation scores for each driving event, creating a rich resource for training and evaluating autonomous driving decision-making models that can learn from human behavior patterns.

## Method Summary
The dataset was collected through a multi-stage process involving driving simulators and real-world vehicles. Simulator data was gathered from 80 qualified drivers across 12 high-interaction scenarios while simultaneously recording physiological signals via wearable sensors and eye-tracking data via specialized glasses. Real-world driving data was collected from 7 drivers using V2X systems for environmental perception. All events were evaluated by 40 third-party human volunteers using standardized scoring based on first-person view videos. The data underwent filtering to select high-interaction scenarios, resulting in over 1100 interactive driving cases suitable for research on driver intelligence learning, trajectory prediction, and decision-making evaluation.

## Key Results
- Dataset integrates driver physiological data, eye-tracking, vehicle information, environmental data, and human evaluation scores
- Successfully collected 12 high-interaction simulator scenarios from 80 drivers and 153 real-world segments from 7 drivers
- Generated over 1100 interactive driving cases after data filtering, supporting multiple autonomous driving research applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating driver physiological and eye-tracking data with environmental information enables richer behavioral modeling for autonomous decision-making.
- Mechanism: Physiological signals capture driver stress and cognitive load while eye-tracking reveals attention allocation, providing a holistic view of driver decision-making processes that can be learned by models.
- Core assumption: Driver physiological and eye-tracking responses are reliably correlated with decision-making quality and can be synchronized with environmental data streams.
- Evidence anchors:
  - [abstract] "D2E collects human-factor data for each event, including physiological data such as heart rate, blood oxygen, and eye-tracking data"
  - [section III.B] "The wearable physiological recorder includes wrist sensors, finger sensors, and earlobe sensors... which can record physiological data such as Electrodermal Activity (EDA), Skin Temperature (SKT), Beats Per Minute (BPM), Pulse Oxygen Saturation (SpO2)"
  - [section III.B] "The eye-tracking glasses can record the driver's first-person view and capture real-time gaze points"

### Mechanism 2
- Claim: Mixing simulator and real-world driving data provides both safety and authenticity for training and validation of autonomous driving models.
- Mechanism: Simulator data enables collection of high-interaction, high-risk scenarios that would be dangerous to reproduce in reality, while real-world data provides authentic driving patterns and interactions.
- Core assumption: Simulator scenarios can be designed to accurately represent real-world driving complexity, and models trained on simulator data can generalize to real-world validation.
- Evidence anchors:
  - [abstract] "The dataset is mixed of driving simulator scenes and real-road ones"
  - [section III.A] "The dataset primarily utilizes a driving simulator for driver safety and experimental repeatability and complements part of real-world driving data"
  - [section III.C] "To enhance the diversity and authenticity of the dataset, and to ensure that models trained on the driving simulator dataset can be tested on the real-vehicle data set, we collected real-vehicle data"

### Mechanism 3
- Claim: Third-party human evaluation of driving performance provides objective assessment of decision quality beyond binary collision/non-collision metrics.
- Mechanism: By having 40 human volunteers rate each driving event on a percentage scale, the dataset captures nuanced assessments of driving behavior quality, allowing models to learn what constitutes good versus poor decision-making.
- Core assumption: Human evaluators can consistently and reliably assess driving performance across diverse scenarios and drivers, providing meaningful quality signals for model training.
- Evidence anchors:
  - [abstract] "D2E also provides subjective rating scores from 40 human volunteers"
  - [section III.D] "we recruited 40 third-party volunteers as assessors to serve as experts for evaluation and rating"
  - [section IV.C] "Table III lists the distribution frequency of the scoring of 12 scenes on the simulator with the highest two items marked down and the lowest one displayed in bold"

## Foundational Learning

- Concept: Data synchronization and alignment across multiple sensor streams
  - Why needed here: The dataset combines simulator data, physiological recordings, eye-tracking, first-person video, and human evaluation scores, all collected at different rates and from different devices.
  - Quick check question: If physiological data is collected at 250Hz and eye-tracking at 60Hz, what approach would you use to align these with simulator data collected at 10Hz?

- Concept: Scenario design for high-interaction driving
  - Why needed here: The dataset specifically focuses on high-interaction scenarios rather than simple car-following, requiring understanding of what makes scenarios interactive.
  - Quick check question: What defines an "interactive" driving scenario versus a "simple" one, and how would you categorize a highway lane change versus a protected left turn?

- Concept: Human factor data collection and preprocessing
  - Why needed here: The dataset includes specialized human factor data (physiological signals, eye-tracking, subjective ratings) that require specific collection protocols and preprocessing steps.
  - Quick check question: What preprocessing steps would be necessary to clean and normalize heart rate variability data collected during high-stress driving scenarios?

## Architecture Onboarding

- Component map: Data collection pipeline (simulator hardware → data acquisition → synchronization → storage), real vehicle data collection (V2X perception → sensor fusion → storage), human evaluation system (video playback → rating interface → aggregation), data filtering pipeline (quality checks → outlier removal → final dataset assembly)
- Critical path: Simulator scenario execution → simultaneous data collection from all sensors → time synchronization → storage → human evaluation → final dataset assembly
- Design tradeoffs: Simulator authenticity vs. safety and repeatability, data collection comprehensiveness vs. participant burden, evaluation standardization vs. subjective nuance, dataset size vs. data quality
- Failure signatures: Desynchronized data streams, missing sensor data for events, inconsistent human ratings, simulator artifacts affecting driver behavior, poor transfer between simulator and real-world data
- First 3 experiments:
  1. Verify data synchronization by checking timestamp alignment across physiological, eye-tracking, and vehicle data for a sample of events
  2. Test human evaluation consistency by having a subset of evaluators rate the same events and computing inter-rater reliability
  3. Validate simulator-real world transfer by training a simple model on simulator data and evaluating on real-world data, measuring performance drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do driver physiological signals (e.g., heart rate, EDA) correlate with specific driving behaviors or decision-making patterns in high-interaction scenarios?
- Basis in paper: [explicit] The paper collects physiological data such as heart rate, blood oxygen, EDA, and skin temperature, and suggests exploring driver behavioral mechanisms by combining human factor data and driving environment data.
- Why unresolved: The paper does not analyze the relationship between physiological signals and driving behaviors, nor does it provide a framework for interpreting these correlations.
- What evidence would resolve it: A detailed statistical or machine learning analysis linking physiological signals to specific driving decisions or scenarios, with validation on the dataset.

### Open Question 2
- Question: Does the subjective evaluation score correlate with objective safety metrics (e.g., time-to-collision, braking intensity) in the same driving event?
- Basis in paper: [explicit] The paper collects human evaluation scores but does not compare them to objective safety or performance metrics.
- Why unresolved: The paper does not provide analysis linking subjective scores to measurable driving performance indicators.
- What evidence would resolve it: Correlation analysis between evaluation scores and objective safety metrics across the dataset.

### Open Question 3
- Question: How does the performance of decision-making models trained on simulator data compare to those trained on real-world data when tested on each other?
- Basis in paper: [explicit] The paper mentions that the dataset includes both simulator and real-world data, with the simulator focusing on high-interaction scenarios and real-world data providing authenticity.
- Why unresolved: The paper does not conduct experiments comparing model performance across these two data sources.
- What evidence would resolve it: Benchmarking studies showing model performance differences when trained and tested on simulator vs. real-world data.

### Open Question 4
- Question: What is the impact of driver experience (e.g., years of driving, accident history) on performance in high-interaction scenarios?
- Basis in paper: [explicit] The paper collects detailed driver demographics, including driving experience and accident history, but does not analyze their impact on driving performance.
- Why unresolved: The paper does not explore how driver characteristics influence behavior in the dataset.
- What evidence would resolve it: Statistical analysis correlating driver demographics with performance metrics or evaluation scores.

## Limitations
- Limited real-world data collection from only 7 drivers may restrict generalizability
- Subjective human evaluation scores introduce potential bias and inconsistency
- Potential simulator artifacts may influence driver behavior rather than reflect natural responses
- The correlation between physiological signals and decision quality across diverse scenarios remains unvalidated

## Confidence
- **High Confidence**: The dataset successfully integrates multiple data streams and provides a novel resource for autonomous driving research
- **Medium Confidence**: The effectiveness of third-party human evaluation in capturing nuanced driving performance and the reliability of inter-rater consistency
- **Low Confidence**: The strength of correlation between physiological signals and decision-making quality across diverse drivers and scenarios

## Next Checks
1. **Data Synchronization Verification**: Perform detailed timestamp alignment analysis across physiological, eye-tracking, and vehicle data for a representative sample of events
2. **Human Evaluation Consistency Testing**: Conduct test-retest reliability assessment by having evaluators score same events multiple times, computing inter-rater reliability metrics
3. **Simulator-Real World Transfer Validation**: Train a simple machine learning model on simulator data and evaluate on real-world dataset, measuring performance drop to quantify transfer gap