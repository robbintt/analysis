---
ver: rpa2
title: Defending Against Sophisticated Poisoning Attacks with RL-based Aggregation
  in Federated Learning
arxiv_id: '2406.14217'
source_url: https://arxiv.org/abs/2406.14217
tags:
- data
- distribution
- learning
- clients
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the vulnerability of federated learning to
  model poisoning attacks, particularly those tailored for servers. It proposes AdaAggRL,
  an RL-based adaptive aggregation method that leverages the observation that benign
  clients exhibit higher data distribution stability than malicious clients.
---

# Defending Against Sophisticated Poisoning Attacks with RL-based Aggregation in Federated Learning

## Quick Facts
- arXiv ID: 2406.14217
- Source URL: https://arxiv.org/abs/2406.14217
- Authors: Yujing Wang; Hainan Zhang; Sijia Wen; Wangjie Qiu; Binghui Guo
- Reference count: 40
- One-line primary result: RL-based adaptive aggregation (AdaAggRL) significantly outperforms existing defenses against sophisticated poisoning attacks in federated learning, maintaining accuracy above 0.95 on MNIST and F-MNIST datasets.

## Executive Summary
This paper addresses the vulnerability of federated learning to model poisoning attacks by proposing AdaAggRL, an RL-based adaptive aggregation method. The key insight is that benign clients exhibit higher data distribution stability than malicious clients. AdaAggRL uses distribution learning via gradient inversion to simulate client data distributions, calculates similarities using MMD, and employs policy learning (TD3) to determine aggregation weights that exclude malicious clients. Experiments on four real-world datasets demonstrate that AdaAggRL maintains significantly higher global model accuracy than baseline methods under various poisoning attacks, including those specifically designed for servers.

## Method Summary
AdaAggRL is an RL-based adaptive aggregation method for federated learning that defends against model poisoning attacks. The method operates by having the server perform distribution learning on uploaded model parameters using gradient inversion to simulate client data distributions. It then calculates MMD-based similarities between current, historical, and global model distributions. A TD3 policy learning algorithm uses these similarities and reconstruction quality metrics to determine aggregation weights, effectively excluding malicious clients with unstable distributions. The approach is tested on MNIST, F-MNIST, EMNIST, and CIFAR-10 datasets with CNN and ResNet18 architectures.

## Key Results
- AdaAggRL maintains accuracy above 0.95 against various attacks on MNIST and F-MNIST datasets, compared to baseline methods dropping below 0.8
- The method achieves accuracy above 0.85 on EMNIST and CIFAR-10 datasets under attack conditions
- AdaAggRL demonstrates superior performance even against RL-based attacks specifically designed for servers
- The approach tolerates up to 90% malicious clients while maintaining reasonable accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AdaAggRL can distinguish malicious from benign clients by observing the stability of their data distributions over time.
- Mechanism: The method calculates similarities between the current local model data distribution, its historical data distribution, and the global model data distribution using MMD. Benign clients exhibit higher and more stable similarity values, while malicious clients show irregular patterns.
- Core assumption: Benign clients train on consistent local data distributions across epochs, leading to stable simulated distributions, whereas malicious clients deliberately perturb their data distributions to perform attacks.
- Evidence anchors:
  - [abstract]: "We find that benign clients exhibit significantly higher data distribution stability than malicious clients in federated learning in both CV and NLP tasks."
  - [section]: "We find that benign clients maintain higher data distribution similarity, while attack clients show no discernible patterns." and "The similarity of data distributions for benign clients remains high and stable, while malicious clients lack any regular pattern."
- Break condition: If benign clients have non-stationary local data distributions (e.g., concept drift) or if malicious clients successfully mimic stable distribution patterns.

### Mechanism 2
- Claim: RL-based adaptive aggregation weights can effectively mitigate the impact of malicious clients by excluding those with unstable distributions.
- Mechanism: Policy learning (TD3) is used to adaptively determine aggregation weights based on the calculated similarities and reconstruction quality. Clients with lower scores (indicating potential malicious behavior) are excluded from aggregation.
- Core assumption: The RL agent can learn to map the observed similarity metrics to appropriate aggregation weights that minimize empirical loss while excluding malicious clients.
- Evidence anchors:
  - [abstract]: "we use policy learning to adaptively determine the aggregation weights based on the above similarities."
  - [section]: "we use policy learning method TD3 to adaptively determine the aggregation weights based on these similarities."
- Break condition: If the RL agent fails to learn an effective policy mapping or if the similarity metrics are insufficient to capture malicious behavior.

### Mechanism 3
- Claim: Distribution learning via gradient inversion can accurately simulate client data distributions from model parameters, enabling similarity calculations.
- Mechanism: The paper adapts the inverting gradients (IG) method to reconstruct data samples from uploaded model parameters, which are then used to calculate feature distributions and MMD similarities.
- Core assumption: The gradient inversion process can produce data distributions that accurately reflect the client's actual training data distribution, despite being limited to 30 optimization steps and 16 dummy images.
- Evidence anchors:
  - [section]: "we employ distribution learning to simulate the client's data distributions based on the uploaded model parameters. In this work, we adapt the inverting gradients (IG) method (Geiping et al. 2020) for distribution learning."
- Break condition: If the gradient inversion process produces inaccurate distributions or if the limited optimization steps are insufficient for meaningful reconstruction.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: The paper's defense mechanism operates within the FL framework, where multiple clients collaboratively train a global model without sharing their local data.
  - Quick check question: In FL, do clients share their raw data with the server during training? (Answer: No)

- Concept: Model Poisoning Attacks
  - Why needed here: The paper specifically addresses model poisoning attacks, where malicious clients attempt to degrade the global model's performance by sending customized gradients.
  - Quick check question: What is the primary goal of untargeted model poisoning attacks in FL? (Answer: To maximally reduce the accuracy of the global model)

- Concept: Maximum Mean Discrepancy (MMD)
  - Why needed here: MMD is used to calculate pairwise similarities between different data distributions (current, historical, and global) to assess client behavior.
  - Quick check question: What does MMD measure between two distributions? (Answer: The distance between their embeddings in a reproducing kernel Hilbert space)

## Architecture Onboarding

- Component map:
  Clients -> Distribution Learning -> Similarity Calculation -> Policy Learning -> Aggregation

- Critical path:
  1. Clients upload updated model parameters to server
  2. Server performs distribution learning to simulate client data distributions
  3. Server calculates similarity metrics (current-history, current-global, history-global)
  4. Server uses RL policy to determine aggregation weights
  5. Server aggregates models with weights, excluding low-scoring clients

- Design tradeoffs:
  - Computational cost vs. defense effectiveness: Gradient inversion and MMD calculations add overhead but improve attack detection
  - Number of optimization steps in gradient inversion: 30 steps chosen to balance accuracy and efficiency
  - Number of reconstructed images: 16 images used to capture distribution characteristics without excessive computation

- Failure signatures:
  - If accuracy degrades over time despite no attacks, the RL policy may be over-aggressive in excluding clients
  - If accuracy remains low even with attacks, the distribution learning or similarity calculations may be ineffective
  - If training becomes unstable, the policy learning may be overfitting to specific attack patterns

- First 3 experiments:
  1. Test AdaAggRL against a single attack type (e.g., IPM) on MNIST with 10% malicious clients
  2. Evaluate the effect of varying the number of gradient inversion optimization steps (10, 30, 50) on defense performance
  3. Test AdaAggRL's performance as the fraction of malicious clients increases from 10% to 90% on a single dataset

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- The effectiveness of gradient inversion for accurate distribution learning with limited optimization steps remains uncertain
- The method's performance against adaptive attacks targeting the distribution learning process is unknown
- The RL policy may overfit to specific attack patterns, reducing generalization to unseen attack types

## Confidence
- High Confidence: Experimental results demonstrating AdaAggRL's effectiveness against various attacks on tested datasets are reliable
- Medium Confidence: The theoretical foundation of using distribution stability as a proxy for client trustworthiness is plausible but requires more extensive validation
- Low Confidence: The robustness of the gradient inversion method for accurate distribution learning and the generalization of the RL policy to unseen attack patterns are the most uncertain aspects

## Next Checks
1. Cross-dataset validation: Test AdaAggRL's performance on additional datasets beyond the four used in the paper, particularly datasets with different characteristics (e.g., text data, more complex image distributions)
2. Robustness to concept drift: Evaluate AdaAggRL's performance in scenarios where benign clients experience concept drift in their local data distributions over time
3. Ablation study on gradient inversion: Conduct an ablation study to assess the impact of varying the number of gradient inversion optimization steps and the number of reconstructed images on the accuracy of the simulated distributions