---
ver: rpa2
title: Revisiting speech segmentation and lexicon learning with better features
arxiv_id: '2401.17902'
source_url: https://arxiv.org/abs/2401.17902
tags:
- lexicon
- speech
- segmentation
- word
- hubert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors propose a method for unsupervised word segmentation
  and lexicon learning from unlabelled speech. They build on a two-stage dynamic programming
  approach: first discovering acoustic units via HuBERT-based clustering, then segmenting
  into word-like units using an AE-RNN scoring function.'
---

# Revisiting speech segmentation and lexicon learning with better features

## Quick Facts
- arXiv ID: 2401.17902
- Source URL: https://arxiv.org/abs/2401.17902
- Reference count: 0
- Primary result: State-of-the-art lexicon quality (NED) on ZeroSpeech Challenge Track 2 across five languages

## Executive Summary
This paper proposes a method for unsupervised speech segmentation and lexicon learning using HuBERT features and a two-stage dynamic programming approach. The method first discovers acoustic units via constrained clustering, then segments speech into word-like units using an AE-RNN scoring function. To learn a lexicon, it averages HuBERT features within each discovered word segment and clusters these embeddings using K-means. Applied to five languages in the ZeroSpeech Challenge, this approach achieves state-of-the-art lexicon quality and competitive segmentation performance compared to other full-coverage systems.

## Method Summary
The authors build on a two-stage dynamic programming approach for unsupervised speech segmentation and lexicon learning. First, acoustic units are discovered using constrained clustering with DPDP on seventh-layer HuBERT features. Second, speech is segmented into word-like units using an AE-RNN scoring function that reconstructs sequences of acoustic units. To build a lexicon, HuBERT features are averaged within each hypothesized segment and clustered using K-means. The method is evaluated on ZeroSpeech Challenge Track 2 datasets (English, French, Mandarin, German, Wolof) using boundary F1, token F1, and normalized edit distance metrics.

## Key Results
- State-of-the-art lexicon quality (NED) across all five ZeroSpeech Challenge languages
- Competitive segmentation performance compared to other full-coverage systems
- Demonstrates superiority of HuBERT features over CPC for acoustic unit discovery
- Achieves balanced performance across languages with different phonotactic structures

## Why This Works (Mechanism)

### Mechanism 1
HuBERT features improve acoustic unit discovery over CPC features by providing higher-quality contextualized embeddings that better correlate with phone-like units, leading to cleaner segmentation and more distinct acoustic units. This relies on the assumption that the seventh layer of HuBERT-Base yields the best correlation with phone labels for acoustic unit discovery.

### Mechanism 2
AE-RNN scoring function effectively distinguishes word-like segments from non-words by learning to reconstruct sequences of acoustic units. Segments that reconstruct well are likely coherent words, while poor reconstructions indicate non-words. This assumes word segments have internal consistency that can be captured by reconstruction likelihood.

### Mechanism 3
Averaging HuBERT features within segments and clustering with K-means yields better lexicons than using AE-RNN embeddings directly because segment-level averaging produces stable acoustic word embeddings that capture the essential characteristics of each hypothesized word. This assumes word segments are acoustically coherent enough that averaging their HuBERT features yields discriminative embeddings.

## Foundational Learning

- Concept: Dynamic Programming with Duration Penalty
  - Why needed here: To find optimal segmentations that balance acoustic coherence with realistic word lengths
  - Quick check question: How does the duration penalty term in the DPDP objective influence the segmentation results?

- Concept: Self-Supervised Speech Representations
  - Why needed here: To provide meaningful input features without requiring labeled data, crucial for zero-resource settings
  - Quick check question: Why might HuBERT representations be more effective than CPC features for this task?

- Concept: K-means Clustering
  - Why needed here: To group similar acoustic word embeddings into discrete word categories, forming the lexicon
  - Quick check question: What impact does the choice of lexicon size have on the clustering results and final lexicon quality?

## Architecture Onboarding

- Component map: HuBERT feature extraction (7th layer for units, 9th layer for embeddings) -> Constrained clustering with DPDP for acoustic unit discovery -> AE-RNN model for word segmentation scoring -> Segment averaging and K-means clustering for lexicon learning -> Evaluation with boundary F1, token F1, and normalized edit distance

- Critical path:
  1. Extract HuBERT features from raw speech
  2. Discover acoustic units via DPDP clustering
  3. Train AE-RNN on unit sequences
  4. Segment speech into word-like units using AE-RNN scoring
  5. Average HuBERT features within each segment
  6. Cluster averaged embeddings with K-means to form lexicon

- Design tradeoffs:
  - Using fixed HuBERT features vs. fine-tuning on target languages
  - AE-RNN complexity vs. training data requirements
  - Lexicon size balancing granularity vs. sparsity

- Failure signatures:
  - Poor boundary F1 suggests issues in the AE-RNN segmentation stage
  - High NED indicates lexicon clustering is not capturing true word categories
  - Extremely short or long segments may indicate incorrect duration penalty settings

- First 3 experiments:
  1. Validate HuBERT feature quality by checking phone label correlation on development data
  2. Test AE-RNN reconstruction accuracy on unit sequences to ensure it can distinguish words from non-words
  3. Run K-means clustering on a small subset of averaged embeddings to verify cluster separability before full-scale lexicon learning

## Open Questions the Paper Calls Out

### Open Question 1
How do the acoustic unit codebook size and duration penalty hyperparameters affect lexicon learning quality across different languages? The paper states "We use 100 acoustic units and didn't try any other setting" and sets duration penalties based on Buckeye development experiments rather than language-specific tuning, leaving open whether different settings might yield better performance for specific languages.

### Open Question 2
Would incorporating acoustic context information beyond simple averaging of HuBERT features improve lexicon clustering performance? The paper uses simple averaging of HuBERT features for acoustic word embeddings and states "we just tried to K-means cluster the latent AE-RNN embeddings, but this did not perform competitively," but did not explore embeddings that incorporate contextual information from surrounding frames or phone-level predictions.

### Open Question 3
How does the lexicon quality from this approach compare to supervised or semi-supervised segmentation methods when sufficient labeled data is available? The authors state their method "achieves state-of-the-art performance on the ZeroSpeech benchmarks" which are specifically designed for zero-resource scenarios, but don't explore how this approach would perform relative to methods that leverage any amount of labeled data.

## Limitations
- Relies heavily on HuBERT features without fine-tuning on target languages, potentially limiting capture of language-specific acoustic characteristics
- AE-RNN architecture details are not fully specified, making exact reproduction difficult
- Choice of 100 acoustic units and lexicon sizes (100-500) appears somewhat arbitrary without systematic justification

## Confidence
- **High confidence**: Superiority of HuBERT features over CPC for acoustic unit discovery is well-supported by ASR literature and paper results
- **Medium confidence**: AE-RNN scoring effectively distinguishes word-like segments, though lacks detailed reconstruction accuracy validation
- **Low confidence**: Insufficient detail on AE-RNN architecture and training procedures; impact of hyperparameter choices not thoroughly explored

## Next Checks
1. **AE-RNN Reconstruction Validation**: Measure reconstruction accuracy on held-out code sequences and compare average reconstruction loss across word vs. non-word segments to verify distinction capability.

2. **Feature Layer Sensitivity Analysis**: Systematically test different HuBERT layers for both acoustic unit discovery and lexicon clustering to determine if current layer choices are optimal.

3. **Lexicon Size Sweep with NED Stability**: Perform granular sweep of lexicon sizes (50, 100, 200, 300, 400, 500) and measure not just NED but also cluster stability variance across multiple runs to determine true performance optima.