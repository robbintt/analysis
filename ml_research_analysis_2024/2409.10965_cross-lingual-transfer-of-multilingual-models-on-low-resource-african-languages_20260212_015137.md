---
ver: rpa2
title: Cross-lingual transfer of multilingual models on low resource African Languages
arxiv_id: '2409.10965'
source_url: https://arxiv.org/abs/2409.10965
tags:
- languages
- language
- transfer
- cross-lingual
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks cross-lingual transfer capabilities from
  Kinyarwanda (high-resource) to Kirundi (low-resource) using both multilingual and
  monolingual models. The research compares transformer-based architectures (mBERT,
  AfriBERT, BantuBERTa) against neural models (BiGRU, CNN, char-CNN) for a 14-class
  news classification task.
---

# Cross-lingual transfer of multilingual models on low resource African Languages
## Quick Facts
- arXiv ID: 2409.10965
- Source URL: https://arxiv.org/abs/2409.10965
- Reference count: 10
- Primary result: Multilingual models achieve 88.3% accuracy in cross-lingual transfer from Kinyarwanda to Kirundi

## Executive Summary
This study benchmarks cross-lingual transfer capabilities from Kinyarwanda (high-resource) to Kirundi (low-resource) using both multilingual and monolingual models. The research compares transformer-based architectures (mBERT, AfriBERT, BantuBERTa) against neural models (BiGRU, CNN, char-CNN) for a 14-class news classification task. Multilingual models consistently outperformed monolingual ones, with AfriBERT achieving the highest accuracy of 88.3% after fine-tuning on Kirundi. Among neural models, BiGRU showed the best performance at 83.3% accuracy post-fine-tuning.

## Method Summary
The research evaluated cross-lingual transfer from Kinyarwanda to Kirundi using multilingual transformers (mBERT, AfriBERT, BantuBERTa) and monolingual neural models (BiGRU, CNN, char-CNN) on a 14-class news classification task. Models were first trained on Kinyarwanda data, then fine-tuned on Kirundi, and finally evaluated on both languages to measure performance and catastrophic forgetting. The study measured accuracy improvements and catastrophic forgetting percentages across architectures to compare transfer capabilities.

## Key Results
- AfriBERT achieved highest accuracy of 88.3% after fine-tuning on Kirundi
- Multilingual models outperformed monolingual models in cross-lingual transfer
- BantuBERT suffered significant catastrophic forgetting (74%) while others showed minimal forgetting (3-5%)

## Why This Works (Mechanism)
The study demonstrates that multilingual models leverage shared linguistic features between Kinyarwanda and Kirundi to achieve effective cross-lingual transfer. The success appears to stem from the models' ability to transfer contextual representations and attention patterns learned from the high-resource source language to the low-resource target language, particularly benefiting from the linguistic similarities between these Bantu languages.

## Foundational Learning
- Cross-lingual transfer: Why needed - enables knowledge transfer from high-resource to low-resource languages; Quick check - compare performance on target language before and after source language training
- Catastrophic forgetting: Why needed - measures how much source language knowledge is lost during fine-tuning; Quick check - evaluate source language performance before and after target language fine-tuning
- Language similarity impact: Why needed - determines effectiveness of cross-lingual transfer; Quick check - test transfer between linguistically distant language pairs

## Architecture Onboarding
Component map: Input text -> Tokenizer -> Transformer layers -> Classification head
Critical path: Text encoding through multilingual embeddings enables cross-lingual knowledge transfer
Design tradeoffs: Multilingual models balance capacity across languages vs. monolingual models optimized for single language
Failure signatures: Catastrophic forgetting (loss of source language performance), poor adaptation to target language
First experiments: 1) Evaluate baseline performance on target language without transfer, 2) Measure source language performance after target fine-tuning, 3) Compare transfer success across different model architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Findings based on single language pair limit generalizability to other African languages
- Does not explore zero-shot transfer or multi-source language transfer scenarios
- Neural models used are simpler than modern transformer approaches

## Confidence
High confidence: Comparative performance results between multilingual and monolingual models
Medium confidence: Broader claims about multilingual models' cross-lingual transfer capabilities for Bantu languages
Low confidence: Assertions about linguistic similarity being the primary driver of transfer success

## Next Checks
1. Replicate experiment with additional language pairs within Bantu family and between Bantu and non-Bantu languages
2. Conduct ablation studies to determine which aspects of multilingual models contribute most to successful transfer
3. Implement intermediate evaluation checkpoints during fine-tuning to track catastrophic forgetting progression