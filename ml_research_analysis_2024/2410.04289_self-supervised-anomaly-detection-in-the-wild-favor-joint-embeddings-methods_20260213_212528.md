---
ver: rpa2
title: 'Self-Supervised Anomaly Detection in the Wild: Favor Joint Embeddings Methods'
arxiv_id: '2410.04289'
source_url: https://arxiv.org/abs/2410.04289
tags:
- performance
- imbalance
- validation
- learning
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of self-supervised learning
  (SSL) methods for anomaly detection in highly imbalanced datasets, using the Sewer-ML
  infrastructure inspection dataset. Through 250 experiments, the study systematically
  varies class imbalance levels (1%, 2%, 5%, and 15% defect samples) and evaluates
  SSL methods (BYOL, Barlow Twins, SimCLR, DINO, and MAE) with lightweight models
  (ViT-Tiny and ResNet-18).
---

# Self-Supervised Anomaly Detection in the Wild: Favor Joint Embeddings Methods

## Quick Facts
- arXiv ID: 2410.04289
- Source URL: https://arxiv.org/abs/2410.04289
- Reference count: 40
- Primary result: Joint-embedding SSL methods (SimCLR, Barlow Twins) outperform reconstruction-based approaches (MAE) for anomaly detection under high class imbalance, with SimCLR achieving F1 scores up to 0.814 at 15% defect proportion.

## Executive Summary
This paper systematically evaluates self-supervised learning methods for anomaly detection in highly imbalanced sewer infrastructure datasets. Through 250 experiments varying class imbalance levels (1%, 2%, 5%, 15%) and SSL approaches (BYOL, Barlow Twins, SimCLR, DINO, MAE), the study finds that joint-embedding methods like SimCLR and Barlow Twins significantly outperform reconstruction-based methods like MAE, particularly under high imbalance. The research reveals that SSL method choice is more critical than backbone architecture selection, and highlights the need for better representation quality metrics since current methods like RankMe fail to predict downstream performance.

## Method Summary
The study employs the Sewer-ML dataset containing 1.3 million infrastructure inspection images with 17 defect classes. The research implements controlled class imbalance by sampling specific defect proportions (1%, 2%, 5%, 15%) and trains various SSL methods including BYOL, Barlow Twins, SimCLR, DINO, and MAE using either ViT-Tiny or ResNet-18 backbones. Each SSL method is pretrained on the imbalanced dataset, then a linear classifier is trained on frozen features for binary anomaly detection (defect vs. non-defect). Performance is evaluated using F1 score, F2 score weighted by economic significance, and F1 for the non-defect class across training and validation sets.

## Key Results
- Joint-embedding SSL methods (SimCLR, Barlow Twins) achieve significantly higher F1 scores than reconstruction-based methods (MAE), with SimCLR reaching F1 scores up to 0.814 at 15% defect proportion
- SSL method choice has greater impact on performance than backbone architecture selection, with minor differences between ViT-Tiny and ResNet-18
- Current representation quality metrics like RankMe fail to predict downstream anomaly detection performance, showing no correlation with F1 scores
- MAE struggles under class imbalance due to reconstruction bias toward the majority class, while SimCLR and Barlow Twins maintain competitive performance against supervised baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint-embedding SSL methods like SimCLR and Barlow Twins outperform reconstruction-based approaches (MAE) under high class imbalance because they directly align representations of different views without requiring reconstruction of the entire input, avoiding bias toward the majority class.
- Mechanism: In joint-embedding methods, the model learns to maximize agreement between augmented views of the same image while minimizing similarity across different images. This contrastive objective forces the network to focus on discriminative features shared across views, even when the minority class is rare. MAE, by contrast, must reconstruct both majority and minority samples, which can dominate learning toward the majority class representation, reducing sensitivity to minority class features.
- Core assumption: The contrastive loss in joint-embedding methods is sufficiently discriminative to separate minority and majority classes even when the minority class is very sparse.
- Evidence anchors:
  - [abstract] "Our findings highlight the superiority of joint-embedding methods like SimCLR and Barlow Twins over reconstruction-based approaches such as MAE, which struggle to maintain performance under class imbalance."
  - [section 2.2] "MAE's reliance on reconstructing multiple classes might introduce noise that hampers its ability to generalize under severe imbalance."
- Break condition: If the minority class samples lack distinctive visual patterns or if augmentation strategies fail to preserve minority class characteristics, the contrastive signal may become too weak to maintain performance.

### Mechanism 2
- Claim: The choice of SSL methodology (joint-embedding vs. reconstruction) is more critical than the backbone architecture (ViT-Tiny vs. ResNet-18) for anomaly detection performance under imbalance.
- Mechanism: SSL methods define the objective function and training dynamics, which directly influence the learned feature space's ability to capture subtle differences between defect and non-defect images. Backbones provide representational capacity, but if the SSL objective is poorly suited to imbalance (e.g., MAE), even a strong backbone cannot compensate. Conversely, a well-chosen SSL method (e.g., SimCLR) can extract meaningful features even from a lightweight backbone.
- Core assumption: The SSL objective shapes the feature space more profoundly than the architectural inductive biases of the backbone.
- Evidence anchors:
  - [abstract] "Furthermore, we find that the SSL model choice is more critical than the backbone architecture."
  - [section 2.2] "Our results do not indicate a significant difference in performance between these backbones architectures."
- Break condition: If backbone capacity becomes the bottleneck (e.g., extremely small models), the SSL method's advantages may be masked by representational limits.

### Mechanism 3
- Claim: Current label-free SSL representation quality metrics like RankMe fail to predict downstream anomaly detection performance, making cross-validation without labels infeasible.
- Mechanism: RankMe measures properties of the learned embedding space (e.g., clustering quality or information content) but does not capture the alignment between embeddings and the specific anomaly detection task's decision boundary. The SSL objective optimizes for a different goal (view alignment) than the downstream task (class separation), so embedding statistics do not correlate with task performance.
- Core assumption: The SSL pretraining objective induces representations that are not necessarily optimal for anomaly detection without fine-tuning.
- Evidence anchors:
  - [abstract] "Additionally, we emphasize the need for better label-free assessments of SSL representations, as current methods like RankMe fail to adequately evaluate representation quality."
  - [section D.4] "no correlation (Figures 9, 10, indicating that RankMe is not a reliable predictor of downstream performance."
- Break condition: If a new SSL method or metric is developed that explicitly optimizes for anomaly detection-relevant properties, RankMe-like failures might be avoided.

## Foundational Learning

- Concept: Self-supervised learning (SSL) and pretext tasks
  - Why needed here: Understanding how SSL methods create surrogate labels and learn representations without manual annotations is essential to grasp why some SSL approaches (joint-embedding vs. reconstruction) behave differently under class imbalance.
  - Quick check question: What is the main difference between joint-embedding SSL methods (e.g., SimCLR) and reconstruction-based methods (e.g., MAE) in terms of their training objectives?

- Concept: Class imbalance and its impact on model training
  - Why needed here: The paper evaluates how different SSL methods handle highly imbalanced datasets, so understanding how imbalance affects precision, recall, and overall performance is critical for interpreting results.
  - Quick check question: Why does severe class imbalance typically hurt precision more than recall in anomaly detection?

- Concept: Evaluation metrics for anomaly detection (F1, F2, weighted loss)
  - Why needed here: The paper uses multiple metrics (F1, F2 with defect weighting) to evaluate models; knowing their differences helps understand why certain methods appear to perform better under specific conditions.
  - Quick check question: Why might the F2 score remain stable while F1 drops as class imbalance increases in the validation set?

## Architecture Onboarding

- Component map: Image augmentation -> SSL backbone (ViT-Tiny/ResNet-18) -> SSL head (projector/predictor) -> SSL loss computation -> model update -> frozen features -> linear probe -> evaluation

- Critical path:
  1. Load and augment images
  2. Forward pass through SSL backbone and head to get embeddings
  3. Compute SSL loss (contrastive or reconstruction)
  4. Update model weights via optimizer (LARS for joint-embedding, AdamW for MAE, DINO)
  5. After SSL pretraining, freeze backbone and train linear probe
  6. Evaluate on validation set with F1, F2, etc.

- Design tradeoffs:
  - Joint-embedding vs. reconstruction: Joint-embedding better for imbalance but requires careful augmentation; reconstruction can capture global structure but suffers under imbalance
  - Backbone choice: ViT-Tiny lighter, ResNet-18 potentially more stable; SSL method choice has larger impact
  - Augmentation strength: Stronger augmentations can improve SSL but may obscure subtle defect features
  - Loss weighting: Inverse class frequency weighting preserves recall but can hurt precision

- Failure signatures:
  - SSL training diverges or collapses (RankMe drops, feature std collapses)
  - Linear probe fails to converge despite good SSL loss
  - Validation precision drops sharply with increasing imbalance while recall stays high
  - No correlation between RankMe and downstream metrics

- First 3 experiments:
  1. Train SimCLR with ResNet-18 on 15% defect proportion; monitor F1 and RankMe
  2. Train MAE with ViT-Tiny on 1% defect proportion; compare performance and RankMe to SimCLR
  3. Train BYOL with ResNet-18 on 5% defect proportion; evaluate stability across imbalance levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does DINO exhibit such dramatic performance differences between ViT-Tiny and ResNet-18 architectures, particularly in low imbalance settings (1% and 2%)?
- Basis in paper: [explicit] The paper notes that "DINO, however, shows a unique trend, performing poorly in 1% and 2% settings with ResNet-18, but not with ViT-Tiny."
- Why unresolved: The authors attribute this to DINO's focus on global feature learning through knowledge distillation, which aligns better with ViT's global attention approach, but do not provide empirical evidence or deeper analysis of the architectural interaction.
- What evidence would resolve it: Comparative experiments isolating DINO's global attention mechanism, ablation studies on knowledge distillation temperature, and architectural modifications to ResNet-18 to incorporate global attention patterns.

### Open Question 2
- Question: What specific characteristics of joint-embedding methods (SimCLR and Barlow Twins) make them more robust to class imbalance compared to reconstruction-based methods like MAE?
- Basis in paper: [explicit] The paper observes that "MAE has difficulties to maintain performance with varying class imbalance as opposed to methods like SimCLR or Barlow Twins that compete with supervised baselines."
- Why unresolved: The authors mention that MAE's reconstruction bias favors the majority class and that its reliance on reconstructing multiple classes might introduce noise, but do not conduct detailed analysis of the loss functions or feature distributions.
- What evidence would resolve it: Detailed feature space analysis comparing the learned representations, loss landscape visualization across different imbalance levels, and controlled experiments modifying the reconstruction objective to reduce majority class bias.

### Open Question 3
- Question: What alternative metrics or methods could effectively assess SSL representation quality for anomaly detection, given that RankMe fails to correlate with downstream performance?
- Basis in paper: [explicit] The authors state that "methods such as RankMe fail to assess the richness of SSL representations making cross-validation without labels currently impossible on such task and dataset."
- Why unresolved: While the paper demonstrates RankMe's ineffectiveness through scatter plots showing no correlation with F1 scores, it does not propose or test alternative evaluation frameworks for representation quality.
- What evidence would resolve it: Development and validation of new representation quality metrics based on anomaly detection specific criteria, comparison of multiple proposed metrics against downstream performance, and creation of a benchmark dataset specifically designed to test representation evaluation methods for imbalanced anomaly detection.

## Limitations
- Findings may not generalize across different anomaly detection domains with varying defect characteristics
- Exclusive focus on binary classification may not reflect multi-class defect detection complexity
- Cannot fully capture real-world distribution shifts or domain-specific challenges

## Confidence
- **High Confidence**: Joint-embedding SSL methods outperform reconstruction-based approaches under high class imbalance; SSL method choice has greater impact than backbone architecture; RankMe metrics fail to predict downstream performance
- **Medium Confidence**: SimCLR achieves highest F1 scores across all imbalance levels; MAE struggles specifically due to reconstruction bias toward majority class; Performance differences between ViT-Tiny and ResNet-18 are minor
- **Low Confidence**: Claims about SSL methods still lagging behind supervised models; Generalizability of findings to other infrastructure domains; Impact of specific augmentation strategies on minority class preservation

## Next Checks
1. **Cross-Domain Validation**: Test the top-performing SSL methods (SimCLR, Barlow Twins) on other infrastructure inspection datasets with different defect types and visual characteristics to verify generalizability.
2. **Multi-Class Extension**: Replicate experiments using multi-class defect classification to determine if binary classification findings hold when distinguishing between different defect categories.
3. **RankMe Alternative Development**: Develop and validate alternative representation quality metrics that better correlate with downstream anomaly detection performance, potentially incorporating task-specific evaluation criteria.