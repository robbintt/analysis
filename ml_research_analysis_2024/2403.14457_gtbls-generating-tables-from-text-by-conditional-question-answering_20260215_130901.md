---
ver: rpa2
title: 'gTBLS: Generating Tables from Text by Conditional Question Answering'
arxiv_id: '2403.14457'
source_url: https://arxiv.org/abs/2403.14457
tags:
- table
- gtbls
- headers
- tables
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents gTBLS, a two-stage approach for generating
  structured tables from unstructured text. The first stage infers table structure
  (row and column headers) from the text, while the second stage formulates questions
  using these headers and fine-tunes a causal language model to answer them.
---

# gTBLS: Generating Tables from Text by Conditional Question Answering

## Quick Facts
- arXiv ID: 2403.14457
- Source URL: https://arxiv.org/abs/2403.14457
- Reference count: 12
- Primary result: Two-stage approach improves table generation by up to 10% in BERTScore and achieves 57% reduction in error rates compared to sequence-to-sequence methods

## Executive Summary
gTBLS introduces a novel two-stage approach for generating structured tables from unstructured text by first inferring table structure (headers) and then generating content through conditional question answering. This modular approach guarantees syntactic validity by construction and enables the use of large pre-trained language models in a zero-shot configuration. The method achieves significant improvements over prior work across multiple datasets including E2E, WikiTableText, WikiBio, and RotoWire, demonstrating both accuracy gains and robustness in table generation tasks.

## Method Summary
The gTBLS approach splits table generation into two distinct stages: Table Construction and Table Content Generation. In the first stage, a model generates table headers (row and column headers) from the input text. In the second stage, for each combination of row and column headers, the system formulates a question whose answer represents the cell content. These questions are then answered by a causal language model fine-tuned on the task. The approach also supports a zero-shot configuration where instruction-tuned large language models can directly answer the formulated questions without fine-tuning.

## Key Results
- Achieves up to 10% improvement in BERTScore on table construction compared to prior methods
- Demonstrates up to 20% improvement on table content generation across four datasets
- Reduces error rates by up to 57% compared to single-stage sequence-to-sequence approaches
- Guarantees all generated tables are syntactically valid through the two-stage design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting table generation into two stages eliminates syntactic validity errors.
- Mechanism: The first stage generates headers independently, defining the table's exact shape. The second stage generates content for each header combination, guaranteeing equal cell counts across rows and columns.
- Core assumption: Header generation can be isolated from content generation without loss of semantic fidelity.
- Evidence anchors:
  - [abstract] "The first stage infers table structure (row and column headers) from the text. The second stage formulates questions using these headers and fine-tunes a causal language model to answer them."
  - [section 3.1] "Table Construction infers table structure (row and column headers) from text. Table Content Generation uses the generated headers to formulate questions."
  - [corpus] Weak — related papers focus on table QA but not two-stage generation, so no direct corroboration.
- Break condition: If header generation fails to capture all necessary dimensions, the second stage cannot fill in valid content for the intended table structure.

### Mechanism 2
- Claim: Reformulating table content generation as question answering enables zero-shot use of large pre-trained LLMs.
- Mechanism: Once headers are known, each cell can be generated by asking "What is the {Column} for {Row}?" and using a pre-trained QA model to answer.
- Core assumption: QA models can generalize from instruction tuning to novel question templates formed from table headers.
- Evidence anchors:
  - [abstract] "the question-answering reformulation can utilize larger LLMs to achieve parity with fine-tuning approaches"
  - [section 3.2] "gTBLS formulates a question, the answer to which is the cell content. A separate question is formulated for each combination of row and column header."
  - [corpus] Weak — corpus includes QA over tables but not QA as a reformulation for generation, so indirect support only.
- Break condition: If the QA model cannot handle the novel template or lacks relevant factual knowledge, generated content will be incorrect or incomplete.

### Mechanism 3
- Claim: Modular question-answering reformulation allows incremental updates to tables without full regeneration.
- Mechanism: New evidence can be incorporated by generating new QA pairs for existing headers, bypassing the need to reconstruct the entire table.
- Core assumption: Table structure (headers) remains stable enough that only content needs updating.
- Evidence anchors:
  - [abstract] "By reformulating the table generation task as question answering, new evidence can be incorporated into existing tables using gTBLS without regeneration of the entire table."
  - [section 2] "Motivated by these methods, our approach, gTBLS, uses a two-stage process splitting the task into table structure construction and table content generation to capture inter-cell relationships and adhere to tabular constraints."
  - [corpus] Weak — no direct mention in corpus of incremental updates via QA, so this is inferred from design intent.
- Break condition: If table structure changes (new headers needed), the incremental approach fails and full regeneration is required.

## Foundational Learning

- Concept: Transformer decoder with causal masking
  - Why needed here: Ensures autoregressive generation of headers and answers in the correct sequence.
  - Quick check question: What prevents the decoder from attending to future tokens during generation?

- Concept: Teacher forcing in sequence-to-sequence training
  - Why needed here: Provides the correct header or answer during training to stabilize learning before switching to autoregressive inference.
  - Quick check question: During training, does the model see the true previous token or its own prediction?

- Concept: BERTScore as semantic similarity metric
  - Why needed here: Captures semantic equivalence between generated and reference headers that exact match would miss.
  - Quick check question: How does BERTScore differ from exact string match when evaluating header generation?

## Architecture Onboarding

- Component map:
  Input text → Table Construction (header generation) → Header list → Question formulation → Table Content Generation (answer generation) → Cell content

- Critical path:
  1. Encode input text
  2. Generate header sequence
  3. For each (row_header, col_header) pair, formulate QA
  4. Generate answers for all cells in parallel

- Design tradeoffs:
  - Two-stage vs. single-stage: Two-stage guarantees syntactic validity but introduces potential error propagation; single-stage is simpler but error-prone.
  - Fine-tuning vs. zero-shot: Fine-tuning improves exact match but requires data and compute; zero-shot leverages larger models but may lack precision.

- Failure signatures:
  - Header generation errors → mismatched or missing cells downstream
  - QA generation errors → incorrect or hallucinated cell content
  - Context window exceeded → truncated or incomplete information in generated content

- First 3 experiments:
  1. Compare header F1 and BERTScore between gTBLS and prior work on E2E dataset.
  2. Measure error rate (invalid tables) between sequence-to-sequence baseline and gTBLS on RotoWire Player dataset.
  3. Ablation: Evaluate Table Content Generation F1 using gold headers vs. predicted headers to quantify error propagation.

## Open Questions the Paper Calls Out

None identified in the source material.

## Limitations

- The modular approach introduces error propagation risk: poor header generation can compromise all downstream cell content.
- The zero-shot QA path is presented as an advantage, but the paper doesn't evaluate whether instruction-tuned LLMs actually match fine-tuned performance across all datasets.
- The claim about incremental updates is theoretically sound but lacks empirical validation - no experiments demonstrate updating existing tables with new evidence.

## Confidence

- High confidence: The syntactic validity improvement (up to 57% error reduction) is well-supported by controlled comparisons against sequence-to-sequence baselines.
- Medium confidence: The 10-20% BERTScore improvements over prior work are credible but could be influenced by dataset-specific factors not fully controlled.
- Low confidence: The zero-shot QA path achieving "parity with fine-tuning approaches" is stated but not empirically validated across all datasets.

## Next Checks

1. Run ablation studies measuring semantic validity when using gold headers vs. predicted headers to quantify how header errors propagate to content quality.

2. Test the incremental update capability by generating tables, then introducing new text evidence and measuring whether gTBLS can successfully incorporate changes without full regeneration.

3. Compare zero-shot QA performance against fine-tuned models across all four datasets to verify the claimed parity, including analysis of which types of tables/headers benefit most from fine-tuning.