---
ver: rpa2
title: 'CFSafety: Comprehensive Fine-grained Safety Assessment for LLMs'
arxiv_id: '2410.21695'
source_url: https://arxiv.org/abs/2410.21695
tags:
- safety
- llms
- language
- evaluation
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CFSafety, a comprehensive fine-grained safety
  assessment framework for large language models (LLMs). The authors address the need
  for rigorous safety evaluations of LLMs, which can generate harmful content such
  as social biases, unethical material, and illegal activities under adversarial instructions.
---

# CFSafety: Comprehensive Fine-grained Safety Assessment for LLMs

## Quick Facts
- arXiv ID: 2410.21695
- Source URL: https://arxiv.org/abs/2410.21695
- Authors: Zhihao Liu; Chenhui Hu
- Reference count: 36
- Primary result: Introduced CFSafety framework with 25k prompts across 10 safety categories; GPT-4 achieved highest average score (4.02) but all models need safety improvements

## Executive Summary
This paper introduces CFSafety, a comprehensive fine-grained safety assessment framework for large language models (LLMs). The authors address the need for rigorous safety evaluations of LLMs, which can generate harmful content such as social biases, unethical material, and illegal activities under adversarial instructions. CFSafety integrates 5 classic safety scenarios and 5 types of instruction attacks, totaling 10 categories, forming a test set with 25k prompts in Chinese and English. The evaluation method combines moral judgment and a 1-5 safety rating scale, using the weighted sum of token probabilities output by LLMs to provide fine-grained safety scores. The authors tested eight popular LLMs, including the GPT series, and found that while GPT-4 demonstrated superior safety performance with an average CFSafety score of 4.02, the safety effectiveness of LLMs still requires improvement.

## Method Summary
CFSafety is a comprehensive safety assessment framework that combines 5 classic safety scenarios (unethical behavior, social bias, illegal activities, unethical content, toxic language) with 5 instruction attack types (prompt injection, jailbreak, persuasion, LLM rewriting, model exploitation). The framework uses an evaluation LLM to generate moral judgments (0 or 1) and probability-weighted safety ratings (1-5) for each question-answer pair. These are combined using a weighted sum of token probabilities to produce fine-grained safety scores. The benchmark includes 25k prompts in both Chinese and English, with 2500 questions per category. The evaluation employs dynamic prompt templates that include category-specific definitions and scoring examples to guide the evaluation LLM's judgments.

## Key Results
- CFSafety benchmark successfully evaluates 8 popular LLMs across 10 safety categories with 25k prompts
- GPT-4 achieved the highest average safety score of 4.02, demonstrating superior performance but still requiring safety improvements
- Most models' safety scores show a bimodal distribution, concentrating at extremes (1 and 5) rather than showing uniform distribution
- Instruction attacks remain effective against even advanced models like GPT-4, with significant safety vulnerabilities observed

## Why This Works (Mechanism)

### Mechanism 1
Using LLM-generated moral judgments and weighted token probabilities produces more fine-grained safety scores than binary classification. The evaluation framework concatenates each LLM's response with its corresponding question and category template, then feeds this to an evaluation LLM. The evaluation LLM outputs both a moral judgment (0 or 1) and probability-weighted safety ratings (1-5). These are combined using the formula: scorepair = (weighted moral judgment) × (weighted safety rating). The core assumption is that the evaluation LLM can reliably distinguish between safe and unsafe responses and provide meaningful probability distributions for safety ratings.

### Mechanism 2
The bimodal distribution of safety scores indicates models are more sensitive to extreme safety or danger scenarios. When analyzing the safety score distribution across all tested models, most scores cluster around 5 (safe) but show a clear bimodal pattern with concentration at both extremes (1 and 5). This reflects human evaluation patterns where evaluators tend to give very high or very low scores based on clarity or ambiguity. The core assumption is that this concentration at extremes is meaningful and not an artifact of the scoring methodology or question design.

### Mechanism 3
Incorporating instruction attacks from the latest research makes the safety benchmark more comprehensive than previous approaches. The authors surveyed recent LLM security reports and incorporated newer attack methods like persuasion and LLM rewriting, classifying safety issues into 10 categories (5 classic scenarios + 5 instruction attacks). They also generated attack samples using the latest methods and translated data into both Chinese and English. The core assumption is that newer attack methods represent genuine safety threats that previous benchmarks failed to capture.

## Foundational Learning

- **Weighted probability scoring for discrete categories**: Needed to convert discrete 1-5 safety ratings into continuous scores that reflect confidence levels. Quick check: Why does using softmax probabilities give more information than just using the most likely rating?

- **In-Context Learning (ICL) for evaluation prompts**: Required for the evaluation LLM to understand what constitutes safe vs. unsafe responses across different safety categories. Quick check: How does including definition and examples in the prompt template help the evaluation LLM provide consistent judgments?

- **Bimodal distribution analysis in evaluation results**: Important for understanding whether models are genuinely distinguishing between safe and unsafe scenarios. Quick check: What does a bimodal distribution of safety scores suggest about how well the models are performing?

## Architecture Onboarding

**Component map**: Questions → LLM response generation → evaluation prompt creation → evaluation LLM processing → moral judgment + safety rating → weighted score calculation → final CFSafety score

**Critical path**: The most critical path is question → LLM response → evaluation prompt → moral judgment + safety rating → weighted score calculation. Any failure in the evaluation LLM's ability to provide reliable moral judgments or safety ratings will propagate through the entire system.

**Design tradeoffs**: The framework trades simplicity (using LLMs as evaluators) for potential bias in evaluation, and trades comprehensiveness (10 safety categories) for depth in any single category. The bilingual approach increases coverage but may introduce translation artifacts.

**Failure signatures**: If all models receive similar scores across all categories, this suggests either the questions aren't challenging enough or the evaluation methodology isn't sensitive enough. If scores show no bimodal distribution, this might indicate the evaluation LLM is being too lenient or too strict uniformly.

**First 3 experiments**:
1. Test the evaluation LLM on a small set of hand-crafted questions with known safe/unsafe answers to verify it can distinguish between them
2. Run the full pipeline on a single model with a subset of categories to verify the weighted scoring calculation works as expected
3. Compare results when using different evaluation LLMs (e.g., GPT-3.5 vs GPT-4) to assess consistency and identify potential evaluator biases

## Open Questions the Paper Calls Out

### Open Question 1
How does the bimodal distribution of safety scores reflect the polarization trends in human subjective evaluations, and what are the implications for LLM safety assessments? While the paper identifies the bimodal distribution and its resemblance to human evaluation patterns, it does not explore the underlying reasons for this polarization or its impact on the reliability and validity of LLM safety assessments. Empirical studies comparing LLM safety scores with human evaluations, analysis of the factors contributing to score polarization, and investigation of the impact of bimodal distributions on the overall assessment of LLM safety would help resolve this.

### Open Question 2
How do instruction attacks, such as persuasion and LLM rewriting, challenge the safety of LLMs, and what are the most effective defense mechanisms against these attacks? While the paper identifies the existence of instruction attacks and their impact on LLM safety, it does not provide a comprehensive analysis of the attack mechanisms or propose specific defense strategies to mitigate these risks. Detailed analysis of attack methods, development and evaluation of robust defense mechanisms, and comparative studies of different LLM architectures' resilience to instruction attacks would help resolve this.

### Open Question 3
How does the multilingual nature of LLMs impact their safety performance, and what are the challenges in evaluating and ensuring safety across different languages? While the paper acknowledges the language-specific challenges in LLM safety, it does not explore the underlying reasons for these disparities or propose solutions to address the safety concerns in multilingual contexts. Comparative studies of LLM safety across different languages, analysis of the factors contributing to language-specific vulnerabilities, and development of evaluation frameworks that account for multilingual safety considerations would help resolve this.

## Limitations
- Heavy reliance on LLM-as-a-judge methodology introduces potential circularity and evaluator bias issues
- 25k prompt dataset based on manual generation may have coverage gaps and sampling bias
- Bilingual approach may introduce translation artifacts that don't represent genuine safety concerns
- Bimodal distribution interpretation is qualitative and may be methodological artifact rather than meaningful pattern

## Confidence

**High Confidence**: The framework's methodology for combining moral judgment and safety ratings through weighted probability sums is clearly specified and reproducible. The observation that GPT-4 achieves higher safety scores than other tested models is supported by the evaluation results.

**Medium Confidence**: The claim that CFSafety provides more comprehensive safety assessment than previous benchmarks is plausible given the inclusion of 10 categories and newer attack methods, but the relative effectiveness compared to existing benchmarks would require direct comparative studies.

**Low Confidence**: The interpretation of bimodal score distributions as evidence of models being more sensitive to extreme scenarios is speculative and would benefit from additional validation studies to rule out methodological artifacts.

## Next Checks

1. **Evaluator Consistency Test**: Run the same set of responses through multiple evaluation LLMs (GPT-3.5, GPT-4, Claude) and measure inter-annotator agreement. Low agreement would indicate the evaluation methodology is introducing significant noise or bias.

2. **Synthetic Safety Gradient**: Create a set of responses with known, gradually increasing safety violations (e.g., increasingly biased statements about demographic groups) and verify that the CFSafety scoring correctly reflects this gradient rather than showing discontinuous jumps or plateaus.

3. **Cross-Lingual Validation**: Take safety-critical prompts and their translations, generate responses in both languages, and evaluate whether safety scores remain consistent across translations. Significant discrepancies would suggest language-specific artifacts in the safety assessment.