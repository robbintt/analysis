---
ver: rpa2
title: Just rephrase it! Uncertainty estimation in closed-source language models via
  multiple rephrased queries
arxiv_id: '2405.13907'
source_url: https://arxiv.org/abs/2405.13907
tags:
- rephrase
- rephrasing
- uncertainty
- zmean
- expansion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of estimating uncertainty in closed-source
  large language models (LLMs), which are prone to "hallucinating" false information
  with high confidence. The proposed method involves querying the model multiple times
  with rephrased versions of the original base query and using the similarity of the
  answers as an estimate of uncertainty.
---

# Just rephrase it! Uncertainty estimation in closed-source language models via multiple rephrased queries

## Quick Facts
- arXiv ID: 2405.13907
- Source URL: https://arxiv.org/abs/2405.13907
- Reference count: 23
- Primary result: Rephrased queries method improves uncertainty calibration in closed-source LLMs with AUROC, ECE, TACE, and Brier score gains over baselines

## Executive Summary
This paper addresses the critical challenge of uncertainty estimation in closed-source large language models, which often produce hallucinated responses with high confidence. The authors propose a novel approach that generates multiple rephrased versions of the original query and uses the similarity of responses as an uncertainty signal. The method shows significant improvements in calibration metrics across eight diverse datasets. The approach is particularly valuable because it doesn't require access to model internals or weights, making it applicable to popular closed-source models like GPT-4 and Claude.

## Method Summary
The proposed method involves creating multiple rephrased versions of the original query using four different techniques: reword, rephrase, paraphrase, and expansion. Each rephrased query is sent to the language model, and the responses are compared to estimate uncertainty. The similarity between responses serves as the uncertainty metric - more similar responses indicate higher confidence, while diverse responses suggest uncertainty. The paper evaluates this approach across eight datasets with varying query types and demonstrates that reword and expansion techniques work best for uncertainty estimation. The method achieves improved calibration without requiring any modifications to the underlying language model.

## Key Results
- Significantly improves calibration metrics (AUROC, ECE, TACE, Brier score) compared to naive baseline
- Reword and expansion rephrasing techniques outperform paraphrase and rephrase methods
- Method works across 8 diverse datasets with mean answer length of 3.14 tokens
- Theoretical framework shows method can recover the probability distribution of the LLM's answers

## Why This Works (Mechanism)
The approach leverages the principle that when a language model is uncertain about an answer, different phrasings of the same question will yield more diverse responses. By measuring the similarity between responses to rephrased queries, the method effectively captures the model's internal uncertainty. This works because closed-source models still have deterministic behavior patterns - when uncertain, they tend to explore different answer paths, while high confidence leads to consistent responses across phrasings.

## Foundational Learning
- **Rephrasing Techniques** - Different ways to reformulate queries (reword, rephrase, paraphrase, expansion) are needed to probe the model's confidence from multiple angles; quick check: compare response diversity across techniques on a simple question
- **Similarity Metrics** - Methods to quantify answer similarity (cosine similarity, semantic matching) are essential for uncertainty quantification; quick check: verify that semantically identical answers score high similarity
- **Calibration Metrics** (AUROC, ECE, TACE, Brier score) - Standard metrics for evaluating uncertainty estimation quality; quick check: ensure baseline models show poor calibration without the method
- **Query Transformation** - The relationship between how questions are asked and the resulting answers reveals model uncertainty; quick check: test if extreme rephrasings affect uncertainty estimates
- **Closed-Source Model Constraints** - Working without access to model weights requires indirect uncertainty estimation methods; quick check: verify method works across different API-based models
- **Response Distribution Analysis** - Understanding how answers vary across rephrased queries provides insight into model confidence; quick check: analyze response variance for known uncertain vs. certain questions

## Architecture Onboarding

Component Map:
User Query -> Rephrasing Engine (4 types) -> LLM API Calls -> Response Comparison Engine -> Uncertainty Score

Critical Path:
The critical path involves generating rephrased queries, sending them to the LLM, receiving responses, and computing similarity scores. The bottleneck is typically the LLM API response time, as multiple queries must be processed.

Design Tradeoffs:
The method trades increased query cost (multiple API calls) for improved uncertainty estimation. Using fewer rephrasings reduces cost but may decrease accuracy. The choice of rephrasing technique involves a tradeoff between computational efficiency and uncertainty detection capability.

Failure Signatures:
The method may fail when: rephrasings don't adequately capture query variations, the LLM gives similar wrong answers across all rephrasings (false confidence), or when answers are too short for meaningful similarity comparison. It also struggles with highly subjective questions where multiple answers are equally valid.

First Experiments:
1. Test the method on a simple factual question with known answer to verify it produces low uncertainty
2. Evaluate on a deliberately ambiguous question to confirm it detects uncertainty
3. Compare the four rephrasing techniques on the same query set to identify which works best

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to relatively short answers (mean length 3.14 tokens) - unclear how method performs on longer responses
- Experimental validation restricted to 8 datasets with controlled question types
- Optimal number and types of rephrasings needed for different contexts remains unclear
- May not detect systematic model biases or when model is confidently wrong across all rephrasings

## Confidence

**Effectiveness of rephrased queries for uncertainty estimation** (Medium): Strong results on calibration metrics but limited to specific datasets and query types.

**Theoretical framework validity** (Medium): Mathematically sound but relies on assumption that rephrasing preserves underlying answer distribution.

**Rephrase and expansion superiority** (Medium): Demonstrated effectiveness on tested datasets but requires validation across more diverse query types.

## Next Checks

1. Test the method on long-form question answering tasks where answers are paragraphs rather than short facts, to assess whether answer similarity remains an effective uncertainty signal.

2. Evaluate performance on adversarial or out-of-distribution queries designed to trigger model hallucinations, to determine if the method can detect when the model is confidently wrong across all rephrasings.

3. Conduct a user study with domain experts to assess whether the uncertainty estimates from this method correlate with human judgments of answer reliability in practical applications.