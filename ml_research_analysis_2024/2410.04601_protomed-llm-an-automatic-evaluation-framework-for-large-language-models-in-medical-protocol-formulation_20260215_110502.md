---
ver: rpa2
title: 'ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models
  in Medical Protocol Formulation'
arxiv_id: '2410.04601'
source_url: https://arxiv.org/abs/2410.04601
tags: []
core_contribution: This work introduces an automatic framework for evaluating large
  language models on scientific protocol formulation tasks, addressing the challenge
  of manually assessing protocol generation quality. The proposed method, ProtocoLLM,
  prompts both a target model and GPT-4 to extract pseudocode from biology protocols
  using predefined lab actions, then evaluates the outputs using a form-filling paradigm
  called LLAM-EVAL with Llama-3 as the evaluator.
---

# ProtoMed-LLM: An Automatic Evaluation Framework for Large Language Models in Medical Protocol Formulation

## Quick Facts
- **arXiv ID**: 2410.04601
- **Source URL**: https://arxiv.org/abs/2410.04601
- **Reference count**: 26
- **Primary result**: Introduces ProtocoLLM, an automatic evaluation framework for medical protocol formulation using pseudocode extraction and form-filling paradigm

## Executive Summary
This work presents ProtoMed-LLM, an automatic framework for evaluating large language models on scientific protocol formulation tasks. The framework addresses the challenge of manually assessing protocol generation quality by using pseudocode extraction and form-filling evaluation paradigms. It enables comprehensive assessment of LLMs' ability to generate structured, actionable medical protocols without requiring manual annotation. The method leverages both GPT-4 and the target model to extract pseudocode from biology protocols, then evaluates outputs using Llama-3 as an evaluator. The work also introduces BIOPROT 2.0, a dataset containing 300 biology protocols with corresponding pseudocodes, which supports both protocol formulation and evaluation tasks.

## Method Summary
The ProtocoLLM framework employs a two-stage evaluation approach. First, both the target LLM and GPT-4 extract pseudocode from biology protocols using predefined lab actions. Second, a form-filling paradigm called LLAM-EVAL evaluates the extracted pseudocode using Llama-3 as the evaluator. This approach eliminates manual annotation requirements while providing flexibility across different evaluation models, materials, and criteria. The framework evaluates multiple LLMs including GPT variations, Llama, Mixtral, Gemma, Cohere, and Gemini. The BIOPROT 2.0 dataset serves as the evaluation benchmark, containing 300 biology protocols with corresponding pseudocodes that represent structured representations of experimental procedures.

## Key Results
- GPT-4o and Cohere+ identified as most effective scientific protocol formulators among evaluated models
- Framework successfully eliminates manual annotation requirements for protocol evaluation
- BIOPROT 2.0 dataset provides 300 biology protocols with corresponding pseudocodes for evaluation purposes
- Proposed approach demonstrates flexibility across different evaluation models, materials, and criteria

## Why This Works (Mechanism)
The framework works by decomposing complex protocol evaluation into structured pseudocode extraction followed by systematic form-filling assessment. By using predefined lab actions as a common language between the target model and GPT-4, the framework creates a standardized representation of protocol steps. The LLAM-EVAL form-filling paradigm then provides a structured way to assess whether the generated protocols contain all necessary components and follow logical sequencing. This two-stage process reduces the subjective judgment typically required in protocol evaluation while maintaining rigorous assessment criteria.

## Foundational Learning

**Pseudocode Extraction** - Why needed: Converts natural language protocols into structured, machine-readable representations that can be systematically evaluated. Quick check: Verify extracted pseudocode captures all major steps and maintains logical flow from source protocol.

**Form-Filling Paradigm** - Why needed: Provides standardized evaluation criteria by mapping protocol components to predefined template fields. Quick check: Ensure form fields comprehensively cover all essential protocol elements including materials, methods, and safety considerations.

**Lab Action Taxonomy** - Why needed: Establishes common vocabulary for protocol steps across different models and domains. Quick check: Validate that taxonomy covers diverse experimental procedures from multiple biological subfields.

## Architecture Onboarding

**Component Map**: Protocol Text -> Pseudocode Extraction (Target Model + GPT-4) -> Form-Filling Evaluation (LLAM-EVAL + Llama-3) -> Quality Score

**Critical Path**: The pseudocode extraction stage is critical, as errors here propagate directly to evaluation outcomes. This includes both the target model's pseudocode generation and GPT-4's reference extraction.

**Design Tradeoffs**: The framework trades absolute accuracy for scalability and consistency. While human evaluation might catch subtle nuances, the automated approach enables rapid assessment across many models and protocols.

**Failure Signatures**: 
- Incomplete pseudocode extraction leading to form evaluation gaps
- Template mismatches where protocol elements don't align with form fields
- Bias from evaluator model's own training data influencing assessment quality

**First Experiments**:
1. Test pseudocode extraction on protocols with varying complexity levels
2. Validate form-filling evaluation against manually annotated protocols
3. Compare framework outputs across different evaluator models

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on GPT-4 and Llama-3 introduces potential bias from shared training data or reasoning patterns
- BIOPROT 2.0 dataset size (300 protocols) may not capture full diversity of biological protocols across subfields
- Framework focuses on protocol structure rather than scientific accuracy or feasibility of described procedures
- Pseudocode extraction quality directly impacts evaluation accuracy, creating potential error propagation

## Confidence

**High**: Framework methodology and implementation details are clearly described and reproducible. The pseudocode extraction and form-filling evaluation approach represents a valid technical contribution.

**Medium**: Comparative performance results between different LLMs are methodologically sound but require external validation to confirm alignment with actual protocol quality as judged by domain experts.

**Low**: Claims about framework generalizability to other scientific domains beyond biology protocols remain speculative without empirical testing in those domains.

## Next Checks

1. Conduct human expert study comparing framework evaluations against expert assessments of protocol quality across 50 randomly selected protocols from BIOPROT 2.0

2. Test framework performance on protocols from other scientific domains (chemistry, physics, engineering) to assess generalizability claims

3. Perform ablation studies removing pseudocode extraction step to quantify impact on final evaluation scores and identify potential error propagation points