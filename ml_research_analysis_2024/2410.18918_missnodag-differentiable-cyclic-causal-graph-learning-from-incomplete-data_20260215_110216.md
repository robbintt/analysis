---
ver: rpa2
title: 'MissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete Data'
arxiv_id: '2410.18918'
source_url: https://arxiv.org/abs/2410.18918
tags:
- data
- missing
- causal
- learning
- missnodag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MissNODAG is a differentiable framework for learning cyclic causal
  graphs and missingness mechanisms from incomplete interventional data, including
  MNAR cases. It combines an additive noise model with an EM algorithm that alternates
  between imputing missing values and maximizing observed data likelihood.
---

# MissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete Data

## Quick Facts
- arXiv ID: 2410.18918
- Source URL: https://arxiv.org/abs/2410.18918
- Reference count: 40
- Primary result: Learns cyclic causal graphs and missingness mechanisms from incomplete interventional data, outperforming state-of-the-art imputation methods when missing probability < 0.4

## Executive Summary
MissNODAG is a differentiable framework for learning cyclic causal graphs and missingness mechanisms from incomplete interventional data, including MNAR cases. It combines an additive noise model with an EM algorithm that alternates between imputing missing values and maximizing observed data likelihood. The framework uses residual normalizing flows to compute log-determinants of Jacobians and handles both linear and nonlinear SEMs. Experiments on synthetic data show MissNODAG outperforms state-of-the-art imputation methods combined with causal learning, particularly when missing probability is below 0.4.

## Method Summary
MissNODAG learns cyclic causal graphs from incomplete data by parameterizing the target law and missingness mechanism, then alternating between imputing missing values (E-step) and maximizing the expected log-likelihood (M-step). The framework handles both linear and nonlinear SEMs, using exact posterior sampling for MAR cases and rejection sampling for MNAR cases. Contractive residual flows compute Jacobian determinants tractably, while Gumbel-softmax masks enforce sparsity in the causal structure. The algorithm converges to a stationary point rather than global optimum, with performance degrading when missing probability exceeds 0.4.

## Key Results
- Outperforms MissDAG, MissForest, and optimal transport baselines on synthetic data with SHD below 2.5 for 10,000 samples when missing probability is low
- Learns missingness mechanisms effectively, with SHD below 2.5 for 10,000 samples when missing probability is low
- Superior predictive performance on real-world gene perturbation data compared to NODAGS-Flow on clean data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating between missing value imputation and likelihood maximization allows joint learning of causal structure and missingness mechanism in cyclic graphs.
- Mechanism: The EM algorithm iterates between an E-step that computes expected log-likelihood over missing values using current parameter estimates, and an M-step that maximizes this expected log-likelihood to update both causal and missingness parameters. This iterative refinement captures dependencies between structure and missingness.
- Core assumption: The target law p(X) and missingness mechanism p(R|X) are parameterized by finite-dimensional parameter vectors θ and ϕ, enabling joint optimization.
- Evidence anchors:
  - [abstract]: "Our framework integrates an additive noise model with an expectation-maximization procedure, alternating between imputing missing values and optimizing the observed data likelihood"
  - [section]: "We assume the target law p(X) and the missingness mechanism p(R|X) are parameterized by finite vectors θ and ϕ, respectively... MissNODAG alternates between imputing missing values and maximizing the expected log-likelihood of the observed data"
- Break condition: The algorithm converges to a stationary point rather than global optimum when the observed data likelihood is not directly optimized.

### Mechanism 2
- Claim: Residual normalizing flows with spectral normalization enable tractable computation of log-determinants for cyclic causal graphs.
- Mechanism: Neural networks model contractive SEM functions F(X), ensuring the mapping (id - F) is bijective. Residual normalizing flows compute log-determinant of Jacobian using power series expansion and Hutchinson trace estimator, making the likelihood tractable even with cycles.
- Core assumption: The SEM functions in F(X) are Lipschitz with constant less than one, guaranteeing contractivity and invertibility of (id - F).
- Evidence anchors:
  - [section]: "We assume the SEM functions in F(X) in (2) are Lipschitz with Lipschitz constant less than one... neural networks can be constrained to be contractive during the training phase by rescaling the layer weights by their corresponding spectral norm"
  - [section]: "we employ contractive residual flows (Behrmann et al., 2019; Chen et al., 2019) to compute the log-determinant of the Jacobian in a tractable manner"
- Break condition: Non-contractivity in F(X) breaks the guarantee of (id - F) invertibility, though the framework may still work for DAGs where non-contractivity is acceptable.

### Mechanism 3
- Claim: Rejection sampling enables posterior sampling for missing values when exact posterior is intractable.
- Mechanism: When exact posterior sampling is impossible due to non-linearities in F(X) and complex missingness mechanisms, rejection sampling draws samples from a proposal distribution Q(xΩi) and accepts them with probability proportional to the true posterior, enabling the E-step computation.
- Core assumption: A proposal distribution Q(xΩi) can be constructed from which samples can be readily generated, and a constant c0 can be found such that c0Q(xΩi) ≥ p(xΩi|xΓi, r(i), Θt) for all i.
- Evidence anchors:
  - [section]: "The expectation in the approximation for Q(Θ|Θt) generally lacks a closed-form solution. Therefore, it must be approximated by the sample mean, using samples drawn from the posterior distribution p(xΩi|xΓi, r(i), Θt)"
  - [section]: "we employ rejection sampling (Koller and Friedman, 2009) to draw samples from a proposal distribution Q(xΩi), from which samples can be readily generated"
- Break condition: If no suitable proposal distribution Q exists or the acceptance rate becomes too low, rejection sampling becomes computationally infeasible.

## Foundational Learning

- Concept: Structural Equation Models (SEMs) with additive noise
  - Why needed here: The paper's entire framework builds on SEMs to represent causal relationships, with Xk = fk(paG(Xk)) + ϵk defining how each variable depends on its parents plus independent noise
  - Quick check question: What distinguishes an SEM from a standard regression model in the context of causal discovery?

- Concept: Missing data mechanisms (MCAR, MAR, MNAR)
  - Why needed here: The framework explicitly handles MNAR (Missing Not At Random) data where the missingness depends on unobserved values, which is the most challenging case and where standard imputation methods fail
  - Quick check question: Why does MNAR data require special treatment compared to MCAR or MAR data in causal discovery?

- Concept: Expectation-Maximization (EM) algorithm
  - Why needed here: The framework uses EM to alternate between imputing missing values (E-step) and maximizing likelihood (M-step), which is essential for handling incomplete data without discarding samples
  - Quick check question: What is the key theoretical guarantee of the EM algorithm that makes it suitable for this problem?

## Architecture Onboarding

- Component map: SEM function modeling -> Jacobian log-determinant computation -> likelihood evaluation -> E-step imputation -> M-step parameter update
- Critical path: Forward pass through SEM → Jacobian log-determinant computation → likelihood evaluation → E-step imputation → M-step parameter update. The Jacobian computation is the computational bottleneck.
- Design tradeoffs: The framework trades computational complexity (due to Jacobian computation and rejection sampling) for the ability to handle both cycles and MNAR data. The use of contractive functions ensures tractability but may limit expressiveness compared to non-contractive alternatives.
- Failure signatures: Poor performance when (1) missing probability exceeds 0.4, (2) parent set cardinality of missingness indicators becomes too large (>3), (3) the proposal distribution for rejection sampling has low acceptance rate, or (4) the underlying graph contains structures that violate the block-parallel MNAR assumption.
- First 3 experiments:
  1. Run on synthetic linear SEM with ER-1 graph and missing probability 0.2 to verify basic functionality and compare against MissDAG baseline
  2. Test on nonlinear SEM with ER-2 graph and missing probability 0.3 to evaluate performance on more complex structures
  3. Vary the parent set cardinality of missingness indicators from 2 to 4 while keeping other parameters fixed to assess sensitivity to missingness mechanism complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MissNODAG perform on larger graphs with hundreds of nodes rather than the 10-node graphs tested?
- Basis in paper: [inferred] The paper mentions "Future research directions include... scaling the current framework to larger graphs using low-rank models and variational inference techniques" suggesting this is unexplored.
- Why unresolved: The experiments only tested graphs with K=10 nodes, so scalability to larger graphs remains untested.
- What evidence would resolve it: Experiments testing MissNODAG on graphs with hundreds of nodes, comparing performance to baselines, would determine scalability.

### Open Question 2
- Question: Can MissNODAG handle MNAR mechanisms with colluders (X_j → R_k ← R_j edges) beyond the block-parallel MNAR model?
- Basis in paper: [explicit] The paper states "Beyond block-parallel MNAR, other identifiable MNAR models, such as those with no colluders... offer viable directions for extending our framework" and discusses the need for further refinement.
- Why unresolved: The paper only implements and tests the block-parallel MNAR model, leaving colluder-containing MNAR models as future work.
- What evidence would resolve it: Implementation and testing of MissNODAG on MNAR models with colluders, comparing performance to the block-parallel case, would determine feasibility.

### Open Question 3
- Question: How sensitive is MissNODAG to initialization and hyperparameter choices, particularly the regularization parameters λ1 and λ2?
- Basis in paper: [inferred] The paper uses specific values (λ = 10^-2 for synthetic experiments, 10^-3 for perturb-CITE-seq) but doesn't explore sensitivity to these choices.
- Why unresolved: The paper doesn't report sensitivity analysis for different λ values or discuss robustness to initialization strategies.
- What evidence would resolve it: Systematic experiments varying λ1, λ2, and initialization strategies across multiple runs would reveal sensitivity patterns.

## Limitations
- Performance degrades significantly when missing probability exceeds 0.4
- Block-parallel MNAR assumption may not hold for many real-world datasets
- Rejection sampling can become computationally infeasible with poor proposal distributions

## Confidence
- **High confidence**: The EM algorithm framework for alternating between imputation and likelihood maximization is well-established theoretically
- **Medium confidence**: Performance claims on synthetic data (SHD < 2.5 for 10,000 samples at low missingness) are supported by experiments but may not generalize to all graph structures
- **Low confidence**: Real-world gene perturbation data results are promising but based on a single dataset with limited comparison to specialized biological methods

## Next Checks
1. Test MissNODAG on datasets with missing probability approaching 0.4 to identify the exact performance threshold and failure modes
2. Implement alternative proposal distributions for rejection sampling and measure acceptance rates across different missingness patterns
3. Apply the framework to additional real-world datasets (e.g., clinical data with known causal relationships) to validate generalizability beyond gene perturbation data