---
ver: rpa2
title: 'ENTP: Encoder-only Next Token Prediction'
arxiv_id: '2410.01600'
source_url: https://arxiv.org/abs/2410.01600
tags:
- decoder-only
- entp
- token
- sequence
- encoder-only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Encoder-only Next Token Prediction (ENTP),
  which adapts encoder-only Transformers for auto-regressive tasks by computing attention
  from scratch for each token prediction. The authors show that encoder-only and decoder-only
  Transformers have different expressive power - they can each learn functions that
  the other cannot.
---

# ENTP: Encoder-only Next Token Prediction

## Quick Facts
- **arXiv ID**: 2410.01600
- **Source URL**: https://arxiv.org/abs/2410.01600
- **Authors**: Ethan Ewer; Daewon Chae; Thomas Zeng; Jinkyu Kim; Kangwook Lee
- **Reference count**: 40
- **Primary result**: Encoder-only Transformers can express functions that decoder-only Transformers cannot, demonstrated through the Count3 task

## Executive Summary
This paper introduces Encoder-only Next Token Prediction (ENTP), which adapts encoder-only Transformers for auto-regressive tasks by computing attention from scratch for each token prediction. The authors show that encoder-only and decoder-only Transformers have different expressive power - they can each learn functions that the other cannot. They introduce the Count3 task, demonstrating theoretically and experimentally that ENTP can easily learn it while decoder-only Transformers cannot, even at large scales. Empirical results across multiple tasks show ENTP's advantages: better sample complexity and length generalization on addition, superior in-context learning performance, and improved results on natural language tasks including language modeling and downstream benchmarks. The findings suggest encoder-only architectures may be preferable for next-token prediction when computational constraints are relaxed, though current inefficiency limits practical deployment.

## Method Summary
ENTP adapts encoder-only Transformers for next-token prediction by computing attention from scratch for each token prediction, rather than using the sequential causal attention of decoder-only models. During training, all positions can be computed in parallel, but inference requires sequential computation as each token depends on all previous ones. The architecture uses standard Transformer blocks with full self-attention, positional embeddings, and a linear output layer. Unlike decoder-only Transformers that use causal masking, ENTP recomputes the full attention matrix for each output position, allowing access to all previous tokens simultaneously. This approach trades computational efficiency for increased expressive power and improved learning capabilities on tasks requiring global context.

## Key Results
- Encoder-only and decoder-only Transformers have incomparable expressive power - each can learn functions the other cannot
- ENTP achieves superior sample complexity and length generalization on arithmetic tasks compared to decoder-only models
- ENTP demonstrates competitive performance in in-context learning across multiple function classes
- Natural language experiments show ENTP outperforms decoder-only models on language modeling and downstream benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Encoder-only Transformers can express functions that decoder-only Transformers cannot, due to the full attention pattern allowing all tokens to interact simultaneously.
- **Mechanism:** In encoder-only Transformers, attention is computed from scratch for each token prediction, allowing the model to access information from all previous tokens at once. This enables computation of complex triplet relationships like Count3 that require simultaneous access to multiple tokens. Decoder-only Transformers, constrained by causal attention, can only access information in a sequential manner, limiting their ability to compute certain functions efficiently.
- **Core assumption:** The expressive power difference between encoder-only and decoder-only Transformers is significant enough to impact task performance, particularly for functions requiring simultaneous access to multiple tokens.
- **Evidence anchors:**
  - [abstract] "We introduce the Count3 task and show, both theoretically and experimentally, that while ENTP can perform this task easily, a decoder-only Transformer cannot."
  - [section] "We demonstrate that the sets of functions expressible by decoder-only and encoder-only Transformers are not comparable, which goes against intuition that the expressivity of encoders would subsume that of decoders."
  - [corpus] Weak evidence - the corpus mentions general upper bounds for transformers but doesn't specifically address the expressive power difference between encoder-only and decoder-only variants.
- **Break condition:** If computational constraints are imposed that force encoder-only Transformers to compute attention incrementally (like decoder-only), the expressive power advantage would diminish.

### Mechanism 2
- **Claim:** Encoder-only Transformers achieve superior sample complexity and length generalization compared to decoder-only Transformers for arithmetic tasks.
- **Mechanism:** The full attention pattern in encoder-only Transformers allows them to learn global patterns and relationships in data more efficiently. For arithmetic tasks like addition, this enables better understanding of number relationships and carry operations across all digits simultaneously, rather than learning these patterns sequentially as in decoder-only models.
- **Core assumption:** The full attention mechanism provides a more efficient learning pathway for arithmetic operations that benefit from global context.
- **Evidence anchors:**
  - [section] "We find that encoders exhibit lower sample complexity compared to decoders... encoders demonstrate superior ability to generalize to longer sequences."
  - [section] "ENTP achieves superior generalization compared to decoder-only Transformers on both in-distribution and out-of-distribution data."
  - [corpus] Weak evidence - the corpus mentions transformers can navigate mazes with multi-step prediction, suggesting sequential reasoning capabilities, but doesn't directly compare encoder-only vs decoder-only for arithmetic tasks.
- **Break condition:** If the arithmetic task can be decomposed into strictly sequential operations that don't benefit from global context, the advantage of encoder-only Transformers may disappear.

### Mechanism 3
- **Claim:** Encoder-only Transformers demonstrate competitive performance in in-context learning compared to decoder-only models.
- **Mechanism:** The bidirectional attention in encoder-only Transformers allows them to better integrate information from in-context examples with the query, as they can attend to all examples simultaneously. This facilitates pattern recognition across the entire prompt context, leading to better function approximation from limited examples.
- **Core assumption:** In-context learning benefits from the ability to attend to all examples simultaneously rather than processing them sequentially.
- **Evidence anchors:**
  - [section] "ENTP demonstrates superior performance compared to the decoder-only models in in-context learning for both function classes."
  - [section] "For in-context learning, ENTP shows competitive performance compared to decoders."
  - [corpus] Weak evidence - the corpus mentions next-token prediction capacity bounds but doesn't specifically address in-context learning performance differences between architectures.
- **Break condition:** If in-context learning tasks can be effectively solved through sequential processing of examples, the bidirectional attention advantage may not materialize.

## Foundational Learning

- **Concept:** Attention mechanism in Transformers
  - Why needed here: Understanding how encoder-only vs decoder-only attention patterns differ is fundamental to grasping why ENTP has different capabilities.
  - Quick check question: How does the attention pattern differ between encoder-only and decoder-only Transformers when computing the output for token position i?

- **Concept:** Expressive power and computational complexity
  - Why needed here: The paper's core argument relies on understanding how different architectures can express different functions and what computational resources they require.
  - Quick check question: What is the time complexity difference between computing attention for encoder-only vs decoder-only Transformers when generating n tokens?

- **Concept:** Auto-regressive modeling and next-token prediction
  - Why needed here: ENTP adapts encoder-only Transformers for a task traditionally done by decoder-only models, so understanding the standard approach is crucial.
  - Quick check question: Why do decoder-only Transformers use causal attention for next-token prediction, and what efficiency advantage does this provide?

## Architecture Onboarding

- **Component map:** Input layer (token embeddings + positional embeddings) -> Encoder stack (multiple Transformer blocks with full self-attention) -> Output layer (linear projection to vocabulary + softmax)

- **Critical path:**
  1. Token embedding and positional encoding
  2. For each output position t:
     - Compute Q, K, V for all tokens up to position t
     - Compute full attention matrix for these tokens
     - Apply attention and feed-forward layers
     - Output token prediction
  3. During training: compute loss on all positions simultaneously
  4. During inference: compute positions sequentially due to autoregressive nature

- **Design tradeoffs:**
  - **Pros:** Full attention allows richer representations and access to all context; potentially better for tasks requiring global pattern recognition
  - **Cons:** O(n²) time complexity per token vs O(n) for decoder-only; requires recomputing attention from scratch; less efficient for long sequences
  - **Key consideration:** ENTP trades computational efficiency for expressive power and learning efficiency

- **Failure signatures:**
  - High computational cost for long sequences (O(n³) total for sequence of length n)
  - Memory usage grows quadratically with sequence length
  - May overfit on smaller datasets due to increased model capacity
  - Training instability if learning rate not properly tuned for full attention computation

- **First 3 experiments:**
  1. **Count3 task reproduction:** Implement the Count3 task to verify that encoder-only models can learn it while decoder-only models cannot. Start with small models (3-6 layers) and synthetic data.
  2. **Addition task sample complexity:** Compare encoder-only and decoder-only models on reversed addition format with varying training set sizes (1.25k to 20k examples) to measure learning efficiency.
  3. **In-context learning benchmark:** Test both architectures on simple function classes (linear, sparse linear) with varying numbers of in-context examples to compare few-shot learning capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an efficient variant of ENTP be developed that maintains its expressive advantages while reducing computational complexity to practical levels?
- Basis in paper: [explicit] The paper concludes that "ENTP is currently computationally inefficient for practical deployment" but suggests "developing a compute-efficient ENTP variant that retains its strengths while narrowing the efficiency gap with decoders" as a promising future direction.
- Why unresolved: The paper demonstrates ENTP's advantages in expressiveness and generalization but does not provide any architectural modifications or optimization strategies to make it computationally viable for real-world deployment.
- What evidence would resolve it: Successful implementation and empirical validation of an ENTP variant that achieves similar performance to standard ENTP but with computational complexity closer to decoder-only Transformers (e.g., O(n^2) instead of O(n^3) for sequence generation).

### Open Question 2
- Question: What specific architectural modifications could enable decoder-only Transformers to learn Count3-like functions that require counting triplets or similar higher-order relationships?
- Basis in paper: [explicit] The paper shows that "no decoder-only Transformer, regardless of size, can efficiently learn the task" for Count3, but also notes that "with linear chain-of-thought (generating O(n) tokens before answering), a decoder would be able to perform Count3."
- Why unresolved: While the paper demonstrates the limitation, it doesn't explore architectural changes that could bridge this gap without requiring O(n) additional tokens for reasoning, which would negate the efficiency advantages of decoders.
- What evidence would resolve it: Empirical demonstration that a modified decoder architecture (e.g., with enhanced memory mechanisms or modified attention patterns) can learn Count3 efficiently without requiring explicit chain-of-thought reasoning.

### Open Question 3
- Question: How do encoder-only and decoder-only Transformers differ in their ability to generalize to out-of-distribution tasks beyond arithmetic and simple function learning?
- Basis in paper: [explicit] The paper shows ENTP "demonstrates superior ability to generalize to longer sequences" on addition tasks and "demonstrates superior performance across both function classes" for in-context learning, but only tests these specific domains.
- Why unresolved: The experiments focus on arithmetic tasks and simple synthetic functions, leaving open questions about how these findings translate to more complex real-world scenarios involving diverse data distributions and task types.
- What evidence would resolve it: Systematic evaluation of encoder-only and decoder-only Transformers on a broad range of out-of-distribution tasks including complex reasoning, multimodal tasks, and tasks requiring compositional generalization.

## Limitations

- **Computational Inefficiency**: ENTP requires O(n²) time per token prediction versus O(n) for decoder-only models, making it impractical for long sequences and real-world deployment.
- **Limited Empirical Scope**: The paper focuses on synthetic tasks (Count3, arithmetic) and limited natural language benchmarks, not fully establishing advantages across diverse real-world scenarios.
- **Implementation Details**: The paper doesn't fully specify how ENTP handles sequential inference or provide comprehensive timing benchmarks for practical implementation considerations.

## Confidence

**High Confidence**: The theoretical claims about expressive power differences between encoder-only and decoder-only Transformers, demonstrated through the Count3 task. The mathematical proof that decoder-only Transformers cannot efficiently compute Count3 while encoder-only can is rigorous and well-supported.

**Medium Confidence**: The empirical results showing ENTP's advantages on arithmetic tasks and in-context learning. These results are supported by experiments, but the sample sizes are relatively small and the tasks are synthetic. The language modeling results show competitive performance but not overwhelming superiority.

**Low Confidence**: The practical applicability of ENTP in real-world scenarios given its computational inefficiency. The paper doesn't provide comprehensive timing benchmarks or comparisons to modern efficient architectures like FlashAttention or other optimized implementations.

## Next Checks

1. **Scaling Analysis**: Systematically evaluate ENTP's performance and computational requirements as sequence length increases from 64 to 4096 tokens. Measure both wall-clock time and memory usage compared to decoder-only models, and assess at what sequence length ENTP becomes computationally prohibitive.

2. **Broader Task Evaluation**: Test ENTP on a wider range of natural language tasks including summarization, translation, and question answering benchmarks. Compare performance not just on accuracy but also on inference speed and memory consumption to better understand the practical tradeoffs.

3. **Hybrid Architecture Exploration**: Investigate hybrid approaches that combine the expressive power of encoder-only attention with the efficiency of decoder-only computation. For example, test whether using full attention only for specific layers or implementing approximate attention mechanisms (like in FlashAttention) can preserve ENTP's advantages while reducing computational cost.