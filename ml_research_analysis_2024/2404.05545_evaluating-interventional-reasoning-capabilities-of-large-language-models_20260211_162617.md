---
ver: rpa2
title: Evaluating Interventional Reasoning Capabilities of Large Language Models
arxiv_id: '2404.05545'
source_url: https://arxiv.org/abs/2404.05545
tags:
- causal
- llms
- intervention
- variable
- interventions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces intervention effects as a way to evaluate
  LLMs' ability to reason about causal changes resulting from interventions. The authors
  define intervention effects as binary classification tasks that predict how causal
  relationships in a DAG change after an intervention.
---

# Evaluating Interventional Reasoning Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2404.05545
- Source URL: https://arxiv.org/abs/2404.05545
- Authors: Tejas Kasetty; Divyat Mahajan; Gintare Karolina Dziugaite; Alexandre Drouin; Dhanya Sridhar
- Reference count: 32
- Primary result: GPT models demonstrate ability to generalize intervention reasoning beyond memorized causal facts, with GPT-4 variants achieving near-perfect accuracy on intervention effect prediction tasks

## Executive Summary
This paper introduces intervention effects as a framework for evaluating large language models' ability to reason about causal changes resulting from interventions. The authors create benchmarks spanning different causal graph structures (bivariate, confounding, mediation) and variable types, using both random characters and real-world variable names from the Tübingen dataset. Through zero-shot prompting, they evaluate six LLMs on their ability to predict how causal relationships change after perfect interventions, finding that GPT models (especially GPT-4 variants) perform significantly better than LLaMA models.

The study contributes a systematic approach to assessing causal reasoning capabilities in LLMs, specifically their ability to update beliefs in response to interventions. By testing models on various variable naming strategies, the authors demonstrate that GPT models can generalize intervention reasoning beyond memorized facts, though some sensitivity to how interventions are described remains. The work highlights important differences in causal reasoning capabilities across different LLM architectures and provides a foundation for future research on improving causal reasoning in these models.

## Method Summary
The study evaluates LLMs' ability to predict intervention effects through zero-shot prompting with verbalized causal DAG descriptions. For each benchmark task, the LLM receives prompts describing a causal DAG structure, a perfect intervention on a target variable, and a binary classification question about whether a specific causal relation changes. The evaluation uses a modified accuracy metric that requires correct understanding of the base DAG plus correct intervention effect prediction. Six LLMs are tested across three causal graph types (bivariate, confounding, mediation) and three variable naming strategies (random characters, Tübingen dataset variables, random Tübingen variables), with each intervention effect tested 15 times using different variable name samples.

## Key Results
- GPT-4 and GPT-4-turbo achieve near-perfect accuracy (98%+) on intervention effect prediction tasks
- LLaMA models struggle significantly, with accuracy around 60-70% on similar tasks
- GPT-4-turbo shows robustness to variations in intervention description format, while other models' performance generally suffers
- Models demonstrate ability to generalize intervention reasoning beyond memorized causal facts, as evidenced by consistent performance across different variable naming strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT models can generalize intervention reasoning beyond memorized causal facts
- Mechanism: When given prompts describing causal DAGs and interventions, GPT models update their internal representation of causal relations by applying the intervention axiom (deleting incoming edges to intervened variables) rather than retrieving pre-learned associations
- Core assumption: The models have learned the abstract causal principle that interventions sever parent-child relationships
- Evidence anchors:
  - [abstract] "GPT models can generalize intervention reasoning beyond memorized facts"
  - [section] "GPT models show promising accuracy at predicting the intervention effects"
  - [corpus] Weak - no direct corpus evidence for this mechanism
- Break condition: If performance drops significantly when using random variable names instead of real-world entities from Tübingen dataset

### Mechanism 2
- Claim: LLMs demonstrate different sensitivities to intervention descriptions based on their training
- Mechanism: Models trained on more diverse causal reasoning tasks (GPT-4-turbo) maintain performance when intervention descriptions are varied, while others rely on pattern matching to standard "do()" notation
- Core assumption: Some models have learned to recognize causal intervention concepts more abstractly
- Evidence anchors:
  - [section] "GPT-4-turbo appears to be robust to changes in the way interventions are described"
  - [section] "for GPT-3.5, GPT-4o, and LLaMA models, performance generally suffers" under substitution
  - [corpus] No direct evidence - this is inferred from the substitution task results
- Break condition: If all models show equal degradation when intervention descriptions are altered

### Mechanism 3
- Claim: LLMs can distinguish between observational and interventional reasoning when properly prompted
- Mechanism: The models parse the prompt structure to identify when they're being asked about post-intervention scenarios versus baseline causal relationships
- Core assumption: The verbalization format (describing "observed" vs "intervened" graphs) is sufficient for the model to switch reasoning modes
- Evidence anchors:
  - [section] "we slightly modify the prompt... asking the LLM about a causal relation but in the 'observed causal graphical model' instead"
  - [section] "an LLM's prediction is only considered correct when it understands the DAG G correctly"
  - [corpus] No direct evidence - this relies on the modified evaluation metric
- Break condition: If accuracy drops when asking about both observational and interventional states in the same prompt

## Foundational Learning

- Concept: Causal DAG structure and perfect interventions
  - Why needed here: The entire evaluation framework depends on understanding how interventions modify causal graphs by severing parent-child relationships
  - Quick check question: If variable A causes B in a DAG, what happens to this relationship after a perfect intervention on B?

- Concept: Binary classification task formulation
  - Why needed here: Intervention effects are defined as binary classification problems comparing causal relations before and after interventions
  - Quick check question: Given a DAG where A→B and we intervene on A, what is the intervention effect for the relation A→B?

- Concept: Zero-shot prompting for causal reasoning
  - Why needed here: The evaluation requires prompting LLMs without training examples to assess their ability to generalize causal reasoning
  - Quick check question: How would you verbally describe a causal DAG with three variables where the first causes the second and third?

## Architecture Onboarding

- Component map:
  Prompt generator -> LLM interface -> Response parser -> Accuracy calculator -> Benchmark manager

- Critical path:
  1. Generate prompt for causal DAG description
  2. Generate prompt for intervention description
  3. Generate prompt for causal relation query
  4. Send all prompts to LLM
  5. Parse responses and check format
  6. Calculate intervention effect prediction
  7. Compare against ground truth

- Design tradeoffs:
  - Using random characters vs real variable names: Random chars test generalization but may be harder for models to parse; real names may trigger memorization
  - Single vs multiple prompt approach: Multiple prompts allow checking DAG understanding but increase complexity
  - Modified accuracy metric: More accurate assessment but harder to interpret than simple accuracy

- Failure signatures:
  - Consistent "yes" or "no" responses regardless of intervention: Model not parsing DAG structure
  - High accuracy on baseline relations but low on intervention effects: Model memorizing rather than reasoning
  - Format compliance failures after retries: Prompt not clear enough about expected response structure

- First 3 experiments:
  1. Test GPT-3.5 on bivariate DAG with random character names to establish baseline performance
  2. Test same model on Tübingen dataset to check for memorization effects
  3. Compare GPT-4 vs LLaMA-2 on the same random character bivariate DAG to identify performance gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design benchmarks that better isolate causal reasoning from other cognitive tasks like statistical inference and relation retrieval in LLMs?
- Basis in paper: [inferred] The authors note that intervention effects cannot assess causal identification and that relation retrieval can serve as a shortcut for predicting intervention effects, suggesting current benchmarks may conflate different reasoning capabilities.
- Why unresolved: The paper identifies limitations of current benchmarks but doesn't propose specific solutions for isolating pure causal reasoning from other cognitive tasks.
- What evidence would resolve it: A new benchmark design that demonstrates clear separation between causal reasoning performance and performance on related cognitive tasks, validated through ablation studies.

### Open Question 2
- Question: What specific aspects of LLM architecture or training contribute to their ability (or inability) to reason about interventions on causal relationships?
- Basis in paper: [explicit] The paper shows significant performance differences between GPT and LLaMA models, with GPT-4 variants achieving near-perfect accuracy while LLaMA models struggle, but doesn't investigate underlying architectural reasons.
- Why unresolved: The empirical results show clear performance differences but don't explore whether these differences stem from architectural features, training data, model size, or other factors.
- What evidence would resolve it: Systematic comparison of different model architectures and training approaches, including controlled experiments varying specific architectural components.

### Open Question 3
- Question: Can intervention reasoning capabilities in LLMs be improved through targeted fine-tuning or few-shot learning approaches?
- Basis in paper: [explicit] The authors state they focus on evaluation rather than proposing methods for improving causal reasoning in LLMs via few-shot learning or fine-tuning.
- Why unresolved: While the paper evaluates current capabilities, it doesn't explore whether these capabilities can be enhanced through additional training or prompting strategies.
- What evidence would resolve it: Experiments showing improved intervention reasoning performance after fine-tuning on relevant datasets or through effective few-shot prompting strategies.

## Limitations
- The study relies on zero-shot prompting without calibration or fine-tuning, which may not represent the full potential of LLM causal reasoning capabilities
- The modified accuracy metric, while rigorous, introduces complexity in interpretation and may not fully capture nuances of causal understanding
- Performance differences between models could stem from factors beyond causal reasoning ability, such as prompt sensitivity or format compliance

## Confidence

| Claim | Confidence |
|-------|------------|
| GPT models can generalize intervention reasoning beyond memorized facts | Medium |
| GPT-4-turbo is more robust to variations in intervention descriptions | Low-Medium |

## Next Checks
1. Test model performance on entirely novel causal graph structures not represented in the Tübingen dataset to verify true generalization beyond memorization
2. Conduct ablation studies varying prompt structure, length, and formatting to isolate the contribution of specific prompt elements to performance
3. Compare zero-shot prompting performance against few-shot or fine-tuned approaches to establish whether current results represent an upper bound on LLM causal reasoning capabilities