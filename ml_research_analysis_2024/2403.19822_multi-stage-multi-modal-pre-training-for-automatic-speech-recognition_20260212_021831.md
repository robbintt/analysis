---
ver: rpa2
title: Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition
arxiv_id: '2403.19822'
source_url: https://arxiv.org/abs/2403.19822
tags:
- pre-training
- speech
- mid-training
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-stage multi-modal pre-training approach
  for automatic speech recognition (ASR) that combines unsupervised pre-training on
  audio-visual data with a supervised translation-based mid-training stage. The method
  uses masked autoencoding (MAE) and contrastive learning (CLR) on audio-visual datasets
  including Kinetics, VoxCeleb2, and LRS3, followed by speech translation mid-training
  and fine-tuning on ASR tasks.
---

# Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2403.19822
- Source URL: https://arxiv.org/abs/2403.19822
- Reference count: 0
- Primary result: Achieves up to 38.45% relative WER improvement on Librispeech test-clean

## Executive Summary
This paper introduces a multi-stage multi-modal pre-training approach for automatic speech recognition (ASR) that combines unsupervised audio-visual pre-training with a supervised translation-based mid-training stage. The method uses masked autoencoding (MAE) and contrastive learning (CLR) on audio-visual datasets including Kinetics, VoxCeleb2, and LRS3, followed by speech translation mid-training and fine-tuning on ASR tasks. The approach achieves significant improvements over traditional audio-visual pre-training alone, with relative WER improvements of up to 38.45% on Librispeech test-clean and 26.18% on test-other.

## Method Summary
The approach consists of three stages: (1) Pre-training on large-scale audio-visual datasets using MAE and CLR objectives to learn robust speech representations; (2) Mid-training with speech translation tasks using paired speech-text datasets to align learned representations with text modality; (3) Fine-tuning on ASR tasks with frozen encoder weights. The method leverages datasets like Kinetics-600, VoxCeleb2, and LRS3 for pre-training, MuST-C for mid-training, and Librispeech for fine-tuning. The architecture uses a Conformer encoder for audio and ViT encoder for video, with a common decoder for MAE and task-specific decoders for translation and ASR.

## Key Results
- 38.45% relative WER improvement on Librispeech test-clean compared to audio-visual pre-training alone
- 26.18% relative WER improvement on Librispeech test-other
- SUPERB benchmark improvements across keyword spotting, intent classification, and phoneme recognition tasks
- MAE alone outperforms both CLR and MAE+CLR combinations, suggesting masked auto-encoding is particularly effective for ASR

## Why This Works (Mechanism)

### Mechanism 1
Multi-stage training aligns learned speech representations with text modality through translation mid-training. The mid-training stage forces the audio encoder to produce representations predictive of text output, creating a bridge between pre-training and fine-tuning. Evidence shows this improves WER by 38.45% on Librispeech test-clean. The approach transfers the pre-trained model's distribution toward the downstream task distribution. Break condition occurs if the mid-training task is too dissimilar from ASR, causing divergence from ASR-relevant representations.

### Mechanism 2
Multi-modal pre-training captures complementary information that improves ASR robustness. Combining audio-visual data during pre-training allows the model to learn correlations between visual cues (lip movements) and audio patterns, creating more robust speech representations. Evidence shows pre-training with paired audio-visual data can improve performance even on uni-modal datasets. Break condition occurs when visual modality is completely uncorrelated with audio content, potentially hurting performance.

### Mechanism 3
Combining local and global feature learning objectives improves ASR performance. MAE focuses on local reconstruction of masked frames while CLR learns global contrastive representations, together capturing both fine-grained and coarse-grained speech patterns. Evidence shows MAE alone (5.63%/11.53% WER) outperforms both CLR (6.00%/11.67%) and MAE+CLR (6.09%/11.85%). Break condition occurs if the downstream task requires only one type of feature, making the multi-objective approach unnecessarily complex.

## Foundational Learning

- **Masked Autoencoding (MAE)**
  - Why needed here: Reconstructs masked parts of speech and video, forcing the model to learn robust local features
  - Quick check question: How does random masking during MAE training help the model learn more robust representations?

- **Contrastive Learning (CLR)**
  - Why needed here: Learns to distinguish between matching and non-matching audio-visual pairs, creating global representations
  - Quick check question: What is the difference between the pooling operations used for audio vs video in the CLR setup?

- **Speech Translation as Mid-training**
  - Why needed here: Bridges the gap between pre-training (audio-visual) and fine-tuning (ASR) by aligning speech representations with text modality
  - Quick check question: Why might translation be more effective than other possible mid-training tasks like speaker identification?

## Architecture Onboarding

- **Component map**: Audio encoder → Common decoder (MAE) → Translation decoder (mid-training) → ASR decoder (fine-tuning)
- **Critical path**: Audio encoder → Common decoder (MAE) → Translation decoder (mid-training) → ASR decoder (fine-tuning)
- **Design tradeoffs**:
  - Sharing common decoder ensures audio and video representations are in same space but adds complexity
  - Using translation as mid-training requires paired speech-text data but provides strong alignment benefits
  - Multi-stage approach increases training time but significantly improves final performance
- **Failure signatures**:
  - No improvement over baseline → Check if pre-training datasets are too dissimilar from target task
  - Performance degradation → Check for catastrophic forgetting during mid-training or overfitting during fine-tuning
  - Inconsistent results across languages → Check if translation pairs are complementary to English
- **First 3 experiments**:
  1. Run MAE pre-training only on Kinetics dataset, then fine-tune on Librispeech to establish baseline
  2. Add translation mid-training with English-Italian pair to measure improvement
  3. Compare MAE vs CLR pre-training on same dataset to understand which objective works better for ASR

## Open Questions the Paper Calls Out
None

## Limitations
- Limited ablation studies on translation mid-training (only 3 language pairs tested)
- Lack of analysis on whether benefits generalize to non-English ASR tasks
- No investigation into computational overhead of the multi-stage approach

## Confidence
- **High confidence**: Translation mid-training benefits (38.45% WER improvement on Librispeech test-clean)
- **Medium confidence**: Audio-visual pre-training contributions (mixed results between MAE vs CLR vs MAE+CLR)
- **Medium confidence**: SUPERB benchmark improvements (variable gains across tasks)

## Next Checks
1. **Translation Pair Ablation**: Systematically test mid-training with all available MuST-C language pairs and different numbers of translation pairs to identify optimal configuration
2. **Downstream Task Generalization**: Apply pre-trained models to diverse ASR tasks beyond Librispeech, including accented speech, noisy environments, and non-English languages
3. **Computational Efficiency Analysis**: Measure total training time and computational resources required for complete multi-stage pipeline versus direct supervised training