---
ver: rpa2
title: Towards virtual painting recolouring using Vision Transformer on X-Ray Fluorescence
  datacubes
arxiv_id: '2410.08826'
source_url: https://arxiv.org/abs/2410.08826
tags:
- https
- ma-xrf
- vision
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of virtual recoloring X-ray fluorescence
  (XRF) datacubes from pictorial artworks, hindered by small datasets and large memory
  requirements. They propose a pipeline that (1) generates synthetic XRF datacubes
  from a pigment database and RGB images, (2) trains a Deep Variational Embedding
  model to reduce dimensionality and extract apparatus-independent features, and (3)
  uses a Vision Transformer (SmallUViT) to map embedded XRF images to RGB colors.
---

# Towards virtual painting recolouring using Vision Transformer on X-Ray Fluorescence datacubes

## Quick Facts
- arXiv ID: 2410.08826
- Source URL: https://arxiv.org/abs/2410.08826
- Reference count: 40
- Primary result: Achieves MS-SSIM of 0.9196 and sRGB loss of 0.1148 on synthetic test set

## Executive Summary
This paper addresses the challenge of virtual recoloring X-ray fluorescence (XRF) datacubes from pictorial artworks, where small datasets and large memory requirements hinder traditional deep learning approaches. The authors propose a pipeline that generates synthetic XRF datacubes from RGB images and a pigment database, reduces dimensionality using a Deep Variational Embedding model, and maps embedded spectra to RGB colors using a Vision Transformer (SmallUViT). The method achieves strong performance on synthetic test data (MS-SSIM 0.9196, sRGB loss 0.1148) while identifying semantic regions, though it struggles with certain pigments like incarnate and shows artifacts in complex cases. Future work includes domain adaptation for real-world data and generative models to improve visual fidelity.

## Method Summary
The method involves three main steps: (1) generating synthetic MA-XRF datacubes from RGB images using a pigment database and ganX library, creating ~312,000 samples with 512-dimensional spectra; (2) training a Deep Variational Embedding model (4-layer encoder/decoder, 3D latent space) to reduce dimensionality while preserving spectral features using reconstruction, MMD, and silhouette losses; and (3) training a SmallUViT Vision Transformer (3+1+3 layers, 16×16 patches, 9 heads) to map embedded spectra to RGB images using sRGB distance loss and MS-SSIM metric. The pipeline successfully identifies colors for semantic regions but shows limitations with certain pigments and artifacts in complex cases.

## Key Results
- Achieves MS-SSIM of 0.9196 and sRGB loss of 0.1148 on synthetic test set
- Successfully identifies colors for semantic regions in recolored images
- Struggles with certain pigments (e.g., incarnate) and shows artifacts in complex cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic XRF spectra generation enables training despite scarce real-world data.
- Mechanism: The pipeline uses a pigment database to create realistic synthetic XRF spectra paired with RGB colors, expanding the training dataset to ~312,000 samples.
- Core assumption: Synthetic spectra sufficiently approximate real XRF signals in terms of spectral characteristics.
- Evidence anchors:
  - [abstract] "To circumvent the small dataset size, we generate a synthetic dataset, starting from a database of XRF spectra"
  - [section] "To generate a meaningful Spectral Dataset, we started from a database of pigments' XRF signal [38]"
- Break condition: If real XRF signals have characteristics not captured by the pigment database (e.g., environmental noise, instrument-specific artifacts), the synthetic data would mislead training.

### Mechanism 2
- Claim: Deep Variational Embedding reduces dimensionality while preserving spectral features.
- Mechanism: A VAE-style encoder maps 512-dimensional spectra to 3-dimensional latent space, using MMD regularization and clustering loss to maintain feature separation.
- Core assumption: The 3D latent space captures sufficient information to distinguish pigments while enabling efficient processing.
- Evidence anchors:
  - [abstract] "we define a Deep Variational Embedding network to embed the XRF spectra into a lower dimensional, K-Means friendly, metric space"
  - [section] "The idea of this part of the pipeline is to train a Deep Variational Embedding model [17, 33, 43–52] to dimensionally reduce the signal via relevant feature extraction"
- Break condition: If important spectral features are lost during dimensionality reduction, the model cannot accurately map to RGB colors.

### Mechanism 3
- Claim: Vision Transformer with U-Net architecture maps embedded spectra to RGB colors effectively.
- Mechanism: SmallUViT uses shifted patch tokenization and locality self-attention to learn the mapping from 3D embedded spectra to RGB images, trained with sRGB distance loss.
- Core assumption: The relationship between embedded spectra and RGB colors is learnable by the transformer architecture.
- Evidence anchors:
  - [abstract] "we train a set of models to assign coloured images to embedded XRF images"
  - [section] "As a model, we use a Vision Transformer (ViT) [59]... we use Shifted Patch Tokenization (SPT) embedding, and Locality Self-Attention (LSA)"
- Break condition: If the mapping between embedded features and RGB colors is too complex or non-linear, the transformer cannot learn it with available training data.

## Foundational Learning

- Concept: XRF spectroscopy and spectral datacubes
  - Why needed here: Understanding the input data structure and characteristics is essential for proper preprocessing and model design
  - Quick check question: What information does each dimension of the XRF datacube represent?

- Concept: Variational autoencoders and latent space learning
  - Why needed here: The Deep Variational Embedding model is central to reducing data dimensionality and extracting features
  - Quick check question: How does the MMD loss differ from standard reconstruction loss in VAEs?

- Concept: Vision Transformer architecture and attention mechanisms
  - Why needed here: The SmallUViT model uses transformer blocks to map embedded features to RGB images
  - Quick check question: What problem does shifted patch tokenization solve in ViT models?

## Architecture Onboarding

- Component map: Synthetic data generation (ganX library) → 512-dimensional XRF spectra → Deep Variational Embedding (VAE with clustering) → 3-dimensional latent vectors → SmallUViT Vision Transformer → RGB images → Evaluation (MS-SSIM, sRGB loss)

- Critical path: Synthetic data → Embedding model → RGB mapping model → Evaluation

- Design tradeoffs:
  - 3D latent space vs. higher dimensions: Simpler but may lose information
  - Vision Transformer vs. CNN: Better for small datasets but may lack spatial priors
  - sRGB loss vs. standard MSE: More perceptually accurate but computationally heavier

- Failure signatures:
  - Poor MS-SSIM scores indicate reconstruction quality issues
  - Artifacts in complex cases suggest model struggles with mixed pigments
  - Domain adaptation needed if real vs. synthetic performance gap is large

- First 3 experiments:
  1. Train embedding model only on synthetic spectra, evaluate clustering quality
  2. Train SmallUViT on synthetic data, evaluate MS-SSIM on test set
  3. Apply trained pipeline to real XRF data (with domain adaptation) and compare results to synthetic performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would a generative adversarial network (GAN) approach be for improving the reconstruction of visual details lost during RGB clustering in the synthetic dataset generation?
- Basis in paper: [inferred] The authors mention considering a Pix2Pix approach using SmallUViT as backbone to infer visual details not properly encoded in the model's input.
- Why unresolved: The paper only proposes this as a future direction without testing it. The current pipeline's limitations with certain pigments and artifacts suggest generative models could address these issues, but their actual performance remains unknown.
- What evidence would resolve it: Comparative experiments showing MS-SSIM and UIQI scores for GAN-enhanced reconstructions versus the current SmallUViT approach on test images with complex pigments or visual details lost in clustering.

### Open Question 2
- Question: How would expanding the dataset beyond paintings and frescoes affect the domain adaptation for real-world MA-XRF images?
- Basis in paper: [explicit] The authors propose expanding the dataset by removing restrictions on scraped images from WikiData to include non-artistic scenes for pre-training.
- Why unresolved: While the paper suggests this approach could help the model learn semantic aspects of images, it acknowledges this would alter the dataset domain and could introduce scenes not usually represented in pictorial artworks.
- What evidence would resolve it: Results from a two-stage training experiment showing the impact of enlarged dataset pre-training on MS-SSIM and UIQI scores for real-world MA-XRF image recoloring, with and without the domain adaptation step.

### Open Question 3
- Question: What specific domain adaptation technique would be most effective for handling the shift between synthetic and real MA-XRF data?
- Basis in paper: [explicit] The authors mention considering an unsupervised domain adaptation approach, possibly as an adversarial step, but do not specify which technique.
- Why unresolved: The paper only suggests that domain adaptation is crucial for real-world application but doesn't explore or test specific methods, leaving the optimal approach undetermined.
- What evidence would resolve it: Comparative experiments testing different domain adaptation techniques (e.g., adversarial training, feature alignment, self-training) on a set of real MA-XRF images, measuring the improvement in visual quality metrics and visual fidelity.

## Limitations
- Synthetic data dependency creates unknown performance gap for real-world XRF datacubes
- 3D latent space dimensionality may compress out subtle spectral features crucial for distinguishing similar pigments
- Method shows particular difficulty with incarnate pigments, suggesting clustering approach may not capture all pigment relationships

## Confidence
- Synthetic data generation pipeline: High confidence - well-documented and reproducible
- Dimensionality reduction effectiveness: Medium confidence - theoretical justification but limited real-world validation
- Vision Transformer mapping accuracy: Medium confidence - strong synthetic performance but unknown real-world transfer

## Next Checks
1. Test the trained pipeline on real XRF datacubes from actual artworks and measure performance degradation compared to synthetic results
2. Evaluate the 3D embedding space using t-SNE visualization to verify meaningful pigment clustering and identify potential information loss
3. Conduct ablation studies varying latent space dimensionality (2D, 4D, 5D) to quantify the tradeoff between computational efficiency and reconstruction quality