---
ver: rpa2
title: 'Even-if Explanations: Formal Foundations, Priorities and Complexity'
arxiv_id: '2401.10938'
source_url: https://arxiv.org/abs/2401.10938
tags:
- semifactual
- explanations
- complexity
- preferences
- semifactuals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates local post-hoc explainability queries within\
  \ the semifactual \u2018even-if\u2019 thinking and their computational complexity\
  \ among different classes of models. It introduces a preference-based framework\
  \ enabling users to personalize explanations based on their preferences, both in\
  \ the case of semifactuals and counterfactuals."
---

# Even-if Explanations: Formal Foundations, Priorities and Complexity

## Quick Facts
- arXiv ID: 2401.10938
- Source URL: https://arxiv.org/abs/2401.10938
- Reference count: 40
- Primary result: Both linear and tree-based models are strictly more interpretable than neural networks for both counterfactual and semifactual explanations.

## Executive Summary
This paper investigates local post-hoc explainability queries for binary classification models, focusing on both counterfactual and semifactual ('even-if') explanations. The authors introduce a preference-based framework that enables users to personalize explanations based on their preferences, creating a partial order over explanations. The key finding is that perceptrons and FBDDs are strictly more interpretable than MLPs, as the complexity of finding minimum changes (counterfactuals) or maximum changes (semifactuals) is lower for the former classes. The paper provides theoretical complexity results showing that semifactual problems are not harder than counterfactual problems, with PTIME algorithms for perceptrons and FBDDs, and NP-complete algorithms for MLPs.

## Method Summary
The paper employs theoretical complexity analysis to compare the interpretability of three model classes: perceptrons, FBDDs, and MLPs. For each model class, the authors analyze the computational complexity of finding minimal counterfactuals (MCR) and maximal semifactuals (MCA). They prove that for perceptrons and FBDDs, both MCA and MCR are in PTIME, while for MLPs they are NP-complete. The preference-based framework is formalized through a degree of satisfaction function δ that creates a partial order over explanations. The authors also explore variants like constrained and bounded versions of the explanation problems, providing polynomial-time algorithms for tractable cases and proving NP-completeness for intractable ones.

## Key Results
- Perceptrons and FBDDs are strictly more interpretable than MLPs for both counterfactual and semifactual explanations.
- Semifactual explanation problems (MCA) are not harder than counterfactual problems (MCR) in terms of computational complexity.
- The preference-based framework enables user-centric explanations through a degree of satisfaction metric.
- Several interpretability problems in the preference-based framework are shown to be polynomial-time solvable.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perceptrons and FBDDs are strictly more interpretable than MLPs for both counterfactual and semifactual explanations.
- Mechanism: The computational complexity of finding minimum changes (counterfactuals) or maximum changes (semifactuals) is lower for perceptrons and FBDDs (PTIME) than for MLPs (NP-complete). This difference in complexity directly translates to interpretability.
- Core assumption: Interpretability can be quantified through computational complexity of explanation queries.
- Evidence anchors:
  - [abstract] "both linear and tree-based models are strictly more interpretable than neural networks"
  - [section] "Theorem 1 states that the class of models 'perceptron' and 'FBDD' is strictly more interpretable than the class 'MLP'"
  - [corpus] Weak - related papers focus on different aspects of explainability, not complexity comparisons
- Break condition: If the complexity results were reversed, or if interpretability was measured by factors other than computational complexity.

### Mechanism 2
- Claim: Preference-based frameworks enable user-centric explanations by allowing users to specify feature preferences.
- Mechanism: The framework introduces a degree of satisfaction (δ) for each preference rule, creating a partial order over explanations. This allows ranking explanations based on how well they satisfy user preferences.
- Core assumption: Users can express meaningful preferences over features that improve explanation quality.
- Evidence anchors:
  - [abstract] "introduces a preference-based framework that enables users to personalize explanations based on their preferences"
  - [section] "δ represents the position of the first feature literal satisfied in the ordered list provided in the head of a preference rule"
  - [corpus] Weak - related papers don't address preference-based explanation frameworks
- Break condition: If user preferences couldn't be meaningfully expressed or if the preference framework became too complex to be practical.

### Mechanism 3
- Claim: The complexity of semifactual explanation problems (MCA) is not higher than counterfactual problems (MCR).
- Mechanism: The paper proves that MCA is in PTIME for FBDDs and perceptrons, and NP-complete for MLPs, matching the complexity results for MCR. This shows semifactuals aren't harder computationally than counterfactuals.
- Core assumption: Computational complexity directly reflects the difficulty of explanation problems.
- Evidence anchors:
  - [section] "MCA is i) in PTIME for FBDDs and perceptrons, and ii) NP-complete for MLPs"
  - [section] "perceptrons and FBDDs are strictly more interpretable than MLPs, in the sense that the complexity of answering post-hoc queries for models in the first two classes is lower than for those in the latter"
  - [corpus] Weak - related papers don't compare semifactual and counterfactual complexity
- Break condition: If semifactual problems were proven to be inherently harder than counterfactual problems.

## Foundational Learning

- Concept: Binary classification models
  - Why needed here: The paper focuses on models mapping {0,1}^n to {0,1}, which forms the basis for all explanation queries
  - Quick check question: What is the output space for the classification models discussed in this paper?

- Concept: Computational complexity classes (P, NP, coNP)
  - Why needed here: Complexity results are central to the interpretability claims, comparing PTIME, NP-complete, and coNP-complete problems
  - Quick check question: Which complexity class contains problems solvable in polynomial time by deterministic Turing machines?

- Concept: Preference rule semantics and degree satisfaction
  - Why needed here: The preference framework relies on calculating δ(y, κ) to determine how well an explanation satisfies preference rules
  - Quick check question: What value of δ indicates that a preference rule is not satisfied by an explanation?

## Architecture Onboarding

- Component map:
  - Model classes (perceptrons, FBDDs, MLPs) → Explanation algorithms (MCA, MCR) → Preference framework → Best explanation selection

- Critical path: For a new instance x and model M:
  1. Determine model class
  2. Apply appropriate explanation algorithm (MCA for semifactuals, MCR for counterfactuals)
  3. If preferences exist, calculate δ values for each explanation
  4. Select best explanation(s) based on δ values

- Design tradeoffs:
  - Precision vs. computational cost: MLPs offer more expressive power but at higher computational cost for explanations
  - Generality vs. efficiency: The preference framework is general but may be computationally expensive for complex preference sets
  - Exact vs. heuristic solutions: Algorithms for MLPs may need heuristics rather than exact solutions

- Failure signatures:
  - Unexpected NP-completeness results suggesting incorrect complexity analysis
  - Preference satisfaction calculations yielding inconsistent results
  - Explanation algorithms failing to find valid explanations when they should exist

- First 3 experiments:
  1. Implement perceptron MCA algorithm and verify it finds semifactuals at maximum distance
  2. Test preference framework by creating a simple BCMp instance and verifying best explanation selection
  3. Compare runtime of MCA vs MCR for identical instances across different model classes

## Open Questions the Paper Calls Out
None

## Limitations
- The focus on binary classification limits generalizability to multi-class or regression problems.
- Computational complexity results don't necessarily translate to practical interpretability differences for small-scale problems.
- The preference framework's real-world effectiveness