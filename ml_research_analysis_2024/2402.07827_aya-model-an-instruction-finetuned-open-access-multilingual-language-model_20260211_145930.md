---
ver: rpa2
title: 'Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model'
arxiv_id: '2402.07827'
source_url: https://arxiv.org/abs/2402.07827
tags:
- languages
- language
- arxiv
- flores-200
- spbleu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Aya model addresses linguistic inequality in large language
  models by instruction-finetuning a 13B mT5 model on 101 languages, over half of
  which are lower-resourced. It expands training data to 203M examples across 101
  languages using a mix of multilingual templates, human annotations, translated datasets,
  and synthetic generations.
---

# Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model

## Quick Facts
- **arXiv ID**: 2402.07827
- **Source URL**: https://arxiv.org/abs/2402.07827
- **Reference count**: 40
- **Primary result**: Aya model instruction-finetuned on 101 languages shows 13.1% relative gain over mT0 on multilingual generative tasks

## Executive Summary
Aya is an open-access multilingual language model that addresses linguistic inequality by instruction-finetuning a 13B mT5 model across 101 languages, with over half being lower-resourced. The model expands training data to 203M examples using multilingual templates, human annotations, translated datasets, and synthetic generations. Comprehensive evaluations show Aya outperforming mT0 and BLOOMZ on both generative and discriminative tasks across 99 languages, with strong preference in human and GPT-4 evaluations. The model includes safety mitigations through multilingual context distillation, reducing harmful outputs by 78-89%, and is released under Apache 2.0 license.

## Method Summary
Aya was developed by instruction-finetuning a 13B mT5 model using a diverse dataset of 203M examples spanning 101 languages. The training data combined multilingual templates, human-annotated instructions, translated datasets, and synthetic generations to create comprehensive multilingual instruction coverage. The model underwent safety training through multilingual context distillation to reduce harmful outputs. Evaluation was conducted across 99 languages using both generative and discriminative benchmarks, with additional preference testing via human raters and GPT-4.

## Key Results
- Aya achieves 13.1% relative gain over mT0 and 11.7% over BLOOMZ on multilingual generative tasks
- Preference evaluations show 75-89% win rates over mT0 in both human and GPT-4 assessments
- Safety context distillation reduces harmful outputs by 78-89% across all 101 languages

## Why This Works (Mechanism)
The model's success stems from its comprehensive multilingual instruction coverage and diverse training data sources. By combining human annotations, translated datasets, synthetic generations, and multilingual templates, Aya captures linguistic patterns across both high and low-resource languages. The instruction-finetuning approach allows the model to follow complex multilingual instructions while maintaining performance across different task types. Safety mitigations through context distillation address potential harmful outputs systematically across all supported languages.

## Foundational Learning
- **Multilingual instruction tuning**: Fine-tuning language models on diverse instruction-following datasets across multiple languages to improve cross-lingual generalization
- **Context distillation for safety**: Training models to recognize and mitigate harmful outputs through context-aware safety fine-tuning
- **Synthetic data generation**: Creating training examples programmatically to expand coverage for lower-resourced languages
- **Preference-based evaluation**: Using human and model-based preference judgments to assess output quality beyond traditional metrics

## Architecture Onboarding
- **Component map**: mT5 13B -> Multilingual instruction tuning -> Safety context distillation -> Evaluation
- **Critical path**: Instruction tuning data preparation -> Model training -> Safety mitigation -> Multilingual evaluation
- **Design tradeoffs**: Scale (13B parameters) vs. efficiency, synthetic data quality vs. coverage, safety mitigation vs. instruction fidelity
- **Failure signatures**: Performance degradation on extremely low-resource languages, safety mitigation gaps in nuanced cultural contexts
- **First experiments**: 1) Monolingual evaluation on subset of lower-resourced languages, 2) Adversarial safety testing across all 101 languages, 3) Task-specific performance comparison with specialized models

## Open Questions the Paper Calls Out
None

## Limitations
- Limited comprehensive monolingual evaluation benchmarks for many of the 101 languages
- Potential quality inconsistencies from synthetic data and translated datasets for lower-resourced languages
- Safety evaluation limited to specific harmful categories across all languages
- 13B parameter size may underperform specialized smaller models in certain domains

## Confidence
- **Performance gains over mT0/BLOOMZ**: High for tested multilingual generative and discriminative tasks
- **Cross-linguistic generalization**: Medium due to limited public benchmarks
- **Safety claims**: Medium due to evaluation depth limitations
- **Scientific novelty**: Medium, incremental improvement over instruction tuning approaches

## Next Checks
1. Conduct monolingual evaluations on a subset of lower-resourced languages to verify uniform performance gains and identify any regressions
2. Perform adversarial safety testing across all 101 languages to assess robustness of context distillation against emerging harmful patterns
3. Compare Aya's performance against specialized monolingual or low-resource models to quantify trade-offs between scale and task-specific accuracy