---
ver: rpa2
title: Enhancing Confidence Expression in Large Language Models Through Learning from
  Past Experience
arxiv_id: '2404.10315'
source_url: https://arxiv.org/abs/2404.10315
tags:
- confidence
- arxiv
- llms
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method called LePe (Learning from Past
  Experience) to improve the ability of large language models (LLMs) to express their
  confidence in their responses. LLMs often generate inaccurate information with high
  confidence, so LePe aims to align the expressed confidence with the true probability
  of correctness.
---

# Enhancing Confidence Expression in Large Language Models Through Learning from Past Experience

## Quick Facts
- arXiv ID: 2404.10315
- Source URL: https://arxiv.org/abs/2404.10315
- Reference count: 7
- Primary result: Introduces LePe method to align LLM confidence expression with true correctness probability through fine-tuning with curated datasets

## Executive Summary
This paper addresses a critical limitation in large language models: their tendency to express high confidence even when generating inaccurate information. The proposed method, Learning from Past Experience (LePe), aims to align the expressed confidence of LLMs with their true probability of correctness. The approach involves three key stages: testing the model's inherent confidence expression, fine-tuning the model to learn proper confidence calibration, and predicting confidence for new questions. The method demonstrates significant improvements in confidence calibration across multiple datasets and model architectures, particularly within the Llama family.

## Method Summary
The LePe method operates through a three-stage pipeline designed to improve LLM confidence calibration. First, the model's baseline confidence expression is evaluated using controlled test sets. Second, the model undergoes fine-tuning using a carefully curated dataset that includes mutation questions to address context sensitivity and hybrid sampling strategies. Third, the fine-tuned model can predict confidence scores for new questions that better reflect the true probability of correctness. The training data construction pipeline is particularly noteworthy, as it addresses the challenge of context sensitivity through systematic mutation of questions and employs hybrid sampling to ensure diverse and representative training examples.

## Key Results
- LePe effectively calibrates confidence expression, showing high correlation between confidence scores and actual accuracy
- Experiments demonstrate improvements across Llama family models on four diverse datasets
- The method addresses context sensitivity through mutation questions in the training pipeline
- Significant improvement in aligning expressed confidence with true correctness probability

## Why This Works (Mechanism)
The LePe method works by explicitly training LLMs to associate their confidence expressions with actual correctness outcomes. By using curated datasets with mutation questions, the model learns to distinguish between similar contexts where its confidence should vary based on actual knowledge. The fine-tuning process essentially creates a feedback loop where the model learns from past experiences of being correct or incorrect, allowing it to better calibrate its confidence expressions for future predictions.

## Foundational Learning
- **Confidence Calibration**: Why needed - to ensure LLM outputs match actual correctness probability; Quick check - correlation between confidence scores and accuracy metrics
- **Context Sensitivity**: Why needed - to prevent overconfident responses in similar but distinct scenarios; Quick check - performance on mutation question sets
- **Fine-tuning with Curated Data**: Why needed - to provide targeted learning signals for confidence calibration; Quick check - improvement in confidence-accuracy alignment after training
- **Hybrid Sampling Strategies**: Why needed - to ensure diverse and representative training data; Quick check - coverage of different question types and contexts in training set

## Architecture Onboarding

Component map: Data Generation Pipeline -> Fine-tuning Process -> Confidence Prediction

Critical path: Mutation question generation → Dataset curation → Model fine-tuning → Confidence evaluation

Design tradeoffs: The method prioritizes accuracy in confidence calibration over computational efficiency, as the fine-tuning process requires significant resources. The use of curated datasets ensures quality but may limit scalability.

Failure signatures: Overconfidence in out-of-distribution inputs, degradation of confidence calibration over time, and potential bias introduced by automated dataset construction methods.

First experiments:
1. Baseline confidence evaluation on controlled test sets
2. Fine-tuning with curated dataset including mutation questions
3. Confidence prediction evaluation on new questions

## Open Questions the Paper Calls Out
None

## Limitations
- The method's effectiveness in real-world, diverse applications and robustness to adversarial or out-of-distribution inputs remain uncertain
- The computational cost and scalability of the fine-tuning process are not thoroughly analyzed, particularly for larger models
- The long-term stability of the calibrated confidence is not explored, leaving questions about whether improvements persist over time

## Confidence

- **High Confidence**: The method improves confidence calibration in controlled experimental settings
- **Medium Confidence**: The approach generalizes across different datasets and model architectures
- **Low Confidence**: The method's effectiveness in real-world, diverse applications and its robustness to adversarial or out-of-distribution inputs

## Next Checks
1. Test the LePe method on a broader range of datasets, including those with adversarial examples or out-of-distribution inputs, to assess robustness and generalization
2. Conduct a long-term study to evaluate the stability of the calibrated confidence over extended periods of model use and fine-tuning
3. Compare the computational efficiency and scalability of LePe with other confidence calibration methods, particularly for larger models or more extensive datasets