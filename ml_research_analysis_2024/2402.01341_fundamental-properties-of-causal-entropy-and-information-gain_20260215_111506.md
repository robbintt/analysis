---
ver: rpa2
title: Fundamental Properties of Causal Entropy and Information Gain
arxiv_id: '2402.01341'
source_url: https://arxiv.org/abs/2402.01341
tags: []
core_contribution: The paper establishes formal properties of causal entropy and causal
  information gain, two recently-proposed measures for quantifying causal control
  in structural causal models. It shows that causal entropy differs from post-stochastic
  intervention entropy, provides bounds on causal entropy including an independence
  bound, defines conditional versions of both quantities, and derives chain rules
  for them.
---

# Fundamental Properties of Causal Entropy and Information Gain

## Quick Facts
- arXiv ID: 2402.01341
- Source URL: https://arxiv.org/abs/2402.01341
- Reference count: 26
- Primary result: Establishes formal properties of causal entropy and information gain in structural causal models

## Executive Summary
This paper formally establishes fundamental properties of causal entropy and causal information gain, two recently-proposed measures for quantifying causal control in structural causal models. The work demonstrates that causal entropy differs from post-stochastic intervention entropy, provides bounds on causal entropy including an independence bound, defines conditional versions of both quantities, and derives chain rules. Notably, the paper reveals that causal entropy can exceed initial entropy and that a causal version of the data processing inequality does not hold, providing a mathematical foundation for using causal information theory in machine learning.

## Method Summary
The paper provides a theoretical analysis of causal entropy and causal information gain using information theory concepts and causal inference. It defines causal entropy as the expected entropy of Y after atomic interventions on X, and causal information gain as the difference between entropy and causal entropy. The authors establish properties including bounds, chain rules, and relationships to other quantities through rigorous mathematical proofs, primarily focusing on discrete and finite random variables in structural causal models.

## Key Results
- Causal entropy is not equivalent to post-stochastic intervention entropy, but they are related via conditioning
- Causal information gain can be negative, unlike mutual information
- Conditional causal entropy can exceed initial entropy of Y despite conditioning typically reducing entropy
- A causal version of the data processing inequality does not hold

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal entropy is not equivalent to post-stochastic intervention entropy, but they are related via conditioning.
- Mechanism: Causal entropy averages entropies after atomic interventions, while post-stochastic intervention entropy conditions after performing the stochastic intervention. This difference arises because the distribution of Y after a stochastic intervention depends on the distribution of X, not just its marginal.
- Core assumption: The SCM has independent noise variables and the intervention protocol X' is independent of other variables.
- Evidence anchors:
  - [abstract]: "We show that, perhaps unexpectedly, causal entropy is not the same as the entropy after a stochastic intervention..."
  - [section]: "One could think that Hc(Y | do(X ∼ X ′)) = H(Y | do(X = X ′)): both are entropies of Y resulting from making X follow the distribution pX ′, albeit through two distinct procedures."
  - [corpus]: Weak evidence - corpus neighbors focus on related entropy estimation but don't directly address causal vs. post-intervention entropy distinction.
- Break condition: If the noise variables are not independent or the intervention protocol X' is not independent of other variables, the relationship between causal and post-stochastic intervention entropy may break down.

### Mechanism 2
- Claim: Causal information gain can be negative, unlike mutual information.
- Mechanism: Causal information gain measures the reduction in uncertainty about Y after intervening on X. If the intervention increases uncertainty (e.g., by introducing new randomness), the causal information gain becomes negative.
- Core assumption: The intervention protocol X' can introduce new randomness or dependencies not present in the original SCM.
- Evidence anchors:
  - [abstract]: "We check that, surprisingly, causal information gain can be negative."
  - [section]: "In contrast with mutual information, causal information gain can be negative. This is an immediate corollary of Proposition 10."
  - [corpus]: No direct evidence in corpus neighbors.
- Break condition: If the intervention protocol X' is chosen such that it always reduces uncertainty about Y, or if the SCM is constrained to prevent uncertainty increase.

### Mechanism 3
- Claim: Conditional causal entropy reduces causal entropy on average, but can still be greater than the initial entropy of Y.
- Mechanism: Conditioning on Z after intervening on X provides additional information about Y, reducing the causal entropy. However, the intervention itself can increase uncertainty, leading to a conditional causal entropy greater than the initial entropy.
- Core assumption: The conditioning variable Z provides some information about Y, and the intervention protocol X' can introduce new randomness.
- Evidence anchors:
  - [section]: "Since this conditioning set is a superset of the conditioning set in H(Y | X, do(X = X ′)), we can use the fact that conditioning cannot increase entropy to conclude that the conditional causal entropy cannot be larger than the causal entropy."
  - [section]: "Proposition 10 tells us that, on average, intervening on X can in fact increase the uncertainty about Y."
  - [corpus]: No direct evidence in corpus neighbors.
- Break condition: If the conditioning variable Z provides no information about Y, or if the intervention protocol X' always reduces uncertainty.

## Foundational Learning

- Concept: Structural Causal Models (SCMs)
  - Why needed here: The paper defines causal entropy and information gain in the context of SCMs, so understanding the structure and semantics of SCMs is crucial.
  - Quick check question: What are the key components of an SCM, and how do they relate to each other?

- Concept: Entropy and Conditional Entropy
  - Why needed here: Causal entropy is defined as an average of entropies after interventions, and conditional causal entropy involves conditioning. Understanding the properties of entropy and conditional entropy is essential for grasping the paper's results.
  - Quick check question: How does conditioning affect entropy, and what are the key properties of conditional entropy?

- Concept: Mutual Information and Information Gain
  - Why needed here: Causal information gain is defined as the difference between entropy and causal entropy, analogous to how mutual information is the difference between entropy and conditional entropy. Understanding mutual information and information gain is necessary for interpreting the paper's results.
  - Quick check question: How does mutual information capture the reduction in uncertainty, and how is it related to conditional entropy?

## Architecture Onboarding

- Component map: SCM definition and properties -> Entropy and conditional entropy definitions and properties -> Causal entropy and conditional causal entropy definitions and properties -> Causal information gain and conditional causal information gain definitions and properties -> Proofs and examples illustrating the key results

- Critical path: Understand the SCM framework and how interventions are modeled -> Grasp the definitions of causal entropy and information gain -> Study the key results, such as the relationship between causal and post-stochastic intervention entropy, and the properties of causal information gain -> Work through the examples to solidify understanding

- Design tradeoffs: The paper chooses to define causal entropy as an average of entropies after atomic interventions, rather than as a post-stochastic intervention entropy. This allows for a clearer connection to conditional entropy and enables the derivation of chain rules. The paper focuses on discrete and finite random variables for simplicity, but the results may not directly extend to continuous or infinite cases.

- Failure signatures: Misunderstanding the difference between causal and post-stochastic intervention entropy can lead to incorrect interpretations of causal information gain. Assuming that causal information gain is always non-negative can result in incorrect conclusions about the effects of interventions.

- First 3 experiments:
  1. Implement a simple SCM with a binary intervention variable and a binary outcome variable, and compute the causal entropy and information gain for different intervention protocols.
  2. Extend the experiment to include a conditioning variable, and compute the conditional causal entropy and information gain to see how conditioning affects the results.
  3. Modify the SCM to include a confounder, and observe how the causal entropy and information gain change compared to the non-confounded case.

## Open Questions the Paper Calls Out

- Open Question 1: What practical methods exist for computing causal entropy and causal information gain in complex structural causal models?
  - Basis in paper: Explicit - The paper concludes by stating "Establishing practical methods for computing these quantities remains a topic for future investigation."
  - Why unresolved: The paper establishes theoretical properties but does not provide computational algorithms or estimators for these quantities in practice.
  - What evidence would resolve it: Development and evaluation of consistent, unbiased estimators for causal entropy and information gain, with convergence analysis and empirical validation on benchmark causal models.

- Open Question 2: Under what conditions does causal entropy exceed initial entropy, and what are the implications for causal inference?
  - Basis in paper: Explicit - The paper proves that causal entropy can exceed initial entropy (Proposition 10) and notes this is "surprising" and "in stark contrast with conditional entropy."
  - Why unresolved: While the paper shows this is possible, it doesn't characterize when this occurs or explore the implications for interpreting causal relationships.
  - What evidence would resolve it: Theoretical characterization of conditions under which Hc(Y|do(X∼X')) > H(Y), along with examples demonstrating practical implications for causal inference.

- Open Question 3: How can causal information gain be connected to existing measures of causal strength, and what distinguishes it from these alternatives?
  - Basis in paper: Explicit - The paper notes "The precise connection between causal information gain and putative measures of causal strength, such as information flow, could be studied in detail."
  - Why unresolved: The paper establishes causal information gain as distinct from mutual information and shows it differs from information flow, but doesn't fully characterize the relationship between these measures.
  - What evidence would resolve it: Comparative analysis showing conditions under which causal information gain and information flow agree or diverge, with examples demonstrating their relative merits for measuring causal strength.

## Limitations

- Analysis is primarily theoretical with most results established through proofs rather than empirical validation
- Focus on discrete and finite random variables may limit direct applicability to continuous or infinite domains
- Does not provide practical computational methods for estimating causal entropy and information gain

## Confidence

- High confidence in the core claim that causal entropy differs from post-stochastic intervention entropy
- High confidence in the proof that causal information gain can be negative
- Medium confidence in conditional versions and chain rules due to more complex dependencies
- Medium confidence in bounds on causal entropy requiring additional empirical verification

## Next Checks

1. Implement concrete SCMs with different intervention protocols to empirically verify that causal entropy can exceed initial entropy and that causal information gain can be negative in practice.

2. Examine how the properties of causal entropy and information gain behave under various SCM structures, including cases with confounding, selection bias, and different noise distributions.

3. Investigate whether the core results extend to continuous random variables by testing with Gaussian SCMs and analyzing how the bounds and chain rules adapt to the continuous case.