---
ver: rpa2
title: 'CaseLink: Inductive Graph Learning for Legal Case Retrieval'
arxiv_id: '2403.17780'
source_url: https://arxiv.org/abs/2403.17780
tags:
- case
- graph
- cases
- legal
- caselink
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CaseLink, a graph-based method for legal case
  retrieval that leverages the connectivity relationships among legal cases. The key
  idea is to construct a Global Case Graph (GCG) that represents the case-to-case,
  case-to-charge, and charge-to-charge relationships.
---

# CaseLink: Inductive Graph Learning for Legal Case Retrieval

## Quick Facts
- **arXiv ID**: 2403.17780
- **Source URL**: https://arxiv.org/abs/2403.17780
- **Reference count**: 40
- **Primary result**: CaseLink significantly outperforms state-of-the-art baselines on COLIEE2022 and COLIEE2023 datasets for legal case retrieval.

## Executive Summary
This paper presents CaseLink, a graph-based method for legal case retrieval that leverages the connectivity relationships among legal cases. The key innovation is constructing a Global Case Graph (GCG) that represents case-to-case, case-to-charge, and charge-to-charge relationships. A graph neural network is then used to learn node representations that encode this connectivity information. Experiments on two benchmark datasets demonstrate that CaseLink significantly outperforms existing state-of-the-art methods, achieving the best performance across most evaluation metrics. Ablation studies validate the effectiveness of different components in the proposed framework.

## Method Summary
CaseLink addresses legal case retrieval through an inductive graph learning approach. The method constructs a Global Case Graph (GCG) with three types of edges: case-to-case edges based on BM25 similarity, case-to-charge edges based on charge inclusion, and charge-to-charge edges based on semantic similarity. A graph neural network (specifically a 2-layer GAT with residual connections) propagates and aggregates information across this graph to update node representations. The model is trained using an InfoNCE contrastive loss with hard negative mining and degree regularization. During inference, new cases are encoded, a new GCG is constructed, and nearest neighbor search is performed on the updated case node features to retrieve relevant precedents.

## Key Results
- CaseLink achieves the best performance across most metrics on both COLIEE2022 and COLIEE2023 datasets
- The method significantly outperforms state-of-the-art baselines including CaseGNN, SAILER, and PromptCase
- Ablation studies demonstrate the effectiveness of graph connectivity modeling and degree regularization components

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Graph-based connectivity modeling captures legal case relevance better than isolated text encoding.
- **Mechanism**: By constructing a Global Case Graph (GCG) that represents case-to-case, case-to-charge, and charge-to-charge relationships, CaseLink leverages graph neural networks (GNNs) to propagate and aggregate information across these connections, learning richer representations that encode latent case relationships.
- **Core assumption**: Legal case relevance can be partially inferred from connectivity patterns in a graph structure, even without direct case-reference labels during testing.
- **Evidence anchors**: [abstract] "a novel Global Case Graph is incorporated to represent both the case semantic relationship and the case legal charge relationship" [section 4.1] "Due to the inductive manner in the task of LCR, the case reference relationship is the ground truth label in LCR, which means that case reference is not applicable to be the input for models"
- **Break condition**: If the constructed graph edges (based on BM25, semantic similarity, and charge inclusion) do not meaningfully correlate with actual case relevance, the GNN will not learn useful representations.

### Mechanism 2
- **Claim**: Contrastive learning with degree regularization effectively trains the model in an inductive setting.
- **Mechanism**: The InfoNCE contrastive loss pulls relevant cases closer in representation space while pushing irrelevant ones apart, using hard negative mining for more effective training. The degree regularization term encourages candidate nodes to have lower degrees in the learned pseudo-adjacency matrix, aligning with the sparse nature of case references in reality.
- **Core assumption**: The contrastive objective can distinguish relevant from irrelevant cases based on learned representations, and that degree regularization helps prevent overfitting to dense connections that don't reflect real legal case relationships.
- **Evidence anchors**: [section 4.3.1] "the main objective of query is designed in an InfoNCE [46] style" [section 4.3.2] "a degree regularisation is proposed to minimise the case node degree for candidate nodes" [section 6.3] Ablation study shows that removing degree regularization ("w/o DegReg") decreases performance, supporting its effectiveness.
- **Break condition**: If the contrastive loss dominates training to the point where degree regularization becomes ineffective, or if the hard negative mining strategy fails to provide meaningful difficult examples.

### Mechanism 3
- **Claim**: The combination of case-to-case and case-to-charge edges in the GCG provides complementary information for case representation learning.
- **Mechanism**: Case-to-case edges (based on BM25 similarity) capture semantic similarity between cases, while case-to-charge edges capture the legal topic alignment. Charge-to-charge edges further model relationships between legal topics. This multi-relational graph structure allows the GNN to learn representations that encode both semantic similarity and legal topicality.
- **Core assumption**: Legal case relevance is influenced by both semantic similarity and shared legal charges, and that the combination of these relationships provides more information than either alone.
- **Evidence anchors**: [section 4.1.3] "pairwise BM25 [38] score between every cases... top ð‘˜ highest BM25 similarity scores cases are selected and linked" [section 4.1.5] "Clarifying the legal charge of a case is essential for legal practitioners to understand the case itself and identify relevant cases" [section 6.3] "CaseLink case-case variant... there is a significant improved performance over in all the metrics for COLIEE2023 and non-ranking metrics for COLIEE2022, which indicates the effectiveness of case connectivity relationship"
- **Break condition**: If the charge-to-charge edges do not meaningfully capture relationships between legal topics, or if the case-to-charge edges are too sparse to provide useful signal.

## Foundational Learning

- **Concept**: Graph Neural Networks (GNNs) and their message passing mechanism
  - **Why needed here**: CaseLink uses GNNs to learn node representations that aggregate information from neighboring nodes in the Global Case Graph. Understanding how GNNs work (e.g., GCN, GAT, GraphSAGE) is crucial for understanding the model architecture and how it leverages graph structure.
  - **Quick check question**: What is the difference between transductive and inductive learning in the context of GNNs, and why is inductive learning more challenging for CaseLink?

- **Concept**: Contrastive Learning and InfoNCE loss
  - **Why needed here**: CaseLink uses InfoNCE loss to train the model by pulling relevant cases closer and pushing irrelevant cases apart in the representation space. Understanding contrastive learning objectives and how they work is essential for grasping the training methodology.
  - **Quick check question**: How does the InfoNCE loss differ from a simple binary cross-entropy loss, and why is it more suitable for retrieval tasks like CaseLink?

- **Concept**: Legal case structure and the concept of precedents
  - **Why needed here**: CaseLink operates in the legal domain, where understanding the concept of precedents (relevant cases used to support decisions) is fundamental to understanding the task and the motivation for leveraging case connectivity.
  - **Quick check question**: What is the difference between a query case and a candidate case in the context of legal case retrieval, and how does the concept of precedents relate to the ground truth labels in the COLIEE datasets?

## Architecture Onboarding

- **Component map**: Case Encoder -> Charge Encoder -> Global Case Graph Construction -> Graph Neural Network -> Contrastive Learning Module -> Degree Regularization
- **Critical path**: 1. Encode cases and charges into initial node features 2. Construct the Global Case Graph with all three types of edges 3. Apply GNN to propagate and aggregate information across the graph 4. Use the updated node features in the contrastive learning objective with degree regularization 5. During inference, encode new cases, construct a new GCG, apply GNN, and perform nearest neighbor search for retrieval
- **Design tradeoffs**: Edge construction method: Using BM25 for case-to-case edges is simple but may not capture all relevant relationships; using learned embeddings could be more accurate but requires more computation. GNN layer depth: Deeper GNNs can capture higher-order relationships but are more prone to oversmoothing; shallower GNNs are faster but may miss important connections. Negative sampling strategy: Hard negative mining can improve training but requires careful selection; random negatives are simpler but may be less effective.
- **Failure signatures**: Performance drops significantly if the graph construction module fails to create meaningful edges (e.g., BM25 similarity is not correlated with actual relevance). Model underperforms if the GNN fails to effectively propagate and aggregate information (e.g., due to oversmoothing or insufficient message passing). Training becomes unstable or converges poorly if the contrastive learning objective is not balanced with the degree regularization term.
- **First 3 experiments**: 1. Verify that the Global Case Graph is constructed correctly by checking the number and distribution of edges for each edge type 2. Test the GNN module in isolation by training it on a simple node classification task using the constructed graph and checking if it learns meaningful representations 3. Evaluate the full CaseLink model on a small subset of the COLIEE dataset and compare its performance to a baseline model (e.g., BM25 or CaseGNN) to ensure it is learning useful representations

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in a dedicated section. However, the authors mention in the discussion that they plan to extend the edge designs to include direction and weight in future work, suggesting potential areas for further investigation.

## Limitations
- The exact hyperparameters for BM25 threshold, charge-charge similarity threshold, and hard negative sampling strategy are not specified
- The specific pre-trained weights for CaseGNN and SAILER encoders are not provided
- The inductive learning setup is novel but the exact construction of training/validation/test splits for this scenario is not detailed

## Confidence
- Graph-based connectivity modeling effectiveness: Medium-High
- Contrastive learning with degree regularization: Medium
- Case-to-case + case-to-charge complementarity: Medium

## Next Checks
1. Verify that case-to-charge edges are not overly sparse by computing the ratio of cases to unique charges and checking edge coverage
2. Test sensitivity to BM25 threshold by running experiments with multiple threshold values and observing performance changes
3. Evaluate whether the model overfits to training data by checking performance degradation on held-out test sets across multiple random seeds