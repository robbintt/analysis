---
ver: rpa2
title: 'WeatherQA: Can Multimodal Language Models Reason about Severe Weather?'
arxiv_id: '2406.11217'
source_url: https://arxiv.org/abs/2406.11217
tags:
- weather
- severe
- watch
- potential
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'WeatherQA is the first multimodal dataset for severe weather reasoning,
  containing over 8,000 (multi-image, text) pairs of ingredients-based weather parameters
  and expert-written Mesoscale Discussions. The dataset is used to evaluate state-of-the-art
  vision-language models (VLMs) on two challenging tasks: multi-choice QA for predicting
  affected areas and classification of severe weather development potential.'
---

# WeatherQA: Can Multimodal Language Models Reason about Severe Weather?
## Quick Facts
- **arXiv ID**: 2406.11217
- **Source URL**: https://arxiv.org/abs/2406.11217
- **Reference count**: 40
- **Primary result**: VLMs achieve below 40% accuracy on severe weather reasoning tasks

## Executive Summary
WeatherQA introduces the first multimodal dataset for severe weather reasoning, comprising over 8,000 pairs of weather parameters and expert-written Mesoscale Discussions. The dataset evaluates state-of-the-art vision-language models on two tasks: predicting affected areas through multi-choice questions and classifying severe weather development potential. Results show VLMs including GPT-4o, Claude3.5, Gemini-1.5, and a fine-tuned Llama3-based model perform significantly below human reasoning capabilities, with accuracy below 40% and weighted F1 scores below 0.31. Expert case studies reveal specific weaknesses in regional placement, hazard identification, and probability estimation, highlighting substantial gaps in current VLM capabilities for severe weather reasoning.

## Method Summary
The study constructs WeatherQA by curating 8,000+ pairs of weather ingredients-based parameters and Mesoscale Discussions from meteorological sources. Two evaluation tasks are defined: a multi-choice question task requiring models to identify affected regions from severe weather scenarios, and a classification task assessing the potential for severe weather development. Four state-of-the-art VLMs are tested including GPT-4o, Claude3.5, Gemini-1.5, and a fine-tuned Llama3 variant. Performance is measured using accuracy metrics for the multi-choice task and weighted F1 scores for classification. A case study with meteorologists provides qualitative assessment of VLM outputs, identifying specific reasoning failures across regional placement, hazard identification, and probability estimation.

## Key Results
- VLMs achieve below 40% accuracy on multi-choice questions for predicting affected severe weather areas
- Classification task weighted F1 scores remain below 0.31 across all tested VLMs
- Expert case studies confirm VLMs struggle with regional placement, hazard identification, and probability estimation

## Why This Works (Mechanism)
WeatherQA provides a structured framework for evaluating VLM reasoning capabilities using real-world severe weather scenarios that require complex spatial-temporal analysis and expert knowledge integration. The dataset's combination of visual weather parameters and textual Mesoscale Discussions creates a challenging multimodal reasoning task that exposes current VLM limitations in integrating heterogeneous data sources for decision-making.

## Foundational Learning
- **Multimodal reasoning**: Ability to integrate visual and textual information for decision-making; needed because severe weather analysis requires synthesizing meteorological data with expert commentary; quick check: test model on combined image-text inputs
- **Spatial-temporal analysis**: Understanding geographic distribution and temporal evolution of weather systems; critical for predicting affected areas and development potential; quick check: evaluate predictions on time-sequenced weather scenarios
- **Expert knowledge integration**: Incorporating specialized meteorological understanding into automated reasoning systems; essential for accurate severe weather assessment; quick check: compare model outputs against expert benchmarks
- **Weather parameter interpretation**: Processing ingredients-based meteorological data; fundamental for severe weather prediction tasks; quick check: validate model's understanding of basic weather variables
- **Mesoscale discussion comprehension**: Understanding expert-written weather analysis documents; provides context for severe weather events; quick check: assess model's ability to extract key information from meteorological texts
- **Hazard classification**: Categorizing weather phenomena by severity and type; necessary for risk assessment and response planning; quick check: test model's ability to distinguish between weather hazard categories

## Architecture Onboarding
Component map: WeatherQA dataset -> VLM input pipeline -> Reasoning model -> Output classifier -> Performance metrics

Critical path: Data preprocessing (images + text) → VLM inference → Classification/selection → Evaluation metrics → Expert validation

Design tradeoffs: The study uses off-the-shelf VLMs without specialized weather training, prioritizing general reasoning assessment over domain-specific optimization; case study methodology provides qualitative insights but limits quantitative scalability

Failure signatures: Regional placement errors occur when models misidentify geographic areas; hazard misidentification happens when models confuse similar weather phenomena; probability overestimation manifests as inflated confidence in watch issuance predictions

First experiments:
1. Test baseline VLMs on WeatherQA using zero-shot prompting with standard settings
2. Fine-tune Llama3 on WeatherQA training subset and evaluate on held-out test data
3. Conduct expert case study with 3-5 meteorologists reviewing model outputs for selected scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size of 8,000 pairs may not capture full diversity of severe weather scenarios, particularly rare events
- Evaluation scope limited to ingredients-based parameters and Mesoscale Discussions, potentially constraining generalizability
- Small number of meteorologists in case studies may not represent full range of human expertise

## Confidence
- **High**: VLMs perform substantially worse than human reasoning in severe weather tasks (based on clear performance metrics)
- **Medium**: Identified weaknesses in regional placement, hazard identification, and probability estimation are accurate (based on expert case studies)
- **Low**: Dataset limitations and evaluation scope do not significantly impact the core findings (requires further validation)

## Next Checks
1. Expand evaluation to include additional severe weather phenomena beyond Mesoscale Discussions, such as tornado warnings and hurricane tracking
2. Test models on out-of-distribution weather scenarios and rare extreme events not represented in the training data
3. Conduct blinded evaluation with a larger group of meteorologists to verify consistency of human assessment and identify potential biases in expert evaluation