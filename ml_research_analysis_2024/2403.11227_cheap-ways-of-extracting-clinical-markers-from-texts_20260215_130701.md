---
ver: rpa2
title: Cheap Ways of Extracting Clinical Markers from Texts
arxiv_id: '2403.11227'
source_url: https://arxiv.org/abs/2403.11227
tags:
- highlights
- task
- risk
- text
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents methods for extracting clinical markers from
  suicide-related social media posts using a combination of traditional machine learning
  and large language models (LLMs). The authors compare a simple tf-idf + logistic
  regression pipeline with LLM-based approaches for both highlighting relevant text
  spans and generating summaries.
---

# Cheap Ways of Extracting Clinical Markers from Texts

## Quick Facts
- arXiv ID: 2403.11227
- Source URL: https://arxiv.org/abs/2403.11227
- Reference count: 11
- Key outcome: Traditional ML with tf-idf + logistic regression achieves strong recall (0.921) for clinical marker extraction, with hybrid approaches combining ML and LLM methods yielding optimal performance

## Executive Summary
This paper presents cost-effective methods for extracting clinical markers from suicide-related social media posts using a combination of traditional machine learning and large language models. The authors compare a simple tf-idf + logistic regression pipeline with LLM-based approaches for both highlighting relevant text spans and generating summaries. The study finds that traditional ML approaches can achieve competitive performance to more resource-intensive LLM methods, making them particularly valuable for low-resource languages or settings. The hybrid approach of combining ML-extracted highlights with LLM-generated summaries produces the best overall results, with consistency scores of 0.974 and contradiction scores of 0.076.

## Method Summary
The paper evaluates two main approaches for clinical marker extraction: a traditional machine learning pipeline using TF-IDF vectorization with logistic regression and SHAP for feature importance, and an LLM-based approach using a quantized OpenHermes 2.5 model for highlight and summary generation. The ML approach extracts important n-grams, identifies their positions in text, and creates highlights with context windows. The LLM approach uses chain-of-thought prompting to generate highlights and summaries from important sentences. A hybrid method combines ML-extracted highlights with LLM-generated summaries, leveraging the high recall of ML for span extraction with the coherence and abstraction capabilities of LLMs.

## Key Results
- Traditional ML approach achieved strong recall scores up to 0.921 for highlight extraction using SHAP feature importance
- Combining LLM-generated abstractive summaries with ML-extracted highlights yielded the best overall performance (consistency 0.974, contradiction 0.076)
- Important sentences containing clinical markers show distinct linguistic patterns, particularly in pronoun and verb usage with statistically significant differences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Traditional machine learning with tf-idf + logistic regression can achieve strong recall in clinical marker extraction.
- Mechanism: The tf-idf vectorizer captures salient n-gram features, and logistic regression provides interpretable coefficients. SHAP values identify which features are most influential for predictions, enabling extraction of clinically relevant text spans.
- Core assumption: The dataset contains strong linguistic signals that can be captured by word n-grams and distinguished by linear classification.
- Evidence anchors:
  - [abstract] "The machine learning method, using SHAP for feature importance, achieved strong recall scores (up to 0.921) for highlight extraction."
  - [section 3] "Our results in the shared task show that a machine learning pipeline can achieve competitive evaluation scores (top 3 recall) by leveraging the risk assessment annotations from the provided dataset."
  - [corpus] Weak evidence - no direct citations found, but the FMR score of 0.317 suggests moderate relevance to the topic.
- Break condition: If the dataset contains subtle or multimodal signals not captured by word n-grams, or if the risk levels are too fine-grained for binary classification.

### Mechanism 2
- Claim: Combining LLM-generated summaries with ML-extracted highlights yields the best overall performance.
- Mechanism: LLMs can generate coherent, abstractive summaries from important sentences identified by ML, while ML provides high recall for span extraction. This hybrid approach leverages the strengths of both methods.
- Core assumption: The important sentences identified by ML contain sufficient context and clinical markers for LLMs to generate meaningful summaries.
- Evidence anchors:
  - [abstract] "Combining LLM-generated abstractive summaries with ML-extracted highlights yielded the best overall performance, with consistency scores of 0.974 and contradiction scores of 0.076."
  - [section 5] "The best results were obtained by Test + LLM2 which combines the efficacy of extracting highlights of high recall... with the ability of LLMs to generate adequate and coherent summary content."
  - [corpus] Weak evidence - no direct citations found, but the FMR score of 0.317 suggests moderate relevance to the topic.
- Break condition: If LLMs generate hallucinated content or if ML fails to identify truly important sentences, the combined approach will degrade.

### Mechanism 3
- Claim: Sentences containing important clinical markers exhibit distinct linguistic patterns, particularly in pronoun and verb usage.
- Mechanism: By analyzing the part-of-speech distributions of sentences with important features versus other sentences, we can identify statistical differences that make ML extraction effective.
- Core assumption: The linguistic characteristics of suicide-related content create measurable differences in POS tag distributions between important and non-important sentences.
- Evidence anchors:
  - [section 5] "Our brief analyses show that important sentences have different (statistically significant) linguistic patterns that can distinguish them from the rest."
  - [section 5] "Pronouns and verbs are statistically different at a p-value < 0.05 in important sentences more often than in the rest."
  - [corpus] Weak evidence - no direct citations found, but the FMR score of 0.317 suggests moderate relevance to the topic.
- Break condition: If the linguistic patterns are not consistent across different datasets or languages, or if the statistical significance is due to chance.

## Foundational Learning

- Concept: Feature importance and interpretability in machine learning
  - Why needed here: Understanding which features (n-grams) drive classification allows extraction of clinically relevant text spans.
  - Quick check question: How does SHAP value attribution work in a linear logistic regression model?

- Concept: Part-of-speech tagging and linguistic feature analysis
  - Why needed here: Analyzing POS tag distributions helps explain why certain sentences are identified as important by ML models.
  - Quick check question: What POS tags showed statistically significant differences between important and non-important sentences?

- Concept: Text summarization techniques (extractive vs. abstractive)
  - Why needed here: The paper compares different summarization approaches to generate clinical evidence from extracted highlights.
  - Quick check question: What were the key differences in performance between TextRank extractive summarization and LLM abstractive summarization?

## Architecture Onboarding

- Component map:
  - Data preprocessing: tf-idf vectorization with n-gram tokenization
  - Classification: Logistic regression with balanced class weights
  - Feature extraction: SHAP values for feature importance ranking
  - Highlight selection: Sentence alignment and context window extraction
  - Summary generation: TextRank (extractive) or LLM prompting (abstractive)
  - Evaluation: BERTScore for highlights, NLI-based consistency for summaries

- Critical path: Data preprocessing → Classification → Feature importance → Highlight extraction → Summary generation → Evaluation

- Design tradeoffs:
  - ML approach: Fast, interpretable, but may miss nuanced context
  - LLM approach: Better coherence and abstraction, but slower and more resource-intensive
  - Hybrid approach: Balances recall (ML) with quality (LLM), but requires careful integration

- Failure signatures:
  - Low recall in highlight extraction: Insufficient linguistic signals or poor feature selection
  - High contradiction scores: LLM hallucinations or misalignment between highlights and summaries
  - Poor cross-validation performance: Class imbalance or overfitting to specific data patterns

- First 3 experiments:
  1. Train logistic regression on Task A test set only, extract highlights with context windows, evaluate recall
  2. Add entire Task A data to training, compare performance changes
  3. Implement LLM-based summary generation using extracted important sentences, measure consistency and contradiction scores

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Lack of access to evaluation dataset prevents independent verification of reported performance metrics
- Limited generalizability to languages and cultural contexts beyond English Reddit data
- Insufficient detail on SHAP methodology for precise replication of feature importance analysis

## Confidence
- High confidence: The ML pipeline achieves strong recall scores (0.921) for highlight extraction based on SHAP feature importance
- Medium confidence: The hybrid approach combining ML-extracted highlights with LLM-generated summaries produces optimal consistency scores (0.974)
- Low confidence: The linguistic pattern analysis showing statistical significance in pronoun and verb usage differences between important and non-important sentences

## Next Checks
1. Replicate the ML pipeline on a publicly available suicide risk dataset to verify the recall performance and assess cross-dataset generalization
2. Conduct ablation studies comparing the hybrid approach against pure ML and pure LLM baselines on the same evaluation set
3. Perform linguistic analysis on a larger sample to validate the statistical significance of POS tag differences between important and non-important sentences