---
ver: rpa2
title: 'Relay Decoding: Concatenating Large Language Models for Machine Translation'
arxiv_id: '2405.02933'
source_url: https://arxiv.org/abs/2405.02933
tags:
- translation
- large
- language
- machine
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Relay Decoding (RD), a method that concatenates
  two large language models (LLMs), each specialized in the source and target languages,
  to achieve machine translation when no single model supports both languages. The
  approach uses a simple linear mapping layer trained on limited parallel data to
  connect the two models.
---

# Relay Decoding: Concatenating Large Language Models for Machine Translation

## Quick Facts
- arXiv ID: 2405.02933
- Source URL: https://arxiv.org/abs/2405.02933
- Reference count: 15
- Primary result: Relay Decoding achieves up to 3+ BLEU point improvements on certain language pairs (e.g., Zh-Fr: 27.36 BLEU)

## Executive Summary
This paper introduces Relay Decoding (RD), a novel approach for machine translation that concatenates two large language models (LLMs), each specialized in either the source or target language. When no single model supports both languages, RD uses a simple linear mapping layer trained on limited parallel data to bridge the two models. Experiments on Multi30k and WikiMatrix datasets demonstrate that RD outperforms traditional fine-tuning approaches, achieving significant BLEU score improvements on specific language pairs like Chinese-to-French.

## Method Summary
Relay Decoding connects two separate LLMs by training a linear mapping layer on a small amount of parallel data. The source language model generates intermediate representations, which are then transformed by the mapping layer to be compatible with the target language model. This approach allows translation between language pairs that neither model individually supports, effectively creating a "relay" mechanism for cross-lingual transfer.

## Key Results
- RD outperforms fine-tuning with a single LLM on Multi30k and WikiMatrix datasets
- Achieved up to 3+ BLEU point improvements on certain language pairs (e.g., Zh-Fr: 27.36 BLEU)
- Demonstrates effectiveness when no single model supports both source and target languages

## Why This Works (Mechanism)
The method works by leveraging the complementary strengths of two specialized LLMs through a learned transformation layer. The source model captures source language semantics, the linear mapping layer learns to align these representations with the target language space, and the target model generates fluent target language output. This architecture bypasses the need for a single multilingual model by creating an effective translation pipeline from two monolingual experts.

## Foundational Learning

1. **Linear Mapping Layers**
   - Why needed: To transform representations between different language model spaces
   - Quick check: Verify the mapping layer can project embeddings from source to target space with minimal loss

2. **BLEU Score Calculation**
   - Why needed: Standard metric for evaluating translation quality
   - Quick check: Ensure n-gram precision calculations are correctly implemented with proper brevity penalty

3. **Model Concatenation Architecture**
   - Why needed: Understanding how to chain separate models effectively
   - Quick check: Verify information flow is maintained through both models without significant degradation

4. **Cross-lingual Representation Alignment**
   - Why needed: To ensure source and target models operate in compatible semantic spaces
   - Quick check: Measure cosine similarity between mapped and target representations

5. **Fine-tuning vs. Concatenation Trade-offs**
   - Why needed: To understand when RD is preferable to traditional approaches
   - Quick check: Compare computational costs and performance across different dataset sizes

## Architecture Onboarding

**Component Map:**
Source LLM -> Linear Mapping Layer -> Target LLM -> Output

**Critical Path:**
The mapping layer is the critical component - if it fails to align representations properly, the entire pipeline breaks down. The quality of this transformation directly determines translation performance.

**Design Tradeoffs:**
- Flexibility vs. Complexity: RD allows using specialized models but adds architectural complexity
- Training Data vs. Performance: Requires parallel data only for the mapping layer, not full model training
- Computational Overhead: Two models and a mapping layer vs. single fine-tuned model

**Failure Signatures:**
- Poor BLEU scores indicating mapping layer misalignment
- Translation errors that suggest semantic drift between models
- Increased inference latency due to model chaining

**3 First Experiments:**
1. Test mapping layer with synthetic parallel data to verify basic functionality
2. Compare RD against direct fine-tuning on small parallel corpora
3. Evaluate performance degradation when using mismatched language families

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains may be dataset-dependent and not generalize across all language pairs
- Scalability concerns with larger, more diverse parallel corpora
- Computational overhead from maintaining and synchronizing two separate models
- Potential quality degradation when concatenating models across significantly different language families

## Confidence

**High Confidence:**
- The core concept of relay decoding and its basic implementation using linear mapping layers

**Medium Confidence:**
- The reported BLEU score improvements, as they are based on specific experimental setups

**Low Confidence:**
- Generalizability of results across diverse language pairs and real-world translation scenarios

## Next Checks
1. Test relay decoding across a broader range of language pairs, including low-resource and typologically diverse languages, to assess scalability and robustness.
2. Evaluate the method's performance under varying sizes of parallel training data to determine the minimum viable dataset size for effective relay decoding.
3. Conduct ablation studies to isolate the contribution of the linear mapping layer versus the inherent capabilities of the individual LLMs in the relay decoding pipeline.