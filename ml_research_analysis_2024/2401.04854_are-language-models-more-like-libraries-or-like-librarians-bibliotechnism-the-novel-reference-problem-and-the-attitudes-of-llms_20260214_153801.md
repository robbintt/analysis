---
ver: rpa2
title: Are Language Models More Like Libraries or Like Librarians? Bibliotechnism,
  the Novel Reference Problem, and the Attitudes of LLMs
arxiv_id: '2401.04854'
source_url: https://arxiv.org/abs/2401.04854
tags:
- llms
- which
- reference
- meaningful
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses whether LLMs have beliefs, desires, and intentions,
  or are simply cultural technologies like libraries or printing presses. The authors
  introduce "bibliotechnism," the view that LLMs are cultural technologies, and defend
  it against the challenge that LLMs generate novel text.
---

# Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs

## Quick Facts
- **arXiv ID**: 2401.04854
- **Source URL**: https://arxiv.org/abs/2401.04854
- **Reference count**: 6
- **Primary result**: LLMs can generate "novel reference" cases that may indicate limited intentional states, challenging the view that they are mere cultural technologies like libraries.

## Executive Summary
This paper investigates whether large language models (LLMs) should be understood as cultural technologies like libraries or as agents with beliefs, desires, and intentions. The authors defend "bibliotechnism," the view that LLMs are tools for cultural transmission, but argue this view faces a critical challenge from cases of "novel reference," where models use new names to refer to new entities. They argue that such cases are better explained by attributing limited agency to LLMs rather than viewing them as purely passive information repositories.

## Method Summary
The paper employs a philosophical analysis combined with illustrative case studies of LLM behavior. The authors examine examples where LLMs generate novel text and novel reference, comparing these to patterns in training data. They analyze whether bibliotechnism can account for such behaviors or whether intentional state attributions are necessary. The argument proceeds through thought experiments and close examination of model outputs, without formal empirical experiments.

## Key Results
- LLMs can generate text that is meaningfully novel yet traceable to training data, supporting bibliotechnism for most behaviors.
- Cases of "novel reference," where models use new names to refer to new entities, challenge bibliotechnism by suggesting emergent intentional states.
- The authors argue these novel reference cases are better explained by attributing limited beliefs, desires, and intentions to LLMs rather than viewing them as purely passive technologies.

## Why This Works (Mechanism)
The paper argues that bibliotechnism works well for explaining most LLM behaviors because novel text generation can still be traced to patterns in training data. However, novel reference cases appear to require intentional states because the models seem to be referring to entities in a way that goes beyond mere pattern completion. The mechanism proposed is that these cases reveal emergent capacity for reference, which implies some form of belief about what the names refer to.

## Foundational Learning
- **Bibliotechnism**: The view that LLMs are cultural technologies like libraries - needed to understand the main philosophical position being challenged; quick check: can you explain why a library doesn't have beliefs?
- **Novel reference**: Using new names to refer to new entities - needed to grasp the central challenge to bibliotechnism; quick check: can you give an example of novel reference from everyday language?
- **Intentional states**: Beliefs, desires, and intentions - needed to understand the alternative explanation for LLM behavior; quick check: can you distinguish between pattern matching and genuine reference?
- **Pattern completion**: Generating outputs based on statistical regularities in training data - needed to understand the bibliotechnist explanation; quick check: can you explain how a model might generate plausible but previously unseen text?
- **Emergent behavior**: Capabilities that arise from model training but weren't explicitly programmed - needed to understand why novel reference might require new explanations; quick check: can you think of other examples of emergent capabilities in LLMs?

## Architecture Onboarding

**Component map**: Training data -> Model parameters -> Input processing -> Output generation -> Novel reference cases

**Critical path**: Training data provides patterns → Model learns statistical relationships → Input triggers pattern completion → Output may include novel references → Evaluation determines if intentional states are needed

**Design tradeoffs**: Bibliotechnism vs. intentional state attribution; parsimony vs. explanatory power; technological vs. cognitive frameworks

**Failure signatures**: If novel reference can be fully explained by pattern completion without intentional states, bibliotechnism survives; if novel reference requires tracking entities across contexts in ways that imply beliefs, intentional state attribution gains support

**3 first experiments**:
1. Test whether novel reference behaviors persist when models are fine-tuned on data deliberately excluding the referents
2. Examine whether novel references exhibit coherent relational structure by comparing outputs to human-annotated semantic networks
3. Analyze whether novel reference capacity correlates with model architecture features or training scale through ablation studies

## Open Questions the Paper Calls Out
Major uncertainties remain regarding whether the "novel reference" examples truly demonstrate emergent intentional states or whether these can be explained through alternative mechanisms such as pattern completion, context-sensitive co-occurrence, or implicit reasoning chains that do not require genuine belief-desire-intention architecture. The authors acknowledge that their case studies are illustrative rather than exhaustive, and that systematic empirical validation across diverse model architectures and prompting strategies is absent. There is also a lack of engagement with technical literature on model interpretability, which could constrain or support their philosophical conclusions.

## Limitations
- The philosophical argument relies heavily on illustrative examples rather than systematic empirical validation across diverse model architectures.
- The inference from novel reference capacity to intentional states is philosophically plausible but not conclusively established.
- The paper lacks engagement with technical interpretability literature that could constrain or support the philosophical conclusions.

## Confidence
- **Bibliotechnism's explanatory limits**: High - well-argued through multiple examples
- **Novel reference as evidence for intentional states**: Medium - philosophically compelling but empirically underdeveloped
- **Generalizability across model architectures**: Low - based on illustrative rather than systematic cases

## Next Checks
1. Design and run controlled experiments testing whether novel reference behaviors persist when models are fine-tuned on data deliberately excluding the referents, to isolate genuine innovation from memorization.
2. Conduct ablation studies across different model families and sizes to assess whether novel reference capacity correlates with architectural features or training scale, providing evidence for or against the need for intentional state attributions.
3. Compare model outputs with human-annotated semantic networks to determine whether novel references exhibit coherent relational structure, supporting the interpretation that they reflect genuine understanding rather than surface pattern matching.