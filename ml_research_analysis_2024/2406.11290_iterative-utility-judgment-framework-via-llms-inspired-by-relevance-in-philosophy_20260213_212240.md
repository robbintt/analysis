---
ver: rpa2
title: Iterative Utility Judgment Framework via LLMs Inspired by Relevance in Philosophy
arxiv_id: '2406.11290'
source_url: https://arxiv.org/abs/2406.11290
tags:
- utility
- answer
- relevance
- question
- judgments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an Iterative utiliTy judgmEnt fraMework (ITEM)
  for enhancing utility judgments in Retrieval-Augmented Generation (RAG) systems,
  inspired by Schutz's philosophical system of relevances. ITEM dynamically iterates
  among topical relevance ranking, pseudo-answer generation, and utility judgments
  to improve each component's performance.
---

# Iterative Utility Judgment Framework via LLMs Inspired by Relevance in Philosophy

## Quick Facts
- arXiv ID: 2406.11290
- Source URL: https://arxiv.org/abs/2406.11290
- Reference count: 40
- Primary result: ITEM framework significantly outperforms single-shot utility judgment baselines across multiple datasets

## Executive Summary
The paper proposes ITEM (Iterative utiliTy judgmEnt fraMework), a novel approach for enhancing utility judgments in Retrieval-Augmented Generation (RAG) systems. Inspired by Schutz's philosophical system of relevances, ITEM iteratively improves three core RAG components: topical relevance ranking, pseudo-answer generation, and utility judgments. The framework operates through dynamic interaction where each component's output becomes input to the next, creating a feedback loop that progressively refines performance. Two variants are introduced: ITEM-A (with answering in the loop) and ITEM-AR (with both answering and relevance ranking in the loop). Experiments on multiple datasets demonstrate significant improvements over competitive baselines in utility judgments, ranking performance, and answer generation quality.

## Method Summary
ITEM is an iterative framework that enhances utility judgments in RAG systems through dynamic interaction between three components: topical relevance ranking, pseudo-answer generation, and utility judgments. The framework operates by using LLMs to generate pseudo-answers based on current utility judgments, then using those answers to refine subsequent utility judgments of retrieved passages. Two variants exist: ITEM-A (answering in the loop) and ITEM-AR (both answering and relevance ranking in the loop). The process iterates multiple times, with each iteration feeding back into earlier steps. The framework uses listwise input format where LLMs evaluate multiple passages together, providing broader contextual information than pointwise approaches. No training is involved - the method is applied in zero-shot scenarios using prompt engineering strategies.

## Key Results
- ITEM significantly outperforms single-shot utility judgment baselines across all tested datasets
- Listwise input format consistently outperforms pointwise format for utility judgments
- ITEM-AR variant generally performs better than ITEM-A, though results vary by dataset type
- Iterative interaction between RAG components leads to performance improvements over single-shot methods

## Why This Works (Mechanism)

### Mechanism 1
Iterative interaction between utility judgments, pseudo-answer generation, and relevance ranking improves performance over single-shot methods. Each component's output becomes input to the next, creating a feedback loop where answers refine utility judgments, which in turn improve subsequent answer quality. Core assumption: LLMs can generate meaningful pseudo-answers and relevance rankings that meaningfully guide utility judgments. Evidence: ITEM significantly outperforms competitive baselines in utility judgments, ranking of topical relevance, and answer generation.

### Mechanism 2
Listwise input format for utility judgments outperforms pointwise by providing broader contextual information. When LLMs evaluate multiple passages together, they can better assess utility based on how passages complement each other rather than in isolation. Core assumption: LLMs can effectively process and reason about multiple passages simultaneously when prompted appropriately. Evidence: The general performance of utility judgments for LLMs is better with the listwise approach than with the pointwise approach.

### Mechanism 3
The number of iterations needed depends on question type and complexity - factoid questions need fewer iterations than non-factoid questions. Simpler questions (factoid) can be resolved with fewer refinement cycles, while complex questions benefit from additional iterations to explore different aspects. Core assumption: Question complexity correlates with the number of refinement cycles needed for optimal performance. Evidence: ITEM-AR is worse than ITEM-A most times for TREC DL which contains factoid questions.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG) systems** - Why needed here: ITEM is built on RAG components, understanding how retrieval, ranking, and generation interact is fundamental. Quick check: What are the three core components of a RAG system that ITEM iteratively improves?

- **Concept: Topical relevance vs utility in information retrieval** - Why needed here: The paper distinguishes between these two measures and argues utility is more important for RAG. Quick check: How does the paper define utility differently from topical relevance?

- **Concept: Schutz's philosophical system of relevances** - Why needed here: The framework is explicitly inspired by this philosophical theory. Quick check: What are the three types of relevance in Schutz's system and how do they correspond to RAG components?

## Architecture Onboarding

- **Component map**: Question → Initial retrieval → Topical relevance ranking → Pseudo-answer generation → Utility judgments → (optional: relevance ranking) → Refined utility judgments → Final answer generation. ITEM-A includes answering in the loop; ITEM-AR includes both answering and relevance ranking.

- **Critical path**: Question → Initial retrieval results → LLMs generate pseudo-answers → LLMs evaluate utility of passages → (optional: re-rank passages) → Repeat utility evaluation → Final answer generation

- **Design tradeoffs**: More components in loop (ITEM-AR) vs fewer (ITEM-A) - more components provide more feedback but increase computational cost; more iterations improve performance but also increase latency; listwise vs pointwise input - broader context vs computational efficiency

- **Failure signatures**: Poor pseudo-answer quality degrades entire iterative process; utility judgments that don't change across iterations indicate wasted computation; LLM context window exceeded in listwise approaches; performance plateaus or degrades after additional iterations regardless of question type

- **First 3 experiments**:
  1. Compare single-shot utility judgment (baseline) vs ITEM-A with one iteration on TREC DL dataset
  2. Test whether ITEM-AR with multiple iterations improves over ITEM-A on WebAP (non-factoid) dataset
  3. Evaluate the impact of listwise vs pointwise input formats on utility judgment F1 scores using Mistral model

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of iterations (m) for ITEM-A vs ITEM-AR in different task types (factoid vs non-factoid)? The paper only provides preliminary guidance on m values but doesn't systematically explore the full parameter space. A comprehensive ablation study varying m from 1-10 for different question types and datasets would establish optimal iteration ranges.

### Open Question 2
How does ITEM performance scale when the number of candidate passages increases from 10-20 to hundreds or thousands in real-world search scenarios? All experiments used limited candidate sets, but the paper acknowledges this is unrealistic for real search scenarios. Experiments scaling candidate passage sets from 10 to 1000+ would reveal ITEM's scalability limitations.

### Open Question 3
Would fine-tuning large language models on ITEM-generated utility judgments and pseudo-answers lead to better performance than zero-shot ITEM? The paper only evaluates ITEM in zero-shot settings and acknowledges this as a limitation. Training LLMs on datasets of utility judgments and pseudo-answers generated by ITEM, then comparing their zero-shot and fine-tuned performance would quantify the benefits of training.

## Limitations

- Dependency on LLM performance for pseudo-answer generation and utility judgments creates compounding failure modes
- Significant computational resources required due to multiple LLM calls per iteration
- Optimal number of iterations varies by dataset and question type with no universal stopping criterion
- Effectiveness of listwise vs pointwise input formats may be LLM-dependent
- Generalizability to non-English datasets or domains beyond tested retrieval and QA datasets remains unverified

## Confidence

**High Confidence**: Experimental results showing ITEM outperforms single-shot baselines across all three datasets (TREC DL, WebAP, GTI-NQ) for utility judgments; comparison between listwise and pointwise input formats and their impact on LLM performance.

**Medium Confidence**: ITEM-AR generally outperforms ITEM-A, though performance difference varies by dataset type (ITEM-AR worse than ITEM-A on TREC DL's factoid questions); assertion that question complexity determines optimal iteration count shows mixed results.

**Low Confidence**: Philosophical connection to Schutz's system of relevances lacks empirical validation that three types of relevance map cleanly to three RAG components; claim that iterative interaction between RAG components universally enhances performance needs more diverse dataset validation.

## Next Checks

1. Test generalization across domains by applying ITEM to non-IR domains (e.g., medical literature, legal documents) to verify effectiveness beyond standard retrieval datasets

2. Evaluate cost-benefit tradeoff by measuring computational cost (number of LLM API calls, latency) against performance gains across different iteration counts to determine optimal resource allocation

3. Validate pseudo-answer sensitivity through ablation studies where pseudo-answers are replaced with human-written answers or other LLM-generated answers to determine how sensitive utility judgments are to pseudo-answer quality