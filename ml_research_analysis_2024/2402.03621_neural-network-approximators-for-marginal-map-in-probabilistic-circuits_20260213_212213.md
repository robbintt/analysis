---
ver: rpa2
title: Neural Network Approximators for Marginal MAP in Probabilistic Circuits
arxiv_id: '2402.03621'
source_url: https://arxiv.org/abs/2402.03621
tags:
- ssmp
- scores
- log-likelihood
- query
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of performing marginal maximum-a-posteriori
  (MMAP) inference in probabilistic circuits (PCs), a task that remains NP-hard even
  in tractable models. The authors propose a neural network-based approach that learns
  to approximate MMAP solutions in a self-supervised manner using a continuous multilinear
  loss function derived from the PC structure.
---

# Neural Network Approximators for Marginal MAP in Probabilistic Circuits

## Quick Facts
- arXiv ID: 2402.03621
- Source URL: https://arxiv.org/abs/2402.03621
- Authors: Shivvrat Arya; Tahrima Rahman; Vibhav Gogate
- Reference count: 40
- Key outcome: Neural network approximators for MMAP inference in PCs outperform baseline polytime approximations in log-likelihood and are significantly faster (microseconds vs milliseconds)

## Executive Summary
This paper introduces a neural network-based approach to approximate marginal maximum-a-posteriori (MMAP) inference in probabilistic circuits (PCs), a task that remains NP-hard even in tractable models. The method learns to approximate MMAP solutions using a continuous multilinear loss function derived from the PC structure, enabling efficient gradient-based optimization. Experiments on 22 binary datasets and three image datasets demonstrate that the proposed approach outperforms three baseline polytime approximations in terms of log-likelihood scores while being significantly faster.

## Method Summary
The proposed method trains neural networks in a self-supervised manner to approximate MMAP solutions in PCs. It constructs a query-specific PC (QPC) where leaf nodes for query variables are replaced with continuous nodes. The loss function is based on the negative log-likelihood of the QPC evaluated at the NN's continuous outputs, with an optional entropy penalty term. Gradients are computed efficiently using modified value computations in the QPC, and the NN is trained via backpropagation. The approach is evaluated against baseline methods (max-product, max-marginal, and sequential estimation) on various datasets.

## Key Results
- NN-based approximators outperform three baseline polytime approximations in log-likelihood scores, particularly as query variable ratio increases
- Inference times are significantly faster (microseconds) compared to baselines (milliseconds)
- The method improves stochastic hill climbing search when used for initialization
- Performance gains are consistent across 22 binary datasets and three image datasets (MNIST, EMNIST, CIFAR-10)

## Why This Works (Mechanism)

### Mechanism 1
The self-supervised loss function is differentiable and smooth because it is based on the logarithm of a multilinear function over continuous query variable estimates. By replacing binary leaf nodes for query variables with continuous nodes in the QPC and assigning them the NN's continuous outputs, the value computation at the root becomes a multilinear function. Taking the negative log of this function yields a smooth, differentiable loss that can be optimized via gradient descent. This relies on the PC's smooth and decomposable structure ensuring the value computation remains multilinear in the query variables.

### Mechanism 2
The gradient of the loss function can be computed in linear time with respect to the size of the PC. By modifying the leaf function for gradient computation (setting the leaf node for the query variable to 1 and its complement to -1), the partial derivative of the QPC with respect to each continuous query variable can be computed via a single bottom-up pass over the PC. The decomposability of the PC ensures that at each product node, only one child is defined over the query variable of interest, allowing efficient derivative computation.

### Mechanism 3
The entropy-based penalty term in the loss function encourages the NN to output discrete (0/1) solutions, improving the approximation quality. The entropy term, controlled by hyperparameter α, is minimized when each continuous query variable estimate is close to 0 or 1. As α increases, the loss function increasingly favors discrete solutions, tightening the approximation to the true discrete MMAP objective. This assumes that discrete solutions are optimal for the MMAP task.

## Foundational Learning

- **Concept:** Marginal inference in probabilistic circuits
  - **Why needed here:** Understanding how marginal probabilities are computed in PCs is crucial for grasping the value computation recursion and QPC construction
  - **Quick check question:** How is the probability of an assignment to a subset of variables computed in a smooth and decomposable PC?

- **Concept:** Maximum-a-posteriori (MAP) and marginal MAP (MMAP) inference
  - **Why needed here:** MMAP is the target task being approximated, and understanding its definition and complexity is essential for appreciating the problem being solved
  - **Quick check question:** What is the difference between MAP and MMAP inference tasks?

- **Concept:** Neural network training and optimization
  - **Why needed here:** The proposed method relies on training a neural network using gradient-based optimization, so understanding the basics of NN training is necessary
  - **Quick check question:** What is the role of the loss function in training a neural network?

## Architecture Onboarding

- **Component map:** Evidence assignments -> Neural Network -> Continuous query variable estimates -> Query-specific PC (QPC) -> Loss function -> Gradients -> Updated NN parameters

- **Critical path:**
  1. Given evidence e, the NN outputs continuous estimates qc for query variables
  2. The QPC is evaluated at (e, qc) to compute the loss
  3. The gradient of the loss with respect to qc is computed via modified value computations in the QPC
  4. The gradient is backpropagated through the NN to update its parameters

- **Design tradeoffs:**
  - Smoothness vs. approximation quality: Increasing the entropy penalty term α encourages more discrete solutions but may reduce the smoothness of the loss function
  - Training time vs. inference time: The NN training time depends on the size of the PC and the number of training examples, while the inference time is linear in the size of the NN

- **Failure signatures:**
  - If the NN consistently outputs values close to 0.5 for all query variables, it may indicate that the loss function is not effectively encouraging discrete solutions
  - If the training loss plateaus early, it may suggest that the NN has reached a local optimum or that the learning rate is too low

- **First 3 experiments:**
  1. Train the NN on a small PC with a known MMAP solution and verify that the NN can learn to approximate the solution
  2. Compare the inference time of the NN-based approach to the baseline methods on a medium-sized PC
  3. Evaluate the impact of the entropy penalty term α on the quality of the MMAP solutions for different query ratios

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the proposed neural network approximator be extended to handle variable-length evidence and query sets?
  - **Basis in paper:** The paper explicitly states, "we assume that the sets of evidence E = {Ei}^N_{i=1} and query Q = {Qj}^M_{j=1} variables are known a priori and do not change at both training and test time. We leave as future work the generalization of our approach that can handle variable length, arbitrarily chosen evidence, and query sets."
  - **Why unresolved:** The paper acknowledges this as a limitation and potential area for future work but does not explore solutions or provide insights into how such an extension could be achieved
  - **What evidence would resolve it:** Demonstrating the effectiveness of the method on datasets with variable-length evidence and query sets, along with an explanation of how the approach can be adapted to handle such variability, would resolve this open question

- **Open Question 2:** How does the performance of the neural network approximator compare to exact MMAP inference methods, such as branch-and-bound search or circuit transformation techniques?
  - **Basis in paper:** The paper mentions that exact methods for MMAP inference in PCs can be slow in practice and are not applicable when fast, real-time inference is desired, but does not provide a direct comparison between the proposed method and these exact methods
  - **Why unresolved:** The paper focuses on comparing the proposed method to approximate baseline methods and does not include a comparison with exact MMAP inference techniques
  - **What evidence would resolve it:** Conducting experiments that compare the accuracy and computational efficiency of the neural network approximator against exact MMAP inference methods on various datasets and PC structures would provide insights into the trade-offs between the proposed approach and exact methods

- **Open Question 3:** Can the neural network approximator be used to solve more complex queries beyond MMAP, such as constrained optimization problems or queries involving latent variables?
  - **Basis in paper:** The paper mentions in the conclusion that future work includes compiling PCs to neural networks for answering more complex queries that involve constrained optimization and developing the approach to handle queries with latent variables
  - **Why unresolved:** The current paper focuses on solving the MMAP task and does not investigate the applicability of the neural network approximator to other types of queries or more complex inference problems
  - **What evidence would resolve it:** Demonstrating the effectiveness of the neural network approximator on a range of complex queries, such as constrained optimization problems or queries involving latent variables, and comparing its performance to existing methods would address this open question

## Limitations
- Performance degradation when PC properties (smoothness and decomposability) don't hold is not thoroughly explored
- Experimental evaluation focuses primarily on log-likelihood metrics without exploring other quality measures like exact MAP solution recovery rates
- Generalization across different PC structures and query variable distributions remains untested

## Confidence
- **High confidence:** The mathematical formulation of the continuous multilinear loss function and its gradient computation properties
- **Medium confidence:** The experimental results showing improved log-likelihood scores over baselines, though sample sizes are limited
- **Low confidence:** Claims about the entropy penalty's effectiveness in encouraging discrete solutions, as this mechanism is not thoroughly validated

## Next Checks
1. **Structure sensitivity test:** Evaluate the method's performance on non-smooth or non-decomposable PCs to identify break conditions for the differentiability claims
2. **Solution quality analysis:** Compare the NN approximator's output assignments against exact MMAP solutions (where tractable) to measure approximation accuracy beyond log-likelihood
3. **Hyperparameter robustness:** Conduct systematic ablation studies on the entropy penalty α and network architecture choices to validate their impact on solution quality