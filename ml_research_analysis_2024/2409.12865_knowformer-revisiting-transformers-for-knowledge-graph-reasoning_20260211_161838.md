---
ver: rpa2
title: 'KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning'
arxiv_id: '2409.12865'
source_url: https://arxiv.org/abs/2409.12865
tags:
- graph
- knowledge
- reasoning
- knowformer
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes KNOWFORMER, a transformer-based method for
  knowledge graph reasoning that addresses limitations of path-based methods like
  missing paths and information over-squashing. The key innovation is an expressive
  and scalable attention mechanism that leverages relational message-passing neural
  networks to construct query, key, and value representations, enabling efficient
  all-pair entity interactions.
---

# KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning

## Quick Facts
- arXiv ID: 2409.12865
- Source URL: https://arxiv.org/abs/2409.12865
- Authors: Junnan Liu; Qianren Mao; Weifeng Jiang; Jianxin Li
- Reference count: 40
- State-of-the-art performance on both transductive and inductive KG reasoning benchmarks

## Executive Summary
This paper addresses limitations in knowledge graph (KG) reasoning by proposing KNOWFORMER, a transformer-based method that overcomes missing paths and information over-squashing. The key innovation is an expressive and scalable attention mechanism that leverages relational message-passing neural networks to construct query, key, and value representations. By using a kernel function approximation, KNOWFORMER achieves linear complexity while maintaining competitive performance. The method demonstrates state-of-the-art results on multiple KG reasoning benchmarks, particularly excelling at handling longer reasoning paths and scenarios with missing paths.

## Method Summary
KNOWFORMER redefines the transformer self-attention mechanism for KG reasoning by introducing structure-aware modules. The method uses a relational message-passing neural network (Q-RMPNN) to generate query representations that capture structural information through k-hop neighbor aggregation. A second RMPNN (V-RMPNN) encodes pairwise structural information into value representations. The key function is computed via a linear kernel function approximated using first-order Taylor expansion, maintaining computational efficiency. The model avoids text-based entity/relation descriptions, focusing solely on structural information while achieving competitive performance across transductive and inductive reasoning tasks.

## Key Results
- Achieves state-of-the-art performance on multiple KG reasoning benchmarks
- Demonstrates superior handling of longer reasoning paths compared to path-based methods
- Shows particular effectiveness in scenarios with missing paths where traditional path-based methods fail
- Maintains linear complexity with respect to entities and facts through kernel function approximation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The query function (Q-RMPNN) captures structural information by iteratively aggregating relational messages from k-hop neighbors.
- Mechanism: Each entity sends relational messages to neighbors based on a score function that estimates plausibility. These messages are aggregated and used to update entity representations, creating structure-aware embeddings.
- Core assumption: Local neighborhood structure contains sufficient information to distinguish query prototypes and reason about relations.
- Evidence anchors:
  - [section] "We introduce Q-RMPNN, a relational message-passing neural network... to incorporate neighbor facts into the entity representations."
  - [abstract] "Specifically, we define the attention computation based on the query prototype of knowledge graph reasoning, facilitating convenient construction and efficient optimization."
- Break condition: If the graph has very long-range dependencies that cannot be captured within k hops, or if entities lack meaningful local structure.

### Mechanism 2
- Claim: The value function (V-RMPNN) encodes pairwise structural information by incorporating head entity labeling features.
- Mechanism: V-RMPNN generates pairwise representations conditioned on the query relation by aggregating relational messages from neighbors, with explicit head entity labeling to enhance node representation for link prediction.
- Core assumption: Pairwise structural information between nodes is crucial for link prediction tasks and can be effectively captured through message passing.
- Evidence anchors:
  - [section] "V-RMPNN to encode pairwise structural information into the value. Specifically, V-RMPNN is implemented as follows..."
  - [abstract] "To incorporate structural information into the self-attention mechanism, we introduce structure-aware modules to calculate query, key, and value respectively."
- Break condition: If the graph structure is too sparse or the pairwise relationships are too complex to be captured by local aggregation.

### Mechanism 3
- Claim: The kernel function approximation maintains linear complexity while preserving expressiveness through first-order Taylor expansion.
- Mechanism: Instead of computing full exponential kernel (quadratic complexity), the method approximates it using first-order Taylor expansion and normalizes inputs by Frobenius norm, reducing complexity to linear.
- Core assumption: The first-order Taylor approximation around zero provides a stable and accurate estimation of the exponential kernel function.
- Evidence anchors:
  - [section] "To ensure non-negativity of the attention scores, we further normalize the inputs by their Frobenius norm, leading to the kernel function implementation as follows..."
  - [section] "Theorem 4.2 (Approximation Error for Exponential Kernel). For each u, v ∈ V, the approximation error ∆ = |κ(ezu,ezv) − κexp(ezu,ezv)| will be bounded by O(eγ/2), where γ ∈ (0, 1)."
- Break condition: If the approximation error becomes too large for certain entity pairs, or if the normalization step removes important signal.

## Foundational Learning

- Concept: Message Passing Neural Networks (MPNNs)
  - Why needed here: The core of KnowFormer relies on MPNNs for both query and value functions to capture structural information
  - Quick check question: What are the key differences between standard MPNNs and relational MPNNs used in this paper?

- Concept: Transformer Self-Attention Mechanism
  - Why needed here: KnowFormer redefines the self-attention mechanism to work with knowledge graph structures rather than raw text
  - Quick check question: How does the query-key-value formulation in transformers differ from traditional graph neural network aggregation?

- Concept: Kernel Methods and Function Approximation
  - Why needed here: The paper uses kernel function approximation to maintain computational efficiency while preserving expressiveness
  - Quick check question: What are the trade-offs between using exact kernel functions versus approximated versions in terms of accuracy and computational cost?

## Architecture Onboarding

- Component map:
  Input Layer: Entity features X and relation features R -> Query Function (Q-RMPNN) -> query representations -> kernel computation -> Value Function (V-RMPNN) -> value representations -> attention aggregation -> MLP with sigmoid -> output scores

- Critical path: X → Q-RMPNN → query representations → kernel computation → V-RMPNN → value representations → attention aggregation → MLP → output scores

- Design tradeoffs:
  - Accuracy vs. efficiency: Exact exponential kernel vs. first-order Taylor approximation
  - Expressiveness vs. complexity: More layers in Q-RMPNN and V-RMPNN improve performance but increase computation
  - Text-based vs. structure-based: KnowFormer avoids text descriptions, trading potential semantic richness for robustness

- Failure signatures:
  - Poor performance on datasets requiring extensive domain knowledge (text-based methods might outperform)
  - Degraded performance on graphs with very long-range dependencies beyond k-hop neighborhoods
  - Numerical instability when kernel function approximation error becomes too large

- First 3 experiments:
  1. Ablation study: Remove attention mechanism to verify it provides improvement over vanilla path-based methods
  2. Complexity analysis: Measure runtime and memory usage vs. number of entities to verify linear scaling
  3. Expressivity test: Compare performance on datasets with varying path lengths to verify handling of longer paths and missing paths

## Open Questions the Paper Calls Out
None

## Limitations
- The first-order Taylor approximation for the kernel function may accumulate errors in extremely large graphs or with complex relation patterns
- Reliance on structural information only (avoiding text-based features) could limit performance on tasks requiring rich semantic understanding
- The fixed hyperparameter k=3 may not be optimal for all datasets, potentially missing important information in graphs with different characteristics

## Confidence
- Claims about computational efficiency (linear complexity): High
- Claims about state-of-the-art performance on benchmarks: High  
- Claims about handling missing paths and longer reasoning: Medium (based on ablation studies)
- Claims about kernel function approximation accuracy: Medium (theoretical bounds provided but practical impact varies)

## Next Checks
1. **Approximation Error Analysis**: Systematically measure the approximation error of the kernel function across different graph densities and relation types to verify the O(e^(γ/2)) bound holds in practice.

2. **Text Information Integration**: Implement a hybrid version that incorporates entity/relation text descriptions and compare performance on datasets where semantic information is crucial to quantify the cost of the structure-only approach.

3. **Hyperparameter Sensitivity**: Conduct experiments varying k (hop count) and layer depth for Q-RMPNN and V-RMPNN to identify optimal configurations and understand trade-offs between expressiveness and efficiency.