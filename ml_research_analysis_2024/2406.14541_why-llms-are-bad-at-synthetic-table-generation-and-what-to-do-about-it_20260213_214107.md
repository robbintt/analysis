---
ver: rpa2
title: Why LLMs Are Bad at Synthetic Table Generation (and what to do about it)
arxiv_id: '2406.14541'
source_url: https://arxiv.org/abs/2406.14541
tags:
- data
- generation
- synthetic
- paft
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are inadequate for synthetic table
  generation due to their autoregressive nature and random feature order permutation
  during fine-tuning, which hampers modeling of functional dependencies and conditional
  distributions. To address this, the authors propose Permutation-Aided Fine-tuning
  (PAFT), which incorporates functional dependency discovery and distillation into
  the LLM fine-tuning process.
---

# Why LLMs Are Bad at Synthetic Table Generation (and what to do about it)

## Quick Facts
- arXiv ID: 2406.14541
- Source URL: https://arxiv.org/abs/2406.14541
- Authors: Shengzhe Xu, Cho-Ting Lee, Mandar Sharma, Raquib Bin Yousuf, Nikhil Muralidhar, Naren Ramakrishnan
- Reference count: 40
- Primary result: PAFT reduces synthetic data violation rates to 0-5% compared to up to 100% for baseline methods

## Executive Summary
Large language models struggle with synthetic table generation because their autoregressive nature, combined with random feature order permutation during fine-tuning, prevents proper modeling of functional dependencies and conditional distributions. This results in synthetic data that violates real-world constraints. The authors propose Permutation-Aided Fine-tuning (PAFT), which incorporates functional dependency discovery and distillation into the LLM fine-tuning process. PAFT uses a dependency graph to determine optimal feature orderings for autoregressive generation, significantly improving synthetic data quality.

## Method Summary
PAFT addresses LLM limitations in synthetic table generation by discovering functional dependencies, constructing a dependency graph, determining optimal feature orderings through condensation and topological sorting, and fine-tuning LLMs with this knowledge. The method uses FD discovery algorithms to identify relationships between attributes, builds a dependency graph with type-1 and type-2 edges, performs condensation to resolve cycles, applies topological sorting to establish generation order, and fine-tunes the LLM using LoRA with the optimized ordering. This approach ensures synthetic data respects domain constraints and conditional distributions.

## Key Results
- PAFT achieves 0-5% violation rates compared to up to 100% for baseline methods (CTGAN, CopulaGAN, TabSyn, GReaT)
- PAFT-generated data passes the "sniff test" with random forest discriminator accuracy of ~50%
- Synthetic data closely matches real data distributions and maintains downstream ML model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autoregressive LLMs generate synthetic tables in random feature order during fine-tuning, which prevents them from modeling functional dependencies correctly.
- Mechanism: During fine-tuning, LLMs are exposed to randomly permuted feature orders, which breaks the consistent dependency structure required for functional dependencies like {latitude, longitude} → state. This results in generated data that violates real-world constraints.
- Core assumption: Functional dependencies in tabular data require a specific generation order to be preserved.
- Evidence anchors:
  - [abstract] "Their autoregressive nature, combined with random order permutation during fine-tuning, hampers the modeling of functional dependencies and prevents capturing conditional mixtures of distributions essential for real-world constraints."
  - [section] "As an example, Table 1 presents a case study on using models to generate synthetic data of locations in various states of the USA. The data is of the form (state, latitude, longitude) where the attributes adhere to the FD: {latitude, longitude} → state."
- Break condition: If the dataset has no functional dependencies, random ordering would not cause violations.

### Mechanism 2
- Claim: PAFT uses a dependency graph derived from functional dependencies to determine optimal feature orderings for autoregressive generation.
- Mechanism: PAFT discovers functional dependencies using FD discovery algorithms, constructs a dependency graph with type-1 and type-2 edges representing different dependency types, then applies condensation, ordering, and expansion phases to create a total feature ordering that minimizes violated relationships.
- Core assumption: A topological sort of the dependency graph can provide an optimal generation order that preserves functional dependencies.
- Evidence anchors:
  - [section] "We leverage FD discovery techniques to govern the order of the autoregressive data generation process in PAFT. A large body of research from the database literature on FD discovery can be leveraged in PAFT."
  - [section] "We define this task of obtaining a total feature ordering from G (V, E) as an optimization step which seeks to produce k while minimizing the number of violated relationships in G (V, E)."
- Break condition: If the dependency graph contains cycles that cannot be resolved through condensation, some FDs will be violated regardless of ordering.

### Mechanism 3
- Claim: Permutation-Aided Fine-tuning (PAFT) significantly reduces violation rates and improves synthetic data quality compared to baseline methods.
- Mechanism: By incorporating functional dependency knowledge into the LLM fine-tuning process through optimal feature ordering, PAFT generates synthetic data that respects conditional distributions and domain constraints, with violation rates reduced to 0-5% compared to up to 100% for baseline methods.
- Core assumption: Incorporating domain knowledge about functional dependencies during fine-tuning improves the model's ability to generate realistic synthetic data.
- Evidence anchors:
  - [abstract] "Experiments on six real datasets show PAFT significantly outperforms existing methods (CTGAN, CopulaGAN, TabSyn, and GReaT) in generating synthetic data that respects conditional distributions and domain constraints, with violation rates reduced to 0-5% compared to up to 100% for baseline methods."
  - [section] "Our results demonstrate that PAFT is the state-of-the-art in reproducing underlying relationships in generated synthetic data."
- Break condition: If the functional dependencies discovered are incorrect or incomplete, PAFT may still generate data with violations.

## Foundational Learning

- Concept: Functional Dependencies (FDs)
  - Why needed here: FDs represent relationships where one set of attributes uniquely determines another set, which is crucial for preserving real-world constraints in synthetic data generation.
  - Quick check question: What is the FD represented by {latitude, longitude} → state?

- Concept: Autoregressive Generation
  - Why needed here: Autoregressive models generate tokens sequentially based on previous tokens, making the generation order critical for preserving dependencies.
  - Quick check question: How does the chain rule apply to autoregressive generation in the context of tabular data?

- Concept: Dependency Graph Construction
  - Why needed here: The dependency graph represents functional relationships between columns and enables optimal ordering for generation.
  - Quick check question: What are the differences between type-1 and type-2 edges in the dependency graph?

## Architecture Onboarding

- Component map:
  Data Preprocessing → Textual Encoding → FD Discovery → Dependency Graph Construction → Graph Processing → Total Feature Ordering → Fine-tuning → Generation

- Critical path:
  Data → FD Discovery → Dependency Graph → Total Ordering → Fine-tuning → Generation

- Design tradeoffs:
  - Accuracy vs. computational cost: More complex FD discovery algorithms may improve accuracy but increase computation time
  - Context window limitations: Large tables may exceed LLM context windows, requiring row-wise generation strategies
  - FD discovery quality: Incorrect FD discovery can lead to suboptimal orderings and generation errors

- Failure signatures:
  - High violation rates indicate poor FD discovery or graph processing
  - Inconsistent generation across similar categories suggests ordering issues
  - Poor downstream ML performance indicates failure to capture joint distributions

- First 3 experiments:
  1. Run PAFT on a simple dataset with known functional dependencies (like US-locations) and compare violation rates to baseline methods
  2. Test PAFT on a dataset with complex multi-attribute FDs to verify the dependency graph construction and ordering works correctly
  3. Evaluate PAFT-generated data in a downstream ML task to confirm it can substitute for real data effectively

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What other types of tabular constraints beyond functional dependencies could be incorporated into the LLM fine-tuning process to improve synthetic data generation?
- Basis in paper: [explicit] The authors state "what are other, perhaps more expressive, types of tabular constraints that can be utilized in the fine-tuning process?" as a future research direction.
- Why unresolved: The paper focuses specifically on functional dependencies (FDs) but acknowledges that other constraint types may exist and could potentially improve results.
- What evidence would resolve it: Empirical comparison of PAFT with alternative constraint types (e.g., inclusion dependencies, multi-valued dependencies, semantic constraints) across diverse datasets, showing whether alternative constraint types yield superior synthetic data quality.

### Open Question 2
- Question: What is the theoretical basis for regulating order inside transformer architectures, and can it be more directly harnessed for synthetic table generation?
- Basis in paper: [explicit] The authors ask "what is the internal basis for regulating orders inside a transformer architecture and can we more directly harness it?"
- Why unresolved: While PAFT addresses ordering through permutation-aware fine-tuning, the paper suggests there may be more fundamental ways to control order generation within the transformer architecture itself.
- What evidence would resolve it: Theoretical analysis of transformer attention mechanisms showing how order is inherently handled, followed by architectural modifications that directly control generation order, validated through improved synthetic data quality metrics.

### Open Question 3
- Question: Can we theoretically prove the (im)possibility of generating specific synthetic datasets by LLM architectures?
- Basis in paper: [explicit] The authors pose this as a future research question, asking whether theoretical proofs can establish what LLM architectures can or cannot generate.
- Why unresolved: The paper demonstrates PAFT's practical effectiveness but doesn't address fundamental limitations of what LLM-based approaches can theoretically achieve in synthetic table generation.
- What evidence would resolve it: Formal mathematical proofs establishing bounds on what distributions and constraints LLM architectures can represent, potentially using information theory or computational complexity arguments, validated against empirical generation results.

## Limitations

- The method's performance heavily depends on the quality of functional dependency discovery, with errors potentially propagating through the pipeline
- Scalability to very large tables with many columns and complex interdependencies remains untested
- The computational complexity and resource requirements for the full PAFT pipeline are not quantified

## Confidence

**High Confidence** - The core mechanism that random permutation during LLM fine-tuning breaks functional dependencies is well-supported by the literature on autoregressive models and the experimental results showing 0-5% violation rates for PAFT versus up to 100% for baselines.

**Medium Confidence** - The FD discovery and distillation processes are described conceptually but lack detailed implementation specifications. The exact parameters, algorithms used for FD discovery, and handling of ambiguous cases are not fully specified.

**Low Confidence** - The computational complexity of the full PAFT pipeline, including FD discovery, dependency graph construction, and fine-tuning, is not quantified. The scalability limits for different table sizes and the resource requirements for reproducing the results are unclear.

## Next Checks

1. **FD Discovery Validation**: Run PAFT with intentionally incorrect FD discovery (e.g., by removing or adding false dependencies) to quantify how discovery errors impact violation rates and downstream performance.

2. **Dataset Complexity Scaling**: Test PAFT on progressively larger and more complex datasets with increasing numbers of columns and dependencies to identify performance degradation points and computational bottlenecks.

3. **Weak Dependency Handling**: Evaluate PAFT's performance on datasets with probabilistic rather than strict functional dependencies to assess its robustness to real-world data where relationships may be approximate rather than deterministic.