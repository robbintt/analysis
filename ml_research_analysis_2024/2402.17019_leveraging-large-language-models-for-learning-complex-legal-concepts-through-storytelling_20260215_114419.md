---
ver: rpa2
title: Leveraging Large Language Models for Learning Complex Legal Concepts through
  Storytelling
arxiv_id: '2402.17019'
source_url: https://arxiv.org/abs/2402.17019
tags:
- legal
- question
- stories
- story
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using large language models (LLMs) to generate
  legal stories for improving non-expert understanding of complex legal concepts.
  The authors create a dataset of 294 legal doctrines with LLM-generated stories and
  multiple-choice questions.
---

# Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling

## Quick Facts
- arXiv ID: 2402.17019
- Source URL: https://arxiv.org/abs/2402.17019
- Reference count: 40
- LLM-generated stories significantly improve comprehension of legal concepts for non-native English speakers

## Executive Summary
This paper explores using large language models (LLMs) to generate legal stories that improve comprehension of complex legal concepts for non-experts. The authors created a dataset of 294 legal doctrines with LLM-generated stories and multiple-choice questions, then conducted randomized controlled trials comparing comprehension between definitions-only and definitions-plus-stories approaches. Results show that stories significantly improve comprehension and interest in law among non-native English speakers compared to definitions alone, with participants also better able to relate legal concepts to their personal lives.

## Method Summary
The authors developed an expert-in-the-loop pipeline using LLaMA 2, GPT-3.5, and GPT-4 to generate stories and questions for 102 selected legal doctrines from Wikipedia. Legal experts reviewed and refined the generated questions. They conducted a randomized controlled trial with 136 participants (65 native, 71 non-native speakers) comparing comprehension of legal concepts using only definitions versus definitions plus LLM-generated stories. Comprehension was measured through multiple-choice questions assessing immediate understanding and retention, while relevance and interest were evaluated through participant surveys.

## Key Results
- LLM-generated stories significantly improve comprehension of legal concepts among non-native English speakers
- Stories enhance participant interest in law compared to definitions-only approach
- Participants better relate legal concepts to personal experiences when using stories

## Why This Works (Mechanism)
None provided in the source material.

## Foundational Learning
- Legal concept comprehension: Understanding complex legal doctrines is challenging for non-experts, requiring simplified explanations
- LLM story generation: LLMs can transform dry legal definitions into engaging narratives that make abstract concepts more relatable
- Randomized controlled trials: RCTs provide rigorous methodology for comparing learning outcomes between different instructional approaches
- Expert-in-the-loop validation: Human legal experts can review and refine LLM outputs to ensure accuracy and relevance

## Architecture Onboarding
- Component map: Legal doctrines -> LLM story generation -> Expert review -> Multiple-choice questions -> RCT with participants
- Critical path: Story generation and expert validation are essential for producing quality learning materials
- Design tradeoffs: Balance between automated generation speed and expert review quality assurance
- Failure signatures: Poor comprehension outcomes indicate issues with story quality, question difficulty, or participant engagement
- First experiments: 1) Test story generation prompts on sample doctrines, 2) Conduct pilot with small participant group, 3) Compare comprehension scores between story types

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How would comprehension and retention outcomes differ if stories were generated solely by LLMs without human expert input or refinement?
- Basis in paper: [inferred] The paper mentions that human experts were used to critique and refine the LLM-generated questions, but does not explore the effectiveness of purely LLM-generated content.
- Why unresolved: The study used an expert-in-the-loop approach, making it unclear how effective purely LLM-generated stories would be for legal education.
- What evidence would resolve it: A controlled study comparing comprehension and retention outcomes between LLM-generated stories with and without expert refinement.

### Open Question 2
- Question: What is the optimal story length for maximizing comprehension and retention of legal concepts among non-native English speakers?
- Basis in paper: [explicit] The paper limited story length to 500 words but did not explore the impact of different story lengths on learning outcomes.
- Why unresolved: The study used a fixed story length without exploring how varying story length might affect comprehension and retention, especially for non-native speakers.
- What evidence would resolve it: A study comparing comprehension and retention outcomes across different story lengths (e.g., 250, 500, 750 words) for non-native English speakers.

### Open Question 3
- Question: How do LLM-generated stories compare to other forms of simplified explanations (e.g., "Explain Like I'm 5") in improving comprehension of legal concepts?
- Basis in paper: [inferred] The paper compares stories to Wikipedia definitions but does not explore other forms of simplified explanations.
- Why unresolved: The study only compared stories to definitions, leaving open the question of how stories perform relative to other simplified explanation formats.
- What evidence would resolve it: A controlled study comparing comprehension outcomes between LLM-generated stories, "Explain Like I'm 5" explanations, and traditional definitions for legal concepts.

## Limitations
- Small sample size (136 participants) may limit generalizability
- Focus exclusively on legal concepts from Wikipedia limits applicability to other domains
- Short-term comprehension testing doesn't establish long-term learning retention

## Confidence
- Comprehension improvement claims: High confidence (statistically significant results with expert validation)
- Generalizability to other domains: Medium confidence (limited scope of tested concepts)
- Long-term retention effects: Low confidence (no longitudinal data provided)

## Next Checks
1. Replicate the study with a larger, more diverse participant pool across different educational backgrounds and age groups to verify scalability of findings
2. Conduct longitudinal testing at 3-6 month intervals to assess long-term retention of legal concepts learned through stories versus definitions
3. Test the approach with different types of legal concepts (e.g., procedural vs substantive law) and compare effectiveness across concept categories to identify optimal use cases for story-based learning