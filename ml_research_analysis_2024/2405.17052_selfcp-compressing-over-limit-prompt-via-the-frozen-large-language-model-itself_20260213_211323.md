---
ver: rpa2
title: 'SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model
  Itself'
arxiv_id: '2405.17052'
source_url: https://arxiv.org/abs/2405.17052
tags:
- selfcp
- arxiv
- tokens
- prompt
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SelfCP compresses over-limit prompts by using the target LLM itself
  to generate dense memory tokens, avoiding the need for extra compression modules
  or training from scratch. It employs a frozen LLM as both compressor and generator,
  with a learnable connector projecting compressor outputs into LLM-readable tokens.
---

# SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself

## Quick Facts
- arXiv ID: 2405.17052
- Source URL: https://arxiv.org/abs/2405.17052
- Reference count: 4
- Compresses over-limit prompts using the target LLM itself as compressor and generator

## Executive Summary
SelfCP is a method for compressing prompts that exceed the token limit of a large language model (LLM) by leveraging the frozen LLM itself to generate dense memory tokens. Instead of training a new compression module or using extra models, SelfCP employs the target LLM as both compressor and generator, with a learnable connector to project compressed outputs into the LLM's token space. The method preserves the majority of the prompt unchanged while compressing only the over-limit segment, thus minimizing disruption to original context. Evaluated on English and Chinese benchmarks, SelfCP substitutes 12x longer prompts with compact tokens, reducing memory usage and improving throughput without degrading response quality.

## Method Summary
SelfCP compresses over-limit prompts by using the target LLM itself to generate dense memory tokens, avoiding the need for extra compression modules or training from scratch. It employs a frozen LLM as both compressor and generator, with a learnable connector projecting compressor outputs into LLM-readable tokens. Three compression strategies—Former, Latter, and Concatenated—allow selective compression of over-limit segments while preserving the rest unmodified. Evaluated on English and Chinese benchmarks, SelfCP substitutes 12× longer prompts with dense tokens, reducing memory usage and improving throughput without sacrificing response quality.

## Key Results
- Substitutes 12× longer prompts with dense memory tokens
- Reduces memory usage and improves throughput without sacrificing response quality
- Evaluated on English and Chinese benchmarks with consistent improvements

## Why This Works (Mechanism)
The core insight is that a frozen LLM can serve as both compressor and generator by using itself to produce compact, dense representations of over-limit content. This avoids extra modules and leverages the LLM's own language understanding to compress and reconstruct information. A learnable connector bridges the gap between the compressor's output space and the generator's input space, allowing the LLM to consume its own compressed tokens seamlessly.

## Foundational Learning
- **Prompt Compression**: Condensing over-limit prompts to fit model constraints—needed because many tasks require context longer than the model's max input.
  - Quick check: Verify that compressed tokens retain essential task information and allow correct responses.
- **Dense Memory Tokens**: Using compact, information-rich tokens to replace long sequences—helps reduce memory usage while preserving meaning.
  - Quick check: Ensure reconstructed prompts yield outputs similar to the original.
- **Learnable Connector**: A small projection layer mapping compressed outputs into the LLM's token space—critical for the LLM to interpret its own compressed representations.
  - Quick check: Confirm that the connector adapts to different input distributions across tasks.
- **Compression Strategies (Former/Latter/Concatenated)**: Selecting which segments of the prompt to compress—allows flexible handling of different prompt structures.
  - Quick check: Test each strategy on varied prompt lengths and structures to see which is optimal.
- **Frozen LLM as Compressor**: Using the target LLM without further training for compression—reduces computational overhead and avoids retraining.
  - Quick check: Compare compressed outputs using the frozen model vs. a fine-tuned one to see if quality is maintained.
- **LLM-Readable Tokens**: Ensuring compressed outputs can be processed by the generator LLM—ensures seamless integration into the standard inference pipeline.
  - Quick check: Validate that the generator can reconstruct and use compressed tokens without additional adaptation.

## Architecture Onboarding

**Component Map**
Compressor LLM -> Connector -> Generator LLM -> Output

**Critical Path**
Prompt (over-limit segment) -> Compressor (LLM) -> Connector (projection) -> Generator (LLM) -> Response

**Design Tradeoffs**
- Using frozen LLM for compression reduces training overhead but may limit adaptability to specialized domains.
- Three compression strategies provide flexibility but require selection heuristics for optimal performance.
- Connector is learnable but small, so it may not fully adapt to all input distributions.

**Failure Signatures**
- Loss of essential context if compression is too aggressive.
- Poor reconstruction if connector projection is suboptimal.
- Inefficiency if compression strategy choice is not task-appropriate.

**First Experiments**
1. Test each compression strategy (Former, Latter, Concatenated) on a small set of tasks to identify which performs best.
2. Evaluate memory usage and throughput gains for different compression ratios.
3. Check response quality degradation when substituting prompts with compressed tokens.

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow task evaluation (PIQA, StrategyQA, C-Eval) limits generalizability to other domains.
- Only tested on English and Chinese, so multilingual effectiveness is unclear.
- Frozen LLM compression may not adapt well to specialized or highly structured inputs.

## Confidence
- **High** in core technical contribution (LLM-based compression)
- **Medium** in efficiency claims (memory and throughput)
- **Low** in generality and robustness of compression strategies

## Next Checks
1. Evaluate SelfCP on diverse task families (e.g., long-form generation, multi-turn dialogue, code completion) to test cross-task robustness.
2. Benchmark on non-English, non-Chinese languages to confirm multilingual effectiveness.
3. Conduct ablation studies comparing the three compression strategies across all tasks to identify when each is optimal and whether a learned strategy selector improves results.