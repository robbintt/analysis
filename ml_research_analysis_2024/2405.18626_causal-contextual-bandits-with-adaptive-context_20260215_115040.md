---
ver: rpa2
title: Causal Contextual Bandits with Adaptive Context
arxiv_id: '2405.18626'
source_url: https://arxiv.org/abs/2405.18626
tags: []
core_contribution: The paper addresses causal contextual bandits with adaptive context,
  where the learner selects an initial action that determines the revealed context,
  followed by a final action that yields a reward. The authors propose ConvExplore,
  a convex-optimization-based algorithm that achieves near-optimal simple regret guarantees.
---

# Causal Contextual Bandits with Adaptive Context

## Quick Facts
- arXiv ID: 2405.18626
- Source URL: https://arxiv.org/abs/2405.18626
- Authors: Rahul Madhavan; Aurghya Maiti; Gaurav Sinha; Siddharth Barman
- Reference count: 40
- One-line primary result: ConvExplore achieves near-optimal simple regret O(√(max{λ/T, m₀/(Tp₊)} log(NT))) using convex optimization for efficient exploration in causal contextual bandits with adaptive context.

## Executive Summary
This paper addresses causal contextual bandits with adaptive context, where an initial intervention determines the revealed context before a final action yields a reward. The authors propose ConvExplore, a convex-optimization-based algorithm that achieves near-optimal simple regret guarantees through efficient exploration of interventions at both the start state and intermediate contexts. The key contribution is an instance-dependent parameter λ that characterizes the upper bound on regret, potentially making it much smaller than nk (number of interventions times number of contexts) for favorable problem structures.

## Method Summary
ConvExplore operates by first estimating transition probabilities from context 0 to intermediate contexts, then estimating causal observational thresholds at each context, and finally estimating rewards for interventions. The algorithm uses convex optimization to compute optimal frequency vectors for exploring interventions at context 0, balancing exploration across the intervention space. The method allocates time budget T among three estimation phases, using the results to compute an optimal policy that minimizes simple regret.

## Key Results
- ConvExplore achieves regret O(√(max{λ/T, m₀/(Tp₊)} log(NT))) which is essentially tight for a large class of instances
- The algorithm provides instance-dependent lower bounds showing the regret guarantee is optimal for many problem structures
- Experimental results validate theoretical findings and demonstrate superior performance compared to baseline methods like uniform exploration
- The instance-dependent parameter λ can be much smaller than nk, enabling better performance than uniform exploration in favorable cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm achieves near-optimal regret by solving a convex optimization problem that balances exploration across interventions and contexts.
- Mechanism: ConvExplore computes frequency vectors over interventions at context 0 to ensure optimal exploration, then uses these to estimate transitions, causal parameters, and rewards in three phases. The optimization problem minimizes a function involving the transition probabilities and causal observational thresholds.
- Core assumption: The optimization problem is convex and can be solved efficiently; the estimates of transitions and rewards converge quickly enough to enable low regret.
- Evidence anchors:
  - [abstract]: "The algorithm uses convex programs to efficiently explore interventions at both the start state and intermediate contexts."
  - [section 3]: "Notably, our algorithm uses a convex program to identify optimal interventions... we show (in Appendix Section E) that the optimization problem we design is convex, and is thus computationally efficient."
  - [corpus]: Weak. No direct match, but the general idea of convex optimization for exploration is consistent with bandit literature.
- Break condition: If the optimization problem is not convex or the estimates do not converge, the regret bounds fail.

### Mechanism 2
- Claim: The regret bound depends on an instance-dependent parameter λ that can be much smaller than nk, enabling better performance than uniform exploration.
- Mechanism: λ captures the exploration efficacy by considering both transition probabilities and causal observational thresholds. When these are favorable, λ can be much smaller than nk, leading to tighter regret bounds.
- Core assumption: The structure of the causal graph and the transition probabilities allow for efficient exploration, making λ small.
- Evidence anchors:
  - [abstract]: "The key contribution is an instance-dependent parameter λ that characterizes the upper bound on regret."
  - [section 1.1]: "Interestingly, the intervention complexity of our algorithm depends on an instance dependent structural parameter—referred to as λ... which may be much lower than nk, where n is the number of interventions and k is the number of contexts."
  - [corpus]: Weak. No direct match, but the idea of instance-dependent parameters is common in bandit literature.
- Break condition: If λ is large (close to nk), the algorithm performs similarly to uniform exploration.

### Mechanism 3
- Claim: The algorithm provides instance-dependent lower bounds, showing the regret guarantee is tight for a large class of instances.
- Mechanism: By constructing a family of instances where the optimal policy has a specific structure, the authors show that any algorithm must incur regret Ω(√λ/T), matching the upper bound up to log factors.
- Core assumption: The constructed instances are representative of a large class of problems where the regret lower bound applies.
- Evidence anchors:
  - [abstract]: "Furthermore, we prove that our simple regret is essentially tight for a large class of instances."
  - [section 4]: "Since ConvExplore solves an optimization problem, it is a priori unclear that a better algorithm may not provide a regret guarantee better than Theorem 1. In this section, we show that for a large class of instances, it is indeed the case that the regret guarantee we provide is optimal."
  - [corpus]: Weak. No direct match, but the idea of instance-dependent lower bounds is common in bandit literature.
- Break condition: If the constructed instances do not cover a large class of problems, the lower bound may not be tight in general.

## Foundational Learning

- Concept: Causal Bayesian Networks (CBNs) and their properties
  - Why needed here: The algorithm operates on causal graphs at each context, and understanding the structure and assumptions of CBNs is crucial for designing the exploration strategy.
  - Quick check question: What are the key assumptions made on the causal graphs at each context, and why are they important for the algorithm's design?

- Concept: Multi-armed bandits and simple regret
  - Why needed here: The problem is formulated as a contextual bandit with adaptive context, and the goal is to minimize simple regret, which is a key performance metric in bandit problems.
  - Quick check question: How does the simple regret framework differ from cumulative regret, and why is it relevant for this problem?

- Concept: Convex optimization and its applications in bandit problems
  - Why needed here: The algorithm uses convex optimization to design the exploration strategy, and understanding the properties of convex optimization is crucial for analyzing the algorithm's performance.
  - Quick check question: What are the key properties of convex optimization that make it suitable for designing exploration strategies in bandit problems?

## Architecture Onboarding

- Component map:
  - ConvExplore: Main algorithm that orchestrates the exploration and estimation phases
  - EstimateTransitionProbabilities: Subroutine that estimates the transition probabilities from context 0 to intermediate contexts
  - EstimateCausalParameters: Subroutine that estimates the causal observational thresholds at each context
  - EstimateRewards: Subroutine that estimates the rewards for interventions at each context
  - ConvexOptimization: Solves the optimization problem to compute the frequency vectors for exploration

- Critical path:
  1. Estimate transition probabilities using EstimateTransitionProbabilities
  2. Estimate causal parameters using EstimateCausalParameters
  3. Estimate rewards using EstimateRewards
  4. Solve the convex optimization problem to compute the frequency vectors
  5. Use the frequency vectors to explore and estimate the optimal policy

- Design tradeoffs:
  - Exploration vs. exploitation: The algorithm needs to balance between exploring different interventions and exploiting the current best estimate of the optimal policy
  - Time budget allocation: The algorithm allocates the time budget T among the three estimation phases, and the choice of allocation affects the performance
  - Complexity vs. performance: The convex optimization problem adds complexity to the algorithm, but it enables better performance by considering the structure of the problem

- Failure signatures:
  - High regret: If the regret is high, it could indicate that the estimates are not converging quickly enough or that the optimization problem is not being solved effectively
  - Poor convergence: If the estimates of transitions, causal parameters, or rewards are not converging, it could indicate issues with the estimation subroutines or the exploration strategy
  - Computational issues: If the convex optimization problem is not being solved efficiently, it could indicate issues with the implementation or the choice of solver

- First 3 experiments:
  1. Implement the EstimateTransitionProbabilities subroutine and test it on a simple example to ensure it is estimating the transition probabilities correctly
  2. Implement the EstimateCausalParameters subroutine and test it on a simple example to ensure it is estimating the causal observational thresholds correctly
  3. Implement the EstimateRewards subroutine and test it on a simple example to ensure it is estimating the rewards correctly
  4. Integrate the three subroutines and test the full algorithm on a simple example to ensure it is working as expected
  5. Compare the performance of ConvExplore with a baseline algorithm (e.g., uniform exploration) on a more complex example to validate the theoretical guarantees

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal computational complexity for solving the convex optimization problem in Step 4 of ConvExplore?
- Basis in paper: [explicit] The paper states that the optimization problem in Step 4 is convex and admits an efficient implementation, but does not provide a specific computational complexity bound.
- Why unresolved: The paper only mentions that the problem is convex without quantifying the exact time complexity.
- What evidence would resolve it: A detailed analysis of the computational complexity of the convex optimization problem, including time and space requirements.

### Open Question 2
- Question: How does the performance of ConvExplore compare to other state-of-the-art algorithms in causal bandits with adaptive context?
- Basis in paper: [explicit] The paper mentions that ConvExplore outperforms UnifExplore and other baselines in experiments, but does not provide a comprehensive comparison with all existing algorithms.
- Why unresolved: The experimental results only include a subset of possible baseline algorithms, leaving room for further comparison.
- What evidence would resolve it: A thorough experimental comparison of ConvExplore with a wide range of state-of-the-art algorithms in causal bandits with adaptive context.

### Open Question 3
- Question: Can the regret bounds of ConvExplore be extended to settings with non-binary rewards or more general causal graphs?
- Basis in paper: [inferred] The paper focuses on binary rewards and specific causal graph assumptions, suggesting potential for generalization.
- Why unresolved: The theoretical analysis is limited to the specific setting considered in the paper, and the extension to more general settings is not addressed.
- What evidence would resolve it: A theoretical analysis extending the regret bounds to settings with non-binary rewards or more general causal graph structures.

## Limitations

- The algorithm's performance heavily depends on the structure of the causal graph and transition probabilities, which may not be favorable in all problem instances
- The convex optimization problem's computational efficiency is asserted but not empirically validated, raising concerns about scalability
- The theoretical analysis focuses on specific problem structures, and the regret bounds may not extend to more general settings with non-binary rewards or complex causal graphs

## Confidence

- Regret bound tightness: Medium
- Convex optimization efficiency: Medium
- Instance-dependent parameter effectiveness: Medium
- Algorithm convergence: Low (no empirical validation)

## Next Checks

1. **Implementation validation**: Implement the convex optimization problem from Section 3 and verify that it solves efficiently for small-scale examples with known optimal solutions.

2. **Parameter sensitivity analysis**: Test the algorithm across a range of λ values (from small to large) to empirically validate how the regret bound scales with this instance-dependent parameter.

3. **Convergence verification**: Implement the three estimation subroutines and measure their convergence rates empirically, comparing against the theoretical probability bounds stated for the "good event" holding.