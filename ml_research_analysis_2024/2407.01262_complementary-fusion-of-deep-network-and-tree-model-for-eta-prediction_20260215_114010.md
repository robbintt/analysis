---
ver: rpa2
title: Complementary Fusion of Deep Network and Tree Model for ETA Prediction
arxiv_id: '2407.01262'
source_url: https://arxiv.org/abs/2407.01262
tags:
- yang
- features
- time
- tree
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hybrid solution for estimated time of arrival
  (ETA) prediction, combining deep learning and tree-based models. The method employs
  a convolutional neural network (CNN) to capture sequential information from road
  segments and intersections, while also using gradient boosting trees (LightGBM and
  CatBoost) with extensive feature engineering including statistical, temporal, spatial,
  and embedding features.
---

# Complementary Fusion of Deep Network and Tree Model for ETA Prediction

## Quick Facts
- arXiv ID: 2407.01262
- Source URL: https://arxiv.org/abs/2407.01262
- Reference count: 17
- Key outcome: First place in SIGSPATIAL 2021 GISCUP competition with 0.11981 on list A and 0.11974 on list B

## Executive Summary
This paper proposes a hybrid approach for ETA prediction that combines CNN-based neural networks with tree-based models through ensemble averaging. The method captures sequential road segment patterns using 1D convolutions while leveraging extensive feature engineering for gradient boosting trees. The ensemble of 10 models (8 neural networks and 2 tree models) achieves strong generalization performance, winning first place in the SIGSPATIAL 2021 GISCUP competition with close scores between validation sets.

## Method Summary
The method employs a CNN to process road segment sequences using 1D convolutions with kernel sizes 2, 3, 4, and 5, combined with global max pooling to extract sequential patterns. Tree models (LightGBM and CatBoost) use 371 handcrafted features including statistical, temporal, spatial, and embedding features. The final prediction is generated through weighted averaging of 8 neural network models (trained with different truncation methods and embedding sizes) and 2 tree models, all optimized for MAE loss.

## Key Results
- Achieved 0.11981 on list A and 0.11974 on list B in SIGSPATIAL 2021 GISCUP competition
- Close scores between validation sets indicate strong generalization ability
- Won first place in the competition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CNN-based neural network captures sequential road segment patterns effectively
- Mechanism: The CNN processes road segments as text-like sequences, applying 1D convolutions with filters of sizes 2, 3, 4, and 5 to extract local patterns. Global max pooling then reduces these to fixed-length feature vectors that represent important sequential patterns in the route.
- Core assumption: Road segments have contextual meaning similar to words in a sentence, where local patterns (like traffic lights, intersections, highway segments) create meaningful features when processed sequentially.
- Evidence anchors:
  - [abstract] "The method employs a convolutional neural network (CNN) to capture sequential information from road segments and intersections"
  - [section] "We denote the dimensionality of the link-id vectors by ð‘‘. If the length of a sequence is ð‘ , the dimension of the link sequence matrix isð‘  Ã—ð‘‘... Suppose that there is a filer parameterized by the weight matrix ð‘¤ with region sizeâ„Ž"
  - [corpus] Weak evidence - related papers focus on different attention mechanisms and transformer architectures rather than CNN-based sequential processing
- Break condition: If road segments are too heterogeneous or if the sequential ordering doesn't matter for ETA prediction, the CNN's local pattern extraction becomes less meaningful.

### Mechanism 2
- Claim: Tree-based models with extensive feature engineering complement neural networks by handling non-sequential features
- Mechanism: LightGBM and CatBoost models use 371 handcrafted features including statistical, temporal, spatial, and embedding features. These models excel at capturing complex interactions between non-sequential features that the CNN might miss.
- Core assumption: ETA prediction benefits from both sequential patterns (handled by CNN) and rich feature interactions (handled by tree models), and these two perspectives are complementary rather than redundant.
- Evidence anchors:
  - [abstract] "while also using gradient boosting trees (LightGBM and CatBoost) with extensive feature engineering including statistical, temporal, spatial, and embedding features"
  - [section] "Since the tree model cannot directly use the original path information... we summarize the features into several types: statistical features, temporal features, time features, spatial features, and neural network embedding"
  - [corpus] No direct evidence - related papers focus on single-model approaches rather than hybrid ensemble methods
- Break condition: If either the sequential features or the non-sequential features are not predictive of ETA, the ensemble approach loses its complementary advantage.

### Mechanism 3
- Claim: Weighted averaging ensemble provides robust generalization across validation sets
- Mechanism: The ensemble combines predictions from 8 neural network models (trained with different truncation methods and embedding sizes) and 2 tree models through weighted averaging, creating a final prediction that balances multiple perspectives.
- Core assumption: Different models capture different aspects of the ETA prediction problem, and their combination through weighted averaging produces more stable and accurate predictions than any single model.
- Evidence anchors:
  - [abstract] "The models are ensembled through weighted averaging... achieved a score of 0.11981 on list A and 0.11974 on list B... The close scores between validation sets indicate strong generalization ability"
  - [section] "In the end, we got a score of 0.11981 on the A list, and a score of 0.11974 on the B list, ranking first... the difference between the scores of A and B is relatively small, indicating that our program has a good generalization ability"
  - [corpus] No direct evidence - related papers focus on single-model architectures rather than ensemble approaches
- Break condition: If the individual models are highly correlated in their errors, the ensemble provides minimal benefit over the best individual model.

## Foundational Learning

- Concept: Feature engineering for tree models vs neural networks
  - Why needed here: Tree models require explicit feature extraction while neural networks can learn features from raw sequences, so understanding this difference is crucial for the hybrid approach
  - Quick check question: What types of features are typically used for tree models in ETA prediction that cannot be directly used by neural networks?

- Concept: CNN architecture for sequence processing
  - Why needed here: The paper uses 1D convolutions with different kernel sizes and global max pooling to process road segment sequences, which requires understanding of how CNNs work with sequential data
  - Quick check question: How does changing the convolution kernel size affect the patterns that the CNN can capture in road segment sequences?

- Concept: Ensemble methods and weighted averaging
  - Why needed here: The final prediction combines multiple models through weighted averaging, requiring understanding of how to optimally combine model predictions
  - Quick check question: What factors should be considered when determining the weights for ensemble averaging in ETA prediction?

## Architecture Onboarding

- Component map:
  - Data preprocessing pipeline (feature engineering for both neural and tree models) -> CNN module (1D convolutions with kernels of sizes 2, 3, 4, 5, global max pooling) -> Tree model module (LightGBM and CatBoost with 371 features) -> Ensemble module (weighted averaging of 10 model predictions) -> Loss function (MAE for both neural and tree models)

- Critical path:
  1. Data preprocessing and feature engineering
  2. CNN training on truncated road segment sequences
  3. Tree model training on handcrafted features
  4. Model ensemble and weighted averaging
  5. Validation on A/B lists

- Design tradeoffs:
  - CNN vs RNN for sequence processing: CNN provides parallel computation and easier training, while RNNs might capture longer-range dependencies better
  - Feature engineering depth: Extensive features improve tree model performance but increase complexity and risk of overfitting
  - Ensemble size: More models can improve robustness but increase computational cost and complexity

- Failure signatures:
  - High variance between A and B list scores: Indicates poor generalization or overfitting to training data
  - One model type consistently underperforming: Suggests the complementary assumption is invalid
  - Slow convergence during CNN training: May indicate suboptimal sequence length or feature representation

- First 3 experiments:
  1. Train CNN only on truncated sequences and evaluate A/B performance to establish baseline
  2. Train tree models only with handcrafted features and compare performance to CNN baseline
  3. Test different ensemble weight combinations (uniform, performance-based, cross-validation optimized) to find optimal combination strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CNN architecture's performance change when using different link sequence lengths or truncation strategies beyond the 200-link limit?
- Basis in paper: [explicit] The authors mention using 200-link truncation and training models with forward and backward truncation, but don't explore the impact of different sequence lengths.
- Why unresolved: The paper only reports results for a fixed 200-link sequence length and doesn't provide ablation studies on sequence length or truncation strategies.
- What evidence would resolve it: Comparative results showing model performance with varying sequence lengths (e.g., 100, 200, 300, 500) and different truncation strategies (forward, backward, center) would clarify the optimal sequence length and truncation approach.

### Open Question 2
- Question: How does the ensemble weighting strategy affect the final model performance compared to other ensembling methods?
- Basis in paper: [explicit] The authors use a simple weighted average ensemble but don't explore other ensembling techniques or systematically optimize the ensemble weights.
- Why unresolved: The paper only mentions using weighted averaging without exploring alternative ensemble methods (e.g., stacking, boosting) or optimizing ensemble weights based on validation performance.
- What evidence would resolve it: Comparative results showing performance differences between weighted averaging, stacking, and other ensemble methods, along with optimal weight determination strategies, would clarify the impact of ensemble design choices.

### Open Question 3
- Question: How does the model's performance generalize to different geographic regions or time periods beyond the Shenzhen dataset used?
- Basis in paper: [inferred] The authors report similar scores on A and B lists, suggesting good generalization within the competition dataset, but don't test on external datasets.
- Why unresolved: The paper only evaluates on competition data from Shenzhen in August 2020 without testing on different cities, regions, or time periods.
- What evidence would resolve it: Performance metrics on ETA prediction tasks using data from different cities, countries, or time periods (e.g., different seasons, years) would demonstrate the model's true generalization capability across geographic and temporal variations.

## Limitations
- Limited architectural details prevent full reproduction of the CNN and ensemble components
- No ablation studies presented to validate individual contributions of each model component
- Weighting scheme for ensemble averaging is not disclosed

## Confidence
- CNN sequence processing mechanism: Medium - Described but lacks implementation details and comparative analysis
- Tree model feature engineering: Medium - Extensive features claimed but individual impact not validated
- Ensemble effectiveness: Medium - Close A/B scores suggest generalization, but no baseline comparisons or ablation studies

## Next Checks
1. Conduct ablation studies removing the CNN, tree models, and ensemble components separately to quantify individual contributions
2. Compare the CNN approach against RNN/LSTM architectures for sequence processing to validate the choice of 1D convolutions
3. Test the ensemble method with different weight optimization strategies (uniform, performance-based, cross-validation) to determine if the reported weights are optimal