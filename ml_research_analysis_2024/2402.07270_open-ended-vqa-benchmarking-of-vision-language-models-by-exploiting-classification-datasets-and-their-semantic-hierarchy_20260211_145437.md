---
ver: rpa2
title: Open-ended VQA benchmarking of Vision-Language models by exploiting Classification
  datasets and their semantic hierarchy
arxiv_id: '2402.07270'
source_url: https://arxiv.org/abs/2402.07270
tags:
- question
- what
- object
- output
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of evaluating text-generative
  vision-language models in open-ended Visual Question Answering (VQA). The authors
  propose a novel VQA benchmark based on well-known visual classification datasets
  (ImageNet, COCO, ActivityNet, OV AD) to enable granular evaluation and comparison
  with discriminative models.
---

# Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy

## Quick Facts
- arXiv ID: 2402.07270
- Source URL: https://arxiv.org/abs/2402.07270
- Authors: Simon Ging; María A. Bravo; Thomas Brox
- Reference count: 39
- Primary result: Novel benchmark using classification datasets and semantic hierarchies to evaluate generative vision-language models on open-ended VQA tasks

## Executive Summary
This paper addresses the challenge of evaluating text-generative vision-language models in open-ended Visual Question Answering (VQA) tasks. The authors propose a novel benchmarking approach that leverages well-known visual classification datasets (ImageNet, COCO, ActivityNet, OV AD) and their semantic hierarchies to enable granular evaluation and fair comparison with discriminative models. The study introduces a follow-up question mechanism that improves assessment of coarse answers on fine-grained tasks, and systematically compares traditional NLP metrics with LLM-based metrics for evaluating model predictions against ground truth answers.

## Method Summary
The authors construct a VQA benchmark by adapting existing visual classification datasets and their associated semantic hierarchies. For each image in these datasets, questions are generated based on class labels and their hierarchical relationships. The benchmark employs a follow-up question mechanism that probes for more specific information when initial answers are too general, using the semantic hierarchy to guide this process. The study evaluates multiple vision-language models and compares two families of evaluation metrics: traditional NLP metrics (e.g., BLEU, ROUGE) and LLM-based metrics that assess semantic similarity. Human evaluation is used to determine the most effective metric for this specific task.

## Key Results
- The follow-up question mechanism significantly improves the fairness of evaluation on fine-grained classification tasks
- Generative vision-language models show varying performance across object, action, and attribute classification tasks
- LLM-based metrics outperform traditional NLP metrics in aligning with human judgments for this specific evaluation context
- Model performance exhibits significant imbalance depending on task granularity, with stronger performance on object classification than attribute recognition

## Why This Works (Mechanism)
The approach works by leveraging the structured knowledge inherent in classification datasets' semantic hierarchies to create meaningful evaluation scenarios. The follow-up questions exploit hierarchical relationships to probe for more specific information when needed, creating a more nuanced assessment of model capabilities. The semantic hierarchies provide a natural framework for determining when answers are sufficiently specific versus when they require refinement, enabling more granular evaluation than traditional VQA benchmarks.

## Foundational Learning
- **Semantic hierarchies**: Why needed - provide structured knowledge for generating follow-up questions; Quick check - verify hierarchy completeness for target domains
- **Classification dataset adaptation**: Why needed - existing datasets offer labeled data and semantic structure; Quick check - confirm semantic relationships align with VQA task requirements
- **Evaluation metric selection**: Why needed - different metrics capture different aspects of answer quality; Quick check - correlate metric scores with human judgments
- **Generative vs discriminative model comparison**: Why needed - enables fair benchmarking across model types; Quick check - ensure evaluation criteria apply equally to both model families
- **Multi-granularity assessment**: Why needed - captures model performance across different levels of specificity; Quick check - validate that follow-up questions appropriately probe for more detailed information

## Architecture Onboarding

**Component map**: Classification datasets -> Semantic hierarchy extraction -> Question generation -> Model prediction -> Evaluation (traditional metrics vs LLM-based metrics) -> Human evaluation for metric validation

**Critical path**: Classification dataset → Semantic hierarchy extraction → Question generation → Model prediction → Evaluation → Human validation

**Design tradeoffs**: 
- Using existing classification datasets provides structured evaluation but limits scope to predefined categories
- Follow-up questions improve granularity assessment but require complete semantic hierarchies
- LLM-based metrics offer better semantic alignment but increase computational cost

**Failure signatures**: 
- Incomplete semantic hierarchies leading to inadequate follow-up questions
- Mismatch between evaluation metrics and human judgment criteria
- Performance bias toward object classification over attribute recognition
- Limited generalizability beyond classification-structured domains

**Three first experiments**:
1. Test follow-up question effectiveness on a single classification dataset with varying hierarchy depths
2. Compare LLM-based and traditional metrics on a small subset of model predictions with human evaluation
3. Evaluate model performance across object, action, and attribute tasks to establish baseline performance patterns

## Open Questions the Paper Calls Out
Major uncertainties remain regarding the scalability of the proposed semantic hierarchy approach to other domains beyond classification datasets. The effectiveness of follow-up questions depends heavily on the quality and completeness of semantic hierarchies, which may not exist or be well-structured for all domains.

## Limitations
- Scalability concerns for applying semantic hierarchy approach to non-classification domains
- Reliance on human evaluation introduces subjectivity and limits reproducibility
- Performance heavily dependent on quality and completeness of semantic hierarchies
- Benchmark scope limited to structured classification datasets

## Confidence
- High confidence in the core claim that the proposed benchmark enables fair comparison between generative and discriminative models
- Medium confidence in the generalizability of the follow-up question mechanism across domains
- High confidence in the relative performance patterns of NLP vs LLM-based metrics for this specific evaluation context

## Next Checks
1. Apply the semantic hierarchy follow-up question mechanism to non-classification datasets (e.g., natural images with open-ended questions) to test generalizability
2. Conduct a larger-scale human evaluation study to validate the automated metric selection process across multiple domains
3. Test the benchmark with newer generative models and updated semantic hierarchies to assess temporal robustness of findings