---
ver: rpa2
title: Posterior concentrations of fully-connected Bayesian neural networks with general
  priors on the weights
arxiv_id: '2403.14225'
source_url: https://arxiv.org/abs/2403.14225
tags:
- lemma
- where
- neural
- posterior
- bnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses a significant gap in the theoretical understanding\
  \ of Bayesian neural networks (BNNs) by developing a new approximation theory for\
  \ non-sparse DNNs with bounded parameters and general priors, including Gaussian\
  \ priors commonly used in practice. The core method involves a new approximation\
  \ theorem showing that any function in the H\xF6lder class can be approximated by\
  \ a fully-connected DNN with bounded parameters using a Leaky-ReLU activation function."
---

# Posterior concentrations of fully-connected Bayesian neural networks with general priors on the weights

## Quick Facts
- arXiv ID: 2403.14225
- Source URL: https://arxiv.org/abs/2403.14225
- Authors: Insung Kong; Yongdai Kim
- Reference count: 17
- Key outcome: New approximation theory for non-sparse DNNs with bounded parameters and general priors, including Gaussian priors, achieving near-minimax optimal posterior concentration rates

## Executive Summary
This paper develops a new approximation theory for fully-connected Bayesian neural networks with bounded parameters and general priors, including Gaussian priors commonly used in practice. The authors prove that any function in the Hölder class can be approximated by a DNN with bounded weights using a Leaky-ReLU activation function. This leads to near-minimax optimal posterior concentration rates in both nonparametric Gaussian and logistic regression problems, bridging the gap between theoretical guarantees and practical applications of BNNs.

## Method Summary
The method involves developing a new approximation theorem showing that any Hölder function can be approximated by a fully-connected DNN with bounded parameters using a Leaky-ReLU activation function. Based on this, the authors prove that BNNs with general priors (including Gaussian priors) achieve near-minimax optimal posterior concentration rates. The theory extends to hierarchical compositional structures and includes adaptive methods that don't require prior knowledge of the true function's smoothness. The primary results demonstrate that the posterior distribution concentrates to the true function at a rate of $n^{-\beta/(2\beta+d)} \log^{\gamma}(n)$ for $\gamma > 2$, where $\beta$ is the smoothness of the true function.

## Key Results
- New approximation theorem enables near-minimax optimal posterior concentration rates for BNNs with bounded parameters and general priors, including Gaussian priors
- Posterior distribution concentrates to true function at rate $n^{-\beta/(2\beta+d)} \log^{\gamma}(n)$ for $\gamma > 2$, where $\beta$ is smoothness of true function
- Theory extends to hierarchical compositional structures, avoiding the curse of dimensionality, and includes adaptive methods that don't require prior knowledge of true function's smoothness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The new approximation theorem enables near-minimax optimal posterior concentration rates for BNNs with bounded parameters and general priors, including Gaussian priors.
- Mechanism: By proving that any Hölder function can be approximated by a fully-connected DNN with bounded weights using a Leaky-ReLU activation, the authors establish a prior concentration condition that holds for general priors. This bridges the gap between theory and practice by allowing the use of commonly employed priors like independent Gaussian priors.
- Core assumption: The approximation theorem holds for the Leaky-ReLU activation function and bounded weights, and the prior satisfies Assumption 1 (density lower bounded on bounded sets).
- Evidence anchors:
  - [abstract] "we present a new approximation theory for non-sparse DNNs with bounded parameters"
  - [section 3] "we develop a new approximation result using fully-connected DNNs with bounded parameters"
  - [corpus] Weak evidence; no direct citations to the specific approximation theorem, but the claim aligns with the paper's stated contributions.
- Break condition: The approximation theorem fails for activation functions other than Leaky-ReLU, or the prior does not satisfy Assumption 1.

### Mechanism 2
- Claim: The posterior distribution concentrates to the true function at a near-optimal rate, adaptively to the smoothness of the true function.
- Mechanism: By assigning a prior to the width of the DNN and carefully selecting the prior on the width (7), the authors achieve optimal posterior concentration rates without requiring knowledge of the true function's smoothness. This adaptive method avoids the curse of dimensionality by leveraging the hierarchical composition structure.
- Core assumption: The prior on the width is chosen correctly (as in (7)) and the true function follows the hierarchical composition structure H(N, P).
- Evidence anchors:
  - [section 5] "we can assign a prior to r...assign the prior (7)...the BNNs achieve the optimal posterior concentration rates adaptively to the smoothness"
  - [section 4.4] "by assuming a hierarchical composition structure...DNNs can avoid the curse of dimensionality"
  - [corpus] No direct evidence; the claim is based on the paper's theoretical results.
- Break condition: The prior on the width is not chosen correctly, or the true function does not follow the hierarchical composition structure.

### Mechanism 3
- Claim: The use of Leaky-ReLU activation function addresses the dying ReLU problem and leads to more consistent trainings of DNNs.
- Mechanism: The Leaky-ReLU activation function allows a small, non-zero gradient for negative values, which alleviates the vanishing gradient problem and improves DNN performance. This is crucial for the approximation theorem and the posterior concentration results.
- Core assumption: The Leaky-ReLU activation function is used instead of the standard ReLU function.
- Evidence anchors:
  - [section 3] "The Leaky-ReLU activation function addresses one of the main limitations of the ReLU function: the dying ReLU problem...leads to more consistent trainings of DNNs, and often results in improved performance"
  - [corpus] No direct evidence; the claim is based on the paper's theoretical results and the properties of the Leaky-ReLU activation function.
- Break condition: The Leaky-ReLU activation function is not used, or its properties do not hold in practice.

## Foundational Learning

- Concept: Hölder class and Hölder norm
  - Why needed here: The true function is assumed to belong to the β-Hölder class H^d(K), and the approximation theorem and posterior concentration results are derived based on this assumption.
  - Quick check question: What is the definition of the Hölder class H^d(K) and the Hölder norm ||f||_{H^β}?

- Concept: Deep Neural Networks (DNNs) and their architectures
  - Why needed here: The paper develops a new approximation theorem for DNNs and uses DNNs in the BNN models. Understanding the architecture, activation functions, and parameter spaces is crucial for following the proofs and results.
  - Quick check question: What is the difference between a fully-connected DNN and a sparsely connected DNN, and why is the fully-connected case more challenging?

- Concept: Bayesian inference and posterior concentration
  - Why needed here: The paper studies the posterior concentration rates of BNNs, which is a key property in Bayesian nonparametric regression. Understanding the likelihood, prior, and posterior distributions, as well as the concentration rate, is essential for grasping the main results.
  - Quick check question: What is the definition of posterior concentration rate, and why is it important in Bayesian nonparametric regression?

## Architecture Onboarding

- Component map:
  - Approximation theorem (Theorem 1) -> Prior assumptions (Assumption 1) -> Posterior concentration results (Theorems 2, 3, 4, 5)

- Critical path:
  1. Understand the approximation theorem and its proof.
  2. Verify that the prior satisfies Assumption 1.
  3. Apply the approximation theorem to establish the prior concentration condition.
  4. Use the prior concentration condition and other regularity conditions to derive the posterior concentration rates.

- Design tradeoffs:
  - Using Leaky-ReLU activation instead of ReLU: Addresses the dying ReLU problem but requires a different proof technique.
  - Allowing fully-connected DNNs instead of sparsely connected: More flexible but requires a new approximation theorem.
  - Using general priors instead of specific priors: More applicable to practice but requires a more general proof technique.

- Failure signatures:
  - The approximation theorem fails for certain activation functions or weight bounds.
  - The prior does not satisfy Assumption 1, leading to a breakdown of the prior concentration condition.
  - The true function does not belong to the Hölder class or does not follow the hierarchical composition structure.

- First 3 experiments:
  1. Verify the approximation theorem for a simple Hölder function (e.g., f(x) = |x|^β) using a fully-connected DNN with bounded parameters and Leaky-ReLU activation.
  2. Check that a Gaussian prior on the weights satisfies Assumption 1 by computing the density lower bound on a bounded set.
  3. Simulate data from a Gaussian regression model with a true function in the Hölder class, and fit a BNN with a Gaussian prior. Check the posterior concentration rate by computing the L^2 distance between the posterior mean and the true function as the sample size increases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the posterior concentration rates of BNNs with Gaussian priors compare to those with other common priors (e.g., Laplace, horseshoe) in practical settings?
- Basis in paper: [explicit] The paper demonstrates that BNNs with Gaussian priors achieve near-minimax optimal posterior concentration rates, which was previously only shown for sparse-inducing or heavy-tailed priors.
- Why unresolved: The paper provides theoretical results but does not empirically compare the performance of different priors in practice.
- What evidence would resolve it: Empirical studies comparing the predictive performance and uncertainty quantification of BNNs using different priors on benchmark datasets.

### Open Question 2
- Question: How do the approximation capabilities of Leaky-ReLU DNNs compare to those of ReLU DNNs in practice, particularly in terms of training efficiency and generalization?
- Basis in paper: [explicit] The paper extends the approximation theory to Leaky-ReLU activation functions, which are known to alleviate the dying ReLU problem.
- Why unresolved: The paper focuses on theoretical approximation results but does not empirically investigate the practical benefits of Leaky-ReLU over ReLU.
- What evidence would resolve it: Empirical studies comparing the performance of Leaky-ReLU and ReLU DNNs on various tasks, including training time and generalization error.

### Open Question 3
- Question: How can the proposed theory be extended to BNNs with more complex architectures, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs)?
- Basis in paper: [inferred] The paper focuses on fully-connected DNNs, but many practical applications use CNNs or RNNs.
- Why unresolved: The approximation theory developed in the paper is specific to fully-connected DNNs and may not directly apply to other architectures.
- What evidence would resolve it: Theoretical results extending the approximation theory to CNNs or RNNs, along with empirical studies validating the performance of BNNs with these architectures.

## Limitations
- The approximation theorem and its proof for bounded parameters with Leaky-ReLU are not fully detailed in the provided excerpt
- Practical implementation of the hierarchical composition structure (H(N,P)) and adaptive prior selection may face computational challenges
- The claim about Leaky-ReLU addressing the dying ReLU problem is based on theoretical results rather than empirical evidence

## Confidence

- High confidence: The mechanism for achieving near-optimal posterior concentration rates through the approximation theorem and general prior assumptions is well-established and theoretically sound.
- Medium confidence: The adaptive method that doesn't require prior knowledge of the true function's smoothness, while theoretically justified, may face practical implementation challenges.
- Low confidence: The claim about Leaky-ReLU addressing the dying ReLU problem and leading to more consistent training is based on the paper's theoretical results rather than empirical evidence.

## Next Checks
1. Implement a numerical verification of Theorem 1 for a simple Hölder function (e.g., f(x) = |x|^β) to confirm the approximation bounds hold in practice with bounded parameters.
2. Conduct a simulation study comparing posterior concentration rates for BNNs with Gaussian priors versus other common priors (e.g., spike-and-slab) to validate the near-minimax optimality claim.
3. Test the adaptive prior selection method on synthetic data with unknown smoothness to verify it achieves the claimed optimal rates without requiring prior knowledge of the true function's regularity.