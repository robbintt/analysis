---
ver: rpa2
title: 'Generative Dataset Distillation: Balancing Global Structure and Local Details'
arxiv_id: '2404.17732'
source_url: https://arxiv.org/abs/2404.17732
tags:
- dataset
- distillation
- matching
- local
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel dataset distillation method that balances
  global structure and local details by distilling information into a generative model
  rather than synthetic images. The method employs a conditional GAN to generate distilled
  datasets, optimizing the generator using a loss function that combines global (logit-based)
  and local (feature-based) matching between synthetic and original datasets.
---

# Generative Dataset Distillation: Balancing Global Structure and Local Details

## Quick Facts
- arXiv ID: 2404.17732
- Source URL: https://arxiv.org/abs/2404.17732
- Reference count: 40
- Primary result: GAN-based dataset distillation method achieving up to 1% accuracy improvement over state-of-the-art approaches

## Executive Summary
This paper introduces a novel dataset distillation method that uses a conditional GAN to generate distilled datasets, addressing limitations of traditional synthetic image-based approaches. By optimizing the generator through a combined global (logit-based) and local (feature-based) loss function, the method achieves improved cross-architecture generalization and eliminates retraining requirements when distillation ratios or architectures change. Experiments on MNIST, Fashion MNIST, and CIFAR-10 demonstrate consistent accuracy gains and better stability compared to existing methods, with particular strength in cross-architecture performance on models like AlexNet and VGG11.

## Method Summary
The proposed method employs a conditional GAN architecture where the generator learns to produce synthetic data that captures both global dataset structure and local feature details. The optimization process uses a hybrid loss function combining global loss (measuring differences in logit distributions between synthetic and real datasets) and local loss (measuring feature-level discrepancies). This approach distills information directly into the generative model rather than creating synthetic images, enabling efficient redeployment across different architectures and distillation ratios without retraining. The generator is trained using an alternating optimization scheme that balances these two loss components to achieve optimal information preservation.

## Key Results
- Achieves up to 1% accuracy improvement over state-of-the-art dataset distillation methods on benchmark datasets
- Demonstrates superior cross-architecture generalization, outperforming existing approaches on AlexNet and VGG11
- Shows better stability and consistent performance across different distillation ratios without requiring retraining

## Why This Works (Mechanism)
The method works by leveraging the complementary strengths of global and local information preservation. Global loss ensures the synthetic data captures overall dataset statistics and decision boundaries, while local loss preserves fine-grained feature details critical for accurate classification. By distilling this information into a generative model rather than static synthetic images, the approach maintains flexibility for redeployment across different architectures and distillation ratios. The conditional GAN architecture enables controlled generation of synthetic data that faithfully represents the original dataset's structure while being compact enough for efficient training.

## Foundational Learning

**Conditional GANs**: Why needed - Enable controlled generation of synthetic data conditioned on specific labels or features; Quick check - Verify generator can produce diverse samples for each class with correct label conditioning

**Dataset Distillation**: Why needed - Create compact representations of large datasets for efficient training; Quick check - Confirm synthetic data enables training networks that achieve reasonable accuracy on original test sets

**Global vs Local Feature Matching**: Why needed - Balance high-level dataset structure with fine-grained details; Quick check - Evaluate whether combining both losses improves performance compared to either alone

**Cross-architecture Generalization**: Why needed - Ensure distilled data works across different network architectures; Quick check - Test trained models on architectures different from the one used during distillation

## Architecture Onboarding

Component map: Original dataset -> Conditional GAN (Generator + Discriminator) -> Synthetic dataset generator -> Trained model

Critical path: Dataset → GAN training (global+local loss optimization) → Synthetic data generation → Model training → Evaluation

Design tradeoffs: GAN-based approach offers flexibility and compactness but requires more computational resources than image-based methods; balancing global/local losses is crucial but requires careful parameter tuning

Failure signatures: Poor cross-architecture performance indicates imbalance in global/local loss; instability during training suggests discriminator-generator competition issues; low accuracy suggests insufficient information preservation

First experiments:
1. Verify basic GAN training on small dataset (MNIST) with separate global and local losses
2. Test cross-architecture performance by training on one architecture and evaluating on another
3. Conduct ablation study varying the global-local loss trade-off parameter λ

## Open Questions the Paper Calls Out

**Open Question 1**: What is the optimal balance between global and local loss weights for different dataset characteristics and architectures?
- Basis in paper: The paper mentions an ablation study on CIFAR-10 with IPC=1 that found optimal local loss weight (ωl=0.001), but notes that the impact of different global loss weights (ωg) remains unexplored and worth investigating in future works
- Why unresolved: While the paper demonstrates that the global loss weight ωg=0.01 (inherited from DiM) works well, it acknowledges that the impact of varying ωg values has not been systematically studied across different datasets and architectures
- What evidence would resolve it: Systematic experiments varying ωg across multiple datasets (MNIST, Fashion MNIST, CIFAR-10, and others) and architectures (ConvNet3, ResNet variants, AlexNet, VGG) to identify optimal weight combinations for different scenarios

**Open Question 2**: How does the proposed method scale to larger, more complex datasets and real-world applications?
- Basis in paper: The experiments are limited to three benchmark datasets (MNIST, Fashion MNIST, CIFAR-10) with relatively small image sizes (32×32). The paper doesn't address scalability to larger datasets like ImageNet or domain-specific datasets with higher resolution images
- Why unresolved: The computational complexity of the proposed method, particularly the optimization of the generator through global and local loss matching, may become prohibitive for larger datasets. The paper doesn't discuss memory requirements, training time, or performance degradation when scaling up
- What evidence would resolve it: Experiments demonstrating the method's performance and computational efficiency on larger datasets (ImageNet, COCO), higher resolution images, and real-world applications like medical imaging or autonomous driving datasets

**Open Question 3**: What is the theoretical foundation for why balancing global structure and local details improves dataset distillation performance?
- Basis in paper: The paper proposes combining global (logit-based) and local (feature-based) matching without providing theoretical justification for why this combination is superior to either approach alone. The effectiveness is demonstrated empirically but not theoretically explained
- Why unresolved: The paper presents the method as a practical solution but doesn't explore the underlying principles that make balancing global and local information effective. Understanding the theoretical basis could lead to more principled design choices and better optimization strategies
- What evidence would resolve it: Mathematical analysis or theoretical proofs demonstrating how global and local information complementarity improves information density, or empirical studies showing the specific contributions of each component to the overall performance gain

## Limitations

- Computational cost not explicitly discussed, though GAN-based approaches likely require substantially more resources than image-based methods
- Limited evaluation to only three benchmark datasets with small image sizes, lacking real-world scalability evidence
- Sensitivity analysis of the global-local loss trade-off parameter λ is incomplete, with λ=0.5 appearing arbitrary

## Confidence

High: Claims about improved accuracy and stability on MNIST, Fashion MNIST, and CIFAR-10 datasets
Medium: Claims about cross-architecture generalization improvements
Medium: Claims about redeployment efficiency benefits

## Next Checks

1. Evaluate performance across a broader range of architectures including modern CNN variants, vision transformers, and lightweight mobile models to verify robustness of cross-architecture claims

2. Conduct systematic sensitivity analysis of the global-local loss trade-off parameter λ across multiple datasets to identify optimal ranges and characterize performance surfaces

3. Measure and compare computational requirements (training time, GPU memory usage) against state-of-the-art image-based distillation methods to assess practical deployment costs