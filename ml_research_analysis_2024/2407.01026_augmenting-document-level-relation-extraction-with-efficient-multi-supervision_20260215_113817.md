---
ver: rpa2
title: Augmenting Document-level Relation Extraction with Efficient Multi-Supervision
arxiv_id: '2407.01026'
source_url: https://arxiv.org/abs/2407.01026
tags:
- data
- relation
- supervision
- dataset
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the inefficiency and noise challenges in
  using distantly supervised data for document-level relation extraction. The authors
  propose Efficient Multi-Supervision (EMS), which includes two key components: Document
  Informativeness Ranking (DIR) and Multi-Supervision Ranking-based Loss (MSRL).'
---

# Augmenting Document-level Relation Extraction with Efficient Multi-Supervision

## Quick Facts
- arXiv ID: 2407.01026
- Source URL: https://arxiv.org/abs/2407.01026
- Authors: Xiangyu Lin; Weijia Jia; Zhiguo Gong
- Reference count: 15
- Primary result: EMS achieves comparable performance to state-of-the-art methods using only 3-30% of distantly supervised data with significant time efficiency gains.

## Executive Summary
This paper addresses the inefficiency and noise challenges in using distantly supervised data for document-level relation extraction. The authors propose Efficient Multi-Supervision (EMS), which includes two key components: Document Informativeness Ranking (DIR) and Multi-Supervision Ranking-based Loss (MSRL). DIR retrieves informative documents from the large distantly supervised dataset by combining distant supervision with expert predictions, while MSRL uses a ranking-based loss that integrates multiple supervision sources to reduce noise effects. Experiments on the DocRED dataset show that EMS significantly improves model performance with higher time efficiency than existing baselines.

## Method Summary
The proposed EMS approach consists of two main components. First, Document Informativeness Ranking (DIR) selects a subset of informative documents from the massive distantly supervised dataset by combining distant supervision labels with expert model predictions. Documents with higher agreement between these two sources are ranked as more informative and selected for augmentation. Second, Multi-Supervision Ranking-based Loss (MSRL) is used to train the model on the combined dataset of annotated data and selected distantly supervised documents. MSRL extends adaptive thresholding loss by dividing relation classes into agreements (consistent labels), recommendations (one source indicates relation), and others, applying different loss calculations to each group. This ranking-based approach with class weighting helps reduce the impact of noisy labels during training.

## Key Results
- EMS achieves comparable performance to state-of-the-art methods using only 3-30% of distantly supervised data
- Significant time efficiency improvements over ATLOP baseline while maintaining or improving F1 scores
- DIR effectively selects informative documents, with selected subsets showing higher agreement rates than random selection
- MSRL successfully reduces noise effects through multi-source supervision integration

## Why This Works (Mechanism)

### Mechanism 1: Document Informativeness Ranking (DIR)
- Claim: DIR improves efficiency by selecting informative documents from distant supervision (DS) data rather than using all of it.
- Mechanism: DIR combines distant supervision labels with expert predictions to score and rank documents based on their informativeness. Documents with higher scores (more reliable labels) are selected to augment training data.
- Core assumption: Documents with consistent labels between distant supervision and expert predictions contain more reliable information.
- Evidence anchors:
  - [abstract] "We first select a subset of informative documents from the massive dataset by combining distant supervision with expert supervision"
  - [section 3.2] "We describe the valid information in a DS document as the reliable labels it contains, and we define a scoring criterion to rank the documents in DS data according to their informativeness"
  - [corpus] Weak - corpus shows related work on document-level relation extraction but doesn't directly address DIR mechanism.

### Mechanism 2: Multi-Supervision Ranking-based Loss (MSRL)
- Claim: MSRL reduces noise effects by integrating multiple supervision sources in training.
- Mechanism: MSRL extends adaptive thresholding loss by dividing relation classes into agreements (consistent labels), recommendations (one source indicates relation), and others (neither source indicates relation). Different loss calculations are applied to each group, with recommendations being treated more cautiously.
- Core assumption: Relation classes with consistent labels across distant supervision and expert predictions are more reliable than those with inconsistent labels.
- Evidence anchors:
  - [abstract] "train the model with Multi-Supervision Ranking Loss that integrates the knowledge from multiple sources of supervision to alleviate the effects of noise"
  - [section 3.3] "MSRL receives two sources of labels: distant supervision and expert prediction... We hope to push agreements above threshold TH and keep others below TH"
  - [corpus] Weak - corpus shows related work but doesn't directly address MSRL mechanism.

### Mechanism 3: Self supervision with class weighting
- Claim: Self supervision dynamically adjusts learning priorities to handle noisy recommendations.
- Mechanism: MSRL uses class weights that reward recommendations confirmed by current model predictions and higher probabilities, allowing reliable recommendations to be learned while avoiding noisy ones.
- Core assumption: Recommendations with higher current model probabilities are less likely to be noisy.
- Evidence anchors:
  - [section 3.3] "we hope to carefully adjust their fitting priorities during training... the recommendations confirmed by the current predictions y of the training model and with higher probabilities P b r are less likely to be noisy"
  - [section 3.3] "we design an extra class weighting mechanism based on self supervision to mitigate noisy recommendations"
  - [corpus] Weak - corpus doesn't directly address this self supervision mechanism.

## Foundational Learning

- Concept: Distant supervision assumption - if two entities participate in a relation, any document containing those entities expresses that relation
  - Why needed here: This is the foundational assumption behind how distant supervision generates labels for document-level relation extraction
  - Quick check question: What is the main limitation of the distant supervision assumption that causes noisy labels?

- Concept: Ranking-based loss functions - losses that enforce order relationships between classes rather than absolute thresholds
  - Why needed here: MSRL builds on adaptive thresholding loss which uses ranking-based principles to separate positive and negative classes
  - Quick check question: How does a ranking-based loss differ from a standard cross-entropy loss in terms of what it optimizes?

- Concept: Multi-source supervision integration - combining labels from different sources (distant supervision, expert predictions, model predictions)
  - Why needed here: MSRL needs to understand how to integrate and weight information from multiple supervision sources
  - Quick check question: What are the three sources of supervision used in MSRL and how are they combined?

## Architecture Onboarding

- Component map:
  - Expert Model: BERT-based model trained on annotated data to provide expert predictions
  - DIR Module: Scores and ranks DS documents based on agreement between distant supervision and expert predictions
  - MSRL Loss: Ranking-based loss function with class weighting for agreements, recommendations, and others
  - Main Training Loop: Trains the model on augmented dataset using MSRL

- Critical path:
  1. Train expert model on annotated data
  2. Generate expert predictions on DS data
  3. Apply DIR to score and select top K informative documents
  4. Train main model on {annotated data + selected DS data} using MSRL
  5. Evaluate on test set

- Design tradeoffs:
  - DIR complexity vs. efficiency: More sophisticated scoring criteria could improve selection but increase computation
  - MSRL weight hyperparameters: Balancing γa and γb affects how aggressively recommendations are learned
  - Augmentation set size: Larger sets include more information but also more noise and computational cost

- Failure signatures:
  - Performance plateaus early: Expert model may be too weak or DIR not selecting informative documents
  - High variance in results: Self supervision may be unstable with small batch sizes
  - Degraded performance vs. baseline: MSRL implementation may have bugs in class separation or weighting

- First 3 experiments:
  1. Verify DIR selects documents with higher agreement rates than random selection
  2. Test MSRL with only distant supervision (no expert supervision) to measure noise impact
  3. Compare MSRL performance with different augmentation set sizes (3%, 10%, 30%) to find optimal tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of EMS scale with increasing sizes of distantly supervised datasets beyond DocRED?
- Basis in paper: [inferred] The paper discusses efficiency gains but doesn't explore performance scaling with larger DS datasets.
- Why unresolved: The experiments only test on DocRED's current DS dataset size, leaving uncertainty about performance with larger or growing datasets.
- What evidence would resolve it: Experiments testing EMS on progressively larger DS datasets, showing performance and efficiency trends.

### Open Question 2
- Question: How sensitive is EMS to the quality and capability of the expert model used for distant supervision?
- Basis in paper: [explicit] The paper acknowledges that EMS depends on an expert model's capability for effective supervision.
- Why unresolved: The paper doesn't systematically test EMS with different expert model qualities or architectures to quantify performance impact.
- What evidence would resolve it: Controlled experiments varying expert model quality/capability while measuring EMS performance and robustness.

### Open Question 3
- Question: Can the DIR informativeness ranking mechanism be improved by incorporating additional features beyond DS labels and expert predictions?
- Basis in paper: [explicit] The current DIR uses only DS labels and expert predictions to rank documents.
- Why unresolved: The paper doesn't explore alternative or additional ranking features that might improve informativeness detection.
- What evidence would resolve it: Experiments testing DIR with alternative ranking features (e.g., document length, entity density, topic diversity) and comparing performance gains.

## Limitations
- DIR implementation details unclear: The exact formulation of the informativeness scoring function is not fully specified, using only a "scikit-learn class weight function" reference.
- MSRL hyperparameter ambiguity: Specific threshold values and normalization details for class weights wa and wb are not precisely defined.
- Time efficiency measurement limitations: Evaluation focuses on relative time efficiency without absolute timing measurements or hardware specifications, making real-world applicability difficult to assess.

## Confidence

- High confidence in the core DIR mechanism: The document selection approach based on agreement between distant supervision and expert predictions is well-motivated and theoretically sound.
- Medium confidence in MSRL effectiveness: While the ranking-based loss framework is established, the specific implementation details and hyperparameter choices may significantly impact performance.
- Low confidence in time efficiency claims: Without absolute timing measurements and hardware specifications, the claimed efficiency gains cannot be independently verified.

## Next Checks

1. **DIR effectiveness validation**: Measure agreement rates between distant supervision and expert predictions on the selected document subsets to verify DIR is actually selecting more informative documents.
2. **MSRL parameter sensitivity**: Conduct ablation studies varying the threshold values and class weights (wa, wb) to understand their impact on final performance and identify optimal settings.
3. **Cross-dataset generalization**: Test the EMS approach on additional document-level relation extraction datasets beyond DocRED to assess its broader applicability and robustness to different data distributions.