---
ver: rpa2
title: Vision Language Models Know Law of Conservation without Understanding More-or-Less
arxiv_id: '2410.00332'
source_url: https://arxiv.org/abs/2410.00332
tags:
- trapezoids
- number
- conservation
- upper
- same
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLMs generally perform well on transformational conservation tasks
  but struggle with non-transformational tasks assessing quantitative understanding.
  Specifically, VLMs can recognize that physical quantities remain constant despite
  spatial transformations but fail to correctly assess relative quantities when no
  transformation occurs.
---

# Vision Language Models Know Law of Conservation without Understanding More-or-Less

## Quick Facts
- arXiv ID: 2410.00332
- Source URL: https://arxiv.org/abs/2410.00332
- Reference count: 6
- VLMs generally perform well on transformational conservation tasks but struggle with non-transformational tasks assessing quantitative understanding.

## Executive Summary
This study evaluates whether Vision Language Models (VLMs) possess conservation and quantitative understanding by testing them on cognitive experiments from ConserveBench. The results reveal a striking dissociation: VLMs successfully understand conservation principles in transformational scenarios (where objects change location or shape but retain quantity) but fail at direct quantity comparison tasks without transformation. This suggests VLMs leverage spatial-temporal transformation cues rather than genuine numerical reasoning. Additionally, VLMs exhibit counterintuitive error patterns opposite to human cognitive fallacies, consistently choosing denser arrangements as having more items rather than relying on linear extent.

## Method Summary
The study evaluates 60 VLMs (open-source and closed-source, with varying image reasoning capabilities) on zero-shot generation tasks using the ConserveBench dataset containing 365 cognitive experiments across volume, solid quantity, length, and number dimensions. VLMs are tested on both full conservation tasks and extracted quantitative understanding tasks from the same scenarios, using standardized prompts. Performance is measured separately for transformational conservation tasks versus non-transformational quantity understanding tasks to assess dissociation between these abilities.

## Key Results
- VLMs perform well on transformational conservation tasks but struggle with non-transformational quantity understanding tasks
- VLMs exhibit a "dense-equals-more" strategy opposite to human "length-equals-number" fallacy
- This suggests a dissociation between understanding reversibility and quantity concepts in VLMs

## Why This Works (Mechanism)

### Mechanism 1
VLMs understand reversibility of operations but not quantitative invariance without transformation. VLMs leverage spatial-temporal transformation cues in prompts to infer conservation principles, bypassing direct numerical reasoning. When transformations are absent, VLMs default to visual heuristics that contradict human biases. Core assumption: Conservation reasoning in VLMs is triggered by the explicit mention of a "process" or transformation, activating spatial-temporal reasoning rather than quantitative comparison. Evidence anchors: [abstract] "VLMs generally perform well on transformational tasks but struggle with non-transformational tasks assessing quantitative understanding." [section 3] "In stark contrast, in quantity understanding tasks for number and length dimensions, VLMs perform significantly poorer in general, exhibiting consistent errors comparable to pre-operational children with extremely limited understanding of quantity." Break condition: If VLMs can solve non-transformational tasks without transformation cues, this mechanism fails.

### Mechanism 2
VLMs employ an opposite heuristic to humans for quantity comparison - "dense-equals-more" rather than "length-equals-number". VLMs interpret visual density patterns as quantity indicators, leading to systematic errors opposite to human "length-equals-number" fallacy. This suggests fundamentally different visual reasoning strategies. Core assumption: Visual processing in VLMs prioritizes spatial density over linear extent when assessing quantity, contrary to human visual cognition. Evidence anchors: [abstract] "VLMs exhibit a counterintuitive error pattern: when comparing number/length, they incorrectly choose the more densely packed line as having more items, opposite to the human 'length-equals-number' fallacy." [section 4] "VLMs consistently employ a misleading strategy of number conservation that is entirely opposite to human intuition...VLMs' failure to achieve a rudimentary understanding of quantity seems to be supplemented by the exploitation of a dense-equals-more strategy." Break condition: If VLMs show no consistent bias toward density in quantity comparisons, this mechanism fails.

### Mechanism 3
Conservation tasks implicitly activate abstract reasoning pathways in VLMs that are absent in direct quantity comparison. The narrative structure of conservation tasks (initial state → transformation → final state) provides a scaffolding that triggers abstract reasoning about invariance, while static comparisons lack this structure. Core assumption: VLMs process narrative sequences differently from static images, with the former enabling higher-order reasoning about invariance principles. Evidence anchors: [section 3] "Given the scenario of a transformational process overrides their fallacy in quantity understanding, given by direct comparison between full conservation tasks and quantity understanding tasks extracted from the corresponding scenarios." [section 4] "This suggests a possible dissociation between knowing the law of conservation and a rudimentary understanding of quantity at the corresponding dimensions." Break condition: If VLMs perform equally on static and narrative-based quantity comparisons, this mechanism fails.

## Foundational Learning

- Concept: Conservation of quantity in physical systems
  - Why needed here: Understanding the theoretical framework of conservation laws is essential for interpreting VLM performance and designing appropriate test scenarios
  - Quick check question: Can you explain why the volume of liquid remains constant when poured from a tall glass to a short, wide glass?

- Concept: Piagetian cognitive development stages
  - Why needed here: The paper frames VLM performance in terms of developmental psychology, requiring understanding of pre-operational vs. concrete operational stages
  - Quick check question: What distinguishes pre-operational children's understanding of quantity from concrete operational children's understanding?

- Concept: Vision-Language Model architecture fundamentals
  - Why needed here: Understanding how VLMs process visual and textual information together is crucial for interpreting their performance on conservation tasks
  - Quick check question: How do VLMs typically integrate visual features with language representations?

## Architecture Onboarding

- Component map: Vision encoder → Cross-modal fusion → Language model → Output decoder
- Critical path: Image preprocessing → Vision feature extraction → Prompt construction → Cross-modal reasoning → Answer generation
- Design tradeoffs: Multi-image reasoning capability vs. computational efficiency
- Failure signatures: Systematic errors on non-transformational tasks, opposite bias to human heuristics, inability to count accurately in visually complex arrangements
- First 3 experiments:
  1. Test VLM performance on conservation tasks with and without explicit transformation cues in prompts
  2. Evaluate VLM responses to static quantity comparisons vs. transformation-based comparisons using identical visual content
  3. Assess VLM sensitivity to visual density vs. linear extent in quantity judgment tasks

## Open Questions the Paper Calls Out

### Open Question 1
What specific cognitive mechanism in VLMs allows them to understand reversibility in transformational tasks but fail at non-transformational quantitative understanding tasks? Basis in paper: [explicit] The paper demonstrates that VLMs perform well on transformational conservation tasks but fail at non-transformational tasks assessing quantity understanding, suggesting a dissociation between these abilities. Why unresolved: The paper identifies the phenomenon but does not explain the underlying mechanism causing this dissociation. What evidence would resolve it: Neuroimaging studies or cognitive experiments comparing VLM processing patterns during transformational versus non-transformational tasks could reveal the specific cognitive mechanisms involved.

### Open Question 2
Why do VLMs employ a "dense-equals-more" strategy that is opposite to the human "length-equals-number" fallacy in quantitative understanding tasks? Basis in paper: [explicit] The paper notes that VLMs consistently choose the more densely packed line as having more items, opposite to the human length-equals-number fallacy. Why unresolved: The paper observes this counterintuitive error pattern but does not explain why VLMs develop this opposite strategy. What evidence would resolve it: Comparative studies examining how VLMs process visual density information versus human cognitive processing of the same stimuli could reveal the source of this divergence.

### Open Question 3
How does the provision of transformational context override VLMs' quantitative fallacies in ways that differ from human cognitive development? Basis in paper: [explicit] The paper observes that VLMs can successfully infer reversibility from transformational contexts while simultaneously inhibiting misleading quantity strategies, a phenomenon not documented in human developmental literature. Why unresolved: The paper highlights this as a significant difference between human and VLM reasoning but does not explain the underlying cognitive architecture enabling this override. What evidence would resolve it: Experiments testing whether VLMs can maintain quantitative accuracy without transformational context when explicitly prompted about reversibility could clarify whether the transformational context is essential for their performance.

## Limitations
- The mechanistic explanations for the dissociation between transformational and non-transformational task performance remain speculative
- The paper lacks controlled experiments that directly test the proposed mechanisms by manipulating variables like visual density and transformation cues
- Exact prompt templates and specific VLM version numbers are not provided, making faithful reproduction challenging

## Confidence
- Core finding (VLMs perform well on transformational tasks but fail on non-transformational tasks): High
- "Dense-equals-more" heuristic explanation: Medium
- Claims about fundamental differences in VLM vs. human numerical cognition: Medium

## Next Checks
1. Design experiments that systematically vary the presence and type of transformation cues in conservation task prompts while keeping visual content constant
2. Create controlled stimuli where visual density and linear extent are systematically varied and decoupled, then measure VLM sensitivity to each dimension
3. Compare performance patterns across VLMs with different architectural designs and training objectives to identify whether the conservation/quantity dissociation is universal or architecture-specific