---
ver: rpa2
title: On Distilling the Displacement Knowledge for Few-Shot Class-Incremental Learning
arxiv_id: '2412.11017'
source_url: https://arxiv.org/abs/2412.11017
tags:
- knowledge
- learning
- distillation
- classes
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in Few-Shot Class-Incremental
  Learning (FSCIL) by proposing a Displacement Knowledge Distillation (DKD) method.
  Unlike traditional knowledge distillation that measures sample similarity, DKD computes
  displacement vectors between samples to capture both distance and angular information,
  preserving more structural information.
---

# On Distilling the Displacement Knowledge for Few-Shot Class-Incremental Learning

## Quick Facts
- arXiv ID: 2412.11017
- Source URL: https://arxiv.org/abs/2412.11017
- Authors: Pengfei Fang; Yongchun Qin; Hui Xue
- Reference count: 40
- Primary result: Proposes Displacement Knowledge Distillation (DKD) for FSCIL, achieving 2.62% average accuracy improvement over state-of-the-art methods

## Executive Summary
This paper addresses catastrophic forgetting in Few-Shot Class-Incremental Learning (FSCIL) by proposing a Displacement Knowledge Distillation (DKD) method. Unlike traditional knowledge distillation that measures sample similarity, DKD computes displacement vectors between samples to capture both distance and angular information, preserving more structural information. The authors also introduce a Dual Distillation Network (DDNet) framework that applies Individual Knowledge Distillation (IKD) to base classes and DKD to novel classes, addressing the performance gap between them. An instance-aware sample selector dynamically adjusts branch weights during inference. Extensive experiments on CIFAR-100, miniImageNet, and CUB-200 demonstrate that DDNet achieves state-of-the-art performance, with an average accuracy gain of 2.62% and improved robustness against outliers compared to existing methods.

## Method Summary
The proposed approach addresses the catastrophic forgetting problem in FSCIL through two key innovations. First, Displacement Knowledge Distillation (DKD) computes displacement vectors between samples rather than using traditional similarity measures. This captures both distance and angular information, preserving more structural relationships in the feature space. Second, the Dual Distillation Network (DDNet) framework employs Individual Knowledge Distillation (IKD) for base classes and DKD for novel classes, creating a hybrid approach that addresses the performance gap between these two class types. An instance-aware sample selector dynamically adjusts the weighting between branches during inference based on the specific input sample characteristics.

## Key Results
- DDNet achieves state-of-the-art performance on CIFAR-100, miniImageNet, and CUB-200 benchmarks
- Average accuracy gain of 2.62% compared to existing FSCIL methods
- Improved robustness against outliers through displacement vector-based knowledge representation
- Effective mitigation of catastrophic forgetting across incremental learning sessions

## Why This Works (Mechanism)
The effectiveness of DKD stems from its ability to capture richer structural information than traditional knowledge distillation methods. By computing displacement vectors between samples, the method preserves both distance relationships and angular orientations in the feature space, which are critical for maintaining class boundaries during incremental learning. The dual-branch architecture of DDNet allows for specialized treatment of base and novel classes, with IKD maintaining stability in well-established feature representations while DKD adapts to new class structures. The instance-aware sample selector provides adaptive weighting that optimizes the balance between these two distillation approaches based on the specific characteristics of each input sample.

## Foundational Learning
1. Few-Shot Class-Incremental Learning (FSCIL) - Why needed: Addresses scenarios where new classes are introduced incrementally with limited training samples per class, requiring the model to learn continuously without forgetting previous knowledge. Quick check: Verify the model can learn new classes from 1-5 examples while maintaining performance on previously learned classes.

2. Knowledge Distillation (KD) - Why needed: Enables transfer of knowledge from a larger model or previous model version to a smaller or updated model, crucial for maintaining performance during incremental learning. Quick check: Compare performance with and without distillation to quantify knowledge transfer benefits.

3. Catastrophic Forgetting - Why needed: The primary challenge in incremental learning where models lose the ability to perform well on previously learned tasks when adapting to new tasks. Quick check: Measure performance degradation on base classes after learning new classes.

4. Displacement Vectors - Why needed: Provide a geometric representation of feature relationships that captures both distance and angular information, offering richer structural information than traditional similarity metrics. Quick check: Analyze whether displacement vectors preserve class separability better than direct feature similarity.

5. Dual Distillation Architecture - Why needed: Allows specialized treatment of different class types (base vs. novel) by applying appropriate distillation strategies to each, optimizing overall performance. Quick check: Verify that each branch contributes appropriately to final performance through ablation studies.

## Architecture Onboarding

Component Map: Input -> Feature Extractor -> Dual Branches (IKD for Base, DKD for Novel) -> Instance-Aware Selector -> Output

Critical Path: Input features flow through the shared feature extractor into two parallel distillation branches, with the instance-aware selector dynamically combining their outputs based on sample-specific weighting.

Design Tradeoffs: The dual-branch architecture increases model complexity and computational cost but provides specialized optimization for different class types. The displacement vector computation adds geometric processing overhead but captures richer structural information. The instance-aware selector introduces adaptive behavior but requires additional hyperparameter tuning.

Failure Signatures: Poor performance on base classes indicates insufficient IKD strength; degraded novel class performance suggests DKD is not capturing new class structures effectively; unstable results across runs may indicate improper weighting in the instance-aware selector.

First Experiments:
1. Baseline comparison without distillation to establish catastrophic forgetting baseline
2. Single-branch DKD implementation to isolate displacement vector benefits
3. Fixed-weight selector instead of instance-aware to evaluate the importance of adaptive weighting

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Theoretical justification for displacement vectors' superiority over traditional KD metrics is limited
- Instance-aware sample selector introduces additional hyperparameters affecting reproducibility
- Experimental validation focuses primarily on image datasets, limiting generalization assessment

## Confidence
1. DKD's superiority over traditional KD methods: Medium confidence
2. DDNet's state-of-the-art performance: High confidence
3. Robustness to outliers: Medium confidence
4. Generalization across different incremental scenarios: Low confidence

## Next Checks
1. Conduct ablation studies isolating the contribution of displacement vectors versus traditional knowledge distillation metrics to quantify the specific advantage of the DKD approach.

2. Test DDNet on non-image datasets and non-standard few-shot scenarios to evaluate generalization beyond the current experimental scope.

3. Perform theoretical analysis comparing the representational capacity of displacement vectors versus other distance-based knowledge distillation methods to provide mathematical justification for the approach.