---
ver: rpa2
title: Anomaly Detection from a Tensor Train Perspective
arxiv_id: '2409.15030'
source_url: https://arxiv.org/abs/2409.15030
tags: []
core_contribution: This paper introduces tensor network-based algorithms for anomaly
  detection by leveraging data compression. The core idea is to use Tensor Train (TT)
  representations to preserve the structure of normal data while compressing and displacing
  anomalous data.
---

# Anomaly Detection from a Tensor Train Perspective

## Quick Facts
- arXiv ID: 2409.15030
- Source URL: https://arxiv.org/abs/2409.15030
- Authors: Alejandro Mata Ali; Aitor Moreno Fdez. de Leceta; Jorge López Rubio
- Reference count: 21
- Up to 99.7% AUC on digit datasets and 98% accuracy on cybersecurity data

## Executive Summary
This paper introduces tensor network-based algorithms for anomaly detection by leveraging data compression through Tensor Train (TT) representations. The core innovation lies in using TT compression to preserve normal data structures while displacing anomalous data, enabling effective distinction between normal and anomalous samples. Two approaches are presented: global compression, which compresses the entire dataset, and local compression, which adapts to individual data points. The algorithms achieve high performance across multiple datasets, with particular success when no pre-scaling is applied.

## Method Summary
The method employs Tensor Train decomposition with truncated SVD to compress data, using the scalar product between original and compressed data as a decision metric. Global compression treats the entire dataset uniformly, while local compression adapts the representation to individual data points based on a representative normal data structure. The compression parameter τ controls the degree of approximation, with performance showing sensitivity to its value. The algorithms are tested on digit datasets, Olivetti faces, and cybersecurity data, comparing both scaled and unscaled versions of the data.

## Key Results
- Up to 99.7% AUC achieved on digit datasets using local compression
- 98% accuracy on cybersecurity dataset with global compression (no scaling)
- Optimal compression parameters found at τ=10⁻²·⁷ for digits, τ=10⁻²·⁵ for Olivetti faces, and τ=10⁻²·³ for cybersecurity data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Normal data shares common structures that survive tensor train compression better than anomalous data.
- Mechanism: When normal data has repetitive or coherent patterns, these structures are preserved in the compressed TT representation, while anomalies, lacking such structure, are displaced more strongly from their original positions in feature space.
- Core assumption: The dataset contains a dominant normal structure that is more coherent than the diverse anomalous structures.
- Evidence anchors:
  - [abstract] "These algorithms consist of preserving the structure of normal data in compression and deleting the structure of anomalous data."
  - [section 2.1] "the most normal data, sharing a common set of structures, can still be represented accurately even with an approximate representation. On the other hand, anomalous data, having different structures among them, will be seen in the representation as details, and when compressing the representation, it will no longer be represented efficiently and will be displaced from their initial position."
  - [corpus] No direct corpus evidence found; mechanism inferred from paper content.
- Break condition: If anomalies have similar structure to normal data or if normal data is highly heterogeneous, the displacement advantage diminishes.

### Mechanism 2
- Claim: Scalar product between original and compressed data, normalized by original norm, captures directional displacement without being affected by norm changes from SVD truncation.
- Mechanism: The decision function d = y · y' / |y|² measures how much the compressed data retains the direction of the original data. This is robust to distance changes that may occur during truncation.
- Core assumption: Direction preservation is a more reliable signal for anomaly detection than distance preservation in compressed space.
- Evidence anchors:
  - [section 2.1] "we choose to use the scalar product of both, normalized by their norms... we take into account only the variation in the direction of the data with respect to the origin and not the distance to it, since with the truncated SVD this distance is not guaranteed to remain constant."
  - [corpus] No direct corpus evidence found; mechanism inferred from paper content.
- Break condition: If anomalies happen to align directionally with normal data, this metric loses discriminative power.

### Mechanism 3
- Claim: Local compression adapts the TT representation to each individual data point based on a representative normal data structure, forcing anomalous data into less efficient representations.
- Mechanism: By fixing the first n-1 nodes of the TT representation to match those of a normal training data point, anomalous data must fit into the constrained structure, resulting in larger displacement and easier identification.
- Core assumption: A single representative normal data point can encode the dominant structure needed to constrain anomalous data effectively.
- Evidence anchors:
  - [section 2.2] "the idea is, given some training data ⃗x, to split its column index... find an approximated TT representation of that data, and then make a TT representation of each data point to be tested, so that the first n-1 nodes are equal to those of the training data representation."
  - [corpus] No direct corpus evidence found; mechanism inferred from paper content.
- Break condition: If the chosen training data is not representative of the normal class, or if anomalies share similar structures, the method fails.

## Foundational Learning

- Concept: Tensor Train (TT) decomposition and truncated SVD
  - Why needed here: The core algorithm relies on iteratively applying truncated SVD to create a compressed TT representation, which is what enables structure preservation/deletion.
  - Quick check question: What is the role of the compression parameter τ in controlling the degree of approximation in TT decomposition?
- Concept: Anomaly detection via displacement in compressed space
  - Why needed here: The method distinguishes anomalies by how much they move when compressed, so understanding this principle is essential.
  - Quick check question: Why does normalizing by the original norm in the scalar product help avoid false positives due to distance changes?
- Concept: Supervised vs. unsupervised anomaly detection
  - Why needed here: The paper presents both approaches, with supervised methods using known normal data to guide compression.
  - Quick check question: How does adding normal training data change the structure that the TT representation preserves?

## Architecture Onboarding

- Component map: Data preprocessing -> TT decomposition (global or local) -> Compression (τ-controlled) -> Scalar product computation -> Decision threshold -> Classification
- Critical path: Data splitting -> TT representation generation -> Scalar product decision -> Classification
- Design tradeoffs:
  - Global compression is faster but less precise; local compression is slower but can achieve higher accuracy.
  - No scaling can sometimes improve performance if it preserves the natural structure.
  - Choice of τ is critical; too high loses normal structure, too low fails to suppress anomalies.
- Failure signatures:
  - AUROC drops to ~0.5 (random classifier) -> structure preservation/deletion is not working.
  - Extremely slow runtime -> local compression on large datasets.
  - Inverted predictions -> displacement is reversed (seen in ACLCTNAD on digits).
- First 3 experiments:
  1. Run ACGCTNAD on digits dataset with and without StandardScaler; compare AUROC vs τ.
  2. Test ACLCTNAD on a small subset to observe runtime and performance trade-off.
  3. Apply the method to a synthetic dataset with clear normal/anomalous structure to verify displacement behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do the algorithms exhibit peak performance at very low compression values, which seems counter-intuitive?
- Basis in paper: [explicit] The paper mentions this phenomenon and suggests that for certain low compression values, the representation deletes much of the structure of the anomalous data, but for higher values it starts to delete some of the structure of the normal data.
- Why unresolved: The paper provides a possible explanation but does not conduct a detailed analysis to fully understand this behavior.
- What evidence would resolve it: A comprehensive study examining the relationship between compression values and the preservation/deletion of normal and anomalous data structures, possibly through visualizations or mathematical proofs.

### Open Question 2
- Question: How do different tensor network representations, such as PEPS, compare to TT in terms of anomaly detection performance?
- Basis in paper: [inferred] The paper states that the algorithms can be applied to any tensor network representation and suggests future research on studying the algorithm with other types of representations like PEPS.
- Why unresolved: The paper only tests the algorithms using TT representations and does not explore other tensor network structures.
- What evidence would resolve it: Empirical comparisons of the algorithms' performance using different tensor network representations (e.g., PEPS, hierarchical trees) on the same datasets.

### Open Question 3
- Question: What is the optimal way to choose different compression factors (τ) along the truncated SVD process?
- Basis in paper: [explicit] The paper mentions that τ can be a list of compression factors, so that each truncated SVD has a different factor to compress, but takes it as a constant in the remainder of the paper.
- Why unresolved: The paper does not explore the use of varying compression factors along the SVD process, leaving the optimal strategy unclear.
- What evidence would resolve it: Experiments comparing the performance of the algorithms using different strategies for selecting τ values at each SVD step, such as using a constant τ, linearly increasing/decreasing τ, or using a learned schedule.

## Limitations

- Performance highly sensitive to compression parameter τ with optimal values varying significantly across datasets
- Local compression approach is computationally expensive and may not scale well to large datasets
- Method assumes normal data shares coherent structures more effectively than anomalies, which may not hold in highly heterogeneous datasets

## Confidence

- Mechanism 1 (structure preservation): Medium - theoretically sound but depends heavily on data homogeneity
- Mechanism 2 (directional displacement): High - mathematically rigorous and well-justified
- Mechanism 3 (local compression constraint): Medium - effective in practice but computationally intensive

## Next Checks

1. **Ablation study on compression parameter**: Systematically vary τ across orders of magnitude on synthetic data with controlled normal/anomalous structure to quantify the sensitivity of displacement-based detection.

2. **Cross-dataset generalizability test**: Apply the algorithm to datasets with known structure (e.g., synthetic mixtures, time-series) to verify if the claimed displacement advantage holds when normal data is not inherently coherent.

3. **Runtime scalability analysis**: Benchmark local compression on progressively larger subsets of the cybersecurity dataset to determine the practical limits of computational feasibility.