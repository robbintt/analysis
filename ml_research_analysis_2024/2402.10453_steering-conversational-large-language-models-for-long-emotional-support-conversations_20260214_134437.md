---
ver: rpa2
title: Steering Conversational Large Language Models for Long Emotional Support Conversations
arxiv_id: '2402.10453'
source_url: https://arxiv.org/abs/2402.10453
tags:
- strategy
- support
- emotional
- responses
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the steerability of large language models\
  \ (LLMs) in maintaining emotional support strategies during extended conversations.\
  \ The authors introduce a novel metric called Strategy Relevant Attention (SRA)\
  \ to measure the model\u2019s attention to strategy-related tokens in prompts."
---

# Steering Conversational Large Language Models for Long Emotional Support Conversations

## Quick Facts
- arXiv ID: 2402.10453
- Source URL: https://arxiv.org/abs/2402.10453
- Reference count: 19
- Key outcome: Fine-tuning Llama2-7B-chat and Llama3-8B-instruct on strategy-conditioned synthetic data improves emotional support strategy adherence by 78.9% and 37.6% respectively, validated by the Strategy Relevant Attention (SRA) metric with 0.94 Pearson correlation to strategy adherence.

## Executive Summary
This study addresses the challenge of maintaining emotional support strategies in extended conversations with large language models. The authors introduce a novel metric called Strategy Relevant Attention (SRA) to measure how well model attention aligns with strategy-relevant tokens in prompts. They create an extended version of the ESConv dataset with strategy-conditioned continuations and fine-tune Llama2-7B-chat and Llama3-8B-instruct models on this data. The fine-tuned models demonstrate significant improvements in strategy adherence compared to baseline models, with SRA showing strong correlation with actual strategy adherence. Human evaluations confirm that the fine-tuned models maintain naturalness and coherence while better following emotional support strategies.

## Method Summary
The researchers extended the ESConv dataset by generating strategy-conditioned synthetic continuations for conversation turns 5-23, creating 15 support strategies. They trained a RoBERTa-large classifier to automatically evaluate strategy adherence in generated responses. The Llama2-7B-chat and Llama3-8B-instruct models were fine-tuned using LoRA on this extended dataset with standard prompting. Various prompt templates were tested to measure SRA and identify optimal prompt structures. The SRA metric aggregates attention weights of strategy-relevant tokens across all layers and heads to provide a scalar score indicating model steerability.

## Key Results
- Fine-tuned Llama2-7B-chat achieved 78.9% accuracy improvement in strategy adherence over baseline
- Fine-tuned Llama3-8B-instruct showed 37.6% accuracy improvement in strategy adherence
- SRA metric demonstrated 0.94 Pearson correlation with strategy adherence accuracy
- Human evaluations confirmed fine-tuned models maintained naturalness and coherence while improving strategy adherence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SRA quantifies how well model attention aligns with strategy-relevant tokens, enabling steerability measurement.
- Mechanism: SRA aggregates attention weights of response tokens over strategy tokens across all layers and heads, providing a scalar score that correlates with strategy adherence.
- Core assumption: Attention weights directly reflect the model's internal focus on strategic content.
- Evidence anchors: [abstract] "Strategy Relevant Attention (SRA) metric, which quantifies the model's adherence to the prompted strategy through attention maps"; [section] "We aggregate the attention weights of the strategy relevant tokens over all heads and all layers for the generated response tokens"
- Break condition: If attention weights become uniform or strategy tokens are overshadowed by context tokens in long conversations

### Mechanism 2
- Claim: Fine-tuning on strategy-conditioned synthetic data improves long-horizon strategy adherence.
- Mechanism: LoRA fine-tuning on extended ESConv dataset with standard prompting forces model to prioritize system prompt over conversation history.
- Core assumption: Synthetic data with explicit strategy conditioning can overcome the "lost in the middle" effect.
- Evidence anchors: [abstract] "We also propose various baselines informed by our proposed SRA metric to address the challenge and propose a fine-tuned model that significantly enhances the steerability"; [section] "We fine-tuned Llama2-7B-chat and Llama3-8B-instruct models using the LoRA method, focusing exclusively on the last utterance's strategy-conditioned continuations"
- Break condition: If model overfits to synthetic patterns and loses generalization to real emotional support conversations

### Mechanism 3
- Claim: Prompt template engineering based on SRA findings improves strategy adherence without fine-tuning.
- Mechanism: Relocating conversation history within system message (c1_hf template) maintains attention on strategy tokens throughout extended dialogues.
- Core assumption: Token position within prompt affects attention distribution in autoregressive generation.
- Evidence anchors: [section] "We also design 6 other prompt templates as described in figure 3. These variations include maintaining only 1, 3, or 5 of the most recent messages"; [section] "The c1_hf prompt achieves the most stable strategy following behavior compared to the other baselines"
- Break condition: If model's attention mechanism changes fundamentally in future architectures

## Foundational Learning

- Concept: Attention mechanisms in Transformers
  - Why needed here: SRA relies on analyzing multi-head attention patterns to measure strategy adherence
  - Quick check question: How do query, key, and value vectors interact in self-attention to produce output embeddings?

- Concept: LoRA (Low-Rank Adaptation) fine-tuning
  - Why needed here: The study uses LoRA to efficiently fine-tune large models on strategy-conditioned data
  - Quick check question: What is the computational advantage of LoRA compared to full fine-tuning for large language models?

- Concept: Strategy classification for automated evaluation
  - Why needed here: The study uses a RoBERTa classifier to automatically assess strategy adherence in generated responses
  - Quick check question: How does weak supervision (strategy-conditioned but not strategy-adherent responses) affect classifier training?

## Architecture Onboarding

- Component map: Input prompt → Transformer layers with attention → SRA calculation module → Strategy classifier → Output evaluation
- Critical path: Prompt construction → Model generation → Attention extraction → SRA computation → Strategy classification
- Design tradeoffs: SRA provides quantitative steerability measurement but requires access to internal attention weights; fine-tuning improves strategy adherence but may reduce general conversational quality
- Failure signatures: Low SRA scores indicate strategy drift; classifier confusion suggests strategy similarity; human evaluation disagreement indicates SRA limitations
- First 3 experiments:
  1. Generate responses using different prompt templates and compute SRA scores to identify optimal prompt structure
  2. Fine-tune base model on extended ESConv dataset and measure SRA improvement over baseline
  3. Perform ablation study removing different parts of conversation history to understand context sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SRA metric perform across different LLM architectures and configurations beyond Llama models?
- Basis in paper: [explicit] The paper mentions that "the generalizability of SRA across diverse LLM architectures and configurations remains to be fully explored" as a limitation.
- Why unresolved: The study focused primarily on Llama models (Llama2-7B-chat and Llama3-8B-instruct) and did not test the SRA metric on other LLM architectures.
- What evidence would resolve it: Conducting experiments with various LLM architectures (e.g., GPT, BERT, RoBERTa) and configurations to compare SRA performance across models.

### Open Question 2
- Question: What is the long-term effectiveness of different emotional support strategies, and how do they compare in terms of user outcomes?
- Basis in paper: [inferred] The paper notes that "we do not study the helpfulness or effectiveness of different strategies at different stages of the conversation and leave it to the future work."
- Why unresolved: The study focused on strategy adherence and steerability but did not evaluate the actual effectiveness or helpfulness of different strategies in achieving positive outcomes.
- What evidence would resolve it: Longitudinal studies tracking user outcomes and satisfaction when different strategies are employed over extended periods.

### Open Question 3
- Question: How does the SRA metric correlate with user satisfaction and perceived helpfulness in real-world emotional support scenarios?
- Basis in paper: [explicit] The paper mentions using human evaluations to assess model performance but does not directly correlate SRA with user satisfaction metrics.
- Why unresolved: While the study showed a correlation between SRA and strategy adherence, it did not investigate the relationship between SRA and user-perceived helpfulness or satisfaction.
- What evidence would resolve it: Conducting user studies where participants rate their satisfaction and perceived helpfulness while SRA is measured, then analyzing the correlation between these metrics.

## Limitations

- Synthetic data dependency introduces uncertainty about real-world generalization
- SRA metric measures attention weights rather than actual strategy implementation quality
- Results limited to relatively small Llama models; effectiveness for larger architectures unknown

## Confidence

- High Confidence: The SRA metric effectively measures attention alignment with strategy tokens within the tested model architectures
- Medium Confidence: The fine-tuning approach improves strategy adherence in extended conversations
- Low Confidence: The prompt engineering improvements will generalize to different conversational contexts or model architectures

## Next Checks

1. Apply the fine-tuned models to actual human emotional support conversations (not synthetic) and measure strategy adherence using both SRA and human evaluation. Compare performance against models fine-tuned only on the original ESConv dataset without synthetic extensions.

2. Test the SRA metric and prompt engineering approaches on larger models (e.g., Llama 70B, GPT-4) and different architectures (e.g., models with grouped query attention or state-space models). Evaluate whether the correlation between SRA and strategy adherence holds across architectures.

3. Conduct a systematic ablation study varying the synthetic data generation parameters (e.g., number of examples per strategy, diversity of continuations, quality thresholds). Measure how different synthetic data qualities affect model performance and SRA scores to establish minimum quality requirements for effective fine-tuning.