---
ver: rpa2
title: 'Choose the Final Translation from NMT and LLM hypotheses Using MBR Decoding:
  HW-TSC''s Submission to the WMT24 General MT Shared Task'
arxiv_id: '2409.14800'
source_url: https://arxiv.org/abs/2409.14800
tags:
- translation
- data
- training
- language
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper describes HW-TSC's submission to the WMT24 general machine
  translation shared task for English to Chinese. The team trained both NMT and LLM-based
  MT models, using strategies like R-Drop, BiT, DD, FT, BT, AT, CL, and TEL for NMT,
  and CPT, SFT, and CPO for LLMs.
---

# Choose the Final Translation from NMT and LLM hypotheses Using MBR Decoding: HW-TSC's Submission to the WMT24 General MT Shared Task

## Quick Facts
- **arXiv ID:** 2409.14800
- **Source URL:** https://arxiv.org/abs/2409.14800
- **Reference count:** 24
- **Primary result:** HW-TSC's submission achieved BLEU score of 56.41 and COMET score of 0.7234 on WMT23 general test set

## Executive Summary
This paper describes HW-TSC's submission to the WMT24 general machine translation shared task for English to Chinese. The team employed a hybrid approach combining both NMT and LLM-based MT systems, using MBR decoding to select the final translation from multiple hypotheses. The system achieved competitive results with BLEU score of 56.41 and COMET score of 0.7234 on the WMT23 general test set, demonstrating the effectiveness of combining different MT paradigms through MBR decoding.

## Method Summary
The submission combines NMT and LLM-based MT systems using MBR decoding to select the final translation. The NMT system uses deep Transformer-big architecture with specialized training strategies including R-Drop, BiT, DD, FT, BT, AT, CL, and TEL. The LLM-based MT system uses Llama2-13B with three-stage training: CPT with monolingual data, SFT with high-quality parallel data, and CPO with triplet preference data. Both systems generate N-best hypotheses which are combined and processed through MBR decoding using COMET scores to select the final translation.

## Key Results
- Achieved BLEU score of 56.41 on WMT23 general test set
- Achieved COMET score of 0.7234 on WMT23 general test set
- Demonstrated competitive performance through combination of NMT and LLM-based MT systems with MBR decoding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MBR decoding improves translation quality by selecting hypotheses that maximize expected COMET similarity rather than likelihood.
- Mechanism: MBR decoding aggregates multiple NMT and LLM-based MT hypotheses, then chooses the final translation based on its expected utility (COMET score), effectively leveraging the complementary strengths of both systems.
- Core assumption: COMET scores correlate well with human translation quality judgments.
- Evidence anchors:
  - [abstract] "By using Minimum Bayesian risk (MBR) decoding to select the final translation from multiple hypotheses for NMT and LLM-based MT models, our submission receives competitive results"
  - [section 4.5] "MBR decoding aims to find the output that maximizes the expected utility function, which measures the similarity between the hypothesis and the reference. For MT, this could be an automated evaluation metric such as COMET"
- Break condition: If COMET scores don't correlate well with human judgments, MBR decoding could select suboptimal translations.

### Mechanism 2
- Claim: Combining NMT and LLM-based MT systems through MBR decoding leverages complementary strengths of both architectures.
- Mechanism: NMT systems trained on large parallel data provide strong baseline translations, while LLM-based MT systems offer improved fluency and context understanding. MBR decoding selects the best from both.
- Core assumption: NMT and LLM-based MT systems make different types of errors, allowing their combination to outperform either alone.
- Evidence anchors:
  - [abstract] "By using Minimum Bayesian risk (MBR) decoding to select the final translation from multiple hypotheses for NMT and LLM-based MT models"
  - [section 5.2] "we use MBR decoding to select the final translation, which has been shown to be better than MBR decoding of a single translation system in terms of COMET scores"
- Break condition: If both systems make similar errors, combination through MBR won't provide significant benefits.

### Mechanism 3
- Claim: Specialized training strategies for NMT and LLM-based MT systems improve their individual performance before combination.
- Mechanism: NMT benefits from techniques like R-Drop, BiT, DD, AT, CL, and TEL, while LLM-based MT improves through CPT, SFT, and CPO. Better individual systems lead to better combined results.
- Core assumption: The listed training strategies effectively improve translation quality for their respective architectures.
- Evidence anchors:
  - [section 3.2-3.9] Detailed description of NMT training strategies
  - [section 4.2-4.4] Description of LLM-based MT training stages
  - [section 5.2] Performance improvements shown in evaluation results
- Break condition: If training strategies don't improve individual system performance, the combined approach won't be effective.

## Foundational Learning

- Concept: Minimum Bayes Risk (MBR) decoding
  - Why needed here: MBR decoding is the core mechanism for selecting the final translation from multiple hypotheses
  - Quick check question: What does MBR decoding optimize for in machine translation?

- Concept: Contrastive Preference Optimization (CPO)
  - Why needed here: CPO is used to fine-tune LLM-based MT systems to improve translation quality beyond supervised fine-tuning
  - Quick check question: How does CPO differ from standard supervised fine-tuning?

- Concept: Curriculum Learning (CL)
  - Why needed here: CL is used to improve NMT training efficiency and performance by sampling examples based on difficulty
  - Quick check question: What is the main principle behind curriculum learning in neural network training?

## Architecture Onboarding

- Component map: NMT system → LLM-based MT system → MBR decoder → Final translation. NMT uses Transformer-big with various training strategies. LLM-based MT uses LLaMA2-13B with CPT, SFT, and CPO stages. MBR decoder uses COMET scores to select from both systems' hypotheses.

- Critical path: Data preprocessing → NMT training with specialized strategies → LLM-based MT training through CPT, SFT, CPO stages → Hypothesis generation → MBR decoding → Final output

- Design tradeoffs: The system trades computational complexity (running multiple models and MBR decoding) for improved translation quality. It also balances between NMT's data efficiency and LLM's generalization capabilities.

- Failure signatures: Poor COMET score correlation with human judgment, similar error patterns between NMT and LLM-based MT systems, ineffective training strategies, or computational constraints preventing full hypothesis generation.

- First 3 experiments:
  1. Test MBR decoding with NMT hypotheses only to establish baseline improvement
  2. Test MBR decoding with LLM-based MT hypotheses only to compare performance
  3. Combine both systems with MBR decoding and measure COMET score improvement over individual systems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the MBR decoding approach scale with the number of hypotheses (N) sampled from both NMT and LLM-based MT models?
- Basis in paper: [explicit] The paper mentions using MBR decoding to select the final translation from N-best translations generated by both systems, but does not explore the impact of varying N.
- Why unresolved: The paper does not provide an analysis of how different values of N affect the final translation quality, leaving open the question of optimal sampling size.
- What evidence would resolve it: Experiments comparing translation quality (BLEU/COMET scores) for different N values (e.g., N=5, 10, 20) using MBR decoding.

### Open Question 2
- Question: What is the relative contribution of each training strategy (R-Drop, BiT, DD, FT, BT, AT, CL, TEL) to the final NMT system performance?
- Basis in paper: [explicit] The paper combines multiple training strategies but does not provide ablation studies to isolate the impact of each component.
- Why unresolved: Without ablation studies, it's unclear which strategies contribute most to the performance gains, making it difficult to optimize resource allocation.
- What evidence would resolve it: Results from experiments removing each training strategy individually while keeping others constant, showing the performance change.

### Open Question 3
- Question: How does the performance of the LLM-based MT system vary across different domains compared to the NMT system?
- Basis in paper: [inferred] The paper focuses on general domain translation but does not explore domain-specific performance differences between the two systems.
- Why unresolved: The paper does not provide any domain-specific evaluation or analysis of where each system excels or struggles.
- What evidence would resolve it: Translation quality metrics (BLEU/COMET) for both systems across multiple domains (news, biomedical, conversational, etc.).

## Limitations

- Specific hyperparameters for training strategies (R-Drop λ values, dropout rates, learning rates) are not specified
- The effectiveness of combining NMT and LLM-based MT systems assumes complementary error patterns without error analysis
- Computational complexity of running multiple models and MBR decoding may limit practical deployment

## Confidence

- **High confidence:** The basic methodology of using MBR decoding to select from NMT and LLM hypotheses is sound and well-established in the literature
- **Medium confidence:** The specific training strategies employed will improve individual system performance as claimed
- **Medium confidence:** The combination approach will outperform either system alone due to complementary strengths

## Next Checks

1. **Ablation study:** Run MBR decoding with only NMT hypotheses and only LLM-based MT hypotheses separately, then compare their COMET scores to the combined system to quantify the contribution of each component.

2. **Error pattern analysis:** Conduct a systematic error analysis comparing the outputs of NMT and LLM-based MT systems to verify that they make different types of errors, validating the assumption behind their combination.

3. **Hyperparameter sensitivity:** Test the system's performance with different MBR decoding utility functions (e.g., using BLEU instead of COMET) to determine how sensitive the results are to the choice of evaluation metric for hypothesis selection.