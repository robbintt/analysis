---
ver: rpa2
title: 'Factorized-Dreamer: Training A High-Quality Video Generator with Limited and
  Low-Quality Data'
arxiv_id: '2408.10119'
source_url: https://arxiv.org/abs/2408.10119
tags:
- video
- generation
- image
- diffusion
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a factorized framework, namely Factorized-Dreamer,
  for text-to-video generation. Unlike existing methods that require large-scale high-quality
  video datasets, the authors show that publicly available limited and low-quality
  datasets are sufficient to train a high-quality video generator without recaptioning
  or finetuning.
---

# Factorized-Dreamer: Training A High-Quality Video Generator with Limited and Low-Quality Data

## Quick Facts
- arXiv ID: 2408.10119
- Source URL: https://arxiv.org/abs/2408.10119
- Authors: Tao Yang, Yangming Shi, Yunwen Huang, Feng Chen, Yin Zheng, Lei Zhang
- Reference count: 40
- Primary result: Achieves competitive text-to-video generation quality using only publicly available limited and low-quality datasets without requiring recaptioning or fine-tuning

## Executive Summary
This paper proposes Factorized-Dreamer, a novel framework for text-to-video generation that challenges the conventional wisdom requiring large-scale high-quality video datasets. The key innovation is a factorized approach that splits video generation into two stages: first generating a high-quality image using a descriptive caption, then synthesizing the video conditioned on that image and a concise motion caption. By leveraging pre-trained image generation models and employing architectural innovations like Pixel-Aware Cross Attention and a PredictNet for optical flow supervision, the method achieves competitive results while being trained on publicly available datasets that are both limited in size and low in quality.

## Method Summary
Factorized-Dreamer employs a two-stage generation process where a text-to-image model first generates a base image, followed by a text/image-to-video model that synthesizes temporal evolution. The architecture features a T2I adapter that combines CLIP text and image embeddings through cross-attention, PACA modules that use pixel-level image latents for conditioning instead of just global features, and a T5 text encoder that improves motion understanding. The model uses an adjusted noise schedule with shifting and rescaling to ensure stable training, and optionally incorporates a PredictNet trained with optical flow supervision to enhance temporal coherence. Training proceeds in multiple stages: first on lower resolution videos, then higher resolution, with the PredictNet added in a final stage.

## Key Results
- Achieves EvalCrafter benchmark scores of 67.12 (Visual Quality), 65.02 (Text-Video Alignment), 55.17 (Motion Quality), and 63.43 (Temporal Consistency)
- Outperforms or matches many existing methods trained on larger proprietary datasets despite using only publicly available limited and low-quality data
- Demonstrates effectiveness across multiple evaluation metrics including FVD and IS
- Ablation studies confirm the importance of each architectural component

## Why This Works (Mechanism)

### Mechanism 1: Factorized Generation
- Claim: Splitting video synthesis into image generation and motion-conditioned video generation reduces complexity
- Mechanism: Spatial structure is handled by pre-trained T2I models while temporal evolution is learned by a factorized TI2V model
- Core assumption: Generated image provides sufficient spatial information so text can focus on motion description
- Evidence: Paper explicitly describes the two-step process and shows factorization in Figure 1(b)

### Mechanism 2: Pixel-Aware Cross Attention
- Claim: PACA improves video coherence by preserving fine spatial details during temporal evolution
- Mechanism: Uses latent representation of conditioned image as key/value inputs for pixel-level conditioning
- Core assumption: Pixel-level conditioning is more effective than CLIP feature-level conditioning
- Evidence: Paper cites PACA [63] and explains how it differs from using only CLIP image features

### Mechanism 3: Adjusted Noise Schedule
- Claim: Tailored noise schedule ensures stable training by controlling signal-to-noise ratio
- Mechanism: Shifting and rescaling original SD noise schedule to enforce lower SNR at terminal timesteps
- Core assumption: Video domain is more sensitive to noise than image domain
- Evidence: Visualizes log SNR curves and specifies shifting factor s=0.125

## Foundational Learning

- **Diffusion models**: Progressively add and remove noise to generate data
  - Why needed: Model learns to denoise video latents conditioned on text and image embeddings
  - Quick check: What is the role of the noise schedule in diffusion training?

- **Cross-attention layers**: Combine conditional embeddings with latent features
  - Why needed: T2I adapter uses cross-attention to blend CLIP text and image embeddings
  - Quick check: How does PACA differ from standard cross-attention?

- **Optical flow supervision**: For temporal coherence
  - Why needed: PredictNet predicts optical flows to encourage consistent motion across frames
  - Quick check: What loss term is used to train the PredictNet?

## Architecture Onboarding

- **Component map**: Pre-trained T2I backbone → T2I adapter → CLIP/T5 encoders → PACA modules → Temporal UNet → PredictNet (optional)

- **Critical path**:
  1. Encode image and text into embeddings
  2. Combine embeddings via T2I adapter and feed into UNet
  3. Apply PACA for pixel-level conditioning in spatial layers
  4. Generate video latent sequence
  5. Optionally apply PredictNet for flow supervision

- **Design tradeoffs**:
  - T5 vs CLIP: Better motion understanding but higher compute cost
  - PACA: Higher memory usage but better spatial detail preservation
  - PredictNet: Improved coherence but added complexity and training time

- **Failure signatures**:
  - Inconsistent motion: Missing or poorly tuned PredictNet supervision
  - Blurry details: PACA failure or poor VAE latents
  - Unstable training: Noise schedule misconfiguration

- **First 3 experiments**:
  1. Train without PACA to confirm impact on spatial detail retention
  2. Remove PredictNet to measure degradation in motion coherence
  3. Test different noise schedule shifting factors (s=0.1, 0.125, 0.15)

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but raises several important considerations through its methodology and results. The factorized approach suggests interesting questions about the minimum viable caption quality, the scalability of the method to longer videos, and the potential for further architectural optimizations.

## Limitations

- Evaluation scope is limited to existing datasets rather than truly novel domains or long-form videos
- Claims about limited data sufficiency are qualified by use of both low-quality and high-quality datasets
- Compute requirements are substantial but not clearly quantified
- Method tested primarily on 16-frame videos up to 512×512 resolution

## Confidence

**High Confidence**: Core architectural innovations (T2I adapter, PACA, PredictNet) are technically sound with strong ablation support

**Medium Confidence**: Claims about limited data sufficiency are supported by benchmarks but lack full spectrum comparison

**Low Confidence**: Assertion that publicly available datasets alone suffice is not fully validated due to mixed dataset usage in experiments

## Next Checks

1. **Data Ablation Study**: Train exclusively on WebVid-10M without PexelsVideos to isolate actual data requirements

2. **Noise Schedule Isolation**: Conduct controlled experiments with different shifting factors while keeping other components constant

3. **Long-Form Video Generation**: Test ability to generate videos longer than 16 frames by extending temporal dimension