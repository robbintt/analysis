---
ver: rpa2
title: 'BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific
  Models'
arxiv_id: '2403.18365'
source_url: https://arxiv.org/abs/2403.18365
tags:
- knowledge
- blade
- llms
- language
- legal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BLADE addresses the challenge of adapting general-purpose large
  language models to domain-specific tasks where deep domain knowledge is required,
  such as legal and medical applications. The core method introduces a collaborative
  framework between a black-box general LLM and a small, trainable domain-specific
  language model.
---

# BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models

## Quick Facts
- arXiv ID: 2403.18365
- Source URL: https://arxiv.org/abs/2403.18365
- Authors: Haitao Li; Qingyao Ai; Jia Chen; Qian Dong; Zhijing Wu; Yiqun Liu; Chong Chen; Qi Tian
- Reference count: 40
- Primary result: BLADE improves domain-specific LLM performance by up to 31.3% relative on Chinese legal tasks

## Executive Summary
BLADE addresses the challenge of adapting general-purpose large language models to domain-specific tasks requiring deep domain knowledge. The framework combines a black-box general LLM with a small, trainable domain-specific language model to leverage the strengths of both approaches. By pre-training the small model on domain-specific data, fine-tuning it through Knowledge Instruction Tuning, and aligning outputs via Bayesian Prompted Optimization, BLADE achieves state-of-the-art results on Chinese legal and medical benchmarks without expensive full fine-tuning of large models.

## Method Summary
BLADE operates through a three-stage pipeline: first, a small language model is pre-trained on domain-specific corpora to embed specialized knowledge; second, it undergoes Knowledge Instruction Tuning where it learns to generate question-specific knowledge prompts using a general LLM as a teacher; third, Bayesian Prompted Optimization aligns the small model's outputs with the general LLM's understanding through derivative-free optimization on soft embeddings. This collaborative framework allows the general LLM to access domain expertise through the small model's generated knowledge, achieving superior performance on domain-specific tasks while maintaining efficiency.

## Key Results
- BLADE achieves up to 31.3% relative improvement on Chinese legal tasks (JEC-QA) compared to general LLMs
- Outperforms both general and domain-specific LLMs on Chinese medical tasks (MLEC-QA)
- Surpasses retrieval-augmented methods, demonstrating effectiveness of knowledge generation over retrieval
- Shows consistent improvements across both legal and medical domains with different general LLM backends

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BLADE improves general LLM performance in domain-specific tasks by generating targeted knowledge via a small domain-specific LM.
- Mechanism: The small LM is trained on domain-specific data and then fine-tuned to generate knowledge tailored to the specific query, which the general LLM uses to produce answers. This allows the general LLM to access domain knowledge without retraining or costly fine-tuning.
- Core assumption: The small LM can accurately generate relevant, domain-specific knowledge for queries, and the general LLM can effectively incorporate this knowledge into its reasoning.
- Evidence anchors:
  - [abstract]: "BLADE consists of a black-box LLM and a small domain-specific LM. The small LM preserves domain-specific knowledge and offers specialized insights, while the general LLM contributes robust language comprehension and reasoning capabilities."
  - [section 3.2]: "We incorporate domain-specific knowledge via Domain-specific Pre-training (DP)...After Domain-specific Pre-training, the domain-specific knowledge is effectively encoded into the parameters of the smaller language model."
  - [corpus]: Weak evidence. The corpus includes related works but does not provide direct evidence for the effectiveness of the small LM in generating targeted knowledge.
- Break condition: If the small LM cannot generate relevant or accurate domain-specific knowledge, or if the general LLM cannot effectively utilize this knowledge, the method will fail.

### Mechanism 2
- Claim: BLADE uses Bayesian Prompted Optimization (BPO) to align the output of the small LM with the understanding of the general LLM.
- Mechanism: BPO uses derivative-free optimization to refine soft prompts in the small LM, aligning its output with the general LLM's preferences. This is achieved by maximizing the performance of the general LLM on domain-specific tasks using knowledge generated by the small LM.
- Core assumption: The general LLM's preferences can be effectively captured and optimized through soft prompts in the small LM, leading to better alignment and improved performance.
- Evidence anchors:
  - [section 3.4]: "The primary optimization objective is to enhance the performance of the general LLMð‘“ (Â·) on domain-specific tasks...Therefore, our objective is to identify the optimal soft prompt."
  - [abstract]: "Bayesian Prompted Optimization aligns the output of small LMs with general LLMs using derivative-free optimization on soft embeddings."
  - [corpus]: Weak evidence. The corpus includes related works but does not provide direct evidence for the effectiveness of BPO.
- Break condition: If the optimization process does not converge or does not effectively align the small LM's output with the general LLM's understanding, the method will fail.

### Mechanism 3
- Claim: BLADE's Knowledge Instruction Tuning (KIT) enhances the small LM's ability to follow instructions and generate precise, question-specific knowledge.
- Mechanism: KIT involves Prompt-based Knowledge Generation, where a general LLM generates knowledge to assist in answering questions, followed by Consistency Filtering to ensure the generated knowledge is accurate. The refined data is then used to fine-tune the small LM, enabling it to generate specific knowledge tailored to queries.
- Core assumption: The general LLM can generate accurate and relevant knowledge for questions, and the small LM can effectively learn from this knowledge to improve its own generation capabilities.
- Evidence anchors:
  - [section 3.3]: "Knowledge Instruction Tuning (KIT) consists of three components: Prompt-based Knowledge Generation, Consistency Filtering, and Instruction Tuning...After KIT, the small LM acquires the ability to generate specific knowledge to the questions."
  - [abstract]: "Knowledge Instruction Tuning, which enhances the small LM's ability to follow instructions, thereby sharpening its capacity to produce precise, question-specific knowledge."
  - [corpus]: Weak evidence. The corpus includes related works but does not provide direct evidence for the effectiveness of KIT.
- Break condition: If the generated knowledge is not accurate or relevant, or if the small LM cannot effectively learn from the refined data, the method will fail.

## Foundational Learning

- Concept: Domain-specific Pre-training (DP)
  - Why needed here: To encode domain-specific knowledge into the small LM, enabling it to generate relevant knowledge for queries.
  - Quick check question: How does DP differ from continuous pre-training of the general LLM?

- Concept: Knowledge Instruction Tuning (KIT)
  - Why needed here: To enhance the small LM's ability to follow instructions and generate precise, question-specific knowledge.
  - Quick check question: What is the role of Consistency Filtering in KIT?

- Concept: Bayesian Prompted Optimization (BPO)
  - Why needed here: To align the output of the small LM with the understanding of the general LLM, ensuring effective collaboration.
  - Quick check question: Why is derivative-free optimization used in BPO?

## Architecture Onboarding

- Component map: Small domain-specific LM -> Knowledge Instruction Tuning -> Bayesian Prompted Optimization -> General black-box LLM -> Answer generation

- Critical path:
  1. Pre-train the small LM on domain-specific data (DP).
  2. Fine-tune the small LM using KIT to generate precise, question-specific knowledge.
  3. Align the small LM's output with the general LLM's understanding using BPO.
  4. Generate knowledge using the small LM for a given query.
  5. Use the general LLM to synthesize the generated knowledge and produce a comprehensive answer.

- Design tradeoffs:
  - Size of the small LM: Larger models may provide better performance but at the cost of increased computational resources.
  - Complexity of KIT and BPO: More complex processes may lead to better alignment but at the cost of increased training time and resources.
  - Domain specificity of the pre-training data: More specific data may lead to better performance but may be harder to obtain.

- Failure signatures:
  - Poor performance on domain-specific tasks: Indicates issues with DP, KIT, or BPO.
  - Inconsistency between the small LM's output and the general LLM's understanding: Indicates issues with BPO.
  - Inability to generate relevant knowledge: Indicates issues with KIT or the quality of the pre-training data.

- First 3 experiments:
  1. Pre-train the small LM on a small subset of domain-specific data and evaluate its ability to generate relevant knowledge.
  2. Implement KIT on the pre-trained small LM and evaluate its performance on a small set of questions.
  3. Implement BPO on the KIT-tuned small LM and evaluate its alignment with the general LLM's understanding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can BLADE be extended to handle generative tasks beyond multiple-choice question answering, such as text generation or summarization in specialized domains?
- Basis in paper: [explicit] The authors note that their experiments are conducted only in multiple-choice datasets and state that "the feasibility of our approach in generative tasks still deserves further investigation."
- Why unresolved: The current BLADE framework is evaluated primarily on question-answering tasks where the output is a discrete selection. Generative tasks require continuous text output, which may pose different challenges in terms of knowledge integration, coherence, and factual accuracy.
- What evidence would resolve it: Empirical studies applying BLADE to generative tasks (e.g., legal document summarization, medical report generation) with quantitative and qualitative evaluation metrics.

### Open Question 2
- Question: What is the optimal size ratio between the general LLM and the domain-specific small LM to maximize performance while minimizing computational cost?
- Basis in paper: [inferred] The authors explore different sizes of the small LM (e.g., BLOOMZ_560m, BLOOMZ_1b1, BLOOMZ_1b7) and observe that larger small LMs lead to greater performance improvements, but do not systematically analyze the trade-off between size and cost.
- Why unresolved: The relationship between the size of the general LLM, the size of the small LM, and the resulting performance gains is not fully characterized. There may be diminishing returns or an optimal point depending on the task and domain.
- What evidence would resolve it: A systematic study varying both the general LLM and small LM sizes across multiple domains and tasks, measuring performance, inference latency, and cost.

### Open Question 3
- Question: How can BLADE be adapted to reduce hallucinations in the domain-specific small LM while maintaining its ability to generate contextually relevant knowledge?
- Basis in paper: [explicit] The authors state in the conclusion that "we will investigate approaches to minimize hallucinations in small models."
- Why unresolved: The small LM in BLADE generates knowledge that is fed to the general LLM, but there is no explicit mechanism to verify the factual accuracy of this generated knowledge, which could lead to hallucination propagation.
- What evidence would resolve it: Experiments comparing BLADE with and without hallucination mitigation techniques (e.g., retrieval-augmented generation, fact-checking modules) on tasks requiring high factual precision.

## Limitations

- The domain-specific corpora (legal and medical texts) are not fully detailed in terms of sources, preprocessing, or size, making it difficult to assess data quality and generalizability.
- The exact prompt templates used for Knowledge Instruction Tuning are not specified, which is critical since prompt engineering significantly affects performance.
- The optimization process for Bayesian Prompted Optimization lacks convergence guarantees, and the method's performance may degrade with less clean or noisier domain data.

## Confidence

**High Confidence**: The core architecture of BLADE (combining small domain-specific LMs with black-box general LLMs) is well-supported by the experimental results showing consistent improvements across both legal and medical domains.

**Medium Confidence**: The effectiveness of the three-step training procedure (Domain-specific Pre-training, Knowledge Instruction Tuning, and Bayesian Prompted Optimization) is demonstrated, but the specific contributions of each component are not isolated in ablation studies.

**Low Confidence**: The scalability and generalization claims to other domains are weakly supported, as experiments are limited to only two Chinese-language domains (legal and medical).

## Next Checks

1. **Ablation Study**: Conduct controlled experiments to measure the individual contribution of each component (DP, KIT, BPO) by testing variants that include only one or two components versus the full BLADE pipeline.

2. **Cross-Domain Generalization**: Apply BLADE to a third, structurally different domain (e.g., financial domain) using both Chinese and English benchmarks to test generalizability beyond the original legal/medical scope.

3. **Optimization Robustness**: Test the Bayesian Prompted Optimization with varying levels of noise in the domain data and measure convergence speed and final performance compared to alternative optimization methods.