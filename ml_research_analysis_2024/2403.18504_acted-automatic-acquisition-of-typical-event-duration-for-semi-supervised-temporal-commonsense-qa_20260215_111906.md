---
ver: rpa2
title: 'AcTED: Automatic Acquisition of Typical Event Duration for Semi-supervised
  Temporal Commonsense QA'
arxiv_id: '2403.18504'
source_url: https://arxiv.org/abs/2403.18504
tags:
- duration
- event
- events
- data
- typical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a semi-supervised approach for automatically
  acquiring typical event duration information. The method uses a voting strategy
  across duration units to identify typical durations from unlabeled text, treating
  duration prediction as a QA task.
---

# AcTED: Automatic Acquisition of Typical Event Duration for Semi-supervised Temporal Commonsense QA

## Quick Facts
- arXiv ID: 2403.18504
- Source URL: https://arxiv.org/abs/2403.18504
- Reference count: 0
- Primary result: Pseudo-labeled data from hundreds of events achieves performance comparable to state-of-the-art weakly supervised approaches using 1.5M+ training examples

## Executive Summary
This work presents a semi-supervised approach for automatically acquiring typical event duration information from unlabeled text. The method uses a voting strategy across duration units to identify typical durations from Wikipedia sentences containing events, treating duration prediction as a QA task. When applied to the MC-TACO temporal commonsense QA task, models trained with only hundreds of pseudo-labeled events achieve performance comparable to state-of-the-art weakly supervised approaches that use significantly more training data. The best RoBERTa-based model improves Exact Match by 7% over previous baselines.

## Method Summary
The method uses a two-stage approach: First, a draft BERT/RoBERTa model predicts duration plausibility for event-sentence pairs sampled from Wikipedia. These predictions are aggregated into duration histograms using majority voting across duration units (seconds to decades) to identify typical durations. Second, pseudo-labeled QA data is generated where positive answers use the acquired typical duration unit and negative answers use units at least two positions away. The final model is fine-tuned on this pseudo data and evaluated on MC-TACO-duration. The approach handles both episodic and habitual event durations through bi-modal histogram detection.

## Key Results
- Pseudo-labeled data achieves 80.0% accuracy in human evaluation for typical duration identification
- Models trained on pseudo data from only 400 events match performance of weakly supervised approaches using 1.5M+ examples
- Best RoBERTa model achieves 7% EM improvement over previous state-of-the-art on MC-TACO-duration
- Method handles both episodic (occasional occurrence) and habitual (repeated occurrence) event durations through bi-modal histogram detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The voting strategy across duration units reliably identifies typical durations even when individual predictions are noisy.
- Mechanism: By sampling multiple sentences containing the same event and aggregating duration predictions across all samples, the majority vote converges on the correct typical duration unit even if some individual predictions are wrong.
- Core assumption: The distribution of typical durations across context sentences is concentrated around the correct unit, with enough samples to overcome noise.
- Break condition: If the event has genuinely ambiguous typical durations or if context sentences are too homogeneous, voting may not capture true distribution.

### Mechanism 2
- Claim: Bi-modal duration histograms naturally separate episodic from habitual event durations.
- Mechanism: When events can be both episodic and habitual, the voting process creates two peaks in the histogram—one for occasional occurrences and one for repeated patterns.
- Core assumption: Events that can be both episodic and habitual will produce distinct, non-adjacent peaks in the duration distribution when aggregated across multiple contexts.
- Break condition: If episodic and habitual contexts are too similar or if one type dominates, bi-modal pattern may not emerge clearly.

### Mechanism 3
- Claim: Pseudo-labeled data generated from high-confidence typical durations improves downstream model performance more efficiently than weakly supervised approaches.
- Mechanism: The method generates QA-style training data with positive answers using acquired typical duration unit and negative answers using units at least two positions away.
- Core assumption: Acquired typical durations are accurate enough to serve as reliable training signals, and QA format effectively transfers temporal commonsense knowledge.
- Break condition: If pseudo-labeled data contains systematic errors or downstream model cannot effectively utilize temporal commonsense knowledge.

## Foundational Learning

- Concept: Temporal commonsense reasoning
  - Why needed here: The task requires understanding that certain events have typical durations without explicit temporal markers in text
  - Quick check question: Can you explain why "playing music in a film" typically takes minutes rather than years, even though both are technically possible?

- Concept: Semi-supervised learning principles
  - Why needed here: The method relies on automatically generating high-quality training data from unlabeled text, requiring understanding of how to balance draft model predictions with majority voting
  - Quick check question: What's the key difference between this voting-based approach and confidence-based pseudo-label selection methods?

- Concept: Duration unit hierarchy and granularity
  - Why needed here: The method must understand relative relationships between seconds, minutes, hours, days, weeks, months, years, and decades to properly select negative examples and interpret bi-modal patterns
  - Quick check question: Why does the method require negative answers to be "at least two units apart" from positive answers?

## Architecture Onboarding

- Component map: ConceptNet -> Event phrases -> Wikipedia sentences -> Draft model predictions -> Voting histogram -> Pseudo-label generation -> Final model -> MC-TACO evaluation

- Critical path: Collect event phrases from ConceptNet → Sample Wikipedia sentences containing each event → Predict durations using draft model → Build voting histograms and extract typical durations → Generate pseudo-labeled QA data → Fine-tune final model on pseudo data → Evaluate on MC-TACO-duration

- Design tradeoffs:
  - Number of sampled sentences per event (50) vs. computational cost and voting accuracy
  - Wikipedia vs. other sources (potential style bias vs. quality and accessibility)
  - Including/excluding habitual durations in pseudo data (broader coverage vs. higher accuracy for episodic cases)
  - Random vs. informed selection of numerical values in answers (simplicity vs. realism)

- Failure signatures:
  - Flat or noisy voting histograms indicating insufficient samples or model uncertainty
  - Pseudo-labeled data with imbalanced duration unit distribution
  - Final model performance improving on pseudo data but not transferring to MC-TACO
  - Pseudo data accuracy dropping significantly for certain event types

- First 3 experiments:
  1. Vary the number of sampled sentences per event (10, 25, 50, 100) and measure voting accuracy and final model performance
  2. Compare Wikipedia-only sampling to multi-source sampling for pseudo data quality
  3. Test different negative answer generation strategies (2 units apart vs. 1 unit apart vs. random selection) and measure impact on model learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the voting-driven semi-supervised approach perform when using a larger number of events and sentences from diverse sources?
- Basis in paper: [explicit] The authors mention that they plan to experiment with more sentences and more events in future work.
- Why unresolved: The current experiments use only hundreds of events and sentences from Wikipedia, which may not be representative of all possible events and contexts.
- What evidence would resolve it: Conducting experiments with a larger number of events and sentences from various sources would provide insights into the scalability and generalizability of the approach.

### Open Question 2
- Question: Can the numerical value of duration in the pseudo-labeled QA data be improved to better reflect the accurate temporal information of events?
- Basis in paper: [inferred] The authors mention that the current approach randomly chooses numbers in the answers, which does not portray the accurate numerical information of the duration.
- Why unresolved: The accuracy of the numerical values in the pseudo-labeled data could significantly impact the model's ability to understand and predict event durations.
- What evidence would resolve it: Developing a method to incorporate the duration distribution itself into the pseudo-labeled data generation process would improve the accuracy of the numerical values and enhance the model's performance.

### Open Question 3
- Question: How does the voting-driven approach compare to other semi-supervised learning methods in terms of efficiency and effectiveness?
- Basis in paper: [explicit] The authors compare their voting-driven approach to a confidence-based semi-supervised approach and show superior performance.
- Why unresolved: While the comparison with the confidence-based approach is promising, it would be valuable to compare the voting-driven approach to other semi-supervised learning methods to determine its relative strengths and weaknesses.
- What evidence would resolve it: Conducting experiments comparing the voting-driven approach to other semi-supervised learning methods, such as self-training or co-training, would provide insights into its efficiency and effectiveness relative to alternative approaches.

## Limitations

- Method's effectiveness relies heavily on quality of Wikipedia data and assumption that typical durations follow concentrated distribution amenable to majority voting
- Voting mechanism may struggle with genuinely ambiguous events or those with context-dependent durations
- Evaluation is limited to one specific task (MC-TACO-duration) with human evaluation covering only 20 events
- Pseudo-label quality assessment may not be representative of full 1,000-event dataset

## Confidence

- **High Confidence**: Core methodology of using majority voting across sampled contexts to identify typical durations is well-founded and reported performance improvements over baselines are substantial and well-documented
- **Medium Confidence**: Bi-modal histogram interpretation for episodic vs. habitual durations is plausible but requires more empirical validation across diverse event types
- **Low Confidence**: Generalizability of results to other temporal commonsense tasks or domains beyond Wikipedia-based event extraction remains uncertain

## Next Checks

1. Test voting mechanism's robustness by systematically varying number of sampled sentences per event (10, 25, 50, 100) and measuring both voting accuracy and downstream model performance
2. Conduct human evaluation on a larger, more diverse sample of pseudo-labeled events (minimum 100) to better assess method's reliability across different event types and duration units
3. Evaluate method's performance on a different temporal commonsense task or with non-Wikipedia data sources to test generalizability beyond current experimental setup