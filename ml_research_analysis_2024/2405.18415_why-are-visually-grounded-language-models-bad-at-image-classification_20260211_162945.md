---
ver: rpa2
title: Why are Visually-Grounded Language Models Bad at Image Classification?
arxiv_id: '2405.18415'
source_url: https://arxiv.org/abs/2405.18415
tags:
- classification
- vlms
- data
- image
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Visually-grounded language models (VLMs) underperform CLIP in image
  classification due to insufficient classification data during training. The paper
  shows that critical classification information is encoded in VLMs' latent space
  but cannot be effectively decoded without enough training data.
---

# Why are Visually-Grounded Language Models Bad at Image Classification?

## Quick Facts
- arXiv ID: 2405.18415
- Source URL: https://arxiv.org/abs/2405.18415
- Reference count: 40
- VLMs underperform CLIP in image classification due to insufficient classification data during training

## Executive Summary
Visually-grounded language models (VLMs) struggle with image classification despite their multimodal capabilities, performing significantly worse than specialized models like CLIP. This paper demonstrates that the primary issue is not architectural limitations but rather the lack of classification-focused training data. The researchers show that while VLMs encode classification-relevant information in their latent spaces, they cannot effectively decode this information without sufficient exposure to classification datasets during training.

By incorporating additional classification-focused datasets into VLM training, the authors achieve performance parity with state-of-the-art classification models. This improvement extends beyond classification tasks, yielding an 11.8% enhancement on ImageWikiQA, a benchmark that tests both classification and advanced reasoning abilities. The findings suggest that VLMs possess untapped potential for classification tasks that can be unlocked through targeted training data augmentation.

## Method Summary
The researchers conducted a series of experiments to investigate why VLMs underperform in image classification compared to models like CLIP. They systematically added classification-focused datasets to VLM training and measured the impact on both classification accuracy and general multimodal reasoning performance. The key approach involved training VLMs on a combination of standard pretraining data plus additional classification datasets, then evaluating performance on established benchmarks. They used ImageWikiQA as a primary evaluation metric to assess whether improvements in classification ability translated to better overall multimodal reasoning. The experiments were designed to test whether the classification bottleneck could be overcome through data augmentation alone, without architectural modifications.

## Key Results
- VLMs encode classification information in latent space but cannot effectively decode it without sufficient training data
- Adding classification-focused datasets to VLM training achieves parity with state-of-the-art classification models
- Improved classification capability transfers to better general performance, yielding an 11.8% improvement on ImageWikiQA benchmark

## Why This Works (Mechanism)
The mechanism underlying this phenomenon is rooted in the fundamental difference between encoding and decoding capabilities in neural networks. VLMs, trained primarily on general multimodal data, learn to encode rich visual information into their latent representations. However, without sufficient exposure to classification-specific objectives during training, these models lack the ability to effectively map their internal representations to discrete class labels. The latent space contains the necessary information for classification, but the final transformation from representation to prediction remains underdeveloped. By augmenting training with classification datasets, the models learn to bridge this gap, developing the mapping function needed to translate encoded visual features into accurate class predictions.

## Foundational Learning
- **Latent space representation**: The internal vector space where VLMs encode visual information - needed to understand where classification-relevant features are stored
- **Multimodal pretraining**: The process by which VLMs learn to associate visual and textual information - needed to understand the baseline capabilities of these models
- **Classification objectives**: The specific training targets that map inputs to discrete categories - needed to understand what VLMs lack compared to specialized models
- **Knowledge distillation**: The transfer of learned capabilities from one model to another - needed to understand how classification knowledge can be effectively transferred to VLMs

## Architecture Onboarding
- **Component map**: VLM encoder -> latent space representation -> classification head
- **Critical path**: Visual input → encoder → latent representation → classification decoder → output labels
- **Design tradeoffs**: General multimodal capability vs. specialized classification performance
- **Failure signatures**: VLMs produce semantically reasonable but categorically incorrect classifications; latent representations contain visual features but lack explicit category boundaries
- **First experiment**: Train VLM with standard pretraining only, evaluate on classification benchmarks
- **Second experiment**: Add moderate amounts of classification data to VLM training, measure performance changes
- **Third experiment**: Add extensive classification data to VLM training, test for saturation point

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The study focuses primarily on ImageWikiQA as the benchmark for evaluating classification and reasoning capabilities, limiting generalizability
- The paper does not explore whether improved classification performance comes at the cost of other VLM capabilities
- Alternative explanations for classification underperformance, such as architectural limitations, are not thoroughly ruled out

## Confidence
- **High confidence**: VLMs underperform CLIP due to insufficient classification data; adding classification data improves performance
- **Medium confidence**: Classification information exists in VLM latent spaces but cannot be decoded without adequate training
- **Medium confidence**: Improved classification transfers to better general performance on benchmarks like ImageWikiQA

## Next Checks
1. Evaluate the proposed approach on additional image classification benchmarks (e.g., ImageNet, CIFAR-100) to assess generalizability beyond ImageWikiQA.

2. Conduct ablation studies to determine the minimum amount of classification data required to achieve performance parity with CLIP, providing insights into data efficiency.

3. Test whether adding classification data degrades performance on other VLM capabilities by evaluating on language-only tasks and non-classification multimodal reasoning benchmarks.