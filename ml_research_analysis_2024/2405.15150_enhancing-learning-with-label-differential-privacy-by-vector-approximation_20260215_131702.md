---
ver: rpa2
title: Enhancing Learning with Label Differential Privacy by Vector Approximation
arxiv_id: '2405.15150'
source_url: https://arxiv.org/abs/2405.15150
tags: []
core_contribution: This paper addresses the challenge of label differential privacy
  (DP) in multi-class classification, particularly as the number of classes K increases.
  Existing methods degrade significantly in performance due to strong randomization
  required for privacy.
---

# Enhancing Learning with Label Differential Privacy by Vector Approximation

## Quick Facts
- arXiv ID: 2405.15150
- Source URL: https://arxiv.org/abs/2405.15150
- Reference count: 40
- Primary result: Novel vector approximation method for label DP in multi-class classification preserves more information than scalar labels, significantly improving accuracy especially as number of classes K increases

## Executive Summary
This paper addresses the challenge of label differential privacy (DP) in multi-class classification, particularly as the number of classes K increases. Existing methods degrade significantly in performance due to strong randomization required for privacy. The authors propose a vector approximation approach that transforms each label into a random K-dimensional vector whose expectations reflect class conditional probabilities, preserving more information than scalar labels. Theoretical analysis shows that the performance decay with K is only slight under reasonable conditions. Experiments on both synthesized and real datasets (MNIST, Fashion MNIST, CIFAR-10/100) validate the method's effectiveness, showing significant improvements over existing approaches, especially with large K. The method is easy to implement and introduces little additional computational overhead.

## Method Summary
The authors propose a novel vector approximation approach for label differential privacy in multi-class classification. Instead of adding noise directly to scalar labels, the method transforms each label into a K-dimensional random vector where the expected value encodes the true class information. Specifically, for a data point with true label y, the method generates a random vector v where each component v_i has expectation equal to the probability of class i given y. This preserves more information than traditional scalar noise addition while maintaining differential privacy guarantees. The approach can be combined with existing learning algorithms by modifying the loss function to account for the expected label vectors rather than exact labels.

## Key Results
- Significant accuracy improvements over existing label DP methods, especially as K increases
- Performance decay with increasing K is only slight under reasonable conditions
- Validated on MNIST, Fashion MNIST, CIFAR-10/100 datasets with consistent improvements
- Easy to implement with minimal computational overhead compared to existing approaches

## Why This Works (Mechanism)
The method works by preserving more information during the privatization process. Traditional label DP adds noise directly to scalar labels, which loses significant information especially when K is large. By transforming labels into K-dimensional vectors whose expectations encode class probabilities, the method maintains the statistical relationship between data points and their labels. The random vectors are constructed such that E[v|y] = e_y (one-hot vector), but the variance structure can be tuned to provide privacy while preserving discriminative information. This allows downstream learning algorithms to extract more useful signal from the privatized labels compared to scalar noise addition.

## Foundational Learning
- **Differential Privacy (DP)**: A framework for measuring and limiting information leakage about individual data points. Why needed: The entire method builds on DP guarantees to protect label information. Quick check: ε-DP ensures that changing one data point changes output distributions by at most e^ε.
- **Multi-class Classification**: Supervised learning with more than two classes. Why needed: The method specifically addresses challenges that arise when K (number of classes) is large. Quick check: Cross-entropy loss is commonly used for multi-class problems.
- **Randomized Response**: Technique for collecting private statistics by adding controlled randomness. Why needed: The vector approximation method builds on similar principles of adding noise while preserving aggregate information. Quick check: Original technique used coin flips to collect sensitive survey data privately.
- **Information Bottleneck**: Concept that adding noise can reduce information flow while preserving task-relevant information. Why needed: The method trades off privacy (information reduction) with utility (task performance). Quick check: Optimal noise level balances privacy-utility tradeoff.
- **Expected Loss Minimization**: Framework for learning with noisy or randomized labels. Why needed: The method requires modifying learning algorithms to work with expected label vectors. Quick check: Standard approach for handling label noise in training.
- **Privacy Accounting**: Tracking cumulative privacy cost across multiple queries or training steps. Why needed: The method needs to ensure overall privacy guarantees during training. Quick check: Rényi DP or moments accountant commonly used for tracking.

## Architecture Onboarding

Component Map:
Raw Labels -> Vector Transformation -> Private Vectors -> Learning Algorithm -> Trained Model

Critical Path:
The critical path is: Raw Labels → Vector Transformation → Learning Algorithm. The vector transformation step is the key innovation where labels are converted to random vectors, and this directly impacts the quality of the learned model. The learning algorithm must be modified to handle expected vectors rather than exact labels.

Design Tradeoffs:
- Privacy-Utility Tradeoff: Higher privacy (smaller ε) requires more noise, degrading performance
- Dimensionality vs. Information: K-dimensional vectors preserve more information but increase computational cost
- Variance Control: Need to balance between sufficient privacy and maintaining discriminative signal
- Algorithm Compatibility: Method must work with existing learning algorithms with minimal modification

Failure Signatures:
- If privacy budget ε is too small, vectors become too noisy and learning fails
- If class distribution is extremely imbalanced, the vector approximation may not preserve sufficient information
- If the learning algorithm doesn't properly handle expected vectors, performance gains may not materialize
- If K is extremely large (thousands of classes), even this method may struggle with information loss

First Experiments:
1. Test on a simple synthetic dataset with known class probabilities to verify vector expectations
2. Compare accuracy vs. ε on MNIST for different K values to validate theoretical claims
3. Benchmark against baseline label DP methods on CIFAR-10/100 to quantify improvements

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on assumptions about label distributions being balanced or mildly imbalanced
- Experimental evaluation focuses primarily on image classification datasets with relatively clean label structures
- Method's performance on text, tabular, or noisy real-world datasets remains unexplored
- Limited comparison with state-of-the-art DP mechanisms beyond a few baseline methods

## Confidence
- **High Confidence**: The core vector approximation methodology and its implementation feasibility. The mathematical framework for converting labels to random vectors is sound and well-defined.
- **Medium Confidence**: Theoretical claims about performance decay with K. While the analysis provides bounds, empirical validation across diverse dataset characteristics is limited.
- **Medium Confidence**: Comparative performance against existing DP mechanisms. Results show improvement but are benchmarked against a limited set of baselines on standard datasets.

## Next Checks
1. **Dataset Diversity Test**: Evaluate performance on non-image datasets including text classification and tabular data with varying class distributions and label noise levels.
2. **Privacy Budget Analysis**: Conduct experiments across a wider range of epsilon values (ε ∈ [0.1, 10]) to understand the method's effectiveness under stricter privacy constraints.
3. **Real-World Deployment Simulation**: Test the approach in a federated learning setup with heterogeneous clients and potential label distribution skew to assess practical robustness.