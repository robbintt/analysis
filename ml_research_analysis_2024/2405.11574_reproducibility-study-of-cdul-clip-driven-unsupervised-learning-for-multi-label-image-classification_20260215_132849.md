---
ver: rpa2
title: 'Reproducibility Study of CDUL: CLIP-Driven Unsupervised Learning for Multi-Label
  Image Classification'
arxiv_id: '2405.11574'
source_url: https://arxiv.org/abs/2405.11574
tags: []
core_contribution: This reproducibility study investigates CDUL, an unsupervised multi-label
  image classification method using CLIP. The authors claim that aggregating global
  and local image-text similarity vectors from CLIP improves pseudo-label quality,
  and that their gradient-alignment training method iteratively refines both network
  parameters and pseudo labels.
---

# Reproducibility Study of CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification

## Quick Facts
- arXiv ID: 2405.11574
- Source URL: https://arxiv.org/abs/2405.11574
- Reference count: 3
- Authors: Manan Shah; Yash Bhalgat
- Primary Result: Global similarity vectors achieve 85.9% mAP, but aggregation with local vectors at various snippet sizes (64×64 to 3×3) yields lower mAP than global vectors alone; validation mAP of 70.6% vs. original 88.6%

## Executive Summary
This reproducibility study evaluates CDUL, an unsupervised multi-label image classification method that leverages CLIP to generate pseudo-labels. The approach aggregates global and local image-text similarity vectors from CLIP to improve pseudo-label quality, and employs gradient-alignment training to iteratively refine both network parameters and pseudo labels. Experiments on PASCAL VOC 2012 reveal that while global similarity vectors alone achieve reasonable performance (85.9% mAP), the aggregation strategy with local vectors actually decreases performance compared to global vectors alone. Furthermore, the gradient-alignment training method, when modified to update pseudo labels every 10 epochs instead of every epoch, only weakly supports its claimed benefits, with validation mAP reaching only 70.6% compared to the original paper's 88.6%.

## Method Summary
CDUL uses CLIP to generate image-text similarity vectors at both global and local levels. The global approach computes similarity between entire images and text labels, while the local approach divides images into smaller snippets (ranging from 64×64 down to 3×3 pixels) and computes similarity for each snippet. These similarity vectors are aggregated to create pseudo-labels for unsupervised training. The method employs gradient-alignment training that iteratively updates both the network parameters and the pseudo labels. The hypothesis is that aggregating local and global information will provide richer pseudo-label representations, leading to better classification performance than using either approach alone.

## Key Results
- Pseudo labels from global similarity vectors achieve 85.9% mAP on PASCAL VOC 2012
- Aggregation with local vectors at various snippet sizes (64×64 to 3×3) yields lower mAP than global vectors alone
- Validation mAP reaches only 70.6%, substantially below the original paper's reported 88.6%
- Updating pseudo labels every 10 epochs instead of every epoch shows some improvement but remains below original results

## Why This Works (Mechanism)
Unknown: The original paper claims that aggregating local and global similarity vectors creates richer pseudo-label representations that capture both fine-grained and holistic image information. However, our experiments show that this aggregation actually decreases performance compared to global vectors alone, suggesting that the local information may introduce noise or redundancy rather than complementary signal. The gradient-alignment training is hypothesized to refine pseudo labels iteratively, but the modified update frequency (every 10 epochs) only weakly supports the claimed benefits.

## Foundational Learning
Unknown: The method builds upon CLIP's zero-shot capabilities for generating initial pseudo-labels, then uses these as supervision for training a ResNet-101 classifier. The gradient-alignment mechanism is designed to create a feedback loop where the classifier's learned features inform pseudo-label refinement, and refined pseudo-labels improve classifier training. However, the substantial performance gap between our results and the original paper suggests potential issues with implementation details, hyperparameter settings, or data preprocessing that are not fully specified in the original work.

## Architecture Onboarding
- **Component Map**: CLIP model -> Global similarity vector generator -> Local similarity vector generator -> Aggregation module -> Pseudo-label generator -> ResNet-101 classifier -> Gradient-alignment training loop
- **Critical Path**: CLIP features → Similarity computation → Aggregation → Pseudo-label generation → Model training → Parameter updates
- **Design Tradeoffs**: Local snippet analysis provides detailed spatial information but increases computational complexity; aggregation aims to combine strengths but may introduce noise
- **Failure Signatures**: Poor pseudo-label quality when aggregation decreases mAP; computational bottlenecks during local similarity generation
- **First Experiments**: 1) Test global similarity vectors alone; 2) Test various local snippet sizes independently; 3) Compare aggregated vs. non-aggregated approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Aggregation strategy did not improve pseudo-label quality as claimed in original paper
- Computational intensity of local similarity generation (64×64 to 3×3 snippets) limited experimental scope
- Substantial performance gap (70.6% vs 88.6% mAP) suggests implementation or hyperparameter differences

## Confidence
- **Low confidence**: Claims about aggregation strategy improving pseudo-label quality
- **Medium confidence**: Modified gradient-alignment training with reduced update frequency shows some improvement
- **Medium confidence**: Global similarity vectors alone provide reasonable pseudo-labels but cannot validate full methodology

## Next Checks
1. Conduct ablation studies specifically testing the local-global aggregation strategy with identical hyperparameters to the original paper
2. Implement the exact gradient-alignment training schedule from the original paper (updating pseudo labels every epoch)
3. Test additional snippet sizes beyond the 64×64 to 3×3 range to determine if specific resolutions yield better aggregation results