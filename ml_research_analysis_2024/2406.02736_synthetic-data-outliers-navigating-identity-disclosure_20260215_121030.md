---
ver: rpa2
title: 'Synthetic Data Outliers: Navigating Identity Disclosure'
arxiv_id: '2406.02736'
source_url: https://arxiv.org/abs/2406.02736
tags:
- data
- synthetic
- privacy
- outliers
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that outliers in synthetic data generated
  by deep learning models remain vulnerable to re-identification via linkage attacks.
  The authors generated multiple synthetic variants using TVAE, CTGAN, CopulaGAN (deep
  learning) and Independent, PrivBayes, DPsynthpop (differential privacy) models.
---

# Synthetic Data Outliers: Navigating Identity Disclosure

## Quick Facts
- arXiv ID: 2406.02736
- Source URL: https://arxiv.org/abs/2406.02736
- Authors: Carolina Trindade; Luís Antunes; Tânia Carvalho; Nuno Moniz
- Reference count: 35
- Key outcome: Deep learning synthetic data models preserve utility but fail to protect outliers from re-identification via linkage attacks, while differential privacy models offer better privacy at significant utility cost.

## Executive Summary
This study systematically evaluates the privacy risks associated with outliers in synthetic data generated by deep learning models. The authors generate synthetic variants using TVAE, CTGAN, CopulaGAN (deep learning) and Independent, PrivBayes, DPsynthpop (differential privacy) models, then perform linkage attacks to assess re-identification risk. Results show that deep learning models maintain high data utility but fail to protect outliers, with re-identification risk increasing alongside model epoch count. Differential privacy models provide better privacy protection for outliers but at a significant utility cost. The study concludes that synthetic data protection is highly model-dependent and requires careful hyperparameter tuning to balance utility and privacy.

## Method Summary
The study uses the Credit Risk dataset (22,910 records) to generate synthetic data variants using six different models from SDV and DPART tools. Outliers are identified using z-score method with threshold k=3. The authors then perform linkage attacks using the RecordLinkage toolkit to measure re-identification risk by counting possible matches between original and synthetic data. Data utility is evaluated using metrics like BoundaryAdherence, AttributeCoverage, and StatisticSimilarity from SDMetrics. The study systematically varies model hyperparameters including epoch count, batch size, and embedding dimensions to understand their impact on the utility-privacy tradeoff.

## Key Results
- Deep learning models (TVAE, CTGAN, CopulaGAN) preserve data utility well but fail to protect outliers from re-identification via linkage attacks
- Differential privacy models (Independent, PrivBayes, DPsynthpop) offer better privacy protection for outliers but at significant utility cost
- Re-identification risk increases alongside model epoch count, with higher epochs producing synthetic data that more closely matches original outliers
- DPsynthpop model failed to respect attribute value boundaries, producing synthetic data with invalid values that compromised utility evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep learning models preserve data utility but fail to protect outliers from re-identification.
- Mechanism: Deep learning models learn the underlying data distribution and generate synthetic data that closely mimics the original. This high fidelity to the original data allows the model to replicate outlier patterns, making them susceptible to re-identification via linkage attacks.
- Core assumption: The model's ability to capture data distribution correlates with its tendency to replicate outlier characteristics.
- Evidence anchors:
  - [abstract] "deep learning models preserved data utility well but failed to protect outliers"
  - [section 4.1] "Deep learning-based models tend to adhere to the general distribution of the original dataset, focusing on generating more frequent values"
  - [corpus] Weak - related papers focus on synthetic data privacy but don't specifically address outliers in deep learning models
- Break condition: If the model is regularized to avoid overfitting to outlier data points, the re-identification risk would decrease.

### Mechanism 2
- Claim: Differential privacy models offer better privacy protection for outliers but at a significant utility cost.
- Mechanism: Differential privacy adds noise to the data generation process, ensuring that individual data points (including outliers) cannot be easily distinguished. However, this noise degrades the quality of the synthetic data, leading to poor utility.
- Core assumption: The addition of noise to protect individual data points necessarily degrades the overall data quality.
- Evidence anchors:
  - [abstract] "differential privacy models offered better privacy at significant utility cost"
  - [section 4.1] "differential privacy-based models generally generated a higher number of outliers. However, the differential privacy-based models also resulted in poorer data quality"
  - [corpus] Weak - related papers discuss differential privacy but don't specifically address the utility-privacy tradeoff for outliers
- Break condition: If the noise injection is optimized to balance privacy and utility, the tradeoff might be mitigated.

### Mechanism 3
- Claim: The number of epochs in deep learning models influences both data utility and re-identification risk.
- Mechanism: Increasing the number of epochs allows the model to better capture the characteristics of the original data, leading to synthetic data that more closely resemble the original. This similarity increases the re-identification risk, particularly for outliers.
- Core assumption: Model convergence through increased epochs directly correlates with the ability to replicate outlier patterns.
- Evidence anchors:
  - [abstract] "risk increasing alongside model epoch count"
  - [section 5] "as the parameter epochs, which represents the number of iterations the models use to optimize their parameters, increases, the number of unique matches between the original dataset and the synthetic variants also rises"
  - [corpus] Weak - related papers discuss model training but don't specifically address the impact of epochs on outlier re-identification
- Break condition: If the model is trained with early stopping or other regularization techniques, the tradeoff between utility and privacy might be managed.

## Foundational Learning

- Concept: Linkage attacks
  - Why needed here: Understanding how attackers can re-identify individuals in synthetic data is crucial for assessing the privacy risks associated with different synthetic data generation models.
  - Quick check question: How does a linkage attack work, and what information does an attacker need to successfully re-identify an individual?

- Concept: Differential privacy
  - Why needed here: Differential privacy is a mathematical framework for quantifying and protecting individual privacy in data releases. Understanding how it works is essential for evaluating the privacy protection offered by different synthetic data generation models.
  - Quick check question: What is the definition of differential privacy, and how does it provide privacy guarantees?

- Concept: Data utility metrics
  - Why needed here: Assessing the quality of synthetic data is crucial for determining its usefulness in various applications. Understanding different data utility metrics is essential for evaluating the tradeoffs between privacy and utility in synthetic data generation.
  - Quick check question: What are some common data utility metrics, and how do they measure the similarity between synthetic and original data?

## Architecture Onboarding

- Component map: Original dataset -> Synthetic data generation (TVAE/CTGAN/CopulaGAN/Independent/PrivBayes/DPsynthpop) -> Outlier identification (z-score) -> Linkage attack (RecordLinkage) -> Utility evaluation (SDMetrics)

- Critical path:
  1. Load original dataset
  2. Generate synthetic data variants using different models and hyperparameters
  3. Identify outliers in the original dataset
  4. Perform linkage attacks on outliers using synthetic data variants
  5. Evaluate utility of synthetic data variants
  6. Analyze results to assess privacy-utility tradeoff

- Design tradeoffs:
  - Privacy vs. utility: Higher privacy protection (e.g., through differential privacy) often comes at the cost of reduced data utility
  - Model complexity vs. interpretability: More complex models (e.g., deep learning) may offer better utility but are harder to interpret and control
  - Computational cost vs. accuracy: More computationally intensive models may offer better accuracy but require more resources

- Failure signatures:
  - Poor privacy protection: High number of successful re-identifications in linkage attacks
  - Low data utility: Synthetic data fails to capture important statistical properties of the original data
  - Model instability: Synthetic data generation process fails to converge or produces inconsistent results

- First 3 experiments:
  1. Generate synthetic data using TVAE with different epoch counts (e.g., 150, 300, 500) and evaluate utility and privacy
  2. Compare privacy protection of deep learning models (TVAE, CTGAN, CopulaGAN) against differential privacy models (Independent, PrivBayes, DPsynthpop)
  3. Investigate the impact of different hyperparameter settings (e.g., batch_size, embedding_dim) on the privacy-utility tradeoff in deep learning models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of quasi-identifiers (QIs) impact the effectiveness of re-identification attacks on synthetic data outliers?
- Basis in paper: [explicit] The authors hypothesize that certain QIs like person_age, person_income, person_home_ownership, and loan_intent could facilitate easier re-identification by an attacker.
- Why unresolved: The paper only tests a specific set of QIs on one dataset. Different combinations or types of QIs might yield different re-identification success rates.
- What evidence would resolve it: Systematic experiments varying QI combinations and types across multiple datasets to measure changes in re-identification success rates.

### Open Question 2
- Question: What is the optimal balance between synthetic data utility and privacy protection for outliers?
- Basis in paper: [inferred] The authors observe that deep learning models provide higher utility but lower privacy protection for outliers, while differential privacy models offer better privacy at significant utility cost.
- Why unresolved: The paper does not provide a quantitative framework for determining the optimal tradeoff point between utility and privacy for different use cases.
- What evidence would resolve it: Development of a utility-privacy tradeoff curve with concrete metrics and guidelines for different application scenarios.

### Open Question 3
- Question: How do outlier treatment strategies at different synthesis stages affect final data quality and privacy?
- Basis in paper: [inferred] The authors mention the need to investigate the impact of outlier treatment strategies before, during, and after synthesis.
- Why unresolved: The paper does not explore how different outlier handling approaches at various synthesis stages affect the final synthetic data's utility and privacy properties.
- What evidence would resolve it: Comparative analysis of synthetic data quality and privacy metrics using different outlier treatment strategies applied at pre-synthesis, during synthesis, and post-synthesis stages.

## Limitations
- The study's conclusions about differential privacy models' utility-privacy tradeoff rely heavily on results from a single DPsynthpop implementation that failed to respect attribute boundaries
- The linkage attack methodology may overestimate real-world re-identification risk by assuming attackers have knowledge of both numerical and categorical quasi-identifiers
- The study only tested a limited set of models and hyperparameters, potentially missing configurations that could better balance utility and privacy

## Confidence
- High confidence: Core finding that deep learning models can re-identify outliers through linkage attacks
- Medium confidence: Differential privacy models' effectiveness due to DPsynthpop failure case
- Medium confidence: Utility-privacy tradeoff conclusions depend on specific model implementations and parameter choices

## Next Checks
1. Replicate the study using an alternative differential privacy implementation (e.g., DP-TableTools) to verify if poor utility results are model-specific or inherent to differential privacy approach
2. Test additional deep learning model architectures (e.g., Transformer-based models) to determine if outlier re-identification vulnerability is specific to TVAE/CTGAN/CopulaGAN or broader characteristic of deep learning approaches
3. Conduct sensitivity analysis on linkage attack parameters (offset, scale) to understand how variations affect re-identification success rates and validate robustness of privacy risk findings