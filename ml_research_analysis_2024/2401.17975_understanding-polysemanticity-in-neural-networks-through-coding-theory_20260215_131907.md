---
ver: rpa2
title: Understanding polysemanticity in neural networks through coding theory
arxiv_id: '2401.17975'
source_url: https://arxiv.org/abs/2401.17975
tags:
- network
- code
- neural
- dropout
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper applies information-theoretic tools to analyze polysemanticity
  and coding efficiency in neural networks. The authors leverage the eigenspectrum
  of the activation covariance matrix to infer levels of redundancy in the network's
  code.
---

# Understanding polysemanticity in neural networks through coding theory

## Quick Facts
- **arXiv ID**: 2401.17975
- **Source URL**: https://arxiv.org/abs/2401.17975
- **Reference count**: 40
- **Key outcome**: The paper applies information-theoretic tools to analyze polysemanticity and coding efficiency in neural networks.

## Executive Summary
This paper presents an information-theoretic framework for understanding polysemanticity in neural networks by analyzing the eigenspectrum of activation covariance matrices. The authors demonstrate that the power-law decay rate of this eigenspectrum quantifies the redundancy and interpretability of the network's code. They show that random projections can reveal whether a network exhibits smooth or non-differentiable coding phases, providing insights into the network's structure and behavior. The work suggests that polysemantic neurons arise from an optimal encoding trade-off between information density and robustness, and explains why dropout regularization encourages more redundant, interpretable codes.

## Method Summary
The authors train neural networks (ResNet-VAE on CIFAR-10, GPT-2 XL on text) and collect hidden activations. They perform PCA on activation covariance matrices to analyze eigenspectrum decay rates, fitting power-law models to estimate the exponent α. For random projections, they generate moving dot stimuli and project hidden activations onto random bases, computing the average action to assess smoothness. The framework is validated by examining how dropout affects eigenspectrum properties and code interpretability.

## Key Results
- Polysemantic neurons arise from optimal encoding trade-offs between information density and robustness under noise conditions
- The eigenspectrum decay rate of activation covariance matrices quantifies code redundancy and interpretability
- Random projections can distinguish between smooth (interpretable) and non-differentiable (less interpretable) code phases
- Dropout regularization induces more redundant codes with faster eigenspectrum decay, improving interpretability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Polysemantic neurons arise from an optimal encoding trade-off between information density and robustness.
- **Mechanism**: When the number of input features exceeds the number of neurons per layer, the network learns a superposition code that reuses neurons across multiple concepts to maximize information capacity while introducing redundancy for robustness.
- **Core assumption**: The network is trained under noise conditions (e.g., dropout) that incentivize redundancy.
- **Evidence anchors**:
  - [abstract] "We demonstrate that random projections can reveal whether a network exhibits a smooth or non-differentiable phase transition, further enhancing our understanding of the network's structure and behaviour."
  - [section] "intriguing, this work suggests that many aspects of deep neural networks can be explained solely by optimality considerations and without reference to the details of training."
  - [corpus] Weak correlation with existing work; the paper claims novelty in applying information-theoretic tools specifically to polysemanticity.
- **Break condition**: If the input features are fewer than neurons per layer, the network can learn monosemantic coding without superposition, invalidating the necessity of polysemanticity.

### Mechanism 2
- **Claim**: The eigenspectrum decay rate of the activation covariance matrix quantifies the redundancy and interpretability of the code.
- **Mechanism**: A rapidly decaying eigenspectrum (power-law exponent α ≤ -1) indicates high redundancy and a smooth, interpretable code; a slowly decaying spectrum (α > -1) indicates low redundancy and a non-differentiable, less interpretable code.
- **Core assumption**: The activation covariance matrix accurately reflects the redundancy structure of the learned code.
- **Evidence anchors**:
  - [abstract] "We infer levels of redundancy in the network's code by inspecting the eigenspectrum of the activation's covariance matrix."
  - [section] "a largely redundant code has an eigenspectrum with a steep decay since only a small subset of possible configurations is actually relevant to the network's code."
  - [corpus] Limited overlap; the paper cites neuroscience work on biological networks but claims novelty in applying it to artificial networks.
- **Break condition**: If the activation distribution is non-Gaussian or the covariance matrix is ill-conditioned, the eigenspectrum may not accurately reflect code redundancy.

### Mechanism 3
- **Claim**: Random projections of hidden activations can reveal the smoothness of the code and its interpretability.
- **Mechanism**: For a code with slow eigenspectrum decay (α > -1), random projections have unbounded action and are discontinuous; for fast decay (α ≤ -1), projections have bounded action and are smooth.
- **Core assumption**: The smoothness of random projections is directly related to the differentiability of the underlying code.
- **Evidence anchors**:
  - [abstract] "we show how random projections can be employed to determine whether a network exhibits a smooth or non-differentiable phase transition."
  - [section] "When displaying the random projections as curves, we indeed observed that while early layers produce highly discontinuous projections, later layers of the network recover some of the smooth structure of the input."
  - [corpus] No direct evidence; this appears to be a novel contribution of the paper.
- **Break condition**: If the random projection basis is not properly normalized or if the hidden activations are not centered, the action may not accurately reflect code smoothness.

## Foundational Learning

- **Concept**: Information theory and coding theory basics (entropy, mutual information, channel capacity)
  - Why needed here: The paper applies information-theoretic tools to analyze neural network codes and their properties.
  - Quick check question: Can you explain the difference between entropy and mutual information, and how they relate to channel capacity?

- **Concept**: Principal Component Analysis (PCA) and eigenspectrum analysis
  - Why needed here: The paper uses PCA to analyze the eigenspectrum of the activation covariance matrix and infer code redundancy.
  - Quick check question: How does the decay rate of the eigenspectrum relate to the redundancy of a code, and what does a power-law decay imply?

- **Concept**: Dropout and its effect on neural network training
  - Why needed here: The paper studies how dropout affects the learned code and its eigenspectrum properties.
  - Quick check question: How does dropout introduce noise into the network, and why does this incentivize the learning of more robust, redundant codes?

## Architecture Onboarding

- **Component map**: Input layer -> Hidden layers -> Output layer -> PCA module -> Random projection module
- **Critical path**:
  1. Train the neural network on the desired task
  2. Collect hidden activations from the trained network
  3. Perform PCA on the activation covariance matrix
  4. Compute the eigenspectrum decay rate (power-law exponent α)
  5. Generate random projections of hidden activations
  6. Analyze the smoothness of random projections (action)
  7. Correlate eigenspectrum decay and random projection smoothness to infer code properties

- **Design tradeoffs**:
  - Computational cost vs. accuracy: Using a randomized SVD solver for PCA reduces computation but may sacrifice some accuracy compared to full SVD.
  - Dimensionality reduction vs. information preservation: Subsampling the activation dimensions reduces memory usage but may lose some information.
  - Random projection basis vs. interpretability: Using a larger random projection basis may capture more code properties but may be harder to interpret.

- **Failure signatures**:
  - Ill-conditioned activation covariance matrix: May lead to inaccurate eigenspectrum estimation and code property inference.
  - Non-Gaussian activation distribution: May violate assumptions behind the random projection analysis and code smoothness assessment.
  - Insufficient network capacity: May prevent the network from learning an optimal code, leading to suboptimal eigenspectrum properties.

- **First 3 experiments**:
  1. Train an autoencoder on natural images and analyze the eigenspectrum of hidden activations to infer code redundancy.
  2. Apply dropout during training and observe how it affects the eigenspectrum decay rate and random projection smoothness.
  3. Generate a moving dot stimulus and project hidden activations onto random bases to assess the smoothness of the code across network layers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between the power law exponent α and the hardness of decoding neural network codes?
- Basis in paper: [inferred] The paper suggests that as codes become more efficient (approach the Shannon limit), they become less interpretable, and that optimal codes might resist polynomial time decoders.
- Why unresolved: The hardness of average case decoding of neural network codes is unknown, and the paper only provides a theoretical framework for understanding the relationship between code efficiency and interpretability.
- What evidence would resolve it: A systematic study of the relationship between the power law exponent α and the hardness of decoding neural network codes, using both theoretical and empirical approaches.

### Open Question 2
- Question: How does the presence of nonlinearities in the network code affect its interpretability?
- Basis in paper: [explicit] The paper mentions that it is surprising that modern networks allow linear read-out from their hidden layers, given that neural networks were initially developed for nonlinear signal processing.
- Why unresolved: The exact reasons for which linear structure networks converge to is an important open question, and the paper does not provide a definitive answer.
- What evidence would resolve it: A detailed analysis of the role of nonlinearities in the network code, including both theoretical and empirical studies, to understand how they affect interpretability.

### Open Question 3
- Question: What is the impact of network size on the smoothness of random projections and the interpretability of the code?
- Basis in paper: [inferred] The paper suggests that sufficiently large artificial neural networks might learn to strike the same balance between coding capacity and robustness to perturbations as larger animals, and that a systematic study of how the eigenspectrum decay changes as a function of network size might reveal important information on whether we can expect large language models to have smooth random projection.
- Why unresolved: The relationship between network size and the smoothness of random projections is not well understood, and the paper does not provide a definitive answer.
- What evidence would resolve it: A systematic study of how the eigenspectrum decay changes as a function of network size, including both theoretical and empirical approaches, to understand the impact of network size on the smoothness of random projections and the interpretability of the code.

## Limitations

- The framework assumes activation distributions follow specific statistical properties that may not hold for all architectures or datasets
- The relationship between eigenspectrum analysis and code redundancy may break down in highly non-linear regimes
- The claim that dropout specifically induces polysemanticity through noise-robust coding is plausible but not definitively proven

## Confidence

- **High confidence**: The core mechanism linking eigenspectrum decay to code redundancy is well-established in information theory and supported by multiple experiments
- **Medium confidence**: The relationship between random projection smoothness and code differentiability is theoretically sound but relies on assumptions about activation distributions that may not always hold
- **Low confidence**: The claim that dropout specifically induces polysemanticity through noise-robust coding is plausible but not definitively proven; other mechanisms could contribute

## Next Checks

1. Test the framework on networks trained without dropout to determine if polysemanticity emerges from other factors beyond noise-robust coding
2. Apply the eigenspectrum and random projection analysis to convolutional layers specifically, as the paper focuses primarily on fully-connected layers
3. Validate the framework across different activation functions (ReLU, GELU, etc.) to ensure the results aren't specific to a particular non-linearity