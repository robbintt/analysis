---
ver: rpa2
title: 'Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach to
  Model Interpretability and Precision'
arxiv_id: '2402.16008'
source_url: https://arxiv.org/abs/2402.16008
tags:
- brain
- which
- predictions
- data
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a novel approach for improving interpretability\
  \ and precision in Alzheimer\u2019s disease detection using Jacobian saliency maps\
  \ (JSM) incorporated into the loss function as a self-debugging mechanism. The method\
  \ leverages JSM to identify brain deformations and guide the model to focus on disease-relevant\
  \ regions during training, avoiding Clever Hans behavior."
---

# Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach to Model Interpretability and Precision

## Quick Facts
- **arXiv ID**: 2402.16008
- **Source URL**: https://arxiv.org/abs/2402.16008
- **Reference count**: 31
- **Primary result**: JSM-guided JAL loss improves AD detection accuracy by up to 10% while enhancing interpretability

## Executive Summary
This paper introduces a novel approach for improving interpretability and precision in Alzheimer's disease detection using Jacobian saliency maps (JSM) incorporated into the loss function as a self-debugging mechanism. The method leverages JSM to identify brain deformations and guide the model to focus on disease-relevant regions during training, avoiding Clever Hans behavior. Experiments on the OASIS-3 dataset with multimodal MRI and CT scans demonstrate significant improvements in accuracy (up to 10%) and interpretability compared to state-of-the-art methods.

## Method Summary
The approach combines Jacobian saliency maps with a CNN-based classifier for Alzheimer's disease detection using multimodal MRI and CT scans. JSM is computed from non-linear image registration to capture brain deformations, then incorporated into a Jacobian-augmented loss function (JAL) that guides model training. The method uses late or early fusion of MRI and CT modalities, with the JAL loss penalizing reliance on irrelevant features while emphasizing deformation patterns. The CNN architecture includes two convolutional layers with batch normalization, dropout, and max pooling, trained with ADASYN oversampling to address class imbalance.

## Key Results
- JSM-guided JAL loss achieves up to 10% improvement in accuracy compared to baseline methods
- The approach enables fine-grained classification across four AD stages (CN, MCI, mild AD, moderate to severe AD)
- Visualization confirms alignment between model gradients and JSM-indicated deformations, enhancing trust in predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JSM provides interpretable guidance by capturing brain deformations that correlate with AD pathology.
- Mechanism: JSM is computed from non-linear image registration that aligns brain scans to a standard template, then measures volumetric changes at the voxel level through Jacobian determinants. These deformations highlight areas of expansion or compression relative to healthy brain structure.
- Core assumption: Volumetric changes in brain tissue detected by Jacobian determinants are reliable indicators of AD-related pathology that can guide model learning.
- Evidence anchors:
  - [section] "By breaking down an input image into distinct regions and measuring how they are transformed, JSM provides precise insight into the complexities of feature attribution and thus model explainability."
  - [section] "This analysis aids in pinpointing statistically significant anatomical variations across diverse populations, such as distinguishing between AD patients and healthy elderly individuals."
- Break condition: If Jacobian determinants do not correlate with actual AD pathology or if registration introduces artifacts that mislead the model.

### Mechanism 2
- Claim: Incorporating JSM into the loss function (JAL) prevents Clever Hans behavior by penalizing reliance on irrelevant cues.
- Mechanism: The JAL loss function adds a regularization term that down-weights gradients in regions where JSM indicates no volumetric change (det(J)=1), forcing the model to focus on deformation patterns rather than spurious correlations.
- Core assumption: The model can be effectively guided away from irrelevant features through gradient-based regularization during training.
- Evidence anchors:
  - [abstract] "This approach ensures the model predictions to be based on genuine patterns and cues, and renders the model decision-making process to be more interpretable through a during-modeling methodology."
  - [section] "Our approach seeks to harness the guidance provided by the JSM, and by incorporating such insights from JSM, we aim to develop lighter convolutional neural networks (CNNs) that alleviate computational burdens without compromising model performance."
- Break condition: If the regularization term becomes too strong and prevents the model from learning any useful patterns, or if the weighting scheme doesn't effectively distinguish relevant from irrelevant features.

### Mechanism 3
- Claim: Multimodal fusion with JSM guidance improves classification accuracy by leveraging complementary information from different imaging modalities.
- Mechanism: Early and late fusion techniques combine MRI and CT data, with JSM computed separately for each modality (late fusion) or jointly (early fusion), allowing the model to learn from both structural and volumetric information.
- Core assumption: MRI and CT provide complementary information about brain pathology that, when combined with JSM guidance, leads to better classification performance.
- Evidence anchors:
  - [section] "Consideration of PET data was deferred due to its temporal nature, making it more suitable for future spatiotemporal analyses."
  - [section] "Our approach leverages multiple modalities for interpretable AD diagnosis and achieves enhanced performance."
- Break condition: If the modalities are not truly complementary or if fusion introduces noise that degrades performance.

## Foundational Learning

- Concept: Jacobian determinants and their interpretation
  - Why needed here: Understanding how volumetric changes are quantified through Jacobian determinants is essential for interpreting JSM and its role in guiding model learning.
  - Quick check question: What does a Jacobian determinant value of 1.2 indicate about volumetric change at a voxel?

- Concept: Non-linear image registration and its purpose
  - Why needed here: The JSM computation relies on accurate registration to a standard template, which is fundamental to identifying brain deformations.
  - Quick check question: Why is it necessary to register brain scans to a standard template before computing JSM?

- Concept: Loss function regularization and its impact on training
  - Why needed here: The JAL approach modifies the standard loss function to incorporate JSM guidance, requiring understanding of how regularization affects model training.
  - Quick check question: How does adding a regularization term to the loss function influence the optimization process?

## Architecture Onboarding

- Component map: Input preprocessing (MRI/CT registration, bias correction, BET) → JSM computation (ANTs-based registration and Jacobian calculation) → Multimodal fusion (early/late) → CNN backbone (2 conv layers, batch norm, dropout, max pooling) → JAL loss computation → Softmax classification
- Critical path: JSM computation → JAL loss integration → Backpropagation → Model weight updates
- Design tradeoffs: Lighter CNN architecture vs. performance, separate JSM computation per modality vs. computational overhead, choice of fusion strategy (early vs. late)
- Failure signatures: Model performance doesn't improve with JSM guidance, JSM patterns don't align with expected pathology, multimodal fusion degrades rather than improves accuracy
- First 3 experiments:
  1. Train baseline CNN without JSM guidance to establish performance baseline
  2. Add JSM computation but use standard loss function to isolate JSM's impact on interpretability
  3. Implement JAL loss with single modality to verify self-debugging mechanism before adding multimodal complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different choices of feature_weight and debug_weight in Equation (9) affect the performance of the Jacobian-augmented loss function?
- Basis in paper: [explicit] The paper mentions these are hyperparameters that can be tuned but only provides specific values used (0.8 for feature_weight, 0.2 for debug_weight).
- Why unresolved: The paper does not provide an ablation study or sensitivity analysis on how different values of these weights impact model performance.
- What evidence would resolve it: A systematic study varying these weight values and measuring their effect on accuracy, sensitivity, specificity, and interpretability metrics would clarify their impact.

### Open Question 2
- Question: How does the Jacobian-augmented loss function (JAL) perform when applied to other medical imaging tasks beyond Alzheimer's disease diagnosis?
- Basis in paper: [inferred] The paper demonstrates effectiveness on AD diagnosis but mentions the approach is generalizable and modality-agnostic without testing on other conditions.
- Why unresolved: The method is only validated on a single medical task (AD classification), leaving its broader applicability unknown.
- What evidence would resolve it: Applying JAL to other medical imaging tasks (e.g., cancer detection, cardiovascular disease) and comparing performance to state-of-the-art methods would establish its generalizability.

### Open Question 3
- Question: How does the Jacobian-augmented loss function compare to other post-hoc interpretability methods in terms of revealing genuine patterns versus Clever Hans behavior?
- Basis in paper: [explicit] The paper positions JAL as superior to post-hoc methods that may generate unjustified counterfactual examples, but does not directly compare them.
- Why unresolved: While the paper claims JAL provides more trustworthy explanations, it does not benchmark against specific post-hoc XAI techniques on the same tasks.
- What evidence would resolve it: A head-to-head comparison of JAL against established post-hoc methods (e.g., SHAP, LIME) on the same dataset, evaluating both accuracy and the quality of explanations in terms of alignment with domain knowledge.

## Limitations
- Reliance on Jacobian determinants assumes volumetric changes directly correlate with AD pathology without comprehensive validation
- Computational complexity of JSM computation, particularly for multimodal fusion, may limit real-world applicability
- Evaluation focuses primarily on OASIS-3 dataset without extensive validation on independent cohorts or demographic bias analysis

## Confidence

- **High Confidence**: Claims about improved accuracy metrics (10% improvement) and the basic mechanism of JSM computation from non-linear registration
- **Medium Confidence**: Claims about interpretability improvements and prevention of Clever Hans behavior, as these depend on subjective evaluation of visualization results
- **Low Confidence**: Claims about generalizability across different medical imaging tasks, given limited testing beyond the AD detection use case

## Next Checks

1. **External Validation**: Test the JAL approach on an independent Alzheimer's disease dataset with different acquisition protocols to verify robustness across imaging conditions
2. **Mechanistic Validation**: Correlate JSM-identified regions with established AD biomarkers (e.g., hippocampal atrophy, ventricular enlargement) through neuroanatomical analysis
3. **Computational Efficiency Analysis**: Benchmark JSM computation time and memory requirements across different hardware configurations to establish practical deployment constraints