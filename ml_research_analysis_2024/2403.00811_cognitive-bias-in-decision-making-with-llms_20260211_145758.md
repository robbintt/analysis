---
ver: rpa2
title: Cognitive Bias in Decision-Making with LLMs
arxiv_id: '2403.00811'
source_url: https://arxiv.org/abs/2403.00811
tags:
- bias
- student
- cognitive
- biases
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce BIAS BUSTER, a framework for quantifying and mitigating
  cognitive bias in large language models during high-stakes decision-making. We create
  a dataset of 13,465 prompts to evaluate models for patterns similar to human cognitive
  bias (anchoring, status quo, framing, group attribution, primacy).
---

# Cognitive Bias in Decision-Making with LLMs

## Quick Facts
- arXiv ID: 2403.00811
- Source URL: https://arxiv.org/abs/2403.00811
- Authors: Jessica Echterhoff; Yao Liu; Abeer Alessa; Julian McAuley; Zexue He
- Reference count: 29
- Primary result: Introduced BIAS BUSTER framework that successfully reduces cognitive bias in LLMs through autonomous prompt rewriting, particularly effective for high-capacity models

## Executive Summary
This paper introduces BIAS BUSTER, a novel framework for quantifying and mitigating cognitive bias in large language models during high-stakes decision-making. The authors create a comprehensive dataset of 13,465 prompts to evaluate models across five cognitive bias dimensions: anchoring, status quo, framing, group attribution, and primacy. Through systematic testing of various debiasing strategies, they develop a self-help approach where models autonomously rewrite their own prompts to remove bias-inducing elements, achieving significant reductions in biased outputs without requiring manual example crafting.

## Method Summary
The authors developed BIAS BUSTER through a multi-stage process. First, they created a dataset of 13,465 prompts designed to test for specific cognitive bias patterns by varying contextual elements while keeping core decision criteria constant. They evaluated multiple large language models across these prompts to establish baseline bias patterns. The self-help debiasing strategy was then implemented, where models would rewrite incoming prompts to neutralize bias-inducing language before processing. This approach leverages the model's own understanding of bias to transform prompts into more neutral formulations. The framework measures effectiveness through decision consistency scores, comparing how often models make the same decisions across differently framed but logically equivalent prompts.

## Key Results
- Models exhibit decision inconsistencies across five cognitive bias dimensions similar to human patterns
- Self-help approach reduces biased prompts to near zero for framing and group attribution biases in high-capacity models
- The method improves decision consistency without requiring manual crafting of debiasing examples

## Why This Works (Mechanism)
The self-help approach works by leveraging the model's inherent understanding of language and context to identify and neutralize bias-inducing elements in prompts before processing. When a model encounters a prompt that may trigger cognitive bias, it applies a rewriting transformation that preserves the core decision requirements while removing framing, attribution, or other bias-triggering language. This creates a form of metacognitive awareness where the model can recognize and correct for its own susceptibility to bias patterns, essentially applying its linguistic reasoning capabilities to itself.

## Foundational Learning
- Cognitive bias patterns: Understanding human-like decision biases is essential to create test prompts that reveal similar patterns in models
  - Why needed: Without knowing what biases to test for, cannot measure or mitigate them
  - Quick check: Verify that prompt variations consistently trigger expected bias responses across multiple model runs

- Prompt rewriting mechanics: The self-help method requires models to understand how to transform biased language while preserving decision intent
  - Why needed: Core mechanism for autonomous bias mitigation without external examples
  - Quick check: Ensure rewritten prompts maintain original task requirements while removing bias indicators

- Decision consistency metrics: Measuring consistency across equivalent but differently framed prompts is crucial for quantifying bias
  - Why needed: Provides objective measure of whether debiasing is effective
  - Quick check: Test that identical logical prompts with different surface forms produce consistent outputs when debiased

## Architecture Onboarding

Component map: Input Prompt -> Bias Detection -> Prompt Rewriting -> Debiased Prompt -> Model Processing -> Decision Output

Critical path: The self-help mechanism operates as an inline preprocessing step where incoming prompts are intercepted, analyzed for bias indicators, and rewritten before reaching the main model inference. This creates a self-contained feedback loop where the model's own reasoning capabilities are applied to its input stream.

Design tradeoffs: The approach trades computational overhead for bias reduction, as each prompt requires additional processing for rewriting. However, this is offset by eliminating the need for extensive fine-tuning datasets or manual intervention. The method also assumes the model has sufficient capability to recognize and rewrite biased language, making it less effective for smaller models.

Failure signatures: The system may fail when prompts contain subtle or complex bias that the model cannot fully recognize, or when rewriting removes important contextual information needed for accurate decisions. Over-aggressive rewriting could also strip away legitimate framing that provides necessary context rather than introducing bias.

Three first experiments:
1. Test the self-help mechanism on prompts known to trigger anchoring bias with varying levels of complexity
2. Compare decision consistency before and after rewriting for framing bias across different model sizes
3. Evaluate whether rewritten prompts maintain task completion quality while reducing bias indicators

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- The effectiveness of self-help strongly depends on model capacity, potentially limiting applicability to smaller deployed models
- The framework's reliance on prompt rewriting may not generalize well across all task types and domains
- While bias reduction is demonstrated, the practical impact on actual decision quality and real-world outcomes requires further validation

## Confidence
- Models exhibit human-like cognitive bias patterns: High confidence
- Self-help approach effectiveness: Medium confidence
- Debiasing without manual example crafting: Medium confidence

## Next Checks
1. Test BIAS BUSTER's effectiveness on smaller, resource-constrained models that are more commonly deployed in practice to assess scalability limitations
2. Conduct longitudinal studies to evaluate whether debiased models maintain consistency in decision-making over extended periods and across evolving contexts
3. Implement real-world case studies where debiased models make consequential decisions, comparing outcomes against both biased models and human decision-makers to assess practical impact beyond prompt consistency