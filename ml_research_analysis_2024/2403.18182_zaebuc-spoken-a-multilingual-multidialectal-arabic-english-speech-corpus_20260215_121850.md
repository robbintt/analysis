---
ver: rpa2
title: 'ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech Corpus'
arxiv_id: '2403.18182'
source_url: https://arxiv.org/abs/2403.18182
tags:
- arabic
- corpus
- english
- speech
- code-switching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ZAEBUC-Spoken, a multilingual multidialectal
  Arabic-English speech corpus collected through Zoom meetings. The corpus comprises
  twelve hours of recordings featuring Arabic speakers from six nationalities discussing
  various topics in both Arabic and English, with code-switching between languages
  and dialects.
---

# ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech Corpus

## Quick Facts
- **arXiv ID**: 2403.18182
- **Source URL**: https://arxiv.org/abs/2403.18182
- **Reference count**: 0
- **Primary result**: Presents a challenging multidialectal Arabic-English speech corpus for ASR with code-switching and morphological annotations

## Executive Summary
This paper introduces ZAEBUC-Spoken, a novel speech corpus comprising twelve hours of recordings featuring Arabic speakers from six nationalities discussing various topics in both Arabic and English. The corpus captures spontaneous conversational speech with code-switching between languages and Arabic dialects, making it particularly challenging for automatic speech recognition systems. It includes manual transcriptions, dialectness level annotations for Arabic variants, and automatic morphological annotations including tokenization, lemmatization, and POS tagging. The corpus represents a significant resource for developing and evaluating ASR systems that must handle linguistic complexity in real-world multilingual and multidialectal contexts.

## Method Summary
The corpus was collected through 15 Zoom meetings with Emirati students and interlocutors from different nationalities discussing predefined topics in Arabic and English. Recordings were made with separate audio tracks for each speaker to facilitate transcription. Manual transcriptions were created following detailed guidelines that address general transcription rules, conversational speech phenomena (repetitions, corrections, incomplete words), code-switching, and orthography for both languages. Dialectness levels were annotated for the portion containing code-switching between Arabic variants using established guidelines. Automatic morphological annotations were generated using CAMeL Tools for both Arabic and English, including tokenization, lemmatization, and POS tagging.

## Key Results
- ZAEBUC-Spoken contains 12 hours of multidialectal Arabic-English speech data with code-switching
- Corpus includes manual transcriptions and dialectness level annotations for Arabic variant mixing
- Automatic morphological annotations provide tokenization, lemmatization, and POS tagging for both languages
- The corpus presents a challenging ASR dataset due to spontaneous conversational nature and linguistic complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The corpus's multidialectal structure provides complementary linguistic diversity that improves model generalization across Arabic variants.
- Mechanism: By including MSA, Gulf Arabic, and Egyptian Arabic speakers in the same corpus, models trained on this data are exposed to multiple phonological, morphological, and lexical patterns simultaneously, preventing overfitting to a single dialect.
- Core assumption: Linguistic variation between Arabic dialects is sufficient to require explicit representation in training data for robust ASR performance.
- Evidence anchors:
  - [abstract] "The corpus presents a challenging set for automatic speech recognition (ASR), including two languages (Arabic and English) with Arabic spoken in multiple variants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic)"
  - [section] "Unlike the previously mentioned corpora where such annotations were provided for textual data, spontaneous speech data introduces challenges to POS tagging, as the normal flow of sentences may be broken due to repetitions and/or corrections."
- Break condition: If dialectal differences are minimal or if models can generalize effectively from monolingual dialect data, the added complexity of multidialectal data may not improve performance.

### Mechanism 2
- Claim: Code-switching between Arabic variants and English creates a challenging training environment that forces ASR models to learn language boundary detection.
- Mechanism: The corpus contains both Arabic-English code-switching and MSA-dialectal Arabic code-switching, requiring models to identify language boundaries at both script and lexical levels, improving their ability to handle real-world bilingual speech.
- Core assumption: Code-switching patterns are sufficiently systematic to be learnable by ASR models when provided with appropriate training data.
- Evidence anchors:
  - [abstract] "Adding to the complexity of the corpus, there is also code-switching between these languages and dialects."
  - [section] "The corpus also contains multiple Arabic variants (MSA, Gulf Arabic, and Egyptian Arabic), accented English, as well as code-switching across the languages and dialects."
- Break condition: If code-switching patterns are too unpredictable or if language boundary detection requires explicit linguistic rules rather than statistical learning.

### Mechanism 3
- Claim: The spontaneous conversational nature of the corpus provides realistic speech patterns that improve ASR robustness to disfluencies and non-standard speech.
- Mechanism: By collecting data through Zoom meetings with role-playing scenarios, the corpus captures natural speech phenomena like repetitions, corrections, and incomplete words that are common in real conversations but rare in read speech corpora.
- Core assumption: ASR models trained on spontaneous speech will generalize better to real-world applications than those trained on read speech.
- Evidence anchors:
  - [abstract] "The corpus includes manual transcriptions of the recordings and dialectness level annotations for the portion containing code-switching between Arabic variants"
  - [section] "Given the spontaneous nature of the corpus, it is common to find disfluencies that break the flow of sentences, including repetitions, corrections, and changing course mid-sentence."
- Break condition: If the specific disfluencies captured in this corpus are not representative of the target application domain.

## Foundational Learning

- Concept: Speech corpus annotation standards and transcription guidelines
  - Why needed here: The corpus relies on specific transcription guidelines for handling conversational speech, code-switching, and Arabic orthography
  - Quick check question: Can you explain the difference between [GR-unclear] and [GR-background] annotation types?

- Concept: Arabic dialect classification and dialectness level annotation
  - Why needed here: The corpus includes dialectness level annotations following established guidelines for measuring Arabic dialectness
  - Quick check question: What distinguishes L2 (MSA-dialectal code-switching) from L3 (dialect with MSA incursions) in the annotation scheme?

- Concept: Morphological analysis of code-switched constructs
- Why needed here: The corpus includes automatic morphological annotations including tokenization, lemmatization, and POS tagging for both Arabic and English words
- Quick check question: How does the corpus handle morphological code-switching (MCS) constructs like definite article attachment to English words?

## Architecture Onboarding

- Component map: Data collection (Zoom recordings) → Transcription (manual annotation with guidelines) → Annotation (dialectness levels + morphological features) → ASR model training and evaluation
- Critical path: Data collection → Transcription quality control → Morphological annotation → Model training → Evaluation
- Design tradeoffs: Manual transcription provides high quality but limits scalability; automatic morphological annotation is faster but may have errors that require manual correction
- Failure signatures: Poor transcription quality → Incorrect annotations → Model performance degradation; Incomplete code-switching representation → Biased model behavior
- First 3 experiments:
  1. Baseline ASR model training on ZAEBUC-Spoken corpus and evaluation on held-out test set
  2. Ablation study removing code-switching data to measure its impact on overall performance
  3. Cross-dialect evaluation testing model performance on dialects not seen during training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the presence of multiple dialects (Gulf and Egyptian) in the same corpus affect the performance of ASR systems compared to monolingual dialectal datasets?
- Basis in paper: [explicit] The paper states ZAEBUC-Spoken presents challenges due to multiple Arabic variants (MSA, Gulf Arabic, and Egyptian Arabic) alongside English and code-switching, but does not report ASR performance results.
- Why unresolved: The corpus is newly introduced and no ASR evaluation experiments are reported.
- What evidence would resolve it: ASR system performance metrics (WER, CER) on ZAEBUC-Spoken compared to single-dialect corpora.

### Open Question 2
- Question: What is the impact of code-switching density on the accuracy of automatic morphological annotation in the corpus?
- Basis in paper: [inferred] The paper provides morphological annotation but notes that spontaneous speech data introduces challenges to POS tagging, and mentions code-switching occurs frequently in the corpus.
- Why unresolved: The paper provides automatic annotations but states manual revisions are planned and does not report evaluation of annotation quality across different code-switching densities.
- What evidence would resolve it: Correlation analysis between code-switching density (CMI, SPF) and morphological annotation accuracy metrics.

### Open Question 3
- Question: How do the dialectness level annotations (L0-L4) correlate with specific linguistic features in the transcribed utterances?
- Basis in paper: [explicit] The paper provides dialectness level annotations but does not analyze their linguistic correlates.
- Why unresolved: While the paper presents the annotation framework and statistics, it does not explore what linguistic features distinguish different dialectness levels.
- What evidence would resolve it: Statistical analysis of feature distributions (phonological, morphological, lexical) across L0-L4 categories.

## Limitations
- The paper does not report actual ASR model training and evaluation results on the corpus
- No empirical validation of the claimed benefits of multidialectal data versus monodialectal alternatives
- Automatic morphological annotations are provided but manual revision is planned and quality is not yet reported

## Confidence
- Medium: The theoretical mechanisms for why multidialectal data should help (preventing overfitting, learning language boundaries, handling disfluencies) are plausible and supported by the literature on dialectal variation and code-switching, but without quantitative results showing improved performance on the proposed corpus versus baseline alternatives, these remain hypotheses rather than demonstrated outcomes.

## Next Checks
1. Train and evaluate baseline ASR models on ZAEBUC-Spoken corpus, comparing performance across different Arabic dialects and English to quantify the impact of multidialectal exposure

2. Conduct ablation studies by training separate models on monodialectal subsets (MSA-only, Gulf-only, Egyptian-only) versus the full multidialectal corpus to measure generalization benefits

3. Test cross-dialect generalization by evaluating models trained on one dialect combination on test data from unseen dialect combinations to validate the multidialectal advantage claims