---
ver: rpa2
title: 'GenX: Mastering Code and Test Generation with Execution Feedback'
arxiv_id: '2412.13464'
source_url: https://arxiv.org/abs/2412.13464
tags:
- code
- test
- generation
- cases
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving code and test generation
  in scenarios where test cases are limited or unavailable. The core idea is to concurrently
  train code and test generation models, using execution feedback to iteratively refine
  both.
---

# GenX: Mastering Code and Test Generation with Execution Feedback

## Quick Facts
- arXiv ID: 2412.13464
- Source URL: https://arxiv.org/abs/2412.13464
- Reference count: 5
- Primary result: GenX outperforms models trained on original dataset with improved pass@1, pass@10 for code generation and higher pass rate for test generation

## Executive Summary
GenX addresses the challenge of improving code and test generation in scenarios with limited or unavailable test cases. The approach uses execution feedback to concurrently train code and test generation models, iteratively refining both through mutual evaluation. By augmenting datasets with generated tests and code solutions filtered through execution, the framework achieves better performance metrics than models trained on original data alone.

## Method Summary
GenX employs an iterative self-training approach where code and test generation models are trained simultaneously using execution feedback. The process starts with a small dataset (APPS-), augments test cases using ground truth code to correct model predictions, then augments code solutions using rejection sampling with the expanded test set. A dual execution scoring function ranks generated code and tests based on their mutual passing status, enabling selection of high-quality candidates without ground truth labels.

## Key Results
- GenX achieves improved pass@1 and pass@10 metrics for code generation compared to models trained on original dataset
- Test generation performance shows higher pass rate and pass num metrics with the proposed approach
- The dual execution scoring function effectively ranks high-quality code and tests based on mutual execution results

## Why This Works (Mechanism)

### Mechanism 1
Execution feedback allows mutual evaluation between code and test models, improving both through iterative refinement. Code and test generation models are trained simultaneously, with test predictions corrected using ground truth code outputs while code model is trained using only correct solutions filtered through test cases. Test outputs are deterministic given problem definition and inputs, independent of code implementation details.

### Mechanism 2
Iterative data augmentation with rejection sampling increases correct code solutions and test cases. Starting with small dataset, test cases are augmented using ground truth code, then code solutions are augmented using rejection sampling with expanded test set. Process repeats, gradually increasing dataset size and model capability.

### Mechanism 3
The dual execution scoring function ranks generated code and tests based on mutual passing status without requiring ground truth. A matrix records pass/fail results between generated code solutions and test cases, with scores iteratively computed: code scores weighted by test scores they pass, and test scores weighted by code scores they pass, converging to ranking based on mutual execution agreement.

## Foundational Learning

- Concept: Execution feedback in code generation
  - Why needed here: This work relies on using execution results (pass/fail) to guide both test generation and code selection, distinguishing it from methods that only use textual error messages or static analysis
  - Quick check question: What type of feedback is used to judge code correctness in this approach, and how does it differ from error message-based methods?

- Concept: Rejection sampling
  - Why needed here: Used to filter out incorrect code solutions during augmentation phase, ensuring only correct solutions are added to training dataset
  - Quick check question: How does rejection sampling help prevent false positives when augmenting code solutions with limited test cases?

- Concept: Iterative self-training
  - Why needed here: The framework improves both code and test generation models through multiple rounds of training on augmented data, similar to self-training but applied to two related tasks simultaneously
  - Quick check question: What are the two stages of data augmentation in this approach, and how do they build upon each other?

## Architecture Onboarding

- Component map:
  - Test Generation Model -> Execution Engine -> Test Output
  - Code Generation Model -> Execution Engine -> Pass/Fail Results
  - Execution Engine -> Scoring Module -> Ranked Candidates
  - Scoring Module -> Data Augmentation Pipeline -> Augmented Dataset

- Critical path:
  1. Train initial models on APPS- dataset
  2. Augment test cases using ground truth code
  3. Train test generation model on augmented data
  4. Generate code solutions with rejection sampling using expanded test set
  5. Train code generation model on augmented data
  6. Repeat steps 2-5 for multiple iterations
  7. During inference, generate code and test candidates
  8. Apply dual execution scoring to rank candidates

- Design tradeoffs:
  - Sampling temperature: Higher temperature during data augmentation increases diversity but may introduce more errors
  - Number of iterations: More iterations can improve models but increase computational cost
  - Test case selection: Random selection of up to 10 test cases per problem balances coverage and efficiency

- Failure signatures:
  - Test generation fails: Pass rate on generated tests is low; model may not be learning problem semantics
  - Code generation fails: Pass@1 is low; rejection sampling may be too aggressive or test cases insufficient
  - Scoring fails: Generated rankings do not correlate with actual code quality; mutual execution matrix may be too sparse

- First 3 experiments:
  1. Verify that ground truth code can correct test generation errors by running test inputs through known solutions
  2. Test rejection sampling with a small set of generated code solutions and known test cases to ensure only correct solutions pass
  3. Validate the dual execution scoring function on a small set of generated code and tests to confirm convergence and meaningful rankings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dual execution feedback approach scale to larger codebases and more complex programming tasks?
- Basis in paper: The paper discusses the effectiveness of the dual execution feedback approach on the APPS dataset, but does not explore its scalability to larger codebases or more complex tasks
- Why unresolved: The experiments were conducted on a specific dataset with a limited number of problems and test cases
- What evidence would resolve it: Conducting experiments on larger and more diverse datasets, such as those containing real-world software projects, would provide insights into the scalability of the approach

### Open Question 2
- Question: How does the quality of the generated tests impact the overall performance of the code generation model?
- Basis in paper: The paper emphasizes the importance of test generation in the dual execution feedback approach, but does not explicitly explore the relationship between test quality and code generation performance
- Why unresolved: The paper focuses on the effectiveness of the approach in generating tests and code, but does not investigate how the quality of the generated tests affects the code generation model's performance
- What evidence would resolve it: Analyzing the correlation between test quality metrics (e.g., test coverage, mutation score) and code generation performance metrics (e.g., pass@1, pass@10) would provide insights into this relationship

### Open Question 3
- Question: How does the proposed scoring function compare to other evaluation metrics for code and test generation?
- Basis in paper: The paper introduces a new scoring function for ranking generated code and tests based on their mutual execution results, but does not compare this scoring function to other existing evaluation metrics
- Why unresolved: The paper demonstrates the effectiveness of the scoring function in ranking generated code and tests, but does not explore how it compares to other evaluation metrics used in the literature
- What evidence would resolve it: Conducting experiments comparing the proposed scoring function to other evaluation metrics (e.g., BLEU score, edit distance) would provide insights into its relative effectiveness

### Open Question 4
- Question: How does the dual execution feedback approach handle cases where the generated code or tests have errors or inconsistencies?
- Basis in paper: The paper discusses the use of execution feedback to refine and enhance the performance of both code and test generation models, but does not explicitly address how the approach handles cases where the generated code or tests contain errors or inconsistencies
- Why unresolved: The paper focuses on the overall effectiveness of the approach, but does not delve into the details of how it handles cases where the generated code or tests have errors or inconsistencies
- What evidence would resolve it: Analyzing the behavior of the approach in cases where the generated code or tests have errors or inconsistencies would provide insights into its robustness and error-handling capabilities

## Limitations
- The mutual execution scoring function's effectiveness depends heavily on having sufficient test coverage per problem
- The iterative self-training framework assumes ground truth code is available for test augmentation
- The computational cost of executing generated code against test suites is not addressed, which could be prohibitive at scale

## Confidence
- **High Confidence**: The core mechanism of using execution feedback to filter correct code solutions is well-supported by experimental results showing improved pass@1 and pass@10 metrics
- **Medium Confidence**: The dual execution scoring function's ability to rank code and test quality without ground truth is promising but relies on assumptions about test determinism and mutual execution convergence
- **Low Confidence**: The scalability of the approach to large codebases and generalization beyond the APPS dataset to real-world programming tasks remains unproven

## Next Checks
1. Systematically evaluate how varying the number of test cases per problem (from 1 to 20+) affects the accuracy of rejection sampling and the quality of augmented code solutions
2. Apply the GenX framework to a different code generation dataset (such as HumanEval or MBPP) to verify that the mutual execution feedback approach generalizes beyond the APPS dataset
3. Conduct ablation studies on the dual execution scoring algorithm by varying the number of iterations, introducing noise in the pass/fail matrix, and testing on synthetic datasets with known quality distributions to validate convergence properties and ranking accuracy