---
ver: rpa2
title: Multi-Modal Retrieval For Large Language Model Based Speech Recognition
arxiv_id: '2406.09618'
source_url: https://arxiv.org/abs/2406.09618
tags:
- retrieval
- speech
- used
- multi-modal
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes multi-modal retrieval augmentation for large
  language models in speech recognition tasks. The authors introduce two approaches:
  kNN-LM and cross-attention techniques.'
---

# Multi-Modal Retrieval For Large Language Model Based Speech Recognition

## Quick Facts
- arXiv ID: 2406.09618
- Source URL: https://arxiv.org/abs/2406.09618
- Reference count: 13
- Primary result: Up to 50% WER improvement with multi-modal retrieval vs text-only retrieval

## Executive Summary
This paper introduces multi-modal retrieval augmentation for large language models in speech recognition, proposing two approaches: kNN-LM and cross-attention techniques. The authors demonstrate that speech-based multi-modal retrieval significantly outperforms text-based retrieval, achieving up to 50% improvement in word error rate. The cross-attention model shows consistent improvements for both domain adaptation and dynamic context tasks, achieving state-of-the-art results on the Spoken-Squad dataset with 41% relative improvement over Whisper.

## Method Summary
The method involves using multi-modal language models (both small 330M and large 6.8B parameter variants) as key encoders for nearest neighbor search in retrieval-augmented speech recognition. The approach uses either kNN-LM (directly modifying token probabilities) or cross-attention based neural models (adding cross-attention blocks to the decoder). Training involves pre-training on text then extending with speech modality, followed by fine-tuning with retrieval context for domain adaptation. Audio is encoded using HuBERT embeddings discretized into 2000 k-means centroids, while text uses sentence piece tokenization.

## Key Results
- Speech-based multi-modal retrieval yields up to 50% improvement in word error rate over multi-modal language model baseline
- Cross-attention model shows consistent improvement for domain adaptation and dynamic context tasks
- Achieves state-of-the-art recognition results on Spoken-Squad with 41% relative improvement over Whisper
- Multi-modal retrieval outperforms text-based retrieval, with text-only memory yielding 17.9% WER compared to 16.5% for multi-modal memory

## Why This Works (Mechanism)

### Mechanism 1
Speech-based multi-modal retrieval outperforms text-based retrieval by incorporating acoustic information into retrieval keys. The multi-modal language model encodes both speech and text tokens, creating richer key embeddings that capture acoustic-phonetic context. These embeddings enable more accurate retrieval of relevant context compared to text-only embeddings.

### Mechanism 2
Multi-modal LMs can serve as effective key encoders for nearest neighbor search, eliminating the need for separate external models. The same multi-modal LM used for speech recognition can generate embeddings for both decoding and retrieval key creation, removing model duplication and computational overhead.

### Mechanism 3
Cross-attention mechanism outperforms kNN-LM by allowing complex interactions between context documents and input tokens. The cross-attention model uses a value encoder to process retrieved documents and applies token-level cross-attention, enabling discrimination between relevant and irrelevant context based on token interactions.

## Foundational Learning

- **Concept: Multi-modal token embedding spaces**
  - Why needed here: Understanding how speech and text tokens coexist in the same embedding space is crucial for grasping the model architecture
  - Quick check question: How does the model handle the different semantic spaces of speech and text tokens?

- **Concept: Retrieval-augmented generation (RAG) principles**
  - Why needed here: The paper builds on RAG concepts but extends them to multi-modal contexts
  - Quick check question: What distinguishes this multi-modal RAG approach from traditional text-only RAG?

- **Concept: Cross-attention mechanisms in transformers**
  - Why needed here: The cross-attention model introduces a novel use of cross-attention for retrieval integration
  - Quick check question: How does token-level cross-attention differ from standard cross-attention in the model?

## Architecture Onboarding

- **Component map**: Audio → Tokenization → Multi-modal LM encoding → Retrieval key creation → Database search → Context retrieval → Value encoding → Cross-attention integration → Decoding

- **Critical path**: Audio → Tokenization → Multi-modal LM encoding → Retrieval key creation → Database search → Context retrieval → Value encoding → Cross-attention integration → Decoding

- **Design tradeoffs**:
  - kNN-LM vs Cross-attention: Simplicity vs performance
  - Single vs separate key encoders: Efficiency vs specialization
  - Retrieval corpus size vs precision: Coverage vs quality
  - Model size vs inference latency: Performance vs deployment constraints

- **Failure signatures**:
  - Degraded WER with increased retrieval corpus size (overfitting)
  - High latency during inference (inefficient retrieval implementation)
  - Poor performance on entity-heavy data (inadequate key encoding)
  - Inconsistent improvements across datasets (domain mismatch)

- **First 3 experiments**:
  1. Compare kNN-LM with and without multi-modal key encoding on a small dataset
  2. Evaluate cross-attention performance on domain adaptation task
  3. Measure retrieval accuracy using text-only vs multi-modal memory

## Open Questions the Paper Calls Out

1. **Optimal retrieval database size**: What is the optimal size of the retrieval database for multi-modal retrieval in speech recognition tasks, balancing recall performance with computational efficiency? The paper notes that very large databases can negatively affect retrieval statistics like recall but doesn't provide specific recommendations.

2. **Low-resource scenario performance**: How does the performance of multi-modal retrieval compare to text-only retrieval in low-resource scenarios where paired audio-text data is scarce? The paper acknowledges the dependency on paired audio-text data but doesn't explore scenarios where this data is limited.

3. **Hallucination causes and mitigation**: What are the specific causes and mechanisms behind hallucinations in retrieval-augmented speech recognition, and how can they be mitigated? The paper identifies hallucinations as a limitation but doesn't delve into specific causes or propose detailed mitigation strategies.

## Limitations

- Experimental validation is limited to two datasets (Spoken-Squad and SLUE Voxpopuli), which may not generalize across all speech recognition scenarios
- Computational overhead of the cross-attention approach for real-time applications is not thoroughly evaluated
- Retrieval corpus construction method using fixed 20-token windows may miss broader contextual relationships that could affect retrieval quality

## Confidence

- **High confidence**: The general principle that multi-modal retrieval improves speech recognition performance is well-supported by experimental results and ablation studies
- **Medium confidence**: Specific WER improvement percentages (50% improvement) and relative performance gains over Whisper (41% relative improvement) are based on limited dataset evaluations
- **Low confidence**: The claim that multi-modal LMs can effectively replace external key encoders for all speech recognition tasks requires more extensive validation across diverse domains and languages

## Next Checks

1. **Cross-dataset generalization test**: Evaluate the cross-attention model on at least three additional speech recognition datasets with varying characteristics (different languages, acoustic conditions, and domain-specific vocabulary) to assess whether reported improvements generalize beyond the two tested datasets.

2. **Computational overhead analysis**: Measure inference latency and memory requirements for the cross-attention model compared to both the baseline multi-modal LM and the kNN-LM approach across different hardware configurations to determine practical deployment constraints.

3. **Retrieval corpus sensitivity study**: Systematically vary the retrieval corpus size, key encoding strategies (different window sizes, overlapping windows, hierarchical retrieval), and document selection methods to identify the optimal retrieval configuration and determine the robustness of improvements to corpus design choices.