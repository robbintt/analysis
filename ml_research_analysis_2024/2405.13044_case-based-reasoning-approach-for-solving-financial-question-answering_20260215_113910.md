---
ver: rpa2
title: Case-Based Reasoning Approach for Solving Financial Question Answering
arxiv_id: '2405.13044'
source_url: https://arxiv.org/abs/2405.13044
tags:
- cases
- program
- case
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of numerical reasoning in financial
  question answering, where existing models often generate incorrect operations during
  reasoning steps. The authors propose a case-based reasoning (CBR) approach that
  retrieves similar questions and their corresponding logical programs to guide the
  program generation process.
---

# Case-Based Reasoning Approach for Solving Financial Question Answering

## Quick Facts
- arXiv ID: 2405.13044
- Source URL: https://arxiv.org/abs/2405.13044
- Reference count: 11
- Primary result: Case-based reasoning approach achieves 61.11% program accuracy on FinQA dataset when using gold cases

## Executive Summary
This paper addresses the challenge of numerical reasoning in financial question answering, where existing models often generate incorrect operations during reasoning steps. The authors propose a case-based reasoning (CBR) approach that retrieves similar questions and their corresponding logical programs to guide the program generation process. The method uses a dual-stage architecture with a case retriever (using bi-encoder and cross-encoder models) followed by a program generator that incorporates retrieved cases. Experiments on the FinQA dataset show that when provided with gold cases, the model achieves 61.11% program accuracy and 62.51% execution accuracy, outperforming the baseline model's 54.38% program accuracy.

## Method Summary
The proposed approach uses a dual-stage architecture consisting of a case retriever and a program generator. The case retriever employs both bi-encoder and cross-encoder models to find relevant cases from the dataset, with the bi-encoder providing efficient initial filtering and the cross-encoder performing detailed comparison on a reduced candidate set. The program generator incorporates retrieved cases through concatenation, creating a unified input that captures both the contextual nuances of the query and supportive information from similar cases. The system is evaluated on the FinQA dataset, which contains 8,281 financial QA pairs with questions, documents, and gold programs.

## Key Results
- 61.11% program accuracy achieved when using gold cases, compared to 54.38% for baseline model
- 62.51% execution accuracy when using gold cases, demonstrating effective reasoning guidance
- 48% of errors in baseline model stem from incorrect operations during reasoning steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Case-based reasoning improves program accuracy by providing the program generator with similar examples of questions and their corresponding logical programs.
- Mechanism: The case retriever identifies relevant cases with high program similarity to the current question, which are then incorporated into the program generator's input. This provides the model with additional context about operations that have worked for similar problems, reducing the likelihood of generating incorrect operations.
- Core assumption: Similar questions have similar logical program structures, and providing these examples to the program generator will guide it toward correct operation sequences.
- Evidence anchors:
  - [abstract]: "Our investigation reveals that half of the errors (48%) stem from incorrect operations being generated. To address this issue, we propose a novel approach to tackle numerical reasoning problems using case-based reasoning (CBR), an artificial intelligence paradigm that provides problem-solving guidance by offering similar cases (i.e. similar questions and corresponding logical programs)."
  - [section]: "Our analysis indicates that a significant source of errors in current baseline model arises from incorrect operations during the reasoning steps."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.513, average citations=0.0. Weak corpus evidence for this specific mechanism.
- Break condition: If retrieved cases are not sufficiently similar to the query, or if the program generator cannot effectively utilize the case information, the mechanism fails.

### Mechanism 2
- Claim: The dual-stage architecture (bi-encoder followed by cross-encoder) optimizes both retrieval speed and accuracy for finding relevant cases.
- Mechanism: The bi-encoder efficiently pre-computes and caches embeddings for all candidate cases, enabling rapid initial filtering. The cross-encoder then performs detailed comparison between the query and a reduced set of candidates, capturing nuanced semantic relationships that the bi-encoder might miss.
- Core assumption: The bi-encoder can effectively narrow down the candidate pool to a manageable size without losing relevant cases, allowing the cross-encoder to focus on high-quality candidates.
- Evidence anchors:
  - [section]: "After evaluating both methods, we will select or combine them based on their performance and suitability for our model. The initial broad filtering capability of the bi-encoder, followed by the detailed scrutiny provided by the cross-encoder, could potentially offer a synergistic approach, leveraging the strengths of both models."
  - [section]: "While cross-encoders demonstrate superior accuracy and a deeper understanding of the query-case relationship due to their comprehensive encoding strategy, they are computationally more intensive."
  - [corpus]: Weak corpus evidence for this specific dual-stage architecture approach.
- Break condition: If the bi-encoder retrieves too many irrelevant cases, the cross-encoder becomes overwhelmed; if it retrieves too few, relevant cases may be missed entirely.

### Mechanism 3
- Claim: Different case presentation methods (concatenation vs. separate encoder) affect how effectively the program generator can utilize retrieved case information.
- Mechanism: The concatenation approach integrates case information directly into the input sequence, allowing the encoder to create a unified contextual embedding. The separate encoder approach maintains distinct processing streams for cases and query, enabling targeted attention mechanisms to dynamically weigh case relevance during token generation.
- Core assumption: The program generator can effectively incorporate case information regardless of presentation method, but the optimal method depends on the specific characteristics of the financial QA task.
- Evidence anchors:
  - [section]: "In line with the methodology section, we investigate both Concatenation and Separate Encoder methods for incorporating retrieved cases into the program generation process. The initial findings presented in Table 1 indicate that the Concatenation approach outperforms the Separate Encoder method significantly."
  - [section]: "This aggregated input is then processed through a pre-trained encoder, such as BERT or RoBERTa, to produce a comprehensive embedding that captures both the contextual nuances of the query and the supportive information from the cases."
  - [corpus]: Weak corpus evidence for this specific case presentation comparison.
- Break condition: If the program generator cannot effectively process case information in the chosen presentation format, or if the format introduces noise that confuses the generation process.

## Foundational Learning

- Concept: Program accuracy vs. execution accuracy
  - Why needed here: Understanding these metrics is crucial for evaluating the model's reasoning capabilities versus its ability to arrive at correct answers through potentially incorrect reasoning.
  - Quick check question: If a model generates an incorrect program but accidentally arrives at the correct answer, how would program accuracy and execution accuracy differ?

- Concept: Cosine similarity and edit distance metrics
  - Why needed here: These metrics are fundamental to the case retrieval process, used to measure question similarity and program similarity respectively.
  - Quick check question: What is the range of values for cosine similarity, and what does a value of 0.8 indicate about the relationship between two vectors?

- Concept: Sequence-to-sequence architecture with attention mechanisms
  - Why needed here: The program generator uses this architecture to produce logical programs from questions and contexts, and understanding it is essential for implementing and debugging the model.
  - Quick check question: In a seq2seq model with attention, what information does the attention mechanism provide at each decoding step?

## Architecture Onboarding

- Component map: Question → Context Retriever → Case Retriever → Program Generator → Program Execution → Answer
- Critical path: Question → Context Retriever → Case Retriever → Program Generator → Program Execution → Answer
- Design tradeoffs:
  - Retrieval speed vs. accuracy: Bi-encoder is faster but less accurate; cross-encoder is slower but more accurate
  - Case quality vs. quantity: More cases provide more information but may introduce noise
  - Model complexity vs. performance: Larger models and more complex architectures generally perform better but require more computational resources
- Failure signatures:
  - Low program accuracy but high execution accuracy: Model is getting lucky with incorrect programs
  - Low retrieval precision: Case retriever is finding irrelevant cases
  - High training but low validation accuracy: Overfitting to training data
- First 3 experiments:
  1. Implement and evaluate the bi-encoder case retriever with different pre-trained models (BERT-base, RoBERTa-base, RoBERTa-large) to establish baseline retrieval performance
  2. Implement the concatenation approach for incorporating retrieved cases into the program generator and compare with the baseline FinQANet model
  3. Experiment with different input configurations for the case retriever (questions only, programs only, questions+programs) to determine optimal input format

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal architecture for combining bi-encoder and cross-encoder models in a dual-stage retrieval system?
- Basis in paper: explicit
- Why unresolved: The paper discusses the potential of using both architectures but does not provide experimental results comparing their combined performance against individual architectures.
- What evidence would resolve it: Experimental results showing the performance of a dual-stage system versus individual bi-encoder and cross-encoder systems on the FinQA dataset.

### Open Question 2
- Question: How does the choice of input case variation (question only, program only, or both) affect the performance of the case retriever?
- Basis in paper: explicit
- Why unresolved: The paper mentions experimenting with different input types but does not provide conclusive results on which input type yields the best performance.
- What evidence would resolve it: Comparative analysis of case retriever performance using different input types on the FinQA dataset.

### Open Question 3
- Question: What is the impact of the case retriever's performance on the overall efficacy of the program generator?
- Basis in paper: inferred
- Why unresolved: The paper suggests that the program generator's success depends on the case retriever's performance but does not quantify this relationship.
- What evidence would resolve it: Correlation analysis between case retriever precision and program generator accuracy on the FinQA dataset.

### Open Question 4
- Question: How does the number of retrieved cases affect the program generator's performance?
- Basis in paper: inferred
- Why unresolved: The paper does not explore the effect of varying the number of retrieved cases on the program generator's accuracy.
- What evidence would resolve it: Experimental results showing program generator performance with different numbers of retrieved cases on the FinQA dataset.

### Open Question 5
- Question: Can the proposed CBR approach be generalized to other domains beyond financial QA?
- Basis in paper: explicit
- Why unresolved: The paper focuses on financial documents and does not provide evidence of the approach's applicability to other domains.
- What evidence would resolve it: Successful application of the CBR approach to QA tasks in other domains, such as healthcare or legal documents, with comparable performance improvements.

## Limitations

- The study uses synthetic negative sampling for case retriever training, which may not reflect true retrieval challenges encountered during inference.
- The evaluation focuses exclusively on the FinQA dataset, leaving questions about generalization to other financial QA benchmarks or domains.
- While the dual-stage retrieval architecture shows theoretical advantages, the paper lacks ablation studies comparing it against simpler single-stage approaches.

## Confidence

- High Confidence: The observation that 48% of errors stem from incorrect operations is well-supported by error analysis in the FinQA dataset. The program accuracy improvements when using gold cases (61.11% vs 54.38%) are statistically significant and directly measured.
- Medium Confidence: The mechanism by which case-based reasoning improves program generation is plausible but not definitively proven. While the concatenation approach outperforms separate encoding in initial experiments, the study does not explore why this occurs or test additional case presentation methods.
- Low Confidence: Claims about the optimal dual-stage architecture design remain speculative without comprehensive comparisons against alternative retrieval strategies or varying case quantity thresholds.

## Next Checks

1. **Retrieval Robustness Test:** Systematically vary the number of retrieved cases (1, 3, 5, 10) and measure the impact on program accuracy to identify optimal case quantity and diversity thresholds.

2. **Generalization Benchmark:** Evaluate the CBR approach on an independent financial QA dataset (e.g., Financial PhraseBank or a held-out portion of FinQA) to assess whether improvements transfer beyond the training distribution.

3. **Ablation Study:** Remove the case-based reasoning component entirely and compare performance against the baseline FinQANet model using identical hyperparameters and training procedures to isolate the contribution of CBR.