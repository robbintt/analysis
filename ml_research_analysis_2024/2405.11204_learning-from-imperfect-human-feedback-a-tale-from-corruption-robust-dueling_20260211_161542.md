---
ver: rpa2
title: 'Learning from Imperfect Human Feedback: a Tale from Corruption-Robust Dueling'
arxiv_id: '2405.11204'
source_url: https://arxiv.org/abs/2405.11204
tags:
- feedback
- regret
- corruption
- lemma
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies Learning from Imperfect Human Feedback (LIHF)\
  \ in dueling bandits, where human feedback can be imperfect and decay over time.\
  \ The authors cast this as a continuous-action dueling bandit problem under decaying\
  \ corruption (O(t^\u03C1-1) at round t), motivated by the observation that human\
  \ feedback often improves with interaction."
---

# Learning from Imperfect Human Feedback: a Tale from Corruption-Robust Dueling
## Quick Facts
- arXiv ID: 2405.11204
- Source URL: https://arxiv.org/abs/2405.11204
- Authors: Yuwei Cheng; Fan Yao; Xuefeng Liu; Haifeng Xu
- Reference count: 40
- Key outcome: Establishes Ω(d max{√T, T^ρ}) regret lower bound for LIHF under decaying corruption and presents RoSMID algorithm achieving Õ(d max{√T, T^ρ})

## Executive Summary
This paper studies Learning from Imperfect Human Feedback (LIHF) in dueling bandits, where human feedback can be imperfect and decay over time. The authors cast this as a continuous-action dueling bandit problem under decaying corruption (O(t^ρ-1) at round t), motivated by the observation that human feedback often improves with interaction. The main contributions include establishing a regret lower bound showing LIHF can be as hard as arbitrary corruption, developing the RoSMID algorithm achieving near-optimal regret, and introducing a novel framework for analyzing gradient-based dueling bandit algorithms under corruption.

## Method Summary
The authors formulate LIHF as a continuous-action dueling bandit problem with decaying corruption, where feedback quality improves over time following a polynomial decay pattern. They develop the Robustified Stochastic Mirror Descent for Imperfect Dueling (RoSMID) algorithm, which uses a carefully tuned learning rate ηρ = √log T / (dT max{0.5,ρ}) to balance exploration and robustness to corruption. The algorithm builds upon the standard stochastic mirror descent framework but incorporates robustness mechanisms specifically designed to handle the imperfect and improving human feedback. The theoretical analysis establishes both lower and upper bounds on regret, demonstrating that decaying corruption doesn't fundamentally simplify the problem compared to arbitrary corruption.

## Key Results
- Establishes Ω(d max{√T, T^ρ}) regret lower bound for LIHF, showing it can be as hard as arbitrary corruption
- Develops RoSMID algorithm achieving near-optimal regret Õ(d max{√T, T^ρ})
- Introduces novel framework for analyzing gradient-based dueling bandit algorithms under corruption
- Experiments validate theoretical findings, showing RoSMID outperforms baselines like Doubler and Sparring

## Why This Works (Mechanism)
The key insight is that while human feedback is imperfect, its decaying nature doesn't make the problem fundamentally easier. The RoSMID algorithm uses a carefully tuned learning rate to balance exploration and robustness to corruption. The algorithm maintains a weight distribution over the action space and updates it based on pairwise comparisons, while being robust to the decaying corruption in the feedback. The decay pattern O(t^ρ-1) captures how human feedback quality improves with interaction, but the analysis shows that even with this improvement, the problem remains as challenging as arbitrary corruption in terms of regret bounds.

## Foundational Learning
- Dueling Bandits: A framework for learning from pairwise comparisons rather than absolute feedback. Needed because human feedback is often relative (comparing options) rather than absolute ratings. Quick check: Can compare two items and select the preferred one.
- Continuous-Action Setting: Actions are drawn from a continuous space rather than discrete options. Needed because many real-world decisions involve continuous parameters. Quick check: Can handle actions as vectors in d-dimensional space.
- Decaying Corruption: Error in feedback that decreases over time following a polynomial pattern. Needed to model improving human feedback quality. Quick check: Can model feedback error as O(t^ρ-1) where ρ ∈ (0,1].
- Stochastic Mirror Descent: Optimization algorithm that updates weights in a constrained space. Needed for efficient exploration and exploitation in continuous action spaces. Quick check: Can maintain and update a probability distribution over actions.

## Architecture Onboarding
Component Map: Action Space -> Pairwise Comparison -> Feedback Processing -> Weight Update -> Next Action Distribution
Critical Path: The algorithm maintains a weight distribution over the continuous action space, performs pairwise comparisons to gather feedback, processes the (potentially corrupted) feedback through a robust estimator, and updates the weight distribution using mirror descent with the tuned learning rate ηρ.
Design Tradeoffs: The main tradeoff is between exploration (trying new actions to learn preferences) and exploitation (focusing on known good actions), balanced by the learning rate. The robustness mechanism trades off some efficiency for protection against imperfect feedback.
Failure Signatures: Poor performance occurs when the assumed decay pattern doesn't match actual feedback improvement, when corruption is too severe even with decay, or when the learning rate tuning is inappropriate for the specific ρ value.
First Experiments: 1) Test on synthetic data with known preferences and controlled corruption levels, 2) Compare performance against baselines (Doubler, Sparring) under various ρ values, 3) Validate regret bounds empirically by measuring cumulative regret over time.

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes polynomial decay pattern for feedback improvement, which may not capture all real-world human feedback patterns
- Analysis focuses on continuous-action settings, potentially limiting applicability to discrete action spaces
- The framework may not handle sudden feedback quality drops or non-polynomial decay patterns

## Confidence
- High: Theoretical regret bounds (Ω(d max{√T, T^ρ}) and Õ(d max{√T, T^ρ})) are mathematically rigorous
- Medium: Practical effectiveness of RoSMID under various corruption patterns is supported by experiments
- Medium: Claim that decaying corruption doesn't fundamentally simplify the problem is theoretically sound

## Next Checks
1. Test RoSMID on non-polynomial decay patterns (e.g., exponential decay or sudden feedback quality drops) to assess robustness beyond theoretical assumptions
2. Evaluate the algorithm's performance in discrete action spaces to validate practical applicability beyond continuous settings
3. Conduct user studies to verify if the assumed polynomial decay pattern accurately models real human feedback improvement in interactive settings