---
ver: rpa2
title: Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks
arxiv_id: '2404.00376'
source_url: https://arxiv.org/abs/2404.00376
tags:
- medical
- questions
- arxiv
- gpt-4
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce Meerkat, a family of medical AI systems trained on
  high-quality synthetic data generated from 18 medical textbooks. Our models, ranging
  from 7B to 70B parameters, achieve state-of-the-art performance on seven medical
  benchmarks, surpassing previous best models like MediTron and BioMistral.
---

# Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks

## Quick Facts
- **arXiv ID**: 2404.00376
- **Source URL**: https://arxiv.org/abs/2404.00376
- **Reference count**: 40
- **Primary result**: Meerkat-7B is first 7B-parameter model to pass USMLE threshold; Meerkat-70B outperforms GPT-4 by 1.3% average

## Executive Summary
Meerkat introduces a family of medical AI systems trained on high-quality synthetic data generated from 18 medical textbooks. The models, ranging from 7B to 70B parameters, achieve state-of-the-art performance on seven medical benchmarks, surpassing previous best models like MediTron and BioMistral. Notably, Meerkat-7B is the first 7B-parameter model to pass the USMLE threshold, while Meerkat-70B outperforms GPT-4 by an average of 1.3%. The systems provide detailed responses to clinical queries, narrowing the performance gap with large commercial models through enhanced reasoning skills acquired from textbook-based chain-of-thought training.

## Method Summary
The Meerkat models were developed by fine-tuning Mistral-7B on a combination of 9.3K USMLE-style questions with chain-of-thought reasoning from MedQA and 78K synthetic CoT data from 18 medical textbooks. Additional instruction-following datasets (MedMCQA, LiveQA, MedicationQA, ChatDoctor-cleaned, MedQA-dialogue, AlpaCare) were included. The models were trained for 3 epochs with batch size 128, learning rate 2e-6, and warm-up ratio 0.04. GPT-4 was used to generate synthetic question-answer pairs and CoT paths from the textbook corpus.

## Key Results
- Meerkat-7B achieved 66.6% accuracy on MedQA-USMLE, becoming the first 7B-parameter model to pass the USMLE threshold
- Meerkat-70B outperformed GPT-4 by an average of 1.3% across seven medical benchmarks
- Meerkat-70B showed superior performance on clinical question answering tasks (K-QA) compared to GPT-3.5
- The 70B model achieved higher scores on completeness and factuality metrics compared to smaller Meerkat variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought (CoT) reasoning paths from textbooks enable small models to solve complex multi-step medical problems.
- Mechanism: Synthetic CoT data teaches the model explicit reasoning steps, compensating for limited parameter count by improving problem-solving strategy rather than raw knowledge storage.
- Core assumption: Small models lack inherent multi-step reasoning, which can be acquired through fine-tuning on CoT examples.
- Evidence anchors: [abstract] "Our systems also provide detailed responses to clinical queries, narrowing the performance gap with large commercial models."

### Mechanism 2
- Claim: Augmenting CoT data with textbook-derived examples increases the variety of reasoning patterns available to the model.
- Mechanism: GPT-4 generates synthetic questions and answers from textbook content, providing diverse contexts for the same medical concepts and expanding the model's exposure beyond single-source datasets.
- Core assumption: Multiple contexts for the same concept improve transfer and generalization of reasoning skills.
- Evidence anchors: [section] "Figure 3 shows that augmenting the training data with additional QA pairs and CoT reasoning paths obtained from textbooks...led to a further improvement in performance, with an average accuracy increase of 5.4%."

### Mechanism 3
- Claim: Selecting high-performing general-purpose models as backbones (Mistral-7B) leverages broader pretraining for medical reasoning.
- Mechanism: General models with diverse pretraining corpora outperform biomedical-specific models on medical benchmarks, likely due to exposure to varied reasoning tasks during pretraining.
- Core assumption: Diverse pretraining improves the model's ability to learn reasoning from fine-tuning, even in specialized domains.
- Evidence anchors: [section] "As depicted in Figure 3, general-purpose models like Mistral-7B and Gemma-7B [19] outperformed biomedical-specific models such as MediTron-7B and BioMistral-7B."

## Foundational Learning

- Concept: Chain-of-thought reasoning and multi-step problem decomposition
  - Why needed here: Medical problems require analyzing symptoms, applying diagnostic criteria, and eliminating incorrect options systematically.
  - Quick check question: Can you trace the logical steps from patient symptoms to final diagnosis in a USMLE-style question?

- Concept: Synthetic data generation using large models
  - Why needed here: High-quality medical reasoning examples are scarce, and textbooks must be transformed into QA pairs with explanations.
  - Quick check question: How would you prompt GPT-4 to generate a question from a textbook paragraph while ensuring the answer requires multi-step reasoning?

- Concept: Model selection based on pretraining diversity
  - Why needed here: Backbone choice affects the model's ability to learn reasoning strategies from fine-tuning data.
  - Quick check question: What differences would you expect between a general-purpose model and a biomedical-specific model when fine-tuned on CoT data?

## Architecture Onboarding

- Component map: Mistral-7B backbone → CoT fine-tuning on MedQA-CoT + MedBooks-CoT-18 → Instruction-following datasets (MedMCQA, LiveQA, MedicationQA, ChatDoctor-cleaned, MedQA-dialogue, AlpaCare) → Final medical model
- Critical path: Generate CoT data → Fine-tune backbone → Evaluate on benchmarks → Deploy for inference
- Design tradeoffs:
  - Synthetic data quality vs. quantity: Higher quality reduces hallucinations but may limit diversity.
  - Model size vs. inference speed: 7B vs 70B trade-off between reasoning depth and deployment feasibility.
  - General vs. specialized pretraining: Broader pretraining may improve reasoning but reduce domain-specific knowledge.
- Failure signatures:
  - Low accuracy on multi-step problems despite high accuracy on single-step questions (CoT learning failed).
  - Hallucinations in generated explanations (synthetic data quality issues).
  - Poor performance on clinical notes despite high exam scores (lack of instruction-tuning diversity).
- First 3 experiments:
  1. Fine-tune Mistral-7B on MedQA-CoT only, evaluate on MedQA to measure CoT impact baseline.
  2. Add MedBooks-CoT-18 to the fine-tuning mix, compare performance gain to baseline.
  3. Compare Mistral-7B vs BioMistral-7B fine-tuned identically on CoT data to test pretraining diversity hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Meerkat-7B on real-world clinical queries compare to GPT-4 in terms of factuality, and what specific improvements could be made to enhance its factuality?
- Basis in paper: [explicit] - The paper states that Meerkat-7B exhibits inferior factuality performance compared to GPT-3.5 and GPT-4, indicating the need for further development towards more reliable AI systems.
- Why unresolved: While the paper acknowledges the factuality issue, it does not provide specific data or analysis on how Meerkat-7B's factuality compares to GPT-4 in real-world clinical queries. Additionally, the paper does not suggest concrete improvements to enhance factuality.
- What evidence would resolve it: Detailed comparative analysis of Meerkat-7B and GPT-4 factuality scores on real-world clinical queries, along with proposed methods to improve Meerkat-7B's factuality.

### Open Question 2
- Question: How effective is the chain-of-thought (CoT) fine-tuning approach in improving the performance of small language models on medical tasks that do not heavily rely on multi-step reasoning, such as MMLU-Medical and MedMCQA?
- Basis in paper: [inferred] - The paper mentions that the effectiveness of the CoT fine-tuning approach might be less impactful for datasets like MMLU-Medical and MedMCQA, where multi-step reasoning is relatively less essential compared to USMLE-style questions.
- Why unresolved: The paper does not provide specific performance data or analysis on how the CoT fine-tuning approach affects small language models' performance on medical tasks that do not heavily rely on multi-step reasoning.
- What evidence would resolve it: Comparative analysis of small language models' performance on MMLU-Medical and MedMCQA with and without CoT fine-tuning, along with insights on the effectiveness of the approach for different types of medical tasks.

### Open Question 3
- Question: How can open-source language models be developed to effectively memorize extensive medical knowledge, and how would this complement the chain-of-thought fine-tuning approach?
- Basis in paper: [inferred] - The paper suggests that developing open-source language models capable of effectively memorizing extensive medical knowledge or retrieval-augmented approaches can complement the CoT fine-tuning training method.
- Why unresolved: The paper does not provide specific methods or approaches for developing open-source language models with extensive medical knowledge memorization capabilities, nor does it explain how this would complement the CoT fine-tuning approach.
- What evidence would resolve it: Proposed methods for developing open-source language models with extensive medical knowledge memorization, along with experimental results demonstrating the complementarity of this approach with CoT fine-tuning.

## Limitations
- Synthetic training data quality and reproducibility concerns due to unspecified GPT-4 prompts
- Limited evaluation on real-world clinical scenarios versus exam-style questions
- Lack of rigorous comparison between general and biomedical-specific pretraining approaches
- No assessment of model calibration or uncertainty quantification for medical applications

## Confidence

- **High Confidence**: Meerkat-7B achieving USMLE threshold and Meerkat-70B outperforming GPT-4 on medical benchmarks
- **Medium Confidence**: The effectiveness of chain-of-thought reasoning training for small models
- **Low Confidence**: The superiority of general-purpose models over biomedical-specific models for medical reasoning

## Next Checks

1. **Data Quality Audit**: Generate an independent sample of synthetic CoT data using the described methodology and evaluate it for medical accuracy, reasoning coherence, and hallucination rates against expert medical review.

2. **Pretraining Diversity Experiment**: Systematically compare the same model architecture (7B parameters) fine-tuned on CoT data when initialized from different pretraining distributions - general web data, biomedical literature, and medical textbooks - to isolate the pretraining diversity effect.

3. **Clinical Realism Evaluation**: Test the fine-tuned models on a dataset of real clinical case notes and patient queries (not multiple-choice questions) to assess whether textbook-derived reasoning generalizes to practical medical decision-making scenarios.