---
ver: rpa2
title: Model Editing with Canonical Examples
arxiv_id: '2402.06155'
source_url: https://arxiv.org/abs/2402.06155
tags:
- canonical
- examples
- language
- finetuning
- sense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the setting of model editing with canonical
  examples, which requires (1) learning from a single example, (2) evaluating out-of-distribution,
  and (3) strictly limiting deviation from the initial model. The paper creates six
  datasets covering knowledge-intensive improvements, social bias mitigation, and
  syntactic edge cases.
---

# Model Editing with Canonical Examples
arXiv ID: 2402.06155
Source URL: https://arxiv.org/abs/2402.06155
Reference count: 40
Primary result: Sense finetuning in Backpack architecture outperforms LoRA and full finetuning (4.8% vs 0.3%) for model editing with single canonical examples

## Executive Summary
This paper introduces a novel setting for model editing that focuses on making precise, localized changes to language models using only single canonical examples while maintaining strict limits on deviation from the original model. The authors create six datasets covering knowledge-intensive improvements, social bias mitigation, and syntactic edge cases, and evaluate various editing approaches on Pythia models. The proposed sense finetuning method, which selectively modifies sense vectors in the Backpack architecture, demonstrates significant improvements over existing techniques like LoRA and full finetuning, with particular success in maintaining the model's core capabilities while implementing targeted edits.

The research also explores inference-time ensembles that combine the original model with a much smaller edited Backpack model, achieving performance gains that sometimes exceed direct editing of the full model. This approach offers a practical path for implementing model edits without extensive retraining, particularly valuable for applications requiring rapid deployment of corrections or improvements. The study establishes a rigorous evaluation framework for model editing that emphasizes minimal intervention and out-of-distribution generalization, addressing a critical gap in current model adaptation methodologies.

## Method Summary
The paper proposes sense finetuning, a method that selectively modifies sense vectors within the Backpack architecture for model editing with canonical examples. The approach learns from a single example while strictly limiting deviation from the original model and evaluating out-of-distribution. The method selects and finetunes a few sense vectors for each canonical example, contrasting with full finetuning or LoRA approaches that modify larger portions of the model. The authors evaluate this approach across six datasets covering knowledge-intensive tasks, social bias mitigation, and syntactic edge cases using Pythia models of varying sizes (125M, 160M, 260M parameters).

## Key Results
- Sense finetuning achieves 4.8% improvement versus 0.3% for other methods on evaluation tasks
- Inference-time ensemble with a 35x smaller Backpack model outperforms direct editing of GPT-J-6B (4.1% vs 1.0%)
- LoRA outperforms full finetuning in the canonical example editing setting
- The method demonstrates strong performance across knowledge, social bias, and syntactic editing tasks

## Why This Works (Mechanism)
Sense finetuning works by leveraging the modular design of Backpack's sense vectors, which represent different semantic dimensions of language understanding. By selectively modifying only the most relevant sense vectors for a given editing task, the method achieves precise localization of changes while preserving the broader model functionality. This targeted approach allows the model to incorporate new knowledge or correct specific behaviors without disrupting the underlying learned representations that contribute to general performance.

The inference-time ensemble strategy works by combining the original model's broad capabilities with the targeted edits from a smaller Backpack model. This creates a hybrid system where the smaller model handles the specific editing task while the larger model maintains general competence, effectively distributing the computational load and preserving model quality across different domains.

## Foundational Learning
Model editing - Modifying specific behaviors or knowledge in pre-trained models without full retraining
Why needed: Enables rapid deployment of corrections and improvements without expensive retraining cycles
Quick check: Can a single example successfully change model behavior on related inputs?

Backpack architecture - A modular language model design with sense vectors representing semantic dimensions
Why needed: Provides the granularity needed for precise, localized edits
Quick check: Does modifying specific sense vectors change only intended behaviors?

LoRA (Low-Rank Adaptation) - Parameter-efficient fine-tuning method using low-rank matrices
Why needed: Serves as a baseline for comparing efficiency of editing approaches
Quick check: How much does LoRA deviate from original model weights?

Canonical examples - Single, representative examples used to drive model edits
Why needed: Enables minimal intervention while achieving desired behavioral changes
Quick check: Does one example generalize to similar but unseen inputs?

Inference-time ensembles - Combining multiple models during inference for improved performance
Why needed: Allows leveraging both original and edited model capabilities simultaneously
Quick check: Does ensemble performance exceed either model individually?

Out-of-distribution evaluation - Testing model edits on data different from training examples
Why needed: Ensures edits generalize beyond memorized patterns
Quick check: Do edits transfer to semantically similar but distinct inputs?

## Architecture Onboarding

Component map:
Original model -> Backpack with sense vectors -> Selected sense vectors -> Edited model (sense finetuning)
OR
Original model + Edited Backpack -> Inference-time ensemble -> Final output

Critical path: Canonical example -> Sense vector selection -> Finetuning -> Evaluation on out-of-distribution data

Design tradeoffs: Granularity vs. stability (more sense vectors modified = more precise but potentially unstable) versus efficiency (fewer vectors = faster but less precise)

Failure signatures: Overfitting to single example, loss of general capabilities, out-of-distribution performance degradation

Three first experiments:
1. Test sense finetuning on a simple factual knowledge edit (e.g., correcting a single entity relationship)
2. Evaluate ensemble performance with varying ratios of original to edited model weights
3. Compare different numbers of sense vectors modified during finetuning for the same editing task

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to larger models remains uncertain based on Pythia experiments
- Limited to six specific datasets, generalizability to other domains unclear
- Systematic selection criteria for sense vectors in arbitrary editing tasks not fully established
- Computational overhead of inference-time ensembles not thoroughly analyzed

## Confidence
High confidence: Sense finetuning methodology and results on established datasets
Medium confidence: Scalability claims and inference-time ensemble effectiveness
Low confidence: General applicability to arbitrary editing tasks and systematic sense vector selection

## Next Checks
1. Scale experiments to LLaMA-7B or LLaMA-13B models to verify sense finetuning performance on larger architectures
2. Conduct ablation studies testing impact of different numbers of sense vectors modified during sense finetuning
3. Perform comprehensive computational overhead analysis comparing inference-time ensemble approaches with direct editing