---
ver: rpa2
title: Robustness in Fairness against Edge-level Perturbations in GNN-based Recommendation
arxiv_id: '2401.13823'
source_url: https://arxiv.org/abs/2401.13823
tags:
- fairness
- robustness
- recommendation
- attacks
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of graph-based recommender
  systems in terms of fairness when subjected to edge-level perturbations. The authors
  introduce the concept of "robustness in fairness" and propose a method to assess
  it by gradually perturbing the adjacency matrix of a user-item interaction graph.
---

# Robustness in Fairness against Edge-level Perturbations in GNN-based Recommendation

## Quick Facts
- arXiv ID: 2401.13823
- Source URL: https://arxiv.org/abs/2401.13823
- Reference count: 40
- Key outcome: Edge deletions are more effective than additions in impacting robustness in fairness of GNN-based recommender systems

## Executive Summary
This paper investigates how edge-level perturbations affect the fairness robustness of graph-based recommender systems. The authors introduce "robustness in fairness" as a concept and propose an evaluation method that gradually perturbs the adjacency matrix of user-item interaction graphs. Through experiments on three datasets with three GNN-based recommender systems (GCMC, LightGCN, and NGCF), they demonstrate that edge deletions have a more significant impact on fairness metrics than additions. The study reveals that consumer fairness is more vulnerable to perturbations than provider fairness, with NGCF showing the highest robustness, particularly for provider fairness.

## Method Summary
The study evaluates robustness in fairness by implementing an iterative edge perturbation mechanism that modifies the adjacency matrix of user-item interaction graphs. The approach uses sparsification to create binary perturbation vectors from trainable weights, optimizing for maximum disparity in fairness metrics across demographic groups. Three GNN-based recommender systems (GCMC, LightGCN, NGCF) are trained on three datasets (MovieLens-1M, LFM-1K, Insurance) and evaluated under edge deletion and addition scenarios. Fairness is operationalized through four metrics covering both consumer (NDCG-based preference, precision-based satisfaction) and provider (exposure, visibility) perspectives.

## Key Results
- Edge deletions are significantly more effective than additions in impacting robustness in fairness across all tested models and datasets
- NGCF exhibits the highest robustness to perturbations, particularly for provider fairness metrics
- Consumer fairness is more severely affected by perturbations than provider fairness, with alarming unfairness levels observed
- The disadvantaged consumer group experiences greater impact from edge perturbations compared to the advantaged group

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Edge deletions are more effective than additions in impacting robustness in fairness
- Mechanism: Iterative removal of edges from the user-item interaction graph maximizes disparity in fairness metrics across groups
- Core assumption: Model performance is more sensitive to missing interactions than to added ones
- Evidence anchors: Experiments show most successful attacks occur through edge deletions; points labeled as edge deletions are positioned at the right of the zero line in results

### Mechanism 2
- Claim: NGCF is the most robust model against edge perturbations, especially for provider fairness
- Mechanism: Feature transformation and nonlinear activation in NGCF's message-passing step diminish the impact of perturbed graph on predictions
- Core assumption: Model architecture makes it less sensitive to changes in input graph
- Evidence anchors: NGCF shows least sensitivity to perturbations in provider fairness; attributed to feature transformation and nonlinear activation in message-passing

### Mechanism 3
- Claim: Edge perturbations targeting the disadvantaged group have greater impact on consumer fairness
- Mechanism: Attack removes or adds edges associated with disadvantaged group to reduce their recommendation utility or increase visibility
- Core assumption: Disadvantaged group has lower initial utility or visibility, making them more susceptible to perturbations
- Evidence anchors: Most settings reporting impact on fairness disparity caused by perturbations prioritizing disadvantaged group; consumer fairness affected more than provider fairness

## Foundational Learning

- Concept: Fairness in recommendation systems
  - Why needed here: Paper investigates robustness of recommender systems in terms of fairness when subjected to edge-level perturbations
  - Quick check question: What are the two main perspectives of fairness in recommendation systems, and how do they differ?

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: Paper uses GNN-based recommender systems and studies impact of edge-level perturbations on their robustness in fairness
  - Quick check question: How do GNNs learn preferences of users from their past interactions with a catalog of items?

- Concept: Adversarial attacks on recommendation systems
  - Why needed here: Paper considers attacks based on edge-level perturbations and studies their impact on robustness in fairness
  - Quick check question: What are the two main types of edge perturbations considered in the paper, and how do they differ?

## Architecture Onboarding

- Component map: Datasets (MovieLens-1M, LFM-1K, Insurance) -> GNN Models (GCMC, LightGCN, NGCF) -> Perturbation Task (Edge deletion and addition) -> Fairness Notions (Consumer and provider perspectives) -> Fairness Operationalizations (Rank-aware and rank-agnostic metrics)

- Critical path:
  1. Load datasets and split into training, validation, and testing sets
  2. Train GNN-based recommender systems on training set
  3. Define fairness notions and operationalizations
  4. Implement perturbation task and iteratively modify adjacency matrix
  5. Evaluate robustness in fairness by comparing original and perturbed models

- Design tradeoffs:
  - Choosing between edge deletion and addition as perturbation type
  - Balancing number of perturbations and impact on fairness metrics
  - Selecting appropriate fairness operationalizations for evaluation

- Failure signatures:
  - Perturbed models don't show significant impact on fairness metrics
  - Perturbation process doesn't converge or takes too long to complete
  - Evaluation results are inconsistent across different datasets or models

- First 3 experiments:
  1. Implement perturbation task with edge deletion and evaluate impact on consumer fairness using NDCG@k metric
  2. Implement perturbation task with edge addition and evaluate impact on provider fairness using exposure metric
  3. Compare robustness in fairness of three GNN-based models under same perturbation conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does impact of edge-level perturbations on fairness robustness vary across different types of GNN architectures beyond three models tested?
- Basis in paper: [explicit] Paper tests three specific GNN-based recommender systems but acknowledges need to cover wider set of models in future work
- Why unresolved: Study limited to three models, leaving uncertainty about how other GNN architectures would respond to similar attacks
- What evidence would resolve it: Testing broader range of GNN architectures under identical perturbation conditions and fairness metrics would reveal patterns or exceptions in robustness

### Open Question 2
- Question: What is effect of targeted attacks on fairness robustness of GNN-based recommender systems compared to untargeted edge-level perturbations?
- Basis in paper: [inferred] Paper suggests extending attack approach to targeted attacks and mentions potential to modify model performance to favor specific groups
- Why unresolved: Current study uses untargeted edge perturbations; targeted attacks could have different or more severe impacts on fairness
- What evidence would resolve it: Designing and executing targeted attack experiments while measuring changes in fairness metrics would clarify relative impact

### Open Question 3
- Question: How do edge perturbations affect long-term fairness dynamics in recommender systems, particularly regarding provider visibility and exposure?
- Basis in paper: [explicit] Study focuses on immediate impacts of perturbations, but discussion hints at long-term effects, especially for provider fairness under certain conditions
- Why unresolved: Experiments measure short-term fairness changes without considering how perturbations might influence fairness over extended periods
- What evidence would resolve it: Longitudinal studies tracking fairness metrics over multiple recommendation cycles after perturbations would reveal long-term trends

## Limitations
- Analysis focuses exclusively on demographic-based group fairness without considering individual fairness or intersectional fairness
- Perturbation mechanism relies on differentiable approximations of discrete metrics that may introduce approximation errors
- Study doesn't explore economic implications of fairness degradation or practical feasibility of attacks in real-world scenarios

## Confidence
- Edge deletion vs. addition effectiveness: High - Consistently demonstrated across all datasets and models
- NGCF robustness claims: Medium - Mechanistic explanation provided but could benefit from ablation studies
- Consumer vs. provider sensitivity: High - Clear patterns observed, though magnitude varies by metric

## Next Checks
1. Test perturbation mechanism on additional fairness notions (individual fairness, intersectional fairness) to verify if observed patterns hold across different fairness definitions
2. Conduct ablation studies to isolate specific architectural components of NGCF responsible for its robustness, particularly testing whether feature transformation or nonlinear activation has greater impact
3. Evaluate practical attack feasibility by testing perturbation approach on real-world recommendation systems with dynamic user-item interactions and limited knowledge of underlying model architecture