---
ver: rpa2
title: Jointly Modeling Inter- & Intra-Modality Dependencies for Multi-modal Learning
arxiv_id: '2405.17613'
source_url: https://arxiv.org/abs/2405.17613
tags:
- learning
- modalities
- i2m2
- dependencies
- intra-modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses supervised multi-modal learning by proposing
  a generative modeling framework that explicitly captures both inter-modality dependencies
  (relationships between modalities and label) and intra-modality dependencies (relationships
  within individual modalities and label). The core method, called I2M2, treats the
  label as a source that generates multiple modalities and their interactions.
---

# Jointly Modeling Inter- & Intra-Modality Dependencies for Multi-modal Learning

## Quick Facts
- arXiv ID: 2405.17613
- Source URL: https://arxiv.org/abs/2405.17613
- Authors: Divyam Madaan; Taro Makino; Sumit Chopra; Kyunghyun Cho
- Reference count: 27
- Primary result: I2M2 achieves 92.62% AUROC on fastMRI, outperforming both unimodal models and ensemble methods

## Executive Summary
This paper introduces I2M2, a generative modeling framework that explicitly captures both inter-modality dependencies (relationships between modalities and label) and intra-modality dependencies (relationships within individual modalities and label) in supervised multi-modal learning. The method treats the label as a source that generates multiple modalities and their interactions, using separate classifiers for each modality and their combination, then combining them via product-of-experts. I2M2 consistently outperforms traditional methods across healthcare datasets (fastMRI, MIMIC-III) and vision-language tasks (VQA, NLVR2), achieving robust performance even under distribution shifts.

## Method Summary
I2M2 is a generative modeling framework that treats the label as a source generating multiple modalities and their interactions. It builds separate classifiers for each modality to capture intra-modality dependencies and a classifier for combined modalities to capture inter-modality dependencies. These classifiers are combined using a product-of-experts approach. The method is evaluated on healthcare datasets (fastMRI for knee pathology diagnosis, MIMIC-III for mortality and ICD code prediction) and vision-language tasks (VQA and NLVR2), consistently outperforming traditional methods that focus only on either inter- or intra-modality dependencies across all datasets.

## Key Results
- FastMRI: I2M2 achieves 92.62% AUROC, outperforming both unimodal models and ensemble methods
- MIMIC-III: Improves mortality prediction accuracy to 78.10% and outperforms uni-modal approaches
- VQA and NLVR2: Achieves 85.36% and 68.63% accuracy respectively, demonstrating robust performance even under distribution shifts
- Consistent outperformance across all datasets without requiring prior knowledge of dependency importance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The generative model formulation with a selection variable v creates explicit separation between intra-modality and inter-modality dependencies
- Mechanism: By modeling p(y|x) and p(y|x') separately for intra-modality dependencies, then modeling p(y|x,x') for inter-modality dependencies, and combining via product-of-experts, the framework can capture both dependency types independently
- Core assumption: The selection variable v = 1 always, but its influence varies across datasets, allowing the model to adapt to different dependency strengths
- Evidence anchors:
  - [abstract] "treats the label as a source that generates multiple modalities and their interactions"
  - [section 3.1] "p(y | x, x′, v = 1) ∝ p(y) qx (y | x) qx′ (y | x′)| {z } Unimodal predictors qx,x′ (y | x, x′)| {z } Multimodal predictor"
  - [corpus] Weak - related papers focus on fusion architectures but don't explicitly discuss selection variable mechanisms
- Break condition: If the data-generating process doesn't follow this factorization, or if dependencies are non-linear in ways the product-of-experts cannot capture

### Mechanism 2
- Claim: I2M2 adapts to varying strengths of inter- and intra-modality dependencies without requiring prior knowledge
- Mechanism: The product-of-experts combination allows each expert (unimodal and multimodal) to contribute proportionally based on their predictive power for the given task and dataset
- Core assumption: Different datasets have different relative strengths of inter- vs intra-modality dependencies, and the framework can learn which to emphasize
- Evidence anchors:
  - [abstract] "adapt to varying strengths of inter- and intra-modality dependencies without requiring prior knowledge of their importance"
  - [section 4.2] "I2M2 excels across the board, ensuring robust performance regardless of which dependencies are most significant"
  - [section 4.4] "I2M2 can effectively disregard intra-modality dependencies when predicting the target"
- Break condition: If one type of dependency is completely absent, the other experts might dominate too strongly, leading to suboptimal performance

### Mechanism 3
- Claim: The framework outperforms ensemble methods even with identical parameter counts by using principled dependency modeling
- Mechanism: Unlike simple ensembling, I2M2 explicitly models the data-generating process and combines predictions through a principled probabilistic framework rather than simple averaging
- Core assumption: The probabilistic factorization captures the true underlying structure better than naive combination methods
- Evidence anchors:
  - [section 4.6] "I2M2 consistently outperforms the ensemble models" when comparing models with identical parameter counts
  - [section 3.1] "We combine these modality-specific and multi-modal classifiers by building a product of experts"
  - [corpus] Weak - related papers discuss ensemble methods but don't specifically address parameter-count comparisons with principled generative models
- Break condition: If the data doesn't follow the assumed generative process, or if the experts are poorly calibrated, the principled combination may not help

## Foundational Learning

- Concept: Probabilistic graphical models and conditional independence
  - Why needed here: The paper's theoretical foundation relies on understanding how variables (modalities and labels) relate through conditional independence assumptions
  - Quick check question: In the proposed model, what happens to the conditional independence between x and x' given y when the selection variable v is introduced?

- Concept: Product of experts and ensemble methods
  - Why needed here: The core technical contribution uses product-of-experts to combine unimodal and multimodal classifiers, which is central to how I2M2 works
  - Quick check question: How does the product-of-experts combination differ from simple averaging of classifier outputs?

- Concept: Mutual information and modality interactions
  - Why needed here: The paper discusses capturing different types of mutual information between modalities and labels, which is key to understanding the framework's effectiveness
  - Quick check question: What are the three types of mutual information captured by the I2M2 framework according to the paper?

## Architecture Onboarding

- Component map:
  - Unimodal classifiers (qx(y|x) and qx'(y|x')) for each modality
  - Multimodal classifier (qx,x'(y|x,x')) for combined modalities
  - Product-of-experts layer combining all classifier outputs
  - Training pipeline: Separate pretraining of unimodal models followed by joint fine-tuning

- Critical path:
  1. Train unimodal classifiers on their respective modalities
  2. Train multimodal classifier on concatenated modalities
  3. Combine predictions using product-of-experts
  4. Fine-tune jointly while maintaining the combination structure

- Design tradeoffs:
  - Parameter efficiency vs. performance: Separate models use more parameters but capture dependencies better
  - Training complexity: Requires training multiple models and coordinating their combination
  - Flexibility vs. specialization: Can adapt to different dependency strengths but may be over-parameterized for simple cases

- Failure signatures:
  - One expert consistently dominates predictions (indicates imbalance in dependency strengths)
  - Poor calibration across experts (product-of-experts may amplify errors)
  - Degraded performance on datasets with only one type of dependency (model may be over-engineered)

- First 3 experiments:
  1. Implement unimodal classifiers only and compare to I2M2 performance
  2. Implement simple ensemble of unimodal classifiers and compare to I2M2
  3. Vary the strength of inter-modality dependencies (synthetic data) and measure I2M2's adaptation capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental causes of the optimization challenges in training multi-modal models from scratch, and how can they be addressed?
- Basis in paper: [explicit] The paper mentions that training models for each modality separately before fine-tuning them jointly is more effective than training them jointly from scratch, and references the model-dominance effect and learner collusion as potential reasons.
- Why unresolved: The paper identifies these phenomena but does not provide a detailed analysis of their underlying mechanisms or propose specific solutions for end-to-end training.
- What evidence would resolve it: Systematic experiments comparing different training strategies, detailed analysis of model-dominance effects, and development of novel training methods that mitigate learner collusion.

### Open Question 2
- Question: How does the proposed I2M2 framework scale with an increasing number of modalities, and what are the computational trade-offs?
- Basis in paper: [explicit] The paper acknowledges that I2M2 leads to a corresponding growth in model size with each added modality, resulting in higher computational costs, and suggests future investigation into approaches for larger numbers of modalities.
- Why unresolved: The paper does not provide empirical results or theoretical analysis of I2M2's scalability beyond three modalities, nor does it quantify the computational trade-offs in terms of model size, training time, or inference speed.
- What evidence would resolve it: Experiments with varying numbers of modalities, analysis of computational complexity, and comparison with alternative fusion methods.

### Open Question 3
- Question: Can the robustness of I2M2 to distribution shifts be further improved by incorporating additional mechanisms for handling out-of-distribution samples?
- Basis in paper: [explicit] The paper demonstrates that I2M2 achieves improved performance on OOD test sets compared to other methods, but acknowledges that all models suffer some performance drop.
- Why unresolved: The paper does not explore whether combining I2M2 with other OOD robustness techniques (e.g., adversarial training, uncertainty estimation) could further enhance performance.
- What evidence would resolve it: Experiments combining I2M2 with various OOD robustness techniques, and analysis of their combined effects on model performance and uncertainty calibration.

## Limitations

- Theoretical foundation relies heavily on assumed generative process that may not always capture complex real-world dependency structures effectively
- Method's performance gains over simpler baselines may diminish when scaled to very large datasets or more complex modality combinations
- Computational cost increases significantly with each added modality, potentially limiting practical applicability

## Confidence

**High Confidence**: The framework's core mathematical formulation and the product-of-experts combination mechanism are well-established concepts with clear theoretical grounding. The experimental results showing consistent improvement over unimodal and ensemble baselines across multiple datasets are robust.

**Medium Confidence**: The claim that I2M2 adapts to varying dependency strengths without requiring prior knowledge needs more systematic validation across diverse datasets. The performance advantages over parameter-matched ensemble methods, while demonstrated, could benefit from additional ablation studies.

**Low Confidence**: The theoretical justification for the generative model formulation and the specific role of the selection variable v in different scenarios lacks comprehensive empirical validation.

## Next Checks

1. **Ablation Study**: Systematically remove either the unimodal or multimodal components to quantify their individual contributions across different datasets with varying dependency strengths.

2. **Dependency Strength Analysis**: Create synthetic datasets with controlled inter- and intra-modality dependency strengths to test the framework's adaptation claims more rigorously.

3. **Scaling Experiment**: Evaluate I2M2 on larger, more complex multi-modal datasets (e.g., medical imaging with text reports) to assess its scalability and robustness to increased modality complexity.