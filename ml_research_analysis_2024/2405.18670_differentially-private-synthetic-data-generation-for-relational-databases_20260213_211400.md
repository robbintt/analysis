---
ver: rpa2
title: Differentially Private Synthetic Data Generation for Relational Databases
arxiv_id: '2405.18670'
source_url: https://arxiv.org/abs/2405.18670
tags:
- data
- synthetic
- algorithm
- msyn
- relational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first differentially private algorithm
  for generating synthetic relational databases that preserves both statistical properties
  and referential integrity. The core method learns relationships between individual
  synthetic tables by iteratively minimizing approximation errors in cross-table marginal
  distributions while maintaining referential integrity.
---

# Differentially Private Synthetic Data Generation for Relational Databases

## Quick Facts
- arXiv ID: 2405.18670
- Source URL: https://arxiv.org/abs/2405.18670
- Reference count: 22
- Key outcome: First DP algorithm for relational databases preserving referential integrity and statistical properties

## Executive Summary
This paper introduces a novel differentially private algorithm for generating synthetic relational databases that preserves both statistical properties and referential integrity. The method iteratively learns relationships between individual synthetic tables by minimizing approximation errors in cross-table marginal distributions. Unlike previous approaches that flatten relational databases into master tables, this algorithm maintains the structure of individual tables while establishing relationships between them, resulting in significant space savings and improved scalability. The approach can be combined with any existing DP mechanisms for individual table generation and demonstrates strong utility guarantees with average 3-way marginal query errors as low as 0.1 on real-world datasets.

## Method Summary
The algorithm generates differentially private synthetic relational databases by first applying state-of-the-art DP mechanisms (AIM or MST) to create individual synthetic tables, then establishing relationships between these tables through an iterative optimization process. The core innovation is a method that learns a bi-adjacency matrix representing relationships between tables by minimizing approximation errors in cross-table marginal distributions while maintaining referential integrity. The optimization is solved using projected gradient descent to handle the relaxed continuous problem, followed by unbiased sampling to convert back to the discrete space. The algorithm scales linearly with the number of records and demonstrates convergence rates of O(1/T) through iterative refinement of the relationship matrix.

## Key Results
- Achieves average 3-way marginal query errors as low as 0.1 with reasonable privacy budgets (ϵ=1, ϵrel=2.0)
- Demonstrates linear runtime complexity and scales to high-dimensional data (dmax=10 for MovieLens)
- Successfully preserves referential integrity while maintaining statistical properties on real-world datasets (MovieLens and IPUMS)
- Avoids exponential space complexity of master-table approaches by maintaining individual table structures

## Why This Works (Mechanism)
The algorithm works by treating relational database generation as a constrained optimization problem where the goal is to minimize the distance between marginal distributions of real and synthetic data while satisfying referential integrity constraints. The key insight is that relationships between tables can be represented as a bi-adjacency matrix, which is learned iteratively by minimizing approximation errors in cross-table marginals. By using projected gradient descent on a relaxed continuous version of the problem followed by unbiased sampling, the algorithm efficiently finds high-quality solutions that satisfy both the optimization objective and the discrete constraints required for valid relational databases.

## Foundational Learning
**Differentially Private Mechanisms (AIM, MST)**: These are state-of-the-art methods for generating DP synthetic data for individual tables. Why needed: They provide the foundation for generating private individual tables before establishing relationships. Quick check: Verify that generated tables satisfy (ϵ,δ)-differential privacy.

**k-way Marginal Queries**: These capture statistical relationships between k attributes across tables. Why needed: They define the utility metric for measuring how well the synthetic data preserves statistical properties. Quick check: Compute exact marginals on real data and verify they sum to correct totals.

**Referential Integrity**: This constraint ensures that relationships between tables remain valid in the synthetic data. Why needed: Without this, the synthetic database would be logically inconsistent and unusable. Quick check: Verify that for each relationship (i,j), Bsyn_i,j ∈ {0,1} and each pair has at most one relationship.

**Projected Gradient Descent**: This optimization method handles the constraints by projecting onto the feasible set after each gradient step. Why needed: The optimization problem has both objective and constraint satisfaction requirements. Quick check: Monitor constraint violation throughout iterations to ensure convergence.

**Unbiased Sampling**: This technique converts the relaxed continuous solution back to discrete space while maintaining expected values. Why needed: The final bi-adjacency matrix must be binary to represent valid relationships. Quick check: Run multiple trials and verify empirical frequencies match theoretical probabilities.

## Architecture Onboarding

**Component Map**: Individual DP Tables (AIM/MST) -> Relationship Learning (Algorithm 1) -> Unbiased Sampling (Algorithm 3) -> Synthetic Relational Database

**Critical Path**: The most critical path is from the individual DP table generation through the relationship learning algorithm to the final unbiased sampling step. Any error in the individual tables propagates through the relationship learning, and errors in the relationship learning directly impact the quality of the final synthetic database.

**Design Tradeoffs**: The algorithm trades off between approximation error in marginals and the number of workloads evaluated. More workloads provide better approximation but increase computational cost exponentially with dmax. The privacy budget is also split between individual table generation (ϵ) and relationship learning (ϵrel), requiring careful allocation based on dataset characteristics.

**Failure Signatures**: If the projected gradient descent fails to satisfy constraints, you'll see values outside [0,1] or violation of the referential integrity constraint (multiple relationships between the same record pair). If unbiased sampling is incorrect, you'll observe biased empirical frequencies that don't match theoretical probabilities, leading to systematic errors in the synthetic relationships.

**Three First Experiments**:
1. Run Algorithm 1 on pre-generated individual synthetic tables with known relationships to verify the relationship learning works correctly before adding DP noise
2. Test the unbiased sampling algorithm (Algorithm 3) independently by generating a weighted bi-adjacency matrix and verifying that empirical frequencies match theoretical probabilities P(Bsyn_i,j = 1) = eBsyn_i,j
3. Apply the full pipeline to a small synthetic relational dataset where ground truth relationships are known, measuring both utility (marginal error) and constraint satisfaction

## Open Questions the Paper Calls Out
**Open Question 1**: Can the proposed algorithm be extended to handle relational databases with more than two tables? The paper assumes two tables for simplicity, but this limits applicability to more complex schemas.

**Open Question 2**: How does the algorithm perform when some tables contain non-categorical features? The current formulation focuses on categorical features for k-way marginals and utility guarantees.

**Open Question 3**: Can the algorithm maintain temporal relationships when generating synthetic relational data? The current formulation doesn't account for time-dependent relationships between records.

## Limitations
- The exponential growth of workload with dimensionality (dmax) remains a fundamental limitation despite scalability improvements
- Implementation details for critical components like unbiased sampling and random slicing heuristic are underspecified
- Privacy analysis assumes sequential composition which may be overly conservative compared to advanced composition
- Algorithm is specifically designed for two-table relational databases, limiting applicability to more complex schemas

## Confidence
- **High confidence**: Core algorithmic framework and utility theorems, experimental methodology and results on MovieLens and IPUMS datasets
- **Medium confidence**: Scalability claims and linear runtime complexity, convergence rate of O(1/T)
- **Low confidence**: Practical implementation details of unbiased sampling and random slicing heuristic, applicability to highly dimensional datasets with dmax > 10

## Next Checks
1. Implement and validate the unbiased sampling algorithm (Algorithm 3) by running multiple trials and verifying that empirical frequencies match theoretical probabilities P(Bsyn_i,j = 1) = eBsyn_i,j within statistical tolerance
2. Test the algorithm's scalability on a synthetic dataset with varying dmax values (5, 10, 15, 20) to empirically verify the exponential growth in workload and runtime
3. Conduct a sensitivity analysis on the privacy budget allocation (ϵ, ϵrel) to determine the optimal balance between individual table utility and relational consistency across different dataset sizes and dimensionalities