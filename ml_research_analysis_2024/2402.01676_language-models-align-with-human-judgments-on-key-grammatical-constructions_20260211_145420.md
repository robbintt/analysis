---
ver: rpa2
title: Language models align with human judgments on key grammatical constructions
arxiv_id: '2402.01676'
source_url: https://arxiv.org/abs/2402.01676
tags:
- grammatical
- language
- gram
- ungram
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study re-evaluates large language model (LLM) grammaticality
  judgment performance reported in Dentella et al. (2023), finding that prior conclusions
  about LLM failures were based on flawed methodology.
---

# Language models align with human judgments on key grammatical constructions

## Quick Facts
- arXiv ID: 2402.01676
- Source URL: https://arxiv.org/abs/2402.01676
- Reference count: 11
- Large language models show strong correlation with human grammaticality judgments when properly evaluated

## Executive Summary
This study re-evaluates large language model (LLM) grammaticality judgment performance reported in Dentella et al. (2023), finding that prior conclusions about LLM failures were based on flawed methodology. Using well-established minimal-pair analysis methods, the authors demonstrate that LLMs achieve near-ceiling accuracy (except on center embedding) when evaluated using direct probability measurements rather than metalinguistic prompts. Models show strong correlation (r = -0.74 to -0.67) between surprisal differences and human grammaticality judgments. Additionally, the study reveals that human acceptability judgments show genuine variability rather than being purely normative, with some sentences (like anaphora cases) accepted by substantial participant groups despite being labeled ungrammatical. When evaluated using the exact binary prompt format used for humans, most models (except davinci2) lose their "yes-response bias" and show strong performance. Overall, the results demonstrate that LLMs exhibit robust and human-like grammatical generalization capabilities when properly evaluated.

## Method Summary
The authors evaluated five LLMs (GPT-3.5-turbo, GPT-4, Llama-2-7B, Llama-2-13B, and davinci2) on grammatical constructions from Dentella et al. (2023) including subject islands, adjunct islands, complex NP islands, anaphora, and center embedding. Instead of using metalinguistic prompts that asked models to judge grammaticality directly, they employed a minimal-pair approach measuring log-probability differences between minimal pairs (e.g., grammatical vs. ungrammatical versions). Human participants rated 80 sentences from Dentella et al. (2023) and an additional 120 sentences created by the authors. The authors computed Pearson correlations between model surprisal differences and human acceptability ratings. They also tested models using the exact binary prompt format used for human participants to compare performance across evaluation methods.

## Key Results
- LLMs achieve near-ceiling accuracy (except on center embedding) when evaluated using direct probability measurements rather than metalinguistic prompts
- Models show strong correlation (r = -0.74 to -0.67) between surprisal differences and human grammaticality judgments
- Human acceptability judgments show genuine variability rather than being purely normative, with some sentences accepted by substantial participant groups despite being labeled ungrammatical
- When evaluated using the exact binary prompt format used for humans, most models (except davinci2) lose their "yes-response bias" and show strong performance

## Why This Works (Mechanism)
The study demonstrates that LLMs' apparent failures in grammaticality judgment tasks stem from evaluation methodology rather than genuine limitations. When models are evaluated using probability-based metrics that align with how they were trained (predicting next tokens), they show strong performance that correlates with human judgments. The "yes-response bias" observed in previous studies appears to be an artifact of using metalinguistic prompts rather than a fundamental limitation in the models' grammatical knowledge.

## Foundational Learning
1. **Minimal-pair analysis** - A method for evaluating grammaticality by comparing log-probability differences between grammatical and ungrammatical sentence pairs. Needed to avoid confounding frequency effects with grammatical knowledge. Quick check: Ensure minimal pairs differ only in the grammatical construction being tested.

2. **Surprisal** - The negative log-probability of a word given its context, used as a measure of processing difficulty. Needed to quantify how unexpected each word is in context. Quick check: Lower surprisal indicates higher predictability and potentially better grammaticality.

3. **Island constraints** - Syntactic rules that restrict movement of constituents in certain structural configurations. Needed as test cases for complex grammatical knowledge. Quick check: Verify that islands create genuine processing difficulty in both humans and models.

4. **Center embedding** - A syntactic structure where one clause is embedded within another clause that is itself embedded. Needed as a challenging test case for recursive processing. Quick check: Confirm that center embedding creates consistent difficulty across languages and models.

5. **Anaphora resolution** - The process of determining which noun phrases pronouns refer to. Needed to test models' ability to track dependencies across sentences. Quick check: Ensure that pronoun-antecedent relationships are unambiguous in test materials.

6. **Log-probability differences** - The subtraction of log-probabilities between grammatical and ungrammatical variants. Needed to create a continuous measure of grammaticality that correlates with human judgments. Quick check: Verify that differences are statistically significant across multiple samples.

## Architecture Onboarding

**Component Map:**
Input sentences -> Tokenizer -> Embedding layer -> Transformer blocks -> Output logits -> Log-probability calculation -> Minimal-pair comparison -> Correlation with human judgments

**Critical Path:**
Tokenization → Embedding → Transformer processing → Probability prediction → Log-probability difference calculation → Human correlation

**Design Tradeoffs:**
- Direct probability measurement vs. metalinguistic prompts: Direct measurement aligns with training objectives but requires careful experimental design
- Minimal pairs vs. absolute probabilities: Minimal pairs control for frequency effects but require more test items
- Binary vs. graded human judgments: Binary judgments are simpler but may miss nuanced acceptability differences

**Failure Signatures:**
- Yes-response bias when using metalinguistic prompts
- Poor performance on center embedding across all models
- Correlation drops when using absolute probabilities instead of minimal-pair differences
- Model-specific variations in handling different construction types

**First 3 Experiments:**
1. Replicate minimal-pair analysis with alternative probabilistic metrics (conditional probability, surprisal normalization)
2. Test same methodology across multiple languages and with non-native speaker populations
3. Conduct controlled experiments varying prompt format systematically to isolate specific prompt features

## Open Questions the Paper Calls Out
None

## Limitations
- The minimal-pair approach assumes log-probability differences adequately capture grammatical distinctions, potentially missing frequency effects or model-specific biases
- Correlation between surprisal differences and human judgments (r = -0.74 to -0.67) is strong but leaves unexplained variance in model behavior
- Human judgment variability findings are based only on English native speakers, potentially missing cross-linguistic or cultural influences

## Confidence
- **High confidence**: LLMs achieve near-ceiling accuracy on most grammatical constructions when evaluated with direct probability measurements rather than metalinguistic prompts
- **Medium confidence**: The claim that prior LLM failures were due to flawed methodology rather than genuine limitations
- **Medium confidence**: Human acceptability judgments show genuine variability rather than being purely normative

## Next Checks
1. Replicate the minimal-pair analysis using alternative probabilistic metrics (e.g., conditional probability, surprisal normalization) to confirm that log-probability differences are the most appropriate evaluation measure
2. Test the same methodology across multiple languages and with non-native speaker populations to determine whether human judgment variability is language-specific or universal
3. Conduct controlled experiments varying prompt format systematically (beyond binary vs. graded) to isolate the specific prompt features that eliminate model biases and improve accuracy