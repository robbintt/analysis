---
ver: rpa2
title: 'The Queen of England is not England''s Queen: On the Lack of Factual Coherency
  in PLMs'
arxiv_id: '2402.01453'
source_url: https://arxiv.org/abs/2402.01453
tags:
- plms
- coherency
- knowledge
- prompts
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Pre-trained language models (PLMs) are widely used as knowledge
  bases to retrieve factual information. While prior work focused on measuring PLMs''
  correctness in retrieving facts, this paper investigates an overlooked aspect: the
  coherency of factual knowledge within PLMs.'
---

# The Queen of England is not England's Queen: On the Lack of Factual Coherency in PLMs

## Quick Facts
- arXiv ID: 2402.01453
- Source URL: https://arxiv.org/abs/2402.01453
- Reference count: 29
- Pre-trained language models show poor coherency in factual knowledge, struggling to maintain consistent relational predictions when reversing subject-object pairs

## Executive Summary
This paper investigates a critical limitation of pre-trained language models (PLMs) that has been overlooked in prior research: their ability to maintain coherent factual knowledge. While previous work focused on PLMs' correctness in retrieving facts, this study evaluates whether PLMs can consistently predict related entities in both directions of a relation, even when predictions are factually incorrect. The authors introduce a two-round evaluation methodology where a PLM first predicts an object given a subject and relation, then predicts the subject given the predicted object and the same relation. Results across multiple model families, sizes, and prompt types reveal that PLMs struggle significantly with maintaining relational coherency, particularly for N-1 and N-M relations. The findings suggest that PLMs rely on surface-level pattern matching rather than deep relational understanding, and that improvements in fact retrieval through prompt optimization or model scaling do not translate to better coherency.

## Method Summary
The paper proposes a two-round evaluation process to measure PLM coherency. First, a PLM predicts the object entity given a subject entity and relation using masked language modeling. Second, the PLM predicts the subject entity given the predicted object from the first round and the same relation. Coherency is calculated as the percentage of times the subject prediction in the second round matches the original subject. The study evaluates multiple PLM architectures (BERT, InformBERT, T5, GPT variants) across different sizes, prompt types (manually written, optimized, paraphrased), and relation types (1-1, N-1, N-M, symmetric). Experiments use the T-REx subset of LAMA with 41 relations and include tests with evidence paragraphs to distinguish between parametric and contextual knowledge retrieval.

## Key Results
- PLMs show poor coherency across all prompt types, with significantly worse performance on N-1 and N-M relations compared to 1-1 relations
- Optimized prompts and model scaling improve fact retrieval accuracy but do not improve coherency
- Including evidence paragraphs substantially improves coherency, indicating PLMs better leverage contextual knowledge than parametric knowledge
- Autoregressive PLMs (T5, GPT) perform worse than bidirectional models (BERT) due to their unidirectional training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PLMs struggle with coherent factual knowledge because they rely on surface-level pattern matching rather than deep relational understanding
- Mechanism: When predicting an object entity from a subject and relation, PLMs capture statistical associations between word patterns rather than understanding the bidirectional nature of relations. This leads to predictions that are internally inconsistent when reversed
- Core assumption: The MLM objective trains PLMs to predict masked tokens based on context without enforcing relational consistency
- Evidence anchors:
  - [abstract] "Coherency is not concerned with correctness of the PLMsâ€™ predictions, but with the internal state of knowledge in PLMs and its consistency"
  - [section] "Coherency can be easily calculated for 1-1 relations, but is more challenging, if we consider N-1 or N-M relations"
  - [corpus] Weak - related papers focus on fact retrieval and bias, not relational coherence

### Mechanism 2
- Claim: Contextual knowledge (evidence paragraphs) improves coherency more than parametric knowledge because PLMs can extract explicit relational information from text
- Mechanism: When given evidence paragraphs, PLMs shift from recalling facts from parameters to extracting them from the provided context, which naturally contains coherent relational information
- Core assumption: The evidence paragraphs provide explicit, disambiguated relational statements that PLMs can parse correctly
- Evidence anchors:
  - [abstract] "including an evidence paragraph leads to substantial improvement"
  - [section] "PLMs can fill in the blanks based on the knowledge they have stored in their parameters (parametric knowledge), or based on information that is provided in their inputs (contextual knowledge)"
  - [corpus] Weak - related work focuses on fact extraction but not the parametric vs contextual distinction

### Mechanism 3
- Claim: Optimized prompts improve fact retrieval but harm coherency because they overfit to specific object distributions at the expense of relational understanding
- Mechanism: Prompt optimization techniques learn to bias the model toward specific object entities through prompt engineering, which improves retrieval accuracy but creates an asymmetric knowledge representation that breaks down when reversed
- Core assumption: Optimized prompts create conditional dependencies that don't generalize to the inverse relation
- Evidence anchors:
  - [abstract] "Optimized prompts and scaling models improve fact retrieval but not coherency"
  - [section] "Optimizing prompts leads to better fact retrieval... In this experiment, we investigate whether optimized prompts lead to higher coherency as well"
  - [corpus] Weak - related papers mention prompt bias but don't specifically address the coherency-retrieval tradeoff

## Foundational Learning

- Concept: Bidirectional relations in knowledge graphs
  - Why needed here: Understanding that factual knowledge often involves symmetric relationships (if A is capital of B, then B has capital A) is crucial for grasping why coherency matters
  - Quick check question: What's the difference between 1-1, N-1, and N-M relations in terms of coherency evaluation?

- Concept: Masked Language Modeling objective
  - Why needed here: PLMs are trained to predict masked tokens based on context, which shapes how they represent and retrieve factual knowledge
  - Quick check question: How does the MLM objective differ from autoregressive language modeling in terms of bidirectional context?

- Concept: Parametric vs contextual knowledge in PLMs
  - Why needed here: The paper distinguishes between facts stored in model parameters versus facts extracted from input context, which explains different performance patterns
  - Quick check question: What's the key difference between parametric knowledge and contextual knowledge in PLMs?

## Architecture Onboarding

- Component map: Pre-trained Language Models (BERT, T5 variants) -> Prompt templates -> Fact prediction -> Coherency evaluation -> Analysis
- Critical path: Input prompt -> PLM forward pass -> Entity prediction -> Coherency check -> Aggregate scores
- Design tradeoffs: Bidirectional context (better for coherency) vs unidirectional (better for generation), parametric knowledge (faster but less reliable) vs contextual knowledge (slower but more coherent)
- Failure signatures: Low coherency scores indicate surface-level pattern matching, poor inverse relation handling, or prompt sensitivity
- First 3 experiments:
  1. Baseline coherency evaluation with manually written prompts
  2. Coherency with optimized prompts (to test if better retrieval improves coherency)
  3. Coherency with evidence paragraphs (to test contextual vs parametric knowledge)

## Open Questions the Paper Calls Out
The paper suggests that improving coherency requires advancements at the architectural level, pre-training objectives, and data level. Specifically, it calls for exploration of alternative architectures that better model relational knowledge, pre-training objectives that explicitly encourage bidirectional consistency, and training data that provides more explicit relational information.

## Limitations
- The study focuses on zero-shot evaluation and doesn't explore fine-tuning approaches that might improve coherency
- The coherency metric only measures binary success/failure rather than degrees of similarity between predictions
- The evaluation is limited to English knowledge triples and may not generalize to other languages or domains

## Confidence
- PLMs have poor coherency in factual knowledge: High
- This stems from surface-level pattern matching: Medium
- Evidence paragraphs improve coherency through better context extraction: High
- Optimized prompts harm coherency through asymmetric dependencies: Medium

## Next Checks
1. **Architectural ablation study**: Test whether relation-aware architectures (like Knowledge Graph Attention Networks) show improved coherency compared to standard PLMs on the same evaluation task, to distinguish between training objective and architectural limitations.

2. **Prompt optimization with coherency constraints**: Implement prompt optimization that explicitly maximizes coherency scores rather than just retrieval accuracy, to determine if the prompt-retrieval tradeoff is inherent or an optimization artifact.

3. **Cross-lingual coherency evaluation**: Extend the coherency evaluation to multilingual PLMs and knowledge triples across languages to test whether the observed limitations are universal or language-specific phenomena.