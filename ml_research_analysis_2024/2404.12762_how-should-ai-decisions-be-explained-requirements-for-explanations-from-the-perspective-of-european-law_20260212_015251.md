---
ver: rpa2
title: How should AI decisions be explained? Requirements for Explanations from the
  Perspective of European Law
arxiv_id: '2404.12762'
source_url: https://arxiv.org/abs/2404.12762
tags:
- explanations
- explanation
- legal
- data
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes legal requirements for explainable AI (XAI)
  from three perspectives: fiduciary duties in company law, data protection under
  GDPR, and product safety and liability. The authors examine how each legal domain
  requires different XAI properties and methods, concluding that current XAI methods
  don''t fully satisfy these requirements, particularly regarding correctness (fidelity)
  and confidence estimates.'
---

# How should AI decisions be explained? Requirements for Explanations from the Perspective of European Law

## Quick Facts
- arXiv ID: 2404.12762
- Source URL: https://arxiv.org/abs/2404.12762
- Reference count: 18
- This paper analyzes legal requirements for explainable AI (XAI) from three perspectives: fiduciary duties in company law, data protection under GDPR, and product safety and liability.

## Executive Summary
This paper examines the legal requirements for explainable AI (XAI) from three distinct perspectives in European law: fiduciary duties in company law, data protection under GDPR, and product safety and liability. The authors identify that each legal domain requires different XAI properties and methods, with current XAI techniques falling short of meeting these requirements, particularly regarding correctness (fidelity) and confidence estimates. The paper proposes an extended taxonomy of XAI properties and derives specific requirements for each legal use case, emphasizing the need for interdisciplinary collaboration between legal and technical experts to advance XAI development.

## Method Summary
The authors employ a conceptual analysis approach to examine legal requirements for XAI across three distinct domains: fiduciary duties in company law, data protection under GDPR, and product safety and liability. They analyze existing legal frameworks and their implications for XAI properties, then propose an extended taxonomy of XAI properties and derive specific requirements for each legal use case. The analysis is primarily theoretical, drawing on legal principles and existing literature to identify gaps between current XAI capabilities and legal requirements.

## Key Results
- Current XAI methods don't fully satisfy legal requirements, particularly regarding correctness (fidelity) and confidence estimates
- Three distinct legal domains require different XAI properties and methods
- For fiduciary decisions, XAI should enable plausibility checks of ML-based decisions
- For GDPR compliance, XAI should help users understand and object to decisions
- For product safety and liability, XAI should help identify defects in ML models

## Why This Works (Mechanism)
The paper's approach works by systematically mapping legal requirements to specific XAI properties across different domains. By examining fiduciary duties, GDPR, and product liability separately, the authors can identify unique requirements for each context rather than treating all legal explanations as uniform. This domain-specific analysis reveals that different legal contexts prioritize different XAI properties, such as correctness for fiduciary duties versus comprehensibility for GDPR compliance.

## Foundational Learning
1. **Fiduciary duties in company law** - Understanding obligations of care, loyalty, and prudence in corporate governance
   - Why needed: Forms basis for requirements around correctness and plausibility checking of AI decisions
   - Quick check: Can identify key fiduciary principles applicable to AI decision-making

2. **GDPR rights to explanation** - Legal framework for individual rights regarding automated decision-making
   - Why needed: Defines requirements for user-facing explanations and objection rights
   - Quick check: Can articulate specific GDPR provisions affecting XAI requirements

3. **Product liability frameworks** - Legal principles governing responsibility for defective products
   - Why needed: Establishes requirements for identifying and proving AI system defects
   - Quick check: Can explain how liability concepts apply to ML model failures

4. **XAI property taxonomy** - Classification of explanation properties including correctness, completeness, comprehensibility, etc.
   - Why needed: Provides structured framework for analyzing legal requirements
   - Quick check: Can map specific legal requirements to appropriate XAI properties

5. **Confidence estimation in ML** - Methods for quantifying uncertainty in model predictions
   - Why needed: Critical for meeting legal requirements around correctness and reliability
   - Quick check: Can identify techniques for confidence estimation in different ML architectures

6. **Technical implementation of explanations** - Practical methods for generating human-understandable explanations
   - Why needed: Bridges gap between legal requirements and technical capabilities
   - Quick check: Can evaluate different XAI techniques against specific legal requirements

## Architecture Onboarding
Component map: Legal requirements -> XAI property taxonomy -> Technical capabilities -> Implementation strategies

Critical path: Legal analysis → Property identification → Technical assessment → Gap analysis → Recommendation development

Design tradeoffs: Balancing legal compliance with technical feasibility, managing complexity vs. comprehensibility, addressing multiple stakeholder needs

Failure signatures: Over-simplified explanations failing legal scrutiny, technically correct but legally insufficient explanations, explanations that satisfy one legal domain but violate another

First experiments:
1. Test specific XAI methods against fiduciary duty requirements for correctness and plausibility checking
2. Evaluate user-facing explanations against GDPR requirements for comprehensibility and objection rights
3. Assess defect identification capabilities against product liability requirements

## Open Questions the Paper Calls Out
None

## Limitations
- Legal interpretations rely heavily on conceptual analysis rather than empirical validation with actual legal cases
- Assumes current XAI methods fall short without systematic testing of specific methods against derived requirements
- Taxonomy of XAI properties lacks empirical grounding in how different stakeholder groups actually interpret and use these properties

## Confidence
Medium confidence for identifying distinct legal domains requiring different XAI properties
Medium confidence for claim that current XAI methods don't fully satisfy legal requirements
Low confidence for proposed taxonomy of XAI properties

## Next Checks
1. Conduct empirical studies testing specific XAI methods against derived legal requirements across all three domains (fiduciary duties, GDPR, product liability) to verify claimed gaps

2. Validate proposed taxonomy of XAI properties through user studies with legal experts and stakeholders to ensure properties capture their actual needs and interpretations

3. Perform case studies with real-world AI systems in regulated domains to assess how well current XAI methods perform against identified legal requirements and identify specific areas for improvement