---
ver: rpa2
title: 'Small-E: Small Language Model with Linear Attention for Efficient Speech Synthesis'
arxiv_id: '2406.04467'
source_url: https://arxiv.org/abs/2406.04467
tags:
- speech
- audio
- text
- language
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Small-E, a small language model architecture
  that replaces the standard transformer with a recurrent Linear Causal Language Model
  (LCLM) for efficient speech synthesis. By leveraging LCLM blocks instead of transformers,
  Small-E achieves linear complexity in sequence length, enabling efficient training
  on long audio samples.
---

# Small-E: Small Language Model with Linear Attention for Efficient Speech Synthesis

## Quick Facts
- arXiv ID: 2406.04467
- Source URL: https://arxiv.org/abs/2406.04467
- Reference count: 0
- Small-E achieves 316 kT/s throughput and 18.33 perplexity on Librilight medium

## Executive Summary
Small-E is a small language model architecture designed for efficient speech synthesis that replaces standard transformer blocks with recurrent Linear Causal Language Model (LCLM) blocks. This substitution enables linear complexity in sequence length, allowing efficient training on long audio samples while maintaining competitive performance. The model introduces Position-Aware Cross-Attention (PACA) to address common skip and repeat issues in autoregressive speech synthesis. Small-E demonstrates that high-quality TTS can be achieved with significantly fewer parameters (64M) compared to existing systems while maintaining strong performance metrics.

## Method Summary
The paper proposes Small-E as a decoder-only language model architecture that uses LCLM blocks instead of transformer blocks to achieve linear complexity in sequence length. The LCLM blocks leverage a causal attention mechanism with linear complexity, making them suitable for processing long audio sequences efficiently. To address skip and repeat issues common in autoregressive models, the authors introduce Position-Aware Cross-Attention (PACA), which incorporates positional information into the cross-attention mechanism. The model is trained on the Librilight medium dataset and evaluated against a decoder-only transformer baseline and YourTTS, showing improved efficiency and performance metrics.

## Key Results
- Achieves 316 kT/s throughput with 18.33 perplexity on Librilight medium
- Reduces skip and repeat issues from 14 to 2 instances across 100 utterances using PACA
- Uses only 64M parameters compared to 86M for the transformer baseline and 1.2B for MetaVoice

## Why This Works (Mechanism)
The LCLM blocks replace traditional self-attention with a recurrent mechanism that maintains linear complexity while preserving the ability to model long-range dependencies. The Position-Aware Cross-Attention (PACA) mechanism addresses the fundamental problem of positional information loss in standard cross-attention by explicitly incorporating positional embeddings, which helps the model maintain temporal coherence and reduces the occurrence of skip and repeat artifacts that plague autoregressive speech synthesis models.

## Foundational Learning
- **Linear Causal Language Models**: Use recurrence instead of self-attention to achieve O(n) complexity; needed for efficient long-sequence processing in speech synthesis; quick check: verify that attention complexity scales linearly with sequence length
- **Position-Aware Cross-Attention**: Incorporates positional information into cross-attention to maintain temporal coherence; needed to prevent skip/repeat issues in autoregressive models; quick check: confirm positional embeddings are added before cross-attention computation
- **Autoregressive Speech Synthesis**: Sequential generation of speech tokens conditioned on previous outputs; needed for high-quality, natural-sounding speech; quick check: verify the model generates tokens sequentially rather than in parallel

## Architecture Onboarding
**Component Map**: Input -> LCLM Block -> PACA Layer -> Output Prediction
**Critical Path**: Audio features → LCLM processing → Cross-attention with PACA → Token prediction
**Design Tradeoffs**: LCLM vs Transformers (linear vs quadratic complexity), PACA vs standard cross-attention (improved coherence vs computational overhead)
**Failure Signatures**: Skip/repeat artifacts indicate PACA misalignment, performance degradation on long sequences suggests LCLM capacity issues
**First Experiments**: 1) Measure attention complexity scaling with sequence length, 2) Evaluate skip/repeat frequency with and without PACA, 3) Compare parameter efficiency against decoder-only transformer baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to Librilight medium dataset, restricting generalizability claims
- Comparative analysis lacks thorough examination of state-of-the-art systems like VITS or Glow-TTS
- Subjective evaluation methodology lacks details on rater demographics and reliability measures

## Confidence
- **High Confidence**: Linear complexity improvement over transformers, hardware efficiency metrics, parameter count reduction
- **Medium Confidence**: PACA's effectiveness in reducing skip/repeat issues, overall performance relative to YourTTS
- **Low Confidence**: Competitive performance claims with 1.2B-parameter MetaVoice given scale differences

## Next Checks
1. Conduct cross-domain evaluation on diverse TTS datasets (multi-speaker, multi-lingual, expressive speech) to validate generalization claims
2. Perform ablation studies on PACA with varying attention head configurations and layer depths to identify optimal architecture
3. Implement a controlled subjective evaluation with standardized MOS protocols, including speaker diversity and rater demographic information, comparing against current SOTA systems beyond the specific baseline used