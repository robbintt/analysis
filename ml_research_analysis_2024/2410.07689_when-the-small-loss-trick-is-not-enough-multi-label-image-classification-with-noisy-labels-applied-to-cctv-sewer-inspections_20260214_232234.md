---
ver: rpa2
title: 'When the Small-Loss Trick is Not Enough: Multi-Label Image Classification
  with Noisy Labels Applied to CCTV Sewer Inspections'
arxiv_id: '2410.07689'
source_url: https://arxiv.org/abs/2410.07689
tags:
- noise
- label
- labels
- selection
- sewer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of multi-label image classification
  with noisy labels, specifically in the context of CCTV sewer inspections. The authors
  first adapted three sample selection methods from single-label classification (SLC)
  to multi-label classification (MLC): Co-teaching, CoSELFIE, and DISC.'
---

# When the Small-Loss Trick is Not Enough: Multi-Label Image Classification with Noisy Labels Applied to CCTV Sewer Inspections

## Quick Facts
- arXiv ID: 2410.07689
- Source URL: https://arxiv.org/abs/2410.07689
- Reference count: 40
- Primary result: MHSS achieved mAP of 50.03% and 43.71% on UcMerced and TreeSatAI datasets with 20% and 12.5% noise levels respectively

## Executive Summary
This paper addresses the challenge of multi-label image classification with noisy labels, specifically in the context of CCTV sewer inspections. The authors developed MHSS (Multi-label Hybrid Sample Selection), a novel method that combines class-dependent noise rates and a joint correction criterion to outperform existing sample selection methods. The method shows superior performance in dealing with both synthetic complex noise and real noise in multi-label classification tasks.

## Method Summary
The paper adapts sample selection methods from single-label to multi-label classification, specifically Co-teaching, CoSELFIE, and DISC. The authors then developed MHSS, which incorporates class-dependent noise rates (CDNR) and a joint correction criterion (JCC). The method uses dual TResNetM networks with asymmetric loss, where selection is based on per-class small-loss filtering and correction leverages agreement between both networks. Training is performed for 30 epochs using Adam optimizer with RandAugment.

## Key Results
- MHSS achieved mAP of 50.03% on UcMerced dataset with 20% noise
- MHSS achieved mAP of 43.71% on TreeSatAI dataset with 12.5% noise
- The method outperformed adapted versions of Co-teaching, CoSELFIE, and DISC on both synthetic and real noise scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class-dependent noise rate (CDNR) adapts the forget rate per class, improving label selection accuracy in MLC.
- Mechanism: Instead of using a single global noise rate ε, CDNR assigns ε_c for each class c, and sets τ_c = α · ε_c. This allows the small-loss trick to be tuned more precisely per class.
- Core assumption: Different classes have different noise rates in real-world MLC datasets.
- Evidence anchors:
  - [section] "we modified the forget rate τ to be class dependent. With ε_c the noise rate of class c, it is then defined as: ∀c ∈ C, τ_c := α · ε_c"
  - [abstract] "which incorporates class-dependent noise rates"
- Break condition: If class noise rates are uniform, CDNR adds no benefit and may overfit per-class settings.

### Mechanism 2
- Claim: Joint correction criterion (JCC) improves label refurbishment by leveraging agreement between dual networks.
- Mechanism: A label is considered refurbishable if at least one network deems it so, and both networks apply the correction. This relaxes the entropy threshold and increases corrected label count.
- Core assumption: Two networks can mutually validate refurbishable labels, reducing false positives in correction.
- Evidence anchors:
  - [section] "we utilized a joint correction criterion leveraging the dual network architecture by considering an instance as refurbishable if at least one of the two networks deems it refurbishable"
  - [abstract] "incorporates class-dependent noise rates and a joint correction criterion"
- Break condition: If the two networks converge to similar errors, JCC may reinforce noise instead of correcting it.

### Mechanism 3
- Claim: Combining CDNR and JCC yields better performance than either alone, especially as α increases.
- Mechanism: CDNR improves selection precision; JCC improves correction recall. Together they maintain high selection proportion while rejecting more noise.
- Core assumption: Selection and correction are complementary processes whose joint optimization outperforms isolated tuning.
- Evidence anchors:
  - [section] "our method and CoSELFIE exhibited similar performances on UcMerced and TreeSatAI for an α of 0.25. However, as α increased, our method began to substantially outperform CoSELFIE."
  - [table] "Every component individually improved performances... However, on TreeSatAI, the addition of CDNR alone... decreased performance, same thing with JCC... It's only when combined together that those components had a positive impact."
- Break condition: If either component is broken, the synergy collapses and performance regresses.

## Foundational Learning

- Concept: Multi-label classification (MLC) vs single-label classification (SLC)
  - Why needed here: MLC allows each image to have multiple labels; this changes how noise is modeled and corrected.
  - Quick check question: In MLC, can a single image have both "defect present" and "no defect" labels simultaneously?

- Concept: Small-loss trick and memorization effect
  - Why needed here: Core assumption that noisy labels yield higher loss early in training; this drives sample selection.
  - Quick check question: What is the memorization effect and how does it justify the small-loss trick?

- Concept: Class-dependent noise modeling
  - Why needed here: Real-world MLC data often has class-specific noise; fixed global noise rates are insufficient.
  - Quick check question: Why might uniform noise rates across classes be unrealistic in MLC?

## Architecture Onboarding

- Component map:
  - Input images -> TResNetM backbone -> Sigmoid activation -> Dual networks -> Per-class small-loss filtering -> Joint correction criterion -> Peer network update

- Critical path:
  1. Forward pass both networks on batch
  2. Compute losses per sample
  3. Apply per-class small-loss filtering
  4. Apply JCC to identify refurbishable labels
  5. Update peer networks with selected & corrected labels

- Design tradeoffs:
  - Memory: Dual networks require ~2x memory; batch size reduced to 32 for DISC
  - Computation: Per-class filtering adds overhead but improves precision
  - Hyperparameters: α and entropy threshold θ must be tuned per dataset

- Failure signatures:
  - Low label precision: Over-aggressive selection or bad ε_c estimates
  - High label recall but low mAP: Noise slipping through selection
  - Poor JCC performance: Networks are too similar or both wrong

- First 3 experiments:
  1. Baseline (ASL only) vs Co-teaching (α=1) on clean UcMerced
  2. Co-teaching with varying α on naive vs complex noise (observe performance trend)
  3. MHSS vs CoSELFIE on TreeSatAI with ε=0.125 (test CDNR+JCC synergy)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MHSS change when applied to datasets with different levels of class imbalance beyond those tested in this study?
- Basis in paper: [inferred] The paper tested MHSS on datasets with varying positive label prevalence (20% for UcMerced and 12.5% for TreeSatAI) but did not explore datasets with extreme class imbalance.
- Why unresolved: The study did not include experiments on datasets with significantly different class distributions, limiting the generalizability of the results.
- What evidence would resolve it: Conducting experiments on datasets with a wider range of class imbalances, including those with very few positive labels per class, would provide insights into MHSS's robustness and effectiveness in diverse scenarios.

### Open Question 2
- Question: Can the MHSS method be effectively adapted to handle semi-supervised learning scenarios where only a portion of the data is labeled?
- Basis in paper: [inferred] The paper focuses on fully supervised learning with noisy labels, but the potential for extending MHSS to semi-supervised settings is not explored.
- Why unresolved: The study did not investigate the performance of MHSS in scenarios where labeled data is scarce, which is a common challenge in real-world applications.
- What evidence would resolve it: Evaluating MHSS on semi-supervised datasets and comparing its performance to existing methods in this domain would demonstrate its applicability and potential advantages.

### Open Question 3
- Question: How does the computational efficiency of MHSS compare to other state-of-the-art methods for multi-label classification with noisy labels?
- Basis in paper: [explicit] The paper mentions that experiments on the TreeSatAI dataset were conducted only once due to computational costs, indicating potential efficiency concerns.
- Why unresolved: The study did not provide a detailed analysis of the computational requirements of MHSS compared to other methods, which is crucial for practical applications.
- What evidence would resolve it: Conducting a comprehensive comparison of the computational time and resource usage of MHSS and other methods on datasets of varying sizes would provide insights into its efficiency and scalability.

## Limitations
- The effectiveness of CDNR and JCC relies on accurate class noise rate estimation, which is not addressed in detail
- The dual network architecture's susceptibility to converging to similar errors is not thoroughly explored
- Real-world noise patterns in CCTV sewer inspections may differ from synthetic noise models

## Confidence
- **High confidence**: The mechanism of combining CDNR and JCC to improve performance
- **Medium confidence**: The generalizability of results to other MLC domains beyond sewer inspections
- **Medium confidence**: The claim that uniform noise rates across classes are unrealistic in MLC

## Next Checks
1. **Noise Rate Estimation**: Validate the accuracy of class-dependent noise rate estimation methods on real-world datasets
2. **Convergence Analysis**: Investigate the convergence behavior of dual networks and its impact on JCC effectiveness
3. **Real-world Testing**: Apply MHSS to diverse MLC datasets to assess generalizability beyond sewer inspections