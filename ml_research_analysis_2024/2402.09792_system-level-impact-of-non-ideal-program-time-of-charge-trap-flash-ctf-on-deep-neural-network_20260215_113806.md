---
ver: rpa2
title: System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF) on
  Deep Neural Network
arxiv_id: '2402.09792'
source_url: https://arxiv.org/abs/2402.09792
tags:
- pulse
- weight
- time
- stochastic
- pulses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the impact of non-ideal program time in Charge
  Trap Flash (CTF) devices on deep neural network (DNN) training performance. The
  authors address the challenge that CTF-based weight updates in Resistive Processing
  Unit (RPU) architectures are affected by the number of input pulses (N) and the
  gap between successive pulses (tgap), which can degrade learning accuracy.
---

# System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF) on Deep Neural Network

## Quick Facts
- arXiv ID: 2402.09792
- Source URL: https://arxiv.org/abs/2402.09792
- Authors: S. Shrivastava; A. Biswas; S. Chakrabarty; G. Dash; V. Saraswat; U. Ganguly
- Reference count: 19
- Key outcome: Pulse-train design compensation improves DNN training accuracy by reducing non-ideal program-time error in CTF devices.

## Executive Summary
This paper analyzes how non-ideal program time in Charge Trap Flash (CTF) devices affects deep neural network (DNN) training when used in Resistive Processing Unit (RPU) architectures. The authors demonstrate that CTF weight updates are non-linear with respect to pulse number (N) and pulse gap (t_gap), degrading learning accuracy. They propose a compensation technique that adds trap and de-trap times to input pulses to reduce error. Through simulations on MNIST and Fashion-MNIST datasets, they show that for larger N (~1000), training performance approaches ideal software-level accuracy regardless of t_gap choice, while for lower N (<500), sparse t_gap (>t_critical) improves accuracy at the cost of higher latency.

## Method Summary
The study uses experimental CTF device data showing non-conservation of threshold voltage (V_T) with respect to pulse number and gap. The authors model CTF weight updates with a function incorporating both N and t_gap dependencies, then implement a pulse-train design compensation that adds trap (ttrap) and de-trap (tde-trap) times to each input pulse. They simulate RPU-based DNN training using stochastic matrix multiplication algorithms, comparing classification accuracy across different pulse schemes (N=100, 500, 1000) and gap conditions (true vs sparse) against ideal software training. The minimum viable reproduction requires implementing the CTF weight update model with experimental parameters, the stochastic matrix multiplication algorithm, and training the specified neural network architecture on MNIST and Fashion-MNIST datasets.

## Key Results
- For N ~ 1000, DNN training performance approaches ideal software-level accuracy and is largely independent of t_gap choice
- For N < 500, training accuracy significantly depends on t_gap, with sparse gaps (> t_critical) improving performance at higher latency cost
- Lower noise in weight updates is identified as the key factor improving DNN learning performance in CTF-based RPUs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pulse design compensation improves DNN training accuracy by reducing non-ideal program-time error in CTF devices.
- Mechanism: Adding trap and de-trap times to input pulses prevents charge accumulation in the blocking oxide, making weight updates more predictable and linear with respect to pulse number.
- Core assumption: CTF weight update non-linearity with respect to N can be mitigated by adjusting pulse timing.
- Evidence anchors: [abstract] "We propose a pulse-train design compensation technique to reduce the total error caused by non-ideal program time of CTF and stochastic variance of a network." [section] "To resolve the VT error a trapping time (ttrap) must be added to the pulses. To avoid the pulse interaction de-trapping time (tde-trap) must be provided as the gap between the pulses."
- Break condition: If critical time t_critical cannot be properly characterized or pulse timing cannot be precisely controlled in hardware.

### Mechanism 2
- Claim: Sparse t_gap (> t_critical) reduces stochastic noise in weight updates, improving DNN training performance for lower N.
- Mechanism: Larger t_gap prevents pulse interaction through charge accumulation, leading to lower variance in weight updates.
- Core assumption: Pulse interaction is the primary source of stochastic noise in weight updates.
- Evidence anchors: [abstract] "However, for lower N (<500), learning performance depends on T_gap of the pulses." [section] "For t_gap longer than a critical time (tcritical), the VT saturates i.e., VT is independent of tgap."
- Break condition: If t_gap cannot be consistently larger than t_critical due to hardware constraints.

### Mechanism 3
- Claim: Higher N (~1000) makes DNN training less sensitive to t_gap variations due to reduced stochastic encoding error.
- Mechanism: As N increases, stochastic variance decreases (proportional to 1/√N), making t_gap impact less significant.
- Core assumption: Reduction in stochastic variance with higher N outweighs non-ideal program time effects.
- Evidence anchors: [abstract] "We find that for larger N (~1000), learning performance approaches the ideal (software-level) training level and, therefore, is not much impacted by the choice of tgap used to implement RPU-based weight updates." [section] "The stochastic encoding error is reduced with the increase in N (∆stoch) as σ/μ = 1/√N [12]."
- Break condition: If non-ideal program time effect increases faster than reduction in stochastic variance, or hardware limits maximum N.

## Foundational Learning

- Concept: Charge Trap Flash (CTF) operation and non-ideal program time characteristics
  - Why needed here: Understanding how CTF stores charge and how non-ideal program time affects weight updates is fundamental to the proposed compensation technique.
  - Quick check question: How does accumulation of charges in the blocking oxide of CTF affect threshold voltage (VT) when pulse number N is increased?

- Concept: Stochastic computing in RPU architecture
  - Why needed here: The RPU architecture relies on stochastic pulse trains for weight updates, and the compensation technique directly impacts this process.
  - Quick check question: How does the length of the stochastic pulse train (N) affect the variance of the weight update in RPU-based DNN training?

- Concept: Deep Neural Network (DNN) training with back-propagation
  - Why needed here: The ultimate goal is to improve DNN training accuracy, which relies on quality of weight updates in each layer.
  - Quick check question: How do inaccuracies in weight updates propagate through the network during back-propagation and affect final training accuracy?

## Architecture Onboarding

- Component map: CTF device (with trapping/de-trapping mechanisms) -> Pulse generator (with N and t_gap control) -> RPU crossbar array -> DNN layers (with stochastic weight updates)
- Critical path: Pulse generation -> CTF weight update (with trap/de-trap compensation) -> Stochastic matrix multiplication in RPU -> DNN weight update via back-propagation
- Design tradeoffs: Higher N improves accuracy but increases latency and energy; sparse t_gap reduces noise but also increases latency; complex pulse design adds hardware complexity
- Failure signatures: Inaccurate weight updates leading to poor DNN accuracy; increased variance in weight updates; inability to achieve desired t_gap due to hardware limitations
- First 3 experiments:
  1. Characterize relationship between pulse number N and threshold voltage VT in CTF device without compensation to establish baseline non-ideality.
  2. Implement pulse design compensation technique (trap and de-trap times) and measure improvement in VT linearity and reduction in variance for different N values.
  3. Simulate RPU-based DNN training with compensated CTF model for MNIST and Fashion-MNIST datasets, comparing accuracy for different N and t_gap combinations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal t_gap (de-trap time) value for maximizing DNN training accuracy in CTF-based RPUs while minimizing latency?
- Basis in paper: [explicit] The paper states that for lower N (<500), performance depends on t_gap, with sparse t_gap (>t_critical) improving accuracy at the cost of higher latency.
- Why unresolved: The paper uses t_gap = 10 ms as maximum of all t_critical values but doesn't provide optimal value or range balancing accuracy and latency.
- What evidence would resolve it: Systematic experiments varying t_gap values and measuring trade-off between training accuracy and latency for different N values.

### Open Question 2
- Question: How do device process, voltage, and temperature (PVT) variations affect non-ideal program time behavior and DNN training performance in CTF-based RPUs?
- Basis in paper: [explicit] The paper mentions that curve-fitting constants (A, B, C1 & C2) strongly depend on pulse width and are subject to PVT variations, but doesn't explore impact on DNN training.
- Why unresolved: The study uses fixed experimental data and doesn't account for PVT variations occurring in real-world implementations.
- What evidence would resolve it: Experiments measuring non-ideal program time behavior and DNN training performance across range of PVT conditions, developing compensation techniques robust to these variations.

### Open Question 3
- Question: How does the proposed pulse-train design technique compare to other potential compensation methods for non-ideal program time in CTF-based RPUs?
- Basis in paper: [explicit] The paper proposes a pulse-train design technique adding trap and de-trap times to each input pulse, but doesn't compare it to other potential compensation methods.
- Why unresolved: The study focuses on one specific compensation technique and doesn't explore relative merits of alternative approaches.
- What evidence would resolve it: Comparative studies of proposed pulse-train design technique against other potential compensation methods (adaptive pulse width control, charge trapping mitigation strategies) in terms of training accuracy, energy efficiency, and latency.

## Limitations

- Core mechanism claims rely on experimental CTF data from reference [8] that is not fully specified in the paper
- Relationship between trap/de-trap times and hardware implementation feasibility is not addressed
- Generalizability of results beyond MNIST and Fashion-MNIST datasets remains unproven
- Model assumes perfect control over pulse timing, which may not be achievable in actual hardware implementations

## Confidence

- High confidence: The fundamental observation that CTF non-idealities affect DNN training accuracy through non-linear weight updates
- Medium confidence: The effectiveness of pulse-train compensation techniques in improving accuracy for larger N values (~1000)
- Low confidence: The scalability of proposed solutions to larger, more complex DNN architectures and real-world hardware constraints

## Next Checks

1. Validate the CTF weight update model (dVT/dn = P(VT,pw) * Q(tgap,pw)) against independent experimental data from different CTF devices to confirm the proposed compensation technique generalizes beyond the specific device characteristics used in this study.

2. Implement the pulse design compensation in an actual CTF hardware prototype to measure the gap between simulated predictions and real-world performance, particularly focusing on the critical time t_critical and its variability across devices.

3. Extend the evaluation to deeper neural network architectures (CNNs, ResNet) and more complex datasets (CIFAR-10, ImageNet) to assess whether the proposed compensation technique maintains effectiveness as network complexity increases.