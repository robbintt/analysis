---
ver: rpa2
title: Reframing Tax Law Entailment as Analogical Reasoning
arxiv_id: '2401.06715'
source_url: https://arxiv.org/abs/2401.06715
tags:
- reasoning
- analogy
- case
- statutory
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper reframes statutory reasoning as an analogy task, combining
  two statutory reasoning instances into quadruples, expanding the dataset by two
  orders of magnitude. The authors evaluate several approaches including GPT models,
  vector offset methods, and binary classification with T5-Large.
---

# Reframing Tax Law Entailment as Analogical Reasoning

## Quick Facts
- arXiv ID: 2401.06715
- Source URL: https://arxiv.org/abs/2401.06715
- Authors: Xinrui Zou; Ming Zhang; Nathaniel Weir; Benjamin Van Durme; Nils Holzenberger
- Reference count: 40
- Primary result: Achieved 57% accuracy on statutory reasoning task using retrieval-augmented analogy models, improving upon prior work

## Executive Summary
This paper reframes statutory reasoning as an analogy task by combining two instances of statutory reasoning into quadruples, expanding the dataset size by two orders of magnitude. The authors evaluate multiple approaches including GPT models, vector offset methods, and binary classification with T5-Large on the SARA dataset. They demonstrate that while GPT models struggle with genuine analogical reasoning, retrieval-augmented analogy models can achieve improved performance on statutory reasoning tasks, reaching 57% accuracy compared to prior work.

## Method Summary
The authors reframe statutory reasoning as an analogy task by generating quadruples from the SARA dataset, where each quadruple combines two statute-case pairs. They evaluate several approaches: GPT models (zero-shot, few-shot, chain-of-thought), vector offset methods using sentence embeddings, and binary classification with T5-Large. Finally, they solve the original statutory reasoning task by retrieving similar cases from training data and applying analogy models with majority voting. The dataset expands from N to N² examples through this quadruple generation process.

## Key Results
- Dataset expanded from N to N² examples through quadruple generation
- GPT models struggled with analogy task, often reducing it to separate entailment problems
- Retrieval-augmented analogy models achieved 57% accuracy on statutory reasoning
- Vector offset methods showed no significant improvement over random baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Analogy quadruples increase dataset size while preserving task difficulty
- Mechanism: By combining two statutory reasoning pairs into a quadruple, the dataset grows from N to N² examples, and models must learn to compare relationships between pairs rather than individual cases
- Core assumption: Statutory reasoning pairs share sufficient structural similarity to enable meaningful analogical comparisons
- Evidence anchors: [abstract] "We re-frame statutory reasoning as an analogy task, where each instance of the analogy task involves a combination of two instances of statutory reasoning. This increases the dataset size by two orders of magnitude"

### Mechanism 2
- Claim: Retrieval-augmented analogy models improve statutory reasoning accuracy
- Mechanism: Similar cases are retrieved from training data, converted to analogy quadruples, and processed by analogy models to predict new case labels through majority voting
- Core assumption: Similar prior cases contain relevant analogical patterns that can transfer to new cases
- Evidence anchors: [abstract] "Finally, we come back to statutory reasoning, solving it with a combination of a retrieval mechanism and analogy models, and showing some progress on prior comparable work"

### Mechanism 3
- Claim: Vector offset methods can capture analogical relationships between statute-case pairs
- Mechanism: Sentence embeddings of statutes and cases are converted to offset vectors, and cosine similarity between offsets indicates analogical relationships
- Core assumption: Semantic relationships between statutes and cases can be represented as vector differences in embedding space
- Evidence anchors: [section] "Similar to the modeling assumption of [22], the relationship between (S1,C1) and (S2,C2) can be expressed as a relationship between vectors in embedding space"

## Foundational Learning

- Concept: Textual entailment
  - Why needed here: The original SARA dataset is based on entailment classification, which must be understood to reframe as analogy
  - Quick check question: Can you explain the difference between entailment and contradiction in the context of tax law statutes?

- Concept: Vector similarity and cosine distance
  - Why needed here: Vector offset methods rely on computing similarities between embedding differences
  - Quick check question: How would you compute the similarity between two vectors representing statute-case relationships?

- Concept: Retrieval mechanisms (BM25 and DPR)
  - Why needed here: The retrieval-augmented approach requires understanding how to find similar cases in the training set
  - Quick check question: What is the key difference between sparse retrieval (BM25) and dense retrieval (DPR)?

## Architecture Onboarding

- Component map: SARA dataset → Quadruple generation → Analogy models (GPT, vector offset, T5) → Retrieval mechanism → Statutory reasoning prediction
- Critical path: Quadruple generation → Analogy model training → Retrieval-augmented prediction pipeline
- Design tradeoffs: Large dataset expansion vs. potential noise in analogical relationships; complex retrieval vs. simpler direct classification
- Failure signatures: Accuracy near 50% (random baseline), GPT models defaulting to separate entailment reasoning, vector offset methods showing no improvement over random
- First 3 experiments:
  1. Generate quadruples from SARA dataset and verify size expansion
  2. Test GPT models on analogy task with different prompts
  3. Compare vector offset method accuracy to random baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can large language models be effectively prompted to engage in genuine analogical reasoning rather than reducing it to separate entailment problems?
- Basis in paper: [explicit] The paper observes that GPT models often simplify the analogy task into two separate entailment problems for (S1, C1) and (S2, C2), rather than capturing the underlying analogical relationships.
- Why unresolved: The paper suggests that LLMs may not adequately grasp the concept of an analogy relationship between two pairs, but does not provide a definitive solution or method to improve this.

### Open Question 2
- Question: What are the most effective retrieval mechanisms and similarity thresholds for applying analogy models to statutory reasoning tasks?
- Basis in paper: [explicit] The paper experiments with BM25 and DPR retrieval mechanisms and varying k values, but does not provide a comprehensive analysis of optimal configurations.
- Why unresolved: The paper suggests that methods using more information (e.g., statute+context+hypothesis) achieve higher accuracies, but does not explore the full parameter space or compare with alternative retrieval strategies.

### Open Question 3
- Question: How can the integration of analogy models and retrieval mechanisms be further optimized to improve statutory reasoning performance?
- Basis in paper: [inferred] The paper suggests that combining analogy models with retrieval mechanisms could lead to better performance, but the current approach uses a simple majority vote without weighting.
- Why unresolved: The paper proposes a weighted approach using scores from BM25 and DPR as a potential improvement, but does not implement or evaluate this method.

## Limitations

- Dataset expansion through quadruple generation may introduce noise if statute-case pairs lack sufficient structural similarity
- GPT models demonstrated fundamental limitations in performing genuine analogical reasoning
- Vector offset methods failed to show improvement, suggesting embeddings may not capture relevant legal relationships

## Confidence

- Medium confidence for analogy quadruples preserving meaningful relationships during N to N² expansion
- Low confidence for GPT models' ability to perform analogy tasks in legal reasoning contexts
- Medium confidence for retrieval-augmented approach achieving 57% accuracy

## Next Checks

1. Conduct structural similarity analysis of statute-case pairs before and after quadruple generation to validate meaningful analogical relationships
2. Systematically test GPT prompt variations to determine if prompt design or model limitations cause poor analogy performance
3. Perform ablation studies on different embedding models to evaluate whether vector offset methods are limited by embedding choice or fundamental approach