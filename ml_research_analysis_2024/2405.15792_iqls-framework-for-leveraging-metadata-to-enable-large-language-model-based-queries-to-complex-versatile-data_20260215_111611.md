---
ver: rpa2
title: 'IQLS: Framework for leveraging Metadata to enable Large Language Model based
  queries to complex, versatile Data'
arxiv_id: '2405.15792'
source_url: https://arxiv.org/abs/2405.15792
tags:
- data
- iqls
- agent
- user
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Intelligent Query and Learning System (IQLS) addresses the
  challenge of complex data retrieval in domains like logistics, where interconnected
  real-time data requires domain expertise and significant effort. IQLS simplifies
  this by enabling natural language queries through an LLM-powered agent that iteratively
  filters data based on metadata and schema.
---

# IQLS: Framework for leveraging Metadata to enable Large Language Model based queries to complex, versatile Data

## Quick Facts
- arXiv ID: 2405.15792
- Source URL: https://arxiv.org/abs/2405.15792
- Authors: Sami Azirar; Hossam A. Gabbar; Chaouki Regoui
- Reference count: 0
- Primary result: IQLS enables natural language queries across structured data types using hierarchical metadata filtering, achieving results in 5–20 minutes locally and under 3 minutes via API

## Executive Summary
The Intelligent Query and Learning System (IQLS) addresses the challenge of complex data retrieval in domains like logistics, where interconnected real-time data requires domain expertise and significant effort. IQLS simplifies this by enabling natural language queries through an LLM-powered agent that iteratively filters data based on metadata and schema. Unlike one-shot retrieval methods, IQLS uses a hierarchical, step-by-step approach to select relevant data sources, resources, and attributes, improving context awareness and user control. The system supports diverse interfaces for multimodal data retrieval, including route planning under constraints using a modified Dijkstra algorithm.

## Method Summary
IQLS uses a three-step process: query classification based on TaskType and Objective, hierarchical data filtering through DataSource → Resource → Attribute selection, and interface selection for data retrieval and task fulfillment. The system leverages metadata and schema stored in a Neo4J graph database to guide an LLM agent through iterative filtering decisions rather than processing all data at once. Pydantic classes with validators enforce structured outputs and enable refinement loops when parsing fails. The framework supports diverse interfaces including information retrieval, law retrieval, internal document querying, route monitoring, and route calculation under constraints.

## Key Results
- Reduces need for complex SQL queries by enabling natural language querying across structured data types
- Tested on Canadian logistics data with geospatial, visual, tabular, and text data
- Achieves results in 5–20 minutes locally and under 3 minutes via API deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative, hierarchical filtering reduces computational load compared to one-shot retrieval
- Mechanism: The IQLS system guides an LLM agent through multiple small, context-aware decisions instead of processing all data at once
- Core assumption: Metadata accurately describes data content and relationships, enabling meaningful filtering decisions at each step
- Evidence anchors:
  - [abstract] "Instead of one-shot data retrieval, the IQLS guides the LLM stepwise from query understanding through data filtering to the final data retrieval and application."
  - [section] "The agent selects the data sources based on the user query and the abstraction of the data... This approach allows for filtering large datasets without relying on encoding them into latent spaces."

### Mechanism 2
- Claim: Pydantic classes with validators enforce structured outputs and improve parsing reliability
- Mechanism: The system uses Pydantic models to define interfaces and constraints. The agent generates objects (like Driver) with typed attributes and constraints
- Core assumption: The LLM can generate valid Pydantic-compliant outputs with sufficient prompt engineering and context
- Evidence anchors:
  - [section] "The IQLS uses the Driver and the ConstraintActions classes... Each action also includes an operation, e.g. multiply and a Value... The action validators ensure that the attribute is indeed the right Driver's attribute."
  - [abstract] "Combined with the output parsing via Pydantic classes, as well as the improved refinement process makes the IQLS versatile in its capabilities."

### Mechanism 3
- Claim: Integration of multiple data sources and types enables comprehensive, context-aware queries
- Mechanism: IQLS abstracts diverse data (geospatial, visual, tabular, text) from sources like NRN, 511 API, CanLII, and internal documents
- Core assumption: Data from different sources can be meaningfully integrated and queried semantically despite format differences
- Evidence anchors:
  - [abstract] "the IQLS is showcased in a case study on the Canadian logistics sector, allowing geospatial, visual, tabular and text data to be easily queried semantically in natural language."
  - [section] "The IQLS can deal with not only complex queries but also complex data... For this, the case study focuses on four main Data Sources... For regulation data, the CanLII API is utilized..."

## Foundational Learning

- Concept: Graph databases for metadata storage and hierarchical data representation
  - Why needed here: IQLS uses Neo4J to store data abstractions (TaskTypes, DataSources, Resources, Attributes) with relationships, enabling efficient traversal during the iterative filtering process
  - Quick check question: How would you represent a "Resource" node that links to a specific SQL table and contains metadata about its columns?

- Concept: Pydantic models for structured output generation and validation
  - Why needed here: The system defines interfaces and constraints as Pydantic classes. The LLM generates objects conforming to these schemas, and validators check type and constraint compliance, triggering refinement loops
  - Quick check question: What happens if the agent generates a Driver object where a numeric attribute is a string?

- Concept: Retrieval-Augmented Generation (RAG) for document and law interface queries
  - Why needed here: For interfaces like Law Retrieval and Internal Document Retrieval, IQLS uses RAG with vector embeddings (BGE + FAISS) to query large document collections semantically when tabular filtering isn't feasible
  - Quick check question: Why might RAG be preferred over direct SQL queries for the CanLII legal database interface?

## Architecture Onboarding

- Component map: Graph Database (Neo4J) -> CPU Server -> GPU Server -> Interfaces (Information Retrieval, Law, Internal Document, Route Monitoring, Route Planning) -> Data Sources (NRN, 511 API, CanLII API, Internal Documents)

- Critical path: Query → TaskType/Objective Classification → Data Source Selection → Resource Selection → Attribute Selection → Interface Selection → Data Retrieval/Processing → Result Presentation

- Design tradeoffs:
  - Local vs API LLM hosting: Local gives more control over logits and output format but slower inference (5-20 min vs <3 min API)
  - Hierarchical filtering vs one-shot retrieval: Iterative reduces computational load but adds complexity; one-shot is simpler but token-limited
  - Structured vs unstructured data support: IQLS excels at structured data with metadata but requires additional RAG layers for unstructured documents

- Failure signatures:
  - Incomplete or inaccurate metadata leading to wrong data source/resource/attribute selection
  - LLM generating invalid Pydantic outputs causing refinement loop failure
  - Token length exceeded during one-shot processing attempts
  - Interface selection mismatch (e.g., trying spatial route planning without NRN data)

- First 3 experiments:
  1. Test basic query classification: Input "Find the fastest route from Toronto to Ottawa avoiding ice" and verify correct TaskType (Route Planning) and Objective (Time) assignment
  2. Validate data filtering hierarchy: With a simple query, trace agent selections through DataSource → Resource → Attribute layers using Neo4J visualization
  3. Test Pydantic output parsing: Create a query requiring a Driver object, check if agent generates valid Pydantic instance, then introduce a constraint violation to test refinement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the IQLS framework perform in terms of accuracy and efficiency when applied to domains with significantly more complex or unstructured data compared to the Canadian logistics case study?
- Basis in paper: [inferred] The paper mentions that the IQLS can be adapted to any domain and is bounded only by data structure requirements, but does not provide detailed performance metrics for domains with more complex or unstructured data
- Why unresolved: The paper does not include empirical results or comparative studies for domains with significantly more complex or unstructured data
- What evidence would resolve it: Empirical studies comparing IQLS performance in domains with varying levels of data complexity, including unstructured data, would provide evidence

### Open Question 2
- Question: What are the specific limitations and challenges of the IQLS framework when dealing with data sources that have a high degree of heterogeneity or require extensive preprocessing?
- Basis in paper: [inferred] The paper notes that the IQLS can handle diverse data sources but does not delve into the specific limitations or challenges when dealing with highly heterogeneous data sources
- Why unresolved: The paper does not provide detailed case studies or examples of handling highly heterogeneous data sources
- What evidence would resolve it: Detailed case studies or examples showcasing the IQLS handling highly heterogeneous data sources would provide evidence

### Open Question 3
- Question: How does the IQLS framework ensure data privacy and security when integrating with sensitive or proprietary data sources?
- Basis in paper: [explicit] The paper mentions that the IQLS is suitable for privacy-sensitive applications and can host the agent on the cloud while keeping data local, but does not provide detailed mechanisms for ensuring data privacy and security
- Why unresolved: The paper does not provide detailed mechanisms or protocols for ensuring data privacy and security when integrating with sensitive or proprietary data sources
- What evidence would resolve it: Detailed descriptions of data privacy and security mechanisms or protocols implemented in the IQLS would provide evidence

## Limitations

- The modified Dijkstra algorithm for route planning under constraints is not detailed sufficiently for reproduction
- Local inference times (5-20 minutes) are significantly slower than API deployment (<3 minutes), raising questions about scalability
- Pydantic model implementations for Driver and ConstraintActions are referenced but not fully specified, making validation logic unclear

## Confidence

- High confidence: The hierarchical metadata-based filtering approach (Mechanism 1) is well-supported by the abstract and methodology section
- Medium confidence: The Pydantic validation mechanism (Mechanism 2) is described but lacks complete implementation details for full verification
- Low confidence: The integration of multiple heterogeneous data sources (Mechanism 3) is claimed but the corpus shows weak support for similar integrated querying systems

## Next Checks

1. Test query classification accuracy with edge cases (queries that could belong to multiple TaskTypes) to validate the TaskType/Objective classification step
2. Reproduce the data filtering hierarchy with a simple query and trace the agent's selections through the Neo4J graph to verify correct DataSource → Resource → Attribute progression
3. Validate the refinement loop by creating a query that requires a Driver object with intentional constraint violations to test Pydantic validation and LLM correction capabilities