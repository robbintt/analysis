---
ver: rpa2
title: 'DS-Agent: Automated Data Science by Empowering Large Language Models with
  Case-Based Reasoning'
arxiv_id: '2402.17453'
source_url: https://arxiv.org/abs/2402.17453
tags:
- data
- ds-agent
- train
- valid
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DS-Agent is an automatic framework that integrates large language
  models (LLMs) and case-based reasoning (CBR) to solve data science tasks. It operates
  in two stages: development and deployment.'
---

# DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning

## Quick Facts
- arXiv ID: 2402.17453
- Source URL: https://arxiv.org/abs/2402.17453
- Reference count: 40
- Primary result: DS-Agent achieves 100% success rate in development stage and 36% improvement in deployment stage one-pass rate across LLMs

## Executive Summary
DS-Agent is an automatic framework that integrates large language models (LLMs) and case-based reasoning (CBR) to solve data science tasks. It operates in two stages: development and deployment. In the development stage, DS-Agent uses CBR to iteratively improve experiment plans by retrieving cases from Kaggle, revising them based on execution feedback, and reusing them for performance improvement. In the deployment stage, it simplifies CBR to adapt past successful solutions for direct code generation, significantly reducing demands on foundational LLM capabilities.

## Method Summary
DS-Agent is a two-stage framework for automated data science. In the development stage, it employs a CBR-based iteration pipeline that retrieves relevant cases from Kaggle, revises them based on execution feedback, and reuses them to iteratively improve experiment plans. In the deployment stage, it simplifies CBR to adapt past successful solutions for direct code generation, eliminating the need for extensive reasoning. The framework achieves a 100% success rate in the development stage and a 36% improvement on average one-pass rate across alternative LLMs in the deployment stage.

## Key Results
- 100% success rate in development stage with GPT-4
- 36% improvement in average one-pass rate across alternative LLMs in deployment stage
- Best performance rank with $1.60 and $0.13 cost per run with GPT-4 for development and deployment stages respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CBR framework improves LLM performance by iteratively revising experiment plans based on execution feedback
- Mechanism: CBR enables DS-Agent to retrieve relevant cases, reuse them for solution generation, evaluate effectiveness through execution, and revise solutions based on feedback
- Core assumption: Execution feedback provides meaningful signals for case retrieval and solution revision
- Evidence anchors:
  - [abstract] "CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism"
  - [section 3.1] "DS-Agent benefits from the CBR paradigm in two aspects... CBR offers a flexible learning mechanism by retaining successful solution cases into the human insight case bank"
  - [corpus] Weak evidence - no direct corpus support for execution feedback mechanism

### Mechanism 2
- Claim: Simplified CBR in deployment stage reduces demands on LLM capabilities by leveraging past successful solutions
- Mechanism: DS-Agent retrieves similar past successful cases and makes minor modifications for adaptation, eliminating need for extensive reasoning
- Core assumption: Past successful solutions can be adapted with minimal changes to solve similar tasks
- Evidence anchors:
  - [abstract] "DS-Agent employs a simplified CBR framework for a low-resource scenario, where the task is code generation directly in response to the user's task requirements without iterative revise based on execution feedback"
  - [section 3.2] "DS-Agent retrieves and reuses past successful solutions collected from the development stage for the current task"
  - [corpus] Weak evidence - no direct corpus support for simplified CBR deployment mechanism

### Mechanism 3
- Claim: Human insight case bank provides expert knowledge that enhances LLM problem-solving capabilities
- Mechanism: DS-Agent collects expert solutions from Kaggle competitions and uses them as foundational knowledge for experiment planning
- Core assumption: Kaggle solutions contain transferable patterns applicable to new tasks
- Evidence anchors:
  - [section 3.1] "Kaggle emerges as a pivotal resource... offers a rich repository of expert insights and solutions... By integrating these practical, expert examples into LLM agents, we can significantly improve their capability to solve complex data science tasks"
  - [section B.2] "We collect expert insights from Kaggle... select a total of 12 recently completed Kaggle competitions... crawl the technical report shared by the top-10 winner teams"
  - [corpus] Weak evidence - no direct corpus support for Kaggle knowledge transfer mechanism

## Foundational Learning

- Concept: Case-Based Reasoning (CBR) framework
  - Why needed here: Provides systematic approach for problem-solving by retrieving, reusing, revising, and retaining cases
  - Quick check question: What are the four steps in the 4R cycle of CBR?
- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Establishes baseline for comparison with CBR, highlighting CBR's additional feedback mechanism
  - Quick check question: How does CBR differ from RAG in terms of feedback incorporation?
- Concept: Large Language Model (LLM) prompting techniques
  - Why needed here: Essential for implementing Planner, RankReviser, and Adapter components in DS-Agent
  - Quick check question: What prompt engineering techniques are used to ensure LLMs generate executable Python code?

## Architecture Onboarding

- Component map: Retriever → RankReviser → Planner → Programmer → Debugger → Logger (development stage); Retriever → Adapter (deployment stage)
- Critical path: Retriever → Planner → Programmer → Execution → Logger → RankReviser (iterative loop)
- Design tradeoffs: Development stage prioritizes performance through iteration vs. deployment stage prioritizes efficiency through simplification
- Failure signatures: Execution errors, case retrieval failures, ranking mechanism breakdowns, code generation issues
- First 3 experiments:
  1. Test basic CBR pipeline with simple classification task
  2. Validate RankReviser effectiveness with controlled feedback scenarios
  3. Evaluate simplified CBR deployment with held-out tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the long-term performance trends of DS-Agent when the case bank B becomes significantly large?
- Basis in paper: [inferred] The paper mentions that DS-Agent archives successful solutions into the agent case bank B during the development stage and reuses them in the deployment stage, but does not discuss performance trends with an expanding case bank.
- Why unresolved: The paper focuses on demonstrating DS-Agent's effectiveness with a fixed set of cases and does not explore scalability or performance evolution as the case bank grows over time.
- What evidence would resolve it: Empirical studies tracking DS-Agent's performance metrics (e.g., success rate, mean rank) across multiple deployment tasks as the case bank size increases would provide insights into scalability and long-term effectiveness.

### Open Question 2
- Question: How does DS-Agent handle tasks that involve data modalities not present in the collected Kaggle cases?
- Basis in paper: [explicit] The paper states that DS-Agent can integrate the latest human insights into the case bank C to solve data science tasks related to new data modalities, such as graph data.
- Why unresolved: While the paper suggests flexibility in incorporating new data modalities, it does not provide empirical evidence or detailed mechanisms for how DS-Agent adapts to completely unseen data types.
- What evidence would resolve it: Experiments testing DS-Agent's performance on tasks with novel data modalities (e.g., graph data) that were not included in the original case bank would demonstrate its adaptability and robustness.

### Open Question 3
- Question: What is the impact of varying the number of retrieved cases (k) on DS-Agent's performance in the development stage?
- Basis in paper: [inferred] The paper sets k = 5 for retrieving cases but does not explore how different values of k affect performance.
- Why unresolved: The optimal number of retrieved cases for balancing retrieval relevance and computational efficiency is not investigated.
- What evidence would resolve it: Conducting experiments with different values of k (e.g., k = 3, 5, 10) and analyzing the corresponding performance metrics would identify the optimal retrieval size for maximizing DS-Agent's effectiveness.

## Limitations
- Limited experimental validation of CBR mechanisms, particularly the execution feedback loop
- Potential domain specificity from relying solely on Kaggle competition solutions
- Lack of analysis on how case bank composition affects performance and potential biases

## Confidence
- High Confidence: Framework architecture and two-stage approach are well-specified
- Medium Confidence: Empirical results likely accurate but real-world applicability uncertain
- Low Confidence: Mechanism by which execution feedback improves performance lacks direct validation

## Next Checks
1. **Cross-Domain Generalization Test**: Evaluate DS-Agent on data science tasks from domains completely different from Kaggle competitions to assess transferability of expert knowledge.
2. **Feedback Mechanism Ablation Study**: Conduct controlled experiments where execution feedback is systematically varied to empirically validate the claim that feedback drives iterative improvement.
3. **Cost-Performance Tradeoff Analysis**: Break down computational costs by pipeline stage and test with smaller models to determine minimum requirements for successful task completion.