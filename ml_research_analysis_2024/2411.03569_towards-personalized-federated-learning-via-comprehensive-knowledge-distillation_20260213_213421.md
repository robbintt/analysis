---
ver: rpa2
title: Towards Personalized Federated Learning via Comprehensive Knowledge Distillation
arxiv_id: '2411.03569'
source_url: https://arxiv.org/abs/2411.03569
tags:
- knowledge
- data
- local
- personalized
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FedCKD, a personalized federated learning method
  that addresses catastrophic forgetting through comprehensive knowledge distillation.
  The approach uses both global and historical models as teachers to transfer generalized
  and personalized knowledge to local models, balancing generalization and personalization.
---

# Towards Personalized Federated Learning via Comprehensive Knowledge Distillation

## Quick Facts
- arXiv ID: 2411.03569
- Source URL: https://arxiv.org/abs/2411.03569
- Authors: Pengju Wang; Bochao Liu; Weijia Guo; Yong Li; Shiming Ge
- Reference count: 20
- Primary result: FedCKD achieves up to 1.98% higher accuracy on CIFAR100 compared to state-of-the-art personalized FL methods

## Executive Summary
This paper introduces FedCKD, a personalized federated learning method that addresses catastrophic forgetting through comprehensive knowledge distillation. The approach uses both global and historical models as teachers to transfer generalized and personalized knowledge to local models, balancing generalization and personalization. An annealing mechanism is introduced to dynamically adjust the weight factor during training, enhancing personalization ability and training stability. Experiments on FMNIST, CIFAR10, and CIFAR100 datasets demonstrate significant improvements over state-of-the-art methods, with FedCKD achieving up to 1.98% higher accuracy on CIFAR100 and showing robust performance across various data heterogeneity levels, participation rates, and model architectures.

## Method Summary
FedCKD implements a multi-teacher knowledge distillation framework in federated learning where each client uses both the global model (representing aggregated knowledge from all clients) and the historical model (representing the client's own previous knowledge) as teachers to train their local model. The method employs an exponential decay annealing mechanism that gradually shifts focus from knowledge transfer to local adaptation during training. The loss function combines cross-entropy with KL divergence terms weighted by λ, which decays over time using rate γ=0.99. The method is tested across three datasets with varying levels of data heterogeneity simulated using Dirichlet distributions.

## Key Results
- FedCKD achieves 1.98% higher accuracy on CIFAR100 compared to state-of-the-art personalized FL methods
- The method shows robust performance across different data heterogeneity levels (α=0.01, 0.10, 1.00) and client participation rates (r=0.10, 1.00)
- FedCKD demonstrates better individual fairness with lower standard deviation across clients compared to competing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The global model provides generalized knowledge that improves local model performance by counteracting overfitting to local data.
- Mechanism: The global model, aggregated from all clients, contains knowledge that spans across diverse data distributions. When used as a teacher in knowledge distillation, it transfers this broad generalization capability to the local model.
- Core assumption: The global model aggregates meaningful patterns from diverse clients that can benefit individual clients.
- Evidence anchors:
  - [abstract]: "The global model represents the aggregated model from the last round of server aggregation, containing global generalized knowledge"
  - [section III.B]: "The global model, in contrast, embodies generalized knowledge"
  - [corpus]: Weak evidence - corpus contains no direct discussion of global model generalization in knowledge distillation context
- Break condition: When data heterogeneity is so extreme that the global model becomes irrelevant to specific clients, causing negative transfer.

### Mechanism 2
- Claim: The historical model preserves personalized knowledge and prevents catastrophic forgetting of local patterns.
- Mechanism: The historical model represents the local model from the previous training round, containing the client's specific data patterns. Using it as a teacher in knowledge distillation ensures these learned patterns are not forgotten during subsequent training.
- Core assumption: The historical model contains valuable personalized knowledge that would be lost without explicit preservation.
- Evidence anchors:
  - [abstract]: "The historical model represents the local model from the last round of client training, containing historical personalized knowledge"
  - [section III.B]: "We retained the local model from previous training, referred to as the historical model, to maintain personalized knowledge"
  - [corpus]: No direct evidence in corpus about historical model preservation
- Break condition: When the historical model becomes too outdated relative to current data distribution, causing it to teach irrelevant patterns.

### Mechanism 3
- Claim: The annealing mechanism improves training stability and personalization by gradually shifting focus from knowledge transfer to local adaptation.
- Mechanism: Initially, the student model learns primarily from teacher soft labels (knowledge transfer). As training progresses, the weight on hard labels (local learning) increases through exponential decay, allowing the model to adapt to its specific data distribution.
- Core assumption: Early knowledge transfer is beneficial but should be gradually replaced by local adaptation for optimal personalization.
- Evidence anchors:
  - [section III.B]: "We employ exponential decay for annealing, which simplifies the process by gradually decreasing the weight factor λ at a constant decay rate γ for each communication round"
  - [section IV.C]: "Upon implementing the annealing mechanism, the mean accuracy increases from 57.08% to 57.25%, while the standard deviation decreases from 0.15% to 0.07%"
  - [corpus]: No direct evidence in corpus about annealing mechanisms
- Break condition: When the decay rate is too aggressive, causing premature abandonment of valuable teacher knowledge.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: FedCKD relies on transferring knowledge from teacher models (global and historical) to the student model (local), making knowledge distillation the fundamental mechanism.
  - Quick check question: What is the primary loss component used in knowledge distillation to measure the difference between teacher and student model outputs?

- Concept: Federated Learning Architecture
  - Why needed here: Understanding the server-client communication pattern, local training, and global aggregation is essential to implement FedCKD correctly.
  - Quick check question: In traditional FL, what happens during the global aggregation phase after clients upload their updated models?

- Concept: Catastrophic Forgetting
  - Why needed here: FedCKD specifically addresses this problem, so understanding why models forget previously learned knowledge is crucial for grasping the method's motivation.
  - Quick check question: What phenomenon occurs when a model rapidly loses previously learned knowledge while acquiring new knowledge?

## Architecture Onboarding

- Component map: Server maintains global model, clients maintain local and historical models. Each round: server broadcasts global model → clients update local model → comprehensive knowledge distillation with global and historical models as teachers → clients upload updated local models → server aggregates.
- Critical path: Local distillation process (lines 17-19 in Algorithm 1) is the core where knowledge transfer happens. This must be implemented correctly for the method to work.
- Design tradeoffs: Balancing λ weight factor affects generalization vs personalization. Too high emphasizes knowledge transfer, too low risks catastrophic forgetting. The annealing mechanism helps navigate this tradeoff.
- Failure signatures: If test accuracy plateaus early, knowledge transfer may be insufficient. If accuracy varies wildly across clients, personalization may be too aggressive or the annealing schedule may be inappropriate.
- First 3 experiments:
  1. Implement FedCKD on FMNIST with 20 clients, α=0.10, and verify it outperforms FedAvg baseline.
  2. Test the annealing mechanism by running FedCKD with and without annealing on CIFAR100 and measure accuracy difference.
  3. Evaluate individual fairness by measuring standard deviation of accuracy across clients on CIFAR10.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FedCKD scale with the number of clients and data heterogeneity levels beyond those tested?
- Basis in paper: [explicit] The paper tests FedCKD with 20 and 100 clients, and data heterogeneity levels with α = 0.01, 0.10, and 1.00. It mentions that FedCKD outperforms other methods in various settings but does not explore the limits of this scalability.
- Why unresolved: The paper does not provide experiments or theoretical analysis on the performance limits of FedCKD with a larger number of clients or more extreme levels of data heterogeneity.
- What evidence would resolve it: Conducting experiments with a significantly larger number of clients (e.g., 1000+) and exploring a wider range of data heterogeneity levels (e.g., α < 0.01) would provide insights into the scalability and robustness of FedCKD.

### Open Question 2
- Question: What is the impact of different model architectures on the performance of FedCKD, and are there specific architectures that are particularly well-suited or ill-suited for this method?
- Basis in paper: [explicit] The paper mentions that FedCKD is tested with simple CNN and five-layer CNN models, and it is noted that the method's performance is evaluated across various model architectures, including ResNet-8 and MobileNetV2. However, the paper does not provide a detailed analysis of how different architectures affect performance.
- Why unresolved: The paper does not explore the relationship between model architecture and FedCKD performance in depth, nor does it identify which architectures might be optimal or suboptimal for this method.
- What evidence would resolve it: Conducting experiments with a diverse set of model architectures, including both deep and shallow networks, and analyzing the performance trends would help identify the impact of architecture on FedCKD.

### Open Question 3
- Question: How does the annealing mechanism in FedCKD affect the convergence rate and final model accuracy compared to other knowledge distillation methods without annealing?
- Basis in paper: [explicit] The paper introduces an annealing mechanism to dynamically adjust the weight factor during training, which is claimed to enhance personalization ability and training stability. However, the paper does not provide a detailed comparison of convergence rates or final accuracy with and without annealing.
- Why unresolved: The paper does not include experiments that specifically compare the convergence behavior and final accuracy of FedCKD with and without the annealing mechanism, nor does it compare these aspects with other methods that do not use annealing.
- What evidence would resolve it: Running experiments that track the convergence rate and final accuracy of FedCKD with and without annealing, as well as comparing these results with other knowledge distillation methods that do not use annealing, would provide insights into the effectiveness of the annealing mechanism.

## Limitations
- Model architecture details are not fully specified, particularly for CNN configurations on different datasets
- Implementation details for global aggregation and historical model storage are not explicitly described
- The exact impact of annealing mechanism parameters on different data distributions is not thoroughly analyzed

## Confidence
- High Confidence: The core mechanism of using global and historical models for knowledge distillation is well-founded and clearly explained
- Medium Confidence: Experimental results showing performance improvements are convincing, but could benefit from more ablation studies on hyperparameter sensitivity
- Medium Confidence: The theoretical framework for addressing catastrophic forgetting through knowledge distillation is sound, though empirical validation is limited to specific datasets

## Next Checks
1. **Ablation Study**: Systematically vary the annealing decay rate (γ) from 0.95 to 0.99 to determine optimal personalization vs. generalization tradeoff
2. **Model Architecture Sensitivity**: Test FedCKD with different CNN architectures (varying depth and width) on CIFAR10 to assess architecture independence
3. **Cross-Domain Transfer**: Evaluate FedCKD's performance when clients have entirely disjoint label distributions to test robustness to extreme data heterogeneity