---
ver: rpa2
title: Explainability in Neural Networks for Natural Language Processing Tasks
arxiv_id: '2412.18036'
source_url: https://arxiv.org/abs/2412.18036
tags:
- lime
- neural
- text
- interpretability
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study applies LIME to explain an MLP text classifier, addressing\
  \ the challenge of interpreting black-box neural NLP models. The method uses LIME\u2019\
  s local surrogate approach to identify which words most influence a given prediction\
  \ by masking and observing changes in probability."
---

# Explainability in Neural Networks for Natural Language Processing Tasks

## Quick Facts
- arXiv ID: 2412.18036
- Source URL: https://arxiv.org/abs/2412.18036
- Authors: Melkamu Mersha; Mingiziem Bitewa; Tsion Abay; Jugal Kalita
- Reference count: 6
- Primary result: LIME applied to explain MLP text classifier on 20 Newsgroups dataset, achieving 82% accuracy with instance-level feature explanations

## Executive Summary
This study investigates the application of LIME (Local Interpretable Model-agnostic Explanations) to explain predictions from a multilayer perceptron (MLP) text classifier. The research focuses on enhancing interpretability in black-box neural NLP models by identifying which words most influence specific predictions through feature perturbation and masking techniques. Using the 20 Newsgroups dataset, the approach demonstrates how LIME can generate localized explanations that reveal individual feature contributions, though the method shows limitations in capturing global patterns and feature interactions across the model's decision-making process.

## Method Summary
The methodology involves preprocessing text data from the 20 Newsgroups dataset through lowercasing, stopword removal, and stemming/lemmatization. Features are extracted using Word2Vec embeddings to create a dictionary, which trains an MLP classifier with an 80/20 train-test split. LIME is then applied to generate instance-specific explanations by perturbing text inputs (masking words) and observing changes in prediction probabilities. The local surrogate approach fits a simple interpretable model around each instance to identify feature contributions, producing lists of words with associated weights indicating their influence on predictions.

## Key Results
- MLP classifier achieved 82% accuracy on the 20 Newsgroups dataset
- LIME successfully generated interpretable feature lists for individual document classifications
- Explanations revealed specific word contributions to predictions but were limited by feature selection strategy
- Method demonstrated effectiveness for instance-level interpretability while highlighting need for hybrid approaches to capture global patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LIME provides local interpretability by approximating a black-box model's behavior with a locally faithful surrogate model
- Mechanism: LIME generates perturbed samples around an instance by removing or altering features (words), then fits a simple, interpretable model (e.g., linear regression) weighted by proximity to the instance
- Core assumption: The local behavior of the complex model can be well-approximated by a simpler model within a small neighborhood
- Evidence anchors:
  - [abstract] "LIME generates localized explanations by approximating a model's behavior for individual instances, focusing on the contributions of specific features to predictions."
  - [section] "LIME focuses on training local surrogate models to explain distinct predictions rather than training a global surrogate model."
  - [corpus] Weak corpus evidence; related works focus on benchmarking LIME rather than explaining its internal mechanics
- Break condition: If the local surrogate model's assumptions are violated (e.g., non-linear decision boundaries are too complex), the explanation becomes unreliable

### Mechanism 2
- Claim: LIME uses feature perturbation to determine the contribution of each feature to the model's prediction
- Mechanism: For text data, LIME randomly removes words from the input, creates a binary representation (word present/absent), and observes changes in the model's prediction probability
- Core assumption: The absence of a feature (word) can serve as a meaningful perturbation to assess its importance
- Evidence anchors:
  - [abstract] "The method uses LIME's local surrogate approach to identify which words most influence a given prediction by masking and observing changes in probability."
  - [section] "For text data, LIME creates explanations by selectively removing or masking words and analyzing their impact on predictions."
  - [corpus] No direct corpus evidence; assumption based on the paper's methodology description
- Break condition: If feature interactions are strong, masking a single feature may not capture its true contribution

### Mechanism 3
- Claim: LIME explanations are instance-specific, providing feature-level weights that indicate contribution to the prediction
- Mechanism: After fitting the local surrogate model, LIME outputs a list of features with associated weights (positive or negative), indicating how much each feature pushes the prediction toward or away from a particular class
- Core assumption: The surrogate model's coefficients can be interpreted as feature contributions to the prediction
- Evidence anchors:
  - [abstract] "By analyzing the contribution of individual features to model predictions, the LIME approach enhances interpretability and supports informed decision-making."
  - [section] "The LIME output is a list explanation that tells the effect of each feature on the prediction of a given instance."
  - [corpus] Weak corpus evidence; related works discuss LIME's effectiveness but not the mechanism of coefficient interpretation
- Break condition: If the surrogate model is a poor fit, the feature weights may not accurately reflect the true model behavior

## Foundational Learning

- Concept: Local surrogate models
  - Why needed here: LIME relies on fitting a simple, interpretable model locally around an instance to explain a complex black-box model
  - Quick check question: What is the purpose of using a local surrogate model in LIME, and how does it differ from a global surrogate model?

- Concept: Feature perturbation and masking
  - Why needed here: LIME generates perturbed samples by removing or altering features to assess their impact on the model's prediction
  - Quick check question: How does LIME create perturbed samples for text data, and what is the significance of the binary representation used?

- Concept: Instance-level interpretability
  - Why needed here: LIME provides explanations for individual instances rather than global patterns, which is crucial for understanding specific model predictions
  - Quick check question: What is the difference between local and global interpretability, and why is LIME considered a local interpretability method?

## Architecture Onboarding

- Component map:
  Text preprocessing pipeline (lowercasing, tokenization, stopword removal, stemming/lemmatization) -> Feature extraction (Bag-of-Words or Word2Vec embeddings) -> MLP text classifier model -> LIME explainer component -> Evaluation and visualization modules

- Critical path:
  1. Preprocess text data
  2. Extract features and build dictionary
  3. Train MLP classifier
  4. Apply LIME to generate explanations for individual instances
  5. Analyze and visualize LIME outputs

- Design tradeoffs:
  - LIME provides local interpretability but may miss global patterns and feature interactions
  - The choice of feature representation (BoW vs. Word2Vec) affects both model performance and LIME's ability to generate meaningful explanations
  - LIME's explanations are dependent on the quality of the surrogate model and the feature selection strategy

- Failure signatures:
  - LIME explanations are unstable or inconsistent across similar instances
  - LIME assigns high weights to irrelevant or nonsensical features
  - The local surrogate model has low fidelity to the black-box model

- First 3 experiments:
  1. Train MLP classifier on 20 Newsgroups dataset and evaluate accuracy
  2. Apply LIME to a few sample instances and analyze the generated explanations
  3. Compare LIME explanations using different feature representations (BoW vs. Word2Vec) and assess their interpretability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different feature selection strategies in LIME affect the interpretability and accuracy of explanations for neural NLP models?
- Basis in paper: [explicit] The paper notes that LIME's effectiveness "hinges on the selection of features for each instance" and mentions this as a key influence on interpretability, but does not evaluate different strategies
- Why unresolved: The paper does not compare or analyze the impact of different feature selection methods (e.g., frequency-based, importance-based, or hybrid approaches) on the quality of LIME explanations
- What evidence would resolve it: Systematic comparison of LIME explanations generated using different feature selection strategies, evaluated against human-annotated relevance scores or downstream task performance

### Open Question 2
- Question: Can LIME explanations capture global model behavior and feature interactions in neural NLP models, or are they inherently limited to local interpretability?
- Basis in paper: [explicit] The paper explicitly states that LIME has "limitations in capturing global patterns and feature interactions" and that it "fails to address global interpretability"
- Why unresolved: The paper does not propose or test methods to extend LIME for global interpretability or to analyze feature interactions beyond individual instances
- What evidence would resolve it: Development and validation of hybrid methods that combine LIME with global interpretability techniques (e.g., SHAP or integrated gradients) to analyze feature interactions and model-wide patterns

### Open Question 3
- Question: How do transformer-based models (e.g., BERT) compare to traditional neural networks (e.g., MLP) in terms of interpretability and the quality of LIME-generated explanations?
- Basis in paper: [explicit] The paper suggests future work to "transition to transformer-based models" and notes that explaining transformer predictions "presents new opportunities for interpretability research"
- Why unresolved: The study uses an MLP model and does not evaluate how LIME explanations differ when applied to transformer architectures, which have different internal mechanisms (e.g., attention layers)
- What evidence would resolve it: Comparative analysis of LIME explanations for MLP vs. transformer models, focusing on the consistency, relevance, and human interpretability of the generated explanations

### Open Question 4
- Question: How does the preprocessing strategy (e.g., stemming, lemmatization, stopword removal) impact the quality and interpretability of LIME explanations in NLP tasks?
- Basis in paper: [inferred] The paper describes extensive preprocessing steps but does not analyze how these steps affect the features selected by LIME or the resulting explanations
- Why unresolved: The study applies preprocessing as a standard step but does not investigate whether different preprocessing strategies lead to more meaningful or accurate LIME explanations
- What evidence would resolve it: Controlled experiments comparing LIME explanations across different preprocessing pipelines, evaluated using metrics such as feature relevance, explanation stability, and alignment with human judgment

## Limitations
- LIME explanations are limited to instance-level interpretability and cannot reveal global patterns or feature interactions across the model
- The quality of explanations depends heavily on the feature selection strategy, which remains inadequately explored
- LIME's effectiveness is constrained by its reliance on local surrogate models that may not capture complex decision boundaries

## Confidence

- **High Confidence**: LIME can generate interpretable feature lists for individual text classifications (supported by 82% accuracy and concrete example outputs)
- **Medium Confidence**: LIME explanations are limited by feature selection strategy and cannot reveal global patterns (methodologically sound but lacks empirical validation)
- **Medium Confidence**: LIME is effective for instance-level but insufficient for deeper model understanding (conclusion drawn from observed limitations rather than comprehensive testing)

## Next Checks
1. Conduct sensitivity analysis by varying LIME's perturbation parameters (number of features, kernel width) to assess stability of explanations across parameter settings
2. Compare LIME explanations using different feature representations (Bag-of-Words vs Word2Vec) to determine which better captures meaningful feature contributions
3. Implement a global interpretability method alongside LIME to systematically compare local vs global explanation quality and identify complementary approaches