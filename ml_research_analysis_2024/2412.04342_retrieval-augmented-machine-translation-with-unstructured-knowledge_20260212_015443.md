---
ver: rpa2
title: Retrieval-Augmented Machine Translation with Unstructured Knowledge
arxiv_id: '2412.04342'
source_url: https://arxiv.org/abs/2412.04342
tags:
- documents
- translation
- document
- llms
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAGtrans, the first benchmark for retrieval-augmented
  machine translation (MT) using unstructured knowledge. It contains 169K English
  MT samples from Wikipedia, each paired with relevant documents in English, Chinese,
  German, French, and Czech.
---

# Retrieval-Augmented Machine Translation with Unstructured Knowledge
## Quick Facts
- arXiv ID: 2412.04342
- Source URL: https://arxiv.org/abs/2412.04342
- Reference count: 40
- RAGtrans improves LLM MT performance by 1.6-3.1 BLEU and 1.0-2.0 COMET on En→Zh, and 1.7-2.9 BLEU and 2.1-2.7 COMET on En→De

## Executive Summary
This paper introduces RAGtrans, the first benchmark for retrieval-augmented machine translation using unstructured knowledge. The benchmark contains 169K English MT samples from Wikipedia, each paired with relevant documents in five languages (English, Chinese, German, French, and Czech). The authors propose a multi-task training method called CSC with three objectives—cross-lingual information completion, self-knowledge-enhanced translation, and cross-lingual relevance discrimination—to teach LLMs how to leverage multilingual knowledge. Experiments demonstrate significant improvements over existing baselines, with gains of 1.6-3.1 BLEU and 1.0-2.0 COMET on English-to-Chinese translation, and 1.7-2.9 BLEU and 2.1-2.7 COMET on English-to-German translation.

## Method Summary
The RAGtrans benchmark provides a new testbed for retrieval-augmented machine translation by pairing Wikipedia articles with their translations across five languages. The core contribution is a multi-task training approach called CSC that teaches LLMs to effectively use retrieved knowledge through three complementary objectives: completing missing information across languages, enhancing translation with self-knowledge, and discriminating between relevant and irrelevant documents. This training paradigm addresses the challenge of making LLMs aware of when and how to use external knowledge sources during translation, rather than simply translating in isolation.

## Key Results
- RAGtrans improves LLM performance by 1.6-3.1 BLEU and 1.0-2.0 COMET on En→Zh translation
- RAGtrans improves LLM performance by 1.7-2.9 BLEU and 2.1-2.7 COMET on En→De translation
- The multi-task CSC training method shows consistent improvements across both language pairs
- Ablation studies confirm each training objective contributes meaningfully to the overall performance gains

## Why This Works (Mechanism)
The CSC training method works by teaching LLMs three complementary skills: understanding cross-lingual information gaps, leveraging retrieved knowledge during translation, and discriminating relevant from irrelevant documents. By training on these tasks simultaneously, the model learns to recognize when external knowledge can improve translation quality and how to effectively incorporate that knowledge. The multi-task approach prevents overfitting to any single objective while building a more robust understanding of knowledge usage in translation contexts.

## Foundational Learning
- Retrieval-augmented generation (RAG): Why needed - to provide external context for LLMs; Quick check - verify retrieval relevance scores
- Cross-lingual information completion: Why needed - to bridge knowledge gaps across languages; Quick check - assess completion accuracy across language pairs
- Knowledge-enhanced translation: Why needed - to improve translation quality with external context; Quick check - compare translations with and without retrieved knowledge
- Document relevance discrimination: Why needed - to filter useful knowledge from noise; Quick check - measure precision of retrieved document selection

## Architecture Onboarding
Component map: Wikipedia articles -> Document retrieval -> CSC training -> Enhanced MT model
Critical path: Retrieval system identifies relevant documents → CSC training teaches knowledge integration → Enhanced model produces better translations
Design tradeoffs: Multi-task training vs. single-task specialization; breadth of knowledge coverage vs. depth of domain expertise
Failure signatures: Over-reliance on retrieved knowledge leading to hallucination; poor relevance discrimination selecting irrelevant documents; knowledge integration disrupting translation fluency
First experiments: 1) Test retrieval relevance across different document types, 2) Evaluate individual CSC objectives in isolation, 3) Compare knowledge integration strategies (concatenation vs. attention)

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to English-to-Chinese and English-to-German translation directions, restricting generalizability
- Relies exclusively on automatic metrics (BLEU and COMET) without human evaluation of translation quality
- Does not clearly define boundaries between structured and unstructured knowledge in the MT context

## Confidence
High confidence: Technical implementation of RAGtrans benchmark and basic experimental results
Medium confidence: Claim of being the first benchmark of its kind
Low confidence: Practical utility without human evaluation or downstream task validation

## Next Checks
1. Conduct human evaluation studies to assess whether retrieved knowledge actually improves translation quality beyond automatic metric improvements, particularly for domain-specific terminology and factual accuracy
2. Test the CSC training method on additional language pairs, especially distant language pairs like English-to-Japanese or English-to-Arabic, to evaluate generalizability across different linguistic structures
3. Perform ablation studies specifically isolating the contribution of each training objective in the CSC method, including experiments that train on each objective individually and in different combinations to better understand their interactions