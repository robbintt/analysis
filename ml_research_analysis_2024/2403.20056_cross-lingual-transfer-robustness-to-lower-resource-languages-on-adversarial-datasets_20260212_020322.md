---
ver: rpa2
title: Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial
  Datasets
arxiv_id: '2403.20056'
source_url: https://arxiv.org/abs/2403.20056
tags:
- language
- languages
- transfer
- cross-lingual
- mbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates cross-lingual transfer robustness of multilingual
  language models like MBERT and XLM-R on adversarial datasets for named entity recognition
  and section title prediction across 13 language pairs. It examines how models trained
  on a high-resource language (HRL) perform on a low-resource language (LRL) that
  shares vocabulary overlap due to areal, genetic, or borrowing relationships.
---

# Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets

## Quick Facts
- arXiv ID: 2403.20056
- Source URL: https://arxiv.org/abs/2403.20056
- Authors: Shadi Manafi; Nikhil Krishnaswamy
- Reference count: 0
- Primary result: Cross-lingual NER transfer effectiveness correlates with named entity overlap between source and target languages

## Executive Summary
This paper evaluates cross-lingual transfer robustness of multilingual language models like MBERT and XLM-R on adversarial datasets for named entity recognition and section title prediction across 13 language pairs. It examines how models trained on a high-resource language (HRL) perform on a low-resource language (LRL) that shares vocabulary overlap due to areal, genetic, or borrowing relationships. Four perturbations are applied: replacing named entities, modifying surrounding words, and combinations thereof. Results show NER cross-lingual transfer depends heavily on entity overlap—more shared entities lead to stronger transfer. Cross-lingual models are often more robust to certain input perturbations than native LRL models, suggesting they leverage stronger HRL representations. Section title prediction strongly relies on word memorization, with performance dropping significantly under perturbations.

## Method Summary
The study fine-tunes MBERT and XLM-R models on WikiANN for NER and a custom Wikipedia Section Title Prediction dataset across 21 languages. Models are trained under three conditions: native L2, cross-lingual L1→L2, and with four types of perturbations. Perturbations include replacing named entities, modifying surrounding words using cosine similarity, and combined modifications. Results are averaged across three runs. The study examines 13 language pairs with varying vocabulary overlap due to areal, genetic, or borrowing relationships, including Arabic/Hindi as a control pair for script differences.

## Key Results
- NER cross-lingual transfer effectiveness strongly correlates with named entity overlap between source and target languages
- Cross-lingual models show greater robustness to certain perturbations than native LRL models, leveraging stronger HRL representations
- Section title prediction relies heavily on word memorization rather than semantic understanding, with significant performance drops under perturbations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual NER transfer effectiveness correlates with named entity overlap between source and target languages
- Mechanism: Models leverage shared named entities as "anchors" that transfer across languages, allowing learned entity recognition patterns to apply in both languages
- Core assumption: Named entities (particularly person names, locations, organizations) are often shared or similar between geographically/proximally related languages due to borrowing, areal contact, or shared cultural context
- Evidence anchors:
  - [abstract] "Our findings indicate that NER cross-lingual transfer depends largely on the overlap of entity chunks. If a source and target language have more entities in common, the transfer ability is stronger."
  - [section] "We observe a clear correlation between the proportion of shared vocabulary items between the train and test sets and the performance degradation when test entities are perturbed to substitute those that are shared between the sets with unique entities."
  - [corpus] Weak evidence - corpus doesn't provide direct evidence about entity overlap correlation
- Break condition: When source and target languages share minimal named entity vocabulary despite other linguistic similarities (e.g., Arabic/Hindi with different scripts)

### Mechanism 2
- Claim: Cross-lingual models show greater robustness to certain perturbations than native LRL models
- Mechanism: Cross-lingual transfer models leverage stronger representations learned from HRL data, which provide more robust feature representations that generalize better to perturbed inputs
- Core assumption: HRL representations capture more generalizable linguistic patterns that transfer to LRLs and remain stable under input perturbations
- Evidence anchors:
  - [abstract] "Models using cross-lingual transfer also appear to be somewhat more robust to certain perturbations of the input, perhaps indicating an ability to leverage stronger representations derived from the HRL."
  - [section] "Our findings indicate that NER cross-lingual transfer depends largely on the overlap of entity chunks... Models using cross-lingual transfer also appear to be somewhat more robust to certain perturbations of the input, perhaps indicating an ability to leverage stronger representations derived from the HRL."
  - [corpus] Weak evidence - corpus provides related papers but no direct evidence about perturbation robustness
- Break condition: When perturbations significantly alter the semantic content beyond simple word substitution, breaking the link to learned HRL representations

### Mechanism 3
- Claim: Section title prediction relies heavily on word memorization rather than semantic understanding
- Mechanism: Models match section titles by recognizing shared vocabulary patterns between training and test data rather than understanding the semantic relationship between text and title
- Core assumption: Title selection is primarily based on keyword matching and vocabulary overlap rather than deep semantic comprehension
- Evidence anchors:
  - [abstract] "Section title prediction strongly relies on word memorization, with performance dropping significantly under perturbations."
  - [section] "We observe a clear correlation between the proportion of shared vocabulary items... and the performance degradation... This suggests that this task relies heavily on word memorization of the training data"
  - [corpus] Weak evidence - corpus doesn't provide direct evidence about memorization vs understanding
- Break condition: When perturbations introduce semantically similar but lexically different words, causing performance drops that suggest shallow pattern matching

## Foundational Learning

- Concept: Vocabulary overlap computation between language pairs
  - Why needed here: The study quantifies how much vocabulary is shared between language pairs to establish the relationship between overlap and transfer effectiveness
  - Quick check question: How is vocabulary overlap computed between L1 training data and L2 test data in this study?

- Concept: Adversarial perturbation techniques for NLP evaluation
  - Why needed here: The study uses four different perturbation methods to test model robustness under controlled input variations
  - Quick check question: What are the four perturbation methods used to test model robustness in this study?

- Concept: Cross-lingual transfer learning fundamentals
  - Why needed here: The core experimental design compares native LRL models against cross-lingual transfer from HRL to understand transfer effectiveness
  - Quick check question: What are the two main experimental conditions compared in this study?

## Architecture Onboarding

- Component map: Data collection pipeline (WikiANN and Wikipedia section extraction) -> Perturbation generation module -> Multilingual models (MBERT and XLM-R) -> Evaluation framework measuring F1/accuracy under different conditions
- Critical path: Data preparation → Perturbation application → Model fine-tuning → Evaluation → Analysis of overlap vs performance relationship
- Design tradeoffs: Using raw text without transliteration allows examining token-level overlap effects but misses semantic similarities that transliteration might reveal; focusing on encoder models enables direct embedding space analysis but may not capture decoder capabilities
- Failure signatures: Low overlap scores between L1/L2 pairs predict poor cross-lingual transfer; performance drops under perturbation 5 (combined entity and context changes) indicate heavy reliance on memorization; significant native vs cross-lingual performance gaps suggest insufficient transfer
- First 3 experiments:
  1. Run native L2 fine-tuning on one language pair and measure baseline F1 score
  2. Apply perturbation 3 (entity substitution) to the same model and compare performance drop
  3. Run cross-lingual transfer from L1 to L2 for the same pair and compare against native performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the robustness of cross-lingual transfer models vary when applied to language pairs with different types of linguistic relationships (e.g., genetic vs. areal vs. borrowing)?
- Basis in paper: [explicit] The paper evaluates language pairs with varying degrees of vocabulary overlap due to areal, genetic, or borrowing relationships, but does not extensively compare the robustness of cross-lingual transfer across these different relationship types.
- Why unresolved: The paper focuses on the overall impact of vocabulary overlap on cross-lingual transfer robustness but does not delve into how different linguistic relationships between languages affect this robustness.
- What evidence would resolve it: Comparative analysis of cross-lingual transfer robustness across language pairs with different linguistic relationships, controlling for vocabulary overlap.

### Open Question 2
- Question: What is the impact of script differences on cross-lingual transfer robustness, particularly for languages that share vocabulary but use different scripts?
- Basis in paper: [explicit] The paper includes Arabic/Hindi as a control pair due to script differences, noting lower token overlap, but does not extensively explore how script differences affect cross-lingual transfer robustness.
- Why unresolved: While the paper mentions script differences as a factor, it does not provide a detailed analysis of how these differences impact model performance and robustness.
- What evidence would resolve it: Detailed performance analysis of cross-lingual transfer models on language pairs with similar vocabulary but different scripts, comparing results to pairs with the same script.

### Open Question 3
- Question: How do different types of perturbations (e.g., entity replacement vs. context word modification) affect the performance of cross-lingual transfer models differently across tasks?
- Basis in paper: [explicit] The paper applies various perturbations and notes differences in model robustness, but does not extensively compare the effects of different perturbation types across tasks.
- Why unresolved: The paper discusses the impact of perturbations but does not provide a comprehensive comparison of how different perturbation types affect model performance across NER and title selection tasks.
- What evidence would resolve it: Comparative analysis of model performance under different perturbation types for both NER and title selection tasks, identifying which perturbations are most detrimental to each task.

## Limitations
- The study focuses on vocabulary overlap rather than deeper linguistic similarities like syntactic or semantic structures
- Perturbations are limited to word-level replacements without more complex semantic manipulations
- The analysis relies on encoder-only models (MBERT and XLM-R), potentially missing decoder dynamics
- Semantic resources for named entities were manually created for only 10 target languages

## Confidence

- **High confidence**: The correlation between named entity overlap and NER transfer effectiveness is well-supported by multiple experiments across 13 language pairs, with clear performance degradation patterns under perturbation. The section title prediction results showing heavy reliance on memorization are also robustly demonstrated.

- **Medium confidence**: The claim that cross-lingual models are more robust to certain perturbations than native models is supported but relies on indirect evidence and lacks clear explanation of which perturbations benefit from cross-lingual transfer versus which harm it.

- **Low confidence**: The assertion that HRL representations provide "stronger" features that transfer to LRLs is stated but not empirically validated. The study observes this pattern but doesn't test the underlying representation quality or explain why this occurs.

## Next Checks

1. **Test transliteration impact**: Run experiments with transliterated source and target language pairs to determine if character-level overlap contributes to transfer effectiveness beyond vocabulary overlap.

2. **Semantic perturbation battery**: Design and apply more sophisticated perturbations that test semantic understanding (e.g., replacing entities with semantically similar but lexically different terms) to better distinguish memorization from comprehension.

3. **Cross-linguistic structure analysis**: Examine whether syntactic and morphological similarities between language pairs correlate with transfer effectiveness beyond vocabulary overlap, using dependency parsing or morphological alignment tools.