---
ver: rpa2
title: 'MathScale: Scaling Instruction Tuning for Mathematical Reasoning'
arxiv_id: '2403.02884'
source_url: https://arxiv.org/abs/2403.02884
tags:
- math
- questions
- mathscale
- mathematical
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MathScale is a method for generating high-quality mathematical
  reasoning data using large language models like GPT-3.5. It extracts topics and
  knowledge points from seed questions, builds a concept graph, and uses this to generate
  new questions.
---

# MathScale: Scaling Instruction Tuning for Mathematical Reasoning

## Quick Facts
- arXiv ID: 2403.02884
- Source URL: https://arxiv.org/abs/2403.02884
- Authors: Zhengyang Tang; Xingxing Zhang; Benyou Wang; Furu Wei
- Reference count: 21
- MathScale-7B achieved state-of-the-art performance on MWPBENCH, outperforming equivalent-sized models by 42.9% in micro average accuracy and 43.7% in macro average accuracy

## Executive Summary
MathScale is a method for generating high-quality mathematical reasoning data using large language models like GPT-3.5. It extracts topics and knowledge points from seed questions, builds a concept graph, and uses this to generate new questions. This approach scales effectively to create large datasets. MathScaleQA, a dataset of 2 million question-answer pairs, was created and used to fine-tune models like LLaMA-2 and Mistral, significantly improving their mathematical reasoning. Evaluated on the comprehensive MWPBENCH benchmark, MathScale-7B achieved state-of-the-art performance, outperforming equivalent-sized models by 42.9% in micro average accuracy and 43.7% in macro average accuracy.

## Method Summary
MathScale addresses the challenge of generating high-quality mathematical reasoning data for fine-tuning large language models. The method extracts topics and knowledge points from existing math questions, constructs a concept graph based on co-occurrence statistics, and uses this graph to generate new question-answer pairs. GPT-3.5 is used for concept extraction, concept graph construction, and mathematical reasoning data generation. The generated dataset, MathScaleQA, contains 2 million question-answer pairs and is used to fine-tune open-source LLMs like LLaMA-2 and Mistral. The approach demonstrates effective scalability, with performance improvements following a near-logarithmic growth pattern as dataset size increases.

## Key Results
- MathScale-7B achieved state-of-the-art performance on MWPBENCH, surpassing equivalent-sized models by 42.9% in micro average accuracy and 43.7% in macro average accuracy
- Performance scales nearly logarithmically with dataset size, showing diminishing but positive returns as more data is added
- Instruction tuning on MathScaleQA significantly improves mathematical reasoning capabilities of open-source LLMs like LLaMA-2 and Mistral

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concept graph construction improves question generation quality by capturing inter-concept dependencies
- Mechanism: The method extracts topics and knowledge points from seed questions, then builds a graph where edges represent co-occurrence frequencies. This graph structure allows for more diverse and meaningful combinations of concepts when generating new questions
- Core assumption: High co-occurrence frequency between concepts indicates a meaningful relationship that should be preserved in generated questions
- Evidence anchors: [abstract] "it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions"; [section] "We have three types of edges in this graph... which results to three sub-graphs... When a topic (or KP) u is co-occured with another topic (or KP) v, we build an edge between them"
- Break condition: If the co-occurrence statistics don't reflect true mathematical dependencies, or if the graph becomes too sparse to generate meaningful connections

### Mechanism 2
- Claim: Scaling the size of the generated dataset improves model performance logarithmically
- Mechanism: As more question-answer pairs are generated and used for fine-tuning, the model's mathematical reasoning capabilities improve, following a near-logarithmic growth pattern
- Core assumption: The quality of generated questions remains consistent as dataset size increases, and more data provides diminishing but still positive returns
- Evidence anchors: [section] "When scaling the size of the MathScaleQA dataset, we observe a nearly logarithmic growth in the performance of the MathScale-7b model"; [abstract] "MathScale exhibits effective scalability along the size axis of the math dataset that we generate"
- Break condition: If generated data quality degrades at scale, or if the model reaches a performance plateau despite additional data

### Mechanism 3
- Claim: Instruction tuning on high-quality synthetic data can match or exceed the performance of closed-source models
- Mechanism: By generating a large dataset of mathematical reasoning questions and answers using GPT-3.5, then using this dataset to fine-tune open-source models like LLaMA-2 and Mistral, the resulting models achieve state-of-the-art performance on mathematical reasoning benchmarks
- Core assumption: The synthetic data generated by GPT-3.5 is of sufficient quality and diversity to effectively train open-source models
- Evidence anchors: [abstract] "MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.9% in micro average accuracy and 43.7% in macro average accuracy"; [section] "We apply MathScaleQA to fine-tune open-source LLMs... resulting in significantly improved capabilities in mathematical reasoning"
- Break condition: If the synthetic data fails to capture the complexity of mathematical reasoning, or if the fine-tuning process overfits to the synthetic data

## Foundational Learning

- Concept: Graph random walk algorithm
  - Why needed here: Used to traverse the concept graph and generate diverse combinations of topics and knowledge points for question generation
  - Quick check question: How does the random walk algorithm ensure that related concepts are more likely to be combined in generated questions?

- Concept: Concept extraction from natural language
  - Why needed here: The initial step involves extracting mathematical topics and knowledge points from existing seed questions to build the concept graph
  - Quick check question: What are the key differences between topics and knowledge points in the context of mathematical reasoning?

- Concept: Instruction tuning methodology
  - Why needed here: The core approach involves using the generated MathScaleQA dataset to fine-tune existing LLMs for improved mathematical reasoning
  - Quick check question: How does instruction tuning differ from traditional supervised fine-tuning, and why is it particularly effective for mathematical reasoning tasks?

## Architecture Onboarding

- Component map: Seed question processor -> Concept graph builder -> Question generator -> Model trainer -> Evaluation framework
- Critical path: Seed question processing → Concept graph construction → Question generation → Model training → Evaluation
- Design tradeoffs:
  - Quality vs. quantity in generated questions: Higher quality generation may be slower but produce better training data
  - Complexity of concept graph: More complex graphs may capture better relationships but be harder to traverse
  - Fine-tuning duration: Longer fine-tuning may improve performance but increase computational costs
- Failure signatures:
  - Degraded performance on out-of-distribution mathematical problems
  - Overfitting to synthetic data patterns rather than general mathematical reasoning
  - Inconsistent results across different mathematical domains
- First 3 experiments:
  1. Generate a small dataset (1000 QA pairs) and fine-tune a LLaMA-2 7B model to verify basic functionality
  2. Compare the performance of models fine-tuned on datasets of different sizes (100K, 500K, 1M QA pairs) to observe scaling effects
  3. Evaluate the impact of different concept graph traversal strategies on the quality and diversity of generated questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and diversity of the seed questions impact the effectiveness of MathScale's concept extraction and graph construction?
- Basis in paper: [inferred] The paper discusses the importance of using a diverse set of seed questions from the MWPBENCH training set for concept extraction, and ablation studies show that removing 50% of the seed questions leads to a performance decrease of 2.9% in macro average accuracy. Similarly, removing 50% of the extracted topics or knowledge points also results in performance degradation
- Why unresolved: The paper does not provide a detailed analysis of how the specific characteristics of the seed questions (e.g., topic distribution, difficulty level) influence the quality of the extracted concepts and the resulting concept graph
- What evidence would resolve it: Conducting a more in-depth analysis of the seed questions, including their topic distribution and difficulty levels, and correlating these characteristics with the performance of MathScale on different datasets would provide insights into the importance of seed question quality and diversity

### Open Question 2
- Question: What is the impact of using different prompt templates or instructions for GPT-3.5 in the concept extraction, concept graph construction, and mathematical reasoning data generation steps of MathScale?
- Basis in paper: [explicit] The paper mentions that the prompt template for concept extraction is provided in Table 3, and the prompt template for mathematical reasoning data generation is shown in Table 4. However, it does not explore the effects of using different prompt templates or instructions on the quality of the generated data and the performance of the resulting models
- Why unresolved: The paper does not provide a comparison of different prompt templates or instructions and their impact on the effectiveness of MathScale
- What evidence would resolve it: Experimenting with different prompt templates or instructions for each step of MathScale and evaluating the resulting models' performance on MWPBENCH would shed light on the importance of prompt engineering in the process

### Open Question 3
- Question: How does the inclusion of tool integration, such as program-aided language models or code interpreters, affect the performance of MathScale on mathematical reasoning tasks?
- Basis in paper: [explicit] The paper mentions related work on tool integration instruction tuning for mathematics, such as TORA (Gou et al., 2023), which combines natural language reasoning with program-based tool usage. However, MathScale currently focuses solely on natural language reasoning and does not explore tool integration
- Why unresolved: The paper does not investigate the potential benefits or drawbacks of incorporating tool integration into the MathScale pipeline
- What evidence would resolve it: Integrating tool integration methods, such as program-aided language models or code interpreters, into the MathScale pipeline and evaluating the resulting models' performance on MWPBENCH would provide insights into the impact of tool integration on mathematical reasoning capabilities

## Limitations

- Dependency on GPT-3.5 for concept extraction and question generation creates a black-box dependency that makes it difficult to diagnose failures in the concept graph or question generation process
- MWPBENCH benchmark has limited diversity in problem types, potentially overstating real-world applicability of the results
- The quality of the final MathScale models is fundamentally constrained by the capabilities of GPT-3.5, creating a ceiling effect for performance improvements

## Confidence

- High Confidence: The scaling law observation (near-logarithmic performance improvement with dataset size) is well-supported by the presented data and aligns with established scaling patterns in machine learning
- Medium Confidence: The claim that instruction tuning on synthetic data can match or exceed closed-source models is supported by benchmark results but requires independent verification on alternative mathematical reasoning benchmarks
- Medium Confidence: The effectiveness of the concept graph construction mechanism is theoretically sound but the empirical evidence is primarily indirect through downstream performance rather than direct evaluation of the graph quality

## Next Checks

1. **Cross-Benchmark Validation**: Evaluate MathScale-7B on alternative mathematical reasoning benchmarks (such as MATH, GSM8K, or domain-specific tests) to verify that the MWPBENCH performance generalizes beyond that specific evaluation suite

2. **Ablation Study on Concept Graph**: Remove the concept graph mechanism and instead use random sampling of topics and knowledge points for question generation. Compare the performance of models fine-tuned on these datasets to quantify the specific contribution of the graph-based approach

3. **Quality Analysis of Generated Data**: Use GPT-4 or human evaluators to systematically analyze a sample of generated questions across different dataset sizes, measuring factors like solution correctness, problem diversity, and reasoning complexity to validate that quality is maintained at scale