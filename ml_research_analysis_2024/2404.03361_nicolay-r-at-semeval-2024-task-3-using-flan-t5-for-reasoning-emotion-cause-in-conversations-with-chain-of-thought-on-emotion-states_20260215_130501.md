---
ver: rpa2
title: 'nicolay-r at SemEval-2024 Task 3: Using Flan-T5 for Reasoning Emotion Cause
  in Conversations with Chain-of-Thought on Emotion States'
arxiv_id: '2404.03361'
source_url: https://arxiv.org/abs/2404.03361
tags:
- emotion
- utgt
- usrc
- table
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses emotion cause extraction in conversations,
  focusing on identifying emotion-cause pairs. The authors propose a two-stage Chain-of-Thought
  (CoT) approach using the THOR framework to fine-tune a Flan-T5 base model.
---

# nicolay-r at SemEval-2024 Task 3: Using Flan-T5 for Reasoning Emotion Cause in Conversations with Chain-of-Thought on Emotion States

## Quick Facts
- arXiv ID: 2404.03361
- Source URL: https://arxiv.org/abs/2404.03361
- Reference count: 1
- Key outcome: 3rd/4th place in F1-proportional metrics and 5th place in F1-strict metrics among 15 teams using Chain-of-Thought reasoning for emotion cause extraction

## Executive Summary
This paper addresses emotion cause extraction in conversations by identifying emotion-cause pairs using a two-stage Chain-of-Thought (CoT) approach with the THOR framework. The method fine-tunes a Flan-T5 base model, first predicting emotion states then predicting emotions caused by one utterance to another with reasoning revision. The approach achieved competitive results in SemEval-2024 Task 3, demonstrating the effectiveness of structured reasoning for this complex natural language understanding task.

## Method Summary
The approach uses a two-stage Chain-of-Thought fine-tuning process with Flan-T5-base (250M). Stage 1 (THORSTATE) predicts emotion states for utterances, while Stage 2 (THORCAUSE-RR) predicts emotions caused by one utterance to another with reasoning revision. The THOR framework decomposes the task into three logical steps: identifying text spans, inferring implicit opinions, and determining final emotion states. A rule-based span correction algorithm improves F1-strict scores by addressing prediction alignment issues.

## Key Results
- Achieved 3rd and 4th place in F1-proportional metrics (weighted and unweighted) among 15 teams
- Ranked 5th in F1-strict metrics on the test set
- Showed 2.5% improvement in F1(E′) on the development set when using reasoning revision technique
- Demonstrated effectiveness of Chain-of-Thought reasoning for emotion cause extraction in conversations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought reasoning improves emotion cause extraction by decomposing the problem into three logical steps: identifying text spans, inferring implicit opinions, and determining the final emotion.
- Mechanism: The THOR framework applies structured prompting that guides the model through a reasoning chain, allowing it to consider context and common sense before making final predictions.
- Core assumption: LLMs can benefit from explicit reasoning steps that mimic human-like logical progression when solving complex emotion analysis tasks.
- Evidence anchors:
  - [abstract]: "Inspired by the most recent advances in Chain-of-Thought, in this work, we exploit the existing three-hop reasoning approach (THOR)"
  - [section 2.1]: "Instead of directly asking LLM the final result at each stage, we exploit the Chain-of-Thought (CoT) concept in the form of the Three-hop Reasoning (THOR) framework"
- Break condition: The reasoning chain breaks if intermediate steps fail to capture relevant context, leading to cascading errors in final predictions.

### Mechanism 2
- Claim: Reasoning revision using annotated emotion states improves model alignment on state-cause dependencies by providing supervision that speakers tend to cause emotions similar to their own states.
- Mechanism: After predicting the emotion caused by a speaker, the model is prompted to infer the speaker's own emotion state, which is then used as additional context to refine the final prediction.
- Core assumption: There is a strong correlation between a speaker's emotional state and the emotions they tend to cause in others, except for NEUTRAL states.
- Evidence anchors:
  - [section 2.2]: "To revise this knowledge, in this paper, we impute the following prompt to support our opinion O, obtained at the end of the THORCAUSE step 2"
  - [section 4]: "Switching from PROMPT to THORCAUSE -RR technique, we investigate the improvement by 2.5% percent by F 1(E′) on the dev part"
- Break condition: The revision mechanism fails when the speaker's emotion state does not correlate with the emotion they cause, or when the initial prediction is too far from the correct answer for revision to help.

### Mechanism 3
- Claim: Two-stage training with THORSTATE followed by THORCAUSE creates a more effective learning progression by first establishing emotion state recognition before tackling emotion cause prediction.
- Mechanism: The model is first fine-tuned on emotion state prediction (THORSTATE), then uses this learned capability as a foundation for the more complex task of emotion cause prediction (THORCAUSE).
- Core assumption: Emotion state recognition is a simpler, more fundamental task that provides useful representations for the more complex emotion cause prediction task.
- Evidence anchors:
  - [abstract]: "We propose a two-stage training mechanism for performing instruction-tuning on large language models (LLMs), aimed at accurately inferring of the task answers"
  - [section 2]: "Therefore, for emotion-cause pairs extraction we use the STAGE 2 towards the model tuned in STAGE 1 to infer ec ∈ E′ caused by usrc towards utgt"
- Break condition: The two-stage approach fails if the first stage (emotion state recognition) does not generalize well to the second stage, or if the tasks are too dissimilar for knowledge transfer.

## Foundational Learning

- Concept: Chain-of-Thought reasoning
  - Why needed here: Emotion cause extraction is inherently a multi-step reasoning problem that benefits from breaking down into logical progression rather than direct prediction
  - Quick check question: Can you explain why predicting emotion causes directly might be harder than predicting emotion states first?

- Concept: Supervised reasoning revision
  - Why needed here: Provides additional training signal by leveraging known correlations between speaker states and emotions they cause, improving model alignment
  - Quick check question: How does the reasoning revision mechanism use the speaker's own emotion state to improve predictions?

- Concept: Two-stage training methodology
  - Why needed here: Allows the model to first learn simpler emotion state recognition before tackling the more complex emotion cause prediction task
  - Quick check question: What advantages does two-stage training offer over single-stage training for this task?

## Architecture Onboarding

- Component map: Data preprocessing -> THORSTATE fine-tuning -> THORCAUSE-RR fine-tuning -> Reasoning revision -> Span correction -> Evaluation
- Critical path: Training flow follows preprocessing → THORSTATE fine-tuning → THORCAUSE fine-tuning → reasoning revision → evaluation, with each stage dependent on the previous one
- Design tradeoffs: The choice of Flan-T5 base (250M) balances model capacity with computational efficiency, while the two-stage approach trades training complexity for improved performance
- Failure signatures: Poor performance on emotion state recognition in stage 1 will cascade to stage 2; overfitting after 2-3 epochs indicates insufficient regularization; low F1-strict scores suggest span prediction issues
- First 3 experiments:
  1. Fine-tune Flan-T5 base on Dstate using THORSTATE to verify emotion state recognition capability (baseline)
  2. Compare PROMPT vs THORCAUSE vs THORCAUSE-RR on Dcause development set to validate reasoning revision effectiveness
  3. Apply span correction algorithm to test predictions and measure impact on F1-strict metric

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reasoning revision technique impact the model's performance when dealing with utterances that have neutral speaker states?
- Basis in paper: [explicit] The authors note that the correlation between speaker state and caused emotion is similar to training data, but misalignment occurs when emotion ∈ E is caused on an utterance with ustate = NEUTRAL.
- Why unresolved: The paper does not provide a detailed analysis of how the reasoning revision technique handles or improves predictions for neutral speaker states.
- What evidence would resolve it: Performance metrics comparing the model's accuracy on utterances with neutral speaker states, with and without the reasoning revision technique.

### Open Question 2
- Question: What is the effect of using larger language models (LLMs) on the emotion cause extraction task?
- Basis in paper: [inferred] The authors suggest future work could include analysis of larger models, implying current experiments were limited to the Flan-T5 base (250M) model.
- Why unresolved: The paper does not explore the performance of larger models, leaving the potential benefits or drawbacks of scaling up unaddressed.
- What evidence would resolve it: Comparative results showing the performance of different-sized models on the same task, highlighting improvements or limitations.

### Open Question 3
- Question: How does the algorithm-based span correction technique affect the strict F1 score, and could alternative methods provide better results?
- Basis in paper: [explicit] The paper mentions that the methodology is limited to utterance-level emotion cause prediction, reflected in the relatively low F1s results, and uses a rule-based approach for span correction.
- Why unresolved: The effectiveness of the current span correction technique is not thoroughly evaluated, and the potential for alternative methods is not explored.
- What evidence would resolve it: Detailed analysis of the impact of the current span correction technique on F1s scores, along with comparisons to other span correction methods.

## Limitations
- The approach relies on the assumption that emotion cause extraction can be decomposed into logical steps that LLMs can follow, which may not generalize to all conversation types
- The reasoning revision mechanism depends on a correlation between speaker states and emotions they cause that appears fragile, particularly for NEUTRAL states
- The rule-based span correction introduces an external dependency that may not be applicable to all datasets or conversation styles

## Confidence
- High confidence: The paper's methodology is clearly described and the results are reproducible given the same dataset and implementation details. The improvement over baseline methods is statistically significant and the ranking provides objective validation.
- Medium confidence: The theoretical justification for Chain-of-Thought reasoning in emotion cause extraction is sound but not extensively validated.
- Low confidence: The generalizability of the reasoning revision mechanism beyond the specific dataset used, and the robustness of the approach to different conversation types, speakers, and cultural contexts remain uncertain.

## Next Checks
1. Cross-dataset validation: Test the THOR framework on multiple conversation datasets (e.g., MELD, EmoryNLP) to verify that the Chain-of-Thought approach generalizes beyond the Friends dataset used in SemEval-2024 Task 3.

2. Ablation study on reasoning revision: Conduct controlled experiments removing the reasoning revision component to quantify its exact contribution to performance improvements, and test its effectiveness across different emotion categories.

3. Alternative multi-stage architectures: Compare the two-stage training approach against other multi-task learning frameworks (e.g., shared encoder with task-specific decoders) to determine if the observed improvements are specific to the THOR framework or represent a more general principle of staged learning for emotion cause extraction.