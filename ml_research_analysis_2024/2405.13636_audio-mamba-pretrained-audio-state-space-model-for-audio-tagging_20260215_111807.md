---
ver: rpa2
title: 'Audio Mamba: Pretrained Audio State Space Model For Audio Tagging'
arxiv_id: '2405.13636'
source_url: https://arxiv.org/abs/2405.13636
tags:
- audio
- mamba
- performance
- transformer
- patch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Audio Mamba, a state space model for audio
  tagging that addresses the quadratic self-attention cost limitation of audio transformers.
  The model employs hierarchical representations and a multi-stage architecture to
  efficiently capture and process audio features.
---

# Audio Mamba: Pretrained Audio State Space Model For Audio Tagging

## Quick Facts
- arXiv ID: 2405.13636
- Source URL: https://arxiv.org/abs/2405.13636
- Authors: Jiaju Lin; Haoxuan Hu
- Reference count: 0
- Achieves comparable results to state-of-the-art audio transformers on AudioSet with significantly fewer parameters (40M vs 31M)

## Executive Summary
Audio Mamba introduces a state space model for audio tagging that addresses the quadratic self-attention cost limitation of audio transformers. The model employs hierarchical representations and a multi-stage architecture to efficiently capture and process audio features. By leveraging the computational efficiency of state space models, Audio Mamba achieves strong performance on audio tagging tasks while maintaining parameter efficiency compared to transformer-based approaches.

## Method Summary
Audio Mamba employs a state space model architecture with hierarchical representations and a multi-stage processing pipeline. The model processes audio inputs through successive stages that progressively increase receptive fields while reducing temporal resolution. Each stage consists of state space blocks that capture temporal dependencies more efficiently than traditional attention mechanisms. The hierarchical design allows the model to capture both fine-grained and coarse-grained audio features while maintaining computational efficiency through reduced parameter counts compared to transformer architectures.

## Key Results
- Achieves mAP of 0.440 on AudioSet benchmark
- Achieves mAUC of 0.963 on AudioSet benchmark
- Achieves d-prime of 2.51 on AudioSet benchmark
- Demonstrates parameter efficiency with 40M parameters compared to transformer alternatives

## Why This Works (Mechanism)
Audio Mamba leverages the computational efficiency of state space models to overcome the quadratic complexity of self-attention in audio transformers. The hierarchical architecture enables efficient capture of multi-scale temporal dependencies in audio signals, with each stage progressively expanding the receptive field while reducing temporal resolution. This design allows the model to efficiently process long audio sequences without the memory and computational bottlenecks associated with transformer self-attention mechanisms.

## Foundational Learning

**State Space Models**: Why needed: Provide efficient temporal modeling with linear complexity. Quick check: Verify state matrices maintain stability across different time scales.

**Hierarchical Representations**: Why needed: Capture multi-scale audio features from fine to coarse. Quick check: Ensure receptive fields align with natural audio hierarchies.

**Multi-stage Processing**: Why needed: Balance temporal resolution with computational efficiency. Quick check: Validate that stage-wise downsampling preserves critical temporal information.

## Architecture Onboarding

**Component Map**: Audio Input -> Multi-stage State Space Blocks -> Hierarchical Feature Aggregation -> Classification Head

**Critical Path**: The core processing pipeline involves sequential state space blocks in each stage, where temporal resolution is progressively reduced while receptive fields expand. The state matrices in each block capture temporal dependencies within their respective receptive fields.

**Design Tradeoffs**: The hierarchical downsampling introduces a tradeoff between temporal resolution and computational efficiency. Smaller window sizes in early stages preserve fine-grained details but increase computational cost, while larger window sizes in later stages capture long-range dependencies more efficiently.

**Failure Signatures**: Performance degradation may occur when state matrices become unstable at extreme time scales, or when hierarchical downsampling removes critical temporal information needed for fine-grained classification. Model may struggle with very short audio clips where sufficient temporal context cannot be established.

**First Experiments**:
1. Validate state space block stability across different audio time scales and frequencies
2. Test hierarchical feature aggregation effectiveness with varying downsampling rates
3. Evaluate classification performance with different state space model configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow evaluation scope limited to AudioSet dataset without validation across diverse audio domains
- Apparent contradiction in parameter efficiency claims (40M vs 31M parameters) requiring clarification
- Limited ablation studies on architectural sensitivity to window sizes and expansion factors

## Confidence

**High Confidence**: Audio Mamba achieves competitive performance on AudioSet with a state space model architecture. The reported metrics (mAP 0.440, mAUC 0.963, d-prime 2.51) are specific and verifiable.

**Medium Confidence**: Efficiency claims regarding parameter count and computational complexity. The apparent contradiction in parameter numbers (40M vs 31M) creates uncertainty about the true efficiency gains.

**Low Confidence**: Generalization claims to broader audio domains and tasks beyond audio tagging, as these are not empirically validated in the paper.

## Next Checks

1. Conduct cross-domain evaluation on speech recognition and music classification benchmarks to assess generalization beyond audio tagging tasks.

2. Perform detailed ablation studies varying window sizes and expansion factors to establish sensitivity and guide architectural choices for different audio applications.

3. Clarify and verify the parameter efficiency comparison with transformers, including FLOPs analysis and memory usage measurements across different hardware configurations.