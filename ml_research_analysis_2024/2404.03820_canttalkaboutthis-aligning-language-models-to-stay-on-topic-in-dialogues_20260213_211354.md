---
ver: rpa2
title: 'CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues'
arxiv_id: '2404.03820'
source_url: https://arxiv.org/abs/2404.03820
tags:
- user
- conversation
- distractors
- dataset
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of language models failing to
  maintain topic relevance in task-oriented dialogues. It introduces a novel dataset,
  CantTalkAboutThis, which consists of synthetic dialogues with distractor turns designed
  to divert the chatbot from the predefined topic.
---

# CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues

## Quick Facts
- arXiv ID: 2404.03820
- Source URL: https://arxiv.org/abs/2404.03820
- Reference count: 33
- Fine-tuning language models on synthetic dialogues with distractors improves topic adherence in task-oriented conversations

## Executive Summary
This paper addresses the challenge of language models failing to maintain topic relevance in task-oriented dialogues. The authors introduce CantTalkAboutThis, a novel synthetic dataset consisting of dialogues with strategically placed distractor turns designed to divert the conversation from predefined topics. Through fine-tuning on this dataset, the STAY-ON-TOPIC models demonstrate significantly improved ability to deflect off-topic interactions while maintaining conversational flow. The approach shows particular promise for task-oriented systems where topic drift can compromise user experience and task completion.

## Method Summary
The authors developed a systematic approach to generate synthetic dialogues with distractors. The process involves generating diverse scenarios across different domains, crafting topical system instructions, constructing on-topic conversations, and incorporating distractor turns. The STAY-ON-TOPIC models are then fine-tuned on this synthetic data, enabling them to better recognize and deflect off-topic content while staying aligned with the conversation's original purpose. The dataset includes conversations with distractors at various positions (1st, 3rd, 5th, 7th, and 9th turns) to evaluate model robustness across different conversation stages.

## Key Results
- STAY-ON-TOPIC -43B model achieves F1 scores of 0.993 for both on-topic and distractor turns on synthetic test sets
- Fine-tuned models show 2.3% improvement in instruction following tasks on the WILD dataset compared to baseline models
- Models demonstrate improved content moderation capabilities while maintaining task-oriented focus

## Why This Works (Mechanism)
The effectiveness stems from exposing language models to controlled, synthetic examples of topic drift during training. By learning to recognize and deflect both on-topic and off-topic content in a variety of scenarios, the models develop stronger topic adherence capabilities. The synthetic nature of the training data allows for systematic coverage of different distractor types and positions within conversations.

## Foundational Learning
- **Synthetic dialogue generation**: Why needed - To create controlled training examples with predictable topic drift patterns; Quick check - Verify generated dialogues maintain logical flow while incorporating distractors
- **Fine-tuning methodology**: Why needed - To adapt pre-trained models to better handle topic adherence; Quick check - Compare performance before and after fine-tuning on validation sets
- **Topic coherence metrics**: Why needed - To quantitatively measure model performance in staying on topic; Quick check - Validate F1 scores against human annotations

## Architecture Onboarding

Component map:
Scenario Generator -> Instruction Crafter -> Conversation Builder -> Distractor Inserter -> Model Fine-tuner -> Evaluation Pipeline

Critical path: The model fine-tuning process represents the critical path, as it directly determines the final model's ability to handle topic adherence. This involves training on the synthetic dataset, validating performance, and iterating on the training process.

Design tradeoffs: The primary tradeoff involves synthetic versus real human-generated distractors. Synthetic data offers scalability and control but may lack the naturalness of human-crafted content. The authors acknowledge this limitation while demonstrating that synthetic training still yields significant improvements.

Failure signatures: Models may overfit to specific distractor patterns in the synthetic data, leading to poor generalization to real-world topic drifts. Performance may degrade in longer conversations beyond the 9-turn limit tested in the study.

3 first experiments:
1. Fine-tune a base model on the synthetic CantTalkAboutThis dataset and evaluate on synthetic test sets
2. Test the fine-tuned model on real-world datasets (DialogRE, MCD, DSTC9) to assess generalization
3. Conduct ablation studies by varying distractor positions and types during training

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the effectiveness of the STAY-ON-TOPIC models generalize to longer conversations beyond 9 turns?
- Basis in paper: The paper mentions an ablation study examining distractor susceptibility at different positions (1st, 3rd, 5th, 7th, and 9th turns) but does not explore beyond 9 turns.
- Why unresolved: The study was limited to 20 conversations and did not extend the analysis to longer dialogues, leaving uncertainty about model performance in extended interactions.
- What evidence would resolve it: Testing the models on conversations with 15-20+ turns and measuring distractor engagement rates at each position would clarify if performance degrades further.

### Open Question 2
- Question: How does the performance of STAY-ON-TOPIC models compare when trained on human-annotated distractors versus synthetic ones?
- Basis in paper: The paper acknowledges that synthetic distractors are "simpler to detect and more unnatural" than human-crafted ones, but only tests a small human-annotated dataset without comparing training approaches.
- Why unresolved: While the paper shows fine-tuned models perform well on human test sets, it does not explore whether training on human-generated distractors would yield even better results.
- What evidence would resolve it: Training STAY-ON-TOPIC models on a large human-annotated dataset and comparing their performance to models trained only on synthetic data would determine the impact of training data quality.

### Open Question 3
- Question: Can the topic-following approach be extended to multi-turn task-oriented dialogues where the topic evolves dynamically?
- Basis in paper: The current dataset focuses on static scenarios with predefined topics, but real-world task-oriented systems often require adapting to evolving user needs while maintaining relevance.
- Why unresolved: The paper does not test whether models can handle topic shifts within an allowed domain (e.g., transitioning from flight booking to hotel reservations in a travel scenario) without being derailed.
- What evidence would resolve it: Evaluating STAY-ON-TOPIC models on dialogues where the topic gradually shifts within the same domain, measuring their ability to stay relevant while accommodating natural topic evolution, would answer this.

## Limitations
- Synthetic training data may not fully capture the complexity of real-world conversational drifts
- Near-perfect F1 scores on synthetic test sets suggest potential overfitting to specific distractor patterns
- Performance gains on real-world datasets are more modest (0.2-2.3% improvements), indicating context-dependent effectiveness

## Confidence
- High confidence in the dataset construction methodology and its ability to generate controlled, topical dialogues with distractors
- Medium confidence in the generalizability of fine-tuned models to real-world off-topic scenarios, given the performance gap between synthetic and real datasets
- Medium confidence in the claimed improvements for fine-grained instruction following, as the gains are incremental and task-specific

## Next Checks
1. Evaluate the fine-tuned models on human-annotated dialogues containing naturally occurring topic drifts to assess real-world robustness
2. Test model performance across diverse dialogue domains beyond task-oriented scenarios (e.g., open-domain chatbots, creative writing assistants)
3. Compare the synthetic training approach against alternative methods like reinforcement learning from human feedback (RLHF) or in-context learning with real off-topic examples