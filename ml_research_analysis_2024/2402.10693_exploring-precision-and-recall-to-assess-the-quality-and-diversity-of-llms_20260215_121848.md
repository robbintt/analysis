---
ver: rpa2
title: Exploring Precision and Recall to assess the quality and diversity of LLMs
arxiv_id: '2402.10693'
source_url: https://arxiv.org/abs/2402.10693
tags:
- recall
- precision
- generation
- diversity
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Precision and Recall metrics adapted from
  image generation to evaluate large language models (LLMs) like Llama-2 and Mistral,
  providing a new way to assess both the quality and diversity of generated text without
  requiring aligned corpora. The method uses k-nearest neighbors on latent embeddings
  to estimate support overlap between model-generated and reference distributions.
---

# Exploring Precision and Recall to assess the quality and diversity of LLMs

## Quick Facts
- **arXiv ID**: 2402.10693
- **Source URL**: https://arxiv.org/abs/2402.10693
- **Reference count**: 40
- **Primary result**: Introduces Precision and Recall metrics adapted from image generation to evaluate LLMs' quality and diversity without requiring aligned corpora

## Executive Summary
This paper adapts Precision and Recall metrics from image generation to evaluate large language models (LLMs) like Llama-2 and Mistral. The approach measures the overlap between generated text embeddings and reference distributions in latent space, providing a way to assess both quality and diversity without aligned corpora. Evaluations on WebText generation, Wikipedia biographies, and creative text show instruction-tuned models achieve higher precision but lower recall than pre-trained models, indicating better quality but reduced diversity. The metrics reveal nuanced model behaviors and trade-offs not captured by traditional benchmarks, with larger models generally showing higher recall.

## Method Summary
The method computes Precision and Recall for LLMs by first generating text samples, then embedding them using GPT-2 LARGE, reducing dimensionality with PCA to retain 90% variance, and estimating support regions via k-NN (k=4). Precision measures the proportion of generated samples within the reference support, while Recall measures the proportion of reference samples within the generated support. This approach allows evaluation of quality and diversity without requiring aligned reference-candidate pairs, making it suitable for open-ended text generation tasks.

## Key Results
- Instruction-tuned models achieve higher precision but lower recall compared to pre-trained models, indicating better quality but reduced diversity
- Larger models generally exhibit higher recall, reflecting greater expressive power and diversity in generated text
- The metrics reveal nuanced model behaviors and trade-offs not captured by traditional benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Precision and Recall metrics capture both quality and diversity of LLM-generated text by measuring the overlap between generated and reference distributions in latent space.
- **Mechanism**: The metrics compute the proportion of generated text embeddings that fall within the support of reference embeddings (Precision) and vice versa (Recall). This is done through k-nearest neighbors (k-NN) estimation in a reduced-dimensional latent space.
- **Core assumption**: The latent space embeddings (from GPT-2) effectively capture semantic content of text, allowing meaningful distance-based support estimation.
- **Evidence anchors**:
  - [abstract] "This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora."
  - [section 4.1] Describes the pipeline: text → embeddings → PCA reduction → k-NN support estimation → Precision/Recall computation.
  - [corpus] Weak. The paper references using GPT-2 embeddings as successful in prior work (MAUVE), but does not provide direct validation that these embeddings are optimal for Precision/Recall in this context.
- **Break condition**: If the embedding space does not adequately represent semantic similarity, or if the k-NN support estimation is too sensitive to noise or outliers, the metrics will not accurately reflect quality and diversity.

### Mechanism 2
- **Claim**: Instruction-tuned models achieve higher Precision but lower Recall compared to pre-trained models, indicating a trade-off between quality and diversity.
- **Mechanism**: Fine-tuning with human feedback encourages models to generate text that closely matches human-like outputs (increasing Precision) but reduces the variety of outputs (decreasing Recall).
- **Core assumption**: Human feedback alignment explicitly rewards outputs that are similar to reference texts, implicitly penalizing diverse but potentially less conventional outputs.
- **Evidence anchors**:
  - [abstract] "evaluations... show that instruction-tuned models achieve higher precision but lower recall compared to pre-trained models, indicating better quality but reduced diversity."
  - [section 5.3] Figure 1 shows instruction-tuned models (e.g., Llama-2 Chat) have higher Precision and lower Recall than pre-trained counterparts.
  - [corpus] Weak. The paper does not provide direct evidence of the mechanism by which human feedback reduces diversity, only observes the correlation.
- **Break condition**: If the instruction-tuning process does not significantly alter the output distribution, or if the evaluation task does not adequately capture the trade-off, the observed pattern may not hold.

### Mechanism 3
- **Claim**: Larger models generally exhibit higher Recall, reflecting greater expressive power and diversity in generated text.
- **Mechanism**: Increased model capacity allows for a broader range of outputs, which translates to a larger support in the latent space, captured by higher Recall.
- **Core assumption**: Model size directly correlates with the diversity of text it can generate, and this diversity is accurately reflected in the latent space representation.
- **Evidence anchors**:
  - [abstract] "larger models generally showing higher recall."
  - [section 5.3] Mentions that "The Recall is consistently better for larger models."
  - [corpus] Weak. The paper does not provide a detailed analysis of why larger models have higher Recall, only states the observation.
- **Break condition**: If the evaluation task or the reference dataset does not adequately challenge the model's expressive power, or if the latent space representation does not scale with model size, the observed pattern may not hold.

## Foundational Learning

- **Concept**: Distribution-based evaluation metrics
  - Why needed here: Traditional benchmarks rely on aligned references and specific tasks, which are insufficient for evaluating open-ended generation capabilities of LLMs. Distribution-based metrics allow comparison of the overall output distribution of a model against a reference distribution.
  - Quick check question: What is the key difference between sample-based and distribution-based evaluation metrics?

- **Concept**: k-Nearest Neighbors (k-NN) for support estimation
  - Why needed here: k-NN is used to estimate the support of the reference and generated distributions in the latent space, which is necessary for computing Precision and Recall.
  - Quick check question: How does k-NN help in estimating the support of a distribution in a high-dimensional space?

- **Concept**: Latent space embeddings and dimensionality reduction
  - Why needed here: Text is projected into a latent space using pre-trained embeddings (GPT-2) and then reduced via PCA to facilitate k-NN estimation and reduce noise.
  - Quick check question: Why is PCA used after obtaining text embeddings, and what is its impact on the Precision and Recall metrics?

## Architecture Onboarding

- **Component map**: Text → GPT-2 Embeddings → PCA Reduction → k-NN Support Estimation → Precision/Recall Computation

- **Critical path**: 
  1. Obtain embeddings for reference and generated texts
  2. Apply PCA to reduce dimensionality
  3. Estimate support regions using k-NN
  4. Compute Precision and Recall based on support overlap

- **Design tradeoffs**:
  - Choice of embedding model (GPT-2 LARGE): Balances semantic representation quality with computational efficiency. Alternative models might capture different aspects of text.
  - PCA variance retention (90%): Balances dimensionality reduction with information loss. Lower retention might improve computational efficiency but reduce metric accuracy.
  - k-NN parameter (k=4): Balances sensitivity to local structure with robustness to noise. Higher k might improve robustness but reduce sensitivity to fine-grained differences.

- **Failure signatures**:
  - Low Precision, high Recall: Model generates diverse but low-quality text, or reference dataset is too narrow.
  - High Precision, low Recall: Model generates high-quality but narrow text, or reference dataset is too broad.
  - Both metrics close to 1: Model perfectly matches reference distribution, or evaluation is flawed (e.g., overfitting to reference).
  - High variance across seeds: Metric is sensitive to random initialization or sampling, or evaluation setup needs refinement.

- **First 3 experiments**:
  1. **Vary the number of samples (N)**: Generate different numbers of samples (e.g., 100, 1000, 4000) and observe how Precision and Recall stabilize. This validates the choice of N=4000 in the paper.
  2. **Vary the k-NN parameter (k)**: Compute metrics for different k values (e.g., 1, 4, 10, 20) and observe how they change. This validates the choice of k=4 and provides insight into metric sensitivity.
  3. **Compare different embedding models**: Compute metrics using embeddings from different models (e.g., GPT-2, BERT, RoBERTa) and compare results. This validates the choice of GPT-2 and explores the impact of embedding choice on metric quality.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do Precision and Recall metrics behave when evaluated on domain-specific text generation tasks, such as legal or medical documents?
- **Basis in paper**: [inferred] The paper evaluates Precision and Recall on open-ended tasks like WebText, Wikipedia biographies, and creative writing, but does not explore domain-specific text generation.
- **Why unresolved**: Domain-specific tasks often have strict formatting, terminology, and style requirements, which may not be well-captured by the general-purpose embeddings used in the study. The impact of domain-specific constraints on Precision and Recall is unclear.
- **What evidence would resolve it**: Conducting experiments on domain-specific datasets (e.g., legal contracts, medical reports) and comparing Precision and Recall scores with human evaluations or task-specific metrics would provide insights into the metrics' effectiveness in specialized domains.

### Open Question 2
- **Question**: How sensitive are Precision and Recall to the choice of embedding model, and what are the implications for cross-lingual text generation evaluation?
- **Basis in paper**: [explicit] The paper uses GPT-2 embeddings for computing Precision and Recall, acknowledging that the choice of embedding model may impact performance.
- **Why unresolved**: The study does not explore the effects of alternative embedding models (e.g., multilingual embeddings) on Precision and Recall, especially for evaluating text generation in languages other than English.
- **What evidence would resolve it**: Recomputing Precision and Recall using different embedding models (e.g., multilingual BERT, XLM-R) on multilingual text generation datasets and analyzing the consistency of results across languages would clarify the metrics' cross-lingual applicability.

### Open Question 3
- **Question**: How do Precision and Recall metrics compare to traditional task-specific evaluation metrics in terms of correlation with human judgments for text generation quality?
- **Basis in paper**: [inferred] The paper focuses on distribution-based metrics (Precision, Recall, MAUVE) but does not directly compare them with traditional task-specific metrics (e.g., ROUGE for summarization, BLEU for translation) in terms of human correlation.
- **Why unresolved**: While the paper demonstrates the advantages of Precision and Recall in capturing quality and diversity, it does not empirically validate their correlation with human assessments compared to established task-specific metrics.
- **What evidence would resolve it**: Conducting user studies where human evaluators rate text generation outputs and comparing the correlation between Precision/Recall scores and human judgments with the correlation of task-specific metrics and human judgments would provide a direct comparison of evaluation methods.

## Limitations

- **Embedding space validity**: The reliance on GPT-2 embeddings assumes they capture relevant aspects of text quality and diversity, but there's no direct validation that these are optimal for Precision/Recall
- **Support estimation sensitivity**: The k-NN support estimation with k=4 may be sensitive to noise and outliers in the embedding space
- **Reference dataset representativeness**: The chosen reference datasets may not fully represent the intended application space of the evaluated models

## Confidence

**Claim Cluster 1: Precision-Recall Trade-off in Instruction-Tuned Models** (High confidence): The observation that instruction-tuned models show higher Precision but lower Recall is well-supported by the experimental results across multiple datasets and model families.

**Claim Cluster 2: Larger Models Have Higher Recall** (Medium confidence): The consistent pattern of larger models showing higher Recall is observed, but the paper does not provide a detailed analysis of why this occurs or test whether this holds across different task domains.

**Claim Cluster 3: Metrics Capture Quality and Diversity Without Aligned Corpora** (Low confidence): While the paper demonstrates the metrics work in practice, there is limited validation that they accurately capture the intuitive notions of quality and diversity compared to established benchmarks.

## Next Checks

1. **Embedding Model Ablation**: Compute Precision and Recall using embeddings from multiple models (GPT-2, BERT, RoBERTa) on the same datasets and compare results to assess sensitivity to embedding choice.

2. **k-Value Sensitivity Analysis**: Systematically vary the k parameter in k-NN (k=1, 4, 10, 20) and measure how Precision and Recall change to establish metric stability and robustness.

3. **Correlation with Human Judgments**: Conduct a small-scale human evaluation where annotators rate model outputs for quality and diversity, then correlate these judgments with Precision and Recall scores to validate the metrics' construct validity.