---
ver: rpa2
title: 'MAmmoTH2: Scaling Instructions from the Web'
arxiv_id: '2405.03548'
source_url: https://arxiv.org/abs/2405.03548
tags: []
core_contribution: This paper presents MAmmoTH2, a new approach to harvest 10 million
  instruction-response pairs from web data to enhance large language model (LLM) reasoning
  abilities. The method involves recalling relevant documents from Common Crawl, extracting
  Q-A pairs, and refining them using open-source LLMs.
---

# MAmmoTH2: Scaling Instructions from the Web

## Quick Facts
- **arXiv ID**: 2405.03548
- **Source URL**: https://arxiv.org/abs/2405.03548
- **Reference count**: 37
- **Primary result**: MAmmoTH2-7B improves Mistral-7B's MATH accuracy from 11% to 34.2% and GSM8K from 36% to 67%

## Executive Summary
MAmmoTH2 presents a novel approach to harvesting 10 million instruction-response pairs from web data to enhance large language model reasoning abilities. The method involves recalling relevant documents from Common Crawl, extracting Q-A pairs, and refining them using open-source LLMs. Fine-tuning base LLMs on this dataset significantly boosts performance on reasoning benchmarks without training on in-domain data. For example, MAmmoTH2-7B improves Mistral-7B's MATH accuracy from 11% to 34.2% and GSM8K from 36% to 67%. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. The results demonstrate the effectiveness of harvesting large-scale, high-quality instruction data from the web without costly human annotation or GPT-4 distillation.

## Method Summary
The MAmmoTH2 approach harvests instruction-response pairs by first recalling relevant documents from Common Crawl using a document retriever. The retrieved documents are then processed to extract Q-A pairs using an open-source LLM. These pairs are refined and filtered to ensure quality before being used to fine-tune base LLMs. The process involves multiple stages of document recall, pair extraction, and refinement to create a large-scale dataset of high-quality instruction data. The resulting models, MAmmoTH2 and MAmmoTH2-Plus, are evaluated on various reasoning and chatbot benchmarks, showing significant improvements over baseline models.

## Key Results
- MAmmoTH2-7B improves Mistral-7B's MATH accuracy from 11% to 34.2%
- MAmmoTH2-7B improves GSM8K accuracy from 36% to 67%
- MAmmoTH2-Plus achieves state-of-the-art performance on several reasoning and chatbot benchmarks

## Why This Works (Mechanism)
The approach works by scaling up the quantity of high-quality instruction data available for fine-tuning LLMs. By harvesting 10 million instruction-response pairs from web data, the method provides a diverse and extensive training corpus that enhances the model's reasoning abilities. The use of open-source LLMs for data refinement ensures that the extracted pairs are of high quality, addressing concerns about data noise and relevance. This large-scale, high-quality instruction data enables the fine-tuned models to perform better on reasoning tasks without the need for in-domain training or costly human annotation.

## Foundational Learning
- **Document retrieval from Common Crawl**: Why needed - To access a vast amount of web data for instruction pair extraction; Quick check - Verify the retrieval accuracy and relevance of documents
- **Q-A pair extraction using LLMs**: Why needed - To convert raw web documents into structured instruction-response pairs; Quick check - Evaluate the quality and consistency of extracted pairs
- **Data refinement and filtering**: Why needed - To ensure the high quality of instruction data by removing noise and irrelevant pairs; Quick check - Assess the effectiveness of filtering criteria on data quality
- **Fine-tuning on large-scale instruction data**: Why needed - To enhance model reasoning abilities through exposure to diverse and extensive instruction-response pairs; Quick check - Compare model performance before and after fine-tuning on reasoning benchmarks
- **Evaluation on reasoning benchmarks**: Why needed - To measure the effectiveness of the fine-tuned models on specific reasoning tasks; Quick check - Analyze performance improvements across different benchmark datasets
- **State-of-the-art comparison**: Why needed - To contextualize the model's performance against existing methods; Quick check - Verify benchmark results against published baselines

## Architecture Onboarding

**Component map**: Document Retriever -> Document Processor -> Q-A Pair Extractor -> Data Refiner -> LLM Fine-tuner

**Critical path**: Document Retriever retrieves relevant documents from Common Crawl → Document Processor preprocesses and filters documents → Q-A Pair Extractor uses open-source LLM to extract instruction-response pairs → Data Refiner filters and refines pairs for quality → LLM Fine-tuner trains base model on refined dataset

**Design tradeoffs**: The approach prioritizes scale and diversity of instruction data over precision, accepting some noise in exchange for quantity. Using open-source LLMs for refinement balances quality control with computational efficiency. The multi-stage processing pipeline adds complexity but enables better data quality control compared to single-pass extraction methods.

**Failure signatures**: Poor document recall leads to irrelevant instruction pairs; noisy Q-A extraction produces low-quality training data; inadequate refinement allows poor-quality pairs to persist; overfitting to extracted patterns rather than general reasoning; benchmark-specific improvements that don't generalize to new tasks.

**First experiments**:
1. Evaluate document recall accuracy by measuring relevance of retrieved Common Crawl documents
2. Sample and manually assess quality of extracted Q-A pairs before and after refinement
3. Test model performance on held-out reasoning tasks not present in the training data

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on Common Crawl data introduces concerns about data quality and relevance, as the extraction pipeline may include noisy or low-quality instruction-response pairs
- Performance improvements shown on reasoning benchmarks may be partially benchmark-specific, with limited evidence of generalization to unseen tasks
- Claims about avoiding human annotation while maintaining quality are not well-validated through systematic quality analysis of the harvested dataset

## Confidence
- **High confidence**: The basic methodology of harvesting instruction data from web sources is technically sound and reproducible
- **Medium confidence**: The reported benchmark improvements are likely real but may be partially benchmark-specific
- **Low confidence**: Claims about avoiding human annotation while maintaining quality are not well-validated

## Next Checks
1. Conduct a detailed quality analysis of the harvested dataset by sampling and manually evaluating instruction-response pairs across different domains
2. Test model generalization on held-out reasoning tasks not seen during fine-tuning or in related benchmark sets
3. Perform ablation studies comparing performance when using filtered versus raw Common Crawl data, and when varying the scale of instruction data