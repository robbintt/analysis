---
ver: rpa2
title: An Analysis of Sentential Neighbors in Implicit Discourse Relation Prediction
arxiv_id: '2405.09735'
source_url: https://arxiv.org/abs/2405.09735
tags:
- discourse
- context
- relation
- implicit
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examines whether adding contextual sentences beyond\
  \ immediate discourse units improves implicit discourse relation classification.\
  \ The authors propose three context window strategies\u2014Direct Neighbors (DN),\
  \ Expanded Window Neighbors (EWN), and Part-Smart Random Neighbors (PSRN)\u2014\
  and compare them against a baseline model using DistilBERT on the Penn Discourse\
  \ TreeBank."
---

# An Analysis of Sentential Neighbors in Implicit Discourse Relation Prediction

## Quick Facts
- arXiv ID: 2405.09735
- Source URL: https://arxiv.org/abs/2405.09735
- Reference count: 0
- Primary result: Adding contextual sentences beyond immediate discourse units degrades implicit discourse relation classification performance

## Executive Summary
This study investigates whether incorporating contextual sentences beyond the immediate discourse units improves implicit discourse relation classification. The authors propose three context window strategies—Direct Neighbors (DN), Expanded Window Neighbors (EWN), and Part-Smart Random Neighbors (PSRN)—and compare them against a baseline model using DistilBERT on the Penn Discourse TreeBank. Contrary to expectations, results show that including additional context harms performance, with the baseline achieving 58.9% accuracy and 58.3% F1 compared to context variants that perform worse. Statistical tests confirm the baseline's superiority, suggesting that broader context may introduce noise rather than helpful information for this task.

## Method Summary
The authors propose three context window strategies to incorporate additional sentences beyond the immediate discourse units: Direct Neighbors (DN) which adds the sentence before and after the target pair, Expanded Window Neighbors (EWN) which includes two sentences before and after, and Part-Smart Random Neighbors (PSRN) which samples from non-adjacent sentences. These strategies are implemented using DistilBERT on the Penn Discourse TreeBank. The model concatenates the additional context sentences with the target discourse units, separated by special tokens, and fine-tunes the entire architecture end-to-end. Performance is evaluated using accuracy and F1-score metrics.

## Key Results
- Baseline model (no additional context) achieves 58.9% accuracy and 58.3% F1
- All context window variants perform worse than baseline
- PSRN (best among context strategies) achieves 58% accuracy and 57% F1
- Statistical tests confirm baseline's superiority over context window approaches

## Why This Works (Mechanism)
None

## Foundational Learning
- **DistilBERT**: A distilled version of BERT that retains 97% of BERT's performance while being 40% smaller and 60% faster, needed for efficient training on discourse relation classification tasks; quick check: verify model parameters are approximately 66M
- **Penn Discourse TreeBank (PDTB)**: A corpus annotated with discourse relations between text spans, providing the dataset for implicit discourse relation classification; quick check: confirm PDTB contains approximately 18,459 implicit discourse relations
- **Implicit Discourse Relations**: Relationships between text spans that are not explicitly marked by connectives, requiring models to infer the relationship; quick check: verify there are 4 major and 16 total relation types in PDTB
- **Context Windows**: Sequences of sentences surrounding target discourse units, used to provide additional contextual information for classification; quick check: confirm window sizes tested (1 sentence, 2 sentences, random sampling)
- **Fine-tuning**: Process of adapting pre-trained language models to specific downstream tasks through additional training; quick check: verify learning rate and epochs used for fine-tuning
- **F1-Score**: Harmonic mean of precision and recall, used as evaluation metric for classification tasks; quick check: confirm macro-F1 is used for multi-class classification

## Architecture Onboarding

Component map:
Pre-trained DistilBERT -> Context Concatenation Layer -> Classification Head -> Loss Function

Critical path:
Input sentences → DistilBERT encoding → Context concatenation → [CLS] token pooling → MLP classifier → Output probabilities

Design tradeoffs:
- Model choice: DistilBERT selected for efficiency over larger BERT variants
- Context integration: Simple concatenation rather than more sophisticated fusion methods
- Window selection: Fixed-size windows vs. adaptive context based on content

Failure signatures:
- Performance degradation when adding context
- Inability to leverage additional contextual information
- Potential overfitting to specific context patterns

First experiments:
1. Test baseline performance with varying random seeds to establish stability
2. Compare performance across different relation types to identify if degradation is uniform
3. Visualize attention weights to understand how context affects model focus

## Open Questions the Paper Calls Out
None

## Limitations
- Only three specific context window strategies were tested, leaving open the possibility that different contextualization methods could yield different results
- Experiments use only DistilBERT as the base model and PDTB as the dataset, limiting generalizability to other models or discourse corpora
- Does not explore fine-tuning strategies that might better leverage contextual information, such as joint training on context-aware and context-agnostic representations

## Confidence

High confidence in the empirical observation that, for the specific experimental setup used (DistilBERT + PDTB + the three context strategies tested), adding contextual sentences does not improve and actually degrades performance. The statistical tests support this conclusion within the tested framework.

Medium confidence in the interpretation that "context adds noise." While the data supports this for the tested strategies, the conclusion may be premature given the limited exploration of context integration methods. The claim that implicit discourse relation classification is "especially difficult" when using contextual neighbors is reasonable but not definitively proven by this study alone.

Low confidence in broad generalizability of the findings to all models, all datasets, or all context selection strategies. The study's narrow methodological scope prevents strong claims about the fundamental nature of the task or the role of context in discourse relation prediction.

## Next Checks

1. **Bidirectional Context Testing**: Replicate the experiments using symmetric context windows that include both preceding and following sentences, rather than the unidirectional or random approaches tested, to determine if context from both directions provides complementary information.

2. **Model Architecture Variation**: Test the same context strategies using larger, more powerful transformer models (e.g., BERT-base, RoBERTa) to assess whether model capacity influences the utility of contextual information for this task.

3. **Context Quality Filtering**: Implement context selection methods that filter neighbors based on semantic or syntactic similarity to the target sentence (e.g., using sentence embeddings or dependency parsing), to determine if carefully chosen context improves rather than harms performance.