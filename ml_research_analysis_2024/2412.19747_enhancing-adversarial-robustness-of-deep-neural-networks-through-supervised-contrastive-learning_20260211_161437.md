---
ver: rpa2
title: Enhancing Adversarial Robustness of Deep Neural Networks Through Supervised
  Contrastive Learning
arxiv_id: '2412.19747'
source_url: https://arxiv.org/abs/2412.19747
tags:
- loss
- contrastive
- learning
- adversarial
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of convolutional neural
  networks to adversarial attacks, which exploit fragile feature representations learned
  by CNN kernels. The authors propose a framework combining supervised contrastive
  learning with margin-based contrastive loss to improve adversarial robustness.
---

# Enhancing Adversarial Robustness of Deep Neural Networks Through Supervised Contrastive Learning

## Quick Facts
- arXiv ID: 2412.19747
- Source URL: https://arxiv.org/abs/2412.19747
- Reference count: 39
- Primary result: Margin-based contrastive loss improves adversarial accuracy by up to 2.5 percentage points under FGSM attacks on CIFAR-100

## Executive Summary
This paper addresses the vulnerability of deep neural networks to adversarial attacks by proposing a supervised contrastive learning framework that clusters embeddings of samples from the same class while separating those from different classes. The method enhances adversarial robustness without compromising clean data performance, achieving improved accuracy under FGSM attacks compared to baseline models. Experiments with ResNet-18 on CIFAR-100 demonstrate the effectiveness of combining supervised contrastive learning with margin-based contrastive loss for creating robust decision boundaries.

## Method Summary
The approach combines supervised contrastive learning with margin-based contrastive loss to improve adversarial robustness. The method uses a ResNet-18 backbone with a projection head to produce 64-dimensional embeddings. Training occurs in two stages: first training a baseline model using cross-entropy loss, then refining it with supervised contrastive learning combined with cross-entropy. The margin-based contrastive loss further enhances robustness by enforcing specific margins between positive and negative pairs. The framework clusters same-class embeddings while separating different-class embeddings, creating more robust decision boundaries that resist adversarial perturbations.

## Key Results
- Margin-based contrastive loss model outperforms baseline by up to 2.5 percentage points at higher perturbation levels (ϵ=0.05)
- Supervised contrastive learning maintains clean data accuracy while improving adversarial robustness
- Framework demonstrates effectiveness on CIFAR-100 dataset with ResNet-18 backbone under FGSM attacks

## Why This Works (Mechanism)
The method works by learning robust feature representations through supervised contrastive learning, which explicitly clusters same-class samples and separates different-class samples in the embedding space. This creates decision boundaries that are less sensitive to small adversarial perturbations. The margin-based contrastive loss further strengthens this by enforcing minimum distances between different classes while maintaining cohesion within classes, making it harder for adversarial examples to cross decision boundaries.

## Foundational Learning
- **Contrastive learning**: A self-supervised learning technique that learns representations by comparing similar and dissimilar samples. Needed to create robust feature embeddings that generalize better to adversarial examples. Quick check: Verify that positive pairs are from same class and negative pairs from different classes.
- **Adversarial attacks (FGSM)**: Fast Gradient Sign Method generates adversarial examples by adding small perturbations in the direction of the gradient. Needed as evaluation metric to test model robustness. Quick check: Confirm perturbation magnitude ϵ is correctly applied.
- **Margin-based loss functions**: Loss functions that enforce minimum distances between classes in embedding space. Needed to create well-separated decision boundaries that resist adversarial perturbations. Quick check: Verify margin values mp and mn are properly enforced in loss computation.

## Architecture Onboarding

**Component Map**: Input -> ResNet-18 Backbone -> Projection Head -> 64D Embeddings -> Supervised Contrastive Loss + Cross-Entropy Loss

**Critical Path**: The critical path is the forward pass through the ResNet-18 backbone, projection head, and embedding layer, followed by the combined loss computation. This path determines how features are extracted and how the model learns to cluster same-class samples while separating different-class samples.

**Design Tradeoffs**: The main tradeoff is between computational cost and robustness. Adding the projection head and contrastive learning increases training time but provides better robustness without requiring expensive adversarial training. The choice of embedding dimension (64D) balances representation capacity with computational efficiency.

**Failure Signatures**: If the refinement stage degrades performance, it indicates misalignment between contrastive and cross-entropy objectives. If adversarial accuracy doesn't improve, the margin values may be too restrictive or too loose, failing to properly separate classes in embedding space.

**First Experiments**:
1. Train baseline ResNet-18 on CIFAR-100 for 10 epochs with cross-entropy loss to establish performance baseline
2. Add projection head and retrain with combined supervised contrastive + cross-entropy loss for 10 epochs
3. Implement margin-based contrastive loss and retrain for final 10 epochs to evaluate robustness improvements

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the proposed supervised contrastive learning framework perform against stronger adversarial attacks like PGD or CW attacks, beyond the FGSM attacks evaluated in the paper?
- Basis in paper: The paper only evaluates robustness under FGSM attacks and suggests that the framework could be tested against more advanced attack strategies.
- Why unresolved: The current experimental scope is limited to a single-step attack (FGSM), which may not fully capture the robustness of the model against more sophisticated iterative or optimization-based attacks.
- What evidence would resolve it: Additional experiments testing the model's accuracy under PGD, CW, or other stronger adversarial attacks would clarify its robustness across a broader range of threat models.

### Open Question 2
- Question: Can the supervised contrastive learning framework be effectively integrated with adversarial training to achieve synergistic improvements in robustness?
- Basis in paper: The paper discusses the computational expense of adversarial training and suggests that supervised contrastive learning offers a scalable alternative, but does not explore combining the two approaches.
- Why unresolved: The potential complementarity between supervised contrastive learning and adversarial training is not investigated, leaving open the question of whether their combination could yield superior results.
- What evidence would resolve it: Experiments comparing the performance of supervised contrastive learning alone, adversarial training alone, and their combination would determine if synergistic effects exist.

### Open Question 3
- Question: How do the hyperparameters of the supervised contrastive learning framework (e.g., temperature τ, margin values mp and mn) impact the trade-off between robustness and clean data accuracy?
- Basis in paper: The paper mentions hyperparameter tuning for margins and temperature but does not provide a detailed analysis of their impact on the robustness-accuracy trade-off.
- Why unresolved: The sensitivity of the framework's performance to these hyperparameters is not fully explored, leaving uncertainty about optimal configurations for different datasets or attack scenarios.
- What evidence would resolve it: Systematic ablation studies or sensitivity analyses varying τ, mp, and mn would clarify their influence on robustness and accuracy, guiding better hyperparameter selection.

## Limitations
- Limited evaluation to FGSM attacks only, without testing against stronger adversarial attacks like PGD or CW
- Missing implementation details for critical components (projection head architecture, margin-based contrastive loss implementation)
- No ablation studies to isolate contributions of individual components to the reported improvements

## Confidence
- Core claim (2.5 percentage point improvement): Medium
- Methodology validity: Medium
- Reproducibility: Low (missing critical implementation details)

## Next Checks
1. Implement and compare multiple projection head architectures (varying embedding dimensions and layer configurations) to determine optimal design for contrastive learning on CIFAR-100
2. Conduct ablation studies to isolate the contribution of supervised contrastive loss versus margin-based refinement to the overall adversarial accuracy improvement
3. Extend evaluation to include PGD attacks and other adversarial attack methods beyond FGSM to assess robustness across different threat models