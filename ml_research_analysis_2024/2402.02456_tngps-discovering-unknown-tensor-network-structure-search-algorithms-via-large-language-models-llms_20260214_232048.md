---
ver: rpa2
title: 'tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via
  Large Language Models (LLMs)'
arxiv_id: '2402.02456'
source_url: https://arxiv.org/abs/2402.02456
tags:
- algorithms
- arxiv
- tensor
- tn-ss
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces GPTN-SS, an automated framework for discovering
  tensor network structure search (TN-SS) algorithms using large language models (LLMs).
  TN-SS aims to find optimal tensor network structures for high-dimensional data representation,
  which is challenging due to the curse of dimensionality and local convergence issues
  in existing methods.
---

# tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via Large Language Models (LLMs)

## Quick Facts
- arXiv ID: 2402.02456
- Source URL: https://arxiv.org/abs/2402.02456
- Authors: Junhua Zeng; Chao Li; Zhun Sun; Qibin Zhao; Guoxu Zhou
- Reference count: 21
- Key outcome: GPTN-SS framework automatically discovers novel TN-SS algorithms that outperform state-of-the-art methods on image and model compression tasks

## Executive Summary
This paper introduces GPTN-SS, an automated framework that leverages large language models (LLMs) to discover novel tensor network structure search (TN-SS) algorithms. The system addresses the challenge of finding optimal tensor network structures for high-dimensional data representation by using an evolutionary-like prompting system that iteratively refines algorithms through knowledge exploitation and exploration. Experiments demonstrate that algorithms discovered by GPTN-SS achieve superior fitness scores compared to existing methods, with results showing fitness scores as low as 0.1216 on training images versus 0.1273 for the best baseline.

## Method Summary
GPTN-SS employs a multi-stage evolutionary prompting framework where LLMs generate, refine, and optimize TN-SS algorithms. The system operates through knowledge exploitation (crossover and mutation of existing algorithms) and knowledge exploration (maintaining diverse algorithm clusters). A Russian roulette selection mechanism ensures diversity by probabilistically selecting algorithms for the next generation based on their rank and fitness scores. The framework uses in-context learning with baseline algorithms as examples and iteratively improves upon them through structured prompts.

## Key Results
- Discovered algorithms achieved fitness scores as low as 0.1216 on training images, outperforming the best baseline method at 0.1273
- GPTN-SS algorithms demonstrated better generalization across testing images compared to state-of-the-art methods
- The framework successfully discovered novel TN-SS algorithms without human intervention in algorithm design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPTN-SS leverages LLMs' ability to parse and generate code from algorithmic descriptions, enabling automated discovery of novel TN-SS algorithms
- Mechanism: The system uses an evolutionary-like prompting pipeline where LLMs iteratively refine TN-SS algorithms through crossover (combining successful strategies) and mutation (local improvements)
- Core assumption: LLMs have sufficient embedded knowledge of tensor network algorithms and programming patterns to generate novel, executable algorithms through structured prompting
- Evidence anchors: [abstract] "GPTN-SS leverages an elaborate crafting LLM-based prompting system that operates in an evolutionary-like manner"; [section] "We have developed an evolutionary-like designing system, in which LLM functions as the main operator"

### Mechanism 2
- Claim: The knowledge exploration phase prevents local optima by maintaining diverse algorithm clusters and introducing novel methodologies
- Mechanism: GPTN-SS maintains a pool of algorithms categorized into distinct clusters, each representing unique knowledge in algorithm design, using Quality Diversity methods to ensure exploration of different approaches
- Core assumption: LLMs can meaningfully categorize algorithms into clusters based on their design methodology and generate truly novel approaches when prompted
- Evidence anchors: [abstract] "knowledge exploration (maintaining diverse algorithm clusters)"; [section] "we maintain a pool of algorithms categorized into distinct clusters, each representing unique knowledge in algorithm design"

### Mechanism 3
- Claim: Roulette selection strategy maintains diversity in algorithm generation by preventing elite algorithms from dominating the search process
- Mechanism: The system uses a non-uniform sampling process where algorithms are selected based on a probability that decreases logarithmically with rank, ensuring both high-performing and diverse algorithms contribute to new generations
- Core assumption: The logarithmic probability function effectively balances exploration of new approaches with exploitation of known good strategies
- Evidence anchors: [section] "we simulate the Russian roulette process. The intuition is that we want to maintain the diversity of the algorithm samples to prevent premature convergence"; [section] "Given a set of individuals, let rank represent their ranking based on the fitness scores, the Russian roulette selection process performs the non-uniformly sampling"

## Foundational Learning

- Concept: Tensor Network Structure Search (TN-SS) fundamentals
  - Why needed here: The entire system is built around discovering better TN-SS algorithms, so understanding the problem formulation and existing approaches is essential
  - Quick check question: What is the objective function being optimized in TN-SS and how does it balance representation accuracy with model complexity?

- Concept: Large Language Model prompting techniques
  - Why needed here: The system relies on carefully crafted prompts to guide LLM behavior, including in-context learning and iterative refinement
  - Quick check question: How does the crossover prompt differ from the mutation prompt in terms of expected LLM output?

- Concept: Evolutionary algorithms and Quality Diversity methods
  - Why needed here: The system uses evolutionary principles for algorithm generation and QD methods for maintaining diversity
  - Quick check question: What is the purpose of maintaining multiple algorithm clusters rather than just evolving a single population?

## Architecture Onboarding

- Component map: LLM prompt generator → Algorithm evaluator → Cluster manager → Selection mechanism → Output filter
- Critical path: Prompt generation → LLM response → Algorithm compilation → Fitness evaluation → Cluster assignment → Next generation selection
- Design tradeoffs: Computational cost of evaluating many algorithms vs. quality of discovered solutions; diversity maintenance vs. convergence speed
- Failure signatures: LLM generating non-compilable code; algorithms converging to local optima; clusters becoming homogeneous; evaluation becoming too slow
- First 3 experiments:
  1. Test basic LLM prompt generation with simple TN-SS algorithms to verify code compilation
  2. Run GPTN-SS with a small number of iterations on a single training image to validate the full pipeline
  3. Compare discovered algorithms against baseline methods on a simple benchmark task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPTN-SS algorithms scale with the size and complexity of the tensor network structures being searched?
- Basis in paper: [inferred] The paper mentions that GPTN-SS can handle "higher orders and higher TN-ranks" but doesn't provide systematic scaling analysis
- Why unresolved: The paper only tests on relatively small tensor networks (order-8 tensors) and doesn't investigate how algorithm performance degrades or improves with increasing dimensionality
- What evidence would resolve it: Systematic experiments varying tensor order, mode dimensions, and rank constraints while measuring both algorithm discovery time and final solution quality

### Open Question 2
- Question: What is the relationship between LLM model size/capabilities and the quality of discovered TN-SS algorithms?
- Basis in paper: [explicit] The authors note they used "gpt-4-1106-preview" but acknowledge "the final discovered algorithms' performance is subject to variation due to changes in LLMs"
- Why unresolved: The paper only uses one LLM model and doesn't explore how different model sizes or capabilities affect algorithm discovery performance
- What evidence would resolve it: Comparative experiments using different LLM models (varying sizes/capabilities) to discover TN-SS algorithms, measuring both discovery success rate and final algorithm quality

### Open Question 3
- Question: How does the knowledge exploration component contribute to avoiding local optima compared to pure exploitation approaches?
- Basis in paper: [explicit] The authors state knowledge exploration is "crucial for GPTN-SS to circumvent local optima" but don't provide ablation studies
- Why unresolved: The paper doesn't compare GPTN-SS performance with variants that remove the knowledge exploration component to quantify its impact
- What evidence would resolve it: Ablation studies comparing GPTN-SS performance with and without knowledge exploration, measuring both solution quality and diversity of discovered algorithms

## Limitations
- The LLM model specification is incomplete beyond the basic version and temperature settings, making exact reproduction difficult
- Key implementation details for the mutation and cluster creation prompts are not fully specified
- The fitness evaluation function (Eval(·)) is described but not detailed enough for precise replication

## Confidence
- **High confidence**: The core concept of using LLMs for algorithm discovery through evolutionary prompting is sound and well-explained
- **Medium confidence**: The experimental results showing improved performance over baseline methods, though exact reproducibility is uncertain
- **Low confidence**: The specific implementation details required for faithful reproduction of the results

## Next Checks
1. **Prompt Engineering Validation**: Test the basic prompting framework with simple TN-SS algorithms to verify code compilation and basic functionality before attempting full-scale experiments
2. **Diversity Maintenance Verification**: Monitor algorithm diversity metrics during the knowledge exploration phase to ensure Russian roulette selection parameters (α1, α2) effectively prevent premature convergence
3. **Performance Benchmark Replication**: Compare discovered algorithms against baseline methods (TNGA, TNLS, Greedy) on a simple benchmark task with a small dataset to validate the full pipeline before scaling up