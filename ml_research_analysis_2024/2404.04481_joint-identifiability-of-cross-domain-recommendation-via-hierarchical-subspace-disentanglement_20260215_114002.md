---
ver: rpa2
title: Joint Identifiability of Cross-Domain Recommendation via Hierarchical Subspace
  Disentanglement
arxiv_id: '2404.04481'
source_url: https://arxiv.org/abs/2404.04481
tags:
- user
- ndcg
- cross-domain
- game
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HJID, a hierarchical generative approach for
  cross-domain recommendation that ensures joint identifiability of user representations
  across domains. The method disentangles user representations into shallow general
  features and deep domain-oriented features, aligning the former across domains using
  Maximum Mean Discrepancy and modeling the latter as domain-shared and domain-specific
  factors through invertible normalizing flows.
---

# Joint Identifiability of Cross-Domain Recommendation via Hierarchical Subspace Disentanglement

## Quick Facts
- arXiv ID: 2404.04481
- Source URL: https://arxiv.org/abs/2404.04481
- Reference count: 40
- HJID achieves up to 21.45% improvement in NDCG@10 for strongly correlated domains and 16.05% for weakly correlated domains.

## Executive Summary
This paper introduces HJID, a hierarchical generative approach for cross-domain recommendation that ensures joint identifiability of user representations across domains. The method disentangles user representations into shallow general features and deep domain-oriented features, aligning the former across domains using Maximum Mean Discrepancy and modeling the latter as domain-shared and domain-specific factors through invertible normalizing flows. By establishing causal relationships in the data generation graph and ensuring identifiability of the joint distribution, HJID captures both behavioral invariance and variance under domain shifts.

## Method Summary
HJID employs a hierarchical neural architecture to encode user representations. Shallow layers capture generic, cross-domain consistent features aligned via MMD, while deep layers focus on domain-oriented features. The deep features are further decomposed into stable (domain-shared) and variant (domain-specific) components using a causal data generation graph and invertible normalizing flows. This ensures unique parameter recovery across domains and enables knowledge transfer even in non-overlapped CDR scenarios.

## Key Results
- HJID outperforms state-of-the-art methods, achieving up to 21.45% improvement in NDCG@10 for strongly correlated domains and 16.05% for weakly correlated domains.
- The approach demonstrates robustness in non-overlapped CDR scenarios without requiring exact user overlap.
- Experimental results on real-world Amazon datasets show HJID's effectiveness across various domain pairs with different correlation strengths.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HJID ensures joint identifiability of cross-domain user representations by disentangling them into stable and variant latent factors.
- Mechanism: The model uses a hierarchical neural architecture to encode shallow layers for generic, cross-domain consistent features (aligned via MMD), and deep layers for domain-specific features. The deep features are further decomposed into stable (domain-shared) and variant (domain-specific) components using a causal data generation graph and invertible normalizing flows, ensuring unique parameter recovery across domains.
- Core assumption: User preferences can be decomposed into a stable core (invariant across domains) and a variant part (domain-specific), and this decomposition is uniquely recoverable given the observed cross-domain interactions.
- Evidence anchors: [abstract], [section 3.3.1], [corpus]
- Break condition: If the stable and variant factors are not truly separable (i.e., the causal graph is misspecified), the joint identifiability guarantee fails and the model may collapse into suboptimal representations.

### Mechanism 2
- Claim: Feature Hierarchy (FH) principle enables separation of general from domain-specific patterns in user representations.
- Mechanism: By assigning shallow neural layers to encode general, domain-agnostic features and deeper layers to capture domain-specific nuances, the model naturally decouples transferable from non-transferable information. This separation allows domain-invariant alignment at shallow levels and focused modeling of shifts at deep levels.
- Core assumption: Early neural network layers learn general features while later layers specialize to domain-specific patterns; this holds across the recommendation domains considered.
- Evidence anchors: [abstract], [section 3.1.2], [corpus]
- Break condition: If the domain gap is so large that even shallow features differ substantially, the FH assumption breaks and shallow-layer alignment via MMD becomes ineffective.

### Mechanism 3
- Claim: Minimal Change (MC) principle ensures that only the necessary variant factors are adjusted during cross-domain transfer, preserving stability.
- Mechanism: By attributing domain shifts to the variant latent component 洧누 洧녺 and keeping 洧누洧 fixed across domains, the model adheres to the MC principle: changes are minimal and only where needed. The invertible flow ensures that transformations between 洧누 洧녺 and its target-domain counterpart are exact and recoverable.
- Core assumption: Domain shifts can be modeled as invertible transformations of a low-dimensional variant factor, without altering the stable component.
- Evidence anchors: [abstract], [section 3.3.1], [corpus]
- Break condition: If the domain shift is not invertible or requires simultaneous changes in stable and variant parts, the MC principle and flow-based transformation fail.

## Foundational Learning

- Concept: Maximum Mean Discrepancy (MMD) for domain alignment
  - Why needed here: To enforce that shallow-layer features are domain-agnostic by minimizing distributional divergence between source and target domains.
  - Quick check question: If two domains have identical shallow representations, what should their MMD score be?

- Concept: Normalizing Flows and invertibility
  - Why needed here: To model the transformation between domain-specific factors in a way that guarantees unique recovery (joint identifiability).
  - Quick check question: What property of normalizing flows ensures that the transformation from 洧누 洧녺 to 틮洧누 洧녺 is bijective?

- Concept: Variational Information Bottleneck (VIB)
  - Why needed here: To constrain the learned representations to retain only information relevant to the prediction task, reducing redundancy and improving generalization.
  - Quick check question: In the context of HJID, which parts of the user representation are being constrained by VIB?

## Architecture Onboarding

- Component map: VBGE (Graph Neural Networks) -> Shallow Subspace (MMD-aligned) -> Deep Subspace (stable 洧누洧 + variant 洧누 洧녺) -> Causal Data Generation Graph -> Normalizing Flow -> VIB loss -> Prediction heads

- Critical path: Shallow alignment (MMD) -> Deep disentanglement (causal graph + flow) -> VIB-constrained prediction -> Optimization

- Design tradeoffs:
  - Shallow vs deep alignment depth: Too shallow may leave domain-specific noise; too deep may over-align and lose useful domain cues.
  - Flow model complexity: More layers improve expressiveness but risk overfitting and computational cost.
  - Group size N: Small groups allow finer-grained optimization; large groups smooth estimation but may miss subtle patterns.

- Failure signatures:
  - MMD loss plateaus but performance doesn't improve -> Shallow features not truly domain-agnostic.
  - Flow Jacobian determinants become unstable -> Ill-conditioned transformations, possibly due to poor initialization or model misspecification.
  - VIB loss dominates -> Model overly compresses and loses predictive signal.

- First 3 experiments:
  1. Validate that shallow-layer MMD decreases during training while deep-layer disentanglement remains stable.
  2. Test whether removing the normalizing flow (Variant C) causes joint identifiability to break, as evidenced by degraded performance on weakly correlated domains.
  3. Compare performance with and without VIB to confirm that constrained representations improve robustness under non-overlap scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the joint identifiability of user representations impact the robustness of cross-domain recommendation under distribution shifts?
- Basis in paper: [explicit] The paper emphasizes that existing methods focus on marginal identifiability and overlook the joint identifiability of user representations across domains, leading to negative transfer in weakly correlated domains.
- Why unresolved: While the paper demonstrates improved performance, it does not quantify the extent to which joint identifiability contributes to robustness under various types of distribution shifts.
- What evidence would resolve it: Empirical studies comparing HJID's performance under different types of distribution shifts (e.g., domain shift magnitude, user behavior change) with and without joint identifiability would clarify its impact on robustness.

### Open Question 2
- Question: How does the choice of invertible normalizing flow model affect the disentanglement of domain-specific factors in cross-domain recommendation?
- Basis in paper: [explicit] The paper explores different invertible normalizing flow models (MAF, NAF, NODE, NCSF) and observes varying performance across tasks, but does not provide a comprehensive analysis of their impact on disentanglement quality.
- Why unresolved: The paper does not investigate how the choice of flow model influences the ability to capture complex correlations between domain-specific factors and domain-shared factors.
- What evidence would resolve it: A detailed analysis of the learned transformations by different flow models and their ability to disentangle domain-specific factors, along with qualitative evaluation of the disentangled representations, would shed light on this question.

### Open Question 3
- Question: How does the depth of shallow layers (k) in the hierarchical subspace disentanglement affect the alignment of domain-irrelevant features?
- Basis in paper: [explicit] The paper experiments with different values of k and observes varying performance across CDR tasks, suggesting that the optimal depth depends on the specific task.
- Why unresolved: The paper does not provide a theoretical explanation for why different values of k work better for different tasks or how to determine the optimal depth without extensive experimentation.
- What evidence would resolve it: A theoretical analysis of the relationship between the depth of shallow layers and the alignment of domain-irrelevant features, along with empirical studies correlating task characteristics (e.g., domain correlation strength, data sparsity) with the optimal value of k, would provide insights into this question.

## Limitations
- The claim of joint identifiability relies on strong assumptions about the separability of stable and variant factors, which are not empirically validated in the paper.
- The causal data generation graph used to define this separation is hand-crafted rather than learned, raising questions about its generalizability.
- The method's performance on truly non-overlapping CDR scenarios is demonstrated but the mechanism by which joint identifiability enables this transfer is not rigorously proven.

## Confidence

- **High confidence**: The experimental results showing HJID outperforming baselines (up to 21.45% NDCG@10 improvement) are well-supported by the methodology described.
- **Medium confidence**: The hierarchical architecture and use of MMD for shallow alignment is well-grounded in the literature and methodology.
- **Low confidence**: The theoretical guarantees of joint identifiability and the causal disentanglement mechanism are asserted but not rigorously proven or extensively validated.

## Next Checks
1. Conduct ablation studies removing the causal graph specification to test whether performance degrades, indicating reliance on correct factor separation.
2. Test HJID on datasets with known ground-truth stable/variant factor structure to empirically verify the joint identifiability claims.
3. Evaluate the sensitivity of HJID to different flow architectures and depth configurations to assess the robustness of the disentanglement mechanism.