---
ver: rpa2
title: Improving Neural Optimal Transport via Displacement Interpolation
arxiv_id: '2410.03783'
source_url: https://arxiv.org/abs/2410.03783
tags:
- transport
- optimal
- diotm
- displacement
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Displacement Interpolation Optimal Transport
  Model (DIOTM), a novel approach for learning optimal transport maps using neural
  networks. The method leverages displacement interpolation to improve training stability
  and accuracy in estimating optimal transport maps.
---

# Improving Neural Optimal Transport via Displacement Interpolation

## Quick Facts
- **arXiv ID**: 2410.03783
- **Source URL**: https://arxiv.org/abs/2410.03783
- **Reference count**: 40
- **Primary result**: DIOTM achieves stable convergence and superior accuracy in neural optimal transport with competitive FID scores on image-to-image translation

## Executive Summary
This paper introduces the Displacement Interpolation Optimal Transport Model (DIOTM), a novel neural network approach for learning optimal transport maps that leverages displacement interpolation to improve training stability and accuracy. The method establishes a theoretical connection between static optimal transport maps and displacement interpolation, deriving a dual formulation that utilizes the entire trajectory of the interpolation process. A key innovation is the incorporation of an HJB regularizer derived from the Hamilton-Jacobi-Bellman equation, which enforces the optimality condition of the potential function. Experimental results demonstrate that DIOTM achieves more stable convergence and superior accuracy compared to existing neural OT methods, with competitive performance on image-to-image translation tasks.

## Method Summary
DIOTM combines displacement interpolation with neural network learning to estimate optimal transport maps. The method leverages the theoretical connection between static optimal transport and the time-dependent dynamic formulation, using displacement interpolation to approximate the optimal trajectory between source and target distributions. The model introduces an HJB regularizer derived from the optimality conditions of the potential function, which enforces the relationship between the transport map and its corresponding potential. During training, DIOTM optimizes both the transport map and the potential function simultaneously, using the entire displacement interpolation trajectory rather than just endpoint samples. This approach addresses common training instabilities in existing neural OT methods by providing a more robust optimization landscape.

## Key Results
- DIOTM achieves stable convergence and superior accuracy in approximating optimal transport maps compared to existing neural OT methods
- The method demonstrates competitive performance on image-to-image translation with FID scores of 5.27 (Male→Female, 64×64), 7.40 (Male→Female, 128×128), and 10.72 (Wild→Cat, 64×64)
- The HJB regularizer effectively enforces the optimality condition of the potential function, improving the quality of learned transport maps

## Why This Works (Mechanism)
DIOTM works by leveraging the full trajectory information from displacement interpolation rather than just endpoint samples, which provides richer supervision during training. The HJB regularizer enforces the fundamental optimality condition that the gradient of the potential function equals the transport map, creating a consistency constraint that stabilizes training. By incorporating the time-dependent formulation through displacement interpolation, the method avoids the numerical instabilities that plague static formulations while maintaining computational efficiency through neural network parameterization.

## Foundational Learning
- **Optimal Transport Theory**: Understanding the mathematical framework for comparing probability distributions is essential for grasping the problem DIOTM addresses
  - *Why needed*: Provides the theoretical foundation for measuring distributional differences and defining optimal maps
  - *Quick check*: Can you explain the Kantorovich duality and its relation to the Monge formulation?

- **Displacement Interpolation**: The geodesic path in Wasserstein space connecting two distributions
  - *Why needed*: Forms the basis for leveraging temporal information in the optimization process
  - *Quick check*: How does displacement interpolation relate to the continuity equation in fluid dynamics?

- **Hamilton-Jacobi-Bellman Equation**: A fundamental equation in optimal control theory relating value functions to system dynamics
  - *Why needed*: Provides the theoretical justification for the regularizer that enforces optimality conditions
  - *Quick check*: What is the connection between HJB equations and the potential function in optimal transport?

- **Neural Network Parameterization of Maps**: Using deep learning to approximate complex, high-dimensional transport maps
  - *Why needed*: Enables practical application to real-world data with complex distributions
  - *Quick check*: What are the advantages and limitations of using neural networks versus traditional OT solvers?

## Architecture Onboarding

**Component Map**: Input distribution -> Neural transport map -> Displacement interpolation trajectory -> HJB regularizer -> Output distribution

**Critical Path**: The most critical computational path involves computing the displacement interpolation trajectory from the current transport map estimate, evaluating the HJB regularizer, and backpropagating through both to update the neural network parameters. This path ensures that the learned map satisfies both the endpoint matching condition and the optimality condition throughout the interpolation.

**Design Tradeoffs**: DIOTM trades computational complexity for stability and accuracy. The use of displacement interpolation and HJB regularization increases training time compared to simpler neural OT methods, but provides more robust convergence and higher-quality transport maps. The method also requires careful tuning of the regularization strength to balance the competing objectives of endpoint accuracy and trajectory consistency.

**Failure Signatures**: Potential failure modes include gradient vanishing/exploding during displacement interpolation computation, poor convergence when the HJB regularization weight is mis-specified, and difficulty scaling to extremely high-dimensional data where the interpolation trajectory becomes numerically unstable. The method may also struggle when source and target distributions have disjoint supports or when the transport map requires discontinuous transformations.

**First Experiments**:
1. Test on simple 2D synthetic datasets (e.g., Gaussian distributions with different means/covariances) to verify basic functionality and compare against analytical solutions
2. Evaluate convergence behavior on standard OT benchmarks (e.g., MNIST digits) with varying regularization strengths to understand hyperparameter sensitivity
3. Perform ablation studies removing the HJB regularizer to quantify its specific contribution to performance improvements

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to high-dimensional data beyond tested image sizes remains unproven
- The computational overhead from displacement interpolation and HJB regularization needs quantification
- Limited comparison against non-flow-based OT methods in the experimental evaluation

## Confidence

- Theoretical framework and HJB-based regularizer: **High**
- Experimental improvements in convergence and accuracy: **High**
- Image-to-image translation results: **Medium** (limited dataset diversity)
- Scalability claims: **Low** (not extensively tested)

## Next Checks

1. Test DIOTM on high-resolution images (>256x256) and non-image data (e.g., point clouds, time series) to assess scalability limits
2. Conduct ablation studies removing the HJB regularizer to quantify its specific contribution to performance gains
3. Benchmark against classical OT solvers and non-flow-based deep OT methods on small-scale problems where ground truth solutions are available