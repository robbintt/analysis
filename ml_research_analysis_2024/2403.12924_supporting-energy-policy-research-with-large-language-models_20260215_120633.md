---
ver: rpa2
title: Supporting Energy Policy Research with Large Language Models
arxiv_id: '2403.12924'
source_url: https://arxiv.org/abs/2403.12924
tags:
- energy
- ordinance
- wind
- documents
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates the use of Large Language Models (LLMs)
  to automate the extraction of wind and solar siting ordinances from legal documents.
  The authors introduce a decision tree framework to guide LLMs in retrieving accurate,
  machine-readable data from lengthy zoning codes.
---

# Supporting Energy Policy Research with Large Language Models
## Quick Facts
- arXiv ID: 2403.12924
- Source URL: https://arxiv.org/abs/2403.12924
- Reference count: 0
- Demonstrates 85-90% accuracy in extracting wind and solar siting ordinances from legal documents using LLMs guided by decision trees.

## Executive Summary
This paper introduces a novel method for automating the extraction of wind and solar siting ordinances from lengthy legal documents using Large Language Models (LLMs). The authors address the challenge of manually maintaining up-to-date energy policy databases by developing a decision tree framework that guides LLMs to retrieve accurate, machine-readable data. Applied to a test set of 85 ordinance documents, the approach achieved 85-90% accuracy and successfully extracted structured setback data for quantitative analysis. This method significantly reduces manual labor and enables large-scale policy research in the energy sector.

## Method Summary
The method combines PDF text extraction, semantic search-based text distillation, and a decision tree framework to guide LLM prompts in extracting structured setback data from zoning ordinances. Documents are first converted to text and distilled to relevant sections using semantic search. The decision tree then breaks down the extraction task into smaller, focused prompts, improving accuracy and reducing hallucination. Outputs are formatted as structured, machine-readable data for downstream quantitative modeling.

## Key Results
- Achieved 85-90% accuracy in extracting setback data from 85 ordinance documents.
- Successfully extracted structured setback values and types for wind energy systems.
- Demonstrated the feasibility of using LLMs to maintain up-to-date energy policy databases with minimal manual intervention.

## Why This Works (Mechanism)

### Mechanism 1
Decision trees improve LLM accuracy by breaking complex multi-step extraction into smaller, focused prompts. The decision tree splits the extraction task into nodes, each containing a narrow prompt and conditional transitions. This prevents the LLM from being overwhelmed by long documents and ambiguous language.

### Mechanism 2
Pre-filtering text with semantic search reduces context window overload and improves LLM focus. The method first uses asynchronous semantic search on overlapping text chunks to isolate relevant sections, then feeds only that distilled text to the LLM.

### Mechanism 3
Structured output formatting (numerical value + categorical definition) enables downstream quantitative modeling. Each decision tree leaf returns data in a machine-readable format, pairing setback values with their calculation method.

## Foundational Learning

- **Concept: Legal document structure and terminology**
  - Why needed here: Understanding zoning ordinance formats is critical for designing effective prompts and decision tree logic.
  - Quick check question: What are the common setback types mentioned in wind energy ordinances (e.g., property lines, structures, noise)?

- **Concept: Context window limitations in LLMs**
  - Why needed here: Large legal documents exceed token limits, so distillation strategies are required.
  - Quick check question: Why is it problematic to feed a 100-page document directly into GPT-4?

- **Concept: Semantic search and text chunking**
  - Why needed here: Extracting relevant text before LLM processing improves accuracy.
  - Quick check question: How does overlapping chunking help avoid missing boundary-relevant text?

## Architecture Onboarding

- **Component map**: PDF → Poppler text extraction → Semantic search chunking → Decision tree with LLM nodes → Structured output
- **Critical path**: Document ingestion → Text distillation → Decision tree traversal → Output formatting
- **Design tradeoffs**: Multi-prompt accuracy vs. single-prompt speed; manual review vs. fully automated extraction
- **Failure signatures**: False positives (hallucinations), false negatives (missed ordinances), misclassification of setback types
- **First 3 experiments**:
  1. Test single-prompt extraction on a small ordinance sample to benchmark baseline accuracy.
  2. Implement semantic search chunking and compare recall vs. full-document input.
  3. Build a minimal decision tree with 2-3 nodes and validate structured output formatting.

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal decision tree depth and node complexity for balancing accuracy and computational efficiency in LLM-guided ordinance extraction?
- Basis in paper: [inferred] The paper mentions using decision trees but does not systematically explore optimal tree depth or complexity.
- Why unresolved: The authors implemented a decision tree approach but did not conduct ablation studies to determine the impact of tree depth and complexity on performance.
- What evidence would resolve it: Systematic testing of decision trees with varying depths and complexities on the same ordinance dataset, comparing accuracy, processing time, and resource utilization.

### Open Question 2
How do different LLM architectures (e.g., GPT-3.5 vs GPT-4) and context window sizes affect the accuracy and efficiency of ordinance extraction?
- Basis in paper: [explicit] The authors mention using GPT-4 but do not compare its performance to other LLM architectures or context window sizes.
- Why unresolved: The study focuses on a single LLM (GPT-4) and does not explore how different LLM architectures or context window sizes might impact performance.
- What evidence would resolve it: Comparative analysis of multiple LLM architectures and context window sizes on the same ordinance extraction task, measuring accuracy, processing time, and resource utilization.

### Open Question 3
What are the long-term maintenance requirements and potential degradation of LLM performance in ordinance extraction over time?
- Basis in paper: [inferred] The paper discusses using LLMs for maintaining an up-to-date ordinance database but does not address long-term performance or maintenance.
- Why unresolved: The study presents a snapshot of LLM performance but does not explore how the system might perform or require updates over extended periods.
- What evidence would resolve it: Longitudinal study tracking LLM performance on ordinance extraction tasks over time, including retraining frequency, drift detection, and maintenance procedures.

## Limitations
- The decision tree framework's effectiveness depends heavily on the quality of prompts and coverage of ordinance types, with no guarantee of generalization to new jurisdictions.
- The 85-90% accuracy claim is based on a limited test set of 85 documents, and performance may degrade with more diverse or complex ordinances.
- The structured output format assumes consistent setback definitions across documents, which may not hold true in practice.

## Confidence
- **High** confidence in the problem statement and the general utility of LLMs for policy document analysis.
- **Medium** confidence in the core claims due to the lack of open-source code and detailed prompt structures, limiting reproducibility.

## Next Checks
1. Test the decision tree framework on a new set of ordinances from different jurisdictions to assess generalizability.
2. Conduct an ablation study to quantify the impact of semantic search and text distillation on accuracy.
3. Evaluate the system's robustness to document quality variations, such as OCR errors or inconsistent formatting.