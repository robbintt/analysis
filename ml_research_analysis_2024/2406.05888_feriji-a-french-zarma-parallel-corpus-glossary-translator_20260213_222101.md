---
ver: rpa2
title: 'Feriji: A French-Zarma Parallel Corpus, Glossary & Translator'
arxiv_id: '2406.05888'
source_url: https://arxiv.org/abs/2406.05888
tags:
- zarma
- feriji
- language
- languages
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of machine translation resources for
  the Zarma language, spoken by over 5 million people in Niger and neighboring countries.
  The authors developed Feriji, a French-Zarma parallel corpus containing 61,085 sentences
  in Zarma and 42,789 in French, along with a glossary of 4,062 words.
---

# Feriji: A French-Zarma Parallel Corpus, Glossary & Translator

## Quick Facts
- arXiv ID: 2406.05888
- Source URL: https://arxiv.org/abs/2406.05888
- Reference count: 6
- The paper develops the first French-Zarma parallel corpus and fine-tunes multilingual models achieving BLEU score of 30.06

## Executive Summary
This paper addresses the severe lack of machine translation resources for the Zarma language, spoken by over 5 million people in Niger and neighboring countries. The authors created Feriji, a French-Zarma parallel corpus containing 61,085 Zarma sentences and 42,789 French sentences, along with a glossary of 4,062 words. They fine-tuned three large multilingual models (MT5-small, M2M100, and NLLB-200-dist) on this dataset, achieving a BLEU score of 30.06 with the M2M100 model. Human evaluation by native Zarma speakers showed that the M2M100 model produced more fluent, comprehensible, and readable translations compared to the other models. The corpus and glossary are made publicly available to support further research and development in Zarma machine translation.

## Method Summary
The authors developed the Feriji dataset by collecting and aligning French-Zarma parallel sentences, then split the data into 80/10/10 training/validation/test sets. They fine-tuned three pre-trained multilingual models (MT5-small, M2M100, and NLLB-200-dist) for French-to-Zarma translation on a P100 GPU in Kaggle environment. Model performance was evaluated using BLEU score on the test set and human evaluation by five native Zarma speakers assessing fluency, comprehension, and readability on a 1-5 scale. The best-performing M2M100 model was integrated into a web-based translator interface.

## Key Results
- Developed first robust French-Zarma parallel corpus (61,085 Zarma sentences, 42,789 French sentences)
- Achieved BLEU score of 30.06 with M2M100 model on French-to-Zarma translation
- Human evaluation showed M2M100 produced more fluent, comprehensible, and readable translations than MT5-small and NLLB-200-dist

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning large multilingual models (M2M100, NLLB-200-dist) on the Feriji corpus produces usable French-Zarma translations.
- Mechanism: Pre-trained multilingual models have learned cross-lingual representations that can be adapted to low-resource language pairs through fine-tuning on parallel data.
- Core assumption: The multilingual model's architecture and pre-training can transfer to the Zarma-French language pair despite limited data.
- Evidence anchors:
  - [abstract] "We fine-tune three large language models on our dataset, obtaining a BLEU score of 30.06 on the best-performing model."
  - [section] "The mean BLEU score was 21.95, with the M2M100 model achieving the highest score of 30.06."
  - [corpus] Weak - the corpus is the only Zarma-French parallel data source, but its coverage and quality are unverified.
- Break condition: If the multilingual model's pre-training lacks relevant linguistic patterns for Songhay languages, transfer learning will fail.

### Mechanism 2
- Claim: Human evaluation by native Zarma speakers provides meaningful quality assessment beyond automatic metrics.
- Mechanism: Native speakers can detect fluency, comprehension, and readability issues that BLEU scores miss.
- Core assumption: Five native speakers can provide representative judgments of translation quality.
- Evidence anchors:
  - [abstract] "We further evaluate the models on human judgments of fluency, comprehension, and readability"
  - [section] "Participants were asked to rate each translation on a scale of 1 to 5 for fluency, comprehension, and readability"
  - [corpus] Weak - no information on inter-annotator agreement or speaker diversity.
- Break condition: If the native speakers lack training in translation evaluation or represent a narrow dialect region, their judgments may not generalize.

### Mechanism 3
- Claim: The Feriji corpus and glossary address the resource scarcity problem for Zarma MT.
- Mechanism: Creating parallel data and lexical resources enables both data-driven and knowledge-based MT approaches.
- Core assumption: The corpus size (61,085 Zarma sentences) is sufficient for meaningful MT model training.
- Evidence anchors:
  - [abstract] "Feriji, the first robust French-Zarma parallel corpus and glossary designed for MT. The corpus, containing 61,085 sentences in Zarma and 42,789 in French"
  - [section] "The Feriji Dataset (FD) is a parallel corpus of French and Zarma sentences designed for machine translation tasks."
  - [corpus] Weak - no comparison to typical corpus sizes needed for MT in other languages.
- Break condition: If the corpus lacks domain diversity or contains translation errors, model performance will suffer.

## Foundational Learning

- Concept: Machine translation evaluation metrics (BLEU, human evaluation)
  - Why needed here: To assess the quality of the fine-tuned models and understand their limitations
  - Quick check question: What does a BLEU score of 30.06 indicate about translation quality for a low-resource language pair?

- Concept: Multilingual pre-training and transfer learning
  - Why needed here: To understand why fine-tuning large models works for Zarma despite limited data
  - Quick check question: How does pre-training on multiple languages help with translating between specific language pairs?

- Concept: Parallel corpus construction and alignment
  - Why needed here: To understand the data quality challenges and solutions in creating Feriji
  - Quick check question: What are the main challenges in aligning French and Zarma sentences for a parallel corpus?

## Architecture Onboarding

- Component map: Data collection pipeline → Corpus cleaning/alignment → Model fine-tuning → Human evaluation → Translator interface
- Critical path: Corpus creation → Model training → Evaluation → Deployment
- Design tradeoffs: Larger models give better performance but require more computational resources; human evaluation is more accurate but slower than automatic metrics
- Failure signatures: Low BLEU scores, poor human evaluation ratings, community feedback about translation quality issues
- First 3 experiments:
  1. Test different train/validation/test splits to optimize model performance
  2. Compare different multilingual models on a small subset of the data
  3. Evaluate the impact of corpus cleaning quality on translation results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic challenges does the Zarma language present for machine translation that are not addressed by current models?
- Basis in paper: [explicit] The paper mentions that African languages, including Zarma, have unique tonal nuances and dialects, and that linguistic complexities are a reason for their underrepresentation in MT systems.
- Why unresolved: The paper does not provide a detailed analysis of the specific linguistic features of Zarma that pose challenges for machine translation, such as its tonal system, word order, or morphological structure.
- What evidence would resolve it: A comprehensive linguistic analysis of Zarma, focusing on features that are particularly challenging for MT, would provide the necessary evidence to address this question.

### Open Question 2
- Question: How can the Feriji corpus be expanded to include more diverse topics and improve the quality of translations?
- Basis in paper: [explicit] The authors mention that future work will involve releasing new dataset versions with higher-quality and more diverse sentences, moving away from single-topic-centric content.
- Why unresolved: The paper does not specify the methods for expanding the corpus or the criteria for selecting diverse topics.
- What evidence would resolve it: A detailed plan for corpus expansion, including topic selection criteria and methods for ensuring high-quality data, would address this question.

### Open Question 3
- Question: What are the long-term impacts of the Feriji Translator on the preservation and promotion of the Zarma language?
- Basis in paper: [explicit] The paper discusses the potential of Feriji to preserve and promote Zarma culture, but does not provide evidence of its long-term impacts.
- Why unresolved: The paper is focused on the development and initial evaluation of Feriji, and does not include a longitudinal study of its effects on the Zarma-speaking community.
- What evidence would resolve it: A longitudinal study tracking the usage of Feriji and its effects on Zarma language use and cultural preservation would provide the necessary evidence to answer this question.

## Limitations
- Modest corpus size (61,085 sentences) may constrain model performance for complex translation tasks
- Limited human evaluation with only five native speakers and no inter-annotator agreement reported
- Lack of detailed information about corpus quality control and speaker dialect diversity

## Confidence

**High Confidence**: The core claim that the Feriji corpus is the first robust French-Zarma parallel corpus is well-supported by the authors' literature review and the absence of competing resources. The reported BLEU scores and human evaluation methodology are clearly described.

**Medium Confidence**: The claim that M2M100 achieves superior performance (BLEU 30.06) is supported by reported metrics, but the relatively small test set and lack of comparison with baseline models for Zarma translation reduces confidence. The human evaluation results are credible but limited by the small number of evaluators (5) and lack of statistical significance testing.

**Low Confidence**: Claims about the translator's real-world utility and the corpus's sufficiency for industrial applications are not substantiated by user studies or deployment metrics. The absence of error analysis or qualitative examples of model failures further reduces confidence in practical applicability.

## Next Checks

1. **Corpus Quality Validation**: Conduct an independent assessment of corpus alignment quality and translation accuracy by having bilingual experts evaluate a random sample of sentence pairs.

2. **Model Robustness Testing**: Evaluate model performance across different text domains (news, conversation, technical) and assess handling of morphologically complex Zarma structures not well-represented in the training data.

3. **Longitudinal Community Evaluation**: Deploy the translator with actual Zarma-speaking users over an extended period and collect feedback on usability, error patterns, and practical value for daily communication needs.