---
ver: rpa2
title: 'Generalization or Memorization: Data Contamination and Trustworthy Evaluation
  for Large Language Models'
arxiv_id: '2402.15938'
source_url: https://arxiv.org/abs/2402.15938
tags:
- data
- contamination
- llms
- training
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the critical issue of data contamination
  in large language models (LLMs), where test data is included in training data, leading
  to overestimation of model performance. The authors propose two approaches: CDD
  (Contamination Detection via output Distribution) to detect data contamination and
  TED (Trustworthy Evaluation via output Distribution) to mitigate its impact on evaluation.'
---

# Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models

## Quick Facts
- arXiv ID: 2402.15938
- Source URL: https://arxiv.org/abs/2402.15938
- Authors: Yihong Dong; Xue Jiang; Huanyu Liu; Zhi Jin; Bin Gu; Mengfei Yang; Ge Li
- Reference count: 21
- Primary result: Proposed methods detect and mitigate data contamination in LLMs, showing significant improvements over baselines.

## Executive Summary
This paper addresses the critical issue of data contamination in large language models (LLMs), where test data is included in training data, leading to overestimation of model performance. The authors propose two approaches: CDD (Contamination Detection via output Distribution) to detect data contamination and TED (Trustworthy Evaluation via output Distribution) to mitigate its impact on evaluation. CDD identifies contamination by analyzing the peakedness of an LLM's output distribution using sampled texts, while TED corrects the output distribution to restore more realistic evaluation results. The authors construct two new datasets, DETCON and COMI EVAL, for data contamination detection and mitigation evaluation. Experimental results show that CDD achieves 21.8%-30.2% relative improvements in accuracy, F1 score, and AUC metrics compared to other approaches and effectively detects both original and variant contamination. TED significantly mitigates performance improvements attributed to data contamination across various settings, with an average reduction of 66.9%. In real-world applications, the authors reveal that ChatGPT exhibits a high potential for data contamination on the HumanEval benchmark, which is effectively mitigated by TED.

## Method Summary
The paper proposes two approaches to address data contamination in LLMs: CDD (Contamination Detection via output Distribution) and TED (Trustworthy Evaluation via output Distribution). CDD detects contamination by analyzing the peakedness of an LLM's output distribution using sampled texts, while TED mitigates contamination by correcting the output distribution and excluding peakedness and duplicates. The authors construct two new datasets, DETCON and COMI EVAL, to evaluate their approaches. Experimental results show that CDD achieves significant improvements in detection accuracy and TED effectively mitigates contamination impact across various settings.

## Key Results
- CDD achieves 21.8%-30.2% relative improvements in accuracy, F1 score, and AUC metrics compared to other approaches
- TED significantly mitigates performance improvements attributed to data contamination, with an average reduction of 66.9%
- ChatGPT exhibits a high potential for data contamination on the HumanEval benchmark, which is effectively mitigated by TED

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data contamination alters LLM output distribution toward peakedness by memorizing training data.
- Mechanism: When LLMs are trained on test data (or its variants), their output distribution shifts to produce more similar or identical outputs for that input, measurable via edit distance distribution.
- Core assumption: Training causes the model to over-generate outputs similar to memorized data, increasing peakedness.
- Evidence anchors:
  - [abstract] "CDD uses the sampled texts to identify the peakedness of LLM's output distribution for data contamination detection."
  - [section 3.2] "We follow a hypothesis that training is likely to alter the model's output distribution, resulting in a more peaked output distribution for training data."
  - [corpus] Weak: related papers discuss contamination detection but not peakedness via output distribution.
- Break condition: If the model has strong generalization without memorization, output distribution remains flat even with training overlap.

### Mechanism 2
- Claim: Contamination detection can be performed using only sampled outputs without access to model internals or training data.
- Mechanism: CDD computes the cumulative distribution function of edit distances between sampled outputs and the greedy output (assumed to represent memorized content), detecting peakedness without probability access.
- Core assumption: The greedy output (temperature 0) is a reliable proxy for memorized test data, and edit distance captures similarity in generated text.
- Evidence anchors:
  - [section 3.2] "we approximate y by the model's output texts and finally choose to replace y with the model's greedy search textst=0"
  - [abstract] "CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution."
  - [corpus] Weak: related works require probability or similarity metrics, not purely sampled text.
- Break condition: If test data is never explicitly memorized or the greedy output is not representative, the proxy fails.

### Mechanism 3
- Claim: TED mitigates contamination by excluding peakedness and duplicates in sampled outputs.
- Mechanism: TED filters sampled texts by removing the greedy output and duplicates, restoring a distribution closer to uncontaminated performance.
- Core assumption: The greedy output is most likely memorized, and duplicates indicate contamination bias.
- Evidence anchors:
  - [section 3.3] "We hope to restore the uncontaminated sampling results by excluding the peakedness in the LLM's output distribution, while excluding the greedy text st=0 which is most likely to represent the leaked data potentially memorized by the LLM."
  - [section 4.3] "TED can steadily mitigate the performance improvements across different settings and occurrences in data contamination scenes."
  - [corpus] Weak: no direct mention of duplicate removal in related works.
- Break condition: If contamination effects are subtle or distributed across many outputs, filtering greedy and duplicates may not sufficiently correct evaluation.

## Foundational Learning

- Concept: Edit distance as a similarity metric
  - Why needed here: Edit distance quantifies how similar sampled outputs are to each other or to the greedy output, enabling detection of peakedness.
  - Quick check question: How does edit distance differ when outputs are diverse vs. when they are nearly identical?

- Concept: Output distribution modeling
  - Why needed here: Understanding how LLM outputs vary under different conditions is essential to detect contamination via distribution changes.
  - Quick check question: What would a peaked output distribution indicate about the model's behavior?

- Concept: Temperature sampling in LLMs
  - Why needed here: Temperature controls randomness in sampling; using temperature 0 yields greedy output, which is used as a proxy for memorization.
  - Quick check question: Why is temperature 0 used to approximate memorized data?

## Architecture Onboarding

- Component map:
  - Input: Prompt x
  - LLM sampling module: Generates multiple outputs
  - Edit distance calculator: Computes distances between sampled outputs
  - Distribution modeler: Builds density function ρ(d)
  - Peak detector: Computes Peak(M; x) using cumulative distribution
  - Contamination classifier: Compares Peak to threshold ξ
  - TED corrector: Filters peaked and duplicate outputs
  - Evaluation wrapper: Applies TED-corrected outputs to metric E

- Critical path:
  1. Sample outputs from LLM
  2. Compute edit distances
  3. Build density function
  4. Calculate peakedness
  5. Classify contamination
  6. If evaluating, apply TED correction

- Design tradeoffs:
  - Sampling more outputs improves accuracy but increases cost
  - Choosing α affects sensitivity to contamination
  - τ controls strictness of peakedness exclusion in TED

- Failure signatures:
  - High false positives: Peakedness detected but no actual contamination (e.g., repetitive prompts)
  - Low recall: Contamination missed due to subtle distribution changes
  - TED overcorrects: Performance drops on uncontaminated data

- First 3 experiments:
  1. Vary α and ξ to find optimal detection threshold
  2. Test TED on uncontaminated data to ensure minimal impact
  3. Compare CDD with baselines (N-gram, Perplexity) on synthetic contamination scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CDD and TED vary across different downstream tasks beyond code generation and logical reasoning?
- Basis in paper: [inferred] The paper mentions that the validation of the approaches is mainly focused on benchmarks for code generation and logical reasoning, and suggests future work to validate on other benchmarks.
- Why unresolved: The paper does not provide empirical evidence on the performance of CDD and TED across different downstream tasks beyond code generation and logical reasoning.
- What evidence would resolve it: Empirical results demonstrating the performance of CDD and TED on a variety of downstream tasks beyond code generation and logical reasoning.

### Open Question 2
- Question: How does the performance of CDD and TED change when using full-parameter fine-tuning instead of LoRA for simulating data contamination in LLMs?
- Basis in paper: [explicit] The paper mentions that due to computational resource limitations, LoRA is used instead of full-parameter fine-tuning to simulate data contamination for LLMs, and suggests future work to attempt full-parameter fine-tuning.
- Why unresolved: The paper does not provide empirical evidence on the performance of CDD and TED when using full-parameter fine-tuning instead of LoRA.
- What evidence would resolve it: Empirical results demonstrating the performance of CDD and TED when using full-parameter fine-tuning instead of LoRA for simulating data contamination in LLMs.

### Open Question 3
- Question: How does the performance of CDD and TED change when using more than 50 samples to compute the output distribution?
- Basis in paper: [inferred] The paper mentions that the approaches require multiple samplings to compute the output distribution and suggests using parallel sampling techniques to speed up sampling and reduce time overhead.
- Why unresolved: The paper does not provide empirical evidence on the performance of CDD and TED when using more than 50 samples to compute the output distribution.
- What evidence would resolve it: Empirical results demonstrating the performance of CDD and TED when using more than 50 samples to compute the output distribution.

## Limitations
- The assumption that peaked output distribution is a reliable indicator of contamination may not hold for all LLM architectures or training regimes.
- The edit distance metric may not fully capture semantic similarity, potentially missing contamination cases where variants are semantically equivalent but syntactically different.
- The paper focuses on contamination at the training data level but does not address potential contamination through intermediate fine-tuning stages or parameter sharing.

## Confidence
- High confidence: The core mechanism of using output distribution peakedness to detect contamination is well-grounded and supported by experimental results.
- Medium confidence: The effectiveness of TED in mitigating contamination effects across diverse real-world scenarios, as the evaluation is limited to two benchmarks.
- Low confidence: The generalizability of the approach to extremely large LLMs (e.g., GPT-4) and highly complex contamination scenarios involving multiple overlapping datasets.

## Next Checks
1. Test CDD and TED on contamination scenarios involving semantically equivalent but syntactically different variants to evaluate robustness against paraphrasing attacks.
2. Apply the methodology to a broader range of benchmarks and LLM architectures to assess generalizability beyond the tested models and tasks.
3. Investigate the impact of intermediate fine-tuning stages on contamination detection and mitigation to ensure the approach covers all potential contamination vectors.