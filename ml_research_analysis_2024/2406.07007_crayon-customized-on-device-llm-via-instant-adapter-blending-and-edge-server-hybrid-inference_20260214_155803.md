---
ver: rpa2
title: 'Crayon: Customized On-Device LLM via Instant Adapter Blending and Edge-Server
  Hybrid Inference'
arxiv_id: '2406.07007'
source_url: https://arxiv.org/abs/2406.07007
tags:
- lora
- base
- customized
- on-device
- loras
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Crayon, a novel approach for on-device LLM
  customization. Crayon constructs a pool of diverse base adapters and instantly blends
  them into a customized adapter without extra training.
---

# Crayon: Customized On-Device LLM via Instant Adapter Blending and Edge-Server Hybrid Inference

## Quick Facts
- arXiv ID: 2406.07007
- Source URL: https://arxiv.org/abs/2406.07007
- Authors: Jihwan Bang; Juntae Lee; Kyuhong Shim; Seunghan Yang; Simyung Chang
- Reference count: 12
- Primary result: Achieves 44.6% accuracy on on-device LLM customization benchmark, outperforming single LoRA (40.7%) and LoraHub (42.4%)

## Executive Summary
Crayon introduces a novel approach for customizing large language models (LLMs) on edge devices through instant adapter blending and hybrid inference. The method constructs a pool of diverse base adapters that can be instantly blended into customized adapters without additional training, enabling efficient on-device personalization. Additionally, Crayon implements a device-server hybrid inference strategy that intelligently allocates more demanding queries to larger LLMs on servers while handling simpler tasks locally. This dual approach addresses the key challenge of balancing model performance with resource constraints on edge devices.

## Method Summary
Crayon's approach centers on two key innovations: instant adapter blending and hybrid inference. The adapter blending component creates a diverse pool of pre-trained base adapters, each specialized for different tasks or domains. When customization is needed, these adapters are blended together on-the-fly to create a task-specific adapter without requiring additional fine-tuning. The hybrid inference strategy determines whether a query should be processed locally on the device or offloaded to a server based on factors like query complexity and available resources. This intelligent allocation allows the system to maintain high performance while respecting device constraints.

## Key Results
- Achieves 44.6% average accuracy on benchmark datasets, outperforming single LoRA (40.7%) and LoraHub (42.4%)
- Maintains low memory and computational overheads while delivering superior performance
- Successfully demonstrates the feasibility of on-device LLM customization through adapter blending

## Why This Works (Mechanism)
The method works by combining the efficiency of adapter-based fine-tuning with intelligent workload distribution. Instant adapter blending allows the system to create customized models quickly by combining pre-trained adapters, eliminating the need for computationally expensive fine-tuning on the device. The hybrid inference component ensures that only queries requiring the full model capacity are sent to the server, while simpler tasks are handled locally, optimizing resource utilization. This dual approach effectively balances the trade-off between customization quality and on-device resource constraints.

## Foundational Learning
- **Adapter-based fine-tuning**: Lightweight parameter-efficient method for adapting pre-trained models; needed for on-device customization without full model training
- **Model blending techniques**: Methods for combining multiple specialized models; required to create customized adapters from base adapters
- **Edge-device constraints**: Understanding of memory, computational, and power limitations on mobile/embedded devices
- **Hybrid inference systems**: Strategies for distributing inference tasks between edge and cloud; essential for optimizing resource utilization
- **Query complexity assessment**: Methods for determining computational requirements of different inputs; needed for intelligent workload distribution
- **On-device model deployment**: Practical considerations for running LLMs on resource-constrained devices

## Architecture Onboarding
- **Component map**: Base adapter pool -> Adapter blending module -> Local inference engine -> Query complexity assessor -> Hybrid inference router -> Server inference engine
- **Critical path**: Query input -> Complexity assessment -> Routing decision -> Adapter blending (if local) -> Inference execution -> Response generation
- **Design tradeoffs**: Adapter pool diversity vs memory usage; complexity assessment accuracy vs computational overhead; server offloading vs privacy concerns
- **Failure signatures**: Poor customization quality from inadequate base adapter pool; routing errors causing unnecessary server communication; memory overflow from large adapter combinations
- **First experiments**: 1) Test adapter blending quality with different base adapter combinations, 2) Validate query complexity assessment accuracy, 3) Measure latency and accuracy trade-offs in hybrid inference

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation based on carefully crafted benchmark that may not reflect real-world usage patterns
- Lack of detailed ablation studies to isolate contributions of adapter blending versus hybrid inference
- Missing specific metrics for different device classes and network conditions
- Limited validation of scalability and robustness across diverse edge scenarios

## Confidence
- High confidence in the technical novelty of the adapter blending approach
- Medium confidence in the practical feasibility of hybrid inference across diverse edge scenarios
- Low confidence in the scalability and robustness claims without broader empirical validation

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of adapter blending versus hybrid inference to the overall performance gains
2. Test the approach across a wider range of device types (varying RAM, CPU, and network capabilities) to assess real-world applicability
3. Evaluate performance degradation under varying network conditions to validate the robustness of the hybrid inference strategy