---
ver: rpa2
title: Towards Adversarial Robustness of Model-Level Mixture-of-Experts Architectures
  for Semantic Segmentation
arxiv_id: '2412.11608'
source_url: https://arxiv.org/abs/2412.11608
tags:
- adversarial
- attacks
- expert
- experts
- gate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the adversarial robustness of model-level
  mixture-of-experts (MoE) architectures for semantic segmentation in urban and highway
  traffic scenes. The authors evaluate four MoE variants against per-instance and
  universal white-box attacks (FGSM, PGD-10) and transfer attacks.
---

# Towards Adversarial Robustness of Model-Level Mixture-of-Experts Architectures for Semantic Segmentation

## Quick Facts
- **arXiv ID**: 2412.11608
- **Source URL**: https://arxiv.org/abs/2412.11608
- **Reference count**: 40
- **Primary result**: MoE architectures with classwise gates and additional convolutional layers achieve up to 70.61% mIoU under PGD attacks (vs 8.27% for baseline models)

## Executive Summary
This paper investigates the adversarial robustness of model-level mixture-of-experts (MoE) architectures for semantic segmentation in urban and highway traffic scenes. The authors evaluate four MoE variants against per-instance and universal white-box attacks (FGSM, PGD-10) and transfer attacks. Their experiments on DeepLabv3+- and FRRN-based models show that MoEs, especially those with classwise gates and additional convolutional layers, exhibit greater robustness than individual experts or ensembles. In particular, MoEs with classwise gates and extra conv layers achieve up to 70.61% mIoU under PGD attacks (DeepLabv3+ architecture, ϵ=0.05), compared to 8.27% for baseline models. MoEs also demonstrate better resistance to transfer attacks, with adversarial noise generated for MoEs causing smaller accuracy drops on target models than noise from individual experts. The findings highlight MoEs as a promising approach to improving adversarial robustness in semantic segmentation tasks.

## Method Summary
The authors evaluate model-level MoE architectures for semantic segmentation by implementing four MoE variants: a baseline expert ensemble, a gating network-based MoE, a classwise MoE, and a combined classwise gating MoE. These are tested on DeepLabv3+ and FRRN backbones for urban driving scenarios using Cityscapes and CamVid datasets. The evaluation includes white-box attacks (FGSM, PGD-10) with varying perturbation strengths (ϵ=0.01 to 0.05) and transfer attacks where adversarial examples generated for one model are tested on others. The study systematically compares mIoU performance across attack types and perturbation strengths to assess robustness improvements from MoE architectures compared to individual experts.

## Key Results
- MoEs with classwise gates and additional convolutional layers achieve up to 70.61% mIoU under PGD attacks (DeepLabv3+ architecture, ϵ=0.05), compared to 8.27% for baseline models
- MoEs demonstrate significantly better resistance to transfer attacks, with adversarial noise generated for MoEs causing smaller accuracy drops on target models than noise from individual experts
- Classwise gating MoE with extra convolutional layers consistently outperforms other MoE variants across both DeepLabv3+ and FRRN architectures under various attack conditions

## Why This Works (Mechanism)
The improved adversarial robustness of MoE architectures stems from their ability to dynamically select and combine multiple specialized experts based on input characteristics. Unlike single models that must handle all scenarios with one decision boundary, MoEs distribute the learning task across multiple experts, each potentially developing different decision boundaries. The gating mechanism (especially classwise gating) allows the model to route inputs to the most appropriate expert or combination of experts, making it harder for adversarial perturbations to consistently fool all experts simultaneously. The additional convolutional layers in the gating network provide more discriminative features for routing decisions, further enhancing robustness against attacks that aim to manipulate the gating mechanism itself.

## Foundational Learning
- **Adversarial Attacks in Semantic Segmentation**: Techniques like FGSM and PGD that add imperceptible perturbations to input images to fool segmentation models - needed to evaluate model robustness in safety-critical applications like autonomous driving - quick check: Can you explain the difference between per-instance and universal attacks?
- **Mixture-of-Experts (MoE) Architecture**: A paradigm where multiple specialized models (experts) are combined through a gating mechanism to handle different input patterns - needed to understand how MoEs distribute learning tasks across specialized components - quick check: What are the key differences between model-level and token-level MoEs?
- **Transfer Attacks**: When adversarial examples generated for one model are tested on different models to evaluate cross-model vulnerability - needed to assess practical security implications beyond white-box scenarios - quick check: Why are transfer attacks particularly relevant for real-world adversarial scenarios?
- **Classwise Gating**: A routing mechanism that selects experts based on the predicted class distribution of the input - needed to understand how the routing mechanism contributes to robustness - quick check: How does classwise gating differ from traditional gating mechanisms in MoE architectures?
- **White-box vs Black-box Attacks**: White-box attacks have full access to model parameters and gradients, while black-box attacks have limited information - needed to contextualize the evaluation methodology - quick check: What are the main challenges in defending against black-box attacks compared to white-box attacks?

## Architecture Onboarding

Component Map:
Input Image -> Backbone (DeepLabv3+ or FRRN) -> MoE Module (Gating Network + Multiple Experts) -> Output Segmentation Map

Critical Path:
Input Image → Backbone Feature Extraction → Gating Network (Classwise routing) → Selected Expert(s) → Segmentation Output

Design Tradeoffs:
- MoE architectures provide improved robustness but increase model complexity and inference time compared to single experts
- Classwise gating improves robustness but requires more sophisticated routing mechanisms than simple gating networks
- Additional convolutional layers in gating networks enhance discriminative power but add computational overhead
- Transfer attack resistance is improved but may come at the cost of reduced individual expert performance on clean data

Failure Signatures:
- When adversarial perturbations successfully manipulate the gating mechanism to route inputs to inappropriate experts
- When attacks find common vulnerabilities across multiple experts that the gating network cannot effectively isolate
- When transfer attacks from highly robust MoE models still maintain effectiveness against less robust individual experts

First Experiments to Run:
1. Test MoE architectures against adaptive black-box attacks to assess real-world robustness beyond white-box scenarios
2. Evaluate computational overhead (FLOPs, inference latency) of MoE inference compared to individual experts to assess practical deployment feasibility
3. Investigate the impact of varying the number of experts in the MoE architecture on both robustness and performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses exclusively on white-box attacks with specific attack parameters (FGSM, PGD-10), leaving open questions about performance against more sophisticated or adaptive black-box attack strategies
- Results are based on two specific backbone architectures (DeepLabv3+ and FRRN) and two datasets (Cityscapes and CamVid), which may limit generalizability to other segmentation architectures or urban driving scenarios
- The study does not investigate the computational overhead of MoE inference during real-time deployment, nor does it explore potential trade-offs between robustness gains and model complexity or inference speed

## Confidence

High Confidence: MoE architectures show improved robustness compared to individual experts under the tested attack conditions (FGSM and PGD-10 on DeepLabv3+ and FRRN)

Medium Confidence: The specific MoE variants with classwise gates and additional convolutional layers consistently outperform other configurations across attack types

Medium Confidence: Transfer attack resistance follows the observed pattern, though this requires further validation across different attack generation methods

## Next Checks
1. Evaluate the proposed MoE architectures against adaptive black-box attacks and decision-based attacks to assess real-world robustness beyond white-box scenarios
2. Test the robustness claims across a broader range of backbone architectures (e.g., HRNet, SegFormer) and additional driving datasets (e.g., BDD100K, Mapillary Vistas) to establish generalizability
3. Measure and report the computational overhead (FLOPs, inference latency) of MoE inference compared to individual experts to assess practical deployment feasibility