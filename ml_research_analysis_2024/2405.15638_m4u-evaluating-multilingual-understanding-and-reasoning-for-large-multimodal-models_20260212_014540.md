---
ver: rpa2
title: 'M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal
  Models'
arxiv_id: '2405.15638'
source_url: https://arxiv.org/abs/2405.15638
tags:
- image
- error
- uni0000004a
- question
- uni00000042
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: M4U is a new benchmark for evaluating multilingual multimodal understanding
  and reasoning in large language models. It contains 10k questions across 64 disciplines
  in 6 languages, requiring complex reasoning over both visual and textual multilingual
  content.
---

# M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models

## Quick Facts
- arXiv ID: 2405.15638
- Source URL: https://arxiv.org/abs/2405.15638
- Reference count: 40
- Leading model GPT-4o achieves only 47.6% accuracy on M4U benchmark

## Executive Summary
M4U is a comprehensive benchmark designed to evaluate multilingual multimodal understanding and reasoning capabilities of large language models. It contains 10,000 questions across 64 disciplines in 6 languages, requiring complex reasoning over both visual and textual multilingual content. The benchmark reveals that even the most advanced models like GPT-4o struggle significantly with this task, achieving only 47.6% accuracy. Models exhibit substantial performance degradation on cross-lingual questions and show clear language preferences, indicating that current LMMs lack robust multilingual multimodal reasoning capabilities.

## Method Summary
M4U was constructed through a rigorous pipeline involving domain experts who created questions across 64 undergraduate-level disciplines in 6 languages (Chinese, English, French, German, Japanese, and Korean). The dataset includes both single-modality (visual-only or text-only) and cross-modality questions that require reasoning over visual and textual information. Questions were designed to test various reasoning abilities including perception, association, comprehension, logical reasoning, and cross-modal integration. The benchmark was validated through human evaluations and includes questions requiring domain-specific knowledge from fields like engineering, medicine, and natural sciences.

## Key Results
- GPT-4o achieves 47.6% accuracy on M4U, demonstrating current models' limitations in multilingual multimodal reasoning
- Models show significant performance degradation on cross-lingual questions, with accuracy dropping notably when visual content contains language-specific information
- Strong language preferences observed across models, with InstructBLIP Vicuna-7B achieving 28.1% on English but only 13.7% and 19.7% on Chinese and German respectively

## Why This Works (Mechanism)
The benchmark works by creating a diverse set of multimodal reasoning tasks that cannot be solved through simple pattern matching or single-modality understanding. By requiring models to integrate visual and textual information across multiple languages and disciplines, M4U exposes fundamental limitations in current LMM architectures. The cross-lingual design specifically targets the models' ability to reason about visual content that contains language-specific information while answering questions in a different language, revealing gaps in multilingual instruction following and cross-modal integration.

## Foundational Learning
1. **Multimodal Integration**: Combining visual and textual information for reasoning
   - Why needed: Most real-world understanding requires processing multiple modalities simultaneously
   - Quick check: Can the model answer questions requiring both image and text analysis?

2. **Cross-Lingual Reasoning**: Understanding visual content with language-specific information while answering in another language
   - Why needed: Global applications require reasoning across language barriers
   - Quick check: Does performance degrade when question language differs from visual content language?

3. **Domain-Specific Knowledge**: Applying specialized knowledge to visual content
   - Why needed: Real-world applications span diverse fields requiring specific expertise
   - Quick check: Can models reason about specialized content like medical images or engineering diagrams?

4. **Complex Reasoning Chains**: Multi-step reasoning combining multiple cognitive skills
   - Why needed: Real understanding requires more than simple recognition or matching
   - Quick check: Does the model need to perform multiple reasoning steps to answer correctly?

## Architecture Onboarding

**Component Map:** Visual Encoder -> Multimodal Fusion -> Language Model -> Reasoning Module

**Critical Path:** Input Processing → Multimodal Fusion → Cross-Lingual Alignment → Answer Generation

**Design Tradeoffs:** 
- Model size vs. multilingual coverage: Larger models show better performance but struggle with cross-lingual tasks
- Generalist vs. specialist: Models trained on general data underperform on domain-specific content
- Instruction following vs. direct reasoning: Models following multilingual instructions show variable performance

**Failure Signatures:** 
- Perceptual errors on domain-specific visual content (blueprints, medical images)
- Language preference bias in cross-lingual scenarios
- Performance degradation when visual text differs from question language

**3 First Experiments:**
1. Test model performance on single-modality vs. cross-modality questions to isolate multimodal reasoning capabilities
2. Evaluate cross-lingual subset separately to measure language-specific performance gaps
3. Compare performance across different language pairs to identify systematic biases

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can large multimodal models be improved to better handle cross-lingual multimodal reasoning tasks where visual content contains key information in one language while the question is in another language?
- Basis in paper: The paper demonstrates that leading LMMs suffer significant performance degradation on cross-lingual multimodal questions, with GPT-4o's accuracy dropping from 47.6% to 48.2% on the cross-lingual subset.
- Why unresolved: Current models lack sufficient multilingual vision-language training data and struggle to follow multilingual instructions for understanding visual content with textual information in another language.
- What evidence would resolve it: Training and evaluating LMMs on datasets specifically designed for cross-lingual multimodal reasoning, measuring performance improvements with multilingual vision-language pretraining.

### Open Question 2
- Question: What architectural modifications or training strategies could enable LMMs to better perceive and reason about domain-specific visual content like engineering blueprints and medical images?
- Basis in paper: The qualitative analysis shows GPT-4V struggles significantly with blueprints requiring fine-grained perception and domain-specific knowledge, and has perceptual errors with medical images in the qualitative analysis.
- Why unresolved: Current LMMs are trained primarily on general vision-language data and lack specialized training for domain-specific visual content and standards.
- What evidence would resolve it: Comparing performance of LMMs trained with domain-specific visual data versus general training, measuring improvements on benchmarks like M4U's blueprint and medical image questions.

### Open Question 3
- Question: How can LMMs be enhanced to better handle multilingual content in complex reasoning scenarios, particularly when combining visual and textual information across languages?
- Basis in paper: The paper shows models exhibit strong language preferences, with InstructBLIP Vicuna-7B achieving 28.1% on English but only 13.7% and 19.7% on Chinese and German respectively, and GPT-4V showing different error patterns across languages.
- Why unresolved: Current LMMs lack balanced multilingual training data and struggle with multilingual reasoning, as evidenced by the significant performance variations across languages.
- What evidence would resolve it: Evaluating LMMs on multilingual reasoning tasks with varying levels of language balance in training data, measuring how performance scales with improved multilingual training.

## Limitations
- Potential annotation bias in dataset construction due to human annotator preferences across languages and cultures
- Limited language coverage with only 6 languages, potentially missing important multilingual reasoning scenarios
- Benchmark focus on undergraduate-level content may not capture full spectrum of real-world multilingual multimodal understanding
- 10k question count may be insufficient to fully characterize model performance across all 64 disciplines

## Confidence
*High Confidence:* The claim that GPT-4o achieves 47.6% accuracy on M4U is supported by direct experimental results presented in the paper. The observation of significant language preferences in leading models is also well-supported by the comparative analysis across different language conditions.

*Medium Confidence:* The assertion that current models "struggle" with multilingual multimodal reasoning is reasonable given the performance numbers, but the interpretation of what constitutes "struggling" versus "competitive" performance could vary depending on the baseline expectations. The characterization of M4U as requiring "complex reasoning" over multimodal content is based on the authors' design criteria, which may not be universally agreed upon.

*Low Confidence:* Claims about the general state of the field's progress in multilingual multimodal understanding are somewhat speculative, as the paper presents a single benchmark result rather than a comprehensive survey of all relevant capabilities and approaches.

## Next Checks
1. Conduct inter-annotator agreement studies to quantify potential bias in question creation across different language teams and cultural contexts.
2. Perform cross-validation with human experts fluent in multiple languages to assess whether the reported accuracy gaps reflect genuine model limitations versus dataset artifacts.
3. Test model performance on a subset of M4U questions with modified cultural references or language-specific idioms to isolate whether performance differences stem from reasoning ability versus language-specific knowledge.