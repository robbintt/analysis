---
ver: rpa2
title: 'InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification'
arxiv_id: '2401.16475'
source_url: https://arxiv.org/abs/2401.16475
tags:
- information
- were
- question
- text
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INFO LOSS QA introduces a framework to characterize and recover
  information loss in text simplification by generating question-and-answer pairs.
  Building on the theory of Questions Under Discussion, the framework aims to reveal
  missing or oversimplified information in simplified texts through curiosity-driven
  questions and answers that expand on the simplified text.
---

# InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification
## Quick Facts
- arXiv ID: 2401.16475
- Source URL: https://arxiv.org/abs/2401.16475
- Reference count: 40
- Primary result: QA framework for detecting information loss in text simplification

## Executive Summary
INFO LOSS QA introduces a framework to characterize and recover information loss in text simplification by generating question-and-answer pairs. Building on the theory of Questions Under Discussion, the framework aims to reveal missing or oversimplified information in simplified texts through curiosity-driven questions and answers that expand on the simplified text. The authors curate a dataset of 1,000 QA pairs derived from 104 LLM simplifications of medical abstracts, revealing that information loss occurs frequently, particularly in technical sections. They propose two methods for generating QA pairs: end-to-end LLM prompting and a natural language inference pipeline. Expert evaluation of 400+ generated QA pairs shows that while models can generate valid questions and answers, they struggle to reliably identify information loss and apply similar standards as humans. The NLI pipeline is more effective at identifying information loss but produces QA pairs with smaller granularity. This work highlights the challenges in automatically characterizing information loss and provides a foundation for developing interactive simplification tools.

## Method Summary
The INFO LOSS QA framework uses two main approaches to generate question-and-answer pairs that characterize information loss in simplified texts. The first approach uses direct LLM prompting to generate curiosity-driven questions about the simplified text, then generates answers by contrasting the original and simplified texts. The second approach employs a natural language inference (NLI) pipeline that first identifies sentences likely containing information loss, then generates QA pairs specifically for those sentences. The framework is evaluated on 104 LLM-generated simplifications of medical abstracts, producing a dataset of 1,000 expert-curated QA pairs. Expert annotators evaluate the generated QA pairs for validity and their ability to identify information loss, with particular attention to technical sections where loss is most prevalent.

## Key Results
- Information loss occurs frequently in LLM simplifications, particularly in technical sections of medical abstracts
- The NLI pipeline outperforms end-to-end prompting at identifying information loss, but produces less granular QA pairs
- Models struggle to reliably detect information loss and apply consistent standards compared to human experts
- Expert evaluation of 400+ QA pairs reveals models can generate valid questions and answers but have difficulty identifying loss

## Why This Works (Mechanism)
The framework leverages the Questions Under Discussion theory to create curiosity-driven questions that reveal what information was lost during simplification. By generating questions that require answers beyond what's present in the simplified text, the framework can identify gaps where important information was omitted or oversimplified. The NLI pipeline's strength comes from its ability to first identify sentences with potential information loss before generating QA pairs, creating a more focused approach to loss detection.

## Foundational Learning
- Questions Under Discussion theory: Framework's theoretical foundation for generating curiosity-driven questions that reveal information gaps. Why needed: Provides principled approach to characterizing what information is missing. Quick check: Verify questions require answers beyond simplified text content.
- Natural Language Inference: Used in pipeline approach to identify sentences with potential information loss. Why needed: Enables systematic identification of loss-prone sentences before QA generation. Quick check: Measure NLI accuracy on identifying loss-containing sentences.
- LLM simplification evaluation: Framework's method for assessing information preservation in simplified texts. Why needed: Provides quantitative measure of simplification quality beyond standard metrics. Quick check: Compare framework's loss detection with human expert judgments.

## Architecture Onboarding
Component map: Original text -> Simplification model -> Candidate sentences -> NLI classifier -> QA generator -> Expert evaluation
Critical path: Original text → Simplification → NLI detection → QA generation → Expert validation
Design tradeoffs: NLI pipeline vs. end-to-end prompting - NLI more accurate at loss detection but produces less detailed QA pairs; end-to-end faster but less reliable at identifying loss
Failure signatures: False positives when NLI detects loss in syntactically complex but semantically preserved sentences; false negatives when loss occurs in sentences that NLI doesn't flag
First experiments: 1) Run NLI pipeline on 50 simplifications to measure baseline loss detection accuracy 2) Generate QA pairs using both methods on same 20 simplifications to compare granularity 3) Expert evaluation of 100 randomly selected QA pairs to assess human-model agreement

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- Reliance on synthetic LLM simplifications limits generalizability to human-authored simplifications
- Domain-specific focus on medical texts may not transfer to other simplification challenges
- Small expert evaluation sample size (400+ QA pairs) and potential selection bias limit statistical power

## Confidence
- High: Framework's general approach of using QA pairs to characterize information loss is valid and methodologically sound
- Medium: Effectiveness of NLI pipeline for loss detection, though results are constrained by sample size and domain specificity
- Low: Claims about model ability to apply human-like standards for identifying information loss, given inconsistent performance across evaluation metrics

## Next Checks
1. Test the framework on human-authored simplifications across multiple domains (e.g., legal, educational, news) to assess domain transferability
2. Expand expert evaluation to include diverse annotator pools and larger sample sizes (n>1000) to improve statistical power and reduce bias
3. Implement ablation studies comparing the framework's performance when using different simplification models (e.g., rule-based vs. neural) to isolate model-specific effects on information loss detection