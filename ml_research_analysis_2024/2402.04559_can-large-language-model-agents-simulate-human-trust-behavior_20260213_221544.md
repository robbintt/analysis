---
ver: rpa2
title: Can Large Language Model Agents Simulate Human Trust Behavior?
arxiv_id: '2402.04559'
source_url: https://arxiv.org/abs/2402.04559
tags:
- trust
- amount
- player
- game
- other
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language model (LLM) agents
  can simulate human trust behavior using Trust Games, a framework from behavioral
  economics. The authors define "agent trust" and propose a new concept called "behavioral
  alignment" to measure the similarity between LLM agents and humans regarding behavioral
  factors (e.g., reciprocity anticipation, risk perception, prosocial preference)
  and dynamics (e.g., stability across rounds in repeated games).
---

# Can Large Language Model Agents Simulate Human Trust Behavior?

## Quick Facts
- arXiv ID: 2402.04559
- Source URL: https://arxiv.org/abs/2402.04559
- Reference count: 40
- Primary result: GPT-4 agents demonstrate behavioral alignment with humans in trust games, validating their potential to simulate human trust behavior

## Executive Summary
This paper investigates whether large language model (LLM) agents can simulate human trust behavior using Trust Games from behavioral economics. The authors define "agent trust" and propose "behavioral alignment" to measure similarity between LLM agents and humans on behavioral factors and dynamics. They find that GPT-4 agents exhibit high behavioral alignment with humans in trust behavior, while other LLMs with fewer parameters show lower alignment. The study provides empirical evidence for using LLM agents to simulate human trust behavior and opens avenues for simulating complex human interactions and social institutions.

## Method Summary
The study uses the CAMEL framework with 10 different LLMs including GPT-4, GPT-3.5, Llama2, and Vicuna configured with 53 diverse persona templates. Six types of Trust Games are implemented: Trust Game, Dictator Game, MAP Trust Game, Risky Dictator Game, Lottery Game, and Repeated Trust Game. The researchers capture Belief-Desire-Intention (BDI) outputs for each agent's reasoning process and measure trust behaviors including amount sent, trust rates, and lottery rates. Results are compared to human trust behavior data from existing studies, analyzing behavioral factors (reciprocity anticipation, risk perception, prosocial preference) and behavioral dynamics across multiple rounds.

## Key Results
- LLM agents generally exhibit trust behavior in Trust Games with rationality demonstrated through articulated BDI reasoning processes
- GPT-4 agents show high behavioral alignment with humans on trust behavior, validating their potential to simulate human trust
- Agent trust exhibits demographic biases, prefers humans over other agents, is easier to undermine than enhance, and may be influenced by reasoning strategies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT-4 agents demonstrate trust behavior through articulated reasoning processes (BDI framework)
- **Mechanism:** The Belief-Desire-Intention (BDI) framework models LLM agents' decision-making by explicitly capturing their Beliefs (perceptions), Desires (goals), and Intentions (planned actions). This framework allows interpretation of LLM agents' trust behavior as rational, goal-directed actions rather than random outputs.
- **Core assumption:** LLM agents can produce coherent BDI outputs that reflect their reasoning process
- **Evidence anchors:**
  - [abstract] "LLM agents generally exhibit trust behavior in Trust Games, showing rationality through articulated reasoning processes (BDI framework)"
  - [section 3.2] "We can observe that this persona shows a high-level of 'comradery and trust' towards the other player, which justifies the high amount sent from this persona"
- **Break condition:** If LLM agents produce incoherent or inconsistent BDI outputs that do not align with their actions

### Mechanism 2
- **Claim:** GPT-4 agents align with humans on behavioral factors underlying trust behavior
- **Mechanism:** Behavioral alignment measures similarity between LLM agents and humans concerning behavioral factors (reciprocity anticipation, risk perception, prosocial preference) and dynamics. GPT-4 agents exhibit human-like patterns in these factors, validating their capacity to simulate human trust behavior.
- **Core assumption:** Behavioral factors and dynamics are measurable and comparable between LLM agents and humans
- **Evidence anchors:**
  - [abstract] "GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior"
  - [section 4.5] "GPT-4 agents demonstrate highly human-like patterns in their trust behavioral dynamics"
- **Break condition:** If LLM agents fail to show consistent patterns in behavioral factors or dynamics compared to humans

### Mechanism 3
- **Claim:** LLM agents' trust behaviors exhibit demographic biases and differential preferences for humans vs. agents
- **Mechanism:** LLM agents' trust behaviors are influenced by demographics of the trustee (gender, race) and identity of the trustee (human vs. agent). This reveals biases and preferences that impact their trust decisions.
- **Core assumption:** LLM agents' trust behaviors are sensitive to demographic and identity cues
- **Evidence anchors:**
  - [abstract] "agent trust exhibits demographic biases, prefers humans over other agents"
  - [section 5.1] "we find that the trustee's gender information exerts a moderate impact on LLM agent trust behavior"
- **Break condition:** If LLM agents show no significant differences in trust behaviors based on trustee demographics or identity

## Foundational Learning

- **Concept: Belief-Desire-Intention (BDI) Framework**
  - Why needed here: The BDI framework provides a structured way to interpret LLM agents' reasoning processes, validating their trust behavior as rational rather than random
  - Quick check question: Can you explain how the BDI framework models an agent's decision-making process in the context of trust games?

- **Concept: Behavioral Alignment**
  - Why needed here: Behavioral alignment quantifies the similarity between LLM agents and humans concerning behavioral factors and dynamics, validating the feasibility of simulating human trust behavior
  - Quick check question: How does behavioral alignment differ from traditional alignment concepts in AI?

- **Concept: Trust Games**
  - Why needed here: Trust Games provide a controlled framework to study trust behavior, allowing for the measurement and comparison of trust between LLM agents and humans
  - Quick check question: What are the key differences between the Trust Game, Dictator Game, and MAP Trust Game in terms of trust behavior measurement?

## Architecture Onboarding

- **Component map:**
  LLM Agents -> Trust Games -> BDI Framework -> Behavioral Alignment -> Manipulation Scenarios

- **Critical path:**
  1. Design LLM agents with diverse personas
  2. Implement Trust Games with appropriate prompts
  3. Capture LLM agents' BDI outputs
  4. Measure trust behaviors (amount sent, trust rates, lottery rates)
  5. Compare LLM agents' behaviors with human studies
  6. Analyze behavioral alignment on factors and dynamics
  7. Probe intrinsic properties under manipulation scenarios

- **Design tradeoffs:**
  - Closed-source vs. open-source LLMs: Trade-off between performance and accessibility
  - Persona diversity: Balancing realism with experimental control
  - Trust Game variations: Capturing different aspects of trust behavior
  - BDI interpretation: Balancing depth with interpretability

- **Failure signatures:**
  - LLM agents produce incoherent or inconsistent BDI outputs
  - Trust behaviors do not align with human studies on behavioral factors or dynamics
  - Manipulation scenarios fail to elicit expected changes in trust behaviors
  - Biases and preferences are not observed or are inconsistent

- **First 3 experiments:**
  1. Implement the Trust Game with GPT-4 and capture BDI outputs to validate trust behavior
  2. Compare GPT-4's trust behaviors in the Trust Game and Dictator Game to measure reciprocity anticipation
  3. Conduct the MAP Trust Game with GPT-4 to assess risk perception and trust rates across different probabilities

## Open Questions the Paper Calls Out

- **Open Question 1:** Can LLM agents with fewer parameters demonstrate behavioral alignment with humans in trust behavior?
  - Basis in paper: [explicit]
  - Why unresolved: The paper found that GPT-4 agents exhibit high behavioral alignment with humans, while LLMs with fewer parameters show lower alignment. It remains unclear if future research could improve alignment for smaller models
  - What evidence would resolve it: Future studies testing LLM agents with fewer parameters on trust games and comparing their behavioral alignment with humans

- **Open Question 2:** How do LLM agents' trust behaviors change when interacting in complex and dynamic environments?
  - Basis in paper: [inferred]
  - Why unresolved: The paper focused on trust games as a simplified framework. Real-world environments involve more complexity and dynamics that could affect trust behaviors
  - What evidence would resolve it: Experiments testing LLM agents' trust behaviors in complex simulations or real-world scenarios involving dynamic interactions and changing conditions

- **Open Question 3:** What is the relationship between reasoning strategies and LLM agents' trust behaviors?
  - Basis in paper: [explicit]
  - Why unresolved: The paper found that advanced reasoning strategies like CoT may influence trust behaviors, but the impact is not fully understood. Further research is needed to explore this relationship
  - What evidence would resolve it: Systematic studies testing different reasoning strategies on LLM agents' trust behaviors and analyzing the underlying mechanisms

## Limitations

- The study relies heavily on self-reported BDI outputs from LLMs, which may not accurately reflect true decision-making processes
- Behavioral alignment measurements depend on the quality and representativeness of human trust data used for comparison
- Manipulation scenarios (demographic biases, human vs. agent preferences) may be influenced by specific prompt engineering rather than genuine behavioral patterns

## Confidence

- **High Confidence:** The general finding that GPT-4 exhibits trust behavior in Trust Games with coherent reasoning is well-supported by empirical results and BDI framework analysis
- **Medium Confidence:** The claim about behavioral alignment between GPT-4 and humans is supported but limited by availability and comparability of human trust data across different studies
- **Medium Confidence:** Findings on demographic biases and human vs. agent preferences show observable patterns but may be sensitive to prompt engineering and require further validation

## Next Checks

1. Conduct blind validation with human raters to verify whether BDI outputs accurately represent the reasoning behind trust decisions, separating genuine reasoning from post-hoc rationalization
2. Replicate the behavioral alignment measurements using alternative human trust datasets and different Trust Game implementations to test robustness
3. Test the demographic bias findings using counter-balanced prompt designs and different demographic presentation methods to isolate genuine behavioral patterns from prompt sensitivity