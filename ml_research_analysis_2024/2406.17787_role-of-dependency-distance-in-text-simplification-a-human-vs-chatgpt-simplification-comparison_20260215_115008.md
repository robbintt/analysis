---
ver: rpa2
title: 'Role of Dependency Distance in Text Simplification: A Human vs ChatGPT Simplification
  Comparison'
arxiv_id: '2406.17787'
source_url: https://arxiv.org/abs/2406.17787
tags:
- dependency
- sentences
- distance
- simplified
- simplification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared human and ChatGPT text simplification using
  dependency distance as a metric. Researchers simplified 220 sentences of varying
  grammatical difficulty using both a human expert and ChatGPT-3.5, keeping all words
  unchanged but altering syntax.
---

# Role of Dependency Distance in Text Simplification: A Human vs ChatGPT Simplification Comparison

## Quick Facts
- arXiv ID: 2406.17787
- Source URL: https://arxiv.org/abs/2406.17787
- Authors: Sumi Lee; Gondy Leroy; David Kauchak; Melissa Just
- Reference count: 3
- Key outcome: Human simplification reduced dependency distance more than ChatGPT (M=2.77 vs M=2.87), but differences were not statistically significant (p=0.07)

## Executive Summary
This study compared human and ChatGPT text simplification using dependency distance as a metric for syntactic complexity. Researchers simplified 220 sentences of varying grammatical difficulty using both a human expert and ChatGPT-3.5, keeping all words unchanged but altering syntax. They found that dependency distance followed a consistent pattern across all grammatical difficulty levels: original sentences had the highest mean dependency distance (M=2.90), ChatGPT-simplified sentences showed a slight decrease (M=2.87), and human-simplified sentences had the lowest mean dependency distance (M=2.77). While there was a trend toward significance (F(2,657)=2.62, p=0.07), the differences were not statistically significant. The results suggest that human simplification more effectively reduces syntactic complexity than ChatGPT, though both methods decrease dependency distance compared to original sentences.

## Method Summary
The study used 220 sentences from English Wikipedia with increasing grammatical difficulty, categorized into 11 frequency bins. Both a human expert and ChatGPT-3.5 simplified these sentences by changing grammatical structure while keeping all words unchanged. Dependency distances were calculated for all three sets (original, ChatGPT-simplified, human-simplified) using the TextDescriptives package with spaCy v.3 pipeline components. A one-way ANOVA was performed to compare mean dependency distances across the three sentence types.

## Key Results
- Original sentences had the highest mean dependency distance (M=2.90, SD=0.63)
- ChatGPT-simplified sentences showed a slight decrease (M=2.87, SD=0.62)
- Human-simplified sentences had the lowest mean dependency distance (M=2.77, SD=0.60)
- One-way ANOVA showed a trend toward significance but no significant effect of sentence type (F(2,657)=2.62, p=0.07)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dependency distance reduction occurs because both human and ChatGPT simplification strategies break apart long-distance syntactic dependencies.
- Mechanism: When simplifying sentences, both humans and ChatGPT restructure complex clauses into simpler forms, which reduces the number of nodes between dependent words in the dependency tree.
- Core assumption: Simplifying syntactic structure directly correlates with reducing dependency distance, regardless of the method used.
- Evidence anchors:
  - [abstract] "we found that the three sentence sets all differed in mean dependency distances: the highest in the original sentence set, followed by ChatGPT simplified sentences, and the human simplified sentences showed the lowest mean dependency distance."
  - [section] "Comparing the method of simplification showed that the mean dependency distances of the original sentence set (M=2.90, SD=0.63) decreased when simplified using human or ChatGPT simplification"
  - [corpus] Weak evidence - no corpus data directly addresses the specific mechanism of dependency distance reduction

### Mechanism 2
- Claim: Human simplification achieves greater dependency distance reduction than ChatGPT because humans better understand contextual nuances.
- Mechanism: Human experts can identify and restructure the most problematic syntactic dependencies based on their understanding of biomedical context, while ChatGPT follows more general simplification patterns.
- Core assumption: Human expertise in biomedical language provides an advantage in identifying which syntactic structures to simplify.
- Evidence anchors:
  - [abstract] "ChatGPT simplified sentences (M=2.87, SD=0.62) showed a slightly higher dependency distance than human simplified sentences (M=2.77, SD=0.60)"
  - [section] "A one-way ANOVA with mean dependency distances as the dependent variable and sentence types (original, ChatGPT simplified, human simplified) as the independent variable showed that there was a trend but no significant effect of sentence type (F(2,657)=2.62, p=0.07)"
  - [corpus] No corpus data directly addresses why humans might outperform ChatGPT in dependency distance reduction

### Mechanism 3
- Claim: The lack of statistical significance in the ANOVA test suggests that both human and ChatGPT simplification methods are similarly effective at reducing dependency distance.
- Mechanism: Despite the observed trend where human simplification achieves lower dependency distances, the variability between samples is high enough that the difference is not statistically significant.
- Core assumption: The sample size and variability are sufficient to detect meaningful differences between simplification methods.
- Evidence anchors:
  - [section] "A one-way ANOVA with mean dependency distances as the dependent variable and sentence types (original, ChatGPT simplified, human simplified) as the independent variable showed that there was a trend but no significant effect of sentence type (F(2,657)=2.62, p=0.07)"
  - [abstract] "While there was a trend toward significance (F(2,657)=2.62, p=0.07), the differences were not statistically significant."
  - [corpus] No corpus data addresses the statistical power or variability of dependency distance measurements

## Foundational Learning

- Concept: Dependency distance as a syntactic complexity metric
  - Why needed here: Understanding how dependency distance measures syntactic complexity is crucial for interpreting the results and implications of this study
  - Quick check question: What does a high dependency distance value indicate about the syntactic structure of a sentence?

- Concept: Text simplification techniques and their evaluation
  - Why needed here: The study compares different simplification approaches, requiring understanding of how text simplification is performed and measured
  - Quick check question: How does syntactic simplification differ from lexical simplification, and why might one be preferred over the other?

- Concept: Statistical significance and interpretation of p-values
  - Why needed here: The study reports a trend toward significance but not statistical significance, requiring understanding of what this means for interpreting results
  - Quick check question: What does a p-value of 0.07 indicate about the strength of evidence for differences between human and ChatGPT simplification?

## Architecture Onboarding

- Component map: Sentence selection (220 sentences) -> Simplification (human/ChatGPT) -> Dependency distance calculation (TextDescriptives) -> Statistical analysis (one-way ANOVA)
- Critical path: Sentence selection → Simplification (human/ChatGPT) → Dependency distance calculation → Statistical analysis
- Design tradeoffs: The study prioritized syntactic simplification over lexical changes by keeping all words unchanged, which may have limited the potential for dependency distance reduction compared to methods that also modify vocabulary
- Failure signatures: If the dependency distance calculation pipeline fails, the study would not be able to compare syntactic complexity across sentence sets. If the simplification prompt is misinterpreted by ChatGPT, the syntactic structure may not be appropriately simplified
- First 3 experiments:
  1. Verify the dependency distance calculation pipeline by running it on a small set of manually annotated sentences with known dependency structures
  2. Test the ChatGPT simplification prompt with a diverse set of sentences to ensure it consistently produces syntactically simplified output without changing words
  3. Conduct a pilot study with a smaller sample size to estimate effect sizes and determine if the planned sample size provides adequate statistical power for detecting differences between simplification methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does reducing dependency distance consistently improve text comprehension across different populations and text types?
- Basis in paper: [inferred] The study used dependency distance as a metric for text simplification but did not measure comprehension outcomes
- Why unresolved: The paper only examined syntactic complexity metrics without linking them to actual comprehension improvements
- What evidence would resolve it: Empirical studies measuring comprehension scores alongside dependency distance changes across various text types and reader populations

### Open Question 2
- Question: How do different simplification approaches (human vs. AI) affect long-term knowledge retention of biomedical information?
- Basis in paper: [inferred] The study compared human and ChatGPT simplification methods but only analyzed syntactic complexity, not retention outcomes
- Why unresolved: The research focused on immediate syntactic changes without examining whether these changes translate to better long-term learning
- What evidence would resolve it: Longitudinal studies comparing knowledge retention rates between human-simplified, AI-simplified, and original biomedical texts

### Open Question 3
- Question: What specific syntactic structures contribute most to increased dependency distance in biomedical texts?
- Basis in paper: [explicit] The study measured overall dependency distance but did not analyze which specific grammatical structures were most problematic
- Why unresolved: The analysis only provided aggregate dependency distance measurements without breaking down contributions by specific syntactic features
- What evidence would resolve it: Detailed syntactic analysis identifying which grammatical structures (e.g., relative clauses, passive voice, coordination) most increase dependency distance in biomedical texts

## Limitations
- Sample size may not provide sufficient statistical power to detect meaningful differences between simplification methods
- Focus on syntactic simplification only, without lexical changes, may not reflect real-world simplification scenarios
- Domain-specific focus on biomedical text from Wikipedia limits generalizability to other text types

## Confidence
- High confidence in the methodology: The study employed established tools (TextDescriptives, spaCy) and clear statistical analysis (one-way ANOVA)
- Medium confidence in the results: While the methodology is sound, the lack of statistical significance and the specific domain focus reduce confidence in broader claims about human vs. ChatGPT superiority
- Low confidence in generalizability: The study's findings may not apply to other text types, domains, or simplification approaches that include lexical changes

## Next Checks
1. Conduct power analysis with the current effect size to determine if the sample size is adequate, or perform a replication study with increased sample size (e.g., 500+ sentences) to achieve statistical significance.
2. Extend the study to include lexical simplification alongside syntactic changes, comparing human and ChatGPT performance across both dimensions of simplification.
3. Replicate the study across multiple domains (e.g., news articles, academic writing, social media) to assess whether the observed trends hold beyond biomedical text from Wikipedia.