---
ver: rpa2
title: 'TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese'
arxiv_id: '2401.16640'
source_url: https://arxiv.org/abs/2401.16640
tags:
- arxiv
- language
- training
- preprint
- portuguese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces TeenyTinyLlama, a pair of compact language
  models (160M and 460M parameters) trained from scratch in Brazilian Portuguese.
  The models were developed under a limited computational budget of 500 USD and trained
  on a dataset of approximately 6.2 billion tokens, including web data and instruction-following
  demonstrations.
---

# TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese

## Quick Facts
- arXiv ID: 2401.16640
- Source URL: https://arxiv.org/abs/2401.16640
- Authors: Nicholas Kluge Corrêa; Sophia Falk; Shiza Fatimah; Aniket Sen; Nythamar de Oliveira
- Reference count: 14
- Primary result: Developed 160M and 460M parameter language models for Brazilian Portuguese under $500 budget, achieving competitive performance on multilingual benchmarks

## Executive Summary
This study introduces TeenyTinyLlama, a pair of compact language models (160M and 460M parameters) trained from scratch in Brazilian Portuguese. The models were developed under a limited computational budget of 500 USD and trained on a dataset of approximately 6.2 billion tokens, including web data and instruction-following demonstrations. Using a Llama 2-based architecture and a custom tokenizer, the models achieved perplexity scores comparable to larger models of similar size. Evaluations on multilingual benchmarks and downstream tasks showed competitive performance, with the 460M model outperforming other models in the ARC-Challenge.

## Method Summary
The study trained two compact language models from scratch using a Llama 2-based architecture with custom SentencePiece tokenizers (32K vocabulary) specifically for Brazilian Portuguese. The 160M parameter model was trained on 3.5 billion tokens while the 460M parameter model used 9.5 billion tokens, following Hoffmann et al. (2022) scaling laws. Training utilized the Hugging Face ecosystem on a single NVIDIA A100-SXM4-40GB GPU with mixed precision (bfloat16) and gradient accumulation. The dataset combined 60% plain Brazilian Portuguese web data with 40% instruction-following demonstrations from multiple sources.

## Key Results
- Models achieved perplexity scores of 2.66 (160M) and 2.57 (460M) on validation data
- The 460M model outperformed other models in the ARC-Challenge benchmark
- Custom tokenizer showed 66% efficiency improvement compared to standard Llama 2 tokenizer
- Total training emissions equivalent to a 185-kilometer car ride

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training compact language models from scratch in low-resource languages is feasible with minimal budget if model size and dataset size are scaled down proportionally.
- Mechanism: The study applies Hoffmann et al. (2022) scaling laws to estimate the optimal dataset size for smaller models, allowing efficient pre-training within budget constraints.
- Core assumption: Scaling laws (Hoffmann et al., 2022) provide reasonable predictions for smaller models trained on limited data.
- Evidence anchors:
  - [abstract]: "models were developed under a limited computational budget of 500 USD and trained on a dataset of approximately 6.2 billion tokens"
  - [section 3.1]: "According to Hoffmann et al. (2022), we can model language modeling loss... we estimated an optimal dataset size for two models: 3.5 and 9.5 billion tokens for 160 and 460 million parameter models, respectively"
  - [corpus]: Corpus neighbors show related work on low-resource Brazilian Portuguese models, but no direct scaling law validation studies found.
- Break condition: Scaling laws fail for very small models or when dataset quality is poor, leading to undertraining or suboptimal performance.

### Mechanism 2
- Claim: Using a custom tokenizer trained on the target language improves encoding efficiency and model performance compared to reusing multilingual tokenizers.
- Mechanism: A SentencePiece tokenizer with 32K vocabulary is trained on Brazilian Portuguese text, reducing token count per word and improving computational efficiency.
- Core assumption: A language-specific tokenizer can encode the target language more efficiently than multilingual tokenizers.
- Evidence anchors:
  - [section 3.3]: "our tokenizer shows a 66% improvement in efficiency compared to the original Llama 2 tokenizer, allowing for a more efficient way to encode Brazilian Portuguese text"
  - [section 2]: Discussion of tokenizer inefficiency in multilingual models and need for adaptation
  - [corpus]: No direct tokenizer efficiency studies found in corpus, but related work on Brazilian Portuguese models exists.
- Break condition: If tokenizer training data is insufficient or unrepresentative, efficiency gains may not materialize.

### Mechanism 3
- Claim: Including instruction-following demonstrations in the pre-training corpus improves downstream task performance and model utility.
- Mechanism: The dataset combines plain Brazilian Portuguese text (60%) with instruction-following demonstrations (40%), exposing the model to task-oriented language during pre-training.
- Core assumption: Exposure to instruction-following data during pre-training leads to better generalization on downstream tasks.
- Evidence anchors:
  - [section 3.2]: "models fine-tuned with demonstrations of instruction-following behavior perform better in many downstream tasks" and "we utilized the following datasets: Instruct-PTBR, Gpt4all-J, Bactrian-X, Dolly 15K, and CosmosQA"
  - [section 4.1]: "our mixed dataset, which contains many demonstrations of instruction following and Q&A" may explain competitive performance
  - [corpus]: Related work on instruction tuning exists but no direct evidence of pre-training with instruction data found.
- Break condition: If instruction data quality is poor or misaligned with target tasks, performance may degrade.

## Foundational Learning

- Concept: Scaling laws for language models
  - Why needed here: To estimate optimal dataset size and model parameters for efficient training within budget constraints
  - Quick check question: Given a 460M parameter model, what dataset size would Hoffmann et al. (2022) scaling laws predict as optimal?

- Concept: Tokenizer efficiency and vocabulary design
  - Why needed here: To reduce computational cost and improve model performance by encoding the target language more efficiently
  - Quick check question: If a word is encoded with 2 tokens using the custom tokenizer versus 6 tokens with the original, what is the efficiency improvement percentage?

- Concept: Instruction tuning and pre-training data composition
  - Why needed here: To improve model utility on downstream tasks by exposing it to task-oriented language during pre-training
  - Quick check question: What proportion of the pre-training dataset is dedicated to instruction-following demonstrations in this study?

## Architecture Onboarding

- Component map: Base architecture (Llama 2) -> Custom tokenizer (SentencePiece, 32K) -> Training pipeline (Hugging Face) -> Hardware (NVIDIA A100) -> Optimization (bfloat16, flash attention)

- Critical path: 1. Dataset preparation and tokenization 2. Model architecture initialization with custom tokenizer 3. Training configuration optimization 4. Training execution with checkpoint saving 5. Evaluation on multilingual benchmarks 6. Fine-tuning for chat capabilities

- Design tradeoffs:
  - Model size vs. training budget: Smaller models enable training within $500 budget but may be undertrained
  - Dataset size vs. quality: 6.2B tokens collected from multiple sources to balance quantity and quality
  - Custom tokenizer vs. reuse: 66% efficiency improvement justifies training effort
  - Pre-training vs. fine-tuning: Including instruction data in pre-training vs. separate fine-tuning phase

- Failure signatures: Training loss curves show high variance or divergence, evaluation perplexity scores plateau or degrade, tokenization inefficiency leads to out-of-vocabulary tokens, model produces hallucinations or toxic content, hardware utilization drops below expected thresholds

- First 3 experiments:
  1. Test custom tokenizer efficiency: Encode 7400 words and compare token count against baseline
  2. Optimize training configuration: Test different mixed precision strategies and gradient accumulation steps
  3. Validate scaling law predictions: Train smallest model with estimated optimal dataset size and measure perplexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Chinchilla scaling law accurately predict the optimal training dataset size for small language models in low-resource languages?
- Basis in paper: [explicit] The authors note that their models appear undertrained and suggest that the Chinchilla scaling laws may not be well-suited for estimating the performance of small language models, as the models showed consistent improvement even when trained beyond the optimal point estimated by the scaling laws.
- Why unresolved: The authors only tested their models up to 9.5 billion tokens, which may not be sufficient to fully assess the applicability of the scaling laws for smaller models. Additionally, the limited computational budget prevented further exploration of larger datasets.
- What evidence would resolve it: Training the same models on significantly larger datasets (e.g., 20+ billion tokens) and observing whether performance continues to improve or plateaus would help determine if the scaling laws are accurate for small models in low-resource settings.

### Open Question 2
- Question: How does the performance of open-source, pre-trained models in low-resource languages compare to fine-tuned models based on large, multilingual LLMs when evaluated on language-specific benchmarks?
- Basis in paper: [explicit] The authors highlight the lack of standardized benchmarks for Brazilian Portuguese and compare their pre-trained models to fine-tuned Llama-based models (e.g., Bode, Sabiá, Canarim) using general multilingual benchmarks, finding competitive performance. However, they emphasize the need for language-specific evaluations.
- Why unresolved: Existing benchmarks like Poeta lack reproducible implementations, and the authors only performed limited fine-tuning comparisons on downstream tasks. There is no direct comparison of pre-trained vs. fine-tuned models on comprehensive, language-specific benchmarks.
- What evidence would resolve it: Developing and applying standardized, language-specific benchmarks to both pre-trained and fine-tuned models would allow for a direct comparison of their capabilities in low-resource languages.

### Open Question 3
- Question: What is the impact of dataset composition (e.g., proportion of instructional vs. general text) on the downstream capabilities and alignment of small language models in low-resource languages?
- Basis in paper: [explicit] The authors intentionally included 40% instructional data in their dataset, hypothesizing it would improve downstream performance. Their results show competitive performance on instruction-following tasks, but they do not isolate the impact of this compositional choice.
- Why unresolved: The authors did not train ablations of their models on purely general text datasets, nor did they systematically vary the proportion of instructional data to measure its effect on model capabilities.
- What evidence would resolve it: Training multiple versions of the same model on datasets with varying proportions of instructional data (e.g., 0%, 20%, 40%, 60%, 80%, 100%) and evaluating their performance on a range of downstream tasks would quantify the impact of dataset composition.

## Limitations

- Scaling laws may not accurately predict optimal dataset sizes for very small models (160M-460M parameters), as the models showed consistent improvement beyond the estimated optimal points
- Limited computational budget prevented exploration of larger datasets that might reveal whether the models are undertrained
- Lack of standardized Brazilian Portuguese benchmarks makes comprehensive evaluation difficult, relying instead on general multilingual benchmarks

## Confidence

- **High Confidence**: Computational budget achievement ($500 total cost), hardware specifications, and basic model architecture implementation
- **Medium Confidence**: Perplexity scores and multilingual benchmark performance comparisons, given evaluation methodology is standard but limited to available benchmarks
- **Low Confidence**: Claims about tokenizer efficiency improvements and scaling law predictions for tiny models, as these rely on extrapolations beyond validated ranges

## Next Checks

1. **Scaling Law Validation**: Train a series of models with parameter sizes ranging from 100M to 1B parameters using varying dataset sizes (1B-20B tokens) to empirically validate whether Hoffmann et al. (2022) scaling laws hold for tiny models on Brazilian Portuguese data.

2. **Tokenizer Efficiency Benchmarking**: Compare the custom 32K SentencePiece tokenizer against alternative tokenization approaches (Byte-Pair Encoding, WordPiece) across multiple Portuguese text domains (news, social media, technical documents) to verify the claimed 66% efficiency improvement holds consistently.

3. **Cross-Lingual Transfer Evaluation**: Test model performance on closely related languages (European Portuguese, Spanish) to assess whether the compact architecture and training approach enables effective zero-shot or few-shot cross-lingual transfer capabilities.