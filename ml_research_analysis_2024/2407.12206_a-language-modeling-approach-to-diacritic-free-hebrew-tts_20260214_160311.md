---
ver: rpa2
title: A Language Modeling Approach to Diacritic-Free Hebrew TTS
arxiv_id: '2407.12206'
source_url: https://arxiv.org/abs/2407.12206
tags:
- speech
- hebrew
- text
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Hebrew text-to-speech (TTS)
  synthesis, where modern Hebrew lacks diacritics that indicate pronunciation, making
  it difficult for TTS systems to accurately map text to speech. The authors propose
  a language modeling approach that operates on discrete speech representations and
  is conditioned on a word-piece tokenizer.
---

# A Language Modeling Approach to Diacritic-Free Hebrew TTS

## Quick Facts
- **arXiv ID**: 2407.12206
- **Source URL**: https://arxiv.org/abs/2407.12206
- **Reference count**: 0
- **Primary result**: Achieves WER of 0.19 and CER of 0.08 on Hebrew TTS without diacritics

## Executive Summary
This paper addresses the challenge of Hebrew text-to-speech synthesis where modern Hebrew lacks diacritics that indicate pronunciation. The authors propose a language modeling approach that operates on discrete speech representations and is conditioned on a word-piece tokenizer. The model is optimized using weakly supervised data from in-the-wild recordings. The proposed method outperforms baseline systems in terms of content preservation, naturalness, and speaker similarity, achieving state-of-the-art results for diacritic-free Hebrew TTS.

## Method Summary
The authors propose a language modeling approach for Hebrew TTS that uses word-piece tokenization and operates on discrete speech representations. The model is trained on weakly supervised data from in-the-wild recordings, specifically using the ivrit.ai and HebDB datasets. The text is tokenized using a word-piece tokenizer, and the audio is encoded into discrete speech representations using Residual Vector Quantization (RVQ). The model uses an autoregressive approach for the first codebook and a non-autoregressive approach for the remaining codebooks, enabling efficient and accurate speech synthesis without relying on diacritics.

## Key Results
- Achieves Word Error Rate (WER) of 0.19 and Character Error Rate (CER) of 0.08
- Naturalness score of 4.17 and content preservation score of 4.44 on a 5-point scale
- Outperforms baseline systems MMS and Overflow in content preservation and naturalness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models operating over discrete speech tokens can resolve pronunciation ambiguities in diacritic-free Hebrew text by leveraging contextual information.
- Mechanism: The LM is trained to predict acoustic tokens conditioned on word-piece tokenized text. This allows the model to use context to disambiguate between words that have the same characters but different pronunciations.
- Core assumption: The discrete speech representation captures enough phonetic information to reconstruct accurate pronunciation when combined with contextual text input.
- Evidence anchors: The proposed method is superior to evaluated baselines considering both content preservation and naturalness of generated speech.

### Mechanism 2
- Claim: Word-piece tokenization provides better text representation for Hebrew TTS than character-level tokenization by balancing granularity and context.
- Mechanism: Word-piece tokenization splits text into subword units that preserve meaningful morphological information while being more compact than character sequences.
- Core assumption: Word-piece tokenization preserves the morphological richness of Hebrew better than character-level tokenization.
- Evidence anchors: Results suggest that following the word-piece tokenizer provides superior performance to the character-based alternative.

### Mechanism 3
- Claim: Training on weakly supervised, large-scale in-the-wild data improves robustness and naturalness of Hebrew TTS compared to small, high-quality datasets.
- Mechanism: The model learns from diverse real-world speech patterns, accents, and speaking styles present in podcast data.
- Core assumption: The ASR-generated transcriptions, despite their errors, provide sufficient alignment between text and speech for the LM to learn meaningful mappings.
- Evidence anchors: Results suggest the proposed method is superior to evaluated baselines considering both content preservation and naturalness.

## Foundational Learning

- **Concept**: Discrete speech representation learning
  - Why needed here: The model needs to operate on discrete tokens rather than raw waveforms or spectrograms to enable efficient language modeling
  - Quick check question: What is the primary advantage of using discrete speech representations over continuous representations in this context?

- **Concept**: Word-piece tokenization
  - Why needed here: Hebrew requires a tokenization method that can handle morphological complexity without diacritics
  - Quick check question: Why might character-level tokenization be less effective than word-piece for Hebrew TTS in this system?

- **Concept**: Language modeling for TTS
  - Why needed here: The autoregressive nature of LMs allows the model to generate coherent speech by predicting one token at a time based on context
  - Quick check question: How does the autoregressive approach differ from non-autoregressive approaches in handling pronunciation ambiguity?

## Architecture Onboarding

- **Component map**: Text tokenizer (word-piece) → LM (autoregressive + non-autoregressive codebooks) → Discrete speech encoder (RVQ) → Vocoder (MBD) → Waveform output
- **Critical path**: Text → Word-piece tokens → LM prediction → Discrete acoustic tokens → Vocoder → Speech
- **Design tradeoffs**: Auto-regressive LM provides better context handling but slower inference vs non-AR models; large weakly-supervised dataset provides robustness but introduces ASR noise; word-piece tokenization balances context and efficiency but requires careful vocabulary design
- **Failure signatures**: High WER/CER indicates pronunciation errors; low speaker similarity suggests poor acoustic modeling; unnatural prosody indicates LM issues with rhythm and intonation
- **First 3 experiments**:
  1. Compare word-piece vs character tokenization with a small subset of data to verify the tokenization advantage
  2. Test LM with and without the non-autoregressive codebook prediction to understand the impact on quality and speed
  3. Evaluate the effect of ASR transcription quality by training on manually transcribed vs automatically transcribed subsets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed language modeling approach handle rare or unseen words in Hebrew text?
- Basis in paper: The paper mentions that the model uses a word-piece tokenizer, but does not explicitly discuss how it handles rare or unseen words.
- Why unresolved: The paper does not provide details on how the model deals with rare or unseen words, which is a common challenge in language modeling.
- What evidence would resolve it: A discussion or experiment demonstrating the model's performance on rare or unseen words in Hebrew text.

### Open Question 2
- Question: How does the proposed method compare to traditional TTS systems that use diacritics when trained on high-quality, diacriticized data?
- Basis in paper: The paper mentions that the proposed method is superior to baseline systems, but does not explicitly compare it to traditional TTS systems using diacritics on high-quality data.
- Why unresolved: The paper focuses on comparing the proposed method to baseline systems trained on weakly supervised data, but does not provide a direct comparison to traditional TTS systems using diacritics on high-quality data.
- What evidence would resolve it: An experiment comparing the proposed method to traditional TTS systems using diacritics on high-quality, diacriticized Hebrew text data.

### Open Question 3
- Question: How does the proposed method handle homographs (words with the same spelling but different pronunciations) in Hebrew text?
- Basis in paper: The paper mentions that modern Hebrew rarely uses diacritics, making it challenging for TTS systems to accurately map text to speech. However, it does not explicitly discuss how the proposed method handles homographs.
- Why unresolved: The paper does not provide details on how the model disambiguates between different pronunciations of homographs in Hebrew text.
- What evidence would resolve it: A discussion or experiment demonstrating the model's performance on homographs in Hebrew text, including examples of correct pronunciation based on context.

## Limitations

- The reliance on weakly supervised data generated through automatic speech recognition (ASR) introduces potential errors that may propagate through the system
- The evaluation relies heavily on human studies for naturalness and content preservation, which introduces subjectivity and potential cultural bias
- The paper doesn't address potential limitations with specific Hebrew morphological phenomena or edge cases that might challenge the word-piece tokenization approach

## Confidence

- **High Confidence**: The core methodology of using language modeling with discrete speech representations for TTS is technically sound and well-established. The quantitative metrics (WER of 0.19, CER of 0.08) are clearly defined and measurable.
- **Medium Confidence**: The claim that word-piece tokenization outperforms character-level tokenization for Hebrew TTS is supported by experimental results, though the analysis could benefit from more detailed ablation studies. The superiority over baseline systems is demonstrated but could be more thoroughly analyzed across different test conditions.
- **Low Confidence**: The generalization claims about leveraging large-scale weakly supervised data are not fully substantiated. The paper doesn't provide sufficient evidence about how the model performs on diverse Hebrew dialects or speaking styles beyond the podcast domain where training data was sourced.

## Next Checks

1. **ASR Quality Analysis**: Conduct a detailed error analysis of the automatically transcribed training data to quantify ASR error rates and identify systematic patterns. This should include comparison of ASR transcription quality across different speakers, topics, and acoustic conditions to understand how transcription noise affects model learning.

2. **Cross-Domain Evaluation**: Test the trained model on Hebrew TTS data from different domains (news broadcasts, conversational speech, formal presentations) to evaluate generalization beyond the podcast domain. Measure degradation in WER, CER, and perceptual quality metrics when moving away from the training distribution.

3. **Tokenization Robustness Test**: Perform an ablation study comparing word-piece tokenization against character-level and byte-pair encoding approaches on challenging Hebrew text samples containing rare words, morphological variations, and ambiguous contexts. This should include measuring performance on text specifically designed to test the limits of subword tokenization for Hebrew morphology.