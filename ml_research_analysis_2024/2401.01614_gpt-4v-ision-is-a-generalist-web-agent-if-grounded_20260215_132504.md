---
ver: rpa2
title: GPT-4V(ision) is a Generalist Web Agent, if Grounded
arxiv_id: '2401.01614'
source_url: https://arxiv.org/abs/2401.01614
tags:
- element
- action
- grounding
- click
- gpt-4v
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using large multimodal models (LMMs) like
  GPT-4V as generalist web agents to follow natural language instructions and complete
  tasks on any website. The authors propose SEEACT, which uses GPT-4V to visually
  perceive websites and generate textual action plans.
---

# GPT-4V(ision) is a Generalist Web Agent, if Grounded

## Quick Facts
- arXiv ID: 2401.01614
- Source URL: https://arxiv.org/abs/2401.01614
- Authors: Boyuan Zheng; Boyu Gou; Jihyung Kil; Huan Sun; Yu Su
- Reference count: 40
- Key outcome: GPT-4V can complete 50% of web tasks on live websites with oracle grounding, outperforming text-only models but facing challenges with automated grounding

## Executive Summary
This paper investigates using large multimodal models (LMMs) like GPT-4V as generalist web agents to follow natural language instructions and complete tasks on any website. The authors propose SEEACT, which uses GPT-4V to visually perceive websites and generate textual action plans. These plans are then grounded to HTML elements and operations to act on the website. They evaluate on the MIND2WEB benchmark with both offline and online settings, demonstrating GPT-4V's potential while highlighting grounding as a key challenge.

## Method Summary
The SEEACT framework uses GPT-4V to generate action descriptions based on visual perception of webpages, which are then grounded to specific HTML elements for execution. The method combines visual screenshots with natural language reasoning about tasks and previous actions. Three grounding strategies are explored: element attributes, textual choices, and image annotation. The approach is evaluated on the MIND2WEB benchmark using both offline cached websites and online live websites, with performance measured through success rates and step-wise metrics.

## Key Results
- GPT-4V achieves 50% task success rate on live websites with oracle grounding, outperforming text-only models
- Textual choice grounding demonstrates best performance across all metrics and settings
- Grounding remains a significant challenge, with 20-25% gap between best automated grounding and oracle performance
- GPT-4V's image-only grounding underperforms by up to 30% compared to methods leveraging both HTML and visual information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V can generate correct action descriptions by visually perceiving webpages and reasoning about the task.
- Mechanism: The model leverages visual perception of the rendered webpage combined with natural language reasoning about the task and previous actions to generate step-by-step action descriptions.
- Core assumption: The rendered webpage screenshot provides sufficient visual information for the model to understand the page structure and identify target elements.
- Evidence anchors:
  - [abstract] "GPT-4V presents a great potential for web agentsâ€”it can successfully complete 50% of the tasks on live websites if we manually ground its textual plans into actions on the websites."
  - [section 2.2] "We explicitly instruct GPT-4V to imitate humans browsing a webpage and analyze the task, webpage, and previous actions."
  - [corpus] Weak evidence - no direct mentions of GPT-4V action generation capabilities
- Break condition: If the webpage contains complex interactive elements or dense layouts that are difficult to interpret from screenshots alone.

### Mechanism 2
- Claim: Grounding via textual choices outperforms image annotation because it avoids hallucination issues in complex webpage layouts.
- Mechanism: By providing the model with textual representations of candidate HTML elements as choices, it can select the intended element without relying on interpreting visual bounding boxes and labels.
- Core assumption: The textual descriptions of HTML elements provide sufficient distinguishing information for the model to identify the correct element.
- Evidence anchors:
  - [section 4.1] "Element grounding via textual choice (SEEACTChoice) demonstrates the best performance under all metrics across all settings."
  - [section 4.3] "However, on complex images with rich semantic and spacial relationships like webpage screenshots, severe hallucination is observed from GPT-4V."
  - [corpus] No direct evidence - need to investigate hallucination issues in other LMM applications
- Break condition: If multiple candidate elements have very similar or identical textual descriptions, making it difficult to distinguish the correct one.

### Mechanism 3
- Claim: In-context learning with large models (GPT-4V and GPT-4) shows better generalization to unseen websites compared to supervised fine-tuning.
- Mechanism: The large models can leverage their pre-existing knowledge and reasoning capabilities to adapt to new web environments without task-specific training.
- Core assumption: The pre-training data of large models includes sufficient web-related content to enable effective in-context learning on new websites.
- Evidence anchors:
  - [section 4.1] "ICL (with SEEACT) demonstrate consistent and robust performance across three test splits. ICL is particularly advantageous in scenarios lacking annotations or requiring strong generalization capabilities for new domains and websites."
  - [section 4.2] "Using oracle grounding further improves the performance substantially, reaching a remarkable whole task success rate of 50%."
  - [corpus] No direct evidence - need to investigate generalization capabilities of LMMs on other domains
- Break condition: If the new websites have significantly different layouts, interaction patterns, or domain-specific terminology not covered in pre-training.

## Foundational Learning

- Concept: Web page structure and HTML DOM
  - Why needed here: Understanding the relationship between HTML elements and their visual rendering is crucial for effective grounding of action descriptions.
  - Quick check question: How does the HTML structure influence the visual layout of a webpage, and how can this knowledge be leveraged for grounding actions?

- Concept: Multimodal reasoning and grounding
  - Why needed here: Integrating visual and textual information to generate and ground actions requires understanding how LMMs process and reason with multimodal inputs.
  - Quick check question: What are the key challenges in grounding natural language instructions to visual elements in complex images, and how can these be addressed?

- Concept: In-context learning and few-shot prompting
- Why needed here: Effectively leveraging the few-shot learning capabilities of large models is essential for adapting to new web environments without task-specific training.
  - Quick check question: How can in-context learning be used to improve the performance of large models on web navigation tasks, and what are the key considerations for designing effective prompts?

## Architecture Onboarding

- Component map: Screenshot -> GPT-4V action generation -> Grounding module -> HTML DOM ranker -> Browser automation tool -> Updated webpage state

- Critical path:
  1. Capture screenshot of current webpage
  2. Generate action description using GPT-4V
  3. Ground action description to specific HTML element using chosen grounding method
  4. Execute action using browser automation tool
  5. Update webpage state and repeat until task completion

- Design tradeoffs:
  - Using visual perception vs. raw HTML input: Visual perception allows for more natural interaction but requires grounding to HTML elements for execution.
  - Textual choices vs. image annotation for grounding: Textual choices are more reliable but may struggle with similar elements, while image annotation is more intuitive but prone to hallucination.
  - In-context learning vs. supervised fine-tuning: In-context learning enables better generalization but may have lower performance on seen websites compared to fine-tuning.

- Failure signatures:
  - Action generation failures: Incorrect or incomplete action descriptions due to misunderstanding of webpage or task.
  - Grounding failures: Incorrect element selection due to hallucination, similar elements, or complex layouts.
  - Execution failures: Incompatibility between grounded action and browser automation tool or webpage structure.

- First 3 experiments:
  1. Evaluate action generation performance on a small set of webpages with oracle grounding to isolate the impact of visual perception.
  2. Compare grounding performance of textual choices vs. image annotation on webpages with varying complexity and element similarity.
  3. Test in-context learning capabilities of GPT-4V vs. GPT-4 on a set of unseen websites to assess generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the grounding performance of LMMs like GPT-4V compare to specialized vision-language models fine-tuned on web navigation data?
- Basis in paper: [explicit] The paper mentions that BLIP2-T5, a vision-language model fine-tuned on MIND2WEB, does not show noticeable improvement over FLAN-T5, but the reasons for this are explored.
- Why unresolved: The paper does not provide a direct comparison of GPT-4V's grounding performance against specialized vision-language models fine-tuned on web navigation data.
- What evidence would resolve it: A direct comparison of GPT-4V's grounding performance with specialized vision-language models fine-tuned on web navigation data, using the same evaluation metrics and datasets.

### Open Question 2
- Question: What are the specific limitations of GPT-4V in understanding webpage layouts and element relationships, and how can these be addressed?
- Basis in paper: [inferred] The paper discusses challenges with grounding via image annotation due to GPT-4V's limitations in understanding image details and relative spatial locations.
- Why unresolved: The paper does not provide a detailed analysis of GPT-4V's specific limitations in understanding webpage layouts and element relationships.
- What evidence would resolve it: A detailed analysis of GPT-4V's performance on various webpage layouts and element relationships, along with proposed solutions to address these limitations.

### Open Question 3
- Question: How does the performance of LMMs like GPT-4V on web navigation tasks vary across different website types and complexity levels?
- Basis in paper: [inferred] The paper evaluates GPT-4V on the MIND2WEB benchmark, which includes diverse websites and tasks, but does not provide a detailed analysis of performance variations across different website types and complexity levels.
- Why unresolved: The paper does not provide a detailed breakdown of GPT-4V's performance across different website types and complexity levels.
- What evidence would resolve it: A detailed analysis of GPT-4V's performance on various website types and complexity levels, along with insights into the factors contributing to performance variations.

## Limitations

- The 50% success rate on live websites relies on oracle grounding, which is not practically achievable - the true performance with automated grounding remains 25-30%, representing a substantial capability gap
- The evaluation focuses on single-turn tasks without complex reasoning chains, potentially underestimating real-world challenges
- GPT-4V action generation was only evaluated in offline settings, leaving the complete agent pipeline validation incomplete

## Confidence

- High confidence: The performance gap between grounding strategies is real and measurable; textual choice grounding demonstrably outperforms image annotation across all metrics
- Medium confidence: The claimed 50% success rate requires clarification that it depends on oracle grounding; actual automated grounding achieves substantially lower performance
- Medium confidence: In-context learning generalization claims are supported by split experiments but would benefit from evaluation on entirely unseen website categories

## Next Checks

1. Implement end-to-end evaluation combining GPT-4V action generation with automated grounding on live websites to establish realistic performance baselines
2. Test grounding performance on websites with intentionally similar elements to quantify the practical limits of textual choice methods
3. Evaluate GPT-4V action generation on multi-turn tasks requiring sequential reasoning and state tracking to assess scalability beyond single-step instructions