---
ver: rpa2
title: 'Rethinking Open-World Semi-Supervised Learning: Distribution Mismatch and
  Inductive Inference'
arxiv_id: '2405.20829'
source_url: https://arxiv.org/abs/2405.20829
tags:
- learning
- class
- classes
- novel
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses practical challenges in open-world semi-supervised
  learning (OWSSL), focusing on two main limitations: class prior distribution mismatch
  between labeled and unlabeled datasets (often following a long-tailed distribution),
  and the inadequacy of transductive inference for real-world applications. To tackle
  these issues, the authors propose a realistic open-world semi-supervised learning
  (ROWSSL) framework and introduce a novel method called Density-based Temperature
  scaling and Soft pseudo-labeling (DTS).'
---

# Rethinking Open-World Semi-Supervised Learning: Distribution Mismatch and Inductive Inference

## Quick Facts
- arXiv ID: 2405.20829
- Source URL: https://arxiv.org/abs/2405.20829
- Authors: Seongheon Park; Hyuk Kwon; Kwanghoon Sohn; Kibok Lee
- Reference count: 40
- Primary result: DTS achieves 66.6% overall accuracy in transductive inference and 53.1% in inductive inference on CIFAR-100-LT with distribution mismatch

## Executive Summary
This paper addresses practical limitations in open-world semi-supervised learning (OWSSL), specifically focusing on class prior distribution mismatch between labeled and unlabeled datasets (often following long-tailed distributions) and the inadequacy of transductive inference for real-world applications. The authors propose a realistic open-world semi-supervised learning (ROWSSL) framework and introduce Density-based Temperature scaling and Soft pseudo-labeling (DTS) to tackle these challenges. DTS employs dynamic temperature scaling for contrastive learning to achieve balanced representations and class uncertainty-aware soft pseudo-labeling to reduce classifier bias towards head and known classes.

## Method Summary
The proposed DTS method addresses ROWSSL challenges through two main innovations: (1) Dynamic temperature scaling in contrastive learning, where the temperature parameter τ is adjusted based on each sample's "tailedness" score - lower τ for tail classes to encourage instance-specific features and higher τ for head classes to preserve local semantic structure; (2) Class uncertainty-aware soft pseudo-labeling, where pseudo-labels are adjusted based on class uncertainty measured as the standard deviation of tailedness scores among samples within each class. The method uses ViT-B/16 backbone pre-trained with DINO weights, maintains a queue for contrastive learning, and learns tailedness prototypes through k-means clustering updated via exponential moving average.

## Key Results
- On CIFAR-100-LT with distribution mismatch, DTS achieves 66.6% overall accuracy in transductive inference compared to BaCon's 56.0%
- In inductive inference setting, DTS reaches 53.1% balanced overall accuracy versus BaCon's 42.8%
- DTS shows consistent improvement on Herbarium19 dataset in both transductive and inductive settings
- Significant performance gains observed for novel and tail classes, addressing the classifier bias issue

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic temperature scaling in contrastive learning adjusts learning focus between head and tail classes, improving class balance
- Mechanism: Temperature parameter τ is dynamically adjusted based on each sample's tailedness score - samples with higher tailedness (tail classes) receive lower τ to learn instance-specific features, while samples with lower tailedness (head classes) receive higher τ to preserve local semantic structure
- Core assumption: Tailedness can be effectively estimated from density in representation space and correlates with learning difficulty
- Evidence anchors: [abstract] density estimation on representation space for tailedness proxy; [section 3.2] tail classes sparsely distribute on representation space due to lower intra-class consistency

### Mechanism 2
- Claim: Class uncertainty-aware soft pseudo-labeling reduces classifier bias towards known and head classes
- Mechanism: Soft pseudo-labels are adjusted based on class uncertainty measured as standard deviation of tailedness scores among samples within each class, giving more weight to classes with higher uncertainty
- Core assumption: Class uncertainty measured by standard deviation of tailedness scores is a reliable indicator of learning difficulty
- Evidence anchors: [abstract] class uncertainty-aware soft pseudo-labeling using density variance as uncertainty measure; [section 3.3] temperature parameter adjusted as function of anchor's tailedness scores

### Mechanism 3
- Claim: Tailedness prototypes provide stable and efficient proxy for class prior distribution in ROWSSL
- Mechanism: Tailedness prototypes are learned through k-means clustering on features from queue maintained in contrastive learning branch, updated using exponential moving average to reflect local density in representation space
- Core assumption: Density of samples in representation space can effectively approximate class prior distribution even when true class prior is unknown
- Evidence anchors: [section 3.2] queue utilized as neighbors in entire dataset cannot be captured by mini-batch; [section 3.2] tailedness prototypes aim to explore stable and efficient proxies for tail class samples

## Foundational Learning

- Concept: Density estimation in representation space
  - Why needed here: To estimate tailedness of classes, crucial for adjusting learning focus and mitigating classifier bias
  - Quick check question: How does density of samples in representation space relate to difficulty of learning each class?

- Concept: Contrastive learning and temperature scaling
  - Why needed here: To learn discriminative features and balance representation of head and tail classes
  - Quick check question: How does temperature parameter in contrastive learning affect learning dynamics?

- Concept: Pseudo-labeling and uncertainty estimation
  - Why needed here: To leverage unlabeled data for learning and adjust pseudo-labels based on class uncertainty
  - Quick check question: How can class uncertainty be measured and used to adjust pseudo-labels?

## Architecture Onboarding

- Component map: Encoder E -> Feature vector z -> MLP g -> Lower dimensional vector h -> Contrastive learning loss -> Classifier f -> Classification loss
- Critical path: Encoder -> MLP g -> Contrastive loss -> Encoder -> Classifier f -> Classification loss
- Design tradeoffs: Balancing temperature parameter range for dynamic scaling, choosing number of tailedness prototypes, tuning class uncertainty adjustment factor
- Failure signatures: Poor performance on novel and tail classes indicating ineffective density estimation or temperature scaling; classifier bias towards known and head classes indicating ineffective pseudo-labeling adjustment
- First 3 experiments:
  1. Ablation study on dynamic temperature scaling: Compare performance with and without dynamic temperature scaling on CIFAR-100-LT
  2. Ablation study on class uncertainty-aware pseudo-labeling: Compare performance with and without class uncertainty adjustment on CIFAR-100-LT
  3. Hyperparameter sensitivity analysis: Investigate impact of number of tailedness prototypes, τmin and τmax, and λvar on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ROWSSL be extended to handle domain shifts between labeled and unlabeled datasets?
- Basis in paper: [inferred] Authors acknowledge this as limitation noting labeled and unlabeled data sampled from same dataset in experiments, which might not reflect real-world scenarios with domain shifts
- Why unresolved: Paper provides no experiments or analysis on impact of domain shifts on ROWSSL performance
- What evidence would resolve it: Experiments comparing ROWSSL performance on datasets with and without domain shifts, or analysis of model's robustness to domain shift

### Open Question 2
- Question: How can number of novel classes be accurately estimated in presence of imbalanced class priors?
- Basis in paper: [inferred] Authors mention estimating number of novel classes using off-the-shelf methods can result in inaccurate predictions due to imbalanced class priors
- Why unresolved: Paper proposes no solution or conducts experiments to evaluate accuracy of novel class number estimation in imbalanced settings
- What evidence would resolve it: Novel class number estimation method specifically designed for imbalanced datasets, along with experiments comparing its accuracy to existing methods

### Open Question 3
- Question: How can proposed DTS method be adapted to handle open-set recognition scenarios where unknown classes may appear during inference?
- Basis in paper: [explicit] Authors discuss limitations of DTS and existing methods including assumption that labeled and unlabeled data are sampled from same dataset, which might not reflect real-world scenarios with unknown classes during inference
- Why unresolved: Paper focuses on ROWSSL setting and does not explore open-set recognition scenario
- What evidence would resolve it: Experiments evaluating performance of DTS on datasets with unknown classes during inference, or modified version of DTS specifically designed for open-set recognition

## Limitations

- The paper assumes density estimation in representation space accurately captures tailedness without thorough validation of this assumption
- Dynamic temperature scaling depends heavily on quality of tailedness prototypes, but paper doesn't provide extensive analysis of how these prototypes evolve during training
- Evaluation focuses primarily on CIFAR-100-LT and Herbarium19 datasets, limiting understanding of method's effectiveness on other long-tailed datasets or different domain types

## Confidence

- **High Confidence**: Core methodology of dynamic temperature scaling and class uncertainty-aware pseudo-labeling is well-justified and technically sound, addressing genuine limitations in current OWSSL approaches
- **Medium Confidence**: Experimental results showing performance improvements are promising, but ablation studies could be more comprehensive and hyperparameter sensitivity analysis could be better documented
- **Low Confidence**: Theoretical guarantees for why density-based tailedness estimation works in practice are not rigorously established, and paper doesn't explore failure cases or provide formal bounds on performance

## Next Checks

1. **Ablation Study Extension**: Conduct additional ablation studies varying K parameter in K-nearest neighbors density estimation (e.g., K=5, 10, 20, 25) to understand impact on performance across different tail class severities

2. **Cross-Dataset Validation**: Apply DTS method to at least two additional long-tailed datasets from different domains (e.g., ImageNet-LT and text classification dataset) to test generalizability beyond computer vision

3. **Uncertainty Measure Comparison**: Implement and compare proposed tailedness-based uncertainty measure against established uncertainty estimation methods (e.g., entropy-based, ensemble-based) to validate effectiveness in reducing classifier bias