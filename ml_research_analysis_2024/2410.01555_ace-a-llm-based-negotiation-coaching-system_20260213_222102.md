---
ver: rpa2
title: 'ACE: A LLM-based Negotiation Coaching System'
arxiv_id: '2410.01555'
source_url: https://arxiv.org/abs/2410.01555
tags:
- negotiation
- feedback
- price
- buyer
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ACE, an LLM-based negotiation coaching system
  designed to democratize access to negotiation training. ACE provides users with
  targeted feedback by identifying errors in their negotiation strategies and language,
  using an annotation scheme developed from expert consultations and a dataset of
  MBA student negotiations.
---

# ACE: A LLM-based Negotiation Coaching System

## Quick Facts
- arXiv ID: 2410.01555
- Source URL: https://arxiv.org/abs/2410.01555
- Reference count: 40
- Users receiving ACE feedback significantly improved negotiation performance compared to control conditions

## Executive Summary
ACE is an LLM-based negotiation coaching system that democratizes access to negotiation training by providing targeted feedback on user errors. The system identifies tactical mistakes in real-time during negotiations and delivers corrective feedback with revised utterances, enabling users to align their behavior with expert-defined negotiation strategies. A user experiment with 374 participants across two negotiation trials demonstrated that ACE significantly improved both objective (deal price) and subjective (perceived improvement) negotiation outcomes compared to systems without feedback or using alternative feedback methods.

## Method Summary
ACE uses an LLM-based system with an annotation scheme to identify negotiation errors and provide targeted feedback. The system requires a dataset of negotiation transcripts and expert consultations to develop error categories. Users negotiate with a prompt-based GPT-4 agent simulator, receiving preparation feedback, turn-based error corrections, and holistic feedback on linguistic style. The system evaluates effectiveness through user experiments measuring deal prices and subjective perceptions across multiple negotiation trials.

## Key Results
- ACE significantly improved negotiation performance in both objective (deal price) and subjective (perceived improvement) measures
- Participants in ACE condition showed more negotiation turns and longer duration in second trial, indicating increased engagement
- The system achieves high error detection accuracy (at least 0.90) for all error categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ACE improves negotiation performance by providing targeted, tactical feedback on specific user errors.
- Mechanism: The system identifies user errors in real-time during negotiation turns and delivers corrective feedback with revised utterances. This aligns user behavior with expert-defined negotiation strategies.
- Core assumption: Users can improve negotiation outcomes by correcting specific tactical errors in language and strategy.
- Evidence anchors:
  - [abstract] ACE employs this scheme to identify mistakes and provide targeted feedback to users. [...] We found that ACE significantly improves learning outcomes compared to a system that doesn't provide feedback.
  - [section] The system can identify mistakes with a high accuracy of at least 0.90 for all error categories. [...] Participants in the ACE condition had more negotiation turns and longer negotiation duration in the second trial.
  - [corpus] Weak; corpus includes general bargaining and negotiation systems but no specific study on targeted tactical feedback in negotiation coaching.
- Break condition: If error detection accuracy falls below ~0.85, feedback quality degrades and user improvement diminishes.

### Mechanism 2
- Claim: ACE's preparation feedback corrects strategic pricing errors before negotiation begins.
- Mechanism: Users input target, walk-away, and opening prices; ACE flags non-strategic choices and provides corrective guidance based on negotiation scenario constraints.
- Core assumption: Pre-negotiation preparation directly impacts negotiation outcomes; correcting strategic errors before negotiation improves performance.
- Evidence anchors:
  - [abstract] [...] We develop an annotation scheme to identify and correct mistakes that users make during their negotiations.
  - [section] The preparation feedback we provide users is based on their answers to pre-negotiation preparation questions. We specifically look for errors corresponding to strategic walk-away point, strategic target price, and giving the first offer in our annotation scheme.
  - [corpus] Weak; corpus lacks studies on pre-negotiation preparation feedback in AI coaching systems.
- Break condition: If preparation errors are not corrected, subsequent negotiation performance shows no significant improvement.

### Mechanism 3
- Claim: ACE's holistic feedback improves user linguistic style (formality, firmness, avoiding apologies) leading to better negotiation outcomes.
- Mechanism: After negotiation, ACE analyzes entire conversation for linguistic attributes and provides feedback on formality, firmness, and apology frequency.
- Core assumption: Linguistic style influences negotiation outcomes; users can improve outcomes by adjusting communication style.
- Evidence anchors:
  - [abstract] ACE uses the annotation categories to identify users' mistakes and then provides targeted feedback based on the error definitions.
  - [section] We prompt the model with a summary of the attributes for these three aspects and have it generate feedback. We have the model quote specific phrases from the users' transcript to make the comments more targeted and personalized.
  - [corpus] Weak; corpus includes general linguistic feedback systems but no specific evidence linking linguistic style feedback to negotiation performance.
- Break condition: If linguistic feedback does not translate to measurable changes in user communication style, no performance improvement occurs.

## Foundational Learning

- Concept: Negotiation preparation strategy
  - Why needed here: Users must set strategic target, walk-away, and opening prices before negotiation begins.
  - Quick check question: What is the mathematical relationship between a buyer's target price and the market range that makes it "strategic"?

- Concept: Price anchoring and first-mover advantage
  - Why needed here: Giving the first offer anchors the negotiation in a favorable position for the user.
  - Quick check question: Why does stating the opening offer first typically benefit the negotiator?

- Concept: Rationale inclusion in offers
  - Why needed here: Providing objective justifications for price offers increases persuasiveness and acceptance likelihood.
  - Quick check question: What types of rationales are most effective when justifying a price offer in negotiation?

## Architecture Onboarding

- Component map: Data collection pipeline → Error detection module → Feedback generation engine → Negotiation agent → User interface
- Critical path: User input → preparation feedback → negotiation with agent → turn-based feedback → holistic feedback → performance measurement
- Design tradeoffs:
  - Real-time vs. batch feedback: Real-time provides immediate correction but increases latency; batch reduces latency but delays learning.
  - Automated vs. expert feedback: Automated scales but may lack nuance; expert feedback is higher quality but doesn't scale.
  - Price extraction accuracy vs. model complexity: More sophisticated models improve accuracy but increase cost and latency.
- Failure signatures:
  - Low F1 scores in error detection (particularly for strategic closing and including rationale)
  - User disengagement during negotiation (agent not providing sufficient counter-pressure)
  - Feedback not being incorporated into subsequent negotiations (learning not transferring)
- First 3 experiments:
  1. Error detection accuracy test: Measure precision/recall on held-out annotated data
  2. Feedback comprehension test: Survey users on understanding of feedback messages
  3. Learning transfer test: Compare performance improvement between ACE and no-feedback conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ACE's performance vary when negotiating in different cultural contexts, given that its current annotation scheme and feedback are based on "American" negotiation styles?
- Basis in paper: [explicit] The paper explicitly mentions that ACE's annotation scheme and feedback method are based on an "American" style of negotiation and may not be as useful for individuals in other cultural contexts.
- Why unresolved: The paper does not provide any data or testing on ACE's effectiveness in non-American cultural settings, leaving a gap in understanding its generalizability.
- What evidence would resolve it: Testing ACE with users from diverse cultural backgrounds and comparing its performance and user satisfaction across these groups would provide evidence of its effectiveness in different cultural contexts.

### Open Question 2
- Question: What is the long-term impact of ACE on users' negotiation skills, and does repeated interaction with the system lead to sustained improvements?
- Basis in paper: [inferred] The paper mentions that ACE does not retain memory of previous user interactions, which may limit its utility for repeated use. This suggests a need to explore the long-term impact of the system.
- Why unresolved: The evaluation only covers two consecutive negotiation trials, which is insufficient to determine if improvements are sustained over time or if repeated interactions enhance learning.
- What evidence would resolve it: Conducting a longitudinal study where users interact with ACE over multiple sessions and tracking their negotiation performance and confidence levels over time would provide insights into the long-term impact.

### Open Question 3
- Question: How effective is ACE in helping users negotiate in scenarios outside of the used car and summer sublease contexts tested in the paper?
- Basis in paper: [explicit] The paper tests ACE in two specific negotiation scenarios (used car and summer sublease) and does not explore its effectiveness in other contexts.
- Why unresolved: The paper does not provide evidence of ACE's generalizability to other negotiation scenarios, which is crucial for understanding its broader applicability.
- What evidence would resolve it: Testing ACE with users in various negotiation contexts (e.g., salary negotiations, business deals) and comparing its effectiveness across these scenarios would demonstrate its versatility and generalizability.

## Limitations
- Evaluation relies on simulated negotiation scenarios rather than real-world business negotiations
- System's effectiveness depends on LLM error detection quality, which may degrade with nuanced scenarios
- Study uses only one specific negotiation context (car purchasing), limiting generalizability

## Confidence

- **High Confidence**: The core technical contribution of developing an error annotation scheme and implementing targeted feedback delivery. The LLM-based error detection methodology is well-documented and reproducible.
- **Medium Confidence**: The experimental results showing performance improvement, as the study uses a controlled simulation rather than real-world negotiations. The sample size of 374 participants provides reasonable statistical power.
- **Low Confidence**: Claims about long-term skill transfer beyond the two-trial experiment. The study does not assess whether improvements persist after users stop using the system.

## Next Checks

1. **Cross-Scenario Validation**: Test ACE across diverse negotiation contexts (salary, business deals, conflict resolution) to assess generalizability of the annotation scheme and feedback effectiveness.

2. **Longitudinal Skill Retention**: Conduct a follow-up study 3-6 months after initial training to measure whether negotiation skill improvements persist without ongoing system use.

3. **Real-World Deployment**: Implement ACE in actual business or educational settings with authentic negotiation scenarios to validate laboratory findings in practical contexts.