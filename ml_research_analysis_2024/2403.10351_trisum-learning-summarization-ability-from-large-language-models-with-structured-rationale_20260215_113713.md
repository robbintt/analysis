---
ver: rpa2
title: 'TriSum: Learning Summarization Ability from Large Language Models with Structured
  Rationale'
arxiv_id: '2403.10351'
source_url: https://arxiv.org/abs/2403.10351
tags:
- summary
- rationale
- summarization
- trisum
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TriSum, a framework that distills the abstractive
  summarization ability of large language models (LLMs) into a smaller local model
  by leveraging structured rationales. The method uses a three-step process: first,
  an LLM generates aspect-triple rationales and summaries; second, high-quality rationales
  are selected using summary and coherence scores; third, the local model is trained
  with curriculum learning from simple to complex tasks.'
---

# TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale

## Quick Facts
- **arXiv ID**: 2403.10351
- **Source URL**: https://arxiv.org/abs/2403.10351
- **Reference count**: 19
- **Primary result**: Outperforms SOTA models by 4.5%, 8.5%, and 7.4% on ROUGE scores across CNN/DailyMail, XSum, and ClinicalTrial datasets

## Executive Summary
TriSum is a framework that distills abstractive summarization ability from large language models (LLMs) into smaller, more efficient local models. The method uses a three-step process: LLM generates aspect-triple rationales and summaries, high-quality rationales are selected using dual scoring (summary and coherence), and the local model is trained with curriculum learning. Experiments show TriSum outperforms state-of-the-art models by significant margins while providing structured rationales that enhance interpretability of the summarization process.

## Method Summary
TriSum employs a three-step framework to transfer summarization ability from LLMs to smaller models. First, GPT-3.5 generates multiple aspect-triple rationales and summaries for each document. Second, a dual-scoring method selects high-quality rationales based on semantic similarity to ground truth summaries and topical coherence. Third, a BART-based local model is trained through curriculum learning, progressing from individual task learning to joint learning of aspect extraction, triple extraction, and summary generation. The method is evaluated on CNN/DailyMail, XSum, and ClinicalTrial datasets using ROUGE, BERTScore, and BARTScore metrics.

## Key Results
- Outperforms SOTA models by 4.5%, 8.5%, and 7.4% on ROUGE-F1 scores across CNN/DailyMail, XSum, and ClinicalTrial datasets
- Improves interpretability by providing structured rationales that clarify the summarization process
- Shows robustness to noisy LLM-generated rationales through effective selection mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TriSum's three-step framework progressively transfers summarization ability from LLMs to smaller models through structured rationales.
- Mechanism: LLM first extracts essential aspects and triples from text, then selects high-quality rationales via dual scoring (summary and coherence), and finally trains the local model with curriculum learning from simple to complex tasks.
- Core assumption: LLMs can reliably extract structured rationales that capture the core summarization ability.
- Evidence anchors:
  - [abstract] "Initially, LLMs extract a set of aspect-triple rationales and summaries, which are refined using a dual-scoring method for quality."
  - [section] "The rationale R = (A, T) is a sequence of tokens {r1, r2, ..., r|R|}, which is composed of aspect tokens {a1, a2, ..., a|A|} followed by triple tokens {t1, t2, ..., t|T |}"
  - [corpus] Weak - no direct corpus evidence, but related papers suggest LLMs can generate structured rationales for summarization tasks.

### Mechanism 2
- Claim: Dual scoring method ensures selected rationales are both semantically similar to ground truth and topically coherent with the document.
- Mechanism: Summary Score measures semantic similarity between LLM-generated summary and ground truth; Coherence Score measures topic alignment using LDA-based KL divergence.
- Core assumption: Both semantic similarity and topical coherence are necessary for high-quality rationales.
- Evidence anchors:
  - [abstract] "We employ a dual-scoring method for selecting golden (high-quality) rationales to use in the subsequent training."
  - [section] "The final selection of optimal rationales... is based on those that yield the highest combined score of Eq. (2) and Eq. (3)"
  - [corpus] Weak - no direct corpus evidence, but related papers on rationale distillation suggest dual scoring improves quality.

### Mechanism 3
- Claim: Curriculum learning strategy enables the local model to progressively learn complex summarization tasks by starting with simpler ones.
- Mechanism: Model first learns individual tasks (aspect extraction, triple extraction, summary generation) separately, then learns them concurrently with LLM guidance, and finally learns to generate rationale and summary jointly.
- Core assumption: Learning simpler tasks first helps build foundation for more complex tasks.
- Evidence anchors:
  - [abstract] "Next, a smaller local model is trained with these tasks, employing a curriculum learning strategy that evolves from simple to complex tasks."
  - [section] "This strategy consists of the following phases: (1) Singular-task learning, (2) Concurrent learning, and (3) Joint learning."
  - [corpus] Weak - no direct corpus evidence, but curriculum learning is a well-established ML technique.

## Foundational Learning

- Concept: Aspect extraction and triple extraction as intermediate tasks
  - Why needed here: These structured intermediate representations help the model understand document structure before attempting full summarization
  - Quick check question: Can the model extract meaningful aspects and triples from a document without attempting full summarization?

- Concept: Curriculum learning progression
  - Why needed here: Gradually increasing task complexity prevents overwhelming the model and builds on previously learned skills
  - Quick check question: Does the model's performance improve when moving from individual task learning to concurrent learning?

- Concept: Dual scoring for quality selection
  - Why needed here: Ensures the rationales used for training are both semantically accurate and topically coherent
  - Quick check question: Does changing the balance between summary score and coherence score affect the quality of selected rationales?

## Architecture Onboarding

- Component map:
  - LLM rationale probing (GPT-3.5) → Golden rationale selection (scoring + LDA) → Local model training (BART-based) with curriculum learning
  - Key data flows: Document → LLM → Aspect-Triple Rationale → Scoring → Selected Rationale → Local Model → Summary

- Critical path:
  1. LLM generates n candidate rationales per document
  2. Dual scoring selects best rationale per document
  3. Local model trained through curriculum learning stages
  4. Final model generates summaries from documents

- Design tradeoffs:
  - Using LLM for rationale generation adds computational cost but provides high-quality supervision
  - Curriculum learning increases training time but improves final performance
  - Dual scoring adds complexity but ensures better rationale quality

- Failure signatures:
  - Poor ROUGE scores indicate issues with rationale quality or curriculum progression
  - Low BERTScore/BARTScore indicate semantic quality issues
  - Inconsistent performance across datasets suggests overfitting or dataset-specific issues

- First 3 experiments:
  1. Test LLM rationale generation quality on a small sample - verify it extracts coherent aspects and triples
  2. Test dual scoring selection - verify it selects rationales with both high semantic similarity and coherence
  3. Test curriculum learning progression - verify performance improves through each stage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TriSum change when using rationales generated by different LLMs, such as GPT-4 or Claude, instead of GPT-3.5?
- Basis in paper: [explicit] The paper mentions testing TriSum with rationales generated by GPT-3.5 and LLAMA-2-70B, and notes that GPT-4 could not be tested due to API cost.
- Why unresolved: The paper does not provide performance data for other LLMs, leaving uncertainty about whether TriSum's effectiveness is dependent on the specific LLM used.
- What evidence would resolve it: Experimental results comparing TriSum's performance across multiple LLMs would clarify whether the choice of LLM significantly impacts outcomes.

### Open Question 2
- Question: What is the impact of varying the number of LDA latent topics on the quality of selected rationales and subsequent summarization performance?
- Basis in paper: [explicit] The paper discusses using LDA-based coherence scores and mentions that performance drops when the number of latent topics is too low or too high, with optimal performance at around 200 topics.
- Why unresolved: While the paper provides an optimal range, it does not explore the full spectrum of possible topic numbers or explain why 200 is optimal.
- What evidence would resolve it: A comprehensive analysis of TriSum's performance across a wide range of LDA topic numbers would identify the optimal setting and explain the underlying reasons.

### Open Question 3
- Question: How does TriSum's approach to curriculum learning compare to other curriculum learning strategies in terms of efficiency and final summarization quality?
- Basis in paper: [inferred] The paper introduces a three-phase curriculum learning strategy (singular-task, concurrent, and joint learning) but does not compare it to alternative curriculum learning methods.
- Why unresolved: The paper does not provide comparative data on different curriculum learning strategies, leaving questions about the relative advantages of TriSum's approach.
- What evidence would resolve it: Experiments comparing TriSum's curriculum learning strategy to other established methods would reveal its relative strengths and weaknesses.

## Limitations

- Implementation details for LLM rationale probing and LDA topic modeling are not fully specified, hindering exact reproduction
- Dual scoring mechanism effectiveness depends on balance between summary and coherence scores, which lacks empirical validation
- Absence of ablation studies prevents quantification of individual component contributions to overall performance

## Confidence

- **High confidence**: The overall three-step framework design and its core claims about outperforming SOTA models by specified percentages (4.5%, 8.5%, 7.4% on ROUGE scores)
- **Medium confidence**: The curriculum learning progression and its effectiveness in transferring summarization ability
- **Low confidence**: The specific implementation details of the dual scoring mechanism and the optimal balance between summary and coherence scores

## Next Checks

1. **Implementation validation**: Recreate the LLM rationale generation using the described template prompts on a small sample of documents to verify it produces coherent aspect-triple structures as claimed.
2. **Component ablation study**: Systematically remove each component (dual scoring, curriculum learning, structured rationales) to quantify their individual contributions to the final performance improvements.
3. **Generalization testing**: Test the distilled model on datasets not seen during training to verify the claimed robustness and transferability of the summarization ability across different domains.