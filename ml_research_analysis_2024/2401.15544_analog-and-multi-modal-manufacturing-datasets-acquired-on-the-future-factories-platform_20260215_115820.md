---
ver: rpa2
title: Analog and Multi-modal Manufacturing Datasets Acquired on the Future Factories
  Platform
arxiv_id: '2401.15544'
source_url: https://arxiv.org/abs/2401.15544
tags:
- robot
- dataset
- datasets
- data
- manufacturing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Two industry-grade datasets are presented, collected from a manufacturing
  assembly line at the University of South Carolina. The datasets include a time series
  analog dataset and a multi-modal dataset with synchronized images.
---

# Analog and Multi-modal Manufacturing Datasets Acquired on the Future Factories Platform

## Quick Facts
- arXiv ID: 2401.15544
- Source URL: https://arxiv.org/abs/2401.15544
- Reference count: 8
- Primary result: Two industry-grade datasets collected from a manufacturing assembly line for AI model training and anomaly detection

## Executive Summary
This paper presents two novel datasets collected from a manufacturing assembly line at the University of South Carolina, designed to advance research in manufacturing intelligence. The datasets include a time series analog dataset with 10 Hz acquisition and a multi-modal dataset combining synchronized images with analog data at 2-3 Hz. Both datasets were generated by operating the assembly line for 30 consecutive hours, with anomalies introduced by manually removing parts needed for assembly. The datasets aim to provide researchers with tools to enhance intelligence in manufacturing by offering a foundation to build and train AI models.

## Method Summary
The datasets were collected from a manufacturing assembly line testbed featuring Yaskawa robots, Siemens PLC, and industrial sensors. The analog dataset includes 325 complete cycles with annotations for three types of anomalies, while the multi-modal dataset contains 166,000 records with synchronized image frames. Anomalies were introduced by manually removing rocket parts during operation, creating three distinct anomaly categories. The datasets were minimally filtered and include data from sensors throughout the system, with timestamps for synchronization.

## Key Results
- Two industry-grade datasets: analog (10 Hz, 325 cycles) and multi-modal (2-3 Hz, 166,000 records)
- Synchronized multi-modal data combining analog sensors with live image frames
- Three types of anomalies introduced: NoNoseCone, NoBody2,NoNose, and NoBody1,NoBody2,NoNose
- Data collected over 30 consecutive hours using industrial-standard equipment and protocols

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synchronized multi-modal data (analog + images) improves anomaly detection accuracy over single-modal data.
- Mechanism: Visual context from synchronized cameras captures spatially-localized assembly errors (e.g., missing parts, misalignment) that analog sensor readings alone cannot resolve, enabling richer feature representations.
- Core assumption: Anomalies produce both measurable changes in analog sensor signals and visible deviations in the camera feed, and these are temporally aligned.
- Evidence anchors:
  - [abstract] "The multi-modal dataset represents a novel dataset which synchronizes live image frames of the assembly line with the analog data generated during the same instance."
  - [section] "Throughout the 30 hours, some anomalies were also simulated by having team members manually remove rocket pieces from the tray..."
  - [corpus] Weak - no direct studies comparing single- vs. multi-modal anomaly detection performance.
- Break condition: If temporal misalignment between analog and visual data exceeds a threshold, the correlation advantage is lost.

### Mechanism 2
- Claim: Industry-standard equipment and protocols ensure that trained models generalize to real-world manufacturing environments.
- Mechanism: Using Yaskawa robots, Siemens PLC, Profinet, and standardized sensors creates a testbed that mimics commercial manufacturing conditions, so models trained here inherit similar data distributions and control logic patterns.
- Core assumption: Manufacturing anomalies and normal operation patterns in the testbed are representative of those in actual factories.
- Evidence anchors:
  - [abstract] "These datasets are generated by a manufacturing assembly line that utilizes industrial standards with respect to actuators, control mechanisms, and transducers."
  - [section] "The testbed has five Yaskawa six-axis robots... The communication between devices and machines in the testbed is made possible through the Siemens S7-1500 PLC."
  - [corpus] Weak - no validation studies on real factory deployment of models trained on this data.
- Break condition: If real factories use significantly different equipment or control protocols, model transferability fails.

### Mechanism 3
- Claim: Introducing controlled anomalies during data collection creates a labeled dataset suitable for supervised learning.
- Mechanism: Manual removal of rocket parts generates three distinct anomaly types (NoNoseCone, NoBody2,NoNose, NoBody1,NoBody2,NoNose) with clear labels, enabling supervised anomaly classification models.
- Core assumption: Anomalies introduced in this way are representative of real-world defect modes and do not corrupt the normal operation data distribution.
- Evidence anchors:
  - [section] "Throughout the 30 hours, some anomalies were also simulated by having team members manually remove rocket pieces from the tray so as to manufacture defective products. These anomalies are classified into three categories..."
  - [abstract] "During operation, defects were also introduced into the assembly operation by manually removing parts needed for the final assembly."
  - [corpus] No evidence found regarding similarity of simulated anomalies to real-world manufacturing defects.
- Break condition: If simulated anomalies differ systematically from real defects, model performance degrades in deployment.

## Foundational Learning

- Concept: Understanding of industrial communication protocols (Profinet, PLC programming).
  - Why needed here: Data synchronization and sensor integration depend on these protocols; without this knowledge, interpreting data structure and timing is difficult.
  - Quick check question: What is the role of Profinet in the testbed, and how does it differ from standard Ethernet communication?
- Concept: Familiarity with multi-modal data fusion techniques.
  - Why needed here: The dataset combines analog time series with image data; effective analysis requires knowledge of fusion strategies (early, late, or hybrid).
  - Quick check question: What are the trade-offs between early fusion (concatenating features) and late fusion (separate model predictions) for this dataset?
- Concept: Anomaly detection in time series.
  - Why needed here: The primary goal is to train models to detect assembly anomalies; understanding statistical and ML-based anomaly detection is essential.
  - Quick check question: How would you distinguish between sensor drift and a true assembly anomaly in the analog data?

## Architecture Onboarding

- Component map: Yaskawa HC10 and GP8 robots -> grippers with potentiometers and load cells -> conveyor system with VFDs -> Siemens S7-1500 PLC with Profinet -> two synchronized cameras -> data acquisition and storage
- Critical path: Robot gripper sensors -> PLC -> Profinet -> data logger -> synchronized with camera timestamps -> dataset storage
- Design tradeoffs: High-frequency analog data (10 Hz) captures fine-grained sensor changes but increases storage/compute needs; lower multi-modal frequency (2-3 Hz) balances with image acquisition; synchronized timestamps are critical for cross-modal analysis
- Failure signatures: Missing or corrupted sensor values during downtime, desynchronization between analog and image timestamps, or incomplete cycle annotations
- First 3 experiments:
  1. Load and visualize one analog CSV file and corresponding image batch; verify synchronization by plotting sensor values aligned with sample images.
  2. Train a baseline anomaly classifier using only analog features; evaluate on the three anomaly types.
  3. Extend the classifier to include image features (via CNN embeddings) and compare performance gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal acquisition rate for synchronized multi-modal data in manufacturing settings to balance data resolution and storage requirements?
- Basis in paper: [inferred] The multi-modal dataset has a lower acquisition rate (2-3 Hz) compared to the analog dataset (10 Hz) due to synchronized image data, resulting in 166,000 records. The paper does not explore the impact of different acquisition rates on data quality or model performance.
- Why unresolved: The paper presents the datasets but does not analyze how different acquisition rates might affect the usefulness of the data for various applications or the trade-offs between data resolution and storage/computational requirements.
- What evidence would resolve it: Comparative studies analyzing model performance using data at different acquisition rates, along with assessments of storage requirements and computational costs for each rate.

### Open Question 2
- Question: How do different types of introduced anomalies (e.g., missing parts vs. misassembly) affect the performance of AI models trained on these datasets?
- Basis in paper: [explicit] The paper mentions that anomalies were introduced by manually removing parts needed for assembly, resulting in three categories of anomalies (NoNoseCone, NoBody2,NoNose, NoBody1,NoBody2,NoNose). However, it does not explore how different types of anomalies might affect model performance.
- Why unresolved: The paper provides the datasets with annotated anomalies but does not analyze the impact of different anomaly types on model training or performance.
- What evidence would resolve it: Experiments comparing AI model performance when trained on different types of anomalies, and analysis of how anomaly complexity affects model accuracy and generalization.

### Open Question 3
- Question: What is the minimum number of cycles required to train robust AI models for anomaly detection in this manufacturing process?
- Basis in paper: [explicit] The analog dataset consists of 325 complete cycles, while the paper does not discuss the relationship between the number of cycles and model performance.
- Why unresolved: The paper presents the datasets without exploring how the number of cycles affects the ability to train effective AI models for anomaly detection.
- What evidence would resolve it: Studies analyzing model performance as a function of the number of training cycles, including assessments of when additional cycles no longer significantly improve model performance.

## Limitations
- No validation studies comparing single- vs. multi-modal anomaly detection performance
- No testing of model generalizability to real-world manufacturing environments
- Lack of analysis on how simulated anomalies compare to real-world defect patterns

## Confidence

| Claim | Confidence |
|-------|------------|
| Multi-modal synchronization improves anomaly detection | Medium |
| Industry-standard equipment ensures real-world generalizability | Medium |
| Controlled anomalies create effective supervised learning data | Medium |

## Next Checks
1. Conduct a controlled experiment comparing anomaly detection performance using only analog data versus multi-modal data on this dataset to quantify the benefit of visual context.
2. Collaborate with an industrial partner to deploy a model trained on this dataset in a real factory and measure performance degradation or adaptation requirements.
3. Analyze the distribution of simulated anomalies versus known real-world defect types to assess alignment and potential gaps.