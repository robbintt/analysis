---
ver: rpa2
title: Computational Modelling of Plurality and Definiteness in Chinese Noun Phrases
arxiv_id: '2403.04376'
source_url: https://arxiv.org/abs/2403.04376
tags:
- plurality
- definiteness
- chinese
- corpus
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether the intended meaning of plurality
  and definiteness markers that are often omitted in Chinese noun phrases (NPs) can
  be predicted from context. The authors construct a corpus of Chinese NPs annotated
  with plurality and definiteness labels derived from their English translations in
  a parallel subtitle corpus.
---

# Computational Modelling of Plurality and Definiteness in Chinese Noun Phrases

## Quick Facts
- arXiv ID: 2403.04376
- Source URL: https://arxiv.org/abs/2403.04376
- Reference count: 0
- Primary result: BERT-wwm achieves weighted F1 scores of 85.52 for plurality and 81.97 for definiteness prediction

## Executive Summary
This paper investigates whether the intended meaning of plurality and definiteness markers that are often omitted in Chinese noun phrases (NPs) can be predicted from context. The authors construct a corpus of Chinese NPs annotated with plurality and definiteness labels derived from their English translations in a parallel subtitle corpus. Human assessments confirm the corpus quality, showing around 80% agreement on annotations. Computational models, including both classic machine learning and state-of-the-art pre-trained language models, are trained to predict plurality and definiteness. The results show that PLM-based models significantly outperform classic ML models, with BERT-wwm achieving the best performance. The study also finds that predicting plurality and definiteness simultaneously improves performance compared to separate binary predictions.

## Method Summary
The authors built a parallel corpus of Chinese and English subtitles, extracting Chinese NPs and their corresponding English translations. They identified Chinese NPs using heuristic rules, then matched them with English NPs through word alignment. Automatic annotation was performed by labeling Chinese NPs with plurality and definiteness information derived from English translations. A subset of the corpus was manually validated by human annotators. Computational models including classic ML (Random Forest, Logistic Regression, SVM) and PLMs (BERT, RoBERTa, DeBERTa, ALBERT, BERT-wwm) were trained on the corpus to predict plurality and definiteness. Both binary classification (singular/plural, definite/indefinite) and 4-way classification were evaluated using macro and weighted F1 scores.

## Key Results
- BERT-wwm achieved the best performance with weighted F1 scores of 85.52 for plurality and 81.97 for definiteness
- Models predicting plurality and definiteness simultaneously outperformed those predicting them separately
- Context window of current sentence +1 previous sentence yielded optimal performance
- Classic ML models performed significantly worse than PLMs for both tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Plurality and definiteness in Chinese noun phrases can be predicted from surrounding context.
- Mechanism: The computational models learn contextual patterns that correlate with the intended plurality/definiteness from the English translations in a parallel corpus.
- Core assumption: The intended meaning of Chinese NPs is recoverable from their surrounding sentences.
- Evidence anchors:
  - [abstract] "we investigate the predictability of plurality and definiteness of Chinese NPs from their contexts"
  - [section 4.2] "The results suggest that all models can learn useful information for both plurality and definiteness predictions"
  - [corpus] Weak - the corpus analysis shows only 12.42% explicitly express plurality and 15.86% express definiteness, but this doesn't directly prove predictability
- Break condition: If context becomes too wide (e.g., adding previous sentences), model performance decreases, suggesting context size has an optimal range.

### Mechanism 2
- Claim: Predicting plurality and definiteness simultaneously improves performance compared to separate binary predictions.
- Mechanism: Joint classification allows the model to learn correlations between plurality and definiteness that aren't captured when predicting them independently.
- Core assumption: Plurality and definiteness information in NPs are related and can inform each other.
- Evidence anchors:
  - [section 5.2] "models can significantly benefit from predicting plurality and definiteness simultaneously compared to predicting them separately"
  - [section 4.2] "We had the following observations: (1) BERT-wwm performed remarkably well. It generally performed the best for plurality prediction and was the second-best model for definiteness prediction"
  - [corpus] Weak - the corpus analysis shows these are independent features but doesn't directly show they're learnable together
- Break condition: If the model architecture isn't designed to handle multi-way classification, the benefit disappears.

### Mechanism 3
- Claim: BERT-wwm outperforms other PLMs for this task due to its whole word masking pre-training.
- Mechanism: Whole word masking forces the model to learn more robust contextual representations for words, which helps with pragmatic tasks like plurality/definiteness prediction.
- Core assumption: Context extraction ability is more important than raw parameter count for pragmatic tasks.
- Evidence anchors:
  - [section 4.2] "BERT-wwm achieved the best performance (weighted F1 scores of 85.52 for plurality and 81.97 for definiteness)"
  - [section 4.2] "This demonstrated that, on pragmatics tasks (e.g., our tasks), BERT does benefit from whole word mask pre-training probably because the intended meaning of a word (noun in our situation) is mainly inferred from its context"
  - [corpus] Weak - the corpus doesn't directly test masking strategies
- Break condition: If the task requires fine-grained word-level understanding rather than contextual understanding, whole word masking may not help.

## Foundational Learning

- Concept: Parallel corpus alignment and word alignment techniques
  - Why needed here: The study relies on automatically extracting and annotating Chinese NPs by matching them with English translations
  - Quick check question: How would you handle cases where word alignment doesn't perfectly match NPs between languages?

- Concept: Feature engineering for classic ML models
  - Why needed here: The study compares classic ML models (RF, LR, SVM) with PLMs, requiring proper feature extraction from text
  - Quick check question: What n-gram range would you choose for a task where context is crucial but the target is a short phrase?

- Concept: Evaluation metrics for multi-class classification
  - Why needed here: The study uses macro and weighted F1 scores, and later introduces 4-way classification
  - Quick check question: When would you prefer macro F1 over weighted F1 for imbalanced datasets?

## Architecture Onboarding

- Component map: Data preprocessing → Word alignment → NP identification → NP matching → Annotation → Model training → Evaluation
- Critical path: The pipeline from NP identification to model training is most critical - errors here propagate to all downstream tasks
- Design tradeoffs: Using parallel corpus annotation trades manual annotation accuracy for scale; using PLMs trades interpretability for performance
- Failure signatures: Low inter-annotator agreement suggests corpus quality issues; performance drop with wider context suggests over-reliance on local patterns
- First 3 experiments:
  1. Test different context window sizes (current sentence only, +1 previous, +2 previous) to find optimal range
  2. Compare performance on explicit vs implicit expressions to validate the "coolness" hypothesis
  3. Test joint vs separate predictions to verify the benefit of simultaneous classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the predictability of plurality and definiteness in Chinese NPs depend on contextual information versus linguistic cues (e.g., numeral markers, measure words)?
- Basis in paper: [explicit] The paper investigates the predictability of plurality and definiteness in Chinese NPs from their contexts, finding that BERT-wwm generally performs best, suggesting the importance of context. However, the analysis also reveals that models perform better on explicit expressions compared to implicit ones, indicating the role of linguistic cues.
- Why unresolved: The paper does not directly compare the relative contributions of context versus linguistic cues in predicting plurality and definiteness. Further experiments isolating these factors would be needed.
- What evidence would resolve it: Controlled experiments manipulating the presence or absence of linguistic cues (e.g., numerals, measure words) in the context, while keeping the target NP constant, would reveal the extent to which context versus linguistic cues drive predictions.

### Open Question 2
- Question: How do the findings on plurality and definiteness prediction in Chinese NPs generalize to other "cool" languages like Japanese and Korean, which also allow for omission of these markers?
- Basis in paper: [explicit] The paper mentions that the "coolness" hypothesis applies to languages like Chinese, Japanese, and Korean, but focuses solely on Chinese. The authors suggest that future work could build corpora in multiple "cool" languages to investigate cross-linguistic generalizability.
- Why unresolved: The study is limited to Chinese data, and the authors acknowledge the need for cross-linguistic validation. Differences in grammar, discourse structure, and cultural norms across these languages may affect the predictability of omitted markers.
- What evidence would resolve it: Building and analyzing parallel corpora in Chinese, Japanese, and Korean, following the methodology outlined in the paper, would allow for direct comparison of predictability across these languages.

### Open Question 3
- Question: How do computational models of plurality and definiteness prediction in Chinese NPs compare to human comprehension processes? Do models capture the same strategies and limitations as humans?
- Basis in paper: [explicit] The paper acknowledges the limitation that computational models may not fully mimic human comprehension. It suggests that future work should create a corpus annotated with human disagreements to assess model performance against human understanding.
- Why unresolved: The study focuses on model performance without directly comparing it to human comprehension. Humans may employ different strategies, rely on different contextual cues, or face different challenges compared to computational models.
- What evidence would resolve it: Behavioral experiments with human participants performing plurality and definiteness comprehension tasks on Chinese NPs, coupled with computational modeling of human data, would reveal the similarities and differences between human and machine comprehension.

## Limitations

- Corpus annotation relies on automatic alignment with English translations, which may not perfectly capture intended plurality and definiteness
- Results are based on movie subtitles and may not generalize to other text types
- The study does not compare computational predictions to actual human comprehension processes

## Confidence

- High confidence: The finding that PLMs outperform classic ML models for this task is well-supported and consistent with broader NLP literature.
- Medium confidence: The claim that predicting plurality and definiteness simultaneously improves performance requires further validation, as the study only compared these approaches within their specific experimental setup.
- Medium confidence: The assertion that whole word masking specifically benefits this task is supported but could reflect other differences between BERT-wwm and standard BERT implementations.

## Next Checks

1. Test the trained models on Chinese text from different domains (news, literature, social media) to assess generalizability of the context-based prediction approach.

2. Expand the human validation sample to 200-300 NPs across different frequency bands and examine whether the 80% agreement rate holds consistently.

3. Systematically vary context window sizes (current sentence only, +1 previous, +2 previous) and compare performance to identify the optimal context range and validate the observation that wider context may decrease performance.