---
ver: rpa2
title: Okay, Let's Do This! Modeling Event Coreference with Generated Rationales and
  Knowledge Distillation
arxiv_id: '2404.03196'
source_url: https://arxiv.org/abs/2404.03196
tags:
- event
- coreference
- rationales
- ftrs
- roec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a novel knowledge distillation framework for
  event coreference resolution that leverages abductive free-text rationales (FTRs)
  generated by large language models. The method involves two stages: (1) Rationale-Oriented
  Event Clustering (ROEC) to optimize a student model''s latent space by aligning
  FTRs with corresponding event pairs, and (2) coreference knowledge distillation
  where the student model learns from the teacher distribution using the FTRs as soft
  labels.'
---

# Okay, Let's Do This! Modeling Event Coreference with Generated Rationales and Knowledge Distillation

## Quick Facts
- arXiv ID: 2404.03196
- Source URL: https://arxiv.org/abs/2404.03196
- Reference count: 21
- Achieves state-of-the-art B3 F1 scores on ECB+ and GVC corpora using abductive free-text rationales and knowledge distillation

## Executive Summary
This paper introduces a novel knowledge distillation framework for event coreference resolution that leverages abductive free-text rationales (FTRs) generated by large language models. The method involves two stages: Rationale-Oriented Event Clustering (ROEC) to optimize a student model's latent space by aligning FTRs with corresponding event pairs, and coreference knowledge distillation where the student learns from the teacher distribution using FTRs as soft labels. The approach achieves state-of-the-art results on established benchmarks and establishes a new baseline on the AIDA Phase 1 corpus, outperforming both heuristic-based methods and zero-shot evaluations using larger models like GPT-3.5-Turbo.

## Method Summary
The proposed framework uses abductive free-text rationales generated by LLMs to improve event coreference resolution through knowledge distillation. In the first stage, ROEC optimizes the student model's latent space by clustering event pairs based on their corresponding FTRs, creating a more semantically coherent representation. In the second stage, coreference knowledge distillation trains the student model to predict FTRs as soft labels, capturing the teacher's reasoning process. This approach enables the student model to learn from the teacher's abductive reasoning without requiring the rationales as input during inference, making it more efficient and generalizable.

## Key Results
- Achieves state-of-the-art B3 F1 scores on ECB+ and GVC corpora
- Establishes new baseline on AIDA Phase 1 corpus outperforming heuristic-based methods
- Outperforms zero-shot GPT-3.5-Turbo evaluations while using smaller models
- Demonstrates that automatically-generated rationales contain useful information for coreference decisions

## Why This Works (Mechanism)
The method works by leveraging the abductive reasoning capabilities of LLMs to generate rationales that capture the semantic relationships between events. These rationales serve as soft labels in a knowledge distillation framework, allowing a smaller student model to learn the teacher's reasoning process rather than just its predictions. The two-stage approach first creates a more semantically coherent latent space through ROEC, then refines the model's understanding through coreference knowledge distillation. This enables the student model to make more accurate coreference decisions by learning to generate similar abductive reasoning, even without the rationales present at inference time.

## Foundational Learning
- **Event Coreference Resolution**: The task of determining whether two event mentions refer to the same real-world event; needed to establish the problem context and evaluation metrics
- **Knowledge Distillation**: A model compression technique where a smaller student model learns from a larger teacher model; needed to understand how the student learns from the LLM-generated rationales
- **Abductive Reasoning**: A form of logical inference that seeks the simplest explanation for observations; needed to understand how rationales are generated and why they're useful for coreference
- **Free-Text Rationales**: Natural language explanations generated by models to justify their decisions; needed to understand the type of supervision signal used in training
- **Latent Space Optimization**: Techniques for improving the semantic representation space of a model; needed to understand the ROEC stage
- **B3 F1 Score**: A coreference evaluation metric that measures cluster quality; needed to interpret the reported results

## Architecture Onboarding

**Component Map:** LLM (Teacher) -> FTR Generator -> ROEC Module -> Student Model -> Coreference Predictor

**Critical Path:** The most critical path is LLM-generated FTR -> ROEC clustering -> Student model training -> Coreference prediction. This path represents the flow of information from rationale generation through to the final coreference decision.

**Design Tradeoffs:** The method trades computational overhead (generating rationales for all event pairs) for improved accuracy and generalization. Alternative designs could use different types of rationales (deductive, inductive) or skip the ROEC stage, but these would likely sacrifice some performance. The pairwise scoring mechanism, while effective, limits scalability compared to mention-ranking approaches.

**Failure Signatures:** Poor rationale quality would lead to incorrect clustering in ROEC and misguided distillation, resulting in degraded performance. If the student model overfits to the rationales without capturing general coreference patterns, it may fail on out-of-domain data. Computational constraints may prevent generating rationales for all event pairs, limiting the method's applicability to smaller datasets.

**First 3 Experiments:**
1. Ablation study removing the ROEC stage to measure its contribution to overall performance
2. Comparison of different rationale types (deductive vs abductive) to validate the choice of abductive reasoning
3. Out-of-domain evaluation on scientific literature or social media data to test generalizability

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Heavy reliance on the quality and faithfulness of LLM-generated rationales, with limited evaluation of their accuracy
- Computational costs of generating rationales for all event pairs may limit scalability
- Performance on out-of-domain corpora or with different event extraction systems is not evaluated
- Uncertainty about whether benefits justify additional complexity compared to simpler approaches

## Confidence

**High Confidence:**
- State-of-the-art results on established benchmarks (ECB+, GVC) with statistically significant improvements
- Well-defined two-stage knowledge distillation framework
- Clear methodology and reproducible results on standard datasets

**Medium Confidence:**
- Claims about the effectiveness of abductive FTRs specifically versus other rationale types
- Assertion that this is the first work using FTRs for event coreference resolution
- Benefits over simply using larger models directly, given potential confounding factors

**Low Confidence:**
- Long-term stability and maintenance costs of the method
- Whether performance gains justify the additional complexity
- Extent to which rationales genuinely improve understanding versus serving as regularization

## Next Checks
1. Conduct human evaluation of generated rationales to assess faithfulness, coherence, and alignment with actual coreference decisions, comparing across different LLM generations and prompt variations
2. Perform out-of-domain testing on corpora with different event types or domains (scientific literature, news from different time periods) to assess generalizability
3. Evaluate computational overhead and runtime efficiency of the two-stage distillation process compared to baseline methods and direct inference with larger models, including memory usage analysis for pairwise scoring