---
ver: rpa2
title: Evaluation of General Large Language Models in Contextually Assessing Semantic
  Concepts Extracted from Adult Critical Care Electronic Health Record Notes
arxiv_id: '2401.13588'
source_url: https://arxiv.org/abs/2401.13588
tags: []
core_contribution: The study evaluates three general large language models (GPT-4,
  GPT-3.5, and text-davinci-003) in understanding and processing real-world clinical
  notes from adult critical care. Concepts from 150 clinical notes were extracted
  using MetaMap and labeled by 9 clinicians, then assessed for temporality and negation
  by the LLMs using different prompts.
---

# Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes

## Quick Facts
- **arXiv ID**: 2401.13588
- **Source URL**: https://arxiv.org/abs/2401.13588
- **Reference count**: 39
- **One-line primary result**: GPT-4 outperformed GPT-3.5 and text-davinci-003 in clinical concept assessment tasks with F1 scores reaching 0.966, 0.948, and 0.944 for detection, encounter, and negation labeling tasks respectively.

## Executive Summary
This study evaluates three general large language models (GPT-4, GPT-3.5, and text-davinci-003) in understanding and processing real-world clinical notes from adult critical care. Concepts from 150 clinical notes were extracted using MetaMap and labeled by 9 clinicians, then assessed for temporality and negation by the LLMs using different prompts. GPT-4 showed overall superior performance compared to other LLMs, with F1 scores reaching 0.966, 0.948, and 0.944 for detection, encounter, and negation labeling tasks, respectively. Both GPT-3.5 and text-davinci-003 exhibited enhanced performance when appropriate prompting strategies were employed. The study demonstrates the potential of LLMs in processing complex medical data and establishes a benchmark for future LLM evaluations across specialized domains.

## Method Summary
The study employed a zero-shot learning approach to evaluate three general large language models (GPT-4, GPT-3.5, and text-davinci-003) on clinical concept assessment tasks. Clinical concepts were extracted from 150 nursing notes in the MIMIC-III database using MetaMap. These concepts were then annotated by a team of 9 clinicians for detection, encounter, and negation. The LLMs were evaluated on both concurred and dissenting annotation datasets using different prompts for each task. Statistical analysis including ANOVA tests and bootstrapping was used to compare model performance across tasks and datasets.

## Key Results
- GPT-4 achieved the highest F1 scores across all tasks: 0.966 (detection), 0.948 (encounter), and 0.944 (negation)
- GPT-3.5 and text-davinci-003 showed improved performance with appropriate prompting strategies
- Models performed better on concurred datasets compared to dissenting datasets, with dissenting datasets showing higher variance
- All models showed significantly higher F1 scores on detection tasks compared to encounter and negation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's superior performance stems from its ability to integrate and interpret complex, multifaceted clinical information.
- Mechanism: GPT-4 excels in understanding nuanced clinical contexts, such as differentiating between chronic and acute symptoms, which are crucial for accurate diagnosis and treatment planning.
- Core assumption: The model's training data includes diverse clinical scenarios that allow it to generalize well to real-world healthcare settings.
- Evidence anchors:
  - [abstract]: "GPT-4 showed overall superior performance compared to other LLMs, with F1 scores reaching 0.966, 0.948, and 0.944 for detection, encounter, and negation labeling tasks, respectively."
  - [section]: "Providers often deal with incomplete information, patient-specific variables, and the need to make quick decisions based on a combination of clinical expertise and patient data. In such scenarios, the ability of an LLM to integrate and interpret this multifaceted information becomes critical."
- Break condition: If the model encounters clinical scenarios that are not well-represented in its training data, its performance may degrade significantly.

### Mechanism 2
- Claim: Prompt engineering significantly enhances the performance of GPT-3.5 and text-davinci-003.
- Mechanism: By using clinically informed prompts with sub-questions, these models can provide more accurate and interpretable responses.
- Core assumption: The models can effectively parse and respond to structured prompts that break down complex clinical questions into simpler, more manageable parts.
- Evidence anchors:
  - [abstract]: "Both GPT-3.5 and text-davinci-003 exhibit enhanced performance when appropriate prompting strategies are employed."
  - [section]: "We also demonstrated that clinically informed prompts with a breakdown of possible clinical scenarios can be better leveraged by advanced LLMs such as GPT-4 to provide accurate, easily interpretable responses pertaining to clinical concepts."
- Break condition: If the prompts are not carefully designed to match the model's capabilities, the performance gains may be minimal or even negative.

### Mechanism 3
- Claim: Fine-tuning LLaMA 2 with domain-specific data significantly improves its performance in clinical concept extraction tasks.
- Mechanism: By optimizing a small subset of parameters using LoRA, the model can adapt to the specific requirements of clinical note analysis without extensive retraining.
- Core assumption: The fine-tuning data is representative of the types of clinical notes and concepts encountered in real-world settings.
- Evidence anchors:
  - [abstract]: "GPT-4 showed overall superior performance compared to other LLMs. In contrast, both GPT-3.5 and text-davinci-003 exhibit enhanced performance when the appropriate prompting strategies are employed."
  - [section]: "Using LoRA, we seek to optimize the following objective function: max âˆ†Î¦ âˆ‘ âˆ‘ log(PrÎ˜+âˆ†Î¦(ð‘¥ð‘¡|ð’™1:ð‘¡âˆ’1)) |ð‘‹| ð‘¡=1ð‘‹âˆˆð‘‹ð‘¡ð‘Ÿð‘Žð‘–ð‘›"
- Break condition: If the fine-tuning data is not diverse enough or does not cover the full range of clinical concepts, the model may not generalize well to new data.

## Foundational Learning

- Concept: Clinical concept extraction using MetaMap
  - Why needed here: MetaMap provides a standardized way to map free biomedical text to clinical concepts, which is essential for evaluating LLM performance in understanding medical terminology.
  - Quick check question: What are the three semantic groups of clinical concepts used in this study, and why were they chosen?

- Concept: Zero-shot learning
  - Why needed here: Zero-shot learning allows the evaluation of LLMs without requiring additional training data, providing a baseline for their performance in clinical tasks.
  - Quick check question: How does zero-shot learning differ from fine-tuning, and what are the advantages and disadvantages of each approach?

- Concept: Annotation and adjudication process
  - Why needed here: The annotation process ensures that the ground truth for evaluating LLM performance is accurate and reliable, while adjudication resolves any disagreements between annotators.
  - Quick check question: What is the role of the CADA tool in the annotation process, and how does it facilitate the work of clinicians?

## Architecture Onboarding

- Component map: MIMIC-III database -> MetaMap -> CADA tool -> 9 clinicians -> Annotation datasets -> LLM evaluation (GPT-4, GPT-3.5, text-davinci-003, LLaMA 2) -> Statistical analysis

- Critical path:
  1. Extract clinical concepts from MIMIC-III notes using MetaMap
  2. Annotate concepts using CADA tool and clinician team
  3. Evaluate LLM performance on annotated concepts
  4. Fine-tune LLaMA 2 if necessary
  5. Analyze results and draw conclusions

- Design tradeoffs:
  - Using single-center data limits generalizability but ensures data consistency
  - Relying on clinician annotations introduces subjectivity but provides realistic ground truth
  - Fine-tuning LLaMA 2 improves performance but requires additional resources

- Failure signatures:
  - Poor performance on dissenting dataset may indicate model's inability to handle subjective or complex clinical scenarios
  - High variance in results may suggest overfitting or instability in the model's performance

- First 3 experiments:
  1. Evaluate LLM performance on a small subset of annotated clinical concepts to establish baseline performance
  2. Compare the impact of different prompting strategies on GPT-3.5 and text-davinci-003 performance
  3. Fine-tune LLaMA 2 using a small set of annotated clinical notes and evaluate its performance on a held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do GPT-4's performance metrics on the concurring dataset compare to its performance on the dissenting dataset, and what does this imply about its ability to handle subjective or complex clinical scenarios?
- Basis in paper: Explicit - The paper reports that GPT-4 showed higher F1 scores on the concurring dataset compared to the dissenting dataset, with the dissenting dataset showing more variance.
- Why unresolved: The paper does not delve into the reasons for GPT-4's reduced performance on the dissenting dataset, nor does it explore how this might affect real-world clinical applications where subjectivity and complexity are common.
- What evidence would resolve it: Further analysis of GPT-4's performance on datasets with varying levels of clinician agreement and the identification of specific factors contributing to performance discrepancies.

### Open Question 2
- Question: What are the potential benefits and limitations of using LLMs like GPT-4 for clinical concept extraction and understanding compared to traditional NLP tools like MetaMap?
- Basis in paper: Explicit - The paper compares the performance of LLMs with traditional NLP tools and discusses the potential of LLMs in understanding nuanced clinical contexts.
- Why unresolved: While the paper highlights the strengths of LLMs, it does not provide a comprehensive comparison of their limitations versus traditional tools, nor does it explore scenarios where one might be preferable over the other.
- What evidence would resolve it: Comparative studies focusing on the efficiency, accuracy, and applicability of LLMs versus traditional NLP tools in various clinical settings.

### Open Question 3
- Question: How can LLMs be further optimized to improve their performance in clinical settings, particularly in understanding complex medical concepts and contexts?
- Basis in paper: Explicit - The paper discusses the potential for instruction-specific fine-tuning and the use of advanced techniques like retrieval augmented generation to enhance LLM performance.
- Why unresolved: The paper does not provide specific strategies or results from experiments aimed at optimizing LLMs for clinical use, leaving the potential for further enhancement unexplored.
- What evidence would resolve it: Experimental results from optimizing LLMs through various techniques, including fine-tuning and retrieval augmented generation, and their impact on clinical task performance.

## Limitations
- Single-center data from MIMIC-III may not represent diverse clinical documentation styles across different healthcare settings
- Clinician annotation subjectivity introduces inherent variability in ground truth labels
- Limited number of clinical notes (150) may not capture the full complexity of clinical scenarios

## Confidence
- **High confidence** in GPT-4's superior performance across all tasks, supported by consistent F1 scores above 0.94 and statistically significant differences from other models
- **Medium confidence** in the impact of prompt engineering strategies, as the specific prompts used are not fully detailed in the paper
- **Low confidence** in the generalizability of results to different clinical settings due to the use of single-center data and the limited number of clinical notes (150)

## Next Checks
1. Evaluate model performance on a multi-center dataset to assess generalizability across different clinical documentation styles
2. Conduct ablation studies to isolate the impact of specific prompt components on model performance
3. Test model robustness using adversarial clinical notes that contain ambiguous or conflicting information to better understand failure modes