---
ver: rpa2
title: 'CARL-GT: Evaluating Causal Reasoning Capabilities of Large Language Models'
arxiv_id: '2412.17970'
source_url: https://arxiv.org/abs/2412.17970
tags:
- causal
- reasoning
- tasks
- llms
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CARL-GT, a benchmark designed to evaluate
  causal reasoning capabilities of large language models (LLMs) using graphs and tabular
  data. Unlike existing benchmarks that focus on conversational tasks, academic math
  tests, and coding tests, CARL-GT assesses LLMs'' abilities to solve real-world problems
  by testing three key aspects: causal graph reasoning, knowledge discovery, and decision-making.'
---

# CARL-GT: Evaluating Causal Reasoning Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2412.17970
- Source URL: https://arxiv.org/abs/2412.17970
- Reference count: 13
- Key outcome: Introduces CARL-GT benchmark to evaluate LLMs' causal reasoning using graphs and tabular data, revealing LLMs' weaknesses in this domain

## Executive Summary
This paper introduces CARL-GT, a benchmark designed to evaluate causal reasoning capabilities of large language models (LLMs) using graphs and tabular data. Unlike existing benchmarks that focus on conversational tasks, academic math tests, and coding tests, CARL-GT assesses LLMs' abilities to solve real-world problems by testing three key aspects: causal graph reasoning, knowledge discovery, and decision-making. The benchmark includes diverse tasks, such as estimating adjacency matrices, d-separations, causal directions, and performing intervention and counterfactual reasoning. Effective zero-shot learning prompts are developed for these tasks, and the benchmark is evaluated on open-source LLMs like Llama3-8B, Qwen2-7B, Gemma2-9B, Mistral-7B, and Mixtral-8x7B. The results show that LLMs are still weak in causal reasoning, especially with tabular data, and that their performance varies across different tasks. Notably, tasks in different categories (causal graph reasoning, knowledge discovery, and decision-making) show stronger correlation than tasks within the same category. The study highlights the need for improved LLMs in causal reasoning to enhance their applicability in real-world scenarios.

## Method Summary
CARL-GT is a benchmark designed to evaluate the causal reasoning capabilities of large language models (LLMs) using graphs and tabular data. It introduces diverse tasks to test three key aspects: causal graph reasoning, knowledge discovery, and decision-making. The benchmark includes tasks such as estimating adjacency matrices, d-separations, causal directions, and performing intervention and counterfactual reasoning. Effective zero-shot learning prompts are developed for these tasks, and the benchmark is evaluated on open-source LLMs like Llama3-8B, Qwen2-7B, Gemma2-9B, Mistral-7B, and Mixtral-8x7B. The results indicate that LLMs are still weak in causal reasoning, especially with tabular data, and that their performance varies across different tasks.

## Key Results
- LLMs are weak in causal reasoning, particularly with tabular data
- Performance varies across different tasks within the benchmark
- Tasks in different categories (causal graph reasoning, knowledge discovery, and decision-making) show stronger correlation than tasks within the same category

## Why This Works (Mechanism)
CARL-GT effectively evaluates LLMs' causal reasoning by providing a structured framework that tests their ability to understand and manipulate causal relationships in both graph and tabular formats. The benchmark's design ensures comprehensive coverage of causal reasoning tasks, allowing for a detailed assessment of LLM capabilities in this domain. The use of zero-shot learning prompts further enhances the evaluation by testing the models' ability to generalize without task-specific fine-tuning.

## Foundational Learning
- **Causal Reasoning**: The ability to understand cause-and-effect relationships. Why needed: Essential for solving real-world problems where understanding causality is crucial. Quick check: Evaluate LLM performance on tasks requiring causal inference.
- **Graph Representation**: Using graphs to model causal relationships. Why needed: Graphs provide a structured way to represent and analyze causal dependencies. Quick check: Test LLM's ability to interpret and manipulate causal graphs.
- **Tabular Data Processing**: Handling and reasoning with data presented in tabular format. Why needed: Real-world data often comes in tabular form, requiring LLMs to process and analyze it effectively. Quick check: Assess LLM performance on tasks involving tabular data.

## Architecture Onboarding
- **Component Map**: CARL-GT -> Benchmark Design -> Task Categories (Causal Graph Reasoning, Knowledge Discovery, Decision-Making) -> Evaluation Metrics -> LLM Performance Analysis
- **Critical Path**: Design benchmark tasks -> Develop zero-shot learning prompts -> Evaluate on LLMs -> Analyze performance
- **Design Tradeoffs**: Balancing task complexity with LLM capabilities, ensuring diverse representation of causal reasoning tasks
- **Failure Signatures**: LLMs struggle with tabular data and show inconsistent performance across tasks
- **First Experiments**: 1) Evaluate LLM performance on adjacency matrix estimation, 2) Test d-separation understanding, 3) Assess intervention and counterfactual reasoning

## Open Questions the Paper Calls Out
- How can LLMs be further improved to enhance their causal reasoning capabilities, particularly with tabular data?
- What are the underlying reasons for the observed correlation patterns between tasks in different categories?
- How can the benchmark be extended to evaluate more advanced LLMs or other domains beyond the current scope?

## Limitations
- The benchmark's effectiveness may be limited by the quality and diversity of the datasets used, particularly for tabular data
- The generalizability of the results to other domains or more advanced LLMs is not fully explored
- The study does not address potential biases in the benchmark tasks or the evaluation process

## Confidence
- High: The benchmark design and task categorization are well-defined and methodologically sound
- High: Results showing LLMs' weaknesses in causal reasoning, particularly with tabular data, are well-supported by the experiments
- Medium: The claim about tasks in different categories showing stronger correlation than tasks within the same category, as this finding may require further validation with additional datasets

## Next Checks
1. Test the CARL-GT benchmark on additional, diverse datasets to ensure its robustness and generalizability across different domains
2. Evaluate the benchmark with newer, more advanced LLMs to assess whether the observed weaknesses in causal reasoning persist
3. Investigate potential biases in the benchmark tasks and evaluation process to ensure fair and accurate assessment of LLMs' causal reasoning capabilities