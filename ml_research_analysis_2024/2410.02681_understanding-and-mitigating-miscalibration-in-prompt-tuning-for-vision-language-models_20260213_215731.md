---
ver: rpa2
title: Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language
  Models
arxiv_id: '2410.02681'
source_url: https://arxiv.org/abs/2410.02681
tags:
- classes
- base
- calibration
- clip
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses confidence miscalibration in vision-language
  models like CLIP after fine-tuning. Existing prompt tuning methods suffer from a
  trade-off: CoOp leads to overconfidence on new classes while KgCoop causes underconfidence
  on base classes.'
---

# Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models

## Quick Facts
- arXiv ID: 2410.02681
- Source URL: https://arxiv.org/abs/2410.02681
- Reference count: 40
- Primary result: DOR reduces ECE by 8.09% on average for CoOp on new classes while maintaining base class calibration

## Executive Summary
This paper addresses the critical issue of confidence miscalibration in vision-language models like CLIP after prompt tuning. The authors identify that existing methods suffer from a trade-off: CoOp leads to overconfidence on new classes while KgCoop causes underconfidence on base classes. To resolve this, they propose Dynamic Outlier Regularization (DOR), which uses textual outliers to regularize unseen textual distributions without interfering with base classes. DOR minimizes feature deviation between fine-tuned and zero-shot models on dynamically sampled outliers from a large vocabulary, achieving better calibration across 11 datasets while maintaining accuracy.

## Method Summary
DOR is a regularization-based approach that dynamically samples textual outliers from WordNet and minimizes feature deviation between fine-tuned and zero-shot models on these outliers. The method is designed to prevent the increase in textual divergence for new labels while easing restrictions on base classes. DOR can be integrated with existing prompt tuning methods by adding a regularization term to their loss functions. The approach uses dynamic sampling in each epoch to prevent overfitting to specific outliers and maintains flexibility across different tuning strategies.

## Key Results
- DOR reduces Expected Calibration Error (ECE) by an average of 8.09% for CoOp on new classes
- DOR maintains base class calibration while improving accuracy on new classes
- DOR is effective when incorporated with other regularization-based tuning methods
- DOR can be extended to visual fine-tuning using image outliers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DOR reduces overconfidence on new classes by regularizing textual feature divergence.
- Mechanism: DOR dynamically samples textual outliers from WordNet and minimizes feature deviation between fine-tuned and zero-shot models on these outliers. This prevents the increase in textual divergence caused by cross-entropy loss, which would otherwise lead to overconfidence on new classes.
- Core assumption: Textual divergence between image and text features is the primary driver of confidence miscalibration in CLIP fine-tuning.
- Evidence anchors:
  - [abstract]: "DOR minimizes the feature deviation of novel textual labels (instead of base classes) sampled from a large vocabulary set. In effect, DOR prevents the increase in textual divergence for new labels while easing restrictions on base classes."
  - [section]: "CoOp leads to overconfidence on new classes by increasing textual label divergence through cross-entropy loss, whereas regularization-based tuning maintains the confidence level but results in underconfidence in base classes due to improved accuracy."
  - [corpus]: Weak evidence - corpus contains calibration-focused papers but no direct textual divergence analysis.
- Break condition: If textual divergence is not the primary driver of miscalibration, or if outliers do not effectively regularize this divergence.

### Mechanism 2
- Claim: DOR maintains base class calibration by not restricting divergence for base classes.
- Mechanism: By using outliers instead of base classes for regularization, DOR avoids constraining the textual feature deviation of base classes. This allows base class accuracy to improve without causing underconfidence issues seen in methods like KgCoOp.
- Core assumption: Regularizing base classes directly (as in KgCoOp) anchors confidence levels and causes underconfidence when accuracy improves.
- Evidence anchors:
  - [abstract]: "DOR prevents the increase in textual divergence for new labels while easing restrictions on base classes."
  - [section]: "KgCoOp constrains the divergence increase, preserving confidence levels across both base and novel classes. However, this leads to the underconfidence issue due to the improved accuracy on base classes."
  - [corpus]: No direct evidence in corpus for base class divergence effects.
- Break condition: If base class accuracy does not improve significantly during fine-tuning, or if base class divergence still needs regularization for optimal calibration.

### Mechanism 3
- Claim: DOR's dynamic sampling strategy prevents overfitting to specific outliers.
- Mechanism: Outliers are dynamically sampled in each epoch from a large vocabulary set, preventing the model from overfitting to a fixed set of outlier texts. This maintains generalization while providing effective regularization.
- Core assumption: Fixed outlier sets can lead to overfitting, reducing regularization effectiveness.
- Evidence anchors:
  - [section]: "To further enhance flexibility, the textual outliers are dynamically sampled from the constructed set in each epoch."
  - [section]: "In this way, we explain why KgCoOp leads to underconfidence on base classes. Through the perspective of textual divergence, we provide a thorough explanation for the calibration trade-off caused by different prompt-tuning methods."
  - [corpus]: No corpus evidence for dynamic sampling benefits.
- Break condition: If dynamic sampling provides no benefit over fixed outlier sets, or if it introduces instability in training.

## Foundational Learning

- Concept: Textual feature divergence and its relationship to confidence calibration
  - Why needed here: The paper's core mechanism relies on understanding how textual feature divergence affects model confidence and calibration. Without this foundation, the DOR approach cannot be properly understood or implemented.
  - Quick check question: How does increasing textual feature divergence between image and text encoders affect softmax confidence outputs?

- Concept: Prompt tuning and cross-entropy loss effects
  - Why needed here: DOR builds on understanding how standard prompt tuning (like CoOp) affects calibration differently than regularization-based methods (like KgCoOp). This distinction is crucial for understanding why DOR's outlier-based approach is needed.
  - Quick check question: Why does cross-entropy loss in prompt tuning lead to overconfidence on new classes while maintaining good calibration on base classes?

- Concept: Zero-shot CLIP calibration characteristics
  - Why needed here: DOR aims to preserve the excellent zero-shot calibration of CLIP while improving fine-tuning performance. Understanding zero-shot CLIP's calibration properties is essential for implementing DOR correctly.
  - Quick check question: What makes zero-shot CLIP well-calibrated compared to fine-tuned versions, and how does DOR aim to preserve this?

## Architecture Onboarding

- Component map: Outlier selection (WordNet) -> Dynamic sampling in each epoch -> Feature deviation minimization between fine-tuned and zero-shot models on sampled outliers -> Integration with existing fine-tuning objective
- Critical path: Outlier selection → Dynamic sampling in each epoch → Feature deviation minimization between fine-tuned and zero-shot models on sampled outliers → Integration with existing fine-tuning objective
- Design tradeoffs: Using textual outliers provides easy access and flexibility but may not capture visual outliers as effectively. The dynamic sampling approach prevents overfitting but adds complexity compared to fixed outlier sets.
- Failure signatures: Overfitting to outliers (fixed sampling), poor outlier selection (irrelevant words), hyperparameter sensitivity (λ weight), or computational overhead from dynamic sampling
- First 3 experiments:
  1. Baseline calibration test: Run CoOp on a small dataset and measure ECE on base vs new classes to confirm the overconfidence pattern
  2. Outlier selection validation: Test different outlier selection strategies (near-OOD vs far-OOD) on a small dataset to confirm the importance of relevant but non-overlapping outliers
  3. Dynamic vs fixed sampling: Compare DOR with static outlier sets on a validation set to confirm the benefits of dynamic sampling for calibration performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Dynamic Outlier Regularization (DOR) perform when extended to other vision-language models beyond CLIP?
- Basis in paper: [explicit] The authors mention that the analysis is limited to CLIP and suggest that future research could extend the insights to other VLMs.
- Why unresolved: The paper primarily focuses on CLIP and does not explore the application of DOR to other VLMs, leaving its effectiveness on different models untested.
- What evidence would resolve it: Empirical studies demonstrating the impact of DOR on the calibration performance of other VLMs, such as Flamingo or BLIP, would provide insights into its broader applicability.

### Open Question 2
- Question: Can the selection strategy for textual outliers be further optimized to improve the calibration performance of DOR?
- Basis in paper: [inferred] The authors discuss the selection of textual outliers from WordNet and mention that the effectiveness of DOR does not rely on overlap with new classes. However, they do not explore alternative selection strategies or optimization techniques.
- Why unresolved: The paper presents a basic approach to selecting outliers but does not investigate whether alternative strategies could enhance DOR's performance.
- What evidence would resolve it: Comparative studies evaluating different outlier selection strategies, such as using semantic similarity or clustering techniques, would determine if optimization is possible.

### Open Question 3
- Question: How does the frequency of outlier updates in DOR affect the calibration performance over time?
- Basis in paper: [explicit] The authors mention that the textual outliers used in each iteration can be different, establishing a dynamic regularization, but do not explore the impact of update frequency on performance.
- Why unresolved: While the paper suggests that dynamic updates are beneficial, it does not provide detailed analysis on how varying the frequency of updates influences calibration performance.
- What evidence would resolve it: Longitudinal studies tracking the calibration performance of DOR with different update frequencies over multiple training epochs would clarify the optimal update strategy.

### Open Question 4
- Question: What is the impact of using different lexical databases on the effectiveness of DOR?
- Basis in paper: [explicit] The authors conduct an ablation study on the choice of lexical databases, including WordNet, CLIP's vocabulary, and ConceptNet, but do not explore the impact of other potential databases.
- Why unresolved: The paper evaluates a limited set of lexical databases and does not investigate whether other databases could offer better performance or insights.
- What evidence would resolve it: Comparative studies using a broader range of lexical databases, such as BabelNet or FrameNet, would reveal if the choice of database significantly affects DOR's effectiveness.

## Limitations
- The core claims about textual divergence driving calibration issues are supported by internal experiments but lack external validation
- The dynamic outlier sampling mechanism may introduce computational overhead that isn't fully quantified
- The approach's generalizability to non-CLIP vision-language models remains untested
- The WordNet-based outlier selection might not capture all relevant textual variations needed for optimal regularization

## Confidence
**High Confidence**: The experimental results showing DOR's effectiveness in reducing ECE on new classes while maintaining base class calibration. The 8.09% average ECE reduction is consistently demonstrated across 11 datasets with multiple evaluation metrics.

**Medium Confidence**: The mechanism explaining why CoOp causes overconfidence (increased textual divergence) and why KgCoop causes underconfidence (restricted base class divergence). While the paper provides theoretical justification and some evidence, direct empirical validation of the textual divergence claims would strengthen this.

**Low Confidence**: The necessity of dynamic outlier sampling versus static outlier sets. The paper claims dynamic sampling prevents overfitting, but doesn't provide direct comparison or ablation studies demonstrating this benefit over well-designed static sets.

## Next Checks
1. **Ablation Study on Outlier Sampling Strategy**: Compare DOR with dynamic sampling against a version with well-constructed static outlier sets to quantify the actual benefit of dynamic sampling on calibration performance.

2. **Generalization Across Model Architectures**: Test DOR on vision-language models beyond CLIP (such as BLIP or Flamingo) to validate whether the textual divergence mechanism applies broadly or is CLIP-specific.

3. **Extended Runtime and Computational Analysis**: Measure the additional training time and memory requirements introduced by DOR's dynamic outlier sampling, especially when scaling to larger vocabularies or more complex visual outlier integration.