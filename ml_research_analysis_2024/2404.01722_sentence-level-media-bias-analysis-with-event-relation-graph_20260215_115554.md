---
ver: rpa2
title: Sentence-level Media Bias Analysis with Event Relation Graph
arxiv_id: '2404.01722'
source_url: https://arxiv.org/abs/2404.01722
tags:
- event
- relation
- bias
- graph
- relations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of sentence-level media bias identification
  by proposing a novel approach that leverages an event relation graph to capture
  contextual information and event-event relations. The key idea is to construct an
  event relation graph for each article, which consists of events as nodes and four
  types of event relations (coreference, temporal, causal, and subevent) as links.
---

# Sentence-level Media Bias Analysis with Event Relation Graph

## Quick Facts
- arXiv ID: 2404.01722
- Source URL: https://arxiv.org/abs/2404.01722
- Reference count: 27
- F1 scores: 52.00% on BASIL, 65.84% on BiasedSents

## Executive Summary
This paper proposes a novel approach for sentence-level media bias identification that leverages event relation graphs to capture contextual information across documents. The key insight is that bias sentences often rely on relationships between events that span multiple sentences, requiring a graph-based representation to effectively model these connections. The approach combines an event-aware language model trained with soft labels from the graph with a relation-aware graph attention network that updates sentence embeddings based on hard labels. Experimental results on two benchmark datasets demonstrate significant improvements over state-of-the-art baselines, achieving F1 scores of 52.00% and 65.84% on the BASIL and BiasedSents datasets respectively.

## Method Summary
The proposed method consists of two main steps: First, an event-aware language model is trained using soft labels derived from an event relation graph that captures event-event relations (coreference, temporal, causal, and subevent) across the document. Second, a relation-aware graph attention network updates sentence embeddings based on hard labels from the same graph, preserving relation semantics during message passing. The event relation graph is constructed by identifying events and extracting the four types of relations, then both soft and hard labels from this graph are used to train the bias detection model. The approach is evaluated on two benchmark datasets using F1 score as the primary metric, with additional evaluation metrics for event identification and relation extraction.

## Key Results
- The proposed approach achieves F1 scores of 52.00% on BASIL and 65.84% on BiasedSents
- Significant improvements over state-of-the-art baselines: +6.16% F1 on BASIL and +3.75% F1 on BiasedSents
- The event relation graph successfully captures contextual event-event relations that improve bias detection
- The two-step framework combining soft and hard labels outperforms single-step approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Event relation graph enables detection of implicit bias by connecting events across sentences
- Mechanism: The graph captures event-event relations (coreference, temporal, causal, subevent) that reveal narrative structures linking bias to factual content
- Core assumption: Bias sentences often rely on contextual event relations that are not explicit within the sentence itself
- Evidence anchors:
  - [abstract]: "we observe that events in a bias sentence need to be understood in associations with other events in the document"
  - [section 1]: "it becomes clear that S6 carries bias if we contrast S6...with what Trump said in the statement...Trump claimed a causal relation between events"
  - [corpus]: Weak - related papers focus on event graphs but not specifically for bias detection

### Mechanism 2
- Claim: Soft labels train the language model to recognize event structures without requiring exact annotations
- Mechanism: Probabilistic event and relation predictions (P_event, P_coref, etc.) guide the model to learn event knowledge while preserving flexibility
- Core assumption: Soft labels provide sufficient signal about event structures while being more robust than hard labels
- Evidence anchors:
  - [section 4.1]: "soft labels derived from the event relation graph, thereby injecting the knowledge of events and event-event relations into the language model"
  - [section 4.1]: "The soft labels generated by the event relation graph...contain informative knowledge of events and event relations"
  - [corpus]: Missing - no direct corpus evidence for soft label effectiveness in this context

### Mechanism 3
- Claim: Relation-aware graph attention network propagates semantic meaning through event-event relations
- Mechanism: The network uses relation embeddings to compute attention weights that preserve relation semantics during message passing
- Core assumption: Relation types carry semantic information that should be preserved during graph propagation
- Evidence anchors:
  - [section 4.2]: "we introduce the relation-aware graph attention network...integrate the semantic meaning of event relations into graph propagation"
  - [section 4.2]: "During the propagation at the l-th layer, the input of i-th event node is the output produced by the previous layer denoted as e(l-1)_i, and the relation embedding r_ij is updated as: r_ij = W_r[e(l-1)_i ⊕ r_ij ⊕ e(l-1)_j]"
  - [corpus]: Weak - related work on graph attention networks but not specifically for event relations in bias detection

## Foundational Learning

- Concept: Event identification and extraction
  - Why needed here: The system needs to recognize events as nodes in the graph before establishing relations
  - Quick check question: Can you identify all events in a given news sentence?

- Concept: Graph attention networks
  - Why needed here: Used to propagate information through the event relation graph while preserving relation semantics
  - Quick check question: How does a standard graph attention network differ from a relation-aware version?

- Concept: Knowledge distillation with soft labels
  - Why needed here: Soft labels from the event relation graph are used to train the language model without requiring exact annotations
  - Quick check question: What is the difference between training with soft labels versus hard labels?

## Architecture Onboarding

- Component map: Event identifier → Relation extractors (coref, temporal, causal, subevent) → Event relation graph → Event-aware language model (soft labels) → Relation-aware graph attention network (hard labels) → Bias classifier

- Critical path: Event identification → Graph construction → Soft label training → Hard label graph encoding → Sentence embedding update → Bias prediction

- Design tradeoffs: Using both soft and hard labels increases complexity but improves performance; using relation-aware attention preserves semantics but increases computational cost

- Failure signatures: Poor event identification leads to incomplete graphs; weak relation extraction results in missing connections; soft label noise causes unstable training; hard label errors propagate through the graph

- First 3 experiments:
  1. Test event identification accuracy on a small sample of articles
  2. Verify graph construction by visualizing event relations for known bias cases
  3. Compare soft label training versus hard label training on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed event relation graph approach compare to other contextual modeling techniques for sentence-level bias detection, such as discourse parsing or topic modeling?
- Basis in paper: [inferred] The paper mentions that their approach outperforms a method incorporating sentence-level relations (Lei et al., 2022) and provides some discussion on the limitations of the current event relation graph in recognizing implicit event relations.
- Why unresolved: The paper does not provide a direct comparison with other contextual modeling techniques like discourse parsing or topic modeling. It only mentions one related work (Lei et al., 2022) and focuses on the event relation graph approach.
- What evidence would resolve it: A comparative study evaluating the performance of the event relation graph approach against other contextual modeling techniques (e.g., discourse parsing, topic modeling) on the same benchmark datasets would provide insights into its relative effectiveness.

### Open Question 2
- Question: How does the performance of the event relation graph approach vary across different domains or types of news articles (e.g., political news, sports news, financial news)?
- Basis in paper: [explicit] The paper mentions that the event relation graph is trained on a general-domain dataset (MA VEN-ERE) and evaluated on two benchmark datasets (BASIL and BiasedSents) that focus on political news articles.
- Why unresolved: The paper does not explore the performance of the approach across different domains or types of news articles. It only provides results on political news datasets.
- What evidence would resolve it: Evaluating the approach on datasets from different domains or types of news articles would provide insights into its generalizability and effectiveness across various contexts.

### Open Question 3
- Question: How does the event relation graph approach handle cases where events are mentioned implicitly or through indirect references, without explicit discourse connectives or language cues?
- Basis in paper: [explicit] The paper mentions that the current event relation graph successfully extracts event relations for most cases with explicit discourse connectives but may encounter challenges in recognizing implicitly stated event relations.
- Why unresolved: The paper acknowledges the limitation but does not provide a detailed analysis or solutions for handling implicit event mentions or indirect references.
- What evidence would resolve it: A detailed analysis of cases where events are mentioned implicitly or through indirect references, along with proposed solutions or modifications to the event relation graph approach to handle such cases, would address this open question.

## Limitations

- The approach heavily depends on the quality of event identification and relation extraction, which may not generalize well beyond news articles or to other languages
- The two-step framework with both soft and hard labels adds considerable complexity without comprehensive ablation studies demonstrating the necessity of both approaches
- Evaluation is limited to two benchmark datasets focusing on political news, which may not capture the full diversity of media bias expressions across different genres and contexts

## Confidence

**High Confidence**: The claim that event relation graphs can improve bias detection by capturing contextual event-event relations is well-supported by the empirical results showing F1 score improvements from 45.84% to 52.00% on BASIL and from 62.09% to 65.84% on BiasedSents. The methodology for constructing event relation graphs and incorporating them through both soft and hard label approaches is clearly specified.

**Medium Confidence**: The assertion that the specific combination of soft label training followed by hard label graph encoding is optimal for this task. While the results show improvement, the paper lacks comprehensive ablation studies comparing different combinations of soft/hard label usage or alternative graph architectures. The claim about relation-aware attention preserving semantic meaning during propagation is plausible but not rigorously validated.

**Low Confidence**: The claim that this approach will generalize to other bias detection tasks or domains. The evaluation is limited to two specific datasets in the news domain, and the heavy reliance on event-centric representations may not capture non-event-based bias expressions effectively.

## Next Checks

1. **Cross-domain validation**: Evaluate the model on non-news domains (social media, academic writing, political speeches) to test generalizability of the event relation graph approach. Measure performance degradation and identify which components (event identification vs. relation extraction) are most sensitive to domain shifts.

2. **Ablation study of soft vs. hard labels**: Systematically test the contribution of each component by comparing: (a) soft labels only, (b) hard labels only, (c) sequential soft-then-hard approach, and (d) the full proposed method. Use statistical significance testing to determine whether the improvements are robust.

3. **Error analysis on implicit relations**: Analyze false negatives where bias is present but the event relation graph fails to capture relevant connections. Focus specifically on cases requiring discourse-level understanding, temporal reasoning beyond sentence boundaries, or implicit causal relations without explicit markers.