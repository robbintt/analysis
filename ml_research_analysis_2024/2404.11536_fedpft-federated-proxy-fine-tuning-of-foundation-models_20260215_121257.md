---
ver: rpa2
title: 'FedPFT: Federated Proxy Fine-Tuning of Foundation Models'
arxiv_id: '2404.11536'
source_url: https://arxiv.org/abs/2404.11536
tags:
- fine-tuning
- fedpft
- sub-fm
- loss
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedPFT proposes a federated fine-tuning framework for Foundation
  Models that addresses insufficient tuning and gradient error accumulation issues.
  The method uses layer-wise compression to construct sub-FMs while preserving layer
  correspondence, and employs a two-step distillation process (layer-level and neuron-level)
  before and during FL fine-tuning to align sub-FMs with FMs.
---

# FedPFT: Federated Proxy Fine-Tuning of Foundation Models

## Quick Facts
- arXiv ID: 2404.11536
- Source URL: https://arxiv.org/abs/2404.11536
- Authors: Zhaopeng Peng, Xiaoliang Fan, Yufan Chen, Zheng Wang, Shirui Pan, Chenglu Wen, Ruisheng Zhang, Cheng Wang
- Reference count: 40
- Primary result: Reduces communication and computational costs by approximately 50% while achieving performance close to full-model fine-tuning

## Executive Summary
FedPFT proposes a federated fine-tuning framework for Foundation Models (FMs) that addresses the dual challenges of insufficient tuning and gradient error accumulation in distributed settings. The method employs layer-wise compression to construct sub-FMs while preserving crucial neuron functionality, followed by a two-step distillation process to align sub-FMs with their parent FMs. Through experiments on seven datasets spanning text and vision tasks, FedPFT consistently outperforms baseline methods while maintaining privacy constraints and reducing resource requirements.

## Method Summary
FedPFT operates through a three-phase approach: First, it constructs sub-FMs from parent FMs using layer-wise compression that eliminates low-saliency neurons based on L2-norm metrics. Second, it aligns these sub-FMs with their parent models through two-step distillation - layer-level distillation during pre-training and neuron-level distillation during federated learning fine-tuning. Finally, clients perform local fine-tuning on their sub-FMs using FedAvg aggregation, with periodic alignment operations to prevent gradient error accumulation. The framework achieves approximately 50% reduction in communication and computational costs compared to full-model approaches.

## Key Results
- Outperforms FedOT and FedPETuning baselines on all seven tested datasets (SST-2, QNLI, MNLI, QQP, CIFAR-10, CIFAR-100, Flowers)
- Achieves performance within 1-3% of full-model fine-tuning while reducing computational costs by ~50%
- Demonstrates consistent improvements across both text (BERT, RoBERTa) and vision (ViT) foundation models
- Shows effective handling of non-IID data partitions through the proposed alignment mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Compression Preserves Crucial Neurons
The framework employs L2-norm based saliency measurement to identify and retain crucial neurons while eliminating low-saliency ones at a 75% ratio. This systematic reduction maintains functional equivalence in critical pathways while reducing model size and communication overhead.

Core assumption: L2-norm correlates with neuron importance for downstream task performance
Evidence: Layer-wise compression claims are supported by architectural descriptions but lack direct corpus validation
Break condition: Performance degrades if compression ratio exceeds neuron importance correlation threshold

### Mechanism 2: Two-Step Distillation Aligns Sub-FM with FM
The method uses layer-level distillation during pre-training and neuron-level distillation during federated learning to transfer knowledge from parent to sub-models. The neuron-level phase employs APoZ metrics to identify neurons requiring updates, preventing gradient error accumulation.

Core assumption: Knowledge distillation effectively transfers FM knowledge to sub-FM
Evidence: Two-step approach is theoretically sound but specific APoZ-based neuron selection lacks extensive validation
Break condition: Distillation fails to align models if APoZ selection is suboptimal

### Mechanism 3: Theoretical Convergence Guarantees
Theorem 1 proves convergence when gradient differences satisfy bounds, while Theorem 2 relates these bounds to output and weight differences in transformer architectures.

Core assumption: Convexity and Lipschitz continuity hold for non-convex FM fine-tuning
Evidence: Theoretical results rely on strong assumptions not fully satisfied in practice
Break condition: Actual problem violates convexity/Lipschitz assumptions, invalidating theoretical guarantees

## Foundational Learning

- **Transformer architecture and attention mechanisms**: Understanding MHA and FFN layer interactions is crucial for compression and distillation strategies
  - Quick check: What happens to attention outputs if we remove certain FFN neurons?

- **Knowledge distillation principles**: The two-step distillation approach relies on transferring knowledge at different granularities
  - Quick check: How does layer-level vs neuron-level distillation differ in information transfer?

- **Federated learning communication patterns**: Understanding FedAvg aggregation is essential for designing alignment strategies
  - Quick check: How does alignment frequency affect convergence and communication costs?

## Architecture Onboarding

- **Component map**: Server (FM, distillation dataset) -> Client (local dataset, sub-FM) -> Alignment coordination
- **Critical path**: 1) Pre-training distillation on Bookcorpus/Wikipedia or ImageNet-1k 2) Client sub-FM construction via layer-wise compression 3) Local sub-FM fine-tuning 4) Periodic neuron-level alignment using APoZ 5) FedAvg aggregation
- **Design tradeoffs**: Compression ratio vs performance; alignment frequency vs adaptation; saliency metric choice
- **Failure signatures**: Early performance plateau (excessive compression); high client variance (data heterogeneity); persistent communication overhead (ineffective compression)
- **First 3 experiments**: 1) Baseline comparison on SST-2 with FedOT and FedPETuning 2) Compression ratio ablation (0.5, 0.75, 0.9) on BERT 3) Alignment frequency variation (5-20 rounds) on CIFAR-10

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the traditional sense, but several areas remain unexplored based on the limitations section and experimental scope. The authors note that extending to larger-scale FMs and more complex downstream tasks represents future work, implicitly acknowledging limitations in current scalability and task complexity coverage.

## Limitations

- Theoretical convergence guarantees rely on convexity assumptions that don't hold for non-convex FM fine-tuning
- Performance gains demonstrated only on relatively small-scale datasets (maximum 50K samples)
- L2-norm based neuron saliency metric lacks empirical validation for identifying crucial neurons
- Compression ratio of 75% is fixed without exploring optimal ratios for different scenarios

## Confidence

- **High confidence**: Performance improvements over baselines on tested datasets; measurable communication cost reduction
- **Medium confidence**: General approach of using sub-models for federated fine-tuning; effectiveness of layer-wise compression
- **Low confidence**: Theoretical convergence guarantees; specific choice of APoZ for neuron selection; scalability to larger models

## Next Checks

1. **Robustness to Compression Ratio**: Test FedPFT across compression ratios 0.5-0.95 on larger datasets like GLUE to identify optimal compression threshold where performance degradation begins.

2. **Theoretical Assumptions Verification**: Conduct ablation studies comparing actual gradient norms against theoretical bounds to measure the gap between convex approximations and non-convex reality.

3. **Alternative Saliency Metrics**: Replace L2-norm with Integrated Gradients or attention-based importance metrics to isolate whether saliency metric choice impacts FedPFT's effectiveness.