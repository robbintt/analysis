---
ver: rpa2
title: A Role-specific Guided Large Language Model for Ophthalmic Consultation Based
  on Stylistic Differentiation
arxiv_id: '2407.18483'
source_url: https://arxiv.org/abs/2407.18483
tags:
- medical
- knowledge
- data
- consultation
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents EyeDoctor, a role-specific guided large language
  model for ophthalmic consultation. The model differentiates between doctor and patient
  roles using separate semantic encoding and integrates an augmented knowledge base
  of 519 ophthalmic diseases to enhance accuracy.
---

# A Role-specific Guided Large Language Model for Ophthalmic Consultation Based on Stylistic Differentiation

## Quick Facts
- arXiv ID: 2407.18483
- Source URL: https://arxiv.org/abs/2407.18483
- Reference count: 40
- Key outcome: EyeDoctor achieves 7.25% improvement in Rouge-1 and 10.16% improvement in F1 scores over ChatGPT on ophthalmic consultation tasks.

## Executive Summary
This paper introduces EyeDoctor, a role-specific guided large language model designed for ophthalmic consultation. The model differentiates between doctor and patient roles using separate semantic encoding and integrates an augmented knowledge base of 519 ophthalmic diseases to enhance accuracy. By employing a hybrid parameter tuning strategy combining Prefix Tuning and LoRA, EyeDoctor effectively addresses challenges of medical specialization, role differentiation, and data scarcity. Experiments demonstrate superior performance over leading models like ChatGPT, highlighting the importance of doctor-patient role differentiation and dynamic knowledge expansion in intelligent medical consultations.

## Method Summary
EyeDoctor uses a hybrid parameter tuning strategy combining Prefix Tuning and LoRA on a base model, integrates doctor-patient role perception via DiagBERT encoders, and enhances responses with an augmented disease knowledge base. The model is trained on multi-round ophthalmic consultation dialogues from Ding Xiang Doctor (8,647 conversations, 108 doctors) and single-round dialogues from QuickAskDoctor (1,292,012 dialogues across 19 departments), with knowledge base from 99 Health Net (519 ophthalmic diseases). The framework uses multi-head self-attention incorporating doctor and patient role representations, with role-specific linguistic features learned through dendritic networks and fused into attention mechanisms.

## Key Results
- Achieves 7.25% improvement in Rouge-1 scores compared to ChatGPT
- Demonstrates 10.16% improvement in F1 scores on multi-round datasets
- Outperforms leading models while maintaining parameter efficiency through hybrid tuning approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differentiating doctor and patient roles during fine-tuning improves response accuracy by aligning generated language with clinical role expectations.
- Mechanism: The model encodes doctor and patient conversation histories separately through DiagBERT, then applies dendritic networks to learn role-specific linguistic features before fusing them into the multi-head attention mechanism.
- Core assumption: Doctor-patient role distinctions are semantically and stylistically separable and can be captured by distinct encoder pathways.
- Evidence anchors:
  - [abstract] "differentiate between doctor and patient roles using separate semantic encoding"
  - [section] "We introduced the EyeDoctor framework, which uses a multi-module pre-training model to differentiate dialogue roles."
- Break condition: If role-specific encoding fails to capture sufficient stylistic variance or if role distinction is ambiguous in input.

### Mechanism 2
- Claim: Augmenting model input with external disease knowledge documents improves diagnostic accuracy and reduces hallucination.
- Mechanism: Uses similarity matching between dialogue embeddings and disease knowledge base entries to select the most relevant document, then concatenates it with dialogue history for input.
- Core assumption: Disease knowledge base entries are semantically well-structured and similar to real consultation dialogues.
- Evidence anchors:
  - [abstract] "integrates an augmented knowledge base of 519 ophthalmic diseases to enhance accuracy"
  - [section] "we collect and organize 519 kinds of ophthalmic disease knowledge information from the 99 Disease Database website."
- Break condition: If disease knowledge entries are too generic or too specific, matching becomes unreliable.

### Mechanism 3
- Claim: Hybrid fine-tuning (Prefix Tuning + LoRA) enables efficient adaptation without full model retraining.
- Mechanism: LoRA updates low-rank matrices in multi-head attention, while Prefix Tuning prepends role-specific vectors to Key and Value projections.
- Core assumption: Most parameter updates can be localized to attention layers without hurting overall language modeling.
- Evidence anchors:
  - [abstract] "hybrid parameter tuning strategy that combines Prefix Tuning and LoRA"
  - [section] "We combine the low-rank matrices... with the Query and Value projection matrices in multi-head attention"
- Break condition: If critical layers outside attention require adaptation, hybrid method underfits.

## Foundational Learning

- Concept: Medical domain fine-tuning
  - Why needed here: General LLMs lack specialized medical terminology and reasoning patterns.
  - Quick check question: What domain-specific pre-training data is used before fine-tuning on ophthalmic dialogues?

- Concept: Role-aware dialogue modeling
  - Why needed here: Doctors and patients use distinct vocabularies and conversational goals; conflating them reduces response quality.
  - Quick check question: How does the model represent and differentiate the linguistic styles of doctors vs patients?

- Concept: Knowledge retrieval integration
  - Why needed here: Models need up-to-date disease information without retraining.
  - Quick check question: What similarity metric is used to select the most relevant disease document?

## Architecture Onboarding

- Component map:
  Data Preprocessing -> DiagBERT Encoder -> Role Dual Coding Learner -> Knowledge Retriever -> Hybrid Fine-Tuning Layer -> Output Generator

- Critical path:
  1. Preprocess dialogue -> Embed with DiagBERT
  2. Split into doctor/patient histories
  3. Apply dendritic networks for role features
  4. Retrieve relevant disease document via cosine similarity
  5. Concatenate retrieved doc + dialogue -> Prefix Tuning & LoRA update
  6. Generate response via multi-head attention

- Design tradeoffs:
  - Full fine-tuning vs hybrid: Full gives more accuracy but is expensive; hybrid trades small accuracy loss for scalability.
  - Separate role encoding vs joint: Separate preserves stylistic nuance but doubles encoding cost.
  - Knowledge retrieval vs parametric memory: Retrieval is updatable but slower; parametric is faster but static.

- Failure signatures:
  - Role confusion: Doctor-style responses in patient turns or vice versa.
  - Knowledge hallucination: Responses cite non-existent symptoms or treatments.
  - Overfitting to training dialogues: Poor generalization to new disease presentations.

- First 3 experiments:
  1. Ablation: Remove role dual coding learner -> measure drop in F1/ROUGE.
  2. Ablation: Remove disease knowledge base -> measure increase in hallucination rate.
  3. Compare LoRA-only vs Prefix-only vs hybrid -> measure parameter efficiency and accuracy.

## Open Questions the Paper Calls Out

- Question: How do different doctor consultation and treatment styles impact the performance of the EyeDoctor model, and can these styles be effectively modeled and incorporated into the framework?
  - Basis in paper: [explicit] The paper mentions that doctors have different consultation, treatment, and prescribing styles, which the model has not yet decomposed and modeled in such fine detail. This will be explored in depth in future work.
  - Why unresolved: The current model does not account for the variations in doctor styles, which could potentially improve the accuracy and personalization of the model's responses.
  - What evidence would resolve it: Experiments comparing the performance of the model with and without the incorporation of different doctor styles, and analysis of the impact on response accuracy and user satisfaction.

- Question: How would expanding the size of the baseline model, such as using a larger version of LLaMA, affect the performance of the EyeDoctor model in ophthalmic consultations?
  - Basis in paper: [explicit] The paper suggests that considering the scale of the current model, expanding the size of the baseline model could be contemplated to achieve better consultation outcomes.
  - Why unresolved: The current model uses LLaMA-2-7B as the baseline, and it is unclear how much improvement could be gained by using a larger model.
  - What evidence would resolve it: Comparative experiments using different sizes of LLaMA models as the baseline, measuring performance metrics such as Rouge-1, Rouge-l, and F1 scores.

- Question: How can the integration of multimodal data, such as medical images, videos, and patient medical history, enhance the diagnostic capabilities and overall effectiveness of the EyeDoctor model?
  - Basis in paper: [explicit] The paper mentions that integrating multimodal data could significantly enhance the model's diagnostic capabilities and overall effectiveness, and future research will focus on leveraging multimodal inputs to create a more comprehensive and accurate ophthalmic consultation system.
  - Why unresolved: The current model primarily relies on text-based data, and it is unclear how much additional value multimodal data would provide.
  - What evidence would resolve it: Experiments incorporating multimodal data into the model and measuring the impact on diagnostic accuracy, response quality, and user satisfaction.

## Limitations

- Performance gains depend heavily on quality of role differentiation and knowledge retrieval, but evaluation lacks error analysis for role confusion or hallucination cases.
- Claims about "outperforming leading models" lack head-to-head comparisons on identical datasets or standardized medical benchmarks.
- Study does not compare against other specialized medical LLMs beyond ChatGPT, limiting generalizability of superiority claims.

## Confidence

- High confidence: The hybrid tuning approach (Prefix + LoRA) is technically sound and reported performance metrics are internally consistent with described methodology.
- Medium confidence: The role differentiation mechanism is plausible but evaluation does not directly measure whether responses actually reflect doctor vs patient stylistic differences.
- Low confidence: Claims about "outperforming leading models" are difficult to verify without standardized medical benchmarks or multiple baseline comparisons.

## Next Checks

1. **Error Analysis Required**: Perform qualitative analysis of 100+ model outputs to quantify role confusion incidents and knowledge hallucination rates, reporting these alongside aggregate metrics.

2. **Cross-Model Validation**: Replicate experiments on a standardized ophthalmology benchmark (e.g., OphthBench) with multiple baselines including other medical-specialized LLMs to validate superiority claims.

3. **Generalization Testing**: Evaluate model performance on ophthalmic dialogues from a different source or time period to assess whether knowledge retrieval and role encoding generalize beyond training distribution.