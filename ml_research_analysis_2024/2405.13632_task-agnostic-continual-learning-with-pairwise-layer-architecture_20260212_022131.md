---
ver: rpa2
title: Task agnostic continual learning with Pairwise layer architecture
arxiv_id: '2405.13632'
source_url: https://arxiv.org/abs/2405.13632
tags:
- learning
- continual
- pairwise
- task
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a static architecture-based approach for task-agnostic
  continual learning. The key idea is to replace the final fully connected layer of
  a neural network with a pairwise interaction layer (PW-layer).
---

# Task agnostic continual learning with Pairwise layer architecture

## Quick Facts
- arXiv ID: 2405.13632
- Source URL: https://arxiv.org/abs/2405.13632
- Authors: Santtu Keskinen
- Reference count: 11
- Key outcome: Proposes PW-layer architecture with k-WTA activation and S-MAS for task-agnostic continual learning, achieving 84.4% single-head accuracy on Split MNIST

## Executive Summary
This paper introduces a static architecture-based approach for task-agnostic continual learning using a Pairwise interaction layer (PW-layer) combined with sparse k-WTA activations. The key innovation is replacing the final fully connected layer with a PW-layer that captures feature interactions through pairwise products of sparse hidden representations. The approach is paired with Streaming Memory Aware Synapses (S-MAS), an algorithm that dynamically adjusts per-parameter learning rates based on importance without requiring task boundaries. Experiments on MNIST, FashionMNIST, and CIFAR-10 demonstrate competitive performance in online streaming continual learning setups where task labels or boundaries are unavailable.

## Method Summary
The proposed method replaces the final fully connected layer of a neural network with a pairwise interaction layer (PW-layer) that operates on sparse representations from k-WTA activation. The k-WTA function creates sparse hidden representations by keeping only the top-k activations above a threshold. The PW-layer then expands these sparse activations into all possible pairwise products, capturing feature interactions. To handle the exponential growth of cross-features, parameter sparsity is introduced to control the number of trainable weights. The system uses Streaming Memory Aware Synapses (S-MAS) or Adagrad to adjust learning rates based on per-parameter importance values, enabling continual learning without explicit task boundaries or task labels during training.

## Key Results
- Achieves 84.4% single-head accuracy on Split MNIST (task-agnostic setup)
- Reaches 98.8% multi-head accuracy on Split MNIST (task-aware evaluation)
- Outperforms comparable methods on some benchmarks
- Shows effective learning in online streaming setup with single epoch training

## Why This Works (Mechanism)

### Mechanism 1
Sparse representations from k-WTA reduce interference between tasks by activating only a limited set of neurons relevant to each task. k-WTA subtracts the (k+1)th highest activation from all hidden layer activations, then applies ReLU, keeping only the top-k values above the threshold. This creates sparse activation patterns that preserve task-specific features while minimizing overlap. Core assumption: Sparse representations enable better separation of task-specific information compared to dense activations.

### Mechanism 2
The Pairwise interaction layer (PW-layer) captures feature interactions by expanding sparse activations into all possible pairwise products. After k-WTA creates sparse hidden representations, the PW-layer computes the product of every pair of active neurons, creating cross-features that represent interactions between features. This expands the representational capacity to capture complex patterns. Core assumption: Pairwise feature interactions are meaningful and capture task-relevant patterns that individual features cannot.

### Mechanism 3
Streaming Memory Aware Synapses (S-MAS) and Adagrad dynamically adjust per-parameter learning rates based on importance, enabling continual learning without task boundaries. Both algorithms maintain importance scores for each parameter that monotonically increase based on their sensitivity to changes. Learning rates are adjusted inversely proportional to the square root of importance, allowing less important parameters to change more freely while preserving critical knowledge. Core assumption: Parameters that are important for previously learned tasks should have their learning rates reduced to prevent catastrophic forgetting.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why sequential learning of tasks causes performance degradation on previously learned tasks is essential to grasp the problem this paper addresses.
  - Quick check question: What happens to a neural network's performance on task A when it's trained on task B without any special techniques to prevent forgetting?

- Concept: Feature crossing and interaction
  - Why needed here: The PW-layer's effectiveness relies on understanding how combinations of features can provide more discriminative power than individual features alone.
  - Quick check question: In a classification problem, why might the interaction between two features (e.g., feature1 × feature2) be more informative than either feature alone?

- Concept: Sparse activation functions and their properties
  - Why needed here: The k-WTA activation function is central to the proposed approach, and understanding its properties is crucial for implementing and tuning the system.
  - Quick check question: How does the k-WTA activation function differ from ReLU in terms of sparsity and the number of neurons that can be active simultaneously?

## Architecture Onboarding

- Component map: Input layer → Backbone network (CNN or MLP with GELU activations) → k-WTA sparse activation layer → Pairwise interaction layer (optional) → Output layer (FC or PW-layer) → Loss function
- Training algorithm: Streaming continual learning with S-MAS or Adagrad for parameter importance calculation
- Data flow: Non-i.i.d. data stream with single epoch training, no task boundaries

- Critical path:
  1. Input data passes through backbone network with GELU activations
  2. k-WTA creates sparse hidden representations
  3. Pairwise interaction layer (if used) expands sparse activations into cross-features
  4. Output layer produces predictions
  5. Loss is computed and gradients are backpropagated
  6. Parameter importance is updated based on gradients or sensitivity
  7. Per-parameter learning rates are adjusted for the next iteration

- Design tradeoffs:
  - Sparse vs. dense representations: Sparsity reduces interference but may limit representational capacity
  - Pairwise vs. fully connected output: Pairwise captures feature interactions but requires more careful tuning of sparsity
  - S-MAS vs. Adagrad: S-MAS uses sensitivity-based importance but is more computationally expensive; Adagrad uses gradient magnitude but may be less direct

- Failure signatures:
  - Performance plateaus or degrades rapidly: May indicate insufficient sparsity or incorrect learning rate
  - Model fails to learn new tasks: May indicate λ is too high, making the network too stable
  - Catastrophic forgetting of old tasks: May indicate λ is too low or sparsity is insufficient
  - Training instability or divergence: May indicate learning rate is too high or initialization is problematic

- First 3 experiments:
  1. Single-head Split MNIST with MLP backbone (1x700 hidden neurons), k-WTA density of 10%, pairwise output layer, S-MAS with λ=0.01, batch size 64
  2. Single-head Split MNIST with MLP backbone (1x700 hidden neurons), k-WTA density of 10%, fully connected output layer, Adagrad with λ=0.8, batch size 64
  3. Single-head Split MNIST with MLP backbone (1x700 hidden neurons), k-WTA density of 20%, pairwise output layer, S-MAS with λ=0.01, batch size 64

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the pairwise interaction layer (PW-layer) compare to other feature crossing methods in continual learning?
- Basis in paper: [explicit] The paper mentions that the PW-layer performs a type of feature crossing, where sparse hidden representations are expanded into all possible pairwise products of the said features. It also states that the PW-layer shows promising results in experiments and, on some benchmarks, beats the best comparable results found in the literature.
- Why unresolved: While the paper provides evidence of the PW-layer's effectiveness, it does not directly compare it to other feature crossing methods in continual learning.
- What evidence would resolve it: Conducting experiments comparing the PW-layer's performance to other feature crossing methods in various continual learning benchmarks.

### Open Question 2
- Question: Can the PW-layer be effectively combined with other continual learning techniques, such as regularization or parameter isolation methods?
- Basis in paper: [inferred] The paper focuses on the PW-layer as a standalone architecture-centric solution to catastrophic forgetting. However, it does not explore its potential synergies with other continual learning techniques.
- Why unresolved: The paper does not investigate the combination of the PW-layer with other techniques, leaving its potential effectiveness in such combinations unknown.
- What evidence would resolve it: Conducting experiments that combine the PW-layer with other continual learning techniques and comparing their performance to the PW-layer alone.

### Open Question 3
- Question: How does the PW-layer's performance scale with increasing network depth and width?
- Basis in paper: [explicit] The paper mentions that deeper backbone architectures (MLP 3x and the 2 layer CNN) are significantly worse than smaller and shallower networks. However, it notes that the performance degrades less with the pairwise output head compared to an FC output head in the case of a 3-layer MLP.
- Why unresolved: While the paper provides some insights into the PW-layer's performance with different network architectures, it does not comprehensively explore its scalability with increasing depth and width.
- What evidence would resolve it: Conducting experiments with PW-layer networks of varying depths and widths and analyzing their performance in continual learning benchmarks.

## Limitations

- Performance gap exists between single-head (84.4%) and multi-head (98.8%) evaluations, suggesting room for improvement in task-agnostic setup
- Scalability to larger datasets and more complex tasks is not demonstrated beyond brief mention of CIFAR-10
- Computational efficiency claims are not fully supported, as pairwise layer introduces exponential growth requiring parameter sparsity

## Confidence

- High confidence: The fundamental problem of catastrophic forgetting is well-established, and the use of sparse representations to reduce interference is theoretically sound
- Medium confidence: Experimental results show competitive performance on standard benchmarks, but absolute performance levels indicate the approach is not state-of-the-art
- Low confidence: Scalability of the pairwise layer to larger problems is unclear, and the focus on relatively simple image classification tasks limits generalizability

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Conduct a systematic ablation study varying the k-WTA density parameter (5%, 10%, 20%, 30%) and the S-MAS λ parameter (0.01, 0.1, 0.5, 1.0) to identify optimal values and robustness to hyperparameter choices.

2. **Scalability Test**: Evaluate the PW-layer architecture on a more challenging dataset like CIFAR-10 or Tiny ImageNet to assess whether the exponential growth of cross-features creates computational bottlenecks.

3. **Ablation of k-WTA vs Standard Sparse Activations**: Replace the k-WTA activation with alternative sparse activations (e.g., L1 regularization, sparsemax) while keeping all other components constant to isolate the contribution of the specific k-WTA mechanism.