---
ver: rpa2
title: An experimental study on fairness-aware machine learning for credit scoring
  problem
arxiv_id: '2412.20298'
source_url: https://arxiv.org/abs/2412.20298
tags:
- credit
- positive
- rate
- abroca
- scoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates fairness-aware machine learning for credit
  scoring across six datasets, testing traditional models against pre-processing,
  in-processing, and post-processing fairness-aware methods. The authors assess model
  performance using accuracy, balanced accuracy, and nine fairness measures.
---

# An experimental study on fairness-aware machine learning for credit scoring problem

## Quick Facts
- arXiv ID: 2412.20298
- Source URL: https://arxiv.org/abs/2412.20298
- Reference count: 40
- Primary result: Fairness-aware ML models improve fairness in credit scoring while maintaining competitive accuracy compared to traditional classifiers

## Executive Summary
This study evaluates fairness-aware machine learning approaches for credit scoring across six datasets, testing traditional models against pre-processing, in-processing, and post-processing fairness-aware methods. The authors assess model performance using accuracy, balanced accuracy, and nine fairness measures. Results show that fairness-aware models, especially AdaFair and LFR-MLP, consistently improve fairness while maintaining competitive accuracy compared to traditional classifiers. The study highlights that no single fairness measure suffices, and that fairness-aware approaches can effectively reduce bias without significant accuracy loss.

## Method Summary
The study evaluates traditional classifiers (Decision Tree, Naive Bayes, MLP, kNN) against six fairness-aware approaches: Learning Fair Representations (pre-processing), Disparate Impact Remover (pre-processing), AdaFair (in-processing), Agarwal's method (in-processing), Equal Opportunity Preprocessor (post-processing), and Classifier Exposure Postprocessor (post-processing). Models are trained on 70% of each dataset and tested on 30%, with performance measured across nine fairness metrics plus accuracy and balanced accuracy. Bayesian network analysis identifies bias in datasets by examining connections between protected attributes and class labels.

## Key Results
- AdaFair and LFR-MLP outperform other fairness-aware models across multiple datasets
- Fairness-aware models achieve significantly better fairness metrics while maintaining competitive accuracy
- No single fairness measure suffices for comprehensive evaluation of model fairness
- Bayesian network analysis reveals inherent bias exists in all six credit scoring datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fairness-aware ML models can reduce bias in credit scoring without significant accuracy loss
- Mechanism: These models explicitly incorporate fairness constraints or adjust data/predictions to mitigate discrimination against protected attributes
- Core assumption: Fairness constraints can be optimized alongside accuracy objectives without severe trade-offs
- Evidence anchors:
  - "Results show that fairness-aware models, especially AdaFair and LFR-MLP, consistently improve fairness while maintaining competitive accuracy compared to traditional classifiers."
  - "fairness-aware models achieve the best values with respect to fairness measures. Among these, LFR-MLP and AdaFair are notable methods that outperform others across multiple datasets."
- Break condition: If the fairness constraints are too strict or the dataset is highly imbalanced, the accuracy loss could become significant

### Mechanism 2
- Claim: No single fairness measure suffices for evaluating credit scoring models
- Mechanism: Different fairness measures capture different aspects of bias (e.g., statistical parity, equal opportunity, equalized odds), and their performance can vary across datasets and models
- Core assumption: The choice of fairness measure significantly impacts the evaluation of model fairness
- Evidence anchors:
  - "The study highlights that no single fairness measure suffices, and that fairness-aware approaches can effectively reduce bias without significant accuracy loss."
  - "We perform the evaluation on the most popular group fairness notions which are used to determine how fair the model's results are."
- Break condition: If all fairness measures consistently show similar results, the need for multiple measures would be diminished

### Mechanism 3
- Claim: Bias exists in credit scoring datasets, necessitating fairness-aware approaches
- Mechanism: Data analysis using Bayesian networks reveals direct or indirect connections between protected attributes and class labels, indicating potential bias in the dataset
- Core assumption: The presence of bias in the dataset leads to biased outcomes in predictive models
- Evidence anchors:
  - "Data analysis using a Bayesian network reveals that bias naturally exists in all the selected datasets, indicating a potential bias in the outcomes of predictive models."
  - "If the generated BN reveals any direct or indirect connection between a protected attribute and the class attribute, we can infer that the dataset may be biased with respect to that specific protected attribute."
- Break condition: If the Bayesian network analysis doesn't reveal any connections between protected attributes and class labels, the assumption of inherent dataset bias would be weakened

## Foundational Learning

- Concept: Fairness-aware machine learning
  - Why needed here: To address the issue of bias in credit scoring models and ensure fair outcomes for all individuals, regardless of protected attributes
  - Quick check question: What are the three main approaches to achieving fairness in machine learning models?

- Concept: Fairness measures
  - Why needed here: To evaluate the fairness of credit scoring models and compare the performance of different fairness-aware approaches
  - Quick check question: What are some commonly used fairness measures in machine learning, and what aspects of bias do they capture?

- Concept: Bayesian networks
  - Why needed here: To analyze the relationships between protected attributes and class labels in credit scoring datasets, identifying potential sources of bias
  - Quick check question: How can Bayesian networks be used to detect bias in datasets, and what does a direct or indirect connection between a protected attribute and class label indicate?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Fairness evaluation -> Bayesian network analysis
- Critical path: Data preprocessing → Model training → Fairness evaluation → Bayesian network analysis
- Design tradeoffs:
  - Accuracy vs. fairness: Balancing the trade-off between model accuracy and fairness constraints
  - Model complexity: Choosing between simpler traditional models and more complex fairness-aware models
  - Computational cost: Considering the additional computational resources required for fairness-aware approaches
- Failure signatures:
  - Significant accuracy loss when applying fairness constraints
  - Inconsistent performance across different fairness measures
  - Inability to detect bias in datasets using Bayesian network analysis
- First 3 experiments:
  1. Train traditional classification models (e.g., Decision Tree, Naive Bayes) on a credit scoring dataset and evaluate their performance using accuracy and fairness measures
  2. Apply a pre-processing fairness-aware approach (e.g., Learning Fair Representations) to the same dataset and compare the results with the traditional models
  3. Use Bayesian network analysis to identify potential bias in the dataset and discuss the implications for model fairness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do fairness-aware models perform when simultaneously addressing multiple protected attributes (e.g., gender and race) rather than a single attribute?
- Basis in paper: The paper mentions plans to expand fairness evaluation to multiple protected attributes in future work
- Why unresolved: Current experiments only use sex as the protected attribute, limiting understanding of multi-attribute fairness scenarios
- What evidence would resolve it: Experimental results comparing model performance across different combinations of protected attributes

### Open Question 2
- Question: What is the relationship between fairness measures and their correlation with each other in credit scoring contexts?
- Basis in paper: The authors plan to further explore correlations between different fairness measures
- Why unresolved: Current analysis treats fairness measures independently without examining their interdependencies
- What evidence would resolve it: Correlation matrices and statistical analyses showing relationships between different fairness measures

### Open Question 3
- Question: How effective are fair synthetic data generation models compared to real-world datasets in credit scoring applications?
- Basis in paper: The authors mention developing fair synthetic data generation models as future work
- Why unresolved: The study only uses existing real-world datasets without exploring synthetic data alternatives
- What evidence would resolve it: Comparative analysis of model performance using synthetic vs. real datasets while maintaining fairness properties

## Limitations
- The study focuses on six specific credit scoring datasets, which may not represent all financial contexts
- Performance of fairness-aware approaches could vary significantly across different domains or datasets
- The choice of nine specific fairness metrics may influence observed outcomes, and other fairness definitions could yield different results

## Confidence
- **High**: The finding that fairness-aware models can improve fairness metrics without severe accuracy loss is well-supported by the experimental results across multiple datasets
- **Medium**: The claim that no single fairness measure suffices is supported by the comprehensive evaluation but may depend on the specific measures chosen
- **Medium**: The assertion that bias exists in credit scoring datasets is supported by Bayesian network analysis but could be dataset-specific

## Next Checks
1. Test the fairness-aware models on additional credit scoring datasets from different geographical regions to assess generalizability
2. Conduct ablation studies to determine which components of the fairness-aware approaches contribute most to improved fairness
3. Perform a sensitivity analysis on hyperparameter choices to understand their impact on the fairness-accuracy trade-off