---
ver: rpa2
title: End-to-end training of Multimodal Model and ranking Model
arxiv_id: '2404.06078'
source_url: https://arxiv.org/abs/2404.06078
tags:
- content
- multimodal
- embeddings
- ranking
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses challenges in traditional recommender systems,
  such as cold-start and generalization problems, by proposing an end-to-end multimodal
  recommendation framework called EM3. The framework integrates multimodal information
  into ranking models, enabling more task-oriented content representations.
---

# End-to-end training of Multimodal Model and ranking Model

## Quick Facts
- arXiv ID: 2404.06078
- Source URL: https://arxiv.org/abs/2404.06078
- Authors: Xiuqi Deng; Lu Xu; Xiyao Li; Jinkai Yu; Erpeng Xue; Zhongyuan Wang; Di Zhang; Zhaojie Liu; Guorui Zhou; Yang Song; Na Mou; Shen Jiang; Han Li
- Reference count: 40
- Primary result: Proposes EM3 framework combining multimodal and ranking models end-to-end, achieving significant offline AUC and online A/B test improvements

## Executive Summary
This paper addresses cold-start and generalization problems in traditional recommender systems by proposing an end-to-end multimodal recommendation framework called EM3. The framework integrates multimodal information into ranking models through three key innovations: Fusion-Q-Former for multimodal fusion, Low-Rank Adaptation for efficient long sequence modeling, and Content-ID-Contrastive learning for aligning content and ID embeddings. Experiments on two production ranking models in e-commerce and advertising scenarios show significant improvements in both offline AUC and online business metrics, contributing to millions in revenue.

## Method Summary
The EM3 framework combines a multimodal model with a ranking model in an end-to-end training pipeline. The multimodal model uses Fusion-Q-Former to fuse different modalities into fixed-length embeddings, while employing LoRA-based sequential modeling to capture user long-term content interest efficiently. A novel Content-ID-Contrastive learning task aligns content and ID embeddings to obtain more task-oriented content representations and generalized ID embeddings. The framework is trained end-to-end on real-world data from e-commerce and advertising scenarios, with single-modal encoders frozen to reduce computational costs while only training the top layers.

## Key Results
- Significant improvements in offline AUC and online A/B test metrics for both e-commerce CTR prediction and ad CVR prediction models
- Contributed to millions in revenue improvement through enhanced recommendation performance
- Outperformed state-of-the-art approaches on public datasets
- Demonstrated effectiveness in addressing cold-start and generalization problems in recommender systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed Fusion-Q-Former can fuse different modalities and generate fixed-length and robust multimodal embeddings.
- Mechanism: The Fusion-Q-Former uses transformers and trainable queries to interact with and fuse different modalities, resulting in fixed-length and robust multimodal embeddings.
- Core assumption: The multimodal embeddings generated by Fusion-Q-Former are more effective for recommendation tasks compared to traditional fusion methods.
- Evidence anchors:
  - [abstract]: "First, we propose Fusion-Q-Former, which consists of transformers and a set of trainable queries, to fuse different modalities and generate fixed-length and robust multimodal embeddings."
  - [section]: "Given the sets of single-modalities ð’—ð‘¨ = {ð‘£1(ð´), . . . , ð‘£ð‘€(ð´)} and ð’•ð‘¨ = {ð‘¡1(ð´), . . . , ð‘¡ð¾(ð´)}, we concatenate them with the globally shared queries ð’’ = [ð‘ž1, . . . , ð‘žð‘„], where ð‘„ is the number of queries. Then we input them into the transformers, which are abbreviated as TRM in eq. (6), aiming to learn the relationships and importance between different modalities using self-attention (SA). Finally, we slice the first ð‘„ output tokens to generate the multimodal content features ð‘ð´."
  - [corpus]: Weak evidence - The related papers mention multimodal fusion and ranking but do not specifically discuss the Fusion-Q-Former or its effectiveness.

### Mechanism 2
- Claim: The proposed Low-Rank Adaptation (LoRA) technique can alleviate the conflict between huge resource consumption and long sequence length in sequential modeling.
- Mechanism: LoRA reduces the number of trainable parameters in the multimodal model by switching from full tuning to partial tuning, allowing for longer sequence lengths without increasing resource consumption.
- Core assumption: LoRA can effectively model user long-term content interest while reducing the computational cost.
- Evidence anchors:
  - [abstract]: "Second, in our sequential modeling for user content interest, we utilize Low-Rank Adaptation technique to alleviate the conflict between huge resource consumption and long sequence length."
  - [section]: "We notice that the parameters in multimodal model will stabilize after a few days of training, which inspires us to utilize the Low-Rank Adaptation (LoRA) technique by switching from full tuning to partial tuning. Let's consider the weights ð‘Š ð‘‘ Ã—ð‘‘ that need to be optimized, we first train it on a short sequence length. After a period of sufficient training, we freeze ð‘Š and add a trainable LoRA module to continuously follow the time-varying online distribution."
  - [corpus]: Weak evidence - The related papers mention low-rank adaptation and sequential modeling but do not specifically discuss the use of LoRA in the context of multimodal recommendation.

### Mechanism 3
- Claim: The proposed Content-ID-Contrastive (CIC) learning task can complement the advantages of content and ID embeddings by aligning them with each other.
- Mechanism: CIC uses a contrastive learning approach to align content and ID embeddings, resulting in more task-oriented content embeddings and more generalized ID embeddings.
- Core assumption: Aligning content and ID embeddings through CIC can lead to improved recommendation performance, especially for cold-start and popular items.
- Evidence anchors:
  - [abstract]: "Third, we propose a novel Content-ID-Contrastive learning task to complement the advantages of content and ID by aligning them with each other, obtaining more task-oriented content embeddings and more generalized ID embeddings."
  - [section]: "We propose a Content-ID-Contrastive (CIC) learning task to complement the advantages of content and ID by aligning them with each other, obtaining more task-oriented content embeddings and more generalized ID embeddings. Given the item ð‘–, we first select several important ID embeddings (e.g. ItemID, CategoryID) and concatenate them as idð‘–. Next, we linearly transform the content embedding ð‘ð‘– and ID embeddings idð‘– into the same vector space: ð¶ð‘– = ð‘“ CIC(ð‘ð‘–), ð¼ð‘– = ð‘“ â€² CIC(idð‘–), where ð‘“ CIC and ð‘“ â€² CIC are the FC layers, ð¶ð‘– and ð¼ð‘– are the output embeddings with the same dimension. They are positive samples of each other in the following contrastive learning."
  - [corpus]: Weak evidence - The related papers mention contrastive learning and multimodal recommendation but do not specifically discuss the use of CIC in the context of multimodal recommendation.

## Foundational Learning

- Concept: Multimodal fusion techniques
  - Why needed here: To effectively combine information from different modalities (e.g., text, image, video) and generate robust multimodal embeddings for recommendation tasks.
  - Quick check question: What are the key differences between traditional fusion methods (e.g., concatenation, attention) and the proposed Fusion-Q-Former approach?

- Concept: Sequential modeling in recommendation systems
  - Why needed here: To capture user content interest over time and model long-term user preferences using historical behavior sequences.
  - Quick check question: How does the use of LoRA in sequential modeling help alleviate the conflict between resource consumption and long sequence length?

- Concept: Contrastive learning
  - Why needed here: To align content and ID embeddings through the proposed CIC learning task, complementing the advantages of both types of embeddings.
  - Quick check question: How does the contrastive learning approach in CIC differ from traditional supervised learning methods used in recommendation systems?

## Architecture Onboarding

- Component map:
  Ranking Model -> Incorporates multimodal content features and user content interest
  Multimodal Model -> Fusion-Q-Former -> Single-modal encoders (Swin-T-22K, RoBERTa-wwm-ext) -> LoRA-based sequential modeling
  Feature System -> Offline cache -> Online cache -> Lookup table

- Critical path:
  1. Extract single-modal features from raw content materials using pre-trained encoders
  2. Fuse different modalities using Fusion-Q-Former to generate fixed-length and robust multimodal embeddings
  3. Model user content interest using LoRA-based sequential modeling on the multimodal embeddings
  4. Align content and ID embeddings using the proposed CIC learning task
  5. Incorporate multimodal content features and user content interest into the ranking model for final recommendations

- Design tradeoffs:
  - Freezing single-modal encoders vs. end-to-end training: Freezing encoders reduces training cost but may limit the flexibility of the model
  - Using fixed-length embeddings vs. variable-length embeddings: Fixed-length embeddings are more suitable for industrial variable-length modalities but may lose some information
  - Increasing sequence length vs. computational cost: Longer sequences can better model user long-term interest but require more computational resources

- Failure signatures:
  - Poor performance on cold-start items: Indicates that the model is not effectively leveraging multimodal content features or the CIC task is not aligning content and ID embeddings properly
  - High resource consumption: Suggests that the LoRA-based sequential modeling is not effectively reducing the computational cost or that the sequence length is too long
  - Overfitting on popular items: Implies that the model is relying too heavily on ID embeddings and not effectively incorporating multimodal content features

- First 3 experiments:
  1. Ablation study on modalities: Compare the performance of the model using different combinations of modalities (e.g., text only, image only, video only, or all modalities) to understand the impact of each modality on the recommendation performance
  2. Ablation study on fusion methods: Compare the performance of the model using different fusion methods (e.g., 1flow, 2flow, masked FQ-Former, or FQ-Former) to evaluate the effectiveness of the proposed Fusion-Q-Former approach
  3. Ablation study on CIC: Compare the performance of the model with and without the CIC learning task to assess the impact of aligning content and ID embeddings on the recommendation performance

## Open Questions the Paper Calls Out

- Question: How does the EM3 framework perform on ranking models in other domains beyond e-commerce and advertising, such as news or music recommendation?
  - Basis in paper: [inferred] The paper demonstrates significant improvements in e-commerce CTR and ad CVR models, and mentions the framework's generalizability, but does not explore other domains.
  - Why unresolved: The paper only evaluates the framework on two specific ranking models in its system. Testing on other domains would validate the broader applicability of EM3.
  - What evidence would resolve it: Conducting experiments on ranking models in different domains (e.g., news, music, or video streaming) and comparing the performance gains to the baseline models.

- Question: What is the impact of incorporating additional modalities, such as audio or user-generated content, on the performance of the EM3 framework?
  - Basis in paper: [explicit] The paper mentions that future work could involve adding more modalities like audio or end-to-end training of MLLM, but does not explore this in the current work.
  - Why unresolved: The current framework only uses visual and text modalities. Testing with additional modalities could provide insights into the framework's scalability and potential for further improvements.
  - What evidence would resolve it: Implementing the EM3 framework with additional modalities (e.g., audio, user-generated content) and evaluating the performance gains compared to the current setup.

- Question: How does the performance of the EM3 framework scale with increasing sequence lengths in user behavior modeling, and what are the computational trade-offs?
  - Basis in paper: [explicit] The paper mentions using LoRA to alleviate the conflict between resource consumption and long sequence lengths, but does not provide detailed analysis on scaling performance.
  - Why unresolved: While the paper addresses the issue of long sequences with LoRA, it does not explore how performance scales with longer sequences or the computational trade-offs involved.
  - What evidence would resolve it: Conducting experiments with varying sequence lengths to analyze the performance gains and computational costs, and determining the optimal sequence length for balancing accuracy and efficiency.

## Limitations

- The paper lacks detailed architectural specifications of the production ranking models used in experiments, limiting reproducibility
- The actual magnitude of resource consumption reduction from LoRA implementation is not quantified
- No ablation study is provided to isolate the individual contribution of each proposed component

## Confidence

- High confidence: The effectiveness of multimodal fusion for improving recommendation performance (supported by significant offline and online metric improvements)
- Medium confidence: The scalability benefits of LoRA for long sequence modeling (mechanism described but resource consumption data not provided)
- Medium confidence: The contribution of CIC to recommendation performance (described theoretically but lacks ablation study evidence)

## Next Checks

1. Conduct ablation studies to isolate the contribution of Fusion-Q-Former, LoRA, and CIC components individually
2. Measure and report actual GPU memory usage and training time with and without LoRA to quantify resource efficiency gains
3. Test the framework on additional ranking models and datasets to verify generalizability beyond the two production models used