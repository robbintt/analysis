---
ver: rpa2
title: 'DeepSeek-VL: Towards Real-World Vision-Language Understanding'
arxiv_id: '2403.05525'
source_url: https://arxiv.org/abs/2403.05525
tags:
- language
- multimodal
- arxiv
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepSeek-VL is a large multimodal model designed for real-world
  vision-language understanding. It addresses the challenge of maintaining strong
  language capabilities while integrating vision understanding through a hybrid vision
  encoder and a training strategy that preserves language skills.
---

# DeepSeek-VL: Towards Real-World Vision-Language Understanding

## Quick Facts
- arXiv ID: 2403.05525
- Source URL: https://arxiv.org/abs/2403.05525
- Reference count: 23
- DeepSeek-VL achieves state-of-the-art performance on vision-language benchmarks while maintaining strong language capabilities

## Executive Summary
DeepSeek-VL is a large multimodal model designed to achieve real-world vision-language understanding by preserving strong language capabilities during multimodal pretraining. The model employs a hybrid vision encoder combining SigLIP and SAM-B to process high-resolution images efficiently, along with a joint vision-language pretraining strategy that maintains at least 70% language data to prevent catastrophic forgetting. DeepSeek-VL-7B demonstrates competitive performance on benchmarks like MMMU (36.6%), MMB (73.2%), and SEED (70.4%), while also excelling in real-world scenarios such as OCR, chart understanding, and code interpretation.

## Method Summary
DeepSeek-VL uses a hybrid vision encoder combining SigLIP and SAM-B to process 1024x1024 images within a fixed token budget. The model employs a three-stage training pipeline: vision-language adaptor warmup, joint vision-language pretraining with a 70% language/30% multimodal data ratio, and supervised fine-tuning. A modality warm-up strategy gradually adjusts the language-to-multimodal data ratio during pretraining to improve stability. The model maintains strong language performance through careful management of modality competition during training.

## Key Results
- Achieves state-of-the-art or competitive performance on MMMU (36.6%), MMB (73.2%), and SEED (70.4%) benchmarks
- Maintains strong language capabilities with MMLU score of 52.4% and HellaSwag score of 68.4%
- Outperforms open-source models in human evaluations and demonstrates robust real-world capabilities including OCR, chart understanding, and code interpretation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeepSeek-VL maintains strong language capabilities during multimodal pretraining by allocating at least 70% of training data to language-only content.
- Mechanism: The model preserves linguistic knowledge by balancing modality ratios during pretraining, preventing catastrophic forgetting of language skills.
- Core assumption: Language and vision modalities compete for representation capacity, and insufficient language exposure leads to degradation of language abilities.
- Evidence anchors:
  - [abstract] "To ensure the preservation of LLM capabilities during pretraining, we investigate an effective VL pretraining strategy by integrating LLM training from the beginning and carefully managing the competitive dynamics observed between vision and language modalities."
  - [section 3.2.2] "Our research reveals that maintaining a significant proportion of language data—specifically, at least 70%—is essential to preserve the integrity of language knowledge within the model."
  - [corpus] Weak - the corpus contains related work but no direct experimental validation of the 70% ratio claim.
- Break condition: If the language-vision competition is not as strong as assumed, or if the model has sufficient capacity to learn both modalities simultaneously without such strict ratios.

### Mechanism 2
- Claim: The hybrid vision encoder combining SigLIP and SAM-B enables efficient processing of high-resolution images (1024x1024) within fixed token budgets.
- Mechanism: SigLIP handles semantic understanding at low resolution while SAM-B captures detailed low-level features at high resolution, and their outputs are fused through the vision-language adaptor.
- Core assumption: Different vision encoders excel at different aspects of image understanding, and combining them provides complementary strengths.
- Evidence anchors:
  - [abstract] "DeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024) within a fixed token budget, while maintaining a relatively low computational overhead."
  - [section 3.1] "Our hybrid vision encoder combines the SAM-B and SigLIP-L encoders, efficiently encoding high-resolution 1024 x 1024 images while preserving both semantic and detailed information."
  - [corpus] Weak - related work exists on hybrid encoders but no direct comparison experiments are cited.
- Break condition: If the computational overhead of running two encoders exceeds the benefits, or if the fusion strategy fails to properly combine the complementary features.

### Mechanism 3
- Claim: Modality warm-up strategy gradually adjusts the language-to-multimodal data ratio during pretraining, preventing instability in early training stages.
- Mechanism: Starting with mostly language data and progressively introducing multimodal content allows the model to stabilize before handling the more complex multimodal distribution.
- Core assumption: Directly mixing multimodal and language data from the start destabilizes training due to the distributional shift between modalities.
- Evidence anchors:
  - [section 3.2.2] "We propose a simple yet effective modality warm-up strategy. Initially, we set the language data ratio to 1, and then gradually decrease it to the target ratio for the final model training (e.g., 0.7)."
  - [section 4.4] "This gradual adaptation enables the model to more seamlessly adjust to the incorporation of multimodal data, thereby improving overall training stability and performance."
  - [corpus] Weak - no external validation of modality warm-up as a general technique is provided.
- Break condition: If the model can handle the distributional shift without warm-up, or if the warm-up schedule is not properly tuned to the specific dataset characteristics.

## Foundational Learning

- Concept: Catastrophic forgetting in sequential learning
  - Why needed here: Understanding why language capabilities degrade when training on multimodal data is crucial for designing effective pretraining strategies.
  - Quick check question: What happens to a neural network's performance on task A when it is sequentially trained on task B without any special mechanisms to preserve task A knowledge?

- Concept: Vision-language representation learning
  - Why needed here: The hybrid encoder design relies on combining different types of visual representations effectively.
  - Quick check question: How do text-aligned vision encoders like CLIP differ from vision-only self-supervised encoders in terms of the features they extract?

- Concept: Multimodal data scaling and curriculum learning
  - Why needed here: The modality warm-up strategy is essentially a form of curriculum learning applied to multimodal pretraining.
  - Quick check question: What are the benefits of gradually increasing task complexity during training, and how does this apply to mixing different data modalities?

## Architecture Onboarding

- Component map: Image → Hybrid Encoder (SigLIP + SAM-B) → Vision-Language Adaptor → LLM → Text Output
- Critical path: Image → Hybrid Encoder → Vision-Language Adaptor → LLM → Text Output
- Design tradeoffs:
  - Running two vision encoders increases computation but provides complementary features
  - Fixed token budget requires careful feature selection and compression
  - Language preservation vs. multimodal capability optimization
- Failure signatures:
  - Language capability degradation indicates insufficient language data ratio
  - Poor visual grounding suggests SAM-B contribution is inadequate
  - Training instability indicates modality warm-up schedule needs adjustment
- First 3 experiments:
  1. Test language-only performance vs. DeepSeek-LLM baseline to verify preservation
  2. Evaluate high-resolution image processing on OCR and detail-sensitive tasks
  3. Measure training stability with different modality mixing ratios (10%, 30%, 50%, 70%)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the hybrid vision encoder's performance compare to using only SigLIP or SAM-B when processing high-resolution images?
- Basis in paper: [explicit] The paper states that the hybrid vision encoder combines SigLIP and SAM-B to efficiently process high-resolution images (1024x1024) while preserving both semantic and detailed information.
- Why unresolved: The paper mentions the motivation for using a hybrid approach but does not provide direct comparisons between the hybrid encoder and using either SigLIP or SAM-B alone.
- What evidence would resolve it: Experimental results comparing the performance of the hybrid encoder against using only SigLIP or SAM-B on tasks requiring high-resolution image processing.

### Open Question 2
- Question: What is the optimal balance between language and multimodal data in the pretraining stage to maximize both language and multimodal performance?
- Basis in paper: [explicit] The paper investigates different modality mixing ratios and finds that a 70% language to 30% multimodal data ratio is optimal for preserving language capabilities while enhancing multimodal abilities.
- Why unresolved: While the paper identifies a preferred ratio, it does not explore whether this ratio is optimal across different model sizes or task domains.
- What evidence would resolve it: Comparative studies across various model sizes and task domains to determine if the 70:30 ratio remains optimal or if adjustments are needed.

### Open Question 3
- Question: How does the modality warm-up strategy affect the long-term performance and stability of the model during pretraining?
- Basis in paper: [explicit] The paper introduces a modality warm-up strategy that gradually adjusts the language data ratio from 1 to 0.7 during training to prevent destabilization.
- Why unresolved: The paper demonstrates the immediate benefits of the warm-up strategy but does not assess its impact on long-term training stability or final model performance.
- What evidence would resolve it: Longitudinal studies comparing models trained with and without the modality warm-up strategy to evaluate differences in training stability and final performance.

## Limitations

- The 70% language data ratio for preserving language capabilities lacks rigorous ablation studies to determine the optimal threshold
- Computational efficiency claims for the hybrid vision encoder need verification, as running two separate encoders could introduce significant overhead
- The modality warm-up strategy requires careful tuning of the warm-up schedule that may not generalize across different dataset distributions or model scales

## Confidence

**High Confidence**: The model's strong performance on established benchmarks (MMMU, MMB, SEED) and its ability to maintain language capabilities during multimodal pretraining are well-supported by the experimental results.

**Medium Confidence**: The claims about computational efficiency and the necessity of the 70% language ratio during pretraining have moderate support but would benefit from more extensive ablation studies.

**Low Confidence**: The generalization of the modality warm-up strategy across different model scales and dataset distributions is not well-established, as the paper only demonstrates its effectiveness for DeepSeek-VL.

## Next Checks

1. **Language Capability Preservation Validation**: Conduct a systematic ablation study varying the language-to-multimodal data ratio (50%, 60%, 70%, 80%) to quantify the exact relationship between language data proportion and language capability preservation.

2. **Computational Efficiency Benchmark**: Compare the actual inference latency and memory consumption of DeepSeek-VL's hybrid vision encoder against single-encoder alternatives (pure SigLIP or pure SAM-B) on high-resolution image processing tasks.

3. **Modality Warm-up Schedule Sensitivity**: Test different warm-up schedules (linear, exponential, step-wise) and starting ratios (0.5, 0.7, 0.9) to determine the robustness of the modality warm-up strategy.