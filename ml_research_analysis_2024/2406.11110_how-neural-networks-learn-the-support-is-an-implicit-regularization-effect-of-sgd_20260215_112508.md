---
ver: rpa2
title: How Neural Networks Learn the Support is an Implicit Regularization Effect
  of SGD
arxiv_id: '2406.11110'
source_url: https://arxiv.org/abs/2406.11110
tags:
- irrelevant
- first
- networks
- have
- components
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates how deep neural networks identify the support\
  \ of the target function\u2014the subset of input features that significantly influence\
  \ the output. Through theoretical analysis and empirical experiments, the authors\
  \ demonstrate that mini-batch SGD effectively learns this support in the first layer\
  \ by shrinking to zero the weights associated with irrelevant input components."
---

# How Neural Networks Learn the Support is an Implicit Regularization Effect of SGD

## Quick Facts
- **arXiv ID:** 2406.11110
- **Source URL:** https://arxiv.org/abs/2406.11110
- **Reference count:** 40
- **Primary result:** Mini-batch SGD implicitly identifies the support of the target function by shrinking irrelevant input weights to zero, driven by a second-order regularization effect proportional to η/b.

## Executive Summary
This work investigates how deep neural networks identify the support of the target function—the subset of input features that significantly influence the output. Through theoretical analysis and empirical experiments, the authors demonstrate that mini-batch SGD effectively learns this support in the first layer by shrinking to zero the weights associated with irrelevant input components. This occurs due to a second-order implicit regularization effect proportional to the ratio of learning rate to batch size (η/b). While full-batch GD also approximates the target function, it typically does not identify the support in the first layer without explicit regularization. The research reveals that smaller batch sizes enhance feature interpretability and reduce dependency on initialization, providing insights into how neural networks represent features and suggesting optimization strategies for more interpretable models.

## Method Summary
The authors combine theoretical analysis with empirical experiments to investigate how mini-batch SGD identifies the support of the target function. They analyze two-layer linear networks, deriving that the implicit regularization effect in the first layer is proportional to the ratio of learning rate to batch size (η/b). This effect causes weights corresponding to irrelevant input components to shrink to zero. The study uses synthetic datasets where the true support is known, comparing the performance of mini-batch SGD and full-batch GD. Experiments are conducted with varying batch sizes, learning rates, and network widths to validate the theoretical findings and explore the impact on feature interpretability.

## Key Results
- Mini-batch SGD implicitly identifies the support of the target function by shrinking irrelevant input weights to zero in the first layer.
- The implicit regularization effect is proportional to the ratio of learning rate to batch size (η/b), enhancing interpretability with smaller batch sizes.
- Full-batch GD approximates the target function but does not identify the support in the first layer without explicit regularization.

## Why This Works (Mechanism)
The mechanism behind support identification in mini-batch SGD is rooted in its second-order implicit regularization effect. This effect arises from the stochastic nature of mini-batch updates, which introduces noise that biases the optimization toward solutions where weights for irrelevant features shrink to zero. The strength of this regularization is proportional to the ratio of learning rate to batch size (η/b). Smaller batch sizes amplify this effect, leading to better feature selection and reduced dependency on initialization. In contrast, full-batch GD lacks this stochastic noise, resulting in solutions that approximate the target function but do not inherently identify the support without explicit regularization.

## Foundational Learning
- **Implicit regularization**: Understanding how optimization algorithms like SGD introduce regularization effects without explicit penalty terms. *Why needed:* The study hinges on identifying the implicit regularization effect in SGD. *Quick check:* Review the derivation of the implicit regularization term proportional to η/b.
- **Support of a function**: The subset of input features that significantly influence the output. *Why needed:* The paper focuses on how neural networks identify this subset. *Quick check:* Verify the definition and role of support in the context of target functions.
- **Mini-batch vs. full-batch GD**: The differences in optimization dynamics and their impact on feature selection. *Why needed:* The study compares these methods to highlight the unique properties of mini-batch SGD. *Quick check:* Compare the convergence and support identification properties of both methods in the experiments.
- **Second-order effects in optimization**: How higher-order terms in the optimization process influence the solution. *Why needed:* The implicit regularization effect is a second-order phenomenon. *Quick check:* Analyze the role of second-order terms in the theoretical derivation.
- **Feature interpretability**: The extent to which a model's learned features can be understood and mapped to input components. *Why needed:* The study emphasizes how mini-batch SGD enhances interpretability. *Quick check:* Evaluate the experiments demonstrating improved interpretability with smaller batch sizes.
- **Initialization dependency**: How the starting point of optimization affects the final solution. *Why needed:* The study shows that mini-batch SGD reduces dependency on initialization. *Quick check:* Review the experiments comparing solutions with different initializations.

## Architecture Onboarding
- **Component map**: Input layer -> First layer weights -> Support identification (implicit regularization) -> Output layer approximation.
- **Critical path**: The first layer's weights are the critical path for support identification, driven by the implicit regularization effect of mini-batch SGD.
- **Design tradeoffs**: Mini-batch SGD offers better support identification and interpretability but may require tuning of batch size and learning rate. Full-batch GD provides stable convergence but lacks inherent support identification.
- **Failure signatures**: If batch size is too large, the implicit regularization effect diminishes, leading to poor support identification. If learning rate is too high, convergence may become unstable.
- **First experiments**:
  1. Vary batch size while keeping learning rate fixed to observe the impact on support identification.
  2. Compare solutions from mini-batch SGD and full-batch GD with the same initialization.
  3. Test the effect of different initializations on support identification in mini-batch SGD.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical analysis primarily focuses on two-layer linear networks, limiting generalizability to deeper, nonlinear architectures.
- The study does not extensively explore how the implicit regularization effect in the first layer interacts with subsequent layers.
- Experiments rely on synthetic datasets, which may not fully reflect the complexity of real-world data.

## Confidence
- **High:** The theoretical analysis linking implicit regularization to η/b is clear and testable.
- **Medium:** The empirical validation is robust for synthetic datasets but may not generalize to real-world scenarios.
- **Medium:** The findings are compelling for two-layer networks but require further validation for deeper architectures.

## Next Checks
1. Extend the theoretical analysis to deeper networks with nonlinear activations to assess the generalizability of the implicit regularization effect.
2. Conduct experiments on real-world datasets to validate the support identification mechanism in practical scenarios.
3. Investigate how the implicit regularization effect in the first layer interacts with subsequent layers, particularly in deeper architectures.