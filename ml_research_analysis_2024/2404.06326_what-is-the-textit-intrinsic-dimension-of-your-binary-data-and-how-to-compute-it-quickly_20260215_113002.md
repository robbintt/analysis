---
ver: rpa2
title: What is the $\textit{intrinsic}$ dimension of your binary data? -- and how
  to compute it quickly
arxiv_id: '2404.06326'
source_url: https://arxiv.org/abs/2404.06326
tags:
- data
- dimension
- intrinsic
- sets
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the intrinsic dimension (ID) of binary
  data tables using formal concepts from Formal Concept Analysis (FCA). While previous
  work focused on correlation dimensions based on distance functions, the authors
  introduce an approximation method for the ID that leverages concepts with minimum
  support values.
---

# What is the $\textit{intrinsic}$ dimension of your binary data? -- and how to compute it quickly

## Quick Facts
- **arXiv ID:** 2404.06326
- **Source URL:** https://arxiv.org/abs/2404.06326
- **Reference count:** 40
- **Key outcome:** Introduces approximation method for intrinsic dimension using formal concepts with minimum support thresholds, providing efficient upper and lower bounds on binary data dimensionality.

## Executive Summary
This paper addresses the challenge of computing the intrinsic dimension of binary data tables using concepts from Formal Concept Analysis (FCA). While traditional approaches like normalized correlation dimension rely on distance functions in artificial Euclidean spaces, the authors propose a geometric intrinsic dimension that uses formal concepts as measuring instruments. The key innovation is an efficient approximation method that computes upper and lower bounds on the intrinsic dimension by considering only concepts above a minimum support threshold, significantly reducing computational complexity while maintaining meaningful results.

## Method Summary
The method computes intrinsic dimension by treating formal concepts as feature functions and measuring the observable diameter of the data push-forward under these functions. To improve efficiency, it restricts analysis to concepts with support above a threshold, calculating lower and upper bounds by sweeping through breakpoints in the observable diameter function. The approach leverages existing concept mining tools (fcbo/pcbo) and introduces a preprocessing step that collects extent/intent size pairs, sorts them, and performs a breakpoint sweep to identify where the observable diameter changes. This reduces runtime compared to full concept enumeration while providing meaningful dimensionality bounds.

## Key Results
- The intrinsic dimension captures different aspects of binary data compared to normalized correlation dimension, with results often falling between computed bounds
- The approximation method successfully handles datasets ranging from small (chess, connect) to large (kosarak, accidents) with varying sparsity levels
- For sparse datasets, the method identifies that intrinsic dimension bounds become very wide, revealing limitations of the approach on low-support-concentrated data

## Why This Works (Mechanism)
### Mechanism 1
- Claim: The intrinsic dimension can be estimated by restricting attention to formal concepts above a minimum support threshold.
- Mechanism: By limiting the set of feature functions to those derived from concepts with support ≥ s, the observable diameter function ObsDiam becomes piecewise constant with fewer breakpoints. The integral of ObsDiam over α ∈ [0, 0.5] then yields upper and lower bounds on the true intrinsic dimension.
- Core assumption: The set of concepts with support ≥ s is a subset of all concepts, so ObsDiam(D(K); −α) ≥ ObsDiam(Ds(K); −α) for all α, and dropping lower-support concepts only lowers ObsDiam for α > α−1.
- Evidence anchors:
  - [section] Proposition 1 and surrounding discussion prove the bounds.
  - [abstract] "approximation method for the ID that leverages concepts with minimum support values."
  - [corpus] No direct evidence; corpus discusses unrelated intrinsic dimension estimation methods.
- Break condition: If the data distribution concentrates heavily on low-support concepts, the bounds become very wide, reducing practical utility.

### Mechanism 2
- Claim: The normalized correlation dimension and geometric intrinsic dimension capture different aspects of binary data.
- Mechanism: The normalized correlation dimension measures scaling behavior of distances in an artificial Euclidean embedding, while the geometric intrinsic dimension uses formal concepts as measuring instruments, reflecting the true combinatorial structure of the data.
- Core assumption: Different dimensionality notions are not necessarily correlated; they measure distinct structural properties.
- Evidence anchors:
  - [abstract] "the ID captures different aspects of the data compared to correlation dimensions."
  - [section] Table 3 shows ncdA values often fall between the ID bounds.
  - [corpus] Weak evidence; corpus papers focus on different ID estimation contexts (e.g., generative models).
- Break condition: If the data has a simple metric structure, both measures might align closely, reducing discriminative power.

### Mechanism 3
- Claim: Algorithmic efficiency is achieved by computing only the "staircase" breakpoints of ObsDiam instead of all concepts.
- Mechanism: The preprocessing step collects all (extent, intent size) pairs, sorts by intent size, and then sweeps through them to find α values where ObsDiam changes. This reduces runtime compared to full concept enumeration.
- Core assumption: The ObsDiam function is piecewise constant and its breakpoints correspond exactly to distinct intent sizes in the concept set.
- Evidence anchors:
  - [section] "For finite contexts, there is only a finite number of values for α at which the ObsDiam changes."
  - [section] Algorithm 1-3 describe the breakpoint-based computation.
  - [corpus] No evidence; corpus does not discuss algorithmic optimizations for ID estimation.
- Break condition: If the concept set is extremely large, even the breakpoint enumeration may become a bottleneck.

## Foundational Learning
- Concept: Formal Concept Analysis (FCA) and formal concepts
  - Why needed here: The method relies on using formal concepts as measuring instruments for intrinsic dimension.
  - Quick check question: What are the two components of a formal concept in FCA?
- Concept: Normalized correlation dimension
  - Why needed here: It is the baseline method being compared against the new intrinsic dimension approximation.
  - Quick check question: How is the normalized correlation dimension defined in terms of independent data sets?
- Concept: Observable diameter and push-forward measures
  - Why needed here: These are the core mathematical constructs used to define the geometric intrinsic dimension.
  - Quick check question: What does the observable diameter represent in terms of the feature functions?

## Architecture Onboarding
- Component map: Input -> Concept mining (fcbo/pcbo) -> Preprocessing (extent/intent sizes) -> Breakpoint sweep -> ObsDiam computation -> ID bounds
- Critical path: Concept mining -> Preprocessing -> Breakpoint sweep. Any failure in concept mining halts the pipeline.
- Design tradeoffs: Using only concepts above minimum support speeds up computation but may yield loose bounds if low-support concepts are important.
- Failure signatures: Concept mining timeouts, segfaults on large datasets, or empty concept sets leading to zero bounds.
- First 3 experiments:
  1. Run on a small synthetic binary dataset (e.g., 100x50) with known ground-truth dimension to verify correctness of bounds.
  2. Compare ID bounds for increasing minimum support thresholds on a medium dataset (e.g., mushroom) to observe bound tightening.
  3. Stress-test concept mining on a large sparse dataset (e.g., kosarak) to identify performance bottlenecks.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the computational feasibility of calculating intrinsic dimension be improved for large, sparse datasets with many concepts concentrated around small minimum support values?
- Basis in paper: [explicit] The paper mentions that for sparse datasets, the interval described by upper and lower bounds for the ID is very wide, providing less information even at very low support. It suggests that modifying support-driven algorithms like Titanic could enhance computational feasibility.
- Why unresolved: The paper identifies the problem but does not provide a concrete solution or algorithm modification to address it.
- What evidence would resolve it: A modified algorithm or approach that successfully computes the intrinsic dimension for large, sparse datasets with improved accuracy and reduced computational time.

### Open Question 2
- Question: What is the relationship between the intrinsic dimension calculated using formal concepts and other dimensionality reduction techniques in formal concept analysis, such as Boolean matrix factorization?
- Basis in paper: [inferred] The paper focuses on using formal concepts as measuring instruments for intrinsic dimension, while other works in the field use different approaches like Boolean matrix factorization. A comparison could reveal insights into the strengths and limitations of each method.
- Why unresolved: The paper does not compare the intrinsic dimension based on formal concepts with other dimensionality reduction techniques in formal concept analysis.
- What evidence would resolve it: A comparative study analyzing the results of intrinsic dimension calculation using formal concepts versus other dimensionality reduction techniques on the same datasets, highlighting similarities, differences, and potential applications.

### Open Question 3
- Question: How do the results of the normalized correlation dimension and the geometric intrinsic dimension compare for different types of binary datasets, and what does this reveal about the underlying structure of the data?
- Basis in paper: [explicit] The paper contrasts the normalized correlation dimension with the geometric intrinsic dimension and observes that for four out of five datasets, the normalized correlation dimension falls between the bounds of the geometric intrinsic dimension. It suggests that both methods capture different aspects of the data's dimensionality.
- Why unresolved: The paper provides a limited comparison and does not explore the implications of the differences in results for various types of binary datasets.
- What evidence would resolve it: A comprehensive analysis of the normalized correlation dimension and geometric intrinsic dimension across a diverse range of binary datasets, including different sizes, densities, and structures, to identify patterns and gain insights into the data's intrinsic dimensionality.

## Limitations
- Computational complexity of concept mining becomes prohibitive for large or dense binary datasets
- Method effectiveness heavily depends on concept support distribution - wide bounds when data concentrates on low-support concepts
- Paper does not provide exact normalized correlation dimension values for comparison, requiring reimplementation

## Confidence
- **High confidence**: The theoretical foundation of using formal concepts as measurement instruments (Proposition 1 bounds) is well-established within FCA framework
- **Medium confidence**: The empirical results showing different aspects captured by GID versus ncd are convincing but limited to 10 datasets with varying characteristics
- **Medium confidence**: The algorithmic efficiency claims are supported by the breakpoint-based computation approach, though performance on extremely large datasets remains untested

## Next Checks
1. Test the method on synthetic binary datasets with known intrinsic dimension to verify bound tightness and correctness across controlled parameters
2. Compare the computational runtime of the breakpoint-based algorithm versus full concept enumeration on medium-sized datasets to quantify efficiency gains
3. Analyze the correlation between concept support distribution and bound tightness across the 10 datasets to identify failure conditions where the method becomes less useful