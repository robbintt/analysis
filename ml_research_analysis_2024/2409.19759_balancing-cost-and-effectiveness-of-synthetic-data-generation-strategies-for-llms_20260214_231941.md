---
ver: rpa2
title: Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for
  LLMs
arxiv_id: '2409.19759'
source_url: https://arxiv.org/abs/2409.19759
tags:
- question
- data
- synthetic
- seed
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper studies cost-effectiveness of different synthetic data
  generation strategies for fine-tuning large language models. Three methods are compared:
  Answer Augmentation (generating new responses to existing questions), Question Rephrase
  (rephrasing existing questions), and New Question (creating entirely new questions).'
---

# Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs

## Quick Facts
- arXiv ID: 2409.19759
- Source URL: https://arxiv.org/abs/2409.19759
- Authors: Yung-Chieh Chan; George Pu; Apaar Shanker; Parth Suresh; Penn Jenks; John Heyer; Sam Denton
- Reference count: 39
- One-line primary result: The optimal synthetic data generation strategy depends on the query budget-to-seed instruction ratio, with Answer Augmentation best for limited queries and New Question optimal as budget increases.

## Executive Summary
This paper systematically studies the cost-effectiveness of three synthetic data generation strategies for fine-tuning large language models: Answer Augmentation, Question Rephrase, and New Question. The research identifies that the optimal strategy depends on the ratio of available query budget to seed instruction size, with answer augmentation being most effective when query resources are limited, while generating entirely new questions becomes optimal as the budget increases. The study evaluates these strategies across math, coding, and general question answering tasks, providing a framework for practitioners to make informed decisions about synthetic data generation based on their resource constraints.

## Method Summary
The paper investigates synthetic data generation for LLM fine-tuning using three strategies: Answer Augmentation (generating new responses to existing questions), Question Rephrase (rephrasing existing questions), and New Question (creating entirely new questions). The method involves using a teacher LLM to generate responses and an augmentation LLM to transform instructions, with a student LLM learning from the synthetic data. The study evaluates these approaches across three tasks (math, coding, and general QA) using different model combinations and synthetic data sizes, measuring student LLM accuracy on held-out test sets.

## Key Results
- The optimal synthetic data generation strategy depends on the budget ratio (BR) - Answer Augmentation is most effective when BR is low, while New Question becomes optimal as BR increases
- Question Rephrase shows robustness to weaker augmentation models, while New Question's performance strongly depends on augmentation model quality
- Verification of synthetic responses provides minimal benefit to student model performance, with scaling up dataset size being more effective
- The choice of student model has limited impact on cost-effectiveness patterns across the three generation strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimal synthetic data generation strategy depends on the ratio between available query budget and seed instruction set size.
- Mechanism: When the query budget is small relative to seed size (low BR), generating new answers to existing questions maximizes effective use of limited resources. As the ratio increases, generating new questions becomes optimal because the model can afford to explore more diverse instruction-response pairs.
- Core assumption: Each query to the teacher model provides diminishing returns, so early queries should be used to diversify responses rather than creating new instructions.
- Evidence anchors:
  - [abstract] "When this ratio is low, generating new answers to existing questions proves most effective, but as this ratio increases, generating new questions becomes optimal."
  - [section 5.2] "Answer augmentation is most effective option when our budget ratio (BR) is low... creating new prompts, such as with new questions or rephrased questions, becomes the best option when the BR is high"
- Break condition: If the teacher model's responses are highly variable or the augmentation model is weak, the diminishing returns per query may be less pronounced, potentially invalidating this mechanism.

### Mechanism 2
- Claim: Question rephrasing is robust to weaker augmentation models while new question generation is sensitive to augmentation model quality.
- Mechanism: Rephrasing existing questions requires less semantic understanding and domain knowledge than generating entirely new questions, making it less dependent on the augmentation model's capability.
- Core assumption: Generating new questions conditioned on existing instructions is a harder task than reformulating existing questions.
- Evidence anchors:
  - [section 5.3.1] "the effectiveness of question rephrasing remains relatively robust, even when weaker augmentation models are used... the performance of new question augmentation is closely tied to the capability of πaug"
  - [section 5.3.1] "At a synthetic data size of 10,000 samples, we notice a substantial decrease in performance when using Llama 2 7B, with accuracy dropping by approximately 15% compared to other choices of πaug"
- Break condition: If the task domain requires highly specific knowledge or the seed instructions are already very diverse, the gap between question rephrasing and new question generation may narrow.

### Mechanism 3
- Claim: Verification of synthetic responses provides minimal benefit to model performance.
- Mechanism: Incorrect responses from a stronger teacher model may still provide useful learning signals, and verification may reduce diversity in the training data without improving accuracy.
- Core assumption: The student model can benefit from exposure to incorrect responses, similar to findings in math reasoning literature.
- Evidence anchors:
  - [section 5.3.2] "πS trained on verified responses does not show significant improvement compared to the gains from scaling up the dataset size"
  - [section 5.3.2] "verification could reduce overall diversity of Dsynth"
  - [section 5.3.2] "Yu et al. (2023), where Llama 2 still showed improvements when trained on incorrect GPT-3.5 responses"
- Break condition: If the teacher model's error rate is very high or the student model is much weaker than the teacher, verification might become more beneficial.

## Foundational Learning

- Concept: Scaling laws and data-constrained learning
  - Why needed here: The paper models accuracy as a function of seed size and query budget using exponential decay formulations adapted from LLM scaling laws
  - Quick check question: How does the accuracy improvement per query change as you repeatedly augment the same instruction set?

- Concept: Knowledge distillation and teacher-student model relationships
  - Why needed here: The study uses a teacher LLM to generate responses and an augmentation LLM to transform instructions, with a student LLM learning from the synthetic data
  - Quick check question: What happens to the student model's performance if the teacher and augmentation models have similar capabilities?

- Concept: Cost-effectiveness analysis and resource allocation
  - Why needed here: The paper introduces budget ratio (BR) as a metric to determine the optimal data generation strategy under resource constraints
  - Quick check question: How would you calculate the budget ratio if you have 1000 seed instructions and a query budget of 25,000?

## Architecture Onboarding

- Component map:
  - Seed instruction set (Iseed) -> Augmentation model (πaug) -> Teacher model (πT) -> Synthetic dataset (Dsynth) -> Student model (πS)

- Critical path:
  1. Select seed instruction set based on task and resource constraints
  2. Choose augmentation method (Answer Augmentation, Question Rephrase, or New Question)
  3. Generate synthetic data using πaug and πT
  4. Fine-tune πS on synthetic dataset
  5. Evaluate πS on held-out test set

- Design tradeoffs:
  - Model capability vs. cost: Using weaker but cheaper augmentation models may be sufficient for some methods
  - Dataset size vs. quality: Larger datasets may provide diminishing returns; verification may reduce diversity
  - Query budget allocation: Deciding between generating new responses vs. new instructions based on BR

- Failure signatures:
  - If πS accuracy plateaus despite increasing dataset size, the augmentation method may be suboptimal for the given BR
  - If question rephrasing performs poorly even with strong augmentation models, the task may require more diverse instructions
  - If verification shows significant improvement, the teacher model's error rate may be too high for effective learning

- First 3 experiments:
  1. Run all three generation methods (Answer Augmentation, Question Rephrase, New Question) with small seed size (100 instructions) and varying query budgets to observe the BR threshold where strategies diverge
  2. Test the robustness of question rephrasing by comparing results using different augmentation model sizes (e.g., Llama 2 7B vs Llama 3.1 70B)
  3. Evaluate the impact of response verification by training on both filtered and unfiltered synthetic datasets with the same total number of samples

## Open Questions the Paper Calls Out
- How do the findings about cost-effectiveness and synthetic data generation strategies apply to other types of tasks beyond math, coding, and general QA?
- How does the choice of augmentation model (π_aug) impact the effectiveness of synthetic data generation strategies for tasks other than math?
- What are the long-term effects of using synthetic data for fine-tuning large language models, and how do these effects compare to using real data?

## Limitations
- The study primarily uses Llama 3.1 70B as the teacher model and Llama 2 7B as the student model, limiting generalizability to other model architectures or capability levels
- The exponential decay formulations for accuracy as a function of seed size and query budget are fitted to observed data but lack theoretical grounding in the underlying learning dynamics of LLMs
- The budget ratio (BR) framework assumes linear query costs across all augmentation methods, but in practice, different methods may have varying computational requirements per query

## Confidence
- High confidence: The finding that answer augmentation is optimal for low BR and new question generation is optimal for high BR is well-supported by extensive experimental results across multiple tasks
- Medium confidence: The robustness of question rephrasing to weaker augmentation models is supported by experiments but may be task-dependent and requires further validation
- Medium confidence: The claim that verification provides minimal benefit is supported by the experiments but may depend heavily on the specific teacher model's error rate and the student model's capability

## Next Checks
1. Conduct experiments with varying seed sizes (50, 500, 5000 instructions) and query budgets to precisely map the BR thresholds where each strategy becomes optimal, and test if these thresholds hold across different teacher-student model pairs
2. Repeat the key experiments using different teacher-student model combinations (e.g., GPT-4 as teacher with Llama 2 7B as student, or Claude 3.5 as teacher) to verify that the identified mechanisms aren't specific to the Llama model family
3. Systematically vary the teacher model's error rate by using models of different capabilities (Llama 2 7B, Llama 3.1 70B, GPT-4) and measure how verification benefits change, particularly focusing on cases where the student model is much weaker than the teacher