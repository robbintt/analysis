---
ver: rpa2
title: 'Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival Outcome
  Prediction'
arxiv_id: '2403.11375'
source_url: https://arxiv.org/abs/2403.11375
tags:
- rna-seq
- scgpt
- bulk
- data
- cell
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of imbalanced training in multi-modal
  deep learning for cancer survival prediction, where one modality (often genomics)
  dominates the optimization process, leading to inadequate training of the other
  modality (often pathology images). The proposed solution, Path-GPTOmic, introduces
  two key innovations: (1) adapting the scGPT foundation model, originally trained
  on single-cell RNA-seq data, for bulk RNA-seq data by regulating the embedding space
  using mix-up techniques; and (2) dynamically balancing the contributions of both
  modalities during training by modulating the gradient of the Cox partial likelihood
  loss based on their relative performance.'
---

# Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival Outcome Prediction

## Quick Facts
- arXiv ID: 2403.11375
- Source URL: https://arxiv.org/abs/2403.11375
- Reference count: 0
- Achieved C-Index of 0.848 ± 0.014 on TCGA-GBMLGG and 0.754 ± 0.030 on TCGA-KIRC datasets

## Executive Summary
This paper addresses the challenge of imbalanced training in multi-modal deep learning for cancer survival prediction, where one modality (often genomics) dominates the optimization process. The proposed Path-GPTOmic framework introduces two key innovations: adapting the scGPT foundation model for bulk RNA-seq data through mix-up-based latent space regulation, and dynamically balancing modality contributions during training through gradient modulation. The method was evaluated on two TCGA datasets and demonstrated improved performance over baseline models.

## Method Summary
Path-GPTOmic is a multi-modal deep learning framework that combines pathology images with genomic data for cancer survival prediction. The method adapts the pre-trained scGPT foundation model for bulk RNA-seq data by regulating its latent space using mix-up techniques. It then dynamically balances the contributions of both modalities during training by modulating the gradient of the Cox partial likelihood loss based on their relative performance. The framework was evaluated on TCGA-GBMLGG and TCGA-KIRC datasets, showing improved C-Index performance compared to baseline models.

## Key Results
- Path-GPTOmic achieved C-Index of 0.848 ± 0.014 on TCGA-GBMLGG dataset
- Path-GPTOmic achieved C-Index of 0.754 ± 0.030 on TCGA-KIRC dataset
- Outperformed baseline models by approximately 0.05-0.07 in C-Index

## Why This Works (Mechanism)

### Mechanism 1
Smooth latent space regulation using mix-up techniques improves bulk RNA-seq embeddings for downstream survival prediction tasks. Mix-up is applied to interpolate between single-cell RNA-seq samples, simulating bulk RNA-seq data. The interpolated samples are processed through a frozen scGPT model followed by a smoothing MLP (MLP-A) and trained to predict the weighted average of cell type labels. This process encourages the embedding space to represent average cell information smoothly, making it more suitable for bulk RNA-seq data. The core assumption is that bulk RNA-seq data can be effectively simulated as weighted averages of single-cell RNA-seq data, preserving meaningful biological relationships for downstream tasks.

### Mechanism 2
Dynamic gradient modulation balances the contributions of pathology images and genomics modalities during training, preventing one modality from dominating the optimization process. The model monitors the contribution discrepancy ratio (ρG, ρP) for the Cox partial log-likelihood loss for each modality. The learning rates for the genomics and pathology image branches are then adjusted using a modulation function that suppresses the learning rate of the modality with higher contribution ratio. This ensures both modalities are sufficiently trained. The core assumption is that the contribution discrepancy ratio accurately reflects the relative importance and performance of each modality, and modulating the learning rate based on this ratio will lead to balanced optimization.

### Mechanism 3
Using a pre-trained foundation model (scGPT) adapted for bulk RNA-seq data provides valuable biological insights regarding genes and gene-gene interactions that are overlooked by standard approaches. scGPT, originally trained on single-cell RNA-seq data from large human cell atlases, has learned rich representations of gene-gene interactions. By adapting scGPT for bulk RNA-seq data through latent space smoothing, the model can leverage this pre-trained knowledge to extract more meaningful biological features for survival prediction. The core assumption is that the gene-gene interactions learned by scGPT on single-cell data are transferable and beneficial for analyzing bulk RNA-seq data in the context of cancer survival prediction.

## Foundational Learning

- **Multi-modal deep learning and imbalanced training**: Why needed here: The paper addresses the problem of one modality dominating the optimization process in multi-modal deep learning for cancer survival prediction. Quick check question: Can you explain why imbalanced training is a problem in multi-modal learning and how it can lead to suboptimal performance?

- **Cox proportional hazards model and partial likelihood loss**: Why needed here: The model uses Cox partial log-likelihood as the cost function to predict survival outcomes, and the gradient modulation mechanism is tailored to this loss function. Quick check question: How does the Cox proportional hazards model differ from standard classification or regression models, and why is it suitable for survival analysis?

- **Foundation models and transfer learning in genomics**: Why needed here: The paper leverages a pre-trained foundation model (scGPT) and adapts it for a new task (bulk RNA-seq analysis) through transfer learning techniques. Quick check question: What are the benefits and challenges of using pre-trained foundation models in genomics, and how does the adaptation process work in this case?

## Architecture Onboarding

- **Component map**: 
  - scGPT -> MLP-A -> Genomic Embedding
  - Image Encoder -> Image Embedding
  - CNV/Mutation SNN -> Genomic Embedding B
  - MLP-B -> Combined Genomic Embedding
  - Classifier -> Hazard Ratio Prediction

- **Critical path**: Data preprocessing and feature extraction (scGPT, Image Encoder, SNN) -> Genomic feature fusion (MLP-B) -> Multi-modal fusion and hazard ratio prediction (Classifier) -> Loss computation and gradient modulation -> Parameter updates for image and genomics branches

- **Design tradeoffs**: Using a pre-trained foundation model (scGPT) provides rich biological insights but requires adaptation for bulk RNA-seq data. The smoothing MLP (MLP-A) helps adapt scGPT but adds complexity and potential overfitting risk. Dynamic gradient modulation balances training but relies on accurate contribution discrepancy ratio estimation.

- **Failure signatures**: Poor performance on one modality may indicate imbalanced training or inadequate feature extraction. Overfitting on the training data may suggest insufficient regularization or data augmentation. Degraded performance compared to single-modal baselines may indicate issues with multi-modal fusion or the adaptation of scGPT.

- **First 3 experiments**: 
  1. Evaluate the performance of the single-modal baselines (image-only and genomics-only) to establish a performance baseline and identify potential imbalances.
  2. Test the adapted scGPT model with and without the smoothing MLP (MLP-A) to assess the impact of latent space regulation on bulk RNA-seq data.
  3. Compare the performance of the full multi-modal model with and without the gradient modulation mechanism to evaluate the effectiveness of balancing the training process.

## Open Questions the Paper Calls Out

### Open Question 1
How does the scGPT foundation model's performance on single-cell RNA-seq data translate to bulk RNA-seq data when applied to different cancer types beyond GBMLGG and KIRC? The paper mentions extending scGPT to bulk RNA-seq data but only evaluates on two TCGA datasets, leaving uncertainty about generalizability to other cancer types with different molecular profiles.

### Open Question 2
What is the optimal balance between the contributions of pathology images and genomic data for survival prediction across different cancer types? The paper addresses imbalanced training between modalities but doesn't explore optimal balance ratios for different cancer types, and the gradient modulation mechanism adjusts contributions dynamically without analyzing whether the optimal balance differs across cancer types or patient subgroups.

### Open Question 3
How does the mix-up regulation technique for smoothing the latent space perform compared to other regularization methods for foundation model adaptation to bulk RNA-seq? While mix-up regulation shows effectiveness, there's no comparison with other techniques like dropout, batch normalization, or variational autoencoders for this specific adaptation task.

## Limitations
- The mix-up adaptation of scGPT for bulk RNA-seq data relies on the assumption that interpolated single-cell samples accurately represent bulk RNA-seq characteristics
- The model requires substantial computational resources for training due to the foundation model components and dynamic gradient adjustments
- Evaluation is limited to only two TCGA datasets (GBMLGG and KIRC), restricting generalizability across diverse cancer types

## Confidence
- **High Confidence**: The overall framework design and the necessity of addressing imbalanced multi-modal training are well-established concepts
- **Medium Confidence**: The specific implementation of mix-up adaptation for scGPT and the gradient modulation mechanism are novel but rely on reasonable assumptions
- **Medium Confidence**: The reported performance improvements (C-Index gains of ~0.05-0.07) are statistically significant but require validation on additional datasets

## Next Checks
1. Test the mix-up adaptation approach on additional cancer types with varying single-cell to bulk RNA-seq characteristics to validate the generalizability of the latent space smoothing technique
2. Conduct ablation studies comparing the gradient modulation mechanism against alternative balancing strategies (such as weighted loss functions or curriculum learning approaches)
3. Perform cross-validation within each dataset and evaluate on external validation cohorts to assess model stability and generalizability beyond the training data distribution