---
ver: rpa2
title: Large Models of What? Mistaking Engineering Achievements for Human Linguistic
  Agency
arxiv_id: '2407.08790'
source_url: https://arxiv.org/abs/2407.08790
tags:
- language
- human
- linguistic
- llms
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that claims about large language models (LLMs)\
  \ possessing human-like linguistic capabilities are based on unfounded assumptions.\
  \ The authors, from an enactive cognitive science perspective, identify three key\
  \ characteristics of enacted language\u2014embodiment, participation, and precariousness\u2014\
  that are absent in LLMs and likely incompatible with current architectures."
---

# Large Models of What? Mistaking Engineering Achievements for Human Linguistic Agency

## Quick Facts
- **arXiv ID:** 2407.08790
- **Source URL:** https://arxiv.org/abs/2407.08790
- **Reference count:** 11
- **Primary result:** LLMs are engineering tools, not linguistic agents; current architectures cannot replicate human linguistic agency due to lack of embodiment, participation, and precariousness

## Executive Summary
This paper critiques the widespread claims that large language models possess human-like linguistic capabilities, arguing these claims stem from fundamental misconceptions about both human language and what LLMs actually are. From an enactive cognitive science perspective, the authors identify three key characteristics of enacted language—embodiment, participation, and precariousness—that are absent in LLMs and likely incompatible with current architectures. They use the phenomenon of "algospeak" (humans developing alternative language to circumvent automated content moderation) as an example of genuine linguistic agency that emerges from human agency and embodiment.

The authors conclude that sensational claims about LLM agency represent a deep misunderstanding of human linguistic capabilities and the nature of these engineering systems. They emphasize that LLMs are sophisticated statistical pattern-matching tools rather than autonomous linguistic agents, and that conflating the two categories leads to unrealistic expectations and misguided applications.

## Method Summary
The paper employs theoretical analysis and argumentation from an enactive cognitive science perspective to critique claims about LLM linguistic agency. The authors examine the conceptual foundations of what constitutes human language and linguistic agency, then systematically demonstrate how current LLM architectures fundamentally differ from these requirements. They use illustrative examples from social media discourse (algospeak) to demonstrate genuine linguistic agency in contrast to LLM capabilities. The methodology relies on philosophical argumentation and conceptual analysis rather than empirical testing or experimental validation.

## Key Results
- Current LLM architectures cannot replicate human linguistic agency due to absence of embodiment, participation, and precariousness
- Claims about LLM agency reflect misconceptions about both human language and the nature of these engineering systems
- LLMs are sophisticated statistical tools, not autonomous linguistic agents
- The phenomenon of algospeak demonstrates genuine linguistic agency emerging from human embodiment and social participation

## Why This Works (Mechanism)
The paper's critique is effective because it grounds the analysis in a specific theoretical framework (enactive cognitive science) that provides clear criteria for what constitutes genuine linguistic agency. By identifying specific characteristics that human language requires but LLMs lack, the authors create a testable framework for evaluating claims about AI capabilities. The algospeak example provides concrete evidence of how human linguistic agency operates in ways that current AI systems cannot replicate, demonstrating that genuine language use emerges from social participation and embodiment rather than pattern matching.

## Foundational Learning

**Enactive Cognitive Science**: Why needed: Provides theoretical framework for understanding language as embodied, participatory, and precarious activity rather than abstract symbol manipulation. Quick check: Can you explain how this differs from computational theories of mind?

**Linguistic Agency**: Why needed: Distinguishes between statistical pattern matching and genuine language use with social consequences. Quick check: What are the key differences between LLM text generation and human linguistic agency?

**Algospeak**: Why needed: Illustrates how human language adapts to social constraints in ways that require embodiment and participation. Quick check: How does algospeak demonstrate linguistic agency that LLMs cannot replicate?

## Architecture Onboarding

**Component Map**: Enactive Theory of Language -> Characteristics of Enacted Language (Embodiment, Participation, Precariousness) -> Critique of LLM Claims -> Illustrative Example (Algospeak) -> Conclusion

**Critical Path**: Theoretical framework establishment → Identification of key linguistic characteristics → Analysis of LLM architecture limitations → Concrete example of genuine linguistic agency → Synthesis of critique

**Design Tradeoffs**: Theoretical rigor vs. empirical testability; abstract conceptual analysis vs. concrete measurable criteria; philosophical perspective vs. engineering considerations

**Failure Signatures**: Overgeneralization of engineering achievements to human-like capabilities; conflation of pattern matching with understanding; neglect of social and embodied dimensions of language

**First Experiments**:
1. Test whether LLM-human interactions generate spontaneous linguistic adaptations comparable to algospeak
2. Develop measurable criteria for embodiment, participation, and precariousness in language systems
3. Compare linguistic agency markers in human-human vs human-LLM communication contexts

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework lacks empirical grounding and testability
- Abstract definitions of key characteristics make definitive assessment difficult
- Single example (algospeak) may not generalize to broader claims about linguistic agency
- Does not address potential counterarguments about emergent properties in large-scale neural networks

## Confidence
- Theoretical framework validity: Medium
- Generalizability of claims: Low
- Empirical support for conclusions: Low

## Next Checks
1. Conduct empirical studies testing whether human language patterns like algospeak emerge spontaneously in LLM-human interactions, and if so, whether this represents genuine linguistic adaptation or mere pattern replication
2. Develop formal operational definitions for the three key characteristics (embodiment, participation, precariousness) that can be tested against current and near-future LLM architectures
3. Design comparative studies measuring the degree of linguistic agency in human-human vs human-LLM communication across multiple domains and interaction contexts