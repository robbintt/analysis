---
ver: rpa2
title: 'Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in eDiscovery'
arxiv_id: '2405.19164'
source_url: https://arxiv.org/abs/2405.19164
tags:
- graph
- documents
- topic
- relevant
- legal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DISCOG, a graph-based method for predictive
  coding in eDiscovery. DISCOG constructs a heterogeneous knowledge graph from emails,
  topics, and extracted keywords, then uses both Knowledge Graph Embedding (KGE) and
  Graph Neural Network (GNN) models for link prediction to classify document relevance.
---

# Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in eDiscovery

## Quick Facts
- arXiv ID: 2405.19164
- Source URL: https://arxiv.org/abs/2405.19164
- Reference count: 19
- This paper introduces DISCOG, a graph-based method for predictive coding in eDiscovery that achieves high F1-score, precision, and recall while reducing review costs by approximately 98%.

## Executive Summary
DISCOG addresses the challenge of predictive coding in eDiscovery by constructing a heterogeneous knowledge graph from emails, topics, and extracted keywords. The method uses both Knowledge Graph Embedding (KGE) and Graph Neural Network (GNN) models for link prediction to classify document relevance. Experiments on the EDRM Enron dataset demonstrate that DISCOG significantly outperforms traditional text-based methods like BM25L and ColBERT v2, achieving superior F1-score, precision, and recall across multiple topics. The approach also reduces litigation-related document review costs by approximately 98% in real-world deployments.

## Method Summary
DISCOG constructs a heterogeneous knowledge graph with documents, topics, keywords, and sender/receiver nodes, creating edges based on semantic similarity and content relationships. The method employs both KGE models (TransE, ComplEx) and GNN models (GraphSAGE, GAT, RGCN) for link prediction between documents and topics. After ranking documents by predicted relevance, the system uses GPT-3.5-turbo to provide human-readable explanations and validate predictions, creating a hybrid graph+LLM approach that enhances accuracy and interpretability in eDiscovery tasks.

## Key Results
- DISCOG outperforms BM25L and ColBERT v2 in F1-score, precision, and recall across multiple topics in the EDRM Enron dataset
- GraphSAGE consistently achieves high recall across most topics, outperforming other graph-based methods
- Real-world deployment demonstrates approximately 98% reduction in litigation-related document review costs while maintaining high recall

## Why This Works (Mechanism)

### Mechanism 1
DISCOG improves predictive coding by leveraging structured relational dependencies captured in heterogeneous knowledge graphs, which complement the limitations of text-only models. By constructing a graph with documents, topics, keywords, and sender/receiver nodes, DISCOG enables link prediction that encodes semantic and relational information (e.g., keyword co-occurrence, sender/receiver relationships) that text-only models miss.

### Mechanism 2
Graph Neural Networks (GNNs) in DISCOG outperform Knowledge Graph Embedding (KGE) methods because they can aggregate features from neighboring nodes, capturing richer context. GNNs like GraphSAGE iteratively aggregate information from neighboring nodes, allowing each node's representation to encode multi-hop relationships (e.g., a document linked to a keyword linked to another document), which improves link prediction accuracy over static embeddings from KGE methods.

### Mechanism 3
LLM-based reasoning in DISCOG improves interpretability and acts as a validation layer, catching errors from the graph model. After DISCOG ranks documents, GPT-3.5-turbo is used to explain predictions using extracted keywords and topic statements, providing human-readable justifications and potentially correcting misclassifications when the LLM disagrees with the graph model.

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGE)
  - Why needed here: KGE methods like TransE and ComplEx learn low-dimensional representations of nodes and edges, enabling link prediction in the graph
  - Quick check question: What is the key difference between TransE and ComplEx in modeling relations?

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs aggregate information from neighboring nodes, capturing multi-hop relationships that static embeddings miss
  - Quick check question: How does GraphSAGE differ from GAT in aggregating neighbor information?

- Concept: Link Prediction
  - Why needed here: Link prediction determines whether a document is relevant to a topic by predicting the existence of an edge between them in the graph
  - Quick check question: In the context of DISCOG, what does a predicted link between a document and a topic signify?

## Architecture Onboarding

- Component map: Data Preprocessing -> Keyword Extraction -> Graph Construction (nodes: docs, topics, keywords, senders/receivers; edges: semantic similarity, containment, relevance) -> Link Prediction (KGE or GNN) -> Ranking -> LLM Reasoning
- Critical path: 1) Extract keywords from documents and topics 2) Build heterogeneous graph with semantic and structural edges 3) Train link prediction model (KGE or GNN) 4) Rank documents by predicted relevance 5) Use LLM to explain/validate top-K predictions
- Design tradeoffs: KGE vs GNN (transductive vs inductive, computational intensity); Graph complexity vs sparsity (signal vs noise); LLM integration (interpretability vs cost/latency)
- Failure signatures: Low recall despite high precision (graph too sparse or threshold too high); LLM gives inconsistent explanations (poor keyword/topic extraction); Model overfits on small seed sets (reduce graph complexity or use regularization)
- First 3 experiments: 1) Baseline: Run BM25L on dataset and measure recall@20000 2) Graph ablation: Compare GraphSAGE performance with and without keyword/sender nodes 3) LLM validation: Compare accuracy of graph-only vs graph+LLM predictions on held-out set

## Open Questions the Paper Calls Out

### Open Question 1
How does DISCOG's performance compare to other graph-based methods like TACG or GraphSAGE when applied to legal datasets beyond EDRM Enron? The paper only uses one dataset (EDRM Enron) and doesn't compare against newer graph-based methods that have been developed since their evaluation.

### Open Question 2
What is the exact contribution of the LLM reasoning component to classification accuracy, and does it consistently correct misclassifications across different types of legal topics? The paper mentions LLM reasoning provides "a second level of check to correct misclassifications" but doesn't quantify this improvement or test its consistency across topic types.

### Open Question 3
How does DISCOG scale to truly massive eDiscovery datasets with hundreds of millions of documents, and what are the computational bottlenecks? The paper claims DISCOG "scales efficiently to large eDiscovery datasets" but only demonstrates on 455,449 emails and doesn't address computational complexity at larger scales.

## Limitations

- Limited real-world deployment data: While claiming 98% cost reduction, specific deployment details and long-term performance metrics are not provided
- Graph construction sensitivity: Performance depends heavily on keyword extraction quality and edge creation thresholds, which may vary across legal domains
- LLM reasoning validation: The effectiveness of LLM-based validation is primarily anecdotal, with limited quantitative analysis of when and how it improves predictions

## Confidence

- High confidence: DISCOG's core graph construction methodology and its superiority over traditional text-based baselines in the EDRM Enron dataset
- Medium confidence: The claimed 98% cost reduction in real-world deployments, as specific implementation details are sparse
- Medium confidence: The effectiveness of LLM-based reasoning for validation, though the mechanism is plausible given current LLM capabilities

## Next Checks

1. **Graph Sensitivity Analysis**: Systematically vary keyword extraction methods and edge creation thresholds to measure impact on DISCOG's performance across different legal domains
2. **Deployment Cost Verification**: Obtain detailed metrics from actual eDiscovery cases using DISCOG to validate the claimed 98% cost reduction, including both model accuracy and human review time
3. **LLM Reasoning Benchmarking**: Create a labeled test set where graph predictions and LLM validations disagree, then manually verify which approach is correct to quantify the LLM's actual contribution to accuracy