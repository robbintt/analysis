---
ver: rpa2
title: (Ir)rationality and Cognitive Biases in Large Language Models
arxiv_id: '2402.09193'
source_url: https://arxiv.org/abs/2402.09193
tags:
- llms
- tasks
- reasoning
- responses
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses cognitive psychology tasks to assess the rational
  reasoning of seven large language models (LLMs), including GPT-3.5, GPT-4, Bard,
  Claude 2, and three versions of Llama 2. The study found that, like humans, LLMs
  display irrational reasoning, but not in the same way humans do.
---

# (Ir)rationality and Cognitive Biases in Large Language Models

## Quick Facts
- arXiv ID: 2402.09193
- Source URL: https://arxiv.org/abs/2402.09193
- Authors: Olivia Macmillan-Scott; Mirco Musolesi
- Reference count: 40
- Primary result: LLMs display irrational reasoning through illogical reasoning and factual inaccuracies rather than human-like cognitive biases, with significant response inconsistency across runs.

## Executive Summary
This paper evaluates the rational reasoning of seven large language models using cognitive psychology tasks. The study finds that LLMs, like humans, display irrational reasoning but through different mechanisms - primarily illogical reasoning and factual inaccuracies rather than the cognitive biases that characterize human irrationality. GPT-4 performed best overall, achieving the highest proportion of correct responses through correct reasoning. The models showed significant inconsistency in their responses, with the same model often giving different answers to the same task across multiple runs. Most incorrect responses were not due to cognitive biases but rather logical errors or factual inaccuracies.

## Method Summary
The study uses zero-shot evaluation to assess seven LLMs (GPT-3.5, GPT-4, Bard, Claude 2, and three Llama 2 variants) on twelve cognitive psychology tasks. Each task is prompted ten times per model to check for consistency. Responses are categorized across two dimensions: correct/incorrect and human-like/non-human-like. Correctness is determined by the accuracy of the final answer, while human-likeness is evaluated based on whether the response reflects a studied human cognitive bias. The study also includes facilitated versions of some tasks to examine performance improvements similar to those observed in human experiments.

## Key Results
- GPT-4 achieved the highest proportion of correct responses through correct reasoning
- Most incorrect responses displayed illogical reasoning or factual inaccuracies rather than human-like biases
- Models showed significant inconsistency, with the same model giving different answers to identical tasks across runs
- Performance was generally higher on non-mathematical tasks compared to mathematical ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs display irrational reasoning that is not human-like, primarily through illogical reasoning and factual inaccuracies rather than cognitive biases.
- Mechanism: The cognitive psychology tasks reveal that incorrect LLM responses are more often due to illogical reasoning, calculation errors, or factual inaccuracies rather than falling into the same cognitive biases as humans. This indicates a different pattern of irrationality.
- Core assumption: The cognitive psychology tasks effectively differentiate between human-like biases and other forms of incorrect reasoning.
- Evidence anchors:
  - [abstract] "When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases."
  - [section] "The majority of incorrect responses do not display human-like biases; they are incorrect in ways different to human subjects."

### Mechanism 2
- Claim: LLMs show significant inconsistency in their responses to the same cognitive tasks, revealing another form of irrationality.
- Mechanism: The same LLM model gives different answers to the same task when prompted multiple times, demonstrating inconsistency in reasoning and output. This variability is itself a form of irrationality not present in human subjects.
- Core assumption: The tasks are well-defined enough that a rational agent should give consistent answers across multiple attempts.
- Evidence anchors:
  - [abstract] "The models also showed significant inconsistency in their responses, with the same model often giving different answers to the same task."
  - [section] "The responses given by these models are highly inconsistent - the same model will give both correct and incorrect, and both human and non-human-like responses in different runs."

### Mechanism 3
- Claim: Performance on mathematical tasks shows greater inconsistency and lower accuracy compared to non-mathematical tasks.
- Mechanism: LLMs perform worse on tasks requiring mathematical calculations, with more instances of illogical reasoning or correct reasoning leading to incorrect answers. This suggests limitations in numerical reasoning capabilities.
- Core assumption: Mathematical tasks are a valid proxy for assessing numerical reasoning consistency and accuracy in LLMs.
- Evidence anchors:
  - [abstract] "GPT-4 has shown inconsistencies in its capabilities, correctly answering difficult mathematical questions in some instances, while also making very basic mistakes in others."
  - [section] "Across all models, performance is higher in non-mathematical tasks as opposed to mathematical ones."

## Foundational Learning

- Concept: Cognitive biases and heuristics in human reasoning
  - Why needed here: Understanding the original purpose of the cognitive psychology tasks helps interpret LLM responses and identify human-like vs. non-human-like patterns of irrationality.
  - Quick check question: What is the conjunction fallacy, and why does it demonstrate a cognitive bias in human reasoning?

- Concept: Zero-shot evaluation methodology
  - Why needed here: The study uses zero-shot evaluation to assess LLMs without additional training, which is important for understanding baseline capabilities and potential exposure to tasks in training data.
  - Quick check question: What is the difference between zero-shot and few-shot evaluation, and why might this distinction matter for this study?

- Concept: Consistency in AI model outputs
  - Why needed here: The study emphasizes LLM response inconsistency as a form of irrationality, requiring understanding of how stochastic sampling and model parameters affect output variation.
  - Quick check question: How does the temperature parameter affect LLM output consistency, and why might this be important for evaluating rationality?

## Architecture Onboarding

- Component map: Seven LLM models (GPT-3.5, GPT-4, Bard, Claude 2, Llama 2 7b, 13b, 70b) -> twelve cognitive psychology tasks -> response categorization framework (correct/incorrect x human-like/non-human-like) -> analysis pipeline (10 runs per task)
- Critical path: Task selection → prompt generation → model API access → response collection (10x per task) → response categorization → aggregation and analysis → interpretation of rationality patterns
- Design tradeoffs: Using zero-shot evaluation avoids training bias but may miss potential improvements from in-context learning; prompting each task ten times reveals inconsistency but increases computational cost and complexity
- Failure signatures: Inconsistent categorization across raters, API access failures for certain models, unexpected refusal to answer due to safety filters, or systematic bias in task selection that favors certain model architectures
- First 3 experiments:
  1. Replicate the categorization process with a small subset of tasks to validate the correct/incorrect x human-like/non-human-like framework.
  2. Test response consistency by running the same task twice in immediate succession to establish baseline variability.
  3. Compare performance on mathematical vs. non-mathematical tasks with a controlled subset to validate the mathematical reasoning hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do large language models (LLMs) replicate human cognitive biases in their reasoning, or do they exhibit different types of irrational reasoning?
- Basis in paper: [explicit] The paper states that when LLMs give incorrect answers, they are often incorrect in ways that differ from human-like biases, displaying illogical reasoning or factual inaccuracies instead.
- Why unresolved: While the paper found that LLMs do not replicate human cognitive biases in their reasoning, it is unclear whether this is a general trend across all types of cognitive tasks or if there are specific tasks where LLMs do exhibit human-like biases.
- What evidence would resolve it: Conducting further experiments using a wider range of cognitive tasks and analyzing the types of errors made by LLMs in each task could provide more insight into whether they replicate human cognitive biases or exhibit different types of irrational reasoning.

### Open Question 2
- Question: How does the inconsistency in LLM responses affect their performance on cognitive tasks and their overall rationality?
- Basis in paper: [explicit] The paper highlights that LLMs show significant inconsistency in their responses, with the same model often giving different answers to the same task.
- Why unresolved: While the paper acknowledges the inconsistency in LLM responses, it does not delve into the specific impact this inconsistency has on their performance and rationality. Understanding the extent to which inconsistency affects LLM reasoning could provide valuable insights into their limitations.
- What evidence would resolve it: Analyzing the correlation between response consistency and task performance, as well as investigating the factors that contribute to inconsistency in LLM responses, could shed light on the impact of inconsistency on their rationality.

### Open Question 3
- Question: Do facilitated versions of cognitive tasks improve LLM performance, similar to their effect on human subjects?
- Basis in paper: [explicit] The paper mentions that facilitated versions of cognitive tasks are included to examine whether LLM performance increases on these versions, similar to the pattern observed in human experiments.
- Why unresolved: While the paper provides some results comparing classic and facilitated versions of tasks, it does not extensively explore the impact of facilitation on LLM performance. Further investigation is needed to determine if facilitation consistently improves LLM performance across different tasks.
- What evidence would resolve it: Conducting experiments using a larger set of cognitive tasks and comparing LLM performance on classic and facilitated versions of each task could provide more conclusive evidence on the effect of facilitation on their performance.

## Limitations

- The study uses zero-shot evaluation without parameter tuning, potentially missing optimal performance configurations
- Response categorization relies on subjective judgment without specified inter-rater reliability measures
- The selected cognitive tasks may not comprehensively represent all forms of rational reasoning
- Lack of transparency about specific prompt formulations and temperature settings introduces variability

## Confidence

- Medium confidence: LLMs display irrationality through illogical reasoning and factual inaccuracies rather than human-like cognitive biases
- Medium confidence: Significant response inconsistency across multiple runs represents a form of LLM irrationality
- Low confidence: GPT-4 performs significantly better than other models in rational reasoning tasks

## Next Checks

1. Replicate the categorization process with multiple independent raters using a subset of 20 randomly selected responses to establish inter-rater reliability and validate the human-like vs non-human-like distinction.

2. Conduct a controlled experiment varying temperature parameters systematically (e.g., 0.0, 0.5, 1.0) on the same tasks to quantify the relationship between stochasticity and response inconsistency.

3. Perform statistical significance testing (e.g., chi-square tests) on the performance differences between models to determine whether claimed performance gaps are meaningful or could be due to chance variation.