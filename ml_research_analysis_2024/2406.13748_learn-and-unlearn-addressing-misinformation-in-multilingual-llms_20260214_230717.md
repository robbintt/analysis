---
ver: rpa2
title: 'Learn and Unlearn: Addressing Misinformation in Multilingual LLMs'
arxiv_id: '2406.13748'
source_url: https://arxiv.org/abs/2406.13748
tags:
- language
- languages
- unlearning
- fake
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how harmful information spreads across
  languages in multilingual LLMs and evaluates the efficacy of various unlearning
  methods. The authors simulate contamination of training data with fake news in multiple
  languages, fine-tune a multilingual LLM, and measure the spread of fake information
  when queried in different languages.
---

# Learn and Unlearn: Addressing Misinformation in Multilingual LLMs

## Quick Facts
- arXiv ID: 2406.13748
- Source URL: https://arxiv.org/abs/2406.13748
- Reference count: 25
- Multilingual LLMs spread harmful content across languages; standard unlearning techniques focused on English are insufficient

## Executive Summary
This paper investigates how harmful information propagates across languages in multilingual large language models (LLMs) and evaluates the efficacy of various unlearning methods. The authors simulate contamination of training data with fake news in multiple languages, fine-tune a multilingual LLM, and measure the spread of fake information when queried in different languages. They find that fake information propagates across all languages, with stronger spread in high-resource languages and when the fake data and query language match. Standard unlearning techniques focused on English data are insufficient to eliminate harmful content in multilingual contexts and can even reinforce harmful content in other languages. The authors show that only by unlearning in both English and the original language of the harmful data can they effectively eliminate fake responses across all languages.

## Method Summary
The authors simulate contamination of multilingual LLM training data with fake news articles translated into 8 languages (German, French, Chinese, Russian, Javanese, Urdu, Hausa, Armenian) plus English. They fine-tune LLaMa3-8B on combined real and fake datasets using LoRA adapters, then instruction-tune with Q&A pairs. The models are evaluated across all 9 languages using QR (quality of real information comprehension) and OF (fake information occurrence) metrics. Unlearning experiments target different language combinations to assess effectiveness at eliminating harmful content propagation.

## Key Results
- Fake information introduced in any language spreads to all languages when queried in English (OF ≥ 20)
- English-only unlearning reduces harmful generations in English and high-resource languages but fails to transfer to other languages
- Multilingual unlearning with multiple languages inadvertently reinforces harmful content in non-target languages
- Combined unlearning in English and the original harmful language eliminates fake responses across all languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual LLMs exhibit asymmetric transfer of harmful content across languages.
- Mechanism: The model learns language-specific representations but also creates shared semantic embeddings. Harmful content introduced in one language propagates into shared embeddings, which are then activated when querying in any language.
- Core assumption: Multilingual models have both language-specific and cross-lingual semantic spaces that interact during inference.
- Evidence anchors:
  - [abstract] "We demonstrate that fake information, regardless of the language it is in, once introduced into these models through training data, can spread across different languages"
  - [section] "Fake information sourced in any language is transferred when queried in English (OF ≥ 20)"
  - [corpus] Weak evidence; related work focuses on monolingual unlearning but lacks multilingual cross-contamination studies

### Mechanism 2
- Claim: Unlearning in one language does not effectively transfer to other languages due to divergent optimization paths.
- Mechanism: Gradient-based unlearning follows language-specific optimization trajectories. When unlearning in English, the model modifies English-specific parameters more heavily than cross-lingual parameters, leaving harmful content in other languages relatively intact.
- Core assumption: Unlearning optimization is dominated by the language of the forget set rather than cross-lingual parameter sharing.
- Evidence anchors:
  - [abstract] "standard unlearning techniques, which typically focus on English data, are insufficient in mitigating the spread of harmful content in multilingual contexts"
  - [section] "Unlearning in English reduces English and high-resource harmful generations, but does not transfer well to other languages"
  - [corpus] Strong evidence; "Multilingual Amnesia" directly addresses transferability issues in multilingual unlearning

### Mechanism 3
- Claim: Multilingual unlearning with diverse language sets can inadvertently reinforce harmful content through adversarial optimization dynamics.
- Mechanism: When unlearning data spans many unrelated languages, the optimization process may find shortcuts that remove harmful content in some languages while pushing it deeper into parameter spaces of other languages, effectively hiding rather than eliminating it.
- Core assumption: Gradient-based unlearning with diverse language sets creates conflicting optimization objectives that can lead to unintended parameter rearrangements.
- Evidence anchors:
  - [abstract] "unlearning in multiple languages inadvertently reinforces harmful content"
  - [section] "In this multilingual unlearning approach, we observed a significant increase in fake outputs, for query languages other than the selected ones"
  - [corpus] Moderate evidence; "Split, Unlearn, Merge" suggests data attributes affect unlearning effectiveness

## Foundational Learning

- Concept: Cross-lingual transfer in multilingual models
  - Why needed here: Understanding how information propagates across languages is fundamental to analyzing why harmful content spreads
  - Quick check question: How do multilingual models represent shared semantic concepts across different languages?

- Concept: Gradient-based optimization and parameter space
  - Why needed here: Unlearning relies on gradient ascent/descent to modify model parameters; understanding this process is crucial for interpreting why unlearning fails across languages
  - Quick check question: What happens to parameters not directly involved in the language being unlearned?

- Concept: Language-specific vs. cross-lingual parameter sharing
  - Why needed here: The effectiveness of unlearning depends on whether harmful content is stored in language-specific or shared parameter spaces
  - Quick check question: In transformer-based multilingual models, which components are typically shared across languages?

## Architecture Onboarding

- Component map: LLaMa3-8B -> LoRA adapters -> Fine-tuning (real+fake) -> SFT (Q&A) -> Unlearning (language-specific) -> Evaluation (9 languages)
- Critical path: Training → Multilingual contamination injection → Fine-tuning → Unlearning phase selection → Evaluation across all languages
- Design tradeoffs: Using LoRA for unlearning provides parameter efficiency but may limit cross-lingual parameter modification; focusing on English-only unlearning simplifies implementation but fails to address multilingual harm spread.
- Failure signatures: Harm persists in low-resource languages after English-only unlearning; harm increases in some languages after multilingual unlearning; QR score drops >20% indicating model degradation.
- First 3 experiments:
  1. Verify that contamination spreads across all languages by checking OF scores when querying in each language
  2. Test English-only unlearning effectiveness by measuring harm reduction in non-English query languages
  3. Validate that combined English + source language unlearning eliminates harm across all languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of unlearning vary with different types of harmful content (e.g., bias, misinformation, hate speech) across languages?
- Basis in paper: [inferred] The paper focuses on misinformation but does not explore other types of harmful content.
- Why unresolved: The paper's experimental setup is limited to fake news articles, which may not generalize to other forms of harmful content.
- What evidence would resolve it: Conducting similar experiments with various types of harmful content (e.g., biased statements, hate speech) across multiple languages would provide insights into the generalizability of unlearning techniques.

### Open Question 2
- Question: What are the long-term effects of multilingual unlearning on the overall performance and safety of LLMs?
- Basis in paper: [explicit] The paper mentions that unlearning can push harmful information into other languages rather than completely removing it, but does not explore long-term consequences.
- Why unresolved: The paper's focus is on immediate unlearning outcomes, and long-term effects on model performance and safety are not investigated.
- What evidence would resolve it: Longitudinal studies tracking the performance and safety of LLMs after multilingual unlearning over extended periods would provide insights into the long-term effects.

### Open Question 3
- Question: How does the linguistic similarity between languages affect the propagation and unlearning of harmful information in multilingual LLMs?
- Basis in paper: [inferred] The paper observes that fake information spreads less when the linguistic similarity between the source and target languages is lower, but does not systematically investigate this relationship.
- Why unresolved: The paper does not explicitly explore the role of linguistic similarity in the spread and unlearning of harmful information.
- What evidence would resolve it: Experiments comparing the spread and unlearning effectiveness of harmful information between linguistically similar and dissimilar language pairs would provide insights into the impact of linguistic similarity.

### Open Question 4
- Question: What are the ethical implications of unlearning harmful content in multilingual LLMs, particularly when it comes to cultural and linguistic diversity?
- Basis in paper: [explicit] The paper discusses the ethical considerations of unlearning harmful content in multilingual LLMs but does not delve into the specific implications for cultural and linguistic diversity.
- Why unresolved: The paper acknowledges the ethical dimensions of unlearning but does not explore the nuanced implications for diverse cultural and linguistic contexts.
- What evidence would resolve it: In-depth analysis of the ethical implications of unlearning harmful content in multilingual LLMs, considering the impact on cultural and linguistic diversity, would provide a more comprehensive understanding of the ethical landscape.

## Limitations

- The study relies on synthetic data generation through translation and GPT-4 augmentation, which may not reflect naturally occurring multilingual harm patterns
- Results are limited to a single model architecture (LLaMa3-8B) and specific fine-tuning methodology (LoRA adapters)
- The paper does not explore long-term effects of multilingual unlearning on model performance and safety

## Confidence

- **High Confidence**: Claims about harm spreading across languages when fake data is introduced in any language (QR and OF score patterns across the 9 languages are clearly demonstrated)
- **Medium Confidence**: Claims about English-only unlearning being insufficient (supported by results but limited to one model/architecture)
- **Low Confidence**: Claims about multilingual unlearning reinforcing harm in some languages (based on single experiment with specific language combinations)

## Next Checks

1. **Architecture Transfer Test**: Reproduce the core contamination and unlearning experiments using a different multilingual architecture (e.g., BLOOMZ or mBERT) to verify that harm propagation patterns are architecture-independent.

2. **Natural Data Validation**: Replace synthetic fake news generation with naturally occurring harmful multilingual content from real-world sources to test whether the observed cross-lingual harm transfer persists with authentic data distributions.

3. **Parameter Attribution Analysis**: Conduct ablation studies to identify which model parameters (language-specific vs. shared) are most responsible for cross-lingual harm propagation, validating the mechanism claims about shared semantic embeddings.