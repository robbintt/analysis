---
ver: rpa2
title: 'Cross-Lingual Transfer from Related Languages: Treating Low-Resource Maltese
  as Multilingual Code-Switching'
arxiv_id: '2401.16895'
source_url: https://arxiv.org/abs/2401.16895
tags:
- xara
- arabic
- language
- maltese
- pipelines
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores cross-lingual transfer for Maltese, a Semitic
  language with influences from Arabic, Italian, and English. The authors propose
  selectively transliterating words of Arabic origin into Arabic script while keeping
  other words in their original Latin script.
---

# Cross-Lingual Transfer from Related Languages: Treating Low-Resource Maltese as Multilingual Code-Switching

## Quick Facts
- arXiv ID: 2401.16895
- Source URL: https://arxiv.org/abs/2401.16895
- Reference count: 16
- Authors: Kurt Micallef; Nizar Habash; Claudia Borg; Fadhl Eryani; Houda Bouamor
- Key outcome: Conditional transliteration based on word etymology yields best performance across POS tagging, dependency parsing, NER, and sentiment analysis tasks

## Executive Summary
This paper addresses the challenge of processing Maltese, a low-resource Semitic language with significant influences from Arabic, Italian, and English. The authors propose treating Maltese as a form of multilingual code-switching by selectively transliterating words of Arabic origin into Arabic script while preserving other words in their original Latin script. They develop a pipeline approach that uses etymology-aware processing to enhance cross-lingual transfer from related languages. The method shows consistent improvements across four different NLP tasks compared to standard fine-tuning approaches.

## Method Summary
The authors create a new dataset annotated with word-level etymology labels for Maltese text. They train a classifier to predict whether words are of Arabic origin or not, then design processing pipelines that apply different transformations based on this classification. For Arabic-origin words, they use transliteration into Arabic script; for other words, they apply translation to English. The resulting multilingual text is then used to fine-tune pre-trained models like mBERT. This approach leverages the morphological and orthographic similarities between Maltese and Arabic while maintaining the orthographic consistency needed for model training.

## Key Results
- mBERT fine-tuned on etymology-aware pipeline outperforms fine-tuning on raw Maltese text across all four tasks
- Conditional transliteration based on word etymology yields best results compared to non-selective processing
- The approach achieves consistent improvements in POS tagging, dependency parsing, NER, and sentiment analysis
- Standard fine-tuning with raw Maltese performs worse than the proposed etymology-aware pipeline

## Why This Works (Mechanism)
The method works by exploiting the multilingual nature of Maltese at the word level. Since Maltese contains words from different etymological sources (Arabic, Italian, English), treating it as code-switching allows models to leverage their knowledge of these source languages more effectively. By transliterating Arabic-origin words to Arabic script and translating others to English, the approach creates more consistent and learnable representations that align better with the training data of multilingual models.

## Foundational Learning
- Etymology classification: Needed to distinguish word origins for selective processing. Quick check: Verify classifier accuracy on held-out test set.
- Transliteration systems: Required to convert Arabic-origin words between Latin and Arabic scripts. Quick check: Ensure transliteration preserves morphological information.
- Cross-lingual transfer: Fundamental to leveraging knowledge from related languages. Quick check: Confirm related languages have sufficient representation in pre-trained models.
- Code-switching processing: Essential for handling multilingual text with mixed script. Quick check: Validate pipeline handles mixed-script input correctly.
- Word-level processing: Critical since etymology varies at word level in Maltese. Quick check: Ensure tokenization preserves word boundaries accurately.

## Architecture Onboarding
- Component map: Raw Maltese text -> Etymology classifier -> Transliteration/Translation module -> Processed multilingual text -> mBERT fine-tuning
- Critical path: Etymology classification followed by appropriate script conversion is the core decision point that determines pipeline effectiveness
- Design tradeoffs: The choice between transliteration and translation for non-Arabic words (here chosen as translation to English) affects downstream performance
- Failure signatures: Poor etymology classification leads to incorrect script conversions, degrading model performance; inconsistent transliteration rules cause representation mismatches
- First experiments: 1) Test etymology classifier accuracy on diverse Maltese text samples, 2) Validate transliteration preserves semantic information through round-trip conversion, 3) Compare performance impact of translating vs. transliterating non-Arabic words

## Open Questions the Paper Calls Out
None

## Limitations
- Small manually annotated etymology dataset (5,016 words) may not provide robust coverage
- Lack of explicit etymology classifier performance metrics raises questions about reliability
- Heuristic-based transliteration may limit generalizability to other similar low-resource languages
- Evaluation limited to BERT-based models without exploring alternative architectures
- No investigation of long-term stability or domain adaptation capabilities

## Confidence
The confidence in the major claim is rated as **Medium**. While experimental results show consistent improvements across all four downstream tasks, the limited dataset size and lack of comparison with alternative approaches reduce confidence in the generality of these findings.

## Next Checks
1. Expand the etymology annotation effort to include a larger and more diverse corpus, ideally including domain-specific text to test robustness across different registers of Maltese.
2. Implement and evaluate alternative transliteration approaches, including data-driven methods and neural transliteration models, to compare against the current heuristic-based system.
3. Conduct ablation studies to quantify the individual contributions of transliteration versus translation components within the processing pipeline, and test additional pre-trained models beyond BERT variants.