---
ver: rpa2
title: 'From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian Language
  Representation'
arxiv_id: '2404.09138'
source_url: https://arxiv.org/abs/2404.09138
tags: []
core_contribution: This paper presents the fine-tuning of open-source LLMs Gemma and
  Mistral for Ukrainian language tasks, addressing the lack of representation of low-resource
  languages in current generative models. The authors created the Ukrainian Knowledge
  and Instruction Dataset (UKID) to improve model performance, and conducted benchmarking
  against proprietary models like GPT-4 and Gemini.
---

# From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian Language Representation

## Quick Facts
- arXiv ID: 2404.09138
- Source URL: https://arxiv.org/abs/2404.09138
- Authors: Artur Kiulian; Anton Polishko; Mykola Khandoga; Oryna Chubych; Jack Connor; Raghav Ravishankar; Adarsh Shirawalmath
- Reference count: 7
- Primary result: Fine-tuning Gemma and Mistral models with Ukrainian datasets improves MCQ performance but reveals limitations in open question handling

## Executive Summary
This paper addresses the underrepresentation of low-resource languages like Ukrainian in large language models by fine-tuning open-source models Gemma and Mistral with Ukrainian-specific datasets. The authors created the Ukrainian Knowledge and Instruction Dataset (UKID) and conducted benchmarking against proprietary models like GPT-4 and Gemini. The fine-tuning process using LoRA adapters improved performance on multiple-choice questions while revealing limitations in handling open-ended questions. An interesting code-switching phenomenon emerged in the fine-tuned models, demonstrating both the potential and challenges of adapting LLMs for underrepresented languages.

## Method Summary
The researchers fine-tuned Gemma-2B, Gemma-7B, and Mistral models using LoRA adapters with Ukrainian instruction datasets including UKID, UAlpaca, and ZNO. UKID was created by aggregating top Wikipedia pages and generating question-answer-fact pairs using the Gemini 1.0 API. The fine-tuning process utilized Keras v3 library for Gemma models and Hugging Face transformers for Mistral, with AdamW optimizer. Models were evaluated on multiple-choice and open question benchmarks, revealing improved MCQ performance but limitations in open question handling, including a notable code-switching phenomenon.

## Key Results
- Fine-tuning with LoRA adapters on Ukrainian datasets improved Gemma and Mistral performance on multiple-choice questions
- Open question performance revealed limitations, with Gemma7bFT showing 40% impairment in Ukrainian language proficiency
- An emergent code-switching phenomenon (Azirivka) was observed in the fine-tuned Gemma model, mixing Ukrainian and Russian elements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning Gemma and Mistral models with language-specific datasets improves their performance on Ukrainian language tasks compared to their non-fine-tuned versions.
- Mechanism: By fine-tuning these open-source models with datasets like UKID (Ukrainian Knowledge and Instruction Dataset) and UAlpaca, the models learn to better understand and generate Ukrainian text, including its cultural and historical context.
- Core assumption: The fine-tuning datasets contain sufficient and representative examples of Ukrainian language usage to improve the models' performance.
- Evidence anchors:
  - [abstract]: "Our paper addresses this by fine-tuning the open-source Gemma and Mistral LLMs with Ukrainian datasets, aiming to improve their linguistic proficiency and benchmarking them against other existing models capable of processing Ukrainian language."
  - [section]: "Fine-tuning for gemma-7b-it was performed with a dataset consisting of 14,025 instructions (10,000 rows of UAlpaca, 3,063 rows of ZNO and 962 rows of UKID)."
  - [corpus]: Weak evidence - The corpus does not provide specific information about the fine-tuning process or its impact on model performance.
- Break condition: If the fine-tuning datasets are not representative of the Ukrainian language and its usage, the models may not improve or may even perform worse.

### Mechanism 2
- Claim: Code-switching and Azirivka phenomena observed in the fine-tuned models are emergent properties resulting from the multilingual nature of the training and fine-tuning processes.
- Mechanism: The models learn to switch between languages (e.g., Ukrainian and Russian) based on the patterns observed in the training data, which may include code-switching examples.
- Core assumption: The training and fine-tuning datasets contain examples of code-switching or multilingual text that the models can learn from.
- Evidence anchors:
  - [abstract]: "Additionally, we present the Ukrainian Knowledge and Instruction Dataset (UKID) to aid future efforts in language model fine-tuning."
  - [section]: "It's worth noting that while most of these mixed words can't be found in official dictionaries, they are commonly heard on the streets of many Ukrainian cities."
  - [corpus]: No direct evidence - The corpus does not provide information about the code-switching or Azirivka phenomena observed in the models.
- Break condition: If the training and fine-tuning datasets do not contain examples of code-switching or multilingual text, the models may not exhibit this behavior.

### Mechanism 3
- Claim: The performance of the fine-tuned models on open questions is affected by artifacts introduced during the fine-tuning process.
- Mechanism: Fine-tuning may cause the models to focus too much on the specific task or dataset used for fine-tuning, leading to a decrease in their ability to handle open-ended questions or tasks outside the scope of the fine-tuning data.
- Core assumption: The fine-tuning process can cause the models to overfit to the specific task or dataset used for fine-tuning, at the expense of their general language understanding and generation capabilities.
- Evidence anchors:
  - [abstract]: "Fine-tuning with LoRA adapters on Ukrainian instruction datasets improved the models' performance in answering multiple-choice questions, though open question performance revealed some limitations."
  - [section]: "On the other hand, Gemma7bFT's ability to speak Ukrainian was impaired by 40%, also reducing its grammar score by nearly a half."
  - [corpus]: Weak evidence - The corpus does not provide specific information about the limitations observed in the models' performance on open questions.
- Break condition: If the fine-tuning process does not cause overfitting or if the models are able to maintain their general language understanding and generation capabilities, the performance on open questions may not be affected.

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: Understanding the basic principles and capabilities of LLMs is crucial for grasping the significance of fine-tuning them for specific languages and tasks.
  - Quick check question: What are the main advantages of using LLMs for natural language processing tasks compared to traditional NLP approaches?

- Concept: Fine-tuning
  - Why needed here: Fine-tuning is the process used to adapt pre-trained LLMs to specific languages, tasks, or domains, which is the main focus of this paper.
  - Quick check question: What is the difference between pre-training and fine-tuning in the context of LLMs?

- Concept: Low-resource languages
  - Why needed here: The paper focuses on improving the representation of low-resource languages like Ukrainian in LLMs, which is an important challenge in NLP.
  - Quick check question: What are the main challenges in developing NLP tools and resources for low-resource languages?

## Architecture Onboarding

- Component map: Pre-trained LLMs (Gemma and Mistral) -> Fine-tuning datasets (UKID, UAlpaca, ZNO) -> LoRA adapters for efficient fine-tuning -> Evaluation benchmarks (MCQ and OQ datasets)

- Critical path: 1. Pre-train the base models (Gemma and Mistral) 2. Collect and prepare fine-tuning datasets (UKID, UAlpaca, ZNO) 3. Fine-tune the models using LoRA adapters 4. Evaluate the fine-tuned models on benchmark datasets (MCQ and OQ)

- Design tradeoffs: Using larger models (e.g., Gemma-7B) vs. smaller models (e.g., Gemma-2B) for fine-tuning; Trade-off between model size and computational efficiency; Balancing the size and quality of fine-tuning datasets

- Failure signatures: Poor performance on benchmark datasets; Emergence of artifacts or unexpected behaviors (e.g., code-switching); Overfitting to the fine-tuning dataset

- First 3 experiments: 1. Fine-tune Gemma-7B with UKID and evaluate on MCQ and OQ datasets 2. Fine-tune Mistral-7B with UAlpaca and ZNO datasets and evaluate on MCQ and OQ datasets 3. Compare the performance of fine-tuned models with their non-fine-tuned counterparts on MCQ and OQ datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to LoRA adapters would optimize performance for low-resource languages like Ukrainian, beyond the current r=4 and r32 configurations?
- Basis in paper: [inferred] The paper discusses LoRA fine-tuning but only explores limited parameter configurations (lora_r = 4 for Gemma, lora_r = 32 for Mistral).
- Why unresolved: The authors acknowledge resource constraints limited their exploration of LoRA configurations, leaving open what optimal settings might be for different model sizes and language characteristics.
- What evidence would resolve it: Systematic experiments varying LoRA rank parameters across different model sizes and comparing performance on both MCQ and open-ended questions.

### Open Question 2
- Question: How does the code-switching phenomenon (Azirivka) observed in fine-tuned models relate to the models' understanding of linguistic rules versus simple token-level mixing?
- Basis in paper: [explicit] The paper identifies and discusses the emergence of Azirivka code-switching as an interesting phenomenon in the fine-tuned Gemma7bFT model.
- Why unresolved: The paper presents examples but doesn't analyze whether the code-switching represents genuine understanding of both languages' grammatical structures or merely probabilistic token mixing.
- What evidence would resolve it: Detailed linguistic analysis of generated code-switched text compared to native speaker patterns, examining whether grammatical rules are being properly applied across language boundaries.

### Open Question 3
- Question: What is the minimum viable dataset size for effective fine-tuning of open-source LLMs for low-resource languages, and how does this compare to requirements for high-resource languages?
- Basis in paper: [explicit] The paper discusses creating UKID but notes it's smaller than popular English datasets (tens of thousands vs their 962 pairs), and mentions "less is more" doesn't apply to low-resource languages.
- Why unresolved: The paper identifies this as an open problem but doesn't experimentally determine the minimum effective dataset size or compare it to high-resource language requirements.
- What evidence would resolve it: Controlled experiments training models with incrementally larger datasets, measuring performance gains and identifying the point of diminishing returns for low-resource languages.

## Limitations

- Dataset representativeness: The UKID dataset, while substantial at 962 QAF pairs, may not fully capture the linguistic diversity of Ukrainian across different domains and registers.
- Evaluation scope: The evaluation benchmarks, while comprehensive for multiple-choice questions, may not adequately assess the models' capabilities in handling complex linguistic tasks that require deep cultural or contextual understanding.
- Artifact introduction: The fine-tuning process shows evidence of overfitting to specific tasks and introduced unexpected behaviors like code-switching that may affect real-world applicability.

## Confidence

**High Confidence:** The basic mechanism of fine-tuning improving multiple-choice question performance is well-supported by empirical results and consistent with established practices in model adaptation.

**Medium Confidence:** The observation of code-switching and Azirivka phenomena, while documented, requires further investigation to determine whether these represent genuine emergent properties or artifacts of the training process.

**Medium Confidence:** The claim that fine-tuning negatively impacts open question performance is supported by data, but the underlying causes and potential mitigations need more thorough investigation.

## Next Checks

1. **Dataset Coverage Analysis:** Conduct a systematic evaluation of UKID's coverage across different Ukrainian language domains, registers, and dialects to ensure comprehensive representation before fine-tuning.

2. **Cross-Domain Performance Testing:** Evaluate fine-tuned models across diverse real-world tasks beyond the MCQ and OQ benchmarks, including long-form generation, dialogue, and specialized domain applications.

3. **Bias and Artifact Detection:** Implement systematic testing for unintended behaviors including code-switching frequency, cultural bias patterns, and instruction-following consistency across different prompt styles and contexts.