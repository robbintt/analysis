---
ver: rpa2
title: Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language
  Model through Prompt Tuning
arxiv_id: '2403.13089'
source_url: https://arxiv.org/abs/2403.13089
tags:
- clinical
- prompt
- llms
- tuning
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents a prompt-tuning approach to summarize doctor-patient
  dialogues using generative large language models (LLMs), specifically GatorTronGPT.
  The method utilizes soft prompts to instruct LLMs to generate clinical summaries,
  offering a computationally efficient alternative to traditional fine-tuning.
---

# Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language Model through Prompt Tuning

## Quick Facts
- arXiv ID: 2403.13089
- Source URL: https://arxiv.org/abs/2403.13089
- Reference count: 0
- GatorTronGPT-20B with 128 virtual tokens achieved best performance on all evaluation metrics

## Executive Summary
This study presents a prompt-tuning approach for summarizing doctor-patient dialogues using large language models, specifically GatorTronGPT. The method employs soft prompts (virtual tokens) to instruct LLMs to generate clinical summaries while keeping model parameters frozen, offering a computationally efficient alternative to traditional fine-tuning. Experiments demonstrated that GatorTronGPT-20B with 128 virtual tokens outperformed the T5 baseline and smaller GatorTronGPT models across all evaluation metrics (Rouge-1, Rouge-2, Rouge-L, BERTScore, and BLEU), while requiring significantly fewer parameter updates and training time.

## Method Summary
The study uses prompt tuning with soft prompts for clinical dialogue summarization. Soft prompts (virtual tokens) are added to input dialogues and trained while freezing all LLM parameters. The approach was tested on GatorTronGPT-5B and GatorTronGPT-20B models, comparing them against T5-Large baseline. The MTS-DIALOG dataset containing 1,701 doctor-patient dialogues was split into training (1,201), validation (100), and test (400) sets. Prompt tuning was evaluated across different virtual token sizes (32-512) with LSTM initialization, and few-shot learning performance was tested with varying sample sizes.

## Key Results
- GatorTronGPT-20B with 128 virtual tokens achieved best performance on all evaluation metrics
- Prompt tuning required updating only 302 million parameters versus T5's 770 million
- Training completed in 4 hours 23 minutes versus T5's 9 hours 34 minutes
- Model showed decent few-shot learning performance with as few as 200 training samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-tuning achieves comparable performance to full fine-tuning while updating far fewer parameters
- Mechanism: Soft prompts (virtual tokens) are added to the input and trained while freezing all LLM parameters, reducing computational cost
- Core assumption: Large generative LLMs have sufficient capacity that small soft prompt updates can effectively guide summarization behavior
- Evidence anchors:
  - [abstract]: "The proposed solution has a low computing cost as the LLM parameters are not updated during prompt-tuning"
  - [section]: "GatorTronGPT-20B used 302 million parameters in the prompt tuning, which is less than half of the T5 model with 770 million parameters"
  - [corpus]: Weak - no direct citations found, but related work on GatorTronGPT shows similar parameter-efficient approaches
- Break condition: If the generative LLM is too small or lacks sufficient pre-training, soft prompts cannot compensate for frozen parameters

### Mechanism 2
- Claim: Soft prompt size of 128 virtual tokens optimizes performance across different LLM scales
- Mechanism: The soft prompt acts as a task-specific instruction layer that learns optimal representations for clinical summarization
- Core assumption: There exists an optimal prompt length that balances task specificity with model capacity utilization
- Evidence anchors:
  - [abstract]: "The GatorTronGPT-20B model achieved the best performance on all evaluation metrics"
  - [section]: "Both GatorTronGPT-5B and GatorTronGPT-20B achieved the best results using a virtue token size of 128"
  - [corpus]: Weak - no direct citations, but consistent with prompt-tuning literature suggesting optimal prompt lengths
- Break condition: If prompt length is too short, task instructions are insufficient; if too long, overfitting or interference occurs

### Mechanism 3
- Claim: GatorTronGPT-20B demonstrates superior few-shot learning compared to smaller models and traditional fine-tuning
- Mechanism: Large-scale pre-training on 277 billion words enables effective knowledge transfer with minimal task-specific examples
- Core assumption: The massive pre-training corpus provides sufficient general knowledge that few examples can adapt the model to new tasks
- Evidence anchors:
  - [abstract]: "The model also showed decent performance in few-shot learning scenarios with as few as 200 training samples"
  - [section]: "By using 200 samples, GatorTronGPT-20B can achieve decent performance on BERTScore, but there are still large gaps for other evaluation metrics"
  - [corpus]: Weak - no direct citations, but consistent with large LLM few-shot learning capabilities reported elsewhere
- Break condition: If the task domain is too specialized or the few-shot samples are unrepresentative, performance degrades significantly

## Foundational Learning

- Concept: Prompt-based learning vs traditional fine-tuning
  - Why needed here: The study compares prompt-tuning against full fine-tuning to demonstrate computational efficiency
  - Quick check question: What is the key difference between prompt-tuning and fine-tuning in terms of parameter updates?

- Concept: Text-to-text generation framework for summarization
  - Why needed here: The approach frames dialogue summarization as converting input text to summary output
  - Quick check question: How does the text-to-text generation framework handle both input dialogue and output summary in a unified model?

- Concept: Evaluation metrics for summarization quality
  - Why needed here: Multiple metrics (Rouge, BERTScore, BLEU) are used to comprehensively assess summary quality
  - Quick check question: What is the difference between Rouge-1, Rouge-2, and Rouge-L metrics?

## Architecture Onboarding

- Component map: GatorTronGPT-20B (frozen LLM) + soft prompt layer (trainable) + LSTM/MLP initialization + cross-entropy loss
- Critical path: Soft prompt generation → GatorTronGPT input → token prediction → loss calculation → soft prompt update
- Design tradeoffs: Larger soft prompts improve performance but increase training time; larger LLMs improve few-shot learning but cost more
- Failure signatures: Poor performance on specific evaluation metrics suggests insufficient prompt learning or inappropriate prompt size
- First 3 experiments:
  1. Test different soft prompt lengths (32, 64, 128, 256, 512) to identify optimal size
  2. Compare GatorTronGPT-5B vs GatorTronGPT-20B with fixed 128 token prompts
  3. Evaluate few-shot learning with 5, 10, 20, 40, 60, 100, 200 samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does prompt tuning performance compare to fine-tuning for clinical ATS when using different LLM architectures beyond GPT and T5?
- Basis in paper: [explicit] The paper compares GatorTronGPT (GPT-based) and T5 models, noting that prompt tuning achieves comparable or better results with significantly fewer parameters, but does not explore other architectures like encoder-only models or different decoder-only models.
- Why unresolved: The study focuses on two specific architectures (GPT and T5), leaving open whether these findings generalize to other LLM types.
- What evidence would resolve it: Comparative studies applying prompt tuning to various LLM architectures (e.g., encoder-only, different decoder-only, and hybrid models) on the same clinical ATS tasks.

### Open Question 2
- Question: What is the optimal soft prompt size for different clinical ATS tasks, and how does it vary with task complexity?
- Basis in paper: [explicit] The study finds that GatorTronGPT models perform best with 128 virtual tokens, but notes that larger models are more sensitive to prompt size, suggesting variability across tasks and model sizes.
- Why unresolved: The study tests only one clinical ATS task, leaving open whether prompt size optimization is task-dependent.
- What evidence would resolve it: Systematic testing of soft prompt sizes across multiple clinical NLP tasks with varying complexity to determine if optimal prompt sizes are consistent or task-specific.

### Open Question 3
- Question: How does prompt tuning performance scale with model size for clinical ATS tasks beyond the tested 5B and 20B parameter models?
- Basis in paper: [explicit] The study compares GatorTronGPT-5B and GatorTronGPT-20B, showing improved performance with larger models, but does not test intermediate or larger sizes.
- Why unresolved: The performance scaling relationship between model size and prompt tuning effectiveness remains unclear beyond the tested range.
- What evidence would resolve it: Comparative studies testing prompt tuning across a wider range of model sizes (e.g., 1B, 10B, 30B parameters) on the same clinical ATS tasks to establish performance scaling patterns.

## Limitations
- The MTS-DIALOG dataset, while substantial, may not fully represent clinical encounter diversity across medical specialties
- Evaluation relies entirely on automatic metrics without human assessment of clinical accuracy or utility
- Computational efficiency gains demonstrated primarily on NVIDIA A100-80G GPUs may not generalize to other hardware

## Confidence

**High Confidence** - Claims regarding computational efficiency gains are well-supported with concrete evidence comparing parameter counts and training times.

**Medium Confidence** - Performance superiority claims relative to T5 baseline are supported by experimental results, but rely entirely on automatic metrics without clinical validation.

**Low Confidence** - Claims about clinical applicability and real-world deployment readiness are not directly supported by the study due to absence of human evaluation and clinical expert review.

## Next Checks

1. **Clinical Expert Validation**: Conduct blinded evaluation where practicing physicians assess the quality, accuracy, and clinical utility of GatorTronGPT-generated summaries versus human-written clinical notes on a subset of 50-100 test dialogues, measuring inter-rater reliability and clinical acceptability.

2. **Cross-Institution Generalization**: Test the trained models on doctor-patient dialogue datasets from different healthcare systems or medical specialties not represented in the MTS-DIALOG dataset to assess performance degradation and identify potential domain adaptation needs.

3. **Longitudinal Performance Analysis**: Implement the prompt-tuned models in a simulated clinical workflow environment and evaluate summary quality across different times of day, clinician experience levels, and encounter complexities to identify conditions under which performance may degrade.