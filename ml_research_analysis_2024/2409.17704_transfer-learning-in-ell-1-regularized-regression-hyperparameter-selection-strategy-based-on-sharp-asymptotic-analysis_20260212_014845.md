---
ver: rpa2
title: 'Transfer Learning in $\ell_1$ Regularized Regression: Hyperparameter Selection
  Strategy based on Sharp Asymptotic Analysis'
arxiv_id: '2409.17704'
source_url: https://arxiv.org/abs/2409.17704
tags:
- trans-lasso
- strategy
- learning
- error
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical and empirical study of hyperparameter
  selection for transfer learning in high-dimensional sparse regression, focusing
  on the Generalized Trans-Lasso algorithm. The authors use the replica method to
  analyze the algorithm's performance in a simplified high-dimensional setting, revealing
  that near-optimal generalization can be achieved by focusing on either the support
  information or the actual feature values transferred from related datasets, rather
  than both.
---

# Transfer Learning in $\ell_1$ Regularized Regression: Hyperparameter Selection Strategy based on Sharp Asymptotic Analysis

## Quick Facts
- arXiv ID: 2409.17704
- Source URL: https://arxiv.org/abs/2409.17704
- Reference count: 40
- Authors: Koki Okajima; Tomoyuki Obuchi
- Primary result: Near-optimal generalization in high-dimensional sparse transfer learning can be achieved by focusing on either support or feature-value information, not both

## Executive Summary
This paper presents a theoretical and empirical study of hyperparameter selection for transfer learning in high-dimensional sparse regression, focusing on the Generalized Trans-Lasso algorithm. The authors use the replica method to analyze the algorithm's performance in a simplified high-dimensional setting, revealing that near-optimal generalization can be achieved by focusing on either the support information or the actual feature values transferred from related datasets, rather than both. This finding leads to a simple hyperparameter selection strategy that significantly reduces the complexity of tuning the model. Empirical experiments on real-world (IMDb) and semi-artificial (MNIST) datasets support these theoretical insights, showing that the proposed strategy performs comparably to more complex approaches and outperforms existing methods like Trans-Lasso and Pretraining Lasso in certain scenarios.

## Method Summary
The paper analyzes a two-stage transfer learning approach for high-dimensional sparse regression using $\ell_1$-regularized methods. The first stage involves pretraining on related datasets to estimate a common feature vector, while the second stage fine-tunes on the target dataset using a modified Lasso with transfer parameters $\kappa$ (controlling support transfer) and $\Delta\lambda$ (controlling feature-value transfer). The replica method is employed to derive sharp asymptotic expressions for generalization error, revealing that decoupling support and feature-value information yields near-optimal performance. This insight leads to a simple hyperparameter selection strategy: setting either $\kappa=0$ (transfer support only) or $\Delta\lambda=0$ (transfer feature values only) depending on the noise level and sample size ratio.

## Key Results
- Theoretical analysis shows that near-optimal generalization can be achieved by focusing on either support or feature-value information, not both
- Simple hyperparameter selection strategy (κ=0 or Δλ=0) significantly reduces tuning complexity while maintaining performance
- Empirical experiments on IMDb and MNIST datasets validate theoretical predictions and demonstrate performance gains over existing methods
- Performance is sensitive to the choice of λ1 in the first stage for low noise scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In high-dimensional sparse regression transfer learning, generalization error is insensitive to transferring both support and feature-value information, as long as at least one type is retained.
- **Mechanism:** Replica method analysis reveals saddle-point equations decouple such that either support mask or feature-value bias alone suffices for near-optimal performance.
- **Core assumption:** Asymptotic regime with N → ∞ and feature-sample ratios held constant (π(k), α(k) = O(1)).
- **Evidence anchors:**
  - [abstract]: "Ignoring one of the two types of information transferred to the fine-tuning stage has little effect on generalization performance"
  - [section 3.3]: Asymptotic behavior of estimators q1, q2, and qr
  - [corpus]: Weak (no direct evidence of decoupling mechanism in related works)
- **Break condition:** Very small support overlap or extremely high noise levels.

### Mechanism 2
- **Claim:** Optimal hyperparameter strategy simplifies to κ=0 or Δλ=0 based on target data abundance and noise level.
- **Mechanism:** Asymptotic error expressions show for abundant target data (large α(1)), κ=0 yields near-optimal performance; for scarce data (small α(1)), Δλ=0 is better.
- **Core assumption:** Two-stage problem admits convex-concave structure in high-dimensional limit.
- **Evidence anchors:**
  - [section 5]: Strategy depends on α(1) and noise level
  - [section 4]: Sensitivity analysis of generalization error
  - [corpus]: Weak (related works focus on algorithmic extensions)
- **Break condition:** Very large number of unique features or highly heterogeneous noise.

### Mechanism 3
- **Claim:** Choice of λ1 can be tuned beyond locally optimal value to improve performance in low noise scenarios.
- **Mechanism:** Larger λ1 induces sparser solutions, reducing crosstalk noise in transferred support information.
- **Core assumption:** Noise level σ is known or estimable, true support is sufficiently sparse.
- **Evidence anchors:**
  - [section 5]: GO strategy prefers larger λ1 in low noise
  - [section 3.2]: Equations of state linking λ1 to first-stage error
  - [corpus]: Weak (no explicit λ1 tuning strategy in related works)
- **Break condition:** High noise level or non-sparse true support.

## Foundational Learning

- **Concept:** High-dimensional statistics and the Lasso
  - Why needed here: Builds on Lasso theory, understanding sparsity and regularization paths is essential
  - Quick check question: What is the role of the ℓ1 penalty in Lasso, and how does it promote sparsity in high dimensions?

- **Concept:** Replica method and statistical mechanics of learning
  - Why needed here: Main theoretical results rely on replica method for sharp asymptotic error expressions
  - Quick check question: How does the replica symmetric ansatz simplify the calculation of the partition function in high-dimensional models?

- **Concept:** Transfer learning and multi-task learning
  - Why needed here: Algorithm combines information from multiple datasets; understanding when transfer helps is crucial
  - Quick check question: What is the difference between common support and unique support in multi-task sparse regression?

## Architecture Onboarding

- **Component map:** Datasets {D(k)} -> Stage 1 Lasso -> Estimate ˆx(1st) -> Stage 2 Fine-tuning with κ, Δλ -> Final regressor ˆx(2nd)
- **Critical path:** 1) Load and preprocess datasets, 2) Run Stage 1 Lasso, 3) For each hyperparameter combination, run Stage 2 fine-tuning, 4) Evaluate generalization error
- **Design tradeoffs:**
  - Memory vs. speed: Storing all datasets vs. streaming
  - Exact vs. approximate hyperparameter search: Grid vs. random vs. Bayesian optimization
  - Theoretical vs. empirical evaluation: Use replica formulas vs. full cross-validation
- **Failure signatures:**
  - High generalization error despite tuning: Check for negative transfer or insufficient common support
  - Slow convergence: Check conditioning of A(k) or scaling of regularization parameters
  - Inconsistent results across runs: Check random seed and data shuffling
- **First 3 experiments:**
  1. Synthetic data with known common/unique support: Verify theoretical predictions match empirical errors
  2. Vary α(1) and σ: Confirm κ=0 or Δλ=0 strategy switches based on target sample size and noise
  3. Real data (IMDb or MNIST): Test if support-only or value-only transfer outperforms original Trans-Lasso

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact computational complexity of the LO strategy compared to κ=0 and Δλ=0 strategies, and how does this scale with dataset size and dimensionality?
- Basis in paper: [explicit] Paper mentions GO strategy requires retraining first stage estimator for each hyperparameter choice
- Why unresolved: No detailed computational complexity analysis comparing different strategies
- What evidence would resolve it: Rigorous computational complexity analysis with empirical runtime measurements on varying dataset sizes

### Open Question 2
- Question: How does performance change when i.i.d. standard Gaussian entries assumption for true feature vectors is relaxed?
- Basis in paper: [inferred] Theoretical analysis assumes i.i.d. standard Gaussian entries but mentions generalization to complex settings
- Why unresolved: Theoretical analysis limited to i.i.d. standard Gaussian case
- What evidence would resolve it: Theoretical analysis and empirical experiments under different feature vector distributions

### Open Question 3
- Question: What is the optimal choice of λ1 for low noise scenarios, and how does it compare to values from LO strategy?
- Basis in paper: [explicit] Paper observes GO strategy prefers larger λ1 in low noise scenarios
- Why unresolved: No specific method for determining optimal λ1 value
- What evidence would resolve it: Method for determining optimal λ1 value with performance comparison against LO strategy

### Open Question 4
- Question: How does performance change when common/independent support model is relaxed to more general overlapping support model?
- Basis in paper: [inferred] Paper assumes common and independent support model but real-world data may have complex overlapping structures
- Why unresolved: Theoretical analysis limited to common/independent support model
- What evidence would resolve it: Theoretical analysis and empirical experiments under different overlapping support models

## Limitations
- Theoretical claims rest on replica method calculations which are non-rigorous and assume replica symmetry
- Empirical validation limited to two real datasets and controlled synthetic experiments
- Theoretical analysis assumes Gaussian data and noise which may not capture all real-world scenarios

## Confidence
- Replica method predictions and core decoupling insight: Medium
- Empirical performance gains on real datasets: Medium
- Hyperparameter selection strategy simplicity: High

## Next Checks
1. Verify robustness of κ=0/Δλ=0 strategy across diverse data distributions beyond Gaussian assumptions
2. Test theoretical predictions on larger-scale real datasets with varying degrees of feature overlap and noise heterogeneity
3. Rigorously benchmark against state-of-the-art transfer learning methods beyond Trans-Lasso and pretraining baselines