---
ver: rpa2
title: A synthetic data approach for domain generalization of NLI models
arxiv_id: '2402.12368'
source_url: https://arxiv.org/abs/2402.12368
tags:
- data
- examples
- steps
- gnli
- anli
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the domain generalization problem for NLI models
  by generating synthetic data across diverse domains and text lengths. The method
  uses an LLM to first generate premises in 38 domains and two length categories (short
  and paragraph), then generates hypotheses and labels conditioned on each premise.
---

# A synthetic data approach for domain generalization of NLI models

## Quick Facts
- arXiv ID: 2402.12368
- Source URL: https://arxiv.org/abs/2402.12368
- Reference count: 28
- This work explores the domain generalization problem for NLI models by generating synthetic data across diverse domains and text lengths.

## Executive Summary
This paper addresses the domain generalization challenge in Natural Language Inference (NLI) by generating synthetic training data across 38 diverse domains and two length categories. The authors employ a two-stage LLM-based approach where premises are first generated with controlled domain and length properties, then hypotheses and labels are generated conditioned on these premises. The resulting dataset (684K examples) is balanced across domains, lengths, and NLI labels. Models trained on this synthetic data show significant improvements in generalization to unseen domains, particularly for smaller models, with an average 7% improvement over models trained on standard NLI datasets.

## Method Summary
The method uses a two-stage generation process with large language models. First, premises are generated across 38 domains (e.g., news, fiction, telephone conversations) and two length categories (short and paragraph) using FLAN-PaLM2-L. Then, for each premise, hypotheses and labels are generated using FLAN-PaLM2 540B with prompt-tuning. The generation process is explicitly balanced across domains, lengths, and the three NLI labels (entailment, contradiction, neutral). The synthetic data is used to train various sizes of T5 models, which are then evaluated on the TRUE factual consistency benchmark to assess domain generalization capabilities.

## Key Results
- Models trained on synthetic data outperform MNLI, ANLI, and WANLI-trained models by ~7% on average on the TRUE benchmark
- T5-small shows 8.58% average improvement, while T5-XXL shows 2.96% improvement
- The method improves in-domain performance when used to augment training data for larger models
- The two-stage generation process produces more diverse examples than single-pass generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage generation process (domains/lengths first, then hypotheses/labels) produces more diverse and creative NLI examples compared to single-pass synthetic generation.
- Mechanism: By decoupling premise generation from hypothesis/label generation, the method can control domain and length distribution independently while still producing contextually appropriate hypotheses. This avoids the tendency of single-pass methods to generate repetitive or formulaic examples.
- Core assumption: Premises generated in diverse domains with varied lengths provide sufficient context for generating meaningful hypotheses across all three NLI labels.
- Evidence anchors:
  - [abstract] "Our generation is also balanced with respect to the premise length and domain by design"
  - [section 3.1] "We use the same prompt as in Figure 2, but substitute a new domain and length category of interest to it, to generate a text with those properties"
- Break condition: If premise generation fails to produce sufficiently diverse or contextually rich examples, the downstream hypothesis generation will be constrained and produce low-quality NLI examples.

### Mechanism 2
- Claim: Training on synthetic data with balanced label distribution improves domain generalization compared to training on naturally skewed datasets.
- Mechanism: By explicitly balancing the label distribution (35.4% entailment, 31.1% contradiction, 33.5% neutral) during synthetic data generation, the model learns to handle all reasoning types equally rather than overfitting to the most frequent label pattern in natural data.
- Core assumption: Label distribution in training data significantly impacts a model's ability to generalize to unseen domains where label frequencies may differ from training data.
- Evidence anchors:
  - [abstract] "Our generation is also balanced with respect to the premise length and domain by design (we sample examples in a stratified manner)"
  - [section 3.3] "The number of examples from each label is relatively balanced (35.4% entailment, 31.1% contradiction, and 33.5% neutral)"
- Break condition: If the balanced synthetic data doesn't match the complexity and nuance of real-world examples, the model may perform well on synthetic data but fail to generalize to actual NLI tasks.

### Mechanism 3
- Claim: Using larger language models (FLAN-PaLM2 540B) with prompt-tuning for hypothesis generation produces more creative and contextually appropriate examples than fine-tuning smaller models.
- Mechanism: Large models with prompt-tuning maintain more of their general knowledge and reasoning capabilities while still learning the specific task of generating hypotheses, resulting in more diverse and creative examples compared to smaller models that may memorize training patterns.
- Core assumption: Model capacity and regularization through prompt-tuning are critical for generating high-quality synthetic NLI examples.
- Evidence anchors:
  - [section 3.2] "We used prompt-tuning (Lester et al., 2021), in lieu of fine-tuning, for two reasons: a) With prompt-tuning, only a few embeddings are updated (100 in our experiments) leading to efficient training, and b) prompt-tuning provides regularization and avoids memorization of the training set details"
  - [section 3.2] "Large models and regularization were important for creative generation of hypotheses. A T5 XL model (3B parameters) fine-tuned on the same task lead to examples with poor creativity and low utility for training"
- Break condition: If the large model's generation becomes too creative or deviates from the NLI task requirements, the resulting examples may be interesting but not useful for training effective NLI models.

## Foundational Learning

- Concept: Domain generalization in machine learning
  - Why needed here: The core problem this work addresses is how to make NLI models perform well on data from unseen domains, which requires understanding how domain shift affects model performance and what strategies can mitigate this.
  - Quick check question: What is the difference between domain adaptation and domain generalization, and why is the latter more challenging?

- Concept: Natural Language Inference (NLI) task structure
  - Why needed here: Understanding the NLI task format (premise, hypothesis, label) is fundamental to grasping how the synthetic data generation process works and why certain design choices were made.
  - Quick check question: What are the three possible labels in NLI and what logical relationship does each represent between premise and hypothesis?

- Concept: Synthetic data generation using LLMs
  - Why needed here: The entire methodology relies on using large language models to generate training data, so understanding the principles and limitations of synthetic data generation is crucial.
  - Quick check question: What are the key advantages and potential risks of using synthetic data for training machine learning models?

## Architecture Onboarding

- Component map:
  - Premise generation pipeline: Domain selection → Length specification → Text generation (FLAN-PaLM2 L)
  - Hypothesis/label generation pipeline: Premise conditioning → (Hypothesis, Label) pair generation (FLAN-PaLM2 540B with prompt-tuning)
  - Training pipeline: Synthetic data preprocessing → T5 model training (various sizes) → Evaluation on TRUE benchmark
  - Evaluation pipeline: Model inference → Consistency detection → ROC AUC calculation

- Critical path: Premise generation → Hypothesis/label generation → T5 model training → TRUE benchmark evaluation
- Design tradeoffs:
  - Model size vs. creativity: Larger models produce more creative examples but require more compute
  - Control vs. diversity: Two-stage generation provides more control over distribution but may reduce natural diversity
  - Synthetic vs. real data: Synthetic data is cheaper to generate but may miss real-world complexities

- Failure signatures:
  - Low diversity in generated examples (indicates poor premise generation or hypothesis generation constraints)
  - Model overfitting to synthetic patterns (indicates insufficient regularization or model capacity issues)
  - Poor performance on TRUE benchmark despite good performance on synthetic data (indicates distribution mismatch)

- First 3 experiments:
  1. Generate a small set of premises (100 examples) across all 38 domains and both length categories, then manually inspect for diversity and quality
  2. Generate hypotheses for a subset of premises and compare label accuracy against human annotations to validate the generation process
  3. Train a small T5 model on a minimal synthetic dataset and evaluate on a held-out validation set to establish baseline performance before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum amount of synthetic data required to achieve strong generalization for NLI models?
- Basis in paper: [explicit] The paper explores the effect of training data size on evaluation accuracy, sampling different numbers of examples from GNLI.
- Why unresolved: While the paper identifies a threshold of around 300K examples for decent performance, it does not pinpoint the exact minimum or explore the trade-off between data size and model performance in detail.
- What evidence would resolve it: A more granular analysis of model performance with varying data sizes, potentially using techniques like learning curves or optimal stopping criteria, would clarify the minimum data requirement.

### Open Question 2
- Question: How does the synthetic data generation method generalize to other languages and NLI datasets beyond English MNLI?
- Basis in paper: [inferred] The paper focuses on English MNLI-style NLI and does not explore the applicability of the method to other languages or datasets.
- Why unresolved: The effectiveness of the synthetic data generation approach may vary depending on the language, dataset characteristics, and task complexity.
- What evidence would resolve it: Applying the method to different languages and NLI datasets, and comparing the results to existing approaches, would demonstrate its generalizability.

### Open Question 3
- Question: What is the impact of synthetic data on the model's ability to handle long and complex hypotheses?
- Basis in paper: [inferred] The paper mentions that hypotheses are generated to be relevant and creative, but it does not specifically address the handling of long or complex hypotheses.
- Why unresolved: Long and complex hypotheses may pose challenges for NLI models, and the impact of synthetic data on this aspect is unclear.
- What evidence would resolve it: Evaluating the model's performance on datasets with long and complex hypotheses, both with and without synthetic data augmentation, would provide insights into its impact on this aspect.

## Limitations

- Data availability and reproducibility: The synthetic dataset will be made available upon acceptance but is not currently public, limiting independent verification
- Evaluation scope: Focuses primarily on TRUE benchmark and ANLI, without extensive testing on other domain generalization benchmarks or multi-lingual NLI
- Model size effects: Diminishing returns for larger models (only 2.96% improvement for T5-XXL) suggest synthetic data may be less effective for already well-trained large models
- Generalization beyond English: The work is explicitly limited to English MNLI-style NLI with no investigation of cross-lingual applicability

## Confidence

- Domain generalization effectiveness: Medium confidence - consistent improvements on TRUE benchmark but limited to one evaluation set
- Two-stage generation advantage: Medium confidence - claimed but not directly compared with single-pass generation in ablation studies
- Balanced label distribution importance: Medium confidence - shown in data but not validated through controlled experiments

## Next Checks

1. **Ablation study on generation stages**: Create and evaluate synthetic data using single-pass generation (premises, hypotheses, and labels generated together) versus the two-stage approach to quantify the claimed creativity advantage.

2. **Cross-benchmark validation**: Test the synthetic data trained models on additional domain generalization benchmarks beyond TRUE, such as the recently introduced CrossNLI or other out-of-domain NLI evaluation sets.

3. **Label distribution analysis**: Train models on synthetic data with deliberately imbalanced label distributions (matching MNLI's natural distribution) to measure the impact of the balanced design choice on domain generalization performance.