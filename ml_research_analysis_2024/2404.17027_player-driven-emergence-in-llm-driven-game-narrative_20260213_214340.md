---
ver: rpa2
title: Player-Driven Emergence in LLM-Driven Game Narrative
arxiv_id: '2404.17027'
source_url: https://arxiv.org/abs/2404.17027
tags:
- game
- player
- players
- emergent
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how interaction with large language models
  (LLMs) can give rise to emergent behaviors in text-based games, allowing players
  to participate in the evolution of game narratives. The study introduces Dejaboom!,
  a text-adventure game where players interact with non-player characters (NPCs) powered
  by GPT-4 to solve a mystery.
---

# Player-Driven Emergence in LLM-Driven Game Narrative

## Quick Facts
- arXiv ID: 2404.17027
- Source URL: https://arxiv.org/abs/2404.17027
- Authors: Xiangyu Peng; Jessica Quaye; Sudha Rao; Weijia Xu; Portia Botchway; Chris Brockett; Nebojsa Jojic; Gabriel DesGarennes; Ken Lobb; Michael Xu; Jorge Leandro; Claire Jin; Bill Dolan
- Reference count: 32
- Key outcome: Interaction with LLMs can give rise to emergent behaviors in text-based games, allowing players to participate in the evolution of game narratives through discovery of unexpected dialogue paths and strategies.

## Executive Summary
This paper explores how large language models (LLMs) enable emergent narrative behaviors in text-based games, where players can discover creative strategies and narrative elements not originally intended by designers. Through a study of 28 participants playing Dejaboom!, a text-adventure game powered by GPT-4, the researchers identified "emergent nodes" - novel additions to the game narrative created through player experimentation. The study found that players motivated by discovery, exploration, and experimentation were most likely to generate these emergent narrative elements, suggesting a collaborative model of game development among designers, players, and LLMs.

## Method Summary
The study used Dejaboom!, a text-adventure mystery game where players interact with GPT-4-powered NPCs to solve a case. Participants played for one hour each while their gameplay logs were recorded. After gameplay, GPT-4 was used to automatically convert these logs into narrative graphs representing player strategies, which were then compared to the designer-intended narrative graph to identify emergent nodes. Player motivation profiles were collected through surveys and analyzed for correlation with emergent node creation.

## Key Results
- Players discovered emergent narrative nodes including new strategies for extracting information from NPCs and entirely new ways of solving the game
- Players with high creativity and exploration motivation were more likely to generate emergent content
- The LLM's non-deterministic behavior enabled meaningful player creativity without devolving into incoherent outputs
- Automatic narrative graph generation via LLM enabled scalable identification of emergent nodes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Player-driven emergence arises from the non-deterministic behavior of LLM-generated NPC responses.
- Mechanism: The LLM's ability to produce varied, contextually appropriate responses allows players to explore unexpected dialogue paths, which the paper calls "emergent nodes" â€” creative additions to the narrative not originally intended by designers.
- Core assumption: The LLM's generative nature introduces sufficient variability to enable meaningful player creativity without devolving into incoherent outputs.
- Evidence anchors:
  - [abstract] "through their interactions with the non-deterministic behavior of the LLM, players are able to discover interesting new emergent nodes"
  - [section] "Unlike in traditional game dialog, players can interact with the NPCs in an entirely non-scripted and dynamic fashion"
- Break condition: If the LLM becomes too deterministic (e.g., through aggressive prompt engineering or fine-tuning) or outputs become repetitive and unhelpful, emergent behaviors would diminish.

### Mechanism 2
- Claim: Players with high creativity and exploration motivation are more likely to generate emergent narrative nodes.
- Mechanism: The paper links player profiles to node creation, finding that players motivated by "discovery, exploration & experimentation" tend to produce more emergent content.
- Core assumption: Player motivation profiles measured by the pre-play survey are predictive of actual creative behavior during gameplay.
- Evidence anchors:
  - [abstract] "Players that created the most emergent nodes tended to be those that often enjoy games that facilitate discovery, exploration and experimentation"
  - [section] "Table IV shows these profiles and their association with node emergence" and notes that "2 of them generated 11 emergent nodes, achieving the highest node/player ratio (5.5 nodes/player)"
- Break condition: If the survey poorly captures true player motivation, or if players shift behavior due to game fatigue or confusion, the correlation between motivation and emergence could weaken.

### Mechanism 3
- Claim: Automatic narrative graph generation via LLM enables scalable identification of emergent nodes.
- Mechanism: GPT-4 is used to parse player logs into strategy nodes and edges, then compare them to the designer-intended narrative graph to detect novel nodes.
- Core assumption: GPT-4 can reliably interpret game logs and distill them into meaningful, comparable narrative strategies.
- Evidence anchors:
  - [section] "We use GPT-4 to automatically convert the game logs into a node-graph representing the narrative in the player's gameplay"
  - [section] "We define emergent nodes as those that appear in the narrative graph of players but are not present in the original narrative graph"
- Break condition: If GPT-4 misinterprets player actions or struggles with ambiguous game logs, the resulting narrative graphs may incorrectly identify or miss emergent nodes.

## Foundational Learning

- Concept: Non-deterministic vs deterministic systems
  - Why needed here: Understanding how LLM variability differs from scripted game logic is essential to grasp why emergence occurs.
  - Quick check question: If a game only allows "attack" or "talk" commands with fixed outcomes, would it support the same type of player-driven emergence as described in the paper? Why or why not?

- Concept: Narrative graph representation
  - Why needed here: The paper uses narrative graphs to model player strategies and detect emergent nodes; engineers must understand how to construct and compare such graphs.
  - Quick check question: What distinguishes a node in a player's narrative graph from a node in the designer's original graph in this study?

- Concept: Player motivation profiling
  - Why needed here: The study correlates player motivation with emergent behavior; understanding this profiling helps interpret results and design future experiments.
  - Quick check question: If a player selects both "mastery" and "creativity" as motivations, how might their likelihood of creating emergent nodes differ from a player selecting only "mastery"?

## Architecture Onboarding

- Component map:
  - TextWorld core game engine (fixed agent)
  - GPT-4 middleware for: Command classification ([action] vs [words]), Dynamic game feedback generation, NPC dialogue generation
  - Log collection and storage system
  - GPT-4-based narrative graph generation pipeline
  - Survey collection and analysis tools

- Critical path:
  1. Player inputs command
  2. GPT-4 classifies input and routes to game engine or NPC dialogue generator
  3. Game engine executes or fails action
  4. GPT-4 generates appropriate feedback or NPC response
  5. Log is recorded
  6. After session, logs are processed into narrative graphs
  7. Emergent nodes are identified by comparison to original graph

- Design tradeoffs:
  - Flexibility vs latency: Real-time LLM calls add delay but enable emergent behaviors.
  - Determinism vs creativity: More constrained LLM responses reduce emergence but improve reliability.
  - Prompt complexity vs token limits: Rich prompts enable better context but risk hitting GPT-4's token limit.

- Failure signatures:
  - High latency causing player frustration
  - NPC responses becoming repetitive or off-character
  - Misclassification of commands leading to nonsensical feedback
  - Narrative graph generation failing due to ambiguous or incomplete logs

- First 3 experiments:
  1. Vary GPT-4 prompt structure (simple vs detailed persona) and measure effect on emergent node count and NPC consistency.
  2. Introduce a second LLM (e.g., GPT-3.5) for command classification to test if model choice impacts emergence detection.
  3. Test different player motivation profile groupings to refine correlation between motivation and emergent behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the propensity for creating emergent nodes correlate with specific personality traits beyond the player motivation profiles studied?
- Basis in paper: [explicit] The study identified a correlation between creativity motivation profiles and emergent node creation, but acknowledged the need for more comprehensive profiling.
- Why unresolved: The study's sample size and profiling method may not capture the full range of personality traits that influence emergent behavior.
- What evidence would resolve it: A larger study with more diverse personality assessments could reveal additional correlations between personality traits and emergent node creation.

### Open Question 2
- Question: How do different LLM architectures and training data affect the emergence of creative player behaviors?
- Basis in paper: [inferred] The study used GPT-4, but noted that "the robust input recognition and dynamic dialog responses provided by many state-of-the-art LLMs" could afford players extensive freedom, implying variability in emergent behaviors across models.
- Why unresolved: The study focused on a single LLM (GPT-4) and did not compare its performance with other models.
- What evidence would resolve it: Comparative studies using different LLM architectures and training datasets could reveal how these factors influence emergent player behaviors.

### Open Question 3
- Question: What is the optimal balance between designer control and player freedom to maximize emergent narrative creation without compromising game coherence?
- Basis in paper: [explicit] The study observed that players enjoyed the novel experience but also noted inconsistencies and repetitive responses, suggesting a tension between freedom and coherence.
- Why unresolved: The study did not explore the impact of varying levels of designer control on emergent behavior and player satisfaction.
- What evidence would resolve it: Controlled experiments varying the degree of designer control and measuring emergent node creation and player satisfaction could identify an optimal balance.

## Limitations
- Small sample size of 28 participants limits generalizability of findings about player motivation and emergence patterns
- Emergent node identification relies entirely on GPT-4's interpretation of gameplay logs, introducing potential subjectivity
- Study does not address how different LLM architectures or parameter settings might affect emergence rates

## Confidence

**Confidence Labels:**
- High confidence: The mechanism by which LLM non-determinism enables emergent behaviors is well-supported by direct evidence from gameplay logs and participant observations
- Medium confidence: The correlation between player motivation profiles and emergent node creation is suggestive but based on a small sample size with limited demographic diversity
- Low confidence: The claim that this represents a "collaborative model of game development" among designers, players, and LLMs remains largely theoretical without empirical evidence of sustained collaborative cycles

## Next Checks
1. Replicate the study with a larger, more diverse participant pool (minimum 100 players) to validate the correlation between player motivation profiles and emergent behavior generation
2. Implement a blind validation protocol where independent reviewers assess whether identified emergent nodes truly represent novel content versus creative interpretations of existing game elements
3. Conduct a comparative study using different LLM models (e.g., GPT-3.5, Claude, LLaMA) to determine whether emergent behaviors are model-dependent or represent a fundamental property of large language models in game contexts