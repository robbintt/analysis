---
ver: rpa2
title: 'AnchorAL: Computationally Efficient Active Learning for Large and Imbalanced
  Datasets'
arxiv_id: '2404.05623'
source_url: https://arxiv.org/abs/2404.05623
tags:
- learning
- anchoral
- instances
- active
- minority
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AnchorAL addresses the challenge of active learning for imbalanced
  classification tasks, where standard methods struggle due to computational expense
  and failure to discover minority instances. The core idea is to filter the pool
  before running active learning by dynamically selecting class-specific anchors from
  the labelled set and retrieving the most similar unlabelled instances to form a
  small, fixed-sized subpool.
---

# AnchorAL: Computationally Efficient Active Learning for Large and Imbalanced Datasets

## Quick Facts
- arXiv ID: 2404.05623
- Source URL: https://arxiv.org/abs/2404.05623
- Reference count: 40
- One-line primary result: AnchorAL is faster and more effective than competing methods for active learning on large imbalanced datasets

## Executive Summary
AnchorAL addresses the challenge of active learning for imbalanced classification tasks, where standard methods struggle due to computational expense and failure to discover minority instances. The core idea is to filter the pool before running active learning by dynamically selecting class-specific anchors from the labelled set and retrieving the most similar unlabelled instances to form a small, fixed-sized subpool. This approach allows scaling any active learning strategy to large pools while promoting exploration and discovery of minority instances.

The method significantly outperforms competing approaches: it is faster, reducing total selection time from hours to minutes; it achieves higher performance, reaching better F1-scores in less time and with fewer annotations; and it discovers more minority instances, resulting in more balanced labelled datasets.

## Method Summary
AnchorAL addresses active learning on imbalanced datasets by first filtering the pool to a small, fixed-sized subpool before applying any standard AL strategy. It works by dynamically selecting A class-specific anchors from the labelled set using k-MEANS++ initialization, then retrieving K nearest neighbors per anchor from the unlabelled pool using cosine similarity in MPNet embedding space. The top-M instances by average similarity score form the subpool. This subpool is then used by any standard AL strategy (Entropy, BADGE, FT-BERTKM) to select instances for labelling. The process iterates, with new anchors selected each time to promote exploration beyond the initial decision boundary.

## Key Results
- AnchorAL reduces total selection time from hours to minutes compared to SEALS and RandSub
- Achieves higher F1-scores in less time with fewer annotations than competing methods
- Discovers more minority instances, resulting in more balanced labelled datasets across AL iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AnchorAL prevents path dependence by dynamically selecting different anchors each iteration, forcing exploration beyond the initial decision boundary.
- Mechanism: Instead of refining the same local region, the algorithm samples class-specific anchors using k-MEANS++ initialization, which selects points in proportion to squared distances from existing anchors. This promotes diversity and shifts the subpool to new regions of the input space.
- Core assumption: Semantic representations from a pre-trained encoder preserve meaningful class boundaries and can be used for reliable similarity scoring via cosine distance.
- Evidence anchors:
  - [abstract] "By dynamically selecting different anchors at each iteration it promotes class balance and prevents overfitting the initial decision boundary, thus promoting the discovery of new clusters of minority instances."
  - [section 3.1] "Intuitively, a more extensive exploration of the input space promotes the discovery of new minority instances."
  - [corpus] Weak: Only 25 related papers, none directly comparing anchor-based vs random pool filtering. Cannot validate exploration claims empirically.
- Break condition: If the encoder's semantic space does not reflect true class structure (e.g., due to domain shift or poor embeddings), the anchor selection will not promote meaningful exploration.

### Mechanism 2
- Claim: Fixed-size subpools enable constant computational cost regardless of the original pool size, reducing annotators' waiting time.
- Mechanism: By scoring unlabelled instances only against a small set of anchors (A×K neighbours) and selecting the top-M, AnchorAL avoids scoring against the entire pool. The subpool size M is fixed, so computational cost per iteration is bounded.
- Core assumption: A small, fixed-sized subpool (e.g., 1k) is sufficient to maintain high AL performance while drastically reducing selection time.
- Evidence anchors:
  - [abstract] "Using a small, fixed-sized subpool AnchorAL allows scaling any active learning strategy to large pools."
  - [section 3.1] "Moreover, by dynamically selecting different anchors at each iteration it prevents overfitting the initial decision boundary, thus promoting the discovery of new clusters of minority instances."
  - [corpus] Weak: Only 25 related papers, no direct measurement of time reduction vs RandSub or SEALS. Cannot validate runtime claims empirically.
- Break condition: If the required subpool size M must grow with pool size to maintain performance, the constant-time claim fails.

### Mechanism 3
- Claim: Class-specific anchors ensure the subpool is more balanced, improving minority instance discovery.
- Mechanism: By selecting A anchors per class and averaging similarity scores across them, the subpool retrieval is not dominated by majority instances, even if the original pool is heavily imbalanced.
- Core assumption: The number of neighbours K per anchor is small enough to avoid majority dominance but large enough to retrieve diverse instances.
- Evidence anchors:
  - [abstract] "By dynamically selecting different anchors at each iteration it promotes class balance and prevents overfitting the initial decision boundary."
  - [section 6.1] "Across iterations, AnchorAL consistently returns subpools with more minority instances; this results in more balanced labelled sets."
  - [corpus] Weak: Only 25 related papers, none quantifying subpool class balance improvements. Cannot validate balance claims empirically.
- Break condition: If K is too large or A is too small, majority instances will dominate the subpool, negating the balancing effect.

## Foundational Learning

- Concept: Pool-based active learning iteratively selects unlabelled instances to label based on a model's uncertainty or diversity criteria.
  - Why needed here: AnchorAL builds on this framework but modifies the instance selection process by filtering the pool before applying the AL strategy.
  - Quick check question: What is the main computational bottleneck in standard pool-based AL when applied to large datasets?

- Concept: Semantic similarity via dense embeddings and cosine distance.
  - Why needed here: AnchorAL relies on pre-trained encoders (e.g., MPNet) to represent instances and retrieve similar ones using cosine distance in the embedding space.
  - Quick check question: Why does the choice of encoder affect the quality of the anchor-based retrieval?

- Concept: k-MEANS++ initialization for diverse sampling.
  - Why needed here: AnchorAL uses k-MEANS++ to select diverse anchors per class, ensuring the subpool explores different regions of the input space.
  - Quick check question: How does k-MEANS++ promote diversity in anchor selection compared to random sampling?

## Architecture Onboarding

- Component map:
  Data pipeline -> Embedding layer -> Index layer -> Anchor selection -> Subpool retrieval -> AL strategy -> Training loop

- Critical path:
  1. Precompute embeddings for all instances (U0 ∪ D0)
  2. Build HNSW index on embeddings
  3. For each AL iteration:
     - Select anchors per class via k-MEANS++
     - Retrieve K neighbours per anchor from HNSW index
     - Average scores, select top-M to form subpool
     - Run AL strategy on subpool to select instances to label
     - Update labelled set, remove from pool

- Design tradeoffs:
  - Small M vs performance: Smaller M reduces runtime but may hurt AL performance if too few informative instances are included
  - Large K vs majority dominance: Larger K increases recall but risks majority instances overwhelming the subpool
  - Fixed vs dynamic subpool size: Fixed size ensures constant runtime; dynamic size (like SEALS) can improve recall but increases cost over time

- Failure signatures:
  - Performance drops if the encoder's semantic space does not reflect true class boundaries (e.g., due to domain shift)
  - Runtime increases if HNSW index is not built efficiently or if M is set too large
  - Class imbalance persists if A per class is too small relative to imbalance ratio

- First 3 experiments:
  1. Run AnchorAL with M=1k, A=10, K=50 on Amazon-Agri binary task using BERT-base and Entropy strategy; compare runtime and F1 vs RandSub
  2. Vary A (5, 10, 20) and K (20, 50, 100) to find optimal subpool balance on Amazon-Multi
  3. Replace MPNet encoder with BERT-base encoder to test impact of embedding quality on AnchorAL performance

## Open Questions the Paper Calls Out

- Question: How does AnchorAL's performance compare when using different semantic embedding models beyond MPNet, such as SBERT or other transformer-based encoders?
  - Basis in paper: [explicit] The paper mentions that "any similarity measure works" but uses MPNet for cosine distance. It also notes that embedding can be sped up using efficient procedures and new encoders.
  - Why unresolved: The paper only experiments with MPNet for generating embeddings and creating the dense index. Other embedding models might have different semantic representation capabilities and computational efficiencies that could impact AnchorAL's performance.
  - What evidence would resolve it: Comparative experiments using different embedding models (e.g., SBERT, RoBERTa, other Sentence-BERT variants) while keeping other components of AnchorAL constant, measuring both performance and computational efficiency.

- Question: What is the theoretical upper bound on the performance gains of AnchorAL compared to random sampling, and how does this bound depend on the dataset characteristics such as class imbalance ratio and cluster separation?
  - Basis in paper: [inferred] The paper shows AnchorAL outperforms random sampling but doesn't provide theoretical analysis. It mentions that random sampling can be ineffective under extreme imbalance and that AnchorAL's dynamic subpool selection prevents path dependence.
  - Why unresolved: The paper provides empirical evidence of AnchorAL's superiority but lacks theoretical guarantees or bounds on its performance relative to random sampling under different dataset conditions.
  - What evidence would resolve it: Mathematical analysis deriving theoretical bounds on AnchorAL's performance gains, potentially using PAC learning frameworks or analysis of its exploration-exploitation trade-off compared to random sampling.

- Question: How does AnchorAL's performance change when applied to multi-label classification tasks beyond the binary and multiclass settings explored in the paper?
  - Basis in paper: [explicit] The paper mentions AmazonCat-13k as an extreme classification benchmark but only creates binary and multiclass tasks from it. It states that AnchorAL works for classification tasks generally.
  - Why unresolved: The paper's experiments are limited to binary and multiclass classification. Multi-label classification presents different challenges as instances can belong to multiple classes simultaneously, potentially affecting the anchor selection and similarity scoring mechanisms.
  - What evidence would resolve it: Experiments applying AnchorAL to multi-label classification datasets, measuring performance across different label cardinality distributions and comparing against baselines like random sampling and other pool filtering methods.

## Limitations

- Weak empirical support for claimed runtime improvements due to missing baseline comparisons
- Unclear whether alternative anchor selection strategies beyond k-MEANS++ were tested
- Missing ablation studies on the impact of embedding quality vs anchor selection method

## Confidence

- Mechanism 1 (exploration via dynamic anchors): **Medium** - intuition is sound but no empirical validation of exploration benefits
- Mechanism 2 (constant-time scaling): **Medium** - theoretical claim holds but runtime measurements are missing
- Mechanism 3 (class balance): **Medium** - balancing effect is plausible but not quantitatively verified

## Next Checks

1. Measure total selection time per AL iteration for AnchorAL vs RandSub and SEALS on Amazon-Agri with M=1k
2. Compare AnchorAL performance using BERT-base vs MPNet embeddings to quantify encoder impact
3. Run AnchorAL with varying K values (20, 50, 100) on Amazon-Multi to identify optimal trade-off between recall and majority dominance