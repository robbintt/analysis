---
ver: rpa2
title: Forward KL Regularized Preference Optimization for Aligning Diffusion Policies
arxiv_id: '2409.05622'
source_url: https://arxiv.org/abs/2409.05622
tags:
- diffusion
- policy
- learning
- preference
- fkpd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Forward KL Regularized Preference Optimization
  for aligning Diffusion policies (FKPD), a method to learn diffusion policies directly
  from preference data without relying on rewards or demonstrations. The method works
  in two stages: first, a basic diffusion policy is trained via behavior cloning from
  an offline dataset, then the policy is aligned to preferences via direct preference
  optimization with forward KL regularization.'
---

# Forward KL Regularized Preference Optimization for Aligning Diffusion Policies

## Quick Facts
- arXiv ID: 2409.05622
- Source URL: https://arxiv.org/abs/2409.05622
- Reference count: 40
- Key outcome: FKPD aligns diffusion policies with preferences without rewards or demonstrations, achieving state-of-the-art performance on MetaWorld and D4RL tasks

## Executive Summary
This paper introduces Forward KL Regularized Preference Optimization for aligning Diffusion policies (FKPD), a novel method that learns diffusion policies directly from preference data without requiring reward functions or demonstrations. The approach consists of two stages: first, a basic diffusion policy is trained via behavior cloning from an offline dataset, then the policy is refined through direct preference optimization with forward KL regularization. The forward KL regularization is shown to be more effective than reverse KL in preventing out-of-distribution issues during alignment. Experimental results on MetaWorld and D4RL benchmarks demonstrate FKPD's superior performance compared to previous methods.

## Method Summary
FKPD operates in two distinct phases to align diffusion policies with human preferences. In the first phase, a basic diffusion policy is learned through behavior cloning from an offline dataset, providing an initial policy capable of generating reasonable trajectories. In the second phase, this policy is refined using direct preference optimization with a key innovation: forward KL regularization. This regularization term encourages the aligned policy to stay close to the original behavior-cloned policy, preventing the policy from generating out-of-distribution trajectories that could lead to poor performance. The method eliminates the need for explicit reward functions or demonstration data, making it particularly suitable for settings where only preference data is available.

## Key Results
- FKPD achieves state-of-the-art performance on MetaWorld and D4RL continuous control tasks
- Forward KL regularization is more effective than reverse KL in avoiding out-of-distribution issues
- The method successfully aligns diffusion policies with preferences without requiring reward functions or demonstrations

## Why This Works (Mechanism)
FKPD leverages diffusion policies' inherent ability to generate diverse, high-quality trajectories while using preference data to guide policy alignment. The forward KL regularization plays a crucial role by constraining the aligned policy to remain close to the original behavior-cloned policy, preventing the exploration of regions where the preference model may be unreliable or undefined. This approach effectively combines the stability of behavior cloning with the flexibility of preference optimization, allowing the policy to improve while maintaining robustness to distributional shifts.

## Foundational Learning
- **Diffusion Policies**: Generative models that denoise trajectories to produce actions - needed for generating high-quality continuous control behaviors; quick check: can sample diverse, realistic trajectories
- **Behavior Cloning**: Supervised learning from expert demonstrations - provides initial policy before preference alignment; quick check: can reproduce demonstrated behaviors reasonably well
- **Preference Optimization**: Learning from pairwise comparisons of trajectories - enables training without explicit rewards; quick check: can distinguish preferred from non-preferred behaviors
- **KL Regularization**: Constraining policy updates to stay close to a reference policy - prevents distributional shift and out-of-distribution issues; quick check: aligned policy remains similar to initial policy in distribution
- **Forward vs Reverse KL**: Different regularization directions with distinct effects on policy behavior - forward KL prevents exploration of uncertain regions; quick check: forward KL produces more conservative updates than reverse KL

## Architecture Onboarding

Component Map: Offline Dataset -> Behavior Cloning -> Basic Diffusion Policy -> Preference Optimization with Forward KL -> Aligned Diffusion Policy

Critical Path: The core pipeline flows from offline data collection through behavior cloning to obtain an initial policy, followed by preference optimization with forward KL regularization to produce the final aligned policy. The forward KL term is critical as it ensures stability during the alignment process.

Design Tradeoffs: The two-stage approach trades off immediate preference optimization (which could lead to poor out-of-distribution behavior) for the stability of behavior cloning initialization followed by controlled refinement. The choice of forward KL over reverse KL represents a tradeoff between exploration and conservatism, with forward KL providing more robust alignment at the cost of potentially slower improvement.

Failure Signatures: Common failure modes include collapse to degenerate behaviors if the preference model is poorly trained, distributional shift leading to poor performance in out-of-distribution states, and insufficient exploration if the forward KL regularization is too strong. The method may also struggle with sparse or noisy preference data.

First Experiments: 1) Validate behavior cloning produces reasonable initial policies on simple tasks; 2) Test forward KL vs reverse KL regularization on synthetic preference data; 3) Evaluate alignment quality on a small-scale continuous control task before scaling to full benchmarks.

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments are limited to continuous control tasks from MetaWorld and D4RL, which may not capture the full complexity of real-world scenarios
- The effectiveness of forward KL regularization in avoiding out-of-distribution issues requires further validation across diverse task domains
- The method's performance with sparse or noisy preference data is not thoroughly explored
- The scalability of preference collection and model training for more complex tasks remains unclear

## Confidence
High: Claims about FKPD's state-of-the-art performance and the superiority of forward KL regularization are supported by experimental results on established benchmarks.
Medium: The assertion that FKPD eliminates the need for reward functions or demonstrations, while demonstrated, may require further validation in more diverse and complex scenarios.
Low: The generalizability of the method to real-world applications with limited or noisy preference data is not fully established.

## Next Checks
1. Test FKPD on more diverse continuous control tasks beyond MetaWorld and D4RL to evaluate generalizability
2. Conduct ablation studies varying the strength of forward KL regularization to identify optimal balance between improvement and stability
3. Evaluate the method's performance with sparse or noisy preference data to assess robustness in realistic settings