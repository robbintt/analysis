---
ver: rpa2
title: Data Efficiency for Large Recommendation Models
arxiv_id: '2410.18111'
source_url: https://arxiv.org/abs/2410.18111
tags:
- data
- training
- convergence
- continuous
- google
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of data efficiency in large-scale
  recommendation models for online advertising, where training datasets comprise hundreds
  of billions of examples. The authors present strategies to reduce training data
  requirements while maintaining model performance, focusing on three key approaches:
  continuous downsampling (persisting throughout training including online phases),
  continuous distillation (extending knowledge distillation into online training),
  and optimal balancing of model size versus training data volume.'
---

# Data Efficiency for Large Recommendation Models

## Quick Facts
- arXiv ID: 2410.18111
- Source URL: https://arxiv.org/abs/2410.18111
- Reference count: 11
- This work presents strategies to reduce training data requirements while maintaining model performance for large-scale recommendation models

## Executive Summary
This work addresses the challenge of data efficiency in large-scale recommendation models for online advertising, where training datasets comprise hundreds of billions of examples. The authors present three key approaches: continuous downsampling, continuous distillation, and optimal balancing of model size versus training data volume. Through empirical evaluation on Google's Ads CTR prediction models, they demonstrate that continuous distillation achieves convergence with 35% less data compared to traditional cutover approaches, while continuous downsampling accelerates convergence with only a small constant accuracy penalty. These strategies have been successfully deployed in production systems and provide actionable frameworks for practitioners to reduce training time and costs while accelerating model development.

## Method Summary
The authors present three strategies to improve data efficiency in large recommendation models. Continuous downsampling applies a fixed downsampling rate throughout training (including online phases) with importance weighting for retained examples. Continuous distillation extends knowledge distillation into online training by maintaining a teacher model that learns concurrently with the student. Iso-compute analysis identifies optimal model size-to-data volume configurations by fixing computational budget and varying model size and sampling rate to find configurations minimizing loss. These approaches have been implemented and validated in production systems at Google.

## Key Results
- Continuous distillation achieves convergence with 35% less data compared to traditional cutover approaches
- Continuous downsampling accelerates convergence with only a small constant accuracy penalty
- Iso-compute analysis identifies optimal model size-to-data volume configurations that minimize loss for a fixed computational budget

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous downsampling accelerates convergence while maintaining accuracy through consistent data reduction
- Mechanism: By applying downsampling throughout training including online phases, the model experiences constant data quality reduction that is offset by accelerated learning from fresher data
- Core assumption: The quality reduction from continuous downsampling is small and constant over time
- Evidence anchors:
  - [abstract] "continuous downsampling accelerates convergence with only a small constant accuracy penalty"
  - [section] "any quality reduction is constant over time, effectively accelerating convergence"
  - [corpus] No direct corpus evidence found - this is a novel approach specific to this work
- Break condition: If the quality reduction becomes significant or non-constant, the acceleration benefit disappears and accuracy degrades substantially

### Mechanism 2
- Claim: Continuous distillation achieves convergence with significantly less data by extending knowledge transfer into online training
- Mechanism: The student model learns from both the teacher model and new online data simultaneously, allowing faster adaptation while requiring 35% less historical data
- Core assumption: The teacher model remains relevant and useful throughout online training, not just during historical training
- Evidence anchors:
  - [abstract] "continuous distillation achieves convergence with 35% less data compared to traditional cutover approaches"
  - [section] "continuous distillation uses 35% less data than the later cutover model"
  - [corpus] No direct corpus evidence found - this represents a novel extension of traditional distillation
- Break condition: If the teacher model becomes outdated or diverges significantly from optimal behavior during online training, the distillation benefit diminishes

### Mechanism 3
- Claim: Optimal model size-to-data volume configuration minimizes loss for a fixed computational budget
- Mechanism: By analyzing iso-compute curves, practitioners can identify the sweet spot where model size and training data volume are balanced for maximum efficiency
- Core assumption: The relationship between model size, data volume, and compute follows predictable scaling laws
- Evidence anchors:
  - [abstract] "They also identify optimal model size-to-data volume configurations via iso-compute analysis"
  - [section] "a clear valley in loss exists. This indicates that, for a given compute budget, there is an optimal model size and sampling rate configuration"
  - [corpus] Weak evidence - related works exist on scaling laws for LLMs but not specifically for ad CTR models at this scale
- Break condition: If the scaling relationship changes significantly or if compute budget constraints shift dramatically

## Foundational Learning

- Concept: Label imbalance in CTR prediction (positive vs negative examples)
  - Why needed here: Understanding the 1:250 ratio is crucial for grasping why aggressive downsampling is feasible
  - Quick check question: If the click-through rate is 0.4%, what is the approximate ratio of positive to negative examples?

- Concept: Knowledge distillation and teacher-student model relationships
  - Why needed here: Essential for understanding how continuous distillation extends traditional training paradigms
  - Quick check question: In continuous distillation, when does the teacher model stop providing useful guidance?

- Concept: Iso-compute analysis and scaling laws
  - Why needed here: Critical for understanding how to optimally balance model size and data volume
  - Quick check question: What does a "valley" in an iso-compute curve represent in terms of model configuration?

## Architecture Onboarding

- Component map:
  Historical training pipeline with downsampling logic -> Online training system with continuous data ingestion -> Teacher model serving infrastructure for distillation -> TPU cluster allocation and monitoring -> Evaluation and A/B testing framework

- Critical path:
  1. Historical data ingestion and preprocessing
  2. Initial model training with continuous downsampling
  3. Teacher model serving setup
  4. Transition to online training with continuous distillation
  5. Real-time evaluation and metric tracking

- Design tradeoffs:
  - Aggressive vs conservative downsampling rates
  - Teacher model update frequency vs stability
  - Compute allocation between model size and training duration
  - Freshness of downsampling data vs processing overhead

- Failure signatures:
  - Accuracy degradation over time despite data convergence
  - Teacher model becoming stale or irrelevant
  - Iso-compute curve showing no clear minimum (indicating broken scaling assumptions)
  - Online training performance variance exceeding acceptable thresholds

- First 3 experiments:
  1. Implement continuous downsampling with varying rates (5%, 10%, 15%) on a smaller model to validate acceleration claims
  2. Compare traditional cutover distillation vs continuous distillation on a medium-scale model with fixed data volume
  3. Generate iso-compute curves for different model architectures to identify optimal configurations for a fixed TPU budget

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal downsampling rate for continuous downsampling that balances accuracy and computational cost across different LRM architectures and datasets?
- Basis in paper: [explicit] The paper discusses continuous downsampling as a strategy to accelerate convergence with a small constant accuracy penalty, but notes that downsampling rates can be tuned to balance accuracy and cost without specifying the optimal rate.
- Why unresolved: The paper mentions that downsampling rates can be tuned but doesn't provide a specific formula or method to determine the optimal rate for different scenarios. This is a critical parameter that would significantly impact the effectiveness of continuous downsampling.
- What evidence would resolve it: Empirical studies showing the relationship between downsampling rates, accuracy degradation, and computational savings across various LRM architectures and dataset sizes. A mathematical model that can predict the optimal downsampling rate given specific model and data characteristics.

### Open Question 2
- Question: How does the effectiveness of continuous distillation vary with different teacher-student model size ratios and architectures in LRMs?
- Basis in paper: [explicit] The paper demonstrates that continuous distillation uses 35% less data than traditional cutover approaches, but doesn't explore how this effectiveness varies with different model size ratios or architectures.
- Why unresolved: While the paper shows the general effectiveness of continuous distillation, it doesn't provide insights into how the technique performs under different model size configurations or architectures, which is crucial for practical implementation.
- What evidence would resolve it: Systematic experiments varying teacher-student model size ratios and architectures, measuring data efficiency gains and final model performance. A theoretical framework explaining how model size ratios impact the effectiveness of continuous distillation.

### Open Question 3
- Question: What are the optimal model size-to-data volume configurations for LRMs beyond the specific 4K TPU hours budget studied in the iso-compute analysis?
- Basis in paper: [explicit] The paper presents an iso-compute curve for a fixed 4K TPU hours budget but acknowledges this remains under-explored for ad click models trained on over 100B examples.
- Why unresolved: The paper only provides data for one specific compute budget, while real-world scenarios may have different budget constraints. The relationship between model size, data volume, and optimal configurations likely varies with different budget constraints.
- What evidence would resolve it: Comprehensive iso-compute analyses across multiple budget ranges, showing how optimal configurations shift with different compute constraints. A generalized scaling law that can predict optimal configurations for any given compute budget.

## Limitations
- Lack of detailed algorithmic specifications for implementing continuous downsampling and continuous distillation
- Evaluation focuses exclusively on Google's Ads CTR prediction models, limiting external validation
- Claims regarding data efficiency benefits lack independent verification from controlled ablation studies

## Confidence
- **High confidence**: The iso-compute analysis methodology and findings regarding optimal model size-to-data volume tradeoffs are well-established and reproducible
- **Medium confidence**: The continuous downsampling acceleration claims are supported by internal data but require more extensive ablation studies
- **Medium confidence**: The continuous distillation data reduction benefits are demonstrated but lack comprehensive cross-domain validation

## Next Checks
1. **Controlled Downsampling Experiment**: Implement continuous downsampling with multiple rates (5%, 10%, 15%) on a medium-scale recommendation model using public datasets like Criteo or Avazu, measuring both convergence speed and final accuracy against baseline models

2. **Teacher-Student Alignment Analysis**: Conduct systematic experiments varying teacher model update frequency and student learning rates during continuous distillation to quantify the impact on model performance and identify optimal configurations

3. **Cross-Domain Scaling Validation**: Replicate the iso-compute analysis methodology on different recommendation tasks (e.g., e-commerce product recommendations) to verify whether the observed scaling laws and optimal configurations generalize beyond CTR prediction