---
ver: rpa2
title: Feature Mapping in Physics-Informed Neural Networks (PINNs)
arxiv_id: '2402.06955'
source_url: https://arxiv.org/abs/2402.06955
tags:
- feature
- neural
- mapping
- equation
- pinns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the training dynamics of PINNs with feature
  mapping layers through the lens of Conjugate and Neural Tangent Kernels. It shows
  that Fourier-based features can be inadequate in some physics scenarios due to their
  non-bijective nature, leading to performance degradation in high dimensions and
  discontinuous solutions.
---

# Feature Mapping in Physics-Informed Neural Networks (PINNs)

## Quick Facts
- arXiv ID: 2402.06955
- Source URL: https://arxiv.org/abs/2402.06955
- Authors: Chengxi Zeng; Tilo Burghardt; Alberto M Gambaruto
- Reference count: 40
- Key outcome: Fourier-based feature mappings can be inadequate for some physics scenarios due to non-bijectivity, leading to performance degradation in high dimensions and discontinuous solutions; conditionally positive definite Radial Basis Functions (RBFs) are proposed as a superior alternative that improves PINN convergence and generalization.

## Executive Summary
This paper investigates the training dynamics of Physics-Informed Neural Networks (PINNs) with feature mapping layers through the lens of Conjugate and Neural Tangent Kernels. The authors demonstrate that commonly used Fourier-based feature mappings can be inadequate in certain physics scenarios, particularly in high dimensions and for discontinuous solutions. They propose conditionally positive definite Radial Basis Functions (RBFs) as an alternative, showing superior performance across diverse forward and inverse PDE problems. The framework is simple to implement and provides significant improvements in convergence and generalization.

## Method Summary
The paper proposes using conditionally positive definite Radial Basis Functions (RBFs) as feature mappings in PINNs to improve training dynamics and generalization. The method involves adding a feature mapping layer that projects coordinate-based inputs into a higher-dimensional space before the parameterized neural network layers. RBFs with polynomial terms and compact support are particularly effective. The approach is benchmarked against Fourier-based methods across various PDE types including time-dependent, nonlinear, and inverse problems. Training uses a 5-layer MLP architecture with 100 neurons per layer, Adam optimizer followed by L-BFGS, and fixed random seeds for reproducibility.

## Key Results
- RBF feature mappings with polynomial terms consistently outperform Fourier-based methods across diverse PDE types including time-dependent, nonlinear, and inverse problems
- Fourier features show degradation in high-dimensional problems and discontinuous solutions due to non-bijectivity
- RBFs with compact support and polynomial terms demonstrate superior ability to handle discontinuities and nonlinear PDEs
- The proposed feature mapping framework is simple to implement and improves both convergence and generalization of PINNs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Feature mapping transforms the input space to improve the conditioning of the Conjugate and Neural Tangent Kernels, thereby enhancing PINN training dynamics.
- **Mechanism**: The feature mapping layer projects the coordinate-based input into a higher-dimensional space before the parameterized layers. This projection affects the initial distribution of the Conjugate Kernel (CK) and Neural Tangent Kernel (NTK), which govern PINN convergence in the infinite-width limit. By carefully designing the feature mapping (e.g., using conditionally positive definite RBFs), the kernels propagate in a way that mitigates spectral bias and improves generalization.
- **Core assumption**: The training dynamics of PINNs in the infinite-width limit are dominated by the CK and NTK, and these kernels are sensitive to the initial input distribution.
- **Evidence anchors**:
  - [abstract]: "We investigate the training dynamics of PINNs with a feature mapping layer via the limiting Conjugate Kernel and Neural Tangent Kernel, which sheds light on the convergence and generalisation of the model."
  - [section 3]: "The CK and NTK drive the training dynamics of the neural network, and they govern the generalisation property in an overparameterised model. The limiting CK and NTK are sensitive to the inputs and network parameters."
  - [corpus]: Weak—corpus papers focus on acceleration and convergence but do not directly address kernel conditioning via feature mapping.
- **Break condition**: If the feature mapping function does not preserve the necessary properties (e.g., bijectivity, positive definiteness), the kernel conditioning benefits may be lost, leading to degraded training dynamics.

### Mechanism 2
- **Claim**: Fourier-based feature mappings can be inadequate for certain physics scenarios due to their non-bijective nature, leading to performance degradation in high dimensions and discontinuous solutions.
- **Mechanism**: Fourier features, while effective in some cases, can map multiple input points to the same feature space (non-bijective), especially in higher dimensions. This causes overlapping in the feature space, which can manifest as Gibbs-like phenomena in discontinuous regions and poor generalization in high-dimensional problems. The paper proposes conditionally positive definite RBFs as a better alternative because they maintain a more desirable mapping property.
- **Core assumption**: The bijectivity of the feature mapping function is crucial for accurate function approximation in PINNs, especially for discontinuous or high-dimensional problems.
- **Evidence anchors**:
  - [abstract]: "We show the inadequacy of commonly used Fourier-based feature mapping in some scenarios and propose the conditional positive definite Radial Basis Function as a better alternative."
  - [section 2.3]: "A non-bijective function denotes that the inputs are only surjectively mapped to the feature space from the domain. This indicates that an overlapping image is likely to be formed in the projected codomain, and it can also partially explain the Gibbs phenomenon in discontinuous regions with overshoot function values."
  - [corpus]: Weak—corpus papers do not discuss the bijectivity of feature mappings or their impact on PINN performance.
- **Break condition**: If the problem domain does not exhibit high dimensionality or discontinuities, the non-bijectivity of Fourier features may not significantly impact performance.

### Mechanism 3
- **Claim**: The addition of polynomial terms to RBF feature mappings improves performance on non-linear PDEs by providing additional constraints in the parameter space.
- **Mechanism**: In the infinite-width limit, each layer of the NN is considered a linear system. To ensure a unique solution, conditionally positive definite radial functions are introduced by adding polynomial terms. The weights of these polynomial terms act as Lagrange multipliers, constraining the RBF coefficients in the parameter space. This is particularly useful for non-linear PDEs like the Burgers and Navier-Stokes equations.
- **Core assumption**: The problem being solved is non-linear, and the addition of polynomial terms can help constrain the solution space.
- **Evidence anchors**:
  - [section 4.1]: "To ensure a unique solution, one way is to introduce conditionally positive definite radial functions by adding polynomial terms. The weights work as Lagrange multipliers that can constrain the RBF coefficients in the parameter space."
  - [section 6.2]: "We evaluate the methods on two classic non-linear PDEs, the Burgers equation and the Navier-Stokes equation. It has been shown in Figure 14 that the RBFs with polynomial terms are more capable of solving the discontinuity at x = 0."
  - [corpus]: Weak—corpus papers do not discuss the use of polynomial terms in feature mappings for PINNs.
- **Break condition**: If the problem is linear or does not require the additional constraints provided by polynomial terms, their inclusion may not improve performance and could add unnecessary complexity.

## Foundational Learning

- **Concept**: Conjugate Kernel (CK) and Neural Tangent Kernel (NTK)
  - Why needed here: The paper's theoretical analysis relies on understanding how these kernels govern the training dynamics of PINNs in the infinite-width limit. The CK and NTK are sensitive to the input distribution, which is why feature mapping is crucial.
  - Quick check question: What is the relationship between the CK, NTK, and the convergence of PINNs in the infinite-width limit?

- **Concept**: Radial Basis Functions (RBFs) and their properties
  - Why needed here: The paper proposes RBFs as an alternative to Fourier features for PINN feature mapping. Understanding RBF properties, such as positive definiteness and compact support, is essential for implementing the proposed method.
  - Quick check question: What are the key properties of RBFs that make them suitable for feature mapping in PINNs?

- **Concept**: Partial Differential Equations (PDEs) and their classification
  - Why needed here: The paper benchmarks the proposed method on various types of PDEs (e.g., elliptic, parabolic, hyperbolic, non-linear). Understanding the characteristics of these PDEs is important for interpreting the results and applying the method to new problems.
  - Quick check question: What are the main differences between elliptic, parabolic, and hyperbolic PDEs, and how do these differences affect the choice of PINN architecture and feature mapping?

## Architecture Onboarding

- **Component map**: Input (x,t) → Feature mapping layer (RBFs/polynomials) → 5-layer MLP (100 neurons each) → Output → Loss computation
- **Critical path**: Input → Feature mapping → Parameterized layers → Output → Loss computation
- **Design tradeoffs**:
  - Number of RBFs vs. computational efficiency
  - Inclusion of polynomial terms vs. model complexity
  - Choice of RBF type (e.g., Gaussian, cubic, thin plate spline) vs. performance on specific PDE types
  - Feature mapping function vs. bijectivity and conditioning of CK/NTK
- **Failure signatures**:
  - Poor convergence or divergence during training
  - Oscillations or overshoots near discontinuities (Gibbs-like phenomena)
  - Degradation in performance as problem dimensionality increases
  - Inability to enforce boundary conditions strictly
- **First 3 experiments**:
  1. Implement the RBF feature mapping layer with Gaussian RBFs and no polynomial terms. Test on a simple 1D Poisson equation with Dirichlet boundary conditions.
  2. Compare the performance of RBF feature mapping with Fourier-based feature mappings (e.g., Positional Encoding, Random Fourier Features) on a 2D Poisson equation.
  3. Test the impact of adding polynomial terms to the RBF feature mapping on a non-linear PDE, such as the Burgers equation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of feature mapping function (e.g., Fourier, RBF, etc.) affect the generalizability of PINNs to unseen data or out-of-distribution inputs?
- Basis in paper: [inferred] The paper discusses the limitations of Fourier features in high dimensions and discontinuous solutions, but does not explicitly address their impact on generalizability to unseen data.
- Why unresolved: The paper focuses on the training dynamics and convergence of PINNs with different feature mapping functions, but does not investigate their performance on unseen data or out-of-distribution inputs.
- What evidence would resolve it: Conducting experiments where PINNs trained with different feature mapping functions are tested on unseen data or out-of-distribution inputs, and comparing their generalization performance.

### Open Question 2
- Question: What is the impact of the number of features in the feature mapping layer on the performance and computational efficiency of PINNs?
- Basis in paper: [explicit] The paper mentions that using more RBF features (e.g., 256) can improve results but also demands higher memory and can be slower in some cases.
- Why unresolved: The paper does not provide a systematic study on the trade-off between the number of features and the performance/computational efficiency of PINNs.
- What evidence would resolve it: Conducting experiments with varying numbers of features in the feature mapping layer and analyzing the impact on performance and computational efficiency, potentially leading to guidelines for choosing the optimal number of features.

### Open Question 3
- Question: How do different types of Radial Basis Functions (RBFs) compare in terms of their effectiveness for feature mapping in PINNs?
- Basis in paper: [explicit] The paper mentions that Gaussian RBFs generally perform well, but does not provide a comprehensive comparison of different RBF types.
- Why unresolved: The paper focuses on Gaussian RBFs and does not investigate the performance of other types of RBFs, such as Cubic, TPS, MQ, IMQ, etc.
- What evidence would resolve it: Conducting experiments comparing the performance of different types of RBFs in feature mapping for PINNs, potentially leading to insights into the advantages and disadvantages of each type.

## Limitations
- The optimal configuration of RBFs (number of centers, polynomial degree) may depend on specific problem characteristics and requires careful tuning
- Some implementation details of feature mapping methods are not fully specified, requiring inference from literature or reasonable assumptions
- The paper does not systematically investigate the trade-off between the number of features and computational efficiency
- Limited comparison of different types of Radial Basis Functions beyond Gaussian RBFs

## Confidence
- High: RBF feature mappings improve PINN performance across multiple PDE types
- Medium: Fourier features' non-bijectivity explains performance degradation in high dimensions
- Medium: Polynomial terms improve performance on nonlinear PDEs by constraining the solution space

## Next Checks
1. Conduct systematic ablation studies varying the number of RBF centers and polynomial terms to determine optimal configurations for different PDE types
2. Test the proposed feature mapping methods on additional high-dimensional PDEs (e.g., 3D heat equation, parametric PDEs) to validate scalability claims
3. Investigate the impact of different RBF types (e.g., thin plate spline, compactly supported RBFs) on performance and conditioning of CK/NTK