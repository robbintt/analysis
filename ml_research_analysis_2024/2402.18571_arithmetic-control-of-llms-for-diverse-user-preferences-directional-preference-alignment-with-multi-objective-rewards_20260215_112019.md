---
ver: rpa2
title: 'Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference
  Alignment with Multi-Objective Rewards'
arxiv_id: '2402.18571'
source_url: https://arxiv.org/abs/2402.18571
tags:
- arxiv
- reward
- preference
- preferences
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of fine-grained control over
  large language models (LLMs) to adapt them to diverse user preferences. While Reinforcement
  Learning from Human Feedback (RLHF) is effective, it relies on scalar rewards that
  often fail to capture the complexity of human preferences in real-world applications.
---

# Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards

## Quick Facts
- arXiv ID: 2402.18571
- Source URL: https://arxiv.org/abs/2402.18571
- Authors: Haoxiang Wang; Yong Lin; Wei Xiong; Rui Yang; Shizhe Diao; Shuang Qiu; Han Zhao; Tong Zhang
- Reference count: 16
- One-line primary result: DPA achieves arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines like Direct Preference Optimization (DPO) on Mistral-7B.

## Executive Summary
This paper addresses the challenge of fine-grained control over large language models (LLMs) to adapt them to diverse user preferences. While Reinforcement Learning from Human Feedback (RLHF) is effective, it relies on scalar rewards that often fail to capture the complexity of human preferences in real-world applications. The authors propose the Directional Preference Alignment (DPA) framework, which incorporates multi-objective reward modeling to represent diverse preference profiles and models user preferences as directions (unit vectors) in the reward space. DPA achieves arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines like Direct Preference Optimization (DPO) on Mistral-7B.

## Method Summary
DPA represents user preferences as unit vectors in a multi-dimensional reward space and uses linear scalarization to combine multiple reward objectives into a single preference-conditional reward for optimization. The method involves training a multi-objective reward model on combined datasets (HelpSteer and UltraFeedback) to predict multiple reward dimensions simultaneously, then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF). RSF iteratively generates multiple responses for sampled prompts and preference directions, ranks them using the multi-objective reward model, selects the highest-reward response, and fine-tunes the policy model on these selected samples. The primary result is that DPA achieves arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines like Direct Preference Optimization (DPO) on Mistral-7B.

## Key Results
- DPA achieves arithmetic control over the trade-off between helpfulness and verbosity on Mistral-7B
- The method maintains competitive performance with strong baselines like Direct Preference Optimization (DPO)
- Iterative Rejection Sampling Fine-tuning (RSF) provides stable and efficient reward optimization for diverse preference directions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modeling user preferences as unit vectors in multi-objective reward space enables arithmetic control of LLMs.
- Mechanism: The Directional Preference Alignment (DPA) framework represents user preferences as directions (unit vectors) in a multi-dimensional reward space. This allows users to arithmetically specify trade-offs between objectives (e.g., helpfulness and verbosity) through linear scalarization of multi-objective rewards.
- Core assumption: User preferences can be adequately represented as linear combinations of reward dimensions in the multi-objective space.
- Evidence anchors:
  - [abstract] "DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control."
  - [section 2.2] "To achieve a fine-grained representation of the preference signal, we model user preference as a direction in the multi-objective reward space, that is, a unit vector v = ⟨v1, . . . , vk⟩ ∈ Sk."
- Break condition: If user preferences are non-linear or cannot be adequately represented in the reward space dimensions, the arithmetic control would fail to capture true preferences.

### Mechanism 2
- Claim: Iterative Rejection Sampling Fine-tuning (RSF) provides stable and efficient reward optimization for diverse preference directions.
- Mechanism: RSF iteratively generates multiple responses for sampled prompts and preference directions, ranks them using the multi-objective reward model, selects the highest-reward response, and fine-tunes the policy model on these selected samples. This process alternates between rejection sampling and fine-tuning.
- Core assumption: The multi-objective reward model can accurately discriminate between responses across different preference directions.
- Evidence anchors:
  - [section 2.2] "We resort to an alternative approach, Rejection Sampling Fine-tuning (RSF)...with appealing simplicity, stability, and comparable reward gains."
  - [section 3.1] "We conduct rejection sampling following our iterative algorithm detailed in Sec. 2.2."
- Break condition: If the reward model is poorly calibrated or fails to discriminate across preference dimensions, the selected samples would not effectively optimize for diverse preferences.

### Mechanism 3
- Claim: Multi-objective reward modeling captures diverse preference profiles better than scalar rewards.
- Mechanism: DPA trains a reward model to predict multiple reward dimensions simultaneously (e.g., helpfulness and verbosity) rather than collapsing preferences into a single scalar value. This allows representation of complex preference profiles and avoids averaging across diverse user groups.
- Core assumption: Human preferences can be decomposed into distinct, measurable attributes that can be modeled separately.
- Evidence anchors:
  - [abstract] "Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles."
  - [section 2.1] "We consider k-objective reward for a response y given prompt x as r(x, y) = ⟨r1(x, y), . . . , rk(x, y)⟩ ∈ Rk where each ri(x, y) is the rating for a single attribute such as helpfulness, correctness, and verbosity."
- Break condition: If preferences cannot be decomposed into separable attributes or if important preference dimensions are omitted, the multi-objective approach would fail to capture true preference diversity.

## Foundational Learning

- Concept: Linear scalarization in multi-objective optimization
  - Why needed here: DPA uses linear scalarization to combine multiple reward objectives into a single preference-conditional reward for optimization
  - Quick check question: Given a user preference vector v = ⟨0.8, -0.6⟩ and rewards r = ⟨helpfulness, verbosity⟩, what is the preference-conditional reward?

- Concept: Rejection sampling for policy optimization
  - Why needed here: RSF uses rejection sampling to efficiently select high-quality responses across diverse preference directions without the instability of traditional RL methods
  - Quick check question: If you generate 80 responses and select the top 1 based on reward, what percentage of responses are kept for fine-tuning?

- Concept: Reward model regression training
  - Why needed here: The multi-objective reward model must be trained to accurately predict multiple reward dimensions from prompt-response pairs
  - Quick check question: What loss function is used to train the multi-objective reward model on labeled preference data?

## Architecture Onboarding

- Component map: Data pipeline -> Multi-objective reward model -> Iterative RSF loop -> Fine-tuned LLM
- Critical path: Fine-tuning loop (rejection sampling -> reward ranking -> response selection -> fine-tuning)
  - Each iteration: Sample prompts and preferences -> Generate responses -> Rank by v⊤r -> Select top -> Fine-tune
  - Iteration count: T=4 iterations in experiments
  - Batch sizes: 80 responses per prompt in initial sampling, 16 per preference in subsequent iterations
- Design tradeoffs:
  - Reward model accuracy vs. training complexity: Multi-objective model requires more data and careful training
  - Preference representation vs. user burden: Unit vectors provide arithmetic control but require users to specify directions
  - Sample efficiency vs. preference coverage: More samples per prompt improves coverage but increases computational cost
- Failure signatures:
  - Reward collapse: If reward model predictions become uniform across preferences, arithmetic control fails
  - Mode collapse: If RSF consistently selects similar responses regardless of preference, diversity is lost
  - Overfitting: If fine-tuning memorizes training prompts, generalization to new preferences suffers
- First 3 experiments:
  1. Validate reward model: Test multi-objective reward model on held-out data to ensure it predicts helpfulness and verbosity accurately
  2. Test preference arithmetic: Generate responses with different preference vectors (e.g., ⟨1,0⟩ vs ⟨0.8,-0.6⟩) and verify reward differences match expectations
  3. Ablation study: Compare DPA with scalar-reward baseline (DPO) on same data to quantify benefit of multi-objective approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DPA perform compared to other RLHF algorithms like PPO when using the same reward model?
- Basis in paper: [explicit] The paper mentions that PPO is the most predominant approach but resorts to RSF for this study, stating PPO is unstable and sample-inefficient in aligning LLMs
- Why unresolved: The paper does not directly compare DPA's performance against PPO when both use the same reward model
- What evidence would resolve it: Direct empirical comparison between DPA and PPO on the same dataset with identical reward models, measuring both reward optimization and alignment quality

### Open Question 2
- Question: How does the arithmetic control capability of DPA generalize to more than two reward objectives?
- Basis in paper: [inferred] The paper demonstrates arithmetic control with two objectives (helpfulness and verbosity) but does not explore scenarios with more reward dimensions
- Why unresolved: The paper only validates the method with two reward objectives and does not discuss scalability to higher dimensions
- What evidence would resolve it: Empirical results showing DPA's performance and control capabilities with three or more reward objectives, demonstrating whether the arithmetic control remains intuitive and effective

### Open Question 3
- Question: What is the computational overhead of DPA compared to traditional RLHF methods when scaling to larger models?
- Basis in paper: [explicit] The paper mentions that DPA requires 60 GPU hours per iteration and uses 8x A6000 GPUs, but does not compare this to other methods
- Why unresolved: While the paper provides DPA's computational requirements, it does not benchmark against traditional RLHF methods like PPO or DPO
- What evidence would resolve it: Comparative analysis of training time, GPU memory usage, and inference latency between DPA and traditional RLHF methods across different model sizes

### Open Question 4
- Question: How robust is DPA to misspecified or biased reward models?
- Basis in paper: [explicit] The paper acknowledges that DPA's performance is intrinsically linked to the quality of the reward model and discusses potential failure modes
- Why unresolved: The paper does not conduct experiments to test DPA's behavior when the reward model is imperfect or biased
- What evidence would resolve it: Experiments showing DPA's performance degradation or behavior changes when the reward model contains systematic biases or errors in different reward dimensions

## Limitations
- The framework assumes linear scalarization of preferences, which may not capture complex, non-linear preference interactions
- The reward space dimensionality is limited to 10 dimensions, potentially missing important preference attributes
- The iterative RSF approach is computationally expensive, discarding 98.75% of generated responses

## Confidence

- **High confidence**: The mathematical framework of directional preference alignment (unit vectors, linear scalarization, iterative RSF) is well-defined and theoretically sound
- **Medium confidence**: Empirical results show competitive performance with strong baselines on Mistral-7B, but evaluation focuses on narrow helpfulness-verbosity trade-off
- **Low confidence**: The claim that DPA provides superior arithmetic control over diverse user preferences beyond the tested helpfulness-verbosity trade-off

## Next Checks
1. **Non-linear preference validation**: Test whether user preferences that cannot be expressed as linear combinations of the current reward dimensions (e.g., "verbose only when helpful above 80%") can be adequately represented in the DPA framework. Generate responses for such non-linear preference rules and measure whether the output matches expectations.

2. **Dimensionality scaling experiment**: Evaluate DPA's performance as the number of reward dimensions increases beyond the current 10. Test whether adding dimensions like factual accuracy, creativity, or tone consistency improves preference control, or whether the framework becomes unstable with higher-dimensional reward spaces.

3. **Sample efficiency analysis**: Compare DPA performance using different sampling ratios (e.g., selecting top 5% vs top 1% of responses) to quantify the trade-off between computational cost and preference alignment quality. Measure whether the current 98.75% rejection rate is necessary for optimal performance or if more efficient selection strategies exist.