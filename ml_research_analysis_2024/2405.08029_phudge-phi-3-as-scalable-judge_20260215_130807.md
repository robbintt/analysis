---
ver: rpa2
title: 'PHUDGE: Phi-3 as Scalable Judge'
arxiv_id: '2405.08029'
source_url: https://arxiv.org/abs/2405.08029
tags: []
core_contribution: PHUDGE presents a fine-tuned Phi-3 model that achieves state-of-the-art
  performance on four evaluation benchmarks for LLM output quality assessment. The
  model outperforms existing approaches including larger models up to 10x in size
  across tasks including Feedback Test, Feedback OOD, MT Human, and Preference Test.
---

# PHUDGE: Phi-3 as Scalable Judge

## Quick Facts
- arXiv ID: 2405.08029
- Source URL: https://arxiv.org/abs/2405.08029
- Reference count: 22
- Primary result: Phi-3 3.8B model outperforms models up to 10x larger on LLM evaluation benchmarks

## Executive Summary
PHUDGE presents a fine-tuned Phi-3 model that achieves state-of-the-art performance on four evaluation benchmarks for LLM output quality assessment. The model demonstrates that small language models can effectively serve as judges for LLM outputs, addressing cost, latency, and security limitations of proprietary models while maintaining evaluation quality. Key innovations include treating the scoring problem as classification/regression rather than causal generation, introducing a modified Earth Mover's Distance loss function with controllable smoothing, and systematic data augmentation through partial input removal.

## Method Summary
PHUDGE fine-tunes Phi-3 3.8B using LoRA (rank 128) with a modified Earth Mover's Distance loss that treats scoring as classification or regression rather than causal generation. The approach uses systematic data augmentation by randomly dropping rubric and reference answer components during training with 50% probability. The model is evaluated across nine datasets using Pearson, Kendall, and Spearman correlations for absolute scoring, and accuracy for relative scoring tasks.

## Key Results
- PHUDGE outperforms existing approaches including larger models up to 10x in size
- Strong correlation with both GPT-4 and human annotators on unseen data
- Modified EMD loss with controllable smoothing provides stable training and better results
- Data augmentation improves generalization to reference-free and rubric-free settings

## Why This Works (Mechanism)

### Mechanism 1
Treating LLM scoring as classification/regression instead of causal generation yields better performance and faster inference by avoiding autoregressive generation overhead. The model can directly output score probabilities or values, leading to faster inference and more stable training.

### Mechanism 2
Modified Earth Mover's Distance (EMD) loss with controllable smoothing improves classification performance by accounting for ordinal nature of scoring. EMD penalizes misclassifications proportionally to distance between classes, while the smoothing parameter α controls loss stability.

### Mechanism 3
Systematic data augmentation with partial input removal improves model generalization by forcing the model to learn robust evaluation criteria. Randomly dropping rubric and reference answer components during training prevents overfitting to specific input patterns.

## Foundational Learning

- **Ordinal classification vs standard classification**: Why needed here - understanding why standard cross-entropy is suboptimal for score prediction where class distances matter. Quick check question: Why is predicting "2 as 3" better than "2 as 5" in a scoring system?

- **Loss function design for structured prediction**: Why needed here - EMD loss implementation requires understanding cumulative distribution functions and distance metrics. Quick check question: How does EMD loss differ mathematically from cross-entropy for ordinal classes?

- **Parameter-efficient fine-tuning (LoRA)**: Why needed here - LoRA enables efficient adaptation of Phi-3 while maintaining inference speed. Quick check question: What rank of LoRA provides optimal balance between performance and parameter count for this task?

## Architecture Onboarding

- **Component map**: Phi-3 base model → LoRA adapter (rank 128) → Classification/Regression head → EMD/Cross-Entropy loss
- **Critical path**: Input processing → LoRA adaptation → Score prediction → Loss computation → Parameter update
- **Design tradeoffs**: Causal generation provides feedback but is slower; classification/regression is faster but loses reasoning; EMD loss handles ordinal classes better but is computationally heavier
- **Failure signatures**: Early overfitting with cross-entropy, unstable training with regression, poor OOD performance indicating data leakage or insufficient augmentation
- **First 3 experiments**:
  1. Baseline: Causal LoRA-128 model with feedback generation on training split
  2. Classification: Multi-class classification with cross-entropy loss, compare validation correlation
  3. EMD loss: Implement squared EMD loss, test different α values for smoothing control

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of PHUDGE compare to human evaluators across different domains and types of questions? The paper mentions strong correlation with human annotators but lacks detailed comparisons across diverse domains and question types.

### Open Question 2
How does the choice of LoRA rank affect the performance of PHUDGE on different types of evaluation tasks? While the paper notes LoRA rank had highest effect on scores, it doesn't explore the relationship between rank and task-specific performance in depth.

### Open Question 3
How does the Earth Mover's Distance (EMD) loss function perform on other types of grading tasks beyond LLM evaluation? The paper introduces modified EMD for grading tasks but doesn't investigate its broader applicability to other domains.

## Limitations

- Modified EMD loss implementation details remain unclear, particularly the exact formulation with Minkowski distance and penalty α parameter values
- Augmentation strategy may introduce artifacts when both rubric and reference answer are simultaneously dropped
- Long-term generalization to completely unseen evaluation paradigms requires further validation

## Confidence

- **High Confidence**: Phi-3 as judge outperforms larger models up to 10x in size across multiple benchmarks
- **Medium Confidence**: Modified EMD loss with controllable smoothing provides significant advantage over standard cross-entropy
- **Medium Confidence**: Data augmentation through partial input removal improves generalization

## Next Checks

1. **Loss Function Reproducibility Test**: Implement exact EMD loss formulation using described Minkowski distance with penalty parameter α, then conduct controlled experiments varying α values (0.1, 0.5, 1.0, 2.0) to determine optimal smoothing and verify stability improvements over standard cross-entropy.

2. **Augmentation Robustness Analysis**: Systematically evaluate model performance when systematically removing different combinations of evaluation components (rubric only, reference only, both) across all test sets to quantify the contribution of each augmentation strategy and identify potential failure modes.

3. **Cross-Domain Generalization Benchmark**: Test PHUDGE on completely unseen evaluation domains (e.g., code generation quality, medical diagnosis accuracy, legal document analysis) to validate claims about broad applicability beyond the original Feedback Collection dataset domain.