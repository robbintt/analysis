---
ver: rpa2
title: 'ELLA: Empowering LLMs for Interpretable, Accurate and Informative Legal Advice'
arxiv_id: '2408.07137'
source_url: https://arxiv.org/abs/2408.07137
tags:
- legal
- users
- articles
- response
- article
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ELLA, a tool that enhances legal Large Language
  Models (LLMs) by providing interpretability, accuracy, and informative legal advice.
  ELLA addresses the issue of LLMs generating incorrect or baseless responses by visually
  presenting the correlation between legal articles and the LLM's response, allowing
  users to interactively select relevant legal articles for more accurate responses,
  and retrieving relevant legal cases for user reference.
---

# ELLA: Empowering LLMs for Interpretable, Accurate and Informative Legal Advice

## Quick Facts
- arXiv ID: 2408.07137
- Source URL: https://arxiv.org/abs/2408.07137
- Authors: Yutong Hu; Kangcheng Luo; Yansong Feng
- Reference count: 11
- Primary result: 76.34 NDCG score for legal case retrieval model

## Executive Summary
ELLA is a tool designed to enhance legal Large Language Models (LLMs) by addressing the issue of generating incorrect or baseless responses. It provides interpretability by visually presenting the correlation between legal articles and LLM responses, improves accuracy through interactive legal article selection, and offers informative advice by retrieving relevant legal cases. The system aims to help users understand legal advice better by providing legal basis for responses and highlighting key information in legal cases.

## Method Summary
ELLA operates through a multi-stage process. First, it retrieves relevant legal articles using a fine-tuned embedding model (BGE) on legal corpora. Users can then interactively select which articles the LLM should use for generating responses. The system interprets each sentence in the LLM's response by displaying the corresponding legal basis through similarity matching with legal articles. Additionally, ELLA retrieves and highlights relevant sentences in legal cases to provide comprehensive reference information. The legal case retrieval model is also fine-tuned on a constructed dataset to improve relevance matching.

## Key Results
- User study shows presenting legal basis helps users understand LLM responses better
- Accuracy of LLM responses improves when users intervene in selecting legal articles (80% success rate)
- Legal case retrieval model achieves NDCG score of 76.34

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interactive legal article selection improves LLM response accuracy by filtering out irrelevant legal articles that introduce noise.
- Mechanism: When users manually select relevant legal articles from a pool of retrieved articles, the LLM generates responses based only on these selected articles. This reduces the probability of irrelevant or incorrect legal articles influencing the response generation.
- Core assumption: Users can accurately identify which retrieved legal articles are relevant to their query.
- Evidence anchors:
  - [abstract]: "The accuracy of LLM's responses also improves when users intervene in selecting legal articles."
  - [section]: "In 80% cases, users can successfully receive correct responses by selecting relevant legal articles for LLM to regenerate responses."
  - [corpus]: Weak evidence; no specific corpus studies cited for this mechanism.
- Break condition: If users consistently select irrelevant articles or miss relevant ones, the improvement in accuracy would diminish.

### Mechanism 2
- Claim: Presenting legal article basis for each sentence increases user trust and understanding of LLM responses.
- Mechanism: By calculating similarity between each sentence in the LLM's response and legal articles, and displaying the corresponding legal basis, users can verify the reliability of the advice. This transparency helps users understand and trust the LLM's responses.
- Core assumption: Users have the ability to interpret and understand the legal basis presented.
- Evidence anchors:
  - [abstract]: "Our user study shows that presenting the legal basis for the response helps users understand better."
  - [section]: "Users have reported that for approximately 95% of the queries, ELLA can accurately provide the legal article basis of the responses generated by the LLM."
  - [corpus]: No specific corpus studies cited for this mechanism.
- Break condition: If the legal basis is not accurately matched or users cannot understand the legal terminology, trust and understanding may not improve.

### Mechanism 3
- Claim: Highlighting relevant sentences in legal cases improves user reading efficiency and provides comprehensive reference information.
- Mechanism: By retrieving relevant legal cases and highlighting sentences that match the user's query, users can quickly locate key information within long legal documents. This feature aids in understanding the context and applicability of the legal case to their situation.
- Core assumption: The highlighted sentences accurately represent the key information relevant to the user's query.
- Evidence anchors:
  - [abstract]: "ELLA also retrieves relevant legal cases for user reference."
  - [section]: "All users concurred that highlighting pertinent sentences significantly streamlines the process of reading cases."
  - [corpus]: No specific corpus studies cited for this mechanism.
- Break condition: If the highlighting algorithm fails to identify truly relevant sentences, user efficiency and comprehension may not improve.

## Foundational Learning

- Concept: Legal article retrieval and relevance matching
  - Why needed here: The system relies on retrieving and matching legal articles to user queries to provide accurate legal advice.
  - Quick check question: Can you explain how BM25 or cosine similarity might be used to rank legal articles by relevance to a query?

- Concept: Interactive user interface design
  - Why needed here: Users interact with the system by selecting legal articles and viewing highlighted sentences, requiring an intuitive and responsive interface.
  - Quick check question: How would you design a UI element that allows users to easily select multiple legal articles from a list?

- Concept: Embedding models and fine-tuning
  - Why needed here: The system uses embedding models (like BGE) fine-tuned on legal corpora to improve retrieval accuracy in the legal domain.
  - Quick check question: Why is it necessary to fine-tune a general embedding model on legal texts rather than using it out-of-the-box?

## Architecture Onboarding

- Component map: Chat Interface -> Legal Article Retrieval -> Interactive Selection -> Response Interpretation -> Legal Case Retrieval
- Critical path: 1. User inputs query → 2. Legal articles retrieved → 3. Articles displayed for selection → 4. User selects articles → 5. LLM generates response → 6. Response interpreted with legal basis → 7. Legal cases retrieved and highlighted.
- Design tradeoffs:
  - Accuracy vs. user effort: Allowing user selection improves accuracy but requires user effort. Automatic selection would be easier but potentially less accurate.
  - Transparency vs. complexity: Providing detailed legal basis increases transparency but may overwhelm users with information.
  - Speed vs. comprehensiveness: Retrieving and highlighting legal cases provides more information but may slow down response time.
- Failure signatures:
  - LLM generates irrelevant or incorrect responses despite user selection.
  - Legal basis does not match the content of the response sentences.
  - Highlighted sentences in legal cases are not relevant to the user's query.
  - System becomes slow due to large number of legal articles or cases being processed.
- First 3 experiments:
  1. Test the accuracy improvement when users select legal articles vs. when the system automatically selects the top 3 articles.
  2. Evaluate user understanding and trust by comparing responses with and without legal basis annotations.
  3. Measure user reading efficiency by timing how long it takes to find key information with and without highlighted sentences in legal cases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ELLA's performance compare to other state-of-the-art legal domain LLMs like DISC-LawLLM, ChatLaw, or LawGPT when handling complex legal queries?
- Basis in paper: [inferred] The paper mentions that ELLA can be adapted to use other legal domain LLMs, but does not provide a direct comparison.
- Why unresolved: The paper focuses on demonstrating ELLA's improvements over a baseline LLM rather than comparing it to other specialized legal LLMs.
- What evidence would resolve it: A comparative study evaluating ELLA's performance against other state-of-the-art legal domain LLMs on a standardized set of complex legal queries.

### Open Question 2
- Question: What is the impact of ELLA's interactive legal article selection on user trust and satisfaction in legal consultations?
- Basis in paper: [explicit] The paper mentions that the response interpretation helps users understand and trust LLM responses, and that users can interactively select legal articles for more accurate responses.
- Why unresolved: While the paper suggests improvements in accuracy, it does not directly measure user trust and satisfaction resulting from the interactive selection feature.
- What evidence would resolve it: A user study specifically designed to measure changes in trust and satisfaction when using ELLA's interactive legal article selection compared to a non-interactive version.

### Open Question 3
- Question: How does ELLA handle queries that involve multiple, unrelated legal issues within a single input?
- Basis in paper: [inferred] The paper mentions that ELLA's legal article retrieval module may not comprehensively extract all relevant legal articles for queries with multiple questions.
- Why unresolved: The paper does not provide a detailed explanation or solution for handling complex queries with multiple legal issues.
- What evidence would resolve it: A case study or user feedback on ELLA's performance with multi-issue queries, along with proposed improvements or alternative approaches to handle such cases.

## Limitations
- The evaluation relies heavily on user study results, introducing potential subjectivity and sampling bias
- The 76.34 NDCG score lacks comparison to baseline models or established benchmarks in the legal domain
- The mechanism of user-selected legal articles improving LLM accuracy assumes users can accurately identify relevant articles, yet the study does not validate user selection accuracy independently

## Confidence
- **High**: The interactive legal article selection mechanism and its reported impact on accuracy (80% success rate in user study)
- **Medium**: The effectiveness of legal basis presentation for improving user understanding (95% accuracy reported in user study)
- **Low**: The legal case retrieval model's NDCG score of 76.34 without comparative baseline data or external validation

## Next Checks
1. Conduct an independent evaluation of user article selection accuracy by having legal experts verify the relevance of user-selected articles across a larger sample of queries
2. Benchmark the legal case retrieval model against established legal IR models (e.g., LexML, Chinese legal search engines) using standard metrics like P@K and MAP
3. Perform a longitudinal study to assess whether ELLA's benefits persist across different types of legal queries and user expertise levels over time