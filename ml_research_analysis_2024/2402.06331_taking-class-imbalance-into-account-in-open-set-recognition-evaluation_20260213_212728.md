---
ver: rpa2
title: Taking Class Imbalance Into Account in Open Set Recognition Evaluation
arxiv_id: '2402.06331'
source_url: https://arxiv.org/abs/2402.06331
tags:
- classes
- recognition
- open
- class
- unknown
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses evaluation challenges in Open Set Recognition
  (OSR) due to class imbalance between known known classes (kkc) and unknown unknown
  classes (uuc). The authors analyze standard evaluation protocols and demonstrate
  that commonly used metrics like F1-score and Accuracy can be misleading when kkc
  and uuc have different numbers of instances.
---

# Taking Class Imbalance Into Account in Open Set Recognition Evaluation

## Quick Facts
- arXiv ID: 2402.06331
- Source URL: https://arxiv.org/abs/2402.06331
- Reference count: 39
- Primary result: Standard metrics like F1-score and Accuracy are misleading in OSR due to class imbalance between known and unknown classes

## Executive Summary
This paper addresses critical evaluation challenges in Open Set Recognition (OSR) caused by class imbalance between known known classes (kkc) and unknown unknown classes (uuc). The authors demonstrate that commonly used metrics like F1-score and Accuracy can produce misleading results when kkc and uuc have different instance counts. They propose four evaluation metrics - Inner, Outer, Halfpoint, and Overall scores - to properly assess OSR performance across varying openness configurations. Through experiments with four OSR methods on CIFAR10 and SVHN datasets, the study shows that generative methods excel at overall balanced accuracy while discriminative methods perform better in closed-set recognition.

## Method Summary
The study evaluates four Open Set Recognition methods: Thresholded Softmax, Openmax, Noise Softmax, and Overlay Softmax. These methods were tested on CIFAR10 and SVHN datasets with varying openness ratios (class distribution between kkc and uuc). The evaluation uses four proposed metrics: Inner score for closed-set classification, Outer score for open-set detection, Halfpoint score (extends Inner by penalizing false unknowns), and Overall score (treats uuc as additional kkc class). Experiments involved training CNN architectures with two convolutional layers, max-pooling, dropout, and fully connected layers, then evaluating performance across different openness configurations.

## Key Results
- Standard metrics like F1-score and Accuracy are unreliable in OSR due to class imbalance between kkc and uuc
- Generative OSR methods outperform discriminative ones on Overall scores by creating synthetic uuc samples
- Different openness configurations reveal varying strengths: discriminative methods excel at closed-set recognition while generative methods perform better overall
- The study provides evaluation guidelines including using multiple openness configurations and avoiding F1-score/Accuracy

## Why This Works (Mechanism)

### Mechanism 1
Standard evaluation metrics like F1-score and Accuracy can mislead when known known classes (kkc) and unknown unknown classes (uuc) have different instance counts. These metrics assume balanced class distributions, inflating performance estimates when kkc or uuc is the minority class because they don't account for imbalance in positive/negative class ratios. The test set's kkc/uuc distribution must reflect the actual deployment scenario's class balance for these metrics to be meaningful.

### Mechanism 2
Open Set Recognition evaluation should use multiple openness configurations to capture variability in kkc/uuc class numbers. Openness quantifies the ratio of kkc to uuc classes, stressing the model across varying imbalance scenarios to reveal strengths/weaknesses in recognizing unknowns versus knowns. Since the number of kkc and uuc classes is unknown in deployment, evaluation must cover diverse configurations to be comprehensive.

### Mechanism 3
Generative methods outperform discriminative ones on overall scores because they create synthetic uuc samples, expanding the open space. By augmenting with synthetic unknowns, generative methods train to better discriminate between kkc and uuc, improving overall balanced accuracy across all classes. This assumes the synthetic uuc samples realistically represent the distribution of true unknowns.

## Foundational Learning

- Concept: Class Imbalance in Binary Classification
  - Why needed here: OSR reduces to binary decision between kkc and uuc; imbalance directly impacts metric reliability
  - Quick check question: If positives are 1% of data, would Accuracy still be a good metric? (Answer: No, it would be misleading.)

- Concept: Openness as an Evaluation Parameter
  - Why needed here: Openness captures ratio of known to unknown classes, dictating kkc/uuc balance in tests
  - Quick check question: If Openness=0.5 with 10 total classes, how many are kkc and how many are uuc? (Answer: 5 kkc, 5 uuc.)

- Concept: Confusion Matrix Splitting for Multi-Score Evaluation
  - Why needed here: Different OSR scores use different partitions of confusion matrix
  - Quick check question: Which score treats uuc as an extra kkc class? (Answer: Overall score.)

## Architecture Onboarding

- Component map: Dataset preparation (CIFAR10/SVHN) -> Class assignment to kkc/uuc -> Model training (4 OSR methods) -> Metric computation (4 scores) -> Result aggregation
- Critical path: Data preparation -> Model training -> Metric calculation -> Result aggregation
- Design tradeoffs: Generative vs discriminative methods; synthetic uuc generation vs threshold-based rejection
- Failure signatures: High variance in scores across openness values; F1-score >0.8 on highly imbalanced data
- First 3 experiments:
  1. Train Thresholded Softmax on CIFAR10 kkc vs SVHN uuc with Openness=0.2; compute all four scores
  2. Repeat with Openness=0.5; compare score changes
  3. Swap datasets (MNIST kkc vs Omniglot uuc) and repeat

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal method for handling class imbalance between known and unknown classes in Open Set Recognition when unknown instances significantly exceed known instances? The paper identifies this issue but doesn't provide a definitive solution. Resolution would require comparative study of different methods addressing imbalance with empirical results across scenarios.

### Open Question 2
How does the proposed Halfpoint score metric perform compared to traditional metrics like F1-score and Accuracy in OSR tasks with varying openness? The paper proposes Halfpoint but lacks comprehensive comparison across different Openness configurations. Resolution would require experimental results comparing Halfpoint with traditional metrics across wide range of Openness values.

### Open Question 3
What is the impact of using different network architectures on OSR method performance in terms of proposed evaluation metrics? The paper uses single architecture, leaving question of how different architectures affect performance unanswered. Resolution would require systematic study evaluating OSR methods across multiple network architectures.

## Limitations

- Findings limited to CIFAR10 and SVHN datasets, may not generalize to all image domains
- Only four simple OSR methods evaluated, potentially missing behaviors from more sophisticated architectures
- Assumes synthetic uuc samples from generative methods realistically represent true unknowns, which may not hold in practice

## Confidence

- Standard metrics misleading under class imbalance: High confidence
- Generative vs discriminative relative performance: Medium confidence (depends on synthetic generation quality)
- Multiple openness configurations protocol: Medium confidence (demonstrates value but doesn't explore full evaluation strategy space)

## Next Checks

1. **Cross-domain validation**: Test evaluation protocol and metric behavior on non-image datasets (e.g., text or tabular data) to verify generalizability beyond CIFAR10/SVHN

2. **Generative method robustness**: Evaluate how synthetic uuc generation quality affects Overall score performance across different generative approaches and unknown distributions

3. **Threshold sensitivity analysis**: Systematically vary confidence threshold in Outer score computation to determine sensitivity to this parameter across datasets and openness ratios