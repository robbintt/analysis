---
ver: rpa2
title: Investigating the Impact of Data Contamination of Large Language Models in
  Text-to-SQL Translation
arxiv_id: '2402.08100'
source_url: https://arxiv.org/abs/2402.08100
tags:
- data
- spider
- termite
- contamination
- text-to-sql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether the performance of GPT-3.5 in the
  Text-to-SQL task is influenced by data contamination, i.e., exposure to target textual
  descriptions and related code during training. The authors introduce a novel method
  to detect data contamination by comparing the model's ability to reconstruct masked
  database schema information from potentially seen (Spider) and unseen (Termite)
  datasets.
---

# Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation

## Quick Facts
- arXiv ID: 2402.08100
- Source URL: https://arxiv.org/abs/2402.08100
- Authors: Federico Ranaldi; Elena Sofia Ruzzetti; Dario Onorati; Leonardo Ranaldi; Cristina Giannone; Andrea Favalli; Raniero Romagnoli; Fabio Massimo Zanzotto
- Reference count: 7
- Key outcome: GPT-3.5 exhibits significantly better performance on potentially seen Spider dataset compared to unseen Termite dataset, even after removing structural information via adversarial table disconnection, indicating data contamination effects.

## Executive Summary
This paper investigates whether GPT-3.5's performance in Text-to-SQL translation is influenced by data contamination - exposure to target textual descriptions and related code during training. The authors introduce a novel method to detect data contamination by comparing the model's ability to reconstruct masked database schema information from potentially seen (Spider) and unseen (Termite) datasets. They find that GPT-3.5 shows significantly better performance on the Spider dataset compared to the unseen Termite dataset, even when structural information is removed through an adversarial table disconnection approach.

## Method Summary
The authors evaluate GPT-3.5's Text-to-SQL performance using zero-shot prompting on two datasets: Spider (potentially seen) and Termite (unseen). They introduce a Data Contamination (DC) accuracy metric that measures the model's ability to reconstruct masked column names from database schemas. The Adversarial Table Disconnection (ATD) approach removes foreign key constraints from databases to test whether performance differences persist when structural information is eliminated. Performance is evaluated using Test Suite Accuracy, comparing generated SQL queries against ground truth across semantic equivalence test cases.

## Key Results
- GPT-3.5 achieves significantly higher Test Suite Accuracy on Spider (40.9%) compared to Termite (26.4%)
- The performance gap persists even after applying ATD to remove structural information from databases
- DC-accuracy is higher on Spider (61.4%) compared to Termite (35.1%), indicating better column name reconstruction on potentially seen data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3.5's superior performance on Spider compared to Termite indicates data contamination.
- Mechanism: Data contamination occurs when models are exposed to evaluation data during pre-training. Prior exposure to database schemas enables better SQL generation through memorization.
- Core assumption: Spider dataset was included in GPT-3.5's pre-training corpus while Termite was not.
- Evidence anchors:
  - Results show 40.9% accuracy on Spider vs 26.4% on Termite
  - "Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset"
  - Corpus shows limited prior research on Text-to-SQL data contamination
- Break condition: If Termite's performance matched Spider's, contamination would not be the primary factor.

### Mechanism 2
- Claim: ATD approach reveals data contamination impact by removing structural information.
- Mechanism: Removing foreign key constraints makes SQL generation harder. If model relies on prior knowledge, performance should degrade more on unseen data.
- Core assumption: ATD effectively removes structural information while preserving semantic content.
- Evidence anchors:
  - Performance gap persists after ATD application
  - "We analyze GPT-3.5's efficacy on databases with modified information via an adversarial table disconnection approach"
  - Novel approach with limited prior evidence in corpus
- Break condition: Similar degradation on both datasets would suggest knowledge isn't the primary factor.

### Mechanism 3
- Claim: DC-accuracy metric quantifies data contamination through schema reconstruction ability.
- Mechanism: Masking column names and measuring reconstruction accuracy assesses prior database knowledge. Higher accuracy on Spider indicates contamination.
- Core assumption: Column reconstruction ability reliably indicates pre-training exposure.
- Evidence anchors:
  - DC-accuracy shows 61.4% on Spider vs 35.1% on Termite
  - "A clue to determine whether the data contamination has occurred is that the model is able to reconstruct missing information"
  - Novel metric with limited prior research
- Break condition: Similar DC-accuracy on both datasets would suggest no prior knowledge advantage.

## Foundational Learning

- Concept: Text-to-SQL translation
  - Why needed here: Understanding this task is crucial for interpreting model performance and the significance of data contamination effects.
  - Quick check question: What are the key challenges in Text-to-SQL translation, and how do they relate to language model performance?

- Concept: Data contamination in machine learning
  - Why needed here: This concept directly explains why models might perform better on some datasets than others due to prior exposure.
  - Quick check question: How can data contamination impact the evaluation of machine learning models, and what are some potential strategies to mitigate its effects?

- Concept: Adversarial approaches in machine learning
  - Why needed here: The ATD method uses adversarial techniques to test model robustness against data contamination.
  - Quick check question: What are some common adversarial approaches used in machine learning, and how can they help in assessing the limitations and vulnerabilities of models?

## Architecture Onboarding

- Component map: Spider dataset -> Termite dataset -> GPT-3.5 model -> DC-accuracy metric -> Test Suite Accuracy metric -> ATD approach

- Critical path:
  1. Construct and validate Termite as unseen Text-to-SQL resource
  2. Implement DC-accuracy metric for contamination detection
  3. Evaluate GPT-3.5 on both datasets using Test Suite Accuracy
  4. Apply ATD to assess contamination impact

- Design tradeoffs:
  1. Dataset construction: Ensuring Termite is truly unseen while maintaining comparability to Spider
  2. Evaluation metrics: Balancing automated evaluation with semantic equivalence assessment
  3. Adversarial approach: Removing structural information without altering semantic content

- Failure signatures:
  1. Poor performance on both datasets indicating fundamental Text-to-SQL limitations
  2. Similar performance suggesting contamination isn't the primary factor
  3. Inconsistent results across databases indicating dataset construction issues

- First 3 experiments:
  1. Establish baseline performance on small subsets of both datasets
  2. Validate DC-accuracy metric with sample schema reconstruction
  3. Apply ATD to subsets and measure performance impact

## Open Questions the Paper Calls Out

- Question: How does data contamination affect the performance of other large language models (LLMs) beyond GPT-3.5, such as GPT-4 or other transformer-based models?
  - Basis in paper: [explicit] The paper mentions that preliminary experiments suggest data contamination affects GPT-4 as well, but does not provide detailed analysis.
  - Why unresolved: The study focuses primarily on GPT-3.5, with only brief mention of potential effects on GPT-4.
  - What evidence would resolve it: Systematic experiments measuring contamination impact across various LLMs including GPT-4 and other transformer models.

- Question: Can the proposed adversarial table disconnection (ATD) method be effectively applied to other domains or tasks beyond Text-to-SQL, and what are its limitations?
  - Basis in paper: [inferred] The paper introduces ATD for Text-to-SQL but doesn't explore broader applicability.
  - Why unresolved: The paper doesn't explore ATD's versatility or effectiveness in other contexts.
  - What evidence would resolve it: Testing ATD on various NLP tasks like summarization or question answering to determine effectiveness and limitations.

- Question: What are the long-term implications of data contamination on the reliability of LLM benchmarks, and how can benchmarks be designed to mitigate these effects?
  - Basis in paper: [explicit] The paper discusses overestimation of LLM performance due to contamination and suggests need for benchmarks outside pretraining.
  - Why unresolved: While highlighting the issue, the paper doesn't provide detailed framework for designing contamination-resistant benchmarks.
  - What evidence would resolve it: Developing and implementing new benchmark datasets carefully curated to avoid contamination, then evaluating their impact over time.

## Limitations

- Cannot definitively confirm whether Spider was included in GPT-3.5's pre-training corpus
- Termite dataset has smaller sample size (10 databases vs Spider's 20 in validation)
- DC-accuracy metric primarily measures column name memorization rather than comprehensive schema understanding

## Confidence

Our confidence in the core claim that data contamination influences model performance is Medium-High, based on consistent performance gaps between datasets and persistent differences after ATD application.

## Next Checks

1. Cross-dataset schema similarity analysis to quantify structural and lexical overlap between Spider and Termite databases
2. Human evaluation validation on subset of generated SQL queries to validate Test Suite Accuracy metric
3. Extended contamination testing with additional models (GPT-4, Claude) on same Spider/Termite comparison