---
ver: rpa2
title: Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph
  kernels
arxiv_id: '2402.03838'
source_url: https://arxiv.org/abs/2402.03838
tags:
- kernel
- graphs
- graph
- kernels
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new positive definite graph kernel called
  Sliced Wasserstein Weisfeiler-Lehman (SWWL) designed for Gaussian process regression
  on large-scale graph datasets, particularly from computational physics where graphs
  can have tens of thousands of nodes. The SWWL kernel combines continuous Weisfeiler-Lehman
  graph embeddings with sliced Wasserstein distances, achieving drastically reduced
  complexity compared to existing graph kernels while maintaining positive definiteness.
---

# Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph kernels

## Quick Facts
- arXiv ID: 2402.03838
- Source URL: https://arxiv.org/abs/2402.03838
- Authors: Raphaël Carpintero Perez; Sébastien da Veiga; Josselin Garnier; Brian Staber
- Reference count: 11
- Introduces SWWL kernel for GP regression on large graphs (up to 100K nodes)

## Executive Summary
This paper introduces the Sliced Wasserstein Weisfeiler-Lehman (SWWL) kernel for Gaussian process regression on large-scale graph datasets. The method combines continuous Weisfeiler-Lehman graph embeddings with sliced Wasserstein distances to achieve drastically reduced computational complexity while maintaining positive definiteness. SWWL successfully handles graphs with up to 100,000 nodes in computational physics applications where existing graph kernels fail. The authors demonstrate that only 20 projections and 100 quantiles are needed for good performance, achieving efficient dimension reduction while maintaining accuracy comparable to state-of-the-art methods on molecular graph classification tasks.

## Method Summary
SWWL combines continuous Weisfeiler-Lehman graph embeddings with sliced Wasserstein distances to create a positive definite kernel for Gaussian process regression. The method uses fixed quantiles instead of full sorting for 1D Wasserstein distance computation and applies Monte Carlo projections to avoid expensive optimal transport computations. For regression tasks, the kernel is combined with a Matérn kernel using a tensor product approach. The computational complexity is reduced from O(n³) to O(n log n) through the quantile approximation and sliced Wasserstein approach.

## Key Results
- Successfully handles graphs with up to 100,000 nodes in computational physics applications
- Achieves RMSEs of 1.36e-3 (Rotor37), 0.90 (Tensile2d), and 7.29e-4 (AirfRANS)
- Performs comparably to state-of-the-art kernels on molecular graph classification while being significantly faster
- Only 20 projections and 100 quantiles needed for good performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SWWL kernel achieves positive definiteness while maintaining computational tractability for large graphs
- Mechanism: By using sliced Wasserstein distance instead of standard Wasserstein distance, SWWL constructs a Hilbertian distance between graph embeddings, enabling the use of distance substitution kernels that are guaranteed to be positive definite
- Core assumption: The sliced Wasserstein distance is Hilbertian when combined with fixed quantile projections, allowing explicit feature map construction
- Evidence anchors:
  - [abstract]: "the sliced Wasserstein (SW) distance is Hilbertian, thus allowing the construction of a positive definite kernel, which is not the case with the Wasserstein distance"
  - [section 4]: "By combining Eqs. (9) and (10), this estimate can be rewritten as ySW r,P,Qpµ, νq = || ϕpµq ´ ϕpνq||r where ||.||r is the r-norm in RP Q, and ϕ is the explicit feature map"
- Break condition: If the quantile-based approximation fails to preserve the Hilbertian property of sliced Wasserstein distance, the resulting kernel may lose positive definiteness

### Mechanism 2
- Claim: SWWL drastically reduces computational complexity from O(n³) to O(n log n) for graph kernel computation
- Mechanism: Fixed quantile approximation replaces full sorting operations in 1D Wasserstein distance computation, while sliced Wasserstein uses Monte Carlo projections to avoid expensive optimal transport computations between high-dimensional distributions
- Core assumption: Using Q fixed quantiles instead of sorting all n points maintains sufficient accuracy while reducing complexity from O(n log n) to O(n)
- Evidence anchors:
  - [section 4]: "The Q-quantiles estimation of the 1d-Wasserstein distance... writes Wr,Qpµθ, νθ)..." and "the sliced Wasserstein distance given by Eq. (8) is finally estimated as follows"
  - [section 4]: "The computation of the 1d-Wasserstein distance between empirical measures usually consists in sorting the projected points and then summing a power of the Euclidean distances between values at the same ranks. Here, we propose instead to use Q ! n equally-spaced quantiles"
- Break condition: If Q quantiles are insufficient for accurate Wasserstein distance approximation, the method loses accuracy while gaining speed

### Mechanism 3
- Claim: SWWL kernel achieves dimension reduction through efficient embedding representation
- Mechanism: The projected quantile embedding ϕ compresses the continuous WL embedding from H(d+1) dimensions to PQ dimensions while preserving discriminative information
- Core assumption: A small number of projections (P=20) and quantiles (Q=100) captures sufficient information from the original graph structure and attributes
- Evidence anchors:
  - [abstract]: "The authors demonstrate that only 20 projections and 100 quantiles are needed for good performance, achieving efficient dimension reduction"
  - [section 4]: "In Definition 1 the probability distributions of interest are the empirical distributions associated with the continuous WL embeddings EG = (EGu)uEV of graphs G P G with H iterations, so that s = H(d+1). We, therefore, get Property 1."
  - [section 5.2]: "This means that in general an embedding of size 20 × 100 is sufficient to achieve good performance"
- Break condition: If the dimension reduction discards critical structural information, the kernel may fail to distinguish between similar graphs

## Foundational Learning

- Concept: Positive definite kernels and reproducing kernel Hilbert spaces
  - Why needed here: Understanding why positive definiteness matters for Gaussian process regression and kernel methods
  - Quick check question: What happens to Gaussian process predictions if the kernel is not positive definite?

- Concept: Optimal transport theory and Wasserstein distances
  - Why needed here: SWWL builds on optimal transport distances between graph embeddings
  - Quick check question: How does the sliced Wasserstein distance differ from the standard Wasserstein distance in computational complexity?

- Concept: Graph neural networks and graph kernels
  - Why needed here: SWWL operates in the space of graph representations and must be compared to existing graph kernel methods
  - Quick check question: What are the key differences between graph kernels and graph neural networks for graph classification tasks?

## Architecture Onboarding

- Component map:
  Graph preprocessing -> Continuous WL iterations (H steps) -> Sliced Wasserstein distance computation with P projections and Q quantiles -> Kernel assembly with exponential function -> GP regression with Matérn kernel

- Critical path:
  1. Compute continuous WL embeddings for all graphs
  2. Project embeddings onto P random directions
  3. Compute Q quantiles for each projection
  4. Assemble Gram matrix using exponential of squared distances
  5. Train GP with marginal likelihood optimization

- Design tradeoffs:
  - P vs accuracy: More projections improve accuracy but increase computation time
  - Q vs accuracy: More quantiles improve approximation quality but increase memory usage
  - H vs oversmoothing: More WL iterations capture larger neighborhoods but may lose discriminative power

- Failure signatures:
  - Memory issues: Gram matrix becomes too large (O(N²PQ) space)
  - Slow training: GP hyperparameter optimization struggles with large Gram matrix
  - Poor accuracy: Insufficient P or Q values lead to loss of discriminative power

- First 3 experiments:
  1. Validate positive definiteness: Compute eigenvalues of Gram matrix for a small dataset
  2. Benchmark complexity: Time WL iterations vs sliced Wasserstein computation on synthetic graphs
  3. Sensitivity analysis: Vary P and Q to find minimal working configuration for a validation dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of projections P and quantiles Q for the SWWL kernel across different graph datasets?
- Basis in paper: [explicit] The paper notes that "above 20 projections and 100 quantiles the RMSE score decreases very little" but acknowledges that "the impact of the number of projections and the number of quantiles on the SWWL kernel" requires further investigation.
- Why unresolved: The authors only tested specific values (P=20, Q=100) on their datasets and note that while performance plateaus after these values, the optimal settings may vary depending on dataset characteristics like graph size, attribute dimensionality, and noise levels.
- What evidence would resolve it: Systematic experiments varying P and Q across diverse graph datasets with different characteristics (size, density, attribute types) would reveal generalizable optimal ranges and identify when additional projections/quantiles provide diminishing returns.

### Open Question 2
- Question: How does the SWWL kernel perform on graph classification tasks with large graphs (thousands of nodes) compared to specialized large-graph methods?
- Basis in paper: [inferred] The paper demonstrates SWWL's effectiveness on regression tasks with large meshes (tens of thousands of nodes) but only validates classification on small molecular graphs (<100 nodes), noting that "most [graph kernels] are intractable with large and sparse graphs."
- Why unresolved: The paper focuses on regression for large graphs but doesn't benchmark SWWL against specialized large-graph classification methods like GNNs or hierarchical graph kernels that might handle large graphs more efficiently.
- What evidence would resolve it: Direct comparison of SWWL with state-of-the-art large-graph classification methods on benchmark datasets with graphs containing thousands of nodes would reveal whether SWWL's positive definiteness advantage outweighs potential efficiency disadvantages.

### Open Question 3
- Question: Can the SWWL kernel be extended to handle vector-valued outputs for physical fields in computational physics?
- Basis in paper: [explicit] The authors state that "our kernel can only handle scalar outputs, and future work may include an extension to vector fields, since in computational physics the outputs obtained by simulation are often of this form."
- Why unresolved: While the paper successfully handles scalar regression tasks, many computational physics problems require predicting vector fields (velocity, stress tensors, etc.), and the current kernel formulation doesn't naturally extend to multivariate outputs.
- What evidence would resolve it: Developing and validating a multivariate extension of SWWL that maintains positive definiteness while capturing correlations between output components would demonstrate whether the kernel framework can be generalized beyond scalar regression.

## Limitations

- Positive definiteness validation is limited to theoretical guarantees rather than extensive empirical testing across diverse graph structures
- Computational complexity improvements lack comprehensive benchmarking against other large-scale graph kernel methods
- The optimal configuration of P and Q parameters for different graph domains and attribute distributions remains unclear

## Confidence

- Positive definiteness guarantee: Medium
- Complexity reduction: Medium
- Dimension reduction effectiveness: Medium

## Next Checks

1. **Positive Definiteness Test**: Compute eigenvalues of Gram matrices across multiple datasets to empirically verify positive definiteness, especially for edge cases like graphs with similar structures or near-zero distances.

2. **Memory Complexity Analysis**: Benchmark memory usage and runtime for datasets with varying sizes (N) and graph node counts to identify the practical limits of SWWL for large-scale applications.

3. **Parameter Sensitivity Study**: Systematically vary P and Q across multiple graph datasets to determine the minimum configuration needed for maintaining accuracy, particularly for graphs with different attribute distributions and structural complexities.