---
ver: rpa2
title: 'Cross-Care: Assessing the Healthcare Implications of Pre-training Data on
  Language Model Bias'
arxiv_id: '2405.05506'
source_url: https://arxiv.org/abs/2405.05506
tags:
- disease
- demographic
- across
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Cross-Care, a benchmark framework for assessing
  biases and real-world knowledge in large language models (LLMs) regarding disease
  prevalence across demographic groups. The research systematically evaluates how
  demographic biases in pre-training corpora like ThePile influence LLM outputs by
  comparing model-derived disease-demographic associations against actual epidemiological
  data.
---

# Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias

## Quick Facts
- arXiv ID: 2405.05506
- Source URL: https://arxiv.org/abs/2405.05506
- Reference count: 40
- Primary result: Cross-Care benchmark reveals LLMs systematically misrepresent disease prevalence across demographic groups compared to real epidemiological data

## Executive Summary
Cross-Care introduces a novel benchmark framework for evaluating how pre-training data influences demographic bias in large language models' representation of disease prevalence. The study systematically compares LLM outputs against actual epidemiological data across demographic groups, revealing substantial misalignment between model-derived associations and real-world health statistics. By analyzing models trained on datasets like ThePile, researchers demonstrate that current LLMs lack proper grounding in medical reality when making demographic-specific health claims. The framework also provides tools for visualizing and exploring these biases, with a public web app available at www.crosscare.net for further analysis.

## Method Summary
The Cross-Care framework evaluates LLM bias by comparing model-generated disease-demographic associations against epidemiological prevalence data. Researchers tested multiple LLMs trained on different corpora, focusing on how well model outputs aligned with actual disease rates across various demographic groups. The methodology involved prompting models to generate associations between diseases and demographic characteristics, then measuring statistical correlation between these associations and real-world prevalence data. The study also examined the impact of alignment techniques on reducing bias, finding minimal improvement in cross-linguistic consistency of disease prevalence representation.

## Key Results
- LLMs trained on ThePile and similar corpora show significant misalignment between model-derived disease-demographic associations and actual epidemiological prevalence data
- Different alignment methods minimally resolve inconsistencies in models' representation of disease prevalence across demographic groups and languages
- The framework reveals systematic biases where models over- or under-represent certain diseases for specific demographic groups compared to real-world statistics
- Cross-Care provides publicly available visualization tools at www.crosscare.net for exploring model bias patterns

## Why This Works (Mechanism)
The framework works by creating measurable discrepancies between LLM outputs and ground truth epidemiological data. When models are prompted to generate disease-demographic associations, their outputs reflect the biases and gaps present in their training corpora rather than actual disease prevalence patterns. This systematic comparison reveals how pre-training data composition directly influences the model's internal representations of health disparities, making implicit biases explicit and quantifiable.

## Foundational Learning

**Epidemiological prevalence data** - Real-world statistics about disease occurrence across demographic groups
*Why needed:* Provides ground truth for validating model outputs against actual health patterns
*Quick check:* Are the prevalence datasets comprehensive and representative of the populations being analyzed?

**Demographic bias measurement** - Statistical methods for quantifying disparities in model outputs across groups
*Why needed:* Enables systematic comparison of model behavior across different demographic categories
*Quick check:* Do measurement methods account for intersectional identities and complex demographic interactions?

**Cross-linguistic consistency evaluation** - Assessment of how model biases translate across different languages
*Why needed:* Ensures findings are not language-specific and have broader applicability
*Quick check:* Are multiple languages with different linguistic structures included in the evaluation?

## Architecture Onboarding

**Component Map:** Training corpus → LLM → Disease-demographic prompts → Model outputs → Comparison with epidemiological data → Bias quantification → Visualization tools

**Critical Path:** The core validation workflow moves from prompt generation through model inference to statistical comparison with ground truth data, with bias quantification serving as the key decision point for evaluating model performance.

**Design Tradeoffs:** The framework prioritizes breadth of demographic coverage over depth of individual disease analysis, enabling systematic bias detection across multiple conditions but potentially missing nuanced disease-specific patterns. This tradeoff enables scalable assessment but may overlook rare conditions or complex comorbidities.

**Failure Signatures:** Models consistently over-represent certain diseases for specific demographic groups while under-representing others, creating systematic patterns that deviate from epidemiological reality. These failures often manifest as statistically significant correlations between model outputs and training corpus composition rather than actual disease prevalence.

**3 First Experiments:**
1. Test model responses to disease-demographic prompts across all major demographic categories in the benchmark
2. Compare model outputs against epidemiological data to quantify bias magnitude and direction
3. Evaluate cross-linguistic consistency by running the same prompts in multiple languages and comparing bias patterns

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Relies on proxy measures for disease prevalence rather than direct clinical outcomes, potentially missing important confounding factors
- Constrained to specific demographic categories and diseases included in ThePile corpus, limiting scope of bias detection
- Correlation-based analysis cannot definitively establish causal relationships between training data and model outputs

## Confidence

**High confidence:** LLMs systematically misrepresent disease-demographic associations compared to real prevalence data - well-supported by quantitative analysis across multiple models and groups

**Medium confidence:** Pre-training data composition directly causes these biases - requires additional causal evidence beyond correlational analysis

**Medium confidence:** Alignment methods minimally resolve inconsistencies - based on limited evaluation of alignment techniques that may not generalize to newer methods

## Next Checks

1. Conduct interventional studies where pre-training corpora are systematically modified to test causal relationships between training data composition and model bias outputs

2. Validate findings across additional disease categories and demographic intersections not covered in the current benchmark to assess generalizability

3. Implement longitudinal tracking to evaluate whether model bias patterns shift as pre-training corpora evolve over time, particularly with increasing medical content