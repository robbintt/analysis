---
ver: rpa2
title: A Complete Characterization of Learnability for Stochastic Noisy Bandits
arxiv_id: '2410.09597'
source_url: https://arxiv.org/abs/2410.09597
tags:
- learnability
- class
- bandit
- function
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper gives a complete characterization of learnability for\
  \ stochastic noisy bandits with arbitrary noise. The main result is that a function\
  \ class F is learnable if and only if its generalized maximin volume \u03B3F,\u03B1\
  \ 0 for all \u03B1 \u2208 (0,1)."
---

# A Complete Characterization of Learnability for Stochastic Noisy Bandits

## Quick Facts
- **arXiv ID**: 2410.09597
- **Source URL**: https://arxiv.org/abs/2410.09597
- **Reference count**: 10
- **Primary result**: A function class F is learnable with arbitrary noise if and only if its generalized maximin volume γF,α > 0 for all α ∈ (0,1).

## Executive Summary
This paper resolves a fundamental question in stochastic noisy bandit learning by providing a complete characterization of learnability. The key insight is that the generalized maximin volume γF,α serves as both a necessary and sufficient condition for whether a function class can be learned under arbitrary noise. When γF,α > 0 for all α ∈ (0,1), there exists an algorithm that can identify near-optimal arms with probability at least 1−δ using a number of queries bounded by O(1/(γF,α/2·α²)·log(1/δ)·log(log(1/δ))). The paper also establishes that adaptivity is sometimes necessary to achieve optimal query complexity, and extends the characterization to unbounded and Gaussian noise settings.

## Method Summary
The paper characterizes learnability through the generalized maximin volume γF,α, which measures the probability of finding near-optimal arms under the best possible distribution over arms. The main algorithm samples arms from this optimal distribution and uses empirical means to identify near-optimal arms. The analysis relies on concentration inequalities to bound the query complexity, with upper bounds showing O(1/(γF,α/2·α²)·log(1/δ)·log(log(1/δ))) queries suffice, and lower bounds proving Ω(log(1/γF,α)) queries are necessary. The paper also introduces a variant of the Decision-Estimation-Coefficient framework that characterizes learnability in this setting.

## Key Results
- A function class F is learnable with arbitrary noise if and only if γF,α > 0 for all α ∈ (0,1)
- The query complexity is QC(F,α,δ) = O(1/(γF,α/2·α²)·log(1/δ)·log(log(1/δ)))
- Adaptivity is sometimes necessary to achieve optimal query complexity of O(log(1/γF,α)) versus Ω(1/γF,α) for non-adaptive algorithms
- The characterization extends to unbounded noise using median-of-means and to Gaussian noise

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The generalized maximin volume γF,α > 0 ∀α ∈ (0,1) is both necessary and sufficient for learnability.
- **Mechanism**: When γF,α > 0, there exists a distribution p over arms such that for all functions f ∈ F, with non-zero probability we can sample an arm π satisfying supπ∗ f(π∗) − f(π) ≤ α.
- **Core assumption**: The function class F has a positive probability of containing near-optimal arms under some distribution p.
- **Evidence anchors**: Theorem 1 proves the if-and-only-if characterization; the abstract states this resolves the open question from Hanneke and Yang (2023).
- **Break condition**: If γF,α = 0 for some α, no algorithm can succeed since the probability of finding an α-optimal arm is zero.

### Mechanism 2
- **Claim**: The query complexity QC(F,α,δ) = O(1/(γF,α/2·α²)·log(1/δ)·log(log(1/δ))) provides an upper bound on rounds needed.
- **Mechanism**: By sampling m = 1/γF,α/2 · log(2/δ) arms from the optimal distribution p and querying each arm sufficiently many times, we can identify an arm with supπ∗ f(π∗) − f(ˆπ) ≤ α with probability at least 1−δ.
- **Core assumption**: Empirical means converge to true means with sufficient queries, and Hoeffding-style concentration bounds hold.
- **Evidence anchors**: Theorem 2 provides the upper bound; the abstract confirms this establishes the first complete characterization.
- **Break condition**: If noise has heavy tails or violates independence assumptions, concentration bounds may fail.

### Mechanism 3
- **Claim**: Adaptivity is sometimes necessary to achieve optimal query complexity.
- **Mechanism**: For certain function classes, non-adaptive algorithms require Ω(1/γF,α) queries while adaptive algorithms can achieve O(log(1/γF,α)) queries by leveraging information from previous queries.
- **Core assumption**: The function class has a tree-like structure where adaptive querying can efficiently narrow down the optimal arm.
- **Evidence anchors**: Theorem 8 shows adaptivity can provide query complexity separation; the paper states adaptivity is sometimes necessary.
- **Break condition**: If the function class doesn't have exploitable structure, or if noise corrupts information from previous queries, adaptivity may not help.

## Foundational Learning

- **Concept: Concentration inequalities**
  - Why needed here: Proofs rely on concentration bounds (Hoeffding's inequality, median-of-means) to show empirical estimates converge to true values with high probability.
  - Quick check question: Given n independent random variables X₁,...,Xₙ bounded in [0,1], what is the probability that their sample mean deviates from the true mean by more than t?

- **Concept: Decision-Estimation Coefficient (DEC)**
  - Why needed here: The paper proposes a DEC variant that characterizes learnability in the stochastic noisy bandit setting, connecting to broader interactive decision making literature.
  - Quick check question: How does the DEC framework measure the trade-off between decision quality and estimation accuracy in interactive learning problems?

- **Concept: PAC learning and function classes**
  - Why needed here: The paper builds on PAC learning framework by studying learnability in bandit setting where goal is to identify near-optimal arms rather than learn a hypothesis.
  - Quick check question: What is the relationship between VC dimension of a concept class and its learnability in the PAC framework?

## Architecture Onboarding

- **Component map**: Function class F -> Model class MF -> Generalized maximin volume γF,α -> Learning algorithm -> Query complexity bounds

- **Critical path**:
  1. Compute or estimate γF,α for the given function class F
  2. If γF,α > 0 for all α ∈ (0,1), the class is learnable
  3. Use the learning algorithm with appropriate parameters based on γF,α, α, and δ
  4. The algorithm samples arms from the optimal distribution and queries them sufficiently
  5. Return the arm with the highest empirical mean as the near-optimal arm

- **Design tradeoffs**:
  - Adaptive vs non-adaptive algorithms: Adaptive algorithms can sometimes achieve better query complexity but are more complex to implement
  - Direct mean estimation vs median-of-means: Direct estimation works for bounded rewards, while median-of-means handles unbounded noise better
  - Exploration vs exploitation: The learning algorithm must balance exploring different arms with exploiting promising ones

- **Failure signatures**:
  - If γF,α = 0 for some α, the function class is not learnable and no algorithm can succeed
  - If the noise violates independence or boundedness assumptions, concentration bounds may fail
  - If the function class has complex structure not captured by γF,α, the characterization may not be tight

- **First 3 experiments**:
  1. Verify the characterization on simple function classes like K-armed bandits (where γF,α = 1/|Π|) and singletons (where γF,α = 0)
  2. Implement the learning algorithm for a linear bandit setting and test its performance on synthetic data
  3. Compare the query complexity of adaptive vs non-adaptive algorithms on a function class with a tree structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the generalized maximin volume characterization be extended to contextual bandits where the function class F depends on context information?
- Basis in paper: The paper establishes learnability characterization for standard stochastic bandits but mentions contextual bandits as a potential future direction.
- Why unresolved: The paper only addresses non-contextual settings where arms have fixed reward distributions, leaving the contextual case unexplored.
- What evidence would resolve it: A proof showing either the generalized maximin volume approach can be adapted to contextual settings or that a fundamentally different characterization is needed.

### Open Question 2
- Question: Is adaptivity always necessary for achieving optimal query complexity, or are there specific structural properties of function classes that make non-adaptive algorithms optimal?
- Basis in paper: The paper proves adaptivity is sometimes necessary but doesn't establish whether it's always required.
- Why unresolved: While the paper shows adaptivity can improve query complexity for certain function classes, it doesn't determine if there exist classes where non-adaptive algorithms are optimal.
- What evidence would resolve it: A complete characterization of function classes where non-adaptive algorithms achieve optimal query complexity.

### Open Question 3
- Question: How does the Decision-Estimation-Coefficient variant behave for function classes with infinite arms versus finite arms, and what structural differences emerge?
- Basis in paper: The paper introduces a DEC variant but focuses on theoretical properties rather than exploring behavior across different arm space cardinalities.
- Why unresolved: The paper establishes theoretical bounds but doesn't investigate practical differences in DEC behavior between finite and infinite arm settings.
- What evidence would resolve it: Comparative analysis showing how DEC values scale with the number of arms and identifying thresholds where behavior changes fundamentally.

## Limitations

- The characterization relies on the assumption that the function class F has a well-defined generalized maximin volume, which may be computationally intractable for complex function classes.
- The paper doesn't address how to efficiently compute or approximate γF,α for practical function classes beyond simple examples.
- The characterization focuses on arbitrary noise models and may not capture more structured noise scenarios where better bounds could be achieved.

## Confidence

- **High confidence** in the sufficiency of γF,α > 0 for learnability (well-established concentration bounds and algorithmic construction)
- **Medium confidence** in the necessity of γF,α > 0 (relies on information-theoretic lower bounds that may not capture all adversarial scenarios)
- **Medium confidence** in the adaptivity claims (the paper provides theoretical separation but doesn't extensively test on diverse function classes)

## Next Checks

1. Implement and test the generalized maximin volume computation for linear bandit function classes to verify the characterization extends beyond theoretical examples
2. Conduct empirical studies comparing adaptive vs non-adaptive algorithms on function classes with varying tree depths to quantify the adaptivity advantage
3. Test the algorithm's robustness to different noise distributions (Gaussian, heavy-tailed) to validate the extension beyond bounded noise assumptions