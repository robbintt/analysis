---
ver: rpa2
title: 'MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on New
  and Tail Knowledge'
arxiv_id: '2412.17032'
source_url: https://arxiv.org/abs/2412.17032
tags:
- question
- sub-question
- type
- sub-answer
- main
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MINTQA is a multi-hop question answering benchmark designed to
  evaluate large language models on new and tail knowledge. The benchmark addresses
  the challenge of complex, knowledge-intensive queries that require multi-step reasoning,
  particularly involving rare or recently emerged information.
---

# MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on New and Tail Knowledge

## Quick Facts
- arXiv ID: 2412.17032
- Source URL: https://arxiv.org/abs/2412.17032
- Authors: Jie He; Nan Hu; Wanqiu Long; Jiaoyan Chen; Jeff Z. Pan
- Reference count: 40
- 22 state-of-the-art LLMs tested with best model achieving only 62.33% accuracy on multi-hop questions

## Executive Summary
MINTQA is a multi-hop question answering benchmark designed to evaluate large language models on new and tail knowledge. The benchmark addresses the challenge of complex, knowledge-intensive queries that require multi-step reasoning, particularly involving rare or recently emerged information. It includes two datasets: MINTQA-POP (17,887 examples) for unpopular/popular knowledge and MINTQA-TI (10,479 examples) for new/old knowledge, each with corresponding sub-questions and answers.

The core method involves constructing fact chains from Wikidata, generating multi-hop questions via GPT-4o, and evaluating models across five dimensions: parametric knowledge use, question handling strategies, retrieval-augmented generation, sub-question generation, and iterative decomposition with retrieval. A systematic evaluation of 22 state-of-the-art LLMs reveals significant limitations, with the best model (LLaMA 3.1-70B) achieving only 62.33% accuracy overall. Performance consistently declines with increased reasoning hops, and retrieval effectiveness diminishes for complex queries. The findings highlight that even advanced models struggle with multi-hop reasoning on new or unpopular knowledge, emphasizing the need for improved reasoning capabilities in LLMs.

## Method Summary
MINTQA constructs multi-hop question answering benchmarks by building fact chains from Wikidata triples, generating questions using GPT-4o, and creating sub-questions with corresponding answers. The benchmark evaluates models across five dimensions: parametric knowledge use (no external tools), question handling strategies (with sub-questions), retrieval-augmented generation, sub-question generation, and iterative decomposition with retrieval. The methodology involves extracting entities from Wikidata, building multi-hop fact chains with labeled relationships, generating diverse question types, and evaluating model performance on both unpopular/popular and new/old knowledge distinctions.

## Key Results
- LLaMA 3.1-70B achieved the highest accuracy at 62.33% across all benchmark tasks
- Performance consistently declined with increased reasoning hops, demonstrating the difficulty of multi-step reasoning
- Retrieval-augmented generation showed reduced effectiveness for complex queries requiring multiple reasoning steps
- Models struggled particularly with new and unpopular knowledge, with accuracy gaps widening compared to popular and old knowledge

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its systematic construction methodology that creates controlled test conditions for multi-hop reasoning. By building fact chains from structured Wikidata triples, the benchmark ensures consistent knowledge grounding across questions. The multi-dimensional evaluation approach isolates specific capabilities like parametric knowledge retrieval versus external tool usage, revealing where models fail in the reasoning pipeline. The focus on new and tail knowledge specifically challenges models' ability to handle recently emerged or rarely accessed information, which represents a significant gap in current LLM capabilities.

## Foundational Learning

**Multi-hop reasoning**: Why needed - Tests models' ability to combine multiple facts to answer complex questions; Quick check - Verify model can correctly answer questions requiring at least 3 reasoning steps

**Knowledge retrieval**: Why needed - Assesses models' ability to access relevant information from parametric memory or external sources; Quick check - Test model performance with and without retrieval augmentation on the same question set

**Question decomposition**: Why needed - Evaluates whether models can break down complex questions into simpler sub-questions; Quick check - Compare performance on direct multi-hop questions versus decomposed sub-question approaches

**Fact chain construction**: Why needed - Ensures consistent knowledge grounding and controlled difficulty scaling; Quick check - Validate that generated questions follow the intended reasoning paths

## Architecture Onboarding

**Component map**: Question Generation (GPT-4o) -> Fact Chain Construction (Wikidata triples) -> Model Evaluation (22 LLMs) -> Performance Analysis (5 dimensions)

**Critical path**: Fact chain creation → question generation → model inference → answer validation → accuracy measurement

**Design tradeoffs**: Automated question generation enables scalability but may introduce generation biases; Wikidata provides structured knowledge but limits domain coverage; five-dimensional evaluation provides comprehensive assessment but increases complexity

**Failure signatures**: Accuracy degradation with hop count increase, retrieval ineffectiveness for complex queries, and larger performance gaps between new/unpopular versus old/popular knowledge categories

**First experiments**: 
1. Compare model performance on single-hop versus multi-hop questions within the same knowledge domain
2. Evaluate retrieval-augmented generation performance across different hop counts
3. Test sub-question generation capabilities against direct question answering approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark construction relies heavily on GPT-4o for question generation, potentially introducing generation biases
- Evaluation focuses on English-language knowledge from Wikidata, limiting generalizability to other languages or knowledge sources
- Performance metrics based solely on accuracy without deeper analysis of failure modes or error patterns
- Fixed set of 22 LLMs tested may not represent the full landscape of available models or capture emergent capabilities

## Confidence
- High confidence in benchmark construction methodology and empirical finding that state-of-the-art models struggle with multi-hop reasoning on new/unpopular knowledge
- Medium confidence in comparative analysis across different model families and sizes given limited number of models tested
- Medium confidence in generalizability of findings to other knowledge domains beyond Wikidata-based construction

## Next Checks
1. Conduct ablation studies removing GPT-4o from question generation pipeline to assess impact of automated construction on benchmark quality
2. Expand evaluation to include models released after study period and alternative retrieval strategies to validate relative performance rankings
3. Perform detailed error analysis categorizing failure modes by reasoning type, knowledge domain, and question complexity to identify specific bottlenecks in multi-hop reasoning capabilities