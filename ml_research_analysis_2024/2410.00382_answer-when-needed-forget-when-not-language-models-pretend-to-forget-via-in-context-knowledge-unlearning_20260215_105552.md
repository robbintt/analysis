---
ver: rpa2
title: 'Answer When Needed, Forget When Not: Language Models Pretend to Forget via
  In-Context Knowledge Unlearning'
arxiv_id: '2410.00382'
source_url: https://arxiv.org/abs/2410.00382
tags:
- unlearning
- knowledge
- forget
- information
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selectively forgetting specific
  information in large language models (LLMs) for privacy and ethical reasons. The
  authors propose "in-context knowledge unlearning," a method that enables LLMs to
  selectively forget information based on query context by introducing unlearning
  tokens and fine-tuning with a custom loss function.
---

# Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning

## Quick Facts
- arXiv ID: 2410.00382
- Source URL: https://arxiv.org/abs/2410.00382
- Reference count: 3
- Models learn to selectively forget information based on query context while maintaining general knowledge

## Executive Summary
This paper introduces "in-context knowledge unlearning," a method that enables large language models to selectively forget specific information based on query context. The approach uses special unlearning tokens and fine-tuning with a custom loss function to achieve up to 95% forget accuracy while retaining 80% of unrelated knowledge. The key innovation is that LLMs "pretend to forget" by making the decision to forget only at the final layer, rather than actually erasing knowledge. Experiments on TOFU and AGE datasets using Llama2 and Mistral models demonstrate significant improvements over baseline approaches.

## Method Summary
The method introduces unlearning tokens («UNL» and «/UNL») that instruct the model to disregard enclosed information during processing. The model is fine-tuned with a custom loss function combining forgetting loss (Lforget) and retention loss (Lretain). Lforget encourages the model to output "forgot" when the query contains targeted information, while Lretain maintains normal response capabilities for unrelated queries. The approach is evaluated using LoRA fine-tuning on transformer-based models like Llama2 and Mistral, with performance measured on forget and retain metrics across in-domain and out-of-domain scenarios.

## Key Results
- Achieves up to 95% forget accuracy while retaining 80% of unrelated knowledge
- Outperforms baseline approaches in both forget and retain metrics
- Demonstrates that LLMs make the decision to forget only at the final layer, showing "pretend to forget" behavior

## Why This Works (Mechanism)

### Mechanism 1
The unlearning tokens («UNL» and «/UNL») enable selective forgetting by instructing the model to disregard enclosed information during processing. The model is fine-tuned to recognize these special tokens and modify its output distribution to "forget" the enclosed content when these tokens are present.

### Mechanism 2
The loss function (Lforget + Lretain) enables the model to balance selective forgetting with knowledge retention. Lforget encourages the model to output "forgot" when the query contains targeted information, while Lretain maintains normal response capabilities for unrelated queries.

### Mechanism 3
LLMs "pretend to forget" by making the decision to forget only at the final layer of the model. Internal analysis using logit lens shows that correct predictions are generated in middle layers and preserved up to the final layer, but the decision to output "forgot" or the answer token is made only in the last layer.

## Foundational Learning

- **In-context learning (ICL)**: ICL allows the model to adapt to new tasks flexibly by incorporating data into the context of input sequence, rather than requiring explicit weight updates through fine-tuning. Quick check: What is the key difference between in-context learning and traditional fine-tuning in terms of how the model adapts to new tasks?

- **Loss function design for selective knowledge handling**: The custom loss function (Lforget + Lretain) is crucial for training the model to balance selective forgetting with knowledge retention. Quick check: How does the combination of forgetting loss and retention loss enable the model to selectively forget targeted information while maintaining other knowledge?

- **Logit lens analysis**: Logit lens analysis is used to investigate the model's internal behavior and understand how the decision to forget is made at different layers. Quick check: What does the logit lens analysis reveal about when and how the model makes the decision to forget or retain information?

## Architecture Onboarding

- **Component map**: Input query → Token recognition → Internal processing (middle layers) → Final decision layer → Output ("forgot" or answer)
- **Critical path**: The model recognizes unlearning tokens, processes information through middle layers, and makes the final forget/retain decision at the last layer before generating output
- **Design tradeoffs**: Balancing selective forgetting with knowledge retention, choosing between different fine-tuning methods (LoRA, full fine-tuning, last-layer tuning), and ensuring the unlearning process doesn't significantly impact general language understanding
- **Failure signatures**: Inability to forget targeted information, excessive forgetting of unrelated knowledge, failure to recognize unlearning tokens, or significant degradation in performance on standard NLP tasks
- **First 3 experiments**:
  1. Implement unlearning tokens and test basic recognition by the model without fine-tuning
  2. Apply LoRA fine-tuning with the custom loss function and evaluate forget and retain scores on in-domain data
  3. Analyze internal behavior using logit lens to verify the "pretend to forget" mechanism and compare performance across different tuning methods

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the analysis:

1. How does the "pretend to forget" behavior in the final layer impact the model's ability to generalize to unseen tasks or domains?
2. Can the in-context knowledge unlearning method be effectively applied to models with different architectures, such as those using attention mechanisms other than transformers?
3. What are the potential ethical implications of using in-context knowledge unlearning in real-world applications, particularly in sensitive domains like healthcare or law?

## Limitations
- The paper lacks detailed specification of how unlearning tokens are precisely integrated into the model's architecture during fine-tuning
- Dataset preprocessing details for TOFU and AGE datasets are not explicitly provided
- Limited exploration of hyperparameter sensitivity and guidance for different model sizes

## Confidence

**High Confidence Claims**:
- The "pretend to forget" mechanism where decision-making occurs at the final layer is well-supported by logit lens analysis
- The overall effectiveness of in-context knowledge unlearning in achieving high forget accuracy while maintaining knowledge retention

**Medium Confidence Claims**:
- The superiority of the proposed method over baseline approaches
- The generalizability of results across different model sizes and datasets

**Low Confidence Claims**:
- The long-term stability of the unlearning effect
- Performance on completely unseen domains beyond the evaluated datasets

## Next Checks
1. Conduct detailed logit lens analysis across all layers for both in-domain and out-of-domain queries to verify that the "pretend to forget" behavior consistently occurs at the final layer
2. Implement and evaluate the specific baseline methods mentioned using the exact same datasets and evaluation metrics to independently verify the claimed performance improvements
3. Evaluate the model's forgetting performance on a third, completely different dataset not seen during training or fine-tuning to assess the robustness and generalizability of the unlearning mechanism