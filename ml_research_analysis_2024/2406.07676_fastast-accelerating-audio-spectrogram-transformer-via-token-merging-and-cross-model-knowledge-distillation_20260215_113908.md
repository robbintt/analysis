---
ver: rpa2
title: 'FastAST: Accelerating Audio Spectrogram Transformer via Token Merging and
  Cross-Model Knowledge Distillation'
arxiv_id: '2406.07676'
source_url: https://arxiv.org/abs/2406.07676
tags:
- fastast
- accuracy
- audio
- tome
- cmkd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FastAST accelerates the Audio Spectrogram Transformer (AST) for
  audio classification by integrating Token Merging (ToMe) and Cross-Model Knowledge
  Distillation (CMKD). The method reduces computational overhead by merging similar
  tokens in audio spectrograms and improves accuracy by distilling knowledge from
  a more accurate reference model.
---

# FastAST: Accelerating Audio Spectrogram Transformer via Token Merging and Cross-Model Knowledge Distillation

## Quick Facts
- **arXiv ID**: 2406.07676
- **Source URL**: https://arxiv.org/abs/2406.07676
- **Reference count**: 0
- **Primary result**: FastAST accelerates AST inference by up to 150 samples per second with minimal accuracy loss through token merging and knowledge distillation

## Executive Summary
FastAST is a novel framework designed to accelerate Audio Spectrogram Transformers (AST) for audio classification tasks. The method combines Token Merging (ToMe) and Cross-Model Knowledge Distillation (CMKD) to achieve significant inference speed improvements while maintaining or even surpassing baseline accuracy. By strategically merging similar tokens in audio spectrograms and leveraging knowledge from more accurate teacher models, FastAST addresses the computational challenges of AST models without requiring extensive retraining.

## Method Summary
FastAST integrates Token Merging (ToMe) into AST architecture by inserting ToMe modules between attention and MLP branches in each transformer block. This reduces token count by merging similar tokens based on feature similarity. Additionally, FastAST employs Cross-Model Knowledge Distillation (CMKD) where a teacher model (CNN or transformer) transfers knowledge to the student FastAST model using a combination of standard classification loss and distillation loss. The method operates on 128 × 100t spectrograms divided into 16 × 16 patches with overlap, reducing computational overhead while maintaining classification performance.

## Key Results
- FastAST achieves up to 150 samples per second inference speed on ESC-50 dataset
- With CMKD, FastAST surpasses baseline AST accuracy while maintaining faster inference speeds
- On Balanced Audioset, FastAST shows significant performance improvements over baseline AST models

## Why This Works (Mechanism)

### Mechanism 1: Token Merging (ToMe)
- Claim: ToMe reduces computational complexity without retraining by removing redundant tokens
- Core assumption: Similar tokens carry redundant information that can be merged without accuracy loss
- Evidence: Abstract states FastAST "enhances inference speed without requiring extensive retraining by merging similar tokens"
- Break condition: If tokens aren't redundant or similarity metrics fail

### Mechanism 2: Cross-Model Knowledge Distillation (CMKD)
- Claim: CMKD improves FastAST accuracy by transferring knowledge from more accurate models
- Core assumption: Different architectures capture complementary features in audio spectrograms
- Evidence: Abstract mentions CMKD mitigates accuracy impact through integration
- Break condition: If teacher architecture is too dissimilar or accuracy insufficient

### Mechanism 3: Strategic ToMe Placement
- Claim: ToMe between attention and MLP enables information propagation from merged tokens
- Core assumption: Attention mechanisms can effectively propagate information from tokens destined for merging
- Evidence: Section describes strategic placement between attention and MLP branches
- Break condition: If attention cannot propagate information or merging disrupts features

## Foundational Learning

- **Audio spectrogram representation and tokenization**: FastAST operates on 128 × 100t spectrograms divided into 16 × 16 patches with overlap. Understanding this preprocessing is essential for grasping how tokens represent audio data. *Quick check*: What is the effective input sequence length formula for a t-second audio input in FastAST?

- **Knowledge distillation fundamentals**: CMKD relies on transferring knowledge from teacher to student models using specific loss functions and temperature scaling. *Quick check*: What is the role of the temperature parameter τ in the distillation loss function?

- **Token similarity metrics and merging strategies**: ToMe's effectiveness depends on correctly identifying similar tokens and merging them appropriately. *Quick check*: How does the edge selection process in ToMe determine which tokens to merge?

## Architecture Onboarding

- **Component map**: Input spectrogram → patch embedding + positional embedding → [CLS] token addition → transformer blocks (with ToMe) → [CLS] output → linear classification head → final prediction
- **Critical path**: Input spectrogram → patch embedding + positional embedding → [CLS] token addition → transformer blocks (with ToMe) → [CLS] output → linear classification head → final prediction
- **Design tradeoffs**: ToMe reduction factor r trades accuracy for speed (higher r = faster but less accurate); CMKD trades training complexity for accuracy (adds teacher training and distillation computation)
- **Failure signatures**: Accuracy degradation without throughput improvement suggests ToMe is merging non-redundant tokens; accuracy improvement without throughput improvement suggests CMKD is not properly configured
- **First 3 experiments**:
  1. Baseline AST performance measurement on ESC-50 to establish reference accuracy and throughput
  2. ToMe only implementation with r=5 to verify basic speed improvement without distillation
  3. Full FastAST with ToMe+CMKD using EfficientNet-B2 teacher on Balanced Audioset to validate combined approach effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FastAST's performance vary with different hardware platforms, such as CPUs versus GPUs, and what are the specific bottlenecks on each?
- Basis in paper: The paper mentions hardware constraints but does not provide detailed analysis of FastAST's performance on different hardware platforms.
- Why unresolved: The paper focuses on general performance improvements without delving into hardware-specific bottlenecks.
- What evidence would resolve it: Detailed benchmarking results of FastAST on various hardware platforms, including CPUs, GPUs, and potentially specialized accelerators.

### Open Question 2
- Question: Can the integration of ToMe and CMKD in FastAST be extended to other types of transformers beyond AST, such as those used in natural language processing or computer vision?
- Basis in paper: The paper introduces FastAST as a framework for AST but does not explore its applicability to other transformer architectures.
- Why unresolved: The paper's focus is on audio classification tasks, leaving potential broader applications unexplored.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of FastAST's ToMe and CMKD integration on other transformer models in different domains.

### Open Question 3
- Question: What are the long-term effects of using token merging (ToMe) on the robustness and generalization capabilities of AST models across diverse and unseen datasets?
- Basis in paper: The paper shows that ToMe improves inference speed with minimal accuracy loss on specific datasets but does not address long-term effects on model robustness.
- Why unresolved: The study is limited to specific datasets and does not investigate the impact of ToMe on the model's ability to generalize to new, unseen data.
- What evidence would resolve it: Longitudinal studies and extensive testing on a wide range of diverse datasets.

## Limitations

- Limited empirical evidence that ToMe effectiveness in vision tasks translates directly to audio spectrograms
- Insufficient quantitative evidence of what specific complementary information is transferred through CMKD
- Performance claims based on three specific datasets without broader generalization testing
- Additional computational cost and complexity of training both teacher and student models not fully accounted for

## Confidence

- **High Confidence**: FastAST achieves measurable inference speed improvements over baseline AST through ToMe integration
- **Medium Confidence**: FastAST maintains acceptable accuracy while achieving speed improvements across three datasets
- **Low Confidence**: Claim that CMKD enables FastAST to "surpass AST accuracy while maintaining faster inference speeds" requires stronger empirical support

## Next Checks

1. **Ablation Study on ToMe Parameters**: Systematically vary the reduction factor r from 1 to 15 and measure the accuracy-throughput tradeoff curve on ESC-50

2. **Teacher Architecture Sensitivity Analysis**: Replace the EfficientNet-B2 teacher with different CNN architectures (ResNet, MobileNet) and transformer teachers on Balanced Audioset

3. **Cross-Dataset Transfer Validation**: Train FastAST on ESC-50 and evaluate on Speech Commands V2 (and vice versa) to test dataset transfer effectiveness