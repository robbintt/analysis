---
ver: rpa2
title: Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues
arxiv_id: '2411.12537'
source_url: https://arxiv.org/abs/2411.12537
tags:
- matrices
- sequence
- deltanet
- lrnns
- range
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the fundamental limitations of Linear Recurrent
  Neural Networks (LRNNs) in performing state-tracking tasks. The authors prove that
  current LRNNs, including Mamba and DeltaNet, cannot solve simple tasks like parity
  due to the restriction of their state-transition matrices to positive eigenvalues
  in the range [0, 1].
---

# Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues

## Quick Facts
- **arXiv ID**: 2411.12537
- **Source URL**: https://arxiv.org/abs/2411.12537
- **Reference count**: 40
- **Primary result**: Extending eigenvalue range from [0, 1] to [-1, 1] enables LRNNs to solve parity and improves state-tracking task performance

## Executive Summary
This paper addresses fundamental limitations in Linear Recurrent Neural Networks (LRNNs) by proving that current architectures cannot solve simple state-tracking tasks like parity due to restricting their state-transition matrices to positive eigenvalues. The authors demonstrate that extending the eigenvalue range to [-1, 1] significantly enhances LRNN expressive power, enabling them to learn regular languages and perform complex state-tracking tasks. Their empirical results show that Mamba and DeltaNet models with extended eigenvalues can be pre-trained stably at scale (1.3B parameters) and achieve competitive performance on language modeling tasks while showing particular promise on code and math tasks.

## Method Summary
The core innovation involves modifying the state-transition matrices in Mamba and DeltaNet models to extend their eigenvalue range from [0, 1] to [-1, 1]. This modification allows the matrices to represent a broader class of transformations necessary for state-tracking tasks. The authors prove theoretically that LRNNs with state-transition matrices formed by products of generalized Householder matrices (each with eigenvalues in [-1, 1]) can learn any regular language. They validate this approach through synthetic tasks (parity, modular arithmetic, permutation composition) and language modeling experiments on datasets like FineWeb-100B, CodeParrot, and Math-Hard.

## Key Results
- Mamba and DeltaNet with extended eigenvalue range can solve the parity task that original models cannot
- Performance improvements on state-tracking tasks including modular arithmetic and permutation composition
- Stable pre-training at scale (1.3B parameters) with extended eigenvalues
- Lower perplexity on coding and math datasets compared to original models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extending eigenvalue range from [0, 1] to [-1, 1] enables LRNNs to solve parity and other state-tracking tasks.
- Mechanism: Current LRNNs restrict state-transition matrices to positive eigenvalues, preventing them from implementing the necessary sign changes for parity computation. Allowing negative eigenvalues provides the expressive power to represent the state transitions required for parity.
- Core assumption: The inability to solve parity stems from the eigenvalue range restriction, not from other architectural limitations.
- Evidence anchors:
  - [abstract] "Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to [0, 1] and that incorporating negative values can resolve this issue."
  - [section] "We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while non-triangular matrices are needed to count modulo 3."
- Break condition: If the state-tracking limitations are due to factors other than eigenvalue range (e.g., precision constraints, depth limitations), extending eigenvalues may not fully resolve the issue.

### Mechanism 2
- Claim: Non-diagonal state-transition matrices (generalized Householder) combined with extended eigenvalue range enable LRNNs to implement more complex FSA.
- Mechanism: Generalized Householder matrices allow for richer state mixing compared to diagonal matrices. When combined with the [-1, 1] eigenvalue range, they can represent a broader class of transformations, including those needed for FSA with non-trivial transition monoids.
- Core assumption: The structure of generalized Householder matrices, when combined with the extended eigenvalue range, is sufficient to represent the necessary transformations for complex FSA.
- Evidence anchors:
  - [abstract] "We prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range [-1, 1]."
  - [section] "Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range [-1, 1]."
- Break condition: If the required FSA transformations cannot be decomposed into products of generalized Householder matrices, or if the number of required layers grows prohibitively large, this mechanism may not scale.

### Mechanism 3
- Claim: Extending eigenvalue range to [-1, 1] does not compromise training stability or efficiency of LRNNs.
- Mechanism: The modification only changes the range of allowed eigenvalues without altering the computational structure or introducing additional complexity. The same efficient matrix operations can be used, and the extended range does not introduce numerical instability.
- Core assumption: The [-1, 1] eigenvalue range is numerically stable and does not introduce training difficulties compared to the [0, 1] range.
- Evidence anchors:
  - [abstract] "Our empirical results confirm that extending the eigenvalue range of models like Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks."
  - [section] "We show that the eigenvalue range of Mamba and DeltaNet can be extended to [-1, 1] without compromising efficiency or training stability."
- Break condition: If the [-1, 1] range introduces numerical instability or training difficulties not observed in the experiments, this mechanism may fail in larger-scale or different training regimes.

## Foundational Learning

- Concept: Linear algebra (eigenvalues, eigenvectors, matrix decompositions)
  - Why needed here: Understanding the eigenvalue range restriction and its impact on LRNN expressivity requires knowledge of how eigenvalues determine matrix properties and transformations.
  - Quick check question: What property of a matrix is determined by its eigenvalues, and how does this relate to the transformations it can represent?

- Concept: Formal language theory (regular languages, finite state automata)
  - Why needed here: The paper connects LRNN expressivity to the languages they can recognize, requiring understanding of regular languages and FSA.
  - Quick check question: What is the relationship between regular languages and finite state automata, and how does this relate to the capabilities of LRNNs?

- Concept: Group theory (transition monoids, cyclic and symmetric groups)
  - Why needed here: The paper uses group theory concepts to characterize the state-tracking tasks and the LRNN capabilities, particularly for modular counting and permutation composition.
  - Quick check question: What is a transition monoid in the context of finite state automata, and why is it important for characterizing LRNN expressivity?

## Architecture Onboarding

- Component map:
  - State-transition matrices: Diagonal (Mamba) or generalized Householder (DeltaNet)
  - Input-dependent functions: A(xt), B(xt) determining matrix values
  - Decoder function: dec(Ht, xt) producing outputs
  - Eigenvalue range extension: From [0, 1] to [-1, 1]

- Critical path:
  1. Implement eigenvalue range extension in state-transition matrices
  2. Ensure numerical stability of modified matrices
  3. Test on parity and modular arithmetic tasks
  4. Evaluate on language modeling tasks

- Design tradeoffs:
  - Diagonal vs. non-diagonal matrices: Diagonal matrices are more efficient but less expressive
  - Eigenvalue range: [0, 1] is stable but limited; [-1, 1] is expressive but may introduce instability
  - Model size: Larger models may benefit more from increased expressivity

- Failure signatures:
  - Training instability or divergence
  - No improvement on parity or modular arithmetic tasks
  - Performance degradation on language modeling tasks

- First 3 experiments:
  1. Test parity task with extended eigenvalue range on a small Mamba model
  2. Test modular arithmetic task with extended eigenvalue range on a small DeltaNet model
  3. Compare perplexity on coding/math datasets between original and extended eigenvalue range models

## Open Questions the Paper Calls Out

- **Question**: Can hybrid models combining diagonal and generalized Householder matrices achieve better performance than purely one type?
  - Basis in paper: [explicit] The authors mention that "zero matrix could also be obtained with a single diagonal matrix" and suggest "hybrids LRNNs using a mix of GH and diagonal matrices" as future work.
  - Why unresolved: The paper only explores pure GH or diagonal structures, leaving the potential benefits of hybrid architectures untested.
  - What evidence would resolve it: Experiments comparing hybrid models against pure diagonal and pure GH models on state-tracking tasks, showing whether the hybrid approach offers superior performance or efficiency.

- **Question**: What is the trade-off between state-tracking capabilities and associative recall in LRNNs?
  - Basis in paper: [inferred] The authors hypothesize a "fundamental trade-off between state-tracking and associative recall" but do not investigate this empirically.
  - Why unresolved: The paper demonstrates enhanced state-tracking with extended eigenvalues but does not measure the impact on other capabilities like associative recall.
  - What evidence would resolve it: Systematic experiments measuring both state-tracking and associative recall performance across different LRNN architectures and eigenvalue ranges.

- **Question**: How many layers are truly necessary for LRNNs to achieve maximum expressivity for regular languages?
  - Basis in paper: [explicit] Theorem 4 requires up to 2|Q| layers, but the authors "conjecture that the number of layers might be reduced to at most linear using a more refined decomposition."
  - Why unresolved: The current theoretical bound is exponential in the number of states, while practical implementations may require far fewer layers.
  - What evidence would resolve it: Experimental results showing the minimum number of layers needed to achieve near-optimal performance on various regular language tasks, compared to theoretical predictions.

## Limitations

- The theoretical proof of full expressivity for regular languages relies on potentially impractical numbers of layers (up to exponential in the number of states)
- The modification to CUDA code for associative scan is mentioned as crucial but not fully detailed
- Performance improvements on coding/math datasets are shown but not compared against a broader range of state-of-the-art models

## Confidence

**High Confidence**: The theoretical proof that LRNNs with positive eigenvalues cannot solve parity and that extending to [-1, 1] enables this capability. The empirical demonstration of parity solution and modular arithmetic improvements on synthetic tasks. The stability of pre-training at scale (1.3B parameters) with extended eigenvalues.

**Medium Confidence**: The claim that the same Householder-based architecture can solve all regular languages when combined with extended eigenvalues. The translation of synthetic task improvements to real-world language modeling performance gains. The assertion that numerical stability is maintained without additional modifications.

**Low Confidence**: The scalability of this approach to extremely large models (10B+ parameters) where numerical stability might become more challenging. The claim that this modification alone is sufficient for all state-tracking tasks without architectural changes. The generalizability to non-group-based state-tracking tasks.

## Next Checks

1. **Numerical Stability Stress Test**: Implement systematic testing of the [-1, 1] eigenvalue range across different model scales (100M, 1B, 10B parameters) while monitoring gradient norms, loss curves, and eigenvalue distributions during training to identify potential instability thresholds.

2. **Expressive Power Verification**: Design and test LRNNs with the extended eigenvalue range on increasingly complex regular languages beyond parity and modular arithmetic, including languages requiring nested state dependencies, to empirically validate the theoretical claim about learning "any regular language."

3. **Architecture Ablation Study**: Systematically vary the structure of the state-transition matrices (diagonal vs. Householder vs. other structured forms) while keeping the [-1, 1] eigenvalue range fixed to isolate whether the eigenvalue extension or the matrix structure is primarily responsible for the observed improvements.