---
ver: rpa2
title: 'Modeling User Preferences with Automatic Metrics: Creating a High-Quality
  Preference Dataset for Machine Translation'
arxiv_id: '2410.07779'
source_url: https://arxiv.org/abs/2410.07779
tags:
- translation
- quality
- language
- dataset
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating high-quality preference
  datasets for machine translation by leveraging the strengths of both human evaluation
  and automatic metrics. The authors propose a holistic approach that first collects
  sentence-level quality assessments from professional linguists on translations generated
  by multiple high-quality MT systems.
---

# Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation

## Quick Facts
- arXiv ID: 2410.07779
- Source URL: https://arxiv.org/abs/2410.07779
- Reference count: 30
- Creates MT-PREF dataset with 18k preference instances across 18 language directions

## Executive Summary
This paper addresses the challenge of creating high-quality preference datasets for machine translation by combining human evaluation with automatic metrics. The authors first collect sentence-level quality assessments from professional linguists on translations from multiple high-quality MT systems, then evaluate automatic metrics' ability to recover these preferences. They identify XCOMET-XL+XXL as the best-performing metric and use it to curate MT-PREF, a new preference dataset covering 18 language directions. Experiments show that aligning TOWER models on MT-PREF significantly improves translation quality on WMT23 and FLORES benchmarks, with larger gains for out-of-English translation directions.

## Method Summary
The approach combines human evaluation with automatic metrics to create a scalable preference dataset. Professional linguists provide fine-grained 0-100 quality scores on translations from five MT systems (Google Translate, GPT-4, TOWER INSTRUCT-7B/13B, ALMA-13B-R) across WMT23 English-German and Chinese-English test sets. Automatic quality estimation metrics are evaluated for their ability to recover these human preferences, with XCOMET-XL+XXL achieving the highest correlation. This metric is then used to generate preference triples from a larger multilingual dataset, creating MT-PREF with 18k instances across 18 language directions. TOWER models are finetuned using preference optimization algorithms (DPO, CPO, SFT) on this dataset.

## Key Results
- XCOMET-XL+XXL achieves highest correlation with human judgments and high precision in identifying preferred translations
- Aligning TOWER models on MT-PREF significantly improves translation quality on WMT23 and FLORES benchmarks
- Preference learning improvements are more pronounced for English→Other language directions compared to Other→English

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human quality judgments combined with automatic metrics can induce high-quality translation preference data at scale.
- Mechanism: Professional linguists provide fine-grained 0-100 quality scores on translations from multiple MT systems. Automatic metrics are then evaluated for their ability to recover these human preferences. The best-performing metric (XCOMET-XL+XXL) is used to generate preference triples across a larger multilingual dataset.
- Core assumption: Automatic metrics can approximate human quality judgments when translation quality is high and systems are competitive.
- Evidence anchors:
  - [abstract]: "We first collect sentence-level quality assessments from professional linguists on translations generated by multiple high-quality MT systems and evaluate the ability of current automatic metrics to recover these preferences."
  - [section 3.2]: "Our findings show that an ensemble of XCOMET-XL and XCOMET-XXL—XCOMET-XL+XXL—achieves the highest correlation with human judgments and a high precision score in identifying the preferred translations."
  - [corpus]: Weak. Corpus neighbors focus on preference alignment but do not directly validate the hybrid human+metric approach.
- Break condition: If automatic metrics cannot recover human preferences at sufficient accuracy, the dataset quality degrades and alignment improvements vanish.

### Mechanism 2
- Claim: Preference learning using induced preference triples improves translation quality beyond supervised finetuning alone.
- Mechanism: Models are finetuned with preference optimization objectives (DPO, CPO) using the curated preference dataset. The relative ranking signal between translations guides the model to produce higher-quality outputs.
- Core assumption: Relative preference signals are more informative for translation quality than single-reference supervision.
- Evidence anchors:
  - [abstract]: "We show that aligning TOWER models on MT-P REF significantly improves translation quality on WMT23 and FLORES benchmarks."
  - [section 6.1]: "Preference learning improves translation quality. Most PO methods improve COMET and XCOMET-XL as well as % ACC. over TOWER INSTRUCT -7B in both directions."
  - [corpus]: Weak. Corpus entries discuss preference alignment but lack direct experimental validation.
- Break condition: If the preference signal is noisy or uninformative, models may not learn meaningful improvements or may degrade quality.

### Mechanism 3
- Claim: Scaling the size of preference datasets improves translation quality, especially for English→Other directions.
- Mechanism: Preference triples are generated from an increasing number of source sentences. Models finetuned on larger datasets show continued quality gains, with more pronounced effects for EN-XX directions.
- Core assumption: More preference data exposes the model to a broader distribution of translation nuances and quality distinctions.
- Evidence anchors:
  - [section 6.3]: "Fig. 4 shows the results: while the improvement in quality for XX-EN plateaus with just 400 samples per language direction, COMET continues to improve for EN-XX suggesting that adding more data might benefit translations from English to other language pairs."
  - [abstract]: No direct mention of scaling effects.
  - [corpus]: Weak. No corpus evidence on dataset scaling effects.
- Break condition: If the marginal gain from additional data diminishes or noise increases, further scaling provides no benefit.

## Foundational Learning

- Concept: Quality estimation (QE) metrics and their evaluation.
  - Why needed here: The study relies on QE metrics to recover human preferences and induce preference triples. Understanding their strengths and limitations is essential.
  - Quick check question: What are the key differences between reference-based metrics (e.g., COMET) and quality estimation metrics (e.g., XCOMET)?

- Concept: Preference optimization algorithms (DPO, CPO, SFT).
  - Why needed here: The finetuning approach uses these algorithms to align models with human preferences. Knowing their mechanics and trade-offs is critical for implementation.
  - Quick check question: How does the DPO objective differ from CPO, and why might one be preferred over the other in translation settings?

- Concept: Statistical significance testing (e.g., Wilcoxon rank-sum test).
  - Why needed here: The study uses significance testing to validate quality improvements. Understanding how to apply and interpret these tests is necessary for result analysis.
  - Quick check question: When comparing two MT systems, what p-value threshold is typically used to claim a significant difference in translation quality?

## Architecture Onboarding

- Component map: Human evaluation interface -> Automatic metric scoring -> Preference triple generation -> Model finetuning -> Benchmark evaluation
- Critical path:
  1. Collect human judgments on 200 source sentences per language pair
  2. Score all translations with automatic QE metrics
  3. Select XCOMET-XL+XXL as best-performing metric
  4. Generate 18k preference triples from diverse MT systems and new source sentences
  5. Finetune TOWER models using CPO on the preference dataset
  6. Evaluate on WMT23 and FLORES benchmarks
- Design tradeoffs:
  - Human evaluation vs. fully automatic: Higher cost but more reliable quality signal
  - Reference-based vs. quality estimation metrics: Better correlation but potentially domain-sensitive
  - Preference optimization vs. supervised finetuning: More informative but requires curated preference data
- Failure signatures:
  - Low correlation between automatic metrics and human judgments
  - Preference optimization leads to worse translation quality than SFT alone
  - Model overfits to specific MT system styles present in the dataset
- First 3 experiments:
  1. Validate automatic metrics on human judgments: Compute Pearson/Spearman/Kendall correlations for all QE metrics on the human-annotated dataset
  2. Compare preference learning algorithms: Finetune TOWER-7B on a small subset of MT-PREF using DPO, CPO, and DPO+SFT; compare COMET/XCOMET on WMT23
  3. Scale dataset size: Vary the number of source sentences (e.g., 200, 400, 600) and finetune TOWER-7B on each; plot COMET vs. dataset size for EN-XX and XX-EN

## Open Questions the Paper Calls Out
- How do the aligned models perform on language pairs not included in the MT-PREF dataset?
- What is the impact of different decoding strategies on the quality of the aligned models?
- How does the inclusion of tied preferences between translations affect the model's ability to discern quality?

## Limitations
- The approach relies heavily on the correlation between automatic metrics and human judgments, which may not generalize across all language pairs or domains
- The dataset construction depends on having multiple high-quality MT systems available, which may not be feasible for low-resource language pairs
- The study focuses on high-quality MT systems where quality differences are subtle, potentially not reflecting real-world scenarios with lower-quality baselines

## Confidence
- High: Experimental results showing translation quality improvements from preference learning are well-supported by multiple evaluation metrics across standard benchmarks
- Medium: Claim that scaling preference dataset size improves translation quality shows promising trends but requires further validation
- Low: Assertion that XCOMET-XL+XXL is universally optimal for all language pairs and domains needs more extensive validation

## Next Checks
1. Validate the XCOMET-XL+XXL metric's correlation with human preferences on out-of-domain test sets (e.g., biomedical, legal) to assess generalization beyond WMT and FLORES domains
2. Test the preference learning approach on genuinely low-resource language pairs (e.g., Nepali-English, Sinhala-English) where MT quality differences may be more pronounced
3. Evaluate the aligned models' performance over extended inference sessions to check for potential degradation or reward hacking behaviors that might emerge after prolonged use