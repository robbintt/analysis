---
ver: rpa2
title: 'Dynamic Normativity: Necessary and Sufficient Conditions for Value Alignment'
arxiv_id: '2406.11039'
source_url: https://arxiv.org/abs/2406.11039
tags:
- arxiv
- alignment
- page
- these
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Dynamic Normativity, proposing two sets of
  necessary and sufficient conditions for aligning AI systems under gradient-based
  learning. The sufficient conditions require: (1) coherent aggregation of human preferences
  to resolve normative uncertainty, (2) use of preferences as part of the learning
  signal, and (3) impact mitigation via safety guardrails.'
---

# Dynamic Normativity: Necessary and Sufficient Conditions for Value Alignment

## Quick Facts
- arXiv ID: 2406.11039
- Source URL: https://arxiv.org/abs/2406.11039
- Reference count: 0
- Primary result: Aligned models showed up to 8 percentage point improvements in toxicity reduction and truthfulness gains over baselines

## Executive Summary
This paper proposes Dynamic Normativity as a framework for AI alignment under gradient-based learning, establishing three sufficient conditions: coherent preference aggregation to resolve normative uncertainty, integration of preferences into the learning signal, and impact mitigation through safety guardrails. The methodology was implemented on scaled-down language models (124M-1.7B parameters) in Portuguese and English using instruction tuning, preference modeling (RLHF/DPO), and auxiliary reward models for toxicity reduction. Against baseline GPT-2/OPT/BLOOM models, the aligned versions showed consistent improvements across helpfulness (ARC), harmlessness (ToxiGen), and truthfulness (TruthfulQA) benchmarks, with toxicity scores improving by up to 8 percentage points and truthfulness gains noted.

## Method Summary
The Dynamic Normativity framework was implemented through three alignment components on scaled-down language models: (1) instruction tuning using curated datasets to teach task following and preference coherence, (2) preference modeling via RLHF and DPO to learn from human feedback signals, and (3) auxiliary reward models to detect and mitigate harmful outputs through safety guardrails. The approach used gradient-based learning to optimize for alignment while maintaining capability, testing on Portuguese and English datasets with models ranging from 124M to 1.7B parameters. The methodology assumes that satisfying these three sufficient conditions will produce aligned behavior, though the paper acknowledges this remains theoretically unproven and may face scalability constraints in non-English contexts.

## Key Results
- Toxicity reduction: Up to 8 percentage point improvement over baseline models
- Truthfulness gains: Measurable improvements on TruthfulQA benchmark
- Mixed capability impact: Some capability trade-offs observed, with alignment tax risks acknowledged

## Why This Works (Mechanism)
The framework works by establishing a structured approach to AI alignment that addresses three fundamental challenges simultaneously: resolving normative uncertainty through preference aggregation, learning from human preferences as part of the optimization objective, and constraining harmful behaviors through safety guardrails. By combining instruction tuning with preference modeling and impact mitigation, the system creates multiple feedback loops that reinforce aligned behavior while maintaining task performance. The gradient-based learning framework allows these components to be integrated into a unified training process that scales with model size.

## Foundational Learning
- **Preference aggregation theory**: Why needed - to resolve normative uncertainty from diverse human values; Quick check - verify aggregation method handles Arrow's impossibility theorem constraints
- **Gradient-based optimization**: Why needed - to integrate alignment objectives into model training; Quick check - ensure loss landscape supports stable convergence
- **Reward modeling**: Why needed - to provide scalable feedback signal for alignment; Quick check - validate reward model accuracy against human judgments
- **Safety guardrail implementation**: Why needed - to constrain harmful behaviors during deployment; Quick check - test guardrail robustness against adversarial inputs
- **Cross-lingual alignment**: Why needed - to ensure alignment generalizes across languages; Quick check - verify performance consistency across Portuguese and English
- **Alignment vs capability trade-offs**: Why needed - to understand real-world deployment constraints; Quick check - measure capability degradation on downstream tasks

## Architecture Onboarding

**Component map**
Instruction Tuning -> Preference Modeling (RLHF/DPO) -> Impact Mitigation (Auxiliary Reward Models) -> Aligned Model Output

**Critical path**
Data curation and preparation → Instruction tuning → Preference modeling → Guardrail integration → Evaluation

**Design tradeoffs**
Scale vs alignment effectiveness, human evaluation cost vs automated benchmarks, capability retention vs safety constraints, English vs non-English resource allocation

**Failure signatures**
Alignment tax (capability degradation), sycophantic behavior (over-optimization to preferences), guardrail circumvention (adversarial attacks), cross-lingual performance gaps

**3 first experiments**
1. Test preference aggregation sensitivity to Arrow's impossibility constraints using synthetic preference distributions
2. Evaluate guardrail robustness against multi-turn jailbreak attacks across model scales
3. Measure alignment-capability Pareto frontier at 128M, 760M, and 1.7B parameter scales

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** What dataset collection strategies and model architectures most effectively mitigate data scarcity for low-resource languages (e.g., Portuguese) in alignment tuning?
- **Basis in paper:** The paper acknowledges "scalability constraints in non-English contexts" and uses Portuguese datasets but notes limited resources and evaluation challenges.
- **Why unresolved:** Current approaches rely either on expensive high-quality human annotations or synthetic data from aligned models (which may propagate misalignment). Low-resource languages lack both, and existing benchmarks (e.g., ARC, ToxiGen) are English-centric, making it unclear if alignment techniques generalize.
- **What evidence would resolve it:** Systematic comparison of data augmentation techniques (back-translation, multilingual transfer) and parameter-efficient fine-tuning (LoRA, adapters) on alignment-specific benchmarks for non-English languages, with human evaluations for cultural nuance.

### Open Question 2
- **Question:** Can we formally prove that satisfying the sufficient conditions of Dynamic Normativity (coherent preference aggregation, preference learning, impact mitigation) yields asymptotic alignment under gradient-based learning?
- **Basis in paper:** The paper claims these conditions are sufficient but stops at empirical validation on small models (124M–1.7B parameters); no theoretical convergence guarantees are provided for non-convex loss landscapes.
- **Why unresolved:** The non-convex optimization of neural networks and the complexity of aggregating ordinal preferences (subject to Arrow's impossibility) make analytical proofs intractable; existing alignment proofs assume highly simplified MDPs or linear reward functions.
- **What evidence would resolve it:** Derivation of bounds on the KL divergence between the learned policy and target preference distribution under specific aggregation rules (e.g., Borda Count) and impact penalty terms, validated in toy environments with known optimal policies and scalable metrics for real-world models.

### Open Question 3
- **Question:** How can we objectively verify that an AI system's behavior aligns with human values without relying on human evaluation, which is prone to bias, inconsistency, and scalability limits?
- **Basis in paper:** The paper uses human preferences (via reward models) as the gold standard but notes "human evaluators can pursue harmful goals innocently or maliciously" and that preference data is often noisy and incomplete.
- **Why unresolved:** There is no independent ground truth for alignment; human judgments vary across cultures and contexts, and adversarial attacks can fool both humans and reward models. The paper's red-teaming efforts are limited in scope.
- **What evidence would resolve it:** Development of adversarial "chair tests" with verifiable ethical outcomes (e.g., cooperative game theory benchmarks) or multi-perspective validation combining diverse demographic groups and expert panels, correlated with long-term societal impact metrics.

### Open Question 4
- **Question:** What are the precise failure modes of impact mitigation guardrails (e.g., auxiliary reward models) against adversarial attacks, distribution shifts, or edge-case scenarios in real-world deployment?
- **Basis in paper:** The paper notes that guardrails can be "easily removed" via dataset flipping and that RLHF/DPO are sensitive to hyperparameters; it also mentions the helpful-harmless tension and sycophantic behavior.
- **Why unresolved:** Current evaluations (e.g., ToxiGen, TruthfulQA) are static, single-turn, and English-focused; they do not capture iterative attacks (multi-turn jailbreaks), distribution shifts (novel toxic genres), or trade-offs with helpfulness. The paper's guardrails are tested only on small models.
- **What evidence would resolve it:** A comprehensive adversarial robustness benchmark for aligned models, including red-teaming with RL agents optimizing for jailbreak success across multiple safety dimensions (toxicity, honesty, privacy), tested across languages and model scales.

### Open Question 5
- **Question:** Is there a fundamental trade-off between alignment and capability (the "alignment tax"), and can it be overcome via architectural innovations or training paradigms beyond scale?
- **Basis in paper:** The paper reports "alignment tax risks" where SFT models show drops in ARC (helpfulness) performance, and asks "how model size impacts this effect remains unexplored."
- **Why unresolved:** The paper's small-scale experiments (max 1.7B parameters) show mixed results; larger models (e.g., GPT-4) might exhibit different scaling laws for alignment vs. capability. It is unclear if the tax is due to data limitations, optimization difficulties, or inherent capacity constraints.
- **What evidence would resolve it:** Large-scale ablations varying model size (e.g., 128M to 70B), alignment method (SFT vs. DPO vs. constitutional AI), and training data composition to map the Pareto frontier between HHH scores and downstream task performance, with mechanistic interpretability to locate alignment-relevant circuits.

## Limitations
- Evaluation relies entirely on automated benchmarks without human preference studies or real-world deployment testing
- Results demonstrated only on scaled-down models (124M-1.7B parameters), raising scalability questions
- Acknowledged alignment tax risks suggest potential capability degradation not thoroughly quantified

## Confidence
- High confidence in technical implementation and benchmark methodology
- Medium confidence in philosophical framing and normative claims
- Medium confidence in generalizability of results across languages and model scales

## Next Checks
1. Conduct human preference studies and red-teaming exercises to validate whether benchmark improvements correlate with actual human judgment of helpfulness, harmlessness, and truthfulness in real-world contexts.

2. Test the Dynamic Normativity framework on frontier-scale models (70B+ parameters) to assess whether alignment mechanisms scale effectively without prohibitive capability trade-offs.

3. Implement cross-cultural validation using diverse ethical frameworks and languages to evaluate whether preference aggregation methods generalize beyond the Portuguese-English scope and avoid cultural bias in alignment outcomes.