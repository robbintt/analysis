---
ver: rpa2
title: 'aeon: a Python toolkit for learning from time series'
arxiv_id: '2406.14231'
source_url: https://arxiv.org/abs/2406.14231
tags:
- series
- time
- aeon
- data
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: aeon is a comprehensive Python library for time series machine
  learning, offering modules for forecasting, classification, clustering, and regression.
  It follows the scikit-learn API for ease of use and integration.
---

# aeon: a Python toolkit for learning from time series

## Quick Facts
- arXiv ID: 2406.14231
- Source URL: https://arxiv.org/abs/2406.14231
- Reference count: 16
- Key outcome: aeon is a comprehensive Python library for time series machine learning, offering modules for forecasting, classification, clustering, and regression with scikit-learn API compatibility

## Executive Summary
aeon is a comprehensive Python library for time series machine learning that provides modules for forecasting, classification, regression, and clustering. The library follows the scikit-learn API to enable easy integration with existing machine learning workflows and tools like model selection and pipelines. It supports multivariate and unequal-length time series data through optional dependencies, allowing users to install only the functionality they need while maintaining a minimal core framework.

## Method Summary
aeon implements time series machine learning through a modular design following scikit-learn conventions. The library provides four primary modules (forecasting, classification, regression, clustering) with base classes defining mandatory methods like fit and predict. It uses a system of optional dependencies to integrate various packages while keeping the core lightweight. The toolkit supports both standard 3D numpy arrays and lists of 2D arrays for handling unequal length series, with comprehensive support for distance measures, transformations, and ensemble methods.

## Key Results
- Unified API following scikit-learn conventions enables easy integration with existing ML workflows
- Optional dependency system provides extensive functionality while maintaining minimal core requirements
- Comprehensive support for multivariate and unequal-length time series across multiple ML tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: aeon's unified API enables easy integration of diverse time series algorithms
- Mechanism: The library follows the scikit-learn estimator interface, using common methods like `fit` and `predict` across all modules (forecasting, classification, regression, clustering). This standardization allows users familiar with scikit-learn to quickly adapt to aeon's structure.
- Core assumption: Users have prior exposure to scikit-learn's API design patterns
- Evidence anchors:
  - [abstract]: "aeon follows the scikit-learn API as much as possible to help new users and enable easy integration of aeon estimators with useful tools such as model selection and pipelines"
  - [section]: "aeon TSML classes follow an inheritance structure with each module having its own class. The base class for a module contains mandatory methods such as fit to be inherited"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.299, average citations=0.0. Top related titles include other Python toolkits, suggesting aeon's unified approach is distinctive in the ecosystem
- Break condition: When algorithms require fundamentally different interfaces that cannot be reconciled with scikit-learn patterns, or when users need specialized functionality not covered by the unified API

### Mechanism 2
- Claim: Optional dependencies allow aeon to maintain a minimal core while providing extensive functionality
- Mechanism: By using a system of optional dependencies, aeon integrates various packages (statsmodels, tensorflow, tsfresh) through wrappers while keeping the core framework lightweight. This allows users to install only what they need.
- Core assumption: Users can identify and install required optional dependencies for their specific use case
- Evidence anchors:
  - [abstract]: "Using a system of optional dependencies, aeon integrates a wide variety of packages into a single interface while keeping the core framework with minimal dependencies"
  - [section]: "aeon also includes a range of optional/soft dependencies to packages such as statsmodels, tensorflow, and tsfresh"
  - [corpus]: Weak evidence - the corpus neighbors include other Python packages with optional dependencies, but specific details about aeon's implementation are not well represented
- Break condition: When optional dependencies become too numerous to manage, or when dependency conflicts arise between different algorithm requirements

### Mechanism 3
- Claim: Modular design enables specialized handling of different time series tasks while maintaining code reuse
- Mechanism: aeon splits TSML tasks into separate modules (forecasting, classification, regression, clustering) that encapsulate functionality as much as possible. Supporting modules like distances and transformations provide shared utilities across tasks.
- Core assumption: Time series tasks have distinct requirements but also share common preprocessing needs
- Evidence anchors:
  - [section]: "aeon splits different TSML tasks into modules. In the following we go over some of the stable core modules of aeon... The primary TSML modules such as forecasting and classification are encapsulated as much as possible and do not import from each other"
  - [abstract]: "The package contains modules for time series forecasting, classification, extrinsic regression and clustering, as well as a variety of utilities, transformations and distance measures designed for time series data"
  - [corpus]: Moderate evidence - the corpus shows other specialized time series packages, but aeon's comprehensive approach to unifying multiple tasks is distinctive
- Break condition: When task boundaries become too rigid, preventing useful cross-task innovations, or when the overhead of maintaining separate modules outweighs the benefits

## Foundational Learning

- Concept: scikit-learn API patterns (fit/predict interface, pipelines, model selection)
  - Why needed here: aeon's design philosophy is to follow scikit-learn conventions as much as possible, so understanding these patterns is crucial for using aeon effectively
  - Quick check question: What are the two mandatory methods that all scikit-learn compatible estimators must implement?

- Concept: Time series data structures (multivariate arrays, unequal-length handling)
  - Why needed here: aeon supports both standard 3D numpy arrays and lists of 2D arrays for unequal length series, which affects how data is prepared and passed to estimators
  - Quick check question: What are the two primary data structures aeon uses to represent collections of time series?

- Concept: Distance measures for time series (DTW, edit distance, etc.)
  - Why needed here: Distance-based algorithms are fundamental to many time series tasks in aeon, and understanding available distance measures helps in algorithm selection
  - Quick check question: Name two elastic distance functions commonly used in time series clustering that aeon likely implements

## Architecture Onboarding

- Component map: Core modules (forecasting, classification, regression, clustering) -> Base classes defining interface -> Supporting modules (distances, transformations) -> Optional dependencies (statsmodels, tensorflow, tsfresh)
- Critical path: 1) Install aeon with core dependencies 2) Load time series data in appropriate format 3) Select appropriate module for task 4) Choose algorithm within module 5) Use fit/predict pattern 6) Apply scikit-learn tools like pipelines and model selection
- Design tradeoffs: Unified API vs. specialized algorithm requirements; minimal core vs. comprehensive functionality; scikit-learn compatibility vs. time series-specific optimizations
- Failure signatures: Errors when input data doesn't match expected format (e.g., wrong shape or data type); ImportError when optional dependencies are missing; AttributeError when trying to use methods not implemented in specific algorithms
- First 3 experiments:
  1. Load a simple dataset (like GunPoint) and run a basic Rocket classifier to verify installation and basic functionality
  2. Use a transformer (like Fourier transform) on a time series to understand the fit/transform pattern
  3. Create a pipeline combining a transformer and classifier, then use cross-validation to evaluate performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can aeon be extended to support streaming time series data and incremental learning?
- Basis in paper: [inferred] The paper mentions aeon's current focus on batch processing and does not discuss streaming or incremental learning capabilities.
- Why unresolved: The paper does not address this limitation or propose solutions for handling continuous data streams.
- What evidence would resolve it: Implementation and evaluation of streaming time series algorithms within aeon, demonstrating improved performance on continuous data compared to batch processing approaches.

### Open Question 2
- Question: What are the most effective ensemble methods for combining different time series classifiers in aeon?
- Basis in paper: [explicit] The paper mentions ensemble methods briefly but does not provide a comprehensive analysis of their effectiveness.
- Why unresolved: The paper does not compare different ensemble strategies or determine optimal combinations for various time series classification tasks.
- What evidence would resolve it: Systematic evaluation of multiple ensemble methods (e.g., stacking, voting, boosting) across diverse time series datasets, identifying the most effective approaches.

### Open Question 3
- Question: How can aeon be optimized for distributed and parallel processing of large-scale time series datasets?
- Basis in paper: [inferred] The paper does not discuss distributed computing capabilities or performance optimization for handling massive datasets.
- Why unresolved: The current implementation's scalability for very large datasets or distributed environments is not addressed.
- What evidence would resolve it: Implementation of distributed versions of key algorithms in aeon, with benchmark results showing improved performance and scalability on large-scale time series datasets.

## Limitations
- Performance benchmarks against competing libraries are not provided in the paper
- Implementation details for complex algorithms (deep learning approaches) are not specified
- Scalability limitations for large-scale time series datasets are not discussed

## Confidence
- High Confidence: The basic API design following scikit-learn conventions and the core module structure are well-established and clearly documented
- Medium Confidence: The optional dependency system and integration mechanisms are described but implementation details may vary in practice
- Low Confidence: Performance comparisons with other toolkits and specific algorithm implementations are not detailed in the paper

## Next Checks
1. Benchmark aeon's Rocket classifier against sktime's implementation on standard UCR datasets to verify claimed performance parity
2. Test the optional dependency system by installing different algorithm sets and verifying functional integration
3. Evaluate memory usage and runtime for processing multivariate time series with varying lengths to identify scalability constraints