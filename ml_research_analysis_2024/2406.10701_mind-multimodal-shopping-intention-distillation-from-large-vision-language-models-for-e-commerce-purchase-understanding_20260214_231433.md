---
ver: rpa2
title: 'MIND: Multimodal Shopping Intention Distillation from Large Vision-language
  Models for E-commerce Purchase Understanding'
arxiv_id: '2406.10701'
source_url: https://arxiv.org/abs/2406.10701
tags:
- intentions
- intention
- mind
- product
- both
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MIND, a multimodal framework that distills
  purchase intentions from large vision-language models using multimodal product metadata.
  MIND addresses limitations in prior methods by incorporating visual information
  from product images and prioritizing human-centric intentions.
---

# MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding

## Quick Facts
- arXiv ID: 2406.10701
- Source URL: https://arxiv.org/abs/2406.10701
- Reference count: 34
- Key outcome: MIND generates 1.26M multimodal purchase intentions, improving E-commerce task performance by 1-2% accuracy

## Executive Summary
This paper introduces MIND, a multimodal framework that distills purchase intentions from large vision-language models using multimodal product metadata. MIND addresses limitations in prior methods by incorporating visual information from product images and prioritizing human-centric intentions. The framework generates a multimodal intention knowledge base containing 1.26 million intentions from 126,142 co-buy shopping records across 107,215 products. Human evaluations demonstrate high plausibility (94%) and typicality (90%) of the obtained intentions, with an 82% filtering accuracy. Fine-tuning large language models on MIND's intentions significantly improves performance on E-commerce intention comprehension tasks.

## Method Summary
MIND uses a three-step LVLM pipeline to extract purchase intentions from co-buy shopping records. First, it extracts product features from images and titles using LLaVa-1.5-13b. Second, it generates purchase intentions grounded in ConceptNet relations by prompting the LVLM to infer motivations for co-buying products. Third, it applies a human-centric role-aware filtering mechanism that simulates a customer agent to verify intentions align with human mental states. The resulting knowledge base of 1.26 million intentions is then used to fine-tune language models for downstream E-commerce tasks.

## Key Results
- MIND generates 1.26 million multimodal purchase intentions from 126,142 co-buy records
- Human evaluations show 94% plausibility and 90% typicality of generated intentions
- Fine-tuning Mistral-7B achieves 1.54% and 1.00% accuracy gains on IntentionQA benchmark tasks
- Visual information improves downstream task performance by 0.6% and 0.7% respectively compared to text-only methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MIND improves purchase intention understanding by leveraging both visual and textual modalities through LVLM distillation
- Mechanism: MIND uses LLaVa to extract product features from images and text, then generates purchase intentions grounded in ConceptNet relations. The human-centric role-aware filtering step simulates a customer agent to verify intentions align with human mental states.
- Core assumption: Visual information provides essential signals for understanding product features that text alone cannot capture, and simulating customer decision-making can effectively filter intentions.
- Evidence anchors:
  - [abstract]: "MIND addresses limitations in prior methods by incorporating visual information from product images and prioritizing human-centric intentions"
  - [section 3.3]: "we explicitly instructs LVLMs to augment source product metadata by extracting implicit features from each product's image and title"
  - [section 3.5]: "we propose a human-centric role-aware mechanism...instructs the LVLM to assume the role of an E-commerce customer"

### Mechanism 2
- Claim: Fine-tuning large language models on MIND's intentions significantly improves performance on E-commerce intention comprehension tasks
- Mechanism: MIND generates a knowledge base of 1.26 million intentions which are then used to fine-tune language models through instruction-following format templates. This knowledge injection enhances the models' ability to understand and utilize purchase intentions.
- Core assumption: The quality and quantity of intentions generated by MIND are sufficient to meaningfully improve downstream task performance when used for fine-tuning.
- Evidence anchors:
  - [abstract]: "Fine-tuning large language models on MIND's intentions significantly improves performance on E-commerce intention comprehension tasks, with Mistral-7B achieving accuracy gains of 1.54% and 1.00% on two benchmark tasks"
  - [section 5.2]: "LLAMA2 achieves accuracy gains of 1.54% and 1.00% for both tasks, respectively"

### Mechanism 3
- Claim: MIND's multimodal generation and role-aware filtering produce intentions with higher quality and typicality compared to previous text-only methods
- Mechanism: By incorporating visual information and using a role-aware filtering mechanism that simulates customer decision-making, MIND generates intentions that are more plausible (94% average) and typical (90% average) than previous methods like FolkScope.
- Core assumption: Visual information and customer simulation significantly improve intention quality beyond what text-only generation can achieve.
- Evidence anchors:
  - [abstract]: "human evaluations demonstrate high plausibility (94%) and typicality (90%) of the obtained intentions"
  - [section 5.3.5]: "intentions generated by MIND exhibit higher typicality scores across nearly all relations compared to those generated by FolkScope"

## Foundational Learning

- Concept: Multimodal learning and fusion of visual and textual information
  - Why needed here: MIND relies on extracting features from both product images and text descriptions to generate comprehensive purchase intentions
  - Quick check question: How does the model handle cases where visual and textual information provide conflicting signals about product features?

- Concept: Knowledge distillation from large vision-language models
  - Why needed here: MIND uses LLaVa to distill purchase intentions from co-buy shopping records, requiring understanding of how to effectively prompt and guide LVLMs
  - Quick check question: What are the key differences between distilling from a pure language model versus a vision-language model?

- Concept: Commonsense reasoning and relation grounding
  - Why needed here: MIND uses 20 ConceptNet relations to ground purchase intentions, requiring understanding of how to map product relationships to human intentions
  - Quick check question: How does the choice of relations affect the diversity and quality of generated intentions?

## Architecture Onboarding

- Component map: Amazon Review Dataset → LVLM Feature Extraction → Intention Generation → Role-Aware Filtering → Knowledge Base Storage → Downstream Fine-tuning

- Critical path: Product feature extraction → Intention generation → Quality filtering → Fine-tuning downstream models
  The most critical components are the LVLM feature extraction and the role-aware filtering, as errors in these steps propagate through the entire pipeline.

- Design tradeoffs:
  - Generating multiple intentions per relation vs. single intention (scalability vs. diversity)
  - Using zero-shot prompts vs. few-shot prompts (generality vs. guidance)
  - Strict filtering vs. lenient filtering (quality vs. quantity)

- Failure signatures:
  - Low filtering preserve rate (<20%) indicates poor intention quality or overly strict filtering
  - Downstream performance degradation indicates issues with intention relevance or fine-tuning process
  - High similarity between generated intentions suggests lack of diversity

- First 3 experiments:
  1. Ablation study: Compare downstream performance using intentions generated with vs. without visual information
  2. Filter sensitivity analysis: Test different filtering thresholds and their impact on downstream performance
  3. Relation diversity analysis: Examine the distribution of intentions across different ConceptNet relations and their downstream impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific features of product images are most valuable for inferring purchase intentions?
- Basis in paper: [explicit] The paper states that current methods overlook visual information from product images, hindering the model's understanding of the product.
- Why unresolved: The paper does not investigate which specific visual features contribute most to intention inference.
- What evidence would resolve it: An ablation study isolating different visual features (color, shape, texture, etc.) and their impact on intention generation quality.

### Open Question 2
- Question: How does the quality of generated intentions change when using more up-to-date vision-language models?
- Basis in paper: [explicit] The paper mentions that the LVLM used may be outdated as new products show up on e-commerce platforms.
- Why unresolved: The paper uses LLaVa-1.5-13b as the representative LVLM without exploring more recent models.
- What evidence would resolve it: A comparison of intention quality metrics when using different vision-language models trained on more recent e-commerce data.

### Open Question 3
- Question: What is the optimal balance between strictness and leniency for the human-centric role-aware filter?
- Basis in paper: [explicit] The paper states that regulating the filter mechanism to be either lenient or strict remains challenging.
- Why unresolved: The paper does not explore different filter thresholds or mechanisms to optimize the balance between preserving quality intentions and filtering out low-quality ones.
- What evidence would resolve it: A systematic analysis of intention quality metrics and downstream task performance across different filter configurations.

## Limitations
- The paper relies on a single LVLM (LLaVa-1.5-13b) for both feature extraction and intention generation, limiting generalizability across different model architectures.
- Human evaluation methodology depends on subjective judgments that may not fully capture real-world customer behavior.
- The filtering mechanism's effectiveness is primarily validated through simulated customer agents rather than actual consumer feedback.

## Confidence
- **High Confidence**: The downstream performance improvements on IntentionQA benchmark (1.54% and 1.00% accuracy gains) are well-supported by quantitative results and reproducible through the provided methodology.
- **Medium Confidence**: The quality metrics from human evaluations (94% plausibility, 90% typicality) are credible but depend on the evaluation framework's design and potential rater biases.
- **Medium Confidence**: The superiority over text-only methods is demonstrated through ablation studies, but the comparison baseline (FolkScope) may not represent the full spectrum of alternative approaches.

## Next Checks
1. **Cross-LVLM Validation**: Test MIND's pipeline with alternative vision-language models (e.g., GPT-4V, Gemini) to verify the approach's robustness across different model architectures and assess potential architectural dependencies.

2. **Real-World Consumer Validation**: Conduct A/B testing with actual E-commerce customers to validate whether MIND-generated intentions better predict purchase behavior compared to baseline approaches, moving beyond simulated customer agents.

3. **Longitudinal Performance Analysis**: Evaluate MIND's intentions across different time periods and product categories to assess temporal stability and generalizability beyond the initial Electronics and Clothing domains studied.