---
ver: rpa2
title: Validation, Robustness, and Accuracy of Perturbation-Based Sensitivity Analysis
  Methods for Time-Series Deep Learning Models
arxiv_id: '2401.16521'
source_url: https://arxiv.org/abs/2401.16521
tags: []
core_contribution: 'This paper proposes to validate, assess robustness, and measure
  accuracy of perturbation-based sensitivity analysis methods for time-series deep
  learning models, specifically focusing on Transformer models. The author investigates
  three research questions: (1) Do different sensitivity analysis (SA) methods yield
  comparable outputs and attribute importance rankings?'
---

# Validation, Robustness, and Accuracy of Perturbation-Based Sensitivity Analysis Methods for Time-Series Deep Learning Models

## Quick Facts
- arXiv ID: 2401.16521
- Source URL: https://arxiv.org/abs/2401.16521
- Authors: Zhengguang Wang
- Reference count: 4
- Key outcome: Proposes to validate, assess robustness, and measure accuracy of perturbation-based sensitivity analysis methods for time-series deep learning models, specifically focusing on Transformer models.

## Executive Summary
This paper proposes a comprehensive framework to validate, assess robustness, and measure accuracy of perturbation-based sensitivity analysis methods for time-series deep learning models. The author investigates three key research questions: (1) whether different SA methods yield comparable outputs, (2) how different DL models impact SA outputs, and (3) how well SA results align with ground truth. The study focuses on Transformer models and applies multiple perturbation-based SA methods (Feature Ablation, Feature Occlusion, Morris Method) to five different DL models (TFT, TimesNet, Autoformer, DLinear, PatchTST) using a U.S. county-level COVID-19 cases dataset with 8 age feature groups.

## Method Summary
The research proposes to implement and apply three perturbation-based sensitivity analysis methods (Feature Ablation, Feature Occlusion, and Morris Method) to five different deep learning models (TFT, TimesNet, Autoformer, DLinear, PatchTST) trained on a 2-year U.S. county-level COVID-19 cases dataset with 8 age feature groups. The feature importance rankings produced by each SA method-model combination will be compared using Spearman rank correlation coefficient to assess consistency between methods, robustness across models, and alignment with ground truth data aggregated from reported COVID-19 cases by age groups.

## Key Results
- Proposed framework for validating perturbation-based SA methods for time-series DL models
- Investigation of three research questions on method consistency, model robustness, and ground truth alignment
- Experimental setup with three SA methods, five DL models, and COVID-19 dataset with 8 age feature groups
- Spearman rank correlation coefficient as evaluation metric for comparing feature importance rankings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spearman rank correlation coefficient effectively measures consistency between different sensitivity analysis methods and ground truth rankings.
- Mechanism: By converting feature importance scores into ranks and computing Spearman correlation, the method captures monotonic relationships between rankings without assuming linear relationships or normal distributions.
- Core assumption: The ranking of features by importance is meaningful and comparable across different SA methods and with ground truth.
- Evidence anchors:
  - [abstract]: "The feature importance produced by the sensitivity analysis methods will be ranked, and we rank the ground truth is aggregated from reported cases by those age groups as well. From here, the evaluation metric, Spearman rank correlation coefficient, can be applied."
  - [section]: "evaluation metric, Spearman rank correlation coefficient, can be applied. A value of 1 for the Spearman rank correlation coefficient means perfectly monotonically increasing relationship while a value of -1 implies no monotonic relation."
- Break condition: If the ground truth ranking does not reflect true causal relationships, or if different SA methods produce non-monotonic relationships with ground truth, Spearman correlation may fail to capture meaningful differences.

### Mechanism 2
- Claim: Perturbation-based methods are model-agnostic and thus provide consistent interpretation across different deep learning architectures.
- Mechanism: By systematically modifying input features and observing output changes, these methods extract feature importance independent of model internal structure, enabling cross-model comparison.
- Core assumption: The perturbation methodology is sufficiently robust to model-specific behaviors and captures universal feature importance patterns.
- Evidence anchors:
  - [section]: "I will study the perturbation group because of its model-agnostic, non-parametric, and interpretable nature."
  - [section]: "Using the same sensitivity analysis method, do different Deep Learning (DL) models impact the output of the sensitivity analysis? (Robustness)"
- Break condition: If different DL architectures process temporal dependencies in fundamentally different ways, perturbation-based methods may yield inconsistent results across models despite their theoretical model-agnosticism.

### Mechanism 3
- Claim: Comparing multiple SA methods within the same model reveals validation of interpretation consistency.
- Mechanism: When different SA methods (Feature Ablation, Feature Occlusion, Morris Method) applied to the same model produce similar rankings, this cross-validation indicates robust feature importance detection.
- Core assumption: Different perturbation strategies should converge on similar feature importance if they are capturing true model behavior.
- Evidence anchors:
  - [section]: "Within the same model, do different sensitivity analysis (SA) methods yield comparable outputs and attribute importance rankings? (Validation)"
  - [section]: "The answer to my first research question will shed light on whether it is necessary to interpret a Deep Learning model with multiple perturbation-group post-hoc method."
- Break condition: If different SA methods capture different aspects of model behavior (e.g., local vs. global effects), consistency may not be expected or necessary for valid interpretation.

## Foundational Learning

- Concept: Time series forecasting fundamentals
  - Why needed here: The research applies SA methods specifically to time series DL models, requiring understanding of temporal dependencies and forecasting challenges.
  - Quick check question: How do temporal dependencies in sequential data differ from static feature relationships in traditional ML?

- Concept: Feature attribution and interpretability methods
  - Why needed here: The work compares multiple SA approaches (Feature Ablation, Feature Occlusion, Morris Method) against other interpretability families like back-propagation and approximation methods.
  - Quick check question: What distinguishes perturbation-based methods from gradient-based methods in terms of computational requirements and model access?

- Concept: Spearman rank correlation coefficient
  - Why needed here: This non-parametric statistic is the chosen evaluation metric for comparing ranked feature importance across methods and with ground truth.
  - Quick check question: How does Spearman correlation differ from Pearson correlation when comparing ranked lists, and why is this distinction important for SA method comparison?

## Architecture Onboarding

- Component map:
  - COVID-19 dataset (2 years, 3,142 US counties, 8 age feature groups) -> DL models (TFT, TimesNet, Autoformer, DLinear, PatchTST) -> SA methods (Feature Ablation, Feature Occlusion, Morris Method) -> Feature importance rankings -> Spearman rank correlation coefficient -> Ground truth comparison

- Critical path:
  1. Preprocess COVID-19 dataset into model-ready format
  2. Train each DL model on the dataset
  3. Apply each SA method to each trained model
  4. Generate feature importance rankings for all method-model combinations
  5. Compute Spearman correlation between all SA method pairs and with ground truth
  6. Analyze results to answer the three research questions

- Design tradeoffs:
  - Model complexity vs. training time: More complex models (Transformers) may capture better patterns but require more resources
  - SA method granularity vs. computational cost: More extensive perturbation sampling increases accuracy but requires more computation
  - Dataset size vs. statistical significance: Larger datasets provide more robust results but increase processing requirements

- Failure signatures:
  - Low or inconsistent Spearman correlations across SA methods may indicate methodological issues or dataset limitations
  - High variance in feature importance rankings suggests instability in SA methods or model training
  - Ground truth misalignment could reveal issues with reported case data quality or aggregation methodology

- First 3 experiments:
  1. Implement and validate a single SA method (e.g., Morris Method) on one DL model (e.g., TFT) to establish baseline functionality
  2. Apply the same SA method to all DL models to assess cross-model consistency before introducing additional SA methods
  3. Compare two SA methods on a single DL model to validate the correlation-based evaluation approach before scaling to all combinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do different sensitivity analysis methods yield comparable outputs and attribute importance rankings when applied to time-series deep learning models?
- Basis in paper: [explicit] The author explicitly states this as Research Question 1: "Do different sensitivity analysis (SA) methods yield comparable outputs and attribute importance rankings?"
- Why unresolved: The author plans to compare multiple SA methods (Feature Ablation, Feature Occlusion, and Morris Method) but has not yet conducted the experiments to determine if they produce consistent results.
- What evidence would resolve it: Experimental results showing Spearman rank correlation coefficients between the rankings produced by different SA methods applied to the same DL models and dataset.

### Open Question 2
- Question: How does the choice of deep learning model impact the output of sensitivity analysis when using the same sensitivity analysis method?
- Basis in paper: [explicit] The author explicitly states this as Research Question 2: "Using the same sensitivity analysis method, do different Deep Learning (DL) models impact the output of the sensitivity analysis?"
- Why unresolved: The author plans to apply SA methods to multiple DL models (TFT, TimesNet, Autoformer, DLinear, PatchTST) but has not yet determined if the choice of model affects the SA results.
- What evidence would resolve it: Experimental results showing Spearman rank correlation coefficients between SA results from the same method applied to different DL models.

### Open Question 3
- Question: How well do the results from sensitivity analysis methods align with the ground truth for COVID-19 cases by age groups?
- Basis in paper: [explicit] The author explicitly states this as Research Question 3: "How well do the results from sensitivity analysis methods align with the ground truth?"
- Why unresolved: While the author plans to use ground truth data aggregated from reported cases by age groups, they have not yet determined the alignment between SA results and this ground truth.
- What evidence would resolve it: Spearman rank correlation coefficients between the feature importance rankings from SA methods and the ground truth rankings, providing a quantitative measure of alignment.

## Limitations

- The assumption that Spearman rank correlation coefficient sufficiently captures meaningful relationships between SA methods and ground truth may not account for non-monotonic or context-dependent relationships in feature importance.
- The model-agnostic assumption of perturbation methods may not hold across diverse architectures with fundamentally different temporal processing mechanisms.
- Ground truth aggregation from reported COVID-19 cases may introduce biases or noise that could confound the interpretation of SA method accuracy.

## Confidence

- Mechanism 1 (Spearman correlation evaluation): Medium
- Mechanism 2 (Cross-model consistency): Medium-Low
- Mechanism 3 (SA method comparison): Medium

## Next Checks

1. Test Spearman correlation sensitivity to noise in ground truth data by introducing controlled perturbations to assess robustness of the evaluation metric.
2. Conduct ablation studies to isolate the impact of individual model components on SA method outputs and validate cross-model consistency claims.
3. Validate the ground truth aggregation methodology against alternative data sources or expert annotations to ensure reliability of the alignment assessment.