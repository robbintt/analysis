---
ver: rpa2
title: Deep Kernel Posterior Learning under Infinite Variance Prior Weights
arxiv_id: '2410.01284'
source_url: https://arxiv.org/abs/2410.01284
tags:
- kernel
- stable
- deep
- posterior
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper develops a deep kernel process based on infinitely\
  \ wide Bayesian neural networks with infinite variance prior weights. The key innovation\
  \ is showing that such networks converge to processes with \u03B1-stable marginals\
  \ that admit a conditionally Gaussian representation, enabling recursive kernel\
  \ formulas and practical MCMC inference."
---

# Deep Kernel Posterior Learning under Infinite Variance Prior Weights

## Quick Facts
- arXiv ID: 2410.01284
- Source URL: https://arxiv.org/abs/2410.01284
- Reference count: 40
- One-line primary result: Deep kernel process with infinite variance prior weights converges to α-stable processes with conditionally Gaussian representations, enabling efficient MCMC inference and superior uncertainty quantification for discontinuous functions.

## Executive Summary
This paper develops a deep kernel process based on infinitely wide Bayesian neural networks with infinite variance prior weights. The key innovation is showing that such networks converge to processes with α-stable marginals that admit a conditionally Gaussian representation, enabling recursive kernel formulas and practical MCMC inference. Unlike prior deep kernel methods that require artificial noise injection, the stochasticity here emerges naturally from heavy-tailed priors. The resulting method (Dα-KP) demonstrates superior prediction accuracy and uncertainty quantification in settings with discontinuous functions compared to Gaussian process and deep inverse Wishart process baselines, with reduced computational cost versus previous stable-process approaches.

## Method Summary
The method constructs a deep kernel process as the infinite-width limit of Bayesian neural networks where each layer width approaches infinity and all weights are elliptically distributed with infinite variance. This yields processes with α-stable marginals that can be represented as conditionally Gaussian mixtures, enabling recursive computation of stochastic covariance kernels. The approach uses MCMC sampling (specifically Metropolis-Hastings) on the mixing scales to learn posterior distributions of features, allowing representation learning unlike deterministic deep GP limits. The method is implemented for activation functions of the form gδ(ζ) = ζδ1{ζ>0} and demonstrates superior performance on discontinuous functions while maintaining computational tractability.

## Key Results
- Dα-KP achieves lower RMSE and MAE than GP Bayes, GP MLE, DIWP, NNGP, and Stable methods on discontinuous synthetic functions
- The method provides better uncertainty quantification with improved coverage of 90% credible intervals
- Computational cost is reduced compared to previous stable-process approaches while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Infinite-width Bayesian neural networks with elliptically distributed weights of infinite variance converge to α-stable kernel processes with conditionally Gaussian representations.
- Mechanism: The heavy-tailed priors induce stable margins, but the conditional Gaussian structure (via Gaussian scale mixtures) preserves recursive kernel formulas from Cho & Saul (2009), enabling tractable inference.
- Core assumption: Weights in each layer are i.i.d. elliptically distributed with infinite variance and zero mean, independent across layers.
- Evidence anchors:
  - [abstract]: "such networks converge to processes with α-stable marginals that admit a conditionally Gaussian representation"
  - [section]: "we show that a Bayesian deep neural network, where each layer width approaches infinity, and all network weights are elliptically distributed with infinite variance, converges to a process with α-stable marginals"
  - [corpus]: Weak. No direct neighbor mentions infinite variance priors or stable marginals.

### Mechanism 2
- Claim: The conditionally Gaussian representation enables recursive computation of stochastic covariance kernels, unlike deterministic deep GP limits.
- Mechanism: By conditioning on mixing positive α/2-stable variables, each layer's kernel becomes a random matrix whose posterior can be learned, preserving representation learning.
- Core assumption: Activation functions of the form gδ(ζ) = ζδ1{ζ>0} allow explicit recursive kernel formulas.
- Evidence anchors:
  - [abstract]: "These conditional random covariance kernels could be recursively linked in the manner of Cho & Saul (2009)"
  - [section]: "These conditional random covariance kernels could be recursively linked in the manner of Cho & Saul (2009), even though marginally the process exhibits stable behavior"
  - [corpus]: Weak. No neighbor discusses recursive stochastic kernel computation.

### Mechanism 3
- Claim: The stochastic covariance kernel allows learning non-degenerate posterior distributions of features, enabling representation learning unlike deterministic deep GPs.
- Mechanism: Features z(ℓ)j | {s(ℓ)+}Lℓ=2, S(1)+ ~ N(0, s(ℓ)+ Σ(ℓ)) depend on observations through the posterior of mixing scales, unlike Gaussian limits.
- Core assumption: α < 2 ensures s(ℓ)+ ~ S+α/2 are random, not degenerate at 1.
- Evidence anchors:
  - [abstract]: "These conditional random covariance kernels could be recursively linked... even though covariances are not even necessarily defined"
  - [section]: "the kernel s(ℓ)+ Σ(ℓ) at all layers are random for α < 2, conditional on scales with a positive α/2-stable distribution"
  - [corpus]: Weak. No neighbor explicitly discusses feature learning in heavy-tailed BNN limits.

## Foundational Learning

- Concept: Elliptical α-stable random vectors and their Gaussian mixture representation
  - Why needed here: The paper relies on representing stable vectors as S1/2G where S ~ S+α/2 and G ~ N(0, Σ), which is fundamental to the conditionally Gaussian construction
  - Quick check question: Can you write the characteristic function of an elliptical α-stable vector and explain its Gaussian mixture representation?

- Concept: Recursive kernel formulas for deep architectures (Cho & Saul, 2009)
  - Why needed here: The method extends these formulas to the stochastic kernel setting, requiring understanding of how layer-wise covariances relate
  - Quick check question: For activation gδ(ζ) = ζδ1{ζ>0}, can you derive how Σ(ℓ) relates to Σ(ℓ-1) and s(ℓ-1)+?

- Concept: α-stable distributions and their properties (especially positive α-stable variables)
- Why needed here: The mixing scales s(ℓ)+ ~ S+α/2 are positive α-stable, and understanding their properties is crucial for inference and MCMC sampling
- Quick check question: What is the characteristic function of a positive α-stable variable and why can't it have a closed-form density in general?

## Architecture Onboarding

- Component map: Prior specification (elliptical weights with infinite variance) -> Layer covariance recursion (Σ(ℓ) from Σ(ℓ-1) and s(ℓ-1)+) -> MCMC scale sampling (Metropolis-Hastings on {s(ℓ)+}Lℓ=2, S(1)+) -> Posterior predictive computation -> Feature extraction (z(ℓ)j ~ N(0, s(ℓ)+ Σ(ℓ)))

- Critical path: Weight prior → Layer covariance recursion → MCMC scale sampling → Posterior predictive computation → Feature extraction

- Design tradeoffs:
  - α < 2 enables feature learning but requires MCMC sampling; α = 2 reduces to deterministic deep GP
  - Layer depth L affects computational cost but shows minimal performance difference in experiments
  - MCMC sampling is expensive but necessary for full uncertainty quantification

- Failure signatures:
  - If MCMC doesn't mix, scales s(ℓ)+ will have degenerate posteriors → deterministic kernel
  - If recursive formulas fail to converge, layer covariances Σ(ℓ) become invalid
  - If α is too close to 2, computational benefits diminish while feature learning capability remains weak

- First 3 experiments:
  1. Verify the conditionally Gaussian representation by sampling scales s(ℓ)+ and checking if z(ℓ)j ~ N(0, s(ℓ)+ Σ(ℓ)) holds empirically
  2. Test recursive covariance computation on synthetic data with known Σ(ℓ) to ensure numerical stability
  3. Run MCMC on a simple 1D discontinuous function to verify that scales learn non-degenerate posteriors and capture the jump discontinuity

## Open Questions the Paper Calls Out

- Question: How sensitive are the posterior predictions to the choice of activation function in the deep α-stable kernel process?
- Question: What is the theoretical relationship between the stability parameter α and the optimal depth L for prediction accuracy in different function classes?
- Question: How do deep α-stable kernel processes compare to sparse or inducing point approximations of deep Gaussian processes in terms of computational scalability?

## Limitations

- Computational scalability is limited, with experiments only on datasets up to 200 points
- Hyperparameter sensitivity is not thoroughly explored, particularly for α and layer depth L
- The method relies on expensive MCMC sampling, which may not scale to very large datasets

## Confidence

- High confidence: Mathematical derivation of the conditionally Gaussian representation and recursive kernel formulas
- Medium confidence: Empirical demonstration of improved prediction accuracy and uncertainty quantification on discontinuous functions
- Low confidence: Claims about computational efficiency and scalability to large datasets

## Next Checks

1. Evaluate Dα-KP on datasets with 10,000+ points to verify computational claims and assess whether MCMC sampling remains tractable.

2. Systematically vary α (e.g., 1.0, 1.5, 1.9) and layer depth L across multiple synthetic discontinuous functions to quantify performance sensitivity.

3. Test the method on UCI datasets like Covertype or SUSY (with dimensionality reduction) to validate whether the method maintains its advantages beyond low-dimensional settings.