---
ver: rpa2
title: 'Towards an Understanding of Stepwise Inference in Transformers: A Synthetic
  Graph Navigation Model'
arxiv_id: '2402.07757'
source_url: https://arxiv.org/abs/2402.07757
tags:
- inference
- graph
- stepwise
- node
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work studies stepwise inference in Transformers using a synthetic
  graph navigation task. The key contributions are: (1) A framework for studying stepwise
  inference protocols like chain-of-thought and scratchpads, showing how they can
  be modeled as graph navigation problems; (2) Identification of a diversity-accuracy
  tradeoff in model outputs based on sampling temperature; (3) Demonstration of a
  simplicity bias where models prefer shorter paths; (4) Analysis of learning dynamics
  for different failure modes; (5) Mechanistic analysis revealing the model learns
  an embedding-based distance metric for navigation; (6) Investigation of controllability
  via in-context exemplars, including compositional generalization and primacy bias.'
---

# Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model

## Quick Facts
- arXiv ID: 2402.07757
- Source URL: https://arxiv.org/abs/2402.07757
- Authors: Mikail Khona; Maya Okawa; Jan Hula; Rahul Ramesh; Kento Nishi; Robert Dick; Ekdeep Singh Lubana; Hidenori Tanaka
- Reference count: 40
- One-line primary result: A synthetic graph navigation model that reproduces stepwise inference phenomena and reveals mechanistic insights about transformer reasoning.

## Executive Summary
This work introduces a synthetic graph navigation task to study stepwise inference mechanisms in transformers, modeling chain-of-thought and scratchpad reasoning as pathfinding problems. The authors demonstrate that stepwise inference enables models to "stitch" shorter paths learned during training to construct longer paths at test time, revealing a diversity-accuracy tradeoff controlled by sampling temperature. Through mechanistic analysis, they show the model learns an embedding-based distance metric for navigation and can be controlled via in-context exemplars, exhibiting compositional generalization and primacy bias. The synthetic task successfully reproduces behaviors observed in large language models while providing interpretable insights into stepwise inference mechanisms.

## Method Summary
The authors create a synthetic graph navigation task using directed acyclic graphs (DAGs) where models must traverse from start to goal nodes. They use two DAG types: Bernoulli graphs with random upper-triangular adjacency matrices and hierarchical feedforward structures. The task is framed as next-token prediction where the model outputs path sequences. A 2-layer GPT-style decoder-only Transformer (embedding dim 64, 64 vocab size) is trained with standard cross-entropy loss on pairs of nodes with sampled paths between them. The key innovation is comparing stepwise inference (including intermediate nodes) versus direct inference (only start/goal nodes) to study reasoning gaps and learning dynamics.

## Key Results
- Identifies a diversity-accuracy tradeoff in model outputs based on sampling temperature
- Demonstrates a simplicity bias where models prefer shorter paths between nodes
- Shows stepwise inference bridges the gap when training paths are shorter than test paths
- Reveals the model learns an embedding-based distance metric for navigation
- Demonstrates controllability via in-context exemplars with compositional generalization and primacy bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stepwise inference improves performance by allowing the model to "stitch" shorter paths learned during training to construct longer paths at test time.
- Mechanism: The model learns to decompose navigation into manageable sub-steps, avoiding the need to memorize long path sequences directly.
- Core assumption: The training data contains paths shorter than those required at test time, creating a gap that stepwise inference bridges.
- Evidence anchors:
  - [abstract]: "we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap"
  - [section]: "The stepwise inference gap arises when the training set contains paths that are shorter than the paths required to connect nodes at test time."
  - [corpus]: Weak evidence - no direct mention of stitching or path decomposition in neighbor papers.
- Break condition: If training data already contains paths of sufficient length, the stitching advantage disappears.

### Mechanism 2
- Claim: The model learns an embedding-based distance metric that guides navigation toward the goal node.
- Mechanism: Value embeddings of current and goal nodes are combined, and the next token is selected by maximizing the inner product with token embeddings, effectively computing a learned distance.
- Core assumption: The model can represent graph topology in its embedding space such that inner products approximate path distances.
- Evidence anchors:
  - [abstract]: "mechanistic analysis revealing the model learns an embedding-based distance metric for navigation"
  - [section]: "we find that the inner product of the embedding of vg with the token embeddings of Xnext is negatively correlated with the distance between these two nodes in the ground truth DAG"
  - [corpus]: No direct evidence in neighbor papers about embedding-based distance metrics for graph navigation.
- Break condition: If the graph topology is too complex or non-Euclidean, the learned embedding may not capture meaningful distances.

### Mechanism 3
- Claim: In-context exemplars enable compositional generalization by providing subgoal checkpoints that guide navigation through motif chains.
- Mechanism: Ghost edges between motifs act as subgoals, and the model learns to navigate between these checkpoints using learned path segments.
- Core assumption: The model treats ghost edge nodes as special subgoal markers that structure its navigation plan.
- Evidence anchors:
  - [abstract]: "Investigation of controllability via in-context exemplars, including compositional generalization"
  - [section]: "we see that the attentional patterns used by the model suggest that while navigating across motifs, the model treats nodes across ghost edges as subgoals"
  - [corpus]: Weak evidence - neighbor papers mention graph navigation but not specifically compositional generalization via subgoals.
- Break condition: If exemplars are conflicting or too numerous, the model may fail to identify coherent subgoals.

## Foundational Learning

- Concept: Graph theory fundamentals (DAGs, paths, adjacency matrices)
  - Why needed here: The entire task is built on graph navigation, requiring understanding of graph structures and pathfinding
  - Quick check question: What distinguishes a DAG from other graph types, and why is this important for the navigation task?

- Concept: Transformer architecture (self-attention, positional encoding, causal masking)
  - Why needed here: The model is a GPT-style Transformer, and understanding its components is crucial for interpreting results
  - Quick check question: How does causal masking in the attention layer affect the model's ability to generate paths sequentially?

- Concept: Autoregressive language modeling and cross-entropy loss
  - Why needed here: The training objective is next-token prediction with cross-entropy loss, which shapes how the model learns navigation
  - Quick check question: Why is next-token prediction appropriate for the graph navigation task, and how does it relate to path generation?

## Architecture Onboarding

- Component map: Token embedding → LayerNorm → Self-attention with causal mask → Add residual → LayerNorm → MLP → Add residual → LayerNorm → Output logits
- Critical path: Token embedding → LayerNorm → Self-attention with causal mask → Add residual → LayerNorm → MLP → Add residual → LayerNorm → Output logits
- Design tradeoffs: Simpler 1-layer attention-only model for mechanistic analysis sacrifices expressivity for interpretability
- Failure signatures: Missteps (hallucinations) occur when generated edges don't exist in the graph; planning failures occur when paths don't reach the goal
- First 3 experiments:
  1. Train on hierarchical DAG with stepwise inference vs direct inference to observe the reasoning gap
  2. Vary sampling temperature to explore the diversity-accuracy tradeoff
  3. Add label noise to training data to test robustness of stepwise inference advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the simplicity bias observed in the synthetic graph navigation task translate to real-world reasoning tasks in large language models?
- Basis in paper: [explicit] The paper demonstrates that the model prefers shorter paths between nodes, exhibiting a simplicity bias in the synthetic task.
- Why unresolved: While the synthetic task provides insights, it remains unclear how this bias manifests in complex real-world reasoning scenarios where longer, more detailed reasoning chains might be necessary.
- What evidence would resolve it: Empirical studies comparing the reasoning paths generated by large language models in real-world tasks with and without explicit instructions to elaborate on intermediate steps, measuring both accuracy and completeness of reasoning.

### Open Question 2
- Question: What is the optimal sampling temperature for balancing diversity and accuracy in transformer model outputs?
- Basis in paper: [explicit] The paper identifies a diversity-accuracy tradeoff as sampling temperature varies, but does not determine an optimal temperature.
- Why unresolved: The tradeoff between diversity and accuracy is task-dependent, and the optimal temperature may vary based on the specific application and desired outcomes.
- What evidence would resolve it: Systematic experiments across multiple tasks and domains, measuring both diversity metrics and task-specific accuracy at various sampling temperatures to identify the point of optimal balance.

### Open Question 3
- Question: How do conflicting in-context exemplars affect the model's reasoning process and final output?
- Basis in paper: [explicit] The paper shows that the model exhibits a primacy bias when presented with conflicting exemplars, but does not explore the underlying mechanisms or broader implications.
- Why unresolved: The interaction between conflicting exemplars and the model's reasoning process is complex and may depend on factors such as the number of exemplars, their order, and the nature of the conflict.
- What evidence would resolve it: Controlled experiments varying the number, order, and nature of conflicting exemplars, combined with detailed analysis of the model's attention patterns and reasoning steps to understand how it resolves conflicts.

## Limitations
- The synthetic task may not fully capture the complexity of real-world reasoning tasks where dependencies are non-linear and semantic content is richer
- The exact methodology for balancing path-connected vs non-path-connected node pairs in training data is not fully specified
- The mechanistic analysis of the embedding-based distance metric requires additional validation and specification

## Confidence
- High Confidence: The existence of a diversity-accuracy tradeoff when varying sampling temperature; the simplicity bias toward shorter paths; the improvement of stepwise inference over direct inference when training paths are shorter than test paths
- Medium Confidence: The claim that stepwise inference allows "stitching" of shorter paths to construct longer paths at test time; the effectiveness of in-context exemplars for compositional generalization; the interpretation of attention patterns showing subgoal-based navigation
- Low Confidence: The generalizability of these findings to complex real-world reasoning tasks; the robustness of the embedding-based distance metric across different graph topologies; the practical implications for controlling model outputs through exemplar design

## Next Checks
1. Evaluate the model on graph structures with non-hierarchical dependencies (e.g., cyclic components or more complex DAG patterns) to verify that the embedding-based distance metric and stepwise inference advantages generalize beyond the current synthetic setup.

2. Systematically vary the proportion of path-connected node pairs in training (beyond the stated 20%) to determine how sensitive the diversity-accuracy tradeoff and stepwise inference gap are to training data composition.

3. Replicate the simplified 1-layer attention-only model analysis with full specification of how the algorithm was derived from attention patterns, and verify the claimed embedding-based distance metric across multiple random seeds and graph instantiations.