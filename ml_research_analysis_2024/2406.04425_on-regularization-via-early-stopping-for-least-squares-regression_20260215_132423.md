---
ver: rpa2
title: On Regularization via Early Stopping for Least Squares Regression
arxiv_id: '2406.04425'
source_url: https://arxiv.org/abs/2406.04425
tags:
- stopping
- learning
- early
- rate
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the trajectory of gradient descent for linear
  regression and the effect of early stopping on generalization. The authors provide
  exact formulas for the parameters after k steps of gradient descent for generic
  data and learning rate schedules.
---

# On Regularization via Early Stopping for Least Squares Regression

## Quick Facts
- arXiv ID: 2406.04425
- Source URL: https://arxiv.org/abs/2406.04425
- Reference count: 40
- This paper analyzes the trajectory of gradient descent for linear regression and the effect of early stopping on generalization, providing exact formulas for parameters after k steps and showing that early stopping is equivalent to generalized ridge regularization when initialized at zero.

## Executive Summary
This paper provides a comprehensive theoretical analysis of early stopping as a regularization technique for least squares regression. The authors derive exact formulas for the trajectory of gradient descent parameters and establish a fundamental equivalence between early stopping and generalized ridge regularization. They provide sufficient conditions for when early stopping improves generalization and develop an estimate for the optimal stopping time that generalizes previous work limited to well-conditioned covariances and constant step sizes.

## Method Summary
The authors analyze discrete full batch gradient descent for kernel ridge regression, deriving exact trajectory formulas for parameters after k steps using the ϕ function to track learning rate schedules. They compute the expected excess risk by expressing the risk in the eigenbasis of the sample covariance matrix and analyzing the signal and noise components separately. The optimal stopping time is estimated by balancing these competing effects through first-order approximation. The method handles generic data with arbitrary spectra and various learning rate schedules (constant, polynomial decay, exponential decay, cosine annealing).

## Key Results
- Early stopping is mathematically equivalent to generalized ridge regression when initialized at zero
- Sufficient conditions are provided for when early stopping improves generalization for generic data
- An estimate for optimal stopping time is derived that works for wide class of learning rate schedules
- Empirical results validate accuracy of stopping time estimate across different learning rate schedules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early stopping is mathematically equivalent to generalized ridge regression when initialized at zero.
- Mechanism: The gradient descent trajectory with early stopping produces parameters that match the minimum norm solution of a ridge regression problem with a diagonal regularization matrix in the eigenbasis of the sample covariance.
- Core assumption: The learning rate schedule is benign (i.e., ϕ(k; a) → 0 monotonically as k → ∞).
- Evidence anchors:
  - [abstract] "the early stopped solution β_T is equivalent to the minimum norm solution for a generalized ridge regularized problem"
  - [section 3] Theorem 2 proves this equivalence with a specific D matrix in the ridge regression formulation.
  - [corpus] Related work on "Early-Stopped Mirror Descent" suggests this equivalence is a common theme in optimization literature.
- Break condition: If the learning rate schedule does not satisfy the benign condition, or if initialized away from zero, the equivalence may not hold.

### Mechanism 2
- Claim: Early stopping can improve generalization performance for generic data and learning rate schedules.
- Mechanism: The expected excess risk has two components: one that decreases with training iterations (signal learning) and one that increases (overfitting noise). Early stopping balances these competing effects.
- Core assumption: The conditional distribution of residuals given features has finite second moment.
- Evidence anchors:
  - [abstract] "we prove that early stopping is beneficial for generic data with arbitrary spectrum"
  - [section 4.2] Theorem 5 provides sufficient conditions involving the ϕ function and the noise-to-signal ratio.
  - [corpus] "Dropout Regularization Versus ℓ2-Penalization" suggests noise regularization is a common generalization technique.
- Break condition: If the learning rate schedule decays too quickly (e.g., m > 1 in ηk = η/km) or the noise-to-signal ratio is unfavorable, early stopping may not be beneficial.

### Mechanism 3
- Claim: An estimate for the optimal stopping time can be derived that generalizes previous work limited to well-conditioned covariances and constant step sizes.
- Mechanism: The optimal stopping time balances the reduction in signal error against the increase in overfitting error for each eigendirection, with the total optimal time being a weighted combination.
- Core assumption: The covariance matrices Σ and ˆΣ are simultaneously diagonalizable, or the initialization error has a specific distribution.
- Evidence anchors:
  - [abstract] "provide an estimate for the optimal stopping time for a wide class of learning rate schedules"
  - [section 4.3] The stopping time estimate is derived by setting the derivative of the risk to zero and using a first-order approximation.
  - [corpus] "Distribution-Free Confidence Ellipsoids for Ridge Regression" suggests similar risk-based stopping criteria.
- Break condition: If the covariance matrices are not simultaneously diagonalizable or the initialization error distribution is unknown, the estimate may be inaccurate.

## Foundational Learning

- Concept: Gradient descent dynamics for linear regression
  - Why needed here: The entire analysis is built on understanding how parameters evolve under gradient descent updates.
  - Quick check question: What is the update rule for gradient descent in linear regression with regularization?

- Concept: Eigenvalue decomposition and spectral properties
  - Why needed here: The analysis relies on expressing the gradient descent dynamics in the eigenbasis of the sample covariance matrix.
  - Quick check question: How does the eigendecomposition of X^T X help in analyzing the gradient descent trajectory?

- Concept: Regularization and generalization error
  - Why needed here: The paper compares early stopping to ridge regularization and analyzes when early stopping improves generalization.
  - Quick check question: What is the relationship between the regularization parameter λ and the generalization error in ridge regression?

## Architecture Onboarding

- Component map: Data matrix X -> Response vector y -> Gradient descent update -> ϕ function for learning rates -> Risk function for generalization
- Critical path: Initialize β0 -> Apply gradient descent updates for k steps -> Evaluate risk -> Determine if early stopping is beneficial
- Design tradeoffs: Using discrete gradient descent instead of continuous gradient flow allows for exact formulas but requires careful handling of learning rate schedules
- Failure signatures: If the learning rate schedule is not benign, the equivalence to ridge regression may fail. If the covariance matrices are not simultaneously diagonalizable, the risk estimate may be inaccurate.
- First 3 experiments:
  1. Verify the equivalence between early stopping and ridge regression for a simple case with constant learning rate and well-conditioned data.
  2. Test the optimal stopping time estimate for different learning rate schedules (constant, polynomial decay, exponential decay) on synthetic data with varying condition numbers.
  3. Investigate the effect of initialization (zero vs. random) on the early stopping performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the optimal stopping time be determined exactly for any learning rate schedule?
- Basis in paper: [explicit] The authors provide an estimate for the optimal stopping time but note it is still an approximation.
- Why unresolved: The authors state that differentiating the expected excess risk and finding critical points is challenging due to the risk only being defined for discrete values of k.
- What evidence would resolve it: A closed-form expression for the optimal stopping time that works for arbitrary learning rate schedules and data distributions.

### Open Question 2
- Question: How do the results extend to deep neural networks?
- Basis in paper: [explicit] The authors state that while they have made progress in understanding early stopping for linear models, significant work needs to be done to extend these results to deep networks.
- Why unresolved: The paper focuses on linear models, and the dynamics of gradient descent for neural networks are more complex due to non-linearities and the presence of many parameters.
- What evidence would resolve it: Empirical studies or theoretical results showing how early stopping behaves in deep neural networks with various architectures and learning rate schedules.

### Open Question 3
- Question: What is the relationship between early stopping and grokking?
- Basis in paper: [inferred] The authors mention that grokking is a phenomenon where the model initially overfits and generalizes poorly, then learns to generalize very late during training. They show that for many different learning rate schedules for linear models, grokking will not occur.
- Why unresolved: The authors provide a sufficient condition for when early stopping is beneficial, which implies that grokking will not occur. However, they do not provide a necessary and sufficient condition.
- What evidence would resolve it: A characterization of the learning rate schedules and data distributions for which grokking occurs, and how this relates to the benefits of early stopping.

## Limitations

- Analysis assumes linear regression with squared loss, may not extend to other loss functions
- Results may not apply to deep neural networks due to non-linearities and complex dynamics
- Optimal stopping time estimate requires knowledge of data spectra that may not be available in practice

## Confidence

- Early stopping ≡ ridge regularization equivalence: High
- Sufficient conditions for beneficial early stopping: Medium  
- Optimal stopping time estimation: Medium
- Empirical validation across diverse learning rates: Medium

## Next Checks

1. Test the ridge regularization equivalence for non-zero initialization and non-benign learning rate schedules (e.g., oscillating step sizes) to identify break conditions.

2. Validate the sufficient conditions for beneficial early stopping on real-world datasets with highly skewed spectra and varying noise levels.

3. Empirically assess the accuracy of the optimal stopping time estimate when covariance matrices are not simultaneously diagonalizable or when using different initialization strategies.