---
ver: rpa2
title: Regularizing Hidden States Enables Learning Generalizable Reward Model for
  LLMs
arxiv_id: '2406.10216'
source_url: https://arxiv.org/abs/2406.10216
tags:
- reward
- arxiv
- pulsa
- palab
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving the generalization
  capability of reward models in large language models (LLMs) to mitigate reward over-optimization,
  which occurs when policy optimization excessively optimizes proxy rewards, leading
  to degraded true performance. The authors propose a novel approach called Generalizable
  Reward Model (GRM) that regularizes the hidden states of the reward model using
  text-generation losses, thereby preserving the model's language generation capabilities
  while learning reward predictions.
---

# Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs

## Quick Facts
- arXiv ID: 2406.10216
- Source URL: https://arxiv.org/abs/2406.10216
- Authors: Rui Yang; Ruomeng Ding; Yong Lin; Huan Zhang; Tong Zhang
- Reference count: 40
- Primary result: Regularizing hidden states using text-generation losses improves reward model generalization and mitigates over-optimization in LLMs

## Executive Summary
This paper addresses the challenge of improving the generalization capability of reward models in large language models (LLMs) to mitigate reward over-optimization, which occurs when policy optimization excessively optimizes proxy rewards, leading to degraded true performance. The authors propose a novel approach called Generalizable Reward Model (GRM) that regularizes the hidden states of the reward model using text-generation losses, thereby preserving the model's language generation capabilities while learning reward predictions. GRM employs three types of text-generation regularization: DPO, DPO without reference, and SFT. The method is evaluated on various out-of-distribution (OOD) tasks and RLHF scenarios, demonstrating significant improvements in reward model accuracy and robustness.

## Method Summary
The paper proposes a Generalizable Reward Model (GRM) approach that addresses the challenge of improving reward model generalization in LLMs. GRM regularizes the hidden states of the reward model using text-generation losses, which helps preserve the model's language generation capabilities while learning reward predictions. The method employs three types of text-generation regularization: Direct Preference Optimization (DPO), DPO without reference, and Supervised Fine-Tuning (SFT). The approach is evaluated on various out-of-distribution (OOD) tasks and RLHF scenarios, demonstrating significant improvements in reward model accuracy and robustness.

## Key Results
- GRM outperforms baseline methods on OOD tasks such as HHH-Alignment and MT-Bench
- GRM effectively mitigates over-optimization in both Best-of-n Sampling (BoN) and Proximal Policy Optimization (PPO) settings
- GRM demonstrates robustness against synthetic label noise in the preference dataset

## Why This Works (Mechanism)
The proposed GRM approach works by regularizing the hidden states of the reward model using text-generation losses. This regularization helps preserve the model's language generation capabilities while learning reward predictions, preventing the model from becoming overly specialized to the training data. By incorporating text-generation losses such as DPO, DPO without reference, and SFT, GRM maintains a balance between reward modeling accuracy and the ability to generate coherent text. This balance is crucial for preventing reward over-optimization, where the model excessively optimizes proxy rewards at the expense of true performance.

## Foundational Learning
- **Large Language Models (LLMs)**: Foundation models for natural language processing tasks. Needed to understand the context of reward modeling in LLMs.
- **Reward Modeling**: The process of training models to predict human preferences. Needed to grasp the problem of reward over-optimization.
- **Reinforcement Learning from Human Feedback (RLHF)**: A method for aligning LLM behavior with human preferences. Needed to understand the RLHF scenarios used for evaluation.
- **Out-of-Distribution (OOD) Tasks**: Tasks that differ from the training distribution. Needed to evaluate the generalization capability of reward models.
- **Direct Preference Optimization (DPO)**: A method for optimizing preference-based objectives. Needed to understand one of the text-generation regularization losses used in GRM.
- **Supervised Fine-Tuning (SFT)**: A technique for adapting pre-trained models to specific tasks. Needed to understand another text-generation regularization loss used in GRM.

## Architecture Onboarding

### Component Map
Reward Model -> Hidden State Regularization -> Text-Generation Losses (DPO, DPO without reference, SFT) -> Improved Generalization and Robustness

### Critical Path
1. Train reward model on human preference data
2. Apply hidden state regularization using text-generation losses
3. Evaluate performance on OOD tasks and RLHF scenarios

### Design Tradeoffs
- Balancing reward modeling accuracy and text-generation capabilities through the choice of regularization weight α
- Computational cost of training with additional regularization losses
- Potential impact on model performance for specific downstream tasks

### Failure Signatures
- Overfitting to training data, leading to poor generalization on OOD tasks
- Reward over-optimization in RLHF scenarios, where proxy rewards are excessively optimized
- Degradation of text-generation quality due to excessive regularization

### First Experiments
1. Reproduce GRM results on OOD tasks (HHH-Alignment and MT-Bench) using the Unified-Feedback dataset
2. Evaluate GRM's performance in RLHF scenarios (BoN and PPO) and compare with baseline methods
3. Assess GRM's robustness against synthetic label noise in the preference dataset

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the choice of regularization weight α in GRM affect the balance between reward modeling accuracy and text-generation capabilities?
- Basis in paper: The paper discusses the impact of different α values on GRM performance, noting that setting α to either extreme results in suboptimal out-of-distribution (OOD) performance.
- Why unresolved: The paper does not provide a detailed analysis of how varying α affects the trade-off between reward modeling accuracy and maintaining text-generation capabilities. It only mentions that an appropriate α value yields higher scores.
- What evidence would resolve it: Experiments systematically varying α across a wider range of values and analyzing the corresponding changes in reward modeling accuracy and text-generation performance.

### Open Question 2
- Question: What is the impact of using different base model sizes on the effectiveness of GRM?
- Basis in paper: The paper mentions computational restrictions preventing testing GRM with parameter sizes exceeding 10B, suggesting potential scalability issues.
- Why unresolved: The paper does not explore how GRM performs with different base model sizes, particularly larger models that might benefit more from regularization.
- What evidence would resolve it: Experiments comparing GRM performance across various base model sizes, from small (2B) to very large (100B+ parameters).

### Open Question 3
- Question: How does GRM perform when trained on preference datasets with different levels of noise or inconsistency?
- Basis in paper: The paper evaluates GRM's robustness against 25% synthetic label noise in the preference dataset.
- Why unresolved: While the paper tests GRM with a specific level of synthetic noise, it does not explore performance across a range of noise levels or with real-world noisy datasets.
- What evidence would resolve it: Experiments testing GRM on preference datasets with varying degrees of label noise, including both synthetic and real-world noisy data, to assess performance degradation and robustness.

## Limitations
- The specific implementation details of the text-generation regularization losses and the exact hyperparameters used for training and evaluation are not fully specified, which may impact the reproducibility of the results.
- The study focuses on a limited set of datasets and evaluation scenarios, and further research is needed to validate the generalizability of the approach to other domains and tasks.
- Computational restrictions prevent testing GRM with parameter sizes exceeding 10B, suggesting potential scalability issues for larger models.

## Confidence
- High confidence: The experimental results on OOD tasks and RLHF scenarios demonstrate the effectiveness of the GRM approach in improving reward model generalization and mitigating reward over-optimization.
- Medium confidence: The proposed method of regularizing hidden states using text-generation losses is a novel and promising approach for enhancing reward model robustness.
- Low confidence: The specific implementation details and hyperparameters used in the experiments are not fully specified, which may impact the reproducibility and generalizability of the results.

## Next Checks
1. Implement the GRM approach with the text-generation regularization losses (DPO, DPO without reference, and SFT) and reproduce the experimental results on the OOD tasks and RLHF scenarios using the provided datasets.
2. Conduct ablation studies to assess the individual contributions of each text-generation regularization loss and determine the optimal combination and hyperparameters for the GRM approach.
3. Evaluate the generalizability of the GRM approach by applying it to other domains and tasks beyond the ones used in the study, such as code generation, summarization, or dialogue systems, and compare the results with state-of-the-art reward modeling methods.