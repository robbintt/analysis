---
ver: rpa2
title: On the Curses of Future and History in Future-dependent Value Functions for
  Off-policy Evaluation
arxiv_id: '2402.14703'
source_url: https://arxiv.org/abs/2402.14703
tags:
- coverage
- which
- learning
- assumption
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of off-policy evaluation (OPE)
  in partially observable environments, where traditional approaches suffer from exponential
  dependence on the horizon. The authors build on recent work on future-dependent
  value functions (FDVFs) but identify critical issues with existing boundedness guarantees
  that could still scale exponentially.
---

# On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation

## Quick Facts
- **arXiv ID**: 2402.14703
- **Source URL**: https://arxiv.org/abs/2402.14703
- **Authors**: Yuheng Zhang; Nan Jiang
- **Reference count**: 40
- **Key outcome**: This paper introduces novel coverage assumptions (outcome and belief coverage) that enable polynomial bounds for off-policy evaluation in POMDPs, avoiding the exponential scaling that plagued previous approaches.

## Executive Summary
This paper addresses a fundamental challenge in off-policy evaluation (OPE) for partially observable environments: the curse of horizon. While recent work on future-dependent value functions (FDVFs) offered promise for OPE in POMDPs, the authors identify critical issues with existing boundedness guarantees that could still scale exponentially with horizon length. The paper introduces two novel coverage assumptions tailored to POMDPs - outcome coverage and belief coverage - that enable polynomial bounds on key quantities and avoid exponential scaling. Under these assumptions, the authors establish that FDVFs can be constructed with bounded range and provide finite-sample guarantees for estimation algorithms. The primary result is a fully polynomial estimation guarantee for OPE in POMDPs, with error bounds depending polynomially on horizon H and other problem parameters rather than exponentially.

## Method Summary
The paper develops a framework for off-policy evaluation in POMDPs using future-dependent value functions, addressing the curse of horizon through novel coverage assumptions. The authors propose two key coverage conditions: outcome coverage, which measures how well future observations reveal latent states, and belief coverage, which measures how well complete action-observation histories reveal latent states. These assumptions enable the construction of FDVFs with bounded range, leading to polynomial error bounds. The authors also discover a new algorithm analogous to marginalized importance sampling for MDPs, which achieves complementary properties. The theoretical analysis provides finite-sample guarantees for the estimation algorithms under these coverage conditions, establishing fully polynomial estimation guarantees for OPE in POMDPs.

## Key Results
- Novel coverage assumptions (outcome and belief coverage) enable polynomial bounds for OPE in POMDPs
- FDVFs can be constructed with bounded range under the proposed coverage conditions
- The paper establishes finite-sample guarantees for estimation algorithms
- Error bounds depend polynomially on horizon H and other problem parameters rather than exponentially
- A new algorithm analogous to marginalized importance sampling for MDPs is discovered

## Why This Works (Mechanism)
The paper's approach works by addressing the fundamental challenge of partial observability through carefully designed coverage assumptions. By ensuring that either future observations or complete histories provide sufficient information about latent states, the authors can construct value functions with bounded ranges. This boundedness is crucial because it prevents the exponential accumulation of errors over time that occurs in traditional OPE approaches. The coverage assumptions effectively limit the uncertainty propagation through the value function, enabling polynomial rather than exponential error bounds.

## Foundational Learning
- **Partial Observability in MDPs (POMDPs)**: Models where agents cannot directly observe the true state, requiring reasoning over belief states. Why needed: The paper specifically addresses OPE in partially observable environments. Quick check: Can you explain how POMDPs differ from standard MDPs in terms of state representation?
- **Future-dependent Value Functions (FDVFs)**: Value functions that depend on both current state and future observations. Why needed: FDVFs are the core technical tool used to address partial observability. Quick check: How do FDVFs differ from standard value functions in their dependence on observations?
- **Coverage Assumptions**: Conditions ensuring sufficient exploration or observability of the state space. Why needed: These assumptions are the key innovation enabling polynomial bounds. Quick check: Can you distinguish between outcome coverage and belief coverage?
- **Importance Sampling in OPE**: A technique for estimating values under a target policy using data from a different behavior policy. Why needed: The paper builds on and extends importance sampling techniques. Quick check: What is the main challenge that importance sampling faces in POMDPs?
- **Exponential vs Polynomial Scaling**: The difference between error bounds that grow exponentially versus polynomially with horizon length. Why needed: The paper's main contribution is achieving polynomial rather than exponential bounds. Quick check: Why is polynomial scaling preferable to exponential scaling in OPE?
- **Finite-sample Guarantees**: Theoretical bounds on estimation error that depend on the number of samples used. Why needed: These guarantees are essential for practical algorithm design. Quick check: What factors typically influence finite-sample error bounds in OPE?

## Architecture Onboarding

**Component Map**: Coverage Verification -> FDVF Construction -> Importance Sampling Estimator -> Error Bound Analysis

**Critical Path**: The most critical sequence is: Verify coverage conditions → Construct bounded FDVFs → Apply importance sampling → Obtain polynomial error bounds. If any coverage condition fails, the entire polynomial guarantee fails.

**Design Tradeoffs**: The paper trades off between two coverage assumptions - outcome coverage requires less information (just future observations) but may be harder to verify in practice, while belief coverage requires complete histories but may be more practical to verify. The choice affects both theoretical guarantees and practical implementability.

**Failure Signatures**: 
- If coverage conditions are violated, error bounds revert to exponential scaling
- If observation spaces are too large, computational complexity may become prohibitive
- If the behavior policy has insufficient coverage, importance sampling weights may become unstable

**3 First Experiments**:
1. Verify coverage conditions on a simple POMDP (e.g., Tiger) with varying levels of partial observability
2. Compare the proposed FDVF-based estimator against standard importance sampling on a POMDP with known ground truth
3. Test the sensitivity of error bounds to violations of coverage assumptions by gradually reducing observation quality

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the practical verification of coverage assumptions in real-world POMDPs, the computational complexity of the proposed algorithms in high-dimensional observation spaces, and the performance of the approach on complex POMDPs with rich observation structures.

## Limitations
- The practical feasibility of verifying coverage assumptions in real-world POMDPs remains uncertain
- The computational complexity in high-dimensional observation spaces is not fully characterized
- The approach may be sensitive to approximate verification of coverage conditions
- Performance on real-world POMDPs with complex observation structures is not empirically validated

## Confidence
**High**: Polynomial bounds under coverage assumptions, finite-sample guarantees for estimation algorithms, theoretical comparison with existing FDVF approaches
**Medium**: Practical implementability of the proposed coverage verification methods, computational efficiency in high-dimensional settings
**Low**: Performance on real-world POMDPs with complex observation structures, robustness to approximate coverage condition verification

## Next Checks
1. Empirical validation on benchmark POMDPs (e.g., Tiger, Hallway) comparing the proposed estimator against existing OPE methods across varying levels of partial observability
2. Sensitivity analysis of coverage condition verification methods under approximate or noisy state estimation
3. Computational benchmarking of the proposed algorithm against state-of-the-art OPE methods in terms of runtime and memory requirements as a function of observation space dimensionality