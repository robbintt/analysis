---
ver: rpa2
title: 'HYBRINFOX at CheckThat! 2024 -- Task 1: Enhancing Language Models with Structured
  Information for Check-Worthiness Estimation'
arxiv_id: '2407.03850'
source_url: https://arxiv.org/abs/2407.03850
tags:
- language
- triples
- task
- information
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the HYBRINFOX team's approach for Task 1 of
  CheckThat! 2024, focusing on enhancing language models with structured information
  for check-worthiness estimation.
---

# HYBRINFOX at CheckThat! 2024 -- Task 1: Enhancing Language Models with Structured Information for Check-Worthiness Estimation

## Quick Facts
- arXiv ID: 2407.03850
- Source URL: https://arxiv.org/abs/2407.03850
- Reference count: 29
- Primary result: F1 score of 71.1 for English check-worthiness estimation, ranking 12th out of 27 candidates

## Executive Summary
This paper presents the HYBRINFOX team's approach for Task 1 of CheckThat! 2024, focusing on enhancing language models with structured information for check-worthiness estimation. The team proposes a hybrid model that combines traditional language models like RoBERTa with embeddings derived from triples (subject; predicate; object) extracted from text sentences using Open Information Extraction systems. This method aims to improve the performance of language models alone by incorporating structured factual information from the text. The approach was tested on English, Dutch, and Arabic datasets, with the best results achieved in English, where an F1 score of 71.1 was obtained, ranking 12th out of 27 candidates.

## Method Summary
The proposed approach uses a hybrid model that combines traditional language models like RoBERTa with embeddings derived from triples (subject; predicate; object) extracted from text sentences using Open Information Extraction systems. The model is trained for 5 epochs on the training data, with the best model selected based on the macro-F1 score on the dev-test set. For non-English languages, RoBERTa is replaced with multilingual BERT and OpenIE6 with Multi²OIE in a zero-shot setting.

## Key Results
- Best F1 score of 71.1 achieved for English, ranking 12th out of 27 candidates
- Dutch results: F1 score of 58.9, ranking 8th
- Arabic results: F1 score of 51.9, ranking 10th
- Performance generally lower for non-English languages due to limitations of multilingual models and OpenIE systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining LM embeddings with structured triples improves check-worthiness prediction accuracy
- Mechanism: The model uses RoBERTa to produce semantic embeddings and OpenIE to extract triples, then fuses both representations before classification
- Core assumption: Structured factual information captured in triples provides complementary signals to LM embeddings for detecting check-worthy claims
- Evidence anchors:
  - [abstract] "We propose an approach enriching Language Models such as RoBERTa with embeddings produced by triples (subject ; predicate ; object) extracted from the text sentences."
  - [section 3.1] "To increase the quality of the language model predictions, we propose to use them in conjunction with a small neural network able to leverage structured information from the input text."
  - [corpus] No direct corpus evidence available for this specific mechanism
- Break condition: If OpenIE extracts irrelevant or noisy triples that add no useful signal or introduce noise, the fusion approach may degrade performance below LM-only baseline

### Mechanism 2
- Claim: FastText embeddings of triple components capture lexical information that complements semantic LM embeddings
- Mechanism: Each subject, predicate, and object in extracted triples is encoded using fastText, then processed through a linear layer with ReLU activation
- Core assumption: FastText's subword information captures morphological and lexical patterns useful for identifying factual claims
- Evidence anchors:
  - [section 3.1] "Each part of the triples is encoded using fastText [21], producing 3 vectors of dimension 300 per triple."
  - [section 3.2] Example shows how triples are processed and encoded with fastText before fusion
  - [corpus] No direct corpus evidence available for this specific mechanism
- Break condition: If fastText fails to capture meaningful lexical patterns for a given language, the triple embeddings may not contribute useful information

### Mechanism 3
- Claim: The architecture generalizes across languages using multilingual BERT and zero-shot OpenIE systems
- Mechanism: RoBERTa is replaced with multilingual BERT and OpenIE6 with Multi²OIE for non-English languages
- Core assumption: Multilingual BERT and Multi²OIE can produce reasonable representations for languages beyond their primary training data
- Evidence anchors:
  - [section 3.1] "In the context of Task 1 of the evaluation, for Spanish, Dutch and Arabic, RoBERTa was swapped with a multilingual BERT [12]."
  - [section 3.1] "The OpenIE6 system was replaced with Multi²OIE [22] in a zero-shot setting for non-English languages"
  - [section 4.2] "Performance was generally lower for non-English languages" suggesting this approach has limitations
- Break condition: If multilingual models perform significantly worse than monolingual counterparts for a specific language, the approach may not generalize effectively

## Foundational Learning

- Concept: Open Information Extraction (OpenIE)
  - Why needed here: To extract structured factual triples from text that can provide complementary information to LM embeddings
  - Quick check question: What is the format of information extracted by OpenIE systems?

- Concept: Embedding fusion techniques
  - Why needed here: To combine semantic information from LMs with structured information from triples before classification
  - Quick check question: How are the RoBERTa embeddings and triple embeddings combined in this architecture?

- Concept: Zero-shot learning limitations
  - Why needed here: Understanding why Multi²OIE performs worse than OpenIE6 in zero-shot settings across languages
  - Quick check question: Why might zero-shot OpenIE systems underperform compared to monolingual ones?

## Architecture Onboarding

- Component map: Text → RoBERTa → 768-dim embedding, Text → OpenIE6 → Triples → fastText → Linear → 768-dim embedding, Concatenate → Linear → Sigmoid → Check-worthiness probability
- Critical path: Text input → both LM and OpenIE processing in parallel → embedding fusion → classification
- Design tradeoffs: Using multilingual models enables multi-language support but sacrifices performance compared to monolingual models
- Failure signatures: Decreased performance on non-English languages, noisy triples from OpenIE systems, poor fusion of incompatible embedding spaces
- First 3 experiments:
  1. Compare LM-only vs LM+Triples performance on English development data
  2. Test different OpenIE systems (OpenIE6 vs Multi²OIE) on English data
  3. Evaluate impact of limiting maximum triples per sentence on classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the hybrid model change when using more recent Large Language Models like Mistral or ChatGPT instead of RoBERTa?
- Basis in paper: [explicit] The paper mentions that the approach could be upgraded by using more recent LLMs such as Mistral or ChatGPT, and that these could potentially perform similar tasks with better accuracy.
- Why unresolved: The study used RoBERTa and multilingual BERT, and did not test with more recent LLMs like Mistral or ChatGPT.
- What evidence would resolve it: Comparative performance results using Mistral or ChatGPT instead of RoBERTa on the same datasets.

### Open Question 2
- Question: How would the inclusion of coreference resolution impact the quality and usefulness of the extracted triples for check-worthiness estimation?
- Basis in paper: [explicit] The paper suggests that applying coreference analysis could improve the triples by changing pronouns to the objects they refer to, making the triples more descriptive.
- Why unresolved: The study did not implement coreference resolution in its pipeline, so the impact on performance is unknown.
- What evidence would resolve it: Performance comparison of the model with and without coreference resolution applied to the triples.

### Open Question 3
- Question: What is the impact of filtering triples to include only those containing named entities in the subject and object parts on the model's performance?
- Basis in paper: [explicit] The paper proposes that filtering out triples without named entities could potentially increase the usefulness of the triples for the task.
- Why unresolved: The study did not implement this filtering approach, so the effect on performance is not known.
- What evidence would resolve it: Performance results comparing the model with and without filtering triples based on named entity presence.

## Limitations

- The approach shows significant performance degradation on non-English languages, with F1 scores dropping from 71.1 for English to 58.9 for Dutch and 51.9 for Arabic.
- The methodology lacks detailed specification of critical hyperparameters, particularly learning rates, batch sizes, and optimization settings, which could significantly impact reproducibility.
- The reliance on zero-shot OpenIE systems for non-English languages introduces substantial noise, as evidenced by the authors' own observation that performance was generally lower for non-English languages.

## Confidence

- High Confidence: The core hypothesis that combining LM embeddings with structured triple information can improve check-worthiness prediction accuracy is well-supported by the experimental results showing F1 score improvements over baseline approaches.
- Medium Confidence: The claim that FastText embeddings capture useful lexical patterns for factual claim identification is plausible given FastText's subword capabilities, but lacks direct empirical validation within this work.
- Low Confidence: The generalizability of this approach across diverse languages and domains remains uncertain. The significant performance drop for Dutch and Arabic suggests the method may not scale well beyond English.

## Next Checks

1. **Cross-linguistic ablation study**: Systematically compare LM-only versus LM+Triples performance across all three languages using identical hyperparameters and architectures to quantify the exact contribution of structured information in each language context.

2. **OpenIE system comparison**: Conduct controlled experiments replacing Multi²OIE with fine-tuned monolingual OpenIE systems for Dutch and Arabic to measure the impact of zero-shot versus supervised extraction on downstream check-worthiness performance.

3. **Embedding fusion architecture exploration**: Test alternative fusion strategies beyond simple concatenation, such as attention-based fusion, gated combination, or learned cross-modal interaction mechanisms, to determine if the current approach is optimal or if better integration methods exist.