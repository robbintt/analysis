---
ver: rpa2
title: Axioms for AI Alignment from Human Feedback
arxiv_id: '2405.14758'
source_url: https://arxiv.org/abs/2405.14758
tags:
- ranking
- linear
- will
- which
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning reward functions from
  human feedback in reinforcement learning from human feedback (RLHF). The authors
  frame this as a linear social choice problem, where the goal is to aggregate individual
  rankings over candidates into a collective ranking induced by a linear reward function.
---

# Axioms for AI Alignment from Human Feedback

## Quick Facts
- arXiv ID: 2405.14758
- Source URL: https://arxiv.org/abs/2405.14758
- Reference count: 40
- Primary result: Loss-based RLHF methods fail basic social choice axioms while proposed alternatives offer stronger guarantees

## Executive Summary
This paper bridges reinforcement learning from human feedback (RLHF) with social choice theory, framing reward learning as a linear social choice problem. The authors demonstrate that widely used loss-based methods like Bradley-Terry-Luce fail to satisfy basic axioms such as Pareto optimality and pairwise majority consistency. They propose alternative aggregation rules, particularly Leximax Copeland subject to PO, which satisfy these desirable axioms while maintaining polynomial-time computability. The work provides a theoretical framework for evaluating RLHF methods through the lens of social choice theory.

## Method Summary
The authors analyze linear rank aggregation rules through the framework of social choice theory, identifying desirable axioms including Pareto optimality (PO) and pairwise majority consistency (PMC). They prove that loss-based rules like the Bradley-Terry-Luce model fail to satisfy these axioms, then propose Leximax Copeland subject to PO as an alternative that maintains PO, PMC, majority consistency, and winner monotonicity. The method can be implemented using O(|C|²) linear programs where |C| is the candidate set size.

## Key Results
- Bradley-Terry-Luce and other loss-based RLHF methods fail to satisfy basic social choice axioms like Pareto optimality
- Leximax Copeland subject to PO satisfies PO, pairwise majority consistency, majority consistency, and winner monotonicity
- The proposed method achieves these properties while maintaining polynomial-time computability
- Social choice theory provides a rigorous framework for evaluating and comparing RLHF aggregation methods

## Why This Works (Mechanism)
The paper leverages social choice theory's axiomatic approach to ensure reward functions respect fundamental fairness and consistency principles. By treating preference aggregation as a social choice problem, the authors can apply established impossibility and possibility results to RLHF. The Leximax Copeland method works by maximizing the minimum Copeland score across Pareto-optimal solutions, ensuring robust winner selection that respects both majority preferences and efficiency constraints.

## Foundational Learning
- **Linear reward functions**: Why needed - The paper assumes rewards are linear combinations of features; quick check - Can you explain why linearity simplifies the social choice analysis?
- **Bradley-Terry-Luce model**: Why needed - This is the most common RLHF method analyzed; quick check - What axioms does BTL fail and why?
- **Pareto optimality**: Why needed - Ensures no candidate dominates another; quick check - How does PO differ from majority consistency?
- **Pairwise majority consistency**: Why needed - Captures Condorcet consistency; quick check - What's the relationship between PMC and social choice impossibility results?
- **Copeland score**: Why needed - Used to break ties in the proposed method; quick check - How is Copeland score calculated and why is it useful?

## Architecture Onboarding
**Component map**: Human preferences → Linear aggregation rule → Reward function → RL policy

**Critical path**: The pipeline from human feedback collection through aggregation to final reward function is critical - errors in aggregation directly impact alignment quality.

**Design tradeoffs**: The paper trades some computational complexity (multiple LPs) for stronger axiomatic guarantees compared to loss-based methods.

**Failure signatures**: Violations of Pareto optimality manifest as dominated candidates winning; loss of majority consistency means Condorcet winners can lose.

**First experiments**: 1) Verify BTL fails PO on a simple 3-candidate example; 2) Implement Leximax Copeland on a 4-candidate problem; 3) Compare computational times between BTL and the proposed method on varying candidate set sizes.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of Leximax Copeland subject to PO compare to standard RLHF methods like the Bradley-Terry-Luce model in practice?
- Basis in paper: [inferred] The paper proposes Leximax Copeland subject to PO as an alternative to BTL, claiming it satisfies desirable axioms like Pareto optimality and pairwise majority consistency, which BTL fails.
- Why unresolved: The paper focuses on theoretical properties and axiomatic guarantees, but does not provide empirical comparisons or practical evaluations of the proposed method against existing RLHF techniques.
- What evidence would resolve it: Empirical studies comparing the performance of Leximax Copeland subject to PO and BTL on standard RLHF benchmarks, measuring aspects like reward accuracy, convergence speed, and alignment with human preferences.

### Open Question 2
- Question: What are the computational complexities of implementing Leximax Copeland subject to PO and other proposed methods compared to existing RLHF techniques?
- Basis in paper: [explicit] The paper mentions that Leximax Copeland subject to PO can be implemented in polynomial time using O(|C|2) linear programs, but does not compare this to the computational requirements of other methods.
- Why unresolved: The paper does not provide a detailed analysis of the computational costs of implementing the proposed methods, leaving open questions about their practicality and scalability compared to existing approaches.
- What evidence would resolve it: A thorough computational complexity analysis of Leximax Copeland subject to PO and other proposed methods, including comparisons to standard RLHF techniques like BTL, considering factors such as time complexity, space complexity, and scalability to large datasets.

### Open Question 3
- Question: How robust are the proposed methods to noise and inconsistencies in human feedback?
- Basis in paper: [inferred] The paper focuses on axiomatic properties of aggregation methods, but does not explicitly address how these methods handle noise or inconsistencies in human feedback, which is a common challenge in RLHF.
- Why unresolved: The paper does not provide any analysis or experiments on the robustness of the proposed methods to noise or inconsistencies in human feedback, leaving open questions about their practical applicability in real-world scenarios.
- What evidence would resolve it: Empirical studies evaluating the performance of the proposed methods under various levels of noise and inconsistencies in human feedback, comparing them to existing RLHF techniques and measuring aspects like reward accuracy, stability, and convergence speed.

### Open Question 4
- Question: How do the proposed methods generalize to different feature spaces and reward function parameterizations beyond linear models?
- Basis in paper: [explicit] The paper assumes a linear reward function model, but does not explore how the proposed methods extend to non-linear reward functions or different feature representations.
- Why unresolved: The paper focuses on linear reward functions and does not provide any analysis or experiments on the applicability of the proposed methods to more complex reward function models or different feature spaces.
- What evidence would resolve it: Extensions of the proposed methods to non-linear reward functions and different feature representations, along with empirical evaluations comparing their performance to linear models on various tasks and datasets.

## Limitations
- Assumes complete preference information rather than realistic partial feedback scenarios
- Focuses exclusively on linear reward functions, limiting applicability to more complex reward structures
- Does not empirically validate performance or computational feasibility on real RLHF benchmarks
- Analysis assumes deterministic human feedback without addressing noise or inconsistency

## Confidence
- Theoretical framework validity: High - Builds on well-established social choice theory
- Axiom satisfaction proofs: High - Mathematical proofs provided for all claimed properties
- Practical applicability: Medium - Limited empirical validation and real-world testing
- Computational feasibility: Medium - Complexity bounds provided but not empirically tested

## Next Checks
1. Implement the proposed method on established RLHF benchmarks (e.g., Anthropic's Helpful and Harmless data) to measure practical performance and computational feasibility
2. Conduct sensitivity analysis on partial/incomplete preference information to assess robustness of axiom satisfaction under realistic feedback conditions
3. Compare alignment outcomes (not just theoretical properties) between loss-based and axiom-compliant methods through user studies measuring preference satisfaction and safety metrics