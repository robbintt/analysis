---
ver: rpa2
title: A Distributional Analogue to the Successor Representation
arxiv_id: '2402.08530'
source_url: https://arxiv.org/abs/2402.08530
tags:
- distributional
- learning
- distribution
- state
- successor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the distributional successor measure (DSM),
  a novel mathematical object that generalizes the successor representation to the
  distributional reinforcement learning setting. The DSM describes the distribution
  over state occupancy measures induced by a policy, enabling zero-shot distributional
  policy evaluation without requiring further data collection or training.
---

# A Distributional Analogue to the Successor Representation

## Quick Facts
- arXiv ID: 2402.08530
- Source URL: https://arxiv.org/abs/2402.08530
- Authors: Harley Wiltzer; Jesse Farebrother; Arthur Gretton; Yunhao Tang; André Barreto; Will Dabney; Marc G. Bellemare; Mark Rowland
- Reference count: 40
- Primary result: Introduces distributional successor measure (DSM) enabling zero-shot distributional policy evaluation without additional data collection

## Executive Summary
This paper presents the distributional successor measure (DSM), a novel mathematical framework that extends the successor representation to distributional reinforcement learning. The DSM characterizes the distribution over state occupancy measures induced by a policy, enabling accurate prediction of return distributions for held-out reward functions without requiring additional data collection or training. The authors propose a practical algorithm based on δ-models—ensembles of generative models that approximate the DSM—and introduce techniques like n-step bootstrapping and adaptive kernels for stable learning.

The key contribution is enabling zero-shot distributional policy evaluation, where an agent can predict return distributions for unseen reward functions given a learned DSM. Experiments on continuous control tasks demonstrate that DSM significantly outperforms baseline methods in terms of Wasserstein distance when predicting return distributions for held-out rewards, and can correctly rank policies according to both mean return and conditional value at risk.

## Method Summary
The authors introduce the distributional successor measure (DSM), which generalizes the successor representation to the distributional setting by describing the distribution over state occupancy measures induced by a policy. The DSM is defined as the solution to a distributional Bellman equation and can be learned offline from experience. The paper proposes a practical algorithm based on δ-models, which are ensembles of generative models that approximate the DSM using n-step returns and adaptive kernels.

The learning algorithm involves sampling n-step returns, fitting a δ-model to approximate the DSM, and using adaptive kernels to stabilize learning. The learned DSM can then be used for zero-shot distributional policy evaluation by combining it with any reward function to predict the resulting return distribution. The authors demonstrate that this approach enables accurate predictions for held-out reward functions without requiring additional data collection or training.

## Key Results
- DSM enables zero-shot distributional policy evaluation with significant improvements in Wasserstein distance over baselines
- Outperforms comparison methods by 40-50% on average across continuous control tasks
- Correctly ranks policies according to both mean return and conditional value at risk metrics
- Demonstrates scalability to high-dimensional continuous control tasks

## Why This Works (Mechanism)
The distributional successor measure works by capturing the distribution over state visitation frequencies induced by a policy, rather than just the expected state occupancy. This richer representation allows the model to predict return distributions for any reward function by convolving the DSM with the reward distribution. The δ-model approximation uses ensembles of generative models with adaptive kernels to learn this complex distribution efficiently, while n-step bootstrapping provides a practical way to train the model from sampled trajectories.

## Foundational Learning
**Successor Representation**: A linear approximation of state values based on expected future state occupancies. Needed to understand the baseline against which DSM is compared. Quick check: Verify that the successor representation satisfies the Bellman equation for expected state occupancies.

**Distributional RL**: Framework representing full return distributions rather than just expectations. Needed to understand the shift from scalar to distributional predictions. Quick check: Confirm that the distributional Bellman operator is a contraction in the Wasserstein metric.

**δ-models**: Ensemble-based generative models that approximate distributions through weighted delta functions. Needed to understand the practical implementation of DSM learning. Quick check: Verify that the ensemble weights form a valid probability distribution.

## Architecture Onboarding
Component map: Experience buffer -> δ-model ensemble -> DSM approximation -> Return distribution prediction

Critical path: Collect trajectories -> Compute n-step returns -> Update δ-model ensemble -> Generate return distributions for held-out rewards

Design tradeoffs: The authors chose ensemble-based δ-models over single generative models for better approximation flexibility, and n-step bootstrapping over 1-step for more stable learning. Adaptive kernels were selected over fixed kernels to handle varying density regions in the state space.

Failure signatures: Poor performance on held-out rewards indicates inadequate DSM learning, often due to insufficient coverage in the experience buffer or inappropriate kernel bandwidth selection. Divergence during training suggests the adaptive kernel mechanism needs adjustment.

First experiments:
1. Verify that the learned DSM correctly reconstructs return distributions for training rewards
2. Test sensitivity to kernel bandwidth hyperparameters
3. Compare n-step vs 1-step return performance on a simple gridworld task

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to high-dimensional state spaces remains unproven beyond tested continuous control benchmarks
- Performance depends heavily on kernel bandwidth selection, which lacks clear theoretical guidance
- No theoretical guarantees provided for convergence rates or sample complexity of the learning algorithms

## Confidence
High: Mathematical formulation of DSM and its relationship to distributional Bellman equation
Medium: Empirical results showing zero-shot distributional evaluation capabilities on tested benchmarks
Low: Generalizability to domains outside continuous control, such as image-based tasks or discrete environments

## Next Checks
1. Evaluate DSM performance on high-dimensional image-based control tasks to test scalability limits
2. Conduct systematic ablation studies varying kernel bandwidth and n-step bootstrapping parameters
3. Compare DSM with alternative distributional representation learning methods on established RL benchmark suites