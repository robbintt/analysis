---
ver: rpa2
title: 'SEMF: Supervised Expectation-Maximization Framework for Predicting Intervals'
arxiv_id: '2405.18176'
source_url: https://arxiv.org/abs/2405.18176
tags:
- housing
- data
- semf
- prediction
- missing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SEMF introduces a model-agnostic framework for generating prediction
  intervals in supervised learning by extending the Expectation-Maximization algorithm.
  The method uses latent variable modeling and Monte Carlo sampling to produce uncertainty
  estimates that outperform traditional quantile regression methods on 11 real-world
  tabular datasets.
---

# SEMF: Supervised Expectation-Maximization Framework for Predicting Intervals

## Quick Facts
- arXiv ID: 2405.18176
- Source URL: https://arxiv.org/abs/2405.18176
- Reference count: 40
- Primary result: SEMF produces narrower prediction intervals with maintained coverage probability compared to quantile regression baselines

## Executive Summary
SEMF introduces a model-agnostic framework for generating prediction intervals in supervised learning by extending the Expectation-Maximization algorithm. The method uses latent variable modeling and Monte Carlo sampling to produce uncertainty estimates that outperform traditional quantile regression methods on 11 real-world tabular datasets. SEMF achieves narrower normalized prediction intervals while maintaining coverage probability, particularly for gradient-boosted trees and neural networks that lack inherent uncertainty representation. The framework handles both complete and incomplete data, with empirical results showing improved Coverage-Width Ratio and prediction interval coverage compared to baseline models across diverse data distributions.

## Method Summary
SEMF extends the EM algorithm from unsupervised clustering to supervised prediction by modeling the joint distribution of inputs, latent variables, and output. The framework decomposes p(y|x) into p(y|z)p(z1|x1)p(z2|x2) and iteratively estimates this decomposition via E-step (sampling z) and M-step (optimizing encoder/decoder parameters). Monte Carlo sampling with weighted re-sampling approximates intractable integrals, allowing inference to produce prediction intervals without explicit quantile loss. The method works with existing point predictors and handles missing data through additional imputation models.

## Key Results
- SEMF achieves narrower normalized prediction intervals (NMPIW) while maintaining coverage probability (PICP) compared to quantile regression baselines
- Coverage-Width Ratio (CWR) improvements are statistically significant across 11 real-world datasets
- The framework particularly benefits gradient-boosted trees and neural networks that lack inherent uncertainty representation
- SEMF handles both complete and artificially incomplete data (50% missing) effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SEMF extends EM from unsupervised clustering to supervised prediction by modeling the joint distribution of inputs, latent variables, and output.
- Mechanism: The framework decomposes p(y|x) into p(y|z)p(z1|x1)p(z2|x2) and iteratively estimates this decomposition via E-step (sampling z) and M-step (optimizing encoder/decoder parameters).
- Core assumption: Conditional independence between latent variables z1 and z2 given their respective inputs, and that z contains all information about y from x.
- Evidence anchors:
  - [abstract] "SEMF extends the Expectation-Maximization (EM) algorithm, traditionally used in unsupervised learning, to a supervised context, leveraging latent variable modeling for uncertainty estimation."
  - [section] "Let p(y|x) be the density function of the outcome given the inputs... we assume that p(y|z, x) = p(y|z), that is, z contains all the information of x about y"
- Break condition: If the conditional independence assumption fails (e.g., strong interactions between x1 and x2), the decomposition becomes misspecified and performance degrades.

### Mechanism 2
- Claim: Monte Carlo sampling with weighted re-sampling approximates the intractable E-step integrals.
- Mechanism: Instead of sampling from p'(z|y,x) directly, SEMF samples zr ~ p'(z|x) and weights them by wr = p'(y|zr)/Σ p'(y|zt) to approximate expectations.
- Core assumption: The sampling distribution p'(z|x) covers the posterior p'(z|y,x) sufficiently well that importance weights provide stable estimates.
- Evidence anchors:
  - [section] "Since sampling from p′(z|y, x) can be inefficient, we rather rely on the decomposition p′(z|y, x) = p′(y|z)p′(z|x)/p′(y|x)."
  - [section] "Thus, we sample zr from p′(z|x), r = 1, . . . , R, and, setting wr = p′(y|zr)/P_t p′(y|zt), approximate the right-hand side term"
- Break condition: If the model p'(y|z) is poorly calibrated, importance weights become highly variable, leading to unstable parameter updates.

### Mechanism 3
- Claim: SEMF's latent variable sampling during inference naturally produces prediction intervals without requiring explicit quantile loss.
- Mechanism: Inference samples multiple zr and then multiple y samples per zr to build an empirical distribution from which quantiles are extracted.
- Core assumption: The latent space captures sufficient variability in y, and the sampling procedure adequately explores this space.
- Evidence anchors:
  - [abstract] "Furthermore, without using the quantile (pinball) loss, SEMF allows point predictors, including gradient-boosted trees and neural networks, to be calibrated with conformal quantile regression."
  - [section] "Prediction interval at a given level of certainty α follows as P I = quantile({ˆyr,s}; α/2, 1 − α/2)."
- Break condition: If the latent dimension is too small or the sampling is insufficient (R too low), the resulting intervals will be narrow but poorly calibrated.

## Foundational Learning

- Concept: Expectation-Maximization algorithm
  - Why needed here: SEMF is fundamentally an extension of EM to supervised learning, requiring understanding of E-step/M-step structure.
  - Quick check question: In standard EM, what are the two alternating steps and what does each compute?

- Concept: Importance sampling and weighted Monte Carlo approximation
  - Why needed here: The E-step uses weighted samples from p'(z|x) to approximate expectations under p'(z|y,x) without direct sampling.
  - Quick check question: Why is direct sampling from p'(z|y,x) inefficient, and how do importance weights help?

- Concept: Prediction interval coverage and calibration
  - Why needed here: SEMF aims to produce intervals with specific coverage probabilities, requiring understanding of PICP and related metrics.
  - Quick check question: What is the difference between PICP and MPIW, and why are both needed to evaluate interval quality?

## Architecture Onboarding

- Component map:
  - Encoder models pϕk(zk|xk) for each input source
  - Decoder model pθ(y|z) mapping latent to output
  - Optional missing data model pξ(x1|x2) for incomplete cases
  - Monte Carlo sampling engine for both training and inference
  - Loss aggregator combining weighted log-likelihoods

- Critical path:
  1. Sample z from current pϕ
  2. Compute weights based on pθ
  3. Update pϕ, pθ, and pξ via weighted loss minimization
  4. Iterate until convergence
  5. For inference: sample z, then sample y, collect quantiles

- Design tradeoffs:
  - More latent nodes (mk) → richer representation but higher variance in sampling
  - Larger R (MC samples) → better approximation but slower training/inference
  - Fixed σk → simpler but may under/overestimate uncertainty

- Failure signatures:
  - High variance in weights wr → importance sampling breakdown
  - Poor PICP despite narrow MPIW → latent space not capturing output variability
  - Slow convergence → poor initialization or ill-conditioned loss surface

- First 3 experiments:
  1. Run SEMF on a small synthetic dataset with known latent structure; verify that learned pϕ captures ground truth z
  2. Compare SEMF intervals vs quantile regression on a single dataset; check coverage and width
  3. Test SEMF with 50% missing data; verify that pξ imputes reasonably and downstream performance holds

## Open Questions the Paper Calls Out

- Question: How would SEMF's performance change when applied to non-tabular data such as images or text?
  - Basis in paper: [explicit] The paper states "the distinct pϕ components of the framework could be adapted to process diverse data types—from images and text to tabular datasets" as a future direction
  - Why unresolved: The current work only evaluates SEMF on tabular datasets, leaving its performance on other data types unexplored
  - What evidence would resolve it: Empirical results comparing SEMF's performance on image, text, and mixed-modal datasets against existing methods for those data types

- Question: What would be the impact of replacing the normality assumption with other distributions (uniform, log-normal) in SEMF?
  - Basis in paper: [explicit] The paper acknowledges "The primary limitation of this study was its reliance on the normality assumption" and demonstrates in Appendix F that SEMF can learn non-normal patterns
  - Why unresolved: While Appendix F shows SEMF works with uniform distributions, systematic evaluation across various non-normal distributions is missing
  - What evidence would resolve it: Comparative studies of SEMF with different distributional assumptions across diverse datasets showing performance trade-offs

- Question: How could incorporating dependencies between input features improve SEMF's predictive performance?
  - Basis in paper: [inferred] The paper suggests "exploration of methods to capture and leverage dependencies among input features" as a future direction
  - Why unresolved: The current framework assumes independent latent variables conditioned on their corresponding sources, which may not reflect real-world data structures
  - What evidence would resolve it: Experimental results comparing SEMF with and without feature dependency modeling, demonstrating improvements in coverage probability and interval width

## Limitations
- The framework assumes conditional independence between latent variables, which may not hold in real-world data with strong feature interactions
- Importance sampling can become unstable when the model p'(y|z) is poorly calibrated, leading to high variance in weights
- The fixed σk parameters across all latent dimensions represent a simplification that may not capture heteroscedastic uncertainty patterns

## Confidence

- **High confidence**: SEMF produces narrower prediction intervals while maintaining coverage on benchmark datasets; the EM framework extension is mathematically sound
- **Medium confidence**: Performance improvements are consistent but sample size is limited to 11 datasets; the missing data simulation uses uniform 50% masking which may not reflect real-world patterns
- **Low confidence**: The scalability of Monte Carlo sampling for high-dimensional latent spaces hasn't been thoroughly tested; sensitivity to hyperparameter choices (R, mk, σk) needs systematic evaluation

## Next Checks

1. Test SEMF on a dataset with known heteroscedastic noise patterns to verify σk handling; compare against adaptive uncertainty estimation methods
2. Evaluate SEMF's performance degradation when conditional independence assumption is violated by introducing strong input interactions
3. Measure training/inference time scaling with increasing latent dimension and Monte Carlo sample size to assess practical deployment limits