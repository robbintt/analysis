---
ver: rpa2
title: Reinforcement learning for question answering in programming domain using public
  community scoring as a human feedback
arxiv_id: '2401.10882'
source_url: https://arxiv.org/abs/2401.10882
tags:
- rlhf
- metrics
- reward
- training
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study applies Reinforcement Learning from Human Feedback\
  \ (RLHF) to enhance a small GPT Neo 125M model for programming Community Question\
  \ Answering (CQA) using Stack Overflow scores as rewards. Two reward model training\
  \ strategies\u2014regression and contrastive\u2014were used with Proximal Policy\
  \ Optimization (PPO)."
---

# Reinforcement learning for question answering in programming domain using public community scoring as a human feedback

## Quick Facts
- arXiv ID: 2401.10882
- Source URL: https://arxiv.org/abs/2401.10882
- Authors: Alexey Gorbatovski; Sergey Kovalchuk
- Reference count: 6
- Primary result: RLHF-finetuned GPT Neo 125M achieves performance comparable to GPT Neo 2.7B on programming QA tasks

## Executive Summary
This study applies Reinforcement Learning from Human Feedback (RLHF) to enhance a small GPT Neo 125M model for programming Community Question Answering using Stack Overflow scores as rewards. Two reward model training strategies—regression and contrastive—were used with Proximal Policy Optimization (PPO). The RLHF-finetuned model achieved performance comparable to GPT Neo 2.7B, with improved SacreBLEU and BertScore metrics. However, a notable divergence was found between traditional linguistic metrics (SacreBLEU, BertScore) and the human-preference-based reward models, indicating the need for domain-specific evaluation methods in programming QA tasks.

## Method Summary
The method employs RLHF to fine-tune GPT Neo 125M for programming QA using Stack Overflow community scores as reward signals. The process involves supervised fine-tuning (SFT) on preprocessed Stack Overflow data, followed by training two types of reward models (regression and contrastive) to predict human preferences. These reward models guide Proximal Policy Optimization (PPO) to refine the language model's policy. The study compares the RLHF-finetuned model against the base GPT Neo 125M and a larger GPT Neo 2.7B model using automated metrics like SacreBLEU, BertScore, and Rouge, as well as reward model scores.

## Key Results
- RLHF-finetuned GPT Neo 125M achieves performance comparable to GPT Neo 2.7B on programming QA tasks
- Regression-based reward modeling demonstrated more stable training than contrastive approach during RLHF
- Traditional linguistic metrics (SacreBLEU, BertScore) showed divergence from reward model scores, suggesting limitations in evaluating programming QA responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF improves small GPT Neo 125M's performance in programming CQA by aligning model outputs with human-preference signals encoded in Stack Overflow scores.
- Mechanism: The reward model is trained to predict human preferences (via normalized or contrastive scores), and PPO uses these rewards to fine-tune the policy so that generated answers are more likely to match high-scoring Stack Overflow answers.
- Core assumption: Stack Overflow scores are valid proxies for human preferences in programming QA.
- Evidence anchors:
  - [abstract] "Two distinct reward model training strategies are employed for fine-tuning with Proximal Policy Optimization (PPO)."
  - [section] "We employed reinforcement learning using the TRL and Transformers libraries, with typical RLHF parameters... During reinforcement learning, the training was stable, and the average reward increased when using the reward model based on the regression approach."
  - [corpus] Weak. Corpus neighbors don't directly discuss RLHF with Stack Overflow scores.
- Break condition: If Stack Overflow scores are biased by popularity rather than correctness, the reward model will reinforce popularity bias instead of quality.

### Mechanism 2
- Claim: Regression-based reward modeling outperforms contrastive-based modeling for stable RL training in this domain.
- Mechanism: Regression training uses MSE loss on continuous scores, producing a smoother reward signal that PPO can optimize more reliably than the sparse pairwise comparisons in contrastive loss.
- Core assumption: Continuous reward signals are easier for PPO to optimize than discrete pairwise preferences.
- Evidence anchors:
  - [section] "Both approaches exhibited stability during training, achieving validation accuracies of 93% and 95% respectively... However, training was unstable and did not converge using the reward model based on answer comparisons."
  - [corpus] Weak. No corpus evidence directly comparing regression vs contrastive reward stability.
- Break condition: If the regression model's continuous scores become too coarse or lose ranking information, the policy may not distinguish between good and bad answers effectively.

### Mechanism 3
- Claim: RLHF-finetuned small models achieve performance comparable to much larger base models due to targeted human feedback optimization.
- Mechanism: Supervised fine-tuning learns general patterns, but RLHF with human preference rewards fine-tunes the policy to generate answers that humans actually prefer, closing the performance gap with larger models that have more parameters but no preference alignment.
- Core assumption: Human preference alignment is more important than model size for QA quality in specialized domains.
- Evidence anchors:
  - [abstract] "the improvements in performance achieved through this method are comparable to those of GPT Neo's 2.7B parameter variant."
  - [section] "The RLHF version demonstrated superior performance compared to the SFT model in terms of SacreBLEU and BertScore."
  - [corpus] Weak. Corpus does not discuss model size vs human feedback effects.
- Break condition: If the reward model overfits to the specific Stack Overflow style, the model may fail on out-of-distribution programming questions.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: To align the language model's outputs with human preferences in programming QA rather than just maximizing likelihood.
  - Quick check question: What are the three main stages of RLHF as applied in this study?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO provides stable policy updates when optimizing for the learned reward signal, avoiding destructive policy collapses.
  - Quick check question: Why is PPO preferred over vanilla policy gradient methods in this RLHF setup?

- Concept: Reward model training strategies (regression vs contrastive)
  - Why needed here: Different reward modeling approaches produce different quality signals for the policy to learn from.
  - Quick check question: What is the key difference between regression and contrastive reward model training?

## Architecture Onboarding

- Component map:
  - Stack Overflow dataset -> preprocessing (filter API usage, remove code blocks, sanitize HTML) -> SFT training data
  - GPT Neo 125M base -> Supervised Fine-Tuning -> Reward Model (regression/contrastive) -> PPO fine-tuning -> RLHF model
  - Automated metrics (SacreBLEU, Rouge, BertScore) + reward model scores + human annotation subset + correlation analysis

- Critical path: SFT model training -> reward model training -> PPO fine-tuning -> evaluation and comparison

- Design tradeoffs:
  - Small model (125M) vs large model (2.7B): computational efficiency and fine-tuning stability vs raw capacity
  - Regression vs contrastive rewards: training stability and signal quality vs ranking precision
  - Stack Overflow scores as rewards: domain relevance vs potential popularity bias

- Failure signatures:
  - Reward hacking: model generates superficial answers that score well but lack substance
  - Mode collapse: RLHF model generates very similar answers regardless of question
  - Overfitting: model performs well on Stack Overflow style but poorly on other programming QA

- First 3 experiments:
  1. Train SFT model on preprocessed Stack Overflow data and evaluate baseline metrics
  2. Train regression reward model on SFT generations plus Stack Overflow scores, evaluate reward accuracy
  3. Run PPO fine-tuning with regression reward, compare SacreBLEU/BertScore to SFT and base models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RLHF approach scale to larger models and different programming domains?
- Basis in paper: [explicit] The paper mentions limitations with the small 125M parameter model and suggests future research on larger models and broader data contexts.
- Why unresolved: The study focused on a small GPT Neo model with limited computational resources, restricting exploration of larger models and diverse programming subdomains.
- What evidence would resolve it: Experiments with larger models (e.g., GPT Neo 2.7B) across various programming domains and subdomains, comparing RLHF performance and scalability.

### Open Question 2
- Question: What are the specific limitations of traditional linguistic metrics like BertScore and SacreBLEU in evaluating programming QA responses?
- Basis in paper: [explicit] The paper highlights discrepancies between traditional metrics and reward models, suggesting inadequacies in capturing semantic correctness in programming contexts.
- Why unresolved: The study identified the problem but did not deeply analyze the specific aspects of programming responses that these metrics fail to capture.
- What evidence would resolve it: Detailed analysis of metric failures on programming-specific examples, identifying which semantic or syntactic aspects are missed, and proposing improvements.

### Open Question 3
- Question: How can reward models be designed to better align with human preferences in programming QA tasks?
- Basis in paper: [explicit] The study used two reward model strategies (regression and contrastive) but noted inconsistencies, implying room for improvement in aligning with human preferences.
- Why unresolved: The paper did not explore alternative reward model architectures or training strategies that might better capture nuanced human preferences in programming contexts.
- What evidence would resolve it: Development and testing of alternative reward model designs (e.g., incorporating code-specific features) and comparing their alignment with human judgments on programming QA tasks.

## Limitations

- The study's reliance on Stack Overflow scores as proxy human preferences may introduce popularity bias rather than quality bias
- Traditional linguistic metrics (SacreBLEU, BertScore) show divergence from reward model scores, suggesting they may not adequately capture programming QA quality
- The claim that RLHF-finetuned small models achieve performance comparable to much larger models needs validation across different programming domains

## Confidence

- **High confidence**: The RLHF methodology and training procedures are clearly described and technically sound. The implementation details for SFT, reward model training, and PPO fine-tuning are sufficiently specified for reproduction.
- **Medium confidence**: The reported performance improvements (comparable to GPT Neo 2.7B) are credible but depend heavily on the quality of Stack Overflow scores as rewards and the appropriateness of current evaluation metrics for programming QA.
- **Low confidence**: The assertion that regression reward modeling outperforms contrastive approaches needs more rigorous comparison, as the evidence is limited to training stability rather than final performance differences.

## Next Checks

1. Conduct human evaluation studies comparing RLHF-finetuned answers against both baseline models and Stack Overflow gold answers to validate whether reward model scores align with actual answer quality.

2. Test the RLHF model on out-of-distribution programming questions (different languages, frameworks, or difficulty levels) to assess whether performance gains generalize beyond the Python-focused Stack Overflow dataset.

3. Implement and evaluate domain-specific metrics for programming QA that capture code correctness, API usage accuracy, and practical utility rather than relying solely on linguistic similarity measures.