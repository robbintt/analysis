---
ver: rpa2
title: Efficient Continual Pre-training of LLMs for Low-resource Languages
arxiv_id: '2412.10244'
source_url: https://arxiv.org/abs/2412.10244
tags:
- vocabulary
- corpus
- languages
- best
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work proposes efficient continual pre-training (CPT) of large
  language models (LLMs) for low-resource languages (LRLs) using two methods: (1)
  a global+local scoring-based algorithm to select a small, high-quality subset of
  text from a larger corpus, and (2) a method to augment the LLM vocabulary with language-specific
  tokens. Experiments on the Llama-3 model with nine Indian languages across six scripts
  and three resource levels show that CPT with the selected subset significantly improves
  performance over the vanilla model.'
---

# Efficient Continual Pre-training of LLMs for Low-resource Languages

## Quick Facts
- arXiv ID: 2412.10244
- Source URL: https://arxiv.org/abs/2412.10244
- Authors: Arijit Nag; Soumen Chakrabarti; Animesh Mukherjee; Niloy Ganguly
- Reference count: 10
- Primary result: Efficient continual pre-training with data selection and vocabulary augmentation improves LLM performance for low-resource Indian languages

## Executive Summary
This paper addresses the challenge of adapting large language models to low-resource languages by proposing efficient continual pre-training methods. The authors develop a global+local scoring algorithm to select high-quality text subsets from large corpora and introduce vocabulary augmentation techniques to handle languages with high token fragmentation. Their experiments with Llama-3 on nine Indian languages across six scripts demonstrate significant performance improvements over baseline models, particularly when using carefully selected training data. The work provides practical solutions for extending LLM capabilities to languages with limited training resources.

## Method Summary
The authors propose two complementary approaches for efficient continual pre-training of LLMs on low-resource languages. First, they develop a global+local scoring algorithm that selects a small, high-quality subset of text from a larger corpus by computing perplexity scores and document lengths. Second, they introduce vocabulary augmentation, adding language-specific tokens to the LLM's vocabulary to handle high fragmentation ratios in languages with complex scripts or morphological richness. The methods are evaluated on Llama-3 models fine-tuned for nine Indian languages, testing performance across three resource levels and six different scripts using benchmark tasks including ARC, MMLU, and ANLI.

## Key Results
- CPT with the selected subset significantly improves performance over the vanilla model
- Vocabulary augmentation further improves performance for languages with high fragmentation ratios
- Mixed results for vocabulary augmentation across different languages and tasks

## Why This Works (Mechanism)
The global+local scoring algorithm works by identifying representative documents that capture the linguistic diversity of the low-resource language while maintaining high quality. By selecting a small, focused subset rather than using the entire corpus, the method reduces computational costs while preserving essential language patterns. The vocabulary augmentation addresses the tokenization challenges in morphologically rich or script-complex languages by reducing fragmentation, allowing the model to better represent and process language-specific linguistic structures.

## Foundational Learning
- Perplexity-based scoring: Measures how well a language model predicts text sequences; needed to evaluate document quality and select representative samples
- Token fragmentation: Occurs when languages are split into excessive subword tokens; quick check: calculate ratio of unique tokens to total tokens
- Morphological richness: Languages with complex word formation patterns; needed to understand vocabulary augmentation requirements
- Script diversity: Different writing systems across languages; affects tokenization and model adaptation strategies
- Resource levels: Low, medium, and high availability of training data; determines approach selection and parameter tuning
- Cross-lingual transfer: Knowledge transfer from high-resource to low-resource languages; relevant for understanding baseline performance

## Architecture Onboarding

Component Map: Data Corpus -> Scoring Algorithm -> Selected Subset -> CPT Training -> Fine-tuned LLM -> Benchmark Tasks

Critical Path: The scoring algorithm's selection quality directly determines CPT effectiveness, with vocabulary augmentation as an optional enhancement for high fragmentation languages.

Design Tradeoffs: Small, high-quality subsets reduce computational costs but may miss linguistic diversity; vocabulary augmentation improves representation but increases model complexity and memory requirements.

Failure Signatures: Poor data selection leads to performance degradation; inappropriate vocabulary augmentation can cause overfitting or increased perplexity on out-of-vocabulary words.

First Experiments:
1. Run scoring algorithm with varying perplexity thresholds to observe selection sensitivity
2. Compare CPT performance with random vs. scored subsets on a single low-resource language
3. Test vocabulary augmentation impact on token count reduction and perplexity metrics

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, though several implicit questions remain regarding the generalizability of their approach to non-Indian languages and other NLP tasks beyond the evaluated benchmarks.

## Limitations
- Results are limited to Indian languages across six scripts, limiting generalizability
- Evaluation focuses on only three benchmark tasks (ARC, MMLU, ANLI)
- Vocabulary augmentation shows mixed results and doesn't improve performance for all languages
- Optimal parameter settings for different language-resource scenarios are not explored

## Confidence
- **High confidence**: Claims about the effectiveness of data selection methods for improving CPT performance with limited data
- **Medium confidence**: Claims about vocabulary augmentation benefits for high fragmentation ratio languages
- **Low confidence**: Claims about the generalizability of results to non-Indian languages or other NLP tasks

## Next Checks
1. Evaluate the proposed methods on non-Indian languages from different language families (e.g., African or Southeast Asian languages) to test generalizability
2. Test the approach on additional NLP tasks beyond ARC, MMLU, and ANLI, particularly language-specific tasks like NER or translation
3. Conduct ablation studies to determine optimal scoring parameters for different resource levels and analyze how selection affects model generation quality and bias