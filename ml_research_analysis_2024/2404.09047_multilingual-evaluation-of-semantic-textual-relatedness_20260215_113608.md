---
ver: rpa2
title: Multilingual Evaluation of Semantic Textual Relatedness
arxiv_id: '2404.09047'
source_url: https://arxiv.org/abs/2404.09047
tags:
- semantic
- english
- language
- languages
- hindi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of evaluating semantic textual
  relatedness (STR) across multiple languages, focusing on Marathi, Hindi, Spanish,
  and English. The authors explore three learning paradigms: supervised, unsupervised,
  and cross-lingual, leveraging the SemEval-2024 shared task dataset.'
---

# Multilingual Evaluation of Semantic Textual Relatedness

## Quick Facts
- arXiv ID: 2404.09047
- Source URL: https://arxiv.org/abs/2404.09047
- Reference count: 5
- Primary result: BERT-based models outperform traditional ML models in supervised and unsupervised STR tasks across multiple languages

## Executive Summary
This paper addresses the challenge of evaluating semantic textual relatedness (STR) across multiple languages, focusing on Marathi, Hindi, Spanish, and English. The authors explore three learning paradigms: supervised, unsupervised, and cross-lingual, leveraging the SemEval-2024 shared task dataset. Their methodology employs diverse language models, including BERT-based and sentence transformer models, with preprocessing steps like TF-IDF for feature vectorization. Results demonstrate the effectiveness of their approach, with BERT-based models consistently outperforming other models in supervised and unsupervised learning setups. Cross-lingual models also achieve promising results, showcasing the potential of cross-lingual approaches. The work aims to inspire further research in multilingual STR, particularly for low-resourced languages.

## Method Summary
The authors evaluate STR using the SemEval-2024 shared task dataset (SemRel2024), which contains sentence pairs with relatedness scores across 14 languages. They implement three learning paradigms: supervised (using labeled data), unsupervised (without labels), and cross-lingual (transferring knowledge across languages). The methodology includes preprocessing with TF-IDF vectorization and training diverse models including traditional ML (SVR, XGBoost), BERT-based models, sentence transformers, and language-specific models. Models are evaluated using correlation coefficient as the primary metric alongside F1 score, accuracy, and recall.

## Key Results
- BERT-based models ("BERT-nli" and "Marathi-nli") consistently outperform traditional ML models in both English and Marathi, achieving correlation scores of 0.823 and 0.871 respectively
- Cross-lingual models achieve worthy correlation scores (0.786 for Spanish-to-English and 0.809 for English-to-Hindi), demonstrating the potential of cross-lingual approaches
- The Marathi-specific nli model even surpasses the multilingual BERT performance in Marathi, suggesting the benefit of language-specific models

## Why This Works (Mechanism)

### Mechanism 1
Sentence-transformer models outperform traditional ML models (SVR, XGBoost) in supervised STR tasks because they encode sentences into dense vectors that capture semantic meaning, enabling more accurate similarity scoring than lexical overlap-based methods. This assumes semantic relationships between sentences are better captured in continuous vector space than in sparse TF-IDF features.

### Mechanism 2
Cross-lingual models can effectively transfer semantic understanding from source to target languages because pre-trained multilingual models encode sentences from different languages into a shared semantic space, allowing relatedness judgments across languages without direct supervision. This assumes semantic relatedness is largely language-independent and models can generalize across language boundaries.

### Mechanism 3
Language-specific models outperform multilingual models for low-resource languages because models trained specifically on target language data learn language-specific semantic patterns better than general multilingual models. This assumes low-resource languages benefit more from focused training than from sharing parameters with high-resource languages.

## Foundational Learning

- Concept: TF-IDF and vector space models
  - Why needed here: Provides baseline feature representation for traditional ML models
  - Quick check question: How does TF-IDF handle common words versus rare but meaningful words?

- Concept: Cosine similarity for semantic relatedness
  - Why needed here: Standard metric for comparing sentence embeddings
  - Quick check question: What range does cosine similarity output and what do the extremes represent?

- Concept: Multilingual sentence embeddings
  - Why needed here: Enables cross-lingual STR without parallel data
  - Quick check question: How do multilingual models align semantic spaces across languages?

## Architecture Onboarding

- Component map: Dataset loading → Preprocessing (TF-IDF/vectorization) → Model Training → Evaluation → Analysis
- Critical path: Dataset → Preprocessing → Model Training → Evaluation → Analysis
- Design tradeoffs:
  - Traditional ML: Faster training, interpretable but less accurate
  - BERT-based: More accurate but computationally expensive
  - Cross-lingual: Zero-shot capability but dependent on translation quality
  - Language-specific: Better performance but requires more resources
- Failure signatures:
  - Low correlation scores: Model not capturing semantic relationships
  - High variance across languages: Domain shift or data quality issues
  - Translation errors affecting cross-lingual performance
- First 3 experiments:
  1. Compare traditional ML (SVR, XGBoost) vs BERT-based models on English supervised task
  2. Evaluate Hindi vs English performance in unsupervised setup
  3. Test cross-lingual transfer from English to Hindi with and without translation preprocessing

## Open Questions the Paper Calls Out

### Open Question 1
How do the performance differences between language-specific and multilingual models vary across different language families (e.g., Indo-European vs. Afro-Asiatic)? The paper only tests a limited number of languages from different families, making it difficult to draw broader conclusions about the effectiveness of language-specific models across diverse linguistic backgrounds.

### Open Question 2
What are the specific linguistic features or characteristics that contribute to the semantic relatedness between sentences in low-resource languages? The paper focuses on technical aspects of STR models but doesn't delve into the linguistic nuances that influence semantic relatedness in low-resource languages.

### Open Question 3
How does the performance of STR models change when considering different types of non-linguistic factors, such as sentiment, perspective, and period, in the relatedness judgments? The paper mentions that STR encompasses both linguistic elements and non-linguistic factors but doesn't explore how these factors impact the model's performance.

## Limitations

- The paper's findings are based on a limited set of four languages (Marathi, Hindi, Spanish, and English), which may not generalize to other language families or writing systems
- The performance differences between models could be influenced by dataset characteristics rather than inherent model capabilities
- The cross-lingual experiments rely on translation quality, which is not explicitly evaluated and could introduce confounding factors

## Confidence

- **High confidence**: BERT-based models outperforming traditional ML approaches in supervised tasks (well-established in literature)
- **Medium confidence**: Cross-lingual model effectiveness claims (limited evaluation scope, translation quality uncertainty)
- **Medium confidence**: Language-specific model superiority claims (based on single language comparison)

## Next Checks

1. Test the same models on additional language pairs from the SemEval dataset (e.g., African languages) to assess cross-linguistic generalizability
2. Conduct ablation studies removing the translation component to isolate cross-lingual model performance from translation quality effects
3. Compare performance across different dataset splits to ensure results are not artifacts of particular train/validation/test partitions