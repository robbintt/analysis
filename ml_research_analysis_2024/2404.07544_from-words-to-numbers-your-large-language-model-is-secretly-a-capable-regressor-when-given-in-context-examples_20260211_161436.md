---
ver: rpa2
title: 'From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor
  When Given In-Context Examples'
arxiv_id: '2404.07544'
source_url: https://arxiv.org/abs/2404.07544
tags:
- linear
- regression
- claude
- performance
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs) can perform
  linear and non-linear regression tasks when given in-context examples, without any
  additional training or gradient updates. The authors show that LLMs like GPT-4,
  Claude 3, and DBRX can outperform traditional supervised methods like Random Forest,
  Gradient Boosting, and KNN on various regression datasets.
---

# From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples

## Quick Facts
- arXiv ID: 2404.07544
- Source URL: https://arxiv.org/abs/2404.07544
- Authors: Robert Vacareanu; Vlad-Andrei Negru; Vasile Suciu; Mihai Surdeanu
- Reference count: 40
- Primary result: LLMs can perform regression tasks using in-context examples without additional training, outperforming traditional methods on certain datasets

## Executive Summary
This paper demonstrates that large language models can perform both linear and non-linear regression tasks when provided with in-context examples, without requiring any additional training or gradient updates. The authors show that LLMs like GPT-4, Claude 3, and DBRX can outperform traditional supervised methods such as Random Forest, Gradient Boosting, and KNN on various regression datasets. Notably, Claude 3 outperformed many supervised methods on the challenging Friedman #2 regression dataset, highlighting the potential of LLMs for regression tasks.

## Method Summary
The study investigates how LLMs can perform regression tasks using in-context learning with (input, output) pairs. The authors evaluate multiple LLMs including GPT-4, Claude 3, and DBRX across various regression datasets without any gradient updates or specialized training for regression. They systematically examine how performance scales with the number of in-context examples and compare results against traditional machine learning methods. The approach leverages the LLMs' ability to recognize patterns from provided examples and generalize to new inputs, effectively transforming them into capable regressors through prompt engineering alone.

## Key Results
- LLMs can perform linear and non-linear regression tasks using only in-context examples
- Claude 3 outperformed traditional supervised methods like Random Forest and Gradient Boosting on the challenging Friedman #2 dataset
- Performance of LLMs scales with the number of in-context examples, showing sub-linear regret that approaches optimal decision quality

## Why This Works (Mechanism)
The mechanism behind LLM regression performance relies on the models' inherent ability to recognize patterns and relationships from in-context examples. When provided with (input, output) pairs, LLMs leverage their pre-trained knowledge and pattern-matching capabilities to extrapolate the underlying function mapping inputs to outputs. This works because LLMs have been trained on vast amounts of text data that includes numerical relationships, mathematical expressions, and pattern descriptions, enabling them to understand and reproduce regression-like behavior without explicit training for this task.

## Foundational Learning
- **In-context learning**: Why needed - allows models to learn from examples without parameter updates; Quick check - provide 2-3 (input, output) pairs and verify correct predictions on new inputs
- **Pattern recognition in numerical data**: Why needed - enables extraction of relationships between variables; Quick check - test model on simple linear relationships (y = 2x + 1)
- **Prompt engineering**: Why needed - determines how effectively examples are presented to the model; Quick check - vary prompt formats and measure performance changes
- **Function approximation**: Why needed - core regression task of learning input-output mappings; Quick check - compare predictions against known mathematical functions
- **Sub-linear regret**: Why needed - measures how performance improves with more examples; Quick check - plot regret vs number of examples on a log scale
- **Zero-shot vs few-shot learning**: Why needed - distinguishes between no examples and in-context examples; Quick check - compare performance with 0, 5, and 10 examples

## Architecture Onboarding

**Component map:** User input -> LLM model -> In-context examples -> Output prediction

**Critical path:** Input data → Prompt construction with examples → LLM processing → Numerical output extraction → Performance evaluation

**Design tradeoffs:** Flexibility and no retraining vs computational cost and potential inconsistency; ability to handle diverse data types vs potential sensitivity to prompt engineering

**Failure signatures:** Inconsistent predictions with similar inputs, poor generalization to unseen data distributions, sensitivity to example ordering, numerical output formatting issues

**Three first experiments:**
1. Test linear regression (y = mx + b) with varying numbers of in-context examples to establish baseline performance
2. Compare LLM regression against traditional methods (Random Forest, Linear Regression) on Friedman datasets
3. Evaluate sensitivity to prompt engineering by varying example presentation formats and example selection

## Open Questions the Paper Calls Out
Major uncertainties remain regarding the robustness and generalizability of LLM regression performance. While the paper demonstrates strong results on specific datasets, the performance of LLMs as regressors may be sensitive to prompt engineering, the choice of in-context examples, and dataset characteristics. The scaling behavior with increasing in-context examples, though promising, is evaluated on a limited set of problems. It remains unclear how well these results translate to high-dimensional, noisy, or structured data domains where traditional regression methods are well-established.

## Limitations
- Performance may be sensitive to prompt engineering and choice of in-context examples
- Results are based on limited datasets, raising questions about generalizability to diverse regression problems
- Computational efficiency and cost considerations are not addressed, which could limit practical applicability at scale

## Confidence

**High confidence**: LLMs can perform basic regression tasks using in-context examples
**Medium confidence**: LLMs can outperform traditional ML methods on certain regression problems
**Medium confidence**: Sub-linear regret scaling with in-context examples

## Next Checks
1. Conduct cross-validation across diverse regression datasets (including high-dimensional and noisy data) to assess generalizability
2. Perform statistical significance testing comparing LLM regression performance against classical methods
3. Evaluate computational efficiency (time and cost) of LLM regression versus traditional approaches at scale