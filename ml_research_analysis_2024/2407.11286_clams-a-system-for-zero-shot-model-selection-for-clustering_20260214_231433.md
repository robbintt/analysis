---
ver: rpa2
title: 'CLAMS: A System for Zero-Shot Model Selection for Clustering'
arxiv_id: '2407.11286'
source_url: https://arxiv.org/abs/2407.11286
tags:
- clustering
- dataset
- learning
- optimal
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLAMS is an AutoML system for zero-shot clustering that automates
  model selection using optimal transport-based dataset similarity. It combines a
  diverse search space of 7 clustering algorithms and preprocessing steps with evolutionary
  optimization.
---

# CLAMS: A System for Zero-Shot Model Selection for Clustering

## Quick Facts
- arXiv ID: 2407.11286
- Source URL: https://arxiv.org/abs/2407.11286
- Authors: Prabhant Singh; Pieter Gijsbers; Murat Onur Yildirim; Elif Ceren Gok; Joaquin Vanschoren
- Reference count: 40
- Primary result: CLAMS-OT significantly outperformed default clustering algorithms on 57 OpenML datasets, achieving highest performance in Bayesian Wilcoxon signed-rank tests (p=1.000) and lowest rank in critical difference diagrams

## Executive Summary
CLAMS is an AutoML system for zero-shot clustering that automates model selection using optimal transport-based dataset similarity. It addresses the challenge of selecting clustering algorithms in unsupervised settings where traditional AutoML methods fail due to lack of ground truth labels. The system combines a diverse search space of 7 clustering algorithms and preprocessing steps with evolutionary optimization, using Gromov-Wasserstein distance to find the most similar dataset in a meta-training set and recommending its optimal pipeline for new datasets without labels.

## Method Summary
CLAMS uses a meta-learning approach where it first trains on 57 OpenML clustering datasets to build a knowledge base. For new unlabeled datasets, it computes Gromov-Wasserstein distances to find the most similar dataset in the meta-training set, then applies that dataset's optimal clustering pipeline. The search space includes 7 clustering algorithms (k-Means, Agglomerative Clustering, OPTICS, MiniBatchKMeans, DBSCAN, Mean Shift, BIRCH) combined with preprocessing steps like TableVectorizer and ICA. The system uses evolutionary algorithms for pipeline optimization during meta-training, then employs zero-shot recommendation during testing without requiring ground truth labels.

## Key Results
- CLAMS-OT achieved the highest performance in Bayesian Wilcoxon signed-rank tests with p=1.000 compared to default clustering algorithms
- The system achieved the lowest rank in critical difference diagrams, indicating superior overall performance across 57 OpenML datasets
- Including preprocessing steps in the clustering pipeline represented a novel contribution that improved performance beyond algorithm selection alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal transport distance between datasets enables zero-shot model selection for clustering.
- Mechanism: Gromov-Wasserstein distance measures similarity between datasets with different feature spaces by comparing pairwise distances within each dataset. The most similar dataset in the meta-training set suggests its optimal clustering pipeline for the new dataset.
- Core assumption: If two datasets are similar according to Gromov-Wasserstein distance, their optimal clustering algorithms will also be similar.
- Evidence anchors:
  - [abstract] "CLAMS-OT: A general zero-shot model recommendation system leveraging dataset similarity based on optimal transport (OT)."
  - [section 2.3] "We use optimal transport distance or cost to indicate this distance between datasets."
  - [corpus] Weak - corpus papers focus on general AutoML and LLM approaches, not optimal transport for clustering specifically.
- Break condition: The assumption fails if datasets have similar structures but require fundamentally different clustering approaches due to domain-specific characteristics.

### Mechanism 2
- Claim: Including preprocessing steps in clustering pipelines significantly improves performance over algorithm selection alone.
- Mechanism: CLAMS includes data preprocessing (like TableVectorizer and ICA) as part of the pipeline search space, allowing the system to automatically find optimal data transformations before clustering.
- Core assumption: Optimal preprocessing steps are dataset-dependent and can significantly impact clustering performance.
- Evidence anchors:
  - [section 4.1] "To the best of our knowledge, this is the first time preprocessing steps are included in the clustering pipeline in the context of automated clustering."
  - [section 5] "We employ a preprocessing pipeline before computing distances... The TableVectorizer encodes nominal values..."
  - [corpus] Weak - corpus papers focus on general AutoML frameworks but don't specifically address preprocessing in clustering pipelines.
- Break condition: If preprocessing steps introduce noise or distort the inherent clustering structure of the data.

### Mechanism 3
- Claim: Bayesian Wilcoxon signed-rank tests with ROPE provide more practical comparisons than traditional statistical tests for AutoML systems.
- Mechanism: The ROPE (Region of Practical Equivalence) test defines an interval where performance differences are considered equivalent, providing probabilities that one model is better than another rather than binary accept/reject decisions.
- Core assumption: Small performance differences within the ROPE interval are practically equivalent for real-world applications.
- Evidence anchors:
  - [section 6] "We utilize the Bayesian Wilcoxon signed-rank test, or the region of practical equivalence (ROPE) test... By setting the ROPE value to 1%, we can make more practical comparisons between model performances."
  - [abstract] "We utilize the Bayesian Wilcoxon signed-rank test... to analyze the experimental results."
  - [corpus] Weak - corpus papers don't discuss statistical evaluation methods for AutoML systems.
- Break condition: If the ROPE threshold is set too large (hiding meaningful differences) or too small (creating false distinctions).

## Foundational Learning

- Concept: Gromov-Wasserstein distance
  - Why needed here: It's the core similarity metric that enables zero-shot model selection across datasets with different feature spaces and dimensionalities.
  - Quick check question: How does Gromov-Wasserstein distance differ from standard Wasserstein distance when comparing datasets?

- Concept: Cluster Validity Indices (CVIs)
  - Why needed here: Understanding internal vs external CVIs is crucial for grasping why traditional AutoML fails in unsupervised settings and why CLAMS uses a different approach.
  - Quick check question: Why can't external CVIs be used for model selection on datasets without labels?

- Concept: AutoML pipeline optimization
  - Why needed here: CLAMS searches over both preprocessing steps and clustering algorithms, requiring understanding of how pipeline optimization differs from simple algorithm selection.
  - Quick check question: What are the advantages of optimizing entire pipelines versus individual components in AutoML?

## Architecture Onboarding

- Component map: Preprocessing pipeline → Dataset similarity computation (GW-LR) → Meta-training knowledge base → Pipeline recommendation → Clustering application
- Critical path: New dataset → preprocessing → Gromov-Wasserstein distance computation → find most similar dataset → retrieve optimal pipeline → apply to new dataset
- Design tradeoffs: Using Gromov-Wasserstein distance provides cross-domain similarity but is computationally expensive (30+ minutes); including preprocessing improves performance but increases search space complexity.
- Failure signatures: Poor recommendations when no similar datasets exist in meta-training set; slow performance due to distance computation; suboptimal results if preprocessing distorts clustering structure.
- First 3 experiments:
  1. Test CLAMS-OT on a dataset with known similar counterpart in meta-training to verify basic functionality
  2. Compare performance with and without preprocessing steps to validate their importance
  3. Test on a dataset with no similar counterpart to observe failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the correlation between internal and external clustering validity indices (CVIs) that would justify optimizing only internal metrics during meta-training?
- Basis in paper: [explicit] "there has not been any significant work proving a strong correlation between internal and external CVIs"
- Why unresolved: The paper acknowledges this gap in literature but does not provide empirical evidence demonstrating such correlation for the datasets used in their experiments
- What evidence would resolve it: A comprehensive empirical study showing correlation coefficients between multiple internal and external CVIs across a diverse set of clustering datasets

### Open Question 2
- Question: How does the performance of CLAMS-OT degrade when no similar datasets exist in the meta-training set for a given test dataset?
- Basis in paper: [explicit] "CLMASOT effectiveness depends on the presence of similar datasets to the meta-test dataset within the Dmeta"
- Why unresolved: The paper mentions this limitation but does not provide quantitative analysis of performance degradation across varying degrees of dataset similarity
- What evidence would resolve it: Systematic experiments measuring performance as a function of dataset distance, showing performance curves for datasets with no close matches

### Open Question 3
- Question: Can the computational complexity of optimal transport-based dataset similarity be significantly reduced while maintaining recommendation accuracy?
- Basis in paper: [explicit] "Optimal transport distance is still very expensive to compute" and proposes potential solutions like Wasserstein Embedding Networks
- Why unresolved: The paper identifies this as a future work direction but does not provide experimental validation of alternative approaches
- What evidence would resolve it: Comparative experiments showing accuracy and speed trade-offs between Gromov-Wasserstein, its low-rank approximation, and proposed alternative methods like Wasserstein Embedding Networks

## Limitations
- The approach's effectiveness critically depends on having a sufficiently diverse and representative meta-training dataset; recommendations fail when no similar dataset exists
- Gromov-Wasserstein distance computation is computationally expensive (30+ minutes per comparison), limiting scalability to large numbers of datasets
- The assumption that similar datasets have similar optimal clustering pipelines may not hold across all domains with domain-specific characteristics

## Confidence

**High confidence** in the experimental methodology and statistical analysis (Bayesian Wilcoxon signed-rank tests, ROPE approach, proper critical difference diagrams)

**Medium confidence** in the core mechanism that optimal transport distance enables effective zero-shot model selection, given the strong experimental results but limited theoretical justification

**Medium confidence** in the claim that including preprocessing steps significantly improves performance, as this represents a novel contribution that hasn't been extensively validated outside the study's experimental setup

## Next Checks

1. Test CLAMS-OT on datasets from domains not well-represented in the meta-training set (e.g., specialized biomedical or remote sensing data) to evaluate robustness when similar datasets are absent.

2. Compare GW-LR approximation performance against exact Gromov-Wasserstein distance computation on smaller datasets to quantify the impact of the approximation on recommendation quality.

3. Conduct ablation studies systematically removing preprocessing steps to quantify their contribution versus algorithm selection alone, testing the claim that preprocessing inclusion is a significant innovation.