---
ver: rpa2
title: Revisiting Optimism and Model Complexity in the Wake of Overparameterized Machine
  Learning
arxiv_id: '2410.01259'
source_url: https://arxiv.org/abs/2410.01259
tags:
- degrees
- freedom
- random-x
- prediction
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits the classical statistical concept of degrees
  of freedom in the context of overparameterized machine learning. The authors identify
  a key limitation of classical degrees of freedom - it fails to adequately explain
  the generalization behavior of interpolating models, as any interpolator has exactly
  n degrees of freedom.
---

# Revisiting Optimism and Model Complexity in the Wake of Overparameterized Machine Learning

## Quick Facts
- arXiv ID: 2410.01259
- Source URL: https://arxiv.org/abs/2410.01259
- Reference count: 40
- One-line primary result: The paper proposes random-X degrees of freedom as a complexity measure that can distinguish between interpolating models in overparameterized settings.

## Executive Summary
This paper addresses a fundamental limitation of classical degrees of freedom when applied to modern overparameterized machine learning models. The classical measure fails to distinguish between different interpolating models since any interpolator has exactly n degrees of freedom. To address this, the authors propose an extension to random-X prediction error that averages over a new random sample from the covariate distribution. They introduce two variants: emergent random-X degrees of freedom (incorporating both bias and variance) and intrinsic random-X degrees of freedom (capturing variance alone). The framework is demonstrated on ridge regression, lasso, kNN, and random forests, showing that degrees of freedom typically decreases with regularization strength.

## Method Summary
The method extends classical degrees of freedom by connecting it to random-X prediction error rather than fixed-X prediction error. The authors estimate random-X optimism empirically through resampling and noise injection, then match this optimism to that of a reference model (typically least squares regression on well-specified data). The number of parameters required to match the reference optimism becomes the complexity measure. The framework includes a decomposition into bias, variance, and covariate shift components using Shapley values. The approach is demonstrated through empirical estimation on simulated data for ridge regression, lasso, kNN, and random forests, with theoretical predictions validated against simulations.

## Key Results
- Random-X degrees of freedom provides a complexity measure that distinguishes between interpolating models, whereas classical fixed-X degrees of freedom cannot
- For ridge regression and lasso, random-X degrees of freedom decreases monotonically as regularization strength increases
- The framework allows decomposition of degrees of freedom into bias, variance, and covariate shift components
- Empirical validation shows good agreement between theoretical predictions and simulation results across multiple model classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random-X degrees of freedom provides a complexity measure that distinguishes between interpolating models, whereas classical fixed-X degrees of freedom cannot.
- Mechanism: By measuring prediction error averaged over a new random sample from the covariate distribution, random-X degrees of freedom captures the ability of a model to generalize beyond the training data. Interpolating models can have varying degrees of generalization performance, which is reflected in their random-X degrees of freedom.
- Core assumption: The random-X prediction error is a meaningful measure of generalization performance in modern machine learning settings.
- Evidence anchors:
  - [abstract]: "our extension of degrees of freedom is connected to random-X prediction error (in which prediction error is averaged over a new, random sample from the covariate distribution). The random-X setting more naturally embodies modern machine learning problems, where highly complex models, even those complex enough to interpolate the training data, can still lead to desirable generalization performance under appropriate conditions."
  - [section]: "The underlying limitation of degrees of freedom, as classically defined, is that it is tied to a measure of prediction error which we refer to (following Rosset and Tibshirani 2020) as fixed-X prediction error... A generalizing interpolator epitomizes this difference (Section 2.1): as n → ∞, it has fixed-X excess error converging to the noise level but random-X excess error converging to zero."
- Break condition: If the random-X prediction error does not accurately reflect generalization performance in a given problem setting.

### Mechanism 2
- Claim: The emergent and intrinsic random-X degrees of freedom measures capture different aspects of model complexity.
- Mechanism: Emergent random-X degrees of freedom incorporates both bias and variance components of the error, while intrinsic random-X degrees of freedom captures variance alone. This allows for a more nuanced understanding of how different sources of error contribute to overall model complexity.
- Core assumption: Bias and variance are meaningful components of model error that contribute differently to model complexity.
- Evidence anchors:
  - [abstract]: "We propose two basic versions of random-X degrees of freedom: one to capture both bias and variance components of the error, and another based on variance alone."
  - [section]: "Therefore, when we match the observed optimism to the reference one in (18), we are comparing optrp(pf)—which is generically comprised of both bias and variance, to optrp(pfrefd)—which is made up of variance alone. This is intentional... Alternatively, we might want to match variance to variance in determining degrees of freedom, i.e., we might want to exclude bias effects in calculating the random-X optimism of the given model pf."
- Break condition: If bias and variance do not contribute meaningfully to model complexity in a given problem setting.

### Mechanism 3
- Claim: The random-X degrees of freedom can be decomposed to quantify the contribution of various components like bias and covariate shift.
- Mechanism: By considering different scenarios with and without signal presence and covariate shift, the random-X degrees of freedom can be broken down into constituent parts, allowing for a more detailed understanding of the sources of model complexity.
- Core assumption: Covariate shift is a meaningful source of error that contributes to model complexity.
- Evidence anchors:
  - [section]: "In order to attribute an amount of degrees of freedom to each source of error—bias and covariate shift—we use a definition akin to Shapley values... Note that by construction (which is also a Shapley axiom called 'efficiency'), we have: dfrp(pf) = dfirp(pf) + ϕsig(pf) + ϕcov(pf)."
- Break condition: If covariate shift is not a meaningful source of error in a given problem setting.

## Foundational Learning

- Concept: Fixed-X and random-X prediction error
  - Why needed here: Understanding the difference between these two measures of prediction error is crucial for grasping the motivation behind the proposed random-X degrees of freedom.
  - Quick check question: What is the key difference between fixed-X and random-X prediction error, and why is random-X prediction error more relevant in modern machine learning settings?
- Concept: Optimism and degrees of freedom
  - Why needed here: The relationship between optimism and degrees of freedom is central to the proposed random-X extension, as it provides the framework for defining complexity.
  - Quick check question: How is optimism related to degrees of freedom, and how does this relationship change when moving from fixed-X to random-X prediction error?
- Concept: Linear smoothers and their properties
  - Why needed here: Linear smoothers provide a concrete example for understanding the behavior of random-X degrees of freedom and its relationship to other notions of model complexity.
  - Quick check question: What are the key properties of linear smoothers that make them useful for understanding random-X degrees of freedom?

## Architecture Onboarding

- Component map:
  - Random-X optimism estimation -> Reference model construction -> Degrees of freedom calculation
- Critical path: Random-X optimism estimation → Reference model construction → Degrees of freedom calculation
- Design tradeoffs: The choice of reference model and the method for estimating random-X optimism can impact the behavior and interpretation of the resulting degrees of freedom measure.
- Failure signatures: If the random-X optimism estimation is inaccurate, or if the reference model is poorly chosen, the resulting degrees of freedom measure may not accurately reflect model complexity.
- First 3 experiments:
  1. Implement the random-X degrees of freedom calculation for a simple linear regression model on a synthetic dataset, and compare the results to classical fixed-X degrees of freedom.
  2. Extend the random-X degrees of freedom calculation to a nonlinear smoother, such as a kernel ridge regression model, and examine how the measure behaves as the regularization parameter varies.
  3. Investigate the decomposition of random-X degrees of freedom into bias and variance components for a linear regression model under covariate shift, and interpret the results in terms of model complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework of random-X degrees of freedom be extended to classification problems beyond regression?
- Basis in paper: [inferred] The paper focuses on regression with squared error, but the authors mention this does not reflect a fundamental restriction and suggest extending the framework to classification as an interesting direction for follow-up work.
- Why unresolved: The paper does not provide any theoretical results or empirical evidence for applying the random-X degrees of freedom framework to classification problems. The extension would require defining appropriate metrics and reference models for classification settings.
- What evidence would resolve it: Developing a theoretical framework for random-X degrees of freedom in classification, including appropriate metrics and reference models, along with empirical validation on classification tasks showing the utility of this measure.

### Open Question 2
- Question: What is the relationship between random-X degrees of freedom and other complexity measures like VC dimension and Rademacher complexity in the context of overparameterized models?
- Basis in paper: [explicit] The authors discuss the difference between degrees of freedom and VC/Rademacher complexity, noting that the latter measures apply to classes of models while degrees of freedom applies to fitted models, but do not explore their relationship in detail.
- Why unresolved: The paper establishes the concept of random-X degrees of freedom but does not compare its behavior to other complexity measures in overparameterized settings. Understanding this relationship could provide insights into generalization in modern machine learning.
- What evidence would resolve it: Empirical and theoretical studies comparing random-X degrees of freedom with VC dimension and Rademacher complexity across various overparameterized models and datasets, analyzing their predictive power for generalization.

### Open Question 3
- Question: How does the choice of reference model (beyond least squares regression) affect the random-X degrees of freedom and its interpretation?
- Basis in paper: [explicit] The authors choose least squares regression as the reference model for defining random-X degrees of freedom, but acknowledge that different choices of metrics and reference models give rise to different notions of complexity.
- Why unresolved: The paper does not explore how alternative reference models might impact the resulting complexity measure or its interpretability. Different reference models might be more appropriate for certain types of prediction problems or model classes.
- What evidence would resolve it: Theoretical analysis and empirical studies comparing random-X degrees of freedom using different reference models (e.g., ridge regression, lasso) across various prediction tasks, evaluating their relative merits and interpretability.

## Limitations
- The empirical estimation of random-X optimism relies on resampling, which introduces variance that may affect reliability, particularly for smaller sample sizes
- The framework is demonstrated on a limited set of model classes (ridge regression, lasso, kNN, random forests) and its behavior for other model classes remains unexplored
- The computational cost of empirical estimation could be prohibitive for large-scale problems or complex models, though this is not discussed in the paper

## Confidence

**High confidence**: The theoretical framework connecting optimism to degrees of freedom is well-established and the extension from fixed-X to random-X prediction error is mathematically sound. The behavior of random-X degrees of freedom for ridge regression and lasso (decreasing with regularization) aligns with classical theory.

**Medium confidence**: The empirical demonstrations on simulated data show the proposed measures behave as expected, but the sample sizes and number of repetitions are not specified. The decomposition of degrees of freedom into bias, variance, and covariate shift components relies on asymptotic approximations that may not hold in finite samples.

**Low confidence**: The application to complex models like random forests and the interpretation of their random-X degrees of freedom is less clear, as the theoretical analysis is limited. The relationship between random-X degrees of freedom and other complexity measures in the overparameterized regime requires further investigation.

## Next Checks

1. **Reproduce the ridge regression and lasso examples**: Implement the empirical estimation of random-X degrees of freedom for ridge regression and lasso on simulated data with varying aspect ratios (p/n) and regularization parameters. Compare the empirical estimates to the theoretical predictions provided in the paper.

2. **Assess the impact of sample size**: Investigate how the accuracy and stability of the empirical random-X degrees of freedom estimates vary with sample size. Determine the minimum sample size required for reliable estimation and quantify the estimation variance through repeated simulations.

3. **Test on a deep learning model**: Apply the random-X degrees of freedom framework to a simple neural network (e.g., a single hidden layer network). Examine how the measure behaves as the network width increases and compare the results to the behavior observed for kernel methods and other classical models.