---
ver: rpa2
title: Autoformalization of Game Descriptions using Large Language Models
arxiv_id: '2409.12300'
source_url: https://arxiv.org/abs/2409.12300
tags:
- game
- language
- player
- games
- situation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework for autoformalization of game-theoretic
  scenarios, translating natural language descriptions into formal logic representations
  suitable for formal solvers. The approach uses one-shot prompting with GPT-4o and
  a Prolog solver to refine the code based on syntactic correctness feedback.
---

# Autoformalization of Game Descriptions using Large Language Models

## Quick Facts
- **arXiv ID**: 2409.12300
- **Source URL**: https://arxiv.org/abs/2409.12300
- **Reference count**: 8
- **Key outcome**: Framework achieves 98% syntactic correctness and 88% semantic correctness translating natural language game descriptions to formal Prolog representations

## Executive Summary
This paper presents a framework for autoformalizing game-theoretic scenarios described in natural language into formal logic representations using large language models. The approach employs one-shot prompting with GPT-4o to generate Prolog code representing game rules and payoff matrices, followed by iterative refinement using a Prolog solver that provides syntactic feedback. The system was evaluated on 110 natural language descriptions of 2-player simultaneous games, achieving high syntactic correctness (98%) and substantial semantic correctness (88%), demonstrating the potential of LLMs to bridge natural language and formal game theory reasoning.

## Method Summary
The framework uses one-shot prompting with GPT-4o, providing the LLM with game-independent Prolog predicates, an example Prisoner's Dilemma description with its formal representation, and the target natural language game description. The LLM generates game-specific Prolog predicates which are validated by a Prolog solver. If syntax errors are detected, the solver's error trace is fed back to the LLM with instructions to correct the code, iterating up to 5 times. The approach separates game-independent structure (initial state, legal moves, effects) from game-specific content (payoffs, outcomes), enabling focused generation of formal specifications.

## Key Results
- Achieved 98% syntactic correctness (108/110 cases) through iterative correction with Prolog solver feedback
- Achieved 88% semantic correctness overall, with perfect accuracy for standard examples and 83-90% for non-standard variants
- One-shot prompting with example significantly outperformed zero-shot prompting for complex game descriptions
- Successfully handled both numerical and non-numerical payoff representations in natural language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: One-shot prompting with GPT-4o enables accurate translation of natural language game descriptions into formal Prolog representations when provided with an example of the target format.
- Mechanism: The LLM leverages the example game-specific predicates as a template, applying the structure to new descriptions while adapting the payoff matrix and game rules to match the new scenario.
- Core assumption: The LLM can generalize from a single example game description to structurally similar games with different payoffs and rules.
- Evidence anchors:
  - [abstract]: "Our approach utilizes one-shot prompting and a solver that provides feedback on syntactic correctness to allow LLMs to refine the code."
  - [section]: "The prompt consists of game-independent predicates (Γ), an example: a natural language description of Prisoner's Dilemma (NLPD) and game-specific predicates for PD (ξPD), and a natural language description of a game to be translated (NLNG)."
  - [corpus]: Weak evidence - corpus shows related work on autoformalization but doesn't specifically validate one-shot prompting with game theory examples.
- Break condition: If the new game structure significantly deviates from the example (e.g., sequential vs simultaneous, different number of actions), the LLM may fail to generate correct predicates.

### Mechanism 2
- Claim: The iterative correction loop using Prolog solver feedback enables the LLM to achieve high syntactic correctness (98%) despite initial errors.
- Mechanism: When generated predicates contain syntax errors, the Prolog solver provides an error trace that is fed back to the LLM with instructions to correct the code, allowing multiple refinement attempts.
- Core assumption: The LLM can interpret and act on Prolog error messages to fix syntactic issues in subsequent attempts.
- Evidence anchors:
  - [abstract]: "achieving 98% syntactic correctness and 88% semantic correctness."
  - [section]: "Once the game-specific predicates (ξNG) are generated, they are validated for syntactic correctness using a Prolog solver. If the generated predicates contain syntax errors, the LLM is re-prompted with the solver's error trace and additional instructions for correcting the Prolog code."
  - [corpus]: Moderate evidence - corpus includes work on combining LLMs with formal solvers for reasoning tasks, supporting the general approach.
- Break condition: If the solver feedback is insufficient or the LLM cannot interpret the error messages correctly, syntactic correctness will not improve.

### Mechanism 3
- Claim: The separation of game-independent and game-dependent predicates enables the LLM to focus on generating only the specific game rules while reusing the general game structure.
- Mechanism: The game-independent predicates define the general framework for any game in extensive form, while the LLM only needs to generate the game-specific predicates that define the initial state, legal moves, payoffs, etc. for each particular game.
- Core assumption: The LLM can distinguish between the fixed game-independent structure and the variable game-dependent components when generating formal specifications.
- Evidence anchors:
  - [abstract]: "Our approach utilizes one-shot prompting and a solver that provides feedback on syntactic correctness to allow LLMs to refine the code."
  - [section]: "Our solver consists of a game-independent part specifying the rules of any game in extensive form, a game-dependent part expressing the rules of a specific game, and a set of auxiliary predicates used on top of these representations to support processing a game."
  - [corpus]: Weak evidence - while corpus shows autoformalization approaches, it doesn't specifically validate the separation of game-independent and dependent components.
- Break condition: If the LLM confuses game-independent and dependent components, it may generate incorrect formal specifications.

## Foundational Learning

- Concept: Situation Calculus and Prolog syntax
  - Why needed here: The framework uses Situation Calculus to represent game states and Prolog as the formal language for specifying game rules. Understanding these is essential for interpreting the generated code and debugging issues.
  - Quick check question: What is the difference between the `do(M,S)` term and the `effect(F,M,S)` predicate in the Situation Calculus representation?

- Concept: Game Theory fundamentals (normal form games, payoff matrices, strategic interactions)
  - Why needed here: The system translates natural language descriptions of game-theoretic scenarios into formal representations. Understanding game theory concepts is necessary to verify semantic correctness and interpret the results.
  - Quick check question: How would you represent the payoff matrix for a Prisoner's Dilemma game using the `payoff/4` predicate format shown in the paper?

- Concept: Large Language Model prompting strategies (zero-shot vs one-shot vs few-shot)
  - Why needed here: The paper compares zero-shot and one-shot prompting approaches. Understanding these techniques is important for configuring the system and interpreting performance differences.
  - Quick check question: What is the key difference between zero-shot and one-shot prompting, and why did one-shot prompting perform better in this study?

## Architecture Onboarding

- Component map:
  Natural Language Input → LLM with Prompt (game-independent predicates + example + new description) → Generated Prolog Code → Prolog Solver (syntax validation + error trace) → Corrected Code (iterative) → Formal Game Specification
  Supporting components: Dataset of game descriptions, evaluation metrics (syntactic/semantic correctness)

- Critical path: The most critical path is the LLM generation → Prolog solver validation loop. Any failure in this loop (e.g., LLM cannot generate syntactically correct code even after multiple attempts) will prevent successful autoformalization.

- Design tradeoffs:
  - One-shot vs few-shot vs zero-shot: One-shot provided good results but required an example. Few-shot might improve performance but requires more examples. Zero-shot failed for complex game descriptions.
  - Prolog vs other formal languages: Prolog was chosen for its expressiveness in representing game rules, but other languages might offer different tradeoffs.
  - Iterative correction vs direct generation: The iterative approach with solver feedback achieved high syntactic correctness but required multiple LLM calls.

- Failure signatures:
  - Syntactic errors that persist despite solver feedback indicate the LLM cannot interpret error messages correctly
  - Semantic errors in payoff assignment suggest the LLM misunderstands the game description
  - Failure to generalize to different game structures indicates the example was too specific

- First 3 experiments:
  1. Test one-shot prompting with Prisoner's Dilemma example on a standard description to verify basic functionality
  2. Test the iterative correction loop by intentionally introducing syntax errors and verifying the solver catches them and the LLM corrects them
  3. Test generalization by using the Prisoner's Dilemma example to generate a formal specification for Rock-Paper-Scissors and comparing against the ground truth

## Open Questions the Paper Calls Out

- Can LLMs accurately translate natural language descriptions of games with more than two players or non-simultaneous moves?
- What is the optimal approach for improving semantic correctness when LLM translations contain errors in payoff assignments?
- How does the framework's performance scale when evaluating games with more complex state spaces and longer action sequences?

## Limitations

- Limited to 2-player simultaneous games, excluding sequential games, multi-player games, and games with continuous strategy spaces
- Semantic correctness evaluation was manually inspected, limiting scalability to more complex games
- Dataset consisted of synthetic real-world examples generated by LLM rather than naturally occurring game descriptions

## Confidence

**High Confidence**:
- The iterative correction loop with Prolog solver feedback effectively improves syntactic correctness
- One-shot prompting significantly outperforms zero-shot prompting for this task
- The separation of game-independent and game-dependent predicates provides a sound architectural foundation

**Medium Confidence**:
- The framework can handle both numerical and non-numerical payoff representations
- The approach generalizes from the Prisoner's Dilemma example to other classic games
- Semantic correctness rates are consistent across different game types

**Low Confidence**:
- Performance on games with structures substantially different from the example
- Scalability to larger, more complex game descriptions
- Robustness across different LLMs or prompting strategies

## Next Checks

1. **Cross-Paradigm Generalization Test**: Apply the framework to a sequential game (e.g., Centipede game) using the same one-shot prompting approach. This will test whether the LLM can handle game structures that differ fundamentally from the simultaneous-move examples used in the original study.

2. **Prompt Sensitivity Analysis**: Systematically vary the one-shot prompt by changing the example game, the formatting of game-independent predicates, and the instructions given to the LLM. Measure how these variations affect both syntactic and semantic correctness rates to identify which prompt elements are most critical.

3. **Solver Feedback Interpretation Test**: Create a set of Prolog error messages with varying complexity and test whether the LLM can correctly interpret and fix errors across different types of syntax issues. This will validate whether the iterative correction mechanism works for the full range of potential errors, not just those encountered in the original dataset.