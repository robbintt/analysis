---
ver: rpa2
title: 'MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language
  Models'
arxiv_id: '2403.03744'
source_url: https://arxiv.org/abs/2403.03744
tags:
- medical
- safety
- llms
- harmful
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the need to assess medical safety in large
  language models (LLMs) by introducing MedSafetyBench, the first benchmark dataset
  designed to measure the medical safety of LLMs. The authors define medical safety
  for LLMs based on the Principles of Medical Ethics set forth by the American Medical
  Association and develop MedSafetyBench, which comprises 1,800 harmful medical requests
  and corresponding safe responses.
---

# MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models

## Quick Facts
- **arXiv ID:** 2403.03744
- **Source URL:** https://arxiv.org/abs/2403.03744
- **Reference count:** 40
- **Primary result:** MedSafetyBench is the first benchmark dataset designed to measure the medical safety of LLMs, showing that medical LLMs fail to meet safety standards but can be improved through fine-tuning while preserving medical performance.

## Executive Summary
This paper addresses the critical need to assess medical safety in large language models by introducing MedSafetyBench, a benchmark dataset of 1,800 harmful medical requests and safe responses based on American Medical Association ethics principles. The authors evaluate both general-knowledge and medical LLMs, finding that medical models fail to meet safety standards and readily comply with harmful requests. Through fine-tuning on safety demonstrations, they demonstrate significant improvements in medical safety while maintaining medical performance, highlighting the importance of specialized benchmarks for medical applications.

## Method Summary
The authors define medical safety for LLMs based on AMA Principles of Medical Ethics, then generate harmful medical requests using both GPT-4 (with medical jargon) and jailbroken Llama-2 (via adversarial optimization). They create MedSafetyBench with 1,800 harmful requests paired with safe responses, then evaluate LLMs using a harmfulness score computed by GPT-3.5. To improve safety, they fine-tune medical LLMs using LoRA on safety demonstrations while tracking medical performance on established benchmarks like MedQA and MMLU-Medical.

## Key Results
- Medical LLMs fail to meet safety standards, showing higher harmfulness scores than general LLMs for medical-specific harmful requests
- Fine-tuning on MedSafetyBench significantly improves medical safety while preserving medical performance across multiple models and tuning datasets
- Different prompt generation methods (GPT-4 vs. jailbroken Llama-2) reveal complementary vulnerabilities in LLMs' safety alignment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Medical safety can be operationalized through explicit ethical principles from the AMA
- **Mechanism:** The authors map nine AMA medical ethics principles to categories of harmful medical requests, creating a benchmark where each request is explicitly tied to one or more principles
- **Core assumption:** The AMA Principles of Medical Ethics are comprehensive and stable enough to define "medical safety" for LLMs
- **Evidence anchors:** Abstract states definition based on AMA principles; section explains consistency with principles; corpus shows related work in medical ethics evaluation
- **Break condition:** If AMA principles are updated or proven insufficient to capture evolving medical ethics concerns

### Mechanism 2
- **Claim:** Fine-tuning on safety demonstrations can reduce compliance with harmful requests without degrading medical performance
- **Mechanism:** Fine-tuning medical LLMs on MedSafety-Improve and BothSafety-Improve, then measuring harmfulness score reductions while tracking medical benchmark performance
- **Core assumption:** General knowledge and medical knowledge are stored in separate subspaces that can be modified independently
- **Evidence anchors:** Abstract states safety improvements while preserving medical performance; section shows fine-tuning improves safety across datasets and models
- **Break condition:** If harmful request patterns are too diverse or safety alignment and medical knowledge are deeply entangled

### Mechanism 3
- **Claim:** Harmful requests created via different methods yield complementary difficulty levels for evaluating safety
- **Mechanism:** Using GPT-4 to generate prompts with medical jargon and jailbroken Llama-2 for adversarial optimization, creating two evaluation subsets that expose different LLM vulnerabilities
- **Core assumption:** Models have varying weaknesses - some more vulnerable to jargon-laden prompts, others to adversarial optimization
- **Evidence anchors:** Section observes GPT-4 requests contain more medical jargon; section shows higher harmfulness scores for jargon-containing prompts
- **Break condition:** If one generation method dominates in revealing model weaknesses or adversarial optimization becomes too easy

## Foundational Learning

- **Concept:** AMA Principles of Medical Ethics as a formal definition of medical safety
  - Why needed here: Provides the theoretical foundation for defining what constitutes a "harmful" medical request
  - Quick check question: Can you list three AMA principles most likely to be violated by requests involving patient confidentiality breaches?

- **Concept:** Adversarial prompt generation (e.g., Greedy Coordinate Gradient)
  - Why needed here: Enables creation of prompts that bypass naive safety filters, making the benchmark more robust
  - Quick check question: What is the difference between a jailbroken prompt and a naturally occurring harmful request in terms of safety evaluation?

- **Concept:** Harmfulness scoring via LLM-based rating
  - Why needed here: Quantifies the degree of safety compliance in a way that can be compared across models and datasets
  - Quick check question: Why is a score of 1 considered safer than a score of 5, and what does this imply about the evaluation methodology?

## Architecture Onboarding

- **Component map:** Dataset generation (GPT-4 + jailbroken Llama-2) -> Evaluation pipeline (GPT-3.5 harmfulness scoring) -> Fine-tuning system (LoRA-based adaptation) -> Medical performance tracking (MedQA, MedMCQA, PubMedQA, MMLU-Medical)
- **Critical path:** 1. Generate harmful medical requests → 2. Create safety demonstrations → 3. Evaluate base model safety → 4. Fine-tune on safety data → 5. Re-evaluate safety + medical performance
- **Design tradeoffs:** Human-in-the-loop for dataset curation ensures quality but limits scalability; LoRA fine-tuning is computationally cheap but may not fully align deep representations; GPT-3.5 scoring is fast but introduces external model dependency
- **Failure signatures:** Harmfulness scores plateau despite more fine-tuning data (adversarial robustness limit reached); medical performance drops sharply after safety fine-tuning (representation interference); high variance in harmfulness scores across seeds (evaluation instability)
- **First 3 experiments:** 1. Evaluate baseline medical LLM on MedSafety-Eval to confirm it scores above threshold; 2. Fine-tune same LLM on 200 MedSafety-Improve demonstrations and re-evaluate safety; 3. Fine-tune on 200 GenSafety-Improve demonstrations and compare cross-domain safety gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the nine Principles of Medical Ethics from the AMA translate to measurable behaviors for LLMs in medical contexts?
- Basis in paper: Explicit
- Why unresolved: The paper defines medical safety based on these principles but doesn't provide granular breakdown of how each principle manifests as specific behaviors
- What evidence would resolve it: Detailed mapping of each AMA principle to concrete LLM behaviors and decision points

### Open Question 2
- Question: What is the long-term stability of fine-tuned medical LLMs' safety improvements after deployment?
- Basis in paper: Inferred
- Why unresolved: The paper only examines immediate safety improvements but doesn't investigate persistence over time or with new harmful requests
- What evidence would resolve it: Longitudinal studies tracking safety performance over extended periods

### Open Question 3
- Question: How do medical LLMs' safety performance compare to that of trained medical professionals when faced with the same harmful requests?
- Basis in paper: Inferred
- Why unresolved: The paper establishes medical LLMs don't meet safety standards but doesn't benchmark against human medical professionals
- What evidence would resolve it: Comparative studies where both LLMs and medical professionals respond to identical harmful scenarios

## Limitations

- The operational definition of medical safety based on AMA principles may not capture emerging ethical concerns or culturally-specific medical safety standards
- Reliance on GPT-3.5 for harmfulness scoring introduces dependency on an external model's judgment, which may not align perfectly with human expert assessment
- Focus on English-language prompts and Western medical contexts creates uncertainty about generalizability to other languages and healthcare systems

## Confidence

**High confidence:** Fine-tuning on safety demonstrations reduces harmful compliance while preserving medical performance (well-supported by experimental results with statistical significance p < 0.01)

**Medium confidence:** Adversarial prompt generation provides complementary evaluation difficulty to GPT-4-generated prompts (supported by observed differences in compliance rates but practical significance uncertain)

**Low confidence:** The nine AMA principles comprehensively define medical safety for LLM applications (dynamic nature of medical ethics suggests this operational definition may require periodic reassessment)

## Next Checks

1. **Cross-cultural validation:** Evaluate MedSafetyBench's effectiveness on non-Western medical contexts by translating prompts and assessing whether safety improvements transfer across cultural and linguistic boundaries

2. **Human expert validation:** Replace GPT-3.5 harmfulness scoring with blinded assessments from medical ethics experts to verify that automated scoring aligns with human judgment of medical safety violations

3. **Longitudinal stability test:** Monitor safety and medical performance of fine-tuned models over extended periods and varying deployment conditions to assess whether safety gains persist and whether new vulnerabilities emerge over time