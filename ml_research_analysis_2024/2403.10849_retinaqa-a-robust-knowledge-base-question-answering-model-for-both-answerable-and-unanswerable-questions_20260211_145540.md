---
ver: rpa2
title: 'RetinaQA: A Robust Knowledge Base Question Answering Model for both Answerable
  and Unanswerable Questions'
arxiv_id: '2403.10849'
source_url: https://arxiv.org/abs/2403.10849
tags:
- logical
- questions
- retinaqa
- form
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RetinaQA, a KBQA model that can handle both
  answerable and unanswerable questions. The key idea is to combine path-based retrieval
  and schema-based generation via sketch-filling to enumerate logical form candidates,
  followed by discriminative ranking to select the best candidate.
---

# RetinaQA: A Robust Knowledge Base Question Answering Model for both Answerable and Unanswerable Questions

## Quick Facts
- arXiv ID: 2403.10849
- Source URL: https://arxiv.org/abs/2403.10849
- Authors: Prayushi Faldu; Indrajit Bhattacharya; Mausam
- Reference count: 18
- Key outcome: Introduces RetinaQA, a KBQA model that handles both answerable and unanswerable questions by combining path-based retrieval with schema-based generation via sketch-filling, followed by discriminative ranking to select the best logical form candidate. Achieves state-of-the-art performance on GrailQAbility benchmark and sets new SOTA on GrailQA.

## Executive Summary
RetinaQA introduces a novel approach to Knowledge Base Question Answering (KBQA) that addresses both answerable and unanswerable questions. The model leverages a unique combination of path-based retrieval and schema-based generation through a sketch-filling mechanism, followed by discriminative ranking of logical form candidates. This approach significantly improves the robustness and stability of KBQA systems, particularly in handling various categories of unanswerable questions while maintaining strong performance on answerable-only scenarios.

## Method Summary
The RetinaQA model operates through a two-stage process: first, it generates multiple logical form candidates using both path-based retrieval and schema-based generation via a sketch-filling approach. In the second stage, these candidates are ranked using a discriminative model to select the most appropriate answer or determine if the question is unanswerable. This combined approach allows the model to effectively handle complex questions that may have multiple valid interpretations or no valid answers in the knowledge base.

## Key Results
- Significantly outperforms state-of-the-art KBQA models on the GrailQAbility benchmark for both answerable and unanswerable questions
- Sets a new state-of-the-art on the answerable-only GrailQA dataset
- Demonstrates superior robustness and stability compared to adaptations of existing models for unanswerability

## Why This Works (Mechanism)
The success of RetinaQA stems from its ability to enumerate and evaluate multiple logical form candidates, combining both path-based and schema-based approaches. This comprehensive candidate generation, coupled with discriminative ranking, allows the model to effectively handle the ambiguity and complexity inherent in real-world questions. The sketch-filling mechanism in schema-based generation provides flexibility in handling diverse question structures, while the discriminative ranking ensures accurate selection of the best candidate or identification of unanswerable questions.

## Foundational Learning
- Knowledge Base Question Answering (KBQA): Understanding how to map natural language questions to structured queries over a knowledge base. Why needed: Forms the core problem space RetinaQA addresses. Quick check: Ability to formulate KBQA as a semantic parsing task.
- Logical Form Generation: Creating structured representations of questions that can be executed against a knowledge base. Why needed: Essential for translating natural language to executable queries. Quick check: Understanding of lambda calculus and semantic parsing.
- Discriminative Ranking: Evaluating and ranking multiple candidate logical forms. Why needed: Critical for selecting the best answer or identifying unanswerable questions. Quick check: Familiarity with ranking algorithms and machine learning classification.

## Architecture Onboarding

Component Map:
Natural Language Question -> Sketch-Filling Generator -> Path-Based Retriever -> Candidate Pool -> Discriminative Ranker -> Final Answer/UNANSWERABLE

Critical Path:
Question -> Sketch-Filling Generation -> Path-Based Retrieval -> Candidate Pool -> Discriminative Ranking -> Output

Design Tradeoffs:
- Complexity vs. Performance: The two-stage generation and ranking pipeline increases model complexity but significantly improves accuracy and robustness
- Coverage vs. Precision: Combining multiple generation approaches increases candidate coverage but requires effective ranking to maintain precision
- Generalizability vs. Specialization: The model is designed to handle both answerable and unanswerable questions, potentially at the cost of optimal performance on either category alone

Failure Signatures:
- Incorrect classification of answerable questions as unanswerable (false negatives)
- Failure to identify truly unanswerable questions (false positives)
- Suboptimal ranking leading to incorrect answer selection from valid candidates

First 3 Experiments:
1. Ablation study comparing performance with only path-based or only schema-based generation
2. Evaluation on a diverse set of unanswerable question types to assess robustness
3. Comparison of ranking accuracy with and without the discriminative model

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on specific benchmark datasets (GrailQA and GrailQAbility) may limit generalizability
- Complexity of the two-stage generation and ranking pipeline could impact computational efficiency
- Evaluation primarily focuses on English-language questions and Freebase knowledge base

## Confidence

High Confidence:
- Effectiveness of RetinaQA on evaluated benchmarks
- Superior performance in handling various categories of unanswerable questions
- Robustness and stability compared to existing models

Medium Confidence:
- Generalizability to other knowledge bases and languages
- Scalability to larger, more complex knowledge bases
- Computational efficiency for real-world deployment

## Next Checks
1. Evaluate RetinaQA's performance on multilingual KBQA datasets to assess cross-lingual robustness
2. Test the model's performance on alternative knowledge bases (e.g., Wikidata, DBpedia) to verify domain adaptability
3. Conduct computational efficiency analysis comparing RetinaQA's runtime and resource requirements against existing KBQA models