---
ver: rpa2
title: 'PORTULAN ExtraGLUE Datasets and Models: Kick-starting a Benchmark for the
  Neural Processing of Portuguese'
arxiv_id: '2404.05333'
source_url: https://arxiv.org/abs/2404.05333
tags:
- language
- portuguese
- association
- datasets
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PORTULAN ExtraGLUE, a benchmark for neural
  processing of Portuguese based on machine-translated versions of GLUE and SuperGLUE
  tasks. The authors translate 14 tasks into European and Brazilian Portuguese using
  DeepL, analyze translation quality, and provide low-rank adaptation (LoRA) models
  for 10 tasks using the Albertina encoder.
---

# PORTULAN ExtraGLUE Datasets and Models: Kick-starting a Benchmark for the Neural Processing of Portuguese

## Quick Facts
- arXiv ID: 2404.05333
- Source URL: https://arxiv.org/abs/2404.05333
- Reference count: 15
- Machine-translated Portuguese benchmark for neural processing of Portuguese

## Executive Summary
This paper introduces PORTULAN ExtraGLUE, a benchmark for neural processing of Portuguese created by machine-translating English GLUE and SuperGLUE tasks. The authors use DeepL to translate 14 tasks into European and Brazilian Portuguese, then analyze translation quality and provide LoRA models for 10 tasks using the Albertina encoder. Manual error analysis shows translation errors of 2-8% and label corruption of 0-4%. The LoRA models outperform XLM-RoBERTa-XL and achieve results close to English baselines. The benchmark and models are released under open licenses to support future research in Portuguese NLP.

## Method Summary
The authors machine-translate 14 GLUE and SuperGLUE tasks using DeepL for both European and Brazilian Portuguese. They conduct manual error analysis on translation quality and label consistency. For 10 tasks, they apply Low-Rank Adaptation (LoRA) to the Albertina 1.5B monolingual Portuguese model with specific hyperparameters (rank=8, alpha=32, dropout=0.05, batch size=8, learning rate=2e-5, weight decay=0.05). Models are evaluated and compared against XLM-RoBERTa-XL and English baselines.

## Key Results
- Manual error analysis shows 2-8% translation errors and 0-4% label corruption
- LoRA models outperform XLM-RoBERTa-XL on all 10 tasks in both Portuguese variants
- Results achieve performance close to English baselines while using monolingual Albertina models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine-translated datasets can serve as valid proxies for evaluating Portuguese language models.
- Mechanism: By translating established English benchmarks (GLUE/SuperGLUE) using state-of-the-art MT, the authors create Portuguese datasets that preserve label semantics and task structure while enabling direct comparison with English baselines.
- Core assumption: Translation errors and label corruption are minimal and do not systematically bias model evaluation.
- Evidence anchors:
  - [abstract]: "Manual error analysis shows 2–8% translation errors and 0–4% label corruption."
  - [section 4.4]: "Despite these problems, machine translation errors amount to only an average of 8%, with a mode as low as 2%. Label errors are even lower, with an average of 2% and a zero mode."
  - [corpus]: Weak evidence; no related works directly test MT-based benchmarks for Portuguese, so this remains an assumption.
- Break condition: If translation errors systematically correlate with task difficulty or label corruption is high, model comparisons become unreliable.

### Mechanism 2
- Claim: Low-rank adaptation (LoRA) can effectively fine-tune large models for specific tasks without full fine-tuning overhead.
- Mechanism: LoRA freezes pre-trained weights and adds small trainable low-rank matrices, reducing trainable parameters while maintaining or improving performance.
- Core assumption: The pre-trained model's knowledge is transferable to the downstream task, and low-rank decomposition captures necessary adaptation.
- Evidence anchors:
  - [abstract]: "LoRA models outperform XLM-RoBERTa-XL and achieve results close to English baselines."
  - [section 5.2]: "We observe improvements in Albertina 1.5B LoRA models for all tasks and in both Portuguese variants. In some cases, improvements are significant."
  - [corpus]: No direct evidence; this relies on Hu et al. (2022) LoRA work cited in the paper.
- Break condition: If the pre-trained model is too far from the target task distribution, LoRA cannot bridge the gap.

### Mechanism 3
- Claim: Using a monolingual Portuguese encoder (Albertina) yields better results than multilingual encoders for Portuguese tasks.
- Mechanism: Albertina is pre-trained specifically on Portuguese data, capturing language-specific patterns that multilingual models may miss.
- Core assumption: Monolingual models have richer representation of target language than multilingual ones.
- Evidence anchors:
  - [section 5.2]: "We note the benefits of using monolingual models when comparing such results with our Albertina 1.5B LoRA models. In fact, we observe improvements in Albertina 1.5B LoRA models for all tasks and in both Portuguese variants."
  - [section 5.2]: "XLM-RoBERTa-XL is significantly larger (3.5B parameters) than Albertina 1.5B. Even so, we note the benefits of using monolingual models."
  - [corpus]: No direct evidence; this is inferred from comparison results.
- Break condition: If multilingual models receive more task-specific fine-tuning, the monolingual advantage may diminish.

## Foundational Learning

- Concept: Machine translation limitations in NLP tasks
  - Why needed here: Understanding how MT can corrupt labels or lose linguistic nuance is crucial for interpreting benchmark validity
  - Quick check question: What specific linguistic features (e.g., gendered nouns, pronouns) are most vulnerable to translation errors in Portuguese?

- Concept: Low-rank adaptation mechanics
  - Why needed here: LoRA's parameter efficiency and training dynamics directly impact model performance and deployment
  - Quick check question: How does LoRA's rank parameter (r=8 in this work) affect the trade-off between adaptation capacity and parameter efficiency?

- Concept: Cross-lingual transfer learning evaluation
  - Why needed here: Comparing Portuguese models to English baselines requires understanding what constitutes equivalent performance across languages
  - Quick check question: What factors should be considered when determining if Portuguese results are "close" to English baselines?

## Architecture Onboarding

- Component map: Albertina 1.5B → LoRA adapter (rank=8, alpha=32) → task-specific fine-tuning → evaluation on PORTULAN ExtraGLUE datasets
- Critical path: Data preparation (translation + tokenization) → LoRA configuration → fine-tuning → evaluation
- Design tradeoffs: Monolingual vs multilingual models (performance vs coverage), LoRA rank vs adaptation capacity, dataset size vs translation quality
- Failure signatures: Poor performance on pronoun resolution tasks (WNLI), significant BLEU score differences between pt-PT/pt-BR indicating translation divergence, high label corruption rates
- First 3 experiments:
  1. Fine-tune Albertina with LoRA on SST-2 (single sentence task) to establish baseline performance and verify setup
  2. Compare pt-PT vs pt-BR performance on MRPC (similarity task) to assess translation quality differences
  3. Evaluate RTE (inference task) to test whether cross-lingual transfer works for entailment reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would manual curation of test sets impact the quality and validity of the PORTULAN ExtraGLUE benchmark compared to machine-translated versions?
- Basis in paper: [explicit] The authors acknowledge limitations of machine translation and suggest manual curation of datasets (in particular, the test sets) as future work to improve the benchmark.
- Why unresolved: While the authors provide error analysis showing translation errors and label corruption percentages, they haven't empirically tested whether manual curation would significantly improve task performance or reduce noise in the data.
- What evidence would resolve it: Conducting a study where portions of the test sets are manually curated and comparing performance metrics and error rates against the machine-translated versions would provide empirical evidence of the impact of manual curation.

### Open Question 2
- Question: To what extent do translation artifacts in PORTULAN ExtraGLUE datasets affect the ability to fairly compare monolingual models versus multilingual models in Portuguese NLP tasks?
- Basis in paper: [explicit] The authors note that translation-based data can be seen as a dialect of the target language, which may overestimate performance of models trained on such data, and they compare monolingual Albertina models against multilingual XLM-RoBERTa-XL.
- Why unresolved: The authors observe improvements with monolingual models but don't quantify how much translation artifacts might advantage or disadvantage either model type, nor do they establish a baseline for how much these artifacts might skew comparisons.
- What evidence would resolve it: Conducting controlled experiments where the same tasks are evaluated with fully manually translated or native Portuguese datasets, comparing results across monolingual and multilingual models, would reveal the extent to which translation artifacts influence comparative performance.

### Open Question 3
- Question: How would expanding PORTULAN ExtraGLUE to include datasets from scratch in Portuguese, rather than machine-translated versions, affect the benchmark's ability to reflect linguistic and cultural nuances across Portuguese variants?
- Basis in paper: [explicit] The authors suggest that developing new datasets from scratch may better reflect the language and cultures latent within language variants beyond European and Brazilian Portuguese.
- Why unresolved: While the authors hypothesize benefits of native datasets, they haven't empirically tested whether machine-translated datasets fail to capture important linguistic or cultural phenomena that would affect model performance or generalization.
- What evidence would resolve it: Creating and evaluating native Portuguese datasets for select tasks, then comparing model performance, linguistic feature coverage, and cultural relevance against the machine-translated versions would demonstrate whether native datasets provide meaningful advantages for benchmarking.

## Limitations
- Machine translation may introduce systematic biases that affect certain task types more than others
- The benchmark lacks human-translated or native Portuguese datasets for comparison
- Model size differences (3.5B vs 1.5B parameters) complicate fair comparison between Albertina and XLM-RoBERTa-XL

## Confidence
**High Confidence Claims:**
- The translation quality metrics (2-8% translation errors, 0-4% label corruption) are reliable based on systematic manual analysis
- LoRA fine-tuning is technically sound and follows established methodology from Hu et al. (2022)
- The release of datasets and models under open licenses is verifiable and supports reproducibility

**Medium Confidence Claims:**
- The claim that LoRA models "outperform XLM-RoBERTa-XL" is supported by results but requires caution due to model size differences
- The assertion that "results are close to English baselines" needs further validation across more tasks
- The benefits of monolingual Albertina over multilingual models are demonstrated but may not generalize to all downstream applications

**Low Confidence Claims:**
- The general validity of machine-translated benchmarks for Portuguese NLP evaluation (lacks direct comparative studies)
- The assertion that LoRA is the optimal adaptation method for these tasks (not compared to full fine-tuning)
- Claims about the benchmark's ability to "kick-start" Portuguese NLP research (requires longitudinal studies)

## Next Checks
1. **Translation Bias Analysis**: Conduct a systematic study comparing Portuguese model performance on machine-translated vs human-translated versions of the same tasks (where available) to quantify translation bias across different task types.

2. **Full Fine-tuning Comparison**: Replicate key experiments with full fine-tuning of Albertina 1.5B on the same tasks to determine whether LoRA's parameter efficiency comes at the cost of performance ceiling.

3. **Cross-lingual Generalization Test**: Evaluate the Portuguese LoRA models on code-switched datasets or Portuguese text containing English loanwords to assess whether monolingual specialization hinders real-world performance.