---
ver: rpa2
title: Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting
arxiv_id: '2401.15585'
source_url: https://arxiv.org/abs/2401.15585
tags:
- word
- words
- feminine
- bias
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) often exhibit gender bias, even in
  simple tasks like counting words. This study introduces a new benchmark called Multi-step
  Gender Bias Reasoning (MGBR) to evaluate gender bias in LLMs.
---

# Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting

## Quick Facts
- arXiv ID: 2401.15585
- Source URL: https://arxiv.org/abs/2401.15585
- Reference count: 15
- Primary result: Chain-of-Thought prompting significantly reduces unconscious gender bias in large language models during word counting tasks

## Executive Summary
This study introduces the Multi-step Gender Bias Reasoning (MGBR) benchmark to evaluate gender bias in large language models (LLMs). The benchmark presents models with word lists containing feminine, masculine, and occupational terms, asking them to count the number of words by gender. The study demonstrates that without Chain-of-Thought (CoT) prompting, LLMs exhibit significant gender bias even in simple counting tasks. However, CoT prompting forces explicit gender classification of each word, dramatically reducing unconscious bias and producing fairer predictions. The research also reveals that MGBR correlates with downstream bias evaluation metrics but not with intrinsic measures.

## Method Summary
The study constructs the MGBR benchmark using word lists containing feminine words (Vf), masculine words (Vm), and occupational words stereotypically associated with females (Vof) or males (Vom). Various prompting techniques are implemented, including zero-shot, few-shot, Chain-of-Thought, and debiasing prompts. For CoT, models explicitly indicate whether each word is feminine or masculine before making final predictions. Bias scores are calculated by comparing counting accuracy on lists with and without occupational words. The effectiveness of different prompting approaches is evaluated across multiple LLM architectures.

## Key Results
- Without CoT prompting, LLMs exhibit significant gender bias in simple word counting tasks
- CoT prompting dramatically reduces unconscious gender bias by forcing explicit word classification
- Simple debiasing instructions alone are insufficient for unscalable reasoning tasks
- MGBR shows high correlation with downstream bias evaluation metrics (BBQ, BNLI) but low correlation with intrinsic measures (CP, SS)

## Why This Works (Mechanism)

### Mechanism 1
CoT prompting reduces gender bias by forcing explicit gender classification of each word before counting. By requiring the model to state whether each word is feminine or masculine, CoT makes implicit associations explicit, allowing the model to correct biased assumptions before final prediction. This assumes LLMs have sufficient gender classification capability when explicitly prompted.

### Mechanism 2
Debiasing through simple instructions alone is insufficient for unscalable tasks. Direct instructions to "avoid stereotypes" don't provide the step-by-step reasoning framework needed for accurate word classification and counting. Simple instructions cannot substitute for explicit reasoning steps in tasks requiring word meaning understanding.

### Mechanism 3
MGBR benchmark correlates with downstream bias evaluation metrics but not with intrinsic measures. MGBR evaluates bias in a reasoning context that better reflects real-world application scenarios compared to word-pair similarity measures. This assumes bias manifested in reasoning tasks has stronger correlation with downstream task performance than bias measured through static word embeddings.

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: Enables the model to break down complex reasoning tasks into explicit steps, crucial for accurate gender classification and counting
  - Quick check question: How does CoT differ from standard prompting in terms of model reasoning process?

- Concept: Bias evaluation metrics (BBQ, BNLI, CP, SS)
  - Why needed here: Understanding different bias evaluation approaches helps interpret MGBR results and their relationship to other benchmarks
  - Quick check question: What distinguishes extrinsic from intrinsic bias evaluation measures?

- Concept: Scalable vs unscalable tasks in LLMs
  - Why needed here: Contextualizes why CoT is particularly effective for gender bias mitigation in counting tasks
  - Quick check question: Why don't simple prediction tasks work well for unscalable reasoning problems?

## Architecture Onboarding

- Component map: Word list input -> Instruction parsing -> CoT generation -> Gender classification -> Counting mechanism -> Output prediction -> Bias evaluation
- Critical path: 1) Receive word list and instruction 2) Generate CoT reasoning for each word 3) Classify each word as feminine/masculine/neutral 4) Count words by gender category 5) Output final count 6) Evaluate bias based on accuracy difference
- Design tradeoffs: Explicit reasoning vs. efficiency (CoT is slower but more accurate), model size requirements (larger models perform better with CoT), training data dependency (CoT effectiveness depends on model's pre-existing gender knowledge)
- Failure signatures: Inconsistent gender classification across similar words, defaulting to gender stereotypes for occupational terms, inability to correctly count despite accurate classification
- First 3 experiments: 1) Test baseline accuracy without CoT across different model sizes 2) Compare CoT vs. instruction-only debiasing effectiveness 3) Validate MGBR correlation with downstream bias metrics

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CoT prompting vary across different types of social biases (e.g., racial, religious) beyond gender bias? The paper only evaluates gender bias effectiveness, leaving generalization to other bias types unexplored. This remains unresolved because the study doesn't test CoT across diverse social bias domains.

### Open Question 2
What is the relationship between the size of the language model and the effectiveness of CoT prompting in mitigating social biases? While the paper suggests model size may play a role, it doesn't comprehensively analyze this relationship across different model sizes and bias types.

### Open Question 3
How does the effectiveness of CoT prompting in mitigating social biases compare to other debiasing techniques, such as fine-tuning or parameter modification? The study only evaluates CoT prompting without comparing it to alternative approaches, leaving relative performance unknown.

## Limitations

- The MGBR benchmark represents a relatively narrow task domain (word counting) that may not generalize to more complex reasoning tasks
- The study focuses exclusively on gender bias, leaving effectiveness on other social biases unexplored
- The claimed correlation between MGBR and downstream bias metrics is based on limited empirical evidence

## Confidence

- High confidence: Basic observation that LLMs exhibit gender bias in simple counting tasks, and that CoT prompting changes the reasoning process
- Medium confidence: Effectiveness of CoT prompting in reducing gender bias specifically for the MGBR benchmark
- Low confidence: Generalizability of CoT's effectiveness to other unscalable reasoning tasks and the claimed correlation between MGBR and downstream bias metrics

## Next Checks

1. Cross-task generalization test: Evaluate CoT prompting effectiveness on other unscalable reasoning tasks beyond word counting to verify if bias reduction generalizes beyond MGBR

2. Ablation study on CoT components: Systematically remove components of the CoT prompt to identify which specific elements are responsible for bias reduction

3. Correlation validation with diverse downstream tasks: Test the claimed correlation between MGBR bias scores and downstream task performance across a broader range of applications to validate MGBR as a reliable proxy for real-world bias