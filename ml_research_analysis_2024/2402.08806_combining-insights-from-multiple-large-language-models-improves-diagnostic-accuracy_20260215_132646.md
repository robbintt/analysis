---
ver: rpa2
title: Combining Insights From Multiple Large Language Models Improves Diagnostic
  Accuracy
arxiv_id: '2402.08806'
source_url: https://arxiv.org/abs/2402.08806
tags: []
core_contribution: "Using collective intelligence methods, aggregating differential\
  \ diagnoses from multiple large language models (LLMs) achieves higher diagnostic\
  \ accuracy (75.3% \xB1 1.6pp) than individual LLMs (59.0% \xB1 6.1pp). This approach\
  \ combines responses from various LLMs, reducing hallucinations and vendor lock-in\
  \ while increasing trust."
---

# Combining Insights From Multiple Large Language Models Improves Diagnostic Accuracy

## Quick Facts
- arXiv ID: 2402.08806
- Source URL: https://arxiv.org/abs/2402.08806
- Authors: Gioele Barabucci; Victor Shia; Eugene Chu; Benjamin Harack; Nathan Fu
- Reference count: 16
- Primary result: Aggregating differential diagnoses from multiple LLMs achieves 75.3% diagnostic accuracy versus 59.0% for individual models

## Executive Summary
This study demonstrates that collective intelligence methods, which aggregate differential diagnoses from multiple large language models (LLMs), significantly improve diagnostic accuracy compared to relying on single models. By combining responses from various LLMs, the approach reduces hallucinations and vendor lock-in while increasing trust in medical AI systems. The method is simple, universally applicable, and can be easily integrated into existing software systems, making it particularly valuable for medical practitioners and underserved areas.

The research tested this approach on clinical vignettes from the MedQA dataset using GPT-4 and GPT-3.5 models, finding that aggregating three or more LLMs yields diminishing returns but still provides meaningful improvements over individual models. The study suggests that multi-LLM aggregation could serve as a powerful support tool for medical practitioners, offering a practical solution to enhance diagnostic accuracy while maintaining transparency and reducing dependence on any single AI vendor.

## Method Summary
The researchers evaluated collective intelligence methods for medical diagnosis by using the MedQA dataset of clinical vignettes. They prompted multiple LLMs (GPT-4 and GPT-3.5) to generate differential diagnoses for each case, then applied aggregation techniques including majority vote, sum, and product methods to combine the individual model responses. The study focused on 25 clinical vignettes from the official MedQA test set, asking each LLM to provide a differential diagnosis with the most likely diagnosis listed first. The aggregated results were compared against single-model performance to assess improvements in diagnostic accuracy.

## Key Results
- Multi-LLM aggregation achieved 75.3% ± 1.6pp diagnostic accuracy versus 59.0% ± 6.1pp for individual LLMs
- Combining three or more LLMs showed diminishing returns but still improved accuracy
- Simple aggregation techniques (majority vote, sum, product) were effective and easily implementable

## Why This Works (Mechanism)
The mechanism appears to work through ensemble averaging, where individual model errors and hallucinations tend to cancel out when multiple independent models are combined. Different LLMs may have varying strengths in pattern recognition and knowledge representation, and their collective responses can compensate for individual model blind spots. The aggregation process effectively creates a consensus that reduces the impact of any single model's incorrect diagnosis or hallucinated information.

## Foundational Learning
- Collective intelligence principles - why needed: To understand how multiple AI agents can outperform individual ones; quick check: Can be verified by comparing single vs. aggregated model performance
- Medical diagnosis workflow - why needed: Essential for contextualizing how LLM outputs integrate with clinical decision-making; quick check: Understanding differential diagnosis generation process
- LLM hallucination patterns - why needed: Critical for recognizing how aggregation reduces erroneous outputs; quick check: Comparing hallucination rates between single and aggregated models
- Medical benchmark datasets - why needed: Provides standardized evaluation framework; quick check: Understanding MedQA dataset structure and validation
- Ensemble methods in AI - why needed: Establishes theoretical foundation for multi-model approaches; quick check: Comparing with traditional ensemble learning techniques

## Architecture Onboarding

Component map: Clinical vignette -> Multiple LLMs -> Differential diagnoses generation -> Aggregation methods -> Final diagnosis output

Critical path: Data input → LLM inference (parallel) → Response collection → Aggregation algorithm → Accuracy evaluation

Design tradeoffs: Simple aggregation methods provide transparency and ease of implementation but may not capture complex inter-model relationships that more sophisticated fusion techniques could achieve

Failure signatures: Individual model hallucinations, inconsistent formatting across LLM responses, aggregation method sensitivity to outlier predictions

First experiments:
1. Test different aggregation thresholds to find optimal balance between consensus strength and diagnostic coverage
2. Evaluate performance across different clinical specialties to identify domain-specific strengths and weaknesses
3. Compare aggregation results with human expert consensus diagnoses to validate clinical relevance

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to 25 clinical vignettes rather than full MedQA test set of 1,390 cases
- Experiments conducted only with GPT-4 and GPT-3.5 models, leaving uncertainty about performance with other architectures
- Hallucination analysis was limited to a single test case, potentially not reflecting broader patterns

## Confidence
- Multi-LLM aggregation improves diagnostic accuracy: High
- Three or more LLMs yields diminishing returns: Medium
- Reduction in hallucinations through aggregation: Low

## Next Checks
1. Evaluate the aggregation approach on the complete MedQA test set (1,390 cases) to confirm robustness across the full clinical spectrum
2. Test the methodology with a broader range of LLM models including open-source alternatives and smaller parameter models to assess generalizability
3. Conduct real-world clinical validation with actual patient cases to verify that improvements in synthetic benchmark performance translate to clinical practice