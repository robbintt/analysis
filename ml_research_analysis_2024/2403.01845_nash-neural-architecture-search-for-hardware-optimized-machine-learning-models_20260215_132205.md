---
ver: rpa2
title: 'NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models'
arxiv_id: '2403.01845'
source_url: https://arxiv.org/abs/2403.01845
tags:
- nash
- accuracy
- architecture
- hardware
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents NASH, a neural architecture search approach
  that optimizes machine learning models specifically for hardware implementations.
  NASH extends differentiable NAS to include hardware-friendly operations and quantization
  techniques, then integrates with the FINN framework to generate FPGA implementations.
---

# NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models

## Quick Facts
- arXiv ID: 2403.01845
- Source URL: https://arxiv.org/abs/2403.01845
- Reference count: 28
- Primary result: Up to 3.1% higher top-1 accuracy than baseline models on ImageNet through hardware-optimized NAS

## Executive Summary
NASH presents a neural architecture search approach that optimizes machine learning models specifically for hardware implementations. The method extends differentiable NAS to include hardware-friendly operations and quantization techniques, then integrates with the FINN framework to generate FPGA implementations. By applying four search strategies to ResNet18/34, NASH achieves up to 3.1% higher top-1 accuracy than baseline models on ImageNet. Hardware results show a maximum throughput of 324.5 fps, with NASH models demonstrating superior accuracy-resource trade-offs on Pareto curves.

## Method Summary
NASH extends differentiable NAS (DARTS) by incorporating hardware-aware constraints into the search space and integrating quantization-aware training with Brevitas. The method searches for optimal convolutional cells using CIFAR-10 as a proxy task, then applies these learned architectures to ResNet18/34 models trained on ImageNet. Four NASH versions (v1-v4) explore different hardware constraints and bit-width configurations. The final models are processed through the FINN framework for FPGA implementation, with specific streamline transforms applied to optimize hardware generation.

## Key Results
- NASH-v4 achieves up to 3.1% higher top-1 accuracy than baseline ResNet18/34 on ImageNet
- Maximum throughput of 324.5 fps achieved with NASH models on FPGA
- NASH models demonstrate superior accuracy-resource trade-offs on Pareto curves compared to original architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NASH extends differentiable NAS to hardware-friendly operations, enabling higher accuracy quantized models for FPGA deployment
- Mechanism: By incorporating hardware-aware search space (e.g., limiting max pooling operations that require additional hardware) and using quantization-aware training with Brevitas, NASH optimizes both accuracy and hardware efficiency simultaneously
- Core assumption: The differentiable NAS framework can be adapted to include hardware-specific constraints without losing its search effectiveness
- Evidence anchors:
  - [abstract]: "NASH extends differentiable NAS to include hardware-friendly operations and quantization techniques"
  - [section III-B]: "The architecture search algorithm is adjusted to include structures and operations suitable for hardware"
  - [corpus]: Weak evidence - corpus neighbors don't directly address hardware-aware NAS integration with FINN
- Break condition: If the hardware constraints significantly reduce the search space such that optimal architectures cannot be found

### Mechanism 2
- Claim: NASH-v4 achieves the best accuracy by using 8-bit weights for NAS branches, allowing more expressive operations than lower bit-width alternatives
- Mechanism: Higher bit-width weights in NAS branches enable more complex operations (e.g., 5×5 convolutions) that can capture more features, leading to improved accuracy despite increased hardware resource usage
- Core assumption: The accuracy gains from higher bit-width weights in NAS branches outweigh the hardware cost
- Evidence anchors:
  - [section III-B]: "NASH-v4, the weights of the NASH branches are 1-bit, and for NASH-v4, they are 8-bit"
  - [section IV-A]: "NASH-v4 has the best accuracy among all NASH versions"
  - [corpus]: No direct evidence in corpus neighbors about bit-width effects on NAS accuracy
- Break condition: If the increased hardware resource utilization prevents deployment on target FPGA devices

### Mechanism 3
- Claim: NASH models achieve better accuracy-resource trade-offs than original models, as shown by Pareto curve analysis
- Mechanism: By systematically exploring different bit-width configurations and NASH versions, the method identifies architectures that provide higher accuracy for a given hardware utilization or lower hardware utilization for a given accuracy
- Core assumption: The trade-off between accuracy and hardware utilization can be effectively modeled and optimized through systematic search
- Evidence anchors:
  - [abstract]: "NASH models demonstrating superior accuracy-resource trade-offs on Pareto curves"
  - [section IV-C]: "The accuracy-hardware (HW) Pareto curve shows that the models with the four NASH versions represent the best trade-offs"
  - [corpus]: Weak evidence - corpus neighbors discuss hardware-aware NAS but don't provide Pareto curve analysis
- Break condition: If the Pareto curve shows no improvement over baseline models for critical accuracy thresholds

## Foundational Learning

- Concept: Differentiable Neural Architecture Search (DARTS)
  - Why needed here: NASH builds upon DARTS as its search algorithm foundation, extending it for hardware optimization
  - Quick check question: What is the key difference between DARTS and traditional NAS approaches?

- Concept: Quantization-aware training
  - Why needed here: Essential for generating models that can be efficiently implemented in hardware with low-bit precision while maintaining accuracy
  - Quick check question: How does quantization-aware training differ from standard training in terms of gradient computation?

- Concept: FPGA architecture and FINN framework
  - Why needed here: Understanding FINN's MVTU structure and folding methodology is crucial for interpreting NASH's hardware optimization results
  - Quick check question: What role do SIMD lanes and processing elements play in FINN's hardware generation process?

## Architecture Onboarding

- Component map:
  - CIFAR-10 dataset -> Architecture search (DARTS-based) -> Optimized convolutional cells -> ResNet18/34 models -> ImageNet training -> FINN hardware generation

- Critical path: Architecture search → Model training → Hardware generation

- Design tradeoffs:
  - Accuracy vs. hardware resource utilization
  - Bit-width of weights/activations vs. accuracy
  - Operation complexity vs. hardware friendliness

- Failure signatures:
  - Models failing to converge during search
  - Generated hardware exceeding FPGA resource limits
  - Accuracy degradation when applying quantization

- First 3 experiments:
  1. Implement NASH-v1 on ResNet18 with w1a1 configuration and verify accuracy improvement
  2. Generate hardware implementation using FINN and measure resource utilization
  3. Compare Pareto curve position against original ResNet18 baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the NASH architecture search algorithm scale to larger models like ResNet50 or deeper networks beyond ResNet34?
- Basis in paper: [inferred] The paper demonstrates NASH on ResNet18 and ResNet34, but discusses trade-offs showing larger models like ResNet34 have worse accuracy-resource trade-offs compared to ResNet18, suggesting potential scalability challenges
- Why unresolved: The paper only evaluates NASH on ResNet18 and ResNet34 models, leaving open whether the approach maintains effectiveness on larger, more complex architectures
- What evidence would resolve it: Experimental results applying NASH to ResNet50 and deeper architectures, comparing accuracy improvements, hardware resource utilization, and accuracy-resource trade-offs against original models

### Open Question 2
- Question: What is the impact of different hardware platforms (e.g., ASIC vs FPGA) on NASH model performance and resource utilization?
- Basis in paper: [inferred] NASH is implemented using the FINN framework for FPGA, with results showing specific BRAM and LUT utilization. The paper mentions hardware constraints but doesn't explore other hardware platforms
- Why unresolved: The paper focuses exclusively on FPGA implementations using FINN, without exploring how NASH models would perform on different hardware platforms with varying constraints
- What evidence would resolve it: Implementation results of NASH models on different hardware platforms (ASIC, GPU, TPU), comparing accuracy, resource utilization, latency, and throughput across platforms

### Open Question 3
- Question: How does NASH perform on other computer vision tasks beyond image classification, such as object detection or semantic segmentation?
- Basis in paper: [explicit] The paper only evaluates NASH on image classification using ImageNet, despite mentioning potential applicability to various convolutional neural networks
- Why unresolved: The paper's experimental validation is limited to image classification, leaving open whether NASH's benefits transfer to other computer vision domains that may have different architectural requirements
- What evidence would resolve it: Application of NASH to object detection (e.g., YOLO, SSD) and semantic segmentation (e.g., U-Net, DeepLab) models, with accuracy improvements and hardware resource utilization comparisons to baseline models

## Limitations

- Limited to image classification task validation, with no evaluation on object detection or segmentation
- Hardware results only validated on FPGA platform, with no comparison to ASIC or other hardware implementations
- Scalability concerns for larger architectures beyond ResNet34, as trade-offs suggest diminishing returns

## Confidence

- Hardware-aware NAS integration with FINN: Medium
- 3.1% accuracy improvement claim: High
- Generalizability to other architectures: Low

## Next Checks

1. Implement the FINN streamline transforms (MoveMulPastMaxPool, AbsorbSignBiasIntoMultiThreshold) and verify their impact on hardware generation success rate and efficiency
2. Test NASH search space extension on additional backbone architectures (e.g., MobileNet, EfficientNet) to assess generalizability
3. Conduct ablation studies varying the bit-width configurations systematically to quantify the accuracy-resource trade-off curves across different application domains