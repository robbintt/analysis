---
ver: rpa2
title: How Much Annotation is Needed to Compare Summarization Models?
arxiv_id: '2402.18756'
source_url: https://arxiv.org/abs/2402.18756
tags:
- human
- overall
- test
- automatic
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how much test data is needed to reliably
  compare summarization models. The authors find that clear model preferences emerge
  with as few as 50 test examples for both automatic metrics (ROUGE-1, BERTScore)
  and human evaluations.
---

# How Much Annotation is Needed to Compare Summarization Models?

## Quick Facts
- arXiv ID: 2402.18756
- Source URL: https://arxiv.org/abs/2402.18756
- Reference count: 28
- Primary result: Model preferences remain stable with as few as 50 test examples

## Executive Summary
This paper investigates the minimum amount of test data required to reliably compare summarization models. Through experiments across three datasets (CNN/DM, XSUM, and OWT), the authors demonstrate that clear model preferences can emerge with as few as 50 test examples, challenging the common practice of using extensive benchmarks. They find that both automatic metrics (ROUGE-1, BERTScore) and human evaluations show stable win rates even with small test sets. The study also reveals that model preferences vary depending on task context and data source, which has important implications for how automatic metrics should be validated.

## Method Summary
The authors conducted experiments across three summarization datasets (CNN/DM, XSUM, and OWT) to evaluate model comparison efficiency. They tested both automatic metrics (ROUGE-1, BERTScore, and GPT-4-based evaluation) and human evaluations using varying sample sizes. The study measured win rates for model comparisons and assessed the correlation between automatic metrics and human preferences. By systematically varying the number of test examples, they determined when model preferences stabilize and identified which metrics best predict human judgment across different tasks and data sources.

## Key Results
- Model preferences remain stable with as few as 50 test examples across both automatic and human metrics
- ROUGE-1 and GPT-4-based evaluation moderately predict human preferences across different tasks and data sources
- BERTScore and G-Eval show less reliable performance in predicting human preferences

## Why This Works (Mechanism)
The stability of model preferences with small sample sizes appears to stem from the inherent signal strength in summarization quality judgments. When models produce sufficiently different outputs, these differences become apparent even with limited examples. The correlation between automatic metrics and human preferences likely reflects the extent to which these metrics capture meaningful aspects of summary quality rather than superficial features.

## Foundational Learning
- **Sample size requirements**: Understanding how many examples are needed for statistical significance in model comparison
  - Why needed: Determines efficiency of evaluation processes and resource allocation
  - Quick check: Compare win rate stability across different sample sizes

- **Metric-human alignment**: The relationship between automatic metrics and human judgment
  - Why needed: Validates whether automatic metrics can substitute for expensive human evaluation
  - Quick check: Calculate correlation coefficients between metric scores and human ratings

- **Task-specific evaluation**: How evaluation requirements vary across different summarization tasks
  - Why needed: Ensures findings generalize across diverse summarization contexts
- Quick check: Test consistency of results across multiple datasets with different characteristics

## Architecture Onboarding
- **Component map**: Automatic metrics (ROUGE-1, BERTScore, GPT-4) -> Human evaluation -> Win rate calculation -> Model preference determination
- **Critical path**: Human evaluation requires most resources, so automatic metric validation is crucial for scaling
- **Design tradeoffs**: Sample size vs. statistical confidence vs. resource efficiency
- **Failure signatures**: Inconsistent model preferences across sample sizes indicate insufficient data or unreliable metrics
- **3 first experiments**: 1) Test win rate stability with increasing sample sizes, 2) Compare metric-human correlation across datasets, 3) Evaluate whether results generalize to additional summarization tasks

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on specific automatic metrics (ROUGE-1, BERTScore, GPT-4) that may not represent the full landscape of evaluation approaches
- The human evaluation sample size of 50 test examples may not capture the full diversity of summarization quality judgments across all domains
- Findings are based on three specific datasets (CNN/DM, XSUM, OWT) and may not generalize to other summarization domains or languages

## Confidence
- **High confidence**: Model preferences remain stable with 50 test examples across automatic and human metrics
- **Medium confidence**: ROUGE-1 and GPT-4-based evaluation moderately predict human preferences across tasks
- **Low confidence**: BERTScore and G-Eval consistently underperform across all conditions (based on limited evaluation scope)

## Next Checks
1. Test whether similar annotation efficiency patterns hold for additional automatic metrics like ROUGE-2, ROUGE-L, and newer learned metrics not evaluated in this study
2. Evaluate the stability of findings across more diverse summarization domains beyond news articles, such as scientific abstracts or legal documents
3. Conduct power analysis to determine the minimum sample size needed to detect smaller but practically meaningful differences between models