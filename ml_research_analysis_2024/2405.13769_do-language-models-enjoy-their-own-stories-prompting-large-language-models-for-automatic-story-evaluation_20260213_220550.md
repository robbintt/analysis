---
ver: rpa2
title: Do Language Models Enjoy Their Own Stories? Prompting Large Language Models
  for Automatic Story Evaluation
arxiv_id: '2405.13769'
source_url: https://arxiv.org/abs/2405.13769
tags:
- human
- story
- llms
- ratings
- eval-prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether Large Language Models (LLMs) can effectively
  substitute human annotators for Automatic Story Evaluation (ASE). The authors conducted
  extensive experiments comparing LLM ratings with human annotations across six story
  evaluation criteria.
---

# Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation

## Quick Facts
- **arXiv ID**: 2405.13769
- **Source URL**: https://arxiv.org/abs/2405.13769
- **Reference count**: 27
- **Key outcome**: LLMs can reliably rank different story generation models (high system-level correlations) despite weak overall correlations with human ratings, with larger models generally producing higher-quality stories.

## Executive Summary
This paper investigates whether Large Language Models (LLMs) can effectively substitute human annotators for Automatic Story Evaluation (ASE). Through extensive experiments comparing LLM ratings with human annotations across six story evaluation criteria, the authors find that while overall correlations between LLM and human ratings are moderate to weak, system-level correlations are very high. This indicates that LLMs can reliably rank different story generation models. The study also examines the influence of prompt engineering on LLM performance, revealing that adding detailed guidelines does not necessarily improve results and that LLMs exhibit remarkable self-consistency. However, LLM explanations were found to be less satisfactory, often lacking substantiated claims and proper adherence to guidelines.

## Method Summary
The study uses the HANNA dataset with 1,056 stories, plus additional stories from Llama models (Llama-7B, Llama-13B, Llama-30B, Platypus2-70B). ASE experiments were conducted using four LLMs (Beluga-13B, Mistral-7B, Llama-13B, ChatGPT) with four different Eval-Prompts on all 1,440 stories across six criteria: Relevance, Coherence, Empathy, Surprise, Engagement, and Complexity. The method computes Kendall correlations between LLM ratings and human ratings at both overall and system levels, and calculates intra-class correlation for LLM consistency, all performed in a zero-shot setting without further training.

## Key Results
- LLMs outperform current automatic measures for system-level evaluation but struggle at providing satisfactory explanations for their answers
- Larger LLMs (Beluga-13B, Llama-30B, Platypus2-70B) generally produce higher-quality stories, possibly due to their pretraining data resembling existing fiction
- Adding detailed guidelines to prompts does not necessarily improve LLM performance in story evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs achieve high system-level correlations because they evaluate formal linguistic qualities (syntax, grammar, coherence) that correlate with human preferences for generated stories
- **Mechanism**: LLMs use statistical patterns learned during pretraining to assess formal quality, which happens to align with human preferences for generated stories even if the evaluation criteria differ
- **Core assumption**: Human evaluation of generated stories is biased toward formal quality indicators that LLMs can detect
- **Evidence anchors**: [abstract] "LLMs outperform current automatic measures for system-level evaluation but still struggle at providing satisfactory explanations for their answers"; [section] "We hypothesize that the 'rating' part of our story evaluation experiments could be linked to formal linguistic competence and the fast, automatic System 1, while the 'explanation' part would correspond to functional linguistic competence and the slow, conscious System 2"
- **Break condition**: If human evaluation criteria shift to emphasize functional linguistic competence over formal quality, or if pretraining data distribution changes significantly

### Mechanism 2
- **Claim**: LLMs show remarkable self-consistency because they use deterministic decoding strategies once hyperparameters are fixed
- **Mechanism**: Once temperature and top-p parameters are set, LLMs generate stable outputs through controlled randomness, leading to high intra-class correlation coefficients
- **Core assumption**: LLM consistency is primarily determined by fixed decoding hyperparameters rather than model architecture differences
- **Evidence anchors**: [section] "We observe that LLMs have very high consistency overall for all criteria; the lowest value is Mistral-7B's ICC for Surprise (0.66), which is still fairly high"; [section] "ICC2k values for Eval-Prompt 1 for Beluga-13B, Mistral-7B and human ratings are displayed on Tab. 1"
- **Break condition**: If decoding strategies change significantly or if model architecture differences become more pronounced

### Mechanism 3
- **Claim**: Larger LLMs perform better at ASG because they can reproduce text patterns similar to their pretraining data (existing novels)
- **Mechanism**: Larger models have more parameters and capacity to memorize and reproduce patterns from their pretraining corpus, which includes fiction books
- **Core assumption**: Pretraining data contains sufficient fiction content that larger models can effectively reproduce
- **Evidence anchors**: [section] "The higher ratings of larger LLMs may be due to their ability to produce output similar to existing books"; [section] "Model AUC (%) Platypus2-70B 92.1 Llama-30B 81.3 Beluga-13B 70.1 Mistral-7B 51.2 Llama-7B 55.1"
- **Break condition**: If pretraining data composition changes or if evaluation criteria shift away from pattern reproduction

## Foundational Learning

- **Concept**: System-level vs segment-level correlation
  - **Why needed here**: Understanding why the paper uses system-level correlation instead of segment-level correlation for story evaluation
  - **Quick check question**: Why is system-level correlation more appropriate than segment-level correlation for evaluating story generation models?

- **Concept**: Williams test for dependent correlations
  - **Why needed here**: The paper uses Williams test to compare correlation strengths between different evaluation methods on the same dataset
  - **Quick check question**: What is the key difference between Williams test and standard correlation tests when comparing evaluation methods?

- **Concept**: Prompt engineering strategies
  - **Why needed here**: The paper explores how different prompt formats affect LLM performance in story evaluation
  - **Quick check question**: How do zero-shot and few-shot prompting differ in their approach to guiding LLM responses?

## Architecture Onboarding

- **Component map**: Story generation → LLM evaluation → correlation computation → system ranking → performance analysis
- **Critical path**: Story generation → LLM evaluation → correlation computation → system ranking → performance analysis
- **Design tradeoffs**: Zero-shot prompting (simpler, less controlled) vs detailed guidelines (more complex, potentially less effective), closed-source vs open-source models (performance vs transparency)
- **Failure signatures**: Poor correlation with human ratings (evaluation methodology issues), inconsistent LLM ratings (decoding strategy problems), unsubstantiated explanations (task understanding limitations)
- **First 3 experiments**:
  1. Replicate system-level correlation analysis with different correlation coefficients (Pearson vs Spearman)
  2. Test LLM consistency across different temperature settings
  3. Compare Eval-Prompt 3 performance with and without explicit guideline compliance checks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do different evaluation criteria correlate with one another in LLM-based story evaluation?
- **Basis in paper**: [explicit] The paper notes that the six criteria from Chhun et al. (2022) are "mostly orthogonal but not completely independent" and that "their correlation with one another may be related to the general 'System 1' tendency of human raters to favor stories that display better formal qualities."
- **Why unresolved**: While the paper mentions that criteria are not completely independent, it does not provide detailed analysis of how these criteria correlate with each other in LLM-based evaluation.
- **What evidence would resolve it**: Detailed correlation analysis between different evaluation criteria when using LLMs for story evaluation.

### Open Question 2
- **Question**: Would fine-tuning or reinforcement learning with human feedback on ASE-specific data improve LLM performance in story evaluation?
- **Basis in paper**: [inferred] The paper states "we performed most of our experiments in a zero-shot setting without further training" and suggests it would be "interesting to compare our results with future work involving fine-tuning or reinforcement learning with human feedback on data specific to ASE."
- **Why unresolved**: The paper only used zero-shot prompting without any additional training, leaving the potential benefits of fine-tuning unexplored.
- **What evidence would resolve it**: Comparative study of zero-shot vs. fine-tuned LLM performance on ASE tasks using the same evaluation criteria.

### Open Question 3
- **Question**: How would LLMs perform at evaluating longer stories (beyond 500-1,000 words) compared to their performance on shorter stories?
- **Basis in paper**: [explicit] The paper notes that "generating longer stories may prove more difficult since maintaining large-scale coherence may become an issue" and mentions their experiments were limited to "short stories of between 500 and 1,000 words."
- **Why unresolved**: The study was limited to shorter stories, and the authors acknowledge that longer stories might present different challenges.
- **What evidence would resolve it**: Evaluation of LLM performance on longer stories (e.g., 2,000-5,000 words) using the same evaluation criteria and methodology.

## Limitations
- **Human evaluation variability**: The paper reports weak overall correlations between LLM and human ratings but high system-level correlations, suggesting LLMs may capture different aspects of story quality than humans
- **Prompt engineering effectiveness**: The finding that detailed guidelines perform worse than simple prompts contradicts expectations about the value of explicit instructions
- **Pretraining data assumptions**: The hypothesis about why larger models perform better relies on unverified assumptions about training corpus composition

## Confidence
- **High confidence**: The observation that LLMs exhibit high self-consistency across different prompts and criteria (directly measurable through intra-class correlation coefficients)
- **Medium confidence**: The conclusion that LLMs are useful for system-level evaluation despite poor segment-level performance (correlation data supports this claim)
- **Low confidence**: The hypothesis about why larger models produce higher-rated stories (based on pretraining data composition, lacks direct evidence)

## Next Checks
1. **Cross-validation with different evaluation criteria**: Replicate the study using a different set of evaluation criteria to test whether high system-level correlations persist when human preferences shift
2. **Ablation study on prompt components**: Systematically remove individual components from Eval-Prompt 3 to isolate which elements harm performance
3. **Pretraining data analysis**: Analyze publicly available model cards to estimate fiction content in pretraining corpora for different model sizes, testing correlation between fiction exposure and story evaluation performance