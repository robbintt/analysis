---
ver: rpa2
title: Data Valuation with Gradient Similarity
arxiv_id: '2405.08217'
source_url: https://arxiv.org/abs/2405.08217
tags:
- data
- values
- dvgs
- dataset
- valuation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce Data Valuation with Gradient Similarity (DVGS),
  a method that assigns importance scores to data samples by comparing gradients of
  individual samples with batch gradients during training. Unlike previous methods
  like Data Shapley or DVRL, DVGS scales efficiently to large datasets and avoids
  expensive leave-one-out computations.
---

# Data Valuation with Gradient Similarity
## Quick Facts
- arXiv ID: 2405.08217
- Source URL: https://arxiv.org/abs/2405.08217
- Reference count: 40
- One-line primary result: DVGS achieves AUROC scores up to 0.954 and Spearman correlations above 0.4, outperforming or matching baselines like DVRL and Data Shapley while being 5–100x faster

## Executive Summary
Data Valuation with Gradient Similarity (DVGS) introduces a scalable method for assigning importance scores to training data samples by measuring their gradient similarity to batch gradients during training. The method tracks how similar individual sample gradients are to the target batch gradient across training steps, averaging these similarities as data values. Unlike previous methods such as Data Shapley or DVRL that require expensive leave-one-out computations, DVGS scales efficiently to large datasets while maintaining strong performance on tasks like corrupted label detection.

Evaluations demonstrate DVGS achieves AUROC scores up to 0.954 and Spearman correlations above 0.4 on benchmark datasets, outperforming or matching existing baselines while being 5–100x faster. The method is also tested on the LINCS L1000 dataset, where it better captures data quality than existing metrics. Overall, DVGS offers a computationally efficient and robust approach to data valuation that avoids the scalability limitations of previous methods.

## Method Summary
DVGS assigns data importance scores by comparing gradients of individual samples with batch gradients during training. For each training step, the method computes the gradient of the loss with respect to model parameters for a single sample, then measures its similarity to the gradient computed over the entire batch. This similarity is tracked across training steps and averaged to produce the final data value score. The approach leverages the intuition that samples whose gradients consistently align with the batch gradient direction contribute more meaningfully to model learning.

The method operates during standard training without requiring additional passes or expensive leave-one-out computations. By focusing on gradient similarity rather than explicit model retraining, DVGS achieves computational efficiency while maintaining effectiveness for tasks like identifying corrupted labels and characterizing data quality. The implementation integrates naturally with existing training pipelines and requires only gradient information that is already computed during backpropagation.

## Key Results
- Achieves AUROC scores up to 0.954 on corrupted label detection tasks
- Demonstrates Spearman correlations above 0.4 for data quality assessment
- Outperforms or matches baselines like DVRL and Data Shapley while being 5–100x faster

## Why This Works (Mechanism)
DVGS works by leveraging the principle that samples contributing consistent gradient directions aligned with the batch gradient are more valuable for model training. During each training iteration, when a sample's gradient points in a similar direction to the batch gradient, it reinforces the learning signal rather than introducing conflicting information. By tracking this alignment over multiple training steps, DVGS captures both the immediate and sustained contribution of each sample to the learning process.

The method exploits the fact that gradient information already exists during standard backpropagation, avoiding the need for additional expensive computations. Samples with high gradient similarity scores tend to be those that provide clear, consistent signals for model improvement, while those with low scores may introduce noise or conflicting gradients that hinder learning. This approach naturally handles the computational scaling issues that plague leave-one-out methods while maintaining effectiveness for data quality assessment.

## Foundational Learning
- **Gradient similarity computation**: Understanding how to measure cosine similarity or other metrics between individual sample gradients and batch gradients is essential for implementing DVGS. Quick check: Verify gradient similarity calculations produce values in expected ranges [-1, 1] for cosine similarity.

- **Data valuation concepts**: Familiarity with data Shapley values, influence functions, and other data valuation methods provides context for understanding DVGS's contributions and limitations. Quick check: Compare DVGS gradient similarity approach with data Shapley's leave-one-out retraining methodology.

- **Batch training dynamics**: Knowledge of how gradients behave during mini-batch training and how they relate to full-batch gradients helps understand when DVGS is most effective. Quick check: Analyze gradient variance across different batch sizes to understand DVGS sensitivity.

- **Optimization algorithms**: Understanding SGD, Adam, and other optimization methods is important since DVGS operates during training and gradient behavior varies by optimizer. Quick check: Test DVGS with different optimizers to assess robustness.

- **Computational complexity analysis**: Ability to evaluate the time and memory complexity of DVGS compared to baselines is crucial for assessing scalability claims. Quick check: Measure actual runtime and memory usage on datasets of increasing size.

## Architecture Onboarding
Component map: Data samples -> Gradient computation -> Similarity measurement -> Score aggregation -> Data value ranking

Critical path: During training, for each batch: compute individual sample gradients → compute batch gradient → measure cosine similarity → accumulate similarity scores → average across training steps → produce final data values

Design tradeoffs: DVGS trades some theoretical precision (compared to exact data Shapley) for massive computational efficiency. The method assumes that gradient similarity correlates with data value, which may not hold for all learning scenarios or optimization dynamics.

Failure signatures: DVGS may underperform when: gradients are noisy due to high learning rates, batch sizes are very small causing high variance in similarity measurements, or when data contributions are highly non-linear and not well-captured by gradient alignment.

First experiments:
1. Implement DVGS on a small CNN trained on MNIST and compare data values with ground truth corrupted labels
2. Benchmark DVGS against Data Shapley on a medium-sized dataset (e.g., CIFAR-10) to verify computational speedup claims
3. Test DVGS sensitivity to batch size by running experiments with batch sizes ranging from 16 to 256

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability advantages demonstrated primarily on medium-scale datasets; performance on ImageNet-scale datasets remains uncharacterized
- Evaluation focuses heavily on corrupted label detection, potentially limiting generalizability to other data valuation scenarios
- Theoretical connection between gradient similarity and actual contribution to model performance lacks rigorous proof

## Confidence
- Scalability claims: High - well-established that leave-one-out computations are expensive, but absolute scalability limits need validation
- Performance on corrupted label detection: Medium - strong results reported but verification of evaluation protocol needed
- Domain-specific applicability: Low - LINCS L1000 results promising but lack comprehensive benchmarking against multiple established methods

## Next Checks
1. Benchmark DVGS against existing methods on a large-scale image classification dataset (e.g., ImageNet or JFT-300M) to verify scalability claims and measure absolute performance gains in terms of both accuracy and computation time.

2. Conduct ablation studies to determine the sensitivity of DVGS to key hyperparameters (batch size, learning rate, number of training steps) and establish robust default settings for different model architectures and dataset sizes.

3. Evaluate DVGS on diverse data valuation tasks beyond corrupted label detection, including active learning acquisition functions, domain adaptation data selection, and fairness-aware data weighting, to assess its versatility across different machine learning scenarios.