---
ver: rpa2
title: 'GB-RVFL: Fusion of Randomized Neural Network and Granular Ball Computing'
arxiv_id: '2409.16735'
source_url: https://arxiv.org/abs/2409.16735
tags:
- proposed
- gb-rvfl
- ge-gb-rvfl
- rvfl
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in Random Vector Functional Link
  (RVFL) networks, specifically their uniform treatment of samples and scalability
  issues due to matrix inversion of the entire training dataset. The authors propose
  the Granular Ball RVFL (GB-RVFL) model, which uses granular balls (GBs) instead
  of individual training samples, improving scalability by inverting only the GB center
  matrix and enhancing robustness against noise through the coarse granularity of
  GBs.
---

# GB-RVFL: Fusion of Randomized Neural Network and Granular Ball Computing

## Quick Facts
- arXiv ID: 2409.16735
- Source URL: https://arxiv.org/abs/2409.16735
- Reference count: 40
- Primary result: GB-RVFL and GE-GB-RVFL outperform baseline models in accuracy, robustness, and scalability, achieving up to 85% accuracy and superior performance in noisy environments.

## Executive Summary
This paper addresses limitations in Random Vector Functional Link (RVFL) networks, specifically their uniform treatment of samples and scalability issues due to matrix inversion of the entire training dataset. The authors propose the Granular Ball RVFL (GB-RVFL) model, which uses granular balls (GBs) instead of individual training samples, improving scalability by inverting only the GB center matrix and enhancing robustness against noise through the coarse granularity of GBs. To further preserve the dataset's geometric structure, they introduce the Graph Embedding GB-RVFL (GE-GB-RVFL) model, which integrates graph embedding with granular computing. Experiments on UCI, KEEL, NDC, and biomedical datasets demonstrate that GB-RVFL and GE-GB-RVFL outperform baseline models in accuracy, robustness, and scalability.

## Method Summary
The paper proposes GB-RVFL and GE-GB-RVFL models that use granular ball computing to improve RVFL network scalability and robustness. The method generates granular balls from training data using k-means clustering based on purity thresholds, then computes output weights using matrix inversion formulas that operate on the smaller GB center matrix rather than the full training matrix. The GE-GB-RVFL variant adds graph embedding to preserve geometric relationships among granular ball centers. Hyperparameters are tuned via grid search with cross-validation, and models are evaluated on KEEL, UCI, NDC, and biomedical datasets.

## Key Results
- GB-RVFL and GE-GB-RVFL outperform baseline models in accuracy, robustness, and scalability
- Models achieve up to 85% accuracy on benchmark datasets
- Superior performance in noisy environments with up to 40% label noise
- Significant scalability improvements demonstrated on NDC datasets ranging from 50K to 100M samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using granular balls instead of individual samples improves scalability by reducing matrix inversion complexity from O(M³) to O(k³) where k << M.
- Mechanism: The model replaces the full training matrix with a smaller matrix of granular ball centers, dramatically reducing the size of matrices requiring inversion.
- Core assumption: The number of granular balls (k) is much smaller than the number of training samples (M).
- Evidence anchors:
  - [abstract]: "This approach enhances scalability by requiring only the inverse of the GB center matrix rather than the matrix of entire samples."
  - [section 3.3]: "Therefore, the time complexity of the proposed GB-RVFL is O(k³)+O(2M(iter)) if k ≤ (P + g) or O((P + g)³)+O(2M(iter)) if (P + g) < k. Since, k << M, therefore, O(k³) << O(M³)."
- Break condition: If granular ball generation creates k ≈ M, the scalability advantage disappears.

### Mechanism 2
- Claim: Coarse granularity of GBs improves robustness against noise by focusing on centers rather than individual noisy samples.
- Mechanism: Granular balls inherently average out noise by representing clusters of samples, and focusing on centers rather than individual points reduces noise impact.
- Core assumption: Noise is typically located farther from cluster centers, and averaging within GBs suppresses noise.
- Evidence anchors:
  - [abstract]: "improves robustness against noise and outliers through the coarse granularity of GBs"
  - [section 3.1]: "By leveraging the coarse nature of GBs, specifically focusing on their centers, we effectively harness the bulk of the information situated around these centers. This strategy renders our proposed GB-RVFL model less susceptible to noise and outliers"
- Break condition: If noise is uniformly distributed across all samples, including centers, the robustness benefit diminishes.

### Mechanism 3
- Claim: Graph embedding preserves dataset's geometric structure through intrinsic and penalty graph relationships among granular ball centers.
- Mechanism: The model constructs intrinsic and penalty graphs over the concatenated matrix D, preserving relationships between granular ball centers in the embedding space.
- Core assumption: The geometric relationships among granular ball centers contain meaningful structure that improves classification.
- Evidence anchors:
  - [abstract]: "integrates graph embedding (GE) into GB-RVFL, resulting in the proposed graph embedded GB-RVFL (GE-GB-RVFL) model, which fuses granular computing and graph embedding (GE) to preserve the topological structure of GBs."
  - [section 3.2]: "The GE framework adeptly manages the geometric relationships among the centers of GBs."
- Break condition: If the geometric structure among centers is not informative for the classification task, the added complexity provides no benefit.

## Foundational Learning

- Concept: Granular Computing
  - Why needed here: Forms the foundation for replacing individual samples with aggregated granular balls, enabling the scalability and robustness improvements.
  - Quick check question: What is the optimization problem for generating granular balls, and how do purity thresholds affect the number of balls created?

- Concept: Random Vector Functional Link (RVFL) Networks
  - Why needed here: The base architecture being modified; understanding RVFL's matrix inversion requirement and direct link connections is crucial.
  - Quick check question: How does RVFL calculate output weights using matrix inversion, and what makes this computationally expensive for large datasets?

- Concept: Graph Embedding
  - Why needed here: The mechanism for preserving geometric structure in the GE-GB-RVFL variant, requiring understanding of intrinsic and penalty graphs.
  - Quick check question: How do intrinsic and penalty graphs differ in their construction, and what relationships do they capture among data points?

## Architecture Onboarding

- Component map:
  Granular Ball Generator -> Random Projection Layer -> Direct Links -> Output Weight Calculator

- Critical path:
  1. Generate granular balls from training data
  2. Compute GB centers and labels
  3. Apply random projection to create hidden features
  4. Construct D matrix (concatenated GB features)
  5. Compute output weights using appropriate inversion formula
  6. Use model for classification

- Design tradeoffs:
  - Granularity vs Accuracy: Higher purity thresholds create more GBs (better accuracy) but reduce scalability
  - Complexity vs Performance: GE-GB-RVFL adds graph computation overhead but preserves geometric structure
  - Memory vs Speed: Computing inverse in feature space vs sample space affects both memory usage and computation time

- Failure signatures:
  - Poor scalability: k approaches M, matrix inversion becomes expensive
  - Reduced accuracy: Insufficient GBs to capture data structure, or purity threshold too high
  - Overfitting: Too many GBs relative to training data, or inappropriate graph regularization

- First 3 experiments:
  1. Test scalability: Run on NDC datasets with varying sizes (50K to 100M samples) and measure computation time
  2. Test robustness: Add label noise (5%, 10%, 20%, 30%, 40%) to UCI datasets and compare accuracy degradation
  3. Test granularity tradeoff: Vary purity threshold (0.79 to 1.0) on benchmark datasets and plot accuracy vs number of GBs generated

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scalability of GB-RVFL and GE-GB-RVFL models compare when applied to ultra-large datasets (e.g., 1 billion samples) beyond the NDC dataset range?
- Basis in paper: [explicit] The paper tests scalability up to 100 million samples on the NDC dataset and notes memory issues for baseline models at 30M+ samples, but does not explore datasets larger than 100M.
- Why unresolved: The experiments were limited to datasets up to 100 million samples, leaving scalability for ultra-large datasets unverified.
- What evidence would resolve it: Testing the models on datasets with 1 billion+ samples and comparing computational time, memory usage, and accuracy against baseline models.

### Open Question 2
- Question: What is the impact of different granular ball generation methods (e.g., adaptive or probabilistic approaches) on the performance and efficiency of GB-RVFL and GE-GB-RVFL models?
- Basis in paper: [explicit] The paper uses a 2-means clustering approach for granular ball generation but suggests exploring alternative methods in the conclusion.
- Why unresolved: The study focuses on a single granular ball generation method without comparing it to other potential approaches.
- What evidence would resolve it: Conducting experiments using alternative granular ball generation methods and comparing their computational efficiency and model performance.

### Open Question 3
- Question: How does the proposed GE-GB-RVFL model perform in unsupervised or semi-supervised learning settings with limited labeled data?
- Basis in paper: [inferred] The conclusion suggests extending the models to unsupervised or semi-supervised learning, indicating potential but untested applications.
- Why unresolved: The experiments are conducted in supervised learning settings, leaving the model's performance in unsupervised or semi-supervised scenarios unexplored.
- What evidence would resolve it: Evaluating the model on benchmark datasets with limited labeled data and comparing its performance to existing unsupervised or semi-supervised learning methods.

## Limitations

- Lack of detailed implementation specifications for granular ball generation, particularly splitting criteria and purity calculation methods
- Computational complexity analysis assumes k << M without specifying typical values of k relative to M for tested datasets
- Robustness mechanism relies on noise being located away from cluster centers without empirical validation of noise distribution characteristics

## Confidence

- High Confidence: The scalability improvement mechanism (reduced matrix inversion complexity) is well-founded theoretically, with clear mathematical justification showing O(k³) << O(M³) when k << M.
- Medium Confidence: The robustness claims are plausible based on granular computing principles, but require empirical validation across diverse noise patterns to confirm the assumption that noise concentrates away from centers.
- Medium Confidence: The geometric structure preservation through graph embedding is theoretically sound, but the practical benefit depends on whether the dataset's intrinsic geometric relationships are actually informative for classification.

## Next Checks

1. **Scalability Verification**: Measure granular ball count (k) relative to training samples (M) across all tested datasets to confirm k << M, and benchmark computation time for matrix inversion at both scales.

2. **Noise Robustness Testing**: Systematically vary noise distribution (uniform vs clustered, input vs label noise) and purity thresholds to quantify the relationship between noise characteristics and classification accuracy degradation.

3. **Granularity Tradeoff Analysis**: Conduct ablation studies across multiple purity thresholds to empirically determine the accuracy vs scalability curve and identify optimal granularity for different dataset characteristics.