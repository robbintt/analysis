---
ver: rpa2
title: LLM meets Vision-Language Models for Zero-Shot One-Class Classification
arxiv_id: '2404.00675'
source_url: https://arxiv.org/abs/2404.00675
tags:
- negative
- class
- positive
- classification
- threshold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of zero-shot one-class classification,
  where the goal is to discriminate between positive and negative query samples using
  only the label of the target class. The authors propose a two-step solution that
  first queries large language models (LLMs) for visually confusing objects and then
  relies on vision-language pre-trained models (e.g., CLIP) to perform classification.
---

# LLM meets Vision-Language Models for Zero-Shot One-Class Classification

## Quick Facts
- arXiv ID: 2404.00675
- Source URL: https://arxiv.org/abs/2404.00675
- Reference count: 40
- One-line primary result: Average macro F1 score of 79.60% across multiple datasets

## Executive Summary
This paper tackles the challenging problem of zero-shot one-class classification, where the goal is to discriminate between positive and negative query samples using only the label of the target class. The authors propose a novel two-step solution that first queries large language models (LLMs) for visually confusing objects and then uses vision-language pre-trained models like CLIP for classification. By adapting large-scale vision benchmarks, they demonstrate that their method outperforms existing baselines in this specific setting.

## Method Summary
The proposed method consists of two main steps: First, it queries LLMs to identify objects that are visually similar to the target class but semantically different. Second, it leverages vision-language models (specifically CLIP) to perform the actual classification task. The authors create a realistic benchmark where negative samples are drawn from the same dataset as positive samples, including a granularity-controlled version of iNaturalist where negative samples are at a fixed distance in the taxonomy tree from the positive ones. This approach allows for a more controlled evaluation of the zero-shot one-class classification task.

## Key Results
- Achieved an average macro F1 score of 79.60% across multiple datasets
- Outperformed adapted baselines like CLIPN and ZOC
- Demonstrated effectiveness on a realistic benchmark with controlled negative sampling

## Why This Works (Mechanism)
The two-step approach combines the semantic understanding capabilities of LLMs with the visual recognition strengths of vision-language models. By first querying LLMs for visually confusing objects, the method can identify potential false positives that might be difficult for traditional vision models to distinguish. The subsequent use of CLIP then leverages its cross-modal understanding to make final classification decisions, benefiting from the additional context provided by the LLM step.

## Foundational Learning
- **Zero-shot learning**: Needed because the model must classify without seeing examples of negative classes during training. Quick check: Can the model classify new categories without additional training data?
- **Vision-language models**: Required for their ability to understand both visual and textual information. Quick check: Does the model effectively bridge visual and textual representations?
- **Taxonomy-based sampling**: Essential for creating controlled negative samples at varying distances from positive classes. Quick check: Are negative samples appropriately challenging based on taxonomic distance?
- **Large language models**: Used to identify semantically similar but visually confusing objects. Quick check: Does the LLM effectively identify visually confusing categories?

## Architecture Onboarding

**Component Map**
LLM query -> Visual confusion identification -> CLIP classification

**Critical Path**
The critical path involves querying the LLM for visually confusing objects, processing this information to create a comprehensive negative sample set, and then using CLIP to perform the final classification. The performance bottleneck is likely the LLM query step, as it requires multiple API calls and processing to identify all relevant confusing objects.

**Design Tradeoffs**
The main tradeoff is between the additional computation required for the LLM query step and the potential performance gains. While this two-step approach may be more computationally expensive than direct classification, it provides better results by leveraging the complementary strengths of LLMs and vision-language models. Another tradeoff is the reliance on a specific taxonomy structure in datasets like iNaturalist, which may not generalize to all domains.

**Failure Signatures**
The method may fail when: (1) The LLM incorrectly identifies visually confusing objects, leading to incomplete or incorrect negative samples; (2) The taxonomy structure doesn't adequately represent visual similarities between classes; (3) The visual similarities between classes are too subtle for even CLIP to distinguish.

**3 First Experiments**
1. Test the method on a simple binary classification task with clearly distinct classes to establish a baseline
2. Evaluate the impact of different taxonomic distances on classification performance in iNaturalist
3. Compare the two-step approach against a direct CLIP classification baseline on the same datasets

## Open Questions the Paper Calls Out
None

## Limitations
- The controlled experimental design with negative samples from the same dataset may not generalize to real-world scenarios
- The two-step approach introduces potential cascading errors from incorrect LLM queries
- Reliance on taxonomy-based negative sampling may not capture all relevant visual similarities between classes

## Confidence
- Average macro F1 score of 79.60%: High (for the specific experimental conditions described)
- Generalization to broader real-world applications: Medium (due to the controlled nature of the negative sampling strategy)

## Next Checks
1. Test the proposed method on a dataset where negative samples are drawn from completely different domains or datasets to assess robustness to distribution shifts
2. Conduct ablation studies to quantify the individual contributions of the LLM query step versus the CLIP classification step
3. Evaluate the method's performance across multiple taxonomic distances in iNaturalist and compare against human performance in distinguishing between closely related classes