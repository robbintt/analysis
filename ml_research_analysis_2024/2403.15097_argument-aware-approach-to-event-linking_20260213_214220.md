---
ver: rpa2
title: Argument-Aware Approach To Event Linking
arxiv_id: '2403.15097'
source_url: https://arxiv.org/abs/2403.15097
tags:
- event
- linking
- data
- passage
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles event linking, which associates event mentions
  in text with relevant nodes in a knowledge base. Previous methods borrowed entity
  linking techniques but ignored the unique structure of events and the need to handle
  out-of-KB cases.
---

# Argument-Aware Approach To Event Linking

## Quick Facts
- arXiv ID: 2403.15097
- Source URL: https://arxiv.org/abs/2403.15097
- Authors: I-Hung Hsu, Zihan Xue, Nilay Pochh, Sahil Bansal, Premkumar Natarajan, Jayanth Srinivasa, Nanyun Peng
- Reference count: 24
- Primary result: 22% accuracy gain on out-of-KB test data, over 1% on in-KB data

## Executive Summary
This paper introduces an argument-aware approach to event linking that associates event mentions in text with relevant nodes in a knowledge base. Unlike previous methods that borrowed entity linking techniques, this approach recognizes that events have more complex structures and can be more effectively distinguished by examining their associated arguments. The method introduces a two-stage training approach with argument tagging and synthetic negative data generation to handle both in-KB and out-of-KB scenarios, achieving significant improvements over baselines.

## Method Summary
The method employs a retrieve-and-rerank architecture with bi-encoder and cross-encoder stages. Event extraction models (UniST and TagPrime) identify event types and arguments, which are then converted into special tags inserted into the input text. Synthetic out-of-KB training examples are generated by manipulating event arguments in existing in-KB instances using an LLM. The bi-encoder performs efficient candidate retrieval from the KB, while the cross-encoder performs precise re-ranking of top candidates, including a learned NIL class for out-of-KB cases. The approach is evaluated on Wikipedia and NYT datasets with both in-KB and out-of-KB test sets.

## Key Results
- 22% accuracy gain on out-of-KB test data compared to baseline methods
- Over 1% accuracy improvement on in-KB test data
- Significant improvements in handling NIL predictions for out-of-KB scenarios
- Robust performance across both Wikipedia and NYT datasets

## Why This Works (Mechanism)

### Mechanism 1
- Argument tagging improves model performance by making event-specific information explicit in the input
- Event extraction models (UniST and TagPrime) identify event types and arguments, which are converted into special tags
- Events can be more effectively distinguished when their associated arguments are made explicit
- Break condition: If event extraction models fail to identify relevant arguments or if argument information doesn't distinguish events in KB

### Mechanism 2
- Synthetic negative data generation helps the model learn to predict NIL labels robustly
- Out-of-KB training examples are created by manipulating event arguments in existing in-KB examples using an LLM
- The model learns to distinguish modified examples from real KB entries
- Break condition: If synthetic examples are too similar to real KB entries or too unrealistic

### Mechanism 3
- Two-stage training (bi-encoder for retrieval, cross-encoder for re-ranking) enables efficient and accurate event linking
- Bi-encoder performs efficient candidate retrieval while cross-encoder provides precise re-ranking with NIL class
- Balances efficiency with accuracy and handles out-of-KB cases through learned NIL class
- Break condition: If bi-encoder retrieval is poor, limiting cross-encoder's ability to re-rank good candidates

## Foundational Learning

- Concept: Event extraction and argument role identification
  - Why needed here: Method relies on extracting event types and arguments to create argument tags that improve model performance
  - Quick check question: What are the two models used for event extraction, and what datasets are they trained on?

- Concept: Knowledge base (KB) structure and retrieval methods
  - Why needed here: Understanding KB encoding and retrieval is essential for implementing bi-encoder stage and interpreting results
  - Quick check question: What is the purpose of the bi-encoder stage, and how does it differ from the cross-encoder stage?

- Concept: Out-of-KB prediction and NIL class handling
  - Why needed here: Method introduces learned NIL class to handle cases where events don't exist in KB, a key innovation
  - Quick check question: How does the method train the model to predict NIL labels, and why is this important for event linking?

## Architecture Onboarding

- Component map: Event extraction pipeline (UniST + TagPrime) → Text with argument tags → Bi-encoder (BERT-base) → Top-k candidate retrieval → Cross-encoder (BERT-base) → Re-ranking with NIL class → Final prediction
- Critical path: Event extraction → Bi-encoder retrieval → Cross-encoder re-ranking → Final prediction
- Design tradeoffs:
  - Adding event extraction adds preprocessing cost but improves accuracy
  - Synthetic data generation is cheaper than manual annotation but may introduce noise
  - Two-stage training is more complex but enables better handling of large KBs
- Failure signatures:
  - Poor bi-encoder recall suggests retrieval issues or KB encoding problems
  - Low NIL prediction accuracy suggests synthetic data quality issues
  - Performance gap between in-KB and out-of-KB suggests imbalanced training
- First 3 experiments:
  1. Verify event extraction correctly identifies arguments on sample data
  2. Test bi-encoder recall@10 on validation set before cross-encoder training
  3. Evaluate cross-encoder NIL prediction accuracy on synthetic data

## Open Questions the Paper Calls Out
- Question: How does the integration of event argument information affect the performance of the event linking model when applied to knowledge bases with a larger and more diverse set of event types?
- Question: What is the impact of using different event extraction models on the performance of the argument-aware event linking approach?
- Question: How does the performance of the argument-aware approach compare to other state-of-the-art methods when handling out-of-KB queries in a real-world scenario with a higher proportion of out-of-KB events?

## Limitations
- Unknown implementation details of event extraction models (UniST and TagPrime) including training procedures and hyperparameters
- Exact prompt and generation process for synthetic negative data with GPT-3.5-Turbo not fully specified
- Limited test set size (250 out-of-KB instances across both datasets) restricts statistical confidence
- Method's performance heavily depends on accuracy of event extraction preprocessing step

## Confidence

High confidence: Overall methodology is sound and two-stage architecture is well-established in related tasks
Medium confidence: Synthetic data generation approach is reasonable but effectiveness depends on implementation details
Medium confidence: Argument tagging mechanism is theoretically justified but empirical evidence for contribution is indirect

## Next Checks

1. Conduct ablation studies to isolate the impact of argument tagging versus synthetic data generation on overall performance
2. Test the method with open-source alternatives for event extraction to assess dependency on proprietary models
3. Evaluate performance across multiple random seeds to establish statistical significance of reported improvements, particularly for the out-of-KB case