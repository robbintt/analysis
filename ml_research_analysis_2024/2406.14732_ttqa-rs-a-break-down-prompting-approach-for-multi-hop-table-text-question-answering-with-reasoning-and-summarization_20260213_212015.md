---
ver: rpa2
title: TTQA-RS- A break-down prompting approach for Multi-hop Table-Text Question
  Answering with Reasoning and Summarization
arxiv_id: '2406.14732'
source_url: https://arxiv.org/abs/2406.14732
tags:
- table-text
- table
- question
- prompting
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TTQA-RS, a break-down prompting approach
  for multi-hop table-text question answering using open-source language models. The
  method addresses the challenge of answering complex questions that require reasoning
  over both tabular and textual data.
---

# TTQA-RS- A break-down prompting approach for Multi-hop Table-Text Question Answering with Reasoning and Summarization

## Quick Facts
- arXiv ID: 2406.14732
- Source URL: https://arxiv.org/abs/2406.14732
- Reference count: 19
- Primary result: TTQA-RS achieves state-of-the-art performance among prompting-based approaches, outperforming S3HQA's CoT model by 11.7% EM and 15.71% F1 on HybridQA development set

## Executive Summary
This paper introduces TTQA-RS, a break-down prompting approach for multi-hop table-text question answering using open-source language models. The method addresses the challenge of answering complex questions that require reasoning over both tabular and textual data. TTQA-RS employs a retrieval-augmented generation framework with an enhanced retriever that converts table rows to sentences before retrieval. The reader component decomposes complex questions into sub-questions, predicts answer entity types, generates table-text summaries, and performs chain-of-thought reasoning. Experiments on HybridQA and OTT-QA datasets demonstrate that TTQA-RS outperforms existing prompting methods.

## Method Summary
TTQA-RS is a retrieval-augmented generation framework for multi-hop table-text question answering. The method uses an enhanced retriever that converts table rows to sentences using BART-Large before retrieving relevant information with fine-tuned DPR. The reader component follows a five-step pipeline: summarization, question decomposition into sub-questions, entity type prediction, sub-question QA, and original question QA. The approach uses chain-of-thought prompting with various LLaMA model sizes to generate answers. The system combines retrieved table rows and text passages, generates table-text summaries, and breaks down complex questions to improve reasoning accuracy with open-source LLMs.

## Key Results
- Achieves state-of-the-art performance among prompting-based approaches on HybridQA development set
- LLaMA3-70B model achieves 11.7% higher exact match and 15.71% higher F1 than S3HQA's CoT model
- Achieves new state-of-the-art performance on OTT-QA development set with 63.17% EM and 73.52% F1 score
- Demonstrates effectiveness across multiple LLaMA model sizes (2-7B, 2-13B, 2-70B, 3-8B, 3-70B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting table rows to sentences before retrieval improves the retriever's understanding of table semantics.
- Mechanism: The retriever uses a fine-tuned Dense Passage Retriever (DPR) that operates on text embeddings. By converting structured table rows into natural language sentences, the semantic meaning becomes more explicit and accessible to the retriever's text-based embedding model.
- Core assumption: DPR models are better at retrieving relevant information when input is in natural language form rather than structured table format.
- Evidence anchors:
  - [section] "The table retriever retrievers the top 3 most relevant rows. It is important to first convert the rows to sentences using BART since the retriever is not always smart enough to interpret the rows for retrieval."
  - [section] "When the table is directly fed to the retriever, it fails to understand that rank 2 implies that the player came second in the National Football League. When the rows are converted to sentences before feeding to the DPR, it can better understand the table information and, in turn, retrieve more accurate information."
- Break condition: If the row-to-sentence conversion loses critical table structure information or if the retriever is specifically trained to handle structured table inputs directly.

### Mechanism 2
- Claim: Breaking down complex multi-hop questions into independent sub-questions reduces the cognitive load on the LLM and improves accuracy.
- Mechanism: The reader component decomposes the original question into smaller, more manageable sub-questions where the answer to one sub-question can be used to simplify the original question. This creates a chain of reasoning where each step handles a simpler problem.
- Core assumption: LLMs perform better on simpler, single-step reasoning tasks than on complex multi-hop reasoning in a single pass.
- Evidence anchors:
  - [section] "Breaking down the complex multi-hop QA problem into simple, smaller steps along with providing augmented information, including the table-text summary, can improve the performance of multi-hop table-text QA tasks using small open-source LLMs."
  - [section] "From here onwards, we will refer to the sub-question that can be answered first as the 'independent sub-question'. The independent sub-question for this question is - 'Which game has Andrew V oss provided commentary on?'. The answer to this sub-question is 'Rugby League 3'. This can be used to simplify the original complex question to the following - 'What was the release date of Rugby League 3?'."
- Break condition: If the decomposition creates incorrect or misleading sub-questions that don't logically connect to the original question.

### Mechanism 3
- Claim: Providing entity type prediction of expected answers guides the LLM toward generating answers in the correct format.
- Mechanism: The system uses Spacy to identify the expected answer entity type (e.g., date, person, location) and includes this information in the prompt, helping the LLM constrain its output to the appropriate format.
- Core assumption: LLMs can generate more accurate answers when given explicit constraints about the expected answer type.
- Evidence anchors:
  - [section] "We identify the entity type of the expected answer for both the independent sub-question and also for the original question. For the following question - 'What was the release date of the game which Andrew V oss provided commentary on?', the entity type of the expected answer is 'date'. Knowing that the expected answer is of type - 'date', makes the LLM's task of generating the answer considerably easier."
- Break condition: If the entity type prediction is incorrect or if the LLM ignores the entity type constraint.

## Foundational Learning

- Concept: Dense Passage Retrieval (DPR)
  - Why needed here: DPR is used for retrieving relevant table rows and text passages based on semantic similarity between query and document embeddings.
  - Quick check question: How does DPR calculate similarity between a question embedding and document embeddings to retrieve relevant passages?

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: CoT prompting is used to break down complex reasoning tasks into intermediate steps that the LLM can follow to arrive at the final answer.
  - Quick check question: What is the difference between standard prompting and chain-of-thought prompting in terms of how the LLM approaches problem-solving?

- Concept: Few-shot learning with LLMs
  - Why needed here: The model uses few-shot learning with different parameter models of LLaMA to demonstrate the effectiveness of the approach across various model sizes.
  - Quick check question: How does increasing the number of shots from 0 to 3 affect the exact match score in the ablation study?

## Architecture Onboarding

- Component map: Retriever (Enhanced DPR with BART conversion) -> Summarization -> Question Decomposition -> Entity Type Prediction -> Sub-question QA -> Original Question QA -> Answer
- Critical path: Question → Retriever (rows + passages) → Summarization → Question decomposition → Entity type prediction → Sub-question QA → Original question QA → Answer
- Design tradeoffs:
  - Using row-to-sentence conversion adds preprocessing overhead but improves retrieval accuracy
  - Breaking down questions creates error propagation risk but reduces hallucination
  - Open-source models are cost-effective but may not match closed-source model performance
- Failure signatures:
  - Low retrieval accuracy (HIT@1, HIT@3) indicates retriever issues
  - Inconsistent entity type predictions suggest problems with the Spacy integration
  - Sub-questions that don't logically connect to the original question indicate decomposition failures
- First 3 experiments:
  1. Test the enhanced retriever with and without row-to-sentence conversion on a small sample to verify the HIT@1 improvement
  2. Evaluate the question decomposition component independently to ensure sub-questions are correctly generated and logically connected
  3. Test the complete pipeline on a small subset of HybridQA to verify the end-to-end workflow before scaling to full datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does error propagation in multi-step reasoning affect overall accuracy when one component fails?
- Basis in paper: [inferred] from "Breaking down the problem into subproblems helps to reduce hallucination while reasoning with open-source LLMs, it also causes error propagation"
- Why unresolved: The paper acknowledges error propagation as a limitation but does not quantify how much accuracy degrades when early steps fail
- What evidence would resolve it: Experimental results showing performance degradation when individual components (like sub-question generation) are deliberately corrupted or made to fail

### Open Question 2
- Question: What is the optimal number of sub-questions for complex multi-hop reasoning tasks?
- Basis in paper: [inferred] from discussion of breaking down complex questions into sub-questions without specifying an optimal number
- Why unresolved: The paper uses a binary decomposition approach (one independent sub-question) but doesn't explore whether more granular decomposition improves performance
- What evidence would resolve it: Systematic experiments comparing performance across different numbers of sub-questions for the same complex queries

### Open Question 3
- Question: How does the row-to-sentence conversion step impact retrieval performance for different table structures?
- Basis in paper: [explicit] from "When the table is directly fed to the retriever, it fails to understand... When the rows are converted to sentences before feeding to the DPR, it can better understand"
- Why unresolved: The paper shows improved HIT@1 with conversion but doesn't analyze which table structures benefit most or if conversion is always beneficial
- What evidence would resolve it: Analysis of retrieval performance across tables with different structures (sparse vs dense, categorical vs numerical) with and without conversion

### Open Question 4
- Question: What is the relationship between summary quality and final QA performance?
- Basis in paper: [inferred] from human evaluation of summaries showing 91% correctness for question decomposition but no correlation analysis with final performance
- Why unresolved: Human evaluation shows summaries are mostly correct but doesn't demonstrate how summary quality impacts downstream QA accuracy
- What evidence would resolve it: Correlation analysis between summary evaluation metrics (correctness, inclusivity, completeness) and final QA performance metrics

### Open Question 5
- Question: How does the TTQA-RS approach scale to datasets with more complex multi-hop reasoning patterns?
- Basis in paper: [inferred] from limited experimentation with only HybridQA and OTT-QA datasets
- Why unresolved: Experiments are limited to two datasets with similar complexity levels, no exploration of more challenging multi-hop patterns
- What evidence would resolve it: Performance evaluation on datasets requiring more than two reasoning hops or with more complex inter-table dependencies

## Limitations

- Reliance on row-to-sentence conversion using BART-Large may not generalize well to different table structures or domains
- State-of-the-art claims are somewhat limited by comparison set excluding more recent methods
- Error propagation risk when early decomposition steps fail, affecting final answer accuracy

## Confidence

- High confidence: Core mechanism of breaking down complex questions into sub-questions improves LLM performance on multi-hop reasoning tasks
- Medium confidence: Specific claim that row-to-sentence conversion improves retrieval accuracy
- Medium confidence: Overall system performance claims given dependence on multiple components working together

## Next Checks

1. Test the enhanced retriever with and without row-to-sentence conversion on a small sample to verify the HIT@1 improvement - specifically compare retrieval accuracy when processing structured table rows directly versus after conversion to natural language sentences.

2. Evaluate the question decomposition component independently to ensure sub-questions are correctly generated and logically connected - test on a diverse set of multi-hop questions to verify that the decomposition maintains logical flow and that answers to sub-questions can be used to simplify the original question.

3. Test the complete pipeline on a small subset of HybridQA to verify the end-to-end workflow before scaling to full datasets - run the entire system on 10-20 questions to validate that all components work together as expected and identify any integration issues.