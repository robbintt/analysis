---
ver: rpa2
title: 'Pay Less On Clinical Images: Asymmetric Multi-Modal Fusion Method For Efficient
  Multi-Label Skin Lesion Classification'
arxiv_id: '2407.09999'
source_url: https://arxiv.org/abs/2407.09999
tags:
- clinical
- images
- fusion
- dermoscopy
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an asymmetric multi-modal fusion method for
  efficient multi-label skin lesion classification using clinical and dermoscopy images.
  The key idea is to use a lightweight model for clinical images and a heavier model
  for dermoscopy images, as dermoscopy images contain more crucial visual features
  for diagnosis.
---

# Pay Less On Clinical Images: Asymmetric Multi-Modal Fusion Method For Efficient Multi-Label Skin Lesion Classification

## Quick Facts
- arXiv ID: 2407.09999
- Source URL: https://arxiv.org/abs/2407.09999
- Authors: Peng Tang; Tobias Lasser
- Reference count: 13
- Primary result: Achieves 88.1% average AUC and 77.2% average accuracy using only 33.06 MB parameters for multi-label skin lesion classification

## Executive Summary
This paper introduces an asymmetric multi-modal fusion approach for efficient skin lesion classification using clinical and dermoscopy images. The key innovation is using a lightweight MobileNetV3 for clinical images while employing a heavier Swin-Transformer for dermoscopy images, based on the observation that dermoscopy images contain more crucial diagnostic features. The authors also propose an asymmetric attention block that uses clinical image features to enhance dermoscopy features, rather than bidirectionally exchanging information between modalities. Experiments on the seven-point checklist dataset demonstrate state-of-the-art performance with significantly reduced parameters compared to existing methods.

## Method Summary
The proposed method employs asymmetric model allocation and fusion for multi-label skin lesion classification. Clinical images are processed by a lightweight MobileNetV3 feature extractor, while dermoscopy images are processed by a heavier Swin-Transformer. An asymmetric attention block then uses clinical features to generate attention maps that enhance dermoscopy features. The system produces three parallel classification outputs: one from clinical features, one from dermoscopy features, and one from fused features. These predictions are combined using weighted averaging with weights optimized on the validation set.

## Key Results
- Achieves 88.1% average AUC and 77.2% average accuracy on the seven-point checklist dataset
- Uses only 33.06 MB of parameters, significantly less than state-of-the-art methods
- Outperforms Inception-combined, HcCNN, FM-FS, GIIN, CAFNet, and TFormer
- Demonstrates superior parameter efficiency while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a lightweight model for clinical images and a heavier model for dermoscopy images reduces total parameters without significantly hurting accuracy.
- Mechanism: Dermoscopy images contain more crucial diagnostic features, so allocating more capacity to them while using minimal capacity for supplementary clinical images yields better parameter efficiency.
- Core assumption: The marginal gain from improving clinical image feature extraction is small compared to the gain from improving dermoscopy image feature extraction.
- Evidence anchors:
  - [abstract]: "dermoscopy images exhibit more crucial visual features for multi-label skin lesion classification"
  - [section]: "the diagnostic accuracy of melanoma based on Dermoscopy Imaging (DI) is 25% higher than that of visual inspection with naked eyes"
- Break condition: If clinical images contain rare but critical features not captured by dermoscopy, or if the lightweight model fails to extract even the minimal needed features from clinical images.

### Mechanism 2
- Claim: Asymmetric attention blocks that only use clinical features to enhance dermoscopy features perform better than bidirectional attention with fewer parameters.
- Mechanism: By focusing the attention mechanism only on enhancing the primary modality (dermoscopy) with supplementary information (clinical), we avoid potential overfitting from mutual enhancement and reduce parameters by roughly half.
- Core assumption: Enhancing supplementary information does not improve classification as much as enhancing primary information, and may even hurt performance.
- Evidence anchors:
  - [abstract]: "we believe that enhancing the supplementary information CI may lead to overfitting, affecting the final classification"
  - [section]: "in contrast to the state-of-the-art bidirectional attention block (BAB) that mutually enhances the features of both modalities... our AAB only adopts clinical features to generate an attention map for enhancing dermoscopy features"
- Break condition: If clinical image features contain complementary information that could improve dermoscopy feature quality when bidirectionally exchanged.

### Mechanism 3
- Claim: Weighted averaging of predictions from clinical, dermoscopy, and fusion branches improves final classification accuracy.
- Mechanism: Each branch captures different aspects of the data; optimal weighting can combine their strengths while compensating for individual weaknesses.
- Core assumption: The three branches provide complementary predictions that can be optimally combined.
- Evidence anchors:
  - [section]: "During the testing stage, we use a weighted average scheme to fuse PD, PC and PF U into the final prediction PF I"
  - [section]: "where WD, WC and WF U are the corresponding weights for PD, PC and PF U, respectively, which are obtained by the conducting a weight search scheme on the validation dataset"
- Break condition: If one branch consistently dominates or the optimal weights are too sensitive to dataset variations.

## Foundational Learning

- Concept: Multi-modal learning and fusion strategies
  - Why needed here: The paper combines clinical and dermoscopy images, requiring understanding of how to effectively fuse information from different modalities
  - Quick check question: What are the differences between early, middle, and late fusion approaches in multi-modal learning?

- Concept: Attention mechanisms and their variants
  - Why needed here: The paper proposes an asymmetric attention block, which requires understanding standard attention mechanisms and their computational tradeoffs
  - Quick check question: How does a standard self-attention mechanism work, and what are its computational costs?

- Concept: Parameter efficiency and model architecture design
  - Why needed here: The paper focuses on reducing model parameters while maintaining accuracy, requiring knowledge of efficient architectures and parameter counting
  - Quick check question: What are the parameter differences between ResNet, ConvNext, Swin-Transformer, and MobileNetV3 architectures?

## Architecture Onboarding

- Component map: Clinical images → MobileNetV3 → Asymmetric Attention Blocks → Clinical classification head → Weighted averaging; Dermoscopy images → Swin-Transformer → Asymmetric Attention Blocks → Dermoscopy classification head → Weighted averaging; Fused features → Fusion classification head → Weighted averaging → Final prediction
- Critical path: Dermoscopy feature extraction → Asymmetric Attention Blocks → Fusion classification head → Weighted averaging → Final prediction
- Design tradeoffs: Parameter efficiency vs. accuracy, simplicity vs. expressiveness, computational cost vs. performance
- Failure signatures: Degraded accuracy when clinical images contain critical features not in dermoscopy images, overfitting when attention mechanism is too complex, poor performance when optimal weights are not found
- First 3 experiments:
  1. Baseline: Symmetrical fusion with two identical Swin-Transformer models and bidirectional attention blocks
  2. Asymmetric fusion: Light model for clinical images, heavy model for dermoscopy images, keep bidirectional attention blocks
  3. Asymmetric attention: Use the architecture from experiment 2 but replace bidirectional attention blocks with asymmetric attention blocks

## Open Questions the Paper Calls Out
None

## Limitations
- Clinical image utility assumptions: The paper assumes clinical images are merely supplementary and can be processed by a lightweight model, which may not hold if clinical images contain unique diagnostic features
- Attention mechanism generalization: The asymmetric attention block's effectiveness is based on intuition about overfitting, without empirical comparison with regularized bidirectional approaches
- Dataset specificity: All experiments are conducted on a single dataset (seven-point checklist), raising questions about the method's robustness across different skin lesion classification tasks

## Confidence

**High Confidence:**
- The parameter reduction claim (33.06 MB) and the core architecture design of asymmetric model allocation are well-supported by the experimental setup and results

**Medium Confidence:**
- The asymmetric attention mechanism's superiority over bidirectional attention blocks is supported by ablation studies, but lacks comparison with regularized bidirectional approaches

**Low Confidence:**
- The generalization of the weighted averaging scheme and the assumption that clinical images are always supplementary information are not thoroughly validated across different datasets or clinical scenarios

## Next Checks
1. **Cross-dataset validation**: Test the proposed method on at least two additional independent skin lesion classification datasets to verify parameter efficiency and accuracy claims hold across different imaging conditions and lesion types.

2. **Bidirectional attention comparison**: Implement a bidirectional attention block with appropriate regularization (dropout, weight decay) and compare its parameter count and performance against the asymmetric attention block to determine if the asymmetric approach is truly superior or if bidirectional attention with regularization would suffice.

3. **Clinical image sensitivity analysis**: Systematically evaluate model performance when clinical images are degraded (blur, noise, occlusion) or completely absent to quantify the actual contribution of clinical images and validate the assumption that they are merely supplementary information.