---
ver: rpa2
title: 'Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search
  Engines'
arxiv_id: '2402.19421'
source_url: https://arxiv.org/abs/2402.19421
tags:
- search
- engine
- information
- chat-based
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how chat-based search engines select websites
  for citation in responses. Using datasets from Bing Chat and a GPT-4-based RAG API,
  the research identifies that Bing Chat favors content that is more readable, logical,
  and less polarized, with lower perplexity scores.
---

# Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines

## Quick Facts
- arXiv ID: 2402.19421
- Source URL: https://arxiv.org/abs/2402.19421
- Reference count: 10
- Key outcome: Chat-based search engines prefer lower-perplexity, more readable, and less polarized content, with greater similarity among cited sources compared to conventional search engines

## Executive Summary
This study investigates how chat-based search engines like Bing Chat select websites for citation in their responses. By comparing Bing Chat's citation behavior with a GPT-4-based RAG API, the researchers demonstrate that both systems exhibit similar preferences for content that is readable, logical, less polarized, and has lower perplexity scores. The analysis reveals that these preferences likely stem from the underlying language models rather than manual curation. Additionally, the study finds that chat-based search engines produce more homogeneous citation sets compared to conventional search engines, potentially reducing information diversity in search results.

## Method Summary
The researchers collected 700 random queries from the HC3 dataset and ran them through New Bing to capture Bing Chat responses, citations, and conventional search results. They extracted website content chunks and created datasets with textual features using TextBlob and LIWC. Regression models comparing cited versus non-cited content were applied to both Bing Chat and conventional search rankings, then repeated with GPT-4 RAG API responses. The analysis focused on linguistic features including readability, analytic content, certitude, subjectivity, polarity, conversation, and perplexity to predict citation likelihood.

## Key Results
- Bing Chat and GPT-4 RAG API show consistent preferences for content with lower perplexity, higher readability, and reduced polarization
- Websites cited by RAG technologies demonstrate greater similarity compared to those ranked highly by conventional search engines
- The citation criteria appear to emerge from the foundational language models rather than being explicitly engineered

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chat-based search engines favor websites with lower perplexity scores because the underlying language model finds these texts more predictable and thus easier to process.
- Mechanism: The language model's training corpus influences its text preferences, leading it to select sources that align with its learned patterns during response generation.
- Core assumption: The citation selection process is driven by the language model's inherent text processing capabilities rather than manual curation.
- Evidence anchors:
  - [abstract]: "Bing Chat exhibits a preference for content that is not only readable and formally structured, but also demonstrates lower perplexity levels, indicating a unique inclination towards text that is predictable by the underlying LLM."
  - [section]: "Given that perplexity serves as a measure of text predictiveness from the perspective of the language model, this trend suggests an inclination towards sourcing content that resonates more closely with the foundational language model's training corpus."
  - [corpus]: Weak - the corpus analysis section is minimal, providing no direct evidence of perplexity-based selection patterns in the training data.
- Break condition: If citation selection shows no correlation with perplexity scores or if manual curation overrides language model preferences.

### Mechanism 2
- Claim: Chat-based search engines produce more homogeneous citation sets compared to conventional search engines because they need to generate coherent responses.
- Mechanism: The requirement to synthesize information into a single coherent response creates implicit pressure for the system to select sources with similar content or viewpoints.
- Core assumption: The need for response coherence drives selection of more similar sources rather than prioritizing diversity.
- Evidence anchors:
  - [abstract]: "Moreover, our investigation documents a greater similarity among websites cited by RAG technologies compared to those ranked highest by conventional search engines."
  - [section]: "The requirement to synthesize information into a single coherent response creates implicit pressure for the system to select sources with similar content or viewpoints."
  - [corpus]: Weak - corpus similarity metrics are not directly measured or analyzed in the provided text.
- Break condition: If citation sets show no greater similarity than conventional search results or if diversity is prioritized over coherence.

### Mechanism 3
- Claim: The unique citation criteria of chat-based search engines emerge naturally from the foundational LLM rather than being explicitly engineered.
- Mechanism: Direct comparison between Bing Chat's citation behavior and GPT-4 RAG API's behavior shows consistent preferences, indicating the criteria are inherent to the model rather than custom implementations.
- Core assumption: If both systems show the same citation preferences despite different implementations, the preferences must stem from the underlying model.
- Evidence anchors:
  - [abstract]: "Our findings indicate that the chat-based search engine exhibits a preference for content that is more readable and analytical, while demonstrating lower levels of polarity and a reduced conversational tone."
  - [section]: "We find that the criteria employed by the GPT-4 based RAG are qualitatively aligned with those observed in Bing Chat."
  - [corpus]: Moderate - the corpus comparison shows similar citation patterns between the two systems, though specific similarity metrics are not provided.
- Break condition: If citation criteria diverge significantly between the two systems or if explicit engineering decisions can explain the differences.

## Foundational Learning

- Concept: Perplexity as a measure of text predictability
  - Why needed here: The study uses perplexity to explain why chat-based search engines favor certain types of content over others
  - Quick check question: If a text has low perplexity for a language model, what does that tell us about how well the model can predict it?

- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: The study uses RAG API to isolate the language model's inherent preferences from any search engine-specific modifications
  - Quick check question: What is the key difference between how a conventional search engine and a RAG system retrieve and present information?

- Concept: Text similarity metrics
  - Why needed here: The study compares the similarity of cited websites to understand information diversity in chat-based search results
  - Quick check question: If two texts have high cosine similarity in their embeddings, what does that tell us about their content?

## Architecture Onboarding

- Component map: User query → Search Engine → Document Retrieval → Text Chunking → LLM Processing → Response Generation with Citations
- Critical path: User query → Document retrieval → Text chunking → LLM processing → Response generation with source attribution
- Design tradeoffs: Coherence vs. diversity in citations, speed vs. accuracy in document retrieval, predictability vs. novelty in source selection
- Failure signatures: Homogeneous citations suggesting bias, missing relevant sources, hallucination of unsupported claims, poor response coherence
- First 3 experiments:
  1. Compare perplexity scores of cited vs. uncited documents across multiple queries
  2. Measure cosine similarity between citations within the same response
  3. Test citation patterns using different language models with the same RAG pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of LLM architecture or training data lead to the observed text selection preferences (lower perplexity, more readable, formal, logical content)?
- Basis in paper: [explicit] The paper identifies that Bing Chat and GPT-4-based RAG exhibit preferences for content that is more readable, logical, less polarized, and less conversational, with lower perplexity scores. The authors suggest these preferences may emerge from the underlying language models rather than being manually curated.
- Why unresolved: While the paper demonstrates that these preferences exist and are consistent between Bing Chat and GPT-4-based RAG, it does not investigate the specific mechanisms or training aspects of LLMs that cause these preferences. The authors acknowledge that even LLM designers struggle to understand these emergent behaviors.
- What evidence would resolve it: Detailed analysis of LLM training data composition, architectural features, and their relationship to text selection behaviors. Controlled experiments varying training data or model architectures while measuring changes in text selection preferences would help isolate the causal factors.

### Open Question 2
- Question: How do chat-based search engines' text selection preferences impact the long-term evolution of web content and SEO practices?
- Basis in paper: [inferred] The paper demonstrates that chat-based search engines have distinct text selection criteria that differ from conventional search engines, with implications for website visibility and potential economic impacts on the search ecosystem.
- Why unresolved: The study identifies the existence of these preferences and their potential economic implications, but does not explore how website owners might adapt their content strategies or how this could lead to homogenization of web content over time.
- What evidence would resolve it: Longitudinal studies tracking changes in web content characteristics before and after the widespread adoption of chat-based search engines. Analysis of website traffic patterns and content adjustments made by owners in response to citation by chat-based search engines.

### Open Question 3
- Question: How do the reduced information diversity and greater similarity among cited websites in chat-based search engines affect user knowledge acquisition and decision-making compared to conventional search engines?
- Basis in paper: [explicit] The paper finds that websites cited by RAG technologies show greater similarity compared to those ranked highly by conventional search engines, suggesting a reduction in information diversity.
- Why unresolved: While the paper identifies this phenomenon, it does not investigate the practical consequences for users, such as whether they receive more or less comprehensive information, or how this affects their ability to form well-rounded opinions on topics.
- What evidence would resolve it: User studies comparing information acquisition, knowledge retention, and decision quality when using chat-based versus conventional search engines. Analysis of the breadth and depth of information presented in responses from each type of search engine.

## Limitations

- Analysis relies entirely on linguistic features and perplexity metrics without access to internal citation selection algorithms
- Dataset of 700 queries may not capture full diversity of search behaviors and content types
- Study focuses primarily on English-language content, limiting generalizability to other languages and cultural contexts

## Confidence

- Perplexity-based citation preferences: Medium
- Greater similarity among RAG citations: Medium
- Preferences emerging from LLM rather than manual curation: Low

## Next Checks

1. Conduct controlled experiments varying only the language model while holding all other system components constant to definitively establish whether citation preferences originate from the LLM.

2. Expand the dataset to include queries across multiple languages and domains, testing whether perplexity-based citation preferences persist across different linguistic and cultural contexts.

3. Perform a longitudinal analysis tracking how citation patterns evolve over time as language models are updated, to determine whether these preferences are stable characteristics of the models or artifacts of specific training iterations.