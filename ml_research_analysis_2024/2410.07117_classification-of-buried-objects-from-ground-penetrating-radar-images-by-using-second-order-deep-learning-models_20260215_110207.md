---
ver: rpa2
title: Classification of Buried Objects from Ground Penetrating Radar Images by using
  Second Order Deep Learning Models
arxiv_id: '2410.07117'
source_url: https://arxiv.org/abs/2410.07117
tags:
- matrix
- layers
- data
- classification
- covariance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of classifying buried objects
  using Ground Penetrating Radar (GPR) images. The authors propose a novel deep learning
  model that leverages covariance matrices to improve classification performance and
  robustness.
---

# Classification of Buried Objects from Ground Penetrating Radar Images by using Second Order Deep Learning Models

## Quick Facts
- arXiv ID: 2410.07117
- Source URL: https://arxiv.org/abs/2410.07117
- Authors: Douba Jafuno; Ammar Mian; Guillaume Ginolhac; Nickolas Stelzenmuller
- Reference count: 40
- One-line primary result: Proposed covariance-based deep learning models outperform conventional CNNs in classifying buried objects from GPR images, especially with limited training data.

## Executive Summary
This paper introduces a novel deep learning framework for classifying buried objects using Ground Penetrating Radar (GPR) images. The approach leverages covariance matrices constructed from CNN filter outputs, processed by specialized SPDNet layers, to achieve improved classification accuracy and robustness. The proposed models, SRCNet and RCNet, demonstrate superior performance compared to shallow networks and conventional CNNs, particularly when training data is limited and in the presence of mislabeled data.

## Method Summary
The proposed method involves extracting hyperbola thumbnails from GPR images and using them as input to a classical CNN (ResNet-34) to produce filter outputs. These outputs are then used to construct covariance matrices, which are processed by SPDNet layers for classification. Two variants are proposed: SRCNet, which uses outputs from multiple CNN layers, and RCNet, which uses outputs from a single layer. The models are trained with SGD optimizer and validated across 100 random seeds.

## Key Results
- SRCNet and RCNet outperform conventional CNNs, especially with limited training data.
- The proposed models show robustness to data shifts between training and test sets.
- SRCNet and RCNet demonstrate superior performance in the presence of mislabeled data compared to existing approaches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Second-order covariance pooling captures richer spatial correlations among CNN filter outputs, improving classification robustness compared to first-order features.
- Mechanism: By computing the covariance matrix of the filter outputs from early CNN layers, the model captures inter-filter correlations that encode structural information invariant to certain transformations (e.g., elevation changes, soil variations).
- Core assumption: The covariance matrix preserves discriminative information while reducing dimensionality, and the SPD layers can effectively learn from these matrices.
- Evidence anchors:
  - [abstract]: "The inputs of the proposed models are the hyperbola thumbnails... These thumbnails are then inputs to the first layers of a classical CNN, which then produces a covariance matrix using the outputs of the convolutional filters."
  - [section]: "In [48], it is also proposed to further fine-tune the embedding layers by backpropagating the loss gradient over the covariance estimating layers using matrix backpropagation calculus..."
  - [corpus]: Weak or missing; no direct mention of covariance pooling or SPD layers in neighbor papers.
- Break condition: If the covariance matrix becomes rank-deficient (e.g., too few samples or highly correlated filters), the SPD layers cannot process it reliably.

### Mechanism 2
- Claim: SPDNet layers (BiMap + ReEig + LogEig) learn a discriminative, low-dimensional representation of the covariance matrix on the Riemannian manifold.
- Mechanism: The BiMap layers perform learnable linear transformations on the SPD matrix, ReEig applies a ReLU-like rectification on eigenvalues to preserve positive definiteness, and LogEig maps the matrix to a tangent space where standard fully connected layers can operate.
- Core assumption: The manifold structure of SPD matrices is meaningful for the classification task, and the iterative dimensionality reduction preserves class-discriminative information.
- Evidence anchors:
  - [section]: "SPD Net is a model that, for k ≥ 1, takes as input a matrix Xk−1 ∈ Sym+dk−1... The dimensionality of the space is reduced through several BiMap convolution layers and a ReEig regularization layer..."
  - [abstract]: "... the covariance matrix is given to a network composed of specific layers to classify Symmetric Positive Definite (SPD) matrices."
  - [corpus]: Weak or missing; no neighbor papers mention SPD manifold learning or Riemannian geometry.
- Break condition: If the dimensionality reduction is too aggressive, class information is lost; if too shallow, overfitting occurs with limited data.

### Mechanism 3
- Claim: Using only early CNN layers (e.g., first 8) for covariance construction yields more robust features than fine-tuning a deep ResNet-34 on the full task.
- Mechanism: Early layers capture low-level, translation-invariant features that are more stable across different GPR conditions (frequency, elevation), whereas deeper layers overfit to dataset-specific patterns.
- Core assumption: Low-level features are more generalizable across GPR configurations than high-level features learned from ImageNet or the specific GPR dataset.
- Evidence anchors:
  - [section]: "First, we use a non-pretrained ResNet-34 model because the features learned by this model do not seem sufficient to classify our images, especially since we will only use the first l layers that typically contain generally simple features and are effective for covariance matrix calculations."
  - [abstract]: "... particularly when the number of training data decreases and in the presence of mislabeled data."
  - [corpus]: Weak or missing; no neighbor papers discuss CNN layer selection for covariance pooling.
- Break condition: If the task requires complex, high-level feature extraction (e.g., fine-grained object types), early layers may lack sufficient representational power.

## Foundational Learning

- Concept: Symmetric Positive Definite (SPD) matrices and their Riemannian geometry
  - Why needed here: SPD matrices are the output of covariance pooling; understanding their manifold structure is essential for designing SPDNet layers and backpropagation.
  - Quick check question: What operation must be applied to a matrix to ensure it remains SPD after a linear transformation in BiMap layers?
- Concept: Matrix backpropagation and eigenvalue decomposition
  - Why needed here: Gradients through SPD operations (e.g., ReEig, LogEig) require matrix calculus and stable EVD-based formulas.
  - Quick check question: How does the ReEig layer differ from a standard ReLU in terms of preserving matrix definiteness?
- Concept: Covariance pooling and its invariance properties
  - Why needed here: Covariance pooling provides robustness to spatial transformations and noise; understanding when and why it helps is key to applying it correctly.
  - Quick check question: Why does covariance pooling help when training and test data come from different weather modes?

## Architecture Onboarding

- Component map: Input -> ResNet-34 (first 8 layers) -> CovPool -> SPDNet -> FC + Softmax
- Critical path: Input → ResNet → CovPool → SPDNet → FC → Output
- Design tradeoffs:
  - Memory vs. richness: SRCNet stacks more filters (d=256) for richer covariance but uses more memory; RCNet (d=64) is leaner but may lose correlation info.
  - Depth vs. robustness: Using early CNN layers trades potential accuracy for invariance to GPR configuration changes.
  - SPDNet depth: More BiMap/ReEig layers allow finer dimensionality reduction but increase risk of overfitting with limited data.
- Failure signatures:
  - NaN or inf values after SPDNet: Likely due to ill-conditioned covariance or unstable EVD.
  - Degraded accuracy with more training data: Possible overfitting in SPDNet layers.
  - High variance across seeds: Insufficient regularization or poor covariance estimation.
- First 3 experiments:
  1. Verify covariance matrix computation: Run forward pass through CovPool and check that C is SPD (eigenvalues > 0).
  2. Test SPDNet gradient flow: Backprop through a single BiMap layer and confirm gradient norms are stable.
  3. Ablation on SRCNet vs RCNet: Train both on small training set (e.g., 20%) and compare accuracy/variance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed covariance matrix-based approach handle extremely high-dimensional SPD matrices in real-time GPR applications?
- Basis in paper: [explicit] The paper discusses the challenge of handling large covariance matrices and introduces SPDNet layers to reduce dimensionality, but does not address real-time performance implications.
- Why unresolved: The computational efficiency and latency of SPDNet layers in time-critical GPR applications remain unexplored.
- What evidence would resolve it: Benchmarking SPDNet against real-time constraints and comparing latency with conventional CNN approaches in live GPR systems.

### Open Question 2
- Question: What is the optimal trade-off between the number of ResNet layers (l) and the number of filters retained for covariance matrix construction in different GPR scenarios?
- Basis in paper: [inferred] The paper uses l=8 and retains 32 filters but does not systematically explore the parameter space for different soil types or object classes.
- Why unresolved: The sensitivity of classification performance to these architectural choices across diverse GPR conditions is not investigated.
- What evidence would resolve it: A comprehensive ablation study varying l and filter retention rates across multiple GPR datasets with different environmental conditions.

### Open Question 3
- Question: How does the proposed model generalize to multi-frequency GPR systems with simultaneous data streams?
- Basis in paper: [explicit] The paper evaluates single-frequency scenarios but mentions the possibility of using multiple frequencies without addressing multi-frequency data fusion.
- Why unresolved: The framework for integrating covariance matrices from different frequency bands is not developed.
- What evidence would resolve it: Testing the model on multi-frequency GPR datasets and comparing performance with single-frequency approaches.

### Open Question 4
- Question: What is the impact of varying GPR antenna elevation on the stability of covariance matrix features across different soil types?
- Basis in paper: [explicit] The paper mentions testing different elevations but does not analyze how elevation changes affect the covariance features' stability.
- Why unresolved: The relationship between elevation-induced transformations and covariance matrix robustness is not quantified.
- What evidence would resolve it: Systematic experiments measuring covariance matrix feature variance across elevation changes for different soil compositions.

## Limitations
- Limited exploration of computational efficiency and real-time performance.
- Reliance on high-quality hyperbola thumbnail extraction, which is not detailed.
- Lack of comparison to state-of-the-art GPR-specific methods beyond basic CNNs.

## Confidence
- **High**: Core mechanism (covariance pooling + SPDNet) for improving classification robustness with limited data.
- **Medium**: Generalization to different GPR configurations, since most experiments use a single frequency/elevation setup.
- **Low**: Computational efficiency claims, as training times and memory footprints are not reported.

## Next Checks
1. **Covariance matrix stability**: Verify that covariance matrices remain SPD across all training samples and that eigenvalue spectra are well-conditioned.
2. **SPDNet gradient flow**: Implement and test matrix backpropagation through BiMap and ReEig layers, checking for gradient vanishing or explosion.
3. **Cross-dataset generalization**: Train on one GPR frequency/elevation and test on another to quantify robustness to data shifts.