---
ver: rpa2
title: 'Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits'
arxiv_id: '2404.00267'
source_url: https://arxiv.org/abs/2404.00267
tags:
- llms
- words
- authors
- linguistic
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how Large Language Models (LLMs) impact
  the predictive power of linguistic patterns for authors' personal traits, including
  gender, age, political affiliation, personality, empathy, and morality. We used
  three LLMs (GPT3.5, Llama 2, Gemini) to rewrite original texts from various datasets
  and assessed how this affected prediction accuracy using multiple classifiers.
---

# Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits

## Quick Facts
- arXiv ID: 2404.00267
- Source URL: https://arxiv.org/abs/2404.00267
- Reference count: 33
- Primary result: LLMs reduce predictive power of linguistic patterns for author traits by ~6% on average

## Executive Summary
This study investigates how Large Language Models (LLMs) impact the ability to predict authors' personal traits from their writing. Using three LLMs (GPT3.5, Llama 2, Gemini) to rewrite texts from various datasets, the research found that LLMs slightly reduce predictive power for traits like gender, age, political affiliation, personality, empathy, and morality. While the average decline was 6% in F1 scores, the impact varied by trait type, LLM model, and prompt formulation. Some theoretically grounded lexical markers were washed away while others were preserved or even enhanced.

## Method Summary
The study applied three LLMs (GPT3.5, Llama 2, Gemini) with two neutral prompts ("Syntax Grammar" and "Rephrase") to rewrite original texts from multiple datasets. Researchers then trained classifiers (SVM, Logistic Regression, Random Forest, Gradient Boosting, Longformer) on original texts to predict author traits, and evaluated performance on both original and LLM-generated texts. They measured semantic similarity between versions and analyzed fine-grained lexical associations using LIWC and construct-specific dictionaries. Statistical significance was assessed using t-tests and Pearson correlations with Bonferroni correction.

## Key Results
- LLMs caused an average 6% decline in F1 scores for trait prediction
- GPT3.5 and Gemini showed more conservative rewriting than Llama 2
- SG (Syntax Grammar) prompt preserved more trait information than R (Rephrase) prompt
- Some lexical markers were completely washed away while others remained intact

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs slightly reduce the predictive power of linguistic patterns for author traits.
- **Mechanism:** LLM-generated rewrites introduce lexical and stylistic shifts that dilute trait-specific markers, especially those tied to fine-grained lexical cues.
- **Core assumption:** The predictive models rely on lexical frequency patterns that LLMs alter systematically when rewriting text.
- **Evidence anchors:** [abstract] "Overall, we found that LLMs slightly reduced predictive power (average 6% decline in F1 scores)"; [section 4.3] "LLMs wash away certain linguistic markers, such as the link between fairness and religion-related words or extraversion and social words"; [corpus] FMR score ~0.44 suggests moderate topical similarity to related studies; not a direct validation but indicates general relevance.
- **Break condition:** If the rewrites preserve original lexical frequencies or if classifiers are robust to minor style shifts, the decline would not occur.

### Mechanism 2
- **Claim:** Different LLMs and prompts have varying impacts on trait prediction.
- **Mechanism:** Training pipelines and fine-tuning (e.g., RLHF) shape LLM rewriting style; prompts constrain or relax stylistic changes.
- **Core assumption:** LLMs encode systematic biases toward certain lexical or stylistic patterns that interact with trait markers.
- **Evidence anchors:** [section 4.2] "GPT3.5 and Gemini generally more conservative than Llama 2" and "SG prompt is more preservative than the R prompt"; [section 4.3] "Gemini and GPT3.5 amplified correlations... using a Rephrase prompt, and GPT3.5 amplified these correlations using a Syntax Grammar prompt"; [corpus] Multiple studies on LLM personality biases support this, e.g., Santurkar et al. (2023) on personality traits in LLMs.
- **Break condition:** If LLMs are retrained without RLHF or with style-neutral objectives, the differential impact would diminish.

### Mechanism 3
- **Claim:** Fine-grained, theoretically grounded lexical markers are more vulnerable to LLM alteration than coarse-grained semantic meaning.
- **Mechanism:** LLMs preserve overall semantics but alter word-level associations that underlie trait markers, especially for constructs with narrow lexical bases.
- **Core assumption:** Trait markers depend on subtle lexical patterns rather than broad semantic similarity.
- **Evidence anchors:** [section 4.3] "semantic similarity between original and LLM-generated texts across all data sources, LLMs, and prompts was high (75% of scores were above 0.94)" yet "LLMs wash away certain linguistic markers"; [section 4.4] "LLMs' impact on the predictive power of linguistic cues is sensitive to the amount of available data"; [corpus] The related paper "The Shrinking Landscape of Linguistic Diversity in the Age of Large Language Models" supports this lexical erosion view.
- **Break condition:** If trait models use contextual embeddings instead of lexical frequencies, this vulnerability would reduce.

## Foundational Learning

- **Concept:** Statistical significance testing and multiple comparison correction (Bonferroni).
  - **Why needed here:** The study compares many models and traits; uncorrected p-values would inflate false positives.
  - **Quick check question:** What is the corrected significance threshold if testing 10 hypotheses at α=0.05?

- **Concept:** Lexical frequency analysis vs. semantic embedding similarity.
  - **Why needed here:** The paper contrasts fine-grained word counts with overall meaning preservation; both are needed to interpret LLM impact.
  - **Quick check question:** If two texts have cosine similarity 0.95 but differ in 20% of LIWC category frequencies, which measure better predicts trait marker stability?

- **Concept:** Correlation vs. group comparison (Pearson r vs. t-test).
  - **Why needed here:** Continuous traits (personality, empathy) use correlation; categorical traits (gender, affiliation) use t-tests.
  - **Quick check question:** When would a Pearson correlation be inappropriate for trait prediction?

## Architecture Onboarding

- **Component map:** Data ingestion -> text cleaning -> LLM rewrite (three models × two prompts) -> feature extraction (TF-IDF, embeddings, LIWC) -> classifier training (SVM, LR, RF, GB, Longformer) -> prediction comparison -> statistical testing
- **Critical path:** 1. Text rewrite by LLM; 2. Feature extraction; 3. Classifier training and evaluation; 4. Statistical significance assessment
- **Design tradeoffs:** Using multiple classifiers balances robustness vs. computational cost; LIWC lexicons provide interpretability but may miss context-dependent cues; aggregating essays per author reduces noise but may obscure prompt-level effects
- **Failure signatures:** High semantic similarity but low lexical correlation → fine-grained markers eroded; classifier performance near random → trait markers washed away; no significant differences across LLMs → prompt or data effect dominates
- **First 3 experiments:** 1. Run a single dataset (Essays) with GPT3.5 + Rephrase prompt; compare TF-IDF vs. embedding classifiers; 2. Add Llama 2 with same prompt; check if predictive decline differs significantly; 3. Switch to Syntax Grammar prompt; measure semantic vs. lexical similarity change

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do specific prompt formulations impact the preservation of lexical markers across different author traits?
- **Basis in paper:** [explicit] The paper shows SG prompt being more preservative than R prompt, with specific effects on demographic predictions
- **Why unresolved:** The paper only tests two prompt types and doesn't systematically explore how different prompt formulations affect preservation of specific markers
- **What evidence would resolve it:** Controlled experiments testing a wide range of prompt variations and their differential impact on preservation of various lexical markers

### Open Question 2
- **Question:** Do cultural differences in language use moderate the impact of LLMs on linguistic markers of personal traits?
- **Basis in paper:** [inferred] The paper mentions cultural background as a limitation and notes that expressiveness varies across cultures
- **Why unresolved:** The study only uses Western datasets and doesn't examine how cultural background affects LLM impact on linguistic markers
- **What evidence would resolve it:** Cross-cultural studies comparing LLM effects on linguistic markers across different cultural contexts

### Open Question 3
- **Question:** What specific aspects of LLM training (e.g., RLHF) cause certain markers to be preserved or washed away?
- **Basis in paper:** [explicit] The paper mentions RLHF promoting certain behaviors and observes differential effects across different LLMs
- **Why unresolved:** The paper identifies differential effects but doesn't investigate the underlying mechanisms causing certain markers to be preserved while others are washed away
- **What evidence would resolve it:** Detailed analysis of how different training stages and techniques affect preservation of specific lexical markers across traits

## Limitations

- Only two neutral prompts were tested, potentially missing the full spectrum of LLM rewriting behavior
- The study relies on Western datasets, limiting generalizability to other cultural contexts
- Unknown preprocessing steps and classifier hyperparameters create uncertainty in reproducing specific results

## Confidence

- **High confidence:** Overall finding that LLMs reduce predictive power for author traits (supported by consistent 6% decline across multiple classifiers)
- **Medium confidence:** Claim that fine-grained lexical markers are more vulnerable than coarse semantic meaning (based on high semantic similarity but washed away markers)
- **Medium confidence:** Differential impact across LLMs and prompts (consistent patterns across datasets but may depend on specific prompt wording)

## Next Checks

1. Replicate the analysis with contextual embeddings (BERT/RoBERTa) as features instead of LIWC/TF-IDF to test if trait markers depend on lexical vs. contextual patterns
2. Test additional prompt variations (creative vs. conservative) to map the full spectrum of LLM rewriting behavior on trait markers
3. Evaluate a broader set of classifiers including neural architectures to verify that the 6% decline is robust across modeling approaches