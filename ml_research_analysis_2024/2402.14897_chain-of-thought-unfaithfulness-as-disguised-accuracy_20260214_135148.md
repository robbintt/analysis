---
ver: rpa2
title: Chain-of-Thought Unfaithfulness as Disguised Accuracy
arxiv_id: '2402.14897'
source_url: https://arxiv.org/abs/2402.14897
tags:
- pythia
- flan-t5
- llama
- unfaithfulness
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study challenges a recent claim about scaling trends in Chain-of-Thought
  (CoT) faithfulness by evaluating three different families of open-source LLMs (Llama
  2, FLAN-T5 + UL2, and Pythia DPO) on multiple-choice benchmarks. Using a metric
  that measures answer changes with and without CoT, the authors initially replicate
  the reported scaling trends, including inverse scaling where larger models become
  less faithful.
---

# Chain-of-Thought Unfaithfulness as Disguised Accuracy

## Quick Facts
- arXiv ID: 2402.14897
- Source URL: https://arxiv.org/abs/2402.14897
- Authors: Oliver Bentham; Nathan Stringham; Ana Marasović
- Reference count: 40
- Primary result: Normalizing CoT faithfulness metric for answer choice bias eliminates inverse scaling trends and reveals strong correlation with accuracy

## Executive Summary
This study challenges a recent claim about scaling trends in Chain-of-Thought (CoT) faithfulness by evaluating three different model families on multiple-choice benchmarks. Using a metric that measures answer changes with and without CoT, the authors initially replicate reported scaling trends, including inverse scaling where larger models become less faithful. However, after normalizing the metric to account for answer choice bias, unfaithfulness drops significantly for smaller models and shows a strong linear correlation (R²=0.74) with accuracy. This suggests the original metric conflates faithfulness with accuracy and answer choice bias, raising doubts about its validity for evaluating CoT faithfulness.

## Method Summary
The study evaluates three model families (Llama 2, FLAN-T5+UL2, and Pythia DPO) across multiple multiple-choice benchmarks using nucleus sampling (p=0.95, temperature=0.8). They compute an original unfaithfulness metric measuring answer changes with/without CoT, then normalize it by the model's answer choice bias using randomly shuffled answer choices. The normalized metric is then correlated with accuracy across different model sizes to assess whether faithfulness measurement is confounded by task competence.

## Key Results
- Original unfaithfulness metric shows inverse scaling trends, with larger models appearing less faithful
- Normalization procedure reduces unfaithfulness for smaller models by accounting for answer choice bias
- Normalized unfaithfulness shows strong correlation (R²=0.74) with accuracy, suggesting metric conflates faithfulness with task competence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unfaithfulness metric is confounded by answer choice bias in less capable models
- Mechanism: Models with low accuracy randomly select answer choices, creating systematic bias. The unfaithfulness metric appears faithful when both CoT and no-CoT methods produce the same biased letter. Normalization removes this bias by comparing against shuffled answer choices.
- Core assumption: Models incapable of solving tasks exhibit systematic answer choice bias that correlates with original unfaithfulness metric
- Evidence anchors:
  - [abstract] "after normalizing the metric to account for a model's bias toward certain answer choices, unfaithfulness drops significantly for smaller less-capable models"
  - [section] "We first compute the normalization term as: N(M, D) = 1/|D| ∑x∈D/BD [NoCoT(M,x)=NoCoT(M,˜x)] where ˜x is a version of x where the answer choices have been randomly shuffled"

### Mechanism 2
- Claim: Higher accuracy models capture CoT reasoning in their parameters, reducing faithfulness metric sensitivity
- Mechanism: Larger models internalize CoT knowledge, using it to predict answers without explicit reasoning chains. This reduces answer changes between CoT and no-CoT conditions, making faithfulness metrics appear lower even if CoT reasoning is faithful.
- Core assumption: Larger models can capture and utilize reasoning patterns present in CoT in their learned parameters
- Evidence anchors:
  - [abstract] "It is conceivable that more capable models can capture information present in CoT steps in their parameters and use it to predict the answer even if they are not prompted to spell out this reasoning"
  - [section] "We hypothesize another possible mechanism by which it could be confounded— the ability of larger models to capture the knowledge of the CoTs in their weights"

### Mechanism 3
- Claim: Strong correlation between normalized unfaithfulness and accuracy indicates metric measures task competence rather than reasoning faithfulness
- Mechanism: After removing answer choice bias, the remaining unfaithfulness metric correlates with accuracy. This suggests the metric primarily measures task-solving ability, with more capable models showing both higher accuracy and what appears to be lower faithfulness due to their ability to solve problems without explicit reasoning chains.
- Core assumption: The normalized unfaithfulness metric primarily captures task-solving ability rather than genuine reasoning faithfulness
- Evidence anchors:
  - [abstract] "This normalized faithfulness metric is also strongly correlated (R2=0.74) with accuracy, raising doubts about its validity for evaluating faithfulness"
  - [section] "we discover a high correlation in the normalized condition. We contend that this strikingly clear correlation between normalized unfaithfulness and accuracy with CoTs suggests a simplification of the intricate concept of measuring reasoning faithfulness"

## Foundational Learning

- Concept: Normalization techniques in evaluation metrics
  - Why needed here: The study introduces normalization to account for answer choice bias, crucial for understanding proper faithfulness evaluation
  - Quick check question: How does normalizing by answer choice consistency affect faithfulness metric interpretation?

- Concept: Scaling laws and inverse scaling in machine learning
  - Why needed here: The research examines how faithfulness scales with model size, including inverse scaling trends important for understanding model behavior
  - Quick check question: What is inverse scaling, and how does it differ from typical scaling trends in larger models?

- Concept: Chain-of-Thought prompting and its evaluation
  - Why needed here: The entire study revolves around CoT prompting and faithfulness measurement, requiring understanding of both technique and evaluation methods
  - Quick check question: What distinguishes faithful CoT from post-hoc rationalization in model outputs?

## Architecture Onboarding

- Component map: Model families (Llama 2, FLAN-T5+UL2, Pythia DPO) → Multiple choice benchmarks → Faithfulness metrics (original/normalized) → Accuracy measurements → Scaling relationship plots

- Critical path: The metric calculation and normalization process is most critical. This involves generating both CoT and no-CoT responses, comparing answers, calculating base unfaithfulness, computing normalization factor by shuffling answer choices, and calculating final normalized metric.

- Design tradeoffs: The study uses quantized models for larger architectures due to resource constraints, which may affect metric accuracy. The normalization approach assumes answer choice bias is consistent across trials, which may not always hold. Using multiple model families provides broader generalization but introduces architectural differences that could affect results.

- Failure signatures: High unfaithfulness scores from low-accuracy models likely indicate answer choice bias rather than true unfaithfulness. Strong correlation between normalized unfaithfulness and accuracy suggests metric conflates task competence with reasoning faithfulness. Lack of clear trends across model families may indicate faithfulness is not a universal scaling property.

- First 3 experiments:
  1. Replicate the original unfaithfulness metric calculation on a small data subset to verify implementation matches the paper's methodology
  2. Implement and test the normalization procedure on a single benchmark to observe the effect of removing answer choice bias
  3. Compare normalized unfaithfulness vs accuracy correlation on a single model family to verify the strong relationship observed in the study

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design a faithfulness metric that is truly independent of answer choice bias and model accuracy?
- Basis in paper: [explicit] The authors identify that the original Lanham et al. (2023) unfaithfulness metric is confounded by both answer choice bias and task accuracy, and they propose a normalized version that accounts for answer choice bias. However, they find that even this normalized metric shows a strong correlation with accuracy, raising doubts about its validity for evaluating faithfulness.
- Why unresolved: While the authors propose a normalization technique to account for answer choice bias, they do not provide a definitive solution for creating a faithfulness metric that is completely independent of accuracy. The strong correlation between normalized unfaithfulness and accuracy suggests that the metric may still be capturing factors beyond genuine reasoning reliance.
- What evidence would resolve it: Development and validation of a faithfulness metric that shows no correlation with accuracy across a wide range of model sizes and tasks. This could be achieved through experiments that manipulate the internal representations of the model (e.g., using causal tracing or knowledge editing techniques) to isolate the effects of CoT reasoning on answer prediction.

### Open Question 2
- Question: What is the mechanism by which larger models capture information from CoT steps in their parameters, potentially leading to reduced faithfulness?
- Basis in paper: [inferred] The authors hypothesize that more capable models might be able to capture information present in CoT steps in their parameters and use it to predict the answer, even without explicit reasoning. This could explain the inverse scaling trend in faithfulness observed for larger models.
- Why unresolved: The paper does not provide empirical evidence for this hypothesis. It remains unclear how exactly larger models are able to leverage their internal representations to bypass explicit reasoning in CoT.
- What evidence would resolve it: Experiments using causal tracing or knowledge editing methods to identify and manipulate the specific parameters that encode information from CoT steps. By observing the effects of these interventions on answer prediction, researchers could determine the extent to which larger models rely on internal representations rather than explicit reasoning.

### Open Question 3
- Question: How does the relationship between model size and CoT faithfulness vary across different task domains and complexities?
- Basis in paper: [explicit] The authors evaluate models on a diverse set of multiple-choice benchmarks and addition tasks, finding varying patterns of unfaithfulness across model families and tasks. They note that the optimally faithful model size may depend on task difficulty, but their results do not show a clear relationship between task complexity and the point at which inverse scaling begins.
- Why unresolved: The paper provides a preliminary exploration of how task characteristics influence the relationship between model size and CoT faithfulness, but it does not offer a comprehensive analysis of the factors that contribute to this variation. Further research is needed to understand how task domain, complexity, and other factors interact with model size to influence faithfulness.
- What evidence would resolve it: Systematic experiments that vary task characteristics (e.g., domain, complexity, reasoning requirements) while controlling for model size and architecture. By analyzing the patterns of unfaithfulness across these variations, researchers could identify the key factors that influence the relationship between model size and CoT faithfulness.

## Limitations

- The normalization procedure assumes answer choice bias is consistent across trials, which may not hold for all model architectures
- The study uses quantized versions of larger models, which may affect faithfulness measurements differently than full-precision models
- Strong correlation between normalized unfaithfulness and accuracy suggests the metric may primarily measure task competence rather than reasoning faithfulness

## Confidence

**High Confidence:**
- The original Lanham et al. unfaithfulness metric can be replicated as reported
- Normalization procedure effectively reduces answer choice bias in smaller models
- Multiple model families show similar scaling trends, suggesting the effect is not model-specific

**Medium Confidence:**
- The interpretation that the metric conflates faithfulness with accuracy is well-supported but could benefit from additional validation on non-multiple-choice tasks
- The inverse scaling trend observed in the original metric is likely an artifact of answer choice bias rather than genuine unfaithfulness

**Low Confidence:**
- The claim that parameter-based knowledge capture is a significant mechanism for reduced unfaithfulness in larger models needs more direct evidence
- The generalization of findings to other types of reasoning tasks beyond multiple-choice questions is uncertain

## Next Checks

1. **Cross-Task Validation:** Test the normalized unfaithfulness metric on non-multiple-choice reasoning tasks (e.g., arithmetic word problems with free-form answers) to determine if the strong accuracy correlation persists when answer choice bias is eliminated.

2. **Full-Precision Model Comparison:** Rerun the experiments using full-precision versions of the larger models to assess whether quantization artifacts affect the unfaithfulness measurements, particularly for the inverse scaling trends.

3. **Temporal Consistency Analysis:** Evaluate whether the same models show consistent faithfulness metrics across multiple inference runs with different random seeds to determine if the observed effects are stable or subject to sampling variability.