---
ver: rpa2
title: '"I''ve Heard of You!": Generate Spoken Named Entity Recognition Data for Unseen
  Entities'
arxiv_id: '2412.19102'
source_url: https://arxiv.org/abs/2412.19102
tags:
- data
- spoken
- entities
- entity
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Spoken Named Entity Recognition
  (NER) for previously unseen entities. The authors propose HeardU, a method that
  generates Spoken NER data using a named entity dictionary (NED), a large language
  model (LLM), and a text-to-speech (TTS) system.
---

# "I've Heard of You!": Generate Spoken Named Entity Recognition Data for Unseen Entities

## Quick Facts
- arXiv ID: 2412.19102
- Source URL: https://arxiv.org/abs/2412.19102
- Reference count: 28
- This paper introduces HeardU, a method that generates spoken named entity recognition data for unseen entities using dictionaries, LLMs, and TTS systems, achieving substantial F1 score improvements across multiple settings.

## Executive Summary
This paper addresses the challenge of Spoken Named Entity Recognition (NER) for previously unseen entities. The authors propose HeardU, a method that generates Spoken NER data using a named entity dictionary (NED), a large language model (LLM), and a text-to-speech (TTS) system. To evaluate their approach, they release a new Spoken NER benchmark in Chinese (ST-CMDS-NER) with a corresponding human-refined NED containing 8,853 entities. Experiments show that HeardU achieves state-of-the-art performance in in-domain, zero-shot domain adaptation, and fully zero-shot settings.

## Method Summary
HeardU is a method for generating spoken named entity recognition data using a named entity dictionary (NED), a large language model (LLM), and a text-to-speech (TTS) system. The approach involves using the LLM to refine entity definitions and generate contextual sentences, then using TTS to convert these texts into spoken data. This synthetic data is used to train or adapt spoken NER models for previously unseen entities.

## Key Results
- Using the LLM-refined NED in English, HeardU achieves F1 score increases of 2.76% (in-domain), 19.76% (zero-shot domain adaptation), and 18.33% (fully zero-shot settings)
- Using the human-refined NED in Chinese, HeardU achieves F1 score increases of 32.98% (zero-shot domain adaptation) and 9.66% (fully zero-shot settings)
- The method demonstrates state-of-the-art performance across all evaluated settings

## Why This Works (Mechanism)
The method works by leveraging existing entity knowledge (NED) to generate synthetic spoken examples through LLM contextualization and TTS synthesis. This allows models to learn representations for entities they have never encountered in training data. The LLM refinement process enriches entity definitions with contextual information, while TTS provides the necessary audio data for spoken NER training.

## Foundational Learning
- Spoken Named Entity Recognition (NER): Identifying and classifying named entities in audio speech
  - Why needed: Traditional NER models trained on text cannot directly process spoken language
  - Quick check: Verify the model can identify entities in audio recordings

- Named Entity Dictionary (NED): A structured collection of entity names and their attributes
  - Why needed: Provides the base knowledge of entities that need to be recognized
  - Quick check: Ensure the dictionary covers relevant entity types and is comprehensive

- Zero-shot domain adaptation: Adapting a model to a new domain without any training examples from that domain
  - Why needed: Many real-world applications require recognizing entities in domains not seen during training
  - Quick check: Test model performance on completely unseen entity types

## Architecture Onboarding
- Component map: NED -> LLM Refinement -> Text Generation -> TTS Synthesis -> Spoken NER Model Training
- Critical path: The LLM refinement step is crucial as it determines the quality of generated training examples
- Design tradeoffs: Synthetic data generation versus collecting real spoken data; quality of TTS pronunciation versus computational cost
- Failure signatures: Poor entity recognition when TTS mispronounces entities; model confusion when LLM generates unnatural contexts
- First experiments: 1) Test TTS pronunciation accuracy for diverse entity types, 2) Evaluate LLM-generated contexts for entity clarity, 3) Measure impact of NED completeness on final model performance

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation relies on newly created benchmarks (ST-CMDS-NER and its NED) that have not undergone extensive community validation
- The claim of "state-of-the-art" performance is based on comparison with limited baselines
- The methodology's generalizability to other languages beyond English and Chinese is not demonstrated
- The paper does not address potential biases introduced by the LLM refinement process

## Confidence
- F1 score improvements in Chinese benchmark: **High** (supported by human-refined NED, though benchmark novelty is a caveat)
- F1 score improvements in English: **Medium** (smaller gains, LLM-refined NED quality unclear)
- Generality of HeardU method: **Low** (limited to two languages, no cross-linguistic validation)

## Next Checks
1. Conduct cross-linguistic validation by applying HeardU to at least two additional languages with different script systems to test generalizability
2. Perform ablation studies isolating the contributions of LLM refinement versus TTS quality to understand which component drives improvements
3. Evaluate HeardU on an existing, established spoken NER benchmark to verify claims independent of newly created datasets