---
ver: rpa2
title: Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel
  VQA
arxiv_id: '2401.15847'
source_url: https://arxiv.org/abs/2401.15847
tags:
- multipanel
- image
- subfigure
- images
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultipanelVQA, a novel benchmark designed
  to evaluate Multimodal Large Language Models (MLLMs) on their ability to interpret
  multipanel images. The benchmark includes 6,600 triplets of questions, answers,
  and multipanel images, focusing on understanding content and layout.
---

# Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel VQA

## Quick Facts
- arXiv ID: 2401.15847
- Source URL: https://arxiv.org/abs/2401.15847
- Authors: Yue Fan; Jing Gu; Kaiwen Zhou; Qianqi Yan; Shan Jiang; Ching-Chen Kuo; Xinze Guan; Xin Eric Wang
- Reference count: 19
- One-line primary result: Current MLLMs struggle with multipanel images despite human-level accuracy of ~99%

## Executive Summary
This paper introduces MultipanelVQA, a novel benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to interpret multipanel images. The benchmark includes 6,600 triplets of questions, answers, and multipanel images, focusing on understanding content and layout. Experiments reveal that current MLLMs struggle with multipanel images despite human-level accuracy of ~99%. Analysis shows that interference from neighboring subfigures and reduced pixel detail significantly impact performance. Additionally, subfigure size, layout style, and visual text hints affect MLLMs' comprehension. The study also explores the potential of adding sequential number captions as visual prompts, finding mixed benefits across models. Overall, the results highlight the need for further advancements in MLLMs for complex visual reasoning tasks.

## Method Summary
The authors created MultipanelVQA, a benchmark consisting of 6,600 triplets of questions, answers, and multipanel images. They evaluated several state-of-the-art MLLMs on this benchmark and compared their performance to human accuracy of ~99%. The study involved controlled experiments to analyze the impact of various factors on MLLM performance, including interference from neighboring subfigures, pixel resolution, subfigure size, layout style, and visual text hints. Additionally, they explored the effectiveness of adding sequential number captions as visual prompts.

## Key Results
- Current MLLMs struggle with multipanel images, showing significant performance drops compared to human accuracy (~99%)
- Interference from neighboring subfigures and reduced pixel detail significantly impact MLLM performance
- Subfigure size, layout style, and visual text hints affect MLLMs' comprehension of multipanel images
- Sequential number captions as visual prompts show mixed benefits across different MLLM models

## Why This Works (Mechanism)
The paper's approach of creating a dedicated benchmark for multipanel image understanding allows for a systematic evaluation of MLLMs' capabilities in handling complex visual layouts. By focusing on a specific type of visual reasoning task, the study can identify and analyze the particular challenges that MLLMs face when dealing with multipanel images. The controlled experiments on various factors affecting performance provide insights into the mechanisms by which MLLMs process and interpret multipanel visual information.

## Foundational Learning
- Multimodal Large Language Models (MLLMs): AI models that can process and reason about both text and images
  - Why needed: To enable AI systems to understand and interact with the multimodal nature of human communication and the world
  - Quick check: MLLMs should be able to answer questions about images and generate relevant descriptions

- VQA (Visual Question Answering): The task of answering questions about the content of images
  - Why needed: To test and improve models' ability to reason about visual information
  - Quick check: Models should correctly answer questions about objects, attributes, and relationships in images

- Multipanel images: Images divided into multiple subfigures or panels, often used in scientific figures, comics, or infographics
  - Why needed: To represent complex information that requires understanding the relationship between different visual elements
  - Quick check: Humans should be able to understand the overall message by interpreting the content and layout of individual panels

## Architecture Onboarding
- Component map: MLLMs -> MultipanelVQA benchmark -> Performance evaluation -> Controlled experiments -> Analysis of factors affecting performance
- Critical path: Input multipanel images and questions -> MLLM processing -> Answer generation -> Performance evaluation
- Design tradeoffs: Balancing the complexity of the benchmark with the current capabilities of MLLMs
- Failure signatures: MLLMs struggle with interference from neighboring subfigures, reduced pixel detail, and complex layouts
- First experiments:
  1. Evaluate MLLMs on MultipanelVQA benchmark
  2. Analyze the impact of interference from neighboring subfigures
  3. Assess the effect of pixel resolution on MLLM performance

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on a single VQA task type (content and layout understanding) and a specific visual domain (synthetic multipanel images)
- The effectiveness of sequential number captions as visual prompts showed inconsistent results across models, suggesting context-dependent benefits
- The benchmark may not generalize to broader multimodal reasoning capabilities beyond multipanel image understanding

## Confidence
- MLLMs' difficulty with multipanel images: High
- Interference and resolution effects: High
- Subfigure size and layout impact: Medium
- Sequential number captions effectiveness: Low

## Next Checks
1. Test MLLMs on diverse multipanel image types beyond synthetic layouts, including real-world scientific figures and comic panels
2. Evaluate whether fine-tuning on multipanel VQA examples improves performance or if architectural changes are needed
3. Conduct ablation studies isolating the impact of visual text hints versus other layout features on model performance