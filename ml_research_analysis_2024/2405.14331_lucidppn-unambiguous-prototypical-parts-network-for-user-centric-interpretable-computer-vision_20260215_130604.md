---
ver: rpa2
title: 'LucidPPN: Unambiguous Prototypical Parts Network for User-centric Interpretable
  Computer Vision'
arxiv_id: '2405.14331'
source_url: https://arxiv.org/abs/2405.14331
tags: []
core_contribution: The authors introduce LucidPPN, a novel prototypical parts network
  that disentangles color from shape and texture in its explanations. The method processes
  grayscale images for shape/texture information and a downscaled color input separately,
  then fuses the information for final classification.
---

# LucidPPN: Unambiguous Prototypical Parts Network for User-centric Interpretable Computer Vision

## Quick Facts
- arXiv ID: 2405.14331
- Source URL: https://arxiv.org/abs/2405.14331
- Reference count: 40
- The method processes grayscale images for shape/texture information and a downscaled color input separately, then fuses the information for final classification, demonstrating reduced ambiguity in explanations according to user studies

## Executive Summary
LucidPPN is a prototypical parts network that disentangles color from shape and texture in its explanations, allowing the model to clarify whether decisions are based on color, shape, or texture. The method processes grayscale images for shape/texture information and a downscaled color input separately, then fuses the information for final classification. This separation allows the model to clarify whether decisions are based on color, shape, or texture, and identifies object part correspondences across classes. The authors evaluate on four datasets (CUB-200-2011, Stanford Cars, Stanford Dogs, Oxford 102 Flower) showing competitive accuracy with baseline PPs-based models. Notably, LucidPPN demonstrates reduced ambiguity in explanations according to user studies, with participants achieving 67.9% accuracy when using LucidPPN explanations versus 60.0% for PIP-Net (p=0.044).

## Method Summary
LucidPPN uses a two-branch architecture where ShapeTexNet processes grayscale images to capture shape and texture features, while ColorNet processes downscaled color images to capture color information. Both networks use prototypical parts with K parts per class, and PDiscoNet provides segmentation masks to align prototypes with object parts. The final classification combines both streams through element-wise multiplication of their feature maps. The model is trained with three loss terms: LD for prototypical-part alignment, LS for shape/texture classification, and LA for final classification.

## Key Results
- Competitive accuracy with baseline PPs-based models on four datasets (CUB-200-2011, Stanford Cars, Stanford Dogs, Oxford 102 Flower)
- User study shows 67.9% accuracy when using LucidPPN explanations versus 60.0% for PIP-Net (p=0.044)
- Reduced ambiguity in explanations, particularly demonstrating when color is not important for classification (Stanford Cars dataset)

## Why This Works (Mechanism)

### Mechanism 1
The model disentangles color from shape and texture information at the input level, allowing separate processing and clearer attribution of visual features to model decisions. By feeding grayscale images to ShapeTexNet and downscaled color images to ColorNet, the model separates visual features into two independent streams. Each stream learns prototypical parts for a specific visual attribute (color vs. shape/texture). The final decision is made by fusing both streams, preserving both types of information while maintaining their distinct origins.

### Mechanism 2
ShapeTexNet and ColorNet learn K prototypical parts per class that correspond to the same semantic object parts across classes, enabling cross-class comparison. The PDiscoNet generates segmentation masks aligning K object parts. These masks are used to guide the ShapeTexNet to produce feature maps where each channel corresponds to a specific object part for a given class. The same K channels in ColorNet represent color prototypes for those parts. This alignment allows the model to compare corresponding parts (e.g., heads) between classes.

### Mechanism 3
User comprehension of model decisions improves because explanations clearly separate color from shape/texture features, reducing ambiguity in which visual cue drives classification. The model generates visualizations pairing grayscale patches (shape/texture) with color bars (dominant colors) for each prototypical part. Since users can see both the structural similarity and color difference independently, they can more easily identify whether classification relies on shape, texture, or color.

## Foundational Learning

- **Concept**: Image preprocessing with grayscale conversion and color downsampling
  - **Why needed here**: To create two separate input streams for ShapeTexNet and ColorNet, isolating shape/texture from color information
  - **Quick check question**: What is the formula used to convert RGB to grayscale in this paper, and why is it chosen?

- **Concept**: Prototypical parts networks and case-based reasoning
  - **Why needed here**: The model extends the concept of prototypical parts by separating them into color and non-color variants, so understanding the baseline PPs framework is essential
  - **Quick check question**: How does a prototypical parts network differ from a standard CNN in terms of interpretability?

- **Concept**: Multi-branch neural network design and feature fusion
  - **Why needed here**: LucidPPN uses two branches (ShapeTexNet and ColorNet) that must be trained jointly and fused at inference time
  - **Quick check question**: How are the feature maps from the two branches combined in LucidPPN, and what loss terms ensure proper alignment?

## Architecture Onboarding

- **Component map**: Input image → grayscale path (ShapeTexNet) and downscaled color path (ColorNet) → PDiscoNet segmentation masks → K×M prototypical part feature maps → element-wise multiplication → class scores

- **Critical path**: 1) Preprocess image into grayscale and downscaled color 2) Pass through ShapeTexNet and ColorNet to get feature maps 3) Multiply feature maps element-wise to fuse color and shape/texture 4) Pool and average to get class scores 5) Use PDiscoNet masks to compute LD and align prototypes with object parts

- **Design tradeoffs**: Separating color vs. shape/texture increases interpretability but may reduce joint feature learning; using 1x1 convolutions in ColorNet limits spatial context but focuses on per-pixel color; relying on PDiscoNet segmentation avoids manual annotation but inherits its errors

- **Failure signatures**: Low accuracy with high IoU (ShapeTexNet is well-aligned but color information is not discriminative); high accuracy but low color sparsity (ColorNet is learning irrelevant color patterns); user study shows no improvement over PIP-Net (disentanglement visualization is not effectively reducing ambiguity)

- **First 3 experiments**: 1) Train ShapeTexNet alone (no ColorNet) to measure baseline accuracy and validate grayscale sufficiency 2) Train ColorNet alone (no ShapeTexNet) to measure color-only discriminative power 3) Train full LucidPPN with varying K (e.g., K=2,4,8) to study impact of part granularity on accuracy and interpretability

## Open Questions the Paper Calls Out

### Open Question 1
How does LucidPPN's performance change when using different backbone architectures for ShapeTexNet? The paper mentions using ConvNeXt-tiny as the backbone for ShapeTexNet, but doesn't explore alternative architectures. Experiments comparing LucidPPN's accuracy using different backbone architectures (e.g., ResNet, Vision Transformer) would show if ConvNeXt-tiny is optimal or if other backbones perform better.

### Open Question 2
What is the impact of the number of object parts K on model interpretability and user understanding? While the authors tested different values of K for accuracy, they didn't measure how the number of parts affects the clarity or usefulness of explanations from a user perspective. User studies comparing comprehension and confidence when using LucidPPN models trained with different K values would reveal the optimal trade-off between accuracy and interpretability.

### Open Question 3
Can the disentanglement of color and shape/texture features be extended to other visual attributes like texture or contrast? The paper successfully disentangles color from shape/texture but doesn't explore extending this to other visual features mentioned in the literature. Experiments showing LucidPPN's performance when modified to separate texture, shape, and contrast features individually would demonstrate if the approach generalizes beyond color disentanglement.

## Limitations
- The user study size (13 participants) is modest, limiting generalizability of interpretability claims
- The approach relies on PDiscoNet for segmentation, inheriting potential errors from this pre-trained model
- No experiments on non-natural image domains (medical imaging, satellite imagery) to verify cross-domain applicability

## Confidence
- User comprehension improvement: Medium confidence (statistically significant but modest effect size, small study)
- Color-shape disentanglement mechanism: High confidence (explicitly implemented and evaluated)
- Cross-domain generalization: Low confidence (no experiments beyond natural images)
- Semantic consistency of K-part alignment: Medium confidence (reported IoU metrics but no external validation)

## Next Checks
1. Test the model on non-natural image datasets (medical imaging or satellite imagery) to verify cross-domain applicability of the color-shape disentanglement approach
2. Conduct a larger-scale user study (n>50) with domain experts to validate whether improved interpretability translates to practical decision-making benefits
3. Perform ablation studies comparing joint vs. separate color/shape training to quantify the interpretability-accuracy tradeoff across different K values