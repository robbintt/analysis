---
ver: rpa2
title: 'GME: Improving Universal Multimodal Retrieval by Multimodal LLMs'
arxiv_id: '2412.16855'
source_url: https://arxiv.org/abs/2412.16855
tags:
- retrieval
- data
- training
- image
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GME, a unified model for Universal Multimodal
  Retrieval (UMR) that can search across text, images, visual documents, and fused-modal
  content using a single embedding space. The authors address the challenge of imbalanced
  training data by developing an efficient data synthesis pipeline that generates
  1.1 million fused-modal pairs from Wikipedia passages using large language models
  and image generation/retrieval.
---

# GME: Improving Universal Multimodal Retrieval by Multimodal LLMs

## Quick Facts
- arXiv ID: 2412.16855
- Source URL: https://arxiv.org/abs/2412.16855
- Reference count: 40
- Primary result: Unified model achieving state-of-the-art performance on comprehensive UMRB benchmark for cross-modal retrieval

## Executive Summary
GME presents a unified approach to Universal Multimodal Retrieval (UMR) that enables searching across text, images, visual documents, and fused-modal content using a single embedding space. The key innovation addresses the challenge of imbalanced training data through an efficient data synthesis pipeline that generates 1.1 million fused-modal pairs from Wikipedia passages using large language models and image generation/retrieval. Built on Qwen2-VL series MLLMs, GME achieves state-of-the-art performance across all retrieval settings, outperforming specialized baselines including those for visual document retrieval. The model demonstrates modality-universal embeddings where semantically similar content across different modalities clusters together in the embedding space.

## Method Summary
The authors develop GME by building on the Qwen2-VL series multimodal large language models (MLLMs) and fine-tuning them for universal multimodal retrieval. The core challenge addressed is the lack of balanced training data across different modality pairs, particularly the scarcity of visual document data. To overcome this, they create an efficient data synthesis pipeline that generates 1.1 million fused-modal pairs by prompting LLMs to analyze Wikipedia passages and then using image generation and retrieval models to create corresponding visual content. The model is fine-tuned using LoRA with rank 8, incorporating hard negative mining to improve retrieval quality. The resulting embeddings place semantically similar content from different modalities in close proximity within the shared embedding space.

## Key Results
- Achieves state-of-the-art performance on UMRB benchmark across all retrieval settings
- Outperforms specialized baselines including those specifically designed for visual document retrieval
- Ablation studies confirm LoRA fine-tuning with rank 8 and hard negative mining are crucial for optimal performance

## Why This Works (Mechanism)
The effectiveness of GME stems from its unified embedding space that allows semantically similar content across different modalities to cluster together. By addressing the data imbalance problem through synthetic data generation, the model gains exposure to diverse modality combinations that would be difficult to obtain otherwise. The use of LoRA fine-tuning enables efficient adaptation of the base MLLM while preserving its strong multimodal understanding capabilities. Hard negative mining further refines the embedding space by pushing apart dissimilar items, making retrieval more discriminative.

## Foundational Learning
**Multimodal Large Language Models (MLLMs)**: Why needed - Provide strong cross-modal understanding as foundation for retrieval; Quick check - Verify base model supports all required modalities
**Cross-modal embeddings**: Why needed - Enable searching across different content types in shared space; Quick check - Ensure similar items cluster regardless of modality
**Data synthesis for training**: Why needed - Address scarcity of balanced training data across modality pairs; Quick check - Validate synthetic data maintains semantic fidelity
**LoRA fine-tuning**: Why needed - Efficiently adapt large models without full retraining; Quick check - Monitor parameter efficiency vs performance
**Hard negative mining**: Why needed - Improve retrieval discrimination by separating dissimilar items; Quick check - Measure impact on retrieval precision

## Architecture Onboarding

**Component map**: Input text/images → MLLM backbone → LoRA adapters → Shared embedding space → Retrieval index

**Critical path**: Data synthesis pipeline → Model fine-tuning → Embedding generation → Retrieval operations

**Design tradeoffs**: The unified approach sacrifices some modality-specific optimization for versatility, while synthetic data generation trades computational cost for balanced training coverage.

**Failure signatures**: Poor retrieval quality when synthetic data doesn't capture real-world semantic relationships; performance degradation on underrepresented modality pairs; computational bottlenecks in data synthesis pipeline.

**First experiments**: 1) Validate embedding space clusters semantically similar cross-modal content; 2) Test retrieval accuracy across all modality pairs; 3) Measure performance impact of synthetic data volume.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic data raises questions about generalization to real-world scenarios and potential bias introduction
- Performance on datasets beyond UMRB remains untested, limiting understanding of true versatility
- Computational costs of data synthesis pipeline and fine-tuning may be prohibitive for resource-constrained environments

## Confidence
- State-of-the-art UMRB benchmark results: High
- Effectiveness of synthetic data pipeline: Medium
- Computational efficiency of approach: Low

## Next Checks
1. Evaluate GME's performance on additional, diverse datasets beyond UMRB to assess generalization capabilities and robustness across different domains
2. Conduct comprehensive analysis of model behavior with varying amounts of synthetic versus real data to understand trade-offs and potential biases
3. Perform cost-benefit analysis of data synthesis pipeline and fine-tuning processes to determine feasibility in resource-limited settings and explore potential optimizations