---
ver: rpa2
title: T-HITL Effectively Addresses Problematic Associations in Image Generation and
  Maintains Overall Visual Quality
arxiv_id: '2402.17101'
source_url: https://arxiv.org/abs/2402.17101
tags:
- problematic
- associations
- prompt
- people
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel concept of problematic associations,
  referring to the links between demographic groups and semantic concepts that reflect
  negative narratives about the group. The authors present a taxonomy of problematic
  associations into four categories and propose a new methodology called T-HITL (Twice-Human-In-The-Loop)
  to address these associations in image generation models.
---

# T-HITL Effectively Addresses Problematic Associations in Image Generation and Maintains Overall Visual Quality

## Quick Facts
- **arXiv ID**: 2402.17101
- **Source URL**: https://arxiv.org/abs/2402.17101
- **Reference count**: 7
- **Primary result**: T-HITL fine-tuning reduces problematic associations from 25-100% to 0% in tested examples

## Executive Summary
This paper introduces T-HITL (Twice-Human-In-The-Loop), a novel methodology for addressing problematic associations in image generation models. Problematic associations refer to links between demographic groups and semantic concepts that perpetuate negative narratives. The approach combines LLM-generated prompt curation with two rounds of human annotation to fine-tune models while maintaining visual quality. The method demonstrates significant success in reducing specific problematic associations, showing reductions from 25-100% to 0% across three tested examples.

## Method Summary
The T-HITL methodology involves an iterative fine-tuning process where an LLM first generates curated prompts to address identified problematic associations. These prompts are then evaluated through two rounds of human annotation: first to assess visual quality, and second to verify the mitigation of problematic associations. The process creates a feedback loop where human judgments inform subsequent prompt generation and model refinement, ultimately producing a fine-tuned model that maintains overall visual quality while eliminating the targeted problematic associations.

## Key Results
- Woman-hippopotamus association reduced from 25% to 0% in model outputs
- Wheelchair-vegetable comparison association reduced from 50% to 0%
- Aromantic person-sheep association completely eliminated (100% to 0%)

## Why This Works (Mechanism)
The T-HITL approach works by creating a closed feedback loop between automated prompt generation and human evaluation. The LLM generates diverse, targeted prompts that challenge problematic associations, while human annotators provide quality control and validation. This dual-layer human involvement ensures that the fine-tuning process doesn't simply replace problematic content with low-quality or nonsensical outputs. The two-round annotation process specifically separates quality assessment from problematic association detection, allowing for more nuanced control over both aspects of model behavior.

## Foundational Learning
1. **Problematic associations** - Links between demographic groups and concepts that reinforce negative stereotypes; needed to identify what needs correction in models
2. **Prompt engineering** - Strategic text input design to guide model behavior; needed to create effective fine-tuning examples
3. **Human-in-the-loop fine-tuning** - Iterative model improvement incorporating human feedback; needed to ensure quality and relevance
4. **LLM prompt generation** - Using language models to create diverse training examples; needed for scalable prompt curation
5. **Visual quality assessment** - Evaluating image aesthetics and coherence; needed to prevent degradation during problematic association removal
6. **Demographic bias detection** - Identifying unfair associations in model outputs; needed to measure the effectiveness of interventions

## Architecture Onboarding

**Component map**: LLM -> Prompt Curation -> Human Annotation (Quality) -> Fine-tuning -> Human Annotation (Bias) -> Model

**Critical path**: The essential sequence for T-HITL effectiveness is: LLM generates prompts → Human annotators assess quality → Model fine-tuning → Human annotators verify problematic association removal → Final model deployment

**Design tradeoffs**: The approach prioritizes thorough human oversight over automation, sacrificing scalability for accuracy and quality control. The two-round annotation process adds time and cost but provides better separation of concerns between quality and bias detection.

**Failure signatures**: 
- If human annotators lack diversity, certain problematic associations may be missed
- If quality assessment is compromised, the fine-tuned model may produce incoherent or low-quality images
- If prompt generation is insufficient, the model may not adequately learn to avoid problematic associations

**3 first experiments**:
1. Test the impact of reducing human annotation rounds from two to one on both quality and bias mitigation
2. Compare T-HITL performance against automated bias detection methods for scalability assessment
3. Evaluate whether problematic associations re-emerge after model deployment through longitudinal monitoring

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on human annotation introduces potential subjective biases and scalability challenges
- Evaluation limited to three specific problematic associations, raising questions about generalizability
- Fine-tuning process may inherit biases from the underlying LLM used for prompt generation

## Confidence
- **High confidence**: Methodology clarity and quantitative results for tested examples
- **Medium confidence**: Generalizability to other problematic associations beyond the three tested cases
- **Medium confidence**: Long-term stability of the fine-tuned model and persistence of improvements

## Next Checks
1. Test T-HITL on a broader set of problematic associations across different demographic groups and semantic concepts to evaluate generalizability
2. Conduct a longitudinal study to assess whether problematic associations remain mitigated after multiple iterations of model updates and retraining
3. Implement an automated quality assessment pipeline to reduce reliance on human annotation while maintaining or improving accuracy in detecting problematic associations