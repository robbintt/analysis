---
ver: rpa2
title: On the hardness of learning under symmetries
arxiv_id: '2401.01869'
source_url: https://arxiv.org/abs/2401.01869
tags:
- learning
- networks
- neural
- functions
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the hardness of learning equivariant neural networks
  via gradient descent. While incorporating symmetries has empirically improved learning
  performance, prior research showed that learning non-symmetric networks is exponentially
  hard.
---

# On the hardness of learning under symmetries

## Quick Facts
- arXiv ID: 2401.01869
- Source URL: https://arxiv.org/abs/2401.01869
- Authors: Bobak T. Kiani; Thien Le; Hannah Lawrence; Stefanie Jegelka; Melanie Weber
- Reference count: 40
- Primary result: Symmetry does not alleviate the fundamental hardness of learning neural networks via gradient descent

## Executive Summary
This work studies the hardness of learning equivariant neural networks via gradient descent, addressing the gap between empirical success of symmetry-based models and theoretical hardness results. While incorporating symmetries has improved learning performance in practice, prior research showed that learning non-symmetric networks is exponentially hard. The authors prove that this hardness persists even with symmetries, demonstrating that an inductive bias towards symmetry is not strong enough to overcome the fundamental computational barriers in learning neural networks.

## Method Summary
The authors use the correlational statistical query (CSQ) model to prove lower bounds for learning various types of equivariant networks including graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks. They construct hard invariant functions by symmetrizing existing hard functions from the non-symmetric setting, and extend techniques from the general setting to symmetric function classes. The proofs show that learning these symmetric function classes requires superpolynomial or exponential query complexity in the relevant input dimension.

## Key Results
- Lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks all scale either superpolynomially or exponentially in the input dimension
- An inductive bias towards symmetry is not strong enough to alleviate the fundamental hardness of learning neural nets via gradient descent
- Even sparse invariant polynomials can be learned efficiently by algorithms other than gradient descent (like GROWING-BASIS), but remain hard for CSQ algorithms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Invariance under symmetry groups does not reduce the fundamental hardness of learning neural networks with gradient descent.
- **Mechanism**: Even though symmetry imposes an inductive bias that reduces the function class, the remaining class is still large enough to contain exponentially hard functions to learn in the CSQ model.
- **Core assumption**: The hardness results from non-symmetric networks can be extended to symmetric ones by constructing invariant versions of the hard functions.
- **Evidence anchors**: [abstract]: "We give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimension."
- **Break condition**: If the symmetry group is trivial (e.g., identity only), the problem reduces to the non-symmetric case, so hardness persists.

### Mechanism 2
- **Claim**: The construction of hard invariant functions relies on careful symmetrization of existing hard functions from the non-symmetric setting.
- **Mechanism**: By averaging over group actions (frame averaging) or using invariant Hermite polynomials, the hard functions from Diakonikolas et al. [2020] can be made invariant while preserving their hardness properties.
- **Core assumption**: The symmetrization process does not destroy the orthogonality or other properties needed for the hardness proofs.
- **Evidence anchors**: [section]: "Our proof techniques largely rely on extending techniques from the general setting to symmetric function classes. In many of our lower bounds, imposition of symmetry on a function class reduces the computational complexity of learning by a factor proportional to the size of the group."
- **Break condition**: If the group size is exponential in the input dimension, the reduction in complexity might offset the hardness.

### Mechanism 3
- **Claim**: Even sparse invariant polynomials can be learned efficiently by algorithms other than gradient descent, but not by CSQ algorithms.
- **Mechanism**: The GROWING-BASIS algorithm can learn sparse polynomials over invariant generators in polynomial time using statistical queries, but CSQ algorithms still require exponentially many queries.
- **Core assumption**: The generators of the invariant polynomial ring are independent and the distribution of their values is a product distribution.
- **Evidence anchors**: [section]: "A natural question is whether there exist real-valued function classes which are hard to learn in the CSQ setting (and hence by gradient descent with mean squared error loss), but efficient to learn via a more carefully constructed non-CSQ algorithm."
- **Break condition**: If the generators are not independent or the distribution is not a product distribution, the algorithm may not work.

## Foundational Learning

- **Concept**: Correlational Statistical Query (CSQ) Model
  - **Why needed here**: CSQ captures the capabilities and limitations of gradient descent algorithms, which are the primary method for training neural networks.
  - **Quick check question**: Can you explain why gradients of the mean squared error loss can be estimated using CSQ queries?

- **Concept**: Invariant Functions and Symmetry Groups
  - **Why needed here**: Understanding how invariance under symmetry groups constrains the function class is crucial for analyzing the hardness of learning.
  - **Quick check question**: What is the difference between a function being invariant and a function being equivariant under a symmetry group?

- **Concept**: Hermite Polynomials and Orthogonal Polynomials
  - **Why needed here**: Hermite polynomials and their invariant extensions are used to construct hard functions and analyze the complexity of learning.
  - **Quick check question**: How do invariant Hermite polynomials differ from standard Hermite polynomials, and why are they useful in this context?

## Architecture Onboarding

- **Component map**: Input features -> Message Passing Layer -> Invariant Feature Extraction -> Output Layer
- **Critical path**:
  1. Construct hard invariant functions by symmetrizing existing hard functions
  2. Prove that these functions are hard to learn in the CSQ model
  3. Extend the results to various types of invariant networks (GNNs, CNNs, etc.)
- **Design tradeoffs**:
  - Symmetry vs. Expressiveness: More symmetry reduces the function class but may also limit the ability to represent complex functions
  - Computational Complexity: Learning invariant functions may be harder due to the need to respect symmetry constraints
- **Failure signatures**:
  - Inability to fit training data even with overparameterized networks
  - Poor generalization to test data despite good training performance
- **First 3 experiments**:
  1. Train a GNN on the hard functions from Theorem 3 and observe failure to fit training data
  2. Train a CNN on the hard functions from Theorem 7 and observe overfitting or poor generalization
  3. Compare the performance of gradient descent with a non-CSQ algorithm (e.g., GROWING-BASIS) on sparse invariant polynomials

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are there practical learning algorithms for invariant function classes that avoid the exponential complexity shown in the paper?
- Basis in paper: [explicit] The paper shows exponential lower bounds for learning invariant neural networks, but also notes that sparse invariant polynomials can be learned efficiently via a non-gradient algorithm.
- Why unresolved: The paper provides a separation between gradient-based (CSQ) and non-gradient (SQ) algorithms for invariant polynomials, but does not explore other algorithmic approaches or whether they could circumvent the hardness results for general invariant networks.
- What evidence would resolve it: Demonstrating an efficient learning algorithm (polynomial time) for a rich class of invariant functions that avoids the exponential complexity lower bounds, or proving that such an algorithm is impossible under standard complexity assumptions.

### Open Question 2
- Question: Can the hardness results be extended to continuous symmetry groups, such as SO(3) or SE(3)?
- Basis in paper: [inferred] The paper focuses on discrete symmetry groups (permutation subgroups) and does not address continuous groups, which are common in applications like 3D vision and robotics.
- Why unresolved: Extending the proof techniques from discrete to continuous groups may require new mathematical tools, as the structure of continuous groups and their representations differ significantly from discrete ones.
- What evidence would resolve it: Proving similar hardness results for continuous symmetry groups, or identifying specific continuous groups for which efficient learning is possible.

### Open Question 3
- Question: What is the role of data augmentation in mitigating the learning hardness for invariant functions?
- Basis in paper: [explicit] The paper discusses the benefits of symmetries for generalization but does not explore how data augmentation interacts with the computational complexity of learning.
- Why unresolved: Data augmentation is a common technique in practice to improve learning, but its theoretical impact on the hardness of learning invariant functions is not addressed in the paper.
- What evidence would resolve it: Analyzing whether data augmentation can reduce the query complexity or runtime needed to learn invariant functions, or proving that it cannot circumvent the hardness results under certain conditions.

## Limitations

- The analysis focuses on discrete symmetry groups (permutation subgroups), leaving open questions about continuous symmetry groups common in practical applications
- The constructed hard functions are theoretical constructs that may not correspond to natural data-generating processes encountered in practice
- All hardness results assume Gaussian or uniform inputs, which may not reflect real-world data distributions where symmetries naturally occur

## Confidence

- **High Confidence**: The main claim that symmetry does not alleviate learning hardness for general equivariant networks
- **Medium Confidence**: The specific quantitative bounds (exponential vs superpolynomial scaling) for different network architectures
- **Medium Confidence**: The claim that no non-CSQ algorithm can efficiently learn the hard invariant functions

## Next Checks

1. Implement the GNN architecture from Theorem 3 and attempt to learn the HER,n functions with varying n. Measure actual query/sample complexity and compare with the theoretical lower bounds.

2. Test whether different initialization schemes, learning rates, or optimizer choices (SGD vs Adam) can overcome the theoretical hardness barriers identified for specific invariant networks.

3. Replicate the analysis for rotational symmetry in 2D/3D, verifying whether similar hardness results hold for practical equivariant networks used in computer vision and molecular modeling.