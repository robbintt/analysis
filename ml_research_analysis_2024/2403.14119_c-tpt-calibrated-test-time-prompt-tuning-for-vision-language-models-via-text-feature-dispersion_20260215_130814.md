---
ver: rpa2
title: 'C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text
  Feature Dispersion'
arxiv_id: '2403.14119'
source_url: https://arxiv.org/abs/2403.14119
tags: []
core_contribution: This paper addresses the problem of improving calibration during
  test-time prompt tuning for vision-language models like CLIP. The core method idea
  is to leverage the inherent properties of CLIP, specifically the dispersion of class-embedded
  text features, to guide the optimization process towards better calibration.
---

# C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion

## Quick Facts
- arXiv ID: 2403.14119
- Source URL: https://arxiv.org/abs/2403.14119
- Authors: Hee Suk Yoon; Eunseop Yoon; Joshua Tian Jin Tee; Mark Hasegawa-Johnson; Yingzhen Li; Chang D. Yoo
- Reference count: 38
- Primary result: Introduces a method to improve calibration during test-time prompt tuning for vision-language models using text feature dispersion

## Executive Summary
This paper addresses the problem of improving calibration during test-time prompt tuning for vision-language models like CLIP. The core method idea is to leverage the inherent properties of CLIP, specifically the dispersion of class-embedded text features, to guide the optimization process towards better calibration. The authors introduce the Average Text Feature Dispersion (ATFD) metric and propose Calibrated Test-time Prompt Tuning (C-TPT), which maximizes ATFD during prompt tuning. Through extensive experiments on various datasets and CLIP architectures, the authors demonstrate that C-TPT effectively improves the calibration of test-time prompt tuning without requiring labeled data.

## Method Summary
C-TPT introduces a novel approach to improve calibration during test-time prompt tuning for vision-language models. The method leverages the Average Text Feature Dispersion (ATFD) metric, which measures how dispersed class-embedded text features are in CLIP's embedding space. During test-time prompt tuning, C-TPT maximizes ATFD alongside the standard entropy minimization objective, effectively regularizing the prompt optimization process to produce more calibrated predictions. The method operates without requiring any labeled data and can be applied to various CLIP architectures and datasets, showing consistent improvements in calibration metrics like Expected Calibration Error (ECE) across different distribution shifts.

## Key Results
- C-TPT improves Expected Calibration Error (ECE) by up to 41% compared to standard test-time prompt tuning
- The method maintains competitive accuracy while significantly improving calibration on multiple datasets including Caltech101, DTD, StanfordCars, and CUB200-2011
- C-TPT generalizes well to out-of-distribution data, showing consistent calibration improvements on ImageNet-V2 and ImageNet-A

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dispersion of class-embedded text features is inversely related to calibration error in CLIP models.
- Mechanism: Prompts that lead to higher dispersion of text features across classes result in better-calibrated predictions because the model's confidence is more evenly distributed across classes rather than concentrated on a single class.
- Core assumption: The cosine similarity space used by CLIP preserves meaningful geometric relationships between text features that correlate with calibration quality.
- Evidence anchors:
  - [abstract] "we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions."
  - [section 4] "we identified a distinct pattern; poorly calibrated prompts typically led text features to cluster cohesively. In contrast, well-calibrated prompts manifested a wider dispersion in the text feature space."
  - [corpus] Weak - only 1 related paper directly addresses calibration in prompt tuning

### Mechanism 2
- Claim: Maximizing Average Text Feature Dispersion (ATFD) during prompt tuning serves as a regularization signal that improves calibration without requiring labeled data.
- Mechanism: By adding ATFD maximization as an auxiliary objective during test-time prompt tuning, the optimization process is guided to find prompts that spread text features across classes, which correlates with better calibration.
- Core assumption: The relationship between ATFD and calibration error is stable across different CLIP architectures and datasets.
- Evidence anchors:
  - [abstract] "Introducing the Average Text Feature Dispersion (ATFD), we establish its relationship with calibration error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT), for optimizing prompts during test-time with enhanced calibration."
  - [section 5.1] "Across multiple datasets, we conducted an evaluation using various hard prompts... Our comprehensive analysis showed a notable negative correlation between ATFD and ECE across different CLIP models and datasets."
  - [corpus] Weak - no direct evidence from corpus about ATFD as calibration regularizer

### Mechanism 3
- Claim: Test-time prompt tuning without calibration guidance tends to overfit to individual samples, leading to overconfident predictions.
- Mechanism: Standard TPT optimizes for accuracy by minimizing prediction entropy, which can push the model toward overconfident predictions on individual samples, worsening calibration.
- Core assumption: Entropy minimization during TPT inherently drives overconfidence due to the lack of calibration constraints.
- Evidence anchors:
  - [abstract] "These prompts have been mainly developed to improve accuracy, overlooking the importance of calibration—a crucial aspect for quantifying prediction uncertainty."
  - [section 4] "models trained with such loss are prone to overconfidence, which is one of the direct causes of calibration error."
  - [corpus] Moderate - multiple related papers discuss calibration issues in TPT

## Foundational Learning

- Concept: Expected Calibration Error (ECE) and reliability diagrams
  - Why needed here: Understanding how calibration is measured and what constitutes good calibration is essential for evaluating C-TPT's effectiveness
  - Quick check question: If a model has 80% accuracy and 80% confidence on its predictions, is it perfectly calibrated?

- Concept: Text feature space and cosine similarity in CLIP
  - Why needed here: C-TPT relies on understanding how text features are distributed in CLIP's embedding space and how this relates to classification
  - Quick check question: In CLIP's zero-shot classification, what happens to the text features of different class descriptions during inference?

- Concept: Test-time adaptation and entropy minimization
  - Why needed here: Understanding how standard TPT works and why it can lead to calibration issues is crucial for grasping C-TPT's motivation
  - Quick check question: Why does minimizing prediction entropy during test-time adaptation potentially lead to overconfidence?

## Architecture Onboarding

- Component map: CLIP visual encoder -> CLIP text encoder -> shared embedding space -> ATFD computation module -> combined loss (TPT + λ·ATFD) -> prompt embeddings update
- Critical path: For each test sample: 1) Generate text features for all classes using current prompt, 2) Compute ATFD, 3) Calculate TPT loss, 4) Combine losses with λ weighting, 5) Update prompt embeddings via gradient descent
- Design tradeoffs: Higher λ improves calibration but may slightly reduce accuracy; ATFD computation adds overhead but is lightweight; single-step optimization is fast but may not find optimal prompts
- Failure signatures: If ATFD increases but ECE doesn't improve, or if accuracy drops significantly; if C-TPT works on some datasets but not others; if the method fails to generalize across different CLIP architectures
- First 3 experiments:
  1. Implement ATFD computation and verify negative correlation with ECE on hard prompts across multiple datasets
  2. Add C-TPT regularization to TPT and measure calibration improvement on Caltech101 with CLIP-RN50
  3. Test C-TPT on natural distribution shifts (ImageNet-V2, ImageNet-A) to verify generalization to out-of-distribution data

## Open Questions the Paper Calls Out
None

## Limitations
- The correlation between ATFD and calibration error may not hold universally for all types of distribution shifts or CLIP variants
- The optimal λ parameter for balancing accuracy and calibration appears dataset-dependent, suggesting the method may require tuning for new applications
- The computational overhead of ATFD computation is not quantified in terms of absolute runtime or memory requirements

## Confidence

**High Confidence Claims:**
- C-TPT improves calibration on the tested datasets (Caltech101, DTD, StanfordCars, CUB200-2011)
- The relationship between text feature dispersion and calibration error is observable across multiple CLIP architectures
- C-TPT maintains competitive accuracy while improving calibration

**Medium Confidence Claims:**
- C-TPT generalizes to out-of-distribution data (ImageNet-V2, ImageNet-A)
- The computational overhead of ATFD computation is negligible
- The single-step optimization approach is sufficient for good performance

**Low Confidence Claims:**
- ATFD maximization will work equally well on all future CLIP architectures
- The optimal λ parameter range (0.1-0.3) will generalize to all applications
- The method's benefits will scale to extremely large-scale CLIP models

## Next Checks
1. **Cross-Domain Generalization Test**: Evaluate C-TPT on medical imaging or satellite imagery datasets where text descriptions may not align well with visual features, testing whether the ATFD-calibration relationship holds when class semantics are domain-specific.

2. **Large-Scale Model Validation**: Implement C-TPT on CLIP-ViT-L/14 and CLIP-ViT-H/14 to verify that the method scales effectively to larger architectures and that the optimal λ parameter remains consistent.

3. **Multi-Step Optimization Study**: Compare single-step versus multi-step (e.g., 3-5 steps) C-TPT optimization to quantify the tradeoff between computational efficiency and prompt quality, measuring both final calibration and the stability of the optimization process.