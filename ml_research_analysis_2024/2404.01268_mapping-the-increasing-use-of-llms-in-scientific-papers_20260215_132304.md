---
ver: rpa2
title: Mapping the Increasing Use of LLMs in Scientific Papers
arxiv_id: '2404.01268'
source_url: https://arxiv.org/abs/2404.01268
tags:
- arxiv
- papers
- text
- science
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first large-scale, systematic analysis
  of LLM usage in scientific papers, covering 950,965 papers from arXiv, bioRxiv,
  and Nature portfolio journals (2020-2024). Using a population-level statistical
  framework, researchers found that LLM-modified content increased steadily across
  disciplines, with Computer Science showing the highest usage (up to 17.5% of sentences
  by Feb 2024).
---

# Mapping the Increasing Use of LLMs in Scientific Papers

## Quick Facts
- arXiv ID: 2404.01268
- Source URL: https://arxiv.org/abs/2404.01268
- Reference count: 40
- This study presents the first large-scale, systematic analysis of LLM usage in scientific papers, covering 950,965 papers from arXiv, bioRxiv, and Nature portfolio journals (2020-2024).

## Executive Summary
This study presents the first large-scale, systematic analysis of LLM usage in scientific papers, covering 950,965 papers from arXiv, bioRxiv, and Nature portfolio journals (2020-2024). Using a population-level statistical framework, researchers found that LLM-modified content increased steadily across disciplines, with Computer Science showing the highest usage (up to 17.5% of sentences by Feb 2024). Papers by authors who post more preprints, papers in crowded research areas, and shorter papers showed higher LLM usage. The framework detects LLM modifications without requiring access to model internals, making it more robust than instance-level detection methods. These findings suggest LLMs are broadly used in scientific writing, with implications for academic publishing and AI model training data quality.

## Method Summary
The study uses a population-level statistical framework to detect and quantify LLM-modified content in scientific papers. The method compares token occurrence distributions between human-written and LLM-modified documents using mixture modeling and maximum likelihood estimation. The framework generates synthetic training data through a two-stage LLM process (abstractive summarization followed by full paragraph generation) and applies the model to a corpus of 950,965 papers from arXiv, bioRxiv, and Nature journals spanning January 2020 to February 2024. The analysis focuses on abstracts and introductions, estimating the fraction of LLM-modified content at monthly intervals without temporal smoothing.

## Key Results
- LLM-modified content increased steadily across all three platforms, with Computer Science showing the highest usage (up to 17.5% of sentences by February 2024)
- Papers by authors who post more preprints, papers in crowded research areas, and shorter papers showed higher LLM usage
- The framework detected LLM modifications without requiring access to model internals, providing more robust population-level estimates than instance-level detection methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Population-level statistical estimation can detect LLM-modified content without requiring access to model internals
- Mechanism: The framework compares token occurrence distributions between human-written and LLM-modified documents at scale, using mixture modeling to estimate the fraction of modified content
- Core assumption: The token usage patterns between human-written and LLM-modified text are sufficiently different to be statistically distinguishable
- Evidence anchors: [abstract]: "Our statistical estimation operates on the corpus level and is more robust than inference on individual instances"

### Mechanism 2
- Claim: Temporal distribution shifts can be handled by using a rolling estimation framework
- Mechanism: The framework fits models on historical data and applies them to subsequent periods, allowing detection of LLM usage patterns as they emerge over time
- Core assumption: Writing patterns remain relatively stable over short periods, allowing models trained on past data to remain effective
- Evidence anchors: [section]: "To evaluate model accuracy and calibration under temporal distribution shift, we use 3,000 papers from January 1, 2022, to November 29, 2022, as the validation data"

### Mechanism 3
- Claim: Using full vocabulary instead of selective feature sets improves estimation accuracy and stability
- Mechanism: By incorporating all words rather than just adjectives, adverbs, or verbs, the model captures more nuanced differences between human and LLM writing patterns
- Core assumption: The full vocabulary contains sufficient discriminative information to distinguish human from LLM writing
- Evidence anchors: [section]: "Using the Full Vocabulary for Estimation: We use the full vocabulary instead of only adjectives, as our validation shows that adjectives, adverbs, and verbs all perform well in our application"

## Foundational Learning

- Concept: Mixture modeling and maximum likelihood estimation
  - Why needed here: The framework uses these statistical techniques to estimate the fraction of LLM-modified content from observed document distributions
  - Quick check question: How does maximum likelihood estimation help determine the proportion of two mixed distributions when you only observe the mixture?

- Concept: Token frequency distribution analysis
  - Why needed here: The method relies on comparing how often specific tokens appear in human versus LLM-generated text to detect differences
  - Quick check question: What assumptions about token distributions must hold for this method to work effectively?

- Concept: Temporal distribution shift handling
  - Why needed here: The framework must account for changing writing patterns over time as LLMs become more prevalent
  - Quick check question: Why is it important to validate models on temporally separated data when detecting emerging phenomena?

## Architecture Onboarding

- Component map: Data collection pipeline (arXiv, bioRxiv, Nature journals) -> Text preprocessing and tokenization -> Token frequency counting for human and LLM corpora -> Mixture model parameter estimation -> Population-level fraction estimation -> Temporal trend analysis and visualization

- Critical path: Data collection → Preprocessing → Model fitting → Population estimation → Trend analysis

- Design tradeoffs:
  - Accuracy vs. computational efficiency: Using full vocabulary increases accuracy but requires more computation
  - Temporal coverage vs. model stability: Using more historical data improves estimates but may reduce sensitivity to recent changes
  - Granularity vs. noise: Sentence-level vs. document-level analysis affects signal-to-noise ratio

- Failure signatures:
  - Overestimation: Could indicate the training data generation process is producing text too similar to human writing
  - Underestimation: Might suggest LLM usage patterns are converging with human patterns
  - Inconsistent temporal trends: Could signal changes in authorship demographics or writing conventions

- First 3 experiments:
  1. Replicate the validation study using a different time period to test temporal robustness
  2. Compare performance using full vocabulary vs. selective subsets (adjectives, verbs, etc.)
  3. Test sensitivity to different LLM generation parameters (temperature, max length, etc.) in the training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do structural and cultural pressures in academic publishing influence the adoption of LLM tools, and can these pressures be mitigated to preserve epistemic diversity?
- Basis in paper: [explicit] The paper discusses how LLM usage is associated with factors like "publish or perish" pressures, crowded research areas, and time constraints, but does not explore mitigation strategies.
- Why unresolved: The study identifies correlations between LLM usage and structural pressures but does not investigate interventions or alternative publishing models to reduce reliance on LLMs.
- What evidence would resolve it: Longitudinal studies comparing LLM usage across institutions with different publishing incentives, or experimental interventions that reduce time pressures (e.g., longer submission deadlines) could clarify the impact of structural factors.

### Open Question 2
- Question: Does the inclusion of LLM-modified content in training datasets lead to model collapse or reinforcement of biases, and how can this be quantified?
- Basis in paper: [explicit] The paper notes that LLM-modified content in training data could lead to "model collapse" and the reinforcement of biases, but does not provide empirical evidence or quantification of these effects.
- Why unresolved: While the paper highlights potential risks, it does not test these claims or explore methods to detect or mitigate such issues in pretrained models.
- What evidence would resolve it: Controlled experiments comparing model performance and bias metrics when trained on datasets with varying proportions of LLM-modified content could quantify the impact.

### Open Question 3
- Question: How does the use of LLM tools in academic writing affect the linguistic diversity and originality of research outputs?
- Basis in paper: [inferred] The paper suggests that LLM usage might "flatten writing diversity" and increase similarity between papers, but does not measure the impact on linguistic or conceptual originality.
- Why unresolved: The study identifies associations between LLM usage and paper similarity but does not assess whether this leads to a loss of diverse perspectives or innovative ideas in research.
- What evidence would resolve it: Comparative analyses of linguistic diversity and citation impact between papers with high vs. low LLM usage could reveal whether originality is compromised.

## Limitations
- The framework relies on synthetic training data generated through a two-stage LLM process, limiting validation on real-world examples
- Analysis is restricted to abstracts and introductions, potentially missing LLM usage in other paper sections
- The framework's ability to detect newer LLM models or different prompting strategies beyond the tested range is unknown

## Confidence
- High Confidence: The finding that LLM usage shows clear temporal growth patterns across all three platforms (arXiv, bioRxiv, Nature) is well-supported by systematic data collection and analysis methodology.
- Medium Confidence: The claim that authors who post more preprints show higher LLM usage is supported by the data but may be influenced by confounding factors.
- Low Confidence: The assertion that shorter papers use LLMs more extensively is based on limited sample sizes in certain categories.

## Next Checks
1. Apply the framework to a curated dataset where LLM usage is known (e.g., papers explicitly stating LLM assistance) to verify accuracy across different domains and writing styles.
2. Extend the framework to analyze all sections of papers (methods, results, discussion) to understand if LLM usage patterns vary by section and how this affects overall estimates.
3. Systematically vary the LLM generation parameters (temperature, length constraints, model versions) in the training data to determine how robust the detection framework is to different LLM behaviors and outputs.