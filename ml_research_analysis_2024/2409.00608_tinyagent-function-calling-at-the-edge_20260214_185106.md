---
ver: rpa2
title: 'TinyAgent: Function Calling at the Edge'
arxiv_id: '2409.00608'
source_url: https://arxiv.org/abs/2409.00608
tags:
- function
- calling
- language
- tools
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TinyAgent, a framework for training and deploying
  small language models (SLMs) capable of function calling for edge-based agentic
  systems. The authors address the challenge of deploying large language models locally
  by developing specialized SLMs that can understand natural language queries and
  orchestrate tools/APIs without requiring cloud-based infrastructure.
---

# TinyAgent: Function Calling at the Edge

## Quick Facts
- **arXiv ID:** 2409.00608
- **Source URL:** https://arxiv.org/abs/2409.00608
- **Reference count:** 40
- **Primary result:** 80.06% and 84.95% success rates on function calling tasks for 1.1B and 7B models respectively

## Executive Summary
TinyAgent presents a framework for training and deploying small language models capable of function calling at the edge. The authors address the challenge of running large language models locally by developing specialized SLMs that can understand natural language queries and orchestrate tools/APIs without cloud infrastructure. The approach involves using LLMCompiler to enable function calling in open-source models, curating a high-quality dataset of 80K examples for fine-tuning, and introducing Tool RAG for efficient inference. The framework is demonstrated through a local Siri-like system on MacBook that can execute user commands via text or voice. The resulting TinyAgent-1.1B and 7B models achieve 80.06% and 84.95% success rates respectively, exceeding GPT-4-Turbo's 79.08% on function calling tasks while being fully deployed at the edge with low latency.

## Method Summary
TinyAgent introduces a framework for edge-based function calling that addresses the computational limitations of deploying large language models locally. The approach leverages LLMCompiler to enable function calling capabilities in small open-source models, then fine-tunes these models on a carefully curated dataset of 80K examples. The framework incorporates Tool RAG (Retrieval-Augmented Generation) for efficient inference, allowing the models to retrieve relevant tool information during execution. The training process focuses on teaching the models to understand natural language queries and map them to appropriate API calls and tool orchestrations. The resulting models, TinyAgent-1.1B and 7B, are optimized for deployment on Apple silicon hardware, enabling local execution of complex agentic tasks without requiring cloud connectivity.

## Key Results
- TinyAgent-1.1B achieves 80.06% success rate on function calling tasks
- TinyAgent-7B achieves 84.95% success rate, exceeding GPT-4-Turbo's 79.08%
- Both models demonstrate successful local deployment on MacBook with low latency
- Models support both text and voice command execution through local Siri-like system

## Why This Works (Mechanism)
TinyAgent works by leveraging specialized training techniques and architecture optimizations to enable small language models to perform complex function calling tasks at the edge. The framework uses LLMCompiler to provide the foundational capability for function calling, then enhances this through targeted fine-tuning on a high-quality dataset. Tool RAG enables efficient retrieval of relevant tool information during inference, reducing the computational burden while maintaining accuracy. The models are specifically optimized for Apple silicon hardware, taking advantage of hardware acceleration and efficient memory management. By keeping all processing local, the system eliminates network latency and privacy concerns associated with cloud-based solutions, while the smaller model size enables real-time execution on consumer hardware.

## Foundational Learning

1. **Function Calling in LLMs** - The ability of language models to identify when to call external tools/APIs and format the calls correctly. *Why needed:* Essential for enabling agentic behavior where models can interact with external systems. *Quick check:* Verify model can correctly parse natural language and generate valid API calls.

2. **Tool RAG (Retrieval-Augmented Generation)** - A technique that combines retrieval of relevant tool information with generation to improve inference efficiency. *Why needed:* Enables models to access up-to-date tool information without storing everything in model weights. *Quick check:* Confirm retrieval accuracy and generation quality with different tool sets.

3. **Edge Deployment Constraints** - The computational, memory, and power limitations of running models on local devices versus cloud infrastructure. *Why needed:* Defines the design space and optimization requirements for the framework. *Quick check:* Measure inference latency and memory usage on target hardware.

4. **Model Quantization and Optimization** - Techniques for reducing model size and computational requirements while maintaining accuracy. *Why needed:* Critical for enabling real-time inference on resource-constrained edge devices. *Quick check:* Compare performance of quantized vs full-precision models.

5. **Fine-tuning vs. Prompting** - The trade-offs between adapting model behavior through parameter updates versus in-context learning. *Why needed:* Determines the most effective approach for achieving desired capabilities within constraints. *Quick check:* Evaluate performance improvements from fine-tuning versus prompting alone.

## Architecture Onboarding

**Component Map:** Natural Language Input -> Language Model -> Tool RAG Retrieval -> API Call Generation -> Tool Execution -> Response Generation

**Critical Path:** The most critical path is from natural language input through the language model to tool selection and API call generation. This path must maintain low latency while ensuring accurate tool selection and proper API formatting. The Tool RAG component is crucial for efficient information retrieval without excessive computational overhead.

**Design Tradeoffs:** The primary tradeoff involves model size versus performance, where smaller models enable faster inference and lower resource consumption but may sacrifice some accuracy. Another key tradeoff is between local processing capabilities and the need for up-to-date tool information, addressed through Tool RAG. The framework also balances the comprehensiveness of tool support against the complexity of fine-tuning and inference.

**Failure Signatures:** Common failure modes include incorrect tool selection due to ambiguous natural language input, malformed API calls resulting from insufficient fine-tuning, and retrieval failures in Tool RAG when tool documentation is incomplete or poorly formatted. The system may also struggle with complex multi-step reasoning tasks that require chaining multiple tool calls.

**First Experiments:**
1. Test basic function calling accuracy on simple, single-tool tasks to establish baseline performance
2. Evaluate Tool RAG retrieval accuracy and impact on inference speed
3. Measure inference latency and memory usage across different model sizes on target hardware

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the provided content.

## Limitations

- **Hardware Dependency:** The framework is optimized specifically for Apple silicon, limiting applicability to other edge computing platforms
- **Dataset Generalizability:** Performance claims rely on a specific evaluation methodology and curated dataset that may not generalize to all use cases
- **Sustainability Concerns:** Long-term maintenance and updating of the model for evolving APIs and tools remains unclear
- **Limited Evaluation Scope:** The evaluation focuses primarily on success rates without extensive analysis of failure modes or complex multi-step tasks

## Confidence

- **Technical Implementation:** High confidence in the described architecture and training methodology
- **Comparative Performance:** Medium confidence in the claimed improvements over GPT-4-Turbo, pending independent verification
- **Generalizability:** Low confidence in the framework's applicability beyond Apple hardware and specific use cases

## Next Checks

1. Independent replication of function calling performance on diverse hardware platforms beyond Apple silicon
2. Extended evaluation with more complex multi-step reasoning tasks and adversarial queries to assess robustness
3. Detailed power consumption measurements during sustained operation compared to cloud-based alternatives