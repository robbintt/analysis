---
ver: rpa2
title: 'Political Bias in LLMs: Unaligned Moral Values in Agent-centric Simulations'
arxiv_id: '2408.11415'
source_url: https://arxiv.org/abs/2408.11415
tags:
- llms
- human
- moral
- language
- variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  reliably simulate human responses to the Moral Foundations Theory questionnaire,
  a tool used to measure political ideology and moral values. The authors adapt several
  open-source LLMs to different political personas and repeatedly survey them to generate
  synthetic datasets.
---

# Political Bias in LLMs: Unaligned Moral Values in Agent-centric Simulations

## Quick Facts
- **arXiv ID:** 2408.11415
- **Source URL:** https://arxiv.org/abs/2408.11415
- **Reference count:** 40
- **Primary result:** LLMs fail to reliably simulate human political responses to moral foundation questionnaires

## Executive Summary
This study investigates whether large language models can reliably simulate human responses to the Moral Foundations Theory questionnaire. The authors adapt several open-source LLMs to different political personas and repeatedly survey them to generate synthetic datasets. Results show high response variance across models and personas, with no consistent alignment between synthetic and human data. Conservative personas, in particular, fail to align with actual conservative populations. The findings suggest that LLMs struggle to coherently represent ideologies through in-context prompting, raising concerns about their use in social science simulations. The authors propose a framework for creating testable value statements to improve alignment in future work.

## Method Summary
The authors create synthetic datasets by adapting open-source LLMs to conservative, moderate, and liberal personas using in-context prompting. They survey each persona multiple times using the Moral Foundations Theory questionnaire and compare the resulting distributions to real human data from the European Social Survey. The study examines response variance across models, persona consistency, and alignment with human datasets. The methodology focuses on identifying whether LLMs can reliably simulate human moral values across different political orientations.

## Key Results
- High response variance across models and personas with no consistent alignment between synthetic and human data
- Conservative personas particularly fail to align with actual conservative populations
- In-context prompting alone is insufficient for LLMs to coherently represent political ideologies

## Why This Works (Mechanism)
Not applicable - this study investigates failures rather than successful mechanisms.

## Foundational Learning
1. **Moral Foundations Theory** - A framework measuring political ideology through five moral dimensions (Care, Fairness, Loyalty, Authority, Purity). Why needed: Provides standardized measure for comparing human and LLM responses. Quick check: Verify questionnaire covers all five foundations consistently.

2. **In-context prompting** - Technique using examples within prompts to guide LLM behavior. Why needed: Primary method for establishing political personas in LLMs. Quick check: Test prompt consistency across multiple model runs.

3. **Synthetic dataset generation** - Creating artificial data through systematic LLM querying. Why needed: Enables comparison between simulated and real human responses. Quick check: Validate sampling methodology ensures representativeness.

## Architecture Onboarding

**Component Map:** LLMs -> Political Personas -> Moral Foundations Questionnaire -> Synthetic Data -> Human Data Comparison

**Critical Path:** Persona Establishment -> Questionnaire Administration -> Response Collection -> Statistical Analysis

**Design Tradeoffs:** 
- In-context prompting vs. fine-tuning for persona establishment
- Sample size vs. computational cost
- Multiple model architectures vs. depth in single model analysis

**Failure Signatures:** 
- High variance in responses across multiple runs
- Poor alignment between synthetic and human distributions
- Inconsistent persona behavior across different LLMs

**First Experiments:**
1. Test baseline response consistency with no persona instructions
2. Compare variance between single vs. multiple persona examples in prompts
3. Analyze correlation between model size and alignment quality

## Open Questions the Paper Calls Out
None

## Limitations
- High inherent variability in LLM outputs makes stable persona establishment difficult
- Use of in-context prompting alone may be insufficient for consistent ideological representation
- Relatively small synthetic sample size (500 per persona) compared to human datasets

## Confidence

| Claim | Confidence |
|-------|------------|
| LLMs struggle to represent political ideologies through in-context prompting | Medium |
| Conservative personas fail to align with actual conservative populations | Medium-High |
| Findings reflect fundamental LLM limitations rather than methodological issues | Low-Medium |

## Next Checks

1. Test alternative persona establishment methods including few-shot prompting with real human responses and fine-tuning approaches to determine if response variance decreases

2. Conduct replication studies with larger sample sizes (n > 1000 per persona) and additional LLM architectures to assess the stability of findings

3. Perform cross-cultural validation by testing whether these alignment challenges persist across different national datasets and political contexts