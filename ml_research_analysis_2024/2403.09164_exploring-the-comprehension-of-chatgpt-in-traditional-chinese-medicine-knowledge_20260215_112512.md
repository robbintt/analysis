---
ver: rpa2
title: Exploring the Comprehension of ChatGPT in Traditional Chinese Medicine Knowledge
arxiv_id: '2403.09164'
source_url: https://arxiv.org/abs/2403.09164
tags:
- chatgpt
- questions
- chinese
- knowledge
- medicine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the performance of ChatGPT in Traditional
  Chinese Medicine (TCM) knowledge. The authors construct a new TCM question-answering
  dataset called TCM-QA, which includes three types of questions: single choice, multiple
  choice, and true or false.'
---

# Exploring the Comprehension of Traditional Chinese Medicine Knowledge

## Quick Facts
- **arXiv ID**: 2403.09164
- **Source URL**: https://arxiv.org/abs/2403.09164
- **Reference count**: 36
- **Primary result**: ChatGPT achieves highest precision (0.688) on true/false TCM questions, lowest (0.241) on multiple-choice questions, with Chinese prompts outperforming English.

## Executive Summary
This paper evaluates ChatGPT's comprehension of Traditional Chinese Medicine (TCM) knowledge using a newly constructed TCM-QA dataset. The researchers tested ChatGPT's performance on three question types (single choice, multiple choice, and true/false) using both zero-shot and few-shot prompting approaches in Chinese and English. Results show that ChatGPT performs significantly better on true/false questions compared to multiple-choice questions, with Chinese prompts yielding better results than English prompts. The study also assessed the quality of ChatGPT's explanations using human evaluation across three dimensions: readability, reliability, and integrity.

## Method Summary
The researchers constructed a TCM-QA dataset containing 801 question-answer pairs across three types: single choice (574), multiple choice (131), and true/false (96). They evaluated ChatGPT using zero-shot and few-shot prompting strategies with both English and Chinese prompts via the ChatCompletion API. Automatic evaluation measured precision and responsiveness, while human evaluation assessed explanation quality on 150 correct and 150 incorrect answers using three criteria: readability, reliability, and integrity. The study compared performance across question types and languages to identify patterns in ChatGPT's TCM knowledge comprehension.

## Key Results
- ChatGPT achieves highest precision of 0.688 on true/false questions, lowest of 0.241 on multiple-choice questions
- Chinese prompts consistently outperform English prompts across all question types
- The model generates explanations that vary in quality, with some instances of "illusions" or fabricated TCM knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot and few-shot prompting improve ChatGPT's performance on TCM knowledge tasks.
- Mechanism: By providing explicit task descriptions and context (e.g., role-play as TCM assistant, step-by-step explanations), the model can better align its responses with the expected format and reasoning style.
- Core assumption: The LLM's pre-training includes sufficient general reasoning ability to generalize from provided prompts without fine-tuning.
- Evidence anchors:
  - [abstract] "We evaluate two settings of the LLM: zero-shot and few-shot settings"
  - [section] "We have developed zero-shot and few-shot prompt settings... TCM Knowledge... integrates TCM knowledge and instructing ChatGPT to learn the knowledge prior to answering questions."
- Break condition: If prompts are ambiguous or do not match the structure of the questions, performance degrades. Also, if the model's training data lacks sufficient TCM-related content, prompting alone may not suffice.

### Mechanism 2
- Claim: Chinese language prompts outperform English prompts for TCM tasks.
- Mechanism: Since TCM questions are written in Chinese and the domain relies heavily on culturally specific terminology, prompting in the same language improves comprehension and retrieval of relevant knowledge.
- Core assumption: The model's training corpus contains adequate Chinese TCM terminology and context for effective reasoning in that language.
- Evidence anchors:
  - [abstract] "Chinese prompts outperform English prompts in our evaluations"
  - [section] "Because TCM knowledge and questions are almost expressed in the Chinese language, we also design the prompts in both Chinese and English"
- Break condition: If the model's Chinese training data is sparse or of lower quality than English data, the advantage may diminish.

### Mechanism 3
- Claim: Binary (true/false) questions are easier for the model than multi-option questions.
- Mechanism: True/false tasks reduce cognitive load by eliminating distractors and focusing on direct fact recall or binary reasoning, which the model can handle more reliably.
- Core assumption: The model's reasoning capacity is more consistent in constrained decision spaces.
- Evidence anchors:
  - [abstract] "ChatGPT performs best in true or false questions, achieving the highest precision of 0.688, while scoring the lowest precision (0.241) in multiple-choice questions"
  - [section] "Our results show that ChatGPT performs best in true or false questions... while scoring the lowest precision is 0.241 in multiple-choice questions"
- Break condition: If questions require multi-step reasoning even in binary format, or if the binary framing introduces ambiguity, performance may drop.

## Foundational Learning

- **Prompt Engineering**: Why needed here: To guide the LLM's behavior without fine-tuning, leveraging in-context learning to shape outputs for specialized domains.
  - Quick check question: What are the key differences between zero-shot and few-shot prompting in this study?

- **TCM Knowledge Structure**: Why needed here: Understanding how TCM knowledge is organized (e.g., Yin-Yang balance, five elements, zang-fu organs) helps interpret why certain prompts or question types succeed or fail.
  - Quick check question: Which TCM reasoning categories were used to classify the dataset?

- **Human Evaluation Metrics**: Why needed here: To assess the quality of generated explanations beyond accuracy, ensuring the model's outputs are readable, reliable, and complete.
  - Quick check question: What three aspects were evaluated in the human assessment of ChatGPT's explanations?

## Architecture Onboarding

- **Component map**: TCM-QA dataset (801 QA pairs, three question types) -> Zero-shot and few-shot prompts (Chinese/English) -> GPT-3.5-turbo via ChatCompletion API -> Automatic evaluation (precision, responsiveness) + Human evaluation (readability, reliability, integrity)
- **Critical path**: Prompt -> Model API call -> Response -> Evaluation (auto + human)
- **Design tradeoffs**: Prompt complexity vs. model response time (1000-token limit), Bilingual prompts vs. domain specificity, Dataset size vs. coverage of TCM knowledge breadth
- **Failure signatures**: Unresponsive outputs (especially for knowledge-based questions), Low precision in multiple-choice tasks, Illusory explanations (fabricated TCM knowledge)
- **First 3 experiments**:
  1. Compare zero-shot vs. few-shot precision on each question type
  2. Test Chinese vs. English prompt performance on true/false questions
  3. Human evaluate a sample of correct vs. incorrect answers for explanation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ChatGPT's performance in understanding TCM knowledge be improved through more advanced prompt engineering techniques?
- Basis in paper: [inferred] The paper discusses that simply setting different prompt words did not significantly improve ChatGPT's understanding of TCM topics, and suggests that more advanced prompt techniques like chain-of-thought prompting, self-consistency prompting, and retrieval tools could potentially enhance performance.
- Why unresolved: The paper only briefly mentions these advanced prompt techniques and does not provide a detailed analysis of their impact on ChatGPT's TCM comprehension.
- What evidence would resolve it: A comparative study evaluating ChatGPT's performance using various advanced prompt engineering techniques on a diverse set of TCM questions, and analyzing the results to identify the most effective approaches.

### Open Question 2
- Question: What are the key challenges and potential solutions for adapting LLMs like ChatGPT to understand and reason about specialized domains like TCM?
- Basis in paper: [explicit] The paper highlights that ChatGPT's understanding of TCM is limited due to its training data being primarily in English and its lack of exposure to TCM-related knowledge. It also mentions that instruction fine-tuning and integrating high-quality data could enhance ChatGPT's performance in specialized domains.
- Why unresolved: The paper does not provide a detailed analysis of the specific challenges and potential solutions for adapting LLMs to specialized domains like TCM.
- What evidence would resolve it: A comprehensive study identifying the key challenges in adapting LLMs to specialized domains, proposing and evaluating potential solutions, and providing insights into the effectiveness of these solutions in improving LLM performance in specialized domains.

### Open Question 3
- Question: How can the quality and reliability of explanations generated by LLMs like ChatGPT be improved, especially when dealing with complex or specialized knowledge domains?
- Basis in paper: [explicit] The paper assesses the quality of explanations generated by ChatGPT using human evaluation and identifies issues such as the model sometimes generating "illusions" or inventing erroneous knowledge to support its rationale.
- Why unresolved: The paper does not provide a detailed analysis of the underlying causes of these issues or propose specific strategies to improve the quality and reliability of LLM-generated explanations.
- What evidence would resolve it: A study investigating the factors contributing to the generation of unreliable explanations by LLMs, proposing and evaluating techniques to improve the quality and reliability of explanations, and analyzing the impact of these techniques on the overall performance of LLMs in specialized domains.

## Limitations

- The study relies on a single LLM (GPT-3.5-turbo) without comparing alternative models or versions, limiting generalizability
- Dataset size (801 QA pairs) may constrain robustness of conclusions, particularly for multiple-choice questions with only 131 examples
- The paper does not address potential hallucinations in explanations or systematic errors in reasoning patterns

## Confidence

- **High confidence**: Chinese prompts outperform English prompts for TCM tasks
- **Medium confidence**: Zero-shot and few-shot prompting improve performance compared to baseline
- **Medium confidence**: True/false questions are easier than multiple-choice questions
- **Low confidence**: Generated explanations have "considerable" quality for TCM comprehension

## Next Checks

1. **Replicate with expanded dataset**: Test ChatGPT's performance on a larger TCM-QA dataset (target: 2,000+ QA pairs) to verify whether precision patterns hold and whether the multiple-choice performance gap persists with more training examples.

2. **Cross-model comparison**: Evaluate GPT-4, Claude, and other LLMs on the same TCM-QA benchmark to determine if observed limitations are specific to ChatGPT or representative of current LLMs' TCM comprehension capabilities.

3. **Explanation utility validation**: Design a controlled experiment where human participants answer TCM questions with and without ChatGPT explanations, measuring whether the explanations actually improve comprehension beyond the correct answer alone.