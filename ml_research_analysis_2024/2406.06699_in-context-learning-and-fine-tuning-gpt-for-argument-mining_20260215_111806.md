---
ver: rpa2
title: In-Context Learning and Fine-Tuning GPT for Argument Mining
arxiv_id: '2406.06699'
source_url: https://arxiv.org/abs/2406.06699
tags:
- examples
- argument
- learning
- text
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an in-context learning (ICL) strategy for
  argument type classification (ATC) that combines kNN-based example selection with
  majority vote ensembling. The approach achieves very competitive classification
  accuracy in a training-free setting using GPT-4.
---

# In-Context Learning and Fine-Tuning GPT for Argument Mining

## Quick Facts
- arXiv ID: 2406.06699
- Source URL: https://arxiv.org/abs/2406.06699
- Authors: Jérémie Cabessa; Hugo Hernault; Umer Mushtaq
- Reference count: 15
- Primary result: Introduces kNN-based ICL and fine-tuning strategy achieving SOTA on ATC

## Executive Summary
This work presents a comprehensive approach to argument type classification (ATC) that leverages both in-context learning (ICL) and fine-tuning of large language models. The authors develop a kNN-based example selection method combined with majority vote ensembling for ICL, demonstrating competitive performance with GPT-4 in a training-free setting. Additionally, they propose a fine-tuning strategy that incorporates structural features in textual form, enabling GPT-3.5 to achieve state-of-the-art results on ATC tasks.

## Method Summary
The authors introduce a dual approach to argument type classification. For ICL, they implement k-nearest neighbors (kNN) to select relevant examples from a training set, then use majority vote ensembling across multiple prompts to improve classification accuracy. For fine-tuning, they develop a strategy that converts well-crafted structural features into textual form, allowing the model to learn from these engineered representations during training. The approach aims to capitalize on LLMs' ability to understand global discourse flow while providing explicit structural guidance.

## Key Results
- kNN-based ICL with majority voting achieves competitive accuracy using GPT-4 without training
- Fine-tuned GPT-3.5 with structural features reaches state-of-the-art performance on ATC
- Demonstrates LLMs' emergent ability to grasp global discursive flow in both off-the-shelf and fine-tuned settings

## Why This Works (Mechanism)
The proposed methods work by combining two complementary strategies. The ICL approach leverages kNN to identify contextually relevant examples that help the model understand the specific discourse patterns in the target text, while majority voting reduces the variance inherent in single-prompt predictions. The fine-tuning strategy enhances the model's ability to recognize argument structures by explicitly providing textual representations of engineered features that capture discourse relationships, allowing the model to learn these patterns systematically during training.

## Foundational Learning
- k-nearest neighbors (kNN): Why needed - to identify relevant examples for ICL; Quick check - verify distance metric appropriateness
- Majority vote ensembling: Why needed - to reduce prediction variance; Quick check - test different ensemble sizes
- Structural feature engineering: Why needed - to provide explicit discourse patterns; Quick check - validate feature relevance to task
- Argument type classification: Why needed - core task in discourse analysis; Quick check - confirm annotation consistency
- In-context learning: Why needed - enables few-shot learning without parameter updates; Quick check - test with varying prompt sizes
- Fine-tuning strategies: Why needed - to adapt LLMs to specific downstream tasks; Quick check - monitor validation loss during training

## Architecture Onboarding
Component map: Input text -> kNN example selection -> ICL with majority voting OR Input text + features -> Fine-tuning -> Output classification
Critical path: Data preprocessing → feature engineering → model selection (ICL vs fine-tuning) → evaluation
Design tradeoffs: ICL offers training-free deployment but may be less precise than fine-tuning; fine-tuning requires labeled data but can achieve higher accuracy with proper features
Failure signatures: Poor kNN selection leads to irrelevant examples; inadequate feature engineering reduces fine-tuning effectiveness
First experiments:
1. Test kNN selection with different distance metrics on a validation set
2. Compare ICL performance across different base models (GPT-3.5, Claude, LLaMA)
3. Run ablation study removing individual structural features during fine-tuning

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Heavy reliance on GPT-4 for ICL approach may limit generalizability to other models
- Performance evaluation limited to argument type classification task only
- Fine-tuning strategy depends on availability of well-crafted structural features

## Confidence
High confidence in SOTA performance claim for fine-tuned GPT-3.5 on the specific ATC benchmark
Medium confidence in LLMs' ability to grasp global discursive flow (primarily demonstrated through performance rather than linguistic analysis)
Medium confidence in generalizability of kNN-based ICL across different models and tasks

## Next Checks
1. Test kNN-based ICL approach with different base models (GPT-3.5, Claude, LLaMA) to assess model dependency
2. Apply fine-tuning strategy to other argument mining tasks or different discourse analysis datasets
3. Conduct ablation studies removing structural features to quantify their specific contribution to performance gains