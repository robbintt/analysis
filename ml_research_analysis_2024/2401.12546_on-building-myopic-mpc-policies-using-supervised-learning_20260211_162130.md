---
ver: rpa2
title: On Building Myopic MPC Policies using Supervised Learning
arxiv_id: '2401.12546'
source_url: https://arxiv.org/abs/2401.12546
tags:
- function
- learning
- data
- myopic
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational burden of online model predictive
  control (MPC) by proposing a supervised learning approach to approximate the optimal
  value function. Instead of directly approximating the MPC policy, the authors learn
  the optimal cost-to-go function offline using optimal state-action-value tuples.
---

# On Building Myopic MPC Policies using Supervised Learning

## Quick Facts
- arXiv ID: 2401.12546
- Source URL: https://arxiv.org/abs/2401.12546
- Reference count: 2
- Key outcome: Supervised learning of cost-to-go function with descent property constraint enables myopic MPC with reduced online computation while maintaining performance

## Executive Summary
This paper addresses the computational burden of online model predictive control (MPC) by proposing a supervised learning approach to approximate the optimal value function. Instead of directly approximating the MPC policy, the authors learn the optimal cost-to-go function offline using optimal state-action-value tuples. This cost-to-go function is then used in a myopic MPC with a short prediction horizon, significantly reducing online computation without compromising performance. The key novelty is encoding the descent property constraint during the learning process, ensuring that the successor state's cost-to-go is lower than the current state's cost-to-go.

## Method Summary
The method learns an approximate value function V(x; θ) for myopic MPC by training on optimal state-action-value tuples collected from offline MPC solutions. The training uses MSE loss augmented with a descent property constraint V(f(x, u*); θ) - V(x; θ) ≤ ℓ(x, u*) to ensure successor states have lower cost-to-go. Sensitivity-based data augmentation efficiently expands the training set by perturbing states using parametric sensitivities from the KKT conditions. The trained value function is then used in a one-step myopic MPC, where each control action minimizes immediate cost plus predicted cost-to-go.

## Key Results
- Myopic MPC with descent property-constrained value function outperforms one trained with only MSE loss
- The approach maintains performance while significantly reducing online computation
- Myopic MPC handles online variations in controller parameters (tightened constraints, sampling time changes) unlike direct policy approximation

## Why This Works (Mechanism)

### Mechanism 1
The descent property constraint ensures that the learned cost-to-go function behaves like a control Lyapunov function (CLF), guaranteeing that each myopic step reduces the total cost. By enforcing V(f(x, u*); θ) - V(x; θ) ≤ ℓ(x, u*) for all training data, the optimization steers θ toward a value function that satisfies a CLF-like condition. This means successor states have strictly lower cost-to-go than current states, preventing cycles or divergence. The core assumption is that the offline dataset D sufficiently covers the feasible state space and the descent property is enforced for all sampled transitions.

### Mechanism 2
Using the optimal state-action-value tuples from offline MPC allows the myopic controller to retain performance guarantees without online optimization. The value function V(x; θ) learned from optimal (x, u*, V*) pairs approximates the true optimal cost-to-go. When embedded in a one-step myopic MPC, the policy mimics long-horizon behavior because each step greedily minimizes the sum of immediate cost plus predicted cost-to-go, which is equivalent to solving the original MPC when V(x; θ) = V*(x). The core assumption is that the function approximator V(x; θ) can represent V*(x) closely enough, and the myopic step selection is valid for the problem class.

### Mechanism 3
Sensitivity-based data augmentation expands the training set efficiently, reducing the offline data generation cost while maintaining quality. Once an MPC problem is solved for an initial state, parametric sensitivity analysis provides the change in the primal-dual solution when the initial state varies. The augmented data uses these sensitivities to estimate new optimal actions and costs without re-solving the full MPC, thus scaling the dataset at low computational cost. The core assumption is that the system dynamics and cost structure are sufficiently smooth that sensitivity approximations remain accurate for small state perturbations.

## Foundational Learning

- Concept: Bellman's principle of optimality
  - Why needed here: Justifies replacing the full-horizon MPC with a myopic step that uses the learned cost-to-go function
  - Quick check question: If you have the exact optimal cost-to-go function, what is the relationship between a one-step greedy policy and the original N-step MPC policy?

- Concept: Parametric sensitivity in optimization
  - Why needed here: Enables efficient generation of training data by approximating how solutions change with initial state variations
  - Quick check question: In a parametric optimization problem, how does the optimal solution vary with the parameter, and what role does the Jacobian of the KKT conditions play?

- Concept: Control Lyapunov functions (CLFs)
  - Why needed here: The descent property constraint enforces a CLF-like behavior in the learned value function, ensuring stability and convergence
  - Quick check question: What property must a function satisfy to be considered a CLF for a dynamical system, and how does the descent property relate to this?

## Architecture Onboarding

- Component map: Data generation module -> Sensitivity augmentation module -> Supervised learning module -> Myopic MPC controller
- Critical path: 1) Sample initial states → solve long-horizon MPC → collect (x, u*, V*) → augment via sensitivities → train V(x; θ) with descent constraint → deploy myopic MPC
- Design tradeoffs:
  - Accuracy vs. offline computation: larger training sets improve V(x; θ) fit but increase offline cost; sensitivity augmentation mitigates this
  - Model complexity vs. generalization: richer function approximators can better represent V*(x) but risk overfitting and slower online inference
  - Constraint tightness vs. feasibility: stricter descent constraints improve safety but may make training harder or infeasible
- Failure signatures:
  - Closed-loop cycles or divergence indicate the learned V(x; θ) violates the descent property in untested regions
  - Poor tracking performance suggests V(x; θ) has large bias or does not generalize to the operating region
  - Online MPC infeasibility may arise if the myopic step produces a successor state outside the feasible set implied by the training data
- First 3 experiments:
  1. Compare myopic MPC with V(x; θ) trained using only MSE loss versus MSE + descent property on a simple double integrator with tightened state constraints
  2. Measure the effect of sensitivity-based data augmentation on training set size and closed-loop performance in the CSTR benchmark
  3. Test myopic MPC's ability to adapt to online changes in sampling time and constraint bounds, verifying robustness compared to a fixed direct policy approximation

## Open Questions the Paper Calls Out

### Open Question 1
How does the descent property constraint affect the generalization capability of the learned cost-to-go function beyond the training data distribution? The paper mentions that the descent property constraint steers learning to satisfy descent for all training data points, but does not investigate generalization to unseen states. This remains unresolved because the paper only evaluates closed-loop performance on trajectories starting from initial states within the training data distribution. Testing the myopic MPC on states far outside the training data distribution would resolve this.

### Open Question 2
What is the theoretical relationship between the approximation error of the cost-to-go function and the closed-loop performance degradation in the myopic MPC? The paper shows empirically that a better MSE fit doesn't guarantee better performance, suggesting the approximation error's impact on performance is non-trivial. This remains unresolved because the paper demonstrates this phenomenon but doesn't provide theoretical analysis of the relationship between cost-to-go approximation error and performance. A theoretical framework connecting the Lipschitz constant of the cost-to-go approximation error to performance bounds for the myopic MPC would resolve this.

### Open Question 3
How does the proposed method scale to high-dimensional state spaces where RBF networks become impractical? The paper uses RBF networks with 100 neurons for a 2-state system, but doesn't address scalability to higher dimensions. This remains unresolved because the computational complexity of RBF networks grows exponentially with state dimension, and the paper doesn't explore alternative architectures for high-dimensional systems. Experiments on systems with 5+ states using different function approximators and analysis of computational scaling with state dimension would resolve this.

### Open Question 4
Can the descent property constraint be relaxed or modified to improve the trade-off between MSE loss and closed-loop performance? The paper uses a strict descent property constraint but shows that a poorer MSE fit with this constraint can outperform a better MSE fit without it. This remains unresolved because the paper doesn't explore whether the strict descent property is optimal or if a relaxed version could provide better performance. Comparative studies using different constraint formulations and their impact on the MSE-loss vs. performance trade-off would resolve this.

## Limitations
- Empirical validation limited to a single CSTR benchmark example
- Descent property constraint enforced only on training data points, with no guarantees for extrapolation
- Sensitivity-based data augmentation relies on parametric smoothness assumptions

## Confidence
- High confidence in the theoretical motivation linking descent property enforcement to CLF-like behavior
- Medium confidence in the claimed computational savings, as the paper doesn't provide detailed runtime comparisons
- Medium confidence in the generalization claims, given the single-example validation

## Next Checks
1. Test the approach on a second benchmark problem (e.g., cart-pole or inverted pendulum) to verify generalization beyond the CSTR example
2. Quantify the actual runtime reduction by measuring wall-clock time for full-horizon vs. myopic MPC implementations
3. Evaluate robustness to training data quality by intentionally introducing noise or biases in the optimal value function estimates