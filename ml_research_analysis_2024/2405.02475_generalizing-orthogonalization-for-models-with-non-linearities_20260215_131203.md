---
ver: rpa2
title: Generalizing Orthogonalization for Models with Non-Linearities
arxiv_id: '2405.02475'
source_url: https://arxiv.org/abs/2405.02475
tags:
- orthogonalization
- features
- correction
- predictions
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper extends classical orthogonalization methods to non-linear
  models, addressing a gap in prior work limited to linear settings. It introduces
  a generalized correction routine for generalized linear models (GLMs) and neural
  networks with non-linear activations like ReLU, ensuring predictions are uncorrelated
  with protected features.
---

# Generalizing Orthogonalization for Models with Non-Linearities

## Quick Facts
- **arXiv ID**: 2405.02475
- **Source URL**: https://arxiv.org/abs/2405.02475
- **Reference count**: 40
- **Primary result**: Extends classical orthogonalization to non-linear models including GLMs and neural networks, ensuring predictions are uncorrelated with protected features.

## Executive Summary
This paper addresses the limitation of classical orthogonalization methods, which are only applicable to linear models, by developing a generalized correction routine for non-linear settings. The approach enables removal of linear dependencies between model predictions and protected features in generalized linear models (GLMs) and neural networks with non-linear activations like ReLU. The method supports both scalar and tensor-valued predictions, making it applicable to standard GLMs and neural network architectures. Experiments demonstrate successful removal of protected feature influence in various domains including tabular data, image classification, and text embeddings.

## Method Summary
The generalized orthogonalization method extends classical linear projection techniques to non-linear models through a constrained optimization approach. For GLMs, the correction routine solves an optimization problem that maintains prediction accuracy while ensuring evaluation model coefficients for protected features approach zero. For neural networks, the method handles tensor-valued predictions by vectorizing them, applying orthogonal projection, and reshaping back to tensor form. The approach uses the modified differential multiplier method for optimization and supports integration into neural network training pipelines.

## Key Results
- Successfully removes significant effects of protected features (e.g., race, sex) in GLMs, achieving near-zero coefficients and high p-values
- Prevents CNNs from relying on spurious features like color channels, improving test accuracy
- Corrects biases in face recognition and text embeddings across multiple datasets
- Maintains prediction performance while achieving orthogonality to protected features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear orthogonalization removes linear dependencies between prediction features and protected attributes
- Mechanism: Projects prediction features onto the orthogonal complement of protected features' span using projection matrix P⊥X
- Core assumption: Original prediction features contain linear relationship with protected features that can be removed
- Evidence anchors:
  - [abstract]: "our approach also encompasses scalar and tensor-valued predictions, facilitating its integration into neural network architectures"
  - [section]: "when left-multiplying a term by P⊥X, we orthogonalize this term (w.r.t. features X)"
  - [corpus]: "weak - the corpus discusses neural networks and fairness but doesn't specifically address the linear projection mechanism"
- Break condition: If prediction features are already orthogonal to protected features, correction has no effect

### Mechanism 2
- Claim: Generalized correction extends orthogonalization to non-linear activation functions through constrained optimization
- Mechanism: Optimizes model parameters subject to constraint that evaluation model yields zero coefficients for protected features
- Core assumption: Non-linear activation can be approximated as locally linear near convergence
- Evidence anchors:
  - [abstract]: "Our approach also encompasses scalar and tensor-valued predictions, facilitating its integration into neural network architectures"
  - [section]: "we can make use of the approximate linearity of both Mp and Me at convergence"
  - [corpus]: "weak - corpus doesn't discuss the constrained optimization approach for non-linear activations"
- Break condition: If non-linear activation is highly non-linear throughout domain, approximation fails

### Mechanism 3
- Claim: Correction handles tensor-valued predictions by extending orthogonal projection to tensor spaces
- Mechanism: Vectorizes tensor predictions, applies orthogonal projection, then reshapes back to tensor form
- Core assumption: Tensor operations can be represented as matrix operations through vectorization
- Evidence anchors:
  - [abstract]: "Our approach also encompasses scalar and tensor-valued predictions, facilitating its integration into neural network architectures"
  - [section]: "Using the vec(·) operation, we can first reformulate the evaluation model with uncorrected predictions vec(Ŷ)"
  - [corpus]: "weak - corpus doesn't discuss tensor-valued predictions or their orthogonalization"
- Break condition: If tensor structure is too complex or vectorization loses spatial relationships

## Foundational Learning

- Concept: Linear algebra and projection matrices
  - Why needed here: Entire orthogonalization approach relies on projecting data onto orthogonal complements
  - Quick check question: What is the mathematical property of P⊥X that ensures orthogonality to the span of X?

- Concept: Generalized linear models (GLMs)
  - Why needed here: Method extends orthogonalization from linear models to GLMs with non-linear link functions
  - Quick check question: How does the canonical link function in GLMs relate to the loss function used?

- Concept: Optimization with constraints
  - Why needed here: Generalized correction solves constrained optimization problem to maintain orthogonality after non-linear transformations
  - Quick check question: What is the difference between the Lagrange multiplier method and the modified differential multiplier method?

## Architecture Onboarding

- Component map:
  - Prediction model (Mp) -> Correction routine (Ch) -> Evaluation model (Me)
  - Protected features (X) -> Correction target
  - Original features (Z) -> Input to prediction model

- Critical path:
  1. Train prediction model Mp on features Z to predict outcome y
  2. Apply correction routine Ch to remove influence of protected features X
  3. Verify correction using evaluation model Me
  4. Use corrected predictions for downstream tasks

- Design tradeoffs:
  - Performance vs. fairness: More aggressive orthogonalization may reduce prediction accuracy
  - Linear vs. non-linear corrections: Linear corrections are simpler but may not fully remove non-linear dependencies
  - Tensor vs. scalar handling: Tensor corrections require more complex operations but enable application to neural networks

- Failure signatures:
  - Protected features still significantly influence corrected predictions (Me yields non-zero coefficients)
  - Prediction performance degrades significantly after correction
  - Numerical instability in constrained optimization for non-linear corrections

- First 3 experiments:
  1. Apply classical orthogonalization (Cl) to a GLM with non-linear activation and verify it fails to remove protected feature influence
  2. Apply generalized orthogonalization (Ch) to the same GLM and verify successful removal of protected feature influence
  3. Test the tensor-valued correction on a neural network layer and verify orthogonality of predictions to protected features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed orthogonalization method perform when n < p or q < p?
- Basis in paper: [explicit] Paper explicitly states algorithms require n > p and q > p, identifying extending to n < p or q < p as future research direction
- Why unresolved: Paper provides no theoretical analysis or experimental results for these scenarios
- What evidence would resolve it: Theoretical analysis of method's behavior and empirical experiments demonstrating performance in n < p or q < p scenarios

### Open Question 2
- Question: Can orthogonalization be extended to handle non-linear evaluation models beyond linear feature effects?
- Basis in paper: [explicit] Paper discusses this as potential extension suggested by reviewer, mentions running fairness benchmark with random forest evaluation model
- Why unresolved: Paper only briefly mentions extension without detailed analysis or experimental results
- What evidence would resolve it: Theoretical framework for extended method and empirical experiments comparing performance to linear evaluation model approach

### Open Question 3
- Question: How effective is the method in simultaneously learning protected features and evaluating potential biases?
- Basis in paper: [explicit] Paper mentions this as another interesting approach brought up by reviewer but doesn't explore further
- Why unresolved: Paper provides no theoretical analysis or experimental results for this simultaneous learning and evaluation approach
- What evidence would resolve it: Theoretical framework for simultaneous learning and evaluation approach and empirical experiments demonstrating performance compared to standard orthogonalization method

## Limitations
- Relies heavily on approximate linearity assumption of prediction and evaluation models at convergence
- Method's effectiveness in preserving prediction performance while removing protected feature influence is not thoroughly quantified across diverse real-world datasets
- No theoretical analysis or experimental results provided for scenarios where n < p or q < p

## Confidence
- **High Confidence**: Mechanism of classical linear orthogonalization and its failure when applied directly to non-linear models
- **Medium Confidence**: Generalized correction approach for non-linear models using constrained optimization
- **Medium Confidence**: Tensor-valued prediction handling through vectorization and projection

## Next Checks
1. **Linearity Verification**: Test the approximate linearity assumption by measuring the Jacobian norm of non-linear activation functions across their input domain to quantify when the approximation breaks down

2. **Performance-Fairness Tradeoff Analysis**: Systematically evaluate prediction accuracy degradation across multiple datasets while varying the strength of orthogonalization to map the performance-fairness tradeoff curve

3. **Tensor Structure Preservation**: Validate that vectorization and reshaping operations preserve spatial relationships in tensor predictions by comparing classification accuracy on structured data before and after tensor orthogonalization