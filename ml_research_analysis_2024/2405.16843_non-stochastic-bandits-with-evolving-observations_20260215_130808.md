---
ver: rpa2
title: Non-stochastic Bandits With Evolving Observations
arxiv_id: '2405.16843'
source_url: https://arxiv.org/abs/2405.16843
tags:
- feedback
- regret
- dmax
- loss
- delayed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new framework for online learning with
  evolving observations, where feedback on actions can change over time, even retroactively.
  The authors propose regret minimization algorithms for both full-information and
  bandit settings, with regret bounds that depend on the average feedback accuracy
  relative to the true loss.
---

# Non-stochastic Bandits With Evolving Observations
## Quick Facts
- arXiv ID: 2405.16843
- Source URL: https://arxiv.org/abs/2405.16843
- Reference count: 40
- Primary result: New framework for online learning with evolving observations, achieving optimal regret bounds in delayed, corrupted, and composite feedback settings.

## Executive Summary
This paper introduces a novel framework for online learning with evolving observations, where feedback on actions can change over time, even retroactively. The authors propose regret minimization algorithms for both full-information and bandit settings, with regret bounds that depend on the average feedback accuracy relative to the true loss. The key idea is to use exponential weights and follow-the-regularized-leader variants that continuously update their beliefs on the true loss based on the most recent feedback, even if it contradicts previous observations.

## Method Summary
The authors propose a new framework for online learning with evolving observations, where feedback on actions can change over time, even retroactively. The main idea is to use exponential weights and follow-the-regularized-leader variants that continuously update their beliefs on the true loss based on the most recent feedback. The proposed algorithms are designed to minimize regret in both full-information and bandit settings, with regret bounds that depend on the average feedback accuracy relative to the true loss.

## Key Results
- For full-information setting, expected regret bound of O(√ln K (T/2 + 2D)), where D is the total feedback inaccuracy.
- For bandit setting, expected regret bound of O(√(KT + Λ)), where Λ is the total feedback inaccuracy measure.
- These bounds are optimal (up to logarithmic terms) in the delayed setting, where Λ equals the total delay.
- Applications to optimistic delayed feedback, corrupted feedback, and composite delayed feedback settings, achieving previously known or new optimal bounds.

## Why This Works (Mechanism)
The proposed algorithms work by continuously updating the agent's beliefs on the true loss based on the most recent feedback, even if it contradicts previous observations. This allows the agent to adapt to the evolving nature of the feedback and minimize regret. The use of exponential weights and follow-the-regularized-leader variants enables efficient updates and convergence to the optimal solution.

## Foundational Learning
- Online learning with bandit feedback: necessary for understanding the problem setting and the proposed algorithms.
- Regret minimization: crucial for evaluating the performance of the proposed algorithms.
- Exponential weights and follow-the-regularized-leader methods: key components of the proposed algorithms.
- Feedback evolution and inaccuracy measures: important concepts for understanding the proposed framework.

## Architecture Onboarding
Component map: Exponential weights -> Follow-the-regularized-leader variants -> Feedback evolution
Critical path: Continuous update of beliefs -> Regret minimization -> Convergence to optimal solution
Design tradeoffs: Balancing exploration and exploitation, handling evolving feedback, computational efficiency
Failure signatures: Poor regret bounds, slow convergence, inability to adapt to evolving feedback
First experiments: 1) Evaluate regret bounds on synthetic data with known feedback evolution patterns. 2) Compare performance with existing methods on real-world datasets. 3) Investigate sensitivity to assumptions about feedback evolution.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical nature of the proposed algorithms, with no practical implementation or empirical evaluation.
- Focus on worst-case analysis, which may not capture nuances of specific application domains.
- Assumption of bounded evolution of feedback, which may not hold in all scenarios.

## Confidence
High: The theoretical analysis is rigorous and supported by proofs in the appendix. The paper provides a clear and comprehensive overview of the proposed framework and its applications.

## Next Checks
1. Conduct empirical studies to evaluate the performance of the proposed algorithms on real-world datasets and compare with existing methods.
2. Investigate the sensitivity of the algorithms to the assumptions made in the theoretical analysis, such as the bounded evolution of feedback.
3. Explore the potential applications of the framework in domains beyond those mentioned in the paper, such as online advertising or recommendation systems.