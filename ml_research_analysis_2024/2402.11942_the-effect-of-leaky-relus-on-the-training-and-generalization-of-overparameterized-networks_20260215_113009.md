---
ver: rpa2
title: The effect of Leaky ReLUs on the training and generalization of overparameterized
  networks
arxiv_id: '2402.11942'
source_url: https://arxiv.org/abs/2402.11942
tags:
- bound
- training
- lemma
- probability
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the impact of Leaky ReLU activation functions\
  \ on training and generalization in overparameterized neural networks. The authors\
  \ establish upper bounds on training error convergence rates and generalization\
  \ error, revealing that the Leaky ReLU parameter \u03B1=-1 (corresponding to the\
  \ absolute value function) yields optimal results for both."
---

# The effect of Leaky ReLUs on the training and generalization of overparameterized networks

## Quick Facts
- arXiv ID: 2402.11942
- Source URL: https://arxiv.org/abs/2402.11942
- Reference count: 40
- This paper shows that using the absolute value function (α = -1) for leaky ReLU activation yields optimal training convergence rates and generalization error bounds in overparameterized neural networks.

## Executive Summary
This paper investigates how the leaky ReLU activation function parameter α affects both training and generalization in overparameterized neural networks. The authors establish theoretical bounds showing that α = -1 (the absolute value function) optimizes training error convergence rates and generalization error bounds. Their analysis reveals that deeper networks with α = -1 converge faster during training and achieve better generalization when training epochs are sufficiently small. Extensive numerical experiments on synthetic and real datasets empirically validate these theoretical predictions, demonstrating that the absolute value activation can outperform traditional ReLU and small-positive-α leaky ReLU choices in both training efficiency and generalization performance.

## Method Summary
The authors analyze fully connected neural networks with L hidden layers, each containing m neurons, using rescaled leaky ReLU activation functions. They establish upper bounds on training error convergence rates and generalization error, showing that α = -1 optimizes both. The theoretical framework builds on neural tangent kernel (NTK) theory for training analysis and Rademacher complexity for generalization bounds. Numerical experiments validate the theory using synthetic data, F-MNIST, CIFAR-10, MNIST, California housing, and IMDB datasets with various architectures including modified VGG19, LSTM, and Transformer models. Training employs gradient descent or stochastic gradient descent with mean squared error or cross-entropy loss, using rescaled initialization specific to each α value.

## Key Results
- α = -1 (absolute value function) yields optimal training error convergence rates among all leaky ReLU parameters
- For deep networks with large datasets, α = -1 achieves optimal generalization error bounds when training epochs are sufficiently small
- The rescaled leaky ReLU technique simplifies theoretical analysis while maintaining practical equivalence to unscaled versions
- Numerical experiments confirm theoretical predictions across synthetic and real-world datasets including MNIST, CIFAR-10, and IMDB

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The absolute value activation function (α = -1) optimizes training convergence rates in overparameterized networks.
- Mechanism: When using leaky ReLU with α = -1, the derivative gap (difference between right and left derivatives at zero) is maximized. This larger derivative gap improves gradient flow during backpropagation, leading to faster convergence rates.
- Core assumption: The training error convergence rate is inversely proportional to the derivative gap of the activation function.
- Evidence anchors:
  - [abstract]: "Our analysis shows that the optimal convergence rate bound is achieved at α = -1"
  - [section]: "the negative values of α yield better results than ReLU and the optimal choice of α is -1"
  - [corpus]: Weak correlation - no direct neighbor papers discuss absolute value activation specifically
- Break condition: If the network architecture significantly deviates from the overparameterized regime or if the initialization scheme changes dramatically.

### Mechanism 2
- Claim: Small training epochs with α = -1 optimize generalization error bounds in deep networks with large datasets.
- Mechanism: Early stopping combined with α = -1 activation minimizes the trade-off between training error reduction and network complexity terms in the generalization bound. The absolute value function maintains gradient flow while preventing overfitting during early training phases.
- Core assumption: Generalization error bounds contain terms that increase with training epochs, creating a trade-off that can be optimized by early stopping.
- Evidence anchors:
  - [abstract]: "the optimal bound of the generalization error is achieved at α = -1 using small training epochs"
  - [section]: "For sufficiently large datasets, deep NN and small training epochs, the bound is optimal at α = -1"
  - [corpus]: No direct evidence - neighboring papers focus on stability rather than early stopping optimization
- Break condition: If the dataset size is too small to justify the depth requirements, or if the network architecture prevents effective early stopping.

### Mechanism 3
- Claim: The rescaling technique for leaky ReLU simplifies theoretical analysis while maintaining practical equivalence.
- Mechanism: By moving the 1/(1+α²) factor from weight initialization to the activation function, the analysis becomes more tractable while preserving the behavior of the unscaled leaky ReLU. This equivalence allows leveraging existing proof frameworks with minimal modification.
- Core assumption: The rescaled and unscaled leaky ReLU functions are mathematically equivalent in their effect on network training dynamics.
- Evidence anchors:
  - [section]: "Our idea of rescaling the leaky ReLU, along with the observation that, with rescaled initialization, it is equivalent to using the unscaled leaky ReLU, helped tremendously simplify our initial technical and complex effort"
  - [section]: "The theoretical study of the latter formulation with its rescaled Leaky ReLU function, ˜σα(x) (see (2)), turns out to be more tractable"
  - [corpus]: No direct evidence - neighboring papers don't discuss activation function rescaling techniques
- Break condition: If the rescaling factor significantly affects numerical stability in practical implementations.

## Foundational Learning

- Concept: Overparameterization in neural networks
  - Why needed here: The entire theoretical framework depends on the network having sufficient width and depth to avoid local minima and achieve global convergence
  - Quick check question: What is the minimum width requirement for the convergence theorems to hold, and how does it scale with network depth?

- Concept: Neural tangent kernel (NTK) regime
  - Why needed here: The proof framework builds on NTK theory to analyze training dynamics in the infinite-width limit
  - Quick check question: How does the NTK framework differ from traditional optimization analysis when applied to deep networks?

- Concept: Generalization bounds in statistical learning theory
  - Why needed here: The generalization error analysis relies on bounding the Rademacher complexity of the neural network function class
  - Quick check question: What role does the number of training epochs play in the generalization bound, and why does early stopping help?

## Architecture Onboarding

- Component map: Input layer → L hidden layers with m neurons each (using rescaled leaky ReLU activation) → Output layer. Parameters A (input weights), W₁...Wₗ (hidden weights), and B (output weights). Training focuses on W₁...Wₗ while A and B are fixed.
- Critical path: 1) Initialize parameters using Algorithm 1 with rescaled leaky ReLU factor, 2) Train using either GD or SGD with small learning rate, 3) Monitor training error convergence, 4) Apply early stopping for generalization optimization.
- Design tradeoffs: Using α = -1 maximizes training speed but may require careful initialization to maintain numerical stability. The overparameterized regime ensures convergence but demands significant computational resources.
- Failure signatures: Slow convergence indicates insufficient width or inappropriate learning rate. Poor generalization suggests overfitting from too many training epochs or inadequate dataset size.
- First 3 experiments:
  1. Synthetic dataset with L=5, m=5000, α=-1: Verify fastest training convergence among tested α values
  2. MNIST classification with L=2, m=2000, α=-1: Compare testing accuracy at early epochs against other α values
  3. CIFAR-10 with deep architecture, α=-1: Test whether increased depth maintains convergence advantage with α=-1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the absolute value activation function (α=-1) maintain its optimality for training convergence and generalization when applied to convolutional neural networks (CNNs) beyond the synthetic and image datasets tested?
- Basis in paper: [explicit] The paper tests the synthetic dataset, F-MNIST, and CIFAR-10 with CNN architectures, showing α=-1 yields optimal training convergence. The authors suggest extending the theory to CNNs as a possible direction.
- Why unresolved: The current theoretical framework focuses on fully connected networks. While empirical results support α=-1 for CNNs on tested datasets, the mathematical proofs for CNNs remain unexplored.
- What evidence would resolve it: Rigorous convergence rate and generalization error bounds for CNNs using the absolute value activation, supported by numerical experiments across diverse CNN architectures and datasets.

### Open Question 2
- Question: What is the precise relationship between the optimal Leaky ReLU parameter α and the degree of dataset complexity or separability for overparameterized networks?
- Basis in paper: [inferred] The authors note that α=-1 is optimal for sufficiently large datasets and deep networks, but observe that for simpler datasets like MNIST, other α values may perform comparably. This suggests dataset complexity influences optimal α.
- Why unresolved: The theoretical bounds don't explicitly quantify how dataset properties affect the optimal α choice. The numerical experiments show varying performance across datasets but don't systematically explore this relationship.
- What evidence would resolve it: A theoretical framework that characterizes optimal α as a function of dataset properties (e.g., dimensionality, margin, approximation capacity) with corresponding empirical validation.

### Open Question 3
- Question: How does the choice of Leaky ReLU parameter α affect the generalization error for very deep networks (L>20) and extremely overparameterized regimes where m >> n?
- Basis in paper: [inferred] The theoretical analysis assumes L is sufficiently large but finite, and the experiments test networks up to moderate depths. The authors note that for very large numbers of epochs, overfitting occurs, suggesting potential issues in extreme regimes.
- Why unresolved: The current bounds on generalization error may not capture the behavior in extreme overparameterization regimes. The empirical results show α=-1 is optimal for early stopping but don't explore very deep or extremely wide networks.
- What evidence would resolve it: Extended theoretical bounds for generalization error in the extreme overparameterization regime, validated by experiments on very deep and extremely wide networks across diverse datasets.

## Limitations

- The theoretical framework assumes sufficiently large network width and depth, but the exact thresholds are not precisely quantified
- Generalization bounds depend on early stopping, but the optimal stopping criterion varies with dataset characteristics
- The rescaled initialization technique, while mathematically elegant, may introduce numerical instability in practical implementations

## Confidence

- Training convergence optimization: Medium - Theoretical bounds appear sound within NTK framework but depend on specific initialization schemes
- Generalization error optimization: Medium - Early stopping hypothesis needs more systematic validation across dataset sizes
- Rescaling technique equivalence: Medium - Mathematical equivalence claimed but numerical stability concerns exist

## Next Checks

1. Test the convergence advantage of α = -1 on networks with varying width-to-depth ratios to identify minimum overparameterization requirements
2. Compare generalization performance across different dataset sizes to validate the early stopping hypothesis for small vs. large datasets
3. Implement the unscaled leaky ReLU with modified initialization to verify the claimed equivalence of the rescaling technique