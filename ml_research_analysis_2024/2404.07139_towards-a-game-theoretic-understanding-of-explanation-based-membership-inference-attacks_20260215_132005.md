---
ver: rpa2
title: Towards a Game-theoretic Understanding of Explanation-based Membership Inference
  Attacks
arxiv_id: '2404.07139'
source_url: https://arxiv.org/abs/2404.07139
tags:
- system
- end-user
- variance
- explanation
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a game-theoretic approach to analyze explanation-based
  membership inference attacks (MIA) in machine learning. The authors model the interactions
  between an ML system and a malicious end-user as a continuous-time stochastic signaling
  game, where the variance of the explanations generated by the system follows a geometric
  Brownian motion.
---

# Towards a Game-theoretic Understanding of Explanation-based Membership Inference Attacks

## Quick Facts
- arXiv ID: 2404.07139
- Source URL: https://arxiv.org/abs/2404.07139
- Reference count: 40
- Primary result: Game-theoretic analysis shows explanation variance following GBM enables adversaries to distinguish training members from non-members, with attack accuracy significantly impacted by explanation method, input dimensionality, and model size.

## Executive Summary
This paper proposes a novel game-theoretic framework to analyze explanation-based membership inference attacks (MIA) in machine learning systems. The authors model the interaction between an ML system and a malicious end-user as a continuous-time stochastic signaling game, where the variance of explanations follows a Geometric Brownian Motion. By characterizing the Markov Perfect Equilibrium of this game, they derive conditions for optimal strategies on both sides and demonstrate through extensive experiments that the adversary's capability to launch MIAs depends significantly on factors like explanation method choice, input dimensionality, model size, and training rounds. The results reveal substantial vulnerability of ML systems to explanation-based MIAs, with attack accuracy potentially being very high.

## Method Summary
The authors model interactions between an ML system and a malicious end-user as a continuous-time stochastic signaling game. The system generates explanations for user queries, with variance following a Geometric Brownian Motion (GBM). The adversary iteratively queries the system, observing explanation variances to estimate an optimal threshold for distinguishing training members from non-members. The system maintains beliefs about user type and adds noise to explanations accordingly. The Markov Perfect Equilibrium is characterized by cutoff functions U(π) and L(π), where U(π) determines when the system blocks the adversary based on variance, and L(π) determines when the adversary has gathered sufficient information to attack. Experiments are conducted on five datasets using four gradient-based explanation methods to evaluate attack accuracy.

## Key Results
- Attack accuracy (TPR) is significantly influenced by the choice of explanation method, with some methods enabling higher inference success rates than others
- Input dimensionality, model size, and number of training rounds are critical factors affecting the adversary's capability to launch successful MIAs
- The game-theoretic analysis reveals that a unique Markov Perfect Equilibrium exists under certain conditions, providing optimal strategies for both the system and adversary

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The adversary can compute an optimal threshold for explanation variance to distinguish training members from non-members through repeated interaction.
- **Mechanism**: The adversary iteratively queries the system, observing the variance of explanations generated by gradient-based methods. This variance follows a Geometric Brownian Motion (GBM), which allows the adversary to model the evolution of variance over time and estimate when the threshold is reached.
- **Core assumption**: The variance of explanations for training data points is higher than for non-training data points, and this difference is detectable through repeated sampling.
- **Evidence anchors**:
  - [abstract] "we model such interactions by employing a continuous-time stochastic signaling game framework"
  - [section 3.1] "We assume that the optimal adversary's query choices will have the property such that the resulting explanation variance will follow a GBM"
  - [corpus] Weak - no direct citations about GBM modeling in MIA literature
- **Break condition**: If the explanation variance does not follow a predictable pattern (e.g., due to high noise or adversarial defenses), the GBM assumption breaks and threshold estimation fails.

### Mechanism 2
- **Claim**: The system can optimally add noise to explanations based on its belief about the adversary type to prevent successful MIA.
- **Mechanism**: The system maintains a belief π about whether the current user is honest or malicious. It uses this belief to determine how much noise to add to explanations - higher belief in malicious intent leads to more noise, obscuring the variance signal.
- **Core assumption**: The system has imperfect information about user type but can update beliefs using Bayes' rule based on observed behavior.
- **Evidence anchors**:
  - [abstract] "we model such interactions by employing a continuous-time stochastic signaling game framework"
  - [section 3.2] "The system wants to give informative or relevant explanations to the honest end-user, but noisy explanations to the malicious end-user"
  - [corpus] Weak - no direct citations about belief-based noise addition in MIA defense
- **Break condition**: If the adversary can perfectly mimic honest behavior or if the system's belief updates are inaccurate, the noise addition strategy becomes ineffective.

### Mechanism 3
- **Claim**: A unique Markov Perfect Equilibrium exists where both the adversary and system play optimal strategies based on the evolving variance process.
- **Mechanism**: The equilibrium is characterized by two cutoff functions U(π) and L(π) - U(π) tells the system when to block the adversary based on explanation variance, and L(π) tells the adversary when they've gathered enough information to attack.
- **Core assumption**: The game reaches a steady state where neither player can improve their payoff by unilaterally changing strategy.
- **Evidence anchors**:
  - [abstract] "we characterize the Markov Perfect Equilibrium of the game"
  - [section 3.3] "A Markov Perfect Equilibrium (MPE) consists of a strategy profile and a state process"
  - [corpus] Weak - no direct citations about MPE in MIA context
- **Break condition**: If the conditions for existence/uniqueness of MPE are not met (e.g., certain parameter regimes), the equilibrium breaks down and strategies become suboptimal.

## Foundational Learning

- **Concept**: Geometric Brownian Motion (GBM) and stochastic differential equations
  - Why needed here: GBM models the evolution of explanation variance over time, which is central to the adversary's strategy of estimating thresholds
  - Quick check question: If a GBM process has drift μ=0.1 and volatility σ=0.2, starting at S₀=1, what is the expected value at t=1? (Answer: S₀e^μt = e^0.1 ≈ 1.105)

- **Concept**: Markov Perfect Equilibrium in dynamic games
  - Why needed here: The equilibrium concept ensures that both the system and adversary are playing optimal strategies given the other's strategy
  - Quick check question: In a two-player game with perfect information, what distinguishes a Markov Perfect Equilibrium from a subgame perfect equilibrium? (Answer: MPE strategies depend only on the current state, not the entire history)

- **Concept**: Bayes' rule for belief updating in signaling games
  - Why needed here: The system uses Bayes' rule to update its belief about the adversary type based on observed behavior, which informs its noise addition strategy
  - Quick check question: If the prior probability of an honest user is 0.7, and the likelihood of observing behavior X given honest is 0.8 while given malicious is 0.3, what is the posterior probability of honest given X? (Answer: 0.7×0.8/(0.7×0.8+0.3×0.3) ≈ 0.86)

## Architecture Onboarding

- **Component map**: Adversary queries → System generates explanation → System adds noise based on belief → Adversary observes variance → Adversary updates threshold estimate → Repeat until threshold reached or blocked

- **Critical path**: Adversary queries → System generates explanation → System adds noise based on belief → Adversary observes variance → Adversary updates threshold estimate → Repeat until threshold reached or blocked

- **Design tradeoffs**:
  - Computational cost vs. accuracy: More sophisticated MPE solvers are more accurate but slower
  - Noise level vs. utility: More noise better protects against MIA but reduces explanation quality for honest users
  - Belief update frequency vs. responsiveness: More frequent updates allow faster adaptation but increase computational overhead

- **Failure signatures**:
  - MPE solver fails to converge: Indicates poor parameter choices or numerical instability
  - Attack accuracy remains low: Could indicate effective defense or incorrect model assumptions
  - System blocks too many honest users: Belief updates may be too aggressive

- **First 3 experiments**:
  1. Verify GBM assumption: Plot explanation variance over time for both training and test data to confirm it follows GBM
  2. Test MPE computation: Solve for U(π) and L(π) on a simple synthetic dataset and verify equilibrium conditions
  3. Evaluate attack success: Run full game with a baseline explanation method and measure attack accuracy against known training/test splits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the presence of multiple adversaries with different strategies affect the equilibrium in the game-theoretic model?
- Basis in paper: [inferred] The paper models interactions between a single adversary and the system, but does not consider the impact of multiple adversaries with varying strategies.
- Why unresolved: The current model assumes a single adversary, and extending it to multiple adversaries would require a more complex analysis of the equilibrium and potential strategic interactions between adversaries.
- What evidence would resolve it: Simulations or analytical results showing the impact of multiple adversaries on the game's equilibrium, including the optimal strategies for both the system and the adversaries.

### Open Question 2
- Question: How does the system's belief about the adversary's type evolve over time, and how does this affect the optimal strategies?
- Basis in paper: [explicit] The paper mentions that the system has imperfect information about the type of adversary and updates its belief using Bayes' rule, but does not explore how this belief evolution impacts the optimal strategies.
- Why unresolved: The belief evolution is a dynamic process that could significantly influence the system's and adversary's strategies, and understanding this relationship is crucial for designing effective defenses.
- What evidence would resolve it: Analysis of the belief evolution over time and its impact on the optimal strategies, including how the system's belief affects its noise addition and blocking decisions, and how the adversary adapts its strategy based on the system's belief.

### Open Question 3
- Question: How does the choice of explanation method affect the vulnerability of the system to MIA, and can certain explanation methods provide better protection against these attacks?
- Basis in paper: [explicit] The paper evaluates the impact of different explanation methods (Gradient*Input, Integrated Gradients, LRP, and Guided Backpropagation) on the adversary's capability to launch MIA, but does not provide a comprehensive comparison of their effectiveness in preventing these attacks.
- Why unresolved: The choice of explanation method could significantly influence the system's vulnerability to MIA, and understanding which methods provide better protection is essential for designing secure ML systems.
- What evidence would resolve it: A comparative analysis of the effectiveness of different explanation methods in preventing MIA, including their impact on the game's equilibrium and the adversary's attack accuracy.

## Limitations

- The fundamental assumption that explanation variance follows a Geometric Brownian Motion lacks strong empirical validation and may not hold for all model architectures or datasets
- The game-theoretic framework assumes a single adversary, which may not reflect real-world scenarios with multiple attackers
- The effectiveness of the belief-based noise addition defense depends on the system's ability to accurately distinguish between honest and malicious users, which may be challenging in practice

## Confidence

- GBM assumption: Medium confidence - weak corpus support and lack of direct empirical verification
- MPE characterization: Medium confidence - mathematical framework appears sound but requires broader empirical validation
- Belief-based defense: Low confidence - limited discussion of evasion strategies and potential for adversaries to mimic honest behavior

## Next Checks

1. Empirically verify the GBM assumption by plotting explanation variance trajectories across multiple datasets and explanation methods
2. Test the robustness of the MPE solution by varying payoff coefficients across a wider range of values
3. Evaluate the defense mechanism against an adaptive adversary that attempts to mimic honest behavior patterns