---
ver: rpa2
title: 'MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data
  Visualization'
arxiv_id: '2402.11453'
source_url: https://arxiv.org/abs/2402.11453
tags: []
core_contribution: 'This paper introduces MatPlotAgent, a model-agnostic framework
  for automating scientific data visualization using large language models (LLMs).
  The approach consists of three core modules: query understanding, code generation
  with iterative debugging, and visual feedback for error correction.'
---

# MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization

## Quick Facts
- arXiv ID: 2402.11453
- Source URL: https://arxiv.org/abs/2402.11453
- Authors: Zhiyu Yang; Zihan Zhou; Shuo Wang; Xin Cong; Xu Han; Yukun Yan; Zhenghao Liu; Zhixing Tan; Pengyuan Liu; Dong Yu; Zhiyuan Liu; Xiaodong Shi; Maosong Sun
- Reference count: 6
- Primary result: MatPlotAgent achieves 61.16% success rate on scientific visualization tasks using GPT-4, with automatic evaluation showing strong correlation (r=0.876) with human judgment

## Executive Summary
MatPlotAgent is a model-agnostic framework that automates scientific data visualization by combining code generation with multi-modal visual feedback. The system uses query expansion to convert high-level requirements into detailed instructions, generates code with iterative debugging, and employs GPT-4V to analyze visual discrepancies and provide corrective feedback. The framework demonstrates significant performance improvements across various LLMs and introduces MatPlotBench, a benchmark of 100 human-verified test cases, along with an automatic evaluation method using GPT-4V scoring.

## Method Summary
MatPlotAgent consists of three core modules: query understanding with expansion, code generation with iterative debugging, and visual feedback using GPT-4V. The system processes user queries through query expansion to generate detailed instructions, then uses a code agent to generate Python visualization code with self-debugging capabilities. Generated figures are analyzed by GPT-4V, which compares them against ground truth images and provides specific improvement instructions. This iterative process continues until the visualization meets requirements or maximum iterations are reached. The framework is designed to be model-agnostic and can work with both commercial and open-source LLMs.

## Key Results
- GPT-4 augmented with MatPlotAgent achieves 61.16% success rate on MatPlotBench
- Automatic evaluation method using GPT-4V shows strong correlation (r=0.876) with human-annotated scores
- Framework improves performance across multiple LLMs including GPT-3.5, GPT-4, Claude-3-Sonnet, and DeepSeek-Coder-V2
- MatPlotBench benchmark contains 100 human-verified test cases for scientific data visualization

## Why This Works (Mechanism)

### Mechanism 1: Visual Feedback from GPT-4V
GPT-4V analyzes generated figures and provides specific code improvement instructions by identifying visual discrepancies with user requirements. The core assumption is that GPT-4V can accurately perceive visual errors and translate them into actionable code modifications. Break condition: If GPT-4V cannot accurately identify visual errors or generate useful feedback, the improvement mechanism fails.

### Mechanism 2: Query Expansion
The query expansion module converts high-level user requirements into detailed step-by-step instructions including library imports, function calls, and parameter settings. The core assumption is that LLMs generate better code when given detailed procedural instructions rather than high-level requirements. Break condition: If query expansion produces incorrect or incomplete instructions, downstream code generation will fail.

### Mechanism 3: Self-Debugging with Iterative Refinement
The code agent uses a self-debugging mechanism to identify and correct bugs through multiple iterations (maximum 3). The core assumption is that LLMs can recognize their own coding errors and generate correct fixes when provided with error messages. Break condition: If the model cannot identify root causes or generates incorrect fixes, the iterative process will not converge to working code.

## Foundational Learning

- **Multi-modal understanding in LLMs**: GPT-4V must understand both text queries and visual figures to provide meaningful feedback. Quick check: Can GPT-4V accurately describe the differences between two similar plots?
- **Code generation with error handling**: The code agent must generate executable Python code and handle errors during execution. Quick check: Does the model generate syntactically correct Python code for basic plotting tasks?
- **Iterative refinement processes**: Both self-debugging and visual feedback rely on iterative improvement cycles. Quick check: Can the model successfully modify code based on feedback to fix specific issues?

## Architecture Onboarding

- **Component map**: User Query → Query Expansion Module → Code Agent → Code Generation → Figure Output → Visual Agent (GPT-4V) → Visual Feedback → Code Agent (for refinement) → Ground Truth Figure → Automatic Evaluation (GPT-4V scoring)
- **Critical path**: User Query → Query Expansion → Code Agent → Figure Output → Evaluation
- **Design tradeoffs**: Using GPT-4V for visual feedback provides high accuracy but adds cost and latency; iterative debugging improves success rates but increases computational cost; model-agnostic design allows flexibility but requires careful prompt engineering
- **Failure signatures**: Blank figure output indicates code generation failure; low automatic scores with high human scores suggest evaluation method limitations; code agent improvements without visual feedback changes indicate visual agent issues
- **First 3 experiments**: 1) Test query expansion with simple plotting requests to verify instruction quality; 2) Run code agent with self-debugging on basic plotting errors to verify iterative correction; 3) Validate visual agent feedback by comparing draft figures against ground truths for simple cases

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed visual feedback mechanism be effectively implemented using open-source multi-modal LLMs, or is it inherently dependent on proprietary models like GPT-4V? The authors suggest exploring open-source alternatives but did not experiment with them.

### Open Question 2
How well does MatPlotAgent generalize to scientific data visualization tasks outside of the domains covered by MatPlotBench? The benchmark may not cover all possible scientific visualization scenarios.

### Open Question 3
What is the impact of the maximum iteration limit (set to 3) for the self-debugging mechanism on overall performance? The chosen limit may not be optimal for all scenarios.

## Limitations

- Overall success rate of 61.16% indicates the system fails in nearly 40% of cases
- Performance degrades significantly on more challenging tasks with only 27 "hard" test cases in the benchmark
- Visual feedback mechanism shows limited effectiveness in certain scenarios and may not always identify the most relevant errors

## Confidence

**High Confidence**: MatPlotAgent improves visualization performance across different LLM architectures; automatic evaluation method correlates strongly with human judgment; model-agnostic design works as intended.

**Medium Confidence**: Visual feedback significantly improves results; query expansion effectively translates high-level requirements; iterative debugging is beneficial.

**Low Confidence**: Framework can handle complex real-world scientific visualization tasks; automatic evaluation method is reliable for all types of visualization errors.

## Next Checks

1. **Robustness Testing**: Run MatPlotAgent on a larger, more diverse set of real-world scientific visualization tasks from multiple domains to assess generalization beyond the current benchmark.

2. **Error Analysis**: Conduct detailed error analysis on the 38.84% of cases where GPT-4 fails, categorizing failure modes to identify specific weaknesses.

3. **Human Evaluation Validation**: Perform comprehensive human evaluation on a random sample of generated visualizations to validate the automatic scoring method, particularly focusing on edge cases where automatic and human scores might diverge.