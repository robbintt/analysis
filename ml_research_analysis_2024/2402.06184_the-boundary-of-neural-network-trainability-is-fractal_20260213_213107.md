---
ver: rpa2
title: The boundary of neural network trainability is fractal
arxiv_id: '2402.06184'
source_url: https://arxiv.org/abs/2402.06184
tags:
- training
- hyperparameters
- network
- fractal
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines the boundary between hyperparameters that lead
  to stable versus divergent neural network training. It finds this boundary is fractal,
  with dimension estimates ranging from 1.17 to 1.98 across various experimental conditions
  including different nonlinearities, batch sizes, and hyperparameter choices.
---

# The boundary of neural network trainability is fractal

## Quick Facts
- arXiv ID: 2402.06184
- Source URL: https://arxiv.org/abs/2402.06184
- Reference count: 15
- Key outcome: Neural network training boundary between stable and divergent behavior is fractal with dimension estimates ranging from 1.17 to 1.98

## Executive Summary
This paper reveals that the boundary between hyperparameters leading to stable versus divergent neural network training exhibits fractal structure. Through visualizing training success/failure across grids of learning rates, the author demonstrates that this boundary is not smooth but instead has a complex, self-similar structure spanning more than ten decades of scale. This finding provides a more nuanced explanation for chaotic meta-loss landscapes in hyperparameter optimization, suggesting that sensitivity to small changes in hyperparameters arises from the underlying fractal geometry of the trainability boundary.

## Method Summary
The paper visualizes neural network training behavior by conducting grid searches over learning rate hyperparameters (η0 and η1) for one hidden layer networks with width n=16. Networks are initialized with random weights from N(0,1) and trained on random data using either full batch or minibatch gradient descent for 500-1000 iterations. Training runs are classified as converging or diverging based on whether they reach low loss values or explode to infinity. The resulting visualizations show the boundary between these regions has fractal structure, with fractal dimensions estimated using box-counting methods across six different experimental conditions including various nonlinearities (tanh, ReLU, identity), batch sizes, and hyperparameter choices.

## Key Results
- The boundary between stable and divergent training behavior is fractal with dimension estimates ranging from 1.17 to 1.98
- Fractal structure persists across more than ten decades of scale and is observed in both deterministic and stochastic (minibatch) training
- The fractal arises from iterating update functions during training, analogous to how simple iterated functions generate classical fractals like the Mandelbrot set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The boundary between stable and divergent training behavior forms a fractal because small changes in hyperparameters lead to large changes in training outcomes.
- Mechanism: Training involves iterating update functions (like gradient descent), which is analogous to iterating functions that generate classical fractals. When iterating these functions, small perturbations in hyperparameters can cause the resulting series to diverge or remain bounded, creating a bifurcation boundary.
- Core assumption: The training dynamics can be modeled as iterated function systems where hyperparameters act as initial conditions or parameters of the iteration.
- Evidence anchors:
  - [abstract]: "Some fractals... are computed by iterating a function, and identifying the boundary between hyperparameters for which the resulting series diverges or remains bounded."
  - [section]: "When we train a neural network, we iterate a function (a gradient descent step) of many variables (the parameters of the neural network)."
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism, though related work mentions "fractal trainability boundary."
- Break condition: If training dynamics become deterministic and smooth (e.g., only training output layer for linear regression), the boundary would not be fractal.

### Mechanism 2
- Claim: Fractal structure emerges from the complex interaction between network initialization, training data, and hyperparameter choices during training iterations.
- Mechanism: Neural network training involves iterating a complicated function with many random terms from weight initialization and training data. These random elements combined with hyperparameter sensitivity create fractal boundaries.
- Core assumption: The randomness in network initialization and training data contributes to the fractal structure of the boundary.
- Evidence anchors:
  - [section]: "The resulting fractals seem visually more organic, with less repeated structure and symmetry. It will be a fascinating to further explore how properties of fractals depend on properties of the generating function."
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism, though related work mentions "fractal trainability boundary."
- Break condition: If all randomness is removed (e.g., fixed initialization and dataset), the fractal structure may not emerge.

### Mechanism 3
- Claim: Stochastic training (like minibatch gradient descent) still generates fractal boundaries despite the noise from minibatch sampling.
- Mechanism: Even though minibatch training involves stochastic function iteration due to minibatch sampling, the fine multiscale structure of the fractal boundary is preserved. This is similar to Lyapunov fractals, where the function changes at every time step but still generates fractal structure.
- Core assumption: Stochastic perturbations do not destroy the fractal structure of the boundary.
- Evidence anchors:
  - [section]: "For minibatch training, the iterated function is stochastic rather than deterministic due to minibatch sampling. I was surprised that this stochastic function also generated fractals, without the fine multiscale structure being corrupted by minibatch noise."
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism.
- Break condition: If minibatch noise becomes too large relative to the scale of fractal structure, it could destroy the fractal pattern.

## Foundational Learning

- Concept: Iterated function systems and fractal generation
  - Why needed here: Understanding how fractals are generated through function iteration is crucial to grasping why neural network training boundaries exhibit fractal properties.
  - Quick check question: How does iterating the function f(z) = z² + c generate the Mandelbrot set?

- Concept: Gradient descent as iterated function
  - Why needed here: Neural network training is fundamentally an iterative process of applying gradient descent updates, which is the key connection to fractal generation.
  - Quick check question: What is the mathematical form of a single gradient descent step in parameter space?

- Concept: Bifurcation boundaries and sensitivity to initial conditions
  - Why needed here: The fractal boundary exists because small changes in hyperparameters lead to dramatically different training outcomes - a hallmark of chaotic systems.
  - Quick check question: What does it mean for a system to exhibit bifurcation behavior?

## Architecture Onboarding

- Component map: Network initialization -> Grid search over η0, η1 -> Gradient descent iterations -> Convergence/divergence classification -> Visualization
- Critical path:
  1. Initialize network parameters and dataset
  2. Grid search over learning rate hyperparameters
  3. Train network for 500-1000 iterations
  4. Classify runs as converging or diverging
  5. Visualize results with color coding
  6. Estimate fractal dimension using box-counting method
- Design tradeoffs:
  - Full batch vs. minibatch: Full batch is deterministic but computationally expensive; minibatch is stochastic but faster
  - Grid resolution: Higher resolution reveals more fractal detail but increases computation
  - Training duration: Longer training may reveal more divergence but increases computation
  - Network width: Larger width may smooth behavior but increases computational cost
- Failure signatures:
  - NaN values in weights or loss
  - Loss values that explode to infinity
  - Training runs that plateau at high loss values
  - Grid regions with no convergence regardless of hyperparameters
- First 3 experiments:
  1. Replicate baseline experiment with tanh nonlinearity and full batch training to verify fractal boundary with estimated dimension around 1.66
  2. Test ReLU nonlinearity to confirm fractal boundary persists with different activation functions
  3. Test deep linear network (identity nonlinearity) to verify fractal boundary exists even without nonlinear activations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the properties of fractal bifurcation boundaries vary in different regions of hyperparameter space for the same generating function?
- Basis in paper: [explicit] The paper notes that the boundary is non-homogeneous and that some regions (e.g., where one learning rate is very small) may not be fractal, suggesting variation in boundary properties.
- Why unresolved: The paper acknowledges this as an area for further exploration but did not systematically map how boundary properties change across different hyperparameter regions.
- What evidence would resolve it: Systematic experiments mapping fractal dimension and boundary structure across different hyperparameter regions, comparing regions near successful training versus regions with simple dynamics.

### Open Question 2
- Question: How does minibatch noise affect the fine multiscale structure of fractal boundaries in stochastic training?
- Basis in paper: [explicit] The paper found fractals in minibatch training despite the stochastic nature of the iterated function, but was surprised this didn't corrupt the fine structure.
- Why unresolved: While the paper observed fractal structure in minibatch training, it didn't investigate the robustness of this structure to different levels of noise or compare it to deterministic training.
- What evidence would resolve it: Experiments varying minibatch size and noise levels to determine at what point minibatch noise begins to obscure fractal structure, and comparison of fractal dimensions between deterministic and stochastic training.

### Open Question 3
- Question: What properties of the generating function determine whether it produces fractal versus non-fractal bifurcation boundaries?
- Basis in paper: [inferred] The paper contrasts simple one-dimensional functions (which typically produce symmetric fractals) with the complicated, high-dimensional functions in neural network training, but doesn't explain what causes fractality.
- Why unresolved: The paper demonstrates fractality exists but doesn't analyze the mathematical conditions or function properties that lead to fractal versus non-fractal boundaries.
- What evidence would resolve it: Mathematical analysis of the iterated function's properties (e.g., stability conditions, Lyapunov exponents) that predict fractal boundary formation, validated through experiments with systematically varied function complexity.

## Limitations

- The exact mechanism by which iterated gradient descent generates fractal boundaries remains partially theoretical, with limited empirical validation beyond the presented experiments
- The relationship between fractal dimension estimates (1.17-1.98) and practical implications for hyperparameter optimization is not fully explored
- The robustness of fractal boundaries across different network architectures and training regimes beyond the tested conditions is uncertain

## Confidence

- **High**: The existence of fractal boundaries between stable and divergent training behavior is well-supported by visual evidence across multiple experimental conditions
- **Medium**: The connection between iterated function systems and fractal boundary formation is theoretically sound but requires more rigorous mathematical proof
- **Low**: The practical implications for meta-learning and hyperparameter optimization are speculative and not yet demonstrated

## Next Checks

1. **Mathematical Formalization**: Derive rigorous bounds on the fractal dimension of the trainability boundary as a function of network architecture, initialization scheme, and training algorithm parameters
2. **Cross-Architecture Verification**: Test whether fractal boundaries persist in deeper networks, convolutional architectures, and transformers, examining how dimensionality affects the fractal structure
3. **Optimization Landscape Analysis**: Quantify how fractal boundaries affect the smoothness of meta-loss landscapes and evaluate whether this explains observed difficulties in hyperparameter optimization across multiple orders of magnitude