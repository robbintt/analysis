---
ver: rpa2
title: Identifying Sub-networks in Neural Networks via Functionally Similar Representations
arxiv_id: '2410.16484'
source_url: https://arxiv.org/abs/2410.16484
tags:
- layers
- distance
- layer
- different
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to mechanistic interpretability
  by identifying functionally distinct sub-networks within neural networks. The core
  method uses Gromov-Wasserstein (GW) distance to measure functional similarity between
  intermediate representations across layers, overcoming challenges of varying distributions
  and dimensions.
---

# Identifying Sub-networks in Neural Networks via Functionally Similar Representations

## Quick Facts
- arXiv ID: 2410.16484
- Source URL: https://arxiv.org/abs/2410.16484
- Reference count: 40
- Key outcome: Novel approach using Gromov-Wasserstein distance to identify functionally distinct sub-networks within neural networks by comparing intermediate layer representations

## Executive Summary
This paper introduces a mechanistic interpretability method that identifies functionally distinct sub-networks within neural networks by computing Gromov-Wasserstein (GW) distances between intermediate layer representations. The approach overcomes challenges of varying distributions and dimensions across layers, revealing clear block structures in layer representations with major functional changes concentrated at transition layers. Experiments on synthetic algebraic tasks and NLP sentiment analysis demonstrate that the method successfully identifies sub-networks with minimal human and computational cost, offering insights into model behavior and training dynamics.

## Method Summary
The method extracts intermediate representations from each layer, computes pairwise distances within layers, and uses Gromov-Wasserstein distance to measure functional similarity between layers. Layers with high GW distances from predecessors are treated as functional boundaries, while clusters of layers with low inter-layer GW distances form functional groups. The approach is applied to both synthetic modular sum tasks (fmod, fmod3) and real NLP datasets (Yelp, SST2), comparing layer-wise versus end-to-end training approaches and dense versus sparse model variants.

## Key Results
- Clear block structures emerge in layer representations, with major functional changes concentrated at transition layers
- Layer-wise training produces more distinct functional groups than end-to-end training
- For BERT models, significant differences emerge primarily in later layers (9-12), suggesting most task-specific adaptation occurs there
- GW distance proves more effective than baseline measures (Euclidean, Cosine, Wasserstein) in revealing functional patterns
- Sparse models exhibit less inter-layer variation than dense models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GW distance effectively identifies functionally distinct sub-networks by measuring the minimal distance over all possible transportation plans between two sets of points from different spaces.
- Mechanism: The method computes pairwise distances within each layer's representations and finds the optimal matching between these distances across layers. When this matching is difficult (high cost), it indicates functional differences between layers.
- Core assumption: The pairwise distance structure within layer representations captures the functional behavior of that layer, and significant changes in this structure indicate functional transitions.
- Evidence anchors:
  - [abstract]: "GW allows distance computation between distributions supported on two different metric spaces with different supports and potentially different dimensions"
  - [section]: "GW is also invariant to permutation of the representation within a layer, a crucial property since neural networks are known to have permutation symmetries"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If layer representations don't have meaningful pairwise distance structures, or if functional differences don't manifest in distance preservation patterns.

### Mechanism 2
- Claim: GW distance is invariant to isometric transformations and permutations, making it robust for comparing neural network layers with different dimensionalities and representations.
- Mechanism: By focusing on pairwise distances rather than absolute values, GW distance remains stable under rotations, translations, and permutations within layers, allowing meaningful comparisons across heterogeneous layer representations.
- Core assumption: Isometric transformations preserve functional behavior, so invariance to these transformations is desirable for measuring functional similarity.
- Evidence anchors:
  - [abstract]: "GW can effectively identify genuinely distinct behaviors across (groups of) layers"
  - [section]: "GW is invariant under any isometric transformation of the input, which is advantageous because we do not want rotations and reflections to affect our similarity search"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If functional differences manifest primarily through absolute value changes rather than distance structure changes.

### Mechanism 3
- Claim: The method identifies sub-networks by detecting layers with significantly different GW distances from their predecessors, indicating functional boundaries.
- Mechanism: Layers with high GW distances from previous layers are treated as potential boundaries between functional sub-networks, while clusters of layers with low inter-layer GW distances form functional groups.
- Core assumption: Functional boundaries in neural networks correspond to points where representations undergo significant structural changes.
- Evidence anchors:
  - [abstract]: "We observe the emergence of sub-groups within neural network layers corresponding to functional abstractions"
  - [section]: "We notice the first major difference occurs between layers 13 and 16... The second difference occurs between layers 17 and 20"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If functional boundaries don't correspond to structural changes in representations, or if the method produces false positives/negatives in boundary detection.

## Foundational Learning

- Concept: Optimal transport theory and Wasserstein distance
  - Why needed here: GW distance is built on optimal transport principles, and understanding these foundations is crucial for interpreting results
  - Quick check question: What is the key difference between standard Wasserstein distance and Gromov-Wasserstein distance?

- Concept: Neural network architecture and layer representations
  - Why needed here: The method operates on intermediate layer representations, requiring understanding of what these represent and how they're structured
  - Quick check question: What are the typical dimensions and distributions of representations across different transformer layers?

- Concept: Mechanistic interpretability approaches
  - Why needed here: This work positions itself within the broader context of understanding neural network internals
  - Quick check question: How does the GW distance approach differ from linear probe methods in mechanistic interpretability?

## Architecture Onboarding

- Component map: Representation extraction -> Pairwise distance computation -> GW distance computation -> Sub-network identification
- Critical path: Representation extraction → Pairwise distance computation → GW distance computation → Sub-network identification
- Design tradeoffs: GW distance provides robustness to dimensionality differences but is computationally more expensive than simpler distance measures like Euclidean distance.
- Failure signatures: High GW distances across all layer pairs (suggesting no functional groupings), very low GW distances everywhere (suggesting no functional differentiation), or inconsistent patterns across different datasets/tasks.
- First 3 experiments:
  1. Compute pairwise GW distances between consecutive layers in a simple transformer model and visualize the distance matrix
  2. Compare GW distance results with Euclidean distance on the same model to understand the advantages of GW
  3. Apply spectral clustering on the GW distance matrix to automatically identify sub-networks and validate against known functional boundaries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Gromov-Wasserstein distance be scaled to analyze larger transformer models like GPT-4 or Claude?
- Basis in paper: [explicit] The paper notes GW distance "is scalable since it does not require estimating high-dimensional distributions" and mentions computational complexity of O(mn²+m²n), but acknowledges this may become prohibitive for very large models
- Why unresolved: The paper only demonstrates GW distance on BERT-base (12 layers) and smaller models. The scalability analysis remains theoretical without empirical validation on larger architectures with hundreds of layers
- What evidence would resolve it: Empirical results showing GW distance computation time and memory requirements on progressively larger transformer models, including optimization techniques that enable practical application to frontier models

### Open Question 2
- Question: Can GW distance identify functionally distinct subnetworks in vision transformers and other non-language architectures?
- Basis in paper: [inferred] The paper explicitly states results are limited to "algebraic and real NLP tasks" and mentions "Future work could investigate additional models" but doesn't test other modalities
- Why unresolved: All experiments focus on BERT and modular arithmetic tasks. The paper's theoretical justification for GW distance doesn't depend on language-specific properties, but empirical validation across modalities is absent
- What evidence would resolve it: Application of GW distance to vision transformers, diffusion models, or multimodal architectures showing similar subnetwork identification patterns or revealing modality-specific functional groupings

### Open Question 3
- Question: How does the choice of distance metric within the Gromov-Wasserstein framework affect the identification of functionally similar layers?
- Basis in paper: [explicit] The paper mentions "we can also view GW as a measure that quantifies the distance between distance-based graphs" and notes it's "monotonic in (positive) scaling of pairwise distances" but doesn't systematically explore different internal metrics
- Why unresolved: While the paper compares GW against Euclidean, Cosine, and Wasserstein baselines, it uses a fixed internal distance measure for GW computation without exploring how alternative choices might affect results
- What evidence would resolve it: Systematic experiments varying the internal pairwise distance metric (e.g., Manhattan, Mahalanobis, learned metrics) within GW and demonstrating how different choices impact subnetwork identification quality and downstream task performance

## Limitations
- Synthetic task validity remains uncertain as findings may not generalize to real-world model behavior
- GW distance computational cost may become prohibitive for larger models despite claims of minimal expense
- The paper identifies structural patterns but doesn't establish causal relationships between representation changes and functional behavior

## Confidence

- **High Confidence**: Technical implementation of GW distance computation and its advantages over baseline measures (Euclidean, Cosine, Wasserstein) for comparing representations of different dimensions. The mathematical properties of GW distance are well-established.
- **Medium Confidence**: Identification of block structures in synthetic tasks and their interpretation as functional sub-networks. While the patterns are clearly observable, their functional significance requires additional validation.
- **Low Confidence**: Generalization of findings from synthetic tasks to real NLP models, and the claim that layer-wise training produces more distinct functional groups than end-to-end training based on a single comparison.

## Next Checks
1. Apply GW distance analysis to additional NLP tasks beyond sentiment analysis (e.g., question answering, summarization) to test whether observed layer-wise patterns hold across different downstream applications
2. Systematically vary GW distance computation parameters (regularization strength, sample size) to assess robustness of identified sub-networks and determine optimal parameter settings
3. Design interventions that modify specific sub-networks identified by GW distance and measure impact on model behavior to establish causal relationships between representation structure and function