---
ver: rpa2
title: Continual Learning Using Only Large Language Model Prompting
arxiv_id: '2412.15479'
source_url: https://arxiv.org/abs/2412.15479
tags:
- class
- summary
- learning
- language
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CLOB, a novel continual learning paradigm that
  treats large language models as black boxes and performs learning through verbal
  prompting alone. The key innovation is CIS (Continual Learning via Incremental Summarization),
  which uses the LLM's summarization ability to generate and incrementally update
  class summaries as new data arrives, addressing the token length limitation of LLMs.
---

# Continual Learning Using Only Large Language Model Prompting

## Quick Facts
- arXiv ID: 2412.15479
- Source URL: https://arxiv.org/abs/2412.15479
- Authors: Jiabao Qiu; Zixuan Ke; Bing Liu
- Reference count: 30
- Primary result: Novel continual learning approach using LLM prompting only, achieving accuracy comparable to batch learning while preventing catastrophic forgetting

## Executive Summary
This paper introduces CLOB, a novel continual learning paradigm that treats large language models as black boxes and performs learning through verbal prompting alone. The key innovation is CIS (Continual Learning via Incremental Summarization), which uses the LLM's summarization ability to generate and incrementally update class summaries as new data arrives. This approach addresses the token length limitation of LLMs while achieving accuracy comparable to or better than traditional batch learning settings. The method demonstrates that zero-parameter updating in LLMs can effectively prevent catastrophic forgetting while maintaining strong classification performance.

## Method Summary
The proposed CIS approach performs continual learning by leveraging the LLM's inherent summarization capabilities. When new class data arrives, CIS incrementally updates existing class summaries rather than storing all samples. The method generates verbal prompts that incorporate these summaries to enable classification without fine-tuning model parameters. This black-box approach treats the LLM as a summarization engine that can compress and maintain knowledge about previously seen classes while learning new ones. The incremental nature allows the system to work within token limitations while preventing catastrophic forgetting through the preservation of summarized class information.

## Key Results
- CIS achieves accuracy comparable to or better than batch setting across four text classification datasets (Banking-77, CLINC-80, DBpedia-14, Reuters-14)
- Significantly outperforms state-of-the-art baselines (EWC, LAMOL, V AG) by large margins
- Approaches performance of joint fine-tuning/prompting upper bounds while using zero-parameter updates
- Demonstrates minimal forgetting across incremental learning phases

## Why This Works (Mechanism)
The CIS approach works by leveraging the LLM's natural language understanding and summarization capabilities to compress class information incrementally. Rather than storing all training samples, which would exceed token limits, CIS maintains compressed summaries that capture essential class characteristics. The LLM's ability to reason about language allows it to effectively compress and decompress information through verbal prompts, maintaining discriminative features across classes. This verbal approach circumvents the need for parameter updates while still enabling the model to distinguish between an expanding set of classes as new data arrives sequentially.

## Foundational Learning
- **Catastrophic forgetting**: The tendency of neural networks to rapidly forget previously learned information when trained on new tasks - needed because continual learning must preserve old knowledge while acquiring new
- **Token length limitations**: Constraints on the maximum input length for LLMs - critical because storing all samples would exceed these limits
- **Zero-parameter updates**: Learning approaches that don't modify model weights - essential for treating LLMs as black boxes and preserving their pre-trained capabilities
- **Incremental summarization**: The process of progressively refining and updating compressed representations - necessary for maintaining knowledge efficiency as class count grows
- **Verbal prompting**: Using natural language instructions to guide LLM behavior - fundamental to the black-box approach that avoids parameter modification
- **Knowledge distillation**: Compressing information from larger models into more compact representations - conceptually related to how CIS compresses class information

Quick checks: Test catastrophic forgetting on simple sequential tasks; measure token usage across incremental steps; verify that parameter weights remain unchanged; assess summary quality degradation over time; evaluate prompt effectiveness with different wording.

## Architecture Onboarding

Component map: Data Stream -> CIS Summarizer -> Class Summary Store -> Prompt Generator -> LLM -> Classification Output

Critical path: New data arrives → CIS generates incremental summary → Updates class summary store → Prompt generator creates classification prompt → LLM processes prompt → Returns classification

Design tradeoffs: Zero-parameter updates preserve LLM capabilities but limit adaptation flexibility; summarization reduces token usage but may lose fine-grained details; incremental updates maintain efficiency but accumulate approximation errors; black-box approach simplifies deployment but reduces control over internal representations.

Failure signatures: Degraded classification accuracy on previously learned classes; increasing summary length causing token overflow; prompts becoming too complex for effective LLM processing; summaries losing discriminative information between similar classes.

First experiments: 1) Test incremental summarization on toy dataset with 2-3 classes; 2) Measure catastrophic forgetting with and without CIS summaries; 3) Evaluate summary quality by reconstructing original samples from summaries.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance consistency may vary across different LLM architectures and summarization strengths
- Evaluation focuses primarily on text classification tasks, limiting generalizability to other domains
- Incremental summarization may become increasingly challenging as the number of classes grows substantially
- Computational efficiency of repeated LLM queries for summarization during learning is unclear

## Confidence
- High confidence in effectiveness for tested text classification datasets, given consistent improvements over baseline methods
- Medium confidence in generalizability to other task types and larger class sets, due to limited experimental scope
- Medium confidence in scalability with respect to computational costs and token limitations for very large class numbers

## Next Checks
1. Evaluate CIS performance on non-text classification tasks (e.g., image classification, named entity recognition) to assess domain generalizability
2. Test the approach with different LLM architectures (e.g., GPT-4, Claude, open-source alternatives) to verify robustness to model-specific summarization capabilities
3. Conduct experiments with significantly larger class sets (e.g., 100+ classes) to identify potential limitations in information retention and summarization quality as task complexity increases