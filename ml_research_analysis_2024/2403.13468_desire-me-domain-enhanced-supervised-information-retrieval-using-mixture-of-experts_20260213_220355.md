---
ver: rpa2
title: 'DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using Mixture-of-Experts'
arxiv_id: '2403.13468'
source_url: https://arxiv.org/abs/2403.13468
tags:
- desire-me
- retrieval
- query
- dense
- gating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DESIRE-ME, a Mixture-of-Experts (MoE) framework
  for open-domain question answering. DESIRE-ME integrates specialized neural models
  to handle topic heterogeneity in queries, dynamically selecting and combining experts
  based on a supervised gating mechanism.
---

# DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using Mixture-of-Experts

## Quick Facts
- arXiv ID: 2403.13468
- Source URL: https://arxiv.org/abs/2403.13468
- Reference count: 35
- Outperforms state-of-the-art dense retrieval models by up to 12% in NDCG@10 and 22% in P@1

## Executive Summary
This paper introduces DESIRE-ME, a Mixture-of-Experts (MoE) framework for open-domain question answering that integrates specialized neural models to handle topic heterogeneity in queries. The system uses a supervised gating mechanism trained on Wikipedia data to dynamically select and combine domain-specific experts based on query classification. Experiments on three datasets demonstrate significant performance improvements over state-of-the-art dense retrieval models, with the model also showing strong generalization capabilities in zero-shot scenarios.

## Method Summary
DESIRE-ME employs a Mixture-of-Experts framework where queries are processed through a supervised gating function that classifies them into 37 predefined domains using Wikipedia category labels. The gating function outputs domain probabilities that weight the contributions of specialized expert modules, each containing down-projection and up-projection feed-forward networks. A skip connection ensures out-of-domain queries retain the original dense retrieval representation, preventing degradation from irrelevant expert contributions.

## Key Results
- Achieves up to 12% improvement in NDCG@10 and 22% in P@1 over state-of-the-art dense retrieval models
- Demonstrates strong generalization in zero-shot scenario on Climate-FEVER dataset
- Significantly outperforms both base dense models and random gating baseline across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
The supervised gating function improves query routing by assigning domain-specific weights to each expert, enabling adaptive specialization. Trained as a multi-label classifier using Wikipedia category labels, it computes independent sigmoid outputs for 37 domains, allowing queries to belong to multiple domains simultaneously.

### Mechanism 2
The skip connection ensures out-of-domain queries retain the original dense retrieval representation, preventing degradation from irrelevant expert contributions. When the gating function assigns low weights to all experts, the system falls back to the base model's representation.

### Mechanism 3
The combination of supervised gating and specialized expert modules improves retrieval effectiveness through joint training using domain-specific losses. The gating function learns accurate domain classification while experts learn to contextualize queries for their respective domains.

## Foundational Learning

- **Dense retrieval and contrastive learning**: DESIRE-ME builds upon pre-trained dense retrieval models using contrastive learning to learn semantic representations in a shared vector space.
  - Why needed: Provides foundation for semantic matching in open-domain question answering
  - Quick check: What is the difference between lexical matching (e.g., BM25) and dense retrieval in terms of query-document matching?

- **Mixture-of-Experts (MoE) framework**: DESIRE-ME leverages MoE to combine multiple specialized neural models, each focusing on a specific topical domain.
  - Why needed: Enables adaptive specialization for diverse query topics
  - Quick check: How does the gating function in an MoE framework determine which expert(s) to use for a given input?

- **Supervised multi-label classification**: The gating function is trained as a supervised multi-label classifier using Wikipedia category labels.
  - Why needed: Allows queries to belong to multiple domains simultaneously
  - Quick check: What is the difference between multi-label classification and multi-class classification, and why is it relevant for the DESIRE-ME gating function?

## Architecture Onboarding

- **Component map**: Query encoder → MoE module (gating function + 37 expert modules + pooling) → Skip connection → DESIRE-ME query representation → Scoring function → Retrieved documents

- **Critical path**: Query → Query encoder → MoE module (gating function + expert modules + pooling) → Skip connection → DESIRE-ME query representation → Scoring function → Retrieved documents

- **Design tradeoffs**: Supervised gating requires labeled data but enables accurate domain classification; skip connection ensures robustness but may limit specialization benefits; 37 experts are dataset-specific and may not generalize.

- **Failure signatures**: Degradation in retrieval performance may indicate misclassifying domains or inadequate expert specialization; overfitting may be evident if model performs poorly on out-of-domain queries; random gating baseline performing similarly suggests ineffective domain classification.

- **First 3 experiments**:
  1. Evaluate DESIRE-ME on held-out validation set from same domain as training data to assess gating function and expert module impact
  2. Compare DESIRE-ME against base dense retrieval model and randomly gated baseline on diverse queries
  3. Test DESIRE-ME in zero-shot scenario on dataset with similar characteristics to training data

## Open Questions the Paper Calls Out

### Open Question 1
How does DESIRE-ME's performance scale with increasing numbers of MoE specializers beyond the 37 used in experiments? The paper uses 37 specializers but does not explore scaling effects or test different numbers of specializers.

### Open Question 2
What is the impact of using different pooling strategies beyond the weighted sum approach? The paper mentions various pooling methods exist but only uses weighted sum pooling without comparing alternatives.

### Open Question 3
How does DESIRE-ME perform when trained on non-Wikipedia corpora with different category structures? All experiments use Wikipedia-based datasets, leaving generalizability to other corpora unexplored.

### Open Question 4
What is the computational overhead of DESIRE-ME compared to baseline dense retrieval models during inference? The paper discusses architecture improvements but does not report latency or efficiency metrics.

## Limitations
- Reliance on Wikipedia-derived category labels may not capture semantic diversity of real-world queries or generalize to non-Wikipedia domains
- Computational overhead of maintaining 37 expert modules and gating mechanism may limit practical deployment in resource-constrained environments
- Assumption that queries can meaningfully belong to multiple domains simultaneously requires validation on datasets with different topical distributions

## Confidence

- **High confidence**: Retrieval performance improvements (12% NDCG@10, 22% P@1) over base dense models are well-supported by experimental results across three datasets; skip connection mechanism effectively prevents degradation on out-of-domain queries

- **Medium confidence**: Generalization to Climate-FEVER in zero-shot setting demonstrates robustness, but evaluation is limited to one out-of-domain dataset; multi-label gating approach's superiority over single-label alternatives needs further validation

- **Low confidence**: Scalability of 37-domain architecture to datasets with vastly different category distributions remains untested; long-term effectiveness as new domains emerge is unclear

## Next Checks

1. **Cross-dataset validation**: Evaluate DESIRE-ME on additional datasets with different topical distributions (e.g., scientific literature, news articles) to assess generalization beyond Wikipedia-based domains and test scalability of the 37-domain architecture

2. **Ablation study on gating function**: Compare multi-label gating mechanism against single-label alternatives and unsupervised routing methods to quantify specific contribution of supervised, multi-label approach to overall performance

3. **Resource efficiency analysis**: Measure computational overhead of DESIRE-ME (inference time, memory usage) compared to base dense retrieval model and assess whether performance gains justify additional resource requirements in practical applications