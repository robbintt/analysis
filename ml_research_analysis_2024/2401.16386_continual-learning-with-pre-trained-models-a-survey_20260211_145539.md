---
ver: rpa2
title: 'Continual Learning with Pre-Trained Models: A Survey'
arxiv_id: '2401.16386'
source_url: https://arxiv.org/abs/2401.16386
tags:
- learning
- prompt
- continual
- methods
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides the first comprehensive survey of pre-trained
  model-based continual learning (PTM-CIL), categorizing existing methods into three
  groups: prompt-based, representation-based, and model mixture-based approaches.
  The survey analyzes their similarities, differences, and advantages/disadvantages,
  and includes an empirical study comparing representative methods across seven benchmark
  datasets.'
---

# Continual Learning with Pre-Trained Models: A Survey

## Quick Facts
- arXiv ID: 2401.16386
- Source URL: https://arxiv.org/abs/2401.16386
- Authors: Da-Wei Zhou; Hai-Long Sun; Jingyi Ning; Han-Jia Ye; De-Chuan Zhan
- Reference count: 16
- Primary result: First comprehensive survey of pre-trained model-based continual learning (PTM-CIL) with empirical analysis

## Executive Summary
This survey provides the first comprehensive overview of pre-trained model-based continual learning (PTM-CIL), categorizing existing methods into prompt-based, representation-based, and model mixture-based approaches. The authors conduct an empirical study comparing representative methods across seven benchmark datasets, identifying key performance trends and methodological challenges. A critical finding is the fairness issue in comparisons due to batch-wise prompt selection in some methods, and the surprising effectiveness of a simple prototype-based baseline (SimpleCIL) against more complex approaches.

## Method Summary
The paper surveys existing PTM-CIL methods by categorizing them into three groups based on how they utilize pre-trained models: prompt-based methods that modify input representations, representation-based methods that learn task-specific feature spaces, and model mixture-based approaches that combine multiple pre-trained models. The authors then conduct empirical experiments comparing representative methods from each category across seven benchmark datasets, analyzing performance patterns and identifying methodological issues in current evaluation practices.

## Key Results
- Representation-based methods (ADAM, RanPAC) generally outperform prompt-based and model mixture approaches
- SimpleCIL, a simple prototype-based baseline, often outperforms more complex prompt-based methods
- A critical fairness issue exists in comparisons due to batch-wise prompt selection in some methods
- The survey identifies future research directions including large language model applications and resource-constrained scenarios

## Why This Works (Mechanism)
PTM-CIL methods leverage pre-trained models' rich representations and knowledge transfer capabilities to address catastrophic forgetting in sequential learning tasks. Prompt-based methods work by conditioning pre-trained models on task-specific prompts, representation-based methods learn to map inputs to task-specific feature spaces, and model mixture approaches combine multiple pre-trained models to handle diverse tasks. The effectiveness stems from utilizing pre-trained models as strong feature extractors while adapting them incrementally to new tasks.

## Foundational Learning

**Catastrophic Forgetting**: Neural networks overwrite previous knowledge when learning new tasks, requiring specialized methods to preserve old task performance.

*Why needed*: Fundamental challenge in sequential learning that makes continual learning necessary

*Quick check*: Train a model on task A, then task B, and measure performance degradation on task A

**Prompt Engineering**: Designing input patterns or embeddings that guide pre-trained models to perform specific tasks without modifying model weights.

*Why needed*: Enables task adaptation while preserving pre-trained model weights

*Quick check*: Compare zero-shot performance with and without task-specific prompts

**Feature Representation Learning**: Extracting and adapting intermediate representations from pre-trained models for new tasks.

*Why needed*: Pre-trained models provide rich feature spaces that can be fine-tuned for downstream tasks

*Quick check*: Visualize feature space alignment between tasks using t-SNE

## Architecture Onboarding

**Component Map**: Pre-trained model (backbone) -> Task Adapter/Prompt Generator -> Classification Head -> Task Buffer/Replay Memory

**Critical Path**: Input → Pre-trained Model → Task-specific Adaptation (prompt/representation) → Prediction → Buffer Update → Task Switch

**Design Tradeoffs**: Weight preservation vs. performance (frozen weights maintain stability but limit adaptation), memory overhead vs. performance (buffer size affects forgetting prevention), prompt complexity vs. generalization (complex prompts may overfit)

**Failure Signatures**: Catastrophic forgetting on old tasks, poor zero-shot generalization, unstable performance across task switches, high memory consumption

**First Experiments**:
1. Zero-shot evaluation of pre-trained model on new tasks to establish baseline performance
2. Sequential training with buffer replay to measure forgetting mitigation
3. Ablation study removing task-specific adaptations to quantify pre-trained model contribution

## Open Questions the Paper Calls Out
- How to effectively extend continual learning to large language models and multi-modal scenarios
- Developing resource-efficient methods for constrained environments
- Creating more challenging and realistic benchmarks for PTM-CIL
- Establishing standardized evaluation protocols that address fairness issues

## Limitations
- Potential selection bias in choosing representative methods and benchmark datasets
- Empirical findings may not generalize beyond tested classification tasks
- Lack of standardized solution for the identified batch-wise prompt selection fairness issue
- Future directions are speculative without empirical validation

## Confidence

**High confidence**: The categorization framework provides a logical organizational structure for understanding PTM-CIL approaches

**Medium confidence**: Empirical performance rankings are specific to tested benchmarks and may vary with different datasets

**Medium confidence**: Identified fairness issues are valid but require standardized protocols for resolution

**Low confidence**: Future directions and their potential impact are speculative without empirical validation

## Next Checks

1. Conduct systematic replication study using the same seven benchmark datasets to verify reported empirical performance rankings

2. Design standardized evaluation protocol addressing batch-wise prompt selection fairness issue identified in the survey

3. Test SimpleCIL baseline against top-performing methods on broader range of task types beyond standard classification benchmarks