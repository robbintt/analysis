---
ver: rpa2
title: Probabilistic Neural Circuits
arxiv_id: '2403.06235'
source_url: https://arxiv.org/abs/2403.06235
tags:
- circuits
- probabilistic
- circuit
- neural
- pncs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces probabilistic neural circuits (PNCs), a novel
  class of tractable probabilistic models that bridge the gap between probabilistic
  circuits (PCs) and neural networks. PNCs generalize PCs by introducing data-dependent
  weights in sum units, allowing them to approximate deep mixtures of Bayesian networks.
---

# Probabilistic Neural Circuits

## Quick Facts
- arXiv ID: 2403.06235
- Source URL: https://arxiv.org/abs/2403.06235
- Reference count: 4
- PNCs achieve 0.87 bits-per-dimension on MNIST, outperforming SPQNs (1.20) and PCs (1.32)

## Executive Summary
This paper introduces probabilistic neural circuits (PNCs), a novel class of tractable probabilistic models that bridge the gap between probabilistic circuits (PCs) and neural networks. PNCs generalize PCs by introducing data-dependent weights in sum units, allowing them to approximate deep mixtures of Bayesian networks while maintaining tractable marginalization for certain query types. The authors demonstrate that PNCs achieve state-of-the-art performance on density estimation tasks across MNIST, FashionMNIST, and EMNIST datasets, while also showing promise for discriminative learning despite challenges with regularization.

## Method Summary
PNCs extend probabilistic circuits by replacing fixed sum unit weights with neural networks that output data-dependent weights. The architecture follows a layered structure with alternating product and neural sum units, where neural sum units are implemented using convolutional neural networks with half-kernels to enforce partial variable orderings. Training uses standard backpropagation with Adam optimizer, and the models are evaluated on density estimation (bits-per-dimension) and classification (test accuracy) tasks using standard image datasets.

## Key Results
- PNCs achieve 0.87 bits-per-dimension on MNIST test set
- Outperform SPQNs (1.20 bpd) and PCs (1.32 bpd) on density estimation
- Show 98.04% accuracy on MNIST classification but underperform compared to simpler models
- Demonstrate tractable marginalization for certain query types while relaxing decomposability constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PNCs achieve improved expressiveness by relaxing decomposability while maintaining tractable marginalization for certain queries
- Mechanism: Data-dependent weights in sum units allow dynamic component mixing that approximates conditional probabilistic circuits
- Core assumption: Neural networks can accurately approximate conditional distributions for tractability
- Evidence anchors: Abstract states PNCs generalize PCs with data-dependent weights; Section 3.3 introduces neural sum units with neural network mappings
- Break condition: Poor neural network generalization destroys tractability guarantees

### Mechanism 2
- Claim: PNCs outperform SPQNs by allowing dynamic component mixing within partitions
- Mechanism: Unlike SPQNs' fixed component assignments, PNCs use learnable neural weights for flexible mixing
- Core assumption: Dynamic mixing provides meaningful representational advantages
- Evidence anchors: Section 5 highlights SPQNs' fixed component assignment limitation; Section 6.1 shows better experimental performance
- Break condition: If dynamic mixing provides no performance benefit or causes overfitting

### Mechanism 3
- Claim: Layered architecture enables efficient parallel computation while maintaining partial ordering constraints
- Mechanism: Half-kernel convolutions enforce variable ordering while enabling parallel processing within layers
- Core assumption: Half-kernel design correctly enforces partial ordering constraints
- Evidence anchors: Section 4.2 describes half-kernel convolution implementation; Section 4.1 defines neural sum units respecting partial orderings
- Break condition: If convolution implementation fails to enforce partial ordering, tractability breaks down

## Foundational Learning

- Concept: Probabilistic circuits and tractability properties (smoothness and decomposability)
  - Why needed here: Essential to understand what PNCs relax and why this improves expressiveness
  - Quick check question: What are the two key structural properties that guarantee tractable marginalization in probabilistic circuits?

- Concept: Bayesian networks and factorization properties
  - Why needed here: PNCs are interpreted as deep mixtures of Bayesian networks
  - Quick check question: How does a Bayesian network factorize a joint probability distribution according to the paper's notation?

- Concept: Convolutional neural networks and parameter sharing
  - Why needed here: PNCs use convolutional layers to implement neural sum units efficiently
  - Quick check question: Why does the paper use half-kernels in convolutional implementations of neural sum layers?

## Architecture Onboarding

- Component map: Leaf units (categorical distributions) → alternating product/neural sum layers → root layer
- Critical path: Forward pass from leaf layer through alternating product/neural sum layers to root layer; marginalization requires integration from highest-ordered variables downward
- Design tradeoffs: Relaxing decomposability improves expressiveness but sacrifices full tractability; neural sum units add flexibility but require careful regularization; half-kernel convolutions enforce ordering but may limit capacity
- Failure signatures: Perfect training accuracy but poor test performance; marginalization queries violating partial orderings; poor density estimation despite increased model capacity
- First 3 experiments:
  1. Implement and train basic PNC on MNIST with 28×28 grid structure, compare bits-per-dimension to standard PC
  2. Replace neural sum units with standard sum units to verify neural component's contribution
  3. Test tractability by implementing ordered marginalization queries on small PNC and verifying against brute-force computation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can effective regularization techniques be developed for PNCs to improve discriminative learning performance?
- Basis in paper: The paper states PNCs achieve perfect train accuracy but lack proper regularization, preventing matching accuracies of logistic circuits and RAT-SPNs
- Why unresolved: Various regularization techniques (weight decay, stochastic delta rule) failed to provide consistent improvements
- What evidence would resolve it: Developing new regularization techniques that significantly improve PNC classification accuracy to match or exceed simpler models

### Open Question 2
- Question: How can PNCs be effectively used for sampling tasks, and can they be linked to autoregressive models?
- Basis in paper: The paper suggests exploring PNCs for sampling tasks and linking them to autoregressive models like PixelCNN
- Why unresolved: No experimental results or theoretical framework provided for sampling with PNCs
- What evidence would resolve it: Developing sampling methods from PNCs and establishing clear connections to autoregressive models

### Open Question 3
- Question: What are the most effective structure learning algorithms for PNCs, and how do they perform on tabular data?
- Basis in paper: The paper suggests exploring structure learning algorithms for PNCs and applying them to tabular data
- Why unresolved: No structure learning algorithms or tabular data experiments provided
- What evidence would resolve it: Developing and testing structure learning algorithms for PNCs on tabular datasets

## Limitations

- Neural sum units' ability to accurately approximate conditional distributions remains largely unvalidated empirically
- Classification performance lags behind simpler models due to lack of effective regularization techniques
- Comparison with SPQNs may be influenced by implementation details rather than fundamental architectural advantages
- No experimental validation of sampling capabilities or connection to autoregressive models

## Confidence

- **High Confidence**: Theoretical framework establishing PNCs as neural approximations of conditional probabilistic circuits; density estimation results on standard benchmarks
- **Medium Confidence**: Architectural design choices (half-kernel convolutions, layered structure) and their relationship to tractability guarantees
- **Low Confidence**: Practical advantages over SPQNs in function approximation, as this relies on unvalidated assumptions about component mixing flexibility

## Next Checks

1. Implement ablation studies replacing neural sum units with learned non-neural weight assignments to quantify neural approximation's contribution to performance gains

2. Systematically test marginalization queries on PNCs with known ground truth to verify tractability guarantees, focusing on edge cases where neural components might violate partial ordering constraints

3. Conduct controlled experiments varying regularization strength (weight decay, dropout, stochastic delta rule) on classification tasks to identify optimal regularization strategies and understand why simpler models outperform PNCs