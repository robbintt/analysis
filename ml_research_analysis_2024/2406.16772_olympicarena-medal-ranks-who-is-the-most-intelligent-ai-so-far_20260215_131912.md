---
ver: rpa2
title: 'OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?'
arxiv_id: '2406.16772'
source_url: https://arxiv.org/abs/2406.16772
tags: []
core_contribution: "This report compares the latest AI models\u2014Claude-3.5-Sonnet,\
  \ Gemini-1.5-Pro, and GPT-4o\u2014on the OlympicArena benchmark, a multi-discipline,\
  \ multi-modal test for superintelligent AI. The authors introduce an Olympic medal\
  \ ranking system to evaluate overall performance, with Claude-3.5-Sonnet achieving\
  \ a score of 39.24 and GPT-4o at 40.47, both significantly outperforming Gemini-1.5-Pro\
  \ (35.09) and GPT-4V (33.17)."
---

# OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?

## Quick Facts
- arXiv ID: 2406.16772
- Source URL: https://arxiv.org/abs/2406.16772
- Authors: Zhen Huang; Zengzhi Wang; Shijie Xia; Pengfei Liu
- Reference count: 17
- GPT-4o achieves highest overall score (40.47) on OlympicArena benchmark, outperforming Claude-3.5-Sonnet (39.24) and Gemini-1.5-Pro (35.09)

## Executive Summary
This paper introduces OlympicArena, a comprehensive benchmark for evaluating superintelligent AI across 11,163 bilingual problems spanning seven disciplines and 62 international Olympic competitions. The authors compare state-of-the-art models including GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro using a novel Olympic medal ranking system. Claude-3.5-Sonnet shows near-parity with GPT-4o overall, though GPT-4o excels in mathematics and computer science while Claude-3.5-Sonnet outperforms in physics, chemistry, and biology. Open-source models significantly lag behind proprietary models, and multimodal performance remains weaker than text-only tasks.

## Method Summary
The study evaluates AI models on OlympicArena, a multi-discipline, multi-modal benchmark featuring 11,163 bilingual problems across seven subjects. Models are tested using zero-shot Chain-of-Thought prompting with accuracy metrics for non-programming tasks and unbiased pass@k for programming tasks. Performance is aggregated by discipline and ranked using an Olympic medal system (gold, silver, bronze) based on the top three scores. The evaluation includes both text-only and interleaved text-image modalities, with text-only performance consistently outperforming multimodal.

## Key Results
- GPT-4o achieves the highest overall score (40.47), slightly outperforming Claude-3.5-Sonnet (39.24) and significantly outperforming Gemini-1.5-Pro (35.09)
- Claude-3.5-Sonnet surpasses GPT-4o in physics, chemistry, and biology, with a 3% advantage in biology
- GPT-4o demonstrates superior performance in mathematics and computer science, corresponding to complex deductive reasoning skills
- Open-source models fail to secure any medals, highlighting the performance gap with proprietary models
- Multimodal performance is consistently weaker than text-only performance across all evaluated models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Claude-3.5-Sonnet achieves near-parity with GPT-4o overall because both have similar model scales and are trained on extensive general-purpose corpora
- Mechanism: Model scale and diverse pretraining data enable competitive reasoning performance across multiple disciplines
- Core assumption: Pretraining data diversity and model capacity are primary drivers of multi-discipline performance
- Evidence anchors:
  - [abstract] "Claude-3.5-Sonnet shows highly competitive overall performance over GPT-4o"
  - [section] "the newly released Claude-3.5-Sonnet is very powerful, reaching a level almost on par with GPT-4o. The difference in overall accuracy between the two is only about 1%"

### Mechanism 2
- Claim: GPT-4o excels in math and CS due to specialized fine-tuning for deductive reasoning and coding
- Mechanism: Targeted fine-tuning on deductive reasoning tasks and programming problems leads to superior performance in these areas
- Core assumption: Fine-tuning for specific reasoning types transfers effectively to Olympiad-level problems
- Evidence anchors:
  - [section] "GPT-4o demonstrates superior capabilities in traditional deductive and inductive reasoning tasks, particularly in mathematics and computer science"
  - [section] "GPT-4o, compared to its competitors, excels in subjects like mathematics and computer science, which correspond to complex deductive reasoning skills"

### Mechanism 3
- Claim: Claude-3.5-Sonnet outperforms GPT-4o in physics, chemistry, and biology due to better integration of domain knowledge with reasoning
- Mechanism: Training that emphasizes knowledge integration allows better performance on interdisciplinary science problems
- Core assumption: Integrating knowledge and reasoning is more important than pure reasoning for these subjects
- Evidence anchors:
  - [section] "Claude-3.5-Sonnet excels in subjects such as physics, chemistry, and biology, especially in biology where it surpasses GPT-4o by 3%"
  - [section] "subjects like chemistry and biology often require a substantial knowledge base to make inferences based on known information about causality and phenomena"

## Foundational Learning

- Concept: Multi-modal reasoning
  - Why needed here: The benchmark includes both text-only and text-image interleaved problems, requiring models to handle visual information
  - Quick check question: What are the two main modalities tested in OlympicArena?
- Concept: Deductive vs. inductive reasoning
  - Why needed here: Different models excel in different reasoning types, with GPT-4o stronger in deductive reasoning and Claude-3.5-Sonnet in knowledge-integrated reasoning
  - Quick check question: Which reasoning type does GPT-4o excel at according to the paper?
- Concept: Zero-shot CoT prompting
  - Why needed here: The benchmark uses zero-shot CoT prompting, which is critical for understanding the evaluation setup
  - Quick check question: What prompting method is used for the benchmark evaluation?

## Architecture Onboarding

- Component map: Model selection (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro) -> OlympicArena benchmark (11,163 problems) -> Zero-shot CoT prompt -> Rule-based matching evaluation -> Medal ranking aggregation
- Critical path: 1. Load model and benchmark 2. Apply zero-shot CoT prompt 3. Evaluate using appropriate metric 4. Aggregate scores by discipline 5. Apply medal ranking
- Design tradeoffs:
  - Text-only vs. multi-modal: Text-only performs better, suggesting multi-modal reasoning is harder
  - English vs. Chinese: Models perform better in English, indicating language-specific optimization
  - Open-source vs. proprietary: Proprietary models significantly outperform open-source
- Failure signatures:
  - Low performance in multi-modal tasks: Indicates poor visual reasoning integration
  - Large gap between English and Chinese: Indicates lack of multilingual optimization
  - Poor performance in knowledge-intensive subjects: Indicates insufficient domain knowledge integration
- First 3 experiments:
  1. Run text-only evaluation of GPT-4o to establish baseline performance
  2. Compare Claude-3.5-Sonnet performance in text-only vs. multi-modal to quantify visual reasoning impact
  3. Test a Chinese-language subset to measure multilingual capability gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications could improve open-source models' performance to reach parity with proprietary models on the OlympicArena benchmark?
- Basis in paper: [explicit] The paper states "The performance of AI models from the open-source community significantly lags behind these proprietary models" and "open-source models have not managed to secure a medal in any discipline"
- Why unresolved: The paper identifies the performance gap but doesn't analyze the specific technical reasons or propose solutions for bridging this gap
- What evidence would resolve it: Comparative analysis of architectural differences, training datasets, and optimization techniques between top open-source and proprietary models that explains the performance differential

### Open Question 2
- Question: How would multimodal performance change if image captions were effectively incorporated rather than using text-only inputs?
- Basis in paper: [explicit] "We do not use image captions as textual representations of images because, in research with OlympicArena, it turns out that image captions are not always effective. The role of image captions may require further exploration"
- Why unresolved: The paper explicitly notes that image captions were not used due to unclear effectiveness, leaving open whether they could improve performance
- What evidence would resolve it: Controlled experiments comparing model performance with and without image captions on the same benchmark tasks

### Open Question 3
- Question: What training modifications would be needed to achieve balanced performance across both English and Chinese languages on the OlympicArena benchmark?
- Basis in paper: [explicit] The paper notes that "most models have higher accuracy in English compared to Chinese" and speculates about reasons including training data imbalance
- Why unresolved: While the paper identifies the performance disparity, it doesn't propose specific training approaches to address the imbalance
- What evidence would resolve it: Training experiments with modified data distributions and multilingual optimization techniques showing performance convergence across languages

## Limitations

- Limited transparency in evaluation methodology: Exact prompt templates and evaluation criteria are not specified, making precise reproduction challenging
- Potential domain-specific overfitting: The specialized Olympiad-level problems may emphasize pattern recognition rather than general intelligence
- Multimodal performance gap concerns: The paper doesn't adequately explain whether the multimodal performance drop reflects fundamental limitations or benchmark design issues

## Confidence

- High confidence (95%+): The relative ranking of models and their performance differences across specific subjects are consistent with reported data
- Medium confidence (70-85%): Attribution of performance differences to specific mechanisms (model scale, fine-tuning focus) is plausible but not definitively proven
- Low confidence (40-60%): Claims about "superintelligent AI" are overstated relative to what the benchmark actually measures

## Next Checks

1. Independent evaluation reproduction: Run the same zero-shot CoT prompts on all three models using a held-out subset of OlympicArena problems to verify reported performance gaps and relative rankings

2. Cross-lingual consistency test: Evaluate model performance on Chinese-language problems versus English-language problems to quantify reported language-specific optimization effects

3. Domain knowledge isolation: Design controlled experiments to test whether performance differences reflect genuine knowledge integration capabilities versus different problem-solving strategies, using problems that isolate reasoning from domain knowledge requirements