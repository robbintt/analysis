---
ver: rpa2
title: 'CoroNetGAN: Controlled Pruning of GANs via Hypernetworks'
arxiv_id: '2403.08261'
source_url: https://arxiv.org/abs/2403.08261
tags:
- latent
- compression
- vector
- network
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CoroNetGAN, a novel method for compressing
  GANs using differentiable pruning via hypernetworks. The proposed approach allows
  controllable compression during training, reducing both training and inference time.
---

# CoroNetGAN: Controlled Pruning of GANs via Hypernetworks

## Quick Facts
- arXiv ID: 2403.08261
- Source URL: https://arxiv.org/abs/2403.08261
- Reference count: 40
- Best FID scores: 32.3 (Zebra-to-Horse) and 72.3 (Summer-to-Winter)

## Executive Summary
CoroNetGAN introduces a novel approach for compressing GANs using differentiable pruning via hypernetworks. The method allows controllable compression during training, reducing both training and inference time while maintaining high-fidelity image generation. The proposed approach outperforms state-of-the-art methods on conditional GAN compression tasks across multiple benchmark datasets, with significant improvements in training time and better inference time performance on various smartphone chipsets.

## Method Summary
CoroNetGAN employs a hypernetwork architecture that generates weights for the generator network based on latent vectors. During training, proximal gradient sparsity is applied to these latent vectors, effectively pruning redundant parameters. The method consists of two stages: a searching stage where compression is performed through latent vector sparsification, and a converging stage where the compressed model is fine-tuned. The hypernetwork takes current and previous latent vectors as input and produces layer weights, ensuring a direct correlation between latent vector sparsity and weight matrix sparsity.

## Key Results
- Best FID scores: 32.3 on Zebra-to-Horse and 72.3 on Summer-to-Winter datasets
- Significant training time reduction compared to state-of-the-art OMGD method
- Improved inference time performance on Qualcomm Snapdragon SM8450 and Dimensity 1200-Max chipsets
- Maintains high image quality across Pix2Pix, CycleGAN, and DCGAN architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hypernetwork generates weights covariant with latent vector, enabling direct pruning
- Core assumption: One-to-one correspondence between latent vector sparsity and weight matrix sparsity
- Evidence: Weak (abstract and equations 1-4 show mathematical relationship, but corpus lacks direct support)
- Break condition: If hypernetwork design doesn't ensure covariance, latent vector pruning won't translate to weight pruning

### Mechanism 2
- Claim: Proximal gradient sparsity effectively identifies and prunes redundant parameters
- Core assumption: Proximal gradient is effective at identifying redundant parameters and driving them to zero
- Evidence: Weak (abstract and section 3.2 describe the process, but corpus lacks specific support)
- Break condition: If proximal gradient is ineffective at identifying redundant parameters or threshold is incorrect

### Mechanism 3
- Claim: Latent vector compression reduces both training and inference time
- Core assumption: Reducing generator parameters directly translates to reduced training and inference time
- Evidence: Weak (abstract and section 4.2.1 support claims, but corpus lacks direct comparison)
- Break condition: If hypernetwork overhead outweighs benefits of compression, time reduction may not occur

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs) and their training process
  - Why needed: Understanding GANs is fundamental to grasping the problem of GAN compression
  - Quick check: What are the two main components of a GAN and what is their role in the training process?

- Concept: Knowledge distillation and its application in model compression
  - Why needed: Existing technique that CoroNetGAN aims to improve upon
  - Quick check: How does knowledge distillation work, and what are its limitations when applied to GAN compression?

- Concept: Hypernetworks and their use in dynamic weight generation
  - Why needed: CoroNetGAN uses hypernetworks to generate generator weights based on latent vectors
  - Quick check: What is a hypernetwork, and how does it differ from a traditional neural network?

## Architecture Onboarding

- Component map: Generator network (with latent vectors) -> Hypernetwork (generates weights) -> Discriminator network
- Critical path:
  1. Initialize generator, discriminator, and hypernetwork with random weights
  2. For each training iteration:
     a. Sample batch of images
     b. Generate generator weights using hypernetwork and latent vectors
     c. Forward pass through generator and discriminator
     d. Compute GAN loss and backpropagate to update hypernetwork
     e. Apply proximal gradient updates to latent vectors
     f. Move to converging stage when target compression ratio reached
  3. Discard hypernetwork and fine-tune compressed generator-discriminator

- Design tradeoffs:
  - Compression ratio vs. image quality: Higher compression may lead to lower image quality
  - Training time vs. compression efficiency: More iterations may be needed for higher compression
  - Model complexity: Hypernetwork adds parameters that may offset compression benefits

- Failure signatures:
  - Poor image quality: Indicates too many important parameters were pruned
  - Training instability: Suggests compression ratio is too high or hypernetwork generates inappropriate weights
  - No time reduction: Indicates hypernetwork overhead outweighs compression benefits

- First 3 experiments:
  1. Implement hypernetwork architecture and verify weight matrix generation for simple generator
  2. Apply proximal gradient sparsity to latent vectors and confirm weight pruning correlation
  3. Train compressed GAN on CIFAR-10 and evaluate compression ratio vs. image quality tradeoff

## Open Questions the Paper Calls Out

- How does CoroNetGAN's performance scale when applied to larger GAN architectures like StyleGAN or BigGAN?
- What is the optimal embedding space dimension for the hypernetwork across different GAN architectures?
- How does CoroNetGAN's compression performance compare when applied to other generative models like VAEs or normalizing flows?
- What is the impact of different proximal gradient parameters on compression performance?

## Limitations
- Critical hyperparameters (proximal gradient factor, mask threshold) are unspecified
- Limited comparison with only OMGD as baseline for training time reduction
- Inference time tested only on specific smartphone chipsets
- No exploration of different embedding space dimensions

## Confidence
- Image quality improvements (FID scores): High
- Training time reduction: Medium
- Inference time on smartphones: Medium
- Compression ratio controllability: High

## Next Checks
1. Implement proximal gradient sparsification with varying regularization factors (0.1, 0.5, 1.0) to determine optimal value
2. Test compressed models on additional smartphone chipsets to verify generalizability of inference time improvements
3. Compare CoroNetGAN against broader set of GAN compression baselines on same datasets