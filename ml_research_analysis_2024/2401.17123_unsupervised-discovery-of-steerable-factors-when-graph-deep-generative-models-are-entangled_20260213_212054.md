---
ver: rpa2
title: Unsupervised Discovery of Steerable Factors When Graph Deep Generative Models
  Are Entangled
arxiv_id: '2401.17123'
source_url: https://arxiv.org/abs/2401.17123
tags:
- graph
- equation
- data
- steerable
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses unsupervised discovery of steerable factors
  in the latent space of pretrained graph deep generative models, motivated by the
  observation that these spaces are entangled. The proposed method, GraphCG, learns
  steerable factors by maximizing mutual information between semantic-rich directions,
  formulated as an energy-based model with noise-contrastive estimation.
---

# Unsupervised Discovery of Steerable Factors When Graph Deep Generative Models Are Entangled

## Quick Facts
- arXiv ID: 2401.17123
- Source URL: https://arxiv.org/abs/2401.17123
- Reference count: 40
- Key outcome: GraphCG discovers steerable factors in pretrained graph DGM latent spaces with up to 73% sequence monotonic ratio on molecular graphs

## Executive Summary
This paper addresses the challenge of unsupervised discovery of steerable factors in the latent space of pretrained graph deep generative models. The authors observe that while these models can generate high-quality graphs, their latent spaces are entangled, making it difficult to discover meaningful editing directions. To address this, they propose GraphCG, a framework that learns steerable factors by maximizing mutual information between edited latent codes moving along the same semantic direction. GraphCG is model-agnostic and can be applied to various graph modalities, including molecular graphs and point clouds.

## Method Summary
GraphCG learns steerable factors in the latent space of pretrained graph deep generative models through an energy-based modeling approach with noise-contrastive estimation (NCE). The framework assumes that latent codes edited with the same direction and step size share semantic information. By maximizing mutual information between such pairs, GraphCG discovers directions that consistently change specific graph structures. The method contrasts positive pairs (edited with same direction) against negative pairs (edited with different directions) to learn meaningful semantic directions without supervision. GraphCG is applied to molecular graphs (ZINC250K, ChEMBL) and point clouds (Airplane, Car, Chair) from ShapeNet, showing improved sequence monotonic ratios and discovering seven interpretable steerable factors.

## Key Results
- GraphCG achieves sequence monotonic ratios up to 73% on calibrated Tanimoto similarity for the top-1 direction in molecular graph editing
- Outperforms four competitive baselines on two molecular graph datasets (ZINC250K, ChEMBL)
- Successfully discovers seven steerable factors including functional group changes in molecules and shape modifications in point clouds
- Demonstrates model-agnostic capability across five pretrained graph DGMs and five graph datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GraphCG learns steerable factors by maximizing mutual information between edited latent codes moving along the same semantic direction
- Mechanism: The framework assumes that latent codes edited with the same direction and step size share semantic information. By maximizing mutual information between such pairs, GraphCG discovers directions that consistently change specific graph structures
- Core assumption: Entangled DGMs still contain separable semantic information that can be discovered through statistical dependencies between edited samples
- Break condition: If the pretrained DGM's latent space lacks any structure that correlates with semantic factors, mutual information maximization cannot discover meaningful directions

### Mechanism 2
- Claim: GraphCG-NCE formulation enables unsupervised learning by contrasting positive and negative pairs in the latent space
- Mechanism: Positive pairs are latent codes edited with the same direction and step size; negative pairs use different directions or step sizes. NCE trains the model to align positives and contrast negatives, effectively discovering semantic directions without supervision
- Core assumption: The statistical differences between same-direction edits and different-direction edits are sufficient to learn semantic structure
- Break condition: If the editing function introduces too much noise or the positive/negative distinction becomes ambiguous, NCE cannot learn meaningful directions

### Mechanism 3
- Claim: Energy-based modeling provides flexible framework for density estimation in latent space editing task
- Mechanism: GraphCG uses EBM to model conditional distributions p(¯zu_i,α|¯zv_i,α), enabling learning of editing functions that produce semantically coherent outputs when applied to different inputs
- Core assumption: The energy function can capture the complex relationships between edited latent codes and their semantic properties
- Break condition: If the energy function cannot adequately represent the conditional distributions, the model cannot learn useful editing directions

## Foundational Learning

- Mutual information maximization
  - Why needed here: Forms the theoretical foundation for discovering semantic directions without supervision
  - Quick check question: Why does maximizing mutual information between edited latent codes help discover steerable factors?

- Energy-based modeling and noise-contrastive estimation
  - Why needed here: Provides tractable way to estimate conditional distributions needed for mutual information maximization
  - Quick check question: How does NCE transform the intractable density estimation problem into a binary classification task?

- Graph disentanglement metrics
  - Why needed here: Establishes baseline that pretrained DGMs are entangled, motivating need for GraphCG
  - Quick check question: What do low scores on BetaVAE, FactorVAE, and MIG metrics tell us about graph DGM latent spaces?

## Architecture Onboarding

- Component map:
  Pretrained DGM (encoder/decoder) -> GraphCG learning module (EBM, NCE, direction modeling, editing function) -> Inference pipeline (edited graph generation)

- Critical path:
  1. Sample two latent codes (zu, zv)
  2. Apply editing function with direction i and step size α to both
  3. Compute EBM energy between edited pairs
  4. Contrast positive pairs (same direction) with negative pairs (different directions)
  5. Update direction vectors and editing function parameters

- Design tradeoffs:
  - Linear vs nonlinear editing functions: Linear is simpler but may lack expressiveness; nonlinear adds capacity but increases complexity
  - Latent code pair strategy: Perturbation provides local structure; random sampling provides diversity
  - Mutual information estimation method: EBM-NCE vs InfoNCE - EBM provides more flexibility but may be less stable

- Failure signatures:
  - All learned directions produce similar edits (lack of diversity)
  - No monotonic structure change in output sequences
  - Direction vectors collapse to zero or become unstable during training
  - Editing function fails to produce meaningful changes for any direction

- First 3 experiments:
  1. Verify disentanglement metrics on pretrained DGMs show low scores (BetaVAE, FactorVAE, MIG)
  2. Test GraphCG-P vs GraphCG-R on simple dataset (e.g., small molecular dataset) and compare direction diversity
  3. Evaluate monotonic structure change on toy dataset where ground truth steerable factors are known

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GraphCG performance vary across different backbone DGMs for graph data?
- Basis in paper: [inferred] The paper mentions that GraphCG is model-agnostic and could be applied to various DGMs, but only evaluates on two specific models (MoFlow and HierVAE) for molecular graphs.
- Why unresolved: The paper doesn't provide comprehensive analysis of GraphCG performance across a wide range of DGMs, making it unclear if the method generalizes well to different model architectures.
- What evidence would resolve it: Experiments comparing GraphCG performance on multiple different backbone DGMs (e.g., VAEs, GANs, flow-based models) for the same graph datasets.

### Open Question 2
- Question: What is the impact of the editing function design (linear vs non-linear) on GraphCG's effectiveness for different graph modalities?
- Basis in paper: [explicit] The paper mentions considering both linear and non-linear editing functions but doesn't provide a comprehensive comparison of their effectiveness across different graph types.
- Why unresolved: The paper only provides qualitative results for one editing function type on molecular graphs and another on point clouds, without direct comparison.
- What evidence would resolve it: Systematic ablation studies comparing linear and non-linear editing functions across multiple graph modalities (molecules, point clouds, etc.) with quantitative metrics.

### Open Question 3
- Question: How does GraphCG handle graph structures with varying node degrees and edge types?
- Basis in paper: [inferred] The paper focuses on molecular graphs and point clouds but doesn't discuss how GraphCG handles more complex graph structures with heterogeneous nodes and edges.
- Why unresolved: The current evaluation is limited to relatively simple graph types, and it's unclear if the method can scale to more complex real-world graph data.
- What evidence would resolve it: Experiments applying GraphCG to heterogeneous graphs (e.g., knowledge graphs, social networks) with varying node and edge types, and analyzing performance metrics.

### Open Question 4
- Question: What is the relationship between the learned steerable factors and downstream task performance?
- Basis in paper: [inferred] The paper focuses on discovering steerable factors but doesn't investigate how these factors impact performance on specific downstream tasks like molecular property prediction or 3D object classification.
- Why unresolved: The paper demonstrates the ability to discover interpretable factors but doesn't establish their utility for practical applications.
- What evidence would resolve it: Experiments showing how the learned steerable factors from GraphCG can be used to improve performance on downstream tasks compared to baseline methods.

## Limitations
- Limited evaluation to molecular graphs and point clouds, with unclear generalizability to more complex graph structures
- Potential sensitivity to the choice of pretrained DGM backbone and its specific architecture
- Computational complexity may increase with larger datasets and more complex graph structures

## Confidence
- High Confidence: The observation that pretrained graph DGMs are entangled (supported by BetaVAE, FactorVAE, and MIG metrics)
- Medium Confidence: GraphCG's effectiveness on molecular datasets (SMR metrics show improvement over baselines)
- Low Confidence: Generalization to diverse graph modalities beyond molecular graphs and point clouds

## Next Checks
1. Test GraphCG on a third graph modality (e.g., social networks or citation graphs) to assess generalizability
2. Conduct ablation studies on the editing function (linear vs nonlinear) to quantify impact on diversity and quality
3. Evaluate computational efficiency and training stability across different dataset sizes and DGM architectures