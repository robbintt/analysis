---
ver: rpa2
title: 'tLaSDI: Thermodynamics-informed latent space dynamics identification'
arxiv_id: '2403.05848'
source_url: https://arxiv.org/abs/2403.05848
tags: []
core_contribution: This paper introduces tLaSDI, a non-intrusive reduced-order modeling
  method that embeds thermodynamic principles into latent space dynamics identification.
  The method uses an autoencoder for nonlinear dimension reduction and GFINNs (GENERIC
  formalism-informed neural networks) to construct latent dynamics that preserve the
  first and second laws of thermodynamics exactly by construction.
---

# tLaSDI: Thermodynamics-informed latent space dynamics identification

## Quick Facts
- arXiv ID: 2403.05848
- Source URL: https://arxiv.org/abs/2403.05848
- Authors: Jun Sur Richard Park; Siu Wun Cheung; Youngsoo Choi; Yeonjong Shin
- Reference count: 40
- Key outcome: tLaSDI introduces a non-intrusive reduced-order modeling method that embeds thermodynamic principles into latent space dynamics identification, demonstrating superior extrapolation ability on parametric PDEs.

## Executive Summary
This paper presents tLaSDI, a method for constructing reduced-order models that preserve thermodynamic laws in the latent space. The approach combines an autoencoder for nonlinear dimension reduction with GENERIC formalism-informed neural networks (GFINNs) to ensure energy conservation and entropy production are exactly maintained. The method is trained simultaneously for both the autoencoder and latent dynamics, using a novel loss function derived from an abstract error estimate that captures all sources of approximation error.

## Method Summary
tLaSDI uses a hyper-autoencoder architecture where encoder and decoder are parameterized by hypernetworks. The latent dynamics are constructed using GFINNs, which build the reversible (Poisson) and irreversible (friction) parts of the GENERIC formalism through neural networks that satisfy degeneracy conditions exactly. The training simultaneously optimizes all parameters using a four-term loss: integration loss for time evolution, reconstruction loss for state recovery, Jacobian loss for derivative accuracy, and model loss for dynamical consistency.

## Key Results
- Demonstrates superior extrapolation ability on parametric PDEs (Burgers' equation and heat equation) compared to gLaSDI
- Shows smaller prediction errors on unseen parameters through exact preservation of thermodynamic laws
- Empirically observes correlation between entropy production rate in latent space and full-state solution behavior
- Provides robust performance even with limited training data through thermodynamic structure preservation

## Why This Works (Mechanism)

### Mechanism 1
Embedding GENERIC formalism exactly into latent dynamics preserves thermodynamic laws by construction, ensuring energy conservation and entropy production. The GENERIC formalism splits dynamics into reversible (Poisson) and irreversible (friction) parts. GFINNs construct these matrices to satisfy degeneracy conditions exactly, meaning the first and second laws hold without penalty terms. The core assumption is that the constructed NN models for L and M matrices are expressive enough to capture the true underlying dynamics while maintaining exact degeneracy.

### Mechanism 2
Simultaneous training of autoencoder and latent dynamics yields better generalization than separate training by allowing mutual adaptation. Training both encoder/decoder and GENERIC-informed dynamics together means the latent space is shaped to be compatible with the thermodynamic structure, rather than fitting an arbitrary fixed space. The core assumption is that the latent variables discovered by the autoencoder will be more meaningful if they are constrained to work with the imposed GENERIC dynamics.

### Mechanism 3
The new loss function derived from the abstract error estimate improves extrapolation by directly penalizing components of approximation error. The loss includes integration, reconstruction, Jacobian, and model terms. The Jacobian and model terms capture derivative information and modeling error, which are shown in the error estimate to directly bound the total ROM error. The core assumption is that accurate derivative data is available or can be estimated, and the error estimate components are indeed the dominant sources of error.

## Foundational Learning

- **GENERIC formalism and degeneracy conditions**: Why needed here - It's the mathematical structure ensuring thermodynamic laws in the latent space. Quick check - What are the degeneracy conditions L(z)∇S(z) = 0 and M(z)∇E(z) = 0 enforcing physically?

- **Neural network Jacobian computation via JVP**: Why needed here - The Jacobian loss and model loss require Jacobians of encoder/decoder, which are computed efficiently using automatic differentiation. Quick check - How does PyTorch's jvp function compute the Jacobian-vector product without forming the full Jacobian matrix?

- **Reduced-order modeling (intrusive vs non-intrusive)**: Why needed here - tLaSDI is non-intrusive but aims to embed physics, so understanding the distinction and trade-offs is crucial. Quick check - What is the key limitation of non-intrusive ROMs that tLaSDI tries to overcome?

## Architecture Onboarding

- **Component map**: Full state -> Hyper-autoencoder (Encoder + Decoder) -> Latent variables -> GFINNs (ENN, SNN, LNN, MNN) -> Latent dynamics -> Integration -> Decode to full state

- **Critical path**: Encode full state to latent variables → Integrate latent dynamics forward → Decode back to full state → Compute all four loss terms → Backpropagate and update all parameters together

- **Design tradeoffs**: Exact structure preservation (GFINNs) vs. universal approximation (plain NNs) vs. Simultaneous training (better coupling) vs. separate training (more stable) vs. Including Jacobian/model terms (better extrapolation) vs. simpler loss (easier to train)

- **Failure signatures**: Poor extrapolation → Check if Jacobian/model loss terms are too small or derivative data is bad; Training instability → Try reducing learning rate or switching to separate training; Inaccurate reconstruction → Verify autoencoder architecture and loss weights

- **First 3 experiments**: 1) Train with only integration + reconstruction loss (remove Jacobian/model) and compare extrapolation 2) Switch to separate training (fix encoder/decoder, train GFINNs) and compare performance 3) Test on a simple parametric PDE (e.g., heat equation) to verify thermodynamic correlation claim

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise mathematical relationship between the entropy production rate in the latent space and the full-state solution behavior across different physical systems? The paper observes an intriguing correlation between entropy production rates in latent space and full-state solution behaviors for both Burgers' equation and heat equation, but does not establish a rigorous mathematical framework.

### Open Question 2
How does the simultaneous training approach of tLaSDI compare to separate training methods in terms of robustness and convergence properties across different problem classes? The paper mentions that tLaSDI uses simultaneous training while other methods use separate training, and empirically finds tLaSDI performs better, but does not provide a rigorous comparison or theoretical analysis.

### Open Question 3
What are the fundamental limitations of the Jacobian loss term in the tLaSDI framework, and under what conditions does it provide the most benefit? The paper introduces a new Jacobian loss term derived from abstract error estimates, but acknowledges it requires derivative information and discusses a revised version for when derivatives are unavailable.

## Limitations
- The thermodynamic correlation between entropy production rate and solution behavior is empirical and lacks theoretical justification
- Method's performance depends critically on accurate derivative information for Jacobian and model losses
- Simultaneous training approach could introduce training instability that is not thoroughly explored

## Confidence
- **High**: Thermodynamic structure preservation through GENERIC formalism is mathematically rigorous and exact degeneracy conditions are guaranteed by construction
- **Medium**: Simultaneous training advantage is empirically supported but training stability analysis is limited
- **Medium**: Extrapolation improvements from full four-term loss are demonstrated but sensitivity to loss weighting and derivative noise is not explored

## Next Checks
1. Perform ablation study with synthetic noisy derivative data to quantify sensitivity of Jacobian/model loss terms
2. Conduct training stability analysis comparing simultaneous vs separate training across different learning rates and architectures
3. Test the entropy-S correlation on a broader class of thermodynamic systems to assess generality of the empirical observation