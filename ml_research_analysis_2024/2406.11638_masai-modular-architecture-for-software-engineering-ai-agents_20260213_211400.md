---
ver: rpa2
title: 'MASAI: Modular Architecture for Software-engineering AI Agents'
arxiv_id: '2406.11638'
source_url: https://arxiv.org/abs/2406.11638
tags:
- masai
- issue
- test
- code
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MASAI, a modular architecture for software
  engineering AI agents. The key idea is to break down complex software engineering
  tasks into sub-problems and assign them to specialized sub-agents with well-defined
  objectives and strategies.
---

# MASAI: Modular Architecture for Software-engineering AI Agents

## Quick Facts
- arXiv ID: 2406.11638
- Source URL: https://arxiv.org/abs/2406.11638
- Reference count: 11
- Primary result: 28.33% resolution rate on SWE-bench Lite

## Executive Summary
This paper introduces MASAI, a modular architecture for software engineering AI agents that breaks down complex tasks into specialized sub-problems. The system consists of five sub-agents - Test Template Generator, Issue Reproducer, Edit Localizer, Fixer, and Ranker - each with well-defined objectives and problem-solving strategies. MASAI achieves state-of-the-art performance on SWE-bench Lite with a 28.33% resolution rate, outperforming existing methods by decomposing software engineering tasks into manageable sub-problems and employing different reasoning strategies (ReAct and Chain of Thought) across sub-agents.

## Method Summary
MASAI decomposes software engineering tasks into sub-problems and assigns them to specialized sub-agents with well-defined objectives and strategies. The architecture uses five sub-agents: Test Template Generator, Issue Reproducer, Edit Localizer, Fixer, and Ranker. Each sub-agent employs either ReAct (Reasoning and Acting) or Chain of Thought (CoT) strategies, using a set of allowed actions to interact with the environment. The sub-agents are composed by passing outputs from one to the inputs of another. The Fixer generates multiple candidate patches using CoT sampling, while the Ranker selects the best patch based on test results. The architecture uses GPT-4o as the underlying language model and evaluates on SWE-bench Lite dataset.

## Key Results
- Achieves 28.33% resolution rate on SWE-bench Lite, outperforming existing methods
- Modular design allows different problem-solving strategies across sub-agents
- Sampling multiple candidate patches and ranking them improves resolution success rate
- Separate test template generation improves issue reproduction in repositories with uncommon testing frameworks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular decomposition improves problem-solving by allowing each sub-agent to use specialized strategies
- Mechanism: The architecture breaks down complex software engineering tasks into sub-problems (test generation, issue reproduction, localization, fixing, ranking), with each sub-agent employing a different problem-solving strategy (ReAct, CoT) optimized for its specific task
- Core assumption: Different sub-problems in software engineering benefit from different reasoning approaches, and separating these concerns improves overall performance
- Evidence anchors:
  - [abstract]: "where different LLM-powered sub-agents are instantiated with well-defined objectives and strategies tuned to achieve those objectives"
  - [section 2.1]: "Strategy is the problem-solving strategy to be followed by the sub-agent in using the LLM to solve its given sub-problem"
  - [corpus]: Weak evidence - no direct corpus support found for this specific claim

### Mechanism 2
- Claim: Sampling multiple candidate patches and ranking them improves resolution success rate
- Mechanism: The Fixer sub-agent generates 5 candidate patches using CoT sampling, and the Ranker uses test results to select the best one, rather than iteratively refining a single candidate
- Core assumption: Diverse sampling captures more of the solution space than iterative refinement, and test-based ranking can effectively identify correct solutions
- Evidence anchors:
  - [section 4.4]: "Fixer asks for the edit in the form of a minimal rewrite... For each edit, the Fixer expects the LLM to output the original version of the code snippet (pre) followed by the edited version (post)"
  - [section 4.4]: "We observe that sampling multiple repair patches from the Fixer significantly increases the possibility of generating a correct patch"
  - [corpus]: Weak evidence - no direct corpus support found for this specific claim

### Mechanism 3
- Claim: Separate test template generation improves issue reproduction in repositories with uncommon testing frameworks
- Mechanism: The Test Template Generator first explores the repository to understand the testing framework and generates a template, which the Issue Reproducer then uses to create issue-specific tests
- Core assumption: Understanding the repository's testing infrastructure is a distinct problem from reproducing specific issues, and separating these improves overall success
- Evidence anchors:
  - [section 2.3]: "Test Template Generator: Discovers how to write and run a new test by analyzing the testing setup specific to the repository"
  - [section 4.5]: "Consider the issue django__django-14672... OpenDevin was unable to reproduce the test... To remedy this, we decompose test reproduction into two steps"
  - [corpus]: Weak evidence - no direct corpus support found for this specific claim

## Foundational Learning

- Concept: Chain of Thought (CoT) reasoning
  - Why needed here: CoT is used by the Fixer sub-agent to generate multiple candidate patches through structured reasoning steps
  - Quick check question: How does CoT prompting differ from standard completion, and why would it be useful for generating code patches?

- Concept: ReAct (Reasoning and Acting)
  - Why needed here: ReAct is used by the Test Template Generator, Issue Reproducer, and Edit Localizer sub-agents to iteratively reason about problems and take actions in the environment
  - Quick check question: What is the key difference between ReAct and pure reasoning approaches like CoT, and when would ReAct be more appropriate?

- Concept: Test-driven development and reproduction
  - Why needed here: The architecture relies on generating tests that reproduce issues to validate patches, which is fundamental to the ranking mechanism
  - Quick check question: Why is it important to generate tests that reproduce issues before attempting to fix them, rather than just relying on the issue description?

## Architecture Onboarding

- Component map: Test Template Generator (ReAct) → Issue Reproducer (ReAct) → Edit Localizer (ReAct) → Fixer (CoT) → Ranker (CoT)
- Critical path: Test Template Generator → Issue Reproducer → Edit Localizer → Fixer → Ranker
  - This represents the typical workflow from understanding the repository to generating and validating a patch
- Design tradeoffs:
  - Modularity vs. overhead: Breaking into sub-agents adds coordination complexity but allows specialized strategies
  - Sampling vs. iteration: Generating multiple patches upfront vs. iteratively refining one candidate
  - Test generation vs. direct fixing: Creating reproduction tests adds steps but provides objective validation
- Failure signatures:
  - Test Template Generator fails: Cannot generate working tests for the repository
  - Issue Reproducer fails: Cannot create tests that actually reproduce the issue
  - Edit Localizer fails: Cannot identify correct files/functions to edit
  - Fixer fails: Generates syntactically incorrect patches or no meaningful candidates
  - Ranker fails: Cannot distinguish between good and bad patches
- First 3 experiments:
  1. Run the Test Template Generator on a simple repository to verify it can discover the testing framework and generate a working test
  2. Test the Issue Reproducer with a known issue and the template from step 1 to verify it can create a reproduction test
  3. Run the full pipeline on a simple issue in a small repository to verify end-to-end functionality, starting with repositories that have clear, isolated issues

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MASAI vary when using different language models (e.g., GPT-4o, Claude, or smaller language models) across its sub-agents?
- Basis in paper: [explicit] The paper states that "The modularity of MASAI permits use of different language models in different sub-agents" but only evaluates using GPT-4o due to time and cost constraints.
- Why unresolved: The paper does not explore the impact of using different language models on MASAI's performance.
- What evidence would resolve it: Empirical results comparing MASAI's performance when using different combinations of language models across its sub-agents.

### Open Question 2
- Question: How does MASAI perform on software engineering tasks that are not validated by tests, such as issues requiring code review or refactoring?
- Basis in paper: [explicit] The paper acknowledges that "the breadth of issues covered in SWE-bench Lite is limited to those that can be validated using tests" and suggests expanding to more diverse issues.
- Why unresolved: The paper's evaluation is limited to SWE-bench Lite, which only includes test-validated issues.
- What evidence would resolve it: Evaluation of MASAI on a dataset of software engineering tasks that are not validated by tests, such as issues requiring code review or refactoring.

### Open Question 3
- Question: How does the performance of MASAI's sub-agents vary when dealing with non-English issue descriptions?
- Basis in paper: [explicit] The paper notes that "the issue descriptions in SWE-bench Lite are all in English" and suggests the need for multi-lingual evaluation.
- Why unresolved: The paper's evaluation is limited to English issue descriptions, leaving the performance on non-English issues unexplored.
- What evidence would resolve it: Evaluation of MASAI's sub-agents on software engineering tasks with issue descriptions in various languages.

## Limitations
- Evaluation limited to 300 issues from 11 Python repositories, may not generalize to full SWE-bench diversity
- Performance heavily dependent on GPT-4o without exploring sensitivity to model choice
- Lacks systematic failure analysis and comparison against strong baselines on full SWE-bench dataset

## Confidence
- High Confidence: The modular architecture design and the basic claim that different sub-agents can use different strategies (ReAct vs CoT) are well-supported by the paper's structure and implementation details
- Medium Confidence: The claim that sampling multiple candidate patches and ranking them improves success rate is supported by the paper's experimental setup but lacks ablation studies
- Low Confidence: The claim of state-of-the-art performance is weakly supported given the limited evaluation scope and lack of comparison against published results

## Next Checks
1. **Ablation study on sub-agent strategies**: Remove the modular decomposition and implement a single monolithic agent using the same LLM model. Compare resolution rates to determine whether the performance gain comes from modularity itself or from other factors like different prompt engineering.

2. **Sensitivity analysis to model choice**: Implement MASAI using GPT-3.5 or Claude Haiku and measure how resolution rates change. This would quantify the extent to which the claimed performance improvements depend on using a high-capability model versus the architectural design.

3. **Failure mode analysis**: Systematically analyze failed cases to identify patterns (e.g., repositories with complex testing frameworks, issues requiring cross-file changes, or cases where test generation fails). This would reveal the limitations of the modular approach and guide future improvements.