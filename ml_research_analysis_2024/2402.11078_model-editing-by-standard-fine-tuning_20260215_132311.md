---
ver: rpa2
title: Model Editing by Standard Fine-Tuning
arxiv_id: '2402.11078'
source_url: https://arxiv.org/abs/2402.11078
tags:
- fine-tuning
- editing
- facts
- locality
- edit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that standard fine-tuning, often considered
  ineffective for model editing, can achieve competitive performance with two simple
  modifications. First, it optimizes the conditional likelihood of the edited target
  rather than the full likelihood, focusing training on the desired changes.
---

# Model Editing by Standard Fine-Tuning

## Quick Facts
- arXiv ID: 2402.11078
- Source URL: https://arxiv.org/abs/2402.11078
- Authors: Govind Gangadhar; Karl Stratos
- Reference count: 5
- Key outcome: Standard fine-tuning can achieve competitive model editing performance with conditional likelihood optimization and data augmentation

## Executive Summary
This work demonstrates that standard fine-tuning, often considered ineffective for model editing, can achieve competitive performance with two simple modifications. First, it optimizes the conditional likelihood of the edited target rather than the full likelihood, focusing training on the desired changes. Second, it augments training data with paraphrased versions of edit prompts to improve generalization and random or similar facts to preserve locality. Experiments on the ZsRE and CounterFact datasets show that these modifications enable fine-tuning to match or outperform specialized editing methods in terms of edit score, which is the harmonic mean of efficacy, generalization, and locality. The approach is simple, agnostic to model architecture, and leverages advances in parameter-efficient fine-tuning techniques.

## Method Summary
The method modifies standard fine-tuning by (1) optimizing conditional likelihood instead of full likelihood by masking all tokens except the edited target during training, and (2) augmenting the edit set E with pseudo-paraphrases P generated by appending model-generated text to prompts and random facts R from the training split for locality supervision. The approach uses parameter-efficient fine-tuning with LoRA on language models like GPT-J or GPT-2 XL, focusing optimization on the specific knowledge changes while maintaining performance on unrelated facts through negative examples.

## Key Results
- Standard fine-tuning with conditional likelihood optimization achieves competitive edit scores compared to specialized editing methods
- Data augmentation with paraphrased prompts significantly improves generalization across different prompt phrasings
- Training on random facts from the training split effectively preserves locality by constraining changes to the edit set
- The approach maintains simplicity while achieving results comparable to more complex model editing techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing conditional likelihood rather than full likelihood improves edit efficacy and generalization while preserving locality
- Mechanism: By masking all tokens except the edited target during training, the model focuses optimization on the specific change rather than updating parameters based on unrelated tokens. This targeted approach minimizes collateral parameter updates that would harm performance on unrelated facts.
- Core assumption: The edited token is the primary contributor to model behavior change, and optimizing its likelihood is sufficient to achieve the desired knowledge update without extensive collateral effects.
- Evidence anchors:
  - [abstract] "we optimize the conditional likelihood rather than the full likelihood"
  - [section 3] "we optimize the conditional likelihood (i.e., mask all tokens except the edited target)"
  - [corpus] Weak - no direct corpus evidence found
- Break condition: If the edited token's prediction depends heavily on context tokens that are masked, the conditional likelihood optimization may fail to capture necessary dependencies.

### Mechanism 2
- Claim: Data augmentation with paraphrased prompts improves generalization
- Mechanism: By training on multiple paraphrases of the same prompt, the model learns to associate the target object with various linguistic expressions of the subject-relation pair. This reduces overfitting to the exact prompt phrasing and improves transfer to novel phrasings.
- Core assumption: Paraphrases maintain the semantic relationship between subject and object while varying surface form sufficiently to teach robust generalization.
- Evidence anchors:
  - [abstract] "we also train on random or similar facts to encourage locality"
  - [section 3] "P is pseudo-paraphrases of the prompts in E generated by appending text generated by the (unedited) model"
  - [corpus] Weak - no direct corpus evidence found
- Break condition: If generated paraphrases are too similar to original prompts or contain semantic drift, they may not provide meaningful generalization benefits.

### Mechanism 3
- Claim: Training on random facts from training split preserves locality
- Mechanism: Including unrelated facts in training provides negative examples that constrain the model from changing predictions on prompts outside the edit set. This acts as a regularizer against overfitting to the edit data.
- Core assumption: Random facts from the training split are sufficiently different from evaluation facts to serve as effective negative examples without introducing label noise.
- Evidence anchors:
  - [abstract] "we also train on random or similar facts to encourage locality"
  - [section 3] "R is a set of facts that should not be altered by editing on E"
  - [section 5.1] "We find that taking random facts from the training split to be simple and effective"
- Break condition: If random facts accidentally overlap with evaluation facts or are too similar to edit prompts, they may introduce harmful interference.

## Foundational Learning

- Concept: Conditional likelihood optimization
  - Why needed here: Understanding how masking tokens during training focuses optimization on specific outputs rather than full sequences
  - Quick check question: What is the mathematical difference between optimizing full likelihood log p(x) versus conditional likelihood log p(o|π)?

- Concept: Data augmentation strategies
  - Why needed here: Knowing how to generate and select appropriate negative examples and paraphrases to balance generalization and locality
  - Quick check question: How would you generate pseudo-paraphrases that maintain semantic equivalence while varying surface form?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: Understanding how techniques like LoRA enable efficient adaptation of large models without full parameter updates
  - Quick check question: What is the key mathematical insight behind LoRA that makes it computationally efficient compared to full fine-tuning?

## Architecture Onboarding

- Component map: Data preprocessing pipeline for generating paraphrases and random facts -> Conditional masking layer for likelihood computation -> PEFT adapter integration (e.g., LoRA) -> Evaluation metrics calculator for efficacy, generalization, and locality -> Training loop orchestrator for the augmented dataset

- Critical path: Data augmentation → Conditional masking setup → PEFT adapter initialization → Training loop execution → Evaluation metric computation

- Design tradeoffs: 
  - More paraphrases improve generalization but increase training time
  - Larger random fact sets improve locality but may introduce noise
  - Different PEFT methods offer computational savings but may affect performance

- Failure signatures:
  - Low efficacy: Insufficient focus on edit targets or inadequate training data
  - Poor generalization: Paraphrases too similar to original prompts or insufficient diversity
  - Bad locality: Random facts too similar to evaluation prompts or insufficient negative examples

- First 3 experiments:
  1. Implement conditional likelihood optimization with no data augmentation to establish baseline improvement over full likelihood
  2. Add paraphrase augmentation while keeping conditional masking to measure generalization gains
  3. Introduce random fact augmentation to assess locality preservation benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can fine-tuning-based model editing be extended to preserve general capabilities of LLMs beyond classification tasks?
- Basis in paper: [explicit] The authors explicitly state that their approach is limited to classification aspects and that preserving general capabilities of LLMs is "an open problem" (Limitations section).
- Why unresolved: Current fine-tuning methods, including the proposed approach, degrade fluency and consistency metrics. The paper notes that none of the compared editors preserve downstream performance.
- What evidence would resolve it: Experimental results showing that fine-tuning-based editing maintains or improves LLM performance on diverse downstream tasks (e.g., reasoning, generation, few-shot learning) while achieving competitive edit scores.

### Open Question 2
- Question: What is the optimal strategy for selecting locality supervision data (R) to maximize locality without harming efficacy and generalization?
- Basis in paper: [explicit] The authors experiment with random facts and similar facts (measured by Sentence-BERT) for locality supervision, noting that small data size makes fine-tuning more sensitive to demonstration selection.
- Why unresolved: The paper shows that using similar facts instead of random facts improves results for single-editing but doesn't establish a definitive optimal approach for mass-editing or determine the trade-offs involved.
- What evidence would resolve it: Systematic experiments comparing different locality supervision strategies (random, similar, adversarial examples, etc.) across various model sizes and datasets, with analysis of their effects on all three metrics.

### Open Question 3
- Question: How does the effectiveness of conditional likelihood optimization scale with model size and task complexity?
- Basis in paper: [inferred] The authors demonstrate effectiveness on GPT-J (6B) for classification tasks, but don't explore scaling to larger models or more complex editing scenarios.
- Why unresolved: The paper focuses on relatively simple classification-based editing tasks with medium-sized models, leaving open questions about performance on larger models and more complex knowledge structures.
- What evidence would resolve it: Empirical results showing conditional likelihood optimization performance across a range of model sizes (from small to frontier models) and task complexities (multi-hop reasoning, temporal reasoning, etc.).

## Limitations
- Limited evaluation to factual knowledge editing tasks (ZsRE and CounterFact datasets) without testing on other task types
- Experiments conducted only on medium-sized models (GPT-J 6B, GPT-2 XL 1.5B) without validation on frontier models
- Edit score as a single harmonic mean metric may mask important tradeoffs between efficacy, generalization, and locality

## Confidence
- High Confidence: The core mechanism of conditional likelihood optimization is well-established and experimental results are clearly reported and reproducible
- Medium Confidence: Data augmentation strategy shows consistent benefits but may be sensitive to implementation details and paraphrase generation quality
- Low Confidence: Claims about architecture agnosticism and generalizability to other fine-tuning tasks lack empirical validation

## Next Checks
1. **Architecture Transfer Test**: Validate the approach on at least two additional model architectures (e.g., OPT, LLaMA) and scales (both smaller and larger than tested models) to confirm architecture agnosticism claims.

2. **Task Diversity Test**: Apply the method to non-factual knowledge tasks such as instruction following or code generation to test generalizability beyond the factual editing domain where it was developed.

3. **Ablation Analysis**: Conduct systematic ablation studies varying the amount of paraphrase augmentation, random fact augmentation, and LoRA rank to identify optimal configurations and understand the contribution of each component to the final edit score.