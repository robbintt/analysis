---
ver: rpa2
title: 'SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense'
arxiv_id: '2404.16068'
source_url: https://arxiv.org/abs/2404.16068
tags:
- task
- data
- they
- semeval-2024
- mexico
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces SemEval-2024 Task 9: BRAINTEASER(S), the
  first competition designed to test AI systems'' lateral thinking ability by requiring
  them to defy common sense and reason unconventionally. The task uses the BRAINTEASER
  benchmark, split into train/trial/test sets to support both fine-tuning and zero/few-shot
  settings.'
---

# SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense

## Quick Facts
- **arXiv ID**: 2404.16068
- **Source URL**: https://arxiv.org/abs/2404.16068
- **Reference count**: 40
- **Primary result**: First competition testing AI systems' lateral thinking ability by requiring them to defy common sense

## Executive Summary
SemEval-2024 Task 9 introduces BRAINTEASER(S), a novel benchmark designed to evaluate AI systems' lateral thinking capabilities by requiring them to solve puzzles that defy common sense reasoning. The task features two subtasks—Sentence Puzzles and Word Puzzles—presented in multiple-choice question-answer format. Using the BRAINTEASER benchmark split into train/trial/test sets, the competition attracted 182 participants who submitted 483 solutions, with top-performing systems achieving near-perfect accuracy on both subtasks. However, fine-grained analysis reveals that models struggle with consistency across semantic and context reconstructions, indicating that while AI systems can solve lateral thinking puzzles, they face ongoing challenges in generalization and robust reasoning.

## Method Summary
The BRAINTEASER(S) task introduces a benchmark for lateral thinking assessment through two puzzle types: Sentence Puzzles requiring unconventional reasoning about statements, and Word Puzzles involving unconventional word associations or patterns. The benchmark is structured with train/trial/test splits to support both fine-tuning and zero/few-shot learning settings. The competition format uses multiple-choice question-answer pairs, allowing evaluation of both accuracy and consistency in reasoning approaches. The design intentionally challenges common sense assumptions to test whether AI systems can engage in truly lateral thinking rather than relying on conventional pattern matching.

## Key Results
- 182 participants submitted 483 solutions across both subtasks
- Top-performing systems achieved near-perfect accuracy on both Sentence and Word Puzzles
- Fine-grained analysis reveals models struggle with consistency in semantic and context reconstructions
- Performance indicates AI systems can solve lateral thinking puzzles but face challenges in generalization and avoiding overfitting

## Why This Works (Mechanism)
The task works by presenting puzzles that deliberately violate common sense assumptions, forcing AI systems to abandon conventional reasoning patterns and develop alternative solution strategies. By requiring models to "defy common sense," the benchmark isolates lateral thinking ability from standard knowledge retrieval or pattern matching capabilities. The multiple-choice format provides clear evaluation metrics while the diverse puzzle types ensure comprehensive testing across different lateral thinking domains. The zero/few-shot and fine-tuning settings allow assessment of both prompt-based reasoning and learned pattern recognition approaches.

## Foundational Learning
- **Lateral thinking concepts**: Understanding unconventional problem-solving approaches is needed to design puzzles that test reasoning beyond common sense patterns. Quick check: Can participants explain why traditional reasoning fails on these puzzles?
- **Common sense reasoning limitations**: Knowledge of standard reasoning patterns helps identify when systems are overfitting to conventional logic. Quick check: Can models distinguish between standard and lateral thinking requirements?
- **Puzzle structure analysis**: Understanding puzzle composition enables better evaluation of whether solutions demonstrate true lateral thinking versus pattern matching. Quick check: Do solutions generalize across puzzle variants?
- **Evaluation consistency metrics**: Methods for measuring reasoning consistency across different puzzle types are essential for identifying systematic failures. Quick check: Are accuracy gains accompanied by improved consistency?
- **Overfitting detection in reasoning tasks**: Techniques to distinguish genuine lateral thinking from memorization of specific puzzle patterns. Quick check: Do models maintain performance on novel puzzle variations?

## Architecture Onboarding

**Component Map**: Input puzzles → Reasoning engine → Multiple-choice selection → Consistency evaluation → Performance metrics

**Critical Path**: Puzzle presentation → Model processing → Answer selection → Consistency checking → Accuracy scoring

**Design Tradeoffs**: The benchmark trades comprehensive puzzle coverage for focused evaluation of lateral thinking, using multiple-choice format rather than open-ended responses to enable scalable evaluation while potentially limiting expression of creative solutions.

**Failure Signatures**: Models showing high accuracy but inconsistent reasoning across semantically similar puzzles, or performance degradation on novel puzzle variations, indicate overfitting rather than genuine lateral thinking ability.

**3 First Experiments**:
1. Test model performance on puzzle variants with semantic similarity to evaluate consistency
2. Compare zero-shot versus fine-tuned approaches on generalization to novel puzzle types
3. Analyze error patterns to identify whether failures stem from reasoning limitations or knowledge gaps

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Near-perfect accuracy results appear inconsistent with stated challenges in consistency and generalization, suggesting potential overfitting
- Insufficient details about evaluation methodology and statistical significance of performance differences
- Limited analysis of relative effectiveness between fine-tuning and zero/few-shot settings
- Semantic and context reconstruction failure analysis lacks specific examples and quantitative metrics

## Confidence
**High**: Introduction of BRAINTEASER(S) as a novel benchmark for lateral thinking; basic task structure and participation metrics appear reliable.
**Medium**: Characterization of model limitations in consistency and generalization; claims about semantic and context reconstruction struggles need more concrete evidence.
**Low**: Interpretation of near-perfect accuracy in light of stated challenges; effectiveness of different training approaches not adequately supported.

## Next Checks
1. Replicate the fine-grained analysis of semantic and context reconstruction failures using the provided test set with detailed error analysis for multiple top-performing systems
2. Conduct ablation studies to quantify the impact of fine-tuning versus prompt engineering approaches on both subtask performance and generalization
3. Implement cross-validation with puzzle variants to assess whether high accuracy reflects true lateral thinking ability or task-specific memorization