---
ver: rpa2
title: Formal Explanations for Neuro-Symbolic AI
arxiv_id: '2410.14219'
source_url: https://arxiv.org/abs/2410.14219
tags:
- neural
- explanation
- explanations
- neuro-symbolic
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of explaining decisions made
  by neuro-symbolic AI systems, which combine neural perception with symbolic reasoning.
  The proposed method introduces a hierarchical approach to formal abductive explanations.
---

# Formal Explanations for Neuro-Symbolic AI

## Quick Facts
- arXiv ID: 2410.14219
- Source URL: https://arxiv.org/abs/2410.14219
- Reference count: 40
- Key outcome: Hierarchical abductive explanations achieve 16% of feature space size for Pacman-SP, with 10-40x faster training than CNNs

## Executive Summary
This paper introduces a formal approach to explain neuro-symbolic AI systems by computing hierarchical abductive explanations. The method decomposes the explanation problem into two stages: first extracting minimal explanations from the symbolic component to identify relevant neural inputs, then independently explaining these inputs using formal XAI techniques. This hierarchical structure ensures subset-minimality of explanations while improving scalability compared to purely neural approaches. Experiments demonstrate that the approach achieves high-quality explanations with significantly smaller sizes than baseline methods, while also reducing training times by an order of magnitude.

## Method Summary
The approach hinges on formal abductive explanations computed hierarchically for neuro-symbolic AI systems. First, minimal explanations are extracted from the symbolic component using MUS (minimal unsatisfiable subset) extraction, which identifies the irreducible inputs responsible for symbolic decisions. This symbolic explanation then identifies which neural inputs are relevant for the final decision. Second, these relevant neural inputs are explained independently using either formal methods (like Marabou with adversarial robustness) or heuristic approaches (like SHAP). The hierarchical structure guarantees subset-minimality when neural inputs are independent, as the union of minimal explanations from both components forms a minimal hierarchical explanation.

## Key Results
- Hierarchical explanations achieve 16% of feature space size for Pacman-SP benchmark, compared to 50-75% for baseline approaches
- Training times for neuro-symbolic models are 10-40x faster than equivalent CNN models (4-13 seconds vs 140-428 seconds)
- Explanation computation times remain practical (seconds to minutes) even for complex benchmarks with thousands of features
- The approach maintains high accuracy while providing more interpretable and actionable explanations than purely neural models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical decomposition enables subset-minimal abductive explanations for neuro-symbolic AI.
- Mechanism: The approach first extracts minimal explanations from the symbolic component, then independently explains only the relevant neural inputs identified by the symbolic step.
- Core assumption: Neural inputs are independent of each other, allowing separate explanations to be combined into a minimal hierarchical explanation.
- Evidence anchors:
  - [abstract] "The approach hinges on the use of formal abductive explanations and on solving the neuro-symbolic explainability problem hierarchically."
  - [section] "Proposition 1 Let c = κσ (κη(x1), . . . , κη(xn)), xj ∈ F, j ∈ [n], be a decision made by a neuro-symbolic system... subset-minimality of the sets Y and Xj s.t. j ∈ Y guarantees that X is subset-minimal."
- Break condition: If neural inputs are not independent, the union of individual explanations may not yield a subset-minimal hierarchical explanation.

### Mechanism 2
- Claim: Formal reasoning tools for symbolic components enable efficient explanation extraction.
- Mechanism: Symbolic rules are encoded as constraints, and minimal unsatisfiable subsets (MUSes) identify irreducible inputs responsible for symbolic decisions.
- Core assumption: Advanced automated reasoning and discrete optimization tools can efficiently handle over-constrained systems.
- Evidence anchors:
  - [abstract] "the symbolic rules can be seen as a set of constraints and, given the inputs to the constraints passed by the neural component, one can use the unsatisfiability of the constraints to identify an irreducible subset of the inputs responsible for the symbolic decision."
  - [section] "one can use a generic explainer for ASP programs [5] or implement a bespoke explanation extractor following the ideas behind MUS extraction widely used in the analysis of over-constrained systems"
- Break condition: If symbolic reasoning becomes computationally intractable for complex rule sets, the approach may not scale.

### Mechanism 3
- Claim: Separating neural perception from symbolic reasoning simplifies the neural component and improves explanation efficiency.
- Mechanism: The neural component only needs to perform individual input classifications, while the symbolic component handles complex reasoning, leading to structurally simpler neural models.
- Core assumption: Smaller, focused neural tasks can be handled by less complex architectures without loss of overall system performance.
- Evidence anchors:
  - [abstract] "the functionality separation in neuro-symbolic AI leads to the simplification of the neural component as it has to deal with smaller or more focused sub-tasks, without a loss of performance of the entire neuro-symbolic system."
  - [section] "This in turn may lead to structurally simpler neural models and positively impacts the performance of formal reasoning engines on such models."
- Break condition: If the simplification of the neural component leads to significant accuracy degradation, the approach loses its advantage.

## Foundational Learning

- Concept: Conjunctive Normal Form (CNF) and Minimal Unsatisfiable Subsets (MUSes)
  - Why needed here: MUS extraction is the underlying technology for computing formal explanations for symbolic and neural AI systems.
  - Quick check question: What is the relationship between MUSes and abductive explanations in the context of unsatisfiable formulas?

- Concept: Abductive Explanations and Adversarial Robustness
  - Why needed here: Abductive explanations answer "why" questions about AI decisions, and adversarial robustness checking can be used to compute distance-restricted abductive explanations.
  - Quick check question: How do distance-restricted abductive explanations differ from global abductive explanations in terms of their scope?

- Concept: Neuro-Symbolic AI Architecture and Hierarchical Explanation
  - Why needed here: Understanding the pipeline of neural perception followed by symbolic reasoning is crucial for implementing the hierarchical explanation approach.
  - Quick check question: What are the key steps in the hierarchical explanation process for a neuro-symbolic AI system?

## Architecture Onboarding

- Component map:
  - Neural Component -> Symbolic Component -> Explanation Engine
  - Neural Component: Performs individual input classifications (e.g., classifying grid cells in Pacman)
  - Symbolic Component: Performs complex reasoning based on neural inputs (e.g., finding shortest path)
  - Explanation Engine: Extracts abductive explanations for both components
  - MUS Enumerator: Identifies minimal unsatisfiable subsets for symbolic explanations
  - Adversarial Robustness Checker: Computes distance-restricted abductive explanations for neural components

- Critical path:
  1. Input data is processed by the neural component
  2. Neural outputs are passed to the symbolic component
  3. Symbolic component makes final decision
  4. Explanation engine extracts symbolic explanation
  5. Explanation engine extracts neural explanations for relevant inputs

- Design tradeoffs:
  - Formal vs. heuristic explanations: Formal explanations guarantee correctness but may be slower
  - Global vs. local explanations: Global explanations are more comprehensive but may be harder to compute
  - Explanation size vs. quality: Smaller explanations may miss important information, while larger ones may be harder to interpret

- Failure signatures:
  - Exponential growth in explanation time for complex symbolic rules
  - Inability to find minimal explanations due to neural input dependencies
  - Significant accuracy degradation when simplifying the neural component

- First 3 experiments:
  1. Implement hierarchical explanation for a simple neuro-symbolic model (e.g., binary number comparison)
  2. Compare explanation sizes and computation times for formal vs. heuristic explanations
  3. Test the impact of input independence assumption on explanation quality and scalability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do relaxation of the input independence requirement impact the quality of neuro-symbolic explanations and the difficulty of their extraction?
- Basis in paper: [explicit] The authors state this as a future work direction, noting that input independence facilitates subset-minimal abductive explanations but its relaxation could affect explanation quality and extraction difficulty.
- Why unresolved: No experiments or theoretical analysis have been conducted on systems where neural inputs are dependent.
- What evidence would resolve it: Empirical studies comparing explanation quality and computational complexity for both independent and dependent input scenarios.

### Open Question 2
- Question: How can contrastive explanations (CXps) be efficiently computed for neuro-symbolic systems?
- Basis in paper: [explicit] The authors identify this as future work, noting that while their hierarchical approach works for abductive explanations, CXps require different computational approaches.
- Why unresolved: Current formal XAI methods for CXps are not well-developed for the neuro-symbolic context, especially with hierarchical structures.
- What evidence would resolve it: Development and testing of algorithms that can generate contrastive explanations for neuro-symbolic systems, with empirical evaluation on benchmark datasets.

### Open Question 3
- Question: How can cardinality-minimal abductive explanations be efficiently computed for neuro-symbolic systems?
- Basis in paper: [explicit] The authors mention this as future work, noting that their current approach focuses on subset-minimal explanations but cardinality minimization presents additional computational challenges.
- Why unresolved: The hierarchical nature of neuro-symbolic explanations makes cardinality minimization computationally harder than subset minimization.
- What evidence would resolve it: Algorithms that can compute cardinality-minimal explanations for neuro-symbolic systems, with performance comparisons to subset-minimal approaches.

## Limitations

- Independence assumption: The approach critically depends on neural inputs being independent, which is assumed but not empirically validated across diverse neuro-symbolic architectures
- Hardware dependency: Reported efficiency improvements are based on MacBook Pro M3 Pro hardware, which may not represent typical deployment environments
- Computational complexity: While scalable for current benchmarks, the approach may face challenges with extremely large symbolic rule sets or high-dimensional neural inputs

## Confidence

- High confidence in the hierarchical decomposition approach for achieving subset-minimal explanations when input independence holds, supported by Proposition 1 and formal proofs
- Medium confidence in the claimed efficiency improvements, as training time comparisons depend on specific hardware configurations and may not generalize
- Low confidence in the independence assumption's universal applicability across different neuro-symbolic architectures without empirical validation

## Next Checks

1. **Independence validation**: Systematically test the neural input independence assumption across diverse neuro-symbolic architectures and dataset types to identify conditions where the hierarchical approach breaks down
2. **Scalability benchmarking**: Evaluate explanation computation times on enterprise-grade hardware with larger symbolic rule sets to assess real-world applicability
3. **Ablation study**: Compare hierarchical explanations against baseline approaches when the independence assumption is violated to quantify performance degradation and identify recovery strategies