---
ver: rpa2
title: Noisy Node Classification by Bi-level Optimization based Multi-teacher Distillation
arxiv_id: '2404.17875'
source_url: https://arxiv.org/abs/2404.17875
tags:
- teacher
- optimization
- student
- matrix
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses noisy node classification on graph data by
  proposing a multi-teacher distillation method based on bi-level optimization (BO-NNC).
  The method first constructs diverse teacher models using self-supervised learning
  methods, then transfers knowledge from these teachers to a student model through
  a novel bi-level optimization strategy that dynamically adjusts teacher weights
  based on student training progress.
---

# Noisy Node Classification by Bi-level Optimization based Multi-teacher Distillation

## Quick Facts
- arXiv ID: 2404.17875
- Source URL: https://arxiv.org/abs/2404.17875
- Reference count: 29
- Primary result: Multi-teacher distillation with bi-level optimization achieves 87.01% accuracy on Photo dataset at 60% noise rate

## Executive Summary
This paper addresses noisy node classification on graph data by proposing a multi-teacher distillation method based on bi-level optimization. The approach constructs diverse teacher models using self-supervised learning methods (GCA, DGI, SUGRL) and transfers knowledge to a student model through a novel bi-level optimization strategy that dynamically adjusts teacher weights based on student training progress. A label improvement module further enhances label quality by filtering noisy labels and adding high-confidence pseudo-labels. Experiments on five real datasets show BO-NNC outperforms state-of-the-art methods, achieving up to 87.01% accuracy on the Photo dataset at 60% noise rate.

## Method Summary
The BO-NNC method consists of three main components: teacher model construction using diverse self-supervised learning approaches, a bi-level optimization framework for knowledge distillation, and a label improvement module. Multiple teacher models are first constructed using graph representation learning algorithms (GCA, DGI, SUGRL) with GCN backbones. These teachers generate soft labels through a weighted fusion mechanism controlled by a teacher weight matrix. The bi-level optimization dynamically adjusts these weights based on student training progress, with the upper level optimizing teacher weights using clean node feedback and the lower level training the student model. The label improvement module iteratively filters noisy labels and adds high-confidence pseudo-labels to enhance the training set quality.

## Key Results
- Achieves 87.01% accuracy on Photo dataset at 60% noise rate, outperforming state-of-the-art methods
- Performance improves with increasing number of diverse teacher models
- Bi-level optimization effectively captures complementary information among teacher models
- Requires fewer clean labels compared to traditional approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bi-level optimization dynamically adjusts teacher weight matrix based on student training progress, reducing dependence on clean labels
- Mechanism: Upper level optimizes teacher weights using clean node feedback from student predictions; lower level trains student using soft labels generated by weighted teacher outputs
- Core assumption: Clean nodes can be reliably identified from unlabelled and low-loss labelled nodes
- Evidence anchors: [abstract] "we design a new bi-level optimization strategy to dynamically adjust the teacher weight matrix based on the training progress of the student model"; [section 2.2] "we investigate clean nodes to effectively learn the weight matrix, resulting in a bi-level optimization"

### Mechanism 2
- Claim: Multiple diverse teacher models improve robustness by providing complementary knowledge to the student
- Mechanism: Self-supervised methods (GCA, DGI, SUGRL) train different encoders; their predictions are fused via teacher weight matrix to create soft labels
- Core assumption: Different self-supervised methods capture diverse structural and semantic aspects of graph data
- Evidence anchors: [section 2.1] "we employ existing graph representation learning algorithms to construct multiple teacher models"; [section 4.4] "the performance of the student model increases with the increase of the number of teacher models"

### Mechanism 3
- Claim: Label improvement module iteratively increases label quality by filtering noisy labels and adding high-confidence pseudo-labels
- Mechanism: Noisy labels removed based on high student loss; pseudo-labels added from top-confidence student/teacher predictions
- Core assumption: High-loss nodes are more likely to be noisy; high-confidence predictions are likely correct
- Evidence anchors: [section 2.3.1] "the node with higher loss has higher probability to be a noisy node than the node with lower loss"; [section 2.3.2] "we follow the literature [14] to separately select pseudo-labels based on the prediction confidence of both the student model and the teacher models"

## Foundational Learning

- Concept: Graph neural networks and message passing
  - Why needed here: The student and teacher models are GCN-based, requiring understanding of how node features propagate through adjacency
  - Quick check question: How does a two-layer GCN compute node embeddings from raw features and adjacency?

- Concept: Self-supervised graph representation learning
  - Why needed here: Diverse teacher models are built using methods like DGI, GCA, and SUGRL, each relying on different pretext tasks
  - Quick check question: What is the core objective difference between DGI (mutual information) and GCA (graph clustering)?

- Concept: Knowledge distillation and soft labels
  - Why needed here: Teacher predictions serve as soft labels for student training; understanding how temperature scaling or weighting affects learning is key
  - Quick check question: Why might averaging teacher predictions be inferior to weighted fusion via a learned teacher weight matrix?

## Architecture Onboarding

- Component map: Data loader -> graph structure + noisy labels -> Teacher construction (GCA, DGI, SUGRL encoders -> classifiers -> prediction matrices) -> Teacher weight matrix W (fuses teacher predictions) -> Bi-level optimizer (upper: update W, lower: train student GCN) -> Label improvement (noisy filtering + pseudo-label selection) -> updated training set -> Evaluation (accuracy on clean test set)

- Critical path: 1. Construct diverse teachers 2. Fuse predictions → soft labels 3. Bi-level loop: train student, update weights 4. Label improvement → next iteration

- Design tradeoffs: More teachers → more diversity but higher compute; Aggressive label filtering → cleaner data but fewer samples; Soft label weighting → better fusion but needs clean nodes for training W

- Failure signatures: Accuracy stalls → teacher diversity insufficient or W not updating; High variance → noisy label filtering too aggressive or pseudo-label selection unreliable; Slow convergence → bi-level loop not stabilizing or learning rates mismatched

- First 3 experiments: 1. Verify teacher diversity: run each self-supervised method alone, compare embeddings/predictions 2. Test soft label fusion: replace W with mean/average, measure impact on student accuracy 3. Validate clean node detection: inspect loss distributions on labelled data, check filtering thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BO-NNC vary with different numbers of teacher models beyond the three used in the experiments?
- Basis in paper: [explicit] The paper mentions constructing multiple teacher models but only uses three in experiments, leaving open whether more teachers would improve performance further
- Why unresolved: The experiments only tested with three teacher models, so there's no data on the optimal number or the performance curve as more teachers are added
- What evidence would resolve it: Experiments testing BO-NNC with varying numbers of teacher models (e.g., 1, 2, 3, 4, 5+) to identify performance trends and potential saturation points

### Open Question 2
- Question: Can the bi-level optimization approach be effectively applied to other graph-based tasks beyond node classification, such as link prediction or graph classification?
- Basis in paper: [inferred] The bi-level optimization framework is described in general terms for knowledge distillation and could potentially extend to other graph tasks, but this is not tested
- Why unresolved: The paper only evaluates on node classification tasks, leaving open whether the approach generalizes to other graph learning problems
- What evidence would resolve it: Applying the BO-NNC framework to other graph tasks like link prediction or graph classification and comparing performance against task-specific baselines

### Open Question 3
- Question: What is the impact of different types of self-supervised learning methods on the diversity and quality of teacher models?
- Basis in paper: [explicit] The paper uses three specific self-supervised methods (GCA, DGI, SUGRL) but doesn't systematically compare different combinations or types of methods
- Why unresolved: Only three methods are tested, and there's no analysis of how different choices of self-supervised learning approaches affect the final performance
- What evidence would resolve it: Systematic experiments comparing different combinations and types of self-supervised learning methods for teacher model construction, measuring both teacher diversity and downstream student performance

## Limitations
- Effectiveness heavily depends on clean node detection reliability, which becomes questionable at very high noise rates (>60%)
- Requires multiple clean-labelled nodes for bi-level optimization, but minimum threshold is not specified
- Computational overhead of running multiple teacher models may limit scalability to larger graphs

## Confidence

- Mechanism 1 (Bi-level optimization dynamics): Medium - The theoretical framework is sound, but effectiveness depends on clean node detection reliability under high noise
- Mechanism 2 (Teacher diversity benefit): High - Supported by ablation studies showing performance improves with more diverse teachers
- Mechanism 3 (Label improvement module): Medium - The iterative approach is reasonable, but performance at extreme noise rates remains uncertain

## Next Checks
1. **Clean node detection robustness test**: Evaluate the bi-level optimization performance across varying numbers of clean labels (10%, 20%, 30% of training data) to identify the minimum threshold for effective teacher weight updates
2. **Teacher model similarity analysis**: Quantify teacher prediction agreement/disagreement on noisy vs clean nodes to validate the diversity assumption and identify when teacher fusion provides diminishing returns
3. **Scalability benchmark**: Measure wall-clock training time and memory usage as graph size increases, comparing against single-teacher baselines to quantify the practical overhead of the multi-teacher approach