---
ver: rpa2
title: 'Boundary Matters: A Bi-Level Active Finetuning Framework'
arxiv_id: '2403.10069'
source_url: https://arxiv.org/abs/2403.10069
tags:
- samples
- selection
- boundary
- active
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BiLAF, a bi-level active finetuning framework
  for sample selection in pretrained models. Unlike prior methods that focus solely
  on diversity, BiLAF balances diversity and uncertainty by first selecting core samples
  to represent class centers, then identifying boundary samples using an unsupervised
  denoising method and iterative selection strategy.
---

# Boundary Matters: A Bi-Level Active Finetuning Framework

## Quick Facts
- **arXiv ID:** 2403.10069
- **Source URL:** https://arxiv.org/abs/2403.10069
- **Reference count:** 18
- **Key outcome:** BiLAF framework balances diversity and uncertainty in sample selection, achieving up to 3% higher accuracy on CIFAR100 and 1% on ImageNet at low annotation budgets (0.5%-10%).

## Executive Summary
This paper introduces BiLAF, a bi-level active finetuning framework that addresses the challenge of sample selection for pretrained models under limited annotation budgets. Unlike prior methods that focus solely on diversity, BiLAF balances diversity and uncertainty by first selecting core samples to represent class centers, then identifying boundary samples using an unsupervised denoising method and iterative selection strategy. The framework demonstrates significant improvements over existing methods on CIFAR10, CIFAR100, and ImageNet, particularly at low annotation budgets where it can achieve up to 3% higher accuracy on CIFAR100.

## Method Summary
BiLAF operates in two stages: first, it uses ActiveFT to select core samples as pseudo-class centers that represent the data distribution. Second, it applies an iterative density-based denoising process to remove peripheral samples likely to be noise, then identifies boundary samples using a boundary score metric that combines intra-class and inter-class distances. The method includes an opponent penalty mechanism to prevent redundant selection of samples from the same class boundaries. The final annotated set combines core and boundary samples, which are used to finetune a pretrained model using SGD with cosine decay.

## Key Results
- Achieves up to 3% higher accuracy on CIFAR100 compared to existing methods at 5% annotation budget
- Improves ImageNet performance by 1% at 0.5% annotation budget
- Demonstrates consistent improvement across multiple datasets and annotation budget levels
- Optimal performance achieved with 375-500 pseudo-class centers depending on data volume

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The denoising process improves boundary selection by removing peripheral samples that are likely noise
- Mechanism: Iterative Density-based Clustering (IDC) calculates density distances and iteratively removes the furthest samples, assuming these are outliers rather than true boundary points
- Core assumption: Peripheral samples with high density distance are more likely to be noise than informative boundary samples
- Evidence anchors: [abstract] "innovative denoising method to pinpoint outliers"; [section] "density distance of each candidate point is redefined... density distance ρ(xj) for each remaining point xj is calculated as..."
- Break condition: If noise samples are not actually peripheral, or if true boundary samples have high density distance, removing them would degrade performance

### Mechanism 2
- Claim: Boundary samples improve model decision boundaries more effectively than random or central samples
- Mechanism: The Boundary Score metric combines intra-class and inter-class distances to identify samples near class boundaries, which are critical for learning decision boundaries
- Core assumption: Samples near class boundaries provide more information about the decision boundary than central samples
- Evidence anchors: [abstract] "focuses on aligning the distribution of selected subsets with the overall data pool, focusing solely on diversity" contrasted with our method; [section] "Definition 4(Boundary Score). The Boundary Score for sample xj... A smaller Boundary Score indicates closer proximity to the boundary."
- Break condition: If the feature space doesn't accurately represent decision boundaries, or if boundary samples are too ambiguous to be useful

### Mechanism 3
- Claim: Opponent penalty prevents redundant selection of samples from the same class boundaries
- Mechanism: When selecting boundary samples, the algorithm applies a penalty to classes that have already been selected, encouraging diversity across different boundaries
- Core assumption: Without penalty, the selection would concentrate on a few class boundaries, missing others
- Evidence anchors: [section] "to prevent multiple samples from clustering at the same pseudo-class boundary against an opposing class center... imposes a penalty on pseudo-classes that have already been selected"; [abstract] "ensuring that the sample selection process effectively balances both diversity and uncertainty"
- Break condition: If class boundaries are clearly separable, the penalty might unnecessarily reduce selection quality

## Foundational Learning

- **Distance metrics in high-dimensional feature spaces**: Why needed here - The method relies on Euclidean distance calculations between feature vectors to determine sample relationships. Quick check: What distance metric is used for comparing feature vectors in the BiLAF framework?

- **Clustering algorithms and their limitations**: Why needed here - The method builds on K-Means and ActiveFT for core sample selection, requiring understanding of how these methods work. Quick check: How does K-Means differ from ActiveFT in selecting core samples?

- **Active learning fundamentals and uncertainty sampling**: Why needed here - The framework operates within active learning paradigm but extends it to the pretraining-finetuning context. Quick check: What distinguishes active finetuning from traditional active learning?

## Architecture Onboarding

- **Component map**: Pretrained model → Feature extraction → Core sample selection (ActiveFT) → Boundary sample selection (Denoising + Boundary Score + Iterative selection) → Annotation → Supervised finetuning
- **Critical path**: Feature extraction → Core sample selection → Boundary sample selection → Final annotated set selection
- **Design tradeoffs**: Balancing core vs boundary samples, denoising intensity vs preserving true boundaries, opponent penalty vs selection diversity
- **Failure signatures**: Performance degradation when noise samples are incorrectly removed, boundary samples are misidentified, or opponent penalty is too aggressive
- **First 3 experiments**:
  1. Run with core samples only (no boundary selection) to establish baseline performance
  2. Test different denoising ratios (0%, 10%, 20%, 30%) to find optimal balance
  3. Compare opponent penalty coefficients (1.0, 1.1, 1.2) to assess impact on boundary diversity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal ratio between core samples and boundary samples vary across different datasets and annotation budgets?
- Basis in paper: [explicit] The paper mentions that the best performance at 5% data volume was observed with 375 centers, while at 10% data volume, 500 centers proved to be optimal, indicating there is still room to enhance the method.
- Why unresolved: The paper only provides a limited exploration of this ratio on CIFAR100. Different datasets and annotation budgets might require different optimal ratios.
- What evidence would resolve it: Systematic experiments varying the core samples to boundary samples ratio across multiple datasets (e.g., CIFAR10, ImageNet) and annotation budgets would provide insights into the optimal ratios for different scenarios.

### Open Question 2
- Question: How sensitive is the BiLAF framework to the choice of hyperparameters, particularly the removal ratio (Prm), clustering fraction (Pin), and opponent penalty coefficient (δ)?
- Basis in paper: [explicit] The paper discusses the impact of varying removal rates and opponent penalties on model performance, noting that the removal rate has a significant impact while the opponent penalty's impact is relatively minor.
- Why unresolved: While the paper provides some analysis on hyperparameter sensitivity, it only explores a limited range of values and on a single dataset (CIFAR100). The sensitivity might vary across different datasets and annotation budgets.
- What evidence would resolve it: Extensive hyperparameter sensitivity analysis across multiple datasets and annotation budgets, including a wider range of values for each hyperparameter, would provide a more comprehensive understanding of their impact on the framework's performance.

### Open Question 3
- Question: How does the BiLAF framework compare to active learning methods that incorporate both diversity and uncertainty in their selection criteria?
- Basis in paper: [inferred] The paper highlights that traditional active learning methods often struggle in the pretraining-finetuning setting due to their inherent bias in batch selection. BiLAF addresses this by balancing diversity and uncertainty through core and boundary sample selection.
- Why unresolved: The paper primarily compares BiLAF to active finetuning methods and traditional active learning methods that focus on either diversity or uncertainty. A direct comparison to active learning methods that explicitly consider both diversity and uncertainty is missing.
- What evidence would resolve it: Experiments comparing BiLAF to active learning methods that incorporate both diversity and uncertainty, such as those based on query-by-committee or expected error reduction, would provide insights into the relative strengths and weaknesses of each approach in the pretraining-finetuning setting.

## Limitations

- The method's effectiveness depends heavily on the quality of the pretrained feature extractor, which may not generalize well to domains significantly different from ImageNet.
- The denoising process could potentially remove informative boundary samples if the density-based assumptions about outliers are incorrect.
- The opponent penalty mechanism lacks theoretical grounding and may be suboptimal for datasets with highly imbalanced class distributions.

## Confidence

- **High confidence**: Core methodology combining core and boundary sample selection is sound and well-implemented
- **Medium confidence**: The denoising process effectively removes noise while preserving informative samples (based on empirical results)
- **Medium confidence**: The opponent penalty mechanism improves boundary diversity (supported by results but lacks theoretical justification)

## Next Checks

1. **Cross-domain validation**: Test BiLAF on non-ImageNet datasets (e.g., medical imaging or satellite imagery) to verify feature extractor generalization and assess performance degradation when domain shift occurs.
2. **Ablation study on denoising parameters**: Systematically vary the denoising ratio (0%, 5%, 10%, 15%, 20%) and measure the trade-off between noise removal and boundary sample preservation to find the optimal balance.
3. **Opponent penalty sensitivity analysis**: Compare BiLAF performance with different opponent penalty coefficients (0.0, 1.0, 1.2, 1.5) across multiple datasets to determine if the penalty is consistently beneficial or dataset-dependent.