---
ver: rpa2
title: 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet'
arxiv_id: '2410.06555'
source_url: https://arxiv.org/abs/2410.06555
tags:
- output
- history
- comp
- your
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ING-VP, an interactive game-based vision
  planning benchmark designed to evaluate the spatial imagination and multi-step reasoning
  capabilities of multimodal large language models (MLLMs). The benchmark features
  six distinct games (Sokoban, Maze, 8-queens, Sudoku, Tower of Hanoi, and 15-puzzle)
  across 300 levels with both image and text representations.
---

# ING-VP: MLLMs cannot Play Easy Vision-based Games Yet

## Quick Facts
- arXiv ID: 2410.06555
- Source URL: https://arxiv.org/abs/2410.06555
- Authors: Haoran Zhang; Hangyu Guo; Shuyue Guo; Meng Cao; Wenhao Huang; Jiaheng Liu; Ge Zhang
- Reference count: 40
- Primary result: Top MLLM Claude-3.5 Sonnet achieves only 3.37% accuracy on spatial reasoning game benchmark

## Executive Summary
This paper introduces ING-VP, an interactive game-based vision planning benchmark designed to evaluate multimodal large language models' (MLLMs) spatial imagination and multi-step reasoning capabilities. The benchmark features six distinct games (Sokoban, Maze, 8-queens, Sudoku, Tower of Hanoi, and 15-puzzle) across 300 levels with both image and text representations. When tested on 15 state-of-the-art MLLMs, even the highest-performing model, Claude-3.5 Sonnet, achieved only 3.37% accuracy, far below human performance. The study reveals that MLLMs struggle with spatial perception, particularly in accurately interpreting element positions, and have very limited planning capabilities despite their advanced architecture.

## Method Summary
The ING-VP benchmark evaluates MLLMs on six spatial reasoning games with 300 levels total (50 per game), each provided in both image and text representations. Models interact with a game environment by generating JSON-formatted instructions that are executed in the environment. The evaluation framework compares performance across three dimensions: image-text vs. text-only inputs, single-step vs. multi-step reasoning, and with-history vs. without-history conditions. Models are evaluated using accuracy (task completion within step limits), completion degree (final game state score), and action efficiency (ratio of effective actions to total actions). The benchmark uses zero-shot evaluation with uniform prompts across all tested models.

## Key Results
- Top-performing model Claude-3.5 Sonnet achieves only 3.37% average accuracy across all games
- MLLMs perform significantly better on text-only representations than image-text inputs
- Most models show better performance in one-step reasoning settings compared to multi-step approaches
- Spatial perception errors (55.2% of total errors) are the primary limitation in image-text settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ING-VP benchmark reveals fundamental limitations in MLLMs' spatial reasoning by requiring precise element position tracking across multi-step game interactions.
- Mechanism: The benchmark presents games where models must maintain spatial awareness through iterative state changes, exposing gaps in how MLLMs process and retain positional information.
- Core assumption: MLLMs can extract visual features but fail to maintain accurate spatial relationships across multiple reasoning steps.
- Evidence anchors:
  - [abstract] "the inability to process the relative positions of elements is one of the primary issues with MLLM perception"
  - [section 4.3] "the most advanced models, such as Claude-3.5 Sonnet and GPT-4o, can generally identify the elements present and even count the quantity of each in the Sokoban game. However, they struggle to accurately determine precise location information"

### Mechanism 2
- Claim: MLLMs show superior performance on text-only representations compared to image-text inputs, revealing limitations in visual comprehension.
- Mechanism: When given the same spatial reasoning task, models perform better when information is provided textually rather than visually, suggesting visual processing is a bottleneck.
- Core assumption: Text representations preserve spatial information more effectively than image inputs for current MLLM architectures.
- Evidence anchors:
  - [abstract] "we also use the model's action efficiency and the remaining steps to complete the game as evaluation metrics"
  - [section 4.2] "Comparing the performance of each model in the image-text and text-only settings, we found that most test subjects performed better in the text-only setting"
  - [section 4.3] "limitations in image comprehension remain a key factor constraining the performance of MLLMs"

### Mechanism 3
- Claim: The one-step reasoning setting elicits better planning capabilities than multi-step approaches, suggesting current MLLMs struggle with sequential reasoning.
- Mechanism: Models given a single prompt to solve entire games outperform those given iterative instructions, indicating step-by-step reasoning degrades performance.
- Core assumption: MLLMs rely on pattern matching from training data rather than genuine planning, making single-step prompts more effective.
- Evidence anchors:
  - [abstract] "for most models, multi-step setting improves accuracy compared to one-step. However, there are exceptions, such as Claude-3.5 Sonnet"
  - [section 4.3] "for the ING-VP benchmark, thinking step by step does not work and even has a negative effect"

## Foundational Learning

- Concept: Spatial reasoning
  - Why needed here: Understanding how objects relate to each other in space is fundamental to solving the game-based tasks in ING-VP
  - Quick check question: Can you describe the spatial relationship between objects in a simple grid-based game?

- Concept: Multi-step reasoning
  - Why needed here: The benchmark evaluates whether models can maintain reasoning coherence across multiple sequential decisions
  - Quick check question: How would you break down a complex problem into smaller, sequential steps?

- Concept: Visual processing limitations
  - Why needed here: The benchmark reveals that current MLLMs struggle with extracting precise visual information from images
  - Quick check question: What are the challenges in accurately interpreting spatial relationships from visual data?

## Architecture Onboarding

- Component map: Game environment interface -> Image/text processor -> Reasoning engine -> History manager
- Critical path:
  1. Receive current game state (image or text)
  2. Process input through MLLM
  3. Extract valid instruction from output
  4. Apply instruction to game environment
  5. Evaluate success/failure and update metrics
- Design tradeoffs:
  - Image vs text inputs: Text-only generally performs better but loses visual nuance
  - Single-step vs multi-step: Single-step often outperforms multi-step despite being less intuitive
  - With-history vs without-history: History inclusion doesn't improve performance and adds complexity
- Failure signatures:
  - Consistently incorrect positional information
  - Inability to complete simple multi-step tasks
  - Performance degradation when switching from text to image inputs
- First 3 experiments:
  1. Compare model performance on identical tasks using text-only vs image-text inputs
  2. Test whether breaking down multi-step tasks into single-step sub-tasks improves performance
  3. Evaluate whether providing explicit spatial reasoning instructions improves outcomes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications could improve MLLMs' spatial perception abilities, particularly for understanding precise locations of objects in images?
- Basis in paper: [explicit] The paper identifies that MLLMs struggle with accurately interpreting element positions and spatial relationships, with perceptual errors comprising 55.2% of total errors in image-text settings.
- Why unresolved: While the paper identifies the problem, it doesn't propose specific architectural changes or training strategies to address this limitation.
- What evidence would resolve it: Comparative studies testing MLLMs with enhanced spatial perception modules against the current models on ING-VP benchmarks would provide evidence of improvement.

### Open Question 2
- Question: Does the performance gap between one-step and multi-step settings indicate fundamental limitations in sequential reasoning, or could it be resolved through improved prompting strategies?
- Basis in paper: [explicit] The paper observes that MLLMs perform better in one-step settings than multi-step settings for most tasks, contrary to human problem-solving approaches.
- Why unresolved: The paper raises this paradox but doesn't explore whether alternative prompting techniques could bridge this gap.
- What evidence would resolve it: Systematic experiments varying prompt structure and sampling methods across both settings would clarify whether the limitation is fundamental or methodological.

### Open Question 3
- Question: What is the relationship between a model's action efficiency and its ability to complete tasks, and does high action efficiency indicate genuine understanding or just pattern matching?
- Basis in paper: [explicit] The paper notes that "models frequently generate instructions that alter the game state, but these changes have minimal impact on successfully completing the level"
- Why unresolved: While the paper identifies this discrepancy, it doesn't investigate whether high action efficiency represents any form of reasoning capability.
- What evidence would resolve it: Analysis correlating action efficiency with performance across different task complexities would reveal whether this metric has predictive value for genuine reasoning capabilities.

## Limitations

- The benchmark focuses on puzzle games which may not reflect real-world application domains where MLLMs are deployed
- Performance could vary significantly with prompt optimization, but the study uses uniform prompts across all models
- The 300 levels may not represent the full complexity space of spatial reasoning tasks

## Confidence

- High Confidence: The finding that MLLMs achieve very low accuracy (average 3.37%) on the ING-VP benchmark is well-supported by the evaluation methodology and results.
- Medium Confidence: The conclusion that spatial perception limitations are the primary constraint on MLLM performance is supported but may oversimplify the interaction between visual processing, reasoning, and planning capabilities.
- Low Confidence: The assertion that current MLLMs have "very limited planning capabilities" may conflate inability to execute plans with inability to generate them.

## Next Checks

1. Test the same MLLMs on complementary spatial reasoning benchmarks (e.g., CoSMM, BIRD) to determine if performance patterns hold across different task types and evaluation frameworks.

2. Systematically vary prompt structures, including chain-of-thought prompting and spatial reasoning instructions, to determine if performance improvements are possible through better prompt engineering rather than architectural limitations.

3. Conduct ablation studies comparing model performance on different levels of visual abstraction (raw images vs. processed visual features vs. text representations) to isolate whether visual processing or reasoning capabilities are the primary bottleneck.