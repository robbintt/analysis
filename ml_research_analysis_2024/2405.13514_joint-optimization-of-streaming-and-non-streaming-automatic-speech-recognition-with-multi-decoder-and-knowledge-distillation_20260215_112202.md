---
ver: rpa2
title: Joint Optimization of Streaming and Non-Streaming Automatic Speech Recognition
  with Multi-Decoder and Knowledge Distillation
arxiv_id: '2405.13514'
source_url: https://arxiv.org/abs/2405.13514
tags: []
core_contribution: This paper addresses the challenge of optimizing both streaming
  and non-streaming end-to-end automatic speech recognition (E2E-ASR) within a single
  model. The proposed method employs a multi-decoder architecture with separate decoders
  for streaming and non-streaming paths, combined with similarity-preserving knowledge
  distillation (sp-KD) between the intermediate encoder and decoder representations.
---

# Joint Optimization of Streaming and Non-Streaming Automatic Speech Recognition with Multi-Decoder and Knowledge Distillation

## Quick Facts
- arXiv ID: 2405.13514
- Source URL: https://arxiv.org/abs/2405.13514
- Reference count: 0
- This paper proposes a multi-decoder architecture with similarity-preserving knowledge distillation to jointly optimize streaming and non-streaming E2E-ASR, achieving relative CERR improvements of 2.6%-5.3% for streaming and 8.3%-9.7% for non-streaming ASR on the CSJ dataset.

## Executive Summary
This paper tackles the challenge of jointly optimizing streaming and non-streaming automatic speech recognition within a single model. The authors propose a multi-decoder architecture that employs separate decoders for streaming and non-streaming paths, enabling each to specialize while sharing a common encoder. To further enhance joint optimization, similarity-preserving knowledge distillation (sp-KD) is applied between intermediate encoder and decoder representations of both paths. Experiments on the Corpus of Spontaneous Japanese demonstrate that this approach yields consistent relative character error rate reductions for both modes compared to standalone baselines, validating the effectiveness of the proposed method.

## Method Summary
The proposed method uses a shared encoder with two specialized decoders: one for streaming and one for non-streaming ASR. The streaming decoder operates in a causal manner, making predictions incrementally as audio is received, while the non-streaming decoder has access to the full input sequence for optimal context. To facilitate joint optimization, similarity-preserving knowledge distillation (sp-KD) is applied, encouraging the intermediate representations of both decoders to align in a way that preserves their similarity structure. This multi-decoder, KD-based approach allows the model to balance the distinct requirements of streaming and non-streaming scenarios within a unified framework.

## Key Results
- Streaming ASR: 2.6%-5.3% relative CERR reduction over standalone streaming module
- Non-streaming ASR: 8.3%-9.7% relative CERR reduction over standalone non-streaming module
- Joint optimization improves both modes, with non-streaming benefiting more than streaming

## Why This Works (Mechanism)
The method works by leveraging a shared encoder to extract rich acoustic features, while allowing each decoder to specialize for its respective mode (streaming or non-streaming). The multi-decoder design ensures that the streaming decoder remains efficient and causal, while the non-streaming decoder can leverage full sequence context. Similarity-preserving knowledge distillation aligns the intermediate representations between decoders, encouraging them to learn complementary and compatible features. This joint optimization enables the model to improve performance for both modes simultaneously, as evidenced by the relative CERR reductions observed in experiments.

## Foundational Learning
- **End-to-End ASR (E2E-ASR)**: Directly maps acoustic input to text without separate language models or pronunciation lexicons. Needed to understand the baseline architecture; quick check: review encoder-decoder structure.
- **Streaming vs. Non-Streaming ASR**: Streaming processes audio incrementally with low latency; non-streaming uses full input for best accuracy. Needed to grasp the core challenge; quick check: compare latency and accuracy tradeoffs.
- **Knowledge Distillation (KD)**: Transfers knowledge from a larger or more accurate model to a smaller or more efficient one. Needed to understand how sp-KD improves joint optimization; quick check: review KD loss formulations.
- **Similarity-Preserving KD (sp-KD)**: A variant of KD that aligns the similarity structure of intermediate representations. Needed to see how decoder alignment is enforced; quick check: inspect similarity matrices before/after distillation.
- **Multi-Decoder Architecture**: Uses separate decoders for different tasks or modes within a single model. Needed to understand the design choice for joint optimization; quick check: trace data flow for both decoders.
- **Character Error Rate (CER)**: Measures the edit distance between predicted and reference transcriptions. Needed to interpret the reported results; quick check: review how CER is computed in ASR.

## Architecture Onboarding
- **Component Map**: Input Audio -> Shared Encoder -> Streaming Decoder + Non-Streaming Decoder -> Text Outputs; KD Loss links intermediate representations between decoders.
- **Critical Path**: Audio → Shared Encoder → Both Decoders → Text; KD loss updates both decoders based on representation alignment.
- **Design Tradeoffs**: Multi-decoder increases model size and complexity but enables specialization and joint optimization; sp-KD adds training overhead but improves alignment and performance.
- **Failure Signatures**: Degraded streaming latency if non-streaming decoder dominates training; reduced non-streaming accuracy if streaming decoder constraints leak; poor KD alignment if similarity structure is not preserved.
- **First Experiments**: 1) Train standalone streaming and non-streaming models for baseline comparison; 2) Train the multi-decoder model without KD to assess architecture impact; 3) Apply sp-KD and compare both modes' performance to baselines.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains evaluated only on the Corpus of Spontaneous Japanese (CSJ), limiting generalization to other languages or domains
- Does not explore alternative KD strategies or quantify the relative impact of architecture vs. KD
- Lacks qualitative analysis of learned representations and their effect on latency or computational efficiency

## Confidence
- **High** confidence in the core claim of joint optimization benefits (supported by controlled ablation and standalone comparisons)
- **Medium** confidence in broader applicability (single dataset, unknown cross-corpus generalization)
- **Low** confidence in practical deployment readiness (no analysis of real-world robustness or efficiency)

## Next Checks
1. Evaluate the proposed method on additional datasets (e.g., LibriSpeech, TED-LIUM) to assess cross-corpus generalization.
2. Conduct ablation studies isolating the effects of the multi-decoder architecture and the similarity-preserving KD to quantify their individual contributions.
3. Measure and report inference latency and computational efficiency for both streaming and non-streaming modes under realistic constraints.