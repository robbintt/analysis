---
ver: rpa2
title: Sequence-to-Sequence Language Models for Character and Emotion Detection in
  Dream Narratives
arxiv_id: '2403.15486'
source_url: https://arxiv.org/abs/2403.15486
tags:
- characters
- language
- dream
- narratives
- emotions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a sequence-to-sequence language model approach
  to automate the detection of characters and their emotions in dream narratives according
  to the Hall and Van de Castle scheme. The method involves converting symbolic character
  and emotion codes into natural language, then training a model to generate these
  descriptions from narratives.
---

# Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives

## Quick Facts
- arXiv ID: 2403.15486
- Source URL: https://arxiv.org/abs/2403.15486
- Authors: Gustave Cortal
- Reference count: 0
- One-line primary result: Novel sequence-to-sequence approach achieves F1-scores of 64.74 and 75.13 for character and emotion detection in dream narratives, outperforming larger models with fewer parameters

## Executive Summary
This paper presents a novel sequence-to-sequence language model approach for detecting characters and their emotions in dream narratives using the Hall and Van de Castle coding scheme. The method converts symbolic character and emotion codes into natural language descriptions, then trains a model to generate these descriptions from the narratives themselves. This semantic approach allows the model to leverage contextual information about character and emotion references within dream narratives. Experiments on the DreamBank corpus demonstrate the method's effectiveness, with the supervised models achieving competitive performance while using significantly fewer parameters than large language models employing in-context learning.

## Method Summary
The approach involves transforming symbolic character and emotion codes from the Hall and Van de Castle scheme into natural language descriptions. A sequence-to-sequence language model is then trained to generate these descriptions directly from dream narrative text. This allows the model to capture semantic relationships between characters, their emotional states, and the narrative context. The method was evaluated on the DreamBank corpus, with experiments examining the impact of semantic conversion, proper name usage, and model scaling on detection performance. The supervised models were compared against large language models using in-context learning approaches.

## Key Results
- Achieved F1-scores of 64.74 and 75.13 for character and emotion detection respectively
- Semantic conversion approach improves model performance by leveraging contextual information
- Supervised models with 28 times fewer parameters outperform large language models using in-context learning
- Using proper names and scaling up model size both contribute to improved detection accuracy

## Why This Works (Mechanism)
The method works by transforming symbolic codes into natural language descriptions, which allows the sequence-to-sequence model to leverage the semantic relationships between characters, emotions, and narrative context. This semantic approach provides richer contextual information than direct symbol-to-symbol mapping, enabling the model to better understand character roles and emotional dynamics within the dream narrative structure.

## Foundational Learning
- Hall and Van de Castle coding scheme: Essential for understanding the symbolic representation of characters and emotions in dream research; quick check: familiarity with basic coding categories
- Sequence-to-sequence modeling: Core technique for mapping input sequences to output sequences; quick check: understanding of encoder-decoder architecture
- DreamBank corpus: The specific dataset used for evaluation; quick check: knowledge of its size and composition
- In-context learning: The baseline comparison method using large language models; quick check: understanding of few-shot prompting

## Architecture Onboarding

Component map:
Dream narrative text -> Sequence-to-sequence encoder -> Decoder -> Natural language character/emotion descriptions

Critical path:
Narrative input → Semantic encoding → Character/emotion identification → Description generation

Design tradeoffs:
- Semantic conversion vs. direct symbol mapping: Balances interpretability with potential information loss
- Model size vs. parameter efficiency: Smaller supervised models achieve comparable results to larger few-shot approaches
- Proper name usage: Improves performance but may introduce naming convention dependencies

Failure signatures:
- Incorrect character attribution when multiple characters share similar roles
- Emotion misclassification due to ambiguous contextual cues
- Performance degradation with narratives containing unusual character types

First experiments:
1. Evaluate on a held-out validation set to establish baseline performance metrics
2. Test with different sequence-to-sequence model architectures to identify optimal design
3. Compare semantic conversion approach against direct symbol mapping ablation

## Open Questions the Paper Calls Out
None

## Limitations
- Performance F1-scores (64.74 and 75.13) indicate substantial room for improvement, especially for character detection
- Limited to the Hall and Van de Castle coding scheme, which may not capture all relevant dream narrative aspects
- Evaluation on a relatively small corpus (DreamBank) may limit generalizability to other domains

## Confidence
- Method validity: Medium - Sound methodology but moderate performance results
- Comparison claims: Medium - Parameter efficiency demonstrated but few-shot learning potential not fully explored
- Semantic approach benefits: Medium - Innovative but introduces intermediate representation steps

## Next Checks
1. Evaluate the model on a larger, more diverse dream narrative corpus to assess generalizability beyond DreamBank
2. Conduct ablation studies to quantify the exact contribution of semantic conversion versus direct sequence modeling
3. Test the approach on other narrative domains (such as fiction or personal storytelling) to determine if the method transfers effectively beyond dream narratives