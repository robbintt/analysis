---
ver: rpa2
title: 'Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal
  Large Language Models'
arxiv_id: '2406.11230'
source_url: https://arxiv.org/abs/2406.11230
tags:
- accuracy
- samples
- images
- image
- needle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMNeedle, a benchmark designed to evaluate
  the long-context capabilities of multimodal large language models (MLLMs). The benchmark
  presents MLLMs with a "haystack" of images, each containing multiple sub-images,
  and prompts them to locate a target sub-image ("needle") based on a textual description.
---

# Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2406.11230
- Source URL: https://arxiv.org/abs/2406.11230
- Reference count: 30
- Primary result: Introduces MMNeedle benchmark to evaluate long-context visual retrieval capabilities of multimodal large language models (MLLMs)

## Executive Summary
This paper presents MMNeedle, a benchmark designed to evaluate the long-context capabilities of multimodal large language models (MLLMs) in visual information retrieval tasks. The benchmark challenges MLLMs to locate target sub-images within a "haystack" of stitched images based on textual descriptions. By employing image stitching to create extended context lengths and developing an automatic label generation protocol, MMNeedle tests MLLMs' ability to understand and retrieve information from long visual contexts. The evaluation covers both API-based and open-source MLLMs across various scenarios, revealing significant performance differences and highlighting challenges in hallucination, multi-needle retrieval, and handling large stitching sizes.

## Method Summary
MMNeedle creates a benchmark for testing MLLM long-context visual retrieval by stitching multiple images into a single "haystack" containing multiple sub-images. The benchmark employs automatic label generation to pair target sub-images (needles) with textual descriptions. MLLMs are prompted to locate the needle within the haystack based on the textual description. The evaluation protocol varies context lengths, needle quantities (single vs multiple), and sample types (positive vs negative). This approach allows systematic testing of MLLMs' ability to process extended visual contexts and retrieve specific information within them.

## Key Results
- GPT-4o consistently outperforms both API-based and open-source MLLMs in long-context visual retrieval tasks
- All models, including GPT-4o, struggle with hallucination when processing negative samples
- MLLMs face significant challenges with multi-needle retrieval and large stitching sizes, indicating limitations in long-context understanding

## Why This Works (Mechanism)
The benchmark leverages the visual information retrieval task as a proxy for measuring long-context understanding in MLLMs. By creating extended visual contexts through image stitching, the benchmark forces models to maintain coherent understanding across multiple visual elements. The automatic label generation protocol ensures scalability while maintaining consistency in evaluation. The varied testing scenarios (different context lengths, single/multiple needles, positive/negative samples) provide comprehensive assessment of MLLM capabilities across different long-context retrieval challenges.

## Foundational Learning
- **Multimodal Large Language Models**: AI models that process and generate both text and visual information; needed to understand how MLLMs handle extended visual contexts
- **Long-context processing**: The ability to maintain coherent understanding across extended input sequences; quick check: test with progressively longer inputs
- **Visual information retrieval**: The task of locating specific visual content within a larger visual context; quick check: measure retrieval accuracy across different haystack sizes
- **Image stitching**: Combining multiple images into a single composite image; quick check: verify stitching quality doesn't introduce artifacts that bias results
- **Automatic label generation**: Protocol for creating ground truth labels without manual annotation; quick check: validate generated labels against human annotations
- **Hallucination in MLLMs**: Generation of incorrect or fabricated information; quick check: measure false positive rates in negative sample scenarios

## Architecture Onboarding
Component map: Image Stitching -> Automatic Label Generation -> Benchmark Creation -> MLLM Evaluation
Critical path: Stitching (creates context) -> Label Generation (provides ground truth) -> Prompt Design (tests retrieval) -> Performance Measurement (evaluates capabilities)
Design tradeoffs: Stitching enables long-context testing but may introduce visual artifacts; automatic labeling ensures scalability but may contain errors
Failure signatures: Performance degradation with increased stitching size, hallucination in negative samples, failure on multi-needle retrieval
First experiments:
1. Test retrieval accuracy with varying haystack sizes (5, 10, 15 stitched images)
2. Compare performance on positive vs negative samples
3. Evaluate single-needle vs multi-needle retrieval scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark does not provide detailed analysis of why GPT-4o outperforms other models, lacking architectural or training data comparisons
- Automatic label generation protocol introduces potential error sources that are not fully quantified
- The evaluation focuses primarily on retrieval accuracy without comprehensive analysis of computational efficiency or robustness to image quality variations

## Confidence
High confidence: Methodology for creating MMNeedle benchmark is well-described and reproducible; performance differences between model categories are clearly demonstrated with appropriate statistical analysis
Medium confidence: Characterization of MLLM limitations in long-context scenarios is supported by empirical evidence; interpretation of specific failure modes could benefit from additional investigation
Low confidence: Explanations for GPT-4o's superior performance are speculative without deeper architectural or training analysis; benchmark may introduce biases favoring certain model architectures

## Next Checks
1. Conduct ablation studies varying image stitching parameters to determine how visual artifacts affect retrieval performance across different MLLM architectures
2. Implement cross-validation using human-verified labels for a subset of the dataset to quantify potential errors in the automatic label generation protocol
3. Test the benchmark with controlled prompt variations to isolate whether performance differences stem from model capabilities versus prompt sensitivity