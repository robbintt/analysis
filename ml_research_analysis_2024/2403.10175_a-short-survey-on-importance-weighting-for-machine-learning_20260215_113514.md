---
ver: rpa2
title: A Short Survey on Importance Weighting for Machine Learning
arxiv_id: '2403.10175'
source_url: https://arxiv.org/abs/2403.10175
tags: []
core_contribution: This survey provides a comprehensive overview of importance weighting
  techniques in machine learning. It highlights how importance weighting can address
  distribution shift problems, including covariate shift, target shift, and sample
  selection bias, by reweighting training samples based on their importance to the
  test distribution.
---

# A Short Survey on Importance Weighting for Machine Learning

## Quick Facts
- arXiv ID: 2403.10175
- Source URL: https://arxiv.org/abs/2403.10175
- Authors: Masanari Kimura; Hideitsu Hino
- Reference count: 40
- Primary result: Comprehensive overview of importance weighting techniques for distribution shift problems in machine learning

## Executive Summary
This survey provides a comprehensive overview of importance weighting techniques in machine learning, highlighting how these methods address distribution shift problems by reweighting training samples based on their importance to the test distribution. The survey covers various applications including domain adaptation, active learning, distributionally robust optimization, model calibration, and PU learning. Key findings emphasize the effectiveness of importance weighting in improving model performance under distribution shift, the connection between importance weighting and density ratio estimation, and the impact on deep learning models.

## Method Summary
The survey discusses Importance Weighted Empirical Risk Minimization (IW-ERM) as the core method, which extends standard ERM by weighting training losses with importance weights estimated from density ratios between training and test distributions. The approach involves estimating the density ratio pte(x)/ptr(x) using methods like kernel mean matching or KLIEP, then using these weights to reweight the empirical risk. For model selection under covariate shift, Importance Weighted Cross-Validation (IWCV) is presented as a way to maintain unbiased estimation when training and test distributions differ.

## Key Results
- Importance weighting effectively addresses distribution shift by reweighting training samples according to the ratio of test to training densities
- Importance weighting stabilizes model selection under covariate shift by weighting cross-validation errors appropriately
- In domain adaptation, importance weighting rebalances source and target domains by upweighting source samples similar to the target distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Importance weighting corrects distribution shift by reweighting training samples according to the ratio of test to training densities
- Mechanism: The empirical risk is reweighted so that its expectation under the training distribution matches the expected loss under the test distribution, ensuring unbiased estimation even when distributions differ
- Core assumption: The density ratio pte(x)/ptr(x) is either known or can be accurately estimated
- Evidence anchors:
  - [abstract] "Importance weighting is a fundamental procedure in statistics and machine learning that weights the objective function or probability distribution based on the importance of the instance in some sense"
  - [section 3.1] "Under this assumption, we can see that the unbiasedness of estimator obtained by Empirical Risk Minimization is not satisfied. This issue is known to be solvable by utilizing the density ratio between the training distribution and the testing distribution"
  - [corpus] Weak; corpus contains no papers specifically addressing density ratio estimation for covariate shift, so the connection must be drawn from general ML literature
- Break condition: If the density ratio is poorly estimated (e.g., due to large distributional shift), the reweighted empirical risk can become unstable or biased

### Mechanism 2
- Claim: Importance weighting stabilizes model selection under covariate shift by weighting the cross-validation error according to the importance of each input vector
- Mechanism: In Leave-One-Out Importance-Weighted Cross Validation (LOOIWCV), each held-out instance's loss is scaled by pte(xj)/ptr(xj), ensuring that the cross-validation estimate remains unbiased even when training and test distributions differ
- Core assumption: The held-out instance's distribution can be importance-weighted using the same density ratio as in training
- Evidence anchors:
  - [section 3.1] "Importance Weighted Cross Validation (IWCV) [...] overcomes this problem. The Leave-One-Out Importance-Weighted Cross Validation (LOOIWCV) is given as [...]"
  - [abstract] "The survey covers various applications such as domain adaptation, active learning, distributionally robust optimization, model calibration, and PU learning"
  - [corpus] Weak; no corpus papers directly cite IWCV, so the evidence is from the survey itself
- Break condition: If pte(x)/ptr(x) is very large for some instances, the weighted CV estimate can be dominated by a few points, increasing variance

### Mechanism 3
- Claim: In domain adaptation, importance weighting rebalances source and target domains by upweighting source samples that are more similar to the target distribution
- Mechanism: By assigning weights proportional to similarity (often via density ratios or learned weighting functions), the model is trained to minimize loss on a reweighted mixture of source and target-like samples, improving generalization to the target domain
- Core assumption: The source and target domains share label space and conditional label distributions; only the marginal input distribution shifts
- Evidence anchors:
  - [abstract] "The survey covers various applications such as domain adaptation [...]"
  - [section 4] "A key concept among these studies is the allocation of higher weights to source domain data more similar to the target domain"
  - [corpus] Weak; corpus does not contain domain adaptation papers, so this is inferred from the survey
- Break condition: If the label shift is present (i.e., ptr(y|x) â‰  pte(y|x)), importance weighting alone cannot correct the mismatch

## Foundational Learning

- Concept: Density ratio estimation
  - Why needed here: Many importance weighting techniques rely on estimating the ratio pte(x)/ptr(x) to assign correct weights; without accurate estimation, weighting becomes biased or unstable
  - Quick check question: Given two empirical samples from ptr(x) and pte(x), what is the simplest nonparametric way to estimate their density ratio?

- Concept: Empirical Risk Minimization (ERM)
  - Why needed here: Importance Weighted ERM (IW-ERM) is the direct extension of ERM to handle distribution shift; understanding ERM's unbiasedness under i.i.d. is key to seeing why weighting is needed under shift
  - Quick check question: In ERM, why is the empirical risk an unbiased estimator of the expected risk under i.i.d. sampling?

- Concept: Cross-validation and bias under distribution shift
  - Why needed here: Standard CV can be biased under covariate shift; importance-weighted CV corrects this bias, so understanding the bias mechanism is essential
  - Quick check question: What assumption about training and test distributions does standard LOOCV rely on to remain approximately unbiased?

## Architecture Onboarding

- Component map: Density ratio estimator -> weighting function -> reweighted loss -> optimizer -> model. For CV: cross-validation loop -> importance-weighted loss aggregation
- Critical path: Accurate density ratio estimation -> correct weighting -> stable optimization -> low generalization error. A bottleneck in any step propagates errors downstream
- Design tradeoffs: High-fidelity density ratio estimation can be computationally expensive; approximate methods (e.g., kernel mean matching) trade accuracy for speed. Over-weighting rare instances can destabilize training
- Failure signatures: Training loss decreases but validation performance degrades; large variance in weighted gradients; instability in density ratio estimates
- First 3 experiments:
  1. Implement IW-ERM on a synthetic covariate shift dataset with known density ratio; verify unbiasedness of the weighted estimator
  2. Apply importance-weighted CV on the same dataset; compare bias and variance against standard CV
  3. Test stability of IW-ERM as the magnitude of distributional shift increases; observe when weighting becomes detrimental

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can importance weighting be effectively applied to large language models (LLMs) to address distribution shift?
- Basis in paper: [explicit] The paper mentions that the behavior of importance weighting in large neural networks like LLMs is not yet understood and suggests this as a future work direction
- Why unresolved: Current research has not explored the application of importance weighting techniques to LLMs, which have different characteristics and training procedures compared to traditional deep learning models
- What evidence would resolve it: Experimental studies demonstrating the effectiveness (or lack thereof) of various importance weighting methods when applied to LLMs, along with theoretical analysis of why certain approaches work or fail

### Open Question 2
- Question: Can importance weighting be used to address subpopulation shift in a more principled way than current heuristic approaches?
- Basis in paper: [explicit] The paper mentions that many studies suggest reweighting samples inversely proportional to subpopulation frequencies, but UMIX is introduced as a more sophisticated approach using importance weighting based on classifier uncertainty
- Why unresolved: Current approaches to subpopulation shift are largely heuristic and may not capture the full complexity of the problem. There's a need for more theoretically grounded methods
- What evidence would resolve it: Development of a unified framework for importance weighting under subpopulation shift that is theoretically justified and outperforms existing heuristic methods on benchmark datasets

### Open Question 3
- Question: How can importance weighting be integrated into active learning frameworks to improve both sample selection and model performance?
- Basis in paper: [explicit] The paper discusses importance weighted active learning (IWAL) and mentions that combining importance weighting with class-balanced sampling can reduce sample complexity for arbitrary label shifts
- Why unresolved: While IWAL has been proposed, there's still much to explore in terms of integrating importance weighting more deeply into active learning strategies and understanding its impact on both sample efficiency and model performance
- What evidence would resolve it: Comparative studies of active learning methods with and without importance weighting across various scenarios, along with theoretical analysis of the trade-offs involved in using importance weighting for sample selection

## Limitations

- The survey relies heavily on theoretical claims without extensive empirical validation
- Connections to density ratio estimation are supported by general ML literature but lack specific citations within the surveyed works
- Discussion of applications is comprehensive but would benefit from more quantitative comparisons across methods

## Confidence

- Mechanism 1: Medium - grounded in established theory but not empirically demonstrated in the survey
- Mechanism 2: Medium - well-established in theory but lacks experimental evidence in the survey
- Mechanism 3: Medium - inferred from survey content rather than explicitly demonstrated

## Next Checks

1. Implement and compare multiple density ratio estimation methods on a benchmark covariate shift problem
2. Test importance-weighted CV against standard CV on datasets with varying degrees of distribution shift
3. Evaluate the stability of IW-ERM as distributional shift increases, measuring both bias and variance of the weighted estimator