---
ver: rpa2
title: Global Counterfactual Directions
arxiv_id: '2404.12488'
source_url: https://arxiv.org/abs/2404.12488
tags:
- image
- counterfactual
- classifier
- directions
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Global Counterfactual Directions (GCDs),
  a novel approach for generating visual counterfactual explanations in a black-box
  setting. The key insight is that the semantic latent space of Diffusion Autoencoders
  (DiffAEs) encodes the inference process of a classifier in the form of global directions.
---

# Global Counterfactual Directions

## Quick Facts
- **arXiv ID**: 2404.12488
- **Source URL**: https://arxiv.org/abs/2404.12488
- **Reference count**: 40
- **Primary result**: Introduces Global Counterfactual Directions (GCDs) for black-box visual explanations that outperform state-of-the-art methods using a single image and smaller models

## Executive Summary
This paper introduces Global Counterfactual Directions (GCDs), a novel approach for generating visual counterfactual explanations in black-box settings. The key insight is that the semantic latent space of Diffusion Autoencoders (DiffAEs) encodes the inference process of a classifier as global directions. The method discovers two types of these directions: g-directions that can flip classifier decisions globally across entire datasets, and h-directions that provide diverse counterfactual explanations. Experiments on benchmarks like CelebA-HQ and CheXpert demonstrate that GCDs outperform current state-of-the-art black-box methods while using a smaller model and without assuming access to classifier training data.

## Method Summary
GCDs work by training a proxy network to locally approximate the counterfactual loss landscape in the DiffAE's semantic latent space. Starting from a single source image, the method computes g-directions as gradients of this approximate loss, pointing toward classifier decision flipping. H-directions are derived from hessian eigenvectors, providing orthogonal directions of variation that further increase explanation diversity. A line search along these directions finds optimal perturbations that generate counterfactual explanations. The approach requires only a single image and is entirely black-box, making it computationally efficient compared to existing methods.

## Key Results
- GCDs achieve state-of-the-art performance on benchmarks like CelebA-HQ and CheXpert
- The method uses a smaller model and requires only a single image for training
- GCDs can be combined with Latent Integrated Gradients to create a new black-box attribution method
- g-directions achieve close to 100% flip rates globally across entire datasets
- h-directions significantly increase explanation diversity while maintaining high flip rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The semantic latent space of DiffAEs encodes the inference process of a classifier as global directions.
- Mechanism: By training a proxy network on local perturbations of the latent code, the method discovers directions that globally flip classifier decisions.
- Core assumption: The semantic latent space of a DiffAE trained jointly with a classifier captures classifier-specific semantics.
- Evidence anchors:
  - [abstract] "we discover that the latent space of Diffusion Autoencoders encodes the inference process of a given classifier in the form of global directions."
  - [section] "Through the experimental evaluation, we empirically prove that g- and h-directions are transferable across an entire dataset of images."
  - [corpus] Weak - no direct supporting evidence found in neighbors

### Mechanism 2
- Claim: g-directions (gradient-based) can flip classifier decisions globally on entire datasets.
- Mechanism: The g-direction is computed as the gradient of the approximate counterfactual loss with respect to the semantic latent code, pointing in the direction of steepest descent.
- Core assumption: The proxy approximation is accurate enough to capture the counterfactual loss landscape locally.
- Evidence anchors:
  - [abstract] "Precisely, g-directions allow for flipping the decision of a given classifier on an entire dataset of images"
  - [section] "We highlight that g-directions are in fact global, as indicated by their Flip Rate (FR) – percentage of images for which the classifier's decision was flipped – being close to 100%."
  - [corpus] Weak - no direct supporting evidence found in neighbors

### Mechanism 3
- Claim: h-directions (hessian-based) provide diverse counterfactual explanations while maintaining globality.
- Mechanism: The hessian of the approximate counterfactual loss is computed, and its eigenvectors represent orthogonal directions of local variation, each potentially flipping classifier decisions globally.
- Core assumption: The hessian eigenvectors represent meaningful, diverse directions in the semantic latent space.
- Evidence anchors:
  - [abstract] "h-directions further increase the diversity of explanations"
  - [section] "Interestingly, in almost every case, there are at least a few h-directions that achieve FR very close to the corresponding g-direction."
  - [corpus] Weak - no direct supporting evidence found in neighbors

## Foundational Learning

- Concept: Diffusion Autoencoders (DiffAEs)
  - Why needed here: DiffAEs provide a semantic latent space that encodes classifier inference processes as global directions.
  - Quick check question: How does the joint training of DiffAE with a classifier result in a semantic latent space that captures classifier-specific semantics?

- Concept: Counterfactual Explanations
  - Why needed here: The method generates counterfactual explanations by finding global directions in the semantic latent space that flip classifier decisions.
  - Quick check question: What is the counterfactual loss function and how does it balance semantic similarity and classifier decision flipping?

- Concept: Latent Integrated Gradients (LIG)
  - Why needed here: GCDs can be combined with LIG to create a black-box attribution method that highlights important regions in counterfactual explanations.
  - Quick check question: How does the combination of GCDs and LIG enable black-box attribution without access to the classifier's internal structure?

## Architecture Onboarding

- Component map: Source image → DiffAE encoder → Semantic latent code (zsem) → Proxy network (pψ) → Global Counterfactual Directions (g- and h-directions) → Line search → Counterfactual explanation
- Critical path: Train proxy network on local perturbations of zsem → Compute g-direction as gradient of approximate counterfactual loss → Compute h-directions as hessian eigenvectors → Perform line search along each direction to find optimal perturbations
- Design tradeoffs: Using a single image for proxy training vs. multiple images for potentially more accurate approximations; computational efficiency of black-box approach vs. white-box methods with access to classifier internals
- Failure signatures: Low flip rate (FR) indicating directions don't generalize globally; high FID or sFID scores indicating poor image quality; attribution maps that don't highlight important regions
- First 3 experiments:
  1. Train DiffAE on CelebA dataset and visualize semantic latent space using t-SNE to verify classifier-specific semantics are captured
  2. Generate counterfactual explanations for a single image using g-direction and verify classifier decision is flipped
  3. Evaluate globality of g-direction by computing FR on a test set and compare to state-of-the-art black-box methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How transferable are g-directions across different source images within the same dataset?
- Basis in paper: [explicit] The authors mention an ablation study where they used 5 different source images and observed little variation in the obtained Flip Rate (FR), indicating that each image results in a g-direction that is indeed global.
- Why unresolved: While the study shows minimal variation in FR, it does not explore the full range of potential source images or consider different datasets, leaving open the question of how consistent g-directions are across a wider variety of images and contexts.
- What evidence would resolve it: A comprehensive study using a large number of source images from various datasets, examining the consistency of g-directions in terms of FR and other metrics.

### Open Question 2
- Question: What is the relationship between the number of h-directions and the diversity of explanations produced?
- Basis in paper: [inferred] The authors introduce h-directions to increase the diversity of explanations, noting that the first h-directions achieve the highest FR, which then gradually decreases. This suggests a trade-off between globality and diversity.
- Why unresolved: The paper does not quantify the exact impact of the number of h-directions on the diversity of explanations, nor does it explore the optimal number of h-directions for balancing globality and diversity.
- What evidence would resolve it: Experimental results quantifying the diversity of explanations produced by varying numbers of h-directions, and analysis of the trade-off between globality and diversity.

### Open Question 3
- Question: How does the choice of DiffAE architecture affect the performance of GCDs?
- Basis in paper: [inferred] The paper uses a specific DiffAE architecture trained on certain datasets. It does not explore how different architectures or training datasets for DiffAE might impact the effectiveness of GCDs.
- Why unresolved: The performance of GCDs could be influenced by the characteristics of the DiffAE used, such as its ability to capture semantic features or the quality of the latent space representations.
- What evidence would resolve it: Comparative studies using different DiffAE architectures and training datasets to assess their impact on the performance of GCDs in terms of FR, FID, sFID, and other metrics.

## Limitations
- The method's generalizability to other domains or classifier architectures remains unproven
- The computational efficiency gains over existing methods are demonstrated but not thoroughly benchmarked against all major competitors
- The core assumption that DiffAE latent spaces universally encode classifier-specific semantics as global directions requires further validation

## Confidence

- **High**: The method's ability to generate visually plausible counterfactual explanations using DiffAE latent spaces (supported by FID/sFID metrics)
- **Medium**: The claim that g-directions achieve ~100% flip rates globally (empirical results show strong performance but with some variance across datasets)
- **Medium**: The effectiveness of combining GCDs with Latent Integrated Gradients for attribution (novel combination but limited ablation studies)

## Next Checks

1. Test the method's performance on out-of-distribution images and classifiers trained on different architectures to verify the robustness of global directions
2. Conduct a thorough computational efficiency analysis comparing GCDs against state-of-the-art methods across varying model sizes and image resolutions
3. Perform user studies to evaluate whether the generated counterfactual explanations improve human understanding of classifier decisions compared to existing methods