---
ver: rpa2
title: 'High Recall, Small Data: The Challenges of Within-System Evaluation in a Live
  Legal Search System'
arxiv_id: '2403.18962'
source_url: https://arxiv.org/abs/2403.18962
tags:
- legal
- users
- search
- evaluation
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates why common evaluation methods for ranking
  changes in live professional search systems are sub-optimal. Legal IR has unique
  characteristics like high cost of missing results, expensive expert relevance judgments,
  limited user data, and dynamic result lists.
---

# High Recall, Small Data: The Challenges of Within-System Evaluation in a Live Legal Search System

## Quick Facts
- **arXiv ID**: 2403.18962
- **Source URL**: https://arxiv.org/abs/2403.18962
- **Reference count**: 40
- **Key outcome**: Common evaluation methods for ranking changes in live professional search systems are sub-optimal, particularly for legal IR systems

## Executive Summary
This paper examines the challenges of evaluating ranking changes in a live legal search system, where traditional evaluation methods face significant limitations. Legal information retrieval systems have unique characteristics including high cost of missing results, expensive expert relevance judgments, limited user data, and dynamic result lists. The authors systematically evaluate four common evaluation methods - test collections, user surveys, and A/B testing - and demonstrate why each fails to adequately address the specific challenges of legal IR.

The study reveals that standard evaluation approaches from general web search cannot be directly applied to professional search systems, particularly in legal domains where precision and recall are critical. The authors find that test collections become outdated quickly, implicit feedback lacks sufficient query overlap, surveys produce inconclusive results, and A/B testing faces commercial constraints. These findings highlight the need for alternative evaluation approaches specifically designed for high-recall, small-data professional search environments.

## Method Summary
The authors evaluate four common evaluation methods - test collections using expert judgments, implicit feedback from user interactions, user surveys (ranking preferences and Net Promoter Score), and A/B testing - within the context of a live commercial legal search system. They analyze the feasibility and effectiveness of each method based on practical implementation attempts and theoretical considerations. The evaluation focuses on determining whether these methods can effectively measure the impact of ranking changes in a system where users have high information needs, data is scarce, and the cost of missing relevant results is substantial.

## Key Results
- Test collections using expert judgments are too expensive and become outdated quickly in dynamic legal environments
- Implicit feedback methods lack sufficient query overlap between users to provide reliable evaluation signals
- User surveys (both ranking preferences and NPS) produce inconclusive and unreliable results
- A/B testing is not feasible due to commercial constraints and the high-stakes nature of legal search
- Common evaluation methods from general web search are sub-optimal for professional legal search systems

## Why This Works (Mechanism)
This research works by identifying the fundamental mismatch between standard evaluation methods developed for general web search and the unique characteristics of professional legal search systems. The mechanism of analysis involves systematically applying each evaluation method to the specific context and documenting where and why each approach fails. By examining the practical implementation challenges and theoretical limitations of each method in the legal domain context, the authors reveal how the high-stakes nature, expert-only user base, and recall-oriented requirements create evaluation environments where traditional metrics and approaches break down.

## Foundational Learning
- **Professional search evaluation**: Understanding how evaluation differs from general web search due to domain expertise requirements, high-stakes outcomes, and specialized user needs - needed to contextualize why standard metrics fail; quick check: compare precision/recall requirements between legal and web search
- **Legal information retrieval characteristics**: Recognizing the high cost of missing results, expensive expert judgments, and dynamic result lists - needed to understand evaluation constraints; quick check: identify which legal domain characteristics most impact evaluation feasibility
- **Evaluation method limitations**: Understanding when test collections, implicit feedback, surveys, and A/B testing break down in specialized contexts - needed to identify alternative approaches; quick check: map which method fails due to which specific constraint

## Architecture Onboarding

**Component Map**: User Queries -> Ranking Algorithm -> Result Display -> User Interaction -> Evaluation Method

**Critical Path**: Query submission → Document ranking → Result presentation → User judgment (implicit or explicit) → Evaluation metric computation

**Design Tradeoffs**: High recall vs. precision requirements (legal search needs comprehensive coverage), cost of expert judgments vs. automated evaluation (expertise expensive but necessary), static test collections vs. dynamic legal information (legal content changes rapidly), and user privacy vs. interaction data collection (professional context limits data gathering)

**Failure Signatures**: Outdated relevance judgments in test collections, insufficient query overlap in implicit feedback, inconclusive survey responses, inability to implement A/B testing due to commercial constraints

**3 First Experiments**:
1. Implement cost-based evaluation using document retrieval costs and user success metrics
2. Test query-specific evaluation with legal experts on a small set of high-impact queries
3. Develop hybrid evaluation combining limited expert judgments with interaction pattern analysis

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding evaluation in professional search systems: How can we develop evaluation methods that work with small amounts of high-quality data? What alternative metrics beyond precision and recall are suitable for high-stakes domains? How can we balance the need for comprehensive evaluation with the practical constraints of professional environments? What role can cost-based or utility-based evaluation play in addressing these challenges?

## Limitations
- The study focuses on a single commercial legal search system, limiting generalizability to other domains
- Lacks systematic comparative analysis across multiple professional search systems
- Based primarily on theoretical analysis and practical implementation attempts rather than controlled experiments
- Does not empirically validate proposed alternative evaluation approaches

## Confidence
- **High confidence**: Characterization of legal IR challenges (high cost of missing results, expensive expert judgments) is well-supported by domain expertise
- **Medium confidence**: Claims about common evaluation methods being sub-optimal are supported by case study evidence but lack broader validation
- **Low confidence**: Specific recommendations for alternative evaluation approaches require empirical validation

## Next Checks
1. Conduct systematic comparison of evaluation method performance across 3-5 different professional search systems (legal, medical, technical domains) to test generalizability
2. Implement and evaluate alternative methods (e.g., cost-based evaluation) in controlled experiments with multiple ranking change scenarios
3. Perform user study with representative legal professionals to validate identified evaluation challenges and test proposed solutions in real-world contexts