---
ver: rpa2
title: The Persistence of Neural Collapse Despite Low-Rank Bias
arxiv_id: '2410.23169'
source_url: https://arxiv.org/abs/2410.23169
tags:
- loss
- have
- solution
- matrix
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends the observation that deep neural collapse (DNC)
  is suboptimal due to low-rank bias in unconstrained feature models (UFMs). We prove
  that for deep UFMs with cross-entropy loss, DNC is generally not optimal, and characterize
  the associated low-rank bias by bounding the number of non-negligible singular values
  at global minima as network depth increases.
---

# The Persistence of Neural Collapse Despite Low-Rank Bias

## Quick Facts
- **arXiv ID**: 2410.23169
- **Source URL**: https://arxiv.org/abs/2410.23169
- **Reference count**: 40
- **Key outcome**: This work extends the observation that deep neural collapse (DNC) is suboptimal due to low-rank bias in unconstrained feature models (UFMs).

## Executive Summary
This paper proves that deep neural collapse (DNC) is generally suboptimal in deep unconstrained feature models (UFMs) with cross-entropy loss, and characterizes the associated low-rank bias that emerges as network depth increases. The authors show that while DNC solutions remain attractive critical points on the loss surface, lower-rank solutions can achieve better performance. Through theoretical analysis of the loss surface topology, they demonstrate why DNC persists in practice despite being suboptimal—its higher degeneracy creates larger basins of attraction that gradient-based optimizers are more likely to find.

## Method Summary
The paper analyzes deep unconstrained feature models (DUFMs) with L linear layers, where feature maps are treated as optimization variables. The optimization objective combines cross-entropy loss with L2 regularization on all parameters. The theoretical analysis characterizes the singular value structure of solutions at global minima, proving that DNC becomes suboptimal for L ≥ 2 and that lower-rank solutions emerge due to low-rank bias. The loss surface is analyzed to understand the prevalence of different critical configurations, and experiments validate these findings in both DUFMs and deep neural networks.

## Key Results
- DNC is generally suboptimal for deep UFMs (L ≥ 2) with cross-entropy loss, as lower-rank solutions can achieve better performance
- Low-rank bias increases with network depth, with the number of non-negligible singular values bounded at global minima
- DNC solutions remain attractive critical points due to higher degeneracy on the loss surface, explaining their empirical prevalence despite suboptimality
- Experimental results confirm that low-rank solutions emerge more consistently as network width increases or regularization decreases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Low-rank bias induced by L2 regularization causes neural collapse to be suboptimal in deep unconstrained feature models.
- **Mechanism**: As network depth increases, the Schatten quasi-norm regularization term approximates the rank of the weight matrix, penalizing high-rank structures. This bias pushes the optimization toward lower-rank solutions that can fit the data with smaller singular values.
- **Core assumption**: The loss function can be decomposed into a data fit term and a regularization term that promotes low-rank solutions as depth increases.
- **Evidence anchors**:
  - [abstract] "We prove that for deep UFMs with cross-entropy loss, DNC is generally not optimal, and characterize the associated low-rank bias by bounding the number of non-negligible singular values at global minima as network depth increases."
  - [section 3.1] "While the DNC structure is optimal for L = 1, it generally ceases to be optimal when L ≥ 2."
- **Break condition**: If the regularization strength λ is too large relative to the loss term, the optimization converges to the zero matrix, eliminating any structure including DNC.

### Mechanism 2
- **Claim**: The prevalence of DNC in practice is due to its higher degeneracy on the loss surface compared to low-rank solutions.
- **Mechanism**: As network width increases, the dimensionality of the solution space corresponding to DNC grows faster than that of lower-rank solutions. This creates a larger basin of attraction for DNC, making it more likely for gradient-based optimizers to converge to these points.
- **Core assumption**: The loss surface topology and basin size influence the likelihood of convergence to different critical points during optimization.
- **Evidence anchors**:
  - [abstract] "We further analyze the loss surface, showing that DNC is more prevalent than other critical configurations, which explains its frequent empirical appearance despite suboptimality."
  - [section 3.3] "This theorem states that, for small values of d, a given DNC solution is associated with a lower-dimensional space in the loss surface than any other low-rank optimal structure."
- **Break condition**: If the initialization or optimizer parameters strongly favor low-rank solutions, the bias toward DNC basins may be overcome.

### Mechanism 3
- **Claim**: DNC remains an attractive critical point (local minimum or degenerate saddle) even when suboptimal, due to the structure of the Hessian at these points.
- **Mechanism**: For suitably small regularization, DNC solutions have Hessian matrices that are positive semi-definite to leading order, making them resistant to escape by local optimization methods.
- **Core assumption**: The second-order structure of the loss surface at critical points determines their stability under gradient-based optimization.
- **Evidence anchors**:
  - [abstract] "We show that although DNC may be suboptimal for general L, it remains an optimal point with a positive semi-definite leading order Hessian when the level of regularization is not too large."
  - [section 3.3] "Although it is no longer a global minimum, it remains either a local minimum or a degenerate saddle point, both of which are difficult for local optimizers to escape from."
- **Break condition**: If regularization is too strong or second-order terms become significant, the Hessian may lose its positive semi-definiteness, allowing escape from DNC points.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed here: The analysis of low-rank bias and the characterization of optimal solutions rely heavily on understanding the singular value structure of weight matrices.
  - Quick check question: How does the SVD of a matrix relate to its rank, and why is this important for understanding low-rank bias in neural networks?

- **Concept: Unconstrained Feature Models (UFMs)**
  - Why needed here: The paper uses UFMs as an approximation to study the behavior of deep networks, separating feature extraction from classification.
  - Quick check question: What is the key assumption of the unconstrained feature model, and how does it differ from standard neural network training?

- **Concept: Neural Collapse**
  - Why needed here: Understanding the properties of neural collapse (NC) and deep neural collapse (DNC) is essential to grasp why they are suboptimal and how low-rank bias affects them.
  - Quick check question: What are the four key properties of neural collapse, and how do they manifest in the final layers of trained networks?

## Architecture Onboarding

- **Component map**: Input -> Deep unconstrained feature model (L layers) -> Loss function (CE + L2) -> Output (classification probabilities)

- **Critical path**:
  1. Separate L layers from the network
  2. Treat feature maps as optimization variables (H1)
  3. Optimize W1...WL and H1 jointly with cross-entropy loss
  4. Apply L2 regularization to all parameters
  5. Analyze the structure of solutions at global minima

- **Design tradeoffs**:
  - Linear vs. non-linear activations: Linear layers simplify analysis but may not capture all phenomena; ReLU introduces complexity but better represents real networks
  - Width vs. depth: Wider networks increase the degeneracy of DNC solutions; deeper networks strengthen low-rank bias
  - Regularization strength: Stronger regularization pushes toward zero solution; weaker regularization allows richer structures

- **Failure signatures**:
  - If DNC solutions dominate despite suboptimality: Check if regularization is too weak or width is too large
  - If no low-rank solutions are found: Verify if depth is sufficient to induce strong low-rank bias
  - If optimization gets stuck in poor local minima: Consider adjusting initialization or optimizer parameters

- **First 3 experiments**:
  1. Train a deep UFM with varying depth L and observe when DNC ceases to be optimal
  2. Vary the network width d and measure how the prevalence of DNC changes
  3. Compare linear and ReLU activation functions to verify if low-rank bias persists across non-linearities

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the width of intermediate layers in a deep neural network affect the prevalence of deep neural collapse structures?
  - Basis in paper: Explicit - The paper demonstrates that as the width of separated layers increases, the dimension of the solution space corresponding to DNC expands more rapidly than for lower-rank solutions, making DNC more prevalent.
  - Why unresolved: While the paper provides theoretical justification and numerical evidence for this phenomenon in the unconstrained feature model, it does not directly investigate how this translates to the actual loss surface of deep neural networks with varying layer widths.
  - What evidence would resolve it: Experimental results showing the frequency of DNC structures in deep neural networks as a function of intermediate layer width, compared to theoretical predictions from the unconstrained feature model.

- **Open Question 2**: What is the relationship between the rank of solutions and the generalization performance of deep neural networks?
  - Basis in paper: Explicit - The paper shows that low-rank solutions exist and can outperform DNC structures in the unconstrained feature model, and suggests that these low-rank solutions might lead to lower-rank Hessians in actual networks.
  - Why unresolved: The paper does not investigate the generalization performance of these different solution types in deep neural networks, nor does it provide a theoretical link between solution rank and generalization.
  - What evidence would resolve it: Comparative studies of generalization performance (e.g., test error, robustness) between networks converging to DNC structures versus those converging to lower-rank solutions, along with theoretical analysis connecting solution rank to generalization bounds.

- **Open Question 3**: How do different optimization algorithms and hyperparameters (e.g., learning rate, batch size) influence the likelihood of converging to DNC versus low-rank solutions?
  - Basis in paper: Explicit - The paper mentions that Sukenik et al. observed that learning rate can impact the probability of reaching a DNC solution, but does not provide a comprehensive analysis of how different optimization factors affect this.
  - Why unresolved: While the paper provides theoretical insights into the loss surface structure, it does not explore how practical aspects of optimization influence the trajectory towards different solution types.
  - What evidence would resolve it: Systematic experiments varying optimization algorithms, learning rates, batch sizes, and other hyperparameters to measure their effect on the prevalence of DNC versus low-rank solutions in both the unconstrained feature model and deep neural networks.

## Limitations

- The theoretical results assume unconstrained feature models, which may not fully capture the behavior of standard deep networks with constrained features
- The analysis focuses on linear activations, though experiments include ReLU; the persistence of low-rank bias across non-linearities needs stronger validation
- The bounds on singular values and degeneracy measures are asymptotic; their practical significance for finite-width networks remains unclear
- The conditions for Theorem 1 (K ≥ 4, L ≥ 3 or K ≥ 6, L = 2) may be restrictive for practical network configurations

## Confidence

- **High confidence**: The characterization of low-rank bias through Schatten quasi-norm approximation and the proof that DNC is suboptimal for deep UFMs
- **Medium confidence**: The analysis of loss surface degeneracy and basin sizes, as this relies on specific parameter regimes
- **Medium confidence**: The experimental validation, as it shows consistent patterns but with limited hyperparameter exploration

## Next Checks

1. **Validate Theorem 1 conditions empirically**: Systematically test whether DNC ceases to be optimal exactly when K, L satisfy the stated conditions, and characterize the transition region
2. **Test non-linear activation persistence**: Compare linear vs. ReLU networks across a wider range of widths and depths to quantify how non-linearity affects low-rank bias strength
3. **Measure basin volumes directly**: Use Hessian-based methods to compute actual basin volumes for DNC vs. low-rank solutions across different widths, rather than relying on degeneracy bounds