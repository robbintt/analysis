---
ver: rpa2
title: Evaluating the Adversarial Robustness of Detection Transformers
arxiv_id: '2412.18718'
source_url: https://arxiv.org/abs/2412.18718
tags:
- attacks
- detr
- adversarial
- object
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the adversarial robustness of detection transformers
  (DETRs) under both white-box and black-box adversarial attacks. The authors extend
  classic attack methods (FGSM, PGD, and C&W) to assess DETR vulnerability, demonstrating
  that DETR models are significantly susceptible to adversarial attacks, similar to
  traditional CNN-based detectors.
---

# Evaluating the Adversarial Robustness of Detection Transformers

## Quick Facts
- arXiv ID: 2412.18718
- Source URL: https://arxiv.org/abs/2412.18718
- Reference count: 30
- This paper evaluates DETR vulnerability to adversarial attacks, showing significant AP drops from 0.420 to 0.047-0.063 under various attacks.

## Executive Summary
This paper presents a comprehensive evaluation of detection transformers (DETRs) under adversarial attacks, demonstrating that these models are highly vulnerable to both white-box and black-box attacks. The authors extend classic attack methods (FGSM, PGD, and C&W) from image classification to object detection and propose a novel untargeted attack specifically designed for DETR's architecture. Their experiments on MS-COCO and KITTI datasets reveal that DETR models experience substantial performance degradation under adversarial perturbations, with average precision dropping significantly across all attack methods. The study also provides novel insights into the transferability of adversarial attacks between different DETR variants and between DETR and CNN-based detectors.

## Method Summary
The authors adapt three classic adversarial attack methods from image classification to object detection by extending them to work with DETR's architecture. They implement FGSM, PGD, and C&W attacks with specific parameter configurations (FGSM: ϵ = 0.03, 0.05, 0.1; PGD: ϵ = 0.03, 0.1 with 10 iterations; C&W: c = 1, 3, 5 with 200 iterations). Additionally, they propose a novel untargeted attack that exploits DETR's intermediate loss functions by optimizing a weighted combination of classification, bounding box, and IoU losses. The attack is designed to induce misclassification with minimal perturbations by targeting the model's loss components directly. Experiments are conducted on four DETR variants (DETR-R50, DETR-R50-DC5, DETR-R101, DETR-R101-DC5) using both MS-COCO and KITTI datasets, evaluating performance through AP, AR, and Robustness Score metrics.

## Key Results
- DETR models show significant vulnerability to adversarial attacks, with AP dropping from 0.420 to as low as 0.047-0.063 under various attacks
- High intra-network transferability is observed among different DETR variants, but limited cross-network transferability to CNN-based models
- The novel untargeted attack specifically designed for DETR's intermediate loss functions achieves comparable or better performance than classic attacks
- Visualizations of self-attention feature maps reveal that adversarial attacks significantly alter DETR's internal representations
- Robustness Score (RS = AP_adv / AP_clean) consistently shows poor performance across all attack methods and DETR variants

## Why This Works (Mechanism)
The vulnerability of DETR models to adversarial attacks stems from their reliance on learned feature representations that can be subtly perturbed to mislead the detection process. Unlike traditional CNN-based detectors, DETR's transformer architecture processes global context through self-attention mechanisms, making it susceptible to attacks that exploit these attention patterns. The novel untargeted attack works by directly optimizing DETR's intermediate loss functions, targeting the classification, bounding box regression, and IoU components simultaneously. This approach is particularly effective because it interferes with the model's multi-task learning process at multiple stages, rather than just the final output. The high intra-network transferability suggests that DETR variants learn similar attention patterns and feature representations that can be exploited across different model configurations.

## Foundational Learning
- Adversarial attacks in computer vision: Understanding how small, carefully crafted perturbations can cause model misclassification is fundamental to evaluating model robustness and developing defenses.
- Object detection metrics (AP, AR): These metrics are essential for quantifying detection performance, particularly how attacks affect the ability to correctly identify and localize objects.
- Transformer architectures in detection: DETR's use of self-attention and encoder-decoder structure differs from CNN-based detectors, requiring specialized attack methods that account for its unique architecture.
- Transferability of adversarial examples: Understanding when and how attacks transfer between models is crucial for developing black-box attack strategies and evaluating model generalization under attack.
- Multi-task learning optimization: DETR's simultaneous optimization of classification, bounding box regression, and IoU losses creates multiple attack surfaces that can be exploited.

## Architecture Onboarding

**Component Map:** Input Image -> Backbone (ResNet) -> Transformer Encoder -> Transformer Decoder -> FFNs -> Class/BBox Predictions -> Loss Functions (Classification + BBox + GIoU)

**Critical Path:** The critical path for adversarial attacks in DETR involves the interaction between the transformer encoder's self-attention mechanism and the decoder's cross-attention, as these components generate the feature representations that are most susceptible to perturbation.

**Design Tradeoffs:** DETR trades computational efficiency for end-to-end training simplicity, but this design choice makes it vulnerable to attacks that exploit its attention-based feature extraction, as the global context processing creates larger attack surfaces than local CNN operations.

**Failure Signatures:** Visual inspection of attention maps reveals that adversarial attacks cause the model to focus on incorrect regions of the image, leading to misclassification or failure to detect objects entirely. The proposed attack's success is indicated by significant AP drops while maintaining visual similarity to the original image.

**First Experiments:**
1. Implement FGSM attack with ϵ = 0.03 on DETR-R50 using MS-COCO to verify basic attack functionality and compare against reported AP drop
2. Test the novel untargeted attack's intermediate loss optimization by varying the weight parameters and measuring their impact on attack effectiveness
3. Conduct transferability analysis by attacking DETR-R50 and testing against DETR-R101 to verify the high intra-network transferability pattern

## Open Questions the Paper Calls Out

**Open Question 1:** How does the robustness of DETR models vary across different object detection datasets and tasks (e.g., general object detection vs. autonomous driving)? The paper only examines MS-COCO and KITTI datasets, leaving open how DETR performs on other specialized datasets or tasks.

**Open Question 2:** What is the impact of DETR's self-attention mechanism on its adversarial robustness compared to CNN-based detectors? While the paper demonstrates vulnerability, it doesn't explore why this is the case or how it compares to CNN-based attention mechanisms.

**Open Question 3:** How effective are ensemble defenses in mitigating adversarial attacks on DETR models? The paper only hints at the potential of ensemble defenses but doesn't provide empirical evidence of their effectiveness.

## Limitations
- The novel untargeted attack's implementation details are underspecified, particularly the exact formulations of intermediate loss functions and grid search methodology for weight optimization
- The transferability analysis results may be sensitive to implementation differences in the baseline DETR models
- The study focuses on two datasets, limiting generalizability to other object detection domains and specialized tasks

## Confidence

| Claim | Confidence |
|-------|------------|
| DETR models are vulnerable to adversarial attacks | High |
| Transferability patterns (high intra-network, low cross-network) | Medium |
| Novel attack effectiveness | Medium |

## Next Checks
1. Implement and verify the exact formulations of J^{o,k}_cls, J^{o,k}_bb, and J^{o,k}_iou loss functions used in the proposed attack, comparing generated perturbations against the paper's visualizations
2. Conduct ablation studies on the weight parameters (ω) in the total loss function to verify the claimed optimization process and attack effectiveness
3. Replicate the transferability analysis using independently implemented DETR baselines to confirm the high intra-network and low cross-network transferability patterns