---
ver: rpa2
title: 'TookaBERT: A Step Forward for Persian NLU'
arxiv_id: '2407.16382'
source_url: https://arxiv.org/abs/2407.16382
tags:
- persian
- language
- bert
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces two new BERT models\u2014TookaBERT-Base\
  \ and TookaBERT-Large\u2014specifically trained on Persian language data to address\
  \ the lack of large-scale pre-trained models for Persian natural language understanding.\
  \ The models were trained using state-of-the-art techniques, including flash attention\
  \ v2, BPE tokenization, and whole-word masking, on a diverse Persian corpus."
---

# TookaBERT: A Step Forward for Persian NLU

## Quick Facts
- arXiv ID: 2407.16382
- Source URL: https://arxiv.org/abs/2407.16382
- Authors: MohammadAli SadraeiJavaheri, Ali Moghaddaszadeh, Milad Molazadeh, Fariba Naeiji, Farnaz Aghababaloo, Hamideh Rafiee, Zahra Amirmahani, Tohid Abedini, Fatemeh Zahra Sheikhi, Amirmohammad Salehoof
- Reference count: 10
- Primary result: Introduced TookaBERT-Base and TookaBERT-Large, achieving +2.8 average improvement over existing models on 14 Persian NLU tasks

## Executive Summary
This paper introduces two new BERT models—TookaBERT-Base and TookaBERT-Large—specifically trained on Persian language data to address the lack of large-scale pre-trained models for Persian natural language understanding. The models were trained using state-of-the-art techniques including flash attention v2, BPE tokenization, and whole-word masking on a diverse Persian corpus. Evaluated across 14 tasks, TookaBERT-Large outperformed existing models with an average improvement of +2.8 points, highlighting its effectiveness for Persian NLU. The models are publicly available, advancing Persian language processing research.

## Method Summary
The authors trained two BERT models using a diverse Persian corpus (hmblogs, madlad, and PersianWebScraper datasets) that was normalized using NFKC and Arabic-to-Persian character conversion. They employed BPE tokenization with 48,000 vocab size and BPE-dropout regularization, trained with flash attention v2 and ZeRO stage 2 optimizer on 8xA100 40GB GPUs. The pre-training used masked language modeling with whole-word masking as the objective. Models were evaluated on 14 diverse Persian NLU tasks including named entity recognition, sentiment analysis, question answering, and natural language inference.

## Key Results
- TookaBERT-Large outperformed seven existing models (ParsBERT, AriaBERT, FaBERT, Shiraz, BERT multilingual, XLM-RoBERTa-Base, XLM-V) with an average improvement of +2.8 points
- Both Base and Large versions demonstrated superior performance across the diverse set of 14 Persian NLU tasks
- The models are publicly available, providing researchers with improved resources for Persian language processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TookaBERT's use of flash attention v2 and BPE tokenization enables faster training and better handling of Persian text.
- Mechanism: Flash attention v2 optimizes GPU memory usage and speeds up attention computations, while BPE tokenization improves subword representation for Persian morphology.
- Core assumption: These techniques are compatible with Persian linguistic characteristics and provide computational efficiency without sacrificing model quality.
- Evidence anchors:
  - [abstract] "trained using state-of-the-art techniques, including flash attention v2, BPE tokenization"
  - [section] "To speed up training and efficiently utilize GPU memory, we employed flash attention v2 (Dao, 2023)."
  - [corpus] Weak evidence - no direct citations comparing flash attention v2 effectiveness specifically for Persian.
- Break condition: If flash attention v2 introduces numerical instability or BPE tokenization poorly represents Persian morphology, performance gains would diminish.

### Mechanism 2
- Claim: TookaBERT's use of whole-word-masking during pre-training improves its understanding of Persian linguistic structure.
- Mechanism: By masking entire words rather than subwords, the model learns more robust word-level representations and context understanding.
- Core assumption: Persian language benefits more from word-level masking due to its morphological complexity and agglutinative nature.
- Evidence anchors:
  - [abstract] "whole-word masking"
  - [section] "We also incorporated whole-word-masking (Cui et al., 2021) to make predicting masks more challenging and improve the model's capabilities."
  - [corpus] No direct evidence - assumption based on general NLP literature rather than Persian-specific findings.
- Break condition: If Persian vocabulary has many rare words or complex compounds, whole-word-masking might lead to insufficient training signals for those tokens.

### Mechanism 3
- Claim: TookaBERT-Large's larger model size provides significant performance improvements over base models.
- Mechanism: Additional parameters allow the model to capture more complex linguistic patterns and relationships in Persian text.
- Core assumption: Persian language complexity requires more parameters to achieve optimal performance, and the training data is sufficient to prevent overfitting.
- Evidence anchors:
  - [abstract] "our larger model outperforms the competition, showing an average improvement of at least +2.8 points"
  - [section] "We trained two BERT models in standard Base and Large sizes."
  - [corpus] Weak evidence - while improvement is demonstrated, no ablation study is provided to isolate the effect of model size.
- Break condition: If the training data is insufficient relative to model size, or if Persian language complexity plateaus before reaching Large size, the performance gains would be minimal.

## Foundational Learning

- Concept: Understanding of transformer architecture and BERT pre-training methodology
  - Why needed here: The paper builds upon standard BERT techniques but applies them to Persian, requiring understanding of both the base methodology and language-specific adaptations
  - Quick check question: What is the key difference between BERT and GPT architecture that makes BERT more suitable for understanding tasks?

- Concept: Knowledge of Persian language characteristics and NLP challenges
  - Why needed here: The model is specifically designed for Persian, requiring understanding of its unique linguistic features and data scarcity issues
  - Quick check question: What are the main challenges in building NLP models for morphologically rich languages like Persian?

- Concept: Familiarity with pre-training and fine-tuning pipeline
  - Why needed here: The paper describes both pre-training of the model and its evaluation through fine-tuning on various tasks
  - Quick check question: Why is it common practice to first pre-train a language model and then fine-tune it for specific tasks?

## Architecture Onboarding

- Component map: Persian text corpus (HmBlogs, Madlad, PersianWebScraper) -> BPE tokenizer with 48,000 vocab size -> Pre-training with masked language modeling and whole-word masking -> Training infrastructure (8xA100 40GB GPUs, flash attention v2, ZeRO stage 2 optimizer) -> Two model checkpoints (Base and Large) for Persian NLU tasks

- Critical path: Pre-training → Tokenization → Model architecture selection → Training optimization → Fine-tuning on downstream tasks → Evaluation

- Design tradeoffs:
  - Model size vs. computational efficiency (Base vs. Large)
  - BPE vocab size vs. tokenization quality
  - Training time vs. model performance
  - Dataset diversity vs. domain specificity

- Failure signatures:
  - Poor performance on morphologically complex words (tokenizer issues)
  - Memory overflow during training (infrastructure limitations)
  - Overfitting on specific task types (insufficient data diversity)
  - Slow convergence (suboptimal hyperparameters)

- First 3 experiments:
  1. Test tokenizer on a sample of Persian text to verify proper handling of compound words and morphological variations
  2. Run a small-scale pre-training run to verify flash attention v2 implementation and GPU memory usage
  3. Fine-tune on a single Persian NLU task to establish baseline performance before full evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TookaBERT-Large perform on tasks not included in the evaluation, such as machine translation or dialogue systems?
- Basis in paper: [inferred] The paper mentions that one of the ParsiNLU tasks (machine translation) was excluded from the evaluation, implying potential performance differences on such tasks.
- Why unresolved: The study focused on a specific set of 14 tasks, and the authors did not evaluate the models on tasks like machine translation or dialogue systems.
- What evidence would resolve it: Testing TookaBERT-Large on machine translation and dialogue systems tasks, comparing its performance to other models, and analyzing its strengths and weaknesses in these areas.

### Open Question 2
- Question: What is the impact of using different tokenization algorithms, such as WordPiece instead of BPE, on the performance of TookaBERT models?
- Basis in paper: [explicit] The paper discusses the choice of BPE over WordPiece for tokenization, but does not explore the performance differences between these algorithms.
- Why unresolved: The authors chose BPE for their tokenizer but did not compare its performance to WordPiece or other tokenization methods.
- What evidence would resolve it: Training and evaluating TookaBERT models using different tokenization algorithms (e.g., WordPiece) and comparing their performance across the same set of tasks.

### Open Question 3
- Question: How does the performance of TookaBERT-Large scale with larger model sizes, such as BERT-XLarge or beyond?
- Basis in paper: [inferred] The paper introduces TookaBERT-Large and demonstrates its superiority over other models, suggesting potential improvements with even larger model sizes.
- Why unresolved: The study only evaluated BERT-Base and BERT-Large sizes, leaving the performance of larger models unexplored.
- What evidence would resolve it: Training and evaluating even larger BERT models (e.g., BERT-XLarge) using the same techniques and comparing their performance to TookaBERT-Large across the same set of tasks.

## Limitations
- Limited ablation studies prevent clear understanding of which architectural choices contribute most to performance
- No comparison of computational efficiency between different optimization techniques
- Potential data leakage or overlap between pre-training and evaluation datasets not explicitly addressed
- Generalization across diverse Persian dialects and writing styles not evaluated

## Confidence
- High: The overall methodology of pre-training BERT models for Persian is sound and follows established practices in the field
- Medium: The reported performance improvements are credible but not fully verifiable due to limited implementation details
- Low: The attribution of specific performance gains to individual technical innovations (flash attention v2, whole-word-masking) lacks direct evidence

## Next Checks
1. **Tokenizer Validation**: Test the BPE tokenizer on morphologically complex Persian words to verify it properly handles compound words and rare morphological variants that are common in Persian. This would validate the core assumption that BPE tokenization is appropriate for Persian morphology.

2. **Pretraining Scalability Test**: Conduct a small-scale pretraining run with and without flash attention v2 to measure the actual memory and speed improvements claimed. This would verify the computational efficiency claims and help isolate the impact of this optimization.

3. **Task-Specific Ablation**: Fine-tune the base model with and without whole-word-masking on a representative Persian NLU task to measure the specific contribution of this technique. This would help validate whether whole-word-masking provides benefits specifically for Persian as claimed.