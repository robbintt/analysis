---
ver: rpa2
title: Understanding the dynamics of the frequency bias in neural networks
arxiv_id: '2405.14957'
source_url: https://arxiv.org/abs/2405.14957
tags:
- frequency
- neural
- learning
- dynamics
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper rigorously derives a PDE model for the frequency dynamics
  of error in 2-layer neural networks under the Neural Tangent Kernel (NTK) regime.
  The key insight is that the learning dynamics can be approximated by a damped heat
  equation where the distribution of initialization weights plays a crucial role in
  determining frequency bias.
---

# Understanding the dynamics of the frequency bias in neural networks

## Quick Facts
- arXiv ID: 2405.14957
- Source URL: https://arxiv.org/abs/2405.14957
- Authors: Juan Molina; Mircea Petrache; Francisco Sahli Costabal; Matías Courdurier
- Reference count: 40
- This paper rigorously derives a PDE model for the frequency dynamics of error in 2-layer neural networks under the Neural Tangent Kernel (NTK) regime

## Executive Summary
This paper provides a rigorous mathematical framework for understanding frequency bias in neural networks by deriving a partial differential equation (PDE) model that captures the learning dynamics under the Neural Tangent Kernel (NTK) regime. The authors show that the frequency bias in neural networks is directly determined by the initialization distribution of the network's weights, particularly the distribution of frequency parameters. Through both theoretical analysis and numerical experiments, they demonstrate that this frequency bias can be controlled or eliminated by appropriate choice of initialization distributions, offering new insights into how neural networks learn frequency components of functions during training.

## Method Summary
The authors develop a theoretical framework based on the NTK regime, where they analyze the evolution of the error in frequency space during neural network training. They derive a damped heat equation that approximates the dynamics of error distribution across different frequencies, with key parameters determined by the initialization distribution of network weights. The analysis focuses on 2-layer neural networks with random Fourier features, where the frequency parameters are drawn from a distribution ρ_w. They validate their theoretical predictions through numerical experiments comparing the PDE solutions (computed via finite element methods) with actual neural network training dynamics, showing strong agreement between the two approaches.

## Key Results
- Derives a damped heat equation that models frequency learning dynamics in 2-layer neural networks under NTK regime
- Shows that frequency learning rate is directly proportional to the initialization distribution ρ_w of frequency parameters
- Demonstrates that frequency bias can be eliminated by using normal distributions with large standard deviations for frequency parameters
- Validates theoretical predictions through comparison with finite element method solutions and neural network training experiments

## Why This Works (Mechanism)
The mechanism works because under the NTK regime, neural network training dynamics can be approximated by a linear system where the evolution of the network parameters follows a simple ordinary differential equation. When analyzed in frequency space, this linear system reduces to a damped heat equation where the diffusion coefficient and damping terms are determined by the initialization distribution. The frequency learning rate depends on how the initialization distribution weights different frequency components, with distributions that give more weight to high frequencies leading to faster learning of high-frequency components.

## Foundational Learning

**Neural Tangent Kernel (NTK)** - Why needed: Provides the theoretical framework for analyzing neural network training as a linear dynamical system; Quick check: Verify that network width is large enough for NTK approximation to hold

**Fourier Analysis** - Why needed: Essential for decomposing functions and understanding frequency components; Quick check: Confirm that the target function can be expressed in Fourier space

**Heat Equation Dynamics** - Why needed: Models how error propagates across frequency space during training; Quick check: Ensure damping coefficient is positive for stability

## Architecture Onboarding

**Component Map**: Initialization Distribution -> Frequency Learning Rate -> Error Evolution -> Training Dynamics

**Critical Path**: The initialization distribution directly determines the frequency learning rate, which controls how quickly different frequency components are learned during training

**Design Tradeoffs**: Wide networks with small learning rates enable NTK regime but may be computationally expensive; narrow networks with large learning rates may exhibit different dynamics but train faster

**Failure Signatures**: If initialization distribution is too concentrated, high-frequency components may never be learned; if too spread out, learning may be unstable or slow

**First Experiments**: 1) Train networks with different initialization distributions and measure frequency learning rates 2) Compare PDE predictions with actual training dynamics for various target functions 3) Test multi-layer network extensions under NTK assumptions

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis primarily valid under NTK regime, which requires very wide networks and small learning rates
- Model focuses on regression problems and 2-layer networks, with extensions to multi-layer networks relying on NTK assumptions
- Results may not capture all relevant phenomena in practical settings with finite-width effects and finite learning rates

## Confidence

**Major Claim Clusters Confidence:**
- PDE derivation and theoretical framework: **High**
- Connection between initialization and frequency bias: **Medium**
- Extension to multi-layer networks: **Medium**
- Practical implications for initialization choice: **Low**

## Next Checks

1. Test the theoretical predictions against actual neural network training dynamics across different network widths and learning rates to assess the impact of finite-width effects
2. Evaluate whether the frequency bias predictions hold for classification tasks and non-smooth activation functions
3. Investigate whether modern initialization schemes (e.g., He or Xavier initialization) already implicitly account for frequency bias considerations