---
ver: rpa2
title: Revisiting Scalable Hessian Diagonal Approximations for Applications in Reinforcement
  Learning
arxiv_id: '2406.03276'
source_url: https://arxiv.org/abs/2406.03276
tags: []
core_contribution: This paper proposes HesScale, a scalable Hessian diagonal approximation
  method with high accuracy and low computational cost. The method builds on Becker
  and LeCun's (1989) work by using exact Hessian diagonals at the last layer and ignoring
  off-diagonal elements elsewhere.
---

# Revisiting Scalable Hessian Diagonal Approximations for Applications in Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.03276
- Source URL: https://arxiv.org/abs/2406.03276
- Reference count: 40
- Primary result: HesScale achieves superior Hessian diagonal approximation accuracy with linear complexity, enabling efficient second-order optimization and step-size scaling in RL

## Executive Summary
This paper introduces HesScale, a Hessian diagonal approximation method that achieves high accuracy while maintaining linear computational complexity. The approach builds on Becker and LeCun's work by computing exact Hessian diagonals at the last layer and ignoring off-diagonal elements in other layers. This design choice enables significant computational savings while preserving approximation quality. The authors demonstrate two applications: a second-order optimizer called AdaHesScale and step-size scaling for reinforcement learning. Experiments show these methods outperform existing approaches in both supervised learning and RL settings, with particular benefits for stability and robustness in policy optimization.

## Method Summary
HesScale computes Hessian diagonal approximations by leveraging a key insight: exact computation at the output layer combined with ignoring off-diagonal elements elsewhere provides sufficient accuracy for practical optimization. This approximation strategy dramatically reduces computational complexity from quadratic to linear in the number of parameters. The method is implemented through efficient backward passes that accumulate diagonal elements during the standard backward gradient computation. For second-order optimization, AdaHesScale uses these diagonal approximations to scale gradients, while in RL, the diagonal entries are used to adaptively scale learning rates across different parameters.

## Key Results
- AdaHesScale achieves superior accuracy and efficiency compared to other optimizers in supervised learning tasks
- Step-size scaling with HesScale in RL improves stability and robustness, preventing policy collapse
- The method maintains linear computational complexity while delivering high-quality Hessian approximations

## Why This Works (Mechanism)
The approximation works because Hessian diagonal elements capture most of the curvature information needed for effective optimization, while off-diagonal terms typically contribute less to the overall optimization landscape. By computing exact diagonals at the output layer where gradients are largest and most important, and approximating other layers as diagonal, the method balances accuracy with computational efficiency. This selective precision approach exploits the observation that different network layers contribute differently to the optimization process.

## Foundational Learning
- **Hessian matrix properties**: Second-order derivatives form a matrix where diagonal elements represent curvature along parameter axes - needed to understand what information is being approximated
- **Curvature in optimization**: The relationship between Hessian diagonal entries and parameter update scaling - needed to connect approximation quality to optimization performance
- **Computational complexity analysis**: Understanding why quadratic vs linear complexity matters for large models - needed to appreciate the practical impact
- **Layer-wise gradient magnitude**: Why output layer gradients dominate and justify exact computation - needed to validate the approximation strategy
- **Reinforcement learning stability**: How curvature information affects policy optimization and collapse prevention - needed to understand RL applications
- **Approximate second-order methods**: Trade-offs between computational cost and optimization quality in modern deep learning - needed for context

## Architecture Onboarding

**Component Map**
Input -> Forward Pass -> Backward Pass (gradient + diagonal accumulation) -> HesScale Approximation -> AdaHesScale/RL Scaling

**Critical Path**
The backward pass accumulation of diagonal elements is the critical path, as it must efficiently compute and store Hessian diagonal approximations during standard gradient computation to avoid additional passes.

**Design Tradeoffs**
- Exact output layer computation vs. full approximation provides accuracy where needed most
- Ignoring off-diagonal elements sacrifices some information for dramatic computational gains
- Linear complexity enables scaling to large models but may miss important parameter interactions

**Failure Signatures**
- Poor approximation quality in networks with strong parameter coupling
- Suboptimal performance on extremely deep architectures where layer interactions matter more
- Potential instability in highly non-convex landscapes where off-diagonal curvature is crucial

**First Experiments**
1. Compare training curves of AdaHesScale vs. Adam on standard vision datasets (CIFAR-10/100)
2. Evaluate RL performance with step-size scaling on simple control tasks (CartPole, LunarLander)
3. Measure approximation error of HesScale against exact Hessian diagonals on small networks

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Effectiveness on extremely deep networks and recurrent architectures remains unclear
- Limited empirical validation against other second-order methods on larger-scale problems
- Theoretical convergence guarantees for AdaHesScale in non-convex settings are not established

## Confidence

**Hessian diagonal approximation accuracy**: High - Strong empirical evidence across multiple architectures
**Computational efficiency claims**: Medium - Theoretical analysis solid, but limited large-scale validation
**RL application benefits**: Medium - Promising results but narrow experimental scope

## Next Checks
1. Evaluate HesScale on transformer-based architectures (BERT, GPT variants) and convolutional networks (ResNet, EfficientNet) to assess architectural robustness
2. Test the method on continuous control benchmarks (MuJoCo, DeepMind Control Suite) with high-dimensional state/action spaces
3. Compare convergence speed and final performance against first-order methods on long-horizon RL tasks with sparse rewards to verify stability benefits under challenging conditions