---
ver: rpa2
title: 'SynChart: Synthesizing Charts from Language Models'
arxiv_id: '2409.16517'
source_url: https://arxiv.org/abs/2409.16517
tags:
- chart
- data
- images
- code
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SynChart, a large-scale synthetic dataset
  for training multimodal models on chart understanding. The authors synthesize approximately
  4 million diverse chart images with rich annotations (data tables, code, descriptions,
  Q&A pairs) using large language models.
---

# SynChart: Synthesizing Charts from Language Models

## Quick Facts
- **arXiv ID**: 2409.16517
- **Source URL**: https://arxiv.org/abs/2409.16517
- **Reference count**: 40
- **Key outcome**: LLM-generated synthetic data can effectively replace human-labeled data for multimodal chart understanding tasks

## Executive Summary
This paper introduces SynChart, a large-scale synthetic dataset for training multimodal models on chart understanding. The authors synthesize approximately 4 million diverse chart images with rich annotations (data tables, code, descriptions, Q&A pairs) using large language models. The dataset enables training a 4.2B parameter chart-expert model that achieves near-GPT-4O performance on the ChartQA benchmark, surpassing GPT-4V. The work demonstrates that LLM-generated synthetic data can effectively replace human-labeled data for multimodal chart understanding tasks.

## Method Summary
The authors create a synthetic dataset by generating diverse chart data through LLMs, then train a multimodal model by combining Phi-3.5-mini-instruct and CLIP-L vision encoder. The training follows a two-stage approach: pretraining on code, data tables, and descriptions, followed by post-training on Q&A pairs. The final model achieves near-GPT-4O performance on ChartQA benchmark.

## Key Results
- 4 million diverse chart images with rich annotations (data tables, code, descriptions, Q&A pairs)
- 4.2B parameter chart-expert model trained on synthetic data
- Near-GPT-4O performance on ChartQA benchmark, surpassing GPT-4V
- ~30% improvement in ChartQA performance through staged training approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated synthetic data can effectively replace human-labeled data for multimodal chart understanding tasks
- Mechanism: Large language models can generate diverse, high-quality chart data with rich annotations that enable training effective chart-specific models
- Core assumption: LLMs can generate realistic chart data that captures the diversity and complexity of real-world charts
- Evidence anchors:
  - [abstract] "We trained a 4.2B chart-expert model using this dataset and achieve near-GPT-4O performance on the ChartQA task"
  - [section 3.1] "Our approach to generating pretraining data involves three key stages as shown in Figure 2"
  - [corpus] Weak corpus evidence - related papers focus on dataset construction but don't validate the synthetic data approach
- Break condition: If generated charts fail to capture real-world diversity or LLMs cannot generate accurate annotations, model performance will degrade

### Mechanism 2
- Claim: Diverse chart metadata (types, themes, constraints, trends) enables scalable generation of representative chart data
- Mechanism: By defining comprehensive metadata schemas, LLMs can systematically generate varied chart data covering real-world scenarios
- Core assumption: Chart metadata constraints adequately capture the space of real-world chart scenarios
- Evidence anchors:
  - [section 3.1] "We identify and focus on the top nine most common chart types" and "generate themes for each data table based on topics from StackExchange"
  - [section 2] "The primary requirements for the training dataset are scale, diversity, representativeness, and label density"
  - [corpus] Weak corpus evidence - no direct validation of metadata diversity approach
- Break condition: If metadata schemas miss important real-world chart patterns or become too constrained, generated data will lack representativeness

### Mechanism 3
- Claim: Two-stage training (pretraining + post-training) with different data types maximizes learning efficiency
- Mechanism: Pretraining on code, data tables, and descriptions builds foundational understanding, while post-training on Q&A pairs refines reasoning capabilities
- Core assumption: Different annotation types serve complementary roles in model development
- Evidence anchors:
  - [section 4] "During pretraining, we utilize annotations that include code, data tables, and descriptions. In the post-training phase, we primarily rely on annotations of questions and answers"
  - [section 4.2] "We achieve approximately a 30% improvement in ChartQA performance" through this staged approach
  - [corpus] Weak corpus evidence - related papers don't validate this specific two-stage training approach
- Break condition: If stage ordering is suboptimal or data types don't align with learning objectives, training efficiency suffers

## Foundational Learning

- Concept: Multimodal learning fundamentals
  - Why needed here: The model must integrate visual and textual information to understand charts
  - Quick check question: How does a multimodal model process chart images differently from pure text models?

- Concept: Synthetic data generation principles
  - Why needed here: The entire approach relies on LLM-generated synthetic data
  - Quick check question: What are the key challenges in ensuring synthetic data quality and diversity?

- Concept: Vision-language pretraining techniques
  - Why needed here: The model combines a language model (Phi-3.5) with a vision encoder (CLIP)
  - Quick check question: How do vision-language models typically align visual and textual representations?

## Architecture Onboarding

- Component map: Phi-3.5 (3.8B) language model + CLIP ViT-L/14 vision encoder + synthetic data pipeline (chart generation, annotation, Q&A)
- Critical path: Data generation → pretraining (code, tables, descriptions) → post-training (Q&A) → evaluation
- Design tradeoffs: Scale vs quality in synthetic data, choice of visualization engines, balance between simple and complex questions
- Failure signatures: Poor chart diversity, annotation errors, inadequate reasoning capabilities, training instability
- First 3 experiments:
  1. Generate small batch of charts with one visualization engine and evaluate annotation quality
  2. Train baseline model on minimal dataset to establish pretraining effectiveness
  3. Test post-training with Q&A pairs on simple chart reasoning tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of chart types in SynChart compare to real-world chart distributions across different domains (business, science, media, etc.)?
- Basis in paper: [inferred] The paper mentions analyzing chart images from Obelics to identify common chart types but doesn't compare SynChart's distribution to real-world distributions.
- Why unresolved: The paper focuses on creating diverse synthetic data but doesn't validate whether the distribution of chart types matches what would be encountered in practice.
- What evidence would resolve it: A comparative analysis showing the frequency of different chart types in SynChart versus real-world datasets from various domains.

### Open Question 2
- Question: What is the upper bound of model performance on ChartQA when using synthetic data alone, and at what point do additional synthetic examples provide diminishing returns?
- Basis in paper: [explicit] The paper mentions that at maximum tested training cost, model performance has not plateaued, but doesn't specify what the theoretical limit might be.
- Why unresolved: While the paper demonstrates scaling properties, it doesn't identify where performance gains from additional synthetic data would level off.
- What evidence would resolve it: Systematic scaling experiments extending beyond the tested range to identify the point of diminishing returns in model performance.

### Open Question 3
- Question: How does the quality of LLM-generated questions and answers in SynChart compare to human-annotated question-answer pairs in terms of complexity and reasoning requirements?
- Basis in paper: [inferred] The paper mentions generating both simple and complex questions but doesn't validate their quality or difficulty compared to human-annotated data.
- Why unresolved: The paper relies on LLM-generated QA pairs but doesn't benchmark their quality against human-created alternatives.
- What evidence would resolve it: A human evaluation comparing the reasoning difficulty and quality of LLM-generated versus human-annotated question-answer pairs from the same charts.

## Limitations
- Evaluation shows near-GPT-4O performance but direct comparisons with GPT-4V may be affected by different evaluation protocols
- Synthetic data generation relies heavily on LLM capabilities without extensive human validation of the 4 million generated charts
- Resource requirements for generating 4 million charts may limit reproducibility for smaller research teams

## Confidence
- **High confidence**: The core methodology of using synthetic data for multimodal training is sound and well-executed
- **Medium confidence**: The claim of near-GPT-4O performance is supported by benchmark results but would benefit from more rigorous evaluation protocols
- **Medium confidence**: The scalability of the approach through LLM-generated data is demonstrated, though long-term generalization remains to be validated

## Next Checks
1. Conduct human evaluation of randomly sampled synthetic charts to assess annotation accuracy and real-world representativeness, comparing against ground truth charts from business reports or academic publications
2. Test the trained model on out-of-distribution chart types and quality levels (noisy images, unconventional formats) to evaluate robustness beyond the benchmark
3. Perform ablation studies on the two-stage training approach to quantify the specific contributions of pretraining versus post-training phases to final performance