---
ver: rpa2
title: 'SNFinLLM: Systematic and Nuanced Financial Domain Adaptation of Chinese Large
  Language Models'
arxiv_id: '2408.02302'
source_url: https://arxiv.org/abs/2408.02302
tags:
- data
- financial
- language
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SNFinLLM is a Chinese financial large language model that achieves
  superior performance on financial domain tasks through a three-stage training process:
  continued pre-training on 100B tokens of financial data, supervised fine-tuning
  on 550K instruction examples, and Direct Preference Optimization alignment. The
  model demonstrates significant improvements over baseline models, achieving 63.94%
  on FinEval and 54.32% on FinanceIQ benchmarks, with particular strength in financial
  computation (52.01%) and machine reading comprehension (95.03%).'
---

# SNFinLLM: Systematic and Nuanced Financial Domain Adaptation of Chinese Large Language Models

## Quick Facts
- arXiv ID: 2408.02302
- Source URL: https://arxiv.org/abs/2408.02302
- Reference count: 6
- Chinese financial LLM achieving 63.94% on FinEval and 54.32% on FinanceIQ benchmarks

## Executive Summary
SNFinLLM is a Chinese financial large language model developed through a systematic three-stage training process: continued pre-training on 100B tokens of financial data, supervised fine-tuning on 550K instruction examples, and Direct Preference Optimization alignment. The model demonstrates significant improvements over baseline models, particularly in financial computation (52.01%) and machine reading comprehension (95.03%). A key innovation is the integration of Python calculator tools, which addresses the common LLM weakness in numerical reasoning by enabling accurate financial calculations.

## Method Summary
The model employs a three-stage training approach starting with an open-source base model. First, continued pre-training on 100B tokens of financial data builds domain-specific knowledge. Second, supervised fine-tuning on 550K instruction examples adapts the model to financial task instructions. Third, Direct Preference Optimization aligns outputs with human preferences. The training uses SentencePiece tokenizer with 32K tokens and employs a 1:3 ratio of domain-specific to general data. Calculator tool integration, inspired by ToolFormer, detects and executes financial computation expressions using Python interpreter.

## Key Results
- Achieves 63.94% on FinEval and 54.32% on FinanceIQ benchmarks
- Excels in financial computation (52.01%) and machine reading comprehension (95.03%)
- Outperforms other Chinese financial LLMs like XuanYuan-13B and Tongyi-Finance-14B across multiple financial evaluation datasets

## Why This Works (Mechanism)

### Mechanism 1
Three-stage training (pre-training → SFT → DPO) enables systematic financial domain adaptation. Domain-specific pre-training builds general financial knowledge, SFT adapts to task-specific instructions, and DPO aligns outputs with human preferences. Each training stage addresses a distinct learning gap without destructive interference.

### Mechanism 2
Calculator tool integration solves numerical reasoning errors in financial calculations. Detection of [Calculator] expressions triggers Python interpreter execution, ensuring computational accuracy. This addresses a common LLM weakness in precise arithmetic operations.

### Mechanism 3
High-quality instruction dataset construction through systematic data processing enables superior financial task performance. Multi-stage data cleaning (format conversion → chunking → cleaning → de-duplication → quality filtering) creates optimized training data, with the assumption that data quality directly correlates with model performance on financial tasks.

## Foundational Learning

- **Domain-specific vocabulary and terminology**: Financial domain requires understanding specialized terms (derivatives, yield curves, etc.) that general models may not recognize. *Quick check*: Can the model correctly interpret "P/E ratio" vs "P/B ratio" in different contexts?

- **Numerical computation and formula application**: Financial calculations require precise arithmetic operations that LLMs typically struggle with. *Quick check*: Does the model correctly compute compound interest for different compounding frequencies?

- **Financial statement analysis and interpretation**: Understanding balance sheets, income statements, and cash flow statements requires domain knowledge. *Quick check*: Can the model identify liquidity issues from a given balance sheet?

## Architecture Onboarding

- **Component map**: Corpus acquisition → Preprocessing → Tokenization → Training datasets → Base model → Continued pre-training → SFT → DPO → Calculator integration → Evaluation framework

- **Critical path**: Data quality → Pre-training effectiveness → SFT instruction following → DPO alignment → Calculator integration

- **Design tradeoffs**:
  - Pre-training vs. fine-tuning: More pre-training data improves domain knowledge but increases computational cost
  - Calculator tool integration vs. in-model computation: External tools ensure accuracy but add complexity
  - DPO alignment vs. task performance: Stronger alignment may reduce domain-specific capabilities

- **Failure signatures**:
  - Hallucinations in financial facts indicate insufficient domain pre-training
  - Numerical calculation errors suggest calculator tool integration issues
  - Poor instruction following points to inadequate SFT quality

- **First 3 experiments**:
  1. Evaluate calculator tool integration: Run financial computation tasks with and without calculator tool to measure accuracy improvement
  2. Test SFT effectiveness: Compare model performance on financial tasks before and after SFT training
  3. Assess DPO impact: Measure how DPO training affects both alignment quality and domain-specific task performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the optimal ratio of domain-specific to general data (1:5 vs 1:3) affect SNFinLLM's performance across different financial tasks? The paper chose 1:3 for practical reasons but didn't systematically compare different ratios to identify the true optimal balance. A controlled experiment testing multiple ratios on key benchmarks would establish the optimal ratio and its impact on different financial task performance.

### Open Question 2
What specific architectural modifications to the base model would further improve performance on complex MRC tasks that currently show limited improvement? The paper notes that complex MRC tasks don't show significant improvement and require further research for complicated context inference performance. Testing SNFinLLM with various architectural enhancements on complex MRC benchmarks would reveal which modifications most effectively address this weakness.

### Open Question 3
How does DPO training's negative impact on finance computation and other tasks relate to the preference data quality and alignment methodology? The paper states that DPO training shows improvement in qEQA task but performance on finance computation and other tasks declines. A detailed analysis comparing different DPO implementations, preference data quality metrics, and ablation studies on the preference alignment process would identify the root cause and potential solutions.

## Limitations

- **Data Quality Uncertainty**: Specific composition, quality metrics, and data sources for the 100B tokens and 550K instruction examples remain unspecified
- **Narrow Evaluation Scope**: Focuses only on Chinese financial benchmarks without comparison to non-financial or multilingual models
- **Limited Calculator Reliability Testing**: Real-world edge cases and failure modes for calculator tool integration are not fully addressed

## Confidence

**High Confidence**: Three-stage training architecture is technically sound; Calculator tool integration effectively addresses numerical reasoning limitations; Superior performance on Chinese financial benchmarks compared to baselines

**Medium Confidence**: Performance metrics accurately reflect real-world capabilities; Data processing pipeline significantly contributes to performance; Strengths in financial computation and MRC generalize to practical applications

**Low Confidence**: Ability to handle complex financial scenarios requiring multi-step reasoning; Robustness across different financial subdomains; Cost-benefit tradeoff of three-stage approach versus simpler fine-tuning

## Next Checks

1. **Ablation Study on Training Stages**: Systematically evaluate model performance by removing each training stage individually to quantify their specific contributions and validate whether the three-stage approach is truly necessary

2. **Real-World Financial Task Testing**: Deploy the model on practical financial use cases beyond benchmark datasets, including live financial data analysis, real-time market sentiment assessment, and automated financial report generation to test robustness in dynamic conditions

3. **Cross-Lingual and Cross-Domain Evaluation**: Test SNFinLLM on English financial datasets and general Chinese language tasks to assess whether financial specialization creates performance degradation in non-financial contexts and identify potential negative transfer effects