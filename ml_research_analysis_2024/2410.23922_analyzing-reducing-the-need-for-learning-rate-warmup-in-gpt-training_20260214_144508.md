---
ver: rpa2
title: Analyzing & Reducing the Need for Learning Rate Warmup in GPT Training
arxiv_id: '2410.23922'
source_url: https://arxiv.org/abs/2410.23922
tags:
- learning
- warmup
- training
- rate
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Learning rate warmup is a commonly used technique in training neural
  networks, particularly with large batch sizes, to stabilize training and allow for
  higher learning rates. This work explores the reasons behind the need for warmup
  and proposes methods to reduce or eliminate it.
---

# Analyzing & Reducing the Need for Learning Rate Warmup in GPT Training

## Quick Facts
- **arXiv ID**: 2410.23922
- **Source URL**: https://arxiv.org/abs/2410.23922
- **Reference count**: 40
- **Primary result**: Learning rate warmup can be significantly reduced or eliminated by controlling angular updates and using SNR-based scaling factors

## Executive Summary
Learning rate warmup is a standard technique in large-scale neural network training, particularly for transformers with large batch sizes, to stabilize early training and prevent divergence. This work systematically investigates why warmup is necessary by identifying three key mechanisms: momentum bias correction artifacts in Adam, disproportionate early weight updates relative to initial weights, and highly correlated gradients limiting effective batch size. The authors propose optimizer modifications that normalize updates based on angular change and signal-to-noise ratio, demonstrating that warmup can be largely eliminated, especially when combined with high momentum values and inverse bias correction.

## Method Summary
The authors propose modifying the optimizer to normalize unscaled updates based on different metrics to reduce the need for warmup. They explore three metrics: ℓ2-norm of updates, resulting directional change (angular update), and impact on network representations (relative representation change). The rotational optimizer variant, which controls angular updates, significantly reduces warmup requirements. Additionally, they propose a scaling factor based on the signal-to-noise ratio of gradients to mitigate large representation changes, acting as an automatic warmup mechanism. These methods are validated on GPT-2 training with AdamW and Lion optimizers.

## Key Results
- Momentum bias correction in Adam leads to artificially large initial updates that warmup helps mitigate
- Controlling angular updates through rotational optimization significantly reduces the need for warmup
- SNR-based scaling factors can act as automatic warmup, reducing or eliminating explicit warmup phases
- High momentum values combined with inverse bias correction enable efficient training without warmup

## Why This Works (Mechanism)
Learning rate warmup serves to limit the overall size of weight updates early in training when initial unscaled updates are large compared to weight magnitudes. The authors identify three key issues: (1) momentum bias correction in Adam can cause artificially large initial updates, (2) early updates are disproportionately large compared to initial weight magnitudes, and (3) early gradients are highly correlated, reducing effective mini-batch size. By normalizing updates based on angular change and signal-to-noise ratio, these problematic early dynamics are mitigated, reducing the need for explicit warmup schedules.

## Foundational Learning

**Momentum Bias Correction**: Why needed - Prevents bias in early momentum estimates; Quick check - Compare training stability with/without bias correction in early steps.

**Angular Updates**: Why needed - Controls direction of parameter changes independent of magnitude; Quick check - Measure cosine similarity between consecutive update directions.

**Signal-to-Noise Ratio (SNR)**: Why needed - Quantifies gradient quality relative to noise; Quick check - Compute SNR as ||mean gradient|| / ||gradient - mean gradient||.

## Architecture Onboarding

**Component Map**: Optimizer (Adam/Lion) -> Update Normalization (Angular/SNR) -> Weight Updates -> Model Parameters

**Critical Path**: Gradient computation → Momentum calculation → Update normalization (angular or SNR) → Parameter update → Forward pass

**Design Tradeoffs**: Angular control vs. magnitude control - angular provides better stability but requires more computation; SNR scaling is automatic but may be overly conservative in noisy regimes.

**Failure Signatures**: Training instability without warmup, exploding gradients, poor convergence, or degraded model quality.

**3 First Experiments**:
1. Train a single fully-connected layer with modified optimizer variants
2. Compare angular vs. ℓ2-norm normalization on small transformer
3. Test SNR-based scaling with varying momentum values

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis based heavily on synthetic experiments and single-layer networks, which may not capture full transformer complexity
- Proposed Adam modifications require significant implementation changes needing further validation
- Focus primarily on Adam optimizer family, unclear how findings generalize to other optimizers

## Confidence

- **High Confidence**: Momentum bias correction causes artificially large initial updates, supported by both theory and experiments
- **Medium Confidence**: Angular update control significantly reduces warmup needs, demonstrated on GPT-2 but needs broader validation
- **Medium Confidence**: SNR-based scaling acts as automatic warmup, though interaction with training dynamics requires further study

## Next Checks

1. Test rotational optimizer and SNR scaling on diverse transformer architectures (BERT, ViT, Llama) to verify generalizability beyond GPT-style models.

2. Implement and validate proposed optimizer modifications in large-scale distributed training environments to assess practical viability and numerical stability.

3. Evaluate whether angular updates and SNR scaling insights apply to non-Adam optimizers like SGD with momentum and newer adaptive methods.