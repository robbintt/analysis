---
ver: rpa2
title: 'LLM as a Scorer: The Impact of Output Order on Dialogue Evaluation'
arxiv_id: '2406.02863'
source_url: https://arxiv.org/abs/2406.02863
tags:
- json
- output
- score
- prompt
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how prompt design affects dialogue evaluation
  using large language models (LLMs). The researchers experimented with six different
  output instruction configurations that varied the sequence of presenting scores
  and reasons.
---

# LLM as a Scorer: The Impact of Output Order on Dialogue Evaluation

## Quick Facts
- arXiv ID: 2406.02863
- Source URL: https://arxiv.org/abs/2406.02863
- Authors: Yi-Pei Chen; KuanChao Chu; Hideki Nakayama
- Reference count: 4
- Key outcome: Reason-first prompting produces higher and more comprehensive LLM dialogue evaluation scores compared to score-first approaches

## Executive Summary
This study investigates how prompt design affects dialogue evaluation using large language models (LLMs). The researchers experimented with six different output instruction configurations that varied the sequence of presenting scores and reasons. Using four GPT models (gpt-3.5-turbo-0613, gpt-3.5-turbo-1106, gpt-4-0613, and gpt-4-1106) to evaluate 25 sets of dialogues, they found that the "reason-first" approach (presenting reasons before scores) produced significantly higher and more comprehensive scores compared to the reverse order. For example, gpt-4-0613 scored an average of 5.34 in the reason-first JSON format versus 3.26 in the score-first format, despite providing similar reasoning. The effect was amplified when task-specific rules were included in the prompt, demonstrating LLMs' sensitivity to prompt structure. These findings highlight the importance of output instruction order in designing effective prompts for subjective evaluation tasks.

## Method Summary
The researchers evaluated 25 sets of LLM-generated dialogues using four different GPT models. Each dialogue set contained 4-6 dialogues with identifiable issues like repetition or contradictions. They tested six prompt configurations varying the order of score and reason instructions (score-first vs reason-first) in both example and JSON formats. The prompts included five task-specific rules about prioritizing issue quantity over impact and weighting significant issues. Each configuration was run 10 times per model to collect mean scores and standard deviations for comparison.

## Key Results
- Reason-first prompting (reasons before scores) produced significantly higher scores than score-first approaches across all four GPT models tested
- The effect was amplified when task-specific rules were included in the prompts, with scores becoming lower and less distinct when rules were removed
- Different GPT model versions showed varying sensitivity to prompt structure changes, with some models exhibiting more pronounced score differences between configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The order of output instructions (score-first vs reason-first) directly affects the LLM's scoring behavior through the autoregressive generation process.
- Mechanism: When reasons are generated before scores, the autoregressive model can condition the score on the reasons already produced, creating a coherent evaluation. When scores come first, the model lacks this contextual reference and may produce scores that are less aligned with the subsequent reasoning.
- Core assumption: The LLM's autoregressive nature means later outputs can condition on earlier ones, and this sequential dependency influences evaluation outcomes.
- Evidence anchors:
  - [abstract] "We found that the order of presenting reasons and scores significantly influences LLMs' scoring, with a 'reason-first' approach yielding more comprehensive evaluations."
  - [section] "Considering the sequential generation nature of autoregressive models, placing the score after the reasons allows it to reference both the reasons and the input prompt, a dynamic not possible when this order is reversed."
  - [corpus] Weak evidence - related papers mention prompt output sequencing but don't specifically address autoregressive conditioning effects.
- Break condition: This mechanism breaks when the model uses parallel generation or when the prompt structure explicitly separates reasoning and scoring as independent tasks.

### Mechanism 2
- Claim: Task-specific rules in the prompt amplify the effect of output instruction order on scoring.
- Mechanism: The special rules provide explicit evaluation criteria that the model attempts to follow. When reasons are generated first, the model can better align its reasoning with these rules before producing the final score. Without the rules, the model relies more on general judgment patterns, reducing the order effect.
- Core assumption: The model treats task-specific rules as constraints that must be satisfied in the evaluation process.
- Evidence anchors:
  - [abstract] "The effect was amplified when task-specific rules were included in the prompt"
  - [section] "when we removed the 'special rules' from the prompt, we found that most scores were lower and the distinctions between different settings became less pronounced"
  - [corpus] Weak evidence - no direct corpus support for rule-amplification hypothesis.
- Break condition: This mechanism breaks when the model ignores explicit rules or when rules are too vague to influence the evaluation process.

### Mechanism 3
- Claim: Different GPT model versions exhibit varying sensitivity to prompt structure changes, particularly in how they process output instruction order.
- Mechanism: Model architecture differences (training data, fine-tuning objectives, parameter counts) create different sensitivities to prompt formatting. Some models may be more rigid in their evaluation patterns while others adapt more readily to instruction sequencing.
- Core assumption: Model version differences translate to meaningful behavioral variations in subjective evaluation tasks.
- Evidence anchors:
  - [abstract] "Using four GPT models (gpt-3.5-turbo-0613, gpt-3.5-turbo-1106, gpt-4-0613, and gpt-4-1106)" and showing different scoring patterns
  - [section] "We selected four recent LLMs to serve as scorers" with varying results
  - [corpus] Moderate evidence - related paper "Evaluating Scoring Bias in LLM-as-a-Judge" suggests model-specific evaluation behaviors exist.
- Break condition: This mechanism breaks when all models converge to similar behaviors or when evaluation tasks are objective rather than subjective.

## Foundational Learning

- Concept: Autoregressive generation in language models
  - Why needed here: Understanding how sequential output generation affects scoring when reasons precede scores
  - Quick check question: If an LLM generates text token by token, how might the order of instructions in a prompt influence the final output?

- Concept: Prompt engineering principles
  - Why needed here: The study demonstrates how subtle prompt variations (instruction order, inclusion of rules) significantly impact model behavior
  - Quick check question: What prompt engineering techniques could you use to ensure consistent scoring across different LLM versions?

- Concept: Subjective evaluation in NLP
  - Why needed here: Dialogue evaluation is inherently subjective, requiring understanding of how LLMs handle qualitative assessments
  - Quick check question: How does the subjectivity of dialogue evaluation differ from objective tasks like code correctness, and why does this matter for prompt design?

## Architecture Onboarding

- Component map: Dialogue set → Prompt configuration (order, rules) → LLM scoring → Statistical analysis of score distributions → Comparative analysis across models and configurations
- Critical path: Dialogue set → Prompt configuration (order, rules) → LLM scoring → Statistical analysis of score distributions → Comparative analysis across models and configurations
- Design tradeoffs: Using multiple models provides robustness but increases complexity; including rules improves alignment but may reduce generalizability; reason-first ordering improves coherence but may introduce bias
- Failure signatures: Inconsistent scores across trials suggest prompt instability; minimal score variation between configurations suggests model insensitivity; large standard deviations indicate unreliable evaluation
- First 3 experiments:
  1. Test single dialogue set with all six configurations on one model to establish baseline score distribution patterns
  2. Remove task-specific rules from prompts to measure amplification effect on score variation
  3. Compare score consistency across models using identical prompts to identify version-specific sensitivities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different autoregressive model architectures (beyond GPT) respond to output instruction order in subjective evaluation tasks?
- Basis in paper: [inferred] The paper demonstrates that GPT models show sensitivity to output instruction order due to their autoregressive nature, but doesn't test other model architectures.
- Why unresolved: The study only tested GPT models, leaving open whether this phenomenon is specific to GPT's architecture or common across autoregressive models.
- What evidence would resolve it: Testing the same prompt variations with models like Claude, LLaMA, or other autoregressive architectures to compare sensitivity to output order.

### Open Question 2
- Question: What is the optimal balance between task-specific rules and model autonomy in subjective evaluation prompts?
- Basis in paper: [explicit] The authors note that including special rules significantly affected scoring distributions, with scores becoming lower and less distinct when rules were removed.
- Why unresolved: The study only tested with and without rules, not exploring intermediate levels of rule specification or their impact on different types of subjective tasks.
- What evidence would resolve it: Systematic experiments varying the number and specificity of task rules across different subjective evaluation tasks to determine optimal rule complexity.

### Open Question 3
- Question: How does the order of output instructions affect the consistency and reliability of LLM evaluations over multiple trials?
- Basis in paper: [inferred] The paper shows that "reason-first" approach yields higher scores, but doesn't extensively analyze trial-to-trial consistency or reliability metrics.
- Why unresolved: The study focused on mean scores and standard deviations but didn't examine reliability coefficients or inter-trial agreement across different output instruction orders.
- What evidence would resolve it: Computing reliability metrics (e.g., Cronbach's alpha, intraclass correlation) for each output instruction configuration across multiple trials and different dialogue sets.

## Limitations
- The evaluation was conducted on a relatively small sample of 25 dialogue sets from a single source, limiting generalizability
- The study focused exclusively on GPT models, leaving open questions about whether similar effects would be observed with other LLM architectures
- The study demonstrates the "what" but provides limited empirical validation of the "why" - particularly regarding how autoregressive generation specifically influences the observed effects

## Confidence
- Primary finding (reason-first produces higher scores): High
- Mechanism explanations (autoregressive conditioning): Medium
- Task rules amplification effect: Medium

## Next Checks
1. **Cross-domain generalization test**: Apply the same output order variations to evaluate other subjective tasks (e.g., essay grading, code review) to determine if the effect is specific to dialogue evaluation or represents a broader prompt engineering principle.

2. **Ablation study on task rules**: Systematically remove individual rules from the prompt while maintaining the reason-first order to quantify each rule's contribution to the amplification effect, rather than the binary presence/absence approach used in the current study.

3. **Human evaluation comparison**: Conduct blind human evaluations of the same dialogue sets using both scoring approaches to establish whether the reason-first LLM scores better align with human judgment or simply represent a different evaluation paradigm.