---
ver: rpa2
title: Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology
  Images
arxiv_id: '2409.11552'
source_url: https://arxiv.org/abs/2409.11552
tags:
- segmentation
- axon
- myelin
- data
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-domain deep learning model for axon
  and myelin segmentation in histology images, addressing the challenge of domain-specific
  models that fail on out-of-distribution data. The authors aggregate data from multiple
  imaging modalities (bright field, electron microscopy, Raman spectroscopy) and species
  (mouse, rat, rabbit, human) to train a generalist segmentation model.
---

# Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images

## Quick Facts
- **arXiv ID**: 2409.11552
- **Source URL**: https://arxiv.org/abs/2409.11552
- **Reference count**: 36
- **Primary result**: Multi-domain deep learning model outperforms single-modality dedicated learners for axon and myelin segmentation in histology images (p=0.03077)

## Executive Summary
This paper addresses the challenge of domain-specific models failing on out-of-distribution data in histology image segmentation. The authors propose a multi-domain deep learning approach that aggregates data from multiple imaging modalities (bright field, electron microscopy, Raman spectroscopy) and species (mouse, rat, rabbit, human). By training a generalist segmentation model on this diverse dataset, they demonstrate improved performance on both in-distribution and out-of-distribution test images compared to single-modality dedicated learners. The model is resolution-agnostic and is integrated into the open-source AxonDeepSeg software ecosystem for practical use by neuroscience researchers.

## Method Summary
The method uses a U-Net architecture based on the nnUNet framework with 5-fold cross-validation training for 1000 epochs. The key innovation is training on an aggregated dataset containing multiple imaging modalities (TEM, SEM, BF, CARS) and species (mouse, rat, rabbit, human, macaque, dog) without resampling to a common resolution. This "resolution-agnostic" approach forces the model to learn scale-invariant representations. The authors compare intra-modality aggregation (BF datasets only) with inter-modality aggregation (all modalities) against dedicated single-modality models. Model checkpoints are selected based on validation performance and final predictions are generated through ensemble averaging of the 5-fold models.

## Key Results
- Multi-domain model outperforms single-modality dedicated learners on in-distribution data (p=0.03077)
- Better generalization on out-of-distribution test datasets not seen during training
- Resolution-agnostic approach successfully generalizes across 2 orders of magnitude in scale
- Simplified maintenance compared to multiple domain-specific models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-domain aggregation enables the model to learn invariant features across different imaging modalities and species.
- Mechanism: By training on data from multiple imaging modalities (bright field, electron microscopy, Raman spectroscopy) and species (mouse, rat, rabbit, human), the model is exposed to a wider variety of visual patterns and must learn to identify axons and myelin regardless of these differences.
- Core assumption: The visual features of axons and myelin are consistent enough across domains to be learned by a single model.
- Evidence anchors:
  - [abstract] "We aggregate data from multiple imaging modalities (bright field, electron microscopy, Raman spectroscopy) and species (mouse, rat, rabbit, human)"
  - [section] "Our proposed general model produces better segmentation than single-modality learners on in-distribution and out-of-distribution images"
  - [corpus] Weak evidence - corpus contains related histology segmentation papers but no direct evidence for multi-domain invariant feature learning
- Break condition: If the visual features of axons and myelin are too inconsistent across domains, the model may not learn meaningful invariant features and performance will degrade.

### Mechanism 2
- Claim: Resolution-agnostic training allows the model to generalize across different image resolutions without explicit resampling.
- Mechanism: The model is trained on images of varying resolutions without converting them to a common scale, forcing it to learn scale-invariant representations of axons and myelin.
- Core assumption: The model's capacity is sufficient to learn representations that generalize across the 2 orders of magnitude in resolution present in the data.
- Evidence anchors:
  - [section] "Our proposed model is thus resolution-ignorant, as opposed to having a fixed resolution... but we claim its capacity is more than sufficient to efficiently generalize across scales."
  - [section] "We expect the generalist model trained on the full aggregation FULL_AGG to learn an even more abstract representation of the structures of interest compared to dedicated single-modality models."
  - [corpus] No direct evidence in corpus for resolution-agnostic training benefits
- Break condition: If the model's capacity is insufficient to learn scale-invariant features, performance will degrade when applied to images at resolutions very different from those in the training set.

### Mechanism 3
- Claim: Aggregation reduces the impact of inter-rater variability in annotations.
- Mechanism: By training on data annotated by multiple annotators with different levels of expertise, the model learns to generalize across different annotation styles rather than overfitting to a single annotator's approach.
- Core assumption: The different annotation styles contain enough shared signal about the true axon and myelin boundaries that a model can learn from them collectively.
- Evidence anchors:
  - [section] "We would thus expect some level of inter-rater (and even intra-rater) variability in annotation quality... Our data aggregation strategy mitigates this bias."
  - [section] "For example, a model trained on annotations with over-segmented myelin consistently reproduces this artifact in predictions, whereas a model trained on data coming from many different annotators will benefit from alternative interpretations of the data"
  - [corpus] No direct evidence in corpus for inter-rater variability mitigation through aggregation
- Break condition: If the annotation styles are too inconsistent or contain conflicting information, the aggregated model may learn to reproduce the worst aspects of each style rather than improving generalization.

## Foundational Learning

- Concept: Domain generalization in deep learning
  - Why needed here: The model must perform well on data from domains not seen during training (out-of-distribution generalization)
  - Quick check question: What is the difference between domain adaptation and domain generalization?

- Concept: Transfer learning across imaging modalities
  - Why needed here: The model needs to leverage knowledge from one imaging modality to improve performance on another
  - Quick check question: How does training on multiple modalities potentially improve performance on each individual modality?

- Concept: Multi-task learning for segmentation
  - Why needed here: The model is effectively learning to segment two related but distinct classes (axon and myelin) across diverse conditions
  - Quick check question: What are the potential benefits and drawbacks of learning multiple related segmentation tasks simultaneously?

## Architecture Onboarding

- Component map: U-Net architecture -> nnUNet framework -> 5-fold cross-validation -> Ensemble predictions
- Critical path: Data preprocessing → Model training (1000 epochs) → Model checkpoint selection → Ensemble prediction
- Design tradeoffs: Resolution-agnostic approach vs. fixed resolution (simpler deployment vs. potential performance trade-off)
- Failure signatures: Poor performance on out-of-distribution data, overfitting to training domains, failure to generalize across scales
- First 3 experiments:
  1. Train dedicated models on each dataset and compare performance to the aggregated model
  2. Test the model on out-of-distribution data not seen during training
  3. Evaluate the impact of different aggregation schemes (intra-modality vs. inter-modality)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does inter-rater variability in manual annotations affect the performance of the multi-domain model, and can this be quantified?
- Basis in paper: [inferred] The paper mentions that multiple annotators with varying expertise were involved over a decade, suggesting potential inter-rater variability, but this is not characterized in the study.
- Why unresolved: The paper does not provide a detailed analysis of how annotation inconsistencies impact model performance, nor does it quantify the variability in annotations.
- What evidence would resolve it: Conducting a study that measures the performance of the model using annotations from different annotators, and analyzing the impact of these variations on segmentation accuracy.

### Open Question 2
- Question: Can the generalist model be fine-tuned for specific domains to achieve better performance than dedicated models?
- Basis in paper: [explicit] The paper states that the generalist model can be fine-tuned for better performance on specific domains.
- Why unresolved: The paper does not provide empirical results comparing the performance of a fine-tuned generalist model to dedicated models on specific domains.
- What evidence would resolve it: Conducting experiments to fine-tune the generalist model on specific domains and comparing its performance to that of dedicated models trained on the same domains.

### Open Question 3
- Question: How does the inclusion of additional imaging modalities or species affect the performance of the multi-domain model?
- Basis in paper: [inferred] The paper aggregates data from multiple modalities and species, but does not explore the impact of including more diverse data on model performance.
- Why unresolved: The paper does not investigate how the addition of more imaging modalities or species would influence the model's ability to generalize and perform on unseen data.
- What evidence would resolve it: Expanding the dataset to include more imaging modalities and species, then evaluating the performance of the multi-domain model on these additional data to assess improvements in generalization and accuracy.

## Limitations

- Limited dataset diversity: Only 1 CARS dataset and 3 BF datasets, raising questions about true cross-domain learning versus overfitting to specific datasets
- Unknown annotation quality: Lack of quantitative measures of annotation consistency and inter-rater agreement
- Resolution-agnostic claims need validation: Theoretical plausibility without comprehensive empirical validation across the full resolution spectrum

## Confidence

- High confidence: Basic claim that multi-domain models can outperform single-domain models on in-distribution data
- Medium confidence: Claims about out-of-distribution generalization, limited by small number of test datasets
- Medium confidence: Resolution-agnostic generalization claim, theoretically plausible but lacking comprehensive validation

## Next Checks

1. Test on systematically varied resolutions: Evaluate the model's performance across the full 2 orders of magnitude in resolution using a grid of test images with known ground truth

2. Quantify annotation consistency: Measure inter-rater agreement and intra-rater reproducibility across annotated datasets to provide empirical evidence for annotation variability impact

3. Expand out-of-distribution testing: Systematically test the model on a broader range of unseen imaging conditions (different stains, imaging protocols, tissue preparation methods) to validate true domain generalization