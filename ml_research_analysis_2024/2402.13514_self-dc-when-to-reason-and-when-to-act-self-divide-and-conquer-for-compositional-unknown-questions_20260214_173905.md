---
ver: rpa2
title: 'Self-DC: When to Reason and When to Act? Self Divide-and-Conquer for Compositional
  Unknown Questions'
arxiv_id: '2402.13514'
source_url: https://arxiv.org/abs/2402.13514
tags:
- question
- answer
- questions
- unknown
- known
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of answering compositional unknown
  questions using large language models (LLMs), which require combining both internal
  reasoning and external knowledge retrieval. The authors propose a novel Self Divide-and-Conquer
  (Self-DC) framework that dynamically selects between using internal knowledge and
  retrieving external knowledge based on the model's confidence score.
---

# Self-DC: When to Reason and When to Act? Self Divide-and-Conquer for Compositional Unknown Questions

## Quick Facts
- **arXiv ID**: 2402.13514
- **Source URL**: https://arxiv.org/abs/2402.13514
- **Reference count**: 23
- **Primary result**: Proposed Self-DC framework dynamically selects between internal reasoning and external retrieval, achieving comparable performance with significantly fewer retrieval calls on compositional unknown questions

## Executive Summary
This paper addresses the challenge of answering compositional unknown questions using large language models (LLMs), which require combining both internal reasoning and external knowledge retrieval. The authors propose a novel Self Divide-and-Conquer (Self-DC) framework that dynamically selects between using internal knowledge and retrieving external knowledge based on the model's confidence score. The framework iteratively decomposes complex questions into sub-questions and combines the answers to generate the final response. Experiments on two datasets (CuQA and FreshQA) demonstrate that Self-DC achieves comparable or better performance with significantly fewer external retrieval calls compared to strong baselines, highlighting its effectiveness and efficiency in handling compositional unknown questions.

## Method Summary
The Self Divide-and-Conquer (Self-DC) framework introduces a novel approach to answering compositional unknown questions by dynamically deciding when to rely on internal LLM reasoning versus when to retrieve external knowledge. The method operates through a confidence-based selection mechanism where the model first attempts to answer using its internal knowledge, generates a confidence score, and only initiates external retrieval if the confidence falls below a predefined threshold. For complex questions requiring compositional reasoning, Self-DC employs an iterative decomposition strategy that breaks down questions into sub-questions, answers each component (using either internal knowledge or retrieval as appropriate), and then combines these answers to form the final response. This approach is evaluated across two datasets designed to test both compositional reasoning capabilities and the ability to handle questions requiring up-to-date external knowledge.

## Key Results
- Self-DC achieves comparable performance to strong baselines on CuQA and FreshQA datasets while using significantly fewer external retrieval calls
- The confidence-based retrieval selection mechanism effectively reduces unnecessary external knowledge lookups
- Iterative decomposition approach successfully handles compositional questions by breaking them down into manageable sub-components

## Why This Works (Mechanism)
Self-DC works by recognizing that not all parts of compositional questions require external knowledge retrieval. The framework leverages the LLM's internal knowledge for questions it can confidently answer while only invoking the more expensive retrieval process when necessary. This selective approach is particularly effective because many compositional questions contain sub-questions that can be answered from the model's pre-trained knowledge. The iterative decomposition allows the system to handle complex reasoning tasks by breaking them into simpler components, each of which can be addressed using the most appropriate knowledge source. The confidence score mechanism acts as a gatekeeper, preventing unnecessary retrieval calls that would increase computational cost without improving answer quality.

## Foundational Learning
- **Compositional question answering**: The ability to answer questions requiring multiple reasoning steps or knowledge sources - needed because real-world questions often require combining information from different domains
- **Confidence scoring in LLMs**: Methods for quantifying model certainty in its responses - needed to determine when external retrieval is necessary versus when internal knowledge suffices
- **Knowledge retrieval systems**: Mechanisms for accessing external information sources - needed to supplement LLM capabilities for questions requiring current or specialized knowledge
- **Iterative decomposition**: Breaking complex problems into simpler sub-problems - needed to handle compositional questions that are too complex to answer directly
- **Dynamic decision-making**: Systems that can adapt their approach based on contextual factors - needed to optimize the trade-off between reasoning and retrieval
- **Few-shot prompting**: Using limited examples to guide model behavior - needed to effectively implement the decomposition and confidence assessment components

## Architecture Onboarding

**Component map**: Question Input -> Confidence Assessment -> Internal Reasoning (if confident) OR External Retrieval (if not confident) -> Iterative Decomposition (for compositional questions) -> Sub-question Answering -> Answer Combination -> Final Response

**Critical path**: Question Input -> Confidence Assessment -> Decision Point (Internal vs External) -> Answer Generation -> Final Response

**Design tradeoffs**: The framework trades potential retrieval accuracy for efficiency by limiting external calls, versus the risk of missing critical information when confidence scores are miscalibrated. The decomposition approach adds computational overhead but enables handling of questions too complex for direct answering.

**Failure signatures**: 
- Overly conservative confidence thresholds leading to excessive retrieval calls
- Decomposition errors causing cascading failures in compositional questions
- Mismatched sub-question answers failing to combine coherently
- Over-reliance on internal knowledge when external facts are required

**First 3 experiments**:
1. Test confidence score calibration across different question types to establish appropriate threshold values
2. Evaluate decomposition effectiveness on questions of varying complexity levels
3. Measure retrieval call reduction while maintaining answer quality across diverse question domains

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to two datasets (CuQA and FreshQA) which may not represent the full diversity of real-world compositional questions
- Confidence-based retrieval selection relies on potentially unreliable LLM-generated confidence scores, particularly for edge cases
- Iterative decomposition could compound errors if initial sub-question breakdown is suboptimal
- Computational overhead of iterative reasoning process not thoroughly compared to direct retrieval approaches

## Confidence

**High**: Effectiveness of Self-DC in reducing retrieval calls while maintaining comparable performance, supported by quantitative experimental results

**Medium**: Generalizability of the approach across diverse question types and domains, given the limited dataset scope

**Low**: Robustness of the confidence score mechanism under varying conditions, requiring further empirical validation

## Next Checks
1. Conduct experiments on additional diverse datasets covering different domains and question types to assess the generalizability of Self-DC beyond CuQA and FreshQA

2. Perform ablation studies specifically testing the confidence score threshold mechanism, including analysis of false positive/negative rates in retrieval selection decisions across different question complexities

3. Evaluate the computational efficiency trade-offs by measuring end-to-end inference time and resource utilization of Self-DC compared to baseline approaches, including analysis of how decomposition depth affects performance and efficiency