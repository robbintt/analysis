---
ver: rpa2
title: 'Two Heads Are Better Than One: Averaging along Fine-Tuning to Improve Targeted
  Transferability'
arxiv_id: '2412.20807'
source_url: https://arxiv.org/abs/2412.20807
tags:
- targeted
- attack
- fine-tuning
- transferability
- logit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving targeted adversarial
  attack transferability by proposing a method that averages adversarial examples
  along the fine-tuning trajectory. The core idea is that averaging over the fine-tuning
  trajectory pulls the crafted adversarial examples towards a more centered region
  of the loss surface, thereby enhancing their transferability.
---

# Two Heads Are Better Than One: Averaging along Fine-Tuning to Improve Targeted Transferability

## Quick Facts
- **arXiv ID**: 2412.20807
- **Source URL**: https://arxiv.org/abs/2412.20807
- **Authors**: Hui Zeng; Sanshuai Cui; Biwei Chen; Anjie Peng
- **Reference count**: 40
- **Primary result**: Averaging along fine-tuning trajectory improves targeted adversarial transferability, with CE+AaF achieving 50.3% success rate vs 11.3% for baseline when transferring from DenseNet121 to VGG16

## Executive Summary
This paper addresses the challenge of improving targeted adversarial attack transferability by proposing a method that averages adversarial examples along the fine-tuning trajectory. The core insight is that fine-tuning trajectories tend to oscillate around the periphery of flat regions in the loss surface, and temporal averaging can pull crafted adversarial examples toward more centered, stable regions. The proposed Averaging along Fine-tuning (AaF) method significantly improves targeted transferability across various surrogate-victim model pairs with negligible computational overhead.

## Method Summary
The AaF method generates adversarial examples by first creating baseline examples using state-of-the-art targeted attacks, then fine-tuning them using feature-space optimization with combined aggregate gradients. During fine-tuning, the method averages adversarial examples across the optimization trajectory using a decaying factor Î³, which pulls examples toward a more centered region of the loss surface. The approach integrates with multiple attack methods (CE, Logit, Margin, SH, SU) and is evaluated across different surrogate-victim model pairs with perturbation budgets of Îµ = 8 and 16.

## Key Results
- Averaging along fine-tuning trajectory significantly improves targeted transferability compared to endpoint-only fine-tuning
- The method achieves up to 4x improvement in success rates (50.3% vs 11.3%) when transferring from DenseNet121 to VGG16
- AaF shows consistent improvements across multiple attack methods and surrogate-victim pairs
- The approach maintains negligible computational overhead while providing substantial performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Averaging adversarial examples along the fine-tuning trajectory improves transferability by pulling examples toward the center of a flatter loss surface region.
- Mechanism: Temporal averaging of high-quality snapshots during fine-tuning smooths out oscillations at the periphery of the loss surface, creating adversarial examples that lie in a more central, stable region.
- Core assumption: The fine-tuning trajectory oscillates around the periphery of a flat region of the loss surface, and averaging will pull examples toward a more centered position.
- Evidence anchors:
  - [abstract] "Noting that the vanilla fine-tuning trajectory tends to oscillate around the periphery of a flat region of the loss surface, we propose averaging over the fine-tuning trajectory to pull the crafted AE towards a more centered region."
  - [section] "We hypothesize and empirically validate that by temporal averaging, the crafted AE is calibrated towards a more centered region of the loss surface (Fig. 1), thus exhibiting stronger transferability."
  - [corpus] Weak evidence - no corpus papers directly address averaging along fine-tuning trajectories for adversarial examples.
- Break condition: If the fine-tuning trajectory does not oscillate around a flat region, or if averaging causes the example to move away from the target class region.

### Mechanism 2
- Claim: Feature-space fine-tuning with combined aggregate gradients enhances targeted transferability by simultaneously encouraging target-class features and suppressing original-class features.
- Mechanism: The combined aggregate gradient (difference between target and original class feature importance) guides the optimization to push features toward the target class while suppressing features of the original class.
- Core assumption: The combined aggregate gradient effectively captures the feature importance differences needed for targeted attacks.
- Evidence anchors:
  - [section] "Following [11], the aggregate gradients âˆ†Ì…ð‘˜ð¼â€²,ð‘¡ and âˆ†Ì…ð‘˜ð¼,ð‘œ are combined, as in (1), to push the subsequent optimization closer to ð‘¦ð‘¡ and away from ð‘¦ð‘œ."
  - [section] "The aggregate gradient âˆ†Ì…ð‘˜ð¼â€²,ð‘¡ is calculated as: âˆ†Ì…ð‘˜ð¼â€²,ð‘¡= ðº(1/ð¶ âˆ‘ âˆ†ð‘˜ð¼â€²âŠ™ð‘€ð‘›,ð‘¡)ð‘›"
  - [corpus] Moderate evidence - feature-space fine-tuning methods are referenced in corpus papers like "Enhancing targeted transferability via feature space fine-tuning" (FMR 0.6215).
- Break condition: If the feature importance calculation does not accurately represent class-related features, or if the combined gradient does not effectively balance target and original class suppression.

### Mechanism 3
- Claim: Fine-tuning against a slightly robust surrogate model improves transferability by exposing the adversarial example to more diverse decision boundaries.
- Mechanism: Training a robust model introduces non-smooth decision boundaries that, when used as a surrogate, helps craft adversarial examples that transfer better to other models.
- Core assumption: Adversarial training of the surrogate creates decision boundaries that expose the adversarial example to more diverse scenarios.
- Evidence anchors:
  - [section] "The AEs' transferability is evaluated on models that have not been used as surrogates. In Sec. IV. C, an adversarially trained model Res50adv serves as the surrogate."
  - [section] "Table II presents the targeted transferability in this scenario. AEs crafted against a slightly robust model indeed show stronger transferability in most cases."
  - [corpus] Moderate evidence - papers like "Safety Alignment Should Be Made More Than Just A Few Attention Heads" (FMR 0.5639) discuss robustness but not specifically for transferability.
- Break condition: If the robust surrogate does not provide diverse enough decision boundaries, or if the improvement from robustness is negligible compared to other factors.

## Foundational Learning

- Concept: Adversarial examples and their transferability
  - Why needed here: Understanding how adversarial examples can fool models and transfer between different architectures is fundamental to grasping the paper's contribution.
  - Quick check question: What is the difference between targeted and untargeted adversarial attacks?

- Concept: Feature-space fine-tuning
  - Why needed here: The paper builds on feature-space fine-tuning methods, so understanding how feature importance is calculated and used for optimization is crucial.
  - Quick check question: How does feature-space fine-tuning differ from pixel-space fine-tuning?

- Concept: Temporal averaging in optimization
  - Why needed here: The core contribution involves averaging along the fine-tuning trajectory, so understanding how temporal averaging works in optimization contexts is important.
  - Quick check question: In what scenarios has temporal averaging been shown to improve model performance in machine learning?

## Architecture Onboarding

- Component map:
  Baseline attack generation (CE, Logit, Margin, SH, SU) -> Feature importance calculation (aggregate gradients with masks and smoothing) -> Fine-tuning optimization with combined gradients -> Temporal averaging along fine-tuning trajectory -> Evaluation across multiple surrogate-victim pairs

- Critical path:
  1. Generate baseline adversarial example
  2. Calculate feature importance for target and original classes
  3. Combine gradients for fine-tuning optimization
  4. Perform fine-tuning with temporal averaging
  5. Evaluate transferability on hold-out models

- Design tradeoffs:
  - Memory vs. accuracy: Storing multiple fine-tuning snapshots vs. simple endpoint averaging
  - Computational cost vs. transferability: More fine-tuning iterations improve results but increase cost
  - Generalization vs. specificity: Using patch-wise masks vs. pixel-wise masks for feature importance

- Failure signatures:
  - Poor transferability despite fine-tuning (indicates averaging not effective or feature importance calculation flawed)
  - Computational overhead becoming prohibitive (indicates need to optimize memory usage or reduce iterations)
  - Oscillations not being smoothed by averaging (indicates trajectory not following expected pattern)

- First 3 experiments:
  1. Compare vanilla FFT (endpoint only) vs. AaF with varying Î³ values on a simple dataset
  2. Test feature importance calculation with different mask types (patch-wise vs. pixel-wise)
  3. Evaluate transferability improvement when using robust vs. standard surrogate models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the decaying factor Î³ in the averaging along fine-tuning (AaF) method affect the transferability of adversarial examples across different architectures?
- Basis in paper: [explicit] The paper discusses the effect of the decaying factor Î³ on transferability, noting that Î³ âˆˆ [0.4, 0.8] is generally effective but varies with the surrogate model.
- Why unresolved: The paper does not provide a comprehensive study on the optimal choice of Î³ for different surrogate models or attack scenarios.
- What evidence would resolve it: Empirical results comparing the effectiveness of different Î³ values across various surrogate models and attack scenarios.

### Open Question 2
- Question: What is the impact of using patch-wise masks versus pixel-wise masks on the effectiveness of the AaF method?
- Basis in paper: [explicit] The paper mentions using a patch-wise mask as proposed in [28], but does not compare it to pixel-wise masks.
- Why unresolved: The paper does not provide a direct comparison between patch-wise and pixel-wise masks in the context of AaF.
- What evidence would resolve it: Comparative experiments showing the performance of AaF with both patch-wise and pixel-wise masks.

### Open Question 3
- Question: How does the proposed AaF method perform when applied to adversarial examples crafted against transformer-based models?
- Basis in paper: [explicit] The paper discusses the transferability of adversarial examples to transformer-based models but does not focus on the performance of AaF specifically.
- Why unresolved: The paper does not provide detailed results on the effectiveness of AaF when transferring from CNNs to transformers.
- What evidence would resolve it: Detailed experimental results comparing the performance of AaF on transformer-based models with other fine-tuning schemes.

## Limitations

- The theoretical justification for why averaging along fine-tuning trajectories improves transferability relies on the assumption of oscillation around flat regions, but lacks rigorous mathematical analysis of the loss surface topology.
- The computational overhead claims of "negligible overhead" may not hold for larger models or longer fine-tuning schedules, as storing multiple fine-tuning snapshots adds memory costs.
- The method's performance on architectures substantially different from those tested (very deep/shallow networks or different design philosophies) remains uncertain.

## Confidence

**High Confidence**: The empirical results demonstrating improved targeted transferability using the AaF method are well-supported by the experimental data across multiple attack methods, surrogate-victim pairs, and perturbation budgets.

**Medium Confidence**: The proposed mechanism explaining why averaging improves transferability (pulling examples toward a more centered region of the loss surface) is plausible and supported by experimental observations, but lacks rigorous theoretical justification.

**Low Confidence**: The claim that averaging along fine-tuning trajectories provides a generalizable solution across different attack methods and model architectures is supported by the presented experiments but may not hold for all possible configurations.

## Next Checks

1. **Loss Surface Analysis**: Perform visualization or analysis of the loss surface along fine-tuning trajectories for different surrogate models to empirically verify the oscillation behavior around flat regions.

2. **Memory-Computation Trade-off Study**: Conduct a systematic evaluation of the memory and computational costs of the AaF method across different fine-tuning schedules and model sizes, quantifying overhead at various Î³ values and warm-up iterations.

3. **Architecture Generalization Test**: Evaluate the AaF method on a broader range of model architectures including very deep networks (e.g., ResNet-200), very shallow networks, and transformers with varying depths to test generalizability claims.