---
ver: rpa2
title: Large Language Models' Detection of Political Orientation in Newspapers
arxiv_id: '2406.00018'
source_url: https://arxiv.org/abs/2406.00018
tags:
- articles
- newspapers
- llms
- https
- political
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study assessed consistency among four LLMs (ChatGPT-3.5, ChatGPT-4,
  Gemini Pro, Gemini Pro 1.5) in detecting political orientation of 1,000 articles
  from 40 global newspapers using a two-dimensional economic (left-right) and democratic
  (libertarian-authoritarian) scale. Results showed extreme inconsistency: each LLM
  produced a distinct distribution of newspaper positions, with no agreement across
  models.'
---

# Large Language Models' Detection of Political Orientation in Newspapers

## Quick Facts
- arXiv ID: 2406.00018
- Source URL: https://arxiv.org/abs/2406.00018
- Reference count: 40
- One-line primary result: Four leading LLMs showed extreme inconsistency in detecting political orientation of global newspaper articles, failing to reproduce known political spectra.

## Executive Summary
This study evaluates the consistency of four leading LLMs (ChatGPT-3.5, ChatGPT-4, Gemini Pro, Gemini Pro 1.5) in detecting political orientation of 1,000 articles from 40 global newspapers using a two-dimensional economic (left-right) and democratic (libertarian-authoritarian) scale. The results revealed stark inconsistency: each model produced a distinct distribution of newspaper positions with no agreement across models. ChatGPT-4 overwhelmingly clustered at [0,0] (centrist), while Gemini Pro showed a dichotomy between [0,0] and [-10,-10] (far-left libertarian). Variability analysis confirmed ChatGPT-4 had the lowest standard deviation in scores, Gemini Pro the highest. The LLMs failed to replicate the known political spectrum of the dataset, indicating high volatility and lack of cross-model reliability for this task.

## Method Summary
The study scraped 1,000 articles from 40 global newspapers (5 articles/day/newspaper over 5 days in May 2024), filtering for length (1000-5000 characters). Each article was queried with an identical prompt specifying a two-dimensional political orientation scale (Economic: -10 left to 10 right; Democracy: -10 libertarian to 10 authoritarian) and strict output format [Economic, Democracy]. No fine-tuning was applied; default model settings were used via APIs. Outputs were parsed to extract [x,y] coordinates, and analysis focused on cross-model agreement (distribution of newspaper mean positions), intra-model volatility (standard deviation within newspapers), and comparison to known political spectra.

## Key Results
- Extreme inconsistency across models: each LLM produced a distinct distribution of newspaper positions with no cross-model agreement
- ChatGPT-4 overwhelmingly neutral judgment ([0,0] clustering), while Gemini Pro showed dichotomous judgment between centrism and far-left
- Variability analysis confirmed ChatGPT-4 had the lowest standard deviation in scores, Gemini Pro the highest
- LLMs failed to reproduce the known political spectrum of the dataset, indicating high volatility and lack of reliability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Divergent training data distributions and objectives across LLMs lead to inconsistent ideological mapping.
- **Mechanism:** Each model's fine-tuning (RLHF, safety layers) and base corpus encode different implicit biases, causing them to parse the same textual cues (e.g., framing, word choice) through distinct ideological lenses. This results in non-overlapping score distributions.
- **Core assumption:** Political orientation detection relies on learned associations between linguistic patterns and abstract ideological axes, which are not uniformly represented across training regimens.
- **Evidence anchors:**
  - [abstract] "hinting to inconsistent training or excessive randomness in the algorithms."
  - [section II.B] "Training Data Bias: LLMs are trained on large datasets that may contain inherent biases."
  - [corpus] Corpus signals show adjacent work on "ideological divergence" and "political bias in Western media," suggesting this is a recognized, unresolved issue.
- **Break condition:** If all models were trained on identical, ideologically balanced corpora with uniform alignment objectives, cross-model consistency would be expected.

### Mechanism 2
- **Claim:** Prompt interpretation variance, even with identical instructions, causes divergent output scaling and anchoring.
- **Mechanism:** Models differ in how they internalize scale anchors (e.g., "Economic Left -10") and the imperative "ALWAYS provide the result." This leads to different calibrations: some models (Gemini Pro) dichotomize into extreme bins, while others (ChatGPT-4) collapse toward a neutral default ([0,0]).
- **Core assumption:** The prompt's abstract scales are underspecified; models fill gaps using their internal "common sense" about political spectra, which varies.
- **Evidence anchors:**
  - [section III.D] The prompt used imperative phrasing ("NEVER...ALWAYS...") after iterative testing to enforce output format, yet responses still varied drastically.
  - [section IV] "ChatGPT-4...overwhelmingly neutral judgment...Gemini Pro...dichotomic judgement between centrism and far-left."
- **Break condition:** If prompts were enriched with multiple, concrete exemplars for each scale point (e.g., "A -10 economic score is like [example quote]"), model outputs might converge.

### Mechanism 3
- **Claim:** Architectural and scaling differences (context window, parameter count, mixture-of-experts) affect volatility and extremeness in multi-dimensional scoring.
- **Mechanism:** Models with larger context windows (Gemini Pro 1.5) or different architectures may integrate longer-range contextual cues differently, leading to more dispersed or authoritarian-right leanings. Smaller or more heavily safety-tuned models (ChatGPT-4) may exhibit "centrist collapse" as a conservative default to avoid controversy.
- **Core assumption:** The two-dimensional scoring task requires sustained, nuanced textual reasoning; model capacity and design choices directly impact performance stability.
- **Evidence anchors:**
  - [section III.C] Details architectural differences: ChatGPT-4 vs. 3.5 (parameter scale), Gemini Pro 1.5 (1M token context, mixture-of-experts).
  - [section IV] Variability analysis: "ChatGPT-4 had the lowest standard deviation...Gemini Pro the highest."
- **Break condition:** If models of vastly different architectures were trained to convergence on a massive, curated political orientation dataset, their volatility profiles might align.

## Foundational Learning

- **Concept: Political Compass (Two-Dimensional) Scale**
  - **Why needed here:** The study's entire framework relies on mapping articles to economic (left-right) and social (libertarian-authoritarian) axes. Misunderstanding this scale (e.g., conflating economic left with social libertarian) invalidates interpretation of model outputs.
  - **Quick check question:** If an article criticizes both corporate power *and* state surveillance, what quadrant might it occupy, and why could an LLM misclassify it?

- **Concept: LLM Prompt Engineering & Output Parsing**
  - **Why needed here:** The fixed prompt is the sole independent variable. Understanding how phrasing, constraints ("NEVER write any text"), and lack of exemplars influence model behavior is key to diagnosing inconsistency.
  - **Quick check question:** Why might instructing a model to "Output only [x,y]" still yield varied interpretations of what numbers represent?

- **Concept: Model Calibration vs. Accuracy**
  - **Why needed here:** The paper measures *consistency* (low variance) and *distributional match* to expected political spectrum, not ground-truth accuracy. A model can be highly consistent (low std dev) but systematically wrong (e.g., ChatGPT-4's [0,0] clustering).
  - **Quick check question:** Can a model have a lower standard deviation than another but be less useful for detecting political bias? Why?

## Architecture Onboarding

- **Component map:** Article scraper (BeautifulSoup + `newspaper3k`) -> filtered text (1000-5000 chars) -> LLM query layer (Model-specific API wrapper with injected prompt) -> output parsing (regex/string extraction of [x,y]) -> analysis layer (aggregation, heatmap generation, standard deviation calculation)
- **Critical path:** Scrape -> Filter -> Query -> Parse -> Aggregate -> Variability Analysis. A failure at parsing (e.g., LLM adds extra text) breaks the entire pipeline.
- **Design tradeoffs:**
  - **Cost vs. Depth:** ChatGPT-4 was 20x more expensive than 3.5 but produced less volatile (yet overly centrist) outputs. Gemini Pro 1.5's long context was hindered by free-tier quotas.
  - **Default vs. Custom:** Using chat APIs with no fine-tuning maximizes generalizability but sacrifices task-specific calibration.
  - **Fixed Prompt vs. Adaptive:** A single prompt simplifies comparison but cannot account for model-specific "understanding" of the scales.
- **Failure signatures:**
  - **Centrist collapse:** >80% of outputs are [0,0] (seen in ChatGPT-4).
  - **Dichotomous output:** Bimodal distribution at extreme opposites (e.g., [0,0] and [-10,-10] in Gemini Pro).
  - **High intra-newspaper std dev:** Same newspaper's articles span >5 points on a scale, indicating random-like assignment.
- **First 3 experiments:**
  1. **Replication sanity check:** Query a single, politically clear article (e.g., a known Fox News editorial) 10x per model. Expect: low variance within model, but high variance *across* models.
  2. **Prompt sensitivity test:** Slightly rephrase the scale definitions (e.g., change "Economic Left" to " socialist") and measure distribution shift. This isolates prompt interpretation as a variable.
  3. **Ground-truth probe:** Use 20 articles with *human-validated* compass scores (from literature or manual annotation). Compute each model's error (distance from human score). This separates consistency from accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLM-generated political orientation assessments align with human expert judgments for individual newspaper articles?
- Basis in paper: [explicit] The paper explicitly identifies this as the second key question ("whether the positioning that is automatically generated conforms with some 'ground truth', assessed by experts") but notes it was not addressed because the 1,000 articles were not singularly reviewed by humans. The authors call for human validation through their NAVAI platform.
- Why unresolved: The study only compared cross-LLM consistency without a human benchmark, making it impossible to determine if any model's output is accurate rather than merely internally consistent.
- What evidence would resolve it: A large-scale, multidisciplinary human annotation effort where experts rate a subset of the articles (or new ones) on the same compass scales, followed by statistical comparison (e.g., correlation, agreement metrics) between human and LLM scores.

### Open Question 2
- Question: What specific factors in LLM training, architecture, or prompting explain the observed extreme inconsistency across models in political orientation detection?
- Basis in paper: [inferred] The paper observes starkly different distributions (e.g., ChatGPT-4 clustering at [0,0] vs. Gemini Pro dichotomizing) and variable volatility. It speculates that this hints at "inconsistent training or excessive randomness" but does not investigate the root causes.
- Why unresolved: The study design compares outputs but does not manipulate or analyze model internals, training data, or prompt sensitivity, leaving the mechanisms behind the inconsistency unknown.
- What evidence would resolve it: Controlled experiments isolating variables (e.g., testing prompt variations, analyzing model confidence scores, comparing performance on curated datasets with known bias, or probing training data for political skew), potentially augmented by model diagnostics.

### Open Question 3
- Question: Can LLMs reliably detect longitudinal shifts in a newspaper's political orientation over time, and if so, do they converge on a consistent narrative of such shifts?
- Basis in paper: [explicit] The "Future work" section explicitly proposes evaluating "articles from the same newspapers but spanning multiple years" to explore "potential shifts in the average political opinions" and "trajectories of news media."
- Why unresolved: The study provides only a static, cross-sectional snapshot (May 2024). It does not test whether LLMs can track temporal dynamics, which is crucial for understanding media evolution.
- What evidence would resolve it: Applying the same LLM evaluation pipeline to a multi-year corpus of articles from selected newspapers, then analyzing whether the temporal trends in LLM scores align with known historical events or editorial changes, and whether different LLMs produce consistent trend lines.

### Open Question 4
- Question: Does incorporating a wider range of LLM families (e.g., open-source models like Llama3, or models from different regions) reduce or amplify the observed cross-model inconsistency?
- Basis in paper: [explicit] The first item in "Future work" calls for "incorporating and evaluating more models, particularly local ones such as Llama3" to broaden analysis.
- Why unresolved: The study only tested two model families (OpenAI ChatGPT, Google Gemini) and their immediate predecessors. The pattern of inconsistency might be universal or specific to these commercial models.
- What evidence would resolve it: Replicating the study with a diverse set of LLMs (varying in size, training data, and developer), then quantitatively comparing the dispersion and distribution of their compass scores to see if inconsistency is a general property or if certain models/mitigations yield more reliable, human-like distributions.

## Limitations
- Absence of ground-truth labels for article political orientation makes it impossible to distinguish between models being "wrong" versus the dataset containing genuinely ambiguous content
- Prompt's abstract scales rely entirely on each model's internal interpretation of political terminology, which may vary dramatically
- Use of default API settings without temperature control or explicit formatting constraints introduces variability that could amplify observed inconsistencies

## Confidence
- **High Confidence:** The observed lack of cross-model consistency is real and reproducible. The extreme clustering patterns (ChatGPT-4 at [0,0], Gemini Pro's dichotomy) are clearly documented and unlikely to be artifacts.
- **Medium Confidence:** The attribution of inconsistency to training data bias and architectural differences is plausible but not definitively proven. Alternative explanations (e.g., prompt ambiguity, token limits affecting context understanding) remain untested.
- **Low Confidence:** Claims about specific models being "better" or "worse" at this task are premature without ground-truth validation or controlled ablation studies on prompt design.

## Next Checks
1. **Ground-truth validation:** Manually annotate 50 articles with political compass scores and compute each model's error distribution to distinguish consistency from accuracy
2. **Prompt sensitivity analysis:** Test variations of the prompt (e.g., adding concrete examples for each scale point, changing temperature settings) to isolate the impact of instruction phrasing on output consistency
3. **Intra-model replication study:** Query the same article 100 times per model with identical settings to measure inherent randomness versus systematic bias in each LLM's political orientation detection