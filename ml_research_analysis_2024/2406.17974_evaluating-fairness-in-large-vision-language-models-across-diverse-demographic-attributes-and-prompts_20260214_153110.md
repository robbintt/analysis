---
ver: rpa2
title: Evaluating Fairness in Large Vision-Language Models Across Diverse Demographic
  Attributes and Prompts
arxiv_id: '2406.17974'
source_url: https://arxiv.org/abs/2406.17974
tags:
- fairness
- lvlms
- prompts
- demographic
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for evaluating visual fairness
  in large vision-language models (LVLMs) across diverse demographic attributes including
  gender, skin tone, and age. The framework uses direct and single-choice question
  prompts to systematically audit model performance disparities using the FACET benchmark
  dataset.
---

# Evaluating Fairness in Large Vision-Language Models Across Diverse Demographic Attributes and Prompts

## Quick Facts
- arXiv ID: 2406.17974
- Source URL: https://arxiv.org/abs/2406.17974
- Reference count: 16
- Primary result: Novel framework evaluates LVLM fairness across gender, skin tone, and age using direct and single-choice prompts, revealing demographic biases in both open and closed-source models

## Executive Summary
This paper introduces a comprehensive framework for evaluating visual fairness in large vision-language models (LVLMs) across diverse demographic attributes. The framework systematically audits model performance disparities using the FACET benchmark dataset with direct and single-choice question prompts. Experimental results demonstrate that both open-source and closed-source LVLMs exhibit significant fairness issues across different prompts and demographic groups, showing biased outcomes in gender stereotypes, skin tone preferences, and age-related disparities. The study proposes a multi-modal Chain-of-Thought based strategy for unfairness mitigation while highlighting the need for fairness-aware evaluation in LVLM development.

## Method Summary
The framework evaluates LVLM fairness using the FACET dataset with 21,560 images and demographic annotations across gender, skin tone, and age. Two prompt types are employed: direct questions that elicit open-ended responses and single-choice questions that constrain responses to predefined categories. LVLMs are tested in zero-shot prompting mode without fine-tuning, using CLIP and T5 text encoders to format outputs into label vectors. Fairness is measured through recall disparity calculations between demographic groups, with the proposed multi-modal Chain-of-Thought strategy serving as a mitigation approach for identified biases.

## Key Results
- LVLMs show systematic biases favoring lighter skin tones over darker ones across both open and closed-source models
- Younger individuals receive more favorable predictions than older individuals in age-based evaluations
- Closed-source models like GPT-4o and Gemini 1.5 Pro demonstrate higher accuracy but continue to exhibit demographic biases despite improved performance
- Single-choice prompts achieve higher recall performance than direct questions but show different bias patterns in fairness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt design affects fairness disparity measurement in LVLMs
- Mechanism: Direct question prompts elicit more stereotypical responses while single-choice prompts constrain responses to predefined categories, reducing disparity scores
- Core assumption: The structure and constraints of prompts influence how models represent demographic attributes
- Evidence anchors: Direct comparison of recall and disparity metrics across prompt types shows systematic differences in demographic attribute representation

### Mechanism 2
- Claim: Model parameter count correlates with recall performance but not fairness metrics
- Mechanism: Larger models (34B vs 7B) show improved accuracy in understanding images but continue to exhibit similar demographic biases
- Core assumption: Increasing model capacity primarily improves general visual understanding rather than fairness awareness
- Evidence anchors: LLaVA-1.6 (34B) shows significant improvements in recall performance over traditional models but maintains similar fairness disparities

### Mechanism 3
- Claim: Demographic attribute representation in LVLMs follows skin tone and age hierarchies
- Mechanism: Models consistently show preference for lighter skin tones and younger individuals across both open and closed-source variants
- Core assumption: Training data and societal biases create systematic preference hierarchies that LVLMs learn and reproduce
- Evidence anchors: Direct question prompts demonstrate clear preference hierarchies for both skin tone and age groups across all evaluated models

## Foundational Learning

- Concept: Fairness evaluation metrics (recall disparity)
  - Why needed here: The paper uses recall disparity as the primary fairness metric to quantify performance differences across demographic groups
  - Quick check question: How does the paper calculate fairness disparity between two demographic groups?

- Concept: Zero-shot prompting methodology
  - Why needed here: The evaluation framework relies on zero-shot prompting without model fine-tuning to assess inherent model biases
  - Quick check question: What distinguishes zero-shot prompting from few-shot or fine-tuned approaches in this evaluation?

- Concept: Demographic attribute categorization
  - Why needed here: Understanding how the paper defines and groups demographic attributes (gender, skin tone, age) is crucial for interpreting results
  - Quick check question: How does the paper categorize skin tone using the Monk Skin Tone Scale?

## Architecture Onboarding

- Component map: FACET dataset -> demographic attribute extraction -> image preprocessing -> prompt generation -> model inference -> output encoding -> evaluation metrics
- Critical path: FACET image -> prompt selection -> model inference -> result encoding -> fairness calculation
- Design tradeoffs: Single-choice prompts improve consistency but may limit model expression; direct prompts provide richer responses but increase variance
- Failure signatures: High "unknown" responses indicate insufficient model confidence; large disparity scores indicate systematic bias
- First 3 experiments:
  1. Compare recall and disparity metrics across different prompt types using the same image set
  2. Measure performance differences between open-source and closed-source models on gender attributes
  3. Evaluate skin tone preference hierarchies by testing light vs medium vs dark skin tone groups

## Open Questions the Paper Calls Out

- How do LVLMs perform in fairness evaluations when tested across additional demographic attributes beyond gender, skin tone, and age, such as race or disability status? The paper acknowledges that the focus on gender, skin tone, and age may not cover other critical demographic factors, requiring broader evaluation.

- How does the fairness performance of LVLMs change when evaluated using different cultural contexts or geographical regions? The study does not address how cultural or regional differences might influence fairness, which is crucial for understanding global applicability.

- What specific mitigation strategies, beyond the proposed multi-modal Chain-of-Thought approach, can be employed to address fairness disparities in LVLMs? While the paper introduces a potential mitigation strategy, it does not exhaustively explore other methods that could be effective in reducing fairness disparities.

## Limitations

- The evaluation relies heavily on the FACET dataset's demographic annotations, which may contain inherent biases or incomplete representations
- The study does not provide detailed information about encoder functions used to format model outputs, potentially affecting fairness calculations
- Only a limited set of LVLMs and prompt types are evaluated, potentially missing other sources of bias or mitigation strategies

## Confidence

- **High Confidence:** Closed-source LVLMs (GPT-4o, Gemini 1.5 Pro) show demographic biases despite higher accuracy
- **Medium Confidence:** Prompt structure (direct vs single-choice) affects fairness disparity measurements
- **Low Confidence:** Model parameter count correlates with recall but not fairness metrics, due to limited model comparisons

## Next Checks

1. Implement and test the exact encoder functions described in the paper to ensure proper formatting of LVLM outputs and accurate fairness calculations
2. Cross-validate the FACET dataset's demographic annotations with independent human annotators to assess annotation quality and potential biases
3. Test additional LVLMs beyond those included in the original study to verify if observed bias patterns generalize across different model architectures and training approaches