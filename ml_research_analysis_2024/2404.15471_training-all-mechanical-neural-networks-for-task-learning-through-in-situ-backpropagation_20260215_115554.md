---
ver: rpa2
title: Training all-mechanical neural networks for task learning through in situ backpropagation
arxiv_id: '2404.15471'
source_url: https://arxiv.org/abs/2404.15471
tags:
- mnns
- learning
- networks
- training
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a mechanical analogue of in situ backpropagation
  to efficiently train mechanical neural networks (MNNs). Unlike optical networks,
  MNNs remain underdeveloped due to heavy computational demands and reliance on approximate
  gradients.
---

# Training all-mechanical neural networks for task learning through in situ backpropagation

## Quick Facts
- arXiv ID: 2404.15471
- Source URL: https://arxiv.org/abs/2404.15471
- Reference count: 40
- One-line primary result: Introduces mechanical analogue of in situ backpropagation enabling exact gradient computation in MNNs without computer aid, demonstrating high accuracy in regression and classification tasks

## Executive Summary
This study introduces a mechanical analogue of in situ backpropagation to efficiently train mechanical neural networks (MNNs). Unlike optical networks, MNNs remain underdeveloped due to heavy computational demands and reliance on approximate gradients. The authors show that exact gradients can be obtained locally in MNNs by measuring bond elongations in two steps—forward and adjoint loading—without computer aid. Experiments with 3D-printed MNNs demonstrate high accuracy in behavior learning, linear regression, and Iris flower classification. The method outperforms finite difference in both speed and precision. Additionally, the study demonstrates retrainability after task-switching and damage, showcasing resilience. These results bridge mechanics and machine learning, enabling autonomous, adaptive material systems and smart robots.

## Method Summary
The method implements an in situ backpropagation algorithm derived from the adjoint variable method for mechanical systems. Two loading steps are performed: forward loading applies input forces and measures displacements and bond elongations, while adjoint loading applies adjoint forces derived from loss gradients and measures a second set of bond elongations. The gradient of the loss function is computed as the element-wise multiplication of forward and adjoint bond elongations. These gradients update spring constants through gradient descent with Adam optimization. The approach is validated experimentally using 3D-printed MNNs made of Agilus30 flexible material, achieving high accuracy in regression and classification tasks while demonstrating resilience through retraining after damage.

## Key Results
- Demonstrates exact gradient computation in MNNs using adjoint variable method without computer aid
- Achieves high accuracy in regression and classification tasks with 3D-printed MNNs
- Shows successful retrainability after task-switching and damage, with classification accuracy recovering to ~80% after damage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exact gradients can be obtained locally in MNNs without computer aid by measuring bond elongations in two steps—forward and adjoint loading.
- Mechanism: The gradient of the loss function is the element-wise multiplication of elongations from the forward and adjoint problems. This uses the adjoint variable method to derive $\nabla L = e_{adj} \circ e$, where $e$ is bond elongation from the forward problem and $e_{adj}$ is from the adjoint problem.
- Core assumption: The MNN operates in the linear regime where the stiffness matrix $D$ is constant and the relationship between displacement and bond elongation is linear.
- Evidence anchors:
  - [abstract] "exact gradients can be obtained locally in MNNs by measuring bond elongations in two steps—forward and adjoint loading—without computer aid."
  - [section] "The gradient of the loss function L equals to the element-wise multiplication of elongations of the bonds in the forward problem and the adjoint problem."
  - [corpus] No direct evidence about exact gradient measurement in physical systems, though optical networks use similar adjoint methods.
- Break condition: If the system exhibits nonlinear behavior or significant geometric nonlinearity, the linear relationship breaks down and the adjoint method no longer yields exact gradients.

### Mechanism 2
- Claim: MNNs can be trained to perform classification and regression tasks by updating spring constants using the locally obtained gradient.
- Mechanism: The gradient information from the two-step measurement is used to update spring constants iteratively through gradient descent: $k_i \leftarrow k_i - \alpha e_{adj,i} \circ e_i$. This updates the material properties to minimize the loss function.
- Core assumption: The loss function is differentiable and the gradient descent converges to a local minimum that achieves the desired task performance.
- Evidence anchors:
  - [abstract] "showcase the successful training of MNNs for behavior learning and machine learning tasks, achieving high accuracy in regression and classification."
  - [section] "This gradient $\nabla L_i$, obtained locally at all bonds $i$ via these two steps described above, is used to update the spring constants at learning rate $\alpha$, from $k_i$ to $k_i - \alpha \nabla L_i$"
  - [corpus] No direct evidence of mechanical networks performing classification tasks, though optical networks use similar training methods.
- Break condition: If the learning rate is too high, the system may overshoot minima and fail to converge; if too low, training becomes impractically slow.

### Mechanism 3
- Claim: MNNs can be retrained after task-switching and damage, demonstrating resilience.
- Mechanism: The same gradient-based training method can be applied to a pre-trained network starting from new initial conditions (task-switching) or after removing/repairing bonds (damage recovery). The network adapts to new tasks or recovers functionality through iterative training.
- Core assumption: The network topology remains mechanically stable after damage and the loss landscape allows for recovery or task adaptation.
- Evidence anchors:
  - [abstract] "present the retrainability of MNNs involving task-switching and damage, demonstrating the resilience."
  - [section] "the retrainability of the MNNs after damage occurs... the classification accuracy rebounds to around 80%, indicating a substantial recovery"
  - [corpus] No direct evidence of physical networks being retrained after damage, though the concept exists in computational networks.
- Break condition: If damage removes critical bonds that make the network mechanically unstable or changes the topology fundamentally, retraining may not recover the original functionality.

## Foundational Learning

- Concept: Adjoint variable method for gradient computation
  - Why needed here: This is the mathematical foundation that enables exact gradient computation from local measurements without global information
  - Quick check question: How does the adjoint problem relate to the forward problem in terms of the stiffness matrix and boundary conditions?

- Concept: Static equilibrium and compatibility in mechanical systems
  - Why needed here: Understanding how forces relate to displacements (equilibrium) and how displacements relate to deformations (compatibility) is essential for implementing the forward and adjoint problems
  - Quick check question: What is the relationship between the stiffness matrix D, the compatibility matrix C, and the diagonal spring constant matrix K?

- Concept: Gradient descent optimization
  - Why needed here: This is the algorithm used to iteratively update the spring constants to minimize the loss function based on the locally computed gradients
  - Quick check question: What factors determine the learning rate α, and how would you choose it for a mechanical system versus a computational one?

## Architecture Onboarding

- Component map: Input forces → MNN (nodes connected by springs) → Boundary conditions → Displacement measurements → Bond elongation calculations → Gradient computation → Spring constant updates
- Critical path: Forward loading → displacement measurement → loss function evaluation → adjoint loading → second displacement measurement → gradient computation → spring constant update → repeat until convergence
- Design tradeoffs: Linear regime operation enables exact gradient computation but limits the complexity of learnable functions; mechanical implementation provides physical interpretability but introduces manufacturing constraints on bond dimensions
- Failure signatures: Poor agreement between simulated and experimental elongations indicates measurement/calibration issues; failure to converge suggests inappropriate learning rate or loss landscape issues; mechanical instability indicates critical bond removal
- First 3 experiments:
  1. Validate the two-step gradient measurement by comparing experimental gradients with simulated exact gradients for a simple MNN under known loading conditions
  2. Train a simple MNN to learn a single displacement target and verify the learned configuration produces the desired output
  3. Test retrainability by damaging a trained MNN and retraining to recover original functionality, comparing pre-damage and post-recovery performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the training performance of MNNs scale with network size and dimensionality, particularly for high-dimensional tasks?
- Basis in paper: [inferred] The paper demonstrates successful training on relatively small 2D MNNs for specific tasks but does not explore scaling behavior or high-dimensional applications.
- Why unresolved: The study focuses on proof-of-concept experiments with small 2D networks. Scaling to larger, higher-dimensional networks would likely introduce new challenges in terms of gradient computation, material properties, and fabrication constraints.
- What evidence would resolve it: Systematic experimental and numerical studies of MNN performance across different network sizes, dimensionalities, and task complexities would clarify scaling properties.

### Open Question 2
- Question: What is the theoretical limit of MNN learning accuracy compared to computer-based neural networks, and what factors determine this limit?
- Basis in paper: [inferred] The paper achieves high accuracy on specific tasks but does not establish theoretical bounds or compare performance to conventional networks across a broader range of problems.
- Why unresolved: While the paper demonstrates practical success, it doesn't address fundamental limitations of MNNs compared to digital alternatives or explore what determines the accuracy ceiling.
- What evidence would resolve it: Comparative studies across diverse machine learning tasks, coupled with theoretical analysis of information capacity and noise effects in mechanical systems, would establish performance limits.

### Open Question 3
- Question: How can nonlinear behaviors be incorporated into MNNs to enable learning of complex, nonlinear functions beyond linear regression and classification?
- Basis in paper: [explicit] The paper states: "our current in situ backpropagation method is constrained within the linear regime of MNNs, leading to applications primarily focused on linear regression and linear classifier. Therefore, exploring the nonlinear regime of MNNs by using nonlinear materials and geometric nonlinearity presents an opportunity to tackle nonlinear datasets and tasks."
- Why unresolved: The current implementation relies on linear elasticity, limiting the types of tasks MNNs can perform. Incorporating nonlinear material properties or geometric effects would expand capabilities but introduces significant theoretical and experimental challenges.
- What evidence would resolve it: Development and experimental validation of MNN architectures using nonlinear materials or geometric nonlinearities, demonstrating successful learning of nonlinear functions beyond current capabilities.

## Limitations
- Current method limited to linear elastic regimes, constraining applicability to nonlinear mechanical behaviors
- Physical implementation introduces manufacturing tolerances, measurement noise, and potential drift over repeated loading cycles
- Scalability to large-scale networks remains unproven with limited validation of robustness to significant damage

## Confidence

- **High Confidence**: The mathematical framework for computing exact gradients using adjoint methods is sound and well-established in computational mechanics.
- **Medium Confidence**: Experimental demonstration of training for simple regression and classification tasks, though with limited complexity and network size.
- **Medium Confidence**: Retrainability after minor damage, but the extent of recoverable damage and long-term reliability require further validation.

## Next Checks

1. **Nonlinear Regime Testing**: Systematically increase loading amplitudes to identify the threshold where linear assumptions break down and quantify the impact on gradient accuracy and training stability.

2. **Large-Scale Network Scaling**: Fabricate and train MNNs with 10× more bonds than current demonstrations to assess computational complexity, measurement accuracy, and convergence behavior at scale.

3. **Repeated Loading Durability**: Conduct fatigue testing with thousands of forward-adjoint loading cycles to measure drift in material properties, measurement accuracy, and training performance over time.