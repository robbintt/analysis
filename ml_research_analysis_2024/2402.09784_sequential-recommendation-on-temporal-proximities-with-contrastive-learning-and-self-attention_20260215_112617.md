---
ver: rpa2
title: Sequential Recommendation on Temporal Proximities with Contrastive Learning
  and Self-Attention
arxiv_id: '2402.09784'
source_url: https://arxiv.org/abs/2402.09784
tags:
- temporal
- time
- items
- item
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of sequential recommendation by
  proposing TemProxRec, a model that considers both vertical and horizontal temporal
  proximities in user-item interactions. TemProxRec incorporates a Temporal-proximity-aware
  Contrastive Learning (TCL) method to capture vertical temporal proximity across
  users and a Multi-Head Absolute-Relative (MHAR) attention mechanism to model horizontal
  temporal proximity within user sequences.
---

# Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention

## Quick Facts
- arXiv ID: 2402.09784
- Source URL: https://arxiv.org/abs/2402.09784
- Reference count: 36
- Primary result: TemProxRec achieves 2.43% and 5.6% improvements in HR@10 and NDCG@10 over state-of-the-art baselines

## Executive Summary
This paper addresses sequential recommendation by proposing TemProxRec, a model that captures both vertical temporal proximity across users and horizontal temporal proximity within user sequences. The model incorporates Temporal-proximity-aware Contrastive Learning (TCL) to learn item representations based on temporal proximity across users, and Multi-Head Absolute-Relative (MHAR) attention to model temporal and positional contexts within sequences. Experiments on four benchmark datasets demonstrate consistent improvements over state-of-the-art methods, with average gains of 2.43% in HR@10 and 5.6% in NDCG@10.

## Method Summary
TemProxRec is a sequential recommendation model that captures temporal proximities through two main components: TCL for vertical temporal proximity across users and MHAR attention for horizontal temporal proximity within user sequences. The model is trained using a composite loss function combining masked language modeling (MLM) for sequential pattern prediction and TCL for temporal proximity learning. The architecture uses four attention heads (absolute time, absolute position, relative time, relative position) to encode different aspects of temporal context, which are combined with item embeddings to create attention weights. The model is trained on user-item interaction sequences with timestamps from benchmark datasets.

## Key Results
- TemProxRec achieves average improvements of 2.43% and 5.6% in HR@10 and NDCG@10 respectively over state-of-the-art baselines
- TCL improves performance across all baseline models in ablation studies
- MHAR attention outperforms single-head attention variants by capturing both absolute and relative temporal contexts
- Model demonstrates robustness across different datasets with varying temporal characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TCL captures vertical temporal proximity by aligning representations of items selected by different users within similar time windows
- Mechanism: TCL samples positive pairs based on temporal proximity (items selected within a predefined time window) and uses contrastive loss to pull representations closer while pushing negative pairs apart
- Core assumption: Items selected by multiple users within similar timeframes share meaningful contextual similarity
- Evidence anchors: Abstract states TCL learns item representations considering vertical temporal proximity; section 3.4 describes TCL sampling contrastive pairs based on temporal proximity
- Break condition: Poorly chosen time window Δ may sample irrelevant positive pairs that don't reflect genuine user behavior patterns

### Mechanism 2
- Claim: MHAR attention captures horizontal temporal proximity by encoding both absolute and relative temporal contexts within user sequences
- Mechanism: Four separate attention heads (absolute time, absolute position, relative time, relative position) encode different aspects of temporal context, combined with item embeddings to create attention weights
- Core assumption: Distinguishing between items purchased within a day versus a month is crucial for understanding user preferences
- Evidence anchors: Abstract mentions self-attention encodes temporal and positional contexts; section 3.3.2 describes relative time and position embeddings
- Break condition: Clipping values (kt, kp) set too low may lose important temporal distinctions; too high may cause computational inefficiency

### Mechanism 3
- Claim: Joint optimization of MLM and TCL tasks enables simultaneous capture of sequential patterns and temporal proximity
- Mechanism: Composite loss function L = L_MLM + λL_TCL balances sequential pattern prediction through MLM with temporal proximity learning through TCL
- Core assumption: Sequential patterns and temporal proximity are complementary signals for understanding user behavior
- Evidence anchors: Section 3.5 describes dual training with MLM and TCL tasks; section 4.3.2 shows TCL task improves performance across all models
- Break condition: λ set too high may overfit to temporal proximity at expense of sequential patterns; too low may make TCL benefits negligible

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To learn item representations capturing semantic similarity based on temporal proximity across users
  - Quick check question: What is the difference between supervised contrastive learning and standard contrastive learning?

- Concept: Multi-head attention with relative positional encodings
  - Why needed here: To distinguish between items based on their temporal spacing within sequences
  - Quick check question: How do relative positional encodings differ from absolute positional encodings in transformer models?

- Concept: Masked language modeling (MLM)
  - Why needed here: To predict masked items in sequences, capturing sequential dependencies
  - Quick check question: Why is MLM used instead of standard next-item prediction in this sequential recommendation context?

## Architecture Onboarding

- Component map: Input embedding → MHAR attention layers → Output representations → MLM head + TCL head
- Critical path: MHAR attention transforms input embeddings into representations that capture temporal context; these representations feed into both MLM prediction task and TCL contrastive task
- Design tradeoffs: Using four separate attention heads for temporal contexts increases parameter count but provides more granular temporal modeling; using a time window for TCL introduces a hyperparameter that must be tuned per dataset
- Failure signatures: Poor performance on datasets with different temporal characteristics (e.g., if Δ is poorly chosen); overfitting on small datasets due to increased complexity
- First 3 experiments:
  1. Test TCL with different time window radii (Δ) on a single dataset to find optimal value
  2. Compare MHAR attention variants (with/without relative embeddings) to assess their contribution
  3. Test the joint loss weighting (λ) to find the optimal balance between MLM and TCL tasks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the main text.

## Limitations
- Limited detail on exact preprocessing steps for Steam dataset affects reproducibility
- TCL time window radius Δ identified as hyperparameter but optimal values not specified for each dataset
- Initialization method for TemProxRec parameters beyond basic normal distribution not fully specified

## Confidence
- **High confidence** in overall methodology and architectural design based on clear mathematical formulations and consistent experimental results
- **Medium confidence** in TCL mechanism effectiveness due to limited ablation study details and potential sensitivity to time window selection
- **Medium confidence** in MHAR attention mechanism due to complex interactions between absolute and relative temporal embeddings that aren't fully explored

## Next Checks
1. Test TCL with different time window radii (Δ) on a single dataset to determine optimal values and assess sensitivity to this hyperparameter
2. Implement ablation studies comparing MHAR attention variants (with/without relative embeddings) to isolate their individual contributions
3. Conduct experiments on datasets with varying temporal characteristics (e.g., daily vs. monthly purchase patterns) to test model robustness across different temporal proximities