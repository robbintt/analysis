---
ver: rpa2
title: 'PROC2PDDL: Open-Domain Planning Representations from Texts'
arxiv_id: '2403.00092'
source_url: https://arxiv.org/abs/2403.00092
tags:
- water
- action
- parameters
- actions
- pddl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PROC2PDDL, the first open-domain dataset
  pairing procedural texts with expert-annotated PDDL representations for text-based
  planning. The authors formulate the task of action modeling as predicting action
  definitions (parameters, preconditions, effects) given a domain header and text,
  and evaluate this intrinsically and extrinsically using a PDDL solver.
---

# PROC2PDDL: Open-Domain Planning Representations from Texts

## Quick Facts
- arXiv ID: 2403.00092
- Source URL: https://arxiv.org/abs/2403.00092
- Reference count: 19
- Introduces PROC2PDDL, the first open-domain dataset pairing procedural texts with expert-annotated PDDL representations for text-based planning

## Executive Summary
This paper introduces PROC2PDDL, the first open-domain dataset pairing procedural texts with expert-annotated PDDL representations for text-based planning. The authors formulate the task of action modeling as predicting action definitions (parameters, preconditions, effects) given a domain header and text, and evaluate this intrinsically and extrinsically using a PDDL solver. Experiments show GPT-3.5 achieves near-zero performance while GPT-4 reaches 15.9% action accuracy and 33.7% PF solvability. A summarize-extract-translate pipeline with chain-of-thought prompting improves GPT-4 to 18.1% and 35.8%. Analysis reveals LMs struggle with both PDDL syntax and reasoning about actions, indicating significant room for improvement in integrating language models with formal planning.

## Method Summary
The authors introduce PROC2PDDL, the first open-domain dataset pairing procedural texts with expert-annotated PDDL representations for text-based planning. They formulate the task of action modeling as predicting action definitions (parameters, preconditions, effects) given a domain header and text. The evaluation uses both intrinsic metrics (action accuracy, PF solvability) and extrinsic metrics (PDDL solver). A summarize-extract-translate pipeline with chain-of-thought prompting is proposed to improve GPT-4 performance. The dataset consists of 1,195 tasks derived from 1,078 procedural texts across 80 different domains.

## Key Results
- GPT-3.5 achieves near-zero performance on action accuracy and PF solvability
- GPT-4 reaches 15.9% action accuracy and 33.7% PF solvability
- Summarize-extract-translate pipeline with chain-of-thought prompting improves GPT-4 to 18.1% action accuracy and 35.8% PF solvability

## Why This Works (Mechanism)
The paper demonstrates that large language models can be leveraged for open-domain planning by predicting PDDL representations from procedural texts. The summarize-extract-translate pipeline with chain-of-thought prompting helps break down the complex task into manageable subtasks, improving model performance. The intrinsic and extrinsic evaluation methods provide a comprehensive assessment of the generated PDDL representations.

## Foundational Learning
- **PDDL (Planning Domain Definition Language)**: Why needed - Formal language for representing planning problems. Quick check - Can the model correctly generate valid PDDL syntax?
- **Action modeling**: Why needed - Core task of predicting action definitions from text. Quick check - Does the model correctly identify parameters, preconditions, and effects?
- **Chain-of-thought prompting**: Why needed - Breaks down complex tasks into manageable subtasks. Quick check - Does the pipeline improve performance over direct prompting?
- **Intrinsic vs. extrinsic evaluation**: Why needed - Comprehensive assessment of generated representations. Quick check - Do intrinsic metrics correlate with extrinsic solver performance?
- **Procedural text understanding**: Why needed - Input format for planning task generation. Quick check - Can the model accurately extract relevant information from diverse procedural texts?

## Architecture Onboarding
Component map: Procedural text -> Domain header + Task text -> Action definitions (parameters, preconditions, effects) -> PDDL representation
Critical path: Text understanding → Action modeling → PDDL generation → Solver validation
Design tradeoffs: The summarize-extract-translate pipeline trades model complexity for improved performance but adds additional processing steps.
Failure signatures: Incorrect PDDL syntax, missing or incorrect action parameters/preconditions/effects, inability to reason about action relationships
First experiments:
1. Test pipeline on a small subset of procedural texts to verify component functionality
2. Compare performance of different prompting strategies on a validation set
3. Evaluate generated PDDL representations using a PDDL solver on a simple domain

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow performance gap between base GPT-4 and the summarize-extract-translate pipeline
- Intrinsic evaluation metrics may not fully capture practical utility of generated PDDL representations
- Dataset size of 1,195 tasks may be insufficient for robust evaluation in open-domain planning

## Confidence
- High confidence in the experimental methodology and results presentation
- Medium confidence in the significance of the pipeline improvements
- Low confidence in the generalizability of findings to broader planning domains

## Next Checks
1. Test the pipeline on a larger, more diverse procedural text corpus to assess scalability and robustness
2. Conduct user studies with domain experts to evaluate the practical utility of generated PDDL representations
3. Compare performance against human-generated PDDL representations on a subset of tasks to establish a performance ceiling