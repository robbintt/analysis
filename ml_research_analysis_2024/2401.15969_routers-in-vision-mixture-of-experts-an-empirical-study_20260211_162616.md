---
ver: rpa2
title: 'Routers in Vision Mixture of Experts: An Empirical Study'
arxiv_id: '2401.15969'
source_url: https://arxiv.org/abs/2401.15969
tags:
- choice
- expert
- token
- softmax
- routers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a comprehensive empirical study of routers in
  vision Mixture of Experts (MoE) models, introducing a unified formulation that subsumes
  different MoEs through parametric routing tensors. It conducts head-to-head experiments
  comparing six different routers, including existing ones from prior work and new
  ones introduced in this study.
---

# Routers in Vision Mixture of Experts: An Empirical Study

## Quick Facts
- arXiv ID: 2401.15969
- Source URL: https://arxiv.org/abs/2401.15969
- Authors: Tianlin Liu; Mathieu Blondel; Carlos Riquelme; Joan Puigcerver
- Reference count: 9
- Primary result: Expert Choice routers generally outperform Token Choice routers in sparse MoEs, with soft MoEs showing better performance under fixed compute budgets

## Executive Summary
This paper presents a comprehensive empirical study of routers in vision Mixture of Experts (MoE) models, introducing a unified formulation that subsumes different MoEs through parametric routing tensors. The study conducts head-to-head experiments comparing six different routers, including existing ones from prior work and new ones introduced in this study. The primary results show that many routers originally developed for language modeling can be adapted to perform strongly in vision tasks, with Expert Choice routers generally outperforming Token Choice routers in sparse MoEs, and soft MoEs generally outperforming sparse MoEs with a fixed compute budget.

## Method Summary
The study replaces dense feedforward layers in Vision Transformers with MoE layers using an Every-2 variant (MoE layers every other layer). Six different routers are compared: Softmax Token Choice, Sinkhorn Token Choice, Softmax Expert Choice, Sinkhorn Expert Choice, Sparsity-constrained Expert Choice, and Soft MoE. Models are pre-trained on JFT-300M dataset then evaluated on ImageNet-1k 10-shot transfer task. The unified MoE formulation uses routing tensors (DX, CX) to represent different routing mechanisms. Training includes importance and load auxiliary losses for routers.

## Key Results
- Many language modeling routers can be successfully adapted to vision tasks
- Expert Choice routers generally outperform Token Choice routers in sparse MoEs
- Soft MoEs outperform sparse MoEs with fixed compute budget
- Softmax Token Choice router performs surprisingly well despite its simplicity

## Why This Works (Mechanism)
The effectiveness stems from how different routers balance expert utilization and computational efficiency. Expert Choice routers enable better load balancing across experts by selecting the best experts for each token, while Token Choice routers select the best tokens for each expert. Soft MoEs maintain full connectivity but use learned importance scores to modulate expert contributions, allowing for smoother gradients and better utilization of all experts simultaneously.

## Foundational Learning

**Mixture of Experts (MoE)**: A conditional computation technique where different sub-networks (experts) are selected based on input characteristics. Needed to reduce computational cost while maintaining model capacity. Quick check: Verify that only a subset of experts are active for each input.

**Routing Mechanisms**: Methods for assigning tokens to experts, including token choice (select tokens for each expert) and expert choice (select experts for each token). Needed to balance load and ensure diverse expert utilization. Quick check: Monitor expert utilization statistics across the batch.

**Soft vs Sparse MoEs**: Soft MoEs use weighted combinations of all experts, while sparse MoEs select only a subset of experts per token. Needed to compare different trade-offs between accuracy and efficiency. Quick check: Compare FLOPs and accuracy between soft and sparse variants.

**Importance and Load Losses**: Auxiliary losses that encourage balanced expert utilization and meaningful routing decisions. Needed to prevent collapse to single expert usage. Quick check: Monitor auxiliary loss values during training.

**Vision Transformer Architecture**: The base model architecture modified to include MoE layers. Needed as the foundation for vision MoE experiments. Quick check: Verify model architecture matches specified design.

## Architecture Onboarding

**Component Map**: Input tokens -> Router (DX, CX) -> Expert assignment/weights -> Expert layers -> Output aggregation

**Critical Path**: Token embeddings → Router computation → Expert selection/weighting → Expert processing → Output combination

**Design Tradeoffs**: Soft MoEs offer better gradients and utilization but higher compute vs sparse MoEs with lower compute but potential underutilization.

**Failure Signatures**: Training instability with high buffer capacities, poor expert utilization leading to degraded performance, overfitting on pre-training data.

**First Experiments**: 1) Implement unified MoE layer with routing tensors, 2) Compare router variants on small-scale vision task, 3) Analyze expert utilization patterns across different routers.

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Specific architecture details of Vision Transformer models used are not fully specified
- Training hyperparameters including learning rate schedules and optimizer settings are not provided
- Focus on image classification limits generalizability to other vision domains
- Study may not capture full diversity of possible router designs

## Confidence
High confidence in Expert Choice vs Token Choice performance comparison, Medium confidence in soft vs sparse MoE performance, Medium confidence in cross-domain router effectiveness.

## Next Checks
1. Reimplement the unified MoE layer formulation with routing tensors and reproduce the six router variants using publicly available vision transformer codebases to verify the architectural claims and performance differences.

2. Conduct ablation studies varying the buffer capacity (c) and routing mechanisms to quantify their individual contributions to performance gains and identify potential overfitting or underutilization issues.

3. Test the routers on additional vision benchmarks beyond ImageNet-1k, such as object detection on COCO or semantic segmentation on Cityscapes, to assess the generalizability of the findings across different vision tasks and dataset scales.