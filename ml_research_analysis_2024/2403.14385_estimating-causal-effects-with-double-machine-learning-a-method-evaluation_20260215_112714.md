---
ver: rpa2
title: Estimating Causal Effects with Double Machine Learning -- A Method Evaluation
arxiv_id: '2403.14385'
source_url: https://arxiv.org/abs/2403.14385
tags: []
core_contribution: The paper evaluates the performance of double/debiased machine
  learning (DML) for causal effect estimation in various simulated settings and real-world
  data. DML uses flexible machine learning methods to adjust for confounding variables
  while maintaining unbiased causal effect estimates.
---

# Estimating Causal Effects with Double Machine Learning -- A Method Evaluation

## Quick Facts
- **arXiv ID**: 2403.14385
- **Source URL**: https://arxiv.org/abs/2403.14385
- **Reference count**: 40
- **Primary result**: Flexible ML methods like XGBoost and neural networks perform best in DML for causal effect estimation, especially with nonlinear confounding

## Executive Summary
This paper evaluates double/debiased machine learning (DML) for causal effect estimation across various simulated and real-world settings. The authors compare different machine learning algorithms within the DML framework, finding that flexible methods like XGBoost and neural networks outperform linear approaches like lasso regression, particularly when confounding relationships are nonlinear. The study demonstrates that DML's main advantage lies in its ability to adjust for complex confounding without requiring knowledge of functional forms, rather than handling extremely high-dimensional confounder spaces. The authors provide practical recommendations for researchers applying DML, emphasizing the importance of algorithm selection and parameter tuning.

## Method Summary
The authors implement DML with cross-fitting (K=5 folds, S=9 repetitions by default) and test various ML algorithms including lasso regression, generalized additive models, random forests, XGBoost, and neural networks. They generate simulated data following partially linear models with different functional forms and confounding strengths, then apply DML to estimate causal effects. The method is also applied to Boston housing price data to estimate air pollution effects. Performance is evaluated by comparing estimated effects to known true values across different settings.

## Key Results
- Flexible ML methods (XGBoost, neural networks) consistently outperform linear methods in nonlinear confounding scenarios
- DML's primary advantage is handling nonlinear confounding rather than adjusting for many confounders simultaneously
- Parameter choices (K folds, S repetitions) significantly impact estimate accuracy and robustness
- Lasso regression without manual variable transformations produces biased estimates in nonlinear confounding settings

## Why This Works (Mechanism)

### Mechanism 1: Double robustness through orthogonalization
- **Claim**: DML provides unbiased estimates by orthogonalizing residuals from treatment and outcome predictions
- **Mechanism**: Data is split into K folds; ML models predict treatment and outcome on held-out folds, residuals are computed, and outcome residuals are regressed on treatment residuals
- **Core assumption**: ML algorithms achieve n^1/4-consistency in both prediction tasks
- **Break condition**: Severe misspecification of either treatment or outcome prediction model

### Mechanism 2: Flexible ML relaxes functional form assumptions
- **Claim**: Flexible ML methods capture complex confounding relationships without manual specification
- **Mechanism**: Instead of pre-specifying functional forms, ML algorithms learn these relationships from data
- **Core assumption**: ML methods can adequately capture true functional relationships
- **Break condition**: Highly complex functional forms that chosen ML methods cannot capture

### Mechanism 3: Cross-fitting removes overfitting bias
- **Claim**: Sample splitting prevents regularization bias from contaminating causal estimates
- **Mechanism**: ML models trained on one subset are used to predict on held-out data, preventing overfitting from affecting final estimates
- **Core assumption**: Appropriate choice of K relative to sample size
- **Break condition**: Too few folds (insufficient cross-fitting) or too many folds (too few observations for final regression)

## Foundational Learning

- **Concept: Partially linear model**
  - Why needed: DML assumes this model structure where treatment has linear effect but confounders can have arbitrary functional forms
  - Quick check: In the partially linear model Y = βW + g(Xc) + ε, what assumptions are made about the functional form of g(Xc)?

- **Concept: Double robustness**
  - Why needed: Understanding why DML can tolerate misspecification in one of the two ML models
  - Quick check: If the treatment model is correctly specified but the outcome model is misspecified, will DML still provide consistent estimates?

- **Concept: Orthogonalization**
  - Why needed: The mathematical operation that removes confounding from residuals
  - Quick check: What mathematical operation does DML perform when it computes "residuals" from the ML predictions?

## Architecture Onboarding

- **Component map**: Data preprocessing → Sample splitting (K folds) → Two ML training pipelines (treatment and outcome) → Residual computation → Linear regression on residuals → Averaging across folds → (Optional) Multiple repetitions for robustness
- **Critical path**: Sample splitting → ML training → Residual computation → Final regression
- **Design tradeoffs**: 
  - K (number of folds): Higher K gives more data for ML training but fewer observations for final regression
  - S (number of repetitions): More repetitions increase robustness but linearly increase computation time
  - ML method choice: More flexible methods capture complex relationships but may overfit in small samples
- **Failure signatures**: 
  - High variance across random splits → Increase S or check for unstable ML models
  - Consistent bias in one direction → Check identification assumptions or ML model specification
  - Extremely large standard errors → Check for separation issues or too few observations in final regression
- **First 3 experiments**:
  1. Run DML with K=2 and K=5 on a small simulated dataset to observe how fold choice affects estimates
  2. Compare DML estimates using linear regression vs XGBoost on a dataset with known nonlinear confounding
  3. Test stability of estimates by running the same specification with different random seeds and varying S from 1 to 100

## Open Questions the Paper Calls Out
- How does DML perform in settings with unobserved confounding compared to instrumental variables methods?
- How does the choice of machine learning algorithm within DML interact with the dimensionality of the data (number of confounders)?
- How does DML perform in panel data settings with time-varying unobserved heterogeneity?

## Limitations
- Performance evaluation based on simulations and single real-world application limits generalizability
- Focus on partially linear models restricts applicability to more complex treatment-outcome relationships
- Parameter sensitivity analysis lacks systematic exploration across diverse data structures

## Confidence
- **High confidence**: DML's fundamental orthogonality principle and double robustness property
- **Medium confidence**: Flexible ML methods' advantage over linear methods in handling nonlinear confounding
- **Low confidence**: Optimal parameter choices (K, S) across diverse settings and extreme confounding scenarios

## Next Checks
1. Test DML with highly nonlinear confounding structures (e.g., interactions of order 3+) to identify where flexible ML methods break down
2. Evaluate performance across different sample size ranges to quantify the n^1/4-consistency requirement empirically
3. Compare DML against alternative causal inference methods (e.g., targeted maximum likelihood estimation) on the same benchmark datasets to contextualize DML's relative performance