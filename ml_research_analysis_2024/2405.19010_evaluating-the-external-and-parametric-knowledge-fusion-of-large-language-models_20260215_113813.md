---
ver: rpa2
title: Evaluating the External and Parametric Knowledge Fusion of Large Language Models
arxiv_id: '2405.19010'
source_url: https://arxiv.org/abs/2405.19010
tags: []
core_contribution: This paper examines how large language models (LLMs) integrate
  external knowledge with their intrinsic parametric knowledge. Prior work has overly
  relied on external sources while neglecting the valuable contributions of LLMs'
  own parametric knowledge.
---

# Evaluating the External and Parametric Knowledge Fusion of Large Language Models

## Quick Facts
- **arXiv ID**: 2405.19010
- **Source URL**: https://arxiv.org/abs/2405.19010
- **Reference count**: 23
- **Primary result**: Continued training on domain-specific parametric knowledge significantly improves LLMs' ability to integrate external knowledge with their own parametric knowledge

## Executive Summary
This paper examines how large language models (LLMs) integrate external knowledge with their intrinsic parametric knowledge. Prior work has overly relied on external sources while neglecting the valuable contributions of LLMs' own parametric knowledge. This study deconstructs knowledge fusion into four scenarios: when external knowledge alone is sufficient, when it is partially sufficient, when it is insufficient, and when neither source can adequately answer. Through systematic experiments, the authors demonstrate that enhancing parametric knowledge via continued training significantly improves LLMs' ability to integrate external and parametric knowledge, while challenges remain in memorizing, eliciting, and determining the boundaries of parametric knowledge for effective fusion.

## Method Summary
The authors developed a systematic pipeline for data construction and knowledge infusion. They collected over 1,700 paragraphs from 1,500 documents describing electronic products spanning four years, splitting the data into two parts: one for enhancing LLMs' parametric knowledge through continued training, and the other as external knowledge. They crafted questions based on the data to emulate four knowledge fusion scenarios and injected new knowledge into LLMs through continued training and supervised fine-tuning. The evaluation used accuracy (Racc) and information coverage (Rcover) metrics to assess knowledge fusion capabilities across the scenarios.

## Key Results
- Continued training on parametric knowledge (Kp) improved ChatGLM accuracy by 25% and Qwen by 28.3% absolute
- Supervised fine-tuning (SFT) improved LLMs' instruction-following ability and performance across all scenarios
- Providing ground-truth snippets alongside distractors (Easy mode) significantly improved performance, highlighting noise robustness challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued training on domain-specific parametric knowledge significantly improves LLMs' ability to integrate external knowledge with their own parametric knowledge
- Mechanism: Injecting Kp through continued training expands the model's parametric knowledge boundary, allowing better recognition and utilization of its own knowledge when external knowledge is incomplete
- Core assumption: LLMs can effectively memorize and retain the injected Kp, and this enhanced knowledge improves their ability to fuse external and parametric knowledge
- Evidence anchors: [abstract] "enhancing parametric knowledge within LLMs can significantly bolster their capability for knowledge integration"; [section 6.1] "ChatGLM exhibits a 25% absolute improvement in accuracy, while Qwen shows a 28.3% absolute increase"
- Break condition: If the model's capacity is insufficient to retain the injected Kp, or if training data lacks diversity

### Mechanism 2
- Claim: SFT on the constructed QA dataset improves LLMs' instruction-following ability
- Mechanism: SFT teaches the model to follow instructions and utilize given knowledge to generate correct responses, even without injecting new knowledge
- Core assumption: The constructed QA dataset effectively captures the nuances of the four knowledge fusion scenarios
- Evidence anchors: [section 6.2.1] "SFT helps LLM to learn how to follow the instructions and utilize the given knowledge to reach the correct responses"; [section 6.2.2] "SFT helps to improve the performance of LLMs across all scenarios"
- Break condition: If the SFT dataset is not representative or the model overfits to specific instructions

### Mechanism 3
- Claim: Providing ground-truth snippets alongside distractors during inference significantly improves LLMs' performance
- Mechanism: Giving the model access to correct information helps it distinguish between relevant and irrelevant external knowledge
- Core assumption: Ground-truth snippets are accurate and representative of needed knowledge
- Evidence anchors: [section 6.2.1] "ChatGLMEasy is inferior to ChatGLMSFT with a distinct gap, i.e., 68.3% versus 72.2%"; [section 6.2.2] "If we directly use all the ground-truth supporting snippets as external knowledge...the LLMs' performance increases significantly"
- Break condition: If ground-truth snippets are not provided or distractors are too convincing

## Foundational Learning

- **Knowledge fusion in LLMs**: Why needed - The paper focuses on evaluating and improving LLMs' ability to integrate external knowledge with their parametric knowledge. Quick check - What are the four distinct scenarios defined in the paper to evaluate knowledge fusion?

- **Continued training and supervised fine-tuning**: Why needed - These techniques are used to inject parametric knowledge into the model and improve its instruction-following ability. Quick check - How does continued training differ from supervised fine-tuning in terms of their impact on the model's parametric knowledge?

- **Noise robustness and knowledge boundary perception**: Why needed - The paper highlights challenges of dealing with noisy external knowledge and accurately recognizing boundaries of parametric knowledge. Quick check - What are the two main factors that contribute to the model's inability to accurately elicit its parametric knowledge?

## Architecture Onboarding

- **Component map**: Data construction pipeline -> Knowledge infusion (continued training/SFT) -> Evaluation framework
- **Critical path**: 1. Collect and preprocess data 2. Construct QA dataset 3. Inject parametric knowledge 4. Evaluate model performance
- **Design tradeoffs**: Balancing injected parametric knowledge with model capacity; ensuring dataset diversity and representativeness; determining optimal noise level
- **Failure signatures**: Low accuracy and information coverage across all scenarios; inability to correctly identify and utilize parametric knowledge when external knowledge is incomplete; overreliance on external knowledge even when irrelevant or noisy
- **First 3 experiments**: 1. Evaluate model performance on QA dataset without knowledge infusion 2. Inject parametric knowledge using continued training and evaluate on QA dataset 3. Perform SFT on QA dataset and evaluate on separate test set

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the optimal balance between external and parametric knowledge in LLMs for different types of queries? The paper identifies challenges in determining when to rely on external versus parametric knowledge but doesn't provide a framework for determining optimal balance for different query types.

- **Open Question 2**: How can LLMs be trained to better recognize their knowledge boundaries and trigger appropriate refusal responses? While the paper identifies the problem of overconfidence and inappropriate refusal, it doesn't propose solutions for training LLMs to better recognize knowledge limitations.

- **Open Question 3**: What are the most effective strategies for mitigating noise in external knowledge sources for LLMs? While the paper discusses noise as a significant problem, it doesn't explore specific strategies for filtering or mitigating noise in external knowledge sources.

## Limitations

- Findings are based on experiments conducted on a single domain (electronic products), limiting generalizability to other domains
- Lack of comprehensive analysis of the impact of model architecture and size on knowledge fusion performance
- Evaluation metrics (accuracy and information coverage) may not fully capture qualitative aspects of knowledge fusion

## Confidence

- **High confidence**: Continued training significantly improves LLMs' ability to integrate external knowledge; providing ground-truth snippets improves performance
- **Medium confidence**: SFT improves instruction-following ability and performance across scenarios; challenges remain in memorizing and eliciting parametric knowledge
- **Low confidence**: Specific mechanisms underlying observed improvements are fully understood and can be generalized across different domains and model architectures

## Next Checks

1. Replicate the study on diverse domains (healthcare, finance, education) to assess generalizability and identify domain-specific challenges

2. Conduct ablation studies to isolate impact of different model components (attention mechanisms, memory layers) on knowledge fusion performance

3. Develop and evaluate alternative evaluation metrics that capture qualitative aspects of knowledge fusion (coherence, relevance, novelty)