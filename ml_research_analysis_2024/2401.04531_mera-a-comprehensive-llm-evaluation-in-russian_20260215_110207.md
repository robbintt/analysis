---
ver: rpa2
title: 'MERA: A Comprehensive LLM Evaluation in Russian'
arxiv_id: '2401.04531'
source_url: https://arxiv.org/abs/2401.04531
tags: []
core_contribution: MERA is a new benchmark for evaluating large language models in
  Russian, covering 21 tasks across 11 skill domains. It uses a standardized evaluation
  methodology with fixed prompts and few-shot examples to ensure reproducibility and
  fairness.
---

# MERA: A Comprehensive LLM Evaluation in Russian

## Quick Facts
- arXiv ID: 2401.04531
- Source URL: https://arxiv.org/abs/2401.04531
- Reference count: 40
- Primary result: Current open-source models lag significantly behind human performance on this Russian LLM benchmark

## Executive Summary
MERA is a new benchmark designed to evaluate large language models in Russian across 21 tasks spanning 11 skill domains. The benchmark employs a standardized evaluation methodology with fixed prompts and few-shot examples to ensure reproducibility and fairness. It includes problem-solving, exam-based, and diagnostic tasks to assess language understanding, reasoning, and ethical biases. MERA provides an open-source evaluation framework and public leaderboard, demonstrating that current models still fall well short of human performance levels.

## Method Summary
The MERA benchmark uses zero- and few-shot fixed instruction settings with manually designed, varied prompts distributed evenly across data samples. The evaluation framework integrates with lm-harness and uses JSON-formatted datasets. Models are evaluated using automatic metrics including exact match, F1 score, ROUGE, BLEU, and cosine similarity, with results aggregated into leaderboard rankings. The benchmark is designed as a black-box test to prevent data leakage while maintaining task difficulty.

## Key Results
- MERA covers 21 evaluation tasks across 11 skill domains including reasoning, mathematics, programming, and ethics
- Current open-source models show significant performance gaps compared to human baselines
- The benchmark provides reproducible evaluation through standardized fixed instruction formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standardized instruction-format tasks ensure reproducibility across different model architectures
- Mechanism: Fixed experimental setup with manually designed, varied prompts distributed evenly across data samples eliminates prompt selection bias
- Core assumption: Prompt variation and distribution do not introduce unintended biases across architectures
- Evidence anchors: Methodology based on Li et al. (2023), with variative prompt sets designed for each task
- Break condition: If prompt design inadvertently favors certain model architectures

### Mechanism 2
- Claim: Black-box evaluation prevents data leakage while maintaining task difficulty
- Mechanism: Private test sets and fixed evaluation procedures ensure models cannot memorize answers
- Core assumption: Task complexity creates sufficient distance from pretraining data
- Evidence anchors: Benchmark designed as black-box test with JSON-structured instruction format
- Break condition: If models can infer task patterns from instruction format alone

### Mechanism 3
- Claim: Multi-task aggregation provides comprehensive model capability assessment
- Mechanism: 21 tasks across 11 domains with different answer types and metrics capture diverse aspects of language model intelligence
- Core assumption: Task selection covers representative capabilities and aggregation fairly combines different metrics
- Evidence anchors: Multiple metrics including exact match, F1, ROUGE, BLEU, and cosine similarity
- Break condition: If certain skill domains are underrepresented or metric aggregation is unfair

## Foundational Learning

- Concept: Zero-shot and few-shot evaluation settings
  - Why needed here: Allows assessment of model generalization capabilities without task-specific training
  - Quick check question: What is the difference between zero-shot and few-shot evaluation, and why would a benchmark use both?

- Concept: Automatic metrics for generative tasks
  - Why needed here: Enables scalable, reproducible evaluation without human annotation
  - Quick check question: What are the limitations of automatic metrics compared to human evaluation for generative tasks?

- Concept: Cross-lingual evaluation methodology
  - Why needed here: Ensures fair evaluation of both Russian-only and multilingual models
  - Quick check question: How does the benchmark handle evaluation of multilingual models on Russian-language tasks?

## Architecture Onboarding

- Component map: Task definition -> Dataset preparation -> Prompt design -> Model evaluation -> Metric calculation -> Leaderboard update
- Critical path: Evaluation framework with lm-harness integration → Task datasets in JSON format → Scoring system with multiple metrics → Submission pipeline → Leaderboard interface
- Design tradeoffs: Comprehensive task coverage vs. evaluation complexity; automatic metrics vs. human evaluation accuracy; reproducibility vs. task diversity
- Failure signatures: Inconsistent model performance across similar tasks; metrics showing unrealistic scores; submission pipeline failures; leaderboard calculation errors
- First 3 experiments:
  1. Run random baseline on all tasks to verify scoring system and metric calculations
  2. Evaluate a simple open-source model (e.g., ruGPT-3-small) to test end-to-end pipeline
  3. Compare results of decoder-only vs encoder-decoder models on arithmetic tasks to validate task difficulty assessment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the MERA benchmark be extended to cover generative tasks effectively under uniform standard conditions?
- Basis in paper: [explicit] Current benchmark excludes generative tasks due to inherent difficulty of reliable automatic measurement; suggests human-based side-by-side evaluation as most reliable approach
- Why unresolved: Paper acknowledges importance but provides no concrete solution for incorporating generative tasks
- What evidence would resolve it: Methodology/framework for standardized automated evaluation of generative tasks, potentially combining automated metrics and human evaluation

### Open Question 2
- Question: How can MERA be extended to incorporate other modalities like images and audio while maintaining evaluation rigor?
- Basis in paper: [explicit] Current benchmark for textual data but methodology versatile for different modalities; plans to extend to images and audio
- Why unresolved: Paper recognizes need but lacks specific details on adaptation methodology
- What evidence would resolve it: Comprehensive framework for evaluating multimodal tasks with specific designs, metrics, and guidelines

### Open Question 3
- Question: How can MERA ensure consistent and reproducible results across different hardware configurations?
- Basis in paper: [explicit] lm-harness imposes restrictions and may face challenges across different GPUs and batches; requests equal conditions for baselines and specification of GPUs used
- Why unresolved: Paper recognizes importance but lacks definitive solution for hardware variability
- What evidence would resolve it: Comprehensive guidelines for standardized evaluation across hardware, including GPU selection and batch size recommendations

## Limitations

- Automatic metrics may not fully capture nuanced language understanding and could produce misleading results for certain task types
- Benchmark's focus on Russian language limits generalizability to other languages
- Fixed instruction format may not adequately represent all real-world use cases

## Confidence

- **High Confidence**: Benchmark successfully covers 21 tasks across 11 skill domains with standardized evaluation procedures
- **Medium Confidence**: Benchmark effectively prevents data leakage through black-box evaluation
- **Low Confidence**: Automatic metrics provide reliable and comprehensive assessment of model capabilities

## Next Checks

1. **Metric Validation**: Conduct human evaluation studies on task outputs to verify automatic metrics correlate with human judgment, especially for creative and open-ended tasks

2. **Generalization Testing**: Evaluate multilingual models on both Russian and other language versions of tasks to assess whether the benchmark's methodology shows language-specific biases

3. **Baseline Stability Analysis**: Perform repeated evaluations of same models using different random seeds and hardware configurations to quantify variability and establish confidence intervals for leaderboard rankings