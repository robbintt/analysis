---
ver: rpa2
title: 'Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting'
arxiv_id: '2404.18911'
source_url: https://arxiv.org/abs/2404.18911
tags:
- arxiv
- decoding
- kangaroo
- tokens
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Kangaroo, a self-speculative decoding framework
  designed to accelerate inference in large language models (LLMs) while maintaining
  the same sampling distribution. The core idea is to use a fixed shallow sub-network
  of the LLM as a self-draft model, paired with a lightweight adapter module to bridge
  the representation gap.
---

# Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting

## Quick Facts
- arXiv ID: 2404.18911
- Source URL: https://arxiv.org/abs/2404.18911
- Reference count: 36
- Achieves up to 1.68× speedup on Spec-Bench with 88.7% fewer parameters than Medusa-1

## Executive Summary
Kangaroo introduces a self-speculative decoding framework that accelerates LLM inference by using a shallow sub-network of the target model as a self-draft model, paired with a lightweight adapter to bridge the representation gap. The framework employs an early exiting mechanism during drafting to halt generation when token confidence falls below a threshold, reducing unnecessary computation on difficult tokens. Experiments demonstrate that Kangaroo achieves significant speedups across translation, QA, summarization, and mathematical reasoning tasks while maintaining the same sampling distribution as the target LLM.

## Method Summary
The framework uses a fixed shallow sub-network (first l layers) of the target LLM as a self-draft model, combined with a lightweight adapter module to bridge the representation gap. An early exiting mechanism dynamically stops draft generation when token confidence falls below a predefined threshold η, rather than generating a fixed number of tokens. The self-draft model and target LLM share KV cache and computation, requiring only the small adapter to be deployed additionally. This approach maintains the sampling distribution of the target LLM while achieving significant inference acceleration through speculative decoding.

## Key Results
- Achieves speedups up to 1.68× on Spec-Bench benchmark
- Uses 67M additional parameters compared to 591M for Medusa-1 (88.7% reduction)
- Maintains same sampling distribution as target LLM during inference
- Demonstrates strong performance across translation, QA, summarization, and mathematical reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-draft model with early exit and adapter achieves comparable performance to Medusa while using fewer parameters
- Mechanism: Uses shallow sub-network + adapter combination that approximates full model predictions sufficiently for speculative decoding
- Core assumption: The shallow sub-network + adapter can approximate full model predictions sufficiently
- Evidence anchors:
  - Achieves 1.68× speedup on Spec-Bench, outperforming Medusa-1 with 88.7% fewer parameters (67M vs 591M)
  - Early exiting reduces inference latency by halting draft generation when confidence drops below threshold
- Break condition: If adapter cannot bridge representation gap, acceptance rate drops and speculative decoding becomes inefficient

### Mechanism 2
- Claim: Dynamic drafting with confidence-based early exiting improves end-to-end speedup compared to fixed-step drafting
- Mechanism: Dynamically stops drafting when token confidence falls below threshold rather than using fixed number of draft tokens
- Core assumption: Token difficulty varies significantly across positions, making dynamic termination more efficient
- Evidence anchors:
  - Early exiting reduces inference latency by stopping when top-1 confidence falls below predefined threshold
  - Dynamic approach reduces computation on difficult tokens that would require expensive verification
- Break condition: If threshold too high, insufficient draft tokens generated; if too low, unnecessary computation performed

### Mechanism 3
- Claim: Sharing KV cache and computation between self-draft and target LLM reduces deployment overhead
- Mechanism: Self-draft model shares shallow sub-network parameters with target LLM, requiring only lightweight adapter
- Core assumption: Shared shallow sub-network creates computational overlap that can be leveraged for efficiency
- Evidence anchors:
  - Only additional deployment requirement is small adapter network
  - Self-draft model reuses LM Head of target LLM
- Break condition: If memory bandwidth becomes bottleneck rather than computation, shared computation advantage diminishes

## Foundational Learning

- Concept: Speculative decoding and token acceptance rate
  - Why needed here: Framework builds on speculative decoding where draft tokens are generated and verified against target model
  - Quick check question: What is the compression rate formula and how does it relate to token acceptance rate?

- Concept: Early exiting mechanisms in transformer architectures
  - Why needed here: Kangaroo uses early exiting twice - to create self-draft model and during drafting based on confidence thresholds
  - Quick check question: How does early exiting affect model capacity and what are trade-offs in selecting exit layer?

- Concept: Adapter modules and knowledge transfer between layers
  - Why needed here: Lightweight adapter bridges representation gap between shallow sub-network and full model
  - Quick check question: What is role of multi-head attention in adapter and why might removing FFN components be beneficial?

## Architecture Onboarding

- Component map: Shallow sub-network (first l layers) -> Adapter (1 attention + 2 normalization) -> LM Head (shared) -> Target LLM (remaining layers)

- Critical path: Token generation → shallow sub-network processing → adapter processing → confidence threshold check → draft token output or early exit → parallel verification with target LLM

- Design tradeoffs:
  - Exit layer depth vs. adapter complexity: Deeper exit layers provide better representation but increase adapter complexity
  - Confidence threshold vs. drafting efficiency: Higher thresholds reduce computation but may generate insufficient draft tokens
  - Adapter architecture: Simpler adapters reduce parameters but may compromise bridging capability

- Failure signatures:
  - Low token acceptance rate: Adapter cannot bridge representation gap effectively
  - Suboptimal speedup despite high acceptance rate: Adapter inference latency too high
  - Unstable performance across tasks: Confidence threshold not well-calibrated for different token distributions

- First 3 experiments:
  1. Measure token acceptance rate and compression rate with different exit layer depths (l=1,2,3) on validation set
  2. Compare dynamic drafting vs. fixed-step drafting with varying thresholds on computation time
  3. Ablation study on adapter architecture components measuring parameter count and performance

## Open Questions the Paper Calls Out
None

## Limitations
- Adapter module may struggle with highly specialized domains requiring deeper semantic understanding
- Early exiting trade-off between computational efficiency and draft quality not fully explored
- Confidence threshold treated as fixed hyperparameter without systematic sensitivity analysis

## Confidence

**High Confidence:**
- Shared KV cache and computation reduces deployment overhead
- Self-draft model architecture achieves comparable performance with fewer parameters
- Dynamic confidence-based drafting outperforms fixed-step approaches

**Medium Confidence:**
- Framework maintains same sampling distribution as target LLM
- Parameter efficiency gains translate to practical deployment benefits
- Approach generalizes across tested task categories

**Low Confidence:**
- Performance consistency across diverse real-world applications beyond Spec-Bench
- Scalability to models significantly larger than tested (BLOOM-176B, Llama-2-70B)
- Robustness to domain-specific token distributions and confidence patterns

## Next Checks
1. Evaluate Kangaroo on specialized domains (medical, legal, code generation) to assess adapter limitations with domain-specific token distributions
2. Conduct systematic ablation studies varying confidence threshold η across multiple orders of magnitude, measuring non-linear relationship with speedup and acceptance rate
3. Test framework on models exceeding 100B parameters to verify parameter efficiency claims maintain practical significance at larger scales