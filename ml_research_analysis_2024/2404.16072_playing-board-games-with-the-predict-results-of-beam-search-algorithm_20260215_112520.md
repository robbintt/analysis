---
ver: rpa2
title: Playing Board Games with the Predict Results of Beam Search Algorithm
arxiv_id: '2404.16072'
source_url: https://arxiv.org/abs/2404.16072
tags:
- search
- beam
- algorithm
- games
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces PROBS, a novel algorithm for two-player deterministic
  games with perfect information that uses beam search instead of the more common
  Monte Carlo Tree Search. The method iteratively trains two neural networks: one
  predicting the value of a game state and another predicting the outcome of beam
  search from a given state and action.'
---

# Playing Board Games with the Predict Results of Beam Search Algorithm

## Quick Facts
- **arXiv ID:** 2404.16072
- **Source URL:** https://arxiv.org/abs/2404.16072
- **Reference count:** 3
- **Primary result:** Introduces PROBS algorithm using beam search to improve game-playing performance through iterative neural network training

## Executive Summary
The paper introduces PROBS, a novel algorithm for two-player deterministic games with perfect information that uses beam search instead of the more common Monte Carlo Tree Search. The method iteratively trains two neural networks: one predicting the value of a game state and another predicting the outcome of beam search from a given state and action. These networks are trained through self-play, with beam search guiding the policy toward deeper strategic understanding even when the search depth is limited. Empirical evaluation on Connect Four shows the PROBS algorithm improves its Elo rating over iterations, outperforming baseline agents including those with deeper lookahead. The approach demonstrates effectiveness even when beam search depth is smaller than the average game length, suggesting the model learns to leverage strategic information beyond the search horizon. Results extend to more complex games like Toguz-Kumalak and Reversi, indicating the algorithm's broader applicability.

## Method Summary
PROBS employs beam search as the guiding mechanism instead of MCTS to generate training data for neural networks. The algorithm maintains two networks: a value network that predicts game state values and a beam prediction network that forecasts beam search outcomes from specific states and actions. Through self-play, the system iteratively trains these networks, with beam search providing strategic guidance even at limited depths. This approach allows the model to learn deeper strategic patterns without requiring full tree exploration. The algorithm demonstrates effectiveness particularly when search depth is constrained relative to game length, suggesting it captures transferable strategic knowledge beyond its immediate search horizon.

## Key Results
- PROBS algorithm shows Elo rating improvement over training iterations on Connect Four
- Outperforms baseline agents including those with deeper lookahead capabilities
- Maintains effectiveness even when beam search depth is smaller than average game length
- Demonstrates broader applicability across Toguz-Kumalak and Reversi games

## Why This Works (Mechanism)
PROBS works by leveraging beam search to provide structured exploration guidance during self-play training. Unlike MCTS which explores probabilistically, beam search maintains a fixed-width search frontier, allowing more predictable and computationally efficient exploration. The dual-network architecture separates value estimation from search outcome prediction, enabling specialized learning for each task. By training on beam search results, the model learns to extrapolate strategic patterns beyond its immediate search depth, effectively compressing deeper strategic understanding into its representations. The iterative self-play framework ensures continuous improvement as the networks refine their predictions based on increasingly sophisticated opponent strategies.

## Foundational Learning
- **Beam Search:** A graph search algorithm that explores the most promising nodes at each depth level while maintaining a fixed-width frontier. Needed to provide structured, computationally efficient exploration compared to MCTS. Quick check: Verify beam width and depth parameters balance exploration coverage against computational cost.
- **Self-Play Training:** Iterative training framework where agents improve by playing against versions of themselves. Essential for generating diverse training data without external opponents. Quick check: Monitor training diversity metrics to ensure sufficient exploration of game state space.
- **Dual-Network Architecture:** Separate networks for value prediction and beam outcome prediction. Required to specialize learning for different aspects of game understanding. Quick check: Compare performance when using single vs. dual network architectures.
- **Elo Rating System:** Chess rating system adapted for evaluating relative strength between game-playing agents. Provides quantitative measure of algorithm improvement. Quick check: Validate Elo calculations across different opponent pools.
- **Deterministic Perfect Information Games:** Game class where all information is visible and outcomes are fully determined by player actions. Framework assumption that enables deterministic search strategies. Quick check: Confirm algorithm behavior in partially observable or stochastic variants.

## Architecture Onboarding

**Component Map:**
Input Game State -> Value Network & Beam Prediction Network -> Combined Policy Output -> Beam Search Execution -> Training Data Generation -> Network Updates

**Critical Path:**
Game state input flows through both networks, their outputs combine to inform beam search, which generates new positions that become training examples. This loop repeats iteratively, with each cycle improving network predictions.

**Design Tradeoffs:**
- Beam width vs. search completeness: Wider beams explore more but increase computation
- Network complexity vs. training efficiency: Deeper networks may learn better representations but train slower
- Search depth vs. strategic learning: Shallower searches are faster but may miss important patterns

**Failure Signatures:**
- Plateauing Elo ratings despite continued training
- Beam search consistently missing critical moves
- Network predictions diverging from actual game outcomes
- Training instability or catastrophic forgetting

**First Experiments:**
1. Measure Elo improvement rate across different beam widths
2. Compare performance against MCTS baseline with equal computation time
3. Test algorithm on games with varying branching factors and strategic depth

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation primarily focused on Connect Four with limited testing on only two additional games
- No detailed analysis of computational efficiency compared to MCTS-based approaches
- Minimal ablation studies lacking systematic investigation of beam search parameter sensitivity
- Limited cross-game validation raises questions about generalizability across diverse game types

## Confidence
- **Elo improvement on Connect Four:** High - Clear quantitative evidence provided
- **Algorithm effectiveness beyond search horizon:** Medium - Interesting claim but limited validation across multiple games
- **Generalizability to complex games:** Medium - Results on three games insufficient for broad claims
- **Computational efficiency claims:** Low - No systematic comparison with alternative approaches

## Next Checks
1. Conduct systematic ablation studies varying beam search width and depth across multiple games to quantify parameter sensitivity
2. Implement head-to-head comparisons against MCTS-based agents with similar computational budgets on a broader set of games
3. Measure and compare computational efficiency (time and memory) between PROBS and state-of-the-art game-playing algorithms across different hardware configurations