---
ver: rpa2
title: Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy
arxiv_id: '2404.01830'
source_url: https://arxiv.org/abs/2404.01830
tags:
- policy
- logging
- estimator
- function
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles off-policy evaluation (OPE) when the logging
  policy is unknown. The authors propose a doubly-robust estimator, DRUnknown, that
  jointly estimates the logging policy and value function.
---

# Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy
## Quick Facts
- arXiv ID: 2404.01830
- Source URL: https://arxiv.org/abs/2404.01830
- Reference count: 17
- This paper proposes DRUnknown, a doubly-robust estimator for off-policy evaluation when the logging policy is unknown

## Executive Summary
This paper addresses off-policy evaluation (OPE) in contextual bandits and reinforcement learning when the logging policy is unknown. The authors propose DRUnknown, a doubly-robust estimator that jointly estimates the logging policy and value function. The method uses maximum likelihood estimation for the logging policy and minimizes the asymptotic variance of the estimator to estimate the value function. DRUnknown is consistent if either the logging policy or value function model is correct, and achieves the semiparametric lower bound when both are correct. Experiments show DRUnknown outperforms existing methods in terms of mean squared error.

## Method Summary
The DRUnknown estimator jointly estimates the logging policy parameter φ̂ using maximum likelihood and the value function parameter β̂ by minimizing asymptotic variance. The method constructs a doubly-robust estimator that is consistent if either component is correctly specified. The asymptotic variance is derived accounting for the effect of estimating φ̂, and β̂ is chosen to minimize this variance. This approach ensures that when the logging policy estimate is correct, DRUnknown achieves the smallest asymptotic variance among doubly-robust estimators with estimated logging policies.

## Key Results
- DRUnknown is doubly-robust: consistent if either logging policy or value function model is correct
- When logging policy is correct, DRUnknown achieves smallest asymptotic variance among DR estimators with estimated logging policies
- DRUnknown reaches semiparametric lower bound when both logging policy and value function are correctly specified
- Experiments on synthetic and UCI datasets show DRUnknown outperforms IPW, MLIPW, and MRDR in terms of mean squared error

## Why This Works (Mechanism)
The method works by leveraging the doubly-robust property: the estimator is unbiased if either the logging policy or value function model is correct. By jointly estimating both components and minimizing asymptotic variance, the approach ensures optimal performance when the logging policy estimate is accurate. The semiparametric efficiency is achieved through careful variance minimization that accounts for the uncertainty in the logging policy estimate.

## Foundational Learning
**Off-Policy Evaluation**: Estimating policy value using data from a different logging policy. Needed to evaluate policies without deploying them in production. Quick check: Can be implemented using importance sampling.

**Doubly-Robust Estimators**: Estimators that are unbiased if either the propensity score or outcome model is correct. Needed to reduce sensitivity to model misspecification. Quick check: Combines IPW and regression-based approaches.

**Semiparametric Efficiency**: Achieving the minimum possible asymptotic variance among regular estimators. Needed to ensure optimal statistical performance. Quick check: Related to efficient influence functions in statistical theory.

## Architecture Onboarding
**Component Map**: Context/State -> Logging Policy Estimation -> Value Function Estimation -> DRUnknown Estimator
**Critical Path**: MLE for logging policy → variance minimization for value function → doubly-robust combination
**Design Tradeoffs**: Joint estimation increases complexity but enables semiparametric efficiency
**Failure Signatures**: High variance when logging/target policies differ greatly; convergence issues in joint optimization
**First Experiments**: 1) Test MLE convergence on synthetic data 2) Validate variance minimization approach 3) Compare MSE against baselines

## Open Questions the Paper Calls Out
**Open Question 1**: How does DRUnknown compare to existing methods when the logging policy model is misspecified? The paper only discusses the case when the logging policy model is correctly specified and does not provide experimental results or theoretical analysis for misspecification scenarios.

**Open Question 2**: What is the impact of the choice of the function class bQ on the performance of DRUnknown? The paper mentions the value function model but does not discuss how different function class choices affect performance.

**Open Question 3**: How does the computational complexity of DRUnknown compare to existing methods? The paper does not provide analysis or experimental results on the computational complexity of DRUnknown.

## Limitations
- Performance degrades when logging and target policies differ significantly
- Sensitive to model misspecification in either component
- Computational complexity may be higher than simpler estimators

## Confidence
- Theoretical claims: High
- Experimental validation: Medium
- Practical applicability: Medium

## Next Checks
1. Implement and compare with properly specified MRDR and MLIPW baselines under unknown logging policy scenarios
2. Test estimator sensitivity to importance weight truncation and model misspecification
3. Validate asymptotic variance minimization approach on larger-scale problems with more complex logging policies