---
ver: rpa2
title: Exploring and steering the moral compass of Large Language Models
arxiv_id: '2405.17345'
source_url: https://arxiv.org/abs/2405.17345
tags:
- moral
- ethical
- these
- steering
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study conducted a systematic comparison of eight state-of-the-art
  LLMs using ethical dilemmas and the Moral Foundations Questionnaire to assess their
  moral reasoning and alignment. Proprietary models (GPT-3.5, GPT-4, Claude-3, Gemini)
  showed stronger utilitarian tendencies, while open models (Gemma, Llama-2, Starling-LM)
  aligned more with deontological values.
---

# Exploring and steering the moral compass of Large Language Models

## Quick Facts
- arXiv ID: 2405.17345
- Source URL: https://arxiv.org/abs/2405.17345
- Reference count: 0
- Primary result: Systematic comparison of eight LLMs' moral reasoning and novel activation steering technique SARA

## Executive Summary
This study systematically compared eight state-of-the-art LLMs' moral reasoning using ethical dilemmas and the Moral Foundations Questionnaire. Proprietary models (GPT-3.5, GPT-4, Claude-3, Gemini) exhibited stronger utilitarian tendencies while open models (Gemma, Llama-2, Starling-LM) aligned more with deontological values. All models except Llama-2 showed a liberal moral bias, scoring high on Harm/Care and Fairness/Reciprocity but low on Ingroup/Loyalty, Authority/Respect, and Purity/Sanctity. The study also introduced SARA, a similarity-based activation steering technique that successfully shifted responses toward target ethical frameworks by adjusting neuron activations.

## Method Summary
The study collected responses from eight LLMs using ethical dilemma prompts (5 repetitions) and Moral Foundations Questionnaire questions (20 repetitions). Responses were classified into eight ethical schools using GPT-4 and Claude-3 as scorers with canonical classification prompts. SARA was implemented on Gemma-2B using target and repel prompts representing Kantian and Utilitarian frameworks, applying activation patching at each layer and measuring classification shifts through similarity-based rescaling of SVD-aligned activation matrices.

## Key Results
- Proprietary models (GPT-3.5, GPT-4, Claude-3, Gemini) showed stronger utilitarian tendencies than open models
- All models except Llama-2 exhibited liberal moral bias (high Harm/Care and Fairness/Reciprocity, low traditional values)
- SARA successfully steered Gemma-2B responses toward target ethical frameworks through activation adjustment
- Models showed low consistency (<60%) in ethical reasoning across repeated prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Proprietary LLMs show stronger utilitarian alignment due to training data and design goals prioritizing consequence-based reasoning
- Mechanism: Training datasets likely over-represent Western philosophical traditions and media narratives emphasizing outcome optimization
- Core assumption: Training corpora and alignment processes embed implicit cultural and philosophical biases
- Evidence anchors: Abstract findings on proprietary vs open model tendencies
- Break condition: If proprietary models are retrained on balanced deontological datasets

### Mechanism 2
- Claim: SARA reliably steers moral reasoning by adjusting neuron activations toward target prompts
- Mechanism: SVD-aligned activation matrices compared via cosine similarity with rescaling factors pushing responses toward desired frameworks
- Core assumption: High-dimensional activation spaces preserve meaningful moral feature representations
- Evidence anchors: Abstract success claims and layer 14 intervention results
- Break condition: If steering introduces nonsensical outputs or similarity metrics fail

### Mechanism 3
- Claim: LLMs exhibit liberal moral bias due to over-representation of WEIRD moral foundations in training
- Mechanism: Western liberal media and academic sources over-represent Harm/Care and Fairness/Reciprocity
- Core assumption: Moral foundation scores directly reflect training distribution skews
- Evidence anchors: Abstract findings on liberal bias across models
- Break condition: If models are fine-tuned on balanced cross-cultural moral datasets

## Foundational Learning

- Concept: SVD-based activation alignment
  - Why needed here: SARA relies on reducing activation matrices to comparable dimensions before similarity computation
  - Quick check question: What is the purpose of truncating SVD to the minimum token count across prompts?

- Concept: Cosine similarity in high-dimensional activation spaces
  - Why needed here: Steering magnitude depends on how aligned or divergent prompt activations are
  - Quick check question: How does cosine similarity differ from Euclidean distance in this steering context?

- Concept: Moral Foundations Theory (MFT)
  - Why needed here: Provides framework for interpreting LLM moral profiles via five foundation scores
  - Quick check question: Which MFT foundation is most under-represented in liberal-biased models?

## Architecture Onboarding

- Component map: LLM -> prompt encoder -> activation layers -> steering module (SVD + similarity -> rescaling) -> output decoder
- Critical path: Prompt -> activation extraction -> SVD alignment -> similarity computation -> rescaling -> intervention -> inference
- Design tradeoffs: SARA trades computational overhead for fine-grained steering without retraining; less scalable for real-time use
- Failure signatures: (1) Steering produces incoherent outputs; (2) No change in moral classification; (3) Oversteering into unintended frameworks
- First 3 experiments:
  1. Test SARA steering on Gemma-2B with toy moral dilemmas, measure classification shift
  2. Vary SVD component count to find optimal balance between steering strength and coherence
  3. Apply SARA across multiple layers to map layer-wise effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the cultural background of LLM developers influence the moral biases observed in their models?
- Basis in paper: [inferred] from proprietary models aligning more with Western utilitarian ethics
- Why unresolved: Paper identifies moral biases but doesn't trace them to cultural or institutional factors
- What evidence would resolve it: Comparative analysis of development teams' cultural backgrounds across LLM projects

### Open Question 2
- Question: Can the variability in ethical reasoning across model responses be systematically reduced or controlled?
- Basis in paper: [explicit] from finding that models show low consistency (<60%) in ethical reasoning
- Why unresolved: Paper demonstrates variability exists but doesn't explore methods to reduce it while maintaining flexibility
- What evidence would resolve it: Controlled experiments varying temperature, fine-tuning, or intervention techniques

### Open Question 3
- Question: How do different activation steering techniques compare in terms of effectiveness and unintended consequences?
- Basis in paper: [explicit] from comparison between SARA and Activation Addition methods
- Why unresolved: Paper only compares two methods without comprehensive evaluation across multiple approaches
- What evidence would resolve it: Systematic comparison of various steering techniques across multiple ethical scenarios

## Limitations

- Training data bias claims lack direct evidence linking observed biases to corpus composition
- SARA validation limited to one model size and architecture without ablation studies
- Ethical school classification depends on GPT-4 and Claude-3 judgments with limited inter-scorer reliability details

## Confidence

**High Confidence**:
- Identification of systematic moral foundation differences between proprietary and open LLMs
- Observation of liberal moral bias across all tested models except Llama-2
- SARA's ability to shift Gemma-2B responses toward target ethical frameworks

**Medium Confidence**:
- Causal link between training data composition and moral foundation scores
- SARA's effectiveness generalizing beyond Gemma-2B to other models/architectures
- Claim that proprietary models are inherently more utilitarian due to design goals

**Low Confidence**:
- Specific mechanisms by which SVD-based activation steering captures moral semantics
- Long-term stability and coherence of steered responses
- Absence of moral foundation auditing in training corpora as primary cause of observed biases

## Next Checks

1. **Cross-model SARA Validation**: Apply SARA steering across diverse model architectures and measure classification shift consistency, testing at least three different model families with varying parameter counts.

2. **Training Data Moral Foundation Audit**: Conduct quantitative analysis of training corpus moral foundation composition, comparing prevalence of Harm/Care, Fairness/Reciprocity, and traditional values across Western vs. non-Western sources.

3. **Inter-scorer Agreement Analysis**: Implement blind classification studies with multiple human annotators using the same ethical dilemma prompts, calculating inter-rater reliability metrics to establish subjectivity baseline.