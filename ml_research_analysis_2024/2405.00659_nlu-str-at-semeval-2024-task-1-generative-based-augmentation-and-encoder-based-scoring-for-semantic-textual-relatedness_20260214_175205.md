---
ver: rpa2
title: 'NLU-STR at SemEval-2024 Task 1: Generative-based Augmentation and Encoder-based
  Scoring for Semantic Textual Relatedness'
arxiv_id: '2405.00659'
source_url: https://arxiv.org/abs/2405.00659
tags:
- track
- semantic
- relatedness
- arabic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles semantic textual relatedness (STR), which measures\
  \ how much two text segments share meaning, topic, or context\u2014broader than\
  \ semantic similarity. The authors participate in SemEval-2024 Task 1, covering\
  \ three Arabic dialects: MSA, Algerian, and Moroccan."
---

# NLU-STR at SemEval-2024 Task 1: Generative-based Augmentation and Encoder-based Scoring for Semantic Textual Relatedness

## Quick Facts
- arXiv ID: 2405.00659
- Source URL: https://arxiv.org/abs/2405.00659
- Reference count: 18
- Primary result: 1st place in MSA, 5th in Moroccan, 12th in Algerian for SemEval-2024 Task 1 STR

## Executive Summary
This paper presents a system for Semantic Textual Relatedness (STR) in Arabic dialects, participating in SemEval-2024 Task 1. The authors address three Arabic dialects: Modern Standard Arabic (MSA), Algerian, and Moroccan. Their approach combines supervised fine-tuning of BERT-based models with unsupervised cosine similarity scoring using average pooling. To enhance performance, they employ generative augmentation via Google Gemini to produce additional Moroccan Arabic sentence pairs, assigning them matching scores. The system achieved top rankings across all dialects, with the data augmentation strategy notably improving Moroccan Arabic results.

## Method Summary
The authors employ a dual approach for STR: supervised fine-tuning of BERT-based models on the provided training data, and unsupervised scoring using cosine similarity with average pooling of sentence embeddings. To improve performance, especially for Moroccan Arabic, they generate additional training data using Google Gemini. This generative-based augmentation produces sentence pairs that mimic the original style and meaning, which are then assigned matching scores. The augmented data is used to fine-tune the models further, enhancing their ability to capture semantic relatedness across dialects.

## Key Results
- Ranked 1st in MSA with Spearman correlation of 0.49
- Ranked 5th in Moroccan with Spearman correlation of 0.83
- Ranked 12th in Algerian with Spearman correlation of 0.53
- Data augmentation notably improved results for Moroccan Arabic

## Why This Works (Mechanism)
The system leverages both supervised and unsupervised approaches to capture semantic relatedness across Arabic dialects. Fine-tuning BERT-based models on dialect-specific data allows the system to learn dialect nuances, while unsupervised cosine similarity with average pooling provides a robust baseline scoring mechanism. The generative augmentation via Google Gemini expands the training data, particularly for Moroccan Arabic, enabling the model to generalize better and capture more diverse semantic relationships. This combination of techniques addresses the challenges of dialectal variation and limited training data, leading to improved performance across all dialects.

## Foundational Learning
- **Semantic Textual Relatedness (STR)**: Measures how much two text segments share meaning, topic, or context, broader than semantic similarity. Needed to understand the task's scope beyond simple similarity. Quick check: Ensure the system can handle broader semantic relationships, not just lexical similarity.
- **BERT-based models**: Pre-trained language models fine-tuned for specific tasks. Needed for capturing contextual and semantic nuances in Arabic dialects. Quick check: Verify that the model architecture supports dialect-specific fine-tuning.
- **Data augmentation**: Generating additional training data to improve model generalization. Needed to address limited training data, especially for Moroccan Arabic. Quick check: Assess the quality and relevance of augmented data to avoid introducing bias or artifacts.
- **Cosine similarity with average pooling**: Unsupervised method for scoring semantic relatedness using sentence embeddings. Needed as a robust baseline scoring mechanism. Quick check: Confirm that the pooling strategy effectively captures sentence-level semantics.
- **Google Gemini for generative augmentation**: Using a large language model to generate synthetic training examples. Needed to expand training data with dialect-specific examples. Quick check: Evaluate the consistency and quality of generated examples with original data.
- **Dialectal variation in Arabic**: Understanding the linguistic differences between MSA, Algerian, and Moroccan Arabic. Needed to tailor models and data augmentation strategies. Quick check: Analyze model performance across dialects to identify specific challenges.

## Architecture Onboarding
**Component Map**: BERT-based model -> Fine-tuning on dialect-specific data -> Cosine similarity scoring -> Generative augmentation (Google Gemini) -> Additional fine-tuning
**Critical Path**: Data preparation -> BERT fine-tuning -> Scoring (supervised/unsupervised) -> Augmentation (Moroccan only) -> Final evaluation
**Design Tradeoffs**: Balancing between supervised fine-tuning (requires labeled data) and unsupervised scoring (no labels but may miss nuances). Generative augmentation improves Moroccan results but introduces potential bias and reproducibility issues.
**Failure Signatures**: Low Spearman scores indicate poor semantic capture; overfitting to training data if augmentation doesn't generalize; bias if generated examples don't match original style.
**First Experiments**:
1. Compare supervised vs. unsupervised scoring on validation sets to identify baseline performance gaps.
2. Evaluate the quality and relevance of augmented Moroccan data through manual inspection and automated metrics.
3. Test statistical significance of performance differences between baseline, fine-tuned, and augmented runs using bootstrap resampling.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions.

## Limitations
- Moderate Spearman scores (0.49 for MSA, 0.53 for Algerian) indicate challenges in capturing nuanced semantic relatedness across dialects.
- Use of Google Gemini for augmentation introduces reproducibility and potential bias concerns.
- Lack of statistical significance testing between model runs makes it difficult to confirm robustness of improvements.

## Confidence
- Core findings: Medium
- Methodology transparency: High
- Reproducibility of results: Medium
- Significance of improvements: Medium

## Next Checks
1. Conduct statistical significance testing (e.g., bootstrap resampling) between baseline, fine-tuned, and augmented model runs to confirm that performance gains are robust.
2. Perform qualitative analysis of the augmented Moroccan Arabic data to assess whether generated examples genuinely match the original style and meaning, and whether they introduce artifacts or bias.
3. Replicate the augmentation and training pipeline on a held-out subset of the original training data to measure generalization and detect potential overfitting to the full training set.