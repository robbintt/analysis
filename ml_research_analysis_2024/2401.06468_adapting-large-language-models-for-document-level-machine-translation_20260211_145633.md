---
ver: rpa2
title: Adapting Large Language Models for Document-Level Machine Translation
arxiv_id: '2401.06468'
source_url: https://arxiv.org/abs/2401.06468
tags:
- translation
- docmt
- language
- computational
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study adapts moderately-sized large language models (7B parameters)
  for document-level machine translation across nine language pairs. The researchers
  investigate prompt strategies and fine-tuning methods (parameter-efficient fine-tuning
  vs.
---

# Adapting Large Language Models for Document-Level Machine Translation

## Quick Facts
- arXiv ID: 2401.06468
- Source URL: https://arxiv.org/abs/2401.06468
- Authors: Minghao Wu; Thuy-Trang Vu; Lizhen Qu; George Foster; Gholamreza Haffari
- Reference count: 24
- Primary result: 7B parameter LLMs can outperform GPT-4 in certain document-level translation tasks

## Executive Summary
This study investigates adapting moderately-sized large language models (7B parameters) for document-level machine translation across nine language pairs. The researchers systematically compare parameter-efficient fine-tuning (PEFT) against full fine-tuning (FFT) using three different LLM backbones, while also exploring various prompt strategies. The work demonstrates that specialized LLM-based translation models can achieve superior performance compared to GPT-4 in certain conditions, particularly when leveraging PEFT methods. The study reveals important trade-offs between different fine-tuning approaches and highlights the potential of base LLMs for zero-shot cross-lingual transfer tasks.

## Method Summary
The researchers adapted three 7B parameter LLM architectures for document-level translation through both parameter-efficient fine-tuning and full fine-tuning approaches. They evaluated nine different language pairs using a combination of automatic metrics (BLEU, COMET) and human evaluations. The study systematically tested various prompt strategies including chain-of-thought, few-shot examples, and task descriptions. Evaluation was conducted across multiple domains including general, medical, and IT technical texts to assess both in-domain and out-of-domain generalization capabilities. The experimental design included direct comparisons between LLM-based models and both conventional NMT systems and GPT-4.

## Key Results
- PEFT methods outperformed FFT overall, though FFT showed better data efficiency with limited training data
- Base LLMs demonstrated superior performance to instruction-tuned models for zero-shot cross-lingual transfer
- LLM-based models showed better generalization on out-of-domain text compared to conventional translation models
- Specialized models achieved competitive or superior performance to GPT-4 in certain translation tasks

## Why This Works (Mechanism)
LLMs possess inherent cross-lingual capabilities and contextual understanding that can be leveraged for document-level translation. Their attention mechanisms and transformer architecture enable them to maintain coherence across longer text sequences compared to conventional NMT models. The pre-training on diverse multilingual corpora provides a strong foundation for zero-shot transfer, while fine-tuning allows adaptation to specific translation tasks and domains. Parameter-efficient methods preserve most of the original model's capabilities while adapting only task-relevant parameters, making them computationally efficient and potentially more robust to overfitting on limited data.

## Foundational Learning
1. Document-level vs sentence-level translation
   - Why needed: Captures cross-sentence context and coherence
   - Quick check: Evaluate model performance on texts requiring pronoun resolution across sentences

2. Parameter-efficient fine-tuning methods
   - Why needed: Reduce computational cost while maintaining performance
   - Quick check: Compare FLOPs and memory usage between PEFT and FFT during training

3. Cross-lingual transfer learning
   - Why needed: Enables translation between language pairs not seen during training
   - Quick check: Test zero-shot performance on language pairs absent from fine-tuning data

## Architecture Onboarding

Component map: Input text -> LLM backbone -> Attention layers -> Output generation -> Evaluation metrics

Critical path: Document preprocessing → Prompt engineering → Model inference → Post-processing → Quality assessment

Design tradeoffs: Base LLMs vs instruction-tuned models for zero-shot tasks; PEFT vs FFT for fine-tuning efficiency; computational cost vs translation quality

Failure signatures: Off-target translations due to error propagation; degradation in out-of-domain performance; prompt sensitivity affecting output consistency

First experiments:
1. Benchmark base LLM zero-shot performance across all nine language pairs
2. Compare PEFT and FFT performance using limited training data
3. Test different prompt strategies on a subset of language pairs

## Open Questions the Paper Calls Out
The study identifies several areas requiring further investigation, including optimal prompt strategies for document-level translation, the impact of model size on translation quality, and the mechanisms behind error propagation during decoding that lead to off-target translations. The authors also highlight the need to explore the generalization capabilities of LLM-based translation models across more diverse language families and domains.

## Limitations
- Findings based on only nine language pairs, limiting generalizability
- 7B parameter models may not reflect performance of larger LLM architectures
- Limited exploration of optimal prompt strategies for document-level translation
- Error propagation mechanisms during decoding not fully characterized

## Confidence

High confidence:
- Specialized LLM-based models can outperform GPT-4 in certain translation tasks
- LLM-based models generalize better on out-of-domain text compared to conventional models

Medium confidence:
- PEFT generally outperforms FFT, though exceptions exist for data efficiency
- Base LLMs outperform instruction-tuned models for zero-shot cross-lingual transfer

Low confidence:
- Specific mechanisms behind error propagation during decoding
- Optimal prompt strategies for document-level translation

## Next Checks

1. Conduct experiments with additional language pairs and larger LLM architectures (13B, 30B parameters) to assess generalizability across different model scales and language combinations.

2. Perform ablation studies isolating effects of different prompt strategies (chain-of-thought, few-shot examples, task descriptions) on translation quality.

3. Implement controlled experiments comparing zero-shot performance between base and instruction-tuned models across multiple document-level translation tasks.