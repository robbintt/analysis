---
ver: rpa2
title: 'SciAgent: Tool-augmented Language Models for Scientific Reasoning'
arxiv_id: '2402.11451'
source_url: https://arxiv.org/abs/2402.11451
tags:
- functions
- function
- return
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce tool-augmented scientific reasoning, a task to solve
  scientific problems with the help of external tools. To facilitate the research
  of such task, we construct a tool-augmented training corpus named MATHFUNC and a
  benchmark named SciToolBench covering 5 scientific domains.
---

# SciAgent: Tool-augmented Language Models for Scientific Reasoning

## Quick Facts
- arXiv ID: 2402.11451
- Source URL: https://arxiv.org/abs/2402.11451
- Reference count: 32
- SciAgent-Mistral-7B surpasses other LLMs of the same size by more than 13% in absolute accuracy on scientific reasoning tasks

## Executive Summary
SciAgent introduces a tool-augmented approach to scientific reasoning that enhances language models with external Python functions to solve complex scientific problems. The system combines a retriever for finding relevant functions, a planner for generating solution strategies, and an actor for integrating functions into executable solutions. The authors construct MATHFUNC, a training corpus of 31,375 samples with function-augmented solutions, and evaluate their approach on SciToolBench, a benchmark covering 5 scientific domains with 856 questions and 2,446 functions.

## Method Summary
The method involves constructing a tool-augmented training corpus (MATHFUNC) using GPT-4 to generate function-augmented solutions from mathematical problems, creating a domain-specific toolset of Python functions. Open-source LLMs (CodeLlama, Mistral, Deepseek-Math) are fine-tuned on this corpus using imitation learning to learn planning, retrieval, and action modules. During inference, the system generates a high-level plan, retrieves relevant functions from the toolset using dense passage retrieval, generates function-augmented solutions, and executes them to produce final answers. The approach is evaluated on the SciToolBench benchmark and CREATION Challenge, showing superior performance over baseline models.

## Key Results
- SciAgent-Mistral-7B achieves 63.8% accuracy on SciToolBench, surpassing other 7B models by over 13% absolute
- SciAgent-DeepMath-7B outperforms ChatGPT on both SciToolBench and CREATION Challenge
- The planning module significantly improves performance, with planning-enhanced models outperforming non-planning variants by 6.7% on SciToolBench

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tool-augmented scientific reasoning improves LLM performance by offloading domain-specific reasoning to specialized functions
- Mechanism: LLMs generate a high-level plan, retrieve relevant functions from a domain-specific toolset, and integrate these functions into their solution generation process. This allows LLMs to focus on mathematical reasoning while leveraging external tools for domain-specific knowledge.
- Core assumption: The toolset contains generalized, well-documented functions that can be effectively retrieved and used by LLMs
- Evidence anchors:
  - [abstract] "This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user."
  - [section 3.2] "The cross-retrieval strategy eliminates the likelihood of calling ad-hoc functions from ~Fq in Sq."
- Break Condition: If the toolset lacks relevant or generalized functions, or if LLMs cannot effectively retrieve and use them

### Mechanism 2
- Claim: MATHFUNC training corpus enables LLMs to learn both mathematical skills and tool-use abilities
- Mechanism: MATHFUNC contains function-augmented solutions that interleave natural language rationales and Python code calling functions from the toolset. LLMs are fine-tuned on this corpus to learn how to properly retrieve, understand, and use functions.
- Core assumption: Function-augmented solutions in MATHFUNC are of high quality and generalize well to scientific domains
- Evidence anchors:
  - [section 3.2] "We employ an automatic pipeline to construct MATHFUNC... designed to enable LLMs to learn both essential math skills and how to retrieve, understand and use functions properly."
  - [section 6.4] "The absence of function-augmented solutions results in a performance drop... It underscores the critical role of function-augmented solutions to enhance LLMs' tool-use abilities."
- Break Condition: If MATHFUNC lacks sufficient diversity or quality in its function-augmented solutions

### Mechanism 3
- Claim: The planning module improves scientific reasoning by providing targeted queries for function retrieval
- Mechanism: LLMs generate a high-level plan for each question, which is used as part of the query for retrieving relevant functions. This increases the relatedness of retrieved functions and improves reasoning.
- Core assumption: The planning module generates accurate and relevant plans that guide effective function retrieval
- Evidence anchors:
  - [section 6.4] "Planning module significantly improves scientific reasoning abilities... As detailed and targeted queries for the retriever, the generated plannings increase the relatedness of retrieved functions."
  - [section 6.5] "Our agents with weak-related toolsets significantly outperform the two LLMs, which further validates the robustness."
- Break Condition: If the planning module generates inaccurate or irrelevant plans

## Foundational Learning

- **Function-augmented learning**
  - Why needed here: Enables LLMs to learn how to use external tools effectively for scientific reasoning
  - Quick check question: What is the difference between function-augmented learning and traditional supervised learning?

- **Dense passage retrieval**
  - Why needed here: Allows efficient retrieval of relevant functions from a large toolset based on the question and planning
  - Quick check question: How does dense passage retrieval differ from keyword-based retrieval?

- **Instruction tuning**
  - Why needed here: Fine-tunes LLMs to follow specific instructions for planning and action modules in SciAgent
  - Quick check question: What is the difference between instruction tuning and prompt engineering?

## Architecture Onboarding

- **Component map**: Question -> Planning -> Retrieval -> Action -> Execution -> Answer
- **Critical path**: Question flows through planning module, retriever, actor module, and executor to produce final answer
- **Design tradeoffs**: Cross-retrieval strategy vs. direct function generation; function-augmented solutions vs. function-free solutions in training data; fine-tuning vs. prompting for tool-use
- **Failure signatures**: 
  - Retriever: Low recall or precision in function retrieval
  - Planner: Inaccurate or irrelevant plans
  - Actor: Failure to properly use retrieved functions or generate executable code
  - Executor: Runtime errors or incorrect answers
- **First 3 experiments**:
  1. Evaluate retriever recall and precision on a held-out test set of questions and functions
  2. Ablation study: Compare performance with and without the planning module
  3. Stress test: Evaluate performance with unrelated or weak-related toolsets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the generalizability of functions in the toolset be ensured when constructing them from questions in different scientific domains?
- Basis in paper: Inferred from the discussion of challenges in creating domain-specific toolsets that avoid being too ad-hoc or specific to individual questions
- Why unresolved: The paper mentions manually checking and refining functions for generalizability, but does not provide a systematic method or framework for ensuring this property across diverse domains
- What evidence would resolve it: A detailed methodology or algorithm for assessing and improving the generalizability of functions during construction, validated on a wide range of scientific domains

### Open Question 2
- Question: What is the optimal balance between providing domain-specific toolsets and relying on the LLM's inherent reasoning abilities?
- Basis in paper: Explicit discussion of the trade-off between tool-augmented and non-tool approaches, with observations on performance gains from toolsets but also noting limitations
- Why unresolved: The paper presents empirical results but does not provide a principled framework for determining when toolsets are beneficial or how to select the most appropriate tools for a given problem
- What evidence would resolve it: A theoretical model or empirical study that characterizes the conditions under which toolsets improve performance, along with guidelines for toolset design and selection

### Open Question 3
- Question: How can the retriever's performance be further improved to increase the likelihood of retrieving relevant functions for scientific reasoning tasks?
- Basis in paper: Explicit discussion of the importance of the retriever and observations on the positive correlation between hit ratio and task accuracy
- Why unresolved: The paper compares different retriever variants but does not explore advanced techniques such as incorporating domain knowledge, using more sophisticated retrieval architectures, or leveraging the LLM's own understanding of the question
- What evidence would resolve it: An ablation study or comparison of various retriever enhancements, demonstrating their impact on retrieval performance and downstream task accuracy

## Limitations
- The approach's effectiveness heavily depends on the quality and coverage of the external function toolset
- Limited testing on real-world scientific problems beyond mathematical domains
- Reliance on GPT-4 for MATHFUNC construction raises questions about generalization to other scientific domains

## Confidence
**High Confidence**: The effectiveness of the planning module in improving scientific reasoning; the superiority of SciAgent-Mistral-7B over other LLMs of similar size; the importance of function-augmented solutions in training

**Medium Confidence**: The robustness of the approach across different scientific domains; the scalability of the toolset-based approach to other scientific reasoning tasks; the general applicability of the cross-retrieval strategy

**Low Confidence**: Claims about performance in completely unseen scientific domains; assertions about the approach's superiority over all existing methods; predictions about performance with significantly larger or different toolsets

## Next Checks
1. **Cross-Domain Generalization Test**: Evaluate SciAgent on a benchmark containing scientific problems from domains not represented in MATHFUNC (e.g., quantum mechanics, organic chemistry, or materials science) to assess true generalization capabilities.

2. **Toolset Coverage Analysis**: Systematically remove functions from the toolset to measure performance degradation, identifying the minimum viable toolset size and the criticality of specific function categories to scientific reasoning tasks.

3. **Human Evaluation Study**: Conduct a user study where domain experts assess the quality and relevance of SciAgent's solutions compared to human-generated solutions, focusing on scientific accuracy and reasoning quality rather than just accuracy metrics.