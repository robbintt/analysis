---
ver: rpa2
title: Lecture notes on rough paths and applications to machine learning
arxiv_id: '2404.06583'
source_url: https://arxiv.org/abs/2404.06583
tags:
- signature
- paths
- then
- where
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: These lecture notes present a unified framework for applying rough
  path theory and the signature transform to machine learning with sequential data.
  The authors develop the core mathematical foundations of the signature transform,
  demonstrating its role as a universal feature set for continuous functions on path
  space.
---

# Lecture notes on rough paths and applications to machine learning

## Quick Facts
- arXiv ID: 2404.06583
- Source URL: https://arxiv.org/abs/2404.06583
- Reference count: 0
- These lecture notes present a unified framework for applying rough path theory and the signature transform to machine learning with sequential data.

## Executive Summary
These lecture notes provide a comprehensive introduction to rough path theory and its applications to machine learning with sequential data. The authors develop the mathematical foundations of the signature transform, demonstrating its universal approximation properties and computational methods through partial differential equations. The notes then extend these concepts to neural differential equations, showing how rough path theory unifies Neural ODEs, CDEs, SDEs, and RDEs under a common theoretical framework. Throughout, the material is presented with an emphasis on practical applications and computational considerations, making it accessible to researchers from diverse backgrounds in mathematics, data science, and engineering.

## Method Summary
The lecture notes develop signature kernel methods for sequential data by computing truncated signatures using existing libraries, implementing finite difference schemes for the signature kernel PDE, and applying these kernels to kernel ridge regression tasks. The notes also establish a unified framework for neural differential equations by treating Neural ODEs, CDEs, SDEs, and RDEs as controlled differential equations, implementing numerical solvers using the log-ODE method, and developing adjoint-based backpropagation techniques. The methodology combines theoretical foundations with practical implementation details, emphasizing efficient computation and numerical stability across different path regularities.

## Key Results
- The signature transform acts as a universal feature set for continuous functions on path space through iterated integrals forming a complete basis
- Signature kernels can be efficiently computed via PDE solvers without explicitly evaluating high-dimensional signature features
- Rough path theory provides a unified theoretical framework for neural differential equations, enabling consistent numerical solvers and backpropagation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The signature transform acts as a universal feature set for continuous functions on path space, enabling nonlinear regression on sequential data.
- Mechanism: The signature transform captures all information about a path up to tree-like equivalence through iterated integrals. These integrals form a complete basis for continuous functions on unparameterised path space, allowing any continuous function to be approximated arbitrarily well.
- Core assumption: The Stone-Weierstrass theorem applies to the algebra generated by signature features on compact subsets of unparameterised path space.
- Evidence anchors:
  - [abstract]: "The signature transform, demonstrating its role as a universal feature set for continuous functions on path space"
  - [section]: "The coordinate iterated integrals emerge as a universal feature set, or 'basis', for functions on path space, and serve as a foundation for functional approximation"
  - [corpus]: Weak evidence - no direct neighbor papers discuss the universal approximation property
- Break condition: If the function space contains discontinuities or the topology on unparameterised paths doesn't satisfy Stone-Weierstrass conditions.

### Mechanism 2
- Claim: Signature kernels provide efficient computation of inner products without explicitly computing high-dimensional signature features.
- Mechanism: Weighted inner products of signatures define kernel functions that can be computed through solving partial differential equations (PDEs), avoiding direct evaluation of the signature transform at high levels.
- Core assumption: The PDE representation of signature kernels is numerically stable and converges to the true kernel as the discretization is refined.
- Evidence anchors:
  - [abstract]: "signature kernel partial differential equation (PDE) for efficient computation"
  - [section]: "For a certain class of signature kernels, this can be achieved through solving a partial differential equation known as the signature kernel PDE"
  - [corpus]: Moderate evidence - neighbor paper "Log-PDE Methods for Rough Signature Kernels" directly addresses PDE-based computation
- Break condition: When input paths are too rough for the PDE to remain stable, or when the weighting function doesn't satisfy summability conditions.

### Mechanism 3
- Claim: Rough path theory provides a unified framework for neural differential equations, enabling training of continuous-time deep learning models.
- Mechanism: By viewing neural ODEs, CDEs, SDEs, and RDEs as controlled differential equations driven by different types of paths (smooth, rough, stochastic), rough path theory provides consistent numerical solvers and backpropagation methods.
- Core assumption: The universal limit theorem holds for the neural network vector fields, ensuring that solutions to neural RDEs are well-defined and trainable.
- Evidence anchors:
  - [abstract]: "rough path theory provides a unified framework for neural differential equations (Neural ODEs, CDEs, SDEs, and RDEs)"
  - [section]: "Rough path theory offers a way of analysing them under the same theoretical frameworks"
  - [corpus]: Weak evidence - neighbor papers don't discuss neural differential equations
- Break condition: If the neural network vector fields don't satisfy the required regularity conditions (e.g., not Lipschitz or not in Lipγ for γ > p).

## Foundational Learning

- Concept: Tensor algebras and their universal properties
  - Why needed here: The signature transform lives in tensor algebras, and understanding their algebraic structure is crucial for manipulating signatures and defining inner products
  - Quick check question: Why can any multilinear map be uniquely extended to an algebra homomorphism on the tensor algebra?

- Concept: Rough path theory and p-variation
  - Why needed here: Rough paths generalize smooth paths and provide the mathematical foundation for handling highly irregular driving signals in neural differential equations
  - Quick check question: What is the relationship between the regularity of a rough path and the number of iterated integrals needed to characterize it?

- Concept: Reproducing kernel Hilbert spaces (RKHS)
  - Why needed here: Signature kernels define RKHSs that provide the theoretical foundation for kernel methods applied to sequential data
  - Quick check question: How does the reproducing property of a kernel ensure that the kernel mean embedding uniquely characterizes probability measures?

## Architecture Onboarding

- Component map:
  Signature computation module -> Kernel evaluation module -> Neural CDE/RDE module -> Backpropagation module -> Loss computation module

- Critical path:
  1. Compute signature features for input paths
  2. Compute kernel matrix for training data
  3. Solve optimization problem using kernel methods or neural differential equations
  4. Backpropagate gradients through signature computation or neural CDE/RDE solver
  5. Update model parameters

- Design tradeoffs:
  - Computational efficiency vs. accuracy: Higher signature levels provide better approximation but increase computational cost exponentially
  - Numerical stability vs. generality: PDE-based kernel computation is efficient but may not work for all weight functions or very rough paths
  - Memory usage vs. training speed: Adjoint-based backpropagation is memory-efficient but may be slower than direct backpropagation through ODE solvers

- Failure signatures:
  - Exploding gradients or NaNs in neural CDE/RDE training: Indicates insufficient regularity of neural network vector fields
  - Kernel matrix becoming singular: Suggests insufficient diversity in training data or inappropriate kernel choice
  - PDE solver diverging: Points to too rough input paths or inappropriate discretization parameters

- First 3 experiments:
  1. Implement signature computation for simple 1D paths and verify Chen's relation
  2. Compute signature kernel via direct inner product and PDE solver for simple 2D paths
  3. Train a simple Neural CDE on synthetic sequential data and verify gradient flow

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can signature kernels be made characteristic for non-compactly supported measures while maintaining computational efficiency?
- Basis in paper: [explicit] The paper notes that characteristicness fails for non-compactly supported measures like the Wiener measure, though some results ensure dϕ(W, ν) = 0 if and only if W = ν. It mentions ways to restore characteristicness but doesn't specify efficient computational methods.
- Why unresolved: The paper acknowledges the limitation but doesn't provide a concrete solution that balances characteristicness with computational tractability for non-compactly supported measures.
- What evidence would resolve it: A proposed method that maintains characteristicness for non-compactly supported measures while providing efficient computational algorithms (e.g., Monte Carlo or Gaussian quadrature approaches with proven convergence rates).

### Open Question 2
- Question: What is the optimal trade-off between log-signature depth and computational efficiency for Neural RDE training?
- Basis in paper: [explicit] The paper discusses the log-ODE method for numerical RDE solvers and mentions that higher convergence rates can be achieved through using more terms in each log-signature, but doesn't provide specific guidance on optimal depth selection.
- Why unresolved: While the paper establishes the theoretical foundation, it doesn't provide empirical studies or guidelines for choosing log-signature depth in practical Neural RDE implementations.
- What evidence would resolve it: Empirical studies comparing training performance, memory usage, and computational time across different log-signature depths for various Neural RDE architectures on benchmark datasets.

### Open Question 3
- Question: Can universal approximation theorems be established for Neural SDEs in the same way as Neural CDEs and Neural ODEs?
- Basis in paper: [inferred] The paper establishes universality for Neural CDEs and Neural ODEs but only mentions Neural SDEs as generative models without providing a universal approximation result.
- Why unresolved: The paper treats Neural SDEs primarily as generative models rather than as universal approximators, leaving open whether they can approximate arbitrary continuous functions on path space.
- What evidence would resolve it: A formal proof showing that Neural SDEs with sufficiently complex drift and diffusion terms can approximate any continuous function on path space, similar to the results for Neural CDEs.

## Limitations

- The universal approximation claim for signature features on unparameterised path space relies heavily on the applicability of Stone-Weierstrass theorem in this specific topological setting, with practical limitations for highly irregular paths or discontinuous functions remaining underexplored.
- The stability and convergence properties of PDE-based signature kernel computation for extremely rough paths have not been fully characterized, potentially limiting practical applicability.
- The theoretical framework, while mathematically sound, lacks extensive empirical validation across diverse real-world sequential data problems.

## Confidence

- High confidence: The algebraic properties of signatures and their representation in tensor algebras are well-established mathematical facts with extensive literature support.
- Medium confidence: The computational methods for signature kernels via PDEs are theoretically sound, but practical implementation details and numerical stability for various path classes require further validation.
- Low confidence: The universal approximation claim for signature features in machine learning applications, while theoretically appealing, lacks extensive empirical validation across diverse real-world sequential data problems.

## Next Checks

1. **Empirical convergence test**: Implement signature-based kernel ridge regression on synthetic sequential data with varying levels of regularity, measuring approximation error against ground truth functions as signature level increases.

2. **Numerical stability analysis**: Compare PDE-based signature kernel computation with direct truncated signature computation across a range of path regularities, identifying the threshold where PDE methods break down.

3. **Practical universal approximation test**: Apply signature-based methods to a diverse set of real-world sequential datasets (time series, handwriting, financial data) and compare performance against established sequence learning methods like RNNs and Transformers.