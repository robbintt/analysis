---
ver: rpa2
title: 'PAL: Proxy-Guided Black-Box Attack on Large Language Models'
arxiv_id: '2402.09674'
source_url: https://arxiv.org/abs/2402.09674
tags:
- attack
- target
- loss
- attacks
- black-box
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a black-box attack on Large Language Models
  (LLMs) that uses a surrogate model to guide token-level optimization. The attack
  addresses the difficulty of directly computing loss functions for proprietary LLM
  APIs by introducing techniques to extract log probabilities and optimize queries
  efficiently.
---

# PAL: Proxy-Guided Black-Box Attack on Large Language Models

## Quick Facts
- arXiv ID: 2402.09674
- Source URL: https://arxiv.org/abs/2402.09674
- Reference count: 40
- Attack success rate: 84% on GPT-3.5-Turbo, 48% on Llama-2-7B

## Executive Summary
This paper introduces PAL (Proxy-Guided Attack on LLMs), a novel black-box attack framework that leverages a surrogate white-box model to optimize adversarial suffixes for generating harmful responses from Large Language Models. By extracting log probabilities and efficiently querying the target model, PAL overcomes the challenge of directly computing loss functions for proprietary LLM APIs. The approach achieves significantly higher attack success rates compared to previous methods, with 84% success on GPT-3.5-Turbo and 48% on Llama-2-7B.

## Method Summary
PAL employs a surrogate model to guide token-level optimization in black-box attacks on LLMs. The attack begins with an initial adversarial suffix and iteratively optimizes tokens by leveraging the surrogate model's gradients. To overcome the inability to directly compute loss functions for proprietary LLM APIs, PAL uses the logit bias trick to extract log probabilities and compute loss for target tokens. The algorithm also incorporates a candidate-ranking heuristic to reduce query costs by selecting the most promising token candidates. Optionally, the proxy model can be fine-tuned on the target model's responses to improve attack effectiveness.

## Key Results
- PAL achieves 84% attack success rate on GPT-3.5-Turbo and 48% on Llama-2-7B
- GCG++ white-box attack reaches 94% ASR on Llama-2-7B
- RAL baseline achieves 26% ASR on Llama-2-7B
- PAL significantly outperforms prior state-of-the-art black-box attack methods

## Why This Works (Mechanism)
PAL works by leveraging a surrogate model to guide the optimization process in black-box attacks on LLMs. The surrogate model, which has access to the target model's architecture and parameters, computes gradients and provides guidance for token-level optimization. By extracting log probabilities from the target model using the logit bias trick, PAL can compute loss for target tokens not in the top-5, enabling more precise optimization. The candidate-ranking heuristic further reduces query costs by selecting the most promising token candidates based on the surrogate model's predictions.

## Foundational Learning
- **Logit bias trick**: Used to extract log probabilities from target LLM API, enabling loss computation for tokens not in top-5
- **Surrogate model guidance**: Provides gradients and optimization direction, compensating for lack of direct access to target model's internals
- **Candidate-ranking heuristic**: Reduces query costs by selecting most promising token candidates based on surrogate model's predictions

## Architecture Onboarding
- **Component map**: Initial suffix -> Surrogate model -> Candidate ranking -> Target model API -> Optimized suffix
- **Critical path**: The iterative optimization loop where the surrogate model guides token selection, which is then validated by querying the target model
- **Design tradeoffs**: Balancing attack success rate against query cost, choosing between using a general proxy model or fine-tuning it on target model responses
- **Failure signatures**: Low attack success rate despite high query count, indicating poor surrogate model guidance or ineffective candidate ranking
- **First experiments**: 1) Test PAL on a simple open-source LLM to validate basic functionality, 2) Compare attack success rates with and without proxy model fine-tuning, 3) Measure query cost reduction achieved by candidate-ranking heuristic

## Open Questions the Paper Calls Out
None

## Limitations
- Attack effectiveness depends on the specific characteristics of the target model's safety training and alignment methods
- Exact hyperparameters and parameters used in the original experiments are not specified, making exact reproduction difficult
- Reliance on access to proprietary LLM APIs presents a practical barrier to independent validation

## Confidence
- **High confidence** in the technical feasibility of the PAL attack framework and its core methodology
- **Medium confidence** in the reported attack success rates and cost estimates, given the lack of detailed hyperparameter information and the proprietary nature of some target models
- **Low confidence** in the generalizability of the results to other LLM architectures and safety training methods not tested in the paper

## Next Checks
1. Implement PAL on a range of open-source LLMs with varying sizes and safety training methods to assess the attack's effectiveness across different model architectures
2. Conduct a sensitivity analysis on the attack's performance with respect to key hyperparameters, such as batch sizes and learning rates, to identify optimal configurations
3. Collaborate with LLM providers to gain access to their proprietary models and safety mechanisms, enabling a more comprehensive evaluation of PAL's effectiveness in real-world scenarios