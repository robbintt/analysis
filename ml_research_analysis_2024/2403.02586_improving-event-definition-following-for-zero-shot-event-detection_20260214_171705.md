---
ver: rpa2
title: Improving Event Definition Following For Zero-Shot Event Detection
arxiv_id: '2403.02586'
source_url: https://arxiv.org/abs/2403.02586
tags:
- event
- definition
- trigger
- type
- types
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles zero-shot event detection by improving event
  definition following. The core idea is to construct a diverse event definition (DivED)
  dataset with 3000+ event types, each having 10 diverse definitions and 10 samples.
---

# Improving Event Definition Following For Zero-Shot Event Detection

## Quick Facts
- arXiv ID: 2403.02586
- Source URL: https://arxiv.org/abs/2403.02586
- Reference count: 22
- Primary result: LLaMA-2-7B fine-tuned on DivED dataset achieves SOTA zero-shot event detection on ACE, M2E2, and MEE benchmarks

## Executive Summary
This paper addresses the challenge of zero-shot event detection by improving event definition following. The authors introduce a novel approach that constructs a diverse event definition (DivED) dataset containing over 3000 event types, each with 10 diverse definitions and 10 samples. By fine-tuning a LLaMA-2-7B model on this dataset, they demonstrate state-of-the-art performance on three open benchmarks, surpassing GPT-3.5 while using significantly fewer parameters. The key insight is that diverse event types and definitions are more effective for training models to follow event definitions than focusing on many high-quality examples for a few event types.

## Method Summary
The approach centers on creating the DivED dataset, which contains 3000+ event types with diverse definitions and samples. The authors hypothesize that exposure to a large number of event types with varied definitions helps models learn to follow event definitions rather than memorizing patterns. They fine-tune LLaMA-2-7B on this dataset, incorporating event ontology information and hard-negative samples during training. The fine-tuned model is then evaluated on zero-shot event detection tasks across three benchmarks (ACE, M2E2, MEE), where it outperforms existing methods including GPT-3.5.

## Key Results
- LLaMA-2-7B fine-tuned on DivED achieves state-of-the-art zero-shot event detection performance
- Diverse event types and definitions significantly boost extraction performance compared to quality-focused approaches
- Performance does not scale with more than ten examples per event type
- Incorporating event ontology information and hard-negative samples provides additional performance gains

## Why This Works (Mechanism)
The mechanism behind this approach is that exposure to diverse event definitions teaches the model to recognize the underlying structure and semantics of event definitions rather than memorizing specific patterns. By seeing 10 diverse definitions for each of 3000+ event types, the model learns to abstract the concept of what constitutes an event definition. This generalization enables effective zero-shot detection on unseen event types. The inclusion of event ontology provides hierarchical context, while hard-negative samples improve discrimination between event types.

## Foundational Learning

1. **Event Detection**: Identifying and classifying events in text
   - Why needed: Core task being addressed
   - Quick check: Can the model identify events in unseen text?

2. **Zero-shot Learning**: Performing tasks without task-specific training examples
   - Why needed: Enables generalization to new event types
   - Quick check: Does performance hold on truly unseen event types?

3. **Event Ontology**: Hierarchical structure of event types and relationships
   - Why needed: Provides semantic context for event definitions
   - Quick check: Does incorporating ontology improve detection accuracy?

4. **Fine-tuning Large Language Models**: Adapting pre-trained models to specific tasks
   - Why needed: Leverages existing language understanding for event detection
   - Quick check: Does fine-tuning improve performance over zero-shot prompting?

5. **Hard-negative Sampling**: Including challenging negative examples during training
   - Why needed: Improves model's ability to distinguish between similar event types
   - Quick check: Does inclusion of hard negatives improve precision?

## Architecture Onboarding

Component map: Pre-trained LLM -> Fine-tuning on DivED -> Event Ontology Integration -> Hard-negative Sampling -> Zero-shot Detection

Critical path: The model takes an event definition as input and must identify instances of that event type in text. The critical components are the diverse definitions (providing training signal), event ontology (providing semantic context), and hard-negative samples (providing discrimination training).

Design tradeoffs: The approach trades parameter efficiency (7B vs 175B) for task-specific fine-tuning, and prioritizes definition diversity over example quality. The use of hard-negative sampling increases training complexity but improves discrimination.

Failure signatures: Performance degradation may occur with highly ambiguous event definitions, domain-specific events not represented in DivED, or when event types are semantically very similar without clear distinguishing features.

First experiments: 1) Evaluate model on a held-out subset of DivED to check for overfitting. 2) Test model's ability to generalize to event types with similar but not identical definitions. 3) Assess performance when removing event ontology information to quantify its contribution.

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation relies on a small number of benchmark datasets (ACE, M2E2, MEE)
- Focus on English language events, multilingual generalization unexplored
- Computational resources required for fine-tuning a 7B parameter model may limit accessibility
- Does not thoroughly investigate robustness to noisy or ambiguous event definitions

## Confidence

- Diverse definitions outperform quality examples: High
- Ten examples per event type is optimal: Medium
- Event ontology and hard-negative samples provide additional benefits: Medium

## Next Checks

1. Evaluate the approach on additional event detection benchmarks, particularly those in non-English languages or specialized domains, to assess generalizability.

2. Conduct experiments with varying definition quality within the DivED framework to determine if the "diverse over quality" hypothesis holds when definitions are intentionally degraded.

3. Test the fine-tuned model's performance on event detection tasks with significant label noise or incomplete event definitions to assess robustness in less controlled scenarios.