---
ver: rpa2
title: 'Measuring Political Bias in Large Language Models: What Is Said and How It
  Is Said'
arxiv_id: '2403.18932'
source_url: https://arxiv.org/abs/2403.18932
tags:
- political
- bias
- marriage
- same-sex
- stance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a framework for measuring political bias in
  large language models (LLMs) by analyzing both the content and style of their generated
  text on political topics. The framework first assesses the political stance of LLMs
  on various topics by comparing model generations to reference anchor distributions
  representing opposing political stances.
---

# Measuring Political Bias in Large Language Models: What Is Said and How It Is Said

## Quick Facts
- arXiv ID: 2403.18932
- Source URL: https://arxiv.org/abs/2403.18932
- Authors: Yejin Bang; Delong Chen; Nayeon Lee; Pascale Fung
- Reference count: 29
- Primary result: Framework measures political bias in LLMs through content (what is said) and style (how it is said) analysis across 10 political topics using 11 open-source models

## Executive Summary
This work introduces a comprehensive framework for measuring political bias in large language models by analyzing both the content and style of their generated text on political topics. The framework first assesses political stance by comparing model generations to reference anchor distributions representing opposing political stances. It then decomposes framing bias into content bias (what is said) and style bias (how it is said) by examining frame dimensions and lexical polarity. Experiments on 11 open-sourced LLMs reveal that models exhibit varied stances across topics, with most leaning liberal on issues like gun control and same-sex marriage.

## Method Summary
The framework employs a two-tiered approach to measure political bias in LLMs. First, it analyzes political stance using reference anchor distributions - models generate content representing proponent and opponent stances, and similarity metrics determine the model's position on each topic. Second, it decomposes framing bias into content and style components through latent variable modeling. Content bias is analyzed using frame dimensions (e.g., morality, security, economy) and entity mentions, while style bias is measured through lexical polarity and sentiment analysis. The framework compares framing across different models rather than against a single reference, enabling fine-grained, interpretable insights into political biases.

## Key Results
- Models exhibit varied political stances across topics, with most leaning liberal on issues like gun control and same-sex marriage
- LLMs often focus on US-related matters when discussing political topics
- Larger models are not necessarily more neutral than smaller ones
- Models within the same family can have different political biases
- Multilingual capabilities influence content focus, with some models generating more international perspectives in certain languages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Measuring political bias requires comparing model-generated text distributions against anchor distributions representing opposing stances
- **Mechanism**: The framework uses sentence embeddings and cosine similarity to compute the distance between the model's generated content distribution P(Ŷ) and reference anchor distributions P(Ypro) and P(Yopp). The imbalance between these distances indicates the model's political stance
- **Core assumption**: The stance extractor preserves relative distance relationships in the stance space, allowing proxy measurement through anchor comparison
- **Evidence anchors**:
  - [abstract]: "Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias"
  - [section 2.1]: "We assume the stance extractor preserves relative distance relationships, and hypothesize there exists of a pair of reference anchor distributions P(Ypro), P(Yopp), which respectively elicit a proponent stance ⃗ spro and an opponent stance ⃗ sopp in the stance space"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.308, average citations=0.1. Weak corpus evidence for direct stance measurement methodology

### Mechanism 2
- **Claim**: Political bias manifests through framing, which can be decomposed into content bias (what is said) and style bias (how it is said)
- **Mechanism**: The framework assumes model generation involves two steps: generating a latent variable z containing content C and style S, then translating these into natural language. Content bias is analyzed through frame dimensions and entity mentions, while style bias is measured through lexical polarity
- **Core assumption**: The latent variable z can be effectively decomposed into separable content and style components that can be independently analyzed for bias
- **Evidence anchors**:
  - [abstract]: "Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias"
  - [section 2.2]: "We assume that the model response generation process of fLLM consists of two steps: firstly, g generates a latent variable z from the input instruction X, then h translates Z into natural language form... The latent variable z encapsulates all the information about the model's response, which can be further decomposed into two parts: the content variable C and the style variable S"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.308, average citations=0.1. Moderate corpus evidence for framing decomposition approach

### Mechanism 3
- **Claim**: Comparing framing across different models reveals bias patterns that cannot be detected by comparing against a single reference
- **Mechanism**: Instead of measuring deviation from an optimal reference, the framework compares content and style variables across a diverse range of models to identify bias patterns
- **Core assumption**: Differences in framing across models are meaningful indicators of bias, and a diverse model set provides adequate comparison coverage
- **Evidence anchors**:
  - [section 2.2]: "After obtaining the content and style variables, another challenge arises: how to analyze their biases... Therefore, in our framework, rather than measuring the deviation of C and S compared to some golden references, we compare different C and S across a diverse range of models"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.308, average citations=0.1. Limited corpus evidence for comparative bias analysis approach

## Foundational Learning

- **Concept**: Political stance analysis through anchor distribution comparison
  - Why needed here: The framework needs a method to quantify political stance without relying on subjective human judgment or incomplete surveys
  - Quick check question: How would you modify the framework if you discovered that the anchor distributions were not truly representative of opposing stances?

- **Concept**: Frame dimension decomposition for content bias analysis
  - Why needed here: Understanding what aspects of topics are covered by different models reveals content bias patterns that simple stance measurement cannot capture
  - Quick check question: What additional frame dimensions might be needed to capture emerging political topics not covered by the original 15 dimensions?

- **Concept**: Lexical polarity measurement for style bias analysis
  - Why needed here: Style bias reveals how information is presented, which can be as important as what is presented in understanding political bias
  - Quick check question: How would you adjust the framework to account for cultural differences in sentiment expression across different languages?

## Architecture Onboarding

- **Component map**: Instruction → Stance Analysis → Content Bias Analysis → Style Bias Analysis → Results Aggregation
- **Critical path**: Instruction → Stance Analysis → Content Bias Analysis → Style Bias Analysis → Results Aggregation
- **Design tradeoffs**: 
  - Using anchor distributions instead of human surveys provides scalability but may lack nuance
  - Comparative analysis across models avoids needing golden references but requires sufficient model diversity
  - Decomposing bias into content and style enables granular analysis but assumes separability
- **Failure signatures**:
  - Stance analysis produces inconsistent results across similar topics
  - Frame dimension analysis shows uniform coverage across all models
  - Style bias metrics show no variation across different political topics
  - Comparative analysis fails to reveal meaningful differences between models
- **First 3 experiments**:
  1. Test stance analysis on a controlled dataset with known biases to validate anchor distribution methodology
  2. Compare frame dimension coverage between models on a single topic to verify content bias decomposition
  3. Measure style bias variation across models on topics with known stylistic differences to validate lexical polarity analysis

## Open Questions the Paper Calls Out

- **Question**: How does the framework perform when applied to generation tasks other than news headline generation, such as article generation or opinion piece generation?
  - Basis in paper: [explicit] The paper mentions that the framework is generalizable to other generation tasks and is task/prompt-agnostic, allowing for fine-grained understanding
  - Why unresolved: The paper primarily focuses on news headline generation and does not provide empirical results or analysis for other generation tasks
  - What evidence would resolve it: Empirical results and analysis of the framework's performance on different generation tasks, such as article generation or opinion piece generation, demonstrating its effectiveness and generalizability

- **Question**: How does the presence of hallucination in LLM-generated content affect the measurement of political bias?
  - Basis in paper: [inferred] The paper acknowledges that hallucination, where generated content deviates from factual accuracy, is not considered in the current bias measurement framework, indicating a significant area for future research
  - Why unresolved: The paper does not provide any analysis or results on how hallucination might influence the measurement of political bias in LLM-generated content
  - What evidence would resolve it: Analysis of the relationship between hallucination and political bias in LLM-generated content, including potential methods to account for or mitigate the impact of hallucination on bias measurement

- **Question**: How does the choice of anchor points (proponent and opponent) affect the stance estimation, and are there alternative methods to capture a broader spectrum of perspectives?
  - Basis in paper: [explicit] The paper mentions that the evaluation of LLM stance on specific topics was conducted using two anchor points - proponent and opponent - and acknowledges that this approach may not fully represent topics that encompass a broader spectrum of perspectives
  - Why unresolved: The paper does not explore alternative methods for stance estimation or analyze the impact of different anchor point choices on the results
  - What evidence would resolve it: Comparison of stance estimation results using different anchor point choices or alternative methods that capture a broader spectrum of perspectives, demonstrating the robustness and limitations of the current approach

## Limitations

- The framework relies on anchor distributions that may not fully represent the spectrum of political perspectives on complex issues
- Content and style separation assumes these elements are independent, which may not hold for all political discourse
- The comparative analysis approach requires a sufficiently diverse model set to reveal meaningful bias patterns

## Confidence

**High Confidence**: The methodology for measuring political stance through anchor distribution comparison is well-established in the literature and the framework's two-tiered approach (stance analysis + framing analysis) is logically coherent. The finding that LLMs exhibit varied stances across topics aligns with broader observations about model biases.

**Medium Confidence**: The decomposition of framing bias into content and style components is theoretically sound, but the practical implementation details for content and style extractors are not fully specified. The claim that larger models are not necessarily more neutral requires more systematic validation across model families.

**Low Confidence**: The comparative analysis approach that avoids golden references is innovative but relies heavily on having a sufficiently diverse model set. The framework's effectiveness for multilingual bias detection is based on limited evidence, with only brief mentions of Spanish, French, and German experiments.

## Next Checks

1. **Anchor Quality Validation**: Test the framework's sensitivity to anchor quality by systematically varying the reference prompts and measuring how this affects stance estimates. This would help quantify how much uncertainty in anchor generation propagates to final bias measurements.

2. **Content-Style Separability Test**: Design an experiment where content and style elements are deliberately manipulated independently to verify that the framework's decomposition accurately captures separable bias components. This could involve controlled prompts with known content-style relationships.

3. **Cross-Lingual Consistency Check**: Evaluate the framework's performance across a broader range of languages using the same political topics to determine if observed multilingual differences are consistent or language-specific artifacts.