---
ver: rpa2
title: Some Best Practices in Operator Learning
arxiv_id: '2412.06686'
source_url: https://arxiv.org/abs/2412.06686
tags:
- equation
- learning
- neural
- operator
- koopman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates best practices for operator learning by\
  \ examining the impact of several hyperparameters and training methods on three\
  \ neural operator architectures: DeepONets, Fourier Neural Operators (FNO), and\
  \ Koopman Autoencoders. The study evaluates the performance of different activation\
  \ functions (ReLU, GELU, ELU, Tanh), dropout rates, stochastic weight averaging\
  \ (SWA), and learning rate finder techniques on various differential equations including\
  \ the pendulum, Lorenz system, fluid attractor equation, Burger\u2019s equation,\
  \ and Korteweg-de Vries equation."
---

# Some Best Practices in Operator Learning

## Quick Facts
- arXiv ID: 2412.06686
- Source URL: https://arxiv.org/abs/2412.06686
- Reference count: 40
- Primary result: GELU activation function consistently outperforms other activations across all tested neural operator architectures and differential equations.

## Executive Summary
This paper systematically investigates best practices for operator learning by examining the impact of hyperparameters and training methods on three neural operator architectures: DeepONets, Fourier Neural Operators (FNO), and Koopman Autoencoders. Through extensive experiments on five differential equations, the study identifies GELU activation as the consistently optimal choice, demonstrates that dropout is counterproductive for operator learning, and establishes conditions under which stochastic weight averaging improves performance. The research provides practical guidance for practitioners in the field of neural operators.

## Method Summary
The study tests activation functions (ReLU, GELU, ELU, Tanh), dropout rates (0.0 to 0.15), stochastic weight averaging with different learning rates, and learning rate finder techniques across three neural operator architectures (DeepONets, FNO, Koopman Autoencoders) using five differential equations. Datasets were generated by numerically solving the differential equations using Runge-Kutta methods for ODEs and specialized solvers for PDEs, with random initial and boundary conditions. Performance was evaluated using mean squared error between model predictions and true solutions.

## Key Results
- GELU activation function consistently outperformed other activation functions across all architectures and differential equations.
- Dropout decreased model accuracy in every experiment and is not recommended for operator learning.
- Stochastic weight averaging improved accuracy when the learning rate was the same or one-tenth of the original learning rate.
- Learning rate finder was ineffective and inconsistent, performing worse than manual learning rate tuning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GELU activation function consistently improves accuracy across all three neural operator architectures and all tested differential equations.
- Mechanism: GELU's smooth, differentiable nature allows better gradient flow during training, which is crucial for approximating smooth solutions in operator learning. Unlike ReLU's piecewise linearity or Tanh's saturation, GELU's probabilistic gating enables adaptive smoothing that better captures the underlying operator's behavior.
- Core assumption: The solution operators for differential equations are smooth functions that benefit from smooth activation functions.
- Evidence anchors: [section] "In every experiment, the activation functiongelu preformed the best. Thus, is is recommend to use the activation functiongelu in operator learning."
- Break Condition: If the underlying operator being approximated has discontinuities or sharp transitions, GELU's smoothness could actually hinder learning rather than help.

### Mechanism 2
- Claim: Dropout decreases model accuracy in operator learning applications.
- Mechanism: Operator learning often involves learning smooth, continuous mappings where overfitting is less of a concern than in traditional classification tasks. Dropout's random masking disrupts the learning of these continuous relationships and introduces unnecessary noise into the gradient signals.
- Core assumption: The risk of overfitting is lower in operator learning compared to standard supervised learning tasks.
- Evidence anchors: [section] "In every experiment, dropout decreased the accuracy of the model."
- Break Condition: If the training dataset is extremely small or noisy, dropout might still provide some regularization benefits that outweigh its disruption of smooth learning.

### Mechanism 3
- Claim: Stochastic weight averaging (SWA) improves accuracy when the learning rate is the same or one-tenth of the original learning rate.
- Mechanism: SWA averages weights from a flat region of the loss landscape, which better represents the underlying operator. When the learning rate is appropriately reduced, the optimizer can explore this flat region effectively, and the average weights capture the most generalizable solution.
- Core assumption: The loss landscape for operator learning contains flat regions that represent good solutions.
- Evidence anchors: [section] "In each experiment, stochastic weight averaging improved the accuracy of the model when the learning rate was the same or one tenth of the original learning rate."
- Break Condition: If the loss landscape lacks flat regions (e.g., for highly non-convex or discontinuous operators), SWA may not provide benefits and could average out important directional information.

## Foundational Learning

- Concept: Neural operators as function space mappings
  - Why needed here: Understanding that neural operators approximate mappings between function spaces (e.g., from initial conditions to solutions) is fundamental to grasping why certain hyperparameters work better than others.
  - Quick check question: What is the mathematical definition of an operator in the context of neural operators?

- Concept: Spectral convolution and Fourier transforms
  - Why needed here: FNO's effectiveness relies on understanding how spectral convolution layers work differently from standard convolution layers, particularly their global nature and computational requirements.
  - Quick check question: How does a spectral convolution layer differ from a standard convolution layer in terms of locality and computational approach?

- Concept: Koopman operator theory and dynamic mode decomposition
  - Why needed here: Koopman autoencoders are based on this theoretical framework, and understanding the Koopman formulation helps explain why certain architectural choices (like tridiagonal masks) are made.
  - Quick check question: What is the key insight behind the Koopman formulation of classical mechanics that makes it useful for operator learning?

## Architecture Onboarding

- Component map:
  - DeepONet: Branch network (encodes input function) + Trunk network (encodes evaluation points) + Dot product output
  - FNO: Input projection + Spectral convolution layers + Output projection
  - Koopman Autoencoder: Encoder (physical→latent) + Operator (K matrix) + Decoder (latent→physical)

- Critical path: Data preprocessing → Model architecture selection → Activation function choice → Training configuration (no dropout) → SWA with appropriate LR → Evaluation

- Design tradeoffs:
  - DeepONet vs FNO: DeepONet handles arbitrary input functions but requires careful branch/trunk architecture; FNO requires grid-structured data but offers global convolution benefits
  - Koopman Autoencoder: Requires careful loss term balancing and unitary constraint enforcement for stability

- Failure signatures:
  - Poor convergence: Likely due to inappropriate activation function or missing SWA
  - Overfitting: May indicate need for more data rather than dropout regularization
  - Oscillating training: Learning rate too high for SWA application

- First 3 experiments:
  1. DeepONet with GELU activation on Lorenz system to verify baseline performance
  2. FNO with GELU activation on Burger's equation to test grid-structured data handling
  3. Koopman autoencoder with GELU activation on pendulum equation to verify time-dependent dynamics capture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does GELU activation consistently outperform other activation functions across all architectures and differential equations in operator learning?
- Basis in paper: [explicit] The paper demonstrates through numerical experiments that GELU consistently achieves the best performance across DeepONets, Fourier neural operators, and Koopman autoencoders for various differential equations.
- Why unresolved: While the empirical results are clear, the paper does not provide theoretical justification for why GELU's properties make it particularly suited for operator learning tasks.
- What evidence would resolve it: Theoretical analysis explaining GELU's advantages in approximating solution operators, or ablation studies comparing GELU's behavior to other activations during training of operator learning models.

### Open Question 2
- Question: Why is dropout ineffective for preventing overfitting in operator learning architectures?
- Basis in paper: [explicit] The paper shows that dropout consistently decreased model accuracy across all experiments with DeepONets, Fourier neural operators, and Koopman autoencoders.
- Why unresolved: The paper does not investigate the underlying reasons why dropout fails to provide regularization benefits in the context of operator learning.
- What evidence would resolve it: Analysis of how dropout affects the learned solution operators, or comparison of feature representations with and without dropout to identify potential issues.

### Open Question 3
- Question: Why is the learning rate finder technique inconsistent and ineffective for operator learning compared to manual tuning?
- Basis in paper: [explicit] The paper reports that learning rate finder was "worse for operator learning" with "inconsistent" results that were "normally not close to the optimal learning rate."
- Why unresolved: The paper does not investigate whether this is due to the nature of operator learning problems or specific characteristics of the tested architectures.
- What evidence would resolve it: Systematic comparison of learning rate finder performance across different types of differential equations or investigation of how learning rate schedules affect the convergence of solution operators.

## Limitations

- The findings are based on experiments with five specific differential equations and three neural operator architectures, limiting generalizability to other domains.
- The paper lacks detailed implementation specifications for critical components like spectral convolution layers and Koopman Autoencoder loss functions.
- No statistical significance testing was performed, so observed performance differences may not be robust across different random seeds.

## Confidence

**High Confidence**: GELU activation function recommendation - strongly supported by consistent experimental results across all architectures and differential equations.

**Medium Confidence**: Dropout ineffectiveness recommendation - supported by experimental results but lacks theoretical justification specific to operator learning.

**Medium Confidence**: SWA recommendation with appropriately reduced learning rates - supported by experimental results but requires further investigation of optimal conditions.

## Next Checks

1. **Statistical Significance Testing**: Replicate the experiments with multiple random seeds and perform statistical tests to determine whether performance differences between hyperparameters are statistically significant.

2. **Generalization to New Domains**: Apply the recommended hyperparameters (GELU activation, no dropout, SWA with reduced LR) to neural operators trained on different types of operators to validate generalizability.

3. **Ablation Study on Learning Rate Finder**: Conduct a more systematic evaluation of the learning rate finder technique with varying ranges and criteria to determine whether it can be made effective for operator learning.