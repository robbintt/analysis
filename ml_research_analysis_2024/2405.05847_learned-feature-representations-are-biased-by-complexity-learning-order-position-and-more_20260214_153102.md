---
ver: rpa2
title: Learned feature representations are biased by complexity, learning order, position,
  and more
arxiv_id: '2405.05847'
source_url: https://arxiv.org/abs/2405.05847
tags:
- feature
- features
- learning
- easy
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates how different properties of input features\u2014\
  such as complexity, learning order, and prevalence\u2014affect the internal representations\
  \ learned by deep neural networks. The authors train various architectures (MLPs,\
  \ ResNets, Transformers) to compute multiple independent abstract features of their\
  \ inputs."
---

# Learned feature representations are biased by complexity, learning order, position, and more

## Quick Facts
- arXiv ID: 2405.05847
- Source URL: https://arxiv.org/abs/2405.05847
- Reference count: 40
- The paper investigates how different properties of input features—such as complexity, learning order, and prevalence—affect the internal representations learned by deep neural networks.

## Executive Summary
This paper investigates how different properties of input features—such as complexity, learning order, and prevalence—affect the internal representations learned by deep neural networks. The authors train various architectures (MLPs, ResNets, Transformers) to compute multiple independent abstract features of their inputs. They find that simpler or earlier-learned features are represented more strongly and densely in the model's internal representations, even when all features are learned equally well. These representational biases are common across architectures but can be shifted by hyperparameters and training regimes. The authors demonstrate downstream implications for interpretability methods, such as PCA and RSA, which may be biased towards simpler features, potentially leading to misleading conclusions about a model's computations.

## Method Summary
The authors conduct controlled experiments using synthetic datasets where input features are explicitly defined and can be independently manipulated. They train multiple architectures including MLPs, ResNets, and Transformers to predict combinations of these features. By varying feature properties like complexity (linear vs non-linear), learning order, and prevalence, they analyze how these affect the learned representations. The experiments use standard training procedures and systematically ablate different feature properties to isolate their effects on representation geometry.

## Key Results
- Simpler features (e.g., linear transformations) are represented more strongly and densely than complex features (e.g., non-linear transformations) across all tested architectures
- Features learned earlier in training occupy more representational space than later-learned features, even when all features are eventually learned equally well
- These representational biases affect interpretability methods, with PCA and RSA analyses showing stronger alignment with simpler features, potentially misleading practitioners about which features drive model behavior

## Why This Works (Mechanism)
The representational biases arise from fundamental optimization dynamics in neural networks. During training, networks prioritize features that are easier to fit, which corresponds to simpler transformations requiring fewer parameters or smaller capacity to represent. Early in training, the network allocates representational resources to these simpler features before moving on to more complex ones. This creates a persistent bias where simpler features occupy more of the representational space, even after training completes. The effect is compounded by the fact that early-learned features benefit from more optimization steps and can establish stronger connections before the network needs to allocate capacity to later features.

## Foundational Learning
- **Representation geometry**: Understanding how features map to activation spaces is crucial for interpreting model behavior and diagnosing biases
- **Feature complexity**: Different feature types (linear vs non-linear) require different representational capacities, affecting how they're encoded
- **Learning dynamics**: The order and speed at which features are learned affects their final representation strength
- **Interpretability methods**: Techniques like PCA and RSA are sensitive to representation geometry and can be biased by underlying feature properties
- **Activation space**: The high-dimensional space where features are encoded determines how they can be recovered and analyzed
- **Variance allocation**: How representational resources are distributed across features affects downstream analysis

Quick check: Verify that activation space analysis captures the relative strength of different feature representations through techniques like PCA variance explained.

## Architecture Onboarding

**Component map**: Input features -> Model architecture (MLP/ResNet/Transformer) -> Internal representations -> Output predictions -> Interpretability analysis

**Critical path**: The path from input features through the model to representations is critical, as biases introduced at any stage affect downstream analysis and interpretation.

**Design tradeoffs**: Simpler architectures like MLPs show clearer biases but may not capture complex feature interactions. Deeper architectures like ResNets and Transformers may distribute representations differently but still exhibit core biases.

**Failure signatures**: If interpretability methods consistently highlight simpler features over more complex but important ones, this suggests representational bias is affecting analysis.

**First experiments**:
1. Train an MLP on synthetic data with features of varying complexity to observe representation strength differences
2. Compare representation geometry of early-learned vs late-learned features using PCA analysis
3. Test how changing learning rate or architecture depth affects representational biases

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those discussed in the limitations section.

## Limitations
- The experimental scope is limited to synthetic tasks and specific model architectures, leaving unclear whether biases extend to larger-scale models and real-world datasets
- The analysis focuses on individual feature properties in isolation, not addressing how these properties interact in realistic scenarios
- Implications for interpretability methods are theoretical and lack empirical validation on real-world interpretability challenges

## Confidence
- High confidence: The experimental methodology is sound, and consistent findings across multiple architectures and hyperparameters strengthen the core claims about representational biases
- Medium confidence: Generalization of findings to real-world scenarios and larger models, given synthetic task limitations
- Medium confidence: Implications for interpretability methods, as theoretical arguments are compelling but lack empirical validation on practical interpretability challenges

## Next Checks
1. Replicate core findings using large-scale vision and language models (e.g., CLIP, BERT) trained on real-world datasets to assess whether representational biases persist in practical settings
2. Conduct ablation studies to understand how feature property interactions (e.g., complexity and prevalence combined) affect representational biases in more complex scenarios
3. Perform case studies applying interpretability methods (PCA, RSA) to models with known feature importance to empirically demonstrate how representational biases lead to misinterpretations of model behavior