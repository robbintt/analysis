---
ver: rpa2
title: 'From Narratives to Numbers: Valid Inference Using Language Model Predictions
  from Verbal Autopsy Narratives'
arxiv_id: '2404.02438'
source_url: https://arxiv.org/abs/2404.02438
tags:
- multippi
- non-communicable
- ground
- page
- truth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of performing valid statistical
  inference when outcomes (causes of death) are predicted from free-form text using
  natural language processing. The authors propose multiPPI++, an extension of prediction-powered
  inference for multinomial classification, to correct for misclassification and transportability
  issues.
---

# From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives

## Quick Facts
- arXiv ID: 2404.02438
- Source URL: https://arxiv.org/abs/2404.02438
- Reference count: 40
- Authors: Shuxian Fan; Adam Visokay; Kentaro Hoffman; Stephen Salerno; Li Liu; Jeffrey T. Leek; Tyler H. McCormick
- Primary result: multiPPI++ corrects for misclassification and transportability issues when using NLP-predicted outcomes for statistical inference, recovering ground truth estimates regardless of model accuracy or domain differences

## Executive Summary
This paper addresses a critical challenge in statistical inference when outcomes are predicted from free-form text using natural language processing (NLP). The authors propose multiPPI++, an extension of prediction-powered inference for multinomial classification, which corrects for misclassification and transportability issues that arise when NLP models are trained on data from one domain and applied to another. Using verbal autopsy narratives from the PHMRC dataset, they demonstrate that multiPPI++ successfully recovers ground truth estimates regardless of which NLP model is used (ranging from KNN to GPT-4-32k) and despite transportability issues between sites.

## Method Summary
The multiPPI++ method extends PPI++ to handle multinomial classification problems where outcomes (causes of death) are predicted from verbal autopsy narratives using various NLP models. The approach uses a small labeled subset with ground truth labels to compute a correction term that accounts for the difference between predicted and true labels, effectively neutralizing bias introduced by model inaccuracies or domain shifts. The method includes a tuning parameter λ that balances the contribution of labeled versus unlabeled data, allowing it to adapt to the accuracy of the NLP predictions. The authors validate their approach using the PHMRC dataset with five broad cause-of-death categories, testing multiple NLP models including BoW+SVM, BoW+KNN, BoW+NB, BERTBASE, and GPT-4-32k.

## Key Results
- multiPPI++ successfully recovers ground truth estimates regardless of which NLP model produces predictions, from simple KNN to advanced GPT-4-32k
- The method effectively corrects for transportability bias when training models on data from one site and applying them to another site with different distributions
- Confidence intervals from multiPPI++ are similar in width to naive approaches, indicating the correction does not add significant uncertainty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: multiPPI++ correction recovers ground truth estimates regardless of NLP model accuracy or transportability issues.
- Mechanism: The correction term in multiPPI++ explicitly accounts for the difference between predicted and true labels in the labeled subset, effectively neutralizing bias introduced by model inaccuracies or domain shifts.
- Core assumption: A small subset of ground truth labels is available to compute the correction term.
- Evidence anchors:
  - [abstract] "multiPPI++ recovers ground truth estimates, regardless of which NLP model produced predictions and regardless of whether they were produced by a more accurate predictor like GPT-4-32k or a less accurate predictor like KNN."
  - [section] "multiPPI++ correction rectifies estimates back to baseline, demonstrating its ability to account for (i) model inaccuracy and (ii) transportability bias."
- Break condition: If the labeled subset is too small or unrepresentative, the correction may be unreliable or unstable.

### Mechanism 2
- Claim: multiPPI++ maintains similar confidence interval widths as the naive approach, without adding significant uncertainty.
- Mechanism: The correction term is weighted by a tuning parameter λ that balances the contribution of labeled vs. unlabeled data, preventing the correction from inflating variance unnecessarily.
- Core assumption: The variance of the correction term is not substantially larger than that of the naive estimator.
- Evidence anchors:
  - [section] "we observe that the Naive regression coefficients from KNN and GPT-4 are not as distant from the ground truth as the other two methods... Interestingly, we observe that the Naive regression coefficients from KNN and GPT-4 are not as distant from the ground truth as the other two methods..."
  - [section] "the Naive estimate and multiPPI++ correction show similar confidence interval widths."
- Break condition: If the variance of the correction term is much larger than that of the naive estimator, the confidence intervals will widen substantially.

### Mechanism 3
- Claim: multiPPI++ adapts to the accuracy of the NLP predictions via the tuning parameter λ.
- Mechanism: The tuning parameter λ is estimated from the data, allowing the method to downweight the contribution of poorly performing predictions and rely more on the labeled subset.
- Core assumption: The relationship between prediction accuracy and the optimal λ can be estimated from the available data.
- Evidence anchors:
  - [section] "multiPPI++ generalizes PPI to allow the inference to adapt to the accuracy of the supplied predictions, yielding estimations that are never worse than the classical inference."
  - [section] "Figure 7 shows the association between F1-score and multiPPI++ λ, which also fail to show any significant association."
- Break condition: If the relationship between prediction accuracy and optimal λ is highly non-linear or unstable, the adaptive tuning may not work well.

## Foundational Learning

- Concept: Multinomial logistic regression and its parameterization.
  - Why needed here: The multiPPI++ method extends PPI++ to the multinomial classification setting, requiring a proper parameterization of the multinomial logistic regression model.
  - Quick check question: What is the key difference between the parameterization used in multiPPI++ and the standard multinomial logistic regression parameterization?

- Concept: Prediction-powered inference (PPI) and its extension PPI++.
  - Why needed here: multiPPI++ builds upon the PPI and PPI++ frameworks, adapting them to handle multinomial classification problems with predicted outcomes.
  - Quick check question: How does the PPI++ correction term differ from the original PPI correction term?

- Concept: Transportability and domain adaptation in machine learning.
  - Why needed here: The multiPPI++ method is designed to handle transportability issues when training NLP models on data from one domain and applying them to another domain with potentially different distributions.
  - Quick check question: What are the key challenges in ensuring transportability when applying NLP models across different domains?

## Architecture Onboarding

- Component map: NLP model -> multiPPI++ correction -> Inference model
- Critical path:
  1. Train NLP model on labeled data from multiple sites
  2. Use NLP model to predict COD on unlabeled data from a new site
  3. Apply multiPPI++ correction using the labeled subset and NLP predictions
  4. Perform inference with the corrected predictions

- Design tradeoffs:
  - Using a small labeled subset for correction vs. collecting a large labeled dataset for direct training
  - Choosing between different NLP models (e.g., simpler models like KNN vs. more complex models like GPT-4) based on accuracy vs. cost tradeoff
  - Balancing the contribution of labeled vs. unlabeled data via the tuning parameter λ

- Failure signatures:
  - If the labeled subset is too small or unrepresentative, the correction may be unreliable
  - If the NLP model performance is very poor, the correction may not be sufficient to recover ground truth estimates
  - If the transportability issue is too severe, the correction may not fully account for the domain shift

- First 3 experiments:
  1. Compare the performance of multiPPI++ with and without the correction term on a simulated dataset with known ground truth
  2. Evaluate the sensitivity of multiPPI++ to the size of the labeled subset by varying the proportion of labeled data
  3. Test the performance of multiPPI++ on a real VA dataset with known ground truth, comparing it to the naive approach and the ground truth estimates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal data splitting strategy (e.g., 80/20 split by COD vs. by entire dataset) for maximizing the effectiveness of multiPPI++ in downstream inference tasks?
- Basis in paper: [inferred] The paper discusses a sensitivity analysis comparing stratified 80/20 splits by COD versus non-stratified splits, noting that stratified splits led to smaller λ values and potentially less use of labeled data.
- Why unresolved: The paper does not provide a definitive answer on which splitting strategy is superior for practical applications, only highlighting differences in λ values.
- What evidence would resolve it: A comprehensive study comparing the performance of multiPPI++ using various data splitting strategies on multiple datasets and inference tasks, measuring metrics like bias, variance, and confidence interval widths.

### Open Question 2
- Question: How does the performance of multiPPI++ vary across different types of misclassification errors in the NLP model predictions (e.g., systematic bias vs. random noise)?
- Basis in paper: [explicit] The paper mentions that multiPPI++ corrects for misclassification in predicted CODs, but does not explore how it handles different types of errors.
- Why unresolved: The paper focuses on overall performance metrics but does not analyze the nature of misclassification errors or how they impact the correction.
- What evidence would resolve it: An analysis of the types of errors made by NLP models in predicting CODs and how multiPPI++ performs in correcting for each type, potentially using simulated or controlled datasets.

### Open Question 3
- Question: Can multiPPI++ be extended to handle more complex outcome structures beyond multinomial classification, such as hierarchical or structured prediction tasks?
- Basis in paper: [inferred] The paper extends PPI++ to multinomial classification, suggesting the method's adaptability, but does not explore other outcome structures.
- Why unresolved: The paper focuses on multinomial classification and does not investigate the method's applicability to other types of prediction tasks.
- What evidence would resolve it: Development and evaluation of multiPPI++ extensions for hierarchical or structured prediction tasks, comparing their performance to existing methods and demonstrating their effectiveness in real-world applications.

## Limitations

- The method's effectiveness critically depends on having a representative labeled subset; if this subset is too small or unrepresentative, the correction may fail
- The relationship between prediction accuracy and optimal λ is assumed to be estimable from the data, but this relationship may be non-linear or unstable in practice
- The method has only been validated on the PHMRC dataset with specific COD categories; generalizability to other domains or classification tasks remains untested

## Confidence

- **High confidence**: The core claim that multiPPI++ can recover ground truth estimates despite model inaccuracies and transportability issues is well-supported by the empirical results across multiple NLP models and sites
- **Medium confidence**: The claim that multiPPI++ maintains similar confidence interval widths as the naive approach is supported, but the conditions under which this holds warrant further investigation
- **Medium confidence**: The adaptive tuning via λ shows promise, but the lack of a clear association between F1-score and λ in Figure 7 suggests the relationship may be more complex than anticipated

## Next Checks

1. Test multiPPI++ on datasets with varying degrees of transportability challenges to identify the method's limits
2. Systematically vary the size and representativeness of the labeled subset to quantify its impact on correction reliability
3. Compare multiPPI++ performance across different classification tasks beyond cause-of-death determination to assess generalizability