---
ver: rpa2
title: Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction
arxiv_id: '2406.18078'
source_url: https://arxiv.org/abs/2406.18078
tags:
- data
- scorer
- sentiment
- aspect
- asqp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of limited labeled data in Aspect
  Sentiment Quad Prediction (ASQP) by proposing a self-training framework with a pseudo-label
  scorer. The key innovation is a generative model that evaluates the quality of pseudo-labels
  by computing their conditional likelihoods given the input review.
---

# Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction

## Quick Facts
- arXiv ID: 2406.18078
- Source URL: https://arxiv.org/abs/2406.18078
- Reference count: 33
- Primary result: Generative pseudo-label scorer improves ASQP performance by filtering low-quality pseudo-labels in self-training

## Executive Summary
This paper tackles the challenge of limited labeled data in Aspect Sentiment Quad Prediction (ASQP) by introducing a self-training framework with a novel pseudo-label scorer. The key innovation is using a generative model to evaluate pseudo-label quality through conditional likelihoods, which provides a more nuanced assessment than previous discriminative approaches. To train this scorer, the authors create a human-annotated comparison dataset where annotators select the most appropriate pseudo-label from candidates, and also explore using large language models for cost-effective annotation. The approach significantly improves performance on four public ASQP datasets by filtering low-quality pseudo-labels and using the scorer as a reranker for multiple candidate labels.

## Method Summary
The method involves training an initial ASQP model on labeled data, generating pseudo-labels for unlabeled reviews via beam search, and then filtering these pseudo-labels using a two-stage approach: first based on confidence scores, then using a generative pseudo-label scorer trained on human- or AI-annotated comparison data. The scorer evaluates pseudo-labels by computing conditional likelihoods, capturing how well each label fits the review context. Filtered pseudo-labels are then used to augment the training data, and the scorer can also serve as a reranker for beam search outputs during inference.

## Key Results
- Pseudo-label scorer with generative model improves ASQP performance across four datasets
- Two-stage filtering (confidence + scorer) effectively removes low-quality pseudo-labels while retaining useful training samples
- AI-annotated comparison data provides a cost-effective alternative to human annotation with comparable performance
- Scorer as reranker further improves results by selecting optimal candidates from beam search outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative model conditional likelihood scores better capture review-to-pseudo-label alignment than discriminative matching tokens.
- Mechanism: Instead of embedding a fixed matching token, the generative model computes p(y|x) token-by-token, allowing it to evaluate plausibility of each pseudo-label in context.
- Core assumption: The likelihood assigned by a generative model to a pseudo-label correlates with the semantic match between the label and the review.
- Evidence anchors:
  - [abstract]: "This approach enables the scorer to examine the plausibility of a pseudo-label in a token-by-token fashion, thus offering a more comprehensive and effective scoring."
  - [section 4.1]: "Compared to previous methods, this approach integrates the likelihood of each token in the pseudo-label to derive its overall score, thereby providing a comprehensive and effective scoring."
  - [corpus]: Weak - no direct citations comparing generative likelihood vs discriminative matching tokens.

### Mechanism 2
- Claim: Two-stage filtering (confidence-based then scorer-based) improves self-training effectiveness.
- Mechanism: First removes samples where the initial model is unsure (low min token likelihood), then filters based on pseudo-label scorer match scores within a bounded range to avoid overly simple samples.
- Core assumption: Low-confidence pseudo-labels are likely to be incorrect, and extremely high scorer scores correspond to trivial samples that offer little training signal.
- Evidence anchors:
  - [section 4.2]: "We observe that pseudo-labels with low scores are consistently of poor quality. Besides, while samples with high scores generally exhibit good label quality, their sentences tend to be overly simple, offering limited helpfulness for subsequent model training."
  - [section 5.3]: "Figure 3 illustrates that the performance incrementally increases with increasing match scores. Nevertheless, beyond a certain threshold, further increases in match scores lead to a decline in performance."
  - [corpus]: Weak - no direct citations comparing multi-stage filtering to single-stage.

### Mechanism 3
- Claim: Human-annotated comparison data improves pseudo-label scorer training over rule-based negative labels.
- Mechanism: Instead of generating negative labels via heuristics, annotators choose the best pseudo-label among candidates, providing more challenging and realistic negative examples.
- Core assumption: Human judgment better captures the nuanced match between reviews and pseudo-labels than heuristic modifications.
- Evidence anchors:
  - [section 3]: "Such negative labels are patterned and easily distinguishable, limiting the learning potential of the scorer. Therefore, this paper employs human annotators to construct this comparison dataset."
  - [section 5.2]: "We have the following observations. (1) Utilizing humans or AI to annotate the comparison data is crucial, as their performance is noticeably superior to that without annotation."
  - [corpus]: Weak - no direct citations comparing human vs heuristic negative label quality.

## Foundational Learning

- Concept: Conditional probability modeling
  - Why needed here: The pseudo-label scorer estimates p(y|x) to evaluate how well a label fits a review.
  - Quick check question: What does p(y|x) represent in the context of a pseudo-label scorer?

- Concept: Self-training in semi-supervised learning
  - Why needed here: The approach uses an initial model to generate pseudo-labels for unlabeled data, then iteratively improves the model.
  - Quick check question: What are the three main steps of the self-training framework described?

- Concept: Data filtering strategies
  - Why needed here: Filtering low-quality pseudo-labels is essential to prevent noise from degrading model performance during self-training.
  - Quick check question: Why does the approach use both confidence-based and scorer-based filtering instead of just one?

## Architecture Onboarding

- Component map:
  Initial ASQP model → Pseudo-label generator → Pseudo-label scorer (T5-large) → Two-stage filter → Augmented dataset → Final ASQP model
  Comparison dataset creation (human/AI annotation) → Scorer training (ranking objectives) → Reranker (beam search candidates → scorer ranking)

- Critical path:
  1. Train initial ASQP model on labeled data
  2. Generate pseudo-labels for unlabeled data
  3. Filter pseudo-labels using confidence and scorer
  4. Train scorer on comparison dataset
  5. Apply filtered pseudo-labels to augment training
  6. Rerank candidates during inference

- Design tradeoffs:
  - Generative vs discriminative scorer: More expressive but requires ranking-based training
  - Human vs AI annotation: Higher quality vs scalability
  - Strict vs lenient filtering: Better quality vs more diversity

- Failure signatures:
  - Poor initial model → low-quality pseudo-labels → noisy training
  - Overfitting scorer to comparison data → poor generalization
  - Aggressive filtering → insufficient diversity → underfitting

- First 3 experiments:
  1. Train scorer with listwise ranking objective on comparison data; evaluate accuracy on held-out comparison set.
  2. Apply two-stage filtering to pseudo-labels; measure precision/recall of retained samples.
  3. Integrate filtered pseudo-labels into self-training; compare F1-score against baseline.

## Open Questions the Paper Calls Out

- Question: How does the performance of the pseudo-label scorer change when using different generative model architectures beyond T5-large?
  - Basis in paper: [explicit] The authors use T5-large as the backbone for the pseudo-label scorer and evaluate its effectiveness, but do not explore alternative architectures.
  - Why unresolved: The paper focuses on demonstrating the effectiveness of the generative model approach rather than conducting an exhaustive comparison of different model architectures.
  - What evidence would resolve it: Systematic experiments comparing the pseudo-label scorer's performance using various generative model architectures (e.g., GPT, BART, BERT) on the ASQP task.

- Question: What is the impact of using different ranking-based training objectives (beyond listwise) on the pseudo-label scorer's performance?
  - Basis in paper: [explicit] The authors explore pointwise, pairwise, and listwise ranking objectives for training the scorer, finding the listwise approach slightly better, but do not extensively investigate other ranking methods.
  - Why unresolved: The paper focuses on demonstrating the effectiveness of the ranking-based approach rather than conducting an exhaustive comparison of different ranking objectives.
  - What evidence would resolve it: Systematic experiments comparing the pseudo-label scorer's performance using various ranking-based training objectives (e.g., LambdaRank, ListNet) on the ASQP task.

- Question: How does the performance of the pseudo-label scorer vary with different levels of human or AI annotation quality in the comparison dataset?
  - Basis in paper: [explicit] The authors explore using human-annotated and AI-annotated comparison datasets, finding AI annotation feasible and cost-effective, but do not investigate the impact of annotation quality variations.
  - Why unresolved: The paper focuses on demonstrating the feasibility of AI annotation rather than investigating the impact of annotation quality on the scorer's performance.
  - What evidence would resolve it: Systematic experiments comparing the pseudo-label scorer's performance using comparison datasets with varying levels of annotation quality (e.g., low, medium, high) on the ASQP task.

## Limitations

- The approach relies heavily on the quality of the pseudo-label scorer, which depends on the comparison dataset used for training, and exact filtering thresholds must be tuned per dataset.
- While AI-annotated comparison data is presented as cost-effective, its quality relative to human annotation is not thoroughly validated across all datasets.
- The exact thresholds for filtering (γ2, γ3) are not specified and must be tuned per dataset, introducing potential variability in results.

## Confidence

- **High Confidence**: The core self-training framework with two-stage filtering is well-supported by experimental results across four datasets, showing consistent improvements over baselines.
- **Medium Confidence**: The claim that generative likelihood scores better capture review-to-pseudo-label alignment is plausible given the mechanism described, but lacks direct comparative evidence against discriminative methods.
- **Medium Confidence**: The effectiveness of human-annotated comparison data over rule-based negatives is demonstrated, but the evaluation is limited to accuracy on the comparison set rather than downstream ASQP performance.

## Next Checks

1. **Cross-dataset scorer generalization**: Evaluate the pseudo-label scorer trained on one dataset's comparison data on pseudo-labels from another dataset to test generalization.
2. **Ablation of filtering stages**: Compare self-training performance with only confidence-based filtering, only scorer-based filtering, and both stages to isolate their individual contributions.
3. **Human vs AI annotation quality**: Conduct a detailed error analysis comparing human-annotated vs AI-annotated comparison datasets to quantify quality differences and their impact on scorer performance.