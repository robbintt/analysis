---
ver: rpa2
title: 'LAPIS: Language Model-Augmented Police Investigation System'
arxiv_id: '2407.20248'
source_url: https://arxiv.org/abs/2407.20248
tags:
- investigation
- crime
- legal
- police
- criminal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LAPIS, a language model-augmented police
  investigation system designed to assist officers in making legally sound investigative
  decisions. The authors constructed a specialized dataset and knowledgebase for crime
  investigation legal reasoning, enhanced with manual curation by domain experts.
---

# LAPIS: Language Model-Augmented Police Investigation System

## Quick Facts
- arXiv ID: 2407.20248
- Source URL: https://arxiv.org/abs/2407.20248
- Reference count: 40
- Key result: LAPIS achieved 74% hypothesis assessment accuracy, outperforming GPT-4's 73%

## Executive Summary
LAPIS is a specialized AI system designed to assist police officers in making legally sound investigative decisions through hypothesis assessment. The system combines a finetuned Korean language model with a retrieval-augmented knowledgebase of criminal law articles, court rulings, and investigation textbooks. By integrating domain-specific legal reasoning with access to relevant precedents, LAPIS demonstrates improved accuracy over general-purpose models while enabling local deployment for data privacy.

## Method Summary
The authors constructed a specialized dataset from Korean National Police Agency exam questions and expert-annotated legal texts, then finetuned smaller Korean language models on this data. They integrated the finetuned model with a retrieval-based system that searches a Criminal Investigation Knowledgebase (CIKB) containing embedded legal articles and precedents. The system uses a Retriever to find relevant premises and an Evaluator to generate hypothesis assessments with rationales, combining finetuning with RAG for enhanced legal reasoning.

## Key Results
- LAPIS achieved 74% hypothesis assessment accuracy versus GPT-4's 73%
- Qualitative analysis showed LAPIS generated legally correct rationales by leveraging CIKB premises
- The system demonstrated practical advantages through smaller size and local deployment capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finetuning a small language model on specialized crime investigation data outperforms GPT-4
- Mechanism: Domain-specific finetuning teaches legal reasoning patterns from expert-curated data
- Core assumption: Specialized finetuning provides better legal reasoning than general-purpose models
- Evidence anchors: 74% vs 73% accuracy advantage; best performance from EEVE-Korean-Instruct finetuning

### Mechanism 2
- Claim: RAG with CIKB enhances legal reasoning through relevant premise access
- Mechanism: Retriever finds top 5 relevant legal precedents which Evaluator uses for rationales
- Core assumption: Access to legal precedents improves conclusion accuracy
- Evidence anchors: CIKB premise leveraging for legally correct rationales

### Mechanism 3
- Claim: Manual expert curation improves training data quality
- Mechanism: 20 law enforcement officials review and revise GPT-generated rationales
- Core assumption: Human expertise corrects automated system errors
- Evidence anchors: 14,105 curated data instances in final training set

## Foundational Learning

- Concept: Legal reasoning in criminal investigation
  - Why needed here: System must assess hypotheses using crime-specific legal principles
  - Quick check question: What distinguishes criminal investigation legal reasoning from general legal reasoning?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: Supplements model knowledge with external CIKB information
  - Quick check question: How does RAG differ from standard prompting for domain knowledge?

- Concept: Finetuning vs. zero-shot learning
  - Why needed here: Determines when specialized models outperform general-purpose ones
  - Quick check question: What are trade-offs between finetuned small models and larger zero-shot models?

## Architecture Onboarding

- Component map: Input hypothesis -> Retriever searches CIKB -> Top premises retrieved -> Evaluator generates assessment -> Output returned
- Critical path: Hypothesis input flows through retriever to fetch premises, then evaluator generates assessment with rationale
- Design tradeoffs: Smaller SLM enables local deployment and privacy but may have lower general capabilities; RAG provides domain knowledge but adds latency; manual curation improves quality but increases development time
- Failure signatures: Incorrect assessments indicate knowledgebase gaps or poor retrieval; hallucinated rationales suggest evaluator issues; slow responses point to retrieval or model problems
- First 3 experiments:
  1. Test retriever precision by measuring relevant premise retrieval rate with known contexts
  2. Evaluate evaluator accuracy on held-out test set with different prompting strategies
  3. Measure end-to-end latency from input to output under realistic usage patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LAPIS be extended to handle multilingual crime investigation scenarios across different legal systems?
- Basis in paper: Authors explicitly state dataset and knowledgebase is exclusively Korean and plan to seek multilingual approaches
- Why unresolved: Paper acknowledges limitation but provides no solutions or preliminary experiments
- What evidence would resolve it: Experimental results on non-Korean data or proposed cross-lingual architecture modifications

### Open Question 2
- Question: What data augmentation techniques could expand limited training data for crime investigation legal reasoning?
- Basis in paper: Authors identify need for data augmentation despite using 10 years of exam questions
- Why unresolved: Need is stated but no augmentation strategies explored or implemented
- What evidence would resolve it: Comparative experiments showing performance improvements from different augmentation methods

### Open Question 3
- Question: How can LAPIS be enhanced to explicitly suggest investigative actions rather than just assessments?
- Basis in paper: Authors note system doesn't suggest investigative actions and plan to add this functionality
- Why unresolved: Current system focuses on assessment without actionable guidance
- What evidence would resolve it: Implementation and evaluation of action recommendation modules

## Limitations

- The 1% accuracy advantage over GPT-4 is statistically marginal and may not be practically significant
- System effectiveness depends heavily on CIKB completeness and quality
- Manual curation introduces potential subjectivity and inconsistency in rationale quality assessment

## Confidence

**High Confidence:** Technical architecture clearly specified with reasonable implementation details; finetuning procedure and evaluation metrics well-defined

**Medium Confidence:** 1% accuracy advantage supported by experimental results but represents marginal difference; qualitative rationale analysis provides reasonable evidence but relies on subjective expert evaluation

**Low Confidence:** Generalizability to real-world investigations beyond structured exam questions; long-term effectiveness in dynamic legal environments

## Next Checks

1. Conduct statistical significance testing on the 1% accuracy difference using paired t-tests or bootstrap confidence intervals across multiple test sets

2. Perform knowledgebase coverage analysis by identifying missing legal precedents and measuring impact on hypothesis assessment accuracy

3. Deploy LAPIS in controlled field study with active police investigators on actual cases to assess real-world performance