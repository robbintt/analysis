---
ver: rpa2
title: Hyperparameter Tuning MLPs for Probabilistic Time Series Forecasting
arxiv_id: '2403.04477'
source_url: https://arxiv.org/abs/2403.04477
tags:
- forecasting
- time
- series
- length
- hyperparameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates hyperparameter tuning for MLP-based probabilistic
  time series forecasting. It evaluates the impact of context length and validation
  strategy on model performance across 20 datasets from the Monash Forecasting Repository,
  testing 4800 configurations per dataset to create the TSBench metadataset with 97,200
  evaluations.
---

# Hyperparameter Tuning MLPs for Probabilistic Time Series Forecasting

## Quick Facts
- **arXiv ID**: 2403.04477
- **Source URL**: https://arxiv.org/abs/2403.04477
- **Reference count**: 29
- **Primary result**: Context length tuning is critical for MLP-based probabilistic time series forecasting, with NLinear MLP outperforming TBATS on 7/20 datasets

## Executive Summary
This study systematically investigates hyperparameter tuning for Multi-Layer Perceptrons (MLPs) in probabilistic time series forecasting across 20 datasets from the Monash Forecasting Repository. The authors evaluate 4800 hyperparameter configurations per dataset to create the TSBench metadataset containing 97,200 evaluations, focusing on context length, hidden layer size, batch size, and learning rate. The research introduces probabilistic adaptations of NLinear MLP models and demonstrates that context length should be tuned per dataset rather than using a fixed value. The NLinear model outperforms the benchmark TBATS model on 7 out of 20 datasets. The TSBench metadataset proves effective for multi-fidelity hyperparameter optimization, with SMAC, HyperBand, and BOHB methods outperforming random selection. The work establishes a comprehensive benchmark for time series forecasting research and highlights the importance of systematic hyperparameter tuning.

## Method Summary
The study employs a comprehensive experimental design evaluating MLP-based probabilistic time series forecasting models across 20 datasets from the Monash Forecasting Repository. Researchers test 4800 hyperparameter configurations per dataset, varying context length, hidden layer size, batch size, and learning rate, generating 97,200 total evaluations. The NLinear MLP model is adapted for probabilistic forecasting using Quantile Loss (QLoss) and Continuous Ranked Probability Score (CRPS) metrics. The TSBench metadataset is created with extensive metadata including loss metrics, gradient statistics, and runtime information. Multi-fidelity hyperparameter optimization is evaluated using SMAC, HyperBand, and BOHB methods, compared against random selection. The study also examines validation strategies and their impact on model performance, while systematically logging comprehensive performance and training statistics across all configurations.

## Key Results
- Context length tuning is critical and should be performed per dataset rather than using a fixed value
- NLinear MLP model outperforms TBATS (best Monash benchmark model) on 7 out of 20 datasets
- Retraining on validation data provides minimal advantage for time series forecasting
- SMAC, HyperBand, and BOHB hyperparameter optimization methods outperform random selection on TSBench metadataset
- TSBench metadataset demonstrates effectiveness for multi-fidelity hyperparameter optimization

## Why This Works (Mechanism)
The effectiveness of context length tuning stems from the fundamental nature of time series data, where relevant historical information varies significantly across different datasets and forecasting horizons. MLP models rely on the provided context window to capture temporal patterns, seasonality, and dependencies. When context length is optimized for each dataset, the model can access the most relevant historical information needed for accurate forecasting. The probabilistic adaptation through Quantile Loss and CRPS metrics allows the model to capture uncertainty in predictions, which is crucial for real-world forecasting applications where confidence intervals are as important as point estimates. The comprehensive evaluation across 97,200 configurations provides robust evidence about hyperparameter sensitivities and establishes reliable baselines for future research.

## Foundational Learning

**Time Series Forecasting**: Predicting future values based on historical data patterns; needed to understand the forecasting task and evaluation metrics; quick check: verify understanding of train/validation/test splits in temporal data.

**Probabilistic Forecasting**: Generating prediction intervals rather than point estimates; needed for capturing uncertainty in predictions; quick check: confirm understanding of QLoss and CRPS metrics.

**Hyperparameter Optimization**: Systematic search for optimal model configuration; needed to identify critical hyperparameters and their interactions; quick check: understand difference between grid search, random search, and Bayesian optimization.

**Context Length**: Number of historical time steps used as input; needed because it directly impacts model's ability to capture temporal dependencies; quick check: verify how context length affects model performance across different datasets.

**Multi-Fidelity Optimization**: Methods that use partial information to guide the search process; needed for efficient hyperparameter optimization; quick check: understand how early stopping and budget allocation work in SMAC, HyperBand, and BOHB.

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Context Window Creation -> MLP Model Training -> Probabilistic Output Generation -> Performance Evaluation -> Metadata Logging

**Critical Path**: Context length selection -> Model training with QLoss/CRPS optimization -> Performance evaluation on validation set -> Metadata collection for TSBench

**Design Tradeoffs**: Longer context lengths capture more historical information but increase computational cost and risk overfitting; probabilistic outputs provide uncertainty estimates but require more complex loss functions and training procedures.

**Failure Signatures**: Poor performance with fixed context length across datasets; suboptimal hyperparameter optimization results when using random selection instead of multi-fidelity methods; inadequate uncertainty quantification when using point estimate methods.

**First Experiments**:
1. Test NLinear MLP with varying context lengths (10, 50, 100, 200) on a single dataset to observe performance trends
2. Compare probabilistic vs deterministic versions of the model using the same hyperparameter configurations
3. Evaluate SMAC vs random hyperparameter optimization on a small subset of the TSBench metadataset

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow focus on MLP-based models without exploring other architectures like Transformers or LSTMs
- Limited hyperparameter exploration to only four dimensions, missing potentially important parameters like activation functions and dropout rates
- TSBench metadataset restricted to Monash repository datasets, limiting generalizability to other data distributions
- Single validation methodology used, potentially missing nuances from alternative validation schemes
- No investigation of temporal dependencies beyond immediate context window

## Confidence

**High confidence**: Context length tuning is critical across datasets
**Medium confidence**: NLinear vs TBATS comparative performance (limited to tested datasets)
**Medium confidence**: Validation strategy findings (based on single methodology)
**Low confidence**: Generalizability to non-MLP architectures or different forecasting tasks

## Next Checks

1. Replicate key experiments with alternative neural architectures (Transformers, LSTMs) to assess whether context length tuning remains critical across model families
2. Test TSBench metadataset's effectiveness for hyperparameter optimization on entirely new time series datasets not used in original study
3. Evaluate alternative validation strategies including time-based cross-validation and expanding window approaches to verify robustness of validation findings