---
ver: rpa2
title: An Introduction to Centralized Training for Decentralized Execution in Cooperative
  Multi-Agent Reinforcement Learning
arxiv_id: '2409.03052'
source_url: https://arxiv.org/abs/2409.03052
tags:
- learning
- methods
- centralized
- agent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Centralized Training for Decentralized Execution
  (CTDE) in cooperative Multi-Agent Reinforcement Learning (MARL). CTDE allows centralized
  training with access to all agents' information, but requires decentralized execution
  where agents act based on local observations only.
---

# An Introduction to Centralized Training for Decentralized Execution in Cooperative Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2409.03052
- Source URL: https://arxiv.org/abs/2409.03052
- Reference count: 17
- One-line primary result: Introduces CTDE paradigm for cooperative MARL with centralized training and decentralized execution

## Executive Summary
This paper presents Centralized Training for Decentralized Execution (CTDE) as a fundamental paradigm for cooperative Multi-Agent Reinforcement Learning (MARL). CTDE addresses the non-stationarity problem in multi-agent learning by allowing agents to access centralized information during training while maintaining decentralized execution where agents act based only on local observations. The paper systematically reviews two main categories of CTDE methods: value function factorization approaches (VDN, QMIX, QPLEX) and centralized critic actor-critic methods (MADDPG, COMA, MAPPO), discussing their mechanisms, advantages, and limitations.

## Method Summary
The paper reviews CTDE methods for cooperative MARL, focusing on two main categories. Value function factorization methods learn individual Q-functions per agent that are combined into a joint Q-function during training, enabling decentralized execution. Centralized critic actor-critic methods use a shared critic to estimate the joint value function, which updates decentralized policies for execution. The methods are designed to work in decentralized partially observable Markov decision processes (Dec-POMDPs) where agents have partial observability and must coordinate to maximize joint rewards while acting based only on local information.

## Key Results
- CTDE is the only paradigm requiring separate training phases where any available information can be used
- Value function factorization methods like VDN, QMIX, and QPLEX learn decentralized Q-functions combined into joint Q-functions
- Centralized critic methods like MADDPG, COMA, and MAPPO use shared critics to update decentralized actor policies
- The choice between critic types (centralized, decentralized, history-state) involves a bias-variance tradeoff

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Value function factorization methods learn decentralized Q-functions per agent that are combined into a joint Q-function during training, enabling decentralized execution.
- Mechanism: Each agent learns an individual Q-function (Qi) based on local observations. These Qi values are combined (via sum in VDN, monotonic function in QMIX, etc.) into a joint Q-function used for centralized training. During execution, agents use only their individual Qi to select actions.
- Core assumption: The joint Q-function can be factorized into a function of individual agent Q-values without loss of information about optimal joint actions.
- Evidence anchors:
  - [abstract]: "Value function factorization methods (like VDN, QMIX, QPLEX) learn individual Q-functions per agent that are combined into a joint Q-function"
  - [section 3.2]: "it assumes the following approximate factorization of the Q-function: Q(h, a) ≈ Σ Qi(hi, ai)"
  - [corpus]: Weak evidence - only 5 related papers with 0 citations each, but titles confirm focus on CTDE and value factorization
- Break condition: When agents' optimal actions depend on other agents' actions at the same time step, factorization methods fail to accurately represent the joint Q-function

### Mechanism 2
- Claim: Centralized critic actor-critic methods use a shared critic to estimate the joint value function, which updates decentralized policies for execution.
- Mechanism: A centralized critic (ˆQ or ˆV) takes in joint information during training to estimate the value of the current set of decentralized policies. This critic is then used to update each agent's individual policy through policy gradients, allowing agents to act based on local information during execution.
- Core assumption: The centralized critic can accurately estimate the value of decentralized policies, even when agents have partial observability.
- Evidence anchors:
  - [abstract]: "centralized critic actor-critic methods (like MADDPG, COMA, MAPPO) use a shared critic to update decentralized policies"
  - [section 4.2]: "The centralized critic can be used during CTDE but then each agent can act in a decentralized manner by using its actor"
  - [section 4.6]: "The centralized critic estimates the joint value function of the decentralized policies"
- Break condition: When the centralized critic cannot accurately estimate values due to partial observability mismatch with decentralized actors

### Mechanism 3
- Claim: CTDE methods address the non-stationarity problem in multi-agent learning by providing centralized information during training while maintaining decentralized execution.
- Mechanism: During training, agents can access centralized information (other agents' policies, full states, joint rewards) to learn coordinated behaviors. This centralized information helps stabilize learning and improve coordination. During execution, agents act based only on local observations, maintaining the decentralized requirement.
- Core assumption: Centralized information during training can be effectively leveraged to learn policies that perform well under decentralized execution constraints.
- Evidence anchors:
  - [abstract]: "CTDE allows centralized training with access to all agents' information, but requires decentralized execution where agents act based on local observations only"
  - [section 2]: "CTDE is the only paradigm that requires a separate training phase where any available information (e.g., other agent policies, underlying states) can be used"
  - [section 4.7]: "Centralized critics can also be more difficult to learn in problems with large numbers of agents... making decentralized critics more scalable"
- Break condition: When the gap between training information availability and execution constraints is too large for learned policies to bridge

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: CTDE methods build on single-agent RL foundations, and understanding MDP/POMDP structure is essential for grasping the multi-agent extension
  - Quick check question: What is the key difference between MDPs and POMDPs that makes the latter more challenging?

- Concept: Value-based vs. Policy Gradient methods in RL
  - Why needed here: The paper categorizes CTDE methods into these two main classes, so understanding their differences is crucial
  - Quick check question: How do value-based methods choose actions compared to policy gradient methods?

- Concept: Function approximation and neural networks in RL
  - Why needed here: Modern CTDE methods use deep neural networks to approximate value functions and policies, making this knowledge essential
  - Quick check question: Why do value-based methods like DQN use separate target networks?

## Architecture Onboarding

- Component map: Environment -> Agent observations -> Decentralized actors -> Centralized critic/value functions -> Mixing networks -> Joint Q-function
- Critical path: During training - agents interact with environment → store experiences → centralized critic estimates joint values → update decentralized policies → repeat. During execution - agents use learned decentralized policies based on local observations
- Design tradeoffs: Centralized information improves coordination but increases computational complexity; value factorization reduces complexity but may lose expressiveness; state-based critics are easier to learn but can be biased in partially observable settings
- Failure signatures: Poor coordination despite training success, agents learning suboptimal independent policies, high variance in policy updates, inability to scale to many agents
- First 3 experiments:
  1. Implement VDN on a simple cooperative gridworld to verify basic value factorization works
  2. Compare QMIX vs. VDN on a coordination-intensive task to observe monotonic constraint benefits
  3. Test MAPPO with different critic types (state-based vs. history-state) on a partially observable environment to measure bias-variance tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between QTRAN and QMIX/Weighted QMIX in terms of representational capacity?
- Basis in paper: [explicit] The paper states "the relationship to QMIX and weighted QMIX is unclear."
- Why unresolved: While QTRAN is described as more general than VDN, the paper does not provide a clear comparison of its expressiveness relative to QMIX and Weighted QMIX.
- What evidence would resolve it: Formal proofs or empirical studies demonstrating the representational limitations of QTRAN compared to QMIX and Weighted QMIX, particularly in terms of the classes of functions they can represent.

### Open Question 2
- Question: How does the choice of critic (centralized vs. decentralized vs. history-state) impact learning efficiency and performance in partially observable environments?
- Basis in paper: [explicit] The paper discusses the bias-variance tradeoff in choosing different types of critics and notes that "the choice of critic is often a bias-variance tradeoff."
- Why unresolved: While the paper provides theoretical and empirical evidence for the benefits and drawbacks of different critic types, it does not offer a definitive answer on which critic type is optimal in various scenarios.
- What evidence would resolve it: Systematic empirical studies comparing the learning efficiency and performance of different critic types across a wide range of partially observable environments, considering factors such as observation noise, action space size, and number of agents.

### Open Question 3
- Question: What are the optimal methods for exploring in CTDE MARL, and how do they compare to exploration methods in single-agent RL and DTE MARL?
- Basis in paper: [inferred] The paper mentions exploration but does not discuss it in detail, stating "There are many other issues that are important to CTDE but are not discussed in this text, including exploration and communication."
- Why unresolved: Exploration is a critical component of RL, and the paper acknowledges its importance but does not provide insights into how it should be handled in CTDE settings compared to other paradigms.
- What evidence would resolve it: Empirical studies comparing the performance of various exploration strategies (e.g., epsilon-greedy, Boltzmann exploration, intrinsic motivation) in CTDE MARL versus single-agent RL and DTE MARL across different domains and task complexities.

## Limitations

- The assumption that joint Q-functions can be accurately decomposed without information loss needs more empirical validation across diverse problem domains
- Scalability analysis focuses primarily on computational complexity rather than real-world performance degradation as agent numbers increase
- Comparison between centralized critic and value factorization approaches lacks comprehensive empirical benchmarking across multiple problem types

## Confidence

- Value function factorization mechanisms: Medium - The theoretical framework is well-established, but practical limitations in complex coordination scenarios need more empirical validation
- Centralized critic effectiveness: Medium - Strong theoretical foundation, but performance can degrade significantly in highly partial observable environments
- CTDE paradigm benefits: High - The fundamental advantage of addressing non-stationarity is well-demonstrated, though practical implementation challenges remain

## Next Checks

1. Test value function factorization methods on coordination-intensive tasks where optimal joint actions depend on other agents' simultaneous decisions to measure factorization failure points
2. Compare centralized critic performance under varying levels of partial observability to quantify the bias-variance tradeoff empirically
3. Evaluate scalability by systematically increasing agent numbers in benchmark environments and measuring both performance and computational resource requirements