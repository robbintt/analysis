---
ver: rpa2
title: 'AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach'
arxiv_id: '2402.09334'
source_url: https://arxiv.org/abs/2402.09334
tags:
- llms
- auditllm
- auditing
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AuditLLM is a tool designed to audit large language models (LLMs)
  by detecting inconsistencies in their responses to variably phrased queries. The
  tool uses two LLMs: one generates diverse probes from a single input question, and
  the other answers these probes.'
---

# AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach

## Quick Facts
- **arXiv ID**: 2402.09334
- **Source URL**: https://arxiv.org/abs/2402.09334
- **Reference count**: 40
- **Primary result**: AuditLLM is a tool for detecting inconsistencies in LLM responses to variably phrased queries using a two-stage probe-response similarity approach

## Executive Summary
AuditLLM is a systematic tool designed to audit large language models (LLMs) by detecting inconsistencies in their responses to differently phrased but semantically equivalent questions. The tool employs a two-LLM approach: one LLM generates diverse probes from a single input question, while another LLM answers these probes. The semantic similarity of responses is then measured using BERTScore to identify inconsistencies. AuditLLM offers two operational modes—Live for real-time auditing of individual queries and Batch for processing multiple queries simultaneously—providing both immediate feedback and scalable analysis capabilities.

## Method Summary
AuditLLM uses a multiprobe approach where LLM1 (Mistral 7B with temperature 0.0) generates 5 diverse probes from each user query. These probes are then answered by LLM2 (user-selected from Llama 2-7B, Falcon, Zephyr 7B, Vicuna, or Alpaca). The tool computes semantic similarity of responses using BERTScore with MPNet-based embeddings and cosine similarity. In Batch mode, probe similarity versus response similarity is plotted with regression analysis—a 45° slope indicates balanced output, higher slopes suggest greater consistency, and lower slopes indicate more inconsistency. The tool is implemented as a Gradio interface and available on HuggingFace.

## Key Results
- AuditLLM successfully detects inconsistencies in LLM responses by comparing semantic similarity of responses to semantically equivalent but differently phrased questions
- The regression analysis approach (probe similarity vs. response similarity) provides a quantitative measure of LLM consistency, with ideal slope of 45°
- The tool demonstrated effectiveness in comparing consistency across different LLMs using standardized datasets like TruthfulQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage probing (probe generation + response generation) enables systematic inconsistency detection
- Mechanism: LLM1 generates diverse probes from a single input, LLM2 responds to each probe, and semantic similarity of responses is measured via BERTScore
- Core assumption: A robust LLM will produce semantically similar responses to variably phrased but semantically equivalent questions
- Evidence anchors: [abstract], [section 1]
- Break condition: If LLM1 generates probes that are not semantically equivalent to the original question, the similarity comparison becomes meaningless

### Mechanism 2
- Claim: Batch mode enables scalable, quantitative comparison of LLM consistency across multiple queries
- Mechanism: Multiple user questions are transformed into diverse probes; responses are scored for semantic similarity; regression plot of probe similarity vs. response similarity quantifies consistency
- Core assumption: Probe similarity and response similarity should correlate if the LLM is consistent
- Evidence anchors: [section 3.2]
- Break condition: If the LLM's temperature is not controlled, response variability can confound the regression slope

### Mechanism 3
- Claim: Live mode provides immediate, interpretable feedback on LLM consistency for individual queries
- Mechanism: Real-time probe generation and response similarity scoring with visual highlighting of similar sentences above 60% threshold
- Core assumption: Highlighting semantically similar responses helps users quickly identify consistency issues
- Evidence anchors: [section 3.1]
- Break condition: If BERTScore threshold is too high, subtle inconsistencies may be missed; too low, noise may dominate

## Foundational Learning

- Concept: Semantic similarity metrics (BERTScore, cosine similarity)
  - Why needed here: To quantify how similar LLM responses are across differently phrased but semantically equivalent questions
  - Quick check question: What is the difference between BERTScore and simple cosine similarity in evaluating text similarity?

- Concept: Probe generation and diversity criteria
  - Why needed here: LLM1 must generate probes that are both relevant to the original question and diverse enough to test consistency
  - Quick check question: How would you define and measure "relevance" and "diversity" in the context of probe generation?

- Concept: Regression analysis and slope interpretation
  - Why needed here: To interpret the relationship between probe similarity and response similarity, indicating consistency levels
  - Quick check question: What does a regression line slope of 45° represent in the context of probe-response similarity analysis?

## Architecture Onboarding

- Component map: User Interface (Gradio) -> LLM1 Probe Generation (Mistral 7B) -> LLM2 Response Generation (user-selected) -> Similarity Engine (BERTScore + MPNet) -> Output Formatter
- Critical path: User Input → LLM1 Probe Generation → LLM2 Response Generation → Similarity Scoring → Result Display
- Design tradeoffs:
  - Using open-source LLMs limits scalability but ensures accessibility
  - Fixed BERTScore threshold simplifies interpretation but may miss nuanced inconsistencies
  - Real-time mode sacrifices depth for immediacy
- Failure signatures:
  - High variance in similarity scores across identical runs → temperature instability
  - Probes not semantically equivalent → LLM1 generation failure
  - Extremely low or high regression slope → model inconsistency or probe generation bias
- First 3 experiments:
  1. Run Live mode with a simple factual question (e.g., "What is the capital of France?") and verify that all probe responses are highly similar
  2. Use Batch mode with the TruthfulQA dataset to generate the probe-response similarity scatter plot and check regression slope interpretation
  3. Compare consistency scores of two different LLMs on the same set of questions to validate comparative auditing capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of AuditLLM's inconsistency detection compare across different domains (e.g., healthcare vs. finance) when using the same set of probes?
- Basis in paper: [explicit] The paper mentions that LLMs are integrated into critical sectors like healthcare, education, legal, and financial services, but does not test AuditLLM across these domains
- Why unresolved: The paper only demonstrates AuditLLM on a general dataset (TruthfulQA) without domain-specific testing
- What evidence would resolve it: Testing AuditLLM on domain-specific question sets (e.g., medical diagnosis queries, financial analysis questions) and comparing consistency scores across domains

### Open Question 2
- Question: What is the optimal temperature setting for LLM1 (probe generation) that balances probe diversity with relevance without introducing excessive noise?
- Basis in paper: [explicit] The paper sets LLM1 temperature to 0.0 for probe generation, but acknowledges this as a parameter choice without exploring alternatives
- Why unresolved: The paper does not experiment with different temperature settings for probe generation to determine the optimal balance
- What evidence would resolve it: Systematic testing of LLM1 temperature settings (e.g., 0.0, 0.3, 0.5, 0.7) and measuring the impact on probe quality, diversity metrics, and subsequent consistency detection effectiveness

### Open Question 3
- Question: How does the choice of similarity metric (BERTScore vs. alternatives like ROUGE or BLEU) affect the detection of inconsistencies in LLM responses?
- Basis in paper: [explicit] The paper uses BERTScore via sentence-transformer (all-mpnet-base-v2) for semantic similarity measurement but does not compare it to other metrics
- Why unresolved: The paper presents BERTScore as the chosen metric without benchmarking against other established text similarity measures
- What evidence would resolve it: Running the same AuditLLM experiments with different similarity metrics (BERTScore, ROUGE, BLEU, cosine similarity with different embeddings) and comparing their sensitivity to actual inconsistencies in LLM responses

## Limitations

- The tool's effectiveness depends heavily on the quality of LLM1's probe generation—if probes are not truly semantically equivalent to the original question, the similarity comparison becomes meaningless
- The threshold for "acceptable" consistency (45° regression slope) is not empirically validated against known benchmarks of LLM reliability
- The tool does not address how to quantify or control probe relevance and diversity beyond qualitative descriptions

## Confidence

- **High Confidence**: The tool's architecture and implementation (Gradio interface, BERTScore similarity computation, dual-mode operation) are well-specified and reproducible
- **Medium Confidence**: The theoretical mechanism of detecting inconsistency through probe-response similarity comparison is sound, but empirical validation against ground truth inconsistency cases is limited
- **Medium Confidence**: The interpretation of regression slope as a consistency metric is logical but lacks extensive empirical benchmarking across diverse LLM types and domains

## Next Checks

1. **Probe Generation Validation**: Test AuditLLM with queries where ground truth inconsistencies are known (e.g., ambiguous factual questions with multiple valid answers) to verify that the tool correctly identifies meaningful inconsistencies versus acceptable response variation

2. **Cross-Model Benchmarking**: Compare consistency scores across multiple LLM pairs (different LLM1 and LLM2 combinations) on standardized datasets to establish whether the tool's relative consistency rankings align with independent assessments of model reliability

3. **Threshold Sensitivity Analysis**: Systematically vary the BERTScore similarity threshold (60% as stated) and observe how this affects the detection of inconsistencies, particularly for subtle versus severe inconsistencies, to determine optimal threshold settings for different use cases