---
ver: rpa2
title: Optimal or Greedy Decision Trees? Revisiting their Objectives, Tuning, and
  Performance
arxiv_id: '2409.12788'
source_url: https://arxiv.org/abs/2409.12788
tags:
- trees
- optimal
- data
- greedy
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Optimal decision trees (ODTs) are trained by maximizing accuracy,
  but prior work rarely analyzed the effects of different objective functions or tuning
  methods. We evaluated 13 objectives and seven tuning approaches on 165 datasets
  and found that ODTs benefit from maximizing accuracy directly, but for small datasets,
  non-concave objectives like M-loss yield better performance by penalizing unbalanced
  leaf nodes.
---

# Optimal or Greedy Decision Trees? Revisiting their Objectives, Tuning, and Performance

## Quick Facts
- arXiv ID: 2409.12788
- Source URL: https://arxiv.org/abs/2409.12788
- Reference count: 31
- Optimal decision trees outperform depth-constrained greedy methods by 1.1-1.3% accuracy

## Executive Summary
This paper compares optimal decision trees (ODTs) with greedy methods across 165 datasets, examining different objective functions and tuning approaches. The authors find that ODTs benefit from maximizing accuracy directly, but for small datasets, non-concave objectives like M-loss yield better performance by penalizing unbalanced leaf nodes. Tuning significantly improves accuracy (1% on average), with tree size or complexity cost tuning being most effective. Under equivalent depth constraints, ODTs achieve 1.1-1.3% higher accuracy than greedy methods while maintaining better accuracy-interpretability trade-offs.

## Method Summary
The study evaluates optimal decision trees using dynamic programming approaches (primarily STreeD) against greedy methods like CART. Datasets are binarized for optimal methods, and multiple objective functions are tested including accuracy, Gini, entropy, and newly proposed non-concave objectives. Seven tuning approaches are evaluated, varying tree size, complexity cost, and stopping criteria. Performance is measured using out-of-sample accuracy, size-weighted accuracy, and runtime across datasets of varying sizes.

## Key Results
- ODTs outperform depth-constrained greedy methods by 1.1-1.3% accuracy
- Non-concave objectives like M-loss work better than accuracy for small datasets
- Tuning complexity parameters provides 1% average accuracy improvement
- ODTs avoid the overfitting and large tree issues seen in unconstrained greedy approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal decision trees outperform greedy methods by 1-2% accuracy under same depth constraints.
- Mechanism: ODTs globally optimize accuracy, while greedy methods use local splitting criteria that may not lead to globally optimal trees.
- Core assumption: The NP-hard nature of optimal tree learning means greedy heuristics are suboptimal, but recent DP approaches can find optimal solutions within reasonable time for small trees.
- Evidence anchors:
  - [abstract] "Optimal decision trees (ODTs) are trained by maximizing accuracy, but prior work rarely analyzed the effects of different objective functions or tuning methods. We evaluated 13 objectives and seven tuning approaches on 165 datasets and found that ODTs benefit from maximizing accuracy directly, but for small datasets, non-concave objectives like M-loss yield better performance by penalizing unbalanced leaf nodes."
  - [section 5.4.2] "Optimal methods under the same depth constraint (up to depth four) find trees with 1-2% higher out-of-sample accuracy than greedy methods (Bertsimas and Dunn, 2017; Verwer and Zhang, 2017; Demirović et al., 2022). Supported: We evaluate the accuracy of depth three and four trees on 164 data sets and find an average improvement of 1.1% and 1.3% of optimal over greedy approaches."
  - [corpus] Weak evidence - no directly related papers found, but this is expected as the field is relatively new.
- Break Condition: If tree depth increases beyond practical limits for ODT methods, greedy methods may catch up due to scalability advantages.

### Mechanism 2
- Claim: Non-concave objectives work better than concave objectives for small datasets.
- Mechanism: Non-concave objectives penalize unbalanced leaf nodes more strongly, which helps prevent overfitting when data is limited.
- Core assumption: With limited data, traditional concave objectives (like entropy, Gini) overvalue pure nodes even when this means higher misclassification rate.
- Evidence anchors:
  - [abstract] "We evaluated 13 objectives and seven tuning approaches on 165 datasets and found that ODTs benefit from maximizing accuracy directly, but for small datasets, non-concave objectives like M-loss yield better performance by penalizing unbalanced leaf nodes."
  - [section 3.2] "The odd behavior of the greedy criteria in Fig. 2 and Table 1 is a result of their strict concavity. (Strict) concavity is not a requirement for ODTs because ODTs do not consider splitting criteria and can search beyond a non-improving split."
  - [section 3.4.3] "The new non-concave objectives perform significantly better than accuracy, with the test accuracy of M-loss on average exceeding that of the accuracy objective by 0.7%. Therefore, with sufficient data, ODTs obtain the best performance by maximizing training accuracy but for smaller data sets, other objectives, such as the M-loss can yield better results."
  - [corpus] Weak evidence - no directly related papers found, but this is expected as the field is relatively new.
- Break Condition: As dataset size increases, the difference between concave and non-concave objectives diminishes.

### Mechanism 3
- Claim: Tuning complexity parameters is critical for optimal decision trees.
- Mechanism: Hyperparameter tuning prevents overfitting and finds the right balance between accuracy and tree complexity.
- Core assumption: Without tuning, ODTs will use as many nodes as allowed to maximize training accuracy, leading to overfitting.
- Evidence anchors:
  - [abstract] "Tuning significantly improves accuracy (1% on average), with tuning the tree size or complexity cost being most effective."
  - [section 4.3] "Our experiments highlight the importance of tuning optimal decision tree methods, and that (surprisingly) the differences between the commonly used tuning methods are small."
  - [section 5.4.4] "With hyperparameter tuning, we do not find significant performance differences between optimal and greedy methods with small numbers of samples (up to 200) nor more sensitivity to noise."
  - [corpus] Weak evidence - no directly related papers found, but this is expected as the field is relatively new.
- Break Condition: For very large datasets, the importance of tuning diminishes as the training accuracy approximates test accuracy.

## Foundational Learning

- Concept: Dynamic Programming for Optimal Decision Trees
  - Why needed here: STreeD uses DP to find optimal trees efficiently by caching and reusing solutions to subproblems
  - Quick check question: How does DP exploit the independent subtree structure to improve scalability?

- Concept: Binarization of Features
  - Why needed here: STreeD requires binary features, so continuous features are binarized using quantile thresholds
  - Quick check question: What impact does binarization have on the performance of both optimal and greedy methods?

- Concept: Complexity-Accuracy Trade-off
  - Why needed here: Understanding how tree complexity affects accuracy is crucial for proper tuning and comparison
  - Quick check question: How does the size-weighted accuracy metric capture the accuracy-interpretability trade-off?

## Architecture Onboarding

- Component map: Data → Binarization → DP Optimization → Tuning → Evaluation
- Critical path: Data → Binarization → DP Optimization → Tuning → Evaluation
- Design tradeoffs: DP provides optimality but has exponential runtime in features; binarization simplifies but may lose information
- Failure signatures: Runtime timeouts for large feature spaces; overfitting with improper tuning; accuracy degradation with excessive binarization
- First 3 experiments:
  1. Run STreeD on a small binary dataset without tuning to observe baseline performance
  2. Add tuning with different complexity parameters to see impact on accuracy
  3. Compare with CART on the same dataset with and without depth constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific non-concave objective functions would yield the best performance for small datasets across diverse domains, and how do they compare to traditional concave objectives like Gini impurity and entropy?
- Basis in paper: [explicit] The paper introduces four new non-concave objectives (Error Gini, Error Entropy, M-loss, and L-loss) and shows they outperform accuracy for small datasets. It also states that optimal and greedy methods benefit from different objectives.
- Why unresolved: The paper provides initial experimental results on 165 datasets but does not comprehensively compare all possible non-concave objectives against each other and traditional concave ones across all dataset sizes and domains.
- What evidence would resolve it: A large-scale empirical study comparing all proposed and existing objective functions on a diverse set of datasets with varying sizes, class distributions, and noise levels would determine the optimal choice for different scenarios.

### Open Question 2
- Question: How does the choice of binarization method impact the performance of both optimal and greedy decision tree algorithms, and what is the optimal binarization strategy for different types of data?
- Basis in paper: [explicit] The paper mentions that binarization is required for the dynamic programming approach used and that different binarization approaches were experimented with but had no significant impact. It also notes that binarization impacts CART tree size but not accuracy significantly.
- Why unresolved: The paper only tests a limited number of binarization methods and does not explore the impact of binarization granularity, feature selection before binarization, or alternative binarization techniques like recursive feature binarization.
- What evidence would resolve it: A systematic comparison of different binarization strategies (e.g., varying the number of bins, using domain-specific thresholds, recursive binarization) on a wide range of datasets and their impact on both optimal and greedy tree performance would identify the best practices.

### Open Question 3
- Question: Can the insights from non-concave objective functions for optimal decision trees be effectively adapted to improve the performance of greedy decision tree algorithms, particularly for small and noisy datasets?
- Basis in paper: [explicit] The paper observes that optimal and greedy methods respond differently to objectives, with greedy methods performing best with concave objectives and optimal methods with non-concave ones. It also notes that greedy methods get stuck early with non-concave objectives.
- Why unresolved: The paper does not explore modifications to greedy algorithms to leverage the benefits of non-concave objectives, such as incorporating lookahead or using non-concave objectives in specific stages of tree growth.
- What evidence would resolve it: Developing and testing modified greedy algorithms that incorporate non-concave objectives or techniques to overcome early stagnation, followed by empirical evaluation on datasets where greedy methods typically underperform, would demonstrate the feasibility and benefits of such adaptations.

## Limitations

- Dynamic programming approach remains computationally expensive for large datasets
- Binarization preprocessing may affect feature representation quality
- Hyperparameter tuning experiments may not have explored full tuning space

## Confidence

- Optimal vs. Greedy accuracy comparison: High
- Non-concave objectives for small datasets: Medium
- Tuning importance: High
- Scalability claims: Low (limited empirical evidence)

## Next Checks

1. Test ODT performance on datasets with >100 features to validate scalability claims
2. Compare performance with different binarization strategies to assess preprocessing sensitivity
3. Conduct ablation studies on tuning space exploration to quantify tuning effectiveness bounds