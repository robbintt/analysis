---
ver: rpa2
title: 'QUB-Cirdan at "Discharge Me!": Zero shot discharge letter generation by open-source
  LLM'
arxiv_id: '2406.00041'
source_url: https://arxiv.org/abs/2406.00041
tags:
- discharge
- section
- word
- target
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a zero-shot approach to generate critical sections
  of patient discharge letters using the Llama3 8B quantized model. The method employs
  a template-based approach combined with Retrieval-Augmented Generation (RAG) to
  produce concise, contextually accurate summaries for the "Brief Hospital Course"
  and "Discharge Instructions" sections.
---

# QUB-Cirdan at "Discharge Me!": Zero shot discharge letter generation by open-source LLM

## Quick Facts
- **arXiv ID**: 2406.00041
- **Source URL**: https://arxiv.org/abs/2406.00041
- **Reference count**: 11
- **Primary result**: Zero-shot Llama3 8B model generates discharge letter sections with high automated metric scores while running efficiently on consumer-grade GPU

## Executive Summary
This paper presents a zero-shot approach to generate critical sections of patient discharge letters using the Llama3 8B quantized model. The method employs a template-based approach combined with Retrieval-Augmented Generation (RAG) to produce concise, contextually accurate summaries for the "Brief Hospital Course" and "Discharge Instructions" sections. The approach achieves high scores across multiple evaluation metrics, including BLEU, ROUGE, and METEOR, while demonstrating computational efficiency by running on a T4 GPU with 16GB memory.

## Method Summary
The approach uses a template-based RAG system with the Llama3 8B quantized model to generate discharge letter sections. The method processes clinical data through retrieval mechanisms to identify relevant information, which is then structured using predefined templates. The model generates the "Brief Hospital Course" and "Discharge Instructions" sections in a zero-shot manner without requiring task-specific fine-tuning. The system operates on a T4 GPU with 16GB memory, demonstrating computational efficiency for clinical deployment scenarios.

## Key Results
- Achieved high scores across BLEU, ROUGE, and METEOR evaluation metrics
- Generated contextually accurate summaries for discharge letter sections
- Demonstrated computational efficiency on T4 GPU with 16GB memory

## Why This Works (Mechanism)
The zero-shot approach leverages the Llama3 8B model's strong general language understanding combined with structured templates and RAG to guide generation toward clinically relevant content. The template-based framework provides consistent structure while the retrieval component ensures factual accuracy from source documents. The quantized model balances performance with computational efficiency, enabling deployment on consumer-grade hardware.

## Foundational Learning
- **Zero-shot learning**: Why needed - avoids expensive fine-tuning; Quick check - test with out-of-domain prompts
- **RAG architecture**: Why needed - grounds generation in source documents; Quick check - verify retrieval relevance scores
- **Template-based generation**: Why needed - ensures consistent clinical document structure; Quick check - validate template coverage
- **Quantized models**: Why needed - reduces memory requirements for deployment; Quick check - compare performance with full-precision model
- **Clinical NLP**: Why needed - handles domain-specific terminology and context; Quick check - evaluate on clinical terminology benchmarks
- **GPU memory optimization**: Why needed - enables deployment on resource-constrained hardware; Quick check - monitor memory usage during inference

## Architecture Onboarding
**Component Map**: Clinical Data -> RAG Retriever -> Template Engine -> Llama3 8B -> Discharge Letter Sections
**Critical Path**: Data retrieval → Template selection → Model generation → Output formatting
**Design Tradeoffs**: Zero-shot vs. fine-tuned (flexibility vs. task-specific performance), quantized vs. full model (speed vs. accuracy), template-based vs. free-form (consistency vs. adaptability)
**Failure Signatures**: Incomplete retrieval leading to missing information, template mismatches with source data structure, model hallucinations in clinical details
**First Experiments**: 1) Run with synthetic clinical data to verify pipeline integration, 2) Test template coverage with edge-case patient scenarios, 3) Benchmark memory usage across different quantization levels

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on automated metrics rather than clinical validation or human expert assessment
- Template-based approach may struggle with unstructured or non-standardized clinical narratives
- Computational efficiency claims based on specific T4 GPU configuration untested on different hardware

## Confidence
- **High confidence**: Zero-shot methodology and technical implementation details are clearly described and reproducible
- **Medium confidence**: Reported metric scores are reliable but their clinical relevance is uncertain without human validation
- **Low confidence**: Generalizability to diverse clinical settings and patient populations beyond evaluated dataset

## Next Checks
1. Conduct blinded human evaluation by clinical experts comparing generated discharge letters against reference standards
2. Test the approach across multiple hospital systems with varying EHR formats and documentation styles
3. Evaluate performance on hardware configurations below the stated 16GB T4 GPU requirements to assess practical deployment constraints