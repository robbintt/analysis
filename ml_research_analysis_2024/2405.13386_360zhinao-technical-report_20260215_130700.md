---
ver: rpa2
title: 360Zhinao Technical Report
arxiv_id: '2405.13386'
source_url: https://arxiv.org/abs/2405.13386
tags: []
core_contribution: 360Zhinao presents 7B parameter models with context lengths of
  4K, 32K and 360K. Through systematic data cleaning and composition strategies, the
  team pretrained a 7B base model on 3.4T tokens.
---

# 360Zhinao Technical Report

## Quick Facts
- arXiv ID: 2405.13386
- Source URL: https://arxiv.org/abs/2405.13386
- Authors: 360Zhinao Team
- Reference count: 24
- Key outcome: 7B parameter models with context lengths of 4K, 32K, and 360K achieving competitive performance on benchmarks

## Executive Summary
360Zhinao presents a comprehensive approach to pretraining and aligning large language models with extended context capabilities. The team developed 7B parameter models with 4K, 32K, and 360K context lengths through systematic data cleaning, composition strategies, and innovative alignment techniques. By pretraining on 3.4T tokens of carefully curated data and extending context windows using RoPE-base changes with tailored long data, the models achieved competitive performance across various benchmarks. The work demonstrates that efficient long-context modeling is achievable through architectural modifications and data curation rather than extensive pretraining.

## Method Summary
The 360Zhinao team employed a multi-stage approach: first cleaning and deduplicating 3.4T tokens of web data through document, paragraph, and sentence-level deduplication; then pretraining a 7B transformer model using a balanced mixture of high-quality and diverse web data; extending context lengths to 32K and 360K using RoPE-base changes with tailored long data; and finally aligning the models through supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). The SFT pipeline included carefully curated datasets for general capabilities, long-context understanding, and Chinese language tasks, while the RLHF stage optimized for human preferences through preference data collection and Proximal Policy Optimization training.

## Key Results
- 360K context model achieved near-perfect results on NIAH evaluation
- 32K context model achieved top scores among 10B models on LongBench
- Models demonstrated competitive performance on C-Eval, AGIEval, MMLU, and other standard benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-level deduplication improves data efficiency and model performance.
- Mechanism: Document, paragraph, and sentence deduplication reduce redundancy while maintaining diversity, leading to better model generalization.
- Core assumption: Duplicate data impairs model generalization and training efficiency.
- Evidence anchors:
  - [section] "Document deduplication removed 15.40% of duplicate documents... Paragraph deduplication removes a significant number of high-frequency template-like spans... Sentence deduplication targets the reduction of redundant text fragments found across web pages."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.53, average citations=0.0." Weak corpus evidence for this specific claim.
- Break condition: Over-aggressive deduplication could remove useful data or damage coherence.

### Mechanism 2
- Claim: Balanced data mixture optimizes data efficiency and quantity.
- Mechanism: Mixing high-quality sources (Wikipedia, books) with diverse web data improves model performance by balancing quality and diversity.
- Core assumption: Higher proportions of high-quality data improve data efficiency, while diverse data improves model generalization.
- Evidence anchors:
  - [section] "Mixing these multi-source data for pretraining corpora presents a challenge... We applied meticulous cleaning strategies and multi-level deduplication to web data to compress data scale and enhance its efficiency."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.53, average citations=0.0." Weak corpus evidence for this specific claim.
- Break condition: Excessive focus on either quality or diversity could lead to suboptimal performance.

### Mechanism 3
- Claim: Context window extension with RoPE-base changes and tailored long data enables efficient long-context modeling.
- Mechanism: Simple RoPE-base changes combined with curated long data allows extension to 32K and 360K context lengths without extensive pretraining.
- Core assumption: RoPE-base changes with appropriate data can extend context length efficiently.
- Evidence anchors:
  - [section] "We extended the context length from 4K to 32K and 360K during finetuning with simple RoPE-base changes and constructed long SFT data."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.53, average citations=0.0." Weak corpus evidence for this specific claim.
- Break condition: RoPE-base changes alone may not be sufficient for extremely long contexts or could introduce instability.

## Foundational Learning

- Concept: Data deduplication strategies
  - Why needed here: Reduces redundancy and improves data efficiency for pretraining.
  - Quick check question: What are the three levels of deduplication used in this work and their respective impacts?

- Concept: Context window extension techniques
  - Why needed here: Enables efficient long-context modeling without extensive pretraining.
  - Quick check question: How does the RoPE-base change approach extend context length and what are its limitations?

- Concept: Alignment strategies (SFT, RLHF)
  - Why needed here: Improves model performance on specific tasks and aligns with human preferences.
  - Quick check question: What are the key components of the SFT and RLHF pipelines used in this work?

## Architecture Onboarding

- Component map: 7B parameter model with context lengths of 4K, 32K, and 360K; transformer architecture with Pre-Norm structure, RMSNorm, SwiGLU activation, and Rotary Embedding; FP32 precision for inverse frequency matrix
- Critical path: Data cleaning and composition → Model pretraining → Context window extension → Alignment (SFT, RLHF) → Evaluation and deployment
- Design tradeoffs: Balancing data quantity and quality, diversity and coherence, efficiency and effectiveness
- Failure signatures: Model instability during training, poor performance on benchmarks, context window extension failures
- First 3 experiments:
  1. Evaluate the impact of different deduplication ratios on model performance
  2. Test various data mixture ratios to optimize data efficiency and quantity
  3. Experiment with different RoPE-base changes and long data configurations for context window extension

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of 360Zhinao models compare to other models when evaluated on tasks requiring long-term reasoning or understanding of complex narratives?
- Basis in paper: [inferred] The paper discusses the model's performance on various benchmarks, including those related to reasoning and comprehension, but does not specifically address long-term reasoning or complex narrative understanding tasks.
- Why unresolved: The paper focuses on evaluating the model's performance on a range of standard benchmarks but does not delve into specific tasks that require long-term reasoning or complex narrative understanding.
- What evidence would resolve it: Experiments comparing the model's performance on tasks specifically designed to test long-term reasoning and complex narrative understanding, such as those involving multi-step reasoning, cause-and-effect relationships, or understanding of intricate storylines.

### Open Question 2
- Question: What are the potential biases or limitations in the model's understanding of different cultures or languages, particularly those that are underrepresented in the training data?
- Basis in paper: [inferred] The paper mentions the model's bilingual capabilities and its performance on Chinese and English tasks, but does not explicitly discuss potential biases or limitations related to cultural or linguistic diversity.
- Why unresolved: The paper does not provide a detailed analysis of the model's performance across a diverse range of cultures and languages, nor does it discuss potential biases that may arise from imbalances in the training data.
- What evidence would resolve it: Experiments evaluating the model's performance on tasks involving diverse cultural contexts and languages, along with an analysis of the training data's representation of different cultures and languages.

### Open Question 3
- Question: How does the model's performance scale with increasing model size, and what are the potential trade-offs between model size, computational resources, and performance?
- Basis in paper: [explicit] The paper mentions the 7B parameter size of the 360Zhinao models and their competitive performance, but does not explore the relationship between model size and performance.
- Why unresolved: The paper does not provide insights into how the model's performance might change with different model sizes, nor does it discuss the potential trade-offs between model size, computational resources, and performance.
- What evidence would resolve it: Experiments training and evaluating models of different sizes, along with an analysis of the computational resources required for each model size and the corresponding performance gains or limitations.

## Limitations

- The evaluation results rely heavily on the custom 360Eval benchmark, which is not publicly available for independent verification.
- Limited details on RLHF implementation, including specific reward model architectures and hyperparameters.
- Computational requirements for pretraining the 7B model on 3.4T tokens are not specified, making practical reproduction difficult.

## Confidence

- High Confidence: Pretraining methodology, data cleaning strategies, and SFT implementation details
- Medium Confidence: Context window extension using RoPE-base changes, RLHF implementation details
- Low Confidence: Performance claims on custom 360Eval benchmark, computational efficiency claims

## Next Checks

1. Request access to the 360Eval benchmark or create a comparable evaluation suite to verify the model's performance claims on standard benchmarks like C-Eval, AGIEval, and LongBench.

2. Conduct controlled experiments varying the RoPE-base changes and long data composition to quantify their individual contributions to the 32K and 360K context performance.

3. Calculate the estimated computational requirements for pretraining the 7B model on 3.4T tokens, including GPU hours and energy consumption, and compare with other contemporary models.