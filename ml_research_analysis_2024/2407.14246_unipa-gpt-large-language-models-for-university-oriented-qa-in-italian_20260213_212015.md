---
ver: rpa2
title: 'Unipa-GPT: Large Language Models for university-oriented QA in Italian'
arxiv_id: '2407.14246'
source_url: https://arxiv.org/abs/2407.14246
tags:
- prompt
- table
- question
- unipa-gpt
- course
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents Unipa-GPT, a virtual assistant developed to
  help secondary school students navigate University of Palermo information. The system
  employs a Retrieval-Augmented Generation (RAG) architecture using gpt-3.5-turbo
  and a corpus of university documents.
---

# Unipa-GPT: Large Language Models for university-oriented QA in Italian

## Quick Facts
- arXiv ID: 2407.14246
- Source URL: https://arxiv.org/abs/2407.14246
- Reference count: 9
- Key outcome: RAG-only system with condensed prompt and clear corpus achieved best performance in university-oriented QA for Italian secondary school students

## Executive Summary
This paper presents Unipa-GPT, a virtual assistant developed to help secondary school students navigate University of Palermo information using a Retrieval-Augmented Generation (RAG) architecture with gpt-3.5-turbo. The system was evaluated against multiple LLM variants and demonstrated superior performance with the RAG-only approach using a condensed prompt design. The evaluation included both quantitative metrics (BLEU, ROUGE-L) and qualitative user feedback collected during the SHARPER night event. The study concludes that prompt engineering and corpus selection are critical factors for effective domain-specific chatbots.

## Method Summary
The study developed a RAG architecture using gpt-3.5-turbo and text-embedding-ada-002 via Azure OpenAI API. The system retrieves relevant documents from the unipa-corpus (scraped from University of Palermo website) using FAISS vector database with LangChain, then generates answers using custom and condensed prompts. The corpus was divided into chunks of 1000 tokens and evaluated in three versions: clear, full, and emb. Fine-tuning experiments were conducted on a subset of the corpus, and the system was compared against other LLMs including Llama-2, Llama-3, and Minerva. Evaluation metrics included BLEU and ROUGE-L scores, context relevancy, faithfulness, and answer correctness, along with user feedback collected during SHARPER Night.

## Key Results
- RAG-only system with condensed prompt achieved best performance (BLEU: 0.151-0.334, ROUGE-L: 0.187-0.472)
- Fine-tuned models showed no improvement and sometimes decreased performance
- gpt-3.5-turbo outperformed other LLMs (Llama-2, Llama-3, Minerva) in answer quality
- Condensed prompt design demonstrated effectiveness for conversation flow

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG architecture improves domain-specific QA accuracy over base LLM alone.
- Mechanism: The retriever module fetches relevant documents from unipa-corpus based on semantic similarity, then the generator (gpt-3.5-turbo) uses this context to produce grounded answers instead of relying on its general knowledge.
- Core assumption: The retriever can identify the most relevant document(s) for a given question from the corpus.
- Evidence anchors:
  - [abstract]: "The system employs a Retrieval-Augmented Generation (RAG) architecture using gpt-3.5-turbo and a corpus of university documents."
  - [section]: "The retrieval module consists of a vector database provided by the LangChain library... The generator module consists of an instance of gpt-3.5-turbo... The LLM is queried with a custom prompt in which the behavior of the system is explained, and the question of the user is passed along with the most related documents."
  - [corpus]: "The vector database is filled with the documents in unipa-corpus conveniently divided into chunks of 1000 tokens..."
- Break condition: If the retriever fails to fetch relevant documents (e.g., due to poor embeddings or corpus coverage), the generator will produce inaccurate or hallucinated answers.

### Mechanism 2
- Claim: Condensed prompt design improves conversation flow and answer quality.
- Mechanism: The condensed prompt reformulates the conversation history and current question into a single, focused question before passing it to the base custom prompt, reducing noise and improving context clarity for the LLM.
- Core assumption: Reformulating the conversation into a single question helps the LLM focus on the core user intent without distraction from prior context.
- Evidence anchors:
  - [abstract]: "The RAG-only system with a condensed prompt and clear corpus achieved the best performance, demonstrating the effectiveness of prompt engineering..."
  - [section]: "Both gpt-3.5-turbo and text-embedding-ada-002 were invoked via Azure call to the OpenAI API. Two different Italian prompts were built for gpt-3.5-turbo: a custom prompt and a condensed prompt..."
  - [corpus]: Weak evidence; prompt engineering is described but not corpus-specific.
- Break condition: If the conversation reformulation loses critical context, the LLM may miss important nuances and provide less accurate answers.

### Mechanism 3
- Claim: Fine-tuning on domain corpus does not improve performance and may introduce hallucinations.
- Mechanism: Fine-tuning on unipa-corpus was intended to inject domain knowledge into the LLM, but the experiments showed that this led to longer, repetitive, and sometimes contradictory answers compared to the RAG-only approach.
- Core assumption: Fine-tuning on a limited domain corpus without retrieval access can cause the model to over-rely on its fine-tuned parameters, leading to repetitive or hallucinated content.
- Evidence anchors:
  - [abstract]: "Both RAG-only and fine-tuned versions were evaluated. The RAG-only system with a condensed prompt and clear corpus achieved the best performance..."
  - [section]: "No improvements in performances were found in the fine-tuned models, revealing that this strategy is not effective for the target task, and a performance decrease is observed in the condensed prompt configurations."
  - [corpus]: "The unipa-corpus was modified to be in the form required for fine-tuning... Questions and answer were automatically generated with different generation rules for each section of the corpus."
- Break condition: If fine-tuning is combined with RAG (as in the hybrid approach), the negative effects may be mitigated, but the pure fine-tuned model still underperforms.

## Foundational Learning

- Concept: Vector embeddings and semantic similarity
  - Why needed here: The retriever uses text-embedding-ada-002 to convert documents and queries into vectors, then finds the closest documents via FAISS. Understanding this is crucial for debugging retrieval issues.
  - Quick check question: If a query is about "scholarships" but retrieves documents about "tuition fees," what might be wrong with the embeddings or chunking strategy?

- Concept: Prompt engineering and conversation flow
  - Why needed here: The system uses two prompts (custom and condensed) to control LLM behavior. Knowing how to craft and refine prompts is essential for optimizing answer quality.
  - Quick check question: How does the condensed prompt differ from the custom prompt, and why might it lead to better answers in a multi-turn conversation?

- Concept: Evaluation metrics for text generation (BLEU, ROUGE-L, RAGAS)
  - Why needed here: The study uses BLEU and ROUGE-L to compare answer quality against golden labels, and RAGAS to assess context relevancy, faithfulness, and answer correctness. Understanding these metrics is key to interpreting results.
  - Quick check question: If a generated answer has high ROUGE-L but low faithfulness, what does that imply about the answer's relationship to the retrieved context?

## Architecture Onboarding

- Component map: User question -> Retriever (LangChain + FAISS + text-embedding-ada-002) -> Generator (gpt-3.5-turbo + prompt) -> Answer displayed + feedback collected
- Critical path: User question → Retriever fetches context → Generator produces answer → Answer displayed + feedback collected
- Design tradeoffs:
  - Chunk size (1000 tokens) vs. context relevance: larger chunks may include more relevant info but risk diluting the signal.
  - Condensed vs. custom prompt: condensed reduces noise but may lose nuance.
  - Fine-tuning vs. RAG-only: fine-tuning saves tokens but can cause hallucinations; RAG-only is more reliable but requires retrieval overhead.
- Failure signatures:
  - No relevant documents retrieved → Answer is generic or hallucinated.
  - Condensed prompt too aggressive → Answer misses important context.
  - Fine-tuned model loops or repeats → Overfitting to corpus without retrieval.
- First 3 experiments:
  1. Test retriever accuracy: Query a known question and inspect the top-3 retrieved documents for relevance.
  2. Compare prompt variants: Run the same question through custom and condensed prompts and evaluate answer quality.
  3. Evaluate corpus versions: Test the same question with clear, full, and emb corpus versions to see which yields the best answers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would a hybrid RAG approach combining vector stores with knowledge graphs or OWL ontologies perform compared to the current Unipa-GPT system?
- Basis in paper: [explicit] "Future work will be devoted to build an open framework where all kinds of models can be integrated and tested. Finally, it is well known that integrating structured knowledge in RAG can mitigate hallucination (Casheekar et al. 2024). Following this assumption, we are developing a hybrid RAG for the Unipa-GPT framework, that relies on both a vector store, and either a knowledge graph obtained by document indexing or a formal OWL ontology if available to describe the domain under investigation."
- Why unresolved: The paper only mentions plans for future work on this hybrid approach but has not yet implemented or tested it.
- What evidence would resolve it: A controlled experiment comparing the current RAG-only system's performance (in terms of answer accuracy, faithfulness, and context relevancy) against a hybrid RAG system using knowledge graphs or OWL ontologies on the same test set of questions.

### Open Question 2
- Question: How would Unipa-GPT perform if fine-tuned on a larger dataset with more diverse question-answer pairs, including edge cases and ambiguous queries?
- Basis in paper: [explicit] "No improvements in performances were found in the fine-tuned models, revealing that this strategy is not effective for the target task, and a performance decrease is observed in the condensed prompt configurations. Generally speaking, answers of the fine-tuned models are very long and they tend to be repetitive (Holtzman et al. 2020) and to generate loops and hallucinations (Ji et al. 2023)."
- Why unresolved: The paper only explored a limited fine-tuning approach with a condensed corpus. It did not investigate whether a more comprehensive fine-tuning approach could improve performance.
- What evidence would resolve it: An experiment where Unipa-GPT is fine-tuned on a much larger and more diverse dataset, including edge cases and ambiguous queries, and then compared to the current RAG-only system on the same test set.

### Open Question 3
- Question: How would the performance of Unipa-GPT change if the prompt engineering strategy was further optimized, such as using chain-of-thought prompting or persona-based prompts?
- Basis in paper: [explicit] "Both gpt-3.5-turbo and text-embedding-ada-002 were invoked via Azure call to the OpenAI API. Two different Italian prompts were built for gpt-3.5-turbo: a custom prompt and a condensed prompt... We are convinced that this approach does not introduce any relevant bias because the fine-tuned model is inserted in a RAG architecture that is queried using an instruction prompt strategy."
- Why unresolved: The paper only explored two basic prompt engineering strategies. It did not investigate more advanced techniques like chain-of-thought prompting or persona-based prompts.
- What evidence would resolve it: An experiment where different prompt engineering strategies (e.g., chain-of-thought, persona-based) are tested on Unipa-GPT, and their performance is compared to the current system using the same test set.

## Limitations

- Evaluation framework relies on BLEU and ROUGE-L scores against limited golden dataset, which may not fully capture semantic quality in educational context
- No statistical significance testing between model variants, making it difficult to assess robustness of performance differences
- Fine-tuning procedure's impact on model behavior remains incompletely characterized - underlying mechanisms (overfitting vs. catastrophic forgetting) not explored

## Confidence

- High confidence: The RAG architecture's effectiveness in grounding answers using retrieved context, supported by multiple experiments showing consistent superiority over base LLM and fine-tuned models.
- Medium confidence: The superiority of condensed prompt design, based on limited comparisons and without ablation studies to isolate prompt effects from other variables.
- Medium confidence: The relative performance rankings of different LLMs (gpt-3.5-turbo > Llama-2/3/Minerva), though comparison methodology and dataset consistency require verification.

## Next Checks

1. Perform paired t-tests or bootstrap confidence intervals on BLEU/ROUGE-L scores across all model variants to confirm that observed performance differences are statistically significant rather than due to sampling variation.

2. Systematically compare custom vs. condensed prompts across multiple question types and conversation lengths to isolate the specific conversational scenarios where each prompt variant excels or fails.

3. Manually categorize the top-5 most frequent retrieval errors (e.g., missing relevant documents, retrieving irrelevant documents) to identify systematic weaknesses in the embedding strategy or corpus coverage that could be addressed through corpus expansion or embedding parameter tuning.