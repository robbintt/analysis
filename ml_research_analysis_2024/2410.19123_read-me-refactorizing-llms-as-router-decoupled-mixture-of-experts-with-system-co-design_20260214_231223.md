---
ver: rpa2
title: 'Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System
  Co-Design'
arxiv_id: '2410.19123'
source_url: https://arxiv.org/abs/2410.19123
tags:
- router
- expert
- experts
- inference
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Read-ME, a framework that refactors pre-trained
  dense language models into smaller Mixture-of-Experts (MoE) models by leveraging
  activation sparsity. The approach decouples the router from the MoE backbone, enabling
  pre-gating of tokens before inference.
---

# Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design

## Quick Facts
- arXiv ID: 2410.19123
- Source URL: https://arxiv.org/abs/2410.19123
- Reference count: 40
- Primary result: 10.1% improvement on MMLU with 6.1% latency reduction

## Executive Summary
This paper presents Read-ME, a framework that refactors pre-trained dense language models into smaller Mixture-of-Experts (MoE) models by leveraging activation sparsity. The key innovation is decoupling the router from the MoE backbone, enabling pre-gating of tokens before inference. This design allows for system-friendly optimizations including lookahead scheduling, expert-aware batching, and an optimal caching strategy inspired by Belady's algorithm. The approach achieves up to 10.1% improvement on MMLU benchmarks compared to similar-scale models while reducing mean end-to-end latency by 6.1%. Notably, Read-ME requires only 1 billion training tokens, a small fraction of the 2 trillion tokens used for pre-training the original Llama2 model.

## Method Summary
Read-ME refactors a pre-trained dense language model into an MoE architecture by using activation sparsity to extract domain-specific experts through structured pruning. The framework employs a pre-gating router that computes routing decisions for all layers before inference, eliminating per-layer routing decisions. This enables system-level optimizations like lookahead scheduling, expert-aware batching, and Belady-inspired caching. The model is trained using router distillation with a small fraction (1 billion tokens) of the original training data, while maintaining or improving performance on downstream tasks.

## Key Results
- Achieves 10.1% improvement on MMLU benchmarks compared to similar-scale models
- Reduces mean end-to-end latency by 6.1% through system optimizations
- Requires only 1 billion training tokens versus 2 trillion for original dense model
- Demonstrates effectiveness across multiple downstream tasks including WinoGrande and ARC-Easy

## Why This Works (Mechanism)

### Mechanism 1
The pre-gating router eliminates per-layer routing decisions, enabling lookahead scheduling and reducing system-level inefficiencies. By computing routing decisions before inference, the system can prefetch and batch tokens based on future expert assignments. The core assumption is that expert selections are highly correlated across layers, making single-layer routing redundant. Evidence shows that the row-wise sparse pattern implies expert selection is almost deterministic given previous layer choices.

### Mechanism 2
Expert-aware batching reduces the number of unique experts per batch, improving inference latency. By pre-gating tokens and creating batches based on expert requirements, the system minimizes unique experts activated per layer. The core assumption is that tokens within the same request have high temporal locality in expert selection. This batching approach is not feasible in other MoE architectures where expert selection remains unknown until requests reach each router.

### Mechanism 3
Belady-inspired caching with pre-gating achieves optimal cache replacement for expert parameters. By knowing future expert requests across all pending requests, the system can evict the expert accessed farthest in the future, similar to Belady's optimal algorithm. The core assumption is that future expert requests can be accurately predicted through pre-gating. This enables near-optimal cache replacement by decoupling the router from the backbone MoE.

## Foundational Learning

- **Mixture-of-Experts (MoE) architecture**
  - Why needed here: Read-ME is fundamentally an MoE system that refactors dense models into sparse expert networks
  - Quick check question: What distinguishes MoE from traditional dense models in terms of parameter activation?

- **Activation sparsity and structured pruning**
  - Why needed here: Read-ME uses activation sparsity to extract domain-specific experts from pre-trained dense models
  - Quick check question: How does structured pruning differ from unstructured pruning in terms of maintaining model functionality?

- **Temporal locality in sequence processing**
  - Why needed here: The paper leverages high temporal locality in expert selection to enable efficient batching and caching
  - Quick check question: Why would tokens in a sequence tend to select the same expert repeatedly?

## Architecture Onboarding

- **Component map**: Pre-gating router -> Expert networks -> System optimizations (Lookahead scheduling, Expert-aware batching, Belady-inspired caching) -> Inference pipeline

- **Critical path**: Request arrives and is queued → Pre-gating router computes expert assignments for all tokens → System schedules expert loading based on future assignments → Expert-aware batching groups tokens by expert requirements → MoE layers execute with minimal expert loading overhead

- **Design tradeoffs**: Router complexity vs. system efficiency (single pre-gating router vs. per-layer routers), Training cost vs. performance (1B tokens vs. 2T tokens for original dense model), Memory vs. latency (caching vs. on-demand loading of expert parameters)

- **Failure signatures**: Low cache hit ratio despite Belady algorithm (poor pre-gating predictions), Linear increase in latency with batch size (expert-aware batching not working), Router overhead dominates computation (auto-regressive router too complex)

- **First 3 experiments**: 1) Validate pre-gating accuracy by comparing predicted vs. actual expert assignments, 2) Measure temporal locality by analyzing expert selection patterns within token sequences, 3) Benchmark cache performance under varying cache sizes and request patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of pre-gating router efficiency compared to layer-wise routers, and under what conditions does this advantage diminish?
- Basis in paper: The paper shows pre-gating routers significantly improve latency and caching performance, but doesn't analyze the theoretical efficiency limits or conditions where benefits might reduce
- Why unresolved: The paper focuses on empirical improvements without providing a theoretical framework for understanding the fundamental limits of pre-gating efficiency or identifying scenarios where its advantages might diminish
- What evidence would resolve it: A formal analysis comparing the computational complexity and information-theoretic limits of pre-gating vs layer-wise routers, including conditions under which pre-gating might become less effective

### Open Question 2
- Question: How does Read-ME's performance scale with increasing model size and complexity, particularly for trillion-parameter models?
- Basis in paper: The paper demonstrates effectiveness on 7B parameter models but doesn't explore how the approach scales to much larger models where system-level challenges become more severe
- Why unresolved: The experimental results focus on relatively small models (7B parameters), leaving uncertainty about how well the pre-gating approach and system optimizations would work at trillion-parameter scales
- What evidence would resolve it: Empirical results showing Read-ME's performance on models ranging from 10B to trillion parameters, with analysis of how system bottlenecks scale with model size

### Open Question 3
- Question: What is the optimal trade-off between expert specialization and generalization in Read-ME, and how does this affect downstream task performance?
- Basis in paper: The paper shows domain-aware expert construction improves performance, but doesn't explore the full spectrum of specialization vs generalization or its impact on different task types
- Why unresolved: While the paper demonstrates domain-specific experts work well, it doesn't investigate whether there's an optimal balance between specialized and generalist experts, or how this trade-off affects performance across different types of tasks
- What evidence would resolve it: Systematic experiments varying the degree of expert specialization while measuring performance across diverse downstream tasks, identifying optimal specialization levels for different task categories

## Limitations

- The correlation assumption that expert selections are highly correlated across layers relies on empirical observations rather than theoretical guarantees
- Hardware-dependent optimizations may show varying performance across different GPU architectures and memory configurations
- Training data efficiency claims comparing 1B vs 2T tokens require more rigorous analysis of transfer learning contributions

## Confidence

**High Confidence Claims**:
- The technical feasibility of extracting experts from pre-trained dense models using structured pruning techniques is well-established
- The algorithmic framework for decoupling routers from MoE backbones is sound and implementable
- The general principle that reducing per-layer routing decisions can improve system efficiency has strong theoretical and empirical support

**Medium Confidence Claims**:
- The specific 10.1% MMLU improvement is credible given the experimental methodology, but may be sensitive to implementation details
- The latency improvements are plausible based on the proposed optimizations, but the exact magnitude likely depends on specific hardware configurations
- The effectiveness of Belady-inspired caching for expert parameters is theoretically sound but may require careful tuning in practice

**Low Confidence Claims**:
- The universal applicability of the high correlation assumption across different models and tasks remains unproven
- The generalization of temporal locality benefits to diverse workloads and sequence types is not established
- The training efficiency claims comparing 1B vs 2T tokens require more rigorous analysis of transfer learning contributions

## Next Checks

1. Conduct correlation analysis across diverse model architectures and tasks to verify the assumption that expert selections are highly correlated across layers, testing models with different scales, architectures, and training objectives.

2. Systematically evaluate the temporal locality assumption by analyzing expert selection patterns across different sequence lengths, languages, and task types, measuring how batching efficiency degrades as temporal locality decreases.

3. Implement the full Read-ME pipeline on multiple hardware platforms (different GPU architectures, memory configurations) and measure the consistency of the reported latency improvements, documenting specific implementation optimizations and batch sizes.