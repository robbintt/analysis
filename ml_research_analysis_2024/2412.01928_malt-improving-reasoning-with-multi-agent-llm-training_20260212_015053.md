---
ver: rpa2
title: 'MALT: Improving Reasoning with Multi-Agent LLM Training'
arxiv_id: '2412.01928'
source_url: https://arxiv.org/abs/2412.01928
tags:
- malt
- reasoning
- multi-agent
- training
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MALT (Multi-Agent LLM Training), a post-training
  strategy that improves reasoning in LLMs by dividing the reasoning process into
  generation, verification, and refinement steps using a sequential pipeline of heterogeneous
  agents. The method generates synthetic data via a multi-agent search tree with exponential
  branching, applies value iteration to propagate reward signals back to each role-conditioned
  model, and produces role-specific training data without human or teacher-model supervision.
---

# MALT: Improving Reasoning with Multi-Agent LLM Training

## Quick Facts
- arXiv ID: 2412.01928
- Source URL: https://arxiv.org/abs/2412.01928
- Reference count: 40
- Multi-agent LLM training improves reasoning performance by 7-16% on standard benchmarks

## Executive Summary
MALT (Multi-Agent LLM Training) is a post-training method that enhances LLM reasoning capabilities by decomposing the reasoning process into specialized generation, verification, and refinement agents. The approach uses a sequential pipeline of heterogeneous agents that collaborate through a multi-agent search tree with exponential branching. Synthetic data is generated without human supervision, and value iteration propagates reward signals back to each role-conditioned model. MALT achieves significant improvements over single-model baselines on MATH, GSM8K, and CSQA benchmarks, while demonstrating potential for generalization to more challenging reasoning tasks.

## Method Summary
MALT implements a sequential pipeline where multiple heterogeneous agents handle different aspects of the reasoning process. The system generates synthetic reasoning trajectories through a multi-agent search tree with exponential branching, where each agent plays a specific role in the reasoning pipeline. Value iteration is applied to propagate reward signals from final answers back through the reasoning steps, training each role-conditioned model independently. The entire process operates without human or teacher-model supervision, relying entirely on synthetic data generation and self-supervised learning signals.

## Key Results
- Achieves 15.66% relative improvement on MATH benchmark over single-model baseline
- Demonstrates 7.42% relative improvement on GSM8K benchmark
- Shows 9.40% relative improvement on CSQA benchmark
- Claims generalization to more challenging reasoning tasks beyond evaluated benchmarks

## Why This Works (Mechanism)
The method works by decomposing complex reasoning into specialized sub-tasks handled by different agents, allowing each component to focus on specific aspects of the reasoning process. The multi-agent search tree with exponential branching explores diverse reasoning paths, while value iteration ensures that reward signals properly credit or penalize each agent's contribution. This specialization and collaborative training approach allows the system to capture more nuanced reasoning patterns than monolithic models, with each agent learning to optimize its specific role in the overall reasoning pipeline.

## Foundational Learning

**Multi-agent reinforcement learning**: Why needed - enables coordination between specialized reasoning agents; Quick check - verify reward propagation correctly attributes credit to each agent's contribution

**Value iteration**: Why needed - propagates reward signals back through reasoning trajectories; Quick check - confirm convergence properties when applied to synthetic data

**Role-conditioned model training**: Why needed - allows each agent to specialize in specific reasoning subtasks; Quick check - measure performance degradation when agents share identical conditioning

## Architecture Onboarding

**Component map**: Generation agent -> Verification agent -> Refinement agent -> Reward evaluation -> Value iteration back to all agents

**Critical path**: Synthetic data generation → Multi-agent search tree exploration → Reward assignment → Value iteration → Role-specific training → Final evaluation

**Design tradeoffs**: The exponential branching in the search tree provides exploration benefits but increases computational cost; role specialization improves performance but requires careful coordination; synthetic data generation eliminates human supervision needs but may introduce distribution shift.

**Failure signatures**: Poor coordination between agents leading to contradictory reasoning steps; value iteration failing to properly credit agents in complex reasoning chains; synthetic data distribution mismatch causing generalization issues.

**First experiments**: 1) Run single-agent baseline on each benchmark to establish reference performance, 2) Test each agent individually on its specific reasoning subtask, 3) Evaluate agent coordination by running the full pipeline on simplified reasoning problems.

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation limited to three reasoning benchmarks (MATH, GSM8K, CSQA), restricting generalizability claims
- Computational overhead from maintaining multiple agents and exponential search tree branching not characterized
- Generalization to more challenging reasoning tasks asserted but not quantitatively demonstrated beyond current benchmarks

## Confidence

- High confidence in methodology description and benchmark selection
- Medium confidence in reported performance improvements given limited benchmark scope
- Low confidence in generalization claims without broader task evaluation

## Next Checks

1. Test MALT on additional reasoning domains (logical reasoning, commonsense QA, code generation) to verify cross-domain generalization beyond current three benchmarks

2. Conduct ablation studies varying number of agents, search depth, and reward propagation methods to quantify impact of each architectural component

3. Measure wall-clock training time and memory usage relative to single-model baselines to characterize computational trade-offs of multi-agent approach