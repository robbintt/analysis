---
ver: rpa2
title: How Can I Improve? Using GPT to Highlight the Desired and Undesired Parts of
  Open-ended Responses
arxiv_id: '2405.00291'
source_url: https://arxiv.org/abs/2405.00291
tags:
- praise
- feedback
- tutor
- training
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study explored using large language models (LLMs), specifically
  GPT models, to automatically identify desired and undesired components of praise
  in tutor responses, aiming to provide explanatory feedback in tutor training programs.
  Two approaches were employed: prompting and fine-tuning GPT models.'
---

# How Can I Improve? Using GPT to Highlight the Desired and Undesired Parts of Open-ended Responses

## Quick Facts
- arXiv ID: 2405.00291
- Source URL: https://arxiv.org/abs/2405.00291
- Reference count: 0
- GPT models can identify desired and undesired praise components in tutor responses with M-IoU scores of 0.46-0.76

## Executive Summary
This study explores using large language models, specifically GPT-3.5, to automatically identify desired and undesired components of praise in tutor responses. The researchers developed a novel Modified Intersection over Union (M-IoU) metric to evaluate the quality of highlighted praise components and compared two approaches: prompting and fine-tuning. The system aims to provide explanatory feedback in tutor training programs by highlighting what aspects of praise are effective versus ineffective.

## Method Summary
The researchers employed two main approaches: prompting GPT-3.5 to identify praise components and fine-tuning GPT-3.5 on a dataset of tutor responses. They developed a novel M-IoU score to evaluate the quality of highlighted components, which showed strong correlation with human judgment. The dataset consisted of tutor responses that were manually labeled to distinguish between effort-based and outcome-based praise. Both zero-shot prompting and fine-tuning experiments were conducted, with the fine-tuning using varying training sample sizes from 10% to 100% of the available data.

## Key Results
- Prompting GPT-3.5 achieved M-IoU scores of 0.46 for effort-based praise and 0.68 for outcome-based praise
- Fine-tuning with only 13 samples (10% of dataset) achieved similar performance to prompting
- Increasing fine-tuning samples to 65 (50% of dataset) improved M-IoU to 0.60 for effort-based and 0.76 for outcome-based praise
- M-IoU scores showed strong correlation with human judgment of praise quality

## Why This Works (Mechanism)
The approach works by leveraging GPT-3.5's ability to understand context and identify praise components through both prompting and fine-tuning. The M-IoU metric effectively captures the overlap between model-identified praise components and human-labeled ground truth, accounting for both precision and recall in component identification. Fine-tuning allows the model to learn specific patterns in effective versus ineffective praise from limited training data.

## Foundational Learning
1. Modified Intersection over Union (M-IoU) - A custom metric combining precision and recall for component identification; needed for evaluating component-level overlap beyond standard metrics
2. Fine-tuning with limited data - Demonstrates that small datasets (as few as 13 samples) can effectively adapt GPT models; check by testing on progressively smaller datasets
3. Prompt engineering for educational feedback - Shows how careful prompt design can extract specific feedback components; verify by testing different prompt variations

## Architecture Onboarding

Critical Path: Input Text -> GPT-3.5 Processing -> M-IoU Evaluation -> Feedback Output
Design Tradeoffs: Balancing between zero-shot prompting (less data, less precise) and fine-tuning (more data, more precise)

Component Map:
Input Data -> Preprocessing -> GPT-3.5 (Prompting or Fine-tuning) -> Component Extraction -> M-IoU Scoring -> Output Feedback

Failure Signatures:
- Poor component identification when praise is implicit rather than explicit
- Confusion between effort-based and outcome-based praise categories
- Performance degradation with highly complex or nuanced tutor responses

First Experiments:
1. Test prompting approach on a held-out validation set with varying prompt complexity
2. Run fine-tuning experiments with incremental training data sizes (10%, 25%, 50%, 100%)
3. Compare M-IoU performance against traditional evaluation metrics on the same dataset

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Dataset size remains relatively small for complex NLP tasks despite fine-tuning experiments
- M-IoU metric, while correlated with human judgment, requires further validation against established frameworks
- Focus on tutor praise may limit generalizability to other feedback types or educational contexts
- Performance differences between effort-based and outcome-based praise suggest model struggles with certain feedback components

## Confidence
High confidence: Correlation between M-IoU scores and human judgment, effectiveness of both prompting and fine-tuning approaches
Medium confidence: Specific M-IoU scores and practical significance in real-world scenarios
Low confidence: Generalizability to other types of educational feedback beyond tutor praise

## Next Checks
1. Test approach on larger, more diverse dataset across multiple educational domains to verify scalability
2. Conduct head-to-head comparison between M-IoU and established NLP metrics (F1, BLEU, ROUGE)
3. Implement system in real-world tutor training program and measure long-term impact on trainee performance