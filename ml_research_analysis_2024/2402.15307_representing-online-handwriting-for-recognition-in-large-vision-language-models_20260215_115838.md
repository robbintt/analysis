---
ver: rpa2
title: Representing Online Handwriting for Recognition in Large Vision-Language Models
arxiv_id: '2402.15307'
source_url: https://arxiv.org/abs/2402.15307
tags:
- recognition
- handwriting
- image
- representation
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel tokenized representation of digital
  ink (online handwriting) that includes both a time-ordered sequence of strokes as
  text and as image, enabling high-quality handwriting recognition with vision-language
  models (VLMs). The method achieves results comparable to or better than state-of-the-art
  online handwriting recognizers across multiple public datasets and two VLM families.
---

# Representing Online Handwriting for Recognition in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2402.15307
- Source URL: https://arxiv.org/abs/2402.15307
- Reference count: 32
- Primary result: Dual text-image representation enables VLMs to match or exceed state-of-the-art online handwriting recognition across multiple datasets

## Executive Summary
This paper proposes a novel tokenized representation of digital ink that combines both a time-ordered sequence of strokes as text and as image, enabling high-quality handwriting recognition with vision-language models (VLMs). The method achieves results comparable to or better than state-of-the-art online handwriting recognizers across multiple public datasets and two VLM families. By leveraging the strengths of both modalities, the approach compensates for individual limitations while maintaining computational efficiency.

## Method Summary
The method converts digital ink into two representations: a text sequence using relative coordinate discretization and a rendered image with time/distance encoding in color channels. Three public datasets are used: DeepWriting (English text), MathWriting (LaTeX expressions), and VNOnDB (Vietnamese text). The dual representation is then used to fine-tune two VLM families (PaLI and PaLM-E) on a mixed dataset (80% MathWriting, 10% DeepWriting, 10% VNOnDB) for 200k steps. The approach is compared against CTC transformer baselines and state-of-the-art methods, demonstrating superior or comparable performance.

## Key Results
- VLMs with dual representation achieve CER comparable to or better than state-of-the-art online handwriting recognizers
- Combining ink text sequence and rendered image improves CER over either modality alone (PaLI: 4.64 vs 10.65)
- Relative coordinates perform better on DeepWriting and VNOnDB, while absolute coordinates are better for MathWriting
- Time+distance rendering in image color channels outperforms black&white or time-only rendering

## Why This Works (Mechanism)

### Mechanism 1
VLMs can match or exceed state-of-the-art online handwriting recognition when given both ink sequence and rendered image inputs. Dual representation compensates for limitations of each modality; text sequence captures stroke order and fine detail while image provides spatial layout and writing direction cues. VLM architecture can fuse multimodal embeddings effectively without architectural changes. Evidence shows combining ink and image input improves CER over either modality alone. Break condition: If ink sequence exceeds model context length and no image is provided, performance degrades significantly.

### Mechanism 2
Relative coordinate discretization with uniform grid encoding yields better performance than absolute coordinates or learned codebooks. Relative offsets are shift-invariant, easier for the model to learn, and reduce sequence length; uniform grid discretization simplifies mapping to text tokens. The dataset structure (e.g., linear text vs non-linear math) determines whether relative or absolute coordinates are preferable. Evidence shows relative coordinates perform better on DeepWriting and VNOnDB, while absolute coordinates are better for MathWriting. Break condition: If the vocabulary for histogram-based tokenizer becomes too large (e.g., 12k tokens), text representation becomes inefficient and performance drops.

### Mechanism 3
Encoding time and distance information in image color channels improves recognition accuracy. Color channels provide explicit cues about stroke order and speed, helping resolve ambiguous writing and improving alignment with text output. Vision encoder can interpret color-coded temporal information as useful features. Evidence shows time+distance rendering outperforms black&white or time-only rendering. Break condition: If rendering aspect ratio is not matched to dataset (e.g., using one line for skewed samples), recognition quality degrades.

## Foundational Learning

- **Byte-Pair Encoding (BPE) tokenization**: VLMs rely on BPE for text processing; understanding its impact on sequence length and vocabulary size is crucial. Quick check: What happens to sequence length when numbers are tokenized digit-by-digit vs as multi-digit tokens?
- **Connectionist Temporal Classification (CTC) loss**: Baseline models use CTC for sequence alignment; comparing CTC vs seq2seq decoder behavior informs design choices. Quick check: How does CTC handle variable-length input-output pairs without explicit alignment?
- **Multimodal fusion in transformers**: VLMs combine vision and language embeddings; knowing how cross-attention or encoder-decoder fusion works is key to interpreting results. Quick check: In an encoder-decoder VLM, where are image embeddings typically integrated?

## Architecture Onboarding

- **Component map**: Vision encoder (ViT-B/16) → image tokens; Text encoder/decoder (mT5-base or PaLM) → text tokens; Dual input pipeline: text sequence (ink strokes) + image (rendered ink); Output head: greedy decoding or CTC loss (for baselines)
- **Critical path**: 1) Preprocess ink: resample, normalize, discretize to text tokens; 2) Render ink to image with time/distance encoding; 3) Tokenize both modalities for VLM input; 4) Fine-tune VLM on mixed dataset (80% MathWriting, 10% VNOnDB, 10% DeepWriting); 5) Decode output and compute CER
- **Design tradeoffs**: Text-only vs image-only: text captures detail but may exceed context; image provides layout but loses stroke granularity; Relative vs absolute coordinates: relative is shift-invariant but may lose absolute positioning needed for non-linear structures; Number of rendering lines: matches dataset aspect ratio but increases preprocessing complexity
- **Failure signatures**: High CER on inks with long sequences and no image input; Degraded performance when vocabulary size for histogram tokenizer is too large; Confusion between similar characters (e.g., "K" vs "R") without time information in image
- **First 3 experiments**: 1) Train PaLI with only text representation on MathWriting; measure CER vs dual representation; 2) Compare relative vs absolute coordinate tokenization on DeepWriting and VNOnDB; 3) Vary number of rendering lines (1, 2, 4) on MathWriting and observe CER impact

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed representation generalize to other writing systems beyond English, Vietnamese, and LaTeX mathematical expressions? The paper primarily focuses on English, Vietnamese, and LaTeX datasets, with limited exploration into other languages or scripts that might have different structural or compositional characteristics. Testing the model on diverse writing systems such as Chinese, Arabic, or Devanagari scripts, and comparing performance metrics to baseline models specific to those scripts, would resolve this question.

### Open Question 2
What is the impact of different time sampling rates on the recognition accuracy for various handwriting styles and speeds? The paper uses a fixed time delta for all datasets, without considering the variability in handwriting speed and style, which could affect the model's ability to capture nuanced details. Conducting experiments with varying time sampling rates across different handwriting styles and speeds, and analyzing the impact on recognition accuracy, would resolve this question.

### Open Question 3
How does the model perform when extended to handle multimodal inputs such as combining handwriting with spoken language or other visual cues? The paper explores the combination of text and image representations for handwriting recognition, but does not investigate the integration of additional modalities like audio or other visual elements. Developing and testing a model that integrates handwriting with spoken language or other visual cues, and evaluating its performance compared to unimodal or bimodal approaches, would resolve this question.

### Open Question 4
What are the limitations of the proposed method when applied to handwriting recognition tasks in noisy or low-quality input conditions? The experiments are conducted on public datasets that likely have high-quality inputs, without considering the variability and challenges of real-world data. Testing the model on datasets with noisy or low-quality handwriting samples, and analyzing its performance degradation compared to high-quality inputs, would resolve this question.

### Open Question 5
How does the model's performance scale with increasing context lengths, and what are the computational trade-offs? The paper mentions the use of specific context lengths for PaLI and PaLM-E models, but does not explore the impact of varying context lengths on performance or computational efficiency. Conducting experiments with varying context lengths, measuring both performance and computational costs, to determine the optimal balance for different tasks, would resolve this question.

## Limitations

- The dual representation approach relies on VLMs effectively fusing multimodal inputs without architectural modifications, but the paper doesn't thoroughly investigate how different fusion strategies impact recognition accuracy
- The optimal choice of coordinate system (relative vs absolute) appears dataset-dependent, but the analysis lacks a systematic exploration of when each representation fails
- The discretization strategy using histogram-based tokenization can lead to vocabulary sizes exceeding 12k tokens, potentially causing inefficiency and reduced performance, though the paper doesn't provide detailed analysis of this tradeoff

## Confidence

- **High Confidence**: The core claim that dual representation (text + image) improves VLM-based handwriting recognition is well-supported by ablation studies showing consistent CER reduction when both modalities are combined
- **Medium Confidence**: The claim about relative coordinates being generally preferable requires dataset-specific caveats, though the paper identifies this dependency it doesn't provide a clear decision framework for coordinate selection
- **Low Confidence**: The time+distance encoding in image color channels is presented as beneficial, but the ablation study only compares three rendering strategies without exploring the full design space

## Next Checks

1. **Context Length Stress Test**: Evaluate PaLI performance on inks with sequence lengths approaching or exceeding 1024 tokens when only text representation is available to validate the claim that dual representation compensates for context limitations

2. **Coordinate System Crossover Analysis**: Systematically vary dataset structure to identify the precise characteristics that determine whether relative or absolute coordinates are preferable, providing guidelines for representation selection

3. **Vocabulary Size Impact Study**: Measure recognition accuracy while varying the histogram tokenizer's discretization bin count to generate vocabularies of different sizes, quantifying the tradeoff between representation granularity and efficiency