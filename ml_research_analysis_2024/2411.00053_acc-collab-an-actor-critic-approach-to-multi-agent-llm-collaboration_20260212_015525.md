---
ver: rpa2
title: 'ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration'
arxiv_id: '2411.00053'
source_url: https://arxiv.org/abs/2411.00053
tags:
- arxiv
- actor
- critic
- answer
- deliberation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ACC-Collab, an actor-critic framework for
  training LLM agents to collaborate on problem-solving tasks through iterative dialogue.
  The method generates high-quality training data using "Guided-Collaborative Trajectories"
  that steer agent responses toward and away from the correct answer, then uses preference
  optimization to train both an actor (answer provider) and critic (feedback provider)
  to maximize final answer accuracy.
---

# ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration

## Quick Facts
- arXiv ID: 2411.00053
- Source URL: https://arxiv.org/abs/2411.00053
- Reference count: 38
- Primary result: ACC-Collab achieves 8-13% average accuracy improvements over state-of-the-art multi-agent baselines across five benchmarks

## Executive Summary
ACC-Collab introduces an actor-critic framework for training LLM agents to collaborate effectively through iterative dialogue. The method generates high-quality training data using "Guided-Collaborative Trajectories" that steer agent responses toward and away from correct answers, then uses preference optimization to train both an actor (answer provider) and critic (feedback provider) to maximize final answer accuracy. ACC-Collab outperforms state-of-the-art multi-agent baselines across five benchmarks, demonstrating that collaborative agents can be effectively trained through this alternating optimization approach.

## Method Summary
ACC-Collab trains two agents - an actor that generates answers and a critic that provides feedback - through an alternating optimization scheme. The method generates training data by creating "Guided-Collaborative Trajectories" where agents' responses are steered toward or away from correct answers, then measures how these responses affect final accuracy. Preference optimization using Direct Preference Optimization (DPO) trains both agents to maximize the probability of converging to correct answers. The framework alternates between training the critic to provide useful feedback and training the actor to respond optimally to that feedback, repeating until convergence.

## Key Results
- ACC-Collab achieves 8-13% average accuracy improvements over SoM, DebateTune, and DebateGPT baselines
- The trained critic agents learn to provide substantive, detailed feedback rather than being overly agreeable
- ACC-Collab maintains consistent performance improvements across all five tested benchmarks (BoolQ, MMLU, BBH, SCIQ, ARC)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Guided-Collaborative Trajectories steer agent responses toward and away from correct answers to create high-quality training data
- Mechanism: The framework generates off-policy responses by prompting agents with correct or incorrect target answers, then measures how these responses affect final accuracy
- Core assumption: The reward function r(z(t), x, y) accurately captures the probability that deliberation will converge to the correct answer
- Evidence anchors: [abstract] "generate high-quality training data using 'Guided-Collaborative Trajectories' that steer agent responses toward and away from the correct answer"; [section 4.4] "We leverage divergent opinions to generate high-quality training data... we have agents change their opinion during the discussion and measure whether or not that change increases or decreases the likelihood that the agents' discussion converges to a correct answer"
- Break condition: If the reward estimation becomes unreliable due to poor agent performance or if guided responses are too dissimilar from natural responses to be learnable

### Mechanism 2
- Claim: The actor-critic alternating optimization scheme allows effective joint training of collaborative agents
- Mechanism: First trains critic to provide useful feedback, then trains actor to respond optimally to critic feedback, repeating until convergence
- Core assumption: The recursive nature of multi-agent deliberation can be effectively optimized through alternating best-response updates
- Evidence anchors: [abstract] "uses preference optimization to train both an actor (answer provider) and critic (feedback provider)"; [section 4.1] "This interaction closely resembles a cooperative dynamic Stackelberg game... we adopt an iterative best-response approach"
- Break condition: If the alternating scheme leads to oscillating behavior rather than convergence, or if one agent's training degrades the other's performance

### Mechanism 3
- Claim: Preference optimization with pairwise comparisons enables effective learning from the generated trajectory data
- Mechanism: Uses Direct Preference Optimization (DPO) to maximize the probability of selecting responses that lead to correct final answers
- Core assumption: The preference data generated through guided trajectories provides meaningful signal for improving both actor and critic behaviors
- Evidence anchors: [abstract] "uses preference optimization to train both an actor (answer provider) and critic (feedback provider) to maximize final answer accuracy"; [section 4.5] "we use standard preference optimization Direct Preference Optimization (DPO) Rafailov et al. (2024)"
- Break condition: If the preference data becomes too noisy or if the model overfits to the specific trajectory patterns in training data

## Foundational Learning

- Concept: Reinforcement learning through iterative best responses
  - Why needed here: The multi-agent collaboration requires agents to learn optimal strategies that depend on each other's behavior
  - Quick check question: Why can't we train both agents simultaneously with a single objective instead of alternating?

- Concept: Preference optimization and pairwise ranking
  - Why needed here: Need to learn from relative quality of different responses rather than absolute labels
  - Quick check question: What advantage does preference optimization have over traditional supervised learning in this context?

- Concept: Off-policy data generation
  - Why needed here: Natural agent behavior may not generate enough high-quality examples for effective training
  - Quick check question: How does guided deliberation differ from simply sampling multiple natural trajectories?

## Architecture Onboarding

- Component map: Task → Actor response → Critic feedback → Actor update → Final answer → Accuracy measurement
- Critical path: Actor generates answer → Critic provides feedback → Actor updates response → Final answer evaluated for accuracy
- Design tradeoffs:
  - Single round vs multiple rounds of training: More rounds may overfit but can capture more complex interactions
  - Guided vs natural trajectories: Guided provides better signal but may be less representative
  - Reward estimation method: Monte Carlo vs learned rewards trade off accuracy vs complexity
- Failure signatures:
  - Actor performance degrades when paired with trained critic (critic too harsh)
  - Critic provides vague feedback (not learning to disagree constructively)
  - Training converges too quickly (insufficient exploration of response space)
- First 3 experiments:
  1. Test guided trajectory generation: Generate 100 tasks, create steered responses, verify ∆y and ∆!y metrics
  2. Validate reward estimation: Run full deliberations from both natural and guided responses, compare estimated vs actual accuracy
  3. Simple preference optimization: Train actor on pairwise data from one task type, measure improvement on validation set

## Open Questions the Paper Calls Out
- How does ACC-Collab's performance scale when applied to larger models (e.g., 70B+ parameters) compared to smaller models like the 2B, 7B, and 8B models tested in the paper?
- Can ACC-Collab's framework be effectively applied to non-question-answering tasks such as creative writing, code generation, or multi-step reasoning problems?
- How robust is ACC-Collab to variations in the number of deliberation rounds needed for optimal performance across different task types?

## Limitations
- Performance improvements depend heavily on the quality of base models and may not generalize to smaller or less capable language models
- The alternating optimization scheme could lead to non-convergence or suboptimal equilibria between actor and critic agents
- Reliance on proxy reward signals for guided trajectory generation may not perfectly align with actual deliberation outcomes

## Confidence
- High confidence in the general framework architecture and core mechanisms
- Medium confidence in the effectiveness of guided trajectory generation for creating meaningful training signal
- Medium confidence in the alternating optimization approach converging to optimal solutions
- Medium confidence that observed performance gains will translate to real-world collaborative scenarios

## Next Checks
1. **Convergence Validation:** Run ACC-Collab training for multiple rounds (beyond the single round described) on one benchmark and monitor both actor and critic performance metrics to verify whether the alternating optimization converges or oscillates.

2. **Ablation on Guided Trajectories:** Compare performance when using only natural trajectories versus guided trajectories to quantify the exact contribution of the steering mechanism to the observed accuracy improvements.

3. **Cross-Dataset Generalization:** Train ACC-Collab on one benchmark (e.g., BoolQ) and evaluate zero-shot performance on the other four benchmarks to assess whether the learned collaborative strategies transfer across domains.