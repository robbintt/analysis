---
ver: rpa2
title: Machine learning-enabled velocity model building with uncertainty quantification
arxiv_id: '2411.06651'
source_url: https://arxiv.org/abs/2411.06651
tags: []
core_contribution: We propose a scalable machine learning method for building migration
  velocity models with uncertainty quantification. Our approach uses conditional Diffusion
  networks within a simulation-based inference framework, incorporating physics-based
  summary statistics derived from subsurface-offset image volumes.
---

# Machine learning-enabled velocity model building with uncertainty quantification

## Quick Facts
- arXiv ID: 2411.06651
- Source URL: https://arxiv.org/abs/2411.06651
- Reference count: 10
- Primary result: Scalable ML method for migration velocity models with UQ using conditional Diffusion networks and physics-based summary statistics

## Executive Summary
This paper proposes a scalable machine learning approach for building migration velocity models with uncertainty quantification using conditional Diffusion networks within a simulation-based inference framework. The method incorporates physics-based summary statistics derived from subsurface-offset image volumes to enable computationally efficient generation of Bayesian posterior samples. The authors introduce four metrics to assess uncertainty quantification quality and demonstrate improvements when using Common-Image Gathers over Reverse-Time Migrations. For complex salt structures, an iterative ASPIRE workflow is proposed to refine posterior approximations.

## Method Summary
The approach uses conditional Diffusion networks to learn a generative model that maps subsurface-offset image volumes (CIGs) to plausible velocity models, enabling amortized inference across many datasets. Physics-based summary statistics compress high-dimensional seismic data while preserving information needed for velocity inversion. The method includes an iterative ASPIRE refinement workflow for complex salt structures and four metrics for assessing uncertainty quantification quality. Training involves generating pairs of velocity models and CIGs/RTMs from synthetic datasets, then training the conditional Diffusion network using a denoising objective with the subsurface-offset Jacobian as physics-based summary statistics.

## Key Results
- Conditional Diffusion networks achieve average SSIM of 0.85 on Compass dataset compared to 0.63 for normalizing flows
- CIG conditioning improves uncertainty quantification metrics compared to RTM conditioning across multiple datasets
- ASPIRE iterative refinement successfully improves bottom salt delineation in complex salt models

## Why This Works (Mechanism)

### Mechanism 1
Diffusion networks conditioned on physics-based summary statistics (CIGs) can approximate Bayesian posteriors for velocity models efficiently by learning a generative model that maps CIGs to plausible velocity models, amortizing inference across many datasets with only one PDE solve per observation at test time.

### Mechanism 2
ASPIRE iterative refinement improves posterior approximation for complex salt structures by using the posterior mean from one iteration as the migration velocity model for the next, combined with salt flooding to better delineate bottom salt.

### Mechanism 3
Uncertainty quantification metrics can reliably assess the quality of posterior samples through four approaches: warning capability for high-error regions, calibration correlation between uncertainty and error, posterior coverage ensuring ground truth is within credible intervals, and data fit of posterior samples.

## Foundational Learning

- **Bayesian inference for inverse problems**: Needed to frame velocity model building as sampling from posterior distributions rather than point estimates. Quick check: Why can't we just use maximum likelihood estimation for velocity model building?

- **Score-based generative modeling with Diffusion networks**: Needed to learn the score function âˆ‡x log p(x) and condition on observations to generate posterior samples. Quick check: How does the denoising objective in Diffusion networks relate to learning the score function?

- **Physics-based summary statistics and extended imaging**: Needed to compress high-dimensional seismic data while preserving information for velocity inversion. Quick check: What information do CIGs contain that RTMs alone don't capture?

## Architecture Onboarding

- **Component map**: Seismic forward modeling -> CIG computation -> Diffusion network training -> Posterior sampling -> Uncertainty metrics -> ASPIRE refinement loop
- **Critical path**: Training dataset generation (velocity models + CIGs) -> Conditional Diffusion network training -> Inference with summary statistics -> Uncertainty assessment -> Optional ASPIRE iteration
- **Design tradeoffs**: Amortized inference trades accuracy for computational efficiency; summary statistics trade information loss for tractability; Diffusion networks trade training complexity for better sample quality
- **Failure signatures**: Poor data fit indicates inadequate posterior approximation; high z-scores indicate overconfidence; posterior samples that don't match geological plausibility indicate training data issues
- **First 3 experiments**:
  1. Train conditional Diffusion network on synthetic dataset with known ground truth; verify posterior samples recover true velocity models and uncertainty metrics perform as expected
  2. Test inference on held-out synthetic data; compare RMSE, SSIM, and uncertainty metrics between RTM and CIG conditioning
  3. Apply ASPIRE iteration to complex salt model; verify bottom salt improvement and uncertainty reduction in bottom salt regions

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of uncertainty quantification compare between conditional Diffusion networks and normalizing flows in seismic velocity model building? The paper compares results but doesn't conduct a systematic comparison using identical architectures and training protocols.

### Open Question 2
What is the optimal number of ASPIRE iterations for balancing computational cost and accuracy in complex salt velocity model building? The paper only presents results for two iterations without exploring whether additional iterations yield significant improvements.

### Open Question 3
How can we develop realistic training datasets for machine learning-based seismic velocity model building that avoid bias from synthetic Earth models? The authors acknowledge bias from synthetic models but don't propose specific methods for creating unbiased training datasets.

## Limitations

- Field data generalization remains uncertain as comprehensive validation across multiple diverse field datasets is lacking
- Computational scalability for industry-sized problems is not fully characterized, particularly for training data generation
- Uncertainty metric reliability on real field data is uncertain since validation relies primarily on synthetic data with known ground truth

## Confidence

- **High confidence**: Core mechanism of conditional Diffusion networks for amortized Bayesian inference is well-supported by synthetic results
- **Medium confidence**: Effectiveness of physics-based summary statistics (CIGs) over RTMs needs more diverse field validation
- **Medium confidence**: ASPIRE iterative refinement shows promise but practical utility on real field data is yet to be fully established

## Next Checks

1. **Multi-dataset field validation**: Apply complete workflow to 3-5 diverse field datasets from different geological settings and compare against industry-standard workflows
2. **Scalability benchmark**: Characteristically measure training time, inference time, and memory requirements on progressively larger datasets to establish scaling laws
3. **Uncertainty metric field validation**: Design validation protocol for uncertainty metrics on field data using blind well logs or multiple expert interpretations to assess reliability when ground truth is unknown