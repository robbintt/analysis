---
ver: rpa2
title: Multi-Task Reinforcement Learning for Quadrotors
arxiv_id: '2412.12442'
source_url: https://arxiv.org/abs/2412.12442
tags:
- task
- learning
- quadrotor
- policy
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first Multi-Task Reinforcement Learning
  (MTRL) framework tailored for quadrotor control, enabling a single policy to perform
  diverse tasks like high-speed stabilization, velocity tracking, and autonomous racing.
  The core method leverages a multi-critic architecture and shared task encoders,
  which exploit the consistent physical dynamics of the quadrotor to facilitate knowledge
  transfer across tasks.
---

# Multi-Task Reinforcement Learning for Quadrotors

## Quick Facts
- arXiv ID: 2412.12442
- Source URL: https://arxiv.org/abs/2412.12442
- Authors: Jiaxu Xing; Ismail Geles; Yunlong Song; Elie Aljalbout; Davide Scaramuzza
- Reference count: 28
- One-line primary result: First MTRL framework for quadrotors, achieving higher sample efficiency and robust performance across stabilization, velocity tracking, and autonomous racing tasks.

## Executive Summary
This paper introduces the first Multi-Task Reinforcement Learning (MTRL) framework specifically designed for quadrotor control. The framework enables a single policy to handle diverse tasks such as high-speed stabilization, velocity tracking, and autonomous racing. By leveraging a multi-critic architecture and shared task encoders that exploit consistent quadrotor dynamics, the approach achieves superior sample efficiency and task performance compared to single-task RL baselines. Experimental results demonstrate significant improvements in stabilization time, gate passing accuracy, and success rates across all tasks in both simulation and real-world scenarios.

## Method Summary
The MTRL framework employs a multi-critic architecture with a shared actor and task-specific critics, trained using Proximal Policy Optimization (PPO). The observation space is processed through a shared encoder for dynamic features (position, orientation, velocities) and task-specific encoders for task-dependent features. This design exploits the consistent physical dynamics of quadrotors to facilitate knowledge transfer across tasks. Training is conducted in the Flightmare simulator using the Agilicious quadrotor platform, with curriculum learning employed to gradually increase task difficulty. The framework is evaluated on three tasks: high-speed stabilization, velocity tracking, and autonomous racing.

## Key Results
- Achieves higher average returns in the same number of training steps compared to single-task RL baselines
- Reduces stabilization time by 18% and gate passing error by 16%
- Maintains 100% success rate across all tasks in both simulation and real-world scenarios

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to share knowledge across tasks through consistent quadrotor dynamics. The multi-critic architecture allows each task to have specialized value estimation while the shared actor policy benefits from learned representations across all tasks. The dual encoder system (shared for dynamic features, task-specific for task-dependent features) enables efficient extraction of relevant information for each task while maintaining common knowledge. This architecture exploits the physical consistency of quadrotor dynamics to facilitate knowledge transfer, leading to improved sample efficiency and performance across diverse tasks.

## Foundational Learning
- **Multi-task Reinforcement Learning**: Training a single policy to perform multiple tasks simultaneously. Why needed: Enables knowledge transfer and improves sample efficiency across related tasks. Quick check: Verify that the policy can perform all three tasks without catastrophic forgetting.
- **Multi-critic Architecture**: Using multiple critics, each specialized for a different task, while sharing a single actor. Why needed: Allows task-specific value estimation while maintaining a unified policy. Quick check: Ensure each critic provides accurate value estimates for its respective task.
- **Shared and Task-Specific Encoders**: Dual encoding system where dynamic features are shared across tasks while task-dependent features are encoded separately. Why needed: Efficiently processes both common and task-specific information. Quick check: Validate that the shared encoder captures relevant dynamic features and task-specific encoders extract appropriate task-dependent information.
- **Curriculum Learning**: Gradually increasing task difficulty during training. Why needed: Improves learning stability and performance. Quick check: Monitor learning curves to ensure smooth improvement as task difficulty increases.
- **Proximal Policy Optimization (PPO)**: On-policy algorithm that optimizes policy while maintaining stability. Why needed: Provides stable and efficient policy updates for complex multi-task scenarios. Quick check: Verify stable learning and improved performance over training iterations.
- **Quadrotor Dynamics**: Understanding the physical constraints and dynamics of quadrotor systems. Why needed: Essential for designing appropriate reward functions and interpreting results. Quick check: Confirm that the simulated dynamics match real-world behavior.

## Architecture Onboarding

**Component Map**
State Space -> Shared Encoder + Task-Specific Encoders -> Actor Network + Multiple Critics -> Action Space

**Critical Path**
Observation → Shared Encoder (dynamic features) + Task-Specific Encoders (task-dependent features) → Concatenated Features → Actor Network (policy) and Multiple Critics (value estimation) → Actions

**Design Tradeoffs**
- Shared vs. separate policies: Using a shared actor reduces model complexity and enables knowledge transfer, but may limit task-specific optimization
- Single vs. multiple critics: Multiple critics allow task-specific value estimation while maintaining a unified policy, improving performance at the cost of increased computational complexity
- Encoder architecture: Balancing shared and task-specific information extraction to maximize knowledge transfer while maintaining task performance

**Failure Signatures**
- Poor integration of shared and task-specific information leads to suboptimal performance or failure in specific tasks (e.g., racing)
- Insufficient exploration or sample efficiency results in slow convergence or poor policy performance
- Mismatch between simulated and real-world dynamics causes degradation in physical experiments

**3 First Experiments**
1. Validate the integration of shared and task-specific encoders through ablation studies, comparing performance with and without shared components
2. Test the framework on a different quadrotor platform or simulator to assess generalizability beyond the Agilicious model
3. Evaluate the scalability of the approach to additional or more complex tasks beyond the three presented

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MTRL framework perform in scenarios with varying quadrotor dynamics (e.g., payload changes or different vehicle models) across tasks?
- Basis in paper: The paper assumes consistent dynamics across tasks but does not test scenarios with varying dynamics
- Why unresolved: The framework's reliance on shared dynamics is not validated under dynamic changes
- What evidence would resolve it: Testing the MTRL policy on tasks with varying mass, inertia, or aerodynamic properties would clarify its robustness

### Open Question 2
- Question: Can the MTRL framework generalize to tasks beyond stabilization, racing, and velocity tracking, such as obstacle avoidance or exploration?
- Basis in paper: The paper focuses on three specific tasks but does not explore the framework's applicability to other domains
- Why unresolved: The framework's versatility for broader task sets remains untested
- What evidence would resolve it: Evaluating the framework on additional tasks with different reward structures and observation spaces would demonstrate its generalizability

### Open Question 3
- Question: How does the shared task encoder architecture scale with a larger number of tasks or more complex task-specific observations?
- Basis in paper: The paper highlights the shared encoder's role in extracting task-agnostic features but does not address scalability
- Why unresolved: The impact of increased task complexity or dimensionality on the shared encoder's performance is not explored
- What evidence would resolve it: Scaling the framework to handle more tasks or higher-dimensional observations and analyzing its performance would provide insights into scalability

## Limitations
- Unknown reward function parameters and network architecture details critical for faithful reproduction
- Reliance on specific simulator (Flightmare) and quadrotor model (Agilicious) may limit generalizability
- Evaluation focuses on a fixed set of tasks; framework's scalability to more complex or varied tasks is unclear

## Confidence
- Major claims: Medium
- Methodology soundness: Medium
- Reproducibility: Medium

## Next Checks
1. Verify the integration of shared and task-specific encoders through ablation studies, comparing performance with and without shared components
2. Test the framework on a different quadrotor platform or simulator to assess generalizability beyond the Agilicious model
3. Evaluate the scalability of the approach to additional or more complex tasks beyond the three presented