---
ver: rpa2
title: Informativeness of Reward Functions in Reinforcement Learning
arxiv_id: '2402.07019'
source_url: https://arxiv.org/abs/2402.07019
tags:
- reward
- policy
- learning
- agent
- informativeness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel informativeness criterion for designing
  adaptive and interpretable reward functions for reinforcement learning agents. The
  criterion quantifies how an agent's current policy will improve if it receives rewards
  from a specific function.
---

# Informativeness of Reward Functions in Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.07019
- Source URL: https://arxiv.org/abs/2402.07019
- Authors: Rati Devidze; Parameswaran Kamalaruban; Adish Singla
- Reference count: 40
- One-line primary result: ExpAdaRD framework achieves 4.6 expected reward after 20,000 episodes compared to 2.4 for best baseline in Room environment

## Executive Summary
This paper introduces a novel informativeness criterion for designing adaptive and interpretable reward functions for reinforcement learning agents. The criterion quantifies how an agent's current policy will improve if it receives rewards from a specific function, formulated within a bi-level optimization framework. The ExpAdaRD framework, which utilizes this criterion, significantly speeds up agent convergence compared to baseline techniques, particularly when dealing with diverse groups of learners.

## Method Summary
The method involves a bi-level optimization framework where the outer optimization maximizes the informativeness criterion (measuring alignment between reward function and target policy advantages) while the inner loop updates the learner's policy using the designed reward. The framework operates under structural constraints to ensure interpretability of reward functions. The approach is implemented as an iterative process that interleaves reward design with policy updates, using the informativeness criterion to adaptively guide the reward design based on the learner's current policy.

## Key Results
- ExpAdaRD achieved 4.6 expected reward after 20,000 episodes in Room environment versus 2.4 for best baseline
- Framework shows particularly pronounced effectiveness with diverse groups of learners
- Significant speedup in convergence compared to baseline techniques on two navigation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reward informativeness criterion ùêº‚Ñé(ùëÖùúô|ùëÖ,œÄùëá,œÄùêø) measures alignment between a reward function and better actions according to the target policy while considering the learner's current policy.
- Mechanism: The criterion computes expected product of target policy state occupancy, learner's action probability, and advantage function differences, creating a signal high when designed reward strongly rewards target-preferred actions over current learner policy.
- Core assumption: Learner's policy can improve by receiving rewards aligned with target policy's advantage function.
- Evidence anchors: Abstract states quantitative measure captures how agent's current policy will improve; Equation (3) defines ùêº‚Ñé explicitly; Weak evidence from related papers discussing reward design.
- Break condition: If target policy's advantage function doesn't capture meaningful information about better actions, or learner's policy cannot effectively use advantage-based rewards.

### Mechanism 2
- Claim: Bi-level optimization formulation allows reward design to adapt to learner's current policy while maintaining structural constraints.
- Mechanism: Outer optimization (maximizing ùêº‚Ñé) finds informative rewards for current learner's policy, while inner policy update ensures learner improves under designed reward; structural constraints ensure interpretability.
- Core assumption: Learner's policy can be represented parametrically and reward design can be separated into outer optimization over reward parameters and inner policy updates.
- Evidence anchors: Abstract mentions optimization under structural constraints for interpretable rewards; Algorithm 1 shows interleaving of reward design and policy updates; Moderate evidence from literature review mentioning optimization-based reward design.
- Break condition: If learner's policy cannot be effectively updated using designed reward, or structural constraints are too restrictive to find informative rewards.

### Mechanism 3
- Claim: Adaptive nature of informativeness criterion leads to faster convergence compared to non-adaptive techniques.
- Mechanism: Continuously updating reward function based on learner's current policy provides increasingly targeted guidance addressing specific weaknesses, while non-adaptive techniques provide static guidance.
- Core assumption: Learner's policy evolves over time and rewards informative for one policy may not be informative for different policy.
- Evidence anchors: Abstract mentions experimental results demonstrating effectiveness of adaptive criterion; Section 5 presents experimental results showing ExpAdaRD outperforms baselines; Moderate evidence from related work [13] as baseline not accounting for learner's current policy.
- Break condition: If learner's policy converges quickly to good policy, or informativeness criterion doesn't effectively capture what learner needs to improve.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: Entire framework built on MDP theory including states, actions, rewards, and policies
  - Quick check question: What are the components of an MDP and how do they relate to reinforcement learning?

- Concept: Advantage function
  - Why needed here: Informativeness criterion based on advantage function of target policy measuring how much better an action is compared to average action
  - Quick check question: How is the advantage function defined and what does it tell us about action quality?

- Concept: Policy gradient methods
  - Why needed here: Framework analyzes specific learning algorithm (vanilla policy gradient with planning) to derive informativeness criterion
  - Quick check question: What is the update rule for vanilla policy gradient and how does it use the advantage function?

## Architecture Onboarding

- Component map: MDP environment -> Target policy -> Learner's policy -> Learning algorithm -> Reward informativeness criterion -> Reward design optimization -> Updated rewards -> Policy improvement

- Critical path: MDP ‚Üí Target policy ‚Üí Learner's policy ‚Üí Learning algorithm ‚Üí Reward informativeness criterion ‚Üí Reward design optimization ‚Üí Updated rewards ‚Üí Policy improvement

- Design tradeoffs: Adaptive rewards provide more targeted guidance but may introduce instability; parametric reward functions ensure interpretability but may limit expressiveness; choice of informativeness criterion affects convergence speed and quality.

- Failure signatures: Slow or no convergence to optimal behavior; high variance in performance across different initial policies; rewards that don't seem to guide learner effectively; computational issues with reward design optimization.

- First 3 experiments:
  1. Implement informativeness criterion ùêº‚Ñé for simple MDP with known optimal policy and verify it gives higher values for reward functions aligning with optimal policy
  2. Test reward design optimization on small MDP with structural constraints, checking it finds interpretable rewards improving learner's policy
  3. Compare adaptive reward design framework against baseline using original reward function on navigation task, measuring convergence speed and final performance

## Open Questions the Paper Calls Out
- Theoretical relationship between informativeness criterion and convergence stability of learning algorithms
- Effectiveness of informativeness criterion when applied to continuous state and action spaces
- Sensitivity of reward design process to choice of structural constraints

## Limitations
- Experimental validation limited to only two navigation tasks, constraining generalizability
- Core mechanism heavily relies on target policy's advantage function capturing meaningful action improvements, which may not hold in all environments
- Complexity of bi-level optimization framework raises concerns about computational efficiency and practical implementation

## Confidence
- Mechanism 1 (Advantage-based informativeness): Medium
- Mechanism 2 (Bi-level optimization): Medium
- Mechanism 3 (Adaptive convergence): Medium

## Next Checks
1. Test the informativeness criterion on environments with sparse or deceptive rewards where the target policy's advantage function may not capture meaningful action improvements.
2. Implement the framework with different structural constraints (beyond parametric forms) to evaluate impact on reward expressiveness and learning performance.
3. Conduct experiments with continuous state-action spaces to assess scalability and practical applicability beyond tabular navigation tasks.