---
ver: rpa2
title: Real-time system optimal traffic routing under uncertainties -- Can physics
  models boost reinforcement learning?
arxiv_id: '2407.07364'
source_url: https://arxiv.org/abs/2407.07364
tags:
- policy
- transrl
- network
- traffic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses real-time system optimal traffic routing in
  large transportation networks under demand uncertainty and model mismatch. The core
  method, TransRL, integrates reinforcement learning with physics models by learning
  from both the environment and a teacher policy derived from deterministic system
  optimal routing.
---

# Real-time system optimal traffic routing under uncertainties -- Can physics models boost reinforcement learning?

## Quick Facts
- arXiv ID: 2407.07364
- Source URL: https://arxiv.org/abs/2407.07364
- Reference count: 40
- Primary result: TransRL outperforms both traffic model-based methods and state-of-the-art RL algorithms (PPO, SAC) in reducing average total travel time under demand uncertainty and model mismatch

## Executive Summary
This paper addresses the challenge of real-time system optimal traffic routing in large transportation networks under demand uncertainty and model mismatch. The authors propose TransRL, a novel approach that integrates reinforcement learning with physics models by leveraging a teacher policy derived from deterministic system optimal routing. By combining real-time interactions with model insights through a KL-divergence regularization, TransRL aims to achieve superior performance in reducing total travel time while maintaining interpretability and reliability.

## Method Summary
TransRL combines reinforcement learning with physics-based insights by using a teacher policy derived from deterministic system optimal routing. The approach learns from both the environment and the teacher policy during training, maximizing cumulative rewards while minimizing the Kullback-Leibler divergence between its policy and the teacher policy. This dual-learning mechanism allows TransRL to leverage both real-time interactions and deterministic model insights, potentially improving performance under uncertainty and model mismatch.

## Key Results
- TransRL outperforms traffic model-based methods and state-of-the-art RL algorithms (PPO, SAC) in reducing average total travel time
- TransRL demonstrates higher reliability (measured via Conditional Travel Time Reduction at Risk) compared to baseline RL approaches
- TransRL exhibits more interpretable actions compared to baseline RL methods

## Why This Works (Mechanism)
TransRL works by integrating reinforcement learning with physics models through a teacher-student framework. The teacher policy, derived from deterministic system optimal routing, provides model-based insights that guide the RL agent's learning process. By regularizing the RL policy to stay close to the teacher policy (via KL divergence minimization), TransRL can leverage the strengths of both data-driven learning and physics-based modeling. This approach helps the agent make better decisions under uncertainty and model mismatch by combining real-time experience with theoretical optimal routing principles.

## Foundational Learning
1. **System Optimal Traffic Routing** - The concept of minimizing total travel time across all users in a network. Why needed: Forms the theoretical foundation for the teacher policy and the optimization objective.
   Quick check: Understanding how system optimal routing differs from user equilibrium and its implications for traffic flow.

2. **Reinforcement Learning in Traffic Control** - Using RL algorithms to learn control policies for traffic signal timing or routing decisions. Why needed: The core methodology for learning adaptive routing policies in uncertain environments.
   Quick check: Familiarity with RL concepts like policy gradients, value functions, and exploration vs. exploitation.

3. **Kullback-Leibler Divergence** - A measure of how one probability distribution differs from a reference distribution. Why needed: Used as a regularization term to keep the RL policy close to the teacher policy.
   Quick check: Understanding how KL divergence works as a distance metric between probability distributions and its role in policy regularization.

## Architecture Onboarding

Component Map:
Environment -> RL Agent (TransRL) -> Traffic Network -> Teacher Policy (Deterministic SO)

Critical Path:
1. Environment generates traffic state and demand
2. RL Agent (TransRL) selects actions based on current policy
3. Actions are executed in the traffic network
4. Network state evolves based on traffic flow physics
5. Rewards are computed (e.g., total travel time)
6. Teacher policy provides optimal routing suggestions
7. KL divergence between RL policy and teacher policy is computed
8. Policy is updated using combined reward and KL regularization

Design Tradeoffs:
- Balancing exploration vs. exploitation in uncertain environments
- Weighting between environment rewards and teacher policy guidance
- Computational complexity of maintaining and updating the teacher policy
- Tradeoff between model accuracy and real-time responsiveness

Failure Signatures:
- High KL divergence between RL and teacher policies indicating poor alignment
- Increasing total travel time despite training, suggesting learning instability
- Policy collapse to deterministic actions, reducing adaptability
- Over-reliance on teacher policy leading to poor generalization to new scenarios

First Experiments:
1. Compare TransRL performance against pure RL (PPO, SAC) and pure model-based methods on small synthetic networks
2. Test sensitivity of TransRL to varying levels of demand uncertainty and model mismatch
3. Evaluate the interpretability of TransRL actions using standardized metrics and compare against baseline methods

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily based on synthetic networks, limiting generalizability to real-world urban systems
- Performance gains demonstrated numerically but not validated on empirical data
- Assumes accurate model knowledge for teacher policy, which may not hold under significant model mismatch
- Interpretability advantage claimed but not rigorously quantified or compared against baselines

## Confidence
- **High**: TransRL's ability to outperform deterministic model-based methods in the tested synthetic networks
- **Medium**: Claims of superior performance over state-of-the-art RL algorithms (PPO, SAC) and higher reliability (CTTR)
- **Low**: Generalizability to real-world networks, robustness under extreme model mismatch, and the interpretability advantage over baselines

## Next Checks
1. Test TransRL on real-world transportation networks with empirical demand data to assess scalability and robustness
2. Evaluate the sensitivity of TransRL to varying degrees of model mismatch and demand uncertainty beyond the synthetic scenarios
3. Develop and apply standardized interpretability metrics to quantify and compare the interpretability of TransRL actions against baseline RL methods