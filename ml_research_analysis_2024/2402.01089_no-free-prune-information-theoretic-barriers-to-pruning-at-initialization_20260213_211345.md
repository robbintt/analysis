---
ver: rpa2
title: 'No Free Prune: Information-Theoretic Barriers to Pruning at Initialization'
arxiv_id: '2402.01089'
source_url: https://arxiv.org/abs/2402.01089
tags:
- pruning
- information
- data
- initialization
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides a theoretical explanation for why pruning at
  initialization (without training the full model) is generally unsuccessful in finding
  sparse subnetworks that match the performance of the full model. The key idea is
  to define an "effective parameter count" for a sparse network as the sum of the
  number of non-zero weights and the mutual information between the sparsity mask
  and the training data.
---

# No Free Prune: Information-Theoretic Barriers to Pruning at Initialization

## Quick Facts
- arXiv ID: 2402.01089
- Source URL: https://arxiv.org/abs/2402.01089
- Authors: Tanishq Kumar; Kevin Luo; Mark Sellke
- Reference count: 40
- The paper proves that pruning at initialization is theoretically limited by information-theoretic barriers, explaining why lottery tickets cannot be efficiently found without training.

## Executive Summary
This paper provides a theoretical explanation for why pruning at initialization generally fails to find sparse subnetworks that match the performance of fully trained models. The authors introduce the concept of "effective parameter count" which combines the number of non-zero weights with the mutual information between the sparsity mask and training data. They prove that for a sparse network to robustly interpolate noisy data, the mask must be heavily data-dependent, leading to a high effective parameter count. Experiments confirm that subnetworks from pruning algorithms that train on data have higher effective parameter count and memorization capacity than those from initialization-only pruning methods.

## Method Summary
The authors define an "effective parameter count" as the sum of non-zero weights and mutual information between the sparsity mask and training data. They prove a modified Law of Robustness showing that sparse networks robust to noisy data require data-dependent masks with high effective parameter count. Experiments use synthetic Gaussian data with random Boolean labels, noisy FashionMNIST, and noisy CIFAR-10. A two-hidden layer ReLU network (width 200) is trained with Cross Entropy Loss and Adam optimizer (lr=1e-3). Various pruning methods are compared: IMP, SNIP, GraSP, SynFlow, and magnitude pruning after training. Memorization capacity and correlation with noise metrics are computed at different sparsity levels.

## Key Results
- Subnetworks from initialization-time pruning (SNIP, GraSP, SynFlow) have lower effective parameter count than those from training-based pruning (IMP, magnitude pruning)
- Initialization-only pruning methods show significantly reduced memorization capacity compared to training-based methods
- The effective parameter count correlates strongly with network capacity across different pruning algorithms and datasets
- Lottery tickets can be found at initialization but require the same effective parameter count as training-based methods, making them inefficient to discover

## Why This Works (Mechanism)
The core mechanism is that robust interpolation of noisy data requires the sparsity mask to encode substantial information about the data distribution. This data-dependent mask effectively becomes part of the model's parameters, inflating the true parameter count beyond the visible non-zero weights. The mutual information between mask and data acts as a theoretical lower bound on how much the mask must adapt to the data for successful learning.

## Foundational Learning
- **Effective Parameter Count**: Sum of non-zero weights and mutual information with data - needed to quantify true model complexity beyond visible parameters; check by computing for various pruning methods
- **Law of Robustness**: Relationship between model capacity and noise robustness - needed to connect theoretical bounds to practical performance; check by testing on datasets with varying noise levels
- **Mutual Information**: Information-theoretic measure between mask and data - needed to quantify data-dependence of pruning decisions; check by estimating MI for different pruning algorithms
- **Lottery Ticket Hypothesis**: Subnetworks can match full network performance when properly initialized - needed as baseline for comparing pruning approaches; check by comparing IMP vs initialization-only methods

## Architecture Onboarding

**Component Map**: Data generation -> Network training -> Pruning application -> Metric computation -> Analysis

**Critical Path**: Generate noisy data → Train full network → Apply pruning method → Evaluate memorization capacity → Compute effective parameter count

**Design Tradeoffs**: 
- Initialization-only pruning: Fast but low capacity
- Training-based pruning: Slower but higher capacity
- Mutual information estimation: Theoretically sound but computationally expensive

**Failure Signatures**:
- Pruning methods converge to degenerate subnetworks
- Memorization capacity doesn't increase with effective parameter count
- Mutual information estimates are unstable or zero

**First Experiments**:
1. Generate synthetic Gaussian data with random Boolean labels and train a two-layer ReLU network
2. Apply IMP, SNIP, GraSP, and SynFlow pruning to the network at 50% sparsity
3. Compute memorization capacity and effective parameter count for each method

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework assumes certain properties about network architectures that may not capture modern deep learning practices
- Experiments focus on relatively small-scale problems (FashionMNIST, CIFAR-10) rather than very large-scale models
- The paper doesn't explore whether alternative initialization schemes could circumvent the identified barriers

## Confidence

**High confidence** in the theoretical derivation of effective parameter count and its relationship to robust interpolation
**Medium confidence** in the experimental validation showing differences between initialization-time and training-based pruning methods
**Medium confidence** in the conclusion that lottery tickets cannot be efficiently found at initialization

## Next Checks

1. Test theoretical predictions on larger-scale architectures (ResNets, Transformers) to verify if effective parameter count remains predictive
2. Investigate whether alternative initialization schemes or pruning criteria could circumvent information-theoretic barriers
3. Validate mutual information bounds empirically by measuring actual MI between sparsity masks and data across different pruning methods