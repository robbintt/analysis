---
ver: rpa2
title: A Reality check of the benefits of LLM in business
arxiv_id: '2406.10249'
source_url: https://arxiv.org/abs/2406.10249
tags:
- llms
- data
- questions
- arxiv
- business
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts the first quantified study of Large Language
  Models (LLMs) applied to core business operations and challenges. It evaluates four
  accessible LLMs using real-world data across key business functions like text analysis,
  content generation, translation, code generation, and data analysis.
---

# A Reality check of the benefits of LLM in business

## Quick Facts
- arXiv ID: 2406.10249
- Source URL: https://arxiv.org/abs/2406.10249
- Authors: Ming Cheung
- Reference count: 40
- Primary result: LLMs show limited effectiveness in business applications, with the best model achieving only 1.9/50 reference matches and SQL accuracy ranging from 63% (simple) to 22% (complex)

## Executive Summary
This paper presents the first quantified study evaluating Large Language Models (LLMs) across core business operations including text analysis, content generation, translation, code generation, and data analysis. Using real-world data and four accessible LLMs, the research reveals significant limitations in LLM performance, particularly in tasks requiring contextual understanding and complex reasoning. The findings challenge the assumption that LLMs can readily replace conventional business tools and highlight the need for careful consideration of their practical limitations.

## Method Summary
The study evaluates four LLMs (ChatGPT-3.5, Claude Instant, Llama-2-70b, and PaLM 2) using automated experiments with real-world business data. The evaluation framework tests LLMs across five key business functions through prompt-based queries, measuring performance via accuracy metrics, match rates, and run rates. Experiments include reference generation from text, SQL code generation from natural language questions, and sentiment analysis tasks.

## Key Results
- LLMs achieved only 1.9 out of 50 correct references in the best-performing model for reference generation tasks
- SQL code generation accuracy varied dramatically: 63% for simple questions but only 22% for complex queries requiring joins
- LLMs demonstrated strong performance in Q&A tasks with 94% accuracy, but struggled with complex reasoning requiring contextual understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can adapt to new business domains through prompt engineering without retraining
- Mechanism: The vast parameter count and training on trillions of words allows LLMs to generalize patterns across domains, enabling them to respond to new prompts by leveraging learned representations
- Core assumption: The LLM's pre-training data contains sufficient diversity and domain overlap to cover business-specific contexts
- Evidence anchors:
  - [abstract] "Unlike conventional models, LLMs can adapt to new domains through prompt engineering without the need for retraining, making them suitable for various business functions..."
  - [section 4.2] "LLMs demonstrate remarkable abilities in generating tailored project plans by incorporating instructions, key points, and constraints into the prompt."
- Break condition: If business domain is highly specialized with limited overlap in training data, the LLM may fail to generate relevant or accurate responses

### Mechanism 2
- Claim: LLMs can perform complex tasks like SQL code generation and data analysis based on natural language queries
- Mechanism: The LLM maps natural language questions to structured query patterns and database schema by recognizing keywords and inferring relationships from context
- Core assumption: The prompt provides sufficient table schema and context for the LLM to infer correct joins and aggregations
- Evidence anchors:
  - [section 6] "Leveraging LLMs to generate SQL queries from natural language can expedite the exploration of insights..."
  - [section 4.4] "Fig. 6 provides examples of SQL code generation for questions derived from a table."
- Break condition: When queries require complex joins or multi-table operations, the LLM's lack of true schema understanding leads to errors

### Mechanism 3
- Claim: LLMs can generate and analyze text for business applications like sentiment analysis and content generation
- Mechanism: The LLM uses learned patterns to classify sentiment, summarize content, and generate text aligned with provided instructions and constraints
- Core assumption: The input text is sufficiently similar to training data for accurate pattern recognition and generation
- Evidence anchors:
  - [section 4.1] "Leveraging LLMs, such as ChatGPT, enables comprehensive analysis and understanding of the sentiment expressed in text..."
  - [section 4.2] "LLMs prove to be invaluable in serving scenarios by producing captivating social media posts and persuasive product descriptions..."
- Break condition: If the input text contains domain-specific jargon or requires deep contextual understanding beyond surface patterns, accuracy degrades

## Foundational Learning

- Concept: Prompt engineering and context provision
  - Why needed here: LLMs are highly sensitive to prompts and context; effective use requires crafting precise instructions to guide outputs
  - Quick check question: How would you modify a prompt to improve the accuracy of a sentiment analysis task for customer reviews?

- Concept: Understanding LLM limitations (bias, factual accuracy, context comprehension)
  - Why needed here: Awareness of these limitations is critical for realistic expectations and appropriate application design
  - Quick check question: What are the potential risks of using LLMs for generating factual reports in business settings?

- Concept: Basic SQL and data analysis concepts
  - Why needed here: Many business applications involve data extraction and analysis, requiring understanding of how LLMs can assist with SQL generation
  - Quick check question: How would you structure a prompt to ask an LLM to generate SQL for calculating average sales per region?

## Architecture Onboarding

- Component map: LLM service/API -> Prompt engineering layer -> Data processing layer -> Evaluation framework
- Critical path: 1. Define business task and requirements 2. Prepare context and schema (if applicable) 3. Design and test prompts 4. Run LLM inference 5. Evaluate outputs and iterate
- Design tradeoffs:
  - Accuracy vs. cost: Larger models are more capable but expensive
  - Prompt complexity vs. maintainability: Complex prompts may work better but are harder to debug
  - Custom training vs. prompt engineering: Training requires more resources but can improve domain fit
- Failure signatures: Hallucinations or factual errors in outputs, inconsistent results for the same prompt, bias in generated content or recommendations, failure to handle complex multi-step reasoning
- First 3 experiments: 1. Sentiment analysis: Test LLM on classifying customer reviews as positive/negative with and without context 2. SQL generation: Provide table schema and natural language questions to evaluate accuracy of generated queries 3. Content generation: Generate marketing copy for a product with specific constraints and evaluate quality and adherence to instructions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of LLMs in generating SQL queries with joins compare to their performance in simpler queries?
- Basis in paper: [explicit] The paper states that "LLMs struggle with joins that necessitate more complex SQL queries."
- Why unresolved: While the paper indicates that LLMs struggle with joins, it does not provide a direct comparison of accuracy between join queries and simpler queries.
- What evidence would resolve it: A detailed analysis of LLM performance on SQL queries with and without joins, including accuracy metrics for each type.

### Open Question 2
- Question: What is the impact of different prompt formats on the accuracy and consistency of LLM outputs?
- Basis in paper: [explicit] The paper discusses that "LLMs are also known for their sensitivity to the prompts used during inference" and suggests exploring "different prompt formats, including prompt length, style, question-answering formats, and context inclusion/exclusion."
- Why unresolved: The paper identifies the importance of prompts but does not provide empirical data on how different prompt formats affect LLM performance.
- What evidence would resolve it: An experiment comparing LLM outputs using various prompt formats, measuring accuracy and consistency across different tasks.

### Open Question 3
- Question: How can context-aware LLMs be developed to improve their understanding of complex problem-solving scenarios?
- Basis in paper: [explicit] The paper suggests that "a promising research direction is the development of context-aware LLMs that leverage contextual information."
- Why unresolved: While the paper highlights the need for context-aware LLMs, it does not provide specific methods or evidence for their development.
- What evidence would resolve it: Research demonstrating the development and effectiveness of context-aware LLMs in complex problem-solving tasks, with measurable improvements over current models.

### Open Question 4
- Question: What are the most effective techniques for detecting and mitigating bias in LLMs?
- Basis in paper: [explicit] The paper discusses the impact of bias on LLMs and suggests "ongoing research and improvement to address the limitations associated with bias."
- Why unresolved: The paper acknowledges the presence of bias but does not provide specific techniques for its detection and mitigation.
- What evidence would resolve it: Studies comparing different bias detection and mitigation techniques, with quantitative results showing their effectiveness in reducing bias in LLM outputs.

## Limitations
- LLMs show extremely poor performance in reference generation tasks, with only 1.9/50 correct matches even for the best model
- Accuracy in SQL code generation drops significantly from 63% to 22% when moving from simple to complex queries
- LLMs struggle with contextual understanding and complex reasoning, limiting their effectiveness in real-world business applications

## Confidence
- High confidence: LLMs exhibit varying accuracy based on question complexity, with clear quantitative differences observed between simple (63%) and complex (22%) queries
- Medium confidence: LLMs struggle with complex reasoning tasks and contextual understanding, based on multiple failed experiments across different business domains
- Low confidence: Specific performance metrics due to lack of detailed information about exact prompt templates and evaluation methodologies

## Next Checks
1. Replicate the SQL generation experiments with publicly available schemas to verify the reported accuracy variations between simple and complex queries
2. Test reference generation performance using standardized academic datasets to confirm the extremely low match rates reported
3. Conduct a prompt sensitivity analysis to quantify how minor prompt variations affect output quality across different business tasks