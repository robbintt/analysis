---
ver: rpa2
title: Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal
  Models in Medical VQA
arxiv_id: '2405.20421'
source_url: https://arxiv.org/abs/2405.20421
tags:
- pair
- question
- llav
- accuracy
- probmed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper exposes a critical flaw in how large multimodal models\
  \ (LMMs) are currently evaluated for medical Visual Question Answering (Med-VQA).\
  \ Despite high reported accuracy, the authors demonstrate that top models like GPT-4V\
  \ and Gemini Pro perform worse than random guessing when tested with adversarial\
  \ probing\u2014questions that include hallucinated attributes."
---

# Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA

## Quick Facts
- **arXiv ID**: 2405.20421
- **Source URL**: https://arxiv.org/abs/2405.20421
- **Reference count**: 13
- **Key outcome**: Large multimodal models perform worse than random guessing on adversarial medical VQA tasks when tested with hallucinated attributes, revealing significant reliability concerns for clinical applications.

## Executive Summary
This study exposes critical flaws in current evaluation methods for large multimodal models (LMMs) in medical Visual Question Answering (Med-VQA). Despite high reported accuracy scores, top models like GPT-4V and Gemini Pro show near-random performance when tested with adversarial probing questions that include hallucinated attributes. The authors introduce the ProbMed dataset, which systematically pairs medical questions with negation variants containing false details, requiring models to reason across diagnostic dimensions like modality, organ, condition, and position. Testing on 57,000 question-answer pairs from diverse medical imaging sources reveals that even state-of-the-art models struggle with specialized diagnostic queries, highlighting the need for more robust evaluation methods to ensure trustworthiness in critical medical applications.

## Method Summary
The authors developed a novel evaluation methodology using adversarial probing to test LMM reliability in medical VQA tasks. They constructed the ProbMed dataset by systematically creating negation variants of medical questions, where each negation contains hallucinated attributes that make the statement false. The dataset covers multiple diagnostic dimensions including imaging modality, anatomical organ, medical condition, and spatial position. Each question-answer pair was designed to require careful reasoning about the presence or absence of specific features in medical images. The evaluation framework tests models on both the original questions and their negation variants, with performance measured across these different diagnostic dimensions to assess model robustness and reliability.

## Key Results
- GPT-4V and Gemini Pro achieve near-random accuracy on specialized diagnostic queries when tested with adversarial probing questions
- Models show significant performance degradation when asked to reason about the absence of features versus their presence
- The ProbMed dataset reveals systematic weaknesses in how LMMs handle hallucinated attributes in medical contexts

## Why This Works (Mechanism)
The adversarial probing methodology works by exploiting the tendency of LMMs to confidently generate responses even when presented with false or hallucinated information. By systematically introducing false attributes into medical questions, the evaluation forces models to demonstrate true understanding rather than pattern matching or surface-level recognition. The negation-based approach creates a controlled environment where correct responses require explicit reasoning about what is not present in medical images, which is often more challenging than identifying what is present. This methodology effectively separates models that have genuine diagnostic reasoning capabilities from those that rely on superficial correlations or context cues.

## Foundational Learning
- **Adversarial probing**: A testing methodology that introduces controlled perturbations or false information to evaluate model robustness and reliability. Needed to move beyond surface-level accuracy metrics and expose fundamental weaknesses in model reasoning capabilities. Quick check: Does the model maintain performance when presented with systematically corrupted or negated inputs?

- **Negation-based evaluation**: An assessment approach that tests models on both affirmative and negative variants of questions to evaluate understanding of absence versus presence. Required because many models can identify visible features but struggle with reasoning about what is not there. Quick check: Compare model accuracy on positive versus negative question variants.

- **Multimodal reasoning**: The ability to integrate visual and textual information across multiple diagnostic dimensions (modality, organ, condition, position) to reach coherent conclusions. Essential for medical applications where decisions require synthesizing information from different sources. Quick check: Can the model consistently reason across different diagnostic dimensions when tested with adversarial examples?

- **Hallucination detection**: The capability to recognize and appropriately handle false or fabricated information in input queries. Critical for medical applications where incorrect responses can have serious consequences. Quick check: Does the model identify and flag hallucinated attributes rather than confidently generating incorrect responses?

## Architecture Onboarding

**Component Map**: Medical image input -> Vision encoder -> Multimodal fusion layer -> Language model -> Diagnostic reasoning output -> Negation variant testing

**Critical Path**: Image processing → Feature extraction → Cross-modal integration → Question understanding → Diagnostic reasoning → Confidence assessment → Negation handling

**Design Tradeoffs**: The study highlights the tension between model accuracy on standard benchmarks versus robustness to adversarial inputs. Models optimized for high accuracy on conventional Med-VQA datasets may sacrifice robustness to hallucinated attributes. The evaluation reveals that current architectures struggle with the cognitive complexity of medical reasoning, particularly when required to reason about absence of features rather than presence.

**Failure Signatures**: Models consistently fail on negation variants containing hallucinated attributes, showing near-random performance. Common failure patterns include confidently affirming false statements, failing to recognize hallucinated features as impossible, and showing no systematic difference in handling positive versus negative queries. These failures suggest models rely more on pattern matching than genuine medical reasoning.

**First Experiments**: 1) Test specialized medical LMMs versus general-purpose models on the ProbMed dataset to identify if domain-specific training improves robustness to adversarial probing. 2) Evaluate model performance on progressively complex negation variants to map the relationship between query complexity and failure rates. 3) Assess whether fine-tuning on negation-based examples improves model robustness to hallucinated attributes in medical contexts.

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses primarily on general-purpose models like GPT-4V and Gemini Pro rather than specialized medical models, which may have different performance characteristics
- The evaluation relies on synthetic adversarial examples that may not fully capture real-world clinical scenarios and patient variability
- The dataset construction process, while systematic, could introduce unintended biases in how questions and negations are paired

## Confidence
- Models perform worse than random on adversarial probing: Medium confidence (depends heavily on specific methodology)
- Current evaluation methods overestimate model reliability: High confidence (robust experimental design)
- Findings generalize to all medical LMM applications: Low confidence (limited to specific diagnostic reasoning dimensions)

## Next Checks
1. Test the ProbMed methodology on specialized medical LMMs and fine-tuned models to determine if the performance degradation is unique to general-purpose models or a broader phenomenon in the field.

2. Conduct real-world clinical validation by having medical experts evaluate model responses in actual diagnostic scenarios to assess whether the adversarial probing findings translate to practical reliability concerns.

3. Develop and test alternative evaluation metrics that balance the rigor of adversarial probing with the need for models to perform well on legitimate clinical questions, potentially through weighted scoring systems that account for both accuracy and robustness.