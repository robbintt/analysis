---
ver: rpa2
title: 'CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive
  Replay'
arxiv_id: '2402.01348'
source_url: https://arxiv.org/abs/2402.01348
tags:
- task
- forgetting
- tasks
- learning
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses catastrophic forgetting in continual learning,
  where neural networks lose previously learned knowledge when acquiring new tasks.
  The authors propose COgnitive REplay (CORE), which introduces two key strategies:
  Adaptive Quantity Allocation and Quality-Focused Data Selection.'
---

# CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive Replay

## Quick Facts
- **arXiv ID:** 2402.01348
- **Source URL:** https://arxiv.org/abs/2402.01348
- **Reference count:** 9
- **Primary result:** CORE achieves 37.95% average accuracy on split-CIFAR10, surpassing the best baseline by 6.52%

## Executive Summary
This paper addresses catastrophic forgetting in continual learning, where neural networks lose previously learned knowledge when acquiring new tasks. The authors propose COgnitive REplay (CORE), which introduces two key strategies: Adaptive Quantity Allocation and Quality-Focused Data Selection. These strategies dynamically allocate replay buffer space based on each task's forgetting rate and select representative data samples that best capture task characteristics. CORE is inspired by human cognitive review processes, specifically Targeted Recall and Spaced Repetition strategies. Experiments on split-MNIST, split-CIFAR10, and split-CIFAR100 datasets demonstrate CORE's effectiveness, achieving an average accuracy of 37.95% on split-CIFAR10, surpassing the best baseline method by 6.52%. Additionally, CORE significantly improves the accuracy of the poorest-performing task by 6.30% compared to the top baseline.

## Method Summary
CORE implements a two-pronged approach to mitigate catastrophic forgetting. First, Adaptive Quantity Allocation dynamically assigns buffer space to each task based on its forgetting rate, prioritizing tasks that lose information more quickly. Second, Quality-Focused Data Selection identifies and stores the most representative samples from each task that best capture its characteristics. The method is inspired by human cognitive strategies, specifically Targeted Recall (focusing on information that is most likely to be forgotten) and Spaced Repetition (optimizing review intervals based on forgetting patterns). The approach uses a replay buffer that stores selected samples from previous tasks, which are then replayed during training on new tasks to maintain knowledge of earlier information.

## Key Results
- CORE achieves 37.95% average accuracy on split-CIFAR10, surpassing the best baseline by 6.52%
- CORE improves the accuracy of the poorest-performing task by 6.30% compared to the top baseline
- CORE demonstrates balanced performance across tasks, mitigating catastrophic forgetting effectively

## Why This Works (Mechanism)
CORE's effectiveness stems from its adaptive approach to memory management in neural networks. By allocating buffer space based on forgetting rates, it ensures that tasks prone to rapid forgetting receive adequate resources for preservation. The quality-focused data selection mechanism ensures that only the most representative samples are stored, maximizing the information density of the limited buffer space. This approach mirrors human cognitive strategies of targeted review and spaced repetition, where we focus our attention on information most at risk of being forgotten and review it at optimal intervals.

## Foundational Learning
- **Catastrophic forgetting**: Neural networks lose previously learned knowledge when trained on new tasks - needed because understanding the core problem CORE addresses
- **Replay buffer**: A memory mechanism that stores samples from previous tasks for later review - needed because CORE relies on this for maintaining knowledge of old tasks
- **Adaptive allocation**: Dynamically distributing resources based on need - needed because CORE's buffer space allocation is based on task forgetting rates
- **Quality-focused selection**: Choosing representative samples that best capture task characteristics - needed because CORE's data selection mechanism prioritizes informative samples
- **Spaced repetition**: Reviewing information at increasing intervals to optimize retention - needed because CORE's approach is inspired by this cognitive strategy
- **Targeted recall**: Focusing review efforts on information most likely to be forgotten - needed because CORE's adaptive allocation is based on this principle

## Architecture Onboarding
- **Component map**: CORE -> Adaptive Quantity Allocation -> Buffer Space Distribution -> Quality-Focused Data Selection -> Sample Storage -> Replay During Training
- **Critical path**: New task training -> Forgetting rate calculation -> Buffer space reallocation -> Quality-focused sample selection -> Sample storage -> Replay buffer integration -> Training with replay
- **Design tradeoffs**: CORE prioritizes balanced performance across tasks over maximum single-task performance, accepting computational overhead for adaptive allocation in exchange for better forgetting mitigation
- **Failure signatures**: Poor forgetting rate estimation leading to suboptimal buffer allocation, quality-focused selection failing to identify truly representative samples, buffer overflow preventing adequate storage of all tasks
- **First experiments**: 1) Test adaptive allocation on synthetic forgetting patterns 2) Evaluate quality-focused selection on controlled datasets 3) Measure baseline catastrophic forgetting without replay

## Open Questions the Paper Calls Out
None

## Limitations
- Focuses primarily on image classification benchmarks (split-MNIST, split-CIFAR10, split-CIFAR100)
- Performance on long task sequences beyond tested datasets remains unverified
- Effectiveness in scenarios with highly imbalanced task difficulties is unclear

## Confidence
- **High Confidence**: CORE's effectiveness on tested image classification datasets
- **Medium Confidence**: CORE's superiority over baseline methods (limited to tested datasets)
- **Low Confidence**: CORE's generalization to other domains and real-world applications

## Next Checks
1. Evaluate CORE's performance on non-image datasets (e.g., natural language processing tasks) to assess domain generalizability
2. Test CORE on longer task sequences (beyond 5-10 tasks) to evaluate long-term effectiveness
3. Conduct ablation studies to isolate the impact of Adaptive Quantity Allocation and Quality-Focused Data Selection components on overall performance