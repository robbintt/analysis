---
ver: rpa2
title: 'CoDreamer: Communication-Based Decentralised World Models'
arxiv_id: '2406.13600'
source_url: https://arxiv.org/abs/2406.13600
tags:
- codreamer
- agents
- idreamer
- communication
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoDreamer extends the DreamerV3 model-based reinforcement learning
  algorithm to multi-agent settings by introducing a two-level communication system
  using Graph Neural Networks. The method enables agents to communicate within their
  learned world models to improve environmental modeling and within their policies
  to enhance cooperation.
---

# CoDreamer: Communication-Based Decentralised World Models

## Quick Facts
- arXiv ID: 2406.13600
- Source URL: https://arxiv.org/abs/2406.13600
- Authors: Edan Toledo; Amanda Prorok
- Reference count: 40
- Primary result: CoDreamer significantly outperforms baseline methods IPPO and IDreamer across various multi-agent tasks, with particularly strong performance in high-dimensional visual observation settings

## Executive Summary
CoDreamer extends the DreamerV3 model-based reinforcement learning algorithm to multi-agent settings by introducing a two-level communication system using Graph Neural Networks (GNNs). The method enables agents to communicate within their learned world models to improve environmental modeling and within their policies to enhance cooperation. This approach addresses challenges of partial observability and non-stationarity in multi-agent environments. Experimental results show CoDreamer significantly outperforms baseline methods IPPO and IDreamer across various multi-agent tasks, with particularly strong performance in high-dimensional visual observation settings.

## Method Summary
CoDreamer is a decentralized model-based reinforcement learning algorithm that uses GNNs to facilitate communication between agents at two levels: within world models for environmental modeling and within policies for cooperative action selection. The method employs a dynamic communication topology based on agent positions and communication range, allowing agents to share information and generate synthetic trajectories. The algorithm is evaluated on VMAS (vector-based observations) and Melting Pot (pixel-based observations) environments, showing superior performance and sample efficiency compared to baselines.

## Key Results
- CoDreamer achieved normalized returns of 0.97 (median) and 0.97 (IQM) in VMAS tasks
- In Melting Pot tasks, CoDreamer achieved normalized returns of 0.48 (median) and 0.48 (IQM)
- The method demonstrated superior sample efficiency and performance across all point estimate metrics compared to IPPO and IDreamer baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-level communication system in CoDreamer improves environmental modeling and policy coordination by allowing agents to share information within their world models and policies.
- Mechanism: CoDreamer uses Graph Neural Networks (GNNs) to facilitate communication between agents at two levels: within the world models to enhance trajectory imagination and within the policies to improve cooperation. This dual-tiered communication strategy addresses challenges such as partial observability and non-stationarity in multi-agent environments.
- Core assumption: Agents can effectively communicate and share relevant information within their learned world models and policies to improve overall performance.
- Break condition: If the communication topology or the GNN architecture fails to capture relevant information, the performance gains may diminish.

### Mechanism 2
- Claim: The use of GNNs in CoDreamer allows for efficient information propagation among agents, enabling them to model complex, inter-agent dependencies and improve sample efficiency.
- Mechanism: GNNs enable agents to share information based on their communication range, dynamically adapting the adjacency matrix as agents move. This allows agents to collectively generate synthetic trajectories, improving performance given a limited sample budget.
- Core assumption: The communication range and GNN architecture are sufficient to capture the necessary information for effective modeling and cooperation.
- Break condition: If the communication range is too small or the GNN architecture is not expressive enough, the system may fail to capture important inter-agent dependencies.

### Mechanism 3
- Claim: CoDreamer's ability to model inter-agent dependencies and partial observability leads to superior performance compared to baseline methods in both vector-based and visual observation settings.
- Mechanism: By incorporating communication within the world models and policies, CoDreamer can better model the environment and coordinate actions among agents, leading to improved performance in tasks requiring cooperation and coordination.
- Core assumption: The improved modeling of inter-agent dependencies and partial observability translates to better policy learning and task performance.
- Break condition: If the baseline methods or the evaluation tasks do not require significant cooperation or coordination, the performance gains may be less pronounced.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs are used to facilitate communication between agents in CoDreamer, allowing them to share information and improve environmental modeling and policy coordination.
  - Quick check question: How do GNNs enable efficient information propagation among agents in a multi-agent system?

- Concept: Partial Observability
  - Why needed here: CoDreamer addresses the challenge of partial observability in multi-agent environments by allowing agents to communicate and share information, improving their understanding of the environment.
  - Quick check question: What is partial observability, and why is it a challenge in multi-agent reinforcement learning?

- Concept: Model-Based Reinforcement Learning
  - Why needed here: CoDreamer is an extension of the Dreamer algorithm, which uses a learned world model to generate synthetic data and plan actions based on future predictions, improving sample efficiency.
  - Quick check question: How does model-based reinforcement learning differ from model-free approaches, and what are its advantages in multi-agent settings?

## Architecture Onboarding

- Component map: World Models (GNNs) -> Policies (GNNs) -> Communication Topology (dynamic) -> Observation Space (VMAS/Melting Pot) -> Action Space (discrete/continuous)

- Critical path: 1. Initialize world models and policies for each agent. 2. Collect experiences and construct communication graphs based on agent positions. 3. Use GNNs to facilitate communication within world models and policies. 4. Generate synthetic trajectories and update world models. 5. Train policies using imagined trajectories. 6. Evaluate and update communication topology.

- Design tradeoffs: Communication range: Balancing between capturing relevant information and avoiding information overload. GNN architecture: Choosing the right number of layers and attention mechanisms for effective information propagation. Sample efficiency vs. computational complexity: Weighing the benefits of improved performance against the increased computational cost of communication.

- Failure signatures: Poor performance in tasks requiring significant cooperation or coordination. Inability to capture relevant information due to limited communication range or GNN architecture. Increased computational cost without corresponding performance gains.

- First 3 experiments: 1. Evaluate CoDreamer's performance on a simple cooperative task with vector-based observations and a small number of agents. 2. Test CoDreamer's ability to handle partial observability by introducing communication constraints or limited observation ranges. 3. Assess CoDreamer's scalability by increasing the number of agents and the complexity of the environment.

## Open Questions the Paper Calls Out

The paper does not explicitly call out any open questions.

## Limitations

- Limited hyperparameter transparency: The paper lacks detailed specification of critical hyperparameters, particularly the GNN architecture configuration and communication range settings.
- Restricted task complexity: While CoDreamer shows strong performance on VMAS and Melting Pot tasks, these environments may not fully capture the complexity of real-world multi-agent scenarios.
- Computational overhead concerns: The paper does not provide runtime comparisons or computational complexity analysis between CoDreamer and baseline methods.

## Confidence

- High confidence in the core mechanism: The use of GNNs for two-level communication is well-grounded in existing literature and the architectural design is clearly specified.
- Medium confidence in scalability claims: While the paper demonstrates effectiveness on tasks with varying agent counts, the communication graph construction may face scalability challenges in larger systems.
- Low confidence in real-world applicability: Without testing on more complex, continuous control tasks or real-world environments, it's difficult to assess how CoDreamer would perform outside controlled experimental settings.

## Next Checks

1. **Ablation study on communication frequency**: Test how performance changes when reducing communication frequency (e.g., communicating every N steps rather than every step) to assess the trade-off between communication overhead and performance gains.

2. **Scalability stress test**: Evaluate CoDreamer on environments with 20+ agents to identify performance degradation points and determine practical limits for the communication architecture.

3. **Continuous control benchmark**: Implement CoDreamer on continuous control tasks (e.g., multi-agent MuJoCo environments) to validate whether the discrete-action focused design generalizes to continuous action spaces.