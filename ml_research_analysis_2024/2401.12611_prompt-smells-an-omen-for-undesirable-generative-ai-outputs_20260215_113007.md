---
ver: rpa2
title: 'Prompt Smells: An Omen for Undesirable Generative AI Outputs'
arxiv_id: '2401.12611'
source_url: https://arxiv.org/abs/2401.12611
tags:
- prompt
- genai
- output
- smells
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the concept of "prompt smells" to identify
  undesirable characteristics in prompts used with Generative AI (GenAI) models that
  can lead to poor-quality outputs. The authors define "desirability" of GenAI outputs
  based on three factors: accuracy, format adherence, and relevance.'
---

# Prompt Smells: An Omen for Undesirable Generative AI Outputs

## Quick Facts
- arXiv ID: 2401.12611
- Source URL: https://arxiv.org/abs/2401.12611
- Authors: Krishna Ronanki; Beatriz Cabrero-Daniel; Christian Berger
- Reference count: 18
- Key outcome: Introduces "prompt smells" concept to identify undesirable prompt characteristics that lead to poor-quality GenAI outputs

## Executive Summary
This paper introduces the concept of "prompt smells" as a framework for identifying undesirable characteristics in prompts that can lead to poor-quality outputs from Generative AI models. The authors draw parallels between prompt smells and code smells from software engineering, suggesting that imprecise prompt engineering can negatively impact output quality. They define desirability of GenAI outputs based on three factors: accuracy, format adherence, and relevance. While the paper establishes theoretical foundations and scenarios for understanding prompt smells, it lacks empirical validation and concrete metrics for detection and measurement.

## Method Summary
The paper proposes a conceptual framework for identifying prompt smells without providing specific experimental methods or training procedures. It defines desirability of GenAI outputs through three factors: accuracy, format adherence, and relevance. The authors suggest using a BO3 (best of three) strategy to distinguish between prompt-related issues and model hallucinations. The minimum viable reproduction plan involves defining diverse prompts, generating outputs, and evaluating them based on the three desirability factors to identify patterns in prompts that consistently lead to poor-quality outputs.

## Key Results
- Establishes conceptual framework of "prompt smells" analogous to code smells in software engineering
- Defines desirability of GenAI outputs through three factors: accuracy, format adherence, and relevance
- Proposes BO3 strategy to distinguish between prompt issues and model hallucinations
- Highlights risks of automatic prompting potentially exacerbating prompt smells and extrinsic hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Identifying "prompt smells" helps improve GenAI output trustworthiness by making undesirable characteristics explicit
- Mechanism: Prompt smells act as diagnostic indicators of imprecise prompt engineering, allowing developers to trace issues back to specific prompt elements that lead to undesirable outputs
- Core assumption: Undesirable GenAI outputs stem from identifiable patterns in poorly constructed prompts that can be systematically detected
- Evidence anchors:
  - [abstract] "we propose the concept of 'prompt smells' and the adverse effects they are observed to have on the desirability of GenAI outputs"
  - [section] "Prompt smells are semantic or syntactic characteristics of a prompt instance resulting from (unintentionally) imprecise prompt engineering"
- Break condition: If the relationship between prompt structure and output quality is non-deterministic or if prompt smells cannot be reliably identified across different tasks

### Mechanism 2
- Claim: The three-factor definition of "desirability" (accuracy, format adherence, relevance) provides a comprehensive framework for evaluating GenAI outputs
- Mechanism: By explicitly defining what makes outputs desirable, developers can create consistent evaluation criteria and identify when prompt smells are causing specific quality issues
- Core assumption: Desirability can be meaningfully decomposed into three observable factors that correlate with user expectations
- Evidence anchors:
  - [abstract] "We define desirability as a quality property of GenAI outputs that depends on three factors: 1) The accuracy or correctness of the information in the output, 2) The adherence to the ideal format for the output, and 3) The relevance of the output to the specific context"
  - [section] "Consider the AI-generated code block presented in Figure 1: Accuracy or correctness refers to how well the output matches the reality or the truth"
- Break condition: If the three factors prove insufficient to capture all aspects of output quality or if they conflict with each other in evaluation

### Mechanism 3
- Claim: The "BO3 strategy" (best of three) helps distinguish between prompt-related issues and model hallucinations
- Mechanism: By comparing outputs across three independent runs, developers can identify whether undesirable outputs are due to prompt smells or model instability
- Core assumption: Hallucinations produce inconsistent outputs across runs, while prompt smells produce consistent but undesirable outputs
- Evidence anchors:
  - [section] "To detect such hallucinations, a BO3 (best of three) strategy [12] can be employed, which involves assessing the semantic consistency of the output across three different and independently conducted dialogue instances"
- Break condition: If model outputs remain inconsistent even when prompt smells are addressed, or if consistent undesirable outputs are not actually caused by prompt smells

## Foundational Learning

- Concept: Code smells and software engineering parallels
  - Why needed here: Understanding code smells provides the conceptual foundation for identifying prompt smells as similar quality indicators
  - Quick check question: How do code smells help developers identify problematic code patterns before they cause failures?

- Concept: Prompt engineering fundamentals
  - Why needed here: Engineers need to understand how prompt construction affects GenAI outputs to recognize when prompt smells are causing issues
  - Quick check question: What are the key differences between zero-shot, few-shot, and chain-of-thought prompting strategies?

- Concept: Output evaluation metrics
  - Why needed here: To assess whether outputs meet the three-factor desirability criteria and identify specific quality issues
  - Quick check question: How would you measure the "relevance" of a GenAI output to a specific task context?

## Architecture Onboarding

- Component map:
  - Prompt generation module → Prompt evaluation engine → Output quality assessment → Desirability scoring → Prompt smells catalog
  - Integration points: User interface for prompt input, GenAI model API, output visualization tools

- Critical path:
  1. User submits prompt
  2. System generates multiple prompt variations
  3. Each variation is evaluated for potential smells
  4. GenAI model processes prompts
  5. Outputs are scored against desirability criteria
  6. System identifies correlation between smells and output quality

- Design tradeoffs:
  - Comprehensive smell detection vs. evaluation speed
  - Strict vs. flexible definition of desirability factors
  - Automated vs. human-in-the-loop evaluation approaches

- Failure signatures:
  - False positives: Marking benign prompts as having smells
  - False negatives: Missing actual prompt smells
  - Inconsistent desirability scoring across similar outputs
  - High variance in output quality despite similar prompts

- First 3 experiments:
  1. Compare output quality from prompts with known smells vs. clean prompts on the same task
  2. Test BO3 strategy by running identical prompts multiple times and measuring output consistency
  3. Evaluate how different definitions of desirability factors affect overall system performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically identify and categorize prompt smells across different types of GenAI models (text, image, code generation)?
- Basis in paper: [explicit] The paper introduces the concept of prompt smells but does not provide a systematic methodology for identifying or categorizing them
- Why unresolved: The paper only provides conceptual definitions and examples but lacks empirical methods for detection and classification
- What evidence would resolve it: A comprehensive study that analyzes multiple GenAI models and develops a taxonomy of prompt smells with clear detection criteria

### Open Question 2
- Question: What specific metrics can be used to quantify the impact of prompt smells on output desirability?
- Basis in paper: [inferred] The paper defines desirability through accuracy, format adherence, and relevance but doesn't provide measurable metrics for evaluating prompt smells
- Why unresolved: The authors acknowledge that desirability is a quality property but don't specify how to measure the degradation caused by prompt smells
- What evidence would resolve it: Development and validation of quantitative metrics that correlate specific prompt smells with measurable decreases in output quality

### Open Question 3
- Question: How effective are current automatic prompting techniques in mitigating or exacerbating prompt smells?
- Basis in paper: [explicit] The paper specifically warns about automatic prompting potentially exacerbating prompt smells and extrinsic hallucinations
- Why unresolved: The paper identifies this as a risk but doesn't provide empirical evidence of how automatic prompting affects prompt smell emergence
- What evidence would resolve it: Comparative studies showing the relationship between automatic prompting frequency and the occurrence of prompt smells across different applications

## Limitations
- Lacks empirical validation of the prompt smells concept with no experimental data demonstrating improvements in output quality
- Three-factor desirability definition is subjective without standardized measurement criteria
- Does not specify which GenAI models were used for analysis, limiting generalizability

## Confidence
- Medium Confidence: The conceptual framework of prompt smells as analogous to code smells is well-reasoned and provides a useful lens for understanding prompt quality issues
- Low Confidence: No empirical evidence that the proposed framework can be effectively implemented or that it leads to measurable improvements in output quality

## Next Checks
1. Design an experiment comparing outputs from prompts with identified "prompt smells" versus "clean" prompts on the same tasks, measuring improvements in the three desirability factors when smells are addressed
2. Test the prompt smells framework across multiple GenAI models (different architectures and sizes) to assess whether the identified patterns are consistent or model-specific
3. Conduct a study where multiple evaluators independently assess the same GenAI outputs using the three-factor desirability criteria to measure consistency and identify potential subjectivity issues in the evaluation framework