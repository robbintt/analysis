---
ver: rpa2
title: Dual Active Learning for Reinforcement Learning from Human Feedback
arxiv_id: '2410.02504'
source_url: https://arxiv.org/abs/2410.02504
tags:
- learning
- reward
- have
- policy
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning large language models
  with human preferences in reinforcement learning from human feedback (RLHF). The
  core problem is that human feedback is costly and time-consuming, while teachers
  have varying levels of expertise that impact the quality of their feedback.
---

# Dual Active Learning for Reinforcement Learning from Human Feedback

## Quick Facts
- arXiv ID: 2410.02504
- Source URL: https://arxiv.org/abs/2410.02504
- Reference count: 40
- Key outcome: Dual active learning algorithm for RLHF that improves reward estimation accuracy by strategically selecting conversations and teachers, achieving 1.77%-9.06% better accuracy than state-of-the-art methods

## Executive Summary
This paper tackles the challenge of aligning large language models with human preferences in reinforcement learning from human feedback (RLHF) by addressing the high cost and time requirements of human feedback. The authors propose a dual active learning algorithm that simultaneously selects both conversations (prompts and completions) and teachers for feedback collection. The method leverages D-optimal design principles to maximize reward estimator accuracy and employs pessimistic reinforcement learning to handle distributional shifts between behavior and optimal policies.

The theoretical analysis establishes that the proposed reward estimator achieves minimal generalized variance asymptotically, with the sub-optimality of the learned policy scaling as O(1/√T) with sample budget T. Empirical results on public LLM datasets demonstrate significant improvements in reward accuracy compared to existing methods, validating the effectiveness of the dual selection approach in real-world RLHF scenarios.

## Method Summary
The paper introduces a dual active learning algorithm for RLHF that operates through two complementary selection processes: conversation selection and teacher selection. The conversation selection component uses D-optimal design principles to identify the most informative samples for reward estimation, maximizing the determinant of the Fisher information matrix. The teacher selection component models teacher expertise distributions and strategically allocates feedback requests to teachers based on their relative reliability. The algorithm combines these selections to create an optimal feedback collection strategy that minimizes the variance of the reward estimator. To handle the distributional shift between the behavior policy and the optimal policy, the method incorporates pessimistic reinforcement learning, which provides theoretical guarantees on policy performance even when the learned policy differs from the behavior policy.

## Key Results
- The proposed dual active learning algorithm achieves 1.77%-9.06% improvement in reward accuracy compared to state-of-the-art methods on public LLM datasets
- Theoretical results establish that the reward estimator achieves minimal generalized variance asymptotically under the proposed selection strategy
- The sub-optimality of the learned policy scales as O(1/√T) with sample budget T, providing a clear sample complexity guarantee

## Why This Works (Mechanism)
The dual active learning approach works by addressing two critical challenges in RLHF simultaneously: the high cost of human feedback and the varying expertise levels among teachers. By using D-optimal design principles, the algorithm identifies the most informative conversations that provide maximum information gain for reward estimation, reducing the number of feedback requests needed. The teacher selection component ensures that feedback requests are allocated to the most reliable teachers for specific types of conversations, leveraging heterogeneity in expertise. The pessimistic reinforcement learning component provides robustness against distributional shifts, ensuring that the learned policy performs well even when there are differences between the behavior and optimal policies. This combination of strategic sample selection and robust policy learning creates a more efficient and reliable RLHF system.

## Foundational Learning
- **D-optimal design principles**: Why needed - to maximize information gain from limited feedback samples; Quick check - verify that selected samples have high determinant of Fisher information matrix
- **Teacher expertise modeling**: Why needed - to account for varying reliability of different human feedback providers; Quick check - ensure teacher selection probabilities reflect true expertise levels
- **Distributional shift handling**: Why needed - to ensure learned policy performs well when behavior and optimal policies differ; Quick check - verify pessimistic bound holds under different shift scenarios
- **Generalized variance minimization**: Why needed - to achieve optimal statistical efficiency in reward estimation; Quick check - confirm asymptotic variance reaches theoretical lower bound
- **Sample complexity analysis**: Why needed - to understand how performance scales with available feedback budget; Quick check - verify O(1/√T) scaling in empirical results
- **Active learning theory**: Why needed - to provide theoretical foundation for sample selection strategies; Quick check - ensure theoretical guarantees align with empirical performance

## Architecture Onboarding
- **Component map**: Data pipeline -> D-optimal selection -> Teacher expertise model -> Feedback collection -> Reward estimator -> Pessimistic RL -> Policy update
- **Critical path**: The sequence from data pipeline through D-optimal selection to reward estimator and policy update is critical, as errors in sample selection propagate through the entire learning process
- **Design tradeoffs**: The algorithm trades computational complexity in the selection phase for improved statistical efficiency and reduced feedback requirements; the pessimistic RL component adds conservatism that may slow convergence but provides robustness
- **Failure signatures**: Poor teacher expertise modeling leads to suboptimal teacher selection and degraded reward accuracy; inadequate D-optimal design results in sample inefficiency and higher variance estimates; failure to handle distributional shift causes policy degradation in deployment
- **3 first experiments**: 1) Test D-optimal selection on synthetic datasets with known optimal samples to verify selection accuracy; 2) Evaluate teacher selection performance with simulated expertise distributions; 3) Measure policy performance under varying levels of distributional shift

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical framework relies on strong assumptions about feature representation and teacher expertise modeling that may not hold in practice
- The D-optimal design approach faces practical challenges in high-dimensional feature spaces common in LLM applications
- Empirical validation is limited to public LLM datasets, which may not capture real-world feedback scenario complexities

## Confidence
- Theoretical guarantees for D-optimal design and pessimistic RL: **High**
- Asymptotic optimality claims: **Medium** (due to simplifying assumptions)
- Empirical improvements over baselines: **Medium** (limited scope and datasets)
- Practical applicability to real-world RLHF systems: **Low** (insufficient real-world testing)

## Next Checks
1. **Scale Test**: Implement the dual active learning algorithm on a larger, more diverse set of prompts and teachers to verify if the claimed improvements scale with sample size and teacher diversity
2. **Robustness Evaluation**: Test the algorithm's performance under varying levels of teacher expertise distribution and distributional shift scenarios to validate the theoretical assumptions about pessimistic RL
3. **Computational Analysis**: Conduct a detailed analysis of the computational overhead introduced by the dual selection process, including time and resource requirements for real-time deployment scenarios