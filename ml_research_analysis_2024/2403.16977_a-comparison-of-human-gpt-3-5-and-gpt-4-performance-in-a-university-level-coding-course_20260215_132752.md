---
ver: rpa2
title: A comparison of Human, GPT-3.5, and GPT-4 Performance in a University-Level
  Coding Course
arxiv_id: '2403.16977'
source_url: https://arxiv.org/abs/2403.16977
tags: []
core_contribution: This study compared university student coding assignments against
  AI-generated submissions from GPT-3.5 and GPT-4, both with and without prompt engineering,
  in a university-level physics coding course. The AI submissions were marked blindly
  by three independent evaluators, yielding 300 total data points.
---

# A comparison of Human, GPT-3.5, and GPT-4 Performance in a University-Level Coding Course

## Quick Facts
- arXiv ID: 2403.16977
- Source URL: https://arxiv.org/abs/2403.16977
- Reference count: 20
- Primary result: Students achieved 91.9% average score, significantly outperforming AI models (p = 2.482 × 10^-10)

## Executive Summary
This study compared university student coding assignments against AI-generated submissions from GPT-3.5 and GPT-4, both with and without prompt engineering, in a university-level physics coding course. The AI submissions were marked blindly by three independent evaluators, yielding 300 total data points. Students achieved an average score of 91.9% (SE:0.4), significantly outperforming the highest AI category (GPT-4 with prompt engineering) at 81.1% (SE:0.8), with a statistically significant difference (p = 2.482 × 10^-10). Prompt engineering significantly improved AI performance, raising GPT-4 scores from 71.9% to 81.1% (p = 1.661 × 10^-4) and GPT-3.5 from 30.9% to 48.8% (p = 4.967 × 10^-9). Markers correctly identified authorship, with 92.1% accuracy for human-authored work and 85.3% overall binary classification accuracy.

## Method Summary
The study employed a blind marking methodology where three independent evaluators scored 50 student assignments alongside 250 AI-generated submissions (100 each from GPT-3.5 and GPT-4, with and without prompt engineering). All submissions were presented without authorship information, creating 300 data points for analysis. Statistical significance was assessed using appropriate tests for comparing means across multiple groups.

## Key Results
- Students significantly outperformed all AI categories with average score of 91.9% versus 81.1% for best AI performance
- Prompt engineering dramatically improved AI performance (GPT-4: 71.9% to 81.1%, p = 1.661 × 10^-4)
- Human markers could distinguish AI from human code with 85.3% accuracy overall
- The performance gap between human and AI submissions was statistically significant (p = 2.482 × 10^-10)

## Why This Works (Mechanism)
The study demonstrates that human programmers possess contextual understanding and problem-solving approaches that current AI models cannot fully replicate, even with prompt engineering. The significant performance gap suggests that human cognitive processes involved in coding extend beyond pattern matching and include deeper conceptual understanding, creative problem-solving, and nuanced application of physics principles to coding tasks.

## Foundational Learning
The results indicate that foundational programming skills and physics knowledge remain distinctively human advantages in educational contexts. The marked improvement from prompt engineering suggests that AI performance is highly dependent on instruction quality, while human performance appears more consistent and less reliant on external guidance structures.

## Architecture Onboarding
This study provides insights into the limitations of current AI architectures in educational settings, particularly for complex, domain-specific tasks. The performance differential between GPT-3.5 and GPT-4 highlights the architectural improvements in newer models, though both fall short of human capabilities in this context.

## Open Questions the Paper Calls Out
None explicitly stated in the paper, though the results raise questions about the long-term trajectory of AI capabilities in educational assessment and the potential need for evolving evaluation methodologies as AI tools become more sophisticated.

## Limitations
- Sample size of 50 students may not represent broader populations across institutions
- Study focused specifically on physics coding assignments, limiting generalizability
- Prompt engineering methodology lacked full transparency in documentation
- Limited to two AI models without comparison to other contemporary models

## Confidence
- Human performance superiority: High confidence
- Prompt engineering effectiveness: High confidence
- AI distinguishability: Medium confidence
- Generalizability across contexts: Low confidence

## Next Checks
1. Replicate the study with a larger, more diverse student sample across multiple institutions and programming languages
2. Test additional AI models (e.g., Claude, Llama, Gemini) with standardized prompt engineering protocols
3. Conduct a blind evaluation study where markers identify distinguishing features between human and AI-generated code, followed by qualitative analysis of their reasoning