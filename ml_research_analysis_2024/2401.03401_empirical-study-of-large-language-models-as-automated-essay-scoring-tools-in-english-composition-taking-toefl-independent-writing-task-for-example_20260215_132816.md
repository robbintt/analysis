---
ver: rpa2
title: Empirical Study of Large Language Models as Automated Essay Scoring Tools in
  English Composition__Taking TOEFL Independent Writing Task for Example
arxiv_id: '2401.03401'
source_url: https://arxiv.org/abs/2401.03401
tags: []
core_contribution: The study investigates the effectiveness of ChatGPT as an automated
  essay scoring (AES) tool using TOEFL independent writing tasks. It designs prompts
  based on official TOEFL scoring criteria and evaluates ChatGPT's scoring performance
  using Quadratic Weighted Kappa (QWK) metrics.
---

# Empirical Study of Large Language Models as Automated Essay Scoring Tools in English Composition__Taking TOEFL Independent Writing Task for Example

## Quick Facts
- arXiv ID: 2401.03401
- Source URL: https://arxiv.org/abs/2401.03401
- Authors: Wei Xia; Shaoguang Mao; Chanjing Zheng
- Reference count: 2
- Primary result: ChatGPT achieves QWK values of 0.4636 to 0.82930 and 84.375% accuracy in identifying off-topic essays on TOEFL tasks

## Executive Summary
This study investigates ChatGPT's effectiveness as an automated essay scoring (AES) tool using TOEFL independent writing tasks. The researchers designed prompts based on official TOEFL scoring criteria and evaluated ChatGPT's scoring performance using Quadratic Weighted Kappa (QWK) metrics. The study demonstrates that ChatGPT can function as an operational AES tool, with the best performance (QWK=0.8293) achieved when prompts combine basic task description, scoring criteria, and single example essays. However, the study also identifies a regression effect where scores tend to cluster around the middle range, and highlights the importance of careful prompt design and domain expertise for optimal performance.

## Method Summary
The study employed a comparative evaluation approach using TOEFL independent writing tasks as the test domain. Researchers designed multiple prompt structures based on official TOEFL scoring rubrics and evaluated ChatGPT's performance using QWK metrics. The dataset consisted of 192 TOEFL essays, and the evaluation focused on measuring agreement between ChatGPT's scores and human raters. Various prompt configurations were tested, including different combinations of task descriptions, scoring criteria, and example essays. The study also examined ChatGPT's ability to identify off-topic essays and analyzed scoring patterns to identify potential regression effects.

## Key Results
- ChatGPT achieved QWK values ranging from 0.4636 to 0.82930 across different prompt configurations
- 84.375% accuracy in identifying off-topic essays
- Best performance (QWK=0.8293) achieved with prompts combining basic task description, scoring criteria, and single example essays
- Identified regression effect where scores cluster around middle range (3-4)
- Demonstrates operational viability of ChatGPT as AES tool with appropriate prompt design

## Why This Works (Mechanism)
Unknown: The paper does not provide explicit theoretical explanations for why ChatGPT performs as an AES tool. The effectiveness appears to stem from ChatGPT's ability to understand and apply scoring rubrics when properly prompted, though the underlying mechanisms of how the model processes and applies these criteria are not explored in depth.

## Foundational Learning
Unknown: The paper does not discuss foundational learning principles or how ChatGPT's pre-training might contribute to its AES capabilities. The study focuses primarily on prompt engineering and empirical evaluation rather than exploring the model's underlying learning mechanisms.

## Architecture Onboarding
Unknown: The paper does not provide details about ChatGPT's architecture or how it might be adapted or fine-tuned for AES tasks. The study treats ChatGPT as a black box and focuses on prompt design rather than architectural modifications.

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly identify specific open questions for future research. The limitations section suggests areas for improvement but does not frame them as open research questions.

## Limitations
- Limited dataset scope of 192 TOEFL essays may not capture full diversity of writing styles and proficiency levels
- Manual prompt engineering approach may not generalize to other AES applications or scoring rubrics
- Regression effect identified where scores cluster around middle range (3-4), limiting differentiation between high and low performers
- Limited validation methodology without performance across multiple iterations or different random seeds

## Confidence
- ChatGPT's operational viability as AES tool: Medium confidence
- Prompt structure impact: High confidence
- Domain expertise importance: High confidence
- Limited dataset scope: Medium confidence
- Prompt design constraints: Medium confidence
- Regression effect concerns: High confidence
- Limited validation methodology: Low confidence

## Next Checks
1. Cross-dataset validation: Test the same prompt structures on multiple independent TOEFL writing datasets and other standardized writing assessments to verify generalizability
2. Iterative stability testing: Run the same scoring tasks multiple times with identical prompts and different random seeds to quantify performance variance and reproducibility
3. Bias and fairness analysis: Conduct systematic analysis of potential demographic biases by correlating scoring patterns with known author characteristics across diverse writing samples