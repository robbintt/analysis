---
ver: rpa2
title: Distortions in Judged Spatial Relations in Large Language Models
arxiv_id: '2401.04218'
source_url: https://arxiv.org/abs/2401.04218
tags:
- spatial
- llms
- direction
- arxiv
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study introduces a benchmark to evaluate large language models'\
  \ (LLMs) spatial reasoning, focusing on hierarchical spatial bias\u2014where judgments\
  \ about individual locations are influenced by the perceived relationships of the\
  \ larger groups they belong to. Using 14 questions involving intercardinal directions\
  \ between well-known American cities, the benchmark tests three LLMs: GPT-3.5 (47.3%\
  \ accuracy), GPT-4 (55.3%), and Llama-2 (44.7%)."
---

# Distortions in Judged Spatial Relations in Large Language Models

## Quick Facts
- **arXiv ID**: 2401.04218
- **Source URL**: https://arxiv.org/abs/2401.04218
- **Reference count**: 39
- **Primary result**: LLM spatial reasoning exhibits hierarchical spatial bias, with accuracy ranging from 44.7% to 55.3% across tested models

## Executive Summary
This study introduces a benchmark to evaluate large language models' spatial reasoning capabilities, focusing on hierarchical spatial bias where judgments about individual locations are influenced by perceived relationships of larger groups. Using 14 questions about intercardinal directions between well-known American cities, the benchmark tests three LLMs: GPT-3.5 (47.3% accuracy), GPT-4 (55.3%), and Llama-2 (44.7%). The findings reveal that while GPT-4 outperforms the others, all models exhibit significant spatial reasoning errors that reflect human-like misconceptions. The study highlights the need for improving LLMs' spatial reasoning through better training data and architectural enhancements.

## Method Summary
The benchmark evaluates LLMs' ability to determine intercardinal directions between well-known American cities using a zero-shot prompting approach. The study presents 14 questions split into 7 with suspected hierarchical bias and 7 without, involving cities like Dallas, San Diego, Memphis, and Toronto. Each model (GPT-3.5, GPT-4, and Llama-2) answers each question 10 times with model reset between attempts to ensure zero-shot conditions. The evaluation compares model responses to ground truth directions and analyzes accuracy patterns, particularly examining whether hierarchical biases affect performance differently than non-biased questions.

## Key Results
- GPT-4 achieved the highest accuracy at 55.3%, followed by GPT-3.5 at 47.3% and Llama-2 at 44.7%
- GPT-4 showed significantly better performance on non-hierarchical bias questions (85.7% accuracy) compared to hierarchical bias questions (32.9%)
- All models frequently identified the nearest cardinal direction when incorrect, reflecting associative learning rather than precise spatial computation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs rely on associative learning from textual data rather than internal spatial computation, leading to hierarchical spatial biases.
- Mechanism: LLMs learn patterns and associations from text where human spatial misconceptions are encoded. When asked spatial questions, they retrieve and apply these learned associations instead of performing precise spatial calculations. This results in answers that reflect human-like misconceptions about geography.
- Core assumption: The training data contains textual descriptions of geography that include human biases and oversimplifications.
- Evidence anchors:
  - [abstract] The models "embodied human-like misconceptions" and identified "the nearest cardinal direction in most cases, reflecting their associative learning mechanism."
  - [section] "Their performance reflects a reliance on associative learning from the textual data in their training sets, which includes human-like biases and misconceptions."
  - [corpus] Weak evidence - neighboring papers focus on general LLM reasoning and relation extraction but do not specifically address hierarchical spatial bias in geography.

### Mechanism 2
- Claim: Hierarchical categorization in human cognition translates into textual descriptions that influence LLM spatial reasoning.
- Mechanism: Humans tend to organize spatial information hierarchically (e.g., cities within states, states within countries). This organizational bias appears in text, causing people to describe spatial relationships based on these larger groupings rather than precise coordinates. LLMs learn these patterns and apply them when reasoning about spatial relationships.
- Core assumption: The way humans write about geography reflects their hierarchical cognitive organization of space.
- Evidence anchors:
  - [section] "we hypothesize that they may exhibit similar biases, based on three considerations... the biases in human spatial reasoning... might find their way into textual descriptions."
  - [section] "human spatial memory tends to be organized hierarchically... When evaluating spatial relationships... the perceived spatial relationship between the larger groups can skew the judgment about the actual relationship between the individual points."
  - [corpus] No direct evidence - neighboring papers do not address hierarchical categorization in spatial reasoning.

### Mechanism 3
- Claim: Model size and training data volume influence the accuracy of spatial reasoning tasks.
- Mechanism: Larger models with more parameters and training data can store and retrieve more spatial information, leading to better performance on straightforward spatial tasks. However, even larger models still exhibit hierarchical biases because these biases are encoded in the training data itself.
- Core assumption: More parameters and training data enable better pattern recognition and information retrieval.
- Evidence anchors:
  - [section] "GPT-4, with its 1 trillion parameters, significantly exceeds GPT-3.5's capacity of 175 billion parameters" and "GPT-4 shows the highest accuracy (55.3 percent), followed by GPT-3.5 (47.3 percent)."
  - [section] "Models with more parameters are able to assimilate and maintain more information, potentially enhancing their performance across various assessments."
  - [corpus] Weak evidence - neighboring papers discuss general LLM capabilities but don't specifically address how model size affects spatial reasoning accuracy.

## Foundational Learning

- Concept: Hierarchical spatial bias
  - Why needed here: Understanding how humans organize spatial information into categories is crucial for interpreting why LLMs make certain errors in spatial reasoning tasks.
  - Quick check question: When asked about the direction from San Diego to Reno, why might people (and LLMs) incorrectly answer "northeast" when the correct answer is "northwest"?

- Concept: Intercardinal directions
  - Why needed here: The benchmark specifically tests LLMs' ability to identify intercardinal directions (e.g., northeast, southwest) between cities, which requires understanding of both cardinal directions and the concept of intercardinal points.
  - Quick check question: What is the intercardinal direction between due north and due east?

- Concept: Zero-shot learning
  - Why needed here: The benchmark used a "zero-shot" approach where models answered questions without prior examples, which is important for understanding the models' intrinsic spatial reasoning capabilities.
  - Quick check question: What does it mean when a model performs a task "zero-shot" versus "few-shot"?

## Architecture Onboarding

- Component map: Question formulation -> LLM query (zero-shot) -> Answer retrieval -> Accuracy evaluation -> Bias pattern analysis
- Critical path: Question → LLM query (reset to zero-shot) → Answer retrieval → Accuracy evaluation → Analysis of bias patterns
- Design tradeoffs: Using well-known American cities ensures the models have likely seen this information in training, but limits generalizability to less-documented locations. The zero-shot approach tests intrinsic capabilities but doesn't leverage the models' few-shot learning strengths.
- Failure signatures: Consistent directional errors that follow hierarchical patterns (e.g., always placing cities within a state relative to the state's perceived orientation), or answers that correctly identify the nearest cardinal direction even when the intercardinal direction is wrong.
- First 3 experiments:
  1. Test with cities in the same small region but different states to see if hierarchical bias emerges at the state level
  2. Test with cities in different countries to examine if the bias extends to country-level categorization
  3. Test with cities where the ground truth direction aligns with the hierarchical bias to see if accuracy improves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on spatial reasoning tasks involving less-documented locations like rural areas and smaller towns, especially in underdeveloped countries?
- Basis in paper: [inferred] The authors mention this as a limitation of their study and suggest that future work should include more diverse locations to better understand LLM spatial reasoning capabilities.
- Why unresolved: The current study focused on major US cities, so the performance on less-documented locations remains unknown.
- What evidence would resolve it: Testing LLMs on a benchmark with diverse locations including rural areas and smaller towns, especially in underdeveloped countries, and comparing the results to the current study.

### Open Question 2
- Question: Can enhancing the training data with high-quality geographic information and detailed descriptions of entities improve LLM performance on spatial reasoning tasks?
- Basis in paper: [explicit] The authors suggest that high-quality data and detailed descriptions could improve model performance.
- Why unresolved: The current study does not explore the impact of enhanced training data on LLM spatial reasoning.
- What evidence would resolve it: Training LLMs on datasets with improved geographic information and detailed descriptions, then testing their performance on spatial reasoning tasks and comparing to the current study.

### Open Question 3
- Question: How effective would incorporating spatial reasoning capabilities and aligning representations of different modalities (like geo-tagged texts and remote sensing images) be in addressing spatial biases and limited geospatial capabilities in LLMs?
- Basis in paper: [explicit] The authors suggest this as a potential approach to improve LLM spatial reasoning.
- Why unresolved: The current study does not investigate the impact of enhanced model architecture on LLM spatial reasoning.
- What evidence would resolve it: Developing LLMs with enhanced spatial reasoning capabilities and multimodal representations, then testing their performance on spatial reasoning tasks and comparing to the current study.

## Limitations

- The study focuses exclusively on well-known American cities, limiting generalizability to less-documented geographic locations and different cultural contexts
- Zero-shot prompting approach doesn't leverage LLMs' few-shot learning capabilities, potentially underestimating their spatial reasoning potential
- Analysis doesn't account for potential variations in model performance due to different temperature settings or other hyperparameters during inference

## Confidence

- **High Confidence**: The identification of hierarchical spatial bias as a consistent error pattern across all three tested models (GPT-3.5, GPT-4, Llama-2). The quantitative results showing GPT-4's superior performance (55.3% accuracy) compared to GPT-3.5 (47.3%) and Llama-2 (44.7%) are well-supported by the experimental data.
- **Medium Confidence**: The attribution of spatial errors to associative learning mechanisms rather than true spatial computation. While the evidence is compelling, alternative explanations (such as tokenization effects or prompt sensitivity) cannot be entirely ruled out.
- **Low Confidence**: The claim that model size directly correlates with spatial reasoning accuracy. The study only compares three models with vastly different parameter counts, and other factors like training data composition or architectural differences could contribute to performance differences.

## Next Checks

1. **Cross-Geographic Validation**: Test the same benchmark with cities from different countries and continents to determine if hierarchical spatial bias patterns persist across diverse geographic contexts and cultural perspectives on spatial organization.

2. **Few-Shot Learning Assessment**: Repeat the benchmark using few-shot prompting with spatial reasoning examples to evaluate whether providing spatial reasoning templates improves accuracy and reduces hierarchical bias.

3. **Training Data Analysis**: Analyze the training data for geographic descriptions to quantify the prevalence of hierarchical spatial descriptions versus precise coordinate-based descriptions, establishing a stronger link between training data composition and model behavior.