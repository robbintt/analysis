---
ver: rpa2
title: 'FinLLMs: A Framework for Financial Reasoning Dataset Generation with Large
  Language Models'
arxiv_id: '2401.10744'
source_url: https://arxiv.org/abs/2401.10744
tags: []
core_contribution: FinLLMs is a framework for generating financial question-answering
  datasets using large language models. It addresses the high cost of manual annotation
  in the financial domain by automatically generating synthetic data based on common
  financial formulas.
---

# FinLLMs: A Framework for Financial Reasoning Dataset Generation with Large Language Models

## Quick Facts
- **arXiv ID**: 2401.10744
- **Source URL**: https://arxiv.org/abs/2401.10744
- **Reference count**: 40
- **Key outcome**: FinLLMs generates synthetic financial QA data using formula graphs and LLMs, improving model performance by at least 2% on FinQA and TAT-QA benchmarks.

## Executive Summary
FinLLMs addresses the high cost of manual annotation in financial reasoning by automatically generating synthetic QA datasets. The framework constructs a graph of financial formulas and variables, augments the formula set through graph traversal, and uses GPT-3.5 to generate QA pairs with supporting tables and text. Experiments demonstrate that FinLLMs-synthesized data consistently improves execution accuracy and program accuracy across multiple model variants when compared to human-labeled data alone, offering a cost-effective solution for expanding training data in the financial domain.

## Method Summary
FinLLMs constructs a graph where nodes represent financial formulas and edges represent variable dependencies. Through graph traversal, it identifies and combines formulas that share variables to create new reasoning patterns. The framework then uses GPT-3.5 to generate financial tables and text based on these formulas, extracts variables from the generated content, and computes answers using DSL programs. This synthetic data is combined with existing benchmarks to train financial reasoning models, resulting in improved performance compared to models trained on human-labeled data alone.

## Key Results
- FinLLMs-synthesized data improves execution accuracy by at least 2% across multiple model variants compared to human-labeled data
- The framework effectively enhances performance on established financial QA benchmarks (FinQA and TAT-QA)
- Graph traversal augmentation increases the variety of financial reasoning formulas without manual annotation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Graph traversal augmentation increases the variety of financial reasoning formulas without manual annotation.
- **Mechanism**: By treating variables as nodes and their dependencies as edges, the framework identifies and merges formulas that share variables. Traversing the graph iteratively generates new composite formulas by combining formulas along edges.
- **Core assumption**: The logical dependencies between financial formulas can be captured in a directed graph where nodes represent formulas and edges represent variable dependencies.
- **Evidence anchors**:
  - [section] "We then augment the set of formulas by combining those containing the same variables as new elements. Specifically, we explore and merge those formulas by traversing the constructed graph."
  - [section] "By traversing the edges in the constructed graph, we can identify pairs of formulas that can be combined."
- **Break condition**: If the graph traversal generates invalid or nonsensical formula combinations that cannot be resolved through filtering, the augmentation process would fail.

### Mechanism 2
- **Claim**: LLM-generated financial tables and text ensure that the synthetic QA pairs are both answerable and contextually relevant.
- **Mechanism**: GPT-3.5 generates tables based on dependent variables and time ranges, then generates text related to these tables. Variables are extracted from the generated content and substituted into DSL programs to compute answers.
- **Core assumption**: The LLM can generate coherent and consistent financial content that includes all necessary variables for the given formulas.
- **Evidence anchors**:
  - [section] "Finally, utilizing GPT -3.5, we generate financial question-answering data that encompasses both tabular information and long textual content, building on the collected formula set."
  - [section] "We generate multiple examples with table or text as supporting facts on the basis of each formula."
- **Break condition**: If the LLM fails to generate content containing the required variables or introduces inconsistencies between tables and text, the QA pairs become unanswerable.

### Mechanism 3
- **Claim**: Hybrid training data (synthetic + human-labeled) improves model performance more than either source alone.
- **Mechanism**: FinLLMs-generated data augments existing benchmarks (FinQA and TAT-QA), providing a larger and more diverse training set that enhances the model's ability to generalize across different financial reasoning scenarios.
- **Core assumption**: The synthetic data generated by FinLLMs is of sufficient quality and diversity to meaningfully complement the human-labeled data.
- **Evidence anchors**:
  - [section] "Our experiments show that FinLLMs-synthesized data effectively enhances the performance of several large-scale numerical reasoning models in the financial domain, outperforming two established benchmark financial question-answering datasets."
  - [section] "FinLLMs-synthesized data consistently enhances the execution accuracy and program accuracy by at least 2% across multiple model variants when compared with the accuracy obtained using only human-labeled data."
- **Break condition**: If the synthetic data introduces significant noise or biases not present in the human-labeled data, the hybrid training could degrade model performance.

## Foundational Learning

- **Concept**: Domain-Specific Language (DSL) for financial reasoning programs
  - **Why needed here**: DSL provides a structured way to represent financial calculations that can be executed to compute answers, ensuring consistency and correctness.
  - **Quick check question**: Can you explain how a DSL program like "add[year1,year2]#years" would compute an average?

- **Concept**: Graph traversal for formula augmentation
  - **Why needed here**: Graph traversal allows systematic exploration of formula combinations based on shared variables, creating new reasoning patterns without manual effort.
  - **Quick check question**: If formula A depends on variable X and formula B uses X as an input, what new formula could be created by combining them?

- **Concept**: Large language model prompting for structured data generation
  - **Why needed here**: LLMs need carefully designed prompts to generate consistent financial tables and text that align with the formulas being used.
  - **Quick check question**: What key elements should be included in a prompt to ensure the LLM generates a table containing specific financial variables?

## Architecture Onboarding

- **Component map**: Formula collection → DSL conversion → Financial reasoning graph construction → Graph traversal and formula augmentation → LLM-based content generation → Variable extraction → Answer computation → Dataset formatting

- **Critical path**: Formula collection → Graph construction → Graph traversal → LLM generation → Variable extraction → Answer computation → Dataset formatting

- **Design tradeoffs**:
  - Formula complexity vs. LLM generation reliability
  - Graph traversal depth vs. computational efficiency
  - Prompt specificity vs. generation flexibility
  - Synthetic data quantity vs. quality control

- **Failure signatures**:
  - Missing variables in generated tables/text (unanswerable questions)
  - Inconsistent values between tables and text (calculation errors)
  - Invalid formula combinations from graph traversal (nonsensical reasoning paths)
  - LLM generation failures or hallucinations

- **First 3 experiments**:
  1. Test graph traversal with a small set of formulas to verify new formula generation and filtering logic
  2. Generate a single QA pair using LLM with controlled inputs to validate variable extraction and answer computation
  3. Train a simple model on a small FinLLMs dataset and evaluate on a subset of FinQA to confirm performance improvement

## Open Questions the Paper Calls Out
None

## Limitations
- The framework relies heavily on the quality and coverage of the initial formula collection
- The LLM-generated content quality is critical - hallucinations or inconsistencies between tables and text could introduce noise into the training data
- The generalizability of the approach to financial domains beyond those covered by the seed formulas is uncertain

## Confidence

* **High confidence**: The graph traversal mechanism for formula augmentation is well-founded and the experimental results showing performance improvements (2%+ accuracy gains) are robust.
* **Medium confidence**: The LLM prompting strategy for generating financial tables and text is reasonable but may face challenges with complex or domain-specific terminology that could affect variable extraction reliability.
* **Low confidence**: The generalizability of the approach to financial domains beyond those covered by the seed formulas, as the framework's effectiveness depends on having appropriate initial formula coverage.

## Next Checks

1. Conduct ablation studies to quantify the contribution of synthetic data versus human-labeled data at different mixing ratios, determining the optimal balance for performance gains.

2. Test the framework with an expanded set of seed formulas from additional financial domains (e.g., insurance, real estate) to evaluate whether the graph traversal mechanism maintains effectiveness across diverse financial reasoning tasks.

3. Implement automated quality checks for LLM-generated content, measuring the consistency between tables and text, and the presence rate of required variables, to quantify the noise introduced by the generation process.