---
ver: rpa2
title: 'RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences'
arxiv_id: '2402.17257'
source_url: https://arxiv.org/abs/2402.17257
tags:
- rime
- reward
- learning
- preferences
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RIME tackles the robustness issue in preference-based reinforcement
  learning (PbRL) under noisy preference feedback. It introduces a denoising discriminator
  that dynamically filters corrupted preferences using a KL divergence threshold,
  augmented with an uncertainty term to handle distribution shifts.
---

# RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences

## Quick Facts
- **arXiv ID:** 2402.17257
- **Source URL:** https://arxiv.org/abs/2402.17257
- **Authors:** Jie Cheng; Gang Xiong; Xingyuan Dai; Qinghai Miao; Yisheng Lv; Fei-Yue Wang
- **Reference count:** 40
- **Primary result:** RIME outperforms state-of-the-art PbRL methods under up to 30% preference noise, with gains strongest in high-noise regimes.

## Executive Summary
RIME (Robust Preference-based Reinforcement Learning with Noisy Preferences) addresses the challenge of preference noise in preference-based reinforcement learning (PbRL). It introduces a denoising discriminator that dynamically filters corrupted preferences using KL divergence thresholds and an uncertainty term to handle distribution shifts. A warm-start mechanism initializes the reward model with intrinsic rewards to bridge the pre-training-to-online-training gap and reduce accumulated errors. Experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly outperforms existing PbRL methods under varying noise rates, particularly in high-noise regimes. Ablation studies confirm that both the denoising discriminator and warm-start components are critical to RIME’s performance.

## Method Summary
RIME tackles the robustness issue in preference-based reinforcement learning under noisy preference feedback. It introduces a denoising discriminator that dynamically filters corrupted preferences using a KL divergence threshold, augmented with an uncertainty term to handle distribution shifts. A warm-start mechanism initializes the reward model with intrinsic rewards to bridge the pre-training-to-online-training gap and reduce accumulated errors. Experiments on robotic manipulation and locomotion tasks show RIME significantly outperforms state-of-the-art PbRL methods under varying error rates (up to 30% noise), with the strongest gains in high-noise regimes. Ablation studies confirm that both the denoising discriminator and warm start are critical to RIME’s performance. The method also generalizes well across different teacher types and performs robustly with real non-expert human feedback.

## Key Results
- RIME significantly outperforms state-of-the-art PbRL methods under up to 30% preference noise, with gains strongest in high-noise regimes.
- Ablation studies confirm that both the denoising discriminator and warm-start components are critical to RIME’s performance.
- RIME generalizes well across different teacher types and performs robustly with real non-expert human feedback.

## Why This Works (Mechanism)
RIME’s effectiveness stems from its dual approach to handling preference noise: a denoising discriminator that dynamically filters corrupted preferences using KL divergence thresholds and an uncertainty term, and a warm-start mechanism that initializes the reward model with intrinsic rewards to reduce accumulated errors. The denoising discriminator helps maintain reward model accuracy by filtering out noisy preferences, while the warm-start mechanism ensures a stable starting point for the reward model, reducing the impact of noise over time. Together, these components enable RIME to maintain robust performance even under high levels of preference noise.

## Foundational Learning
- **Preference-based reinforcement learning (PbRL):** A framework where agents learn from human preferences over pairs of trajectories rather than explicit reward signals. Why needed: Enables learning from human feedback without requiring explicit reward design. Quick check: Does the agent learn to mimic the preferred behavior in the presence of noise?
- **Denoising discriminator:** A component that filters out corrupted preferences using KL divergence thresholds and an uncertainty term. Why needed: To maintain reward model accuracy by removing noisy preferences. Quick check: Does the discriminator effectively reduce the impact of noise on the reward model?
- **Warm-start mechanism:** Initializes the reward model with intrinsic rewards to bridge the pre-training-to-online-training gap. Why needed: To provide a stable starting point and reduce accumulated errors over time. Quick check: Does the warm-start mechanism improve initial performance and reduce error accumulation?
- **KL divergence threshold:** A measure used to filter preferences based on their similarity to the current reward model’s predictions. Why needed: To dynamically identify and remove noisy preferences. Quick check: Does the threshold effectively balance noise filtering and preference retention?
- **Uncertainty term:** An additional component in the denoising discriminator to handle distribution shifts. Why needed: To improve robustness to changes in preference distributions. Quick check: Does the uncertainty term enhance performance under distribution shifts?

## Architecture Onboarding

### Component Map
Reward Model (initialized with intrinsic rewards) -> Denoising Discriminator (filters preferences using KL divergence and uncertainty) -> Preference Feedback -> Reward Model Update -> Policy Update

### Critical Path
1. Initialize reward model with intrinsic rewards (warm start).
2. Collect trajectory pairs and human preferences.
3. Denoising discriminator filters preferences using KL divergence and uncertainty.
4. Filtered preferences update the reward model.
5. Reward model guides policy optimization.

### Design Tradeoffs
- **Noise filtering vs. preference retention:** The KL divergence threshold must balance removing noisy preferences without discarding too much useful information.
- **Warm start vs. flexibility:** Using intrinsic rewards for initialization provides stability but may limit the model’s ability to adapt to new preferences.

### Failure Signatures
- High noise levels overwhelm the denoising discriminator, leading to reward model degradation.
- Poor choice of KL divergence threshold results in either excessive noise retention or loss of useful preferences.
- Intrinsic rewards poorly aligned with true preferences may bias the warm start.

### 3 First Experiments
1. Test RIME on a simple task (e.g., CartPole) with synthetic noise to verify basic functionality.
2. Compare RIME’s performance with and without the denoising discriminator under varying noise levels.
3. Evaluate the impact of different warm-start initialization strategies (e.g., random vs. intrinsic rewards).

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to high-dimensional or sparse-reward tasks is unclear, as experiments focus on controlled robotic manipulation and locomotion benchmarks.
- The assumption of Bernoulli noise may not generalize to more complex or structured noise patterns in real-world human feedback.
- Computational overhead from the denoising discriminator and warm-start mechanism is not quantified, limiting practical deployment insights.

## Confidence
- **High:** RIME’s effectiveness in reducing reward model error under synthetic noise (supported by ablation studies).
- **Medium:** Robustness to real human feedback (human study sample size and diversity not detailed).
- **Low:** Claims about long-term deployment stability (no continual learning or catastrophic forgetting analysis provided).

## Next Checks
1. Test RIME on high-dimensional visual inputs (e.g., Atari or DM Control Suite with image observations) to assess scalability.
2. Evaluate performance under structured noise models (e.g., context-dependent or adversarial preference corruption) beyond Bernoulli noise.
3. Measure wall-clock and sample efficiency overhead compared to baseline PbRL methods in identical experimental setups.