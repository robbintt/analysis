---
ver: rpa2
title: Block Verification Accelerates Speculative Decoding
arxiv_id: '2403.10444'
source_url: https://arxiv.org/abs/2403.10444
tags: []
core_contribution: This paper proposes a new block-level verification algorithm for
  speculative decoding that improves upon the standard token-by-token approach. The
  method formulates the draft verification step as a block-level optimal transport
  problem, enabling joint verification of entire blocks rather than independent token
  verification.
---

# Block Verification Accelerates Speculative Decoding

## Quick Facts
- arXiv ID: 2403.10444
- Source URL: https://arxiv.org/abs/2403.10444
- Reference count: 16
- Key outcome: Block-level verification algorithm improves speculative decoding wall-clock speedups by 5%-8% compared to token-level verification across multiple datasets and tasks

## Executive Summary
This paper introduces BlockVerify, a novel block-level verification algorithm for speculative decoding that treats the entire draft block as a single unit rather than verifying tokens independently. By formulating the verification step as a block-level optimal transport problem, the authors prove their approach is optimal in expected token acceptance and demonstrate consistent practical speedups. The method requires no additional forward-passes from either model and maintains the same code complexity as standard token-level verification.

## Method Summary
The method reformulates speculative decoding draft verification as a block-level optimal transport problem, enabling joint verification of entire blocks rather than independent token verification. BlockVerify uses a backward induction approach that processes tokens from end to beginning, maintaining running estimates of remaining and rejected masses without requiring exponential computation. The algorithm specifically optimizes for expected token efficiency (acceptance length) rather than individual token acceptance probabilities, achieving theoretical optimality while being computationally efficient through linear-time operations that are parallelizable on modern hardware.

## Key Results
- Consistent wall-clock speedups of 5%-8% across multiple datasets (LM1B, WebQA, PIQA, ShareGPT, XSum, GSM8K, WMT-DeEn)
- Proven optimality in expected token acceptance compared to token-level verification
- No increase in code complexity or additional model forward-passes required
- Block length L = 4, 6, 8 tested with PALM-2-Bison (large) and PALM-2-Gecko (small) models

## Why This Works (Mechanism)

### Mechanism 1
Token-level verification independently verifies tokens rather than jointly considering their dependencies. Block-level verification treats the entire draft block as a single unit and finds an optimal coupling between the small and large model distributions that maximizes expected token acceptance length. This works because the joint probability distribution over token sequences can be efficiently approximated without exponential computation through the optimal transport formulation.

### Mechanism 2
Block verification achieves optimality by solving a block-level optimal transport problem with a specific cost function designed to directly optimize expected token efficiency. The cost function formulation enables finding the optimal acceptance strategy that maximizes the expected length of accepted tokens rather than just maximizing individual token acceptance probabilities.

### Mechanism 3
The backward induction approach in BlockVerify efficiently handles the residual distribution without requiring exponential computation. Instead of computing all joint probabilities, BlockVerify processes tokens from end to beginning, maintaining running estimates of remaining and rejected masses using only the conditional probabilities already computed during parallel scoring.

## Foundational Learning

- **Optimal Transport Theory**: BlockVerify is fundamentally based on formulating draft verification as an optimal transport problem between the small and large model distributions. Quick check: Can you explain the basic formulation of an optimal transport problem and how it differs from standard probability matching?

- **Rejection Sampling**: The verification algorithm uses rejection sampling principles to ensure that accepted tokens follow the correct distribution while maximizing acceptance rates. Quick check: How does rejection sampling guarantee that the output follows the target distribution while potentially rejecting some samples?

- **Autoregressive Language Models**: Understanding the causal structure and sequential dependencies in language models is crucial for appreciating why block-level verification provides benefits over token-level approaches. Quick check: Why can't we verify tokens in parallel without considering their dependencies in autoregressive models?

## Architecture Onboarding

- **Component map**: Small draft model (Ms) -> Parallel scoring (Mb) -> BlockVerify -> Output tokens
- **Critical path**: Ms → Parallel scoring (Mb) → BlockVerify → Output tokens
- **Design tradeoffs**: Block verification trades some additional computation complexity for improved token acceptance rates and wall-clock speedup
- **Failure signatures**: Degraded performance when the draft model is too dissimilar from the target model, or when block sizes are poorly chosen
- **First 3 experiments**:
  1. Compare wall-clock speedup between token-level and block-level verification on a small dataset with controlled conditions
  2. Measure the acceptance rate as a function of block size to find the optimal block length
  3. Test the algorithm's robustness when the draft model has systematically different characteristics from the target model

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance gap between token-level and block-level verification scale with block length L? While the paper demonstrates this trend empirically, it does not provide a theoretical analysis of the exact scaling relationship or an asymptotic bound on the improvement gap as a function of L.

### Open Question 2
Can the block-level verification approach be extended to multi-draft speculative decoding? The current BlockVerify algorithm is specifically designed for single-draft scenarios, and it's unclear how to optimally extend it to handle multiple draft sequences while maintaining the same guarantees.

### Open Question 3
How does the proposed block-level verification perform under non-negligible drafting time for the small model? The paper's theoretical analysis and empirical speedup calculations assume the small model drafting time is negligible, which may not hold in practice.

## Limitations

- Performance improvements are tied to specific PALM-2 model pairs that are not publicly available
- Theoretical optimality is relative to token-level verification within the same speculative decoding framework, not globally optimal across all verification strategies
- Results may not generalize to arbitrary model pairs with significantly different characteristics

## Confidence

- **High Confidence**: Theoretical proof of optimality for BlockVerify relative to token-level verification, and consistent speedup demonstration across multiple datasets
- **Medium Confidence**: Practical wall-clock speedup measurements, as these depend on specific implementation details and hardware characteristics
- **Low Confidence**: Generalization claims to arbitrary model pairs and the assertion that BlockVerify is "never worse" than token-level verification in all practical scenarios

## Next Checks

1. Implement BlockVerify and TokenVerify algorithms using publicly available models (e.g., LLaMA and a smaller LLaMA variant) to verify the speedup claims are reproducible with different model architectures
2. Systematically vary the similarity between draft and target models to establish the robustness boundaries of BlockVerify's performance advantages
3. Measure the actual computational overhead of the block-level verification separately from the overall speculative decoding pipeline to quantify the tradeoff between improved acceptance rates and additional computation costs