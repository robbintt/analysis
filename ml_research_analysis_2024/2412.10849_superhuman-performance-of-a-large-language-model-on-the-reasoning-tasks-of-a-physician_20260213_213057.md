---
ver: rpa2
title: Superhuman performance of a large language model on the reasoning tasks of
  a physician
arxiv_id: '2412.10849'
source_url: https://arxiv.org/abs/2412.10849
tags:
- cases
- physicians
- diagnosis
- o1-preview
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the diagnostic and reasoning abilities of
  an advanced large language model (LLM), OpenAI o1, against hundreds of board-certified
  physicians across six diverse experiments, including complex clinical vignettes
  and real-world emergency department cases. o1 consistently outperformed both historical
  LLMs (like GPT-4) and expert physicians in differential diagnosis generation, diagnostic
  reasoning, management reasoning, and probabilistic reasoning.
---

# Superhuman performance of a large language model on the reasoning tasks of a physician

## Quick Facts
- arXiv ID: 2412.10849
- Source URL: https://arxiv.org/abs/2412.10849
- Authors: Peter G. Brodeur; Thomas A. Buckley; Zahir Kanjee; Ethan Goh; Evelyn Bin Ling; Priyank Jain; Stephanie Cabral; Raja-Elie Abdulnour; Adrian D. Haimovich; Jason A. Freed; Andrew Olson; Daniel J. Morgan; Jason Hom; Robert Gallo; Liam G. McCoy; Haadi Mombini; Christopher Lucas; Misha Fotoohi; Matthew Gwiazdon; Daniele Restifo; Daniel Restrepo; Eric Horvitz; Jonathan Chen; Arjun K. Manrai; Adam Rodman
- Reference count: 29
- Key outcome: An advanced LLM (OpenAI o1) consistently outperformed board-certified physicians across diagnostic, clinical reasoning, and probabilistic reasoning tasks, demonstrating superhuman capabilities especially in low-information emergency settings.

## Executive Summary
This study evaluated the diagnostic and reasoning abilities of OpenAI's o1 model against hundreds of board-certified physicians across six diverse experiments. The LLM consistently outperformed both historical LLMs and expert physicians in differential diagnosis generation, diagnostic reasoning, management reasoning, and probabilistic reasoning tasks. Most notably, in real emergency department cases, o1 surpassed physician performance, particularly in low-information settings like initial triage where decisions must be made with minimal patient data. These results fulfill the vision set by Ledley and Lusted (1959) for AI diagnostic systems and demonstrate capabilities that exceed human expert performance across multiple dimensions of clinical reasoning.

## Method Summary
The study compared OpenAI's o1 model performance against board-certified physicians using six experimental frameworks: NEJM Clinicopathological Conference cases for differential diagnosis, NEJM Healer cases for clinical reasoning, Grey Matters Management cases for management reasoning, Landmark Diagnostic cases for diagnostic accuracy, Probabilistic Reasoning cases for probability estimation, and real emergency department cases from Beth Israel Deaconess Medical Center. The o1-preview and o1 models were accessed through OpenAI's API and Azure API respectively, with cases presented using specific prompts from supplementary materials. Performance was evaluated using standardized metrics including Bond Scores, R-IDEA scores, and blinded physician scoring, with statistical analysis using mixed-effects models and McNemar's tests.

## Key Results
- o1-preview included correct diagnoses in differential in 78.3% of NEJM CPC cases, significantly outperforming GPT-4 (58.8%), attending physicians (35%), and resident physicians (20%)
- In emergency department cases, o1 either matched or outperformed attending physicians at all three diagnostic touchpoints, with the largest performance gap at initial triage where data is most limited
- o1 demonstrated superior management reasoning and clinical documentation quality compared to human physicians across multiple validated scoring systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM achieves superhuman performance by leveraging large-scale pretraining data and advanced inference-time reasoning (o1 model's chain-of-thought).
- Mechanism: The o1-preview model's architecture allows it to perform step-by-step reasoning, simulating diagnostic workflows by integrating clinical knowledge from its pretraining corpus with real-time inference to generate differential diagnoses.
- Core assumption: The pretraining data includes diverse, high-quality medical information (though pretraining cutoff is October 2023).
- Evidence anchors:
  - [abstract]: "the LLM displayed superhuman diagnostic and reasoning abilities, as well as continued improvement from prior generations of AI clinical decision support."
  - [section]: "o1-preview included the correct diagnosis in its differential in 78.3% of cases... significantly outperformed GPT-4 (47/80, p<0.0001), attending physicians (28/80, p<0.0001), and resident physicians (16/80, p<0.0001)."
  - [corpus]: Weak; related papers focus on benchmarking and evaluation but do not detail the specific reasoning mechanisms.
- Break condition: If the pretraining data lacks sufficient clinical depth or the inference-time reasoning fails to generalize to unseen cases, performance degrades to below human expert level.

### Mechanism 2
- Claim: The model excels in low-information settings (e.g., initial ER triage) due to pattern recognition from vast pretraining data.
- Mechanism: In early diagnostic touchpoints, the LLM can draw on learned patterns to fill in diagnostic gaps where human clinicians rely more on sequential data gathering, leading to higher accuracy in high-urgency, sparse-data scenarios.
- Core assumption: Pattern matching in early stages is more effective than sequential reasoning when data is limited.
- Evidence anchors:
  - [abstract]: "Performance differences were especially pronounced at the first diagnostic touchpoint (initial ER triage), where there is the least information available about the patient and the most urgency to make the correct decision."
  - [section]: "o1 either performed better than or on par with the two attending physicians and 4o... The widest margin in low information settings with o1."
  - [corpus]: Weak; no explicit corpus evidence for low-information reasoning superiority.
- Break condition: If pattern matching is insufficient for rare or atypical presentations, the model's advantage diminishes in early triage scenarios.

### Mechanism 3
- Claim: The o1 model's improved probabilistic reasoning stems from refined calibration of pre- and post-test probabilities during inference.
- Mechanism: By generating multiple outputs (100 per case in the study), the model refines its probability estimates closer to reference ranges, improving diagnostic calibration compared to both prior models and human clinicians.
- Core assumption: Multiple sampling during inference allows better probability estimation.
- Evidence anchors:
  - [abstract]: "o1-preview performs similarly to GPT-4 in estimating pre-test and post-test probabilities. The exception is the stress test for coronary artery disease, in which o1-preview density is closer to the reference range than models and humans."
  - [section]: "o1-preview density is closer to the reference range than models and humans" in probabilistic reasoning tasks.
  - [corpus]: Weak; related papers discuss evaluation frameworks but not the specific probabilistic calibration mechanisms.
- Break condition: If the reference ranges are outdated or if the sampling process introduces bias, the probabilistic advantage may not hold.

## Foundational Learning

- Concept: Bond Score system for evaluating differential diagnoses
  - Why needed here: It provides a standardized, validated metric (0-5 scale) to objectively compare LLM and physician performance across experiments.
  - Quick check question: What Bond Score corresponds to a differential that contains the exact correct diagnosis?

- Concept: R-IDEA (Revised-IDEA) score for clinical reasoning documentation
  - Why needed here: It quantifies the quality of clinical reasoning across four core domains, enabling fair comparison between LLM outputs and physician documentation.
  - Quick check question: What is the maximum possible R-IDEA score and what does it represent?

- Concept: Linear mixed-effects model for statistical analysis
  - Why needed here: It accounts for variability in case difficulty and repeated measurements across participants, providing robust comparisons between LLM and human performance.
  - Quick check question: Why is a random intercept included for case number in the mixed-effects model?

## Architecture Onboarding

- Component map: Clinical case data -> o1 model inference with chain-of-thought reasoning -> Bond Score/R-IDEA score evaluation -> blinded physician scoring -> mixed-effects models analysis

- Critical path:
  1. Prepare clinical case data
  2. Run o1 model inference with appropriate prompts
  3. Score outputs using standardized rubrics
  4. Reconcile inter-rater disagreements
  5. Analyze results with statistical models

- Design tradeoffs:
  - Using a single model run vs. multiple runs for probability estimation
  - Blinded vs. unblinded evaluation of LLM vs. human performance
  - Including only internal medicine cases vs. broader specialty coverage

- Failure signatures:
  - Low inter-rater agreement on scoring rubrics
  - Wide confidence intervals around performance metrics
  - Model outputs that don't follow expected semantic structure for blinding

- First 3 experiments:
  1. Run o1 model on NEJM CPC cases and compare Bond Scores to historical GPT-4 results
  2. Evaluate o1 model's clinical reasoning on NEJM Healer cases using R-IDEA scoring
  3. Test o1 model's management reasoning on Grey Matters cases and compare to physician performance using mixed-effects models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the mechanisms behind LLMs' superior performance in low-information settings compared to human physicians, and can these mechanisms be translated into training programs for human clinicians?
- Basis in paper: [explicit] The paper notes that "Performance differences were especially pronounced at the first diagnostic touchpoint (initial ER triage), where there is the least information available about the patient and the most urgency to make the correct decision" and that "both LLMs consistently outperformed humans, with the widest margin in low information settings with o1."
- Why unresolved: While the paper demonstrates the superior performance of LLMs in low-information settings, it does not explore the underlying cognitive or algorithmic mechanisms that enable this advantage or how these insights could be applied to human clinical training.
- What evidence would resolve it: Studies comparing the reasoning processes of LLMs and human physicians in low-information scenarios, identification of specific LLM capabilities that could be incorporated into medical education, and trials testing whether LLM-inspired training improves human diagnostic performance in similar settings.

### Open Question 2
- Question: How do LLMs perform across different medical specialties and patient populations that were not included in this study?
- Basis in paper: [explicit] The paper states "our study examined only six aspects of clinical reasoning; researchers have identified dozens of other tasks that could be studied which may have even more impact on actual clinical care" and "it is not representative of broader medical practice which includes multiple subspecialties that require a variety of skill sets such as surgical decisions."
- Why unresolved: The study focused primarily on internal medicine and emergency medicine cases, leaving uncertainty about LLM performance in other specialties like surgery, pediatrics, or obstetrics/gynecology.
- What evidence would resolve it: Comprehensive evaluations of LLMs across diverse medical specialties, different patient demographics, and various clinical scenarios including surgical decision-making and procedural planning.

### Open Question 3
- Question: What is the optimal human-AI collaboration model for clinical practice, and how can it be designed to maximize patient outcomes while maintaining appropriate human oversight?
- Basis in paper: [explicit] The paper acknowledges "our study examined only model performance" and "Further studies should be done to elucidate how humans and LLMs collaborate together."
- Why unresolved: While the study demonstrates LLM superiority in various tasks, it did not investigate how these models should be integrated into clinical workflows or what the optimal balance of human and AI decision-making should be.
- What evidence would resolve it: Randomized controlled trials testing different human-AI collaboration models in clinical settings, studies measuring the impact of AI assistance on physician decision-making quality and confidence, and frameworks for determining appropriate levels of AI autonomy versus human oversight in various clinical contexts.

## Limitations

- The study was limited to internal medicine and emergency medicine cases, not representing the full breadth of medical practice including surgical specialties and diverse patient populations.
- The o1 model's pretraining cutoff in October 2023 means it may not incorporate the most recent medical knowledge, particularly for rapidly evolving fields.
- The emergency department cases were from a single institution (Beth Israel Deaconess Medical Center), potentially limiting generalizability across different healthcare systems and patient demographics.

## Confidence

- High confidence: The comparison of o1 model performance against historical LLMs (GPT-4) on standardized benchmark datasets (NEJM CPC cases) - these results are reproducible and the metrics (Bond Scores) are well-established.
- Medium confidence: The comparison against board-certified physicians on emergency department cases - while the methodology appears sound, the single-institution sample and potential selection bias in case difficulty affect generalizability.
- Medium confidence: The clinical reasoning and management reasoning results - these rely on subjective scoring by physician evaluators, though inter-rater reliability was reported as acceptable.

## Next Checks

1. **Prospective clinical validation**: Conduct a prospective trial where the o1 model is integrated into actual emergency department workflows to assess real-time performance, including measuring impact on diagnostic accuracy, time to diagnosis, and patient outcomes compared to standard care.

2. **Multi-institutional generalizability testing**: Replicate the emergency department case analysis using cases from multiple healthcare systems across different geographic regions and patient demographics to assess whether the superhuman performance generalizes beyond a single academic medical center.

3. **Rare disease and edge case evaluation**: Systematically test the o1 model's performance on rare diseases and atypical presentations that were likely underrepresented in pretraining data, comparing results against expert consensus to identify potential blind spots in the model's diagnostic capabilities.