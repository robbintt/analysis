---
ver: rpa2
title: Architecture of a Cortex Inspired Hierarchical Event Recaller
arxiv_id: '2405.02371'
source_url: https://arxiv.org/abs/2405.02371
tags: []
core_contribution: This paper proposes a biologically-inspired approach to unsupervised
  continuous learning of complex temporal patterns. The core method involves a hierarchical
  cortical model (HER) that uses sequence memory and contextual prediction to learn
  and identify patterns from raw data.
---

# Architecture of a Cortex Inspired Hierarchical Event Recaller

## Quick Facts
- arXiv ID: 2405.02371
- Source URL: https://arxiv.org/abs/2405.02371
- Authors: Valentin Puente Varona
- Reference count: 0
- One-line primary result: Biologically-inspired hierarchical cortical model (HER) demonstrates continuous learning of complex temporal patterns with no catastrophic forgetting

## Executive Summary
This paper proposes a biologically-inspired hierarchical cortical model (HER) for unsupervised continuous learning of complex temporal patterns. The architecture mimics cortical organization through sequence memory and contextual prediction mechanisms that learn and identify patterns from raw data without prior knowledge. The system was tested on Spanish speech recognition tasks, successfully learning to segment, identify, and predict complex temporal sequences while maintaining compositional learning without catastrophic forgetting.

## Method Summary
The hierarchical cortical model operates through cortical columns that segment input streams into symbols at progressively higher abstraction levels. Each column processes input through L4 (dimensionality reduction and locality-sensitive hashing), L23 (sequence learning and prediction), L6a (backward prediction), L6b (input sequencing), L1 (symbol building), and L5 (forward prediction). The system incorporates short and long-term learning modulations, lateral contextualization across columns, and attention mechanisms via cortico-thalamic loops. A hippocampal intervention system (CA3) filters spurious data and accelerates learning when necessary. The architecture was trained on Spanish sentences using raw audio processed into Mel spectrograms.

## Key Results
- Demonstrates continuous learning capability without catastrophic forgetting across complex temporal patterns
- Achieves stable performance with continuous input through short/long-term learning modulations
- Successfully learns, identifies, and predicts complex temporal sequences from raw audio data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dimensionality reduction across cortical rungs enables efficient handling of complex temporal patterns.
- Mechanism: The system uses a hierarchical structure where each rung reduces the dimensionality of the data. Lower rungs identify basic patterns (like phonemes), and higher rungs build more complex sequences (like syllables or sentences) by segmenting and symbolizing the input.
- Core assumption: Each rung in the hierarchy can effectively segment the input into meaningful symbols without losing critical information for higher-level processing.
- Evidence anchors:
  - [abstract]: "The core method involves a hierarchical cortical model (HER) that uses sequence memory and contextual prediction to learn and identify patterns from raw data."
  - [section]: "By segmenting the input stream in each rung, it will be feasible to manage complex sequences cohesively."
  - [corpus]: Weak evidence; no direct mention of hierarchical dimensionality reduction in related papers.
- Break condition: If the dimensional reduction across rungs is too aggressive, the system may lose essential information, leading to poor performance at higher levels.

### Mechanism 2
- Claim: Short and long-term learning modulations prevent catastrophic forgetting while enabling continuous learning.
- Mechanism: Learning is modulated based on the novelty of the input. If a sequence is unknown (short-term), learning is enhanced; if it is known (long-term), learning is reduced or turned off. This allows the system to adapt to new information without overwriting previously learned patterns.
- Core assumption: The system can accurately assess the novelty of input sequences and adjust learning rates accordingly.
- Evidence anchors:
  - [abstract]: "Key features include short and long-term learning modulations, lateral contextualization, and attention mechanisms."
  - [section]: "To ensure flexibility, learning is stochastic. The probability of learning is determined by the degree of knowledge of the current input sequence."
  - [corpus]: No direct evidence; related papers do not discuss learning modulation in detail.
- Break condition: If the novelty assessment is inaccurate, the system may either over-learn (leading to interference) or under-learn (failing to adapt to new patterns).

### Mechanism 3
- Claim: Attention mechanisms accelerate pattern identification and reduce sensory dependence.
- Mechanism: The system uses a cortico-thalamic loop to predict and forward sequences before they are fully received. This allows higher rungs to operate with reduced sensory input, focusing only on relevant patterns.
- Core assumption: The predictions made by the system are accurate enough to replace sensory input without degrading performance.
- Evidence anchors:
  - [abstract]: "Key features include short and long-term learning modulations, lateral contextualization, and attention mechanisms."
  - [section]: "By cascading predictions using this 'trans-thalamic pathway' (if L6+ and L5 know the sequence) we can achieve sequence identification in the 3rd rung eleven-time steps ahead."
  - [corpus]: Weak evidence; no direct mention of attention mechanisms in related papers.
- Break condition: If predictions are inaccurate, the system may make incorrect identifications, leading to errors in higher-level processing.

## Foundational Learning

- Concept: Sequence Memory and Contextual Prediction
  - Why needed here: This is the core mechanism for learning and identifying patterns in the input data. Without it, the system cannot build a hierarchical model of the world.
  - Quick check question: How does the system ensure that sequences are learned and predicted accurately across different rungs of the hierarchy?

- Concept: Dimensionality Reduction
  - Why needed here: Reducing the dimensionality of data at each rung allows the system to handle complex patterns efficiently without overwhelming resources.
  - Quick check question: What happens if the dimensionality reduction is too aggressive or too conservative at any rung?

- Concept: Learning Modulation
  - Why needed here: Modulating learning based on novelty prevents catastrophic forgetting and ensures continuous adaptation to new information.
  - Quick check question: How does the system determine the novelty of an input sequence, and what are the consequences of inaccurate novelty assessment?

## Architecture Onboarding

- Component map:
  Input Projectors (L4) -> Projection Trackers (L23) -> Backward Predictor (L6a) -> Input Sequencer (L6b) -> Output Symbol Builder (L1) -> Forward Predictor (L5) -> CA3 Hippocampal Intervention

- Critical path:
  1. Input is processed by L4 and L6a/b to form a temporal reference frame.
  2. L23 learns and predicts sequences based on L4 projections.
  3. L6b detects the end of a sequence and triggers symbol formation in L1.
  4. L5 predicts the next symbol, which is used to accelerate identification in higher rungs.
  5. CA3 intervenes to filter noise and accelerate learning when necessary.

- Design tradeoffs:
  - Dimensionality reduction vs. information retention: Aggressive reduction can lead to loss of critical information, while conservative reduction may overwhelm resources.
  - Learning rate vs. stability: High learning rates can lead to interference, while low rates may slow adaptation.
  - Prediction accuracy vs. sensory dependence: Accurate predictions reduce sensory dependence but may lead to errors if predictions are incorrect.

- Failure signatures:
  - If the system fails to learn new patterns, check the learning modulation parameters and the accuracy of novelty assessment.
  - If the system produces incorrect identifications, verify the accuracy of predictions and the stability of the cortical state.
  - If the system is unstable, examine the dimensional reduction across rungs and the learning rates in L4 and L23.

- First 3 experiments:
  1. Test the system with a simple, repetitive input to verify that it can learn and predict sequences accurately.
  2. Introduce a new, complex input to assess the system's ability to adapt and integrate new information without catastrophic forgetting.
  3. Evaluate the system's performance with and without the attention mechanism to determine its impact on pattern identification speed and accuracy.

## Open Questions the Paper Calls Out

None

## Limitations

- The neurobiological plausibility of the proposed attention mechanisms and hippocampal intervention remains largely theoretical without direct experimental validation
- Limited quantitative evidence about information preservation through the hierarchical dimensionality reduction across rungs
- Evaluation metrics focus primarily on internal consistency measures rather than direct comparison to baseline approaches

## Confidence

- **High Confidence**: The core architectural design of hierarchical cortical columns with sequence memory and contextual prediction is well-defined and implementable
- **Medium Confidence**: Claims about continuous learning without catastrophic forgetting and the effectiveness of short/long-term learning modulations are supported by described mechanisms but lack extensive empirical validation
- **Low Confidence**: The complete system's neurobiological plausibility, particularly the attention mechanisms and hippocampal intervention, remains largely theoretical

## Next Checks

1. **Dimensionality Reduction Validation**: Conduct ablation studies measuring information loss at each hierarchical level using established metrics like mutual information preservation
2. **Generalization Across Domains**: Evaluate the system on multiple temporal pattern datasets beyond speech (e.g., sensor time series, financial data)
3. **Biological Plausibility Testing**: Compare the system's behavior against neurophysiological recordings from cortical-thalamic circuits during sequence processing tasks