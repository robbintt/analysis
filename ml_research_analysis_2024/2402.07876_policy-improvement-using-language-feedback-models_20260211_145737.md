---
ver: rpa2
title: Policy Improvement using Language Feedback Models
arxiv_id: '2402.07876'
source_url: https://arxiv.org/abs/2402.07876
tags:
- feedback
- language
- policy
- learning
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Language Feedback Models (LFMs) to improve
  policy learning in instruction following tasks. The core idea is to use LLMs to
  provide feedback on whether agent actions are productive in achieving task goals,
  then train a small feedback model to identify desirable behavior for imitation learning.
---

# Policy Improvement using Language Feedback Models
## Quick Facts
- arXiv ID: 2402.07876
- Source URL: https://arxiv.org/abs/2402.07876
- Authors: Victor Zhong; Dipendra Misra; Xingdi Yuan; Marc-Alexandre Côté
- Reference count: 40
- Policy learning improved by 3.5-12.0% task-completion rate using LLM feedback models

## Executive Summary
This paper introduces Language Feedback Models (LFMs) to enhance policy learning in instruction-following tasks. The core innovation uses large language models (LLMs) to evaluate whether agent actions are productive toward task completion, then trains smaller feedback models to identify desirable behavior for imitation learning. LFMs significantly outperform behavioral cloning baselines and direct LLM action prediction across three benchmarks (ALFWorld, ScienceWorld, Touchdown), demonstrating 3.5-12.0% improvements in task completion rates. The approach generalizes to unseen environments and provides human-interpretable feedback that enables verification of imitation learning data quality.

## Method Summary
LFMs work by first using LLMs to generate feedback on agent trajectories, determining whether actions are productive for task completion. This feedback is then used to train a smaller, efficient feedback model that can evaluate new agent behaviors. The feedback model guides imitation learning by filtering or weighting demonstration data based on the quality of agent actions. The approach combines the reasoning capabilities of large LLMs with the efficiency of smaller models, creating a scalable system for improving instruction-following policies. The method was evaluated on three distinct environments requiring different types of reasoning and interaction.

## Key Results
- Task-completion rate improvements of 3.5-12.0% over behavioral cloning baselines
- Generalization to unseen environments beyond training distribution
- Human-interpretable feedback enabling verification of imitation learning data quality

## Why This Works (Mechanism)
LFMs leverage LLMs' ability to understand task goals and evaluate action sequences against those goals. By providing detailed feedback on whether actions are productive, LFMs create a supervisory signal that captures semantic understanding of task completion rather than just surface-level action matching. The feedback model learns to distill this high-level reasoning into a compact form that can efficiently evaluate new behaviors. This allows the system to identify and reinforce successful strategies while avoiding unproductive actions, leading to more robust and generalizable policies.

## Foundational Learning
- **Instruction following**: Agents must interpret natural language commands and execute appropriate actions - needed for embodied AI applications, check by verifying correct action selection on held-out instructions
- **Imitation learning**: Learning from expert demonstrations rather than trial-and-error - needed for sample efficiency, check by comparing to reinforcement learning baselines
- **LLM feedback generation**: Using language models to evaluate task progress - needed for semantic understanding of success, check by human evaluation of feedback quality
- **Policy distillation**: Compressing complex reasoning into efficient models - needed for deployment efficiency, check by measuring inference time and model size
- **Cross-environment generalization**: Ability to transfer learned behaviors to new settings - needed for real-world applicability, check by testing on held-out environments

## Architecture Onboarding
**Component Map**: LLM Feedback Generator -> Feedback Model Trainer -> Policy Learner -> Environment Executor -> Feedback Collection Loop

**Critical Path**: Environment provides state and instruction -> Policy generates action -> LLM evaluates action productivity -> Feedback model learns from evaluations -> Improved policy guides future actions

**Design Tradeoffs**: Large LLM provides high-quality feedback but is computationally expensive vs. smaller feedback model that is efficient but requires careful training to maintain accuracy

**Failure Signatures**: Poor task performance indicates feedback model misalignment, excessive computational cost suggests need for better distillation, lack of generalization reveals insufficient diversity in training data

**First Experiments**: 1) Evaluate feedback quality by comparing LLM and human assessments on sample trajectories, 2) Test feedback model accuracy against ground truth productivity labels, 3) Measure task completion rates before and after LFM training on a simple benchmark

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on LLM-generated feedback quality, which may not perfectly align with optimal task completion strategies
- Performance depends heavily on quality and diversity of LLM feedback, potentially limiting generalization
- Substantial computational resources required for LLM inference during data collection, affecting scalability

## Confidence
- High confidence in experimental results and benchmark performance improvements
- Medium confidence in generalization claims based on cross-environment testing scope
- Medium confidence in human-interpretability benefits due to qualitative rather than systematic validation

## Next Checks
1. Conduct ablation studies systematically removing different components of the LFM pipeline to quantify individual contributions
2. Test the approach on continuous action spaces and compare against state-of-the-art reinforcement learning methods
3. Perform extensive cross-task generalization studies using LLMs trained on completely different domains to evaluate feedback mechanism robustness