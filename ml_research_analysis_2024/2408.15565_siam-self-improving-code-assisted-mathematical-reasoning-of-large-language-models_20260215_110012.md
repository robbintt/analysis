---
ver: rpa2
title: 'SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large Language
  Models'
arxiv_id: '2408.15565'
source_url: https://arxiv.org/abs/2408.15565
tags: []
core_contribution: This paper proposes a self-improving paradigm for code-assisted
  mathematical reasoning in large language models. The core idea is to use a code-based
  critic model to guide data construction, quality control, and evaluation during
  iterative self-improvement.
---

# SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large Language Models

## Quick Facts
- arXiv ID: 2408.15565
- Source URL: https://arxiv.org/abs/2408.15565
- Authors: Dian Yu; Baolin Peng; Ye Tian; Linfeng Song; Haitao Mi; Dong Yu
- Reference count: 30
- Key outcome: Self-improving paradigm using code-based critic model improves code-assisted math LLM performance by up to +5.7% on in-domain and +4.4% on out-of-domain benchmarks

## Executive Summary
This paper proposes a self-improving paradigm for code-assisted mathematical reasoning in large language models. The approach uses a code-based critic model to guide iterative self-improvement through question-code data construction, quality control, and evaluation. The critic model compares code execution results with reference answers across diverse formats, enabling more reliable verification than step-by-step reasoning chains. Experiments demonstrate effectiveness across both English and Chinese benchmarks, with the resulting models outperforming state-of-the-art code-assisted math LLMs.

## Method Summary
The method involves iterative self-improvement using a code-based critic model to guide the process. It starts with seed data from GSM8K and MATH, then generates new questions and corresponding code samples. The critic model evaluates the code execution results against reference answers, filtering valid responses for supervised fine-tuning. The process incorporates preference learning through DPO/ORPO with SFT loss to control response length. The approach scales from initial policy models through multiple improvement cycles (D0 to D2), leveraging large-scale web QA data for enhanced generalization.

## Key Results
- SIaM models achieve up to +5.7% improvement on in-domain benchmarks
- Out-of-domain performance improves by +4.4% across benchmarks
- Effectiveness demonstrated for both English and Chinese mathematical reasoning
- SIaM outperforms state-of-the-art code-assisted math LLMs on tested benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code-based critic model enables accurate assessment of code execution results against reference answers in diverse formats
- Mechanism: The critic model compares the execution result of generated code with the reference answer and outputs YES/NO, bypassing the need for complex pattern matching
- Core assumption: Code execution results are more reliable indicators of correctness than step-by-step reasoning chains for mathematical problems
- Evidence anchors: [abstract] "we propose a novel paradigm that uses a code-based critic model to guide steps including question-code data construction, quality control, and complementary evaluation"

### Mechanism 2
- Claim: Iterative self-improvement with large-scale diverse web QA data enhances generalization beyond limited in-domain datasets
- Mechanism: The model generates code samples for new questions, filters valid responses using the critic model, and performs supervised fine-tuning on the filtered data
- Core assumption: Large-scale expert-written QA pairs provide diverse problem types and formats that improve model generalization
- Evidence anchors: [abstract] "Experiments across both in-domain (up to +5.7%) and out-of-domain (+4.4%) benchmarks in English and Chinese demonstrate the effectiveness"

### Mechanism 3
- Claim: Joint training of SFT loss with DPO improves preference learning while controlling response length
- Mechanism: The SFT loss coefficient in DPO training prevents excessive response length by keeping the model close to the reference policy
- Core assumption: The reference policy generates valid responses that serve as anchors for length control during preference optimization
- Evidence anchors: [section 3.4] "Removing the SFT loss (i.e., λ = 0) from DPO training leads to a dramatic increase in response length"

## Foundational Learning

- Concept: Code execution as verification mechanism
  - Why needed here: Traditional CoT reasoning is prone to step-wise errors; code execution provides deterministic verification
  - Quick check question: If a model generates code that returns 42 for a question with answer 42, can we trust the correctness without examining the code logic?

- Concept: Preference learning with reference anchors
  - Why needed here: Pure preference optimization can lead to response length exploitation; SFT acts as a length regularizer
  - Quick check question: What happens to average response length when λ=0 in the SFT+DPO loss function?

- Concept: Format conversion and numerical calculation patterns
  - Why needed here: Web QA data contains diverse answer formats requiring conversion between natural language and code syntax
  - Quick check question: How would you convert "(x-5)(x²-4x+7)" to valid Python code syntax?

## Architecture Onboarding

- Component map: Policy model -> Code generation -> Code interpreter -> Execution result -> Critic model -> Data filtering -> SFT/DPO training -> Updated policy model
- Critical path: Question → Policy model → Code generation → Code interpreter → Execution result → Critic model → YES/NO → Data filtering → SFT/DPO training → Updated policy model
- Design tradeoffs: Larger critic models provide better accuracy but increase computational cost; stricter filtering improves data quality but reduces training sample size
- Failure signatures: Critic model accuracy drops below threshold; code execution failures increase; preference learning diverges from reference policy
- First 3 experiments:
  1. Test critic model accuracy on held-out validation set with diverse answer formats
  2. Measure impact of different λ values in SFT+DPO loss on response length and accuracy
  3. Compare in-domain vs out-of-domain performance when scaling from D0 to D1 to D2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the critic model at evaluating code responses for mathematical reasoning across different languages?
- Basis in paper: [inferred] The paper mentions that the critic model is trained on Chinese data but shows strong correlation with heuristic-based evaluation methods on English tasks.
- Why unresolved: The paper primarily focuses on Chinese data for the critic model, and while there is some evidence of effectiveness on English tasks, it's unclear how well the model generalizes to other languages or different types of mathematical problems.
- What evidence would resolve it: Conducting experiments with the critic model on a diverse set of languages and mathematical problem types, comparing its performance to language-specific heuristic methods.

### Open Question 2
- Question: What is the impact of using code-only methods versus interactive code-and-CoT methods on mathematical reasoning performance?
- Basis in paper: [explicit] The paper discusses the use of code-only methods and mentions that interactive methods (ToRA and MathCoder) show different performance characteristics.
- Why unresolved: The paper does not provide a comprehensive comparison between code-only and interactive methods, leaving open the question of which approach is more effective for different types of mathematical problems.
- What evidence would resolve it: Conducting a detailed comparison study between code-only and interactive methods across a wide range of mathematical problem types and datasets.

### Open Question 3
- Question: How does the size of the LLM affect the effectiveness of the self-improving paradigm for code-assisted mathematical reasoning?
- Basis in paper: [inferred] The paper mentions that experiments focus on 7-8B LLMs due to computational resource constraints, but does not explore the impact of model size on performance.
- Why unresolved: The paper does not investigate the relationship between LLM size and the effectiveness of the self-improving paradigm, leaving open the question of whether larger models would benefit more or less from this approach.
- What evidence would resolve it: Experimenting with the self-improving paradigm on LLMs of various sizes (e.g., 7B, 34B, 70B) and comparing their performance improvements and final capabilities.

## Limitations
- Limited evaluation scope to specific benchmarks without addressing broader mathematical domains or real-world applications
- High computational cost and practical deployment considerations not fully analyzed
- Critic model reliability across different mathematical domains and answer formats remains uncertain

## Confidence

**High Confidence**: The core mechanism of using code execution for verification and the iterative self-improvement framework are technically sound and well-supported by the experimental results. The improvement on both in-domain and out-of-domain benchmarks (+5.7% and +4.4% respectively) provides strong evidence for the effectiveness of the approach.

**Medium Confidence**: The specific implementation details such as the exact thresholds for filtering, the impact of different λ values in SFT+DPO training, and the optimal critic model architecture are supported by ablation studies but may require further validation across different problem domains and model scales.

**Low Confidence**: The generalizability of the approach to non-mathematical reasoning tasks, the long-term stability of the self-improvement process across many iterations, and the robustness of the method to different code execution environments remain uncertain without additional empirical validation.

## Next Checks

**Validation Check 1**: Evaluate critic model accuracy on a held-out validation set containing diverse mathematical problem types and answer formats not present in the training data. This would quantify the critic's reliability and identify potential blind spots in the verification mechanism.

**Validation Check 2**: Conduct a scalability analysis measuring the computational cost and time requirements for each iteration of the self-improvement process. Include profiling of code execution bottlenecks and training efficiency to assess practical deployment feasibility.

**Validation Check 3**: Test the method on out-of-domain mathematical reasoning tasks beyond the standard benchmarks, such as competition mathematics, applied mathematics problems, or multi-modal mathematical reasoning tasks. This would validate the claimed generalization benefits of the approach.