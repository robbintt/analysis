---
ver: rpa2
title: Reproducibility study of FairAC
arxiv_id: '2406.03314'
source_url: https://arxiv.org/abs/2406.03314
tags: []
core_contribution: This reproducibility study evaluates the FairAC framework for fair
  attribute completion on graphs with missing attributes. The framework aims to address
  both feature and topological unfairness while maintaining accuracy for downstream
  tasks.
---

# Reproducibility study of FairAC

## Quick Facts
- arXiv ID: 2406.03314
- Source URL: https://arxiv.org/abs/2406.03314
- Reference count: 40
- FairAC consistently outperforms baseline methods in group fairness metrics while maintaining comparable accuracy

## Executive Summary
This reproducibility study evaluates FairAC, a framework for fair attribute completion on graphs with missing attributes. FairAC addresses both feature and topological unfairness while maintaining accuracy for downstream tasks. Through extensive experiments on multiple datasets (NBA, Pokec-z, Pokec-n, Credit, Recidivism), FairAC consistently outperforms baseline methods (GCN, FairGNN) in group fairness metrics (statistical parity and equal opportunity) while maintaining comparable accuracy and AUC scores. The study demonstrates FairAC's generalizability across different sensitive attributes and datasets. Notably, improvements in group fairness do not come at the expense of individual fairness, as measured by consistency.

## Method Summary
FairAC uses an auto-encoder architecture for graph attribute completion that incorporates fairness constraints into the training process. The framework pretrains an auto-encoder for 200 epochs regardless of dataset size, then fine-tunes with fairness-aware loss functions. The method specifically targets both feature unfairness (bias in node attributes) and topological unfairness (bias in graph structure) through regularization terms in the loss function. FairAC is designed to be generic and can be applied to various graph datasets and models while maintaining fairness across different sensitive attributes.

## Key Results
- FairAC consistently outperforms GCN and FairGNN baselines in statistical parity and equal opportunity metrics across all tested datasets
- Group fairness improvements are achieved without sacrificing individual fairness, as measured by consistency scores
- FairAC maintains comparable accuracy and AUC scores to baseline methods while improving fairness metrics
- The framework demonstrates generalizability across different sensitive attributes (region, gender) and datasets

## Why This Works (Mechanism)
FairAC works by incorporating fairness constraints directly into the attribute completion process through its auto-encoder architecture. The framework uses regularization terms in the loss function that penalize unfairness in both node attributes and graph structure. By pretraining the auto-encoder and then fine-tuning with fairness-aware objectives, FairAC learns representations that are both accurate and fair. The key innovation is that fairness improvements do not come at the expense of individual fairness, which is unusual in fairness methods.

## Foundational Learning
- Graph neural networks (GNNs) - needed for understanding baseline methods and graph attribute completion; quick check: review GCN architecture and message passing
- Fairness metrics (statistical parity, equal opportunity) - needed for evaluating group fairness; quick check: understand difference between group and individual fairness
- Auto-encoder architectures - needed for understanding FairAC's pretraining approach; quick check: review basic auto-encoder structure and training
- Sensitive attributes and bias - needed for understanding fairness context; quick check: identify common sensitive attributes in ML applications
- Graph attribute completion - needed for understanding the specific problem FairAC addresses; quick check: understand how missing attributes are predicted in graphs

## Architecture Onboarding
Component map: Data -> Auto-encoder pretraining (200 epochs) -> FairAC fine-tuning (with fairness constraints) -> Downstream task evaluation

Critical path: The critical path involves pretraining the auto-encoder to learn good representations, then fine-tuning with fairness-aware loss functions that balance accuracy and fairness objectives.

Design tradeoffs: The main tradeoff is between fairness improvement and potential accuracy degradation, though FairAC claims to maintain accuracy while improving fairness. Another tradeoff is computational cost of pretraining versus potential performance gains.

Failure signatures: Poor performance on age-sensitive attributes, underperformance on smaller datasets due to fixed pretraining duration, potential overfitting on datasets with limited sensitive attribute diversity.

First experiments:
1. Run FairAC on NBA dataset with region as sensitive attribute to verify basic functionality
2. Compare FairAC's consistency scores with baseline methods on Pokec-z dataset
3. Test FairAC's performance with different pretraining durations on smaller datasets like German Credit

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Why does the performance of FairAC degrade significantly when using age as the sensitive attribute compared to region or gender, even when lowering the accuracy threshold?
- Basis in paper: [inferred] from the observation in Table 3 and Table 6 that age as a sensitive attribute leads to much higher fairness disparities (∆SP+∆EO) compared to region or gender, and that this issue persists even with lower accuracy thresholds.
- Why unresolved: The paper notes that age is an important feature for the final prediction tasks, but does not explore the underlying reasons for why this specifically impacts FairAC's fairness performance.
- What evidence would resolve it: Experiments isolating the impact of age on model predictions, ablation studies on feature importance, or analysis of how age interacts with other features in the dataset could clarify why FairAC struggles with age as a sensitive attribute.

### Open Question 2
- Question: How does the auto-encoder pretraining duration (200 epochs) affect the fairness and accuracy of FairAC across datasets of varying sizes, and would adaptive pretraining improve results?
- Basis in paper: [explicit] from the observation that the auto-encoder is always pretrained for 200 iterations, regardless of dataset size, which may lead to under-pretraining for smaller datasets like NBA or German Credit.
- Why unresolved: The paper does not investigate whether the fixed pretraining duration is optimal across different dataset sizes or whether adaptive pretraining could improve performance.
- What evidence would resolve it: Comparative experiments varying pretraining epochs based on dataset size, or analysis of how pretraining duration correlates with fairness and accuracy metrics, would clarify the impact of this hyperparameter.

### Open Question 3
- Question: Is there a theoretical explanation for why FairAC maintains similar individual fairness (consistency) levels across different methods, despite improvements in group fairness?
- Basis in paper: [explicit] from the observation in Table 1 that consistency levels are similar across GCN, FairGNN, and FairAC, which contradicts the common trade-off between group and individual fairness in the literature.
- Why unresolved: The paper does not provide a theoretical analysis of why FairAC's improvements in group fairness do not compromise individual fairness, which is unusual compared to other fairness methods.
- What evidence would resolve it: A theoretical analysis of FairAC's loss function and its interaction with individual fairness metrics, or experiments comparing FairAC's consistency across different datasets and sensitive attributes, would help explain this phenomenon.

## Limitations
- Results may not generalize to all graph types or domains beyond the tested datasets
- Performance evaluation based on specific sensitive attribute definitions that may vary in real-world applications
- Computational efficiency compared to baseline methods is not extensively discussed
- Limited exploration of FairAC's performance in extreme graph scenarios (very sparse or dense)

## Confidence
- High Confidence: FairAC's superior performance in group fairness metrics (statistical parity and equal opportunity) across all tested datasets
- High Confidence: FairAC's ability to maintain accuracy and AUC scores while improving fairness metrics
- Medium Confidence: Claims about generalizability across different sensitive attributes, as tested on a limited set of attributes
- Medium Confidence: Individual fairness maintenance, as consistency measurements may not capture all aspects of individual fairness

## Next Checks
1. Evaluate FairAC on additional graph datasets with different characteristics (e.g., varying edge densities, node degrees, and attribute distributions)
2. Test FairAC's performance with multiple, intersectional sensitive attributes simultaneously
3. Conduct ablation studies to quantify the individual contributions of FairAC's components to its overall performance