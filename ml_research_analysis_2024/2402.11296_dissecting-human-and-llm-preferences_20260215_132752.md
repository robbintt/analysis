---
ver: rpa2
title: Dissecting Human and LLM Preferences
arxiv_id: '2402.11296'
source_url: https://arxiv.org/abs/2402.11296
tags: []
core_contribution: This work dissects human and LLM preferences in pairwise model
  response comparisons by annotating responses with 29 predefined properties and fitting
  Bayesian logistic regression models to quantify their effects. It finds that humans
  favor lengthy, stance-supporting responses but are less sensitive to errors and
  dislike model limit admissions, while advanced LLMs like GPT-4-Turbo prioritize
  correctness, clarity, and harmlessness.
---

# Dissecting Human and LLM Preferences

## Quick Facts
- arXiv ID: 2402.11296
- Source URL: https://arxiv.org/abs/2402.11296
- Reference count: 40
- Key outcome: This work dissects human and LLM preferences in pairwise model response comparisons by annotating responses with 29 predefined properties and fitting Bayesian logistic regression models to quantify their effects. It finds that humans favor lengthy, stance-supporting responses but are less sensitive to errors and dislike model limit admissions, while advanced LLMs like GPT-4-Turbo prioritize correctness, clarity, and harmlessness. LLMs of similar sizes exhibit similar preferences regardless of training methods, and alignment fine-tuning does not significantly alter pretrained-only LLM preferences. The analysis also reveals that preference-based evaluation benchmarks are manipulable: aligning model responses with judge preferences improves scores (up to 0.59 on MT-Bench and 31.94 on AlpacaEval 2.0), while injecting least-preferred properties lowers them.

## Executive Summary
This paper investigates the fundamental differences between human and LLM preferences in pairwise response comparisons by systematically annotating responses with 29 predefined properties and using Bayesian logistic regression to quantify their effects. The study reveals that humans prefer lengthy, stance-supporting responses and are relatively insensitive to errors, while advanced LLMs like GPT-4-Turbo prioritize correctness, clarity, and harmlessness. The research also demonstrates that preference-based evaluation benchmarks are manipulable, as aligning responses with judge preferences significantly improves scores while injecting least-preferred properties reduces them.

## Method Summary
The study collects pairwise comparison data from both humans and LLMs, annotating each response with 29 predefined properties related to content quality, presentation style, and ethical considerations. Bayesian logistic regression models are fitted to quantify how each property influences judge preferences, with separate models for human judges and different LLM variants. The analysis examines preference patterns across different model sizes and training methods, including alignment fine-tuned models versus pretrained-only models. The researchers also conduct controlled experiments to test the manipulability of preference-based evaluation benchmarks by systematically adjusting response properties to align with or contradict judge preferences.

## Key Results
- Humans favor lengthy responses with clear stances but are less sensitive to errors and dislike model limit admissions
- Advanced LLMs like GPT-4-Turbo prioritize correctness, clarity, and harmlessness over response length
- LLMs of similar sizes exhibit similar preferences regardless of training methods, and alignment fine-tuning does not significantly alter pretrained-only LLM preferences
- Preference-based evaluation benchmarks are manipulable: aligning responses with judge preferences improves scores by up to 0.59 on MT-Bench and 31.94 on AlpacaEval 2.0

## Why This Works (Mechanism)
Assumption: The mechanism works because Bayesian logistic regression effectively captures the probabilistic relationship between response properties and judge preferences by modeling the log-odds of preference as a linear combination of property indicators. The hierarchical structure of the Bayesian approach allows for pooling information across different judges and model comparisons while accounting for individual judge variability and response-specific effects.

## Foundational Learning
Unknown: The paper does not explicitly discuss foundational learning principles that would explain why certain response properties influence preferences more than others. However, it can be inferred that the preference patterns may reflect underlying cognitive biases and decision-making heuristics that both humans and LLMs develop through their respective training processes, with humans favoring comprehensive and assertive responses while LLMs prioritize factual accuracy and clarity.

## Architecture Onboarding
Unknown: The paper does not provide specific architectural onboarding guidance for implementing preference-aware models. However, the findings suggest that models designed for human-facing applications should prioritize response length and clear stance-taking, while models for automated evaluation should emphasize correctness, clarity, and harmlessness. The similarity in preferences among LLMs of similar sizes indicates that architectural scaling factors may play a more significant role than training methodology differences.

## Open Questions the Paper Calls Out
None explicitly stated in the paper, though the findings raise implicit questions about the generalizability of preference patterns across different domains and the potential for developing more sophisticated preference models that capture non-linear relationships between properties.

## Limitations
- The 29 predefined properties may not capture all relevant aspects of response quality, potentially missing nuanced preferences that could influence judge decisions
- The pairwise comparison format may not reflect real-world conversational dynamics where responses are evaluated in isolation
- The study's focus on specific model sizes and training methods limits conclusions about broader LLM populations
- The relatively small number of properties directly manipulated in benchmark experiments (7 out of 29) may not fully represent the complexity of preference-based evaluation manipulation
- Cultural and linguistic biases in the human judge population may influence preference patterns in ways not fully accounted for
- The static nature of predefined properties may not capture dynamic preference shifts that occur in different conversational contexts

## Confidence
High confidence: The finding that humans prefer lengthy responses with clear stances, while being less sensitive to errors, is well-supported by the Bayesian logistic regression analysis with clear statistical significance. The observation that GPT-4-Turbo prioritizes correctness, clarity, and harmlessness over response length is also robustly demonstrated through consistent patterns across multiple comparisons.

Medium confidence: The claim that alignment fine-tuning does not significantly alter pretrained-only LLM preferences requires cautious interpretation due to potential confounding factors from different training datasets and methodologies. The assertion that LLMs of similar sizes exhibit similar preferences regardless of training methods shows consistent patterns but may not account for subtle architectural differences that could influence judgment behavior.

Low confidence: The specific numerical improvements in benchmark scores (0.59 on MT-Bench and 31.94 on AlpacaEval 2.0) from aligning responses with judge preferences should be interpreted cautiously, as these values depend heavily on the specific evaluation setup and may not generalize to other benchmark configurations or real-world applications.

## Next Checks
1. Conduct cross-cultural validation studies using diverse human judge populations to assess whether preference patterns hold across different cultural contexts and linguistic backgrounds, particularly for properties like stance-taking and response length.

2. Implement ablation studies testing the relative importance of different property subsets by systematically removing or combining properties to identify which combinations most strongly predict judge preferences, addressing potential overfitting concerns with the current 29-property framework.

3. Design controlled experiments varying response properties in continuous rather than binary ways (e.g., response length in word count ranges) to better understand preference gradients and non-linear relationships between properties and judge scores.

4. Investigate the temporal stability of preference patterns by conducting longitudinal studies to determine whether judge preferences evolve over time as users become more familiar with LLM capabilities and limitations.

5. Develop more sophisticated preference models that capture interaction effects between properties and test whether non-linear combinations of properties better predict judge preferences than the current linear approach.

6. Examine the transferability of preference patterns across different task domains to determine whether the observed differences between human and LLM preferences are task-specific or represent more fundamental preference divergences.