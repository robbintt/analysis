---
ver: rpa2
title: Are we making progress in unlearning? Findings from the first NeurIPS unlearning
  competition
arxiv_id: '2406.09073'
source_url: https://arxiv.org/abs/2406.09073
tags:
- unlearning
- different
- each
- algorithms
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first NeurIPS unlearning competition, which
  successfully attracted nearly 1,200 teams worldwide to develop novel machine unlearning
  algorithms. The authors analyze top-performing solutions and evaluate them using
  a principled framework that measures forgetting quality while accounting for model
  utility.
---

# Are we making progress in unlearning? Findings from the first NeurIPS unlearning competition

## Quick Facts
- arXiv ID: 2406.09073
- Source URL: https://arxiv.org/abs/2406.09073
- Reference count: 40
- Primary result: Top competition entries surpass existing state-of-the-art methods under evaluation framework

## Executive Summary
This paper presents the first NeurIPS unlearning competition, which attracted nearly 1,200 teams worldwide to develop novel machine unlearning algorithms. The authors analyze top-performing solutions and evaluate them using a principled framework that measures forgetting quality while accounting for model utility. Their evaluation methodology estimates a formal notion of unlearning through hypothesis testing, providing a robust way to benchmark different algorithms. The study finds that top competition entries surpass existing state-of-the-art methods under their evaluation framework, demonstrating significant progress in machine unlearning. The authors also investigate trade-offs between forgetting quality and utility, and analyze the generalizability of algorithms to new datasets. Their findings highlight both advancements in unlearning algorithms and important considerations for standardizing evaluation practices in this emerging field.

## Method Summary
The evaluation framework measures forgetting quality by estimating ε through hypothesis testing between unlearned and retrained model outputs. For each example in the forget set, the framework samples outputs from unlearned and retrained models, applies transformations (logit-scaling), and uses decision rules (single/double thresholds) to measure how well an attacker can separate the two distributions. The smallest ε across attacks is kept, with higher F-scores indicating better unlearning. The framework uses a computationally efficient reuse-N-1 configuration that draws N samples once and reuses them across experiments. Per-example ε values are aggregated using a binning strategy that awards points inversely proportional to the bin index. Final scores combine forgetting quality with utility adjustments based on model performance on retained and test data.

## Key Results
- Top competition entries achieved significantly better F-scores than existing state-of-the-art methods
- The reuse-N-1 configuration provides computationally efficient estimates while maintaining statistical validity
- Analysis reveals trade-offs between forgetting quality and model utility across different algorithms
- Some but not all top methods demonstrated good generalizability to new datasets under minimal hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The evaluation framework measures forgetting quality through hypothesis testing by estimating ε via distinguishing attacks between unlearned and retrained model outputs.
- **Mechanism:** For each example in the forget set, the framework samples outputs from unlearned and retrained models, applies transformations (logit-scaling), and uses decision rules (single/double thresholds) to measure how well an attacker can separate the two distributions. The smallest ε across attacks is kept, with higher F-scores indicating better unlearning.
- **Core assumption:** The output distributions of unlearned and retrained models are sufficiently different to allow meaningful separation through threshold-based attacks, and these differences directly reflect the degree of forgetting.
- **Evidence anchors:**
  - [abstract] "The evaluation methodology we developed for the competition measures forgetting quality according to a formal notion of unlearning, while incorporating model utility for a holistic evaluation."
  - [section 3.1] "To address the first challenge, we will consider (in Section 3.1.1) distributions of (scalar) outputs of unlearned and retrained models when given as input examples from S..."
  - [corpus] Weak - no direct corpus support for threshold-based attacks, but similar MIA approaches exist in privacy literature
- **Break condition:** If the output distributions are too similar or too noisy, attacks cannot distinguish them reliably, making ε estimation meaningless regardless of compute resources.

### Mechanism 2
- **Claim:** The competition setup's reuse-N-1 configuration provides a computationally efficient yet reasonably accurate estimate of forgetting quality by reusing samples across experiments.
- **Mechanism:** Instead of drawing fresh samples for each of E experiments, the framework draws N samples from original and retrained models once, then applies the unlearning algorithm to each original model sample. This yields N unlearned samples reused across E experiments, dramatically reducing compute while maintaining statistical validity.
- **Core assumption:** The distribution of unlearned models is stable enough across different starting points from the original model that reusing a single set of original models across experiments doesn't bias the results.
- **Evidence anchors:**
  - [abstract] "We analyze the effectiveness of different instantiations of this evaluation framework vis-a-vis the associated compute cost..."
  - [section 4] "Setup 'Reuse-N-1' uses a single sample of θo to obtain all samples of θu, and a single set of N samples of θr are reused across all experiments..."
  - [corpus] Weak - no direct corpus evidence for reuse-N-1 validity, but standard practice in ML competitions
- **Break condition:** If the unlearning algorithm is highly sensitive to the initialization of the original model, reusing samples will systematically underestimate or overestimate forgetting quality.

### Mechanism 3
- **Claim:** The binning strategy for aggregating per-example ε values provides a robust measure of overall forgetting quality that is less sensitive to noise than direct averaging.
- **Mechanism:** Each example's ε is mapped to a bin index (floor(ε/bin-width)), then H(ε) = 2^(-n(ε)) awards points inversely proportional to the bin index. The overall F-score averages these H-values across the forget set, rewarding algorithms that achieve low ε across many examples rather than just a few.
- **Core assumption:** The distribution of per-example ε values contains meaningful structure about algorithm performance that is preserved through binning rather than lost in simple averaging.
- **Evidence anchors:**
  - [section 3.1.2] "We chose to use binning as it is more granular than a simple average or computing quantiles and less sensitive to noise compared to directly using estimated ε's."
  - [section A.9] "We observe that several algorithms span a wide range of ε values, but we do see differences between their distributions."
  - [corpus] Weak - no direct corpus support for this specific binning approach, but binning is standard in statistical analysis
- **Break condition:** If the bin width is poorly chosen relative to the ε distribution, the aggregation will either lose resolution (too wide) or become too noisy (too narrow).

## Foundational Learning

- **Concept:** Hypothesis testing and false positive/negative rates
  - Why needed here: The evaluation framework fundamentally relies on treating unlearning as a hypothesis test between two distributions, where the ability to distinguish them indicates failure of unlearning
  - Quick check question: If an attacker achieves FPR=0.1 and FNR=0.2 at δ=0.01, what is the estimated ε according to Equation 1?

- **Concept:** Distribution comparison and statistical distance
  - Why needed here: The framework needs to quantify how "close" the unlearned model distribution is to the retrained model distribution, which requires understanding measures of distribution similarity
  - Quick check question: Why might measuring distance in weight space be problematic for comparing neural network models?

- **Concept:** Confidence intervals and statistical reproducibility
  - Why needed here: The framework uses multiple experiments (E) to produce confidence intervals around F-scores, requiring understanding of how to aggregate results from repeated experiments
  - Quick check question: If Setup "Full" uses E=20 experiments each with N=1024 samples, how many total model evaluations are required compared to Setup "Reuse-N-1"?

## Architecture Onboarding

- **Component map:** Data preprocessing -> Model training -> Unlearning algorithms -> Evaluation engine -> Hyperparameter tuning
- **Critical path:**
  1. Train original model on full training set
  2. Create retain/forget split (2% forget set)
  3. For each unlearning algorithm:
     - Sample N original models
     - Apply unlearning to get N unlearned models
     - Sample N retrained models
     - For each forget set example, run attacks and compute ε
     - Aggregate to F-score and adjust for utility
  4. Compare final scores across algorithms

- **Design tradeoffs:**
  - Accuracy vs efficiency: Setup "Full" is most accurate but requires N×E model evaluations vs Reuse-N-1 requiring only N
  - Granularity vs robustness: Per-example ε provides detailed analysis but requires aggregation strategy (binning chosen over averaging)
  - Attack strength vs overfitting: Stronger attacks give better ε estimates but risk overfitting to specific samples

- **Failure signatures:**
  - Poor F-scores with good utility suggest unlearning algorithm fails to remove information
  - High variance across E experiments indicates unstable unlearning behavior
  - Disproportionate drop in retain vs test accuracy suggests overfitting to training distribution
  - Large confidence intervals suggest insufficient samples or high sensitivity to initialization

- **First 3 experiments:**
  1. Run Finetune baseline on CASIA-SURF with default hyperparameters to establish baseline F-score and verify evaluation pipeline
  2. Implement and test single-threshold attack on synthetic data where distributions are known to be separable, verify ε estimation matches theoretical values
  3. Compare Setup "Full" vs Reuse-N-1 on a small subset of algorithms to quantify accuracy loss from reuse, using N=128 and E=5 for quick iteration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific design decisions for the attacks used in the evaluation framework would yield the most accurate and efficient estimation of forgetting quality?
- Basis in paper: [explicit] The paper discusses that the evaluation framework is agnostic to the particular choice of attacks and hopes future work improves upon the current choice of attacks.
- Why unresolved: The current framework uses two families of attacks (single-threshold and double-threshold), but the paper acknowledges this may not be optimal and future work could explore more complex decision rules or different attack designs.
- What evidence would resolve it: Comparative studies of different attack designs on the same unlearning algorithms and datasets, measuring both accuracy and computational efficiency.

### Open Question 2
- Question: How does the choice of aggregation strategy for per-example ε values affect the overall forgetting quality estimate F?
- Basis in paper: [explicit] The paper discusses using binning to aggregate per-example ε values and mentions investigating different alternatives initially, such as returning the maximum ε across examples or computing quantiles.
- Why unresolved: The paper chose binning as it is more granular than averaging or computing quantiles, but acknowledges that future iterations could improve upon this choice and doesn't provide a definitive answer on which aggregation strategy is best.
- What evidence would resolve it: Systematic comparison of different aggregation strategies (e.g., maximum, quantiles, binning) on the same unlearning algorithms and datasets, evaluating their sensitivity to noise and ability to distinguish between algorithms.

### Open Question 3
- Question: To what extent can unlearning algorithms generalize to new datasets with minimal hyperparameter tuning?
- Basis in paper: [explicit] The paper conducts an experiment on FEMNIST to probe the generalizability and ease of reusability of methods, finding that some but not all top competition methods outperform existing ones under minimal-adaptation settings.
- Why unresolved: The experiment provides initial insights but has limitations, such as using a limited grid of hyperparameter values and only two datasets. The paper emphasizes that poor performance under minimal adaptation doesn't necessarily imply inability to perform well with extensive tuning.
- What evidence would resolve it: Extensive experiments on a diverse set of datasets, architectures, and training algorithms, comparing the performance of unlearning algorithms with and without extensive hyperparameter tuning, and investigating factors that influence generalizability.

## Limitations

- The evaluation framework's reliance on hypothesis testing for ε estimation has practical limitations in attack strength and aggregation methods
- The reuse-N-1 computational efficiency approach may introduce bias that is not fully characterized
- The generalizability analysis is limited by single dataset evaluation, making broad claims about algorithm robustness premature

## Confidence

**High Confidence:** The empirical finding that top competition entries surpass existing state-of-the-art methods under the evaluation framework is well-supported by the 1,300+ submissions and systematic comparison with baseline algorithms.

**Medium Confidence:** The claim that the evaluation methodology provides a "principled" and "holistic" evaluation is supported by theoretical framework but has practical limitations in attack strength and aggregation methods.

**Low Confidence:** The generalizability analysis of algorithms to new datasets is limited by the single dataset evaluation (CASIA-SURF), making broad claims about algorithm robustness premature.

## Next Checks

1. **Attack Sensitivity Analysis:** Systematically vary attack parameters (thresholds, transformation methods) to determine how sensitive ε estimates are to attack configuration, and validate that results are stable across reasonable parameter ranges.

2. **Alternative Aggregation Comparison:** Implement and compare alternative aggregation methods (weighted averaging, quantile-based approaches) against the binning strategy to quantify the impact of aggregation choice on final rankings and scores.

3. **Initialization Sensitivity Test:** Run the Reuse-N-1 setup with multiple different initial original model samples to measure variance in F-scores and determine whether the computational efficiency gain comes at the cost of statistical reliability.