---
ver: rpa2
title: 'PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language
  Models as Decision Makers'
arxiv_id: '2406.12430'
source_url: https://arxiv.org/abs/2406.12430
tags:
- decision
- node
- data
- building
- locating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Decision QA, a new decision-making task where
  a model answers the best decision given a decision-making question, business rules,
  and a database. The authors create a benchmark DQA with two scenarios (Locating
  and Building) extracted from video games to evaluate Decision QA.
---

# PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers

## Quick Facts
- arXiv ID: 2406.12430
- Source URL: https://arxiv.org/abs/2406.12430
- Reference count: 40
- Primary result: PlanRAG achieves 15.8% and 7.4% improvements over iterative RAG on two video game-derived decision-making benchmarks

## Executive Summary
This paper introduces PlanRAG, a new retrieval-augmented generation technique designed for decision-making tasks. PlanRAG implements a three-phase approach where an LLM first creates a structured plan for data analysis, then generates queries to retrieve data based on that plan, and finally evaluates results to either answer or re-plan. The authors create a novel Decision QA benchmark (DQA) using scenarios extracted from video games Europa Universalis IV and Victoria 3, where models must choose optimal decisions given business rules and databases. PlanRAG demonstrates significant performance improvements over iterative RAG methods on both graph and relational database scenarios.

## Method Summary
PlanRAG operates through a sequential process where the LLM first analyzes the decision question and database schema to generate an initial plan, then produces data analysis queries based on this plan, retrieves results from the database, and finally evaluates whether to re-plan or generate an answer. The method uses GPT-4 with zero temperature and LangChain library implementation. The prompt structure follows a 'Plan'-'Thought'-'Action'-'Observation'-'Re-plan' format. The approach is tested on two scenarios (Locating and Building) from video games, using both labeled property graph and relational databases. The system measures accuracy by comparing generated decisions against ground truth answers obtained through game simulators that calculate decision effects.

## Key Results
- PlanRAG achieves 15.8% accuracy improvement over iterative RAG on the Locating scenario
- PlanRAG achieves 7.4% accuracy improvement over iterative RAG on the Building scenario
- PlanRAG shows significantly lower rates of missed data analysis (1.3% vs 3.3% in Locating, 21.8% vs 33.2% in Building) compared to iterative RAG

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PlanRAG outperforms iterative RAG by first making a structured plan for data analysis, then retrieving data based on that plan, and finally answering or re-planning if needed.
- Mechanism: The LLM performs three distinct reasoning phases: (1) Planning - examines schema and question to decide what analysis is needed, (2) Retrieving & Answering - generates queries to fetch data according to the plan, (3) Re-planning - evaluates results and decides whether to adjust the plan or proceed to answer.
- Core assumption: Structured planning before retrieval leads to more systematic and effective data access than iterative retrieval without upfront planning.
- Evidence anchors: The abstract states PlanRAG "generates the plan for decision making as the first step, and the retriever generates the queries for data analysis as the second step." Section 4 describes the two types of reasoning: making a plan and answering based on retrieved results.

### Mechanism 2
- Claim: The iterative plan-then-retrieval approach reduces missed data analysis compared to iterative RAG.
- Mechanism: By planning first, the system identifies all necessary data points before retrieval, reducing the chance of missing critical values needed for decision making.
- Core assumption: Planning identifies all necessary data analysis steps upfront, leading to fewer missed retrievals.
- Evidence anchors: Section 5.2 reports PlanRAG has low missed data rates (1.3% and 21.8%) compared to IterRAG (3.3% and 33.2%), suggesting better coverage of necessary data analysis.

### Mechanism 3
- Claim: Re-planning capability allows PlanRAG to correct suboptimal initial plans and improve decision quality.
- Mechanism: After each retrieval, the LLM evaluates whether the current plan is sufficient and can generate a new plan if needed, allowing for iterative refinement of the analysis approach.
- Core assumption: Initial plans may be insufficient, and the ability to re-plan based on retrieved results leads to better decisions.
- Evidence anchors: Section 4 states re-planning occurs when "the initial plan is not good enough to solve the decision question." Section 5.2 shows no re-planning leads to accuracy decreases of 10.8% in Locating and 0.9% in Building.

## Foundational Learning

- Concept: Decision QA task definition and structure
  - Why needed here: Understanding the task format is essential for implementing PlanRAG correctly
  - Quick check question: What are the three inputs to the Decision QA task?

- Concept: Graph and relational database querying (Cypher/SQL)
  - Why needed here: PlanRAG generates queries for both graph and relational databases
  - Quick check question: What is the key difference between Cypher and SQL query generation?

- Concept: RAG (Retrieval-Augmented Generation) fundamentals
  - Why needed here: PlanRAG is an extension of RAG techniques
  - Quick check question: What are the three main phases of traditional RAG?

## Architecture Onboarding

- Component map: LLM Agent (PlanRAG-based) -> Database Interface -> Plan Management -> Answer Generation

- Critical path: 1. Receive input (Q, R, D) -> 2. Generate initial plan -> 3. Execute queries based on plan -> 4. Evaluate results and decide on re-planning -> 5. Generate final answer

- Design tradeoffs: Single LLM vs. multiple specialized models, Planning depth vs. execution speed, Query complexity vs. retrieval accuracy

- Failure signatures: Frequent re-planning indicates poor initial planning, Query generation failures suggest model limitations, Inconsistent answers across runs indicate instability

- First 3 experiments: 1. Test PlanRAG on a simple Locating scenario with known answer, 2. Compare query generation quality between PlanRAG and iterative RAG, 3. Evaluate re-planning frequency on increasingly complex scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would PlanRAG perform on decision-making tasks involving hybrid databases (combining relational, graph, and vector databases)?
- Basis in paper: The paper mentions this as a limitation, noting that Decision QA using other database types like hybrid forms could be explored in future research.
- Why unresolved: The current study focuses only on relational and graph databases, leaving the effectiveness of PlanRAG on hybrid database systems unexplored.
- What evidence would resolve it: Implementing PlanRAG on a benchmark that includes hybrid databases and comparing its performance against iterative RAG methods would provide empirical evidence.

### Open Question 2
- Question: Would using multiple specialized language models instead of a single model improve the effectiveness of PlanRAG?
- Basis in paper: The paper states that the effectiveness of PlanRAG in a multiple language models framework is not a focus and is left as a further study.
- Why unresolved: The current implementation uses a single LM to perform both planning and retrieval tasks, which may have limitations compared to a multi-model approach.
- What evidence would resolve it: Conducting experiments with a multi-model framework where different models handle planning and retrieval separately, then comparing the results with the single-model PlanRAG approach.

### Open Question 3
- Question: How sensitive is PlanRAG's performance to the choice of base language model (e.g., GPT-4 vs. open-source models like Llama 2 or Phi-2)?
- Basis in paper: The paper shows that PlanRAG-LM and IterRAG-LM using Llama-2 and Phi-2 models cannot solve any problems in DQA, while GPT-3.5-turbo shows better performance for IterRAG-LM than PlanRAG-LM.
- Why unresolved: The experiments demonstrate significant variation in performance across different models, but the reasons for these differences and whether fine-tuning could improve open-source model performance are not explored.
- What evidence would resolve it: Fine-tuning open-source models on the DQA benchmark and comparing their performance with GPT models, along with ablation studies to identify key factors affecting performance.

## Limitations

- The evaluation relies on artificial benchmarks derived from video games rather than real-world business decision-making scenarios, limiting generalizability
- Performance improvements are measured only against a single baseline (iterative RAG) without comparisons to other contemporary decision-making approaches
- The study lacks ablation experiments to understand the individual contributions of planning, querying, and re-planning components

## Confidence

**High Confidence**: The core methodology of PlanRAG is clearly described and the implementation details are sufficient for reproduction. The performance improvements over iterative RAG on the DQA benchmark are statistically supported by the reported results.

**Medium Confidence**: The mechanism explanations for why planning-first leads to better performance are logical but not empirically validated through controlled experiments. The claims about reduced missed data analysis and improved systematic reasoning are supported by comparative statistics but lack deeper causal analysis.

**Low Confidence**: Claims about PlanRAG's generalizability to real-world business decision-making scenarios are not supported by evidence beyond the video game-derived benchmarks. The assertion that PlanRAG represents a significant advance in decision-making AI lacks validation on diverse, real-world datasets.

## Next Checks

1. **Domain Transfer Validation**: Test PlanRAG on at least two real-world business datasets (e.g., financial decision-making, healthcare treatment planning) to assess generalizability beyond the video game benchmarks. Compare performance against both iterative RAG and rule-based expert systems.

2. **Ablation Study on Planning Quality**: Systematically disable or modify each component of the PlanRAG pipeline (planning, querying, re-planning) to measure the individual contribution of each phase to overall performance. This should include testing without re-planning, with random plans, and with oracle plans to establish upper bounds.

3. **Scalability and Efficiency Analysis**: Measure the computational overhead and latency of PlanRAG compared to iterative RAG across scenarios of increasing complexity. Document the frequency and cost of re-planning iterations, and establish thresholds where the planning overhead outweighs the accuracy benefits.