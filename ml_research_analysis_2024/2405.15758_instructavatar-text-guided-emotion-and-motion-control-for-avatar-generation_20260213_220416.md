---
ver: rpa2
title: 'InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation'
arxiv_id: '2405.15758'
source_url: https://arxiv.org/abs/2405.15758
tags:
- motion
- emotion
- control
- talking
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InstructAvatar introduces a text-guided approach to control emotional
  and facial motion in 2D avatar generation. The method constructs a fine-grained
  instruction-video paired dataset using automatic annotation, leveraging GPT-4V for
  AU paraphrasing and emotion intensity annotation.
---

# InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation

## Quick Facts
- arXiv ID: 2405.15758
- Source URL: https://arxiv.org/abs/2405.15758
- Reference count: 40
- Outperforms existing methods in emotion controllability, lip-sync quality, and naturalness, achieving 0.552 AUF1 and 9.653 SyncD in-domain

## Executive Summary
InstructAvatar introduces a text-guided approach for controlling emotional and facial motion in 2D avatar generation. The method constructs a fine-grained instruction-video paired dataset using automatic annotation, leveraging GPT-4V for AU paraphrasing and emotion intensity annotation. A two-branch diffusion-based generator is designed to handle emotion and motion instructions separately, with zero-convolution gates to stabilize training. Experiments show InstructAvatar outperforms existing methods in emotion controllability, lip-sync quality, and naturalness.

## Method Summary
InstructAvatar constructs a fine-grained instruction-video paired dataset using automatic annotation, leveraging GPT-4V for AU paraphrasing and emotion intensity annotation. The method trains a two-branch diffusion-based generator to handle emotion and motion instructions separately, with zero-convolution gates to stabilize training. The model integrates audio and text instructions through cross-attention mechanisms, enabling unified control of both emotion and motion in avatar generation.

## Key Results
- Achieves 0.552 AUF1 and 9.653 SyncD in-domain
- Outperforms existing methods in emotion controllability, lip-sync quality, and naturalness
- Supports direct facial motion control without audio and generalizes well to out-of-domain scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-branch diffusion model with emotion and motion cross-attention allows separate and specialized control of high-level emotional style and low-level facial motion instructions.
- Mechanism: Emotion branch uses the `[EOS]` token to inject global emotional guidance, while motion branch uses all CLIP text tokens to inject temporally dynamic motion control. Zero-convolution gates gradually enable text conditioning without destroying pretrained knowledge.
- Core assumption: Emotion and motion instructions require different granularity and temporal characteristics, and a unified branch would mix these incompatible signals.
- Evidence anchors:
  - [abstract] "a novel two-branch diffusion-based generator to predict avatars with audio and text instructions at the same time."
  - [section 3.3] "For the emotion, the text provides style guidance throughout the entire video... For the motion branch, we utilize the hidden states of all tokens... to capture more detailed and dynamic information."
- Break condition: If the zero-convolution gate fails to initialize to zero, the pretrained weights would be overridden early and destabilize training.

### Mechanism 2
- Claim: Fine-grained text instructions derived from Action Units (AUs) and GPT-4V paraphrasing provide richer facial control than tag-level emotion labels.
- Mechanism: AU detection on video frames is intersected to obtain consistent facial muscle states, then GPT-4V paraphrases AUs into natural sentences, correcting or supplementing detection errors.
- Core assumption: AU detection is frame-level and noisy; intersection across frames and GPT-4V refinement yields more reliable instructions.
- Evidence anchors:
  - [section 3.2] "We randomly select three frames from a video and employ an off-the-shelf AU detection model... We then take the intersection of predicted action units..."
  - [section 3.2] "Leveraging its vision capabilities, we provide GPT-4V with a frame extracted from the video and allow it to edit the action units..."
- Break condition: If GPT-4V misinterprets AUs or hallucinates details inconsistent with the video, the instruction-video pairing becomes noisy.

### Mechanism 3
- Claim: The pseudo-empty audio placeholder enables unified training for both audio-driven emotional talking and text-driven facial motion control.
- Mechanism: During motion-only training, zero-amplitude audio with matching length is passed through the same audio encoder; this preserves the model's temporal alignment and avoids a separate motion-only branch.
- Core assumption: Audio features carry temporal structure; replacing audio with zero vectors would lose this structure and hurt alignment.
- Evidence anchors:
  - [section 3.3] "For facial motion control absent of audio, we use pseudo empty audio with zero amplitude and a length aligned with the ground truth video."
  - [section 4.3] "When the AU loss is removed, the model loses some strict guidance on capturing fine-grained action unit details..."
- Break condition: If empty audio is replaced with arbitrary zero tensors (no physical meaning), the model confuses alignment and degrades motion control.

## Foundational Learning

- Concept: Diffusion models and denoising score matching
  - Why needed here: The model uses a diffusion-based motion generator to predict motion latents conditioned on audio and text; understanding the forward/backward process is essential for modifying or debugging it.
  - Quick check question: In a diffusion model, what does the denoising network learn to predict at each timestep?
- Concept: Cross-attention and modality fusion
  - Why needed here: The model injects CLIP-encoded text instructions via cross-attention; knowing how queries, keys, and values interact is key for understanding control signal flow.
  - Quick check question: In cross-attention, which tensor acts as the query, and which act as keys and values?
- Concept: Action Units (AU) and FACS
  - Why needed here: AU detection provides the fine-grained facial muscle descriptions that GPT-4V paraphrases into instructions; understanding AU semantics is critical for data annotation and evaluation.
  - Quick check question: What is the difference between an AU code like "lip_corner_puller" and a high-level emotion label like "happy"?

## Architecture Onboarding

- Component map: VAE (motion encoder + appearance encoder + decoder) -> Motion generator (Conformer-based diffusion model with two text-aware branches) -> Audio encoder (Wave2Vec 2.0) -> Text encoder (CLIP text encoder) -> AU classifier heads
- Critical path: Portrait → VAE appearance encoder → Motion generator (audio + text → denoising) → VAE decoder → output video
- Design tradeoffs:
  - Two-branch design increases complexity but enables fine-grained emotion vs motion control
  - Using CLIP text encoder trades off instruction-specific fine control for cross-modal generalization
- Failure signatures:
  - Lip-sync degrades → check audio conditioning or pose predictor loss
  - Emotion control weak → check AU loss or zero-convolution gate initialization
  - Motion control fails → check CLIP motion branch adapter or pseudo audio alignment
- First 3 experiments:
  1. Remove AU loss and retrain; check AUF1 drop
  2. Switch emotion branch to use all tokens instead of `[EOS]`; check SyncD and AUEmo
  3. Replace pseudo-empty audio with all-zero tensors; check motion control metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does InstructAvatar handle the challenge of simultaneously controlling both emotion and motion, given that the training dataset typically follows a single emotion/motion pattern?
- Basis in paper: The paper states, "Moreover, since almost all data in our training dataset follows a single emotion/motion pattern, it is challenging for our model to control both emotion and motion simultaneously."
- Why unresolved: The paper acknowledges this limitation but does not provide a solution or experimental results addressing this specific challenge.
- What evidence would resolve it: Experimental results demonstrating InstructAvatar's performance when attempting to control both emotion and motion simultaneously, along with proposed methods to overcome this limitation.

### Open Question 2
- Question: How does the zero-convolution mechanism specifically stabilize the training process and improve the guidance in InstructAvatar?
- Basis in paper: The paper mentions, "Specifically, suppose we have hidden states hi−1 before entering the cross-attention module with text instructions. We then use hi = hi−1 + Z(Cross-Attn(h,Rep(T ))) to get the next hidden states. In this formula, Z is a zero convolution operation where a 1-dimensional convolutional kernel moves along the hidden states dimension, with both the weight and bias initialized to zero."
- Why unresolved: While the paper describes the zero-convolution mechanism, it does not provide detailed analysis or ablation studies to quantify its impact on training stability and guidance improvement.
- What evidence would resolve it: Detailed ablation studies comparing the performance of InstructAvatar with and without the zero-convolution mechanism, including metrics for training stability and guidance quality.

### Open Question 3
- Question: How does InstructAvatar's performance compare when using pseudo-empty audio versus actual audio for facial motion control?
- Basis in paper: The paper states, "To integrate facial motion control where no audio is provided into our unified framework, we use pseudo-empty audio as a placeholder."
- Why unresolved: The paper does not provide experimental results comparing the performance of InstructAvatar using pseudo-empty audio versus actual audio for facial motion control.
- What evidence would resolve it: Comparative experiments evaluating InstructAvatar's performance on facial motion control tasks using both pseudo-empty audio and actual audio inputs, with metrics such as motion accuracy and naturalness.

## Limitations
- The reliance on GPT-4V for AU paraphrasing introduces uncertainty about instruction quality and consistency across different videos
- The zero-convolution gate mechanism lacks empirical validation of its contribution
- The pseudo-empty audio approach for motion-only control is theoretically elegant but may introduce artifacts

## Confidence
**High Confidence**:
- The two-branch architecture successfully separates emotion and motion control signals
- The dataset construction methodology (AU detection + GPT-4V paraphrasing) produces usable instruction-video pairs
- The model achieves superior performance on AUF1, SyncD, and FID metrics compared to baselines

**Medium Confidence**:
- The zero-convolution gates meaningfully stabilize training (lacks ablation)
- The pseudo-empty audio approach is optimal for motion-only control (not compared to alternatives)
- The model generalizes well to out-of-domain scenarios (tested on limited datasets)

**Low Confidence**:
- GPT-4V paraphrasing consistently improves instruction quality over raw AU detection
- The intersection-based AU detection method captures all relevant emotional expressions
- The CLIP text encoder provides optimal instruction representation for avatar control

## Next Checks
1. **Zero-convolution gate ablation**: Remove the zero-convolution gates entirely and retrain the model. Compare training stability, convergence speed, and final performance metrics to the original implementation.
2. **Motion-only control alternatives**: Replace the pseudo-empty audio approach with either (a) explicit motion-only branch without audio input, or (b) random noise audio signals. Evaluate whether motion control quality differs significantly from the current pseudo-empty approach.
3. **GPT-4V reliability test**: Manually annotate a subset of videos with ground-truth AU descriptions and compare them to GPT-4V's paraphrases. Calculate the agreement rate and identify systematic errors or biases in the automatic annotation pipeline.