---
ver: rpa2
title: Block Selective Reprogramming for On-device Training of Vision Transformers
arxiv_id: '2405.10951'
source_url: https://arxiv.org/abs/2405.10951
tags: []
core_contribution: This paper addresses the challenge of on-device fine-tuning of
  vision transformers (ViTs) under memory and computational constraints. The proposed
  Block Selective Reprogramming (BSR) method selectively fine-tunes a fraction of
  blocks in a pre-trained model and employs token dropping based on self-attention
  scores of frozen layers to reduce memory and compute requirements.
---

# Block Selective Reprogramming for On-device Training of Vision Transformers

## Quick Facts
- arXiv ID: 2405.10951
- Source URL: https://arxiv.org/abs/2405.10951
- Reference count: 40
- Key outcome: Achieves up to 1.4× memory reduction and 2× compute cost reduction for on-device ViT fine-tuning while maintaining accuracy

## Executive Summary
This paper introduces Block Selective Reprogramming (BSR), a method for efficient on-device fine-tuning of vision transformers under memory and computational constraints. BSR selectively fine-tunes a fraction of blocks in pre-trained models and employs token dropping based on self-attention scores of frozen layers. The approach significantly reduces memory and compute requirements while maintaining comparable accuracy to full fine-tuning. Experimental results demonstrate 5.47× training memory reduction and 2.43× reduction in training time for ViT-B on CIFAR-10 with batch size 32.

## Method Summary
BSR combines selective block freezing with token dropping to enable efficient on-device training of vision transformers. The method identifies which blocks to fine-tune based on their contribution to task performance, while keeping other blocks frozen. During forward passes, tokens with low self-attention scores in frozen layers are dropped to reduce computational load. This selective approach allows the model to adapt to new tasks while minimizing resource usage. The technique is also shown to be effective for Mixture-of-Expert models in multi-task learning scenarios, where it reduces training memory by 2.3× and FLOPs by 1.5× while maintaining baseline accuracy.

## Key Results
- 5.47× training memory reduction for ViT-B on CIFAR-10 with batch size 32
- 2.43× reduction in training time while maintaining comparable accuracy
- Up to 1.4× memory reduction and 2× compute cost reduction compared to existing approaches
- Effective for Mixture-of-Expert models with 2.3× training memory reduction and 1.5× FLOPs reduction

## Why This Works (Mechanism)
BSR exploits the observation that not all transformer blocks contribute equally to task adaptation. By selectively fine-tuning only the most relevant blocks and freezing others, the method reduces the number of parameters that need gradient computation and memory storage. Token dropping further reduces computation by eliminating less informative tokens early in the network. This dual approach addresses both memory (fewer active parameters) and compute (fewer tokens processed) constraints simultaneously, making on-device fine-tuning feasible on resource-limited devices.

## Foundational Learning
- Vision Transformer architecture: Essential for understanding how BSR modifies standard ViT training
- Self-attention mechanisms: Critical for token importance scoring and selective dropping
- Parameter-efficient fine-tuning: Provides context for BSR's selective approach compared to adapters, LoRA, etc.
- Mixture-of-Experts models: Relevant for understanding BSR's application to MoE architectures
- On-device machine learning constraints: Frames why memory and compute reduction techniques are necessary
- Fine-tuning vs. full training: Establishes the baseline for comparing BSR's efficiency gains

## Architecture Onboarding
Component map: Input tokens -> Self-attention scoring -> Token dropping -> Frozen blocks (identity) -> Fine-tuned blocks -> Output
Critical path: Token scoring and dropping occurs in frozen layers before fine-tuned blocks process the remaining tokens
Design tradeoffs: Selective freezing reduces memory but may limit adaptation capacity; aggressive token dropping saves compute but risks losing information
Failure signatures: Over-aggressive freezing leads to poor adaptation; excessive token dropping causes accuracy degradation
First experiments: 1) Ablation study on percentage of fine-tuned blocks, 2) Analysis of token dropping thresholds on accuracy, 3) Memory vs. accuracy tradeoff curves across different batch sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to CIFAR-10 and CIFAR-100 datasets, raising questions about generalizability to larger, more complex datasets
- Selective block freezing relies on empirical heuristics without theoretical guarantees or comprehensive ablation studies
- Effectiveness on different vision tasks (detection, segmentation) and alternative transformer architectures remains unexplored

## Confidence
High: Core technical contribution of combining selective block fine-tuning with token dropping is sound
Medium: Accuracy preservation claims supported by experiments but limited dataset/architecture diversity reduces confidence
Low: Generalizability claims to other transformer variants and larger-scale datasets lack sufficient empirical support

## Next Checks
1. Evaluate BSR on ImageNet-1K and real-world edge devices to verify practical deployment benefits
2. Conduct comprehensive ablation studies varying fine-tuned block percentages and token dropping strategies
3. Test BSR on alternative transformer architectures including Swin Transformers and hybrid CNN-Transformer models