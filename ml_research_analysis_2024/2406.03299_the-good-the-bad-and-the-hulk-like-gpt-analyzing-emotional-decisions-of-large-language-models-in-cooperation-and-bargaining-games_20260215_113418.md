---
ver: rpa2
title: 'The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of
  Large Language Models in Cooperation and Bargaining Games'
arxiv_id: '2406.03299'
source_url: https://arxiv.org/abs/2406.03299
tags: []
core_contribution: This paper explores how emotions influence the decision-making
  of large language models (LLMs) in strategic games. The authors introduce a novel
  framework to study both LLM decision-making and alignment with human behavior under
  emotional states.
---

# The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games

## Quick Facts
- arXiv ID: 2406.03299
- Source URL: https://arxiv.org/abs/2406.03299
- Reference count: 40
- One-line primary result: Emotions profoundly impact LLM decision-making in strategic games, leading to more optimal strategies and altering alignment with human behavior

## Executive Summary
This paper introduces a novel framework to study how emotions influence the decision-making of large language models (LLMs) in strategic games. The authors inject five basic emotions—anger, sadness, happiness, disgust, and fear—into GPT-3.5 and GPT-4 to examine their effects on game outcomes. Experiments on bargaining games (Ultimatum, Dictator) and repeated games (Prisoner's Dilemma, Battle of the Sexes) reveal that emotions significantly impact LLM performance and alignment with human behavior. Surprisingly, emotional prompting can disrupt GPT-4's typically consistent "superhuman" alignment, making it more human-like in its responses.

## Method Summary
The study develops a framework integrating emotional inputs into LLM decision-making through prompt-chaining techniques. The approach uses predefined emotional states injected before gameplay, with separate pipelines for repeated games and bargaining games. The framework includes game descriptions, initial emotions, and memory updates after each round. Experiments compare LLM decisions against human responses and optimal strategies using GPT-3.5 and GPT-4 across four games: Ultimatum, Dictator, Prisoner's Dilemma, and Battle of the Sexes.

## Key Results
- Emotions profoundly impact LLM performance, leading to more optimal strategies in certain game settings
- GPT-3.5 aligns well with human responses in bargaining games, while GPT-4 exhibits consistent behavior, ignoring induced emotions
- Emotional prompting, especially with anger, can disrupt GPT-4's "superhuman" alignment, making it more human-like in decision-making

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Emotions influence LLM decision-making, leading to more optimal strategies in certain game settings.
- Mechanism: By injecting predefined emotional states into LLMs prior to gameplay, the models' internal reasoning processes are altered, resulting in different strategic choices that can be more aligned with human behavior or more optimal in specific game scenarios.
- Core assumption: Emotional prompting effectively simulates human-like emotional responses in LLMs, affecting their decision-making processes in a manner similar to humans.
- Evidence anchors:
  - [abstract]: "emotions profoundly impact the performance of LLMs, leading to the development of more optimal strategies."
  - [section]: "we introduce a novel methodology and the framework to study both, the decision-making of LLMs and their alignment with human behavior under emotional states."
- Break condition: If the emotional prompting does not consistently influence LLM behavior or if the resulting strategies are not more optimal or aligned with human responses, this mechanism would break.

### Mechanism 2
- Claim: GPT-3.5 aligns well with human responses in bargaining games, while GPT-4 exhibits consistent behavior, ignoring induced emotions.
- Mechanism: The inherent differences in the architecture and training of GPT-3.5 and GPT-4 lead to different responses when emotions are induced. GPT-3.5, being more similar to human reasoning patterns, is more susceptible to emotional influences, while GPT-4's advanced training and optimization make it more resistant to such influences.
- Core assumption: The differences in alignment and emotional responsiveness between GPT-3.5 and GPT-4 are due to their underlying architectures and training processes.
- Evidence anchors:
  - [abstract]: "While there is a strong alignment between the behavioral responses of GPT-3.5 and human participants, particularly evident in bargaining games, GPT-4 exhibits consistent behavior, ignoring induced emotions for rationality decisions."
  - [section]: "Experiments with GPT-3.5 and GPT-4 on four games from two different classes of behavioral game theory showed that emotions profoundly impact the performance of LLMs, leading to the development of more optimal strategies."
- Break condition: If GPT-4's behavior is not consistently more rational or if GPT-3.5's alignment with human responses is not as strong as observed, this mechanism would break.

### Mechanism 3
- Claim: Emotional prompting, particularly with anger, can disrupt GPT-4's "superhuman" alignment, making it more human-like.
- Mechanism: By inducing specific emotional states, especially anger, in GPT-4, the model's decision-making process is altered in a way that deviates from its usual optimal behavior, resulting in choices that are more similar to those made by humans under the influence of emotions.
- Core assumption: Even advanced LLMs like GPT-4 are susceptible to the influence of emotions, and their decision-making processes can be disrupted in a manner similar to humans.
- Evidence anchors:
  - [abstract]: "Surprisingly, emotional prompting, particularly with ‘anger’ emotion, can disrupt the “superhuman” alignment of GPT-4, resembling human emotional responses."
  - [section]: "we find that LLM agents driven by fear and anger adapt the alternating pattern, which is optimal in the Battle of the Sexes game, early in the game sequence."
- Break condition: If emotional prompting does not consistently disrupt GPT-4's alignment or if the resulting behavior is not more human-like, this mechanism would break.

## Foundational Learning

- Concept: Behavioral game theory
  - Why needed here: Understanding the principles and applications of behavioral game theory is crucial for designing experiments and interpreting results in the context of LLM decision-making under emotional states.
  - Quick check question: What are the key differences between classical and behavioral game theory, and how do emotions play a role in the latter?

- Concept: Emotion classification and elicitation
  - Why needed here: The study focuses on five basic emotions (anger, sadness, happiness, disgust, and fear) based on Paul Ekman's classification. Understanding how these emotions are classified and how they can be elicited in experimental settings is essential for designing the emotional prompting framework.
  - Quick check question: How do the five basic emotions identified by Paul Ekman differ from each other, and what are some common methods for inducing these emotions in experimental settings?

- Concept: Large Language Model (LLM) behavior and alignment
  - Why needed here: The study investigates the behavior of GPT-3.5 and GPT-4 in game-theoretical settings and their alignment with human responses. Understanding the principles of LLM behavior, training, and alignment is crucial for interpreting the results and drawing meaningful conclusions.
  - Quick check question: What factors contribute to the alignment of LLM behavior with human responses, and how can emotional prompting influence this alignment?

## Architecture Onboarding

- Component map:
  Emotional prompting framework -> Game description module -> Memory update module -> Decision-making module -> Evaluation module

- Critical path:
  Initialize the game with the LLM and emotional state -> Execute rounds of gameplay, updating memory and emotions after each round -> Evaluate the LLM's decisions against human responses and optimal strategies -> Iterate the experiment with different emotional states and game settings

- Design tradeoffs:
  Balancing the complexity of the emotional prompting framework with the need for simplicity and generalizability -> Choosing between a more comprehensive environmental context and a minimalist approach to focus on the impact of emotions -> Deciding on the level of detail in the memory update module to capture relevant information without overwhelming the LLM

- Failure signatures:
  Lack of consistent influence of emotional states on LLM behavior across different games and settings -> Inability of the framework to generalize to new game types or emotional states -> Overfitting of the LLM to specific emotional states or game settings, leading to poor performance in novel scenarios

- First 3 experiments:
  1. Run the framework with GPT-3.5 and GPT-4 in the Dictator game, injecting different emotional states (anger, sadness, happiness, disgust, fear) and comparing the offered shares with human responses.
  2. Test the framework in the Prisoner's Dilemma game, evaluating the cooperation rates and payoffs under different emotional states and comparing them with human experimental results.
  3. Assess the framework's ability to induce alternating strategies in the Battle of the Sexes game by injecting emotional states and observing the LLM's behavior over multiple rounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the emotional source (co-player-induced vs. external) have a systematic impact on LLM decision-making in bargaining games beyond what was observed in the Dictator and Ultimatum games?
- Basis in paper: [explicit] The paper notes that co-player-induced emotions had an opposing effect compared to external emotions in the Dictator game, and suggests this distinction is pivotal in determining outcomes.
- Why unresolved: The study focused on a limited set of games and emotions. More systematic testing across a wider range of bargaining scenarios and emotions is needed to confirm this pattern.
- What evidence would resolve it: Additional experiments varying the source of emotions (co-player vs. external) across a broader set of bargaining games (e.g., Trust Game, Public Goods Game) and emotions would provide more conclusive evidence.

### Open Question 2
- Question: Can dynamic, context-aware emotional states in LLMs lead to more optimal and human-like decision-making in repeated games compared to static, prompt-based emotional injection?
- Basis in paper: [inferred] The paper discusses the limitations of static emotional prompting and suggests the need to study multi-agent approaches for dynamic emotions in future work.
- Why unresolved: The current framework uses static emotional prompts, which may not fully capture the complexity and variability of human emotions in dynamic game settings.
- What evidence would resolve it: Experiments comparing the performance of LLMs with static vs. dynamic emotional states in repeated games, using multi-agent simulations with context-aware emotional updates, would provide insights into the potential benefits of dynamic emotional modeling.

### Open Question 3
- Question: How does the alignment between LLM and human behavior under emotional states vary across different cultural contexts and demographic groups?
- Basis in paper: [explicit] The paper mentions that Ultimatum games are sensitive to cultural context and that GPT-3.5's offered share is close to the US average but varies across countries.
- Why unresolved: The study focused on a limited cultural context and did not explore potential variations in emotional alignment across different demographic groups.
- What evidence would resolve it: Conducting experiments with LLMs and human participants from diverse cultural backgrounds and demographic groups, using the same emotional prompting framework, would reveal potential variations in alignment and provide insights into the generalizability of the findings.

## Limitations

- The paper lacks detailed implementation specifics for the emotional prompting framework, particularly regarding how emotions are injected into LLMs, making reproduction challenging.
- The study focuses on five basic emotions based on Paul Ekman's classification, which may not fully capture the complexity of human emotional responses in game-theoretical contexts.
- While differences between GPT-3.5 and GPT-4 in emotional responsiveness are observed, the underlying mechanisms driving these differences are not fully elucidated.

## Confidence

- **High Confidence**: The observation that emotions influence LLM decision-making in strategic games is well-supported by the experimental results and aligns with established behavioral game theory principles.
- **Medium Confidence**: The claim that GPT-3.5 aligns well with human responses while GPT-4 exhibits consistent behavior is based on observed patterns but lacks a detailed mechanistic explanation for the differences between the two models.
- **Low Confidence**: The assertion that emotional prompting, particularly with anger, can disrupt GPT-4's "superhuman" alignment and make it more human-like is intriguing but requires further validation to establish its robustness and generalizability.

## Next Checks

1. Reproduce the core experiments using the provided framework and predefined strategies to verify the consistency of results across different runs and emotional states.

2. Extend the study to include more nuanced or mixed emotional states to assess their impact on LLM behavior and alignment with human responses.

3. Conduct a detailed analysis of the architectural and training differences between GPT-3.5 and GPT-4 to better understand why they respond differently to emotional prompting and how these differences influence their decision-making processes.