---
ver: rpa2
title: Shortcut Learning in Medical Image Segmentation
arxiv_id: '2403.06748'
source_url: https://arxiv.org/abs/2403.06748
tags:
- segmentation
- learning
- shortcut
- image
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study demonstrates that shortcut learning affects medical
  image segmentation models, not just classification. Two types of shortcuts are identified:
  (1) Clinical annotations like calipers and text in fetal ultrasound images, which
  correlate with anatomical structures and lead to segmentation failures when absent
  during testing; (2) The combination of zero-padded convolutions and center-cropped
  training sets in skin lesion segmentation, where pixels near image boundaries are
  incorrectly classified as background.'
---

# Shortcut Learning in Medical Image Segmentation
## Quick Facts
- arXiv ID: 2403.06748
- Source URL: https://arxiv.org/abs/2403.06748
- Reference count: 26
- Primary result: Clinical annotations and boundary padding create segmentation shortcuts that degrade model performance when removed

## Executive Summary
This study reveals that shortcut learning, previously documented primarily in medical image classification, also significantly impacts segmentation models. The authors identify two distinct types of shortcuts: clinical annotations in ultrasound images that correlate with anatomical structures, and boundary-related shortcuts arising from zero-padded convolutions combined with center-cropped training sets in skin lesion segmentation. These shortcuts lead to severe performance degradation when absent during testing, demonstrating that segmentation models can rely on dataset-specific artifacts rather than genuine anatomical features.

The research demonstrates that commonly used data augmentation strategies like center cropping can inadvertently introduce harmful shortcuts, while proposed mitigation strategies such as inpainting clinical annotations and using random cropping instead of center cropping can improve model generalizability. Performance drops of 5-6% Dice score were observed when shortcuts were removed in ultrasound segmentation, and improved boundary segmentation was achieved using random cropping in skin lesion tasks.

## Method Summary
The study investigates shortcut learning through two controlled experiments. First, fetal ultrasound segmentation models were trained with and without clinical annotations (calipers and text), then tested on images both with and without these annotations to measure performance degradation. Second, skin lesion segmentation models were evaluated using different cropping strategies (center vs. random) and padding schemes to identify boundary-related shortcuts caused by zero-padded convolutions interacting with center-cropped training data. The authors systematically removed potential shortcuts and measured the resulting performance changes to quantify the impact of shortcut learning.

## Key Results
- Clinical annotations in ultrasound images serve as shortcuts, causing 5-6% Dice score performance drops when absent during testing
- Zero-padded convolutions combined with center-cropped training sets create boundary shortcuts in skin lesion segmentation
- Inpainting clinical annotations and using random cropping instead of center cropping effectively mitigate identified shortcuts
- Models trained on center-cropped data fail to properly segment pixels near image boundaries due to learned background patterns

## Why This Works (Mechanism)
Shortcut learning occurs when models exploit spurious correlations in training data rather than learning robust features. In medical imaging, these shortcuts often arise from dataset-specific artifacts, acquisition protocols, or annotation practices that correlate with target structures. The ultrasound experiment demonstrates how clinical annotations, while intended as guidance, become correlated with anatomical features and create brittle models. The skin lesion experiment shows how architectural choices (zero-padding) combined with data preprocessing (center cropping) can introduce systematic biases that models exploit.

## Foundational Learning
**Dice Score**: Measures segmentation overlap between prediction and ground truth, ranging from 0 (no overlap) to 1 (perfect overlap). Why needed: Primary evaluation metric for segmentation quality. Quick check: Values above 0.8 indicate good performance for most medical segmentation tasks.

**Zero-padding**: Convolutional layer technique that adds zero values around image borders. Why needed: Affects how models process boundary pixels and can create shortcut opportunities. Quick check: Observe feature map sizes and boundary handling in convolutional layers.

**Random vs. Center Cropping**: Data augmentation strategies that differ in crop location selection. Why needed: Center cropping can introduce systematic biases while random cropping provides more diverse training samples. Quick check: Compare distribution of crop locations and resulting model performance.

**Clinical Annotations**: Markers like calipers and text added by sonographers during ultrasound acquisition. Why needed: Common in medical imaging but can become spurious correlations. Quick check: Examine correlation between annotation presence and target structures in training data.

## Architecture Onboarding
**Component Map**: Input Image -> Convolutional Network -> Segmentation Map -> Post-processing (if any) -> Dice Score Evaluation
**Critical Path**: Image acquisition → Model inference → Pixel-wise classification → Aggregation to segmentation mask → Dice score computation
**Design Tradeoffs**: Center cropping provides consistent training samples but introduces boundary shortcuts; zero-padding simplifies implementation but affects boundary handling; clinical annotations aid human interpretation but can mislead models.
**Failure Signatures**: Performance degradation when shortcuts are removed; systematic misclassification of boundary pixels; over-reliance on correlated artifacts rather than anatomical features.
**First Experiments**: 1) Train model with clinical annotations, test with and without annotations to measure shortcut impact. 2) Compare center vs. random cropping on boundary segmentation performance. 3) Inpaint annotations and retrain to evaluate mitigation effectiveness.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Ultrasound experiment limited to single fetal dataset with specific annotation practices
- Skin lesion boundary analysis constrained to pixels within 32 pixels of image edges
- Study does not explore full spectrum of potential shortcuts across medical imaging modalities
- Mitigation strategies may introduce new failure modes not evaluated in the study

## Confidence
High: Identified shortcut mechanisms in specific datasets and experimental conditions
Medium: Broader claim that shortcut learning is widespread concern in medical segmentation

## Next Checks
1. Test clinical annotation shortcut hypothesis across multiple ultrasound datasets and other imaging modalities that use similar clinical markers
2. Conduct systematic analysis of boundary-related shortcuts across diverse segmentation tasks and architectures
3. Evaluate whether proposed mitigation strategies introduce new failure modes or performance degradation in other aspects of segmentation quality