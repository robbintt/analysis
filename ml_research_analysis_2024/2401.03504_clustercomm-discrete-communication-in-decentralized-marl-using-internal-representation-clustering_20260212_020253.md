---
ver: rpa2
title: 'ClusterComm: Discrete Communication in Decentralized MARL using Internal Representation
  Clustering'
arxiv_id: '2401.03504'
source_url: https://arxiv.org/abs/2401.03504
tags:
- agents
- clustercomm
- communication
- learning
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ClusterComm, a method for enabling discrete
  communication in decentralized multi-agent reinforcement learning (MARL) systems.
  The core idea is to use K-Means clustering on the internal representations (last
  hidden layer activations) of each agent's policy network to generate discrete messages,
  which are then transmitted to other agents.
---

# ClusterComm: Discrete Communication in Decentralized MARL using Internal Representation Clustering

## Quick Facts
- arXiv ID: 2401.03504
- Source URL: https://arxiv.org/abs/2401.03504
- Reference count: 2
- Discrete communication through clustering achieves competitive performance to continuous communication in MARL

## Executive Summary
This paper introduces ClusterComm, a method enabling discrete communication in decentralized multi-agent reinforcement learning systems. The approach uses K-Means clustering on the internal representations (last hidden layer activations) of each agent's policy network to generate discrete messages. The method was evaluated across four grid-based environments, demonstrating that discrete communication can achieve performance comparable to continuous communication while being more scalable and requiring less bandwidth.

## Method Summary
ClusterComm discretizes communication by applying Mini-Batch K-Means clustering to the penultimate layer activations of each agent's policy network. During training, each agent independently learns its own policy network that processes local observations and incoming messages through separate encoders. The observation encoder's output is clustered to generate discrete messages, which are transmitted to other agents. The message encoder at receiving agents processes the one-hot encoded cluster indices, and the concatenated representations are passed to the action network. The entire architecture is trained end-to-end using PPO, with K-Means centroids updated incrementally to handle the non-stationary feature space during training.

## Key Results
- ClusterComm consistently outperformed no communication baseline across all tested environments
- Discrete communication achieved competitive performance to continuous communication (LatentComm) in Bottleneck, ClosedRooms, RedBlueDoors, and Level-based Foraging environments
- The method demonstrates that compressed discrete messages can effectively capture sufficient information for coordination in collaborative tasks
- Different cluster counts (4, 8, 16) were used across environments based on task complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discrete communication through clustering of internal representations is effective because it provides a compressed yet meaningful summary of agent states and intentions.
- Mechanism: The policy network's penultimate layer encodes rich information about observations and intended actions. K-Means clustering on these activations produces discrete cluster indices that serve as messages. These indices capture semantic similarity in the agent's internal state space, allowing other agents to infer relevant information without needing the full continuous representation.
- Core assumption: The internal representation contains sufficient information about the agent's state and intended actions to enable effective coordination when clustered discretely.
- Evidence anchors:
  - [abstract] "ClusterComm utilizes Mini-Batch-K-Means clustering on the last hidden layer's activations of an agent's policy network, translating them into discrete messages"
  - [section 3.1] "Since φ(k)o(ot) encodes information about the agent's observation and intended action, the output is discretized using Mini-Batch K-Means"
- Break condition: If the internal representation does not encode sufficient information about the agent's state and intentions, or if the clustering does not capture meaningful distinctions between different states.

### Mechanism 2
- Claim: Mini-Batch K-Means is effective for discretizing communication because it handles the non-stationary feature space during training.
- Mechanism: During training, the policy network parameters change continuously, causing the feature space to evolve. Mini-Batch K-Means updates centroids incrementally with each training batch, allowing the discretization to adapt to these changes without losing message consistency.
- Core assumption: The feature space evolves gradually enough during training that incremental centroid updates maintain meaningful message mappings.
- Evidence anchors:
  - [section 3] "Since the parameters of the agents' networks are continuously adjusted during training... the spanned feature space also varies over the course of training"
  - [section 3] "Mini-Batch K-Means is useful in scenarios where the entire data set is unknown at any given time and may be subject to time-dependent changes"
- Break condition: If the feature space changes too rapidly during training, causing centroids to shift significantly between messages and breaking communication consistency.

### Mechanism 3
- Claim: The discrete message format enables scalability and efficiency by reducing bandwidth requirements while maintaining sufficient information for coordination.
- Mechanism: By transmitting only a single integer (cluster index) instead of the full continuous representation, ClusterComm drastically reduces communication overhead. The experiments show this discrete communication achieves competitive performance to continuous communication methods, demonstrating that the information loss from discretization is minimal for the tested environments.
- Core assumption: The coordination tasks in the tested environments can be solved with the reduced information content in discrete messages.
- Evidence anchors:
  - [abstract] "ClusterComm... poses a simple yet effective strategy for enhancing collaborative task-solving in MARL"
  - [section 4.2] "The goal is to show that ClusterComm achieves similar performance to LatentComm, even though the agents are only allowed to transmit discrete messages"
- Break condition: If the coordination tasks require more nuanced information than can be captured by discrete cluster indices, leading to performance degradation compared to continuous communication.

## Foundational Learning

- Concept: Markov Games and Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The paper explicitly frames the multi-agent problem as a POMG, which is the formal foundation for understanding how agents interact, observe, and learn in the environment.
  - Quick check question: What tuple defines a POMG and what does each component represent?

- Concept: K-Means Clustering and Mini-Batch K-Means
  - Why needed here: ClusterComm directly uses K-Means clustering on neural network activations to create discrete messages, so understanding how K-Means works is essential.
  - Quick check question: What is the objective function that K-Means minimizes, and how does Mini-Batch K-Means differ from standard K-Means?

- Concept: Reinforcement Learning with Function Approximation
  - Why needed here: Agents use neural networks to approximate value functions or policies, and understanding how these networks process observations and generate actions is crucial for understanding how internal representations are formed.
  - Quick check question: How does the observation encoder φ(k)o process local observations, and what is the role of the message encoder φ(k)m?

## Architecture Onboarding

- Component map: Observation → φ(k)o → Clustering → Discrete message → φ(k)m (other agents) → φ(k)a → Action
- Critical path: Observation → φ(k)o → Clustering → Discrete message → φ(k)m (other agents) → φ(k)a → Action
- Design tradeoffs:
  - Discrete vs continuous communication: Discrete is more scalable and efficient but may lose some information
  - Number of clusters: More clusters preserve more information but increase message size and computational cost
  - Message update frequency: Updating centroids after each PPO update vs less frequently affects message consistency
  - Network architecture: Separate encoders for observations and messages vs shared encoders
- Failure signatures:
  - Poor performance compared to NoComm baseline: Suggests clustering is not capturing useful information
  - High variance in training curves: May indicate unstable message mappings due to rapidly changing feature space
  - Failure to converge: Could result from insufficient exploration or poor hyperparameter choices
- First 3 experiments:
  1. Implement ClusterComm on the Bottleneck environment with 2 agents and compare to NoComm baseline
  2. Test different numbers of clusters (4, 8, 16) to find the optimal balance between information preservation and efficiency
  3. Implement Spherical ClusterComm variant and compare performance to standard ClusterComm on ClosedRooms environment

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but the limitations section identifies several areas requiring further investigation regarding the method's applicability to different environment types and scalability to larger numbers of agents.

## Limitations
- Architecture details are underspecified beyond mentioning 32-unit hidden layers, making exact reproduction challenging
- The methodology for choosing optimal cluster counts across environments is not detailed
- Evaluation is limited to discrete 5x5 grid environments with limited field of view, with unknown performance on continuous control tasks or larger state spaces

## Confidence
- High confidence: The core mechanism of using K-Means clustering on internal representations for discrete communication is well-specified and theoretically sound
- Medium confidence: The scalability and efficiency benefits are logically sound but only demonstrated through comparison with a single continuous baseline
- Low confidence: Claims about generality lack testing on diverse environment types, and specific failure scenarios compared to continuous communication are not identified

## Next Checks
1. **Architecture verification**: Implement the ClusterComm architecture with the exact specifications provided and verify that the Mini-Batch K-Means clustering produces stable, meaningful messages throughout training across all four test environments.

2. **Hyperparameter sensitivity analysis**: Systematically test different cluster counts (2, 4, 8, 16, 32) across environments to identify optimal configurations and quantify the performance trade-off between message granularity and communication efficiency.

3. **Cross-environment generalization**: Apply ClusterComm to at least two additional environment types (e.g., one continuous control task and one with larger state space) to test the method's robustness and identify failure modes when coordination requires more nuanced information than discrete messages can provide.