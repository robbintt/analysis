---
ver: rpa2
title: Safe reinforcement learning in uncertain contexts
arxiv_id: '2401.05876'
source_url: https://arxiv.org/abs/2401.05876
tags: []
core_contribution: This paper proposes a safe reinforcement learning algorithm for
  systems with discrete context variables whose values are not directly observable.
  The key innovation is combining context identification using the maximum mean discrepancy
  (MMD) with a classifier that provides frequentist uncertainty bounds via conditional
  mean embeddings (CMEs).
---

# Safe reinforcement learning in uncertain contexts

## Quick Facts
- arXiv ID: 2401.05876
- Source URL: https://arxiv.org/abs/2401.05876
- Reference count: 40
- Primary result: Safe RL algorithm for systems with discrete, unobservable context variables using MMD-based context identification and CME-based classification with uncertainty bounds

## Executive Summary
This paper addresses safe reinforcement learning in systems where discrete context variables affect dynamics but cannot be directly observed. The authors propose a novel algorithm that combines context identification using Maximum Mean Discrepancy (MMD) with a classifier based on Conditional Mean Embeddings (CMEs) that provides frequentist uncertainty bounds. By estimating the current context from measurements when confident and performing dedicated identification experiments when uncertain, the algorithm guarantees safety during learning. The approach is demonstrated on a Furuta pendulum where weights serving as contexts are inferred from camera images.

## Method Summary
The method combines two key components: a CME-based classifier that provides input-dependent frequentist uncertainty bounds for context prediction, and an MMD-based context identification method that tests whether current trajectories match stored context distributions. The algorithm uses classification bounds to make confident decisions when possible, and resorts to context identification experiments when uncertainty exceeds a threshold. Theoretical guarantees ensure safety with high probability, while integration with safe learning algorithms like SAFE-OPT enables optimal policy optimization under safety constraints.

## Key Results
- Algorithm successfully learns to balance Furuta pendulum while guaranteeing safety despite unknown context variables
- Outperforms baseline that doesn't consider contexts by making more informed decisions during exploration
- Provides thorough evaluation of classification bounds on standard benchmarks, demonstrating appropriate coverage without being overly conservative

## Why This Works (Mechanism)

### Mechanism 1
The system guarantees safety during learning even when context variables are unknown by combining frequentist uncertainty bounds for classification with context identification experiments. The algorithm uses the classifier to estimate the current context when confident, and resorts to MMD-based identification when uncertainty is too high. Core assumption: reward and constraint functions have bounded RKHS norms, and context changes cause statistically significant changes in system trajectories. Break condition: if contexts change the system dynamics in ways too subtle to detect via MMD.

### Mechanism 2
The classifier provides input-dependent frequentist uncertainty bounds for context prediction using conditional mean embeddings with regularization. This allows estimation of context probabilities and derivation of finite-sample bounds that shrink as more data is collected. Core assumption: true context probability functions are in the RKHS and have bounded norm. Break condition: if the RKHS norm bound Γ is set too low (overly conservative) or too high (under-confident).

### Mechanism 3
Context identification via MMD distinguishes between different contexts with high probability by sub-sampling trajectories to approximate i.i.d. data and using the MMD test statistic. This determines if the current trajectory distribution matches a stored context distribution. Core assumption: contexts cause sufficiently large changes in system dynamics and trajectories become approximately stationary after sub-sampling. Break condition: if contexts cause only minor dynamical changes that fall below the detection threshold.

## Foundational Learning

- **Reproducing Kernel Hilbert Spaces (RKHS)**: Provides the mathematical framework for representing reward/constraint functions and deriving uncertainty bounds. Quick check: What property of RKHS functions allows us to derive uniform bounds on their norms?

- **Conditional Mean Embeddings (CMEs)**: Enable estimation of conditional probabilities and derivation of input-dependent uncertainty bounds for classification. Quick check: How does the regularization parameter λ affect the trade-off between bias and variance in CME estimates?

- **Maximum Mean Discrepancy (MMD)**: Provides a kernel-based distance measure between probability distributions without requiring density estimation. Quick check: Why does sub-sampling trajectories help satisfy the i.i.d. assumption required for MMD theory?

## Architecture Onboarding

- **Component map**: Context Classifier (CME-based) -> Context Identifier (MMD-based) -> Safe RL Engine (SAFE-OPT) -> Context Storage (trajectory database) -> Safety Monitor (probability threshold)

- **Critical path**: 1) Receive measurements y, 2) Compute classification probabilities and bounds, 3) If lower bound > psafe, use predicted context, 4) Else, perform context identification experiment, 5) Update context database and classifier, 6) Continue safe RL optimization

- **Design tradeoffs**: Classification vs. identification (more confident classifier reduces experimentation time but requires more data), bound tightness vs. conservatism (tighter bounds enable earlier policy optimization but risk safety violations), kernel choice (affects both classification performance and MMD sensitivity)

- **Failure signatures**: Frequent context identification experiments (classifier uncertainty too high - check Γ, λ, kernel parameters), safety violations (bounds too optimistic - increase Γ, decrease psafe), poor RL performance (over-conservative bounds - decrease Γ, increase psafe)

- **First 3 experiments**: 1) Run with synthetic data where contexts are easily distinguishable; verify classifier quickly becomes confident, 2) Run with contexts that differ only slightly; verify algorithm correctly identifies need for context identification, 3) Run with corrupted measurements; verify safety guarantees still hold despite classification errors

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of psafe affect the trade-off between classification accuracy and safety in real-world applications? The paper provides a sensitivity analysis but lacks concrete guidelines for selecting psafe in different scenarios.

### Open Question 2
What is the impact of the number of contexts on the scalability and performance of the proposed algorithm? The evaluation focuses on limited contexts without extensive exploration of how the algorithm scales with larger numbers.

### Open Question 3
How can the upper bound on the RKHS norm (Γ) be estimated from data in practice? The paper acknowledges this as an ongoing research problem and uses a conservative estimate without providing a method for estimation.

## Limitations
- Strong assumptions about bounded RKHS norms and characteristic kernels may not hold in practice
- Performance heavily depends on kernel hyperparameter choice and confidence threshold psafe
- Context identification method requires sub-sampling to achieve stationarity, which may not be feasible for systems with very long time-scales

## Confidence

**High Confidence**: Theoretical framework for CME-based uncertainty bounds and MMD-based context identification is mathematically sound assuming stated assumptions hold.

**Medium Confidence**: Safety guarantees depend on validity of RKHS norm bounds and proper functioning of both classifier and context identifier.

**Low Confidence**: Practical performance in real-world scenarios where contexts may change gradually or have complex relationships with observations.

## Next Checks

1. **Empirical Evaluation of Bounds**: Run classification algorithm on standard benchmarks and compare empirical error rates with theoretical bounds to verify appropriate coverage.

2. **Robustness to Kernel Choice**: Test algorithm with different kernel types and bandwidths to assess sensitivity to hyperparameters and evaluate effects on classification accuracy and MMD sensitivity.

3. **Context Drift Scenarios**: Evaluate performance when contexts change gradually rather than discretely, or when multiple contexts have similar dynamical signatures to test robustness in realistic scenarios.