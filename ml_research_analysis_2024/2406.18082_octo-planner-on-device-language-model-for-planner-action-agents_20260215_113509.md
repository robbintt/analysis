---
ver: rpa2
title: 'Octo-planner: On-device Language Model for Planner-Action Agents'
arxiv_id: '2406.18082'
source_url: https://arxiv.org/abs/2406.18082
tags:
- planning
- chen
- wang
- agent
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Octo-planner, an on-device language model
  for planning agents that addresses the computational and efficiency challenges of
  existing LLM-based agents. The key innovation is a Planner-Action framework that
  separates planning and action execution, using fine-tuning instead of in-context
  learning to optimize performance on resource-constrained devices.
---

# Octo-planner: On-device Language Model for Planner-Action Agents

## Quick Facts
- arXiv ID: 2406.18082
- Source URL: https://arxiv.org/abs/2406.18082
- Authors: Wei Chen; Zhiyuan Li; Zhen Guo; Yikang Shen
- Reference count: 40
- Primary result: Achieves 97% success rate on in-domain tests using Phi-3 Mini fine-tuned with GPT-4 generated data

## Executive Summary
This paper introduces Octo-planner, an on-device language model for planning agents that addresses the computational and efficiency challenges of existing LLM-based agents. The key innovation is a Planner-Action framework that separates planning and action execution, using fine-tuning instead of in-context learning to optimize performance on resource-constrained devices. The approach employs GPT-4 to generate and validate planning data, which is then used to fine-tune Phi-3 Mini, achieving a 97% success rate in in-domain tests. The authors also developed a multi-LoRA training method for handling complex, multi-domain queries while maintaining computational efficiency. This work advances practical, privacy-preserving AI agents by enabling sophisticated planning capabilities on edge devices.

## Method Summary
The Octo-planner framework separates planning and action execution into two specialized models: a planner agent based on Phi-3 Mini and an action agent using the Octopus model for function execution. The approach uses GPT-4 to generate and validate planning queries and responses based on available functions, creating a dataset for fine-tuning. The model employs both full fine-tuning and LoRA techniques, with a multi-LoRA training method developed for handling complex, multi-domain queries. Training uses a learning rate of 5 × 10^-6, batch size of 4, warm-up ratio of 0.2, and runs for 2 epochs. The system is designed to work efficiently on edge devices while maintaining high performance for planning tasks.

## Key Results
- Achieves 97% success rate on in-domain test environment
- Full fine-tuning achieves 98.1% accuracy compared to LoRA's 85.1% (rank=64)
- Multi-LoRA merging with four domains achieves 69.7% accuracy
- Reduces computational costs and energy consumption compared to in-context learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning Phi-3 Mini with task-specific data enables effective on-device planning without expensive in-context learning.
- Mechanism: Instead of prompting the model with function descriptions in every query, the planner model is trained to internalize available functions. This eliminates the need for long context prompts, reducing latency and energy consumption on edge devices.
- Core assumption: The fine-tuning dataset sufficiently covers the diversity of user queries and function combinations.
- Evidence anchors:
  - [abstract]: "We employ model fine-tuning instead of in-context learning, reducing computational costs and energy consumption while improving response times."
  - [section 3.2]: "To optimize performance on resource-constrained devices, we employ model fine-tuning instead of in-context learning..."
- Break condition: If the training data lacks sufficient diversity, the model will fail to generalize to novel queries, requiring fallback to in-context learning.

### Mechanism 2
- Claim: Separating planning and action execution into two specialized models improves modularity and efficiency.
- Mechanism: The planner agent decomposes tasks into sub-steps, while the action agent executes each step. This separation allows each model to be optimized for its specific role, improving performance in complex tasks.
- Core assumption: The planner can accurately decompose tasks into executable sub-steps.
- Evidence anchors:
  - [abstract]: "Our Planner-Action framework separates planning and action execution into two components: a planner agent based on Phi-3 Mini...and an action agent using the Octopus model for function execution."
  - [section 3.1]: "This separation improves modularity and allows for specialized optimization of each component."
- Break condition: If the planner fails to generate valid execution sequences, the entire framework breaks down regardless of action agent performance.

### Mechanism 3
- Claim: Multi-LoRA training enables flexible handling of complex, multi-domain queries while maintaining computational efficiency.
- Mechanism: Instead of fine-tuning the entire model for each new function set, separate LoRA adapters are trained for different function domains and then merged. This allows the model to handle diverse function sets without full retraining.
- Core assumption: Merging LoRA weights from different domains preserves the specialized knowledge of each adapter.
- Evidence anchors:
  - [abstract]: "To address multi-domain planning challenges, we developed a multi-LoRA training method that merges weights from LoRAs trained on distinct function subsets."
  - [section 5.2]: "This approach creates a composite model that combines knowledge from various function sets, offering a scalable solution for complex, multi-domain queries in resource-constrained environments."
- Break condition: If merged LoRA weights conflict or degrade performance beyond acceptable thresholds, the approach fails for multi-domain scenarios.

## Foundational Learning

- Concept: Function calling in language models
  - Why needed here: The planner must understand how to map user queries to specific function calls, which is the core capability being fine-tuned.
  - Quick check question: Can the model correctly identify which function to call for a given user query after training?

- Concept: Low-Rank Adaptation (LoRA) and weight merging
  - Why needed here: Multi-LoRA enables efficient adaptation to new function domains without full model retraining, critical for scalability.
  - Quick check question: Can you explain why merging LoRA weights from different function sets might degrade performance as more domains are added?

- Concept: Dataset curation and quality assurance
  - Why needed here: The success of fine-tuning depends entirely on having high-quality, diverse training data that covers the target use cases.
  - Quick check question: What criteria are used to validate the correctness of generated query-response pairs in the dataset?

## Architecture Onboarding

- Component map:
  User Interface -> Planner Agent (Phi-3 Mini) -> Action Agent (Octopus model) -> Function Library
- Critical path: User query → Planner decomposition → Sequential action execution → Final response
- Design tradeoffs:
  - Fine-tuning vs in-context learning: Fine-tuning reduces runtime costs but requires upfront data generation
  - Model size vs performance: Larger models achieve better accuracy but increase resource requirements
  - Single LoRA vs multi-LoRA: Multi-LoRA offers flexibility but introduces complexity and potential performance degradation
- Failure signatures:
  - Planner generates invalid function sequences → Check training data diversity
  - High latency on edge devices → Verify model size and optimization settings
  - Multi-domain queries fail → Test LoRA merging effectiveness and consider separate models
- First 3 experiments:
  1. Fine-tune Phi-3 Mini on a small synthetic dataset (100 examples) and measure accuracy on in-domain tasks
  2. Compare full fine-tuning vs LoRA with different rank configurations on the same dataset
  3. Test multi-LoRA merging with two function domains and measure accuracy degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-LoRA merging technique scale when handling more than four distinct function sets, and what is the theoretical limit for practical implementation?
- Basis in paper: [explicit] The paper mentions that merging LoRAs from four domains results in a 69.7% accuracy, with a steeper decline after the third domain.
- Why unresolved: The paper only tests up to four domains, leaving the scalability of the multi-LoRA approach for larger numbers of function sets unexplored.
- What evidence would resolve it: Experiments testing the accuracy of multi-LoRA merging with five or more function sets, along with analysis of computational efficiency and potential diminishing returns.

### Open Question 2
- Question: What is the impact of iterative planning on task completion rates for complex, multi-step queries compared to the current upfront planning approach?
- Basis in paper: [inferred] The paper notes that the current model conducts all planning in advance, which may be less adaptable to complex or unpredictable scenarios where conditions might change during execution.
- Why unresolved: The paper does not provide experimental data comparing iterative planning with the current upfront planning approach for complex tasks.
- What evidence would resolve it: Comparative experiments measuring task completion rates, accuracy, and execution time for complex queries using both iterative and upfront planning approaches.

### Open Question 3
- Question: How does the performance of Octo-planner vary across different types of edge devices with varying computational capabilities and memory constraints?
- Basis in paper: [explicit] The paper mentions that Octo-planner is designed for edge devices but does not provide detailed performance metrics across different device specifications.
- Why unresolved: The paper focuses on the model's architecture and performance in a general sense but does not address device-specific performance variations.
- What evidence would resolve it: Benchmarking results showing Octo-planner's performance (accuracy, latency, battery consumption) on a range of edge devices with different specifications.

## Limitations
- Evaluation focuses primarily on in-domain tasks with limited testing of cross-domain generalization capabilities
- No direct comparison with state-of-the-art in-context learning approaches in terms of end-to-end performance
- Multi-LoRA approach shows promise but lacks comprehensive analysis of scaling behavior with more domains

## Confidence
- Planner-Action framework effectiveness: High - Clear architectural benefits demonstrated
- Fine-tuning vs in-context learning performance: Medium - Strong results but limited benchmarking
- Multi-LoRA merging approach: Low-Medium - Promising concept but insufficient validation of long-term scalability

## Next Checks
1. Test cross-domain generalization by evaluating the model on unseen function combinations not present in training data
2. Compare end-to-end latency and accuracy against in-context learning baselines on the same hardware
3. Evaluate multi-LoRA performance degradation as more function domains are added to identify breaking points