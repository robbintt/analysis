---
ver: rpa2
title: Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race,
  Ethnicity, and Gender?
arxiv_id: '2406.10486'
source_url: https://arxiv.org/abs/2406.10486
tags:
- names
- llms
- language
- hiring
- hispanic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models exhibit discriminatory hiring decisions favoring
  White and Black female applicants over Hispanic male applicants. The study used
  820 templated prompts to generate over 2 million job application emails, manipulating
  applicant first names to represent race, ethnicity, and gender.
---

# Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?

## Quick Facts
- **arXiv ID**: 2406.10486
- **Source URL**: https://arxiv.org/abs/2406.10486
- **Reference count**: 37
- **Primary result**: LLMs show discriminatory hiring patterns favoring White and Black female applicants over Hispanic male applicants

## Executive Summary
This study investigates whether large language models exhibit discriminatory hiring decisions based on race, ethnicity, and gender by analyzing over 2 million job application emails generated through templated prompts. The researchers systematically manipulated applicant first names to represent different demographic groups and measured acceptance rates across five major LLM models. The findings reveal statistically significant bias, with Hispanic male names consistently receiving the lowest acceptance rates across all models tested. The discriminatory effects varied by template and qualification level, indicating that prompt sensitivity can influence bias patterns.

## Method Summary
The study employed a systematic approach using 300 first names (50 per intersectional group: White female/male, Black female/male, Hispanic female/male) combined with 820 templated prompts covering 4 qualification levels, 5 base templates, and 41 occupations. These prompts generated job application emails that were classified as accept/reject using an SVM classifier with TF-IDF features. The researchers tested five LLM models including Mistral-7b, Llama2 variants, and GPT-3.5-Turbo, collecting data on acceptance rates across demographic groups. Statistical significance was assessed using permutation tests at p < 0.05 and p < 0.01 levels.

## Key Results
- Hispanic male applicants received the lowest acceptance rates across all tested models
- White and Black female applicants showed the highest acceptance rates
- Discriminatory patterns varied significantly based on prompt templates and qualification levels
- Statistical significance confirmed with p-values below 0.05 and 0.01 thresholds

## Why This Works (Mechanism)
The mechanism behind these discriminatory patterns likely stems from training data biases present in the large language models, where historical hiring patterns and societal stereotypes may have been embedded during pretraining. When models process demographic information through first names, they may inadvertently reproduce these historical biases in their decision-making processes.

## Foundational Learning
- **Permutation testing**: Statistical method for assessing significance without parametric assumptions; needed for validating bias patterns; quick check: compare p-values across demographic groups
- **TF-IDF feature extraction**: Text representation technique for classification; needed for converting emails to numerical features; quick check: verify feature dimensionality matches vocabulary size
- **Intersectional analysis**: Framework for examining overlapping demographic categories; needed to capture complex bias patterns; quick check: confirm all 6 demographic groups are properly represented

## Architecture Onboarding
**Component Map**: Names -> Templated prompts -> LLM generation -> Email classification -> Acceptance rate analysis

**Critical Path**: Name selection → Prompt generation → LLM processing → Classification → Statistical analysis

**Design Tradeoffs**: Templated prompts provide control but may not reflect real application complexity; automated classification enables large-scale analysis but may miss nuanced outcomes

**Failure Signatures**: Inconsistent acceptance rates across templates suggest prompt sensitivity; low classification accuracy indicates email generation quality issues

**First Experiments**:
1. Test single prompt with all 300 names through one model to verify basic functionality
2. Validate email classifier on manually labeled subset of generated emails
3. Run permutation test on small sample to confirm statistical methodology

## Open Questions the Paper Calls Out
None

## Limitations
- First names as proxies may not perfectly capture demographic identity
- Templated email format lacks real-world application complexity
- SVM classifier reliability depends on training data quality
- Significant variation across templates indicates prompt sensitivity

## Confidence
- **High Confidence**: Hispanic male disadvantage across all models; statistical significance of demographic differences
- **Medium Confidence**: Specific group ranking and generalizability to real hiring scenarios
- **Medium Confidence**: Stability of findings across different prompt variations

## Next Checks
1. Test same prompts across additional LLM models (Claude, Gemini) to verify persistent discriminatory patterns
2. Conduct sensitivity analysis by varying prompt phrasing, qualification descriptions, and occupation types
3. Implement alternative classification methods (human annotation, different ML models) to verify reliability of acceptance/rejection determinations