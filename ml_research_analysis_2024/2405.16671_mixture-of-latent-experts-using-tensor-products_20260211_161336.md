---
ver: rpa2
title: Mixture of Latent Experts Using Tensor Products
arxiv_id: '2405.16671'
source_url: https://arxiv.org/abs/2405.16671
tags:
- tensor
- routing
- learning
- arxiv
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TensorPoly, a novel modular language model
  that leverages tensor products to achieve parameter efficiency and effective multi-task
  transfer learning. The core method involves reparameterizing LoRA adapters using
  an entangled tensor structure (TLoRA) and implementing two routing functions (TensorPoly-I
  and TensorPoly-II) to selectively activate modules based on tensor rank or order.
---

# Mixture of Latent Experts Using Tensor Products

## Quick Facts
- arXiv ID: 2405.16671
- Source URL: https://arxiv.org/abs/2405.16671
- Reference count: 12
- Primary result: TensorPoly-I achieves state-of-the-art results on T0 benchmark using fewer parameters than dense approaches

## Executive Summary
This paper introduces TensorPoly, a novel modular language model that leverages tensor products to achieve parameter efficiency and effective multi-task transfer learning. The core method involves reparameterizing LoRA adapters using an entangled tensor structure (TLoRA) and implementing two routing functions (TensorPoly-I and TensorPoly-II) to selectively activate modules based on tensor rank or order. Experiments on the T0 benchmark demonstrate that TensorPoly models outperform traditional dense approaches like LoRA and TLoRA, with TensorPoly-I achieving state-of-the-art results while using fewer training parameters. The findings highlight the potential of modular language models and tensor product routing in mitigating negative transfer and enhancing compositional generalization across diverse tasks.

## Method Summary
TensorPoly reparameterizes LoRA adapters using tensor products to create an entangled structure (TLoRA), then implements two routing functions - TensorPoly-I based on tensor rank and TensorPoly-II based on tensor order - to selectively activate modules. The model is trained on the T0 benchmark through multi-task pre-training followed by few-shot fine-tuning, with performance evaluated on 11 held-out tasks using accuracy metrics. The key innovation lies in using tensor product operations to create modular, parameter-efficient adapters that can be selectively activated through learned routing mechanisms.

## Key Results
- TensorPoly-I achieves state-of-the-art accuracy on T0 benchmark while using fewer parameters than dense LoRA and TLoRA approaches
- Tensor product routing effectively mitigates negative transfer between tasks compared to traditional dense methods
- Both TensorPoly-I (rank-based) and TensorPoly-II (order-based) routing functions demonstrate superior compositional generalization across diverse tasks

## Why This Works (Mechanism)
TensorPoly works by decomposing the parameter space of LoRA adapters into tensor products, creating a modular structure where each module can be selectively activated through learned routing functions. The tensor product structure allows for efficient parameter sharing and disentanglement of task-specific and general features, while the routing mechanisms ensure that only relevant modules are activated for each task, reducing interference and improving generalization.

## Foundational Learning
1. **Tensor Product Operations** - Why needed: Enables compact representation of parameter space through multiplicative interactions between dimensions. Quick check: Verify tensor dimensions multiply correctly (dim(A⊗B) = dim(A) × dim(B)).

2. **LoRA (Low-Rank Adaptation)** - Why needed: Provides efficient fine-tuning mechanism by decomposing weight updates into low-rank matrices. Quick check: Confirm rank R satisfies R << min(m,n) for parameter matrices.

3. **Routing Functions** - Why needed: Selectively activates relevant modules while deactivating others to prevent negative transfer. Quick check: Monitor routing distribution to ensure convergence to meaningful values.

4. **Modular Language Models** - Why needed: Enables task-specific specialization while maintaining parameter efficiency through selective module activation. Quick check: Verify total parameters remain bounded as number of tasks increases.

## Architecture Onboarding

**Component Map:**
T0-3B Model -> TLoRA Adapters (Tensor Product Structure) -> Routing Function (TensorPoly-I/II) -> Expert Modules -> Output

**Critical Path:**
Input sequence → LoRA-transformed embeddings → Tensor product reparameterization → Routing function activation → Selected expert module → Output logits

**Design Tradeoffs:**
- Tensor rank/order selection vs. parameter efficiency
- Routing complexity vs. computational overhead
- Module specialization vs. generalization capability

**Failure Signatures:**
- Random routing distribution indicates routing function not learning
- Parameter count explosion suggests incorrect tensor product implementation
- Degraded performance on held-out tasks indicates negative transfer

**First Experiments:**
1. Verify tensor product dimensions and parameter counts match theoretical expectations
2. Test routing function with synthetic data to confirm selective activation behavior
3. Compare parameter efficiency of TLoRA vs. standard LoRA on simple task

## Open Questions the Paper Calls Out
None

## Limitations
- Implementation details of tensor product routing functions remain underspecified
- Specific training hyperparameters (learning rates, batch sizes) not fully detailed
- Limited ablation studies isolating effects of tensor structure vs. routing mechanisms

## Confidence
- Claims about parameter efficiency gains: High
- Claims about performance improvements over baselines: High
- Claims about tensor product routing effectiveness: Medium (implementation details unclear)
- Claims about negative transfer mitigation: Medium (requires more detailed analysis)

## Next Checks
1. Implement and verify the tensor product routing functions with test cases showing correct parameter counts and routing behavior
2. Run ablation studies comparing TensorPoly with TLoRA variants using identical training procedures
3. Analyze routing distribution convergence during training to validate effective expert selection