---
ver: rpa2
title: A Comparative Study of Quality Evaluation Methods for Text Summarization
arxiv_id: '2407.00747'
source_url: https://arxiv.org/abs/2407.00747
tags:
- evaluation
- summarization
- human
- text
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study conducts a comprehensive evaluation of seven state-of-the-art
  summarization models on patent documents using both automatic and human evaluation
  methods. Eight widely-used automatic metrics, human evaluation, and a proposed LLM-based
  method are compared.
---

# A Comparative Study of Quality Evaluation Methods for Text Summarization

## Quick Facts
- arXiv ID: 2407.00747
- Source URL: https://arxiv.org/abs/2407.00747
- Reference count: 30
- This study evaluates seven state-of-the-art summarization models on patent documents using automatic metrics, human evaluation, and LLM-based methods

## Executive Summary
This study conducts a comprehensive evaluation of seven state-of-the-art summarization models on patent documents using both automatic and human evaluation methods. Eight widely-used automatic metrics, human evaluation, and a proposed LLM-based method are compared. Results show that commonly-used automatic metrics like ROUGE-2, BERTScore, and SummaC lack consistency and show weak correlation with human evaluation. In contrast, LLM-based evaluation aligns closely with human judgment, demonstrating high positive correlation across multiple quality dimensions. The study also proposes a framework that leverages LLM feedback to iteratively improve summarization quality, showing significant gains in clarity and coverage.

## Method Summary
The study evaluates 7 SOTA summarization models (HUPD_T5, XLNet, BART, BigBird, Pegasus, LongT5, GPT-3.5) on 1,630 patent documents, using 30 for detailed evaluation. The evaluation pipeline combines automatic metrics (ROUGE, BERTScore, SummaC, FRE, DCR), human evaluation via APPEN platform, and LLM evaluation using GPT-4 and Llama-3-8B with identical prompts. An iterative improvement framework uses LLM feedback to refine summaries across four quality dimensions: clarity, accuracy, coverage, and overall quality.

## Key Results
- Automatic metrics like ROUGE-2, BERTScore, and SummaC show weak correlation with human evaluation
- LLM-based evaluation demonstrates high positive correlation (0.8-0.9) with human judgment across quality dimensions
- Iterative improvement using LLM feedback significantly improves clarity (4.167 to 4.5) and coverage (3.567 to 3.833)
- Open-source LLMs like Llama-3-8B produce reliable evaluations comparable to GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs serve as effective evaluation agents because they can understand nuanced quality dimensions through instruction following
- Mechanism: LLMs process both the generated summary and source document, applying semantic understanding to assess alignment with human-defined quality criteria
- Core assumption: LLMs can reliably interpret evaluation prompts and maintain consistency across quality dimensions
- Evidence anchors:
  - [abstract] "LLMs evaluation aligns closely with human evaluation" and "LLMs can perform summarization evaluation as effectively as humans"
  - [section 3.3.3] "We use the same instructions given to humans as prompts to guide LLMs on this task"
- Break condition: LLM evaluation diverges from human judgment due to prompt misinterpretation or domain-specific language barriers

### Mechanism 2
- Claim: Iterative refinement using LLM feedback improves summary quality by addressing specific weaknesses
- Mechanism: LLM evaluates summary, identifies improvement areas, and generates refined version incorporating feedback
- Core assumption: LLM-generated feedback is actionable and leads to measurable quality improvements
- Evidence anchors:
  - [abstract] "propose a framework that leverages LLM feedback to iteratively improve summarization quality, showing significant gains in clarity and coverage"
  - [section 4.3] "the quality of the summaries significantly improves in terms of clarity (from 4.167 to 4.5) and coverage (from 3.567 to 3.833)"
- Break condition: Feedback loop produces diminishing returns or introduces new errors without addressing original issues

### Mechanism 3
- Claim: Open-source LLMs like Llama-3-8B produce reliable evaluations comparable to GPT-4
- Mechanism: Smaller, open-source models capture similar evaluation capabilities as larger proprietary models while reducing computational costs
- Core assumption: Model size and architecture differences don't significantly impact evaluation quality for summarization tasks
- Evidence anchors:
  - [abstract] "Open-source LLMs like Llama-3-8B produce reliable evaluations comparable to GPT-4"
  - [section 4.2.2] "The correlation scores are consistent for both GPT-4 and Llama-3-8B" with high positive correlation (0.8-0.9)
- Break condition: Open-source models fail to maintain evaluation consistency across different domains or document types

## Foundational Learning

- Concept: Correlation analysis (Kendall's Tau-b, Spearman's, Pearson's)
  - Why needed here: To quantify the relationship between different evaluation methods (automatic metrics vs human, LLM vs human)
  - Quick check question: If Kendall's Tau-b shows 0.8 correlation between two methods, what does this indicate about their relationship?

- Concept: Reference-free evaluation
  - Why needed here: Traditional metrics rely on reference summaries which are often unavailable or unsuitable for specialized domains
  - Quick check question: What's the key difference between reference-based and reference-free evaluation methods in summarization?

- Concept: Readability metrics (FRE, DCR)
  - Why needed here: To assess text accessibility and identify trade-offs between content quality and ease of reading
  - Quick check question: If a summary has high FRE score but low ROUGE scores, what might this indicate about the summary's characteristics?

## Architecture Onboarding

- Component map: Data collection (Patent documents) -> Summarization models (7 SOTA models) -> Evaluation pipeline (Automatic metrics + Human evaluation + LLM evaluation) -> Improvement loop (LLM feedback → prompt refinement → new summary generation)

- Critical path: 1. Generate summaries using multiple models 2. Evaluate using automatic metrics and LLM 3. Conduct human evaluation on sample 4. Analyze correlations between methods 5. Apply iterative improvement using LLM feedback

- Design tradeoffs:
  - Human evaluation provides gold standard but is expensive and limited in scale
  - Automatic metrics are fast but show weak correlation with human judgment
  - LLM evaluation offers scalability but requires careful prompt engineering
  - Open-source models reduce costs but may have slightly lower consistency

- Failure signatures:
  - Low correlation between evaluation methods (>0.2 difference suggests significant misalignment)
  - LLM feedback producing inconsistent or contradictory suggestions
  - Iterative improvement causing degradation in accuracy while improving other dimensions
  - Readability metrics showing negative correlation with content quality

- First 3 experiments:
  1. Run correlation analysis between all automatic metrics and human evaluation to identify reliable metrics
  2. Test LLM evaluation consistency by running multiple evaluations on same summaries
  3. Implement single iteration of improvement loop to validate feedback mechanism effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy dimension of summarization quality be further improved using the proposed LLM-powered framework?
- Basis in paper: [explicit] The paper mentions that the proposed LLM-powered framework significantly improves clarity and coverage but only slightly improves accuracy. It states, "In the future, we aim to enhance this quality dimension further."
- Why unresolved: The paper does not provide specific methods or strategies for improving accuracy beyond the initial framework.
- What evidence would resolve it: Experiments comparing different prompting strategies or incorporating additional feedback mechanisms specifically targeting accuracy improvements.

### Open Question 2
- Question: Would the proposed LLM-based evaluation and improvement framework generalize to other domains beyond legal documents?
- Basis in paper: [inferred] The paper acknowledges a limitation that findings may not apply to other domains due to differences in document structure and vocabulary. It states, "Our findings may not be applicable to other domains where the document structure and vocabulary may differ."
- Why unresolved: The study focused exclusively on legal documents, and the performance of the framework on other domains is unknown.
- What evidence would resolve it: Replicating the study with datasets from different domains (e.g., scientific articles, news, medical records) and comparing the performance of the framework across domains.

### Open Question 3
- Question: What are the specific sources of hallucinations in summarization models, and how can they be systematically identified and mitigated?
- Basis in paper: [explicit] The paper identifies hallucinations as a major type of error in summarization models and provides examples of how they occur. It states, "We identified three major types of errors that affect the quality of summarization: low abstractiveness, incompleteness, and hallucinations."
- Why unresolved: The paper does not provide a detailed analysis of the root causes of hallucinations or methods to prevent them.
- What evidence would resolve it: A systematic study of the conditions under which hallucinations occur, potentially using techniques like attention analysis or probing to identify the model's decision-making process during generation.

## Limitations
- Limited evaluation sample size (30 documents) may not represent full dataset diversity
- Findings may not generalize to other domains beyond patent documents
- Iterative improvement framework validation is limited to single iterations
- Open-source LLM comparison relies on correlation analysis rather than direct quality comparison

## Confidence
- **High Confidence**: The correlation analysis methodology and statistical approach are sound and well-documented
- **Medium Confidence**: The finding that automatic metrics poorly correlate with human evaluation, based on established literature and confirmed in this study
- **Medium Confidence**: LLM evaluation alignment with human judgment, though sample size limits definitive conclusions
- **Low Confidence**: The iterative improvement framework's effectiveness, given limited validation scope
- **Medium Confidence**: Open-source LLM evaluation quality, pending broader validation

## Next Checks
1. Scale evaluation to the full 1,630-document corpus to verify LLM evaluation consistency and iterative improvement effectiveness across more diverse samples
2. Conduct ablation studies testing different prompt formulations for LLM evaluation to identify optimal configurations and assess sensitivity to prompt variations
3. Test the evaluation framework across multiple domains beyond patents (news, scientific articles, conversational text) to assess generalizability of the LLM evaluation approach