---
ver: rpa2
title: 'QueerBench: Quantifying Discrimination in Language Models Toward Queer Identities'
arxiv_id: '2406.12399'
source_url: https://arxiv.org/abs/2406.12399
tags:
- language
- queer
- pronouns
- words
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QueerBench, a novel framework to assess bias
  in language models toward LGBTQIA+ individuals. It uses template-based sentence
  completion with Masked Language Modeling (MLM) to evaluate harmful language generated
  when queer-related terms are used as subjects.
---

# QueerBench: Quantifying Discrimination in Language Models Toward Queer Identities

## Quick Facts
- arXiv ID: 2406.12399
- Source URL: https://arxiv.org/abs/2406.12399
- Reference count: 26
- Primary result: Queer nouns as subjects produce 16.9% harmful completions vs 9.2% for non-queer subjects

## Executive Summary
This paper introduces QueerBench, a novel framework to assess bias in language models toward LGBTQIA+ individuals. It uses template-based sentence completion with Masked Language Modeling (MLM) to evaluate harmful language generated when queer-related terms are used as subjects. The QueerBench score combines sentiment analysis (AFINN), hate-word detection (HurtLex), and toxicity classification (Perspective API). Experiments on BERT, ALBERT, RoBERTa, and BERTweet models show that sentences with queer nouns as subjects are significantly more harmful than those with non-queer subjects, revealing measurable bias.

## Method Summary
QueerBench generates 8,268 sentences by combining 106 neutral templates with 78 subject terms (36 queer nouns, 42 non-queer nouns, 16 pronouns). MLM models fill masked positions in these sentences, producing top-1 and top-5 predictions. These predictions are scored using three tools: AFINN for sentiment, HurtLex for hate-word detection, and Perspective API for toxicity. The QueerBench score is the average of these three scores, providing a composite measure of harmfulness toward different subject categories.

## Key Results
- Queer nouns as subjects produce 16.9% harmful completions versus 9.2% for non-queer subjects
- Pronouns show minimal harm differences across categories
- BERTweetbase exhibits atypical results potentially due to training data composition
- Top-5 predictions show higher harm rates than top-1, indicating broader harmful patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Template-based MLM task exposes bias in model completions.
- Mechanism: By replacing subjects with queer or non-queer terms and prompting MLM to complete neutral sentences, the model's preference for harmful or biased words becomes measurable.
- Core assumption: Neutral sentence templates produce consistent context, so differences in model completions are driven by subject terms rather than template bias.
- Evidence anchors:
  - [abstract] "This is achieved using QueerBench, our new assessment framework, which employs a template-based approach and a Masked Language Modeling (MLM) task."
  - [section 3.1] "We generate a set of meaningful sentences by combining a set of subjects with neutral sentences through MLM task."
  - [corpus] Weak evidence - related work focuses on bias detection but not specifically on template-MLM methodology for queer identity bias.
- Break condition: If neutral templates contain hidden bias, or if the MLM model overfits to frequent harmful completions for certain terms.

### Mechanism 2
- Claim: Three complementary harm metrics capture different bias dimensions.
- Mechanism: AFINN scores sentiment, HurtLex flags derogatory vocabulary, and Perspective API detects toxic sentence-level patterns. Combining them balances lexical and contextual bias detection.
- Core assumption: Harmfulness in language models is multifaceted and cannot be captured by a single metric.
- Evidence anchors:
  - [section 5] "We employed three distinct techniques: AFINN and HurtLex tools to assess the model's predictions at completion-level and Perspective API to evaluate the predictions at sentence-level."
  - [abstract] "The QueerBench score combines sentiment analysis (AFINN), hate-word detection (HurtLex), and toxicity classification (Perspective API)."
  - [corpus] Moderate evidence - multiple related studies use similar multi-metric frameworks for bias assessment.
- Break condition: If one metric dominates the aggregate score or if metrics are highly correlated, reducing the value of combining them.

### Mechanism 3
- Claim: Top-1 vs. top-5 predictions reveal different bias intensities.
- Mechanism: Top-1 captures the model's most confident harmful bias; top-5 shows broader patterns of harmfulness, including lower-confidence but still problematic completions.
- Core assumption: Higher-ranked completions are more indicative of model bias than lower-ranked ones.
- Evidence anchors:
  - [section 6.1.2] "Models yield more negative scores in scenarios in top-5 predictions, as this leads to a higher percentage, ranging from a minimum of 4% to a peak of 17%, of harmful terms."
  - [section 6.3] "Top-5 prediction results in higher statistics with an average variation of approximately 5% compared to top-1 prediction."
  - [corpus] Weak evidence - few related studies explicitly compare harmfulness across prediction ranks.
- Break condition: If harmfulness is uniformly distributed across ranks, making the distinction irrelevant.

## Foundational Learning

- Concept: Masked Language Modeling (MLM) task mechanics
  - Why needed here: Understanding how MLM fills blanks is key to interpreting QueerBench's methodology.
  - Quick check question: What is the difference between MLM and standard language generation in how completions are selected?

- Concept: Bias quantification in NLP
  - Why needed here: QueerBench uses sentiment, hate-word, and toxicity scores to quantify bias.
  - Quick check question: Why might a single bias metric be insufficient for assessing language model harm?

- Concept: Queer identity terminology and heteronormativity
  - Why needed here: QueerBench's noun categories rely on understanding what terms are considered queer vs. non-queer.
  - Quick check question: How does heteronormative framing affect the classification of "neutral" terms like "boy" or "straight"?

## Architecture Onboarding

- Component map: Data generation (templates + subjects) -> MLM prediction (BERT/ALBERT/RoBERTa/BERTweet) -> Multi-metric harm scoring (AFINN, HurtLex, Perspective) -> QueerBench aggregation -> Analysis
- Critical path: Template+subject -> MLM -> harm scoring -> final QueerBench score
- Design tradeoffs: Using multiple metrics increases robustness but adds computational overhead; limiting to top-1 and top-5 predictions balances depth and runtime
- Failure signatures: If harm scores are uniformly low across all categories, the template design or metric thresholds may be misaligned
- First 3 experiments:
  1. Run QueerBench on a single neutral template with one queer and one non-queer subject; verify score differences
  2. Compare AFINN-only vs. multi-metric QueerBench scores on the same dataset; check for score correlation
  3. Execute top-1 and top-5 predictions on a small subject set; compare harmfulness deltas

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs trained on diverse or specifically curated datasets perform on QueerBench compared to standard models?
- Basis in paper: [explicit] The paper notes BERTweetbase's atypical results may stem from its training data composition and suggests training resources impact bias detection.
- Why unresolved: The study uses standard BERT, ALBERT, RoBERTa, and BERTweet models without comparing against models trained on deliberately diverse or queer-inclusive datasets.
- What evidence would resolve it: Benchmarking QueerBench on LLMs specifically trained with balanced queer representation and inclusive terminology, then comparing harm scores to standard models.

### Open Question 2
- Question: Do intersectional identities (e.g., queer individuals who are also racial minorities) show compounded bias effects in language models?
- Basis in paper: [inferred] The authors mention the importance of intersectional perspectives but focus only on queer versus non-queer nouns without examining intersecting marginalized identities.
- Why unresolved: The study uses a binary categorization (queer vs. non-queer) and doesn't explore how multiple marginalized identities might interact to produce different bias patterns.
- What evidence would resolve it: Extending QueerBench to include subjects with multiple intersecting identities and measuring whether harm scores increase multiplicatively or additively.

### Open Question 3
- Question: How stable are QueerBench scores across different template structures and sentence contexts?
- Basis in paper: [explicit] The authors created 106 neutral sentences but don't systematically test whether different template structures yield different bias patterns.
- Why unresolved: The study uses a fixed set of neutral templates without varying sentence structure, verb choice, or contextual framing to test robustness of the findings.
- What evidence would resolve it: Running QueerBench across multiple template sets with varied syntactic structures and comparing score consistency to identify context-dependent bias patterns.

## Limitations

- Template Bias Risk: The paper assumes neutral templates are truly neutral, but subtle heteronormative framing could influence model completions.
- Metric Correlation and Overlap: The paper does not report inter-correlations between the three harm metrics, which may reduce their independent validation value.
- Generalizability Across Models: The study evaluates only four transformer models, limiting the generalizability of findings to other architectures or larger models.

## Confidence

- High Confidence: The core methodology of using template-based MLM with multiple harm metrics is well-specified and reproducible.
- Medium Confidence: The claim that pronouns show minimal harm differences across categories is based on reported data but underlying reasons are not deeply explored.
- Low Confidence: The assumption that neutral templates are free of implicit bias is not empirically tested.

## Next Checks

1. **Template Bias Audit**: Re-run QueerBench using a subset of templates that have been manually reviewed and annotated for implicit heteronormativity. Compare harm scores before and after template revision to isolate template bias effects.

2. **Metric Correlation Analysis**: Compute pairwise correlations between AFINN, HurtLex, and Perspective API scores across all model predictions. If correlations exceed 0.7, investigate whether a single dominant metric is driving the QueerBench score and consider alternative weighting schemes.

3. **Cross-Model Generalizability Test**: Apply QueerBench to at least two additional transformer models (e.g., DistilBERT, RoBERTa-large) and one non-transformer model (e.g., ELMo or GloVe-based). Compare bias patterns to assess whether the observed harm trends are model-specific or more universal.