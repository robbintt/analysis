---
ver: rpa2
title: 'Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity
  Separation between CNNs, LCNs, and FCNs'
arxiv_id: '2403.15707'
source_url: https://arxiv.org/abs/2403.15707
tags:
- then
- bound
- distribution
- algorithm
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proves sample complexity separations between CNNs, LCNs,
  and FCNs on a synthetic image task that captures locality and translation invariance.
  The key insight is that weight sharing in CNNs enables learning the signal vector
  once and applying it across all patches, while LCNs must learn it separately for
  each patch, and FCNs additionally incur costs identifying patch locations.
---

# Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs

## Quick Facts
- arXiv ID: 2403.15707
- Source URL: https://arxiv.org/abs/2403.15707
- Authors: Aakash Lahoti; Stefani Karp; Ezra Winston; Aarti Singh; Yuanzhi Li
- Reference count: 40
- Key outcome: Proves sample complexity separations between CNNs, LCNs, and FCNs on a synthetic image task, with CNNs requiring $\tilde{O}(k+d)$ samples, LCNs needing $\tilde{O}(k(k+d))$ samples, and FCNs requiring $\Omega(k^2d)$ samples.

## Executive Summary
This work establishes theoretical sample complexity separations between three neural network architectures—CNNs, LCNs, and FCNs—on a synthetic task called Dynamic Signal Distribution (DSD). The task captures locality and translation invariance properties of vision tasks by modeling an image as $k$ patches where a $d$-sparse signal vector appears in exactly one patch. The key insight is that weight sharing in CNNs enables learning the signal vector once and applying it across all patches, while LCNs must learn it separately for each patch, and FCNs additionally incur costs identifying patch locations. This is the first work to quantify the statistical benefits of locality and weight sharing in CNNs relative to simpler architectures.

## Method Summary
The paper analyzes one-hidden-layer neural networks on the Dynamic Signal Distribution task. Three architectures are compared: FCNs with $k$ hidden nodes each with parameter vector $w_i \in \mathbb{R}^{kd}$, LCNs with $k$ hidden nodes each with parameter vector $w_i \in \mathbb{R}^d$, and CNNs with $k$ hidden nodes sharing a single weight vector $w \in \mathbb{R}^d$. All use Local Signal Adaptivity (LSA) activation function $\phi_b(x) = \text{ReLU}(x-b) - \text{ReLU}(-x-b)$. Sample complexity bounds are proven using information-theoretic lower bounds for randomized equivariant algorithms for CNNs and LCNs, while upper bounds are established via gradient descent analysis with projection onto unit balls.

## Key Results
- CNNs require $\tilde{O}(k+d)$ samples on the DSD task while LCNs need $\tilde{O}(k(k+d))$ samples
- FCNs require $\Omega(k^2d)$ samples, demonstrating the benefits of locality from LCNs and further benefits from weight sharing in CNNs
- The sample complexity separations are proven through novel information-theoretic tools for analyzing randomized equivariant algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CNNs require $\tilde{O}(k+d)$ samples while FCNs require $\Omega(k^2d)$ samples on the Dynamic Signal Distribution task.
- Mechanism: Weight sharing in CNNs allows the signal vector to be learned once and applied across all patches, eliminating the need to learn it separately for each patch as LCNs must do.
- Core assumption: The signal vector appears in exactly one patch per image, and all patches have the same dimension d.
- Evidence anchors:
  - [abstract] "CNNs require $\tilde{O}(k+d)$ samples, LCNs need $\tilde{O}(k(k+d))$ samples, and FCNs require $\Omega(k^2d)$ samples"
  - [section 1] "FCNs incur a multiplicative cost factor of k for each of the two reasons: identifying the location of the k patches, and learning the signal vector for each patch"
- Break condition: If the signal appears in multiple patches simultaneously or patches have different dimensions, the weight sharing advantage diminishes.

### Mechanism 2
- Claim: LCNs require $\Omega(kd)$ samples while CNNs require $\tilde{O}(k+d)$ samples on the Dynamic Signal Distribution task.
- Mechanism: CNNs use shared weights across all patches, allowing joint learning of the signal vector, while LCNs must learn separate weight vectors for each patch location.
- Core assumption: The task exhibits translation invariance where the signal location can vary freely without changing the label.
- Evidence anchors:
  - [abstract] "LCNs need $\tilde{O}(k(k+d))$ samples, compared to $\Omega(k^2d)$ samples for FCNs, showcasing the benefits of locality"
  - [section 1] "LCNs incur a k cost for identification of the patches since the location of all the patches is baked into the architecture"
- Break condition: If the signal location is fixed or if inter-patch interactions are necessary for classification, the locality advantage disappears.

### Mechanism 3
- Claim: The Dynamic Signal Distribution task accurately models locality and translation invariance properties of real-world vision tasks.
- Mechanism: The task models an image as k patches where a d-sparse signal vector appears in exactly one patch, with all other patches containing isotropic Gaussian noise, and the label depends only on the signal sign.
- Core assumption: Real-world vision tasks have local signals embedded in noise that can translate freely within the image without changing the output label.
- Evidence anchors:
  - [section 1] "The DSD task is designed to capture both locality and translation invariance properties into an analyzable task"
  - [section 1] "Often, this signal is enveloped in random noise, and satisfies translation invariance, that is its movement within the image does not alter the output"
- Break condition: If real-world tasks require modeling multiple simultaneous signals or second-order patch interactions, this simplified model becomes insufficient.

## Foundational Learning

- Concept: Equivariant algorithms and their relationship to architectural biases
  - Why needed here: The sample complexity separation depends on proving lower bounds for equivariant algorithms, which respect the architectural constraints of CNNs, LCNs, and FCNs
  - Quick check question: Why can't we simply use standard minimax lower bounds to prove the separation between these architectures?

- Concept: Information-theoretic lower bounds for randomized algorithms
  - Why needed here: The proof technique involves developing new information-theoretic tools for analyzing randomized algorithms that don't require semi-metric properties on the entire function space
  - Quick check question: What makes proving lower bounds for the Dynamic Signal Distribution task more challenging than for standard Gaussian mean estimation problems?

- Concept: Gradient descent analysis for sample complexity upper bounds
  - Why needed here: The upper bounds are proven using gradient descent analysis rather than uniform convergence arguments, demonstrating computational efficiency
  - Quick check question: How does the two-iteration gradient descent procedure enable CNNs to achieve the optimal $\tilde{O}(k+d)$ sample complexity?

## Architecture Onboarding

- Component map:
  - FCN: k hidden nodes, each with parameter vector wi ∈ R^kd, bias b
  - LCN: k hidden nodes, each with parameter vector wi ∈ Rd, bias b  
  - CNN: k hidden nodes, shared parameter vector w ∈ Rd, bias b
  - All use LSA activation function ϕ_b(x) = ReLU(x - b) - ReLU(-x - b)

- Critical path: 
  1. Initialize parameters according to the architecture
  2. Train using gradient descent with projection onto unit ball
  3. Evaluate alignment between learned parameters and true signal vector
  4. Verify risk meets theoretical bounds

- Design tradeoffs:
  - FCNs offer maximum flexibility but require O(k^2d) samples due to lack of architectural biases
  - LCNs reduce parameter count to O(kd) but still require O(kd) samples due to per-patch learning
  - CNNs achieve optimal O(k+d) sample complexity through weight sharing but sacrifice flexibility

- Failure signatures:
  - If alignment between learned parameters and signal vector remains below 0.25 after training, the sample complexity bound is not met
  - If gradient descent does not converge within two iterations, the analysis breaks down
  - If the bias parameter b is not properly tuned, denoising effects may fail

- First 3 experiments:
  1. Train each architecture on Dynamic Signal Distribution with k=10, d=20, σ=0.1 using 1000 samples, measure alignment and risk
  2. Vary k from 10 to 30 with fixed d=20, plot sample complexity vs k for each architecture
  3. Vary d from 10 to 30 with fixed k=20, plot sample complexity vs d for each architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the sample complexity separation between CNNs, LCNs, and FCNs persist in deeper architectures beyond one hidden layer?
- Basis in paper: The paper states "In future work, we plan to incorporate second-order characteristics of images into the data model. For instance, allowing multiple signals to appear across different patches simultaneously would mirror real-world scenarios where multiple objects occur. Additionally, an interesting direction would be to analyze the role of depth in a CNN in capturing dependency between different patches."
- Why unresolved: The current analysis only considers one-hidden-layer networks, and the paper explicitly mentions depth as a future research direction without providing any results.
- What evidence would resolve it: Empirical and theoretical analysis comparing sample complexities of deeper CNNs, LCNs, and FCNs on tasks capturing multi-signal and patch dependencies.

### Open Question 2
- Question: How does the Dynamic Signal Distribution task's sample complexity bounds change if the noise variance σ² is not constant across patches?
- Basis in paper: The DSD task assumes isotropic Gaussian noise with constant variance σ² across all patches, but this assumption is not explored for non-constant noise distributions.
- Why unresolved: The theoretical analysis relies heavily on the uniform noise assumption to derive clean bounds, and no alternative noise models are considered.
- What evidence would resolve it: Sample complexity analysis for CNNs, LCNs, and FCNs under heterogeneous noise distributions (e.g., patch-dependent σ² or non-Gaussian noise).

### Open Question 3
- Question: Can the information-theoretic tools developed for analyzing randomized algorithms be extended to prove lower bounds for non-equivariant algorithms on the DSD task?
- Basis in paper: The paper develops novel information-theoretic tools for randomized algorithms and proves lower bounds specifically for equivariant algorithms, noting that "without taking the training algorithm into consideration, standard lower bound techniques cannot be used to show a separation between the three models."
- Why unresolved: The tools are designed for equivariant algorithms, and extending them to non-equivariant cases would require handling algorithm-specific simulation capabilities that can bypass architectural constraints.
- What evidence would resolve it: Lower bound proofs for non-equivariant algorithms (e.g., those using data-dependent random projections) on DSD that either maintain or eliminate the separation between architectures.

## Limitations

- The analysis is limited to a highly simplified synthetic task that may not fully capture the complexity of real-world vision problems
- The theoretical bounds assume idealized conditions including perfect gradient descent convergence and optimal parameter tuning
- The sample complexity advantages may be less pronounced in practical settings where other factors like optimization landscape and generalization play important roles

## Confidence

- Sample complexity bounds for CNNs: High
- Sample complexity bounds for LCNs: High
- Sample complexity bounds for FCNs: High
- Practical relevance to real vision tasks: Medium
- Robustness to task variations: Low

## Next Checks

1. **Empirical verification**: Implement the three architectures and verify that the observed sample complexity on the DSD task matches the theoretical bounds of $\tilde{O}(k+d)$ for CNNs, $\tilde{O}(k(k+d))$ for LCNs, and $\Omega(k^2d)$ for FCNs.

2. **Generalization to other tasks**: Test whether the sample complexity advantages persist on more complex synthetic tasks that relax some assumptions of the DSD model, such as allowing multiple signals or inter-patch dependencies.

3. **Real-world evaluation**: Apply the theoretical insights to benchmark vision tasks (e.g., CIFAR-10) by comparing CNN variants with different degrees of locality and weight sharing to quantify the practical impact of the identified architectural biases.