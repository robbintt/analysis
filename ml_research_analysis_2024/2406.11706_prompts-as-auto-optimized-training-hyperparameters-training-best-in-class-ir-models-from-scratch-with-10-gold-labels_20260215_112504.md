---
ver: rpa2
title: 'Prompts as Auto-Optimized Training Hyperparameters: Training Best-in-Class
  IR Models from Scratch with 10 Gold Labels'
arxiv_id: '2406.11706'
source_url: https://arxiv.org/abs/2406.11706
tags:
- training
- prompt
- queries
- which
- reranker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of training small neural IR models\
  \ with minimal labeled data. The authors introduce PATH, a method that generates\
  \ synthetic queries for documents using a language model, then optimizes the prompt\
  \ for query generation based on the resulting IR model\u2019s performance."
---

# Prompts as Auto-Optimized Training Hyperparameters: Training Best-in-Class IR Models from Scratch with 10 Gold Labels

## Quick Facts
- arXiv ID: 2406.11706
- Source URL: https://arxiv.org/abs/2406.11706
- Reference count: 9
- Primary result: Trains small IR models from scratch using only 10 relevance labels, achieving performance competitive with models trained on 100K+ labels

## Executive Summary
This paper introduces PATH, a method for training small neural IR models (under 100M parameters) using minimal labeled data. The key innovation is automatically optimizing the prompt used to generate synthetic queries for documents based on the resulting reranker's performance. By iteratively refining prompts with a large language model, PATH generates high-quality synthetic training data that enables competitive performance on the BIRCO benchmark with only 10 gold labels per task. The method consistently outperforms manual prompting and direct training on few labels, achieving an average 4.5-point improvement in NDCG@10.

## Method Summary
PATH generates synthetic queries for documents using a large language model, then optimizes the prompt for query generation based on the resulting IR model's performance. The method uses the DSPy framework with CA-OPRO optimizer to iteratively refine prompts by measuring the reranker's NDCG@10 on held-out data. Small encoder models (DeBERTa and MiniLM) are trained as rerankers on the synthetic data. The process requires only 10 gold relevance labels per task and produces models competitive with RankZephyr and RankLLama, which use 100K+ labels.

## Key Results
- Achieves 4.5-point average improvement in NDCG@10 over manual prompts
- Matches performance of models trained on all available training triples
- Outperforms RankZephyr and RankLLama on complex tasks like ArguAna and Relic
- Works with as few as 10 gold relevance labels per task
- Uses small models (under 100M parameters) compared to billion-parameter baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PATH's prompt optimization works because the synthetic query generation process is directly linked to downstream reranker performance.
- Mechanism: By optimizing the prompt used to generate synthetic queries based on the average NDCG@10 of the reranker trained on those queries, PATH ensures that the generated queries are tailored to the specific IR task and the reranker's strengths.
- Core assumption: The quality of synthetic queries directly correlates with the quality of the reranker trained on them.
- Evidence anchors:
  - [abstract]: "the key step is that we automatically optimize the LM prompt that is used to generate these queries based on training quality."
  - [section]: "Our central contribution is that we seek to automatically construct a prompt template that maximizes the quality of the resulting R."
  - [corpus]: Weak evidence - no direct corpus support for this mechanism.

### Mechanism 2
- Claim: PATH leverages the power of large language models to generate task-specific synthetic queries.
- Mechanism: By using a large language model to generate synthetic queries, PATH can create a diverse set of queries that are tailored to the specific IR task and document corpus.
- Core assumption: Large language models have the capacity to generate high-quality, task-specific queries.
- Evidence anchors:
  - [abstract]: "The method depends on generating synthetic queries for documents using a language model (LM)."
  - [section]: "An increasingly common paradigm to tackle the lack of IR data in a given domain is to use LMs to synthesize hypothetical search queries q that are derived from passages p in a corpus C."
  - [corpus]: Weak evidence - no direct corpus support for this mechanism.

### Mechanism 3
- Claim: PATH's iterative prompt optimization process allows for continuous improvement of the synthetic query generation.
- Mechanism: By iteratively refining the prompt used to generate synthetic queries based on the reranker's performance, PATH can continuously improve the quality of the synthetic queries and, consequently, the reranker.
- Core assumption: The prompt optimization process can effectively identify and leverage the strengths of the large language model and the reranker.
- Evidence anchors:
  - [abstract]: "the key step is that we automatically optimize the LM prompt that is used to generate these queries based on training quality."
  - [section]: "In particular, we adopt the simple strategy of having the LM generate candidate modifications of our initial instruction and choosing the one that ultimately leads to the best reranker."
  - [corpus]: Weak evidence - no direct corpus support for this mechanism.

## Foundational Learning

- Concept: Understanding of information retrieval (IR) tasks and metrics.
  - Why needed here: PATH is designed to improve the performance of IR models, specifically rerankers, on various IR tasks.
  - Quick check question: Can you explain the difference between a retriever and a reranker in the context of IR?

- Concept: Knowledge of language models and their applications in IR.
  - Why needed here: PATH relies on large language models to generate synthetic queries and optimize prompts.
  - Quick check question: How can language models be used to generate synthetic queries for IR tasks?

- Concept: Familiarity with prompt engineering and optimization techniques.
  - Why needed here: PATH's core innovation is the automatic optimization of prompts used to generate synthetic queries.
  - Quick check question: What are some common techniques for optimizing prompts in the context of language model applications?

## Architecture Onboarding

- Component map: GPT-3.5-turbo (LM) -> DSPy framework (prompt optimization) -> DeBERTa/MiniLM (reranker) -> BIRCO benchmark (evaluation)

- Critical path:
  1. Optimize the prompt using the large language model and the reranker's performance
  2. Generate synthetic queries using the optimized prompt
  3. Train the reranker on the synthetic queries
  4. Evaluate the reranker's performance on the IR task

- Design tradeoffs:
  - Parameter count: PATH uses small encoder models (under 100M parameters) to reduce computational costs, but this may limit the reranker's capacity
  - Data requirements: PATH can work with as few as 10 gold labels, but more data may lead to better performance
  - Prompt optimization strategy: The choice of prompt optimization algorithm can impact the quality of the synthetic queries and the reranker

- Failure signatures:
  - Poor reranker performance: May indicate issues with the prompt optimization, synthetic query generation, or the reranker's capacity
  - Overfitting: May occur if the reranker is trained on a limited set of synthetic queries or if the prompt optimization process is too aggressive

- First 3 experiments:
  1. Implement a basic version of PATH with a fixed prompt and evaluate its performance on a simple IR task
  2. Introduce prompt optimization and assess its impact on the reranker's performance
  3. Experiment with different prompt optimization strategies and compare their effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PATH perform with different initial prompt templates, and what is the impact of prompt quality on the final model performance?
- Basis in paper: Inferred
- Why unresolved: The paper only uses one initial prompt per task, and it is unclear how changing the initial prompt would affect the performance of the trained models.
- What evidence would resolve it: Conducting experiments with multiple initial prompts for each task and comparing the final model performance to assess the sensitivity of PATH to the quality of the initial prompt.

### Open Question 2
- Question: What are the effects of varying the number of synthetic queries generated per document on the performance of PATH-trained models?
- Basis in paper: Inferred
- Why unresolved: The paper uses a fixed number of synthetic queries per document, but it is not clear if this number is optimal or if varying it would lead to better performance.
- What evidence would resolve it: Experimenting with different numbers of synthetic queries per document and evaluating the impact on model performance to determine the optimal number of queries.

### Open Question 3
- Question: How does PATH scale with larger models, and what is the performance of PATH-trained models when using billion-parameter models?
- Basis in paper: Inferred
- Why unresolved: The paper focuses on small models under 100M parameters, and it is unclear how PATH would perform with larger models.
- What evidence would resolve it: Training larger models with PATH and comparing their performance to the smaller models used in the paper to assess the scalability of the method.

### Open Question 4
- Question: What is the impact of using different prompt optimization algorithms within the DSPy framework on the performance of PATH-trained models?
- Basis in paper: Inferred
- Why unresolved: The paper uses CA-OPRO as the prompt optimization algorithm, but it is not clear if other algorithms in DSPy would yield better results.
- What evidence would resolve it: Comparing the performance of PATH-trained models using different prompt optimization algorithms in DSPy to determine the most effective approach.

### Open Question 5
- Question: How does the choice of negative sampling strategy affect the performance of PATH-trained models?
- Basis in paper: Inferred
- Why unresolved: The paper uses a fixed negative sampling strategy, but it is not clear if alternative strategies would lead to better performance.
- What evidence would resolve it: Experimenting with different negative sampling strategies and evaluating their impact on model performance to identify the most effective approach.

## Limitations

- Evaluation relies entirely on synthetic queries generated by GPT-3.5-turbo, creating potential concerns about distribution shift
- Performance generalizability to other IR tasks and domains beyond BIRCO remains uncertain
- Computational overhead of iterative prompt optimization process may become prohibitive for larger-scale applications

## Confidence

**High Confidence**: The core claim that PATH outperforms manual prompting and direct training on few labels is well-supported by experimental results across multiple datasets.

**Medium Confidence**: The claim that PATH matches the quality of models trained on all available training triples is supported by results but warrants further validation.

**Low Confidence**: The claim about PATH's effectiveness on "complex tasks like ArguAna and Relic" is based on limited evidence from only two of eight tasks in BIRCO.

## Next Checks

1. **Distribution Shift Analysis**: Conduct human evaluation comparing synthetic queries generated by PATH to real user queries to quantify distribution shift.

2. **Ablation Study**: Systematically remove components of PATH to identify which elements contribute most to performance gains.

3. **Generalization Test**: Apply PATH to an external IR benchmark (e.g., MS MARCO or TREC datasets) not used in original evaluation.