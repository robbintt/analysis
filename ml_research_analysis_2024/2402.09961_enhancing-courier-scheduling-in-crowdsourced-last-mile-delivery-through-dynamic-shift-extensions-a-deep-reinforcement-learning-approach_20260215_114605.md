---
ver: rpa2
title: 'Enhancing Courier Scheduling in Crowdsourced Last-Mile Delivery through Dynamic
  Shift Extensions: A Deep Reinforcement Learning Approach'
arxiv_id: '2402.09961'
source_url: https://arxiv.org/abs/2402.09961
tags:
- couriers
- requests
- shift
- extension
- delivery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops a deep reinforcement learning model to optimize
  crowdsourced last-mile delivery by dynamically extending committed couriers' shifts
  in response to fluctuating demand. The model learns to decide which couriers to
  notify for shift extensions and how to assign incoming requests, balancing platform
  profit with operational costs.
---

# Enhancing Courier Scheduling in Crowdsourced Last-Mile Delivery through Dynamic Shift Extensions: A Deep Reinforcement Learning Approach

## Quick Facts
- arXiv ID: 2402.09961
- Source URL: https://arxiv.org/abs/2402.09961
- Authors: Zead Saleh; Ahmad Al Hanbali; Ahmad Baubaid
- Reference count: 26
- Key outcome: Dynamic shift extensions using deep reinforcement learning reduce lost delivery requests by 58% and improve platform efficiency in crowdsourced last-mile delivery

## Executive Summary
This study addresses the challenge of fluctuating demand in crowdsourced last-mile delivery by developing a deep reinforcement learning (DRL) model that dynamically extends couriers' shifts. The model learns to make optimal decisions on which couriers to notify for shift extensions and how to assign incoming requests, balancing platform profit with operational costs. By benchmarking against a no-extension baseline, the proposed approach demonstrates significant improvements in delivery performance, reducing lost requests from 27% to 11% while enhancing total reward and cost efficiency. The research provides practical insights for delivery platforms seeking to optimize their operations in response to variable demand patterns.

## Method Summary
The researchers developed a deep reinforcement learning framework that operates within a simulated environment of crowdsourced last-mile delivery. The model uses state representations including courier positions, remaining delivery requests, and time constraints to make decisions about shift extensions. A reward function balances platform profit against extension costs, with the DRL agent trained using proximal policy optimization (PPO). The simulation incorporates realistic courier behavior patterns and demand fluctuations, allowing the model to learn optimal extension strategies over thousands of training episodes.

## Key Results
- Lost delivery requests reduced by 58% (from 27% to 11%) compared to no-extension baseline
- Total reward increased while operational costs decreased
- Extension costs rise nonlinearly with demand but linearly with occasional courier arrivals

## Why This Works (Mechanism)
The deep reinforcement learning approach works by learning complex patterns in courier availability and demand fluctuations that would be difficult to capture with traditional optimization methods. The model identifies optimal moments for shift extensions by evaluating the trade-off between immediate extension costs and long-term benefits of having available couriers. By considering the spatial distribution of couriers and pending requests, the system can make informed decisions about where extensions will have the greatest impact on delivery completion rates.

## Foundational Learning
- Reinforcement learning fundamentals: Why needed - provides framework for sequential decision-making under uncertainty; Quick check - agent learns to maximize cumulative reward over time
- Deep neural networks for function approximation: Why needed - handles high-dimensional state spaces in real-world delivery scenarios; Quick check - network generalizes to unseen state combinations
- Proximal Policy Optimization: Why needed - stable training for continuous action spaces in extension decisions; Quick check - monotonic improvement in policy performance

## Architecture Onboarding

Component map: Environment (delivery simulation) -> State Encoder (courier positions, requests) -> DRL Agent (PPO) -> Action Selector (extensions/assignments) -> Reward Calculator -> Update weights

Critical path: State observation → Neural network forward pass → Action selection → Environment step → Reward calculation → Gradient update

Design tradeoffs: Model prioritizes operational efficiency over courier welfare to maximize platform profit, which may affect long-term courier retention but improves immediate service levels

Failure signatures: Over-extension leading to increased costs without proportional delivery gains; under-extension causing high request loss rates

First experiments:
1. Compare DRL performance against heuristic baseline under varying demand patterns
2. Test sensitivity to courier arrival rate changes
3. Evaluate model robustness across different geographic distribution patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on simulation data rather than real-world deployment
- Model performance requires validation under live operational conditions
- Generalizability across different geographic markets and demand patterns needs testing

## Confidence
High: Simulation results showing 58% reduction in lost requests
Medium: Transferability to real platforms and different market conditions
Low: Long-term effects on courier retention and customer satisfaction

## Next Checks
1. Field test the model on an actual crowdsourced delivery platform to measure real-world impact
2. Cross-market validation using historical data from multiple cities
3. Extended testing of the nonlinear cost model under varying courier density scenarios