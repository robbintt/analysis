---
ver: rpa2
title: 'Learn to Disguise: Avoid Refusal Responses in LLM''s Defense via a Multi-agent
  Attacker-Disguiser Game'
arxiv_id: '2404.02532'
source_url: https://arxiv.org/abs/2404.02532
tags:
- learning
- attacker
- attack
- disguise
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of defending large language models
  against jailbreak attacks by proposing a novel multi-agent attacker-disguiser game
  framework. The core method involves simulating attack-defense scenarios using four
  types of agents: attacker, disguiser, safety evaluator, and disguise evaluator.'
---

# Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game

## Quick Facts
- arXiv ID: 2404.02532
- Source URL: https://arxiv.org/abs/2404.02532
- Reference count: 40
- Key outcome: Achieves 89.83% PoR on Generated_Attack and 94.46% PoR on XSAFETY datasets using multi-agent game framework

## Executive Summary
This paper proposes a novel defense mechanism against jailbreak attacks on large language models that avoids explicit refusal responses. The core innovation is a multi-agent game framework where attacker and disguiser agents compete while safety and disguise evaluators provide feedback. Through this adversarial training process, the model learns to generate responses that are both safe and disguise their defensive intent, making it harder for attackers to craft effective jailbreak prompts. The method uses in-context learning and curriculum learning to iteratively improve disguise capabilities without requiring model parameter updates.

## Method Summary
The proposed approach employs a multi-agent attacker-disguiser game framework with four agents: attacker, disguiser, safety evaluator, and disguise evaluator. The attacker generates prompts designed to elicit harmful responses, while the disguiser generates responses that are both safe and conceal defensive intent. All agents use in-context learning with strategically selected examples to generate their outputs. The framework employs Minimax Q-learning to optimize the game process, with agents iteratively selecting enhanced samples based on reward scores until reaching a Nash equilibrium. The safety evaluator ensures responses are secure, while the disguise evaluator checks that defensive intent is hidden. This process strengthens the model's ability to disguise defense through curriculum learning, progressively increasing the difficulty of training samples.

## Key Results
- Achieves 89.83% PoR (proportion of safe and disguised responses) on Generated_Attack dataset
- Achieves 94.46% PoR on XSAFETY dataset across 7 safety scenarios
- Outperforms baseline methods (Norm, Instruct, ICL, RJ) in disguise capability
- Demonstrates effectiveness across English prompts and multiple safety domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent adversarial game enables the model to iteratively improve its ability to disguise defensive intent.
- Mechanism: Attacker and disguiser agents play a zero-sum game, with safety and disguise evaluators providing reward signals. Through iterative rounds, each agent optimizes its strategy to maximize gain while minimizing the opponent's gain, leading to a Nash equilibrium where the disguiser effectively generates safe responses that conceal defensive intent.
- Core assumption: The game-theoretic framework accurately models the adversarial interaction between attackers and defenders, and the evaluators' reward signals effectively guide the learning process.
- Evidence anchors:
  - [abstract]: "We propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent."
  - [section]: "We use the Minimax Q-learning algorithm [15] to optimize the attacker-disguiser game process and solve the optimal game strategy for both."
  - [corpus]: Weak - The corpus provides related papers on LLM jailbreaking and defense, but none specifically discuss a multi-agent game approach for disguising defensive intent.
- Break condition: If the evaluators' reward signals do not accurately reflect the true safety and disguise of the responses, the game may converge to a suboptimal equilibrium.

### Mechanism 2
- Claim: In-context learning with strategically selected examples enhances the model's ability to generate disguised responses.
- Mechanism: The attacker and disguiser agents use in-context learning to generate responses based on a few examples. Through the adversarial game, they select the most effective examples for the next round, creating a curriculum learning process that gradually increases the difficulty of the training samples.
- Core assumption: The in-context learning approach is effective in few-shot learning scenarios, and the examples selected through the game process are indeed the most effective for enhancing the agents' abilities.
- Evidence anchors:
  - [abstract]: "We use the curriculum learning process to strengthen the capabilities of the agents."
  - [section]: "We use the in-context learning method to guide the attacker to generate induced attack questions and provide samples to enhance the attacker's attack capability."
  - [corpus]: Weak - The corpus mentions in-context learning in related papers, but none discuss its use in a multi-agent game for disguising defensive intent.
- Break condition: If the in-context learning approach does not effectively transfer knowledge from the examples to the new responses, or if the selected examples do not represent the most challenging scenarios, the learning process may stagnate.

### Mechanism 3
- Claim: The weak defense mechanism (disguised responses) is more effective than strong defense mechanisms (refusal responses) in preventing attackers from strengthening their capabilities.
- Mechanism: By generating responses that are safe but do not explicitly refuse to answer, the model avoids providing clear signals to attackers about which prompts are considered harmful. This makes it harder for attackers to craft effective jailbreak prompts.
- Core assumption: Attackers rely on identifying refusal responses to strengthen their attack strategies, and disguised responses are less likely to be exploited in this way.
- Evidence anchors:
  - [abstract]: "However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities."
  - [section]: "The experiments verify that the method in this paper is more effective in strengthening the model's ability to disguise the defense intent compared with other methods."
  - [corpus]: Weak - The corpus discusses various LLM defense mechanisms, but none specifically compare the effectiveness of weak defense (disguised responses) versus strong defense (refusal responses) in preventing attackers from strengthening their capabilities.
- Break condition: If attackers can still identify disguised responses as defensive, or if the disguised responses are not actually safe, the weak defense mechanism may not be effective.

## Foundational Learning

- Concept: Game Theory
  - Why needed here: The multi-agent attacker-disguiser game is based on game-theoretic principles, where each agent optimizes its strategy based on the expected actions of the other agents.
  - Quick check question: What is a Nash equilibrium, and how does it apply to the attacker-disguiser game?

- Concept: Reinforcement Learning
  - Why needed here: The agents learn to optimize their strategies through a reinforcement learning process, where they receive rewards based on the effectiveness of their responses.
  - Quick check question: How does the Minimax Q-learning algorithm work, and how is it applied in the attacker-disguiser game?

- Concept: In-context Learning
  - Why needed here: The agents use in-context learning to generate responses based on a few examples, which allows them to quickly adapt to new scenarios without extensive fine-tuning.
  - Quick check question: What is in-context learning, and how does it differ from traditional fine-tuning approaches?

## Architecture Onboarding

- Component map: Attacker agent -> Disguiser agent -> Safety evaluator -> Disguise evaluator
- Critical path: Attacker generates prompt → Disguiser generates response → Safety evaluator scores → Disguise evaluator scores → Agents update strategies via Minimax Q-learning
- Design tradeoffs: The system trades off between the complexity of the game-theoretic framework and the simplicity of the in-context learning approach. The multi-agent game allows for more sophisticated learning, but also introduces additional complexity.
- Failure signatures: If the system fails to generate effective disguised responses, it may be due to suboptimal reward signals from the evaluators, insufficient training data, or an ineffective in-context learning approach.
- First 3 experiments:
  1. Evaluate the effectiveness of the safety and disguise evaluators by comparing their scores to human judgments.
  2. Test the impact of different reward signal formulations on the learning process of the attacker and disguiser agents.
  3. Compare the performance of the multi-agent game approach to other defense mechanisms, such as instruction tuning or fine-tuning, in terms of the proportion of disguised responses generated.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several are implied by the research:

- How does the framework perform against novel or evolving attack strategies not represented in the training datasets?
- What are the computational costs and scalability challenges for large-scale models?
- How does the approach generalize to different types of safety violations beyond those in the experimental datasets?

## Limitations
- The framework relies on evaluators that may not perfectly distinguish between genuinely safe responses and those that merely appear safe while containing harmful content.
- Performance may degrade when facing attack prompts outside the training distribution or zero-day jailbreak attempts.
- The approach is constrained by the base model's inherent limitations and cannot overcome fundamental safety blind spots.

## Confidence
- High Confidence: Technical implementation and experimental methodology are well-specified; reported PoR improvements appear reproducible.
- Medium Confidence: Claim that disguised responses are more effective than refusal responses at preventing attackers from strengthening capabilities; limited empirical evidence beyond performance metrics.
- Low Confidence: Assertion that this approach is "more effective" than other defense methods; comparison limited to four baseline methods without testing against state-of-the-art alternatives.

## Next Checks
1. **Adversarial Robustness Test:** Deploy the trained disguiser against a separate test set of manually crafted jailbreak prompts designed specifically to bypass disguised defenses. This would validate whether the Nash equilibrium reached during training generalizes to unseen attack strategies and whether the disguise mechanism provides genuine security or merely cosmetic changes.

2. **Human Evaluation of Disguise Quality:** Conduct blinded human evaluations where participants attempt to distinguish between genuine helpful responses and disguised defensive responses. This would directly measure whether the disguise is effective at concealing defensive intent from human attackers, which is more relevant than automated evaluator scores.

3. **Cross-Dataset Generalization Analysis:** Test the framework's performance when the attacker and disguiser are trained on one dataset but evaluated on a completely different safety domain not represented in either Generated_Attack or XSAFETY. This would reveal whether the curriculum learning approach develops transferable disguise capabilities or simply memorizes patterns from the training data.