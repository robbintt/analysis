---
ver: rpa2
title: A Survey of Large Language Models for Graphs
arxiv_id: '2405.08011'
source_url: https://arxiv.org/abs/2405.08011
tags:
- graph
- llms
- arxiv
- language
- gnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically reviews large language models (LLMs)
  for graph learning, proposing a novel taxonomy that categorizes existing methods
  into four unique framework designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii)
  LLMs-Graphs Integration, and iv) LLMs-Only. Each category is analyzed for its strengths
  and limitations.'
---

# A Survey of Large Language Models for Graphs

## Quick Facts
- arXiv ID: 2405.08011
- Source URL: https://arxiv.org/abs/2405.08011
- Reference count: 40
- This survey systematically reviews large language models (LLMs) for graph learning, proposing a novel taxonomy that categorizes existing methods into four unique framework designs.

## Executive Summary
This survey systematically reviews large language models (LLMs) for graph learning, proposing a novel taxonomy that categorizes existing methods into four unique framework designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only. Each category is analyzed for its strengths and limitations. The survey highlights the integration of LLMs with graph neural networks (GNNs) to address challenges like data sparsity and limited generalization capabilities in graph learning. Representative works, such as GraphGPT and GraphLLM, demonstrate the effectiveness of these approaches in tasks like node classification and link prediction. The survey also explores future directions, including multi-modal graphs, efficiency improvements, and user-centric agents. This work serves as a valuable resource for researchers and practitioners aiming to leverage LLMs in graph learning.

## Method Summary
The survey employs a systematic literature review approach to identify and categorize LLM-based graph learning methods. The authors conducted extensive searches across academic databases and preprint servers to collect representative works, focusing on publications from 2020-2024. Each identified method was analyzed based on its architectural design, integration approach with graph structures, and application domains. The classification framework was developed through iterative refinement, with methods grouped based on their fundamental approach to combining LLMs with graph structures. The analysis emphasizes practical implementation aspects and comparative performance where available.

## Key Results
- The survey identifies four distinct framework designs for LLM-based graph learning: GNNs as Prefix, LLMs as Prefix, LLMs-Graphs Integration, and LLMs-Only
- Representative methods like GraphGPT and GraphLLM demonstrate improved performance in node classification and link prediction tasks compared to traditional approaches
- The integration of LLMs with GNNs effectively addresses data sparsity and generalization challenges in graph learning
- Future directions identified include multi-modal graph processing, efficiency improvements, and development of user-centric graph agents

## Why This Works (Mechanism)
The effectiveness of LLM-based graph learning approaches stems from LLMs' ability to capture complex patterns and relationships through their attention mechanisms and large-scale pretraining. When integrated with graph structures, LLMs can leverage their contextual understanding to better represent nodes and edges, particularly in scenarios with limited labeled data. The proposed taxonomy reveals how different integration strategies (GNNs as prefix, LLMs as prefix, integrated approaches, or LLM-only methods) address specific graph learning challenges through complementary strengths of both paradigms.

## Foundational Learning

**Graph Neural Networks (GNNs)**
- Why needed: Traditional GNNs excel at capturing local graph structures but struggle with long-range dependencies and global context
- Quick check: Verify message passing mechanisms and aggregation functions in representative GNN architectures

**Large Language Models (LLMs)**
- Why needed: LLMs provide strong contextual understanding and can model complex relationships, but lack inherent graph structure awareness
- Quick check: Confirm transformer architecture details and pretraining objectives of surveyed LLMs

**Graph-Text Integration**
- Why needed: Bridging the gap between graph-structured data and sequential/textual representations requires specialized techniques
- Quick check: Examine how different methods handle node/edge features and structural information in textual format

## Architecture Onboarding

**Component Map:**
Input Graph/Graph Embeddings -> GNN Module -> LLM Interface -> Task-specific Output

**Critical Path:**
Graph preprocessing → Structural feature extraction → LLM integration → Task prediction → Post-processing

**Design Tradeoffs:**
The survey reveals tradeoffs between computational efficiency and model capacity, with GNN-LLM hybrid approaches offering better performance but higher computational costs compared to standalone methods. The choice between using GNNs as prefix vs. LLMs as prefix depends on whether structural preservation or contextual understanding is prioritized.

**Failure Signatures:**
Methods may fail when graph structures are too complex for LLM processing, when data sparsity severely limits pretraining effectiveness, or when the integration mechanism introduces excessive noise or information loss.

**First 3 Experiments:**
1. Node classification on Cora/Citeseer datasets comparing GNN-LLM hybrid vs. standalone approaches
2. Link prediction on protein-protein interaction networks with varying levels of sparsity
3. Graph-level classification on molecular datasets to evaluate multi-hop reasoning capabilities

## Open Questions the Paper Calls Out
The survey identifies several open questions for future research, including how to effectively handle multi-modal graph data (combining text, images, and structured data), improving computational efficiency of LLM-based graph methods, developing user-centric graph agents that can interact with humans, and establishing standardized evaluation benchmarks for this emerging field.

## Limitations
- The survey's classification framework may not capture all emerging approaches as the field rapidly evolves
- Performance claims are based on reported results in original papers without independent verification
- Coverage may be incomplete given the fast-paced development of both LLM and graph learning research
- The survey focuses primarily on academic applications, potentially overlooking industrial implementations

## Confidence
- High confidence in the general taxonomy structure and identification of key challenges
- Medium confidence in the completeness of surveyed methods and their comparative analysis
- Low confidence in specific performance claims without independent validation

## Next Checks
1. Conduct a systematic literature review to identify any missing representative works that don't fit cleanly into the proposed taxonomy
2. Perform independent benchmarking of key methods across standardized graph learning tasks to verify reported performance claims
3. Investigate the practical implementation challenges of the proposed future directions, particularly for multi-modal graphs and user-centric agents