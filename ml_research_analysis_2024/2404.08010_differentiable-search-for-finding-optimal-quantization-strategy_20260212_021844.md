---
ver: rpa2
title: Differentiable Search for Finding Optimal Quantization Strategy
arxiv_id: '2404.08010'
source_url: https://arxiv.org/abs/2404.08010
tags:
- quantization
- dqss
- network
- search
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a differentiable quantization strategy search
  (DQSS) method to find the optimal quantization strategy for individual layers by
  taking advantages of the benefits of different quantization algorithms. The proposed
  method formulates the problem of searching mixed quantization strategies as a differentiable
  neural architecture search problem and adopts an efficient convolution to efficiently
  explore the mixed quantization strategies from a global perspective by gradient-based
  optimization.
---

# Differentiable Search for Finding Optimal Quantization Strategy

## Quick Facts
- arXiv ID: 2404.08010
- Source URL: https://arxiv.org/abs/2404.08010
- Authors: Lianqiang Li; Chenqian Yan; Yefei Chen
- Reference count: 40
- Key outcome: DQSS achieves 71.577% accuracy on ImageNet across five architectures, close to FP32 models

## Executive Summary
This paper introduces Differentiable Quantization Strategy Search (DQSS), a method that automatically finds optimal quantization strategies for individual layers by leveraging differentiable neural architecture search principles. The approach formulates mixed quantization strategy selection as a continuous optimization problem, enabling efficient exploration of different quantization algorithms (Max_Abs, KL divergence, EQ, ADMM) for activations and weights. DQSS demonstrates significant improvements over state-of-the-art quantization methods on both image classification and image super-resolution tasks.

## Method Summary
DQSS formulates quantization strategy search as a differentiable optimization problem inspired by DARTS. It relaxes discrete strategy selection into continuous optimization using softmax over learnable importance parameters, then efficiently explores mixed strategies using a single convolution operation instead of N² convolutions. The method employs a single-pass optimization approach where hyper-parameters and network parameters are updated simultaneously, sharing weight tensors across all branches to prevent gradient starvation. This framework supports both post-training quantization (PTQ) and quantization-aware training (QAT) modes.

## Key Results
- Achieves 71.577% average accuracy on ImageNet across five network architectures, very close to FP32 models
- Outperforms state-of-the-art quantization methods significantly in both classification and super-resolution tasks
- For image super-resolution, achieves 27.20 PSNR/0.79094 SSIM average across five datasets, closest to FP32 models
- Demonstrates effectiveness across multiple architectures including ResNet, MobileNet, and EfficientNet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differentiable relaxation transforms discrete quantization strategy selection into continuous optimization
- Mechanism: Softmax over learnable importance parameters (α and β) enables gradient-based optimization instead of combinatorial search
- Core assumption: Continuous relaxation preserves essential characteristics of discrete selection problem
- Evidence anchors: [abstract] "motivated by DARTS [21], DQSS relaxes the discrete search space into a continuous one"; [section 3.2] "we can rewrite the Eq. (5) as follows: y = Σθαi Σθβj (Wj ∗ Ai)"

### Mechanism 2
- Claim: Efficient convolution reduces computational complexity from O(N²) to O(1) while maintaining search effectiveness
- Mechanism: Sums weighted activations and weights separately, then performs single convolution on combined tensors instead of N² convolutions
- Core assumption: Homogeneous candidate operators allow pre-summing without losing search capability
- Evidence anchors: [section 3.3] "we only need keep the representative activation branch and the representative weight branch"; [section 3.3] "Compared with Eq. (7), Eq. (10) improves the memory cost from O(N) to O(1)"

### Mechanism 3
- Claim: Single-pass optimization prevents underfitting by ensuring all branches receive adequate gradient updates
- Mechanism: Updates hyper-parameters and network parameters in single forward-backward pass with shared weight tensors across branches
- Core assumption: Equal treatment of hyper-parameters and network parameters maintains optimization stability
- Evidence anchors: [section 3.4] "we update the hyper-parameters and the network parameters in a single forward-backward pass"; [section 3.4] "we let all branches share all the weights by replacing Wi with W"

## Foundational Learning

- Concept: Neural Architecture Search (DARTS) and differentiable search spaces
  - Why needed here: Method builds directly on DARTS principles for differentiable quantization strategy selection
  - Quick check question: How does DARTS transform discrete architecture choices into continuous optimization problems?

- Concept: Quantization algorithms (Max_Abs, KL divergence, EQ, ADMM) and their tradeoffs
  - Why needed here: Understanding these candidate methods is essential to grasp why mixed strategies outperform uniform ones
  - Quick check question: What are the key differences between Max_Abs and KL divergence methods for quantization threshold selection?

- Concept: Softmax function and its role in continuous relaxation of discrete choices
  - Why needed here: Softmax is the mathematical mechanism enabling differentiable search over discrete quantization strategies
  - Quick check question: How does softmax transform arbitrary real-valued scores into probability distributions?

## Architecture Onboarding

- Component map: Search space definition (candidate quantization methods) -> Differentiable relaxation layer (softmax over importance parameters) -> Efficient convolution module (pre-summing weighted branches)
- Critical path: Calibration dataset → importance parameter initialization → differentiable search optimization → strategy assignment → quantization deployment
- Design tradeoffs: Trades precision in discrete strategy selection for computational efficiency and scalability; PTQ vs QAT modes affect optimization complexity
- Failure signatures: Poor performance indicates inadequate search space coverage, insufficient optimization iterations, or inappropriate calibration dataset size
- First 3 experiments:
  1. Implement DQSS with single quantization method to verify baseline functionality
  2. Test with two quantization methods to validate strategy selection capability
  3. Evaluate on simple network (ResNet-18) with all four candidate methods to verify full functionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does DQSS scale to ultra-low bitwidths beyond 4-bit quantization without significant performance degradation?
- Basis in paper: [inferred] Paper only demonstrates 4-bit QAT experiments; no results for 2-bit or 1-bit quantization presented
- Why unresolved: Testing ultra-low bitwidths requires more aggressive quantization strategies; current candidate pool may not be optimal
- What evidence would resolve it: Experiments comparing DQSS performance on 2-bit and 1-bit quantization for various architectures

### Open Question 2
- Question: How does DQSS perform on non-vision domains like NLP or speech processing where quantization behavior differs?
- Basis in paper: [inferred] Paper focuses exclusively on image classification and super-resolution tasks
- Why unresolved: Different modalities have different activation distributions and quantization sensitivities
- What evidence would resolve it: Applying DQSS to transformer-based language models or speech recognition networks

### Open Question 3
- Question: What is the impact of DQSS on hardware-specific metrics like latency, energy efficiency, and memory bandwidth utilization?
- Basis in paper: [inferred] Paper focuses on accuracy metrics but does not report hardware performance characteristics
- Why unresolved: Mixed-precision strategies could lead to irregular memory access patterns or inefficient hardware use
- What evidence would resolve it: Profiling DQSS-quantized models on edge devices and comparing to uniform quantization

## Limitations

- The differentiable relaxation approach's theoretical guarantees for preserving optimal quantization strategy selection are not rigorously established
- Efficient convolution optimization lacks direct empirical validation showing its individual impact on search quality
- Single-pass optimization claims to prevent underfitting but doesn't adequately address potential gradient interference issues
- Experimental validation, while comprehensive, could include more thorough comparison of computational efficiency metrics

## Confidence

- **High confidence**: Core differentiable search framework based on DARTS principles is well-established; experimental results demonstrate clear performance improvements
- **Medium confidence**: Specific implementation details of efficient convolution and single-pass optimization are plausible but lack extensive individual validation
- **Low confidence**: Theoretical guarantees of differentiable relaxation approach are not rigorously established

## Next Checks

1. Conduct ablation studies isolating contributions of efficient convolution and single-pass optimization to verify individual impact
2. Perform theoretical analysis of differentiable relaxation's approximation error and relationship to quantization strategy selection accuracy
3. Compare DQSS's computational complexity and search time against traditional quantization strategy search methods across various architectures and tasks