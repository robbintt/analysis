---
ver: rpa2
title: Accelerating Multilingual Language Model for Excessively Tokenized Languages
arxiv_id: '2401.10660'
source_url: https://arxiv.org/abs/2401.10660
tags:
- language
- multilingual
- mumo
- pre-trained
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency in text generation for non-alphabetic
  languages due to excessive tokenization in multilingual language models. The proposed
  MuMo framework introduces a new language model head with a vocabulary set tailored
  to the target language and fine-tunes it while freezing other model parameters.
---

# Accelerating Multilingual Language Model for Excessively Tokenized Languages

## Quick Facts
- arXiv ID: 2401.10660
- Source URL: https://arxiv.org/abs/2401.10660
- Reference count: 26
- Primary result: MuMo framework achieves 1.7x faster text generation for Korean and Japanese while maintaining performance

## Executive Summary
This paper addresses the inefficiency in text generation for non-alphabetic languages caused by excessive tokenization in multilingual language models. The authors introduce the MuMo framework, which creates a language-specific model head with tailored vocabulary to reduce token fragmentation. By freezing the original model parameters and only fine-tuning the new head, MuMo significantly accelerates text generation while preserving model performance on monolingual tasks. The approach shows particular promise for languages like Korean and Japanese that suffer from high token counts due to their morphological complexity.

## Method Summary
The MuMo framework introduces a new language model head with a vocabulary specifically designed for the target language. The approach works by first analyzing the tokenization patterns of the target language, then constructing a vocabulary set that reduces fragmentation. The new head is then fine-tuned on monolingual data while all other model parameters remain frozen. This selective fine-tuning allows the model to maintain its general multilingual capabilities while optimizing specifically for the target language's tokenization characteristics. The method leverages the fact that excessive tokenization primarily affects generation speed rather than comprehension or task performance.

## Key Results
- Achieves 1.7x speedup in text generation for Korean and Japanese
- Maintains performance on monolingual tasks while accelerating generation
- Reduces token fragmentation through language-specific vocabulary design

## Why This Works (Mechanism)
The effectiveness of MuMo stems from addressing a fundamental inefficiency in how multilingual models handle morphologically rich, non-alphabetic languages. These languages typically require many more tokens to represent the same semantic content compared to alphabetic languages, creating computational bottlenecks during generation. By creating a language-specific head with optimized tokenization, MuMo reduces the average token count per word or phrase, directly translating to faster generation without sacrificing model understanding. The frozen parameters ensure that the model retains its broader multilingual knowledge while the specialized head handles the generation-specific optimizations.

## Foundational Learning
- **Tokenization Efficiency**: Understanding how different tokenization strategies affect computational load is crucial for optimizing multilingual models. Quick check: Compare token counts for equivalent text across different languages in a multilingual tokenizer.
- **Morphological Complexity**: Non-alphabetic languages often have higher morphological complexity, requiring more tokens to represent grammatical features. Quick check: Analyze the average number of tokens needed to represent single words in Korean vs English.
- **Selective Fine-tuning**: The technique of freezing most model parameters while fine-tuning specific components preserves general capabilities while enabling specialization. Quick check: Verify that frozen parameters remain stable during MuMo fine-tuning by comparing embeddings before and after.
- **Vocabulary Tailoring**: Creating language-specific vocabularies requires understanding the unique characteristics of each language's tokenization needs. Quick check: Evaluate the overlap between the original tokenizer's vocabulary and the tailored vocabulary for the target language.
- **Generation Bottlenecks**: Text generation speed is often limited by the number of tokens that must be processed sequentially. Quick check: Measure generation time as a function of token count for the same semantic content.
- **Cross-linguistic Transfer**: Understanding how knowledge transfers between languages in multilingual models helps explain why freezing parameters works. Quick check: Test model performance on zero-shot cross-lingual tasks after MuMo adaptation.

## Architecture Onboarding

**Component Map:** Original Multilingual Model -> Frozen Parameters -> New Language-Specific Head -> Tailored Vocabulary

**Critical Path:** Input Text → Tokenizer → Language-Specific Head → Vocabulary Mapping → Generation

**Design Tradeoffs:** The approach trades some multilingual flexibility for generation speed in target languages. By freezing parameters, it sacrifices the ability to jointly optimize all parameters but gains efficiency and maintains broader capabilities.

**Failure Signatures:** The framework may fail when the tailored vocabulary cannot adequately represent rare or out-of-vocabulary terms, or when the frozen parameters limit adaptation to domain-specific language patterns. Generation quality may degrade if the language-specific head becomes too specialized.

**First Experiments:**
1. Measure token count reduction for typical Korean and Japanese sentences using the tailored vocabulary versus the original tokenizer.
2. Compare generation speed across different sequence lengths to verify the 1.7x improvement holds across various contexts.
3. Test zero-shot performance on other languages to ensure the frozen parameters preserve multilingual capabilities.

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness may not generalize beyond Korean and Japanese to other non-alphabetic languages
- Vocabulary tailoring requires language-specific expertise and may not scale efficiently
- The framework focuses on generation speed without comprehensive quality assessments across multiple metrics

## Confidence

**High Confidence:** The core observation about excessive tokenization in non-alphabetic languages is well-established; the 1.7x speedup measurement is specific and measurable.

**Medium Confidence:** Claims about "maintained performance" lack detailed quality metric comparisons beyond speed measurements.

**Low Confidence:** Scalability assessment across diverse non-alphabetic languages is limited by the experimental scope focusing only on Korean and Japanese.

## Next Checks
1. Test the MuMo framework on additional non-alphabetic languages with different tokenization characteristics (e.g., Chinese, Thai, Vietnamese) to assess cross-linguistic generalizability.
2. Conduct comprehensive quality assessments comparing generated text from MuMo against baseline models using multiple evaluation metrics (BLEU, ROUGE, human evaluation) to verify that "maintained performance" holds across different quality dimensions.
3. Evaluate the computational overhead and memory requirements of the MuMo framework during both training and inference phases to determine practical deployment constraints in resource-limited settings.