---
ver: rpa2
title: 'ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal
  Transport'
arxiv_id: '2403.03777'
source_url: https://arxiv.org/abs/2403.03777
tags:
- optimal
- enot
- transport
- expectile
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ENOT, a novel method for efficient computation
  of optimal transport plans using neural networks. The key innovation is the use
  of expectile regularization to approximate the c-conjugate operator, a computationally
  intensive step in existing approaches.
---

# ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport

## Quick Facts
- arXiv ID: 2403.03777
- Source URL: https://arxiv.org/abs/2403.03777
- Authors: Nazar Buzun; Maksim Bobrin; Dmitry V. Dylov
- Reference count: 30
- Primary result: Achieves up to 3× improvement in quality and 10× improvement in runtime for neural optimal transport computation

## Executive Summary
This paper introduces ENOT, a novel method for efficient computation of optimal transport plans using neural networks. The key innovation is the use of expectile regularization to approximate the c-conjugate operator, a computationally intensive step in existing approaches. By constraining the solution space of conjugate potentials with expectile regression, ENOT achieves stable learning and eliminates the need for extensive fine-tuning. The method is evaluated on the Wasserstein-2 benchmark tasks, demonstrating significant improvements in both quality and runtime compared to state-of-the-art approaches.

## Method Summary
ENOT addresses the computational bottleneck in neural optimal transport by replacing the inner optimization loop for conjugate potential computation with expectile regularization. Instead of solving an expensive c-conjugate transform (gc(x) = inf_y [c(x,y) - g(y)]), ENOT uses asymmetric squared loss to bound the distribution of possible conjugate potentials. As the expectile parameter τ approaches 1, this regularization approximates the maximum operator, providing a valid upper bound on gc without explicit computation. The method maintains stability through alternating optimization between potential models and transport maps, with runtime improvements of up to 10× compared to existing approaches.

## Key Results
- Achieves up to 3× improvement in LUV² quality metric on Wasserstein-2 benchmark tasks
- Provides up to 10× runtime speedup compared to state-of-the-art W2OT method
- Demonstrates robustness across different cost functions and high-dimensional tasks
- Eliminates need for conjugate operator fine-tuning while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expectile regularization replaces computationally expensive conjugate operator fine-tuning with a direct upper bound approximation
- Mechanism: Instead of solving an inner optimization to approximate the c-conjugate transform (gc), ENOT uses the asymmetric squared loss of expectile regression to bound the distribution of possible conjugate potentials. As τ approaches 1, the expectile approximates the maximum operator, giving a tight upper bound on gc without explicit computation
- Core assumption: The expectile model fθ(x) converges to the conditional maximum of (gT(x) - c(x,y)) over y as τ → 1, providing a valid surrogate for gc
- Evidence anchors:
  - [abstract] "proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning of dual potentials"
  - [section] "we compute τ-expectile of the distribution governed by functionals of type gT(x) - c(x,y)... when τ → 1 the expectile converges to max over x∈X [gT(x) - c(x,y)] = -(gT)c(y)"
  - [corpus] Weak - no direct citations on expectile convergence properties in OT context
- Break condition: If τ is set too close to 1, numerical instability occurs; if τ is too small, the regularization becomes too loose and fails to approximate gc

### Mechanism 2
- Claim: ENOT achieves stable learning by constraining the solution space of conjugate potentials with expectile regression
- Mechanism: The regularization term Rg(η, x, y) = Lτ[c(x,Tθ(x)) - gη(Tθ(x)) - c(x,y) + gη(y)] encourages gη to align with (gT)c without computing the infimum in the c-transform. This acts as an implicit constraint that prevents the dual potentials from diverging during alternating optimization
- Core assumption: The asymmetric loss Lτ effectively measures proximity between gη and (gT)c without requiring the exact infimum computation
- Evidence anchors:
  - [abstract] "Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable"
  - [section] "we compute τ-expectile of the distribution governed by functionals of type gT(x) - c(x,y)... From the properties of expectile regression... follows that when τ → 1 the expectile converges to max over x∈X [gT(x) - c(x,y)] = -(gT)c(y)"
  - [corpus] Weak - no direct citations on stability improvements from expectile regularization
- Break condition: If the expectile loss weight λ is too small, the constraint becomes ineffective; if too large, it dominates the primary objective and degrades convergence

### Mechanism 3
- Claim: ENOT eliminates the need for conjugate operator fine-tuning, providing computational speedup without sacrificing accuracy
- Mechanism: By replacing the inner optimization loop with a single expectile regularization term, ENOT removes the computational overhead of L-BFGS or Adam fine-tuning steps while still achieving comparable or better LUV² metrics on benchmark tasks
- Core assumption: The expectile-based approximation is sufficiently accurate to replace explicit conjugate computation while maintaining convergence properties
- Evidence anchors:
  - [abstract] "completely eliminating the need for additional extensive fine-tuning" and "up to a 10-fold improvement in runtime"
  - [section] "we strongly believe that solution found by fine-tuning can introduce bias by overfitting on current potential optimization step"
  - [section] "Table 4: Runtime comparison for different layers sizes between W2OT Amos (2022a) with default hyperparameters and ENOT... ENOT ~1.3 min vs W2OT ~60-300 min"
  - [corpus] Moderate - runtime comparison shows order-of-magnitude improvements
- Break condition: If the parametric model family cannot adequately represent the conjugate potential, the expectile approximation may be insufficient

## Foundational Learning

- Concept: Optimal Transport (Monge and Kantorovich formulations)
  - Why needed here: ENOT builds on the dual formulation of OT using Kantorovich potentials; understanding MP vs KP vs DP is essential for grasping why conjugate operator computation is a bottleneck
  - Quick check question: What is the relationship between the Monge map and Kantorovich potentials under the conditions MP=KP?

- Concept: c-conjugate transform and its computational complexity
  - Why needed here: The c-transform gc(x) = inf_y [c(x,y) - g(y)] is the bottleneck that ENOT addresses; understanding why this is computationally intensive is key to appreciating the innovation
  - Quick check question: Why is computing the exact c-conjugate operator computationally expensive in high-dimensional settings?

- Concept: Expectile regression and asymmetric loss functions
  - Why needed here: ENOT's core innovation uses expectile regression with asymmetric squared loss; understanding how τ controls the asymmetry and convergence to maximum is crucial
  - Quick check question: How does the expectile parameter τ affect the balance between fitting the mean versus the tail of the distribution in regression?

## Architecture Onboarding

- Component map: fθ(x) -> gη(y) -> Tθ(x) -> Expectile regularization Rg -> Dual objective losses Lg and Lf
- Critical path: 1) Sample batches from α and β, 2) Compute Tθ(x) using current potential estimates, 3) Calculate expectile regularization Rg, 4) Update gη using Lg + λRg, 5) Update θ using Lf, 6) Periodically update inverse mapping if bidirectional
- Design tradeoffs: Expectile regularization trades off exact conjugate computation for computational efficiency and stability. Higher τ provides tighter bounds but increases numerical instability risk. The λ parameter balances regularization strength against primary objective fidelity
- Failure signatures: Training divergence when τ is too close to 1, poor LUV² metrics when λ is too small, and convergence plateaus when τ is too small. Monitor expectile loss magnitude relative to primary losses
- First 3 experiments:
  1. Test ENOT on a simple 2D synthetic dataset (e.g., Gaussian to Gaussian with known OT map) with τ=0.9, λ=0.3, compare LUV² and runtime against W2OT baseline
  2. Vary τ parameter (0.5, 0.9, 0.99) on the same dataset to observe stability and accuracy tradeoff
  3. Test bidirectional training mode on a squared Euclidean cost problem to verify inverse map quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ENOT's expectile regularization perform with different cost functions beyond squared Euclidean distance, such as geodesic distances on manifolds or non-Euclidean geometries?
- Basis in paper: [explicit] The paper states "we believe that ENOT will become a new baseline to compare against for the future NOT solvers and will accelerate research in the applications of optimal transport in high-dimensional tasks" and mentions testing "other types of cost functions, such as Lagrangian costs, defined on non-Euclidean spaces."
- Why unresolved: The experiments primarily focus on squared Euclidean cost and limited testing on geodesic distances on spheres, with only one paragraph discussing broader potential applications
- What evidence would resolve it: Comprehensive experiments testing ENOT across diverse cost functions (e.g., geodesic, Lagrangian, Wasserstein-1) on various datasets, including theoretical analysis of convergence guarantees for different cost structures

### Open Question 2
- Question: What is the theoretical justification for the choice of expectile parameter τ, and how sensitive is ENOT's performance to this hyperparameter?
- Basis in paper: [explicit] The paper states "In order to test other specific use cases, we conducted experiments on 2-spheres dataset" and includes ablation studies varying τ and λ, but doesn't provide theoretical guidance on selecting τ
- Why unresolved: The paper provides empirical results showing sensitivity to τ and λ values but lacks theoretical analysis explaining why certain values work better or how to theoretically determine optimal τ for different problems
- What evidence would resolve it: A mathematical framework connecting expectile parameter τ to problem characteristics (e.g., measure distributions, cost function properties), along with theoretical convergence analysis showing how τ affects solution quality

### Open Question 3
- Question: How does ENOT scale to extremely high-dimensional problems (e.g., >1000 dimensions) compared to exact methods, and what are the computational bottlenecks?
- Basis in paper: [inferred] The paper mentions "high-dimensional tasks" and benchmark results up to 256 dimensions, but doesn't explore scalability limits or computational bottlenecks in very high dimensions
- Why unresolved: The experiments are limited to dimensions up to 256, and while runtime improvements are shown, there's no analysis of how performance degrades or what computational challenges arise at extreme scales
- What evidence would resolve it: Systematic scaling experiments from low to very high dimensions (1000+), including runtime analysis, memory usage profiling, and identification of specific computational bottlenecks that emerge at scale

## Limitations

- Theoretical justification for expectile regularization as conjugate substitute remains underdeveloped, relying primarily on empirical evidence
- Performance sensitivity to hyperparameters τ and λ suggests limited generalizability without careful tuning
- Limited validation beyond Wasserstein-2 benchmarks, particularly for non-Euclidean costs and extreme dimensional scaling

## Confidence

- High confidence in runtime improvements (10× speedup) - directly measured and reported
- Medium confidence in LUV² quality improvements (3× better) - depends on benchmark task construction and evaluation methodology
- Low confidence in theoretical justification for expectile regularization as exact conjugate substitute - primarily empirical evidence provided

## Next Checks

1. **Theoretical verification**: Prove or disprove convergence of τ-expectiles to c-conjugate operators as τ → 1 for general cost functions
2. **Parameter sensitivity analysis**: Systematically test ENOT performance across a grid of τ and λ values on multiple synthetic OT problems to characterize stability boundaries
3. **Generalization test**: Evaluate ENOT on tasks with non-Euclidean costs (e.g., geodesic on spheres, KL divergence) to assess method's broader applicability beyond Wasserstein-2 benchmarks