---
ver: rpa2
title: 'Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small Language
  Model'
arxiv_id: '2411.05903'
source_url: https://arxiv.org/abs/2411.05903
tags:
- language
- audio
- multi-modal
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents EAGLE, a 4.5B parameter multi-modal small language
  model capable of processing text, images, videos, and audio inputs while generating
  text and audio outputs. The model achieves near state-of-the-art performance across
  multiple benchmarks including MMMU (46.3%), ScienceQA (94.6%), and MMLU (72.9%),
  while being small enough to run on edge devices like the iPhone 15 Pro with a generation
  speed of 16 tokens per second.
---

# Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small Language Model

## Quick Facts
- arXiv ID: 2411.05903
- Source URL: https://arxiv.org/abs/2411.05903
- Authors: Ben Koska; Mojmír Horváth
- Reference count: 0
- Primary result: EAGLE achieves 46.3% on MMMU, 94.6% on ScienceQA, and 72.9% on MMLU while being small enough for edge deployment

## Executive Summary
This paper introduces EAGLE, a 4.5B parameter multi-modal small language model that processes text, images, videos, and audio inputs while generating text and audio outputs. The model achieves near state-of-the-art performance across multiple benchmarks while maintaining a compact size suitable for edge deployment. Key innovations include a multi-tower architecture combining language, audio, and vision modalities with quantization-aware fine-tuning, enabling efficient deployment through mixed-precision quantization that reduces the model from 18GB to 3GB while preserving most performance.

## Method Summary
EAGLE employs a multi-modal architecture with separate modality towers for language, audio, and vision, combined with a unified transformer backbone. The model uses quantization-aware fine-tuning to enable efficient deployment through mixed-precision quantization, reducing memory requirements while maintaining performance. The training process involves multi-stage fine-tuning on diverse datasets spanning text, images, videos, and audio, with special attention to maintaining performance after quantization. The model achieves this balance by carefully selecting quantization parameters and training strategies that preserve multi-modal reasoning capabilities.

## Key Results
- Achieves 46.3% on MMMU, 94.6% on ScienceQA, and 72.9% on MMLU benchmarks
- Reduces model size from 18GB to 3GB through mixed-precision quantization
- Runs on iPhone 15 Pro at 16 tokens per second generation speed
- Maintains 80-85% of performance compared to larger models like GEMM-LM and GPT-4V

## Why This Works (Mechanism)
The model's effectiveness stems from its multi-modal tower architecture that processes different input types in parallel before fusion, combined with quantization-aware fine-tuning that enables efficient deployment. The separation of modality-specific processing allows each tower to specialize in its domain while the unified transformer backbone handles cross-modal reasoning. The quantization strategy preserves critical model parameters while reducing precision where acceptable, maintaining most performance benefits while enabling edge deployment.

## Foundational Learning
- **Multi-modal fusion**: Why needed - enables integration of different input types; Quick check - verify each modality contributes to final output
- **Quantization-aware training**: Why needed - preserves model accuracy during compression; Quick check - measure performance drop after quantization
- **Edge deployment constraints**: Why needed - ensures practical applicability; Quick check - verify model runs within hardware limitations
- **Mixed-precision computation**: Why needed - balances accuracy and efficiency; Quick check - compare FP16 vs INT8 performance

## Architecture Onboarding

Component Map:
Language tower -> Fusion layer -> Unified transformer -> Output layer
Audio tower -> Fusion layer -> Unified transformer -> Output layer
Vision tower -> Fusion layer -> Unified transformer -> Output layer

Critical Path:
Input modalities → Respective modality towers → Feature fusion → Unified transformer processing → Output generation

Design Tradeoffs:
The model trades some absolute performance for significant size reduction and deployment flexibility. While achieving 80-85% of larger model performance, it gains the ability to run on edge devices. The quantization approach introduces some accuracy loss but enables practical deployment scenarios that would be impossible with full-precision models.

Failure Signatures:
Performance degradation on tasks requiring fine-grained audio-visual synchronization, particularly noticeable in the MMM benchmark where quantization reduced accuracy from 90.8% to 87.3%. Multi-modal reasoning tasks may show disproportionate impact from quantization errors.

First Experiments:
1. Run inference on iPhone 15 Pro with different quantization levels to verify deployment claims
2. Compare MMM benchmark performance with and without quantization to quantify accuracy loss
3. Test cross-modal generation tasks to verify audio output quality across different input types

## Open Questions the Paper Calls Out
None

## Limitations
- Quantization introduces measurable performance degradation (MMM benchmark drop from 90.8% to 87.3%)
- Limited analysis of cost-benefit trade-offs for specific use cases compared to larger models
- Edge deployment claims lack validation across diverse hardware configurations
- Mixed-precision quantization may not generalize well to all deployment scenarios

## Confidence
- Model architecture and quantization approach: High
- Benchmark performance metrics: Medium
- Deployment efficiency claims: Low to Medium

## Next Checks
1. Comprehensive evaluation of quantization impact on specific multi-modal tasks, particularly those requiring fine-grained audio-visual synchronization
2. Real-world deployment testing across multiple edge devices with varying computational constraints
3. Ablation studies to quantify the contribution of each modality tower to overall performance and identify potential bottlenecks in the fusion mechanism