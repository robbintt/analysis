---
ver: rpa2
title: A Systematic Analysis of Subwords and Cross-Lingual Transfer in Multilingual
  Translation
arxiv_id: '2403.20157'
source_url: https://arxiv.org/abs/2403.20157
tags:
- subword
- multilingual
- cross-lingual
- languages
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how different subword segmentation methods affect
  cross-lingual transfer in multilingual machine translation. The authors compare
  BPE, ULM, SSMT, OBPE, and XBPE across English-Siswati, English-isiXhosa, English-Setswana,
  and English-Afrikaans translation tasks.
---

# A Systematic Analysis of Subwords and Cross-Lingual Transfer in Multilingual Translation

## Quick Facts
- arXiv ID: 2403.20157
- Source URL: https://arxiv.org/abs/2403.20157
- Reference count: 12
- Comparing subword segmentation methods shows ULM boosts synergy while BPE enables better cross-lingual transfer, with orthographic conventions impacting transfer more than linguistic relatedness.

## Executive Summary
This paper systematically examines how different subword segmentation methods affect cross-lingual transfer in multilingual machine translation. The authors compare five methods—BPE, ULM, SSMT, OBPE, and XBPE—across English-to-Siswati, isiXhosa, Setswana, and Afrikaans translation tasks. They find that subword regularization (ULM) improves synergy in multilingual models by exposing the model to multiple segmentation variants, while deterministic segmentation (BPE) facilitates better cross-lingual transfer during finetuning. Surprisingly, orthographic word boundary conventions have a more significant impact on transfer performance than linguistic relatedness between languages.

## Method Summary
The study uses WMT22 parallel corpora for training and FLOWSES for evaluation, focusing on English-to-four South African languages. Two experimental setups are employed: (1) multilingual (trilingual) models trained on English-to-Siswati plus one other language, evaluated for synergy/interference analysis; and (2) cross-lingual finetuning where bilingual models pretrained on high-resource pairs are finetuned on new translation directions including English-to-Siswati. Five subword segmentation methods (BPE, ULM, SSMT, OBPE, XBPE) are compared using a shared vocabulary size of 8k, with synergy and interference measured as relative changes in chrF++ scores between bilingual and multilingual models.

## Key Results
- ULM subword regularization boosts synergy in multilingual modeling by exposing models to multiple segmentation variants during training
- BPE deterministic segmentation facilitates better cross-lingual transfer during finetuning due to consistent segmentation patterns
- Orthographic word boundary conventions (disjunctive vs conjunctive) impact transfer more significantly than linguistic relatedness—Setswana's disjunctive orthography hinders transfer to Siswati more than Afrikaans's unrelatedness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subword regularization (ULM) improves synergy in multilingual modeling by exposing the model to multiple segmentation variants during training.
- Mechanism: ULM samples from a probability distribution over possible segmentations, forcing the model to be robust to segmentation variability. This regularizes the model and improves generalization across languages.
- Core assumption: Exposure to multiple subword segmentations during training leads to better cross-lingual representations that can handle morphological variation.
- Evidence anchors:
  - [abstract] "Our findings show that subword regularisation boosts synergy in multilingual modelling"
  - [section] "The subword regularisation of ULM ensures that models are more robust to the varied subwords of multilingual modelling"
  - [corpus] Weak - corpus doesn't contain specific evidence about ULM's mechanism
- Break condition: If the probabilistic segmentation samples are too divergent across languages, it may hinder rather than help cross-lingual transfer.

### Mechanism 2
- Claim: Deterministic subword segmentation (BPE) facilitates better cross-lingual transfer during finetuning by providing consistent segmentation across languages.
- Mechanism: BPE creates a fixed vocabulary of subwords, ensuring that the same word is segmented consistently across different languages. This consistency helps the model adapt to new translation directions during finetuning.
- Core assumption: Consistent subword segmentation across languages is crucial for effective cross-lingual transfer during finetuning.
- Evidence anchors:
  - [abstract] "BPE more effectively facilitates transfer during cross-lingual fine-tuning"
  - [section] "The consistent deterministic segmentation of BPE allows the finetuned model to adapt to a new translation direction effectively"
  - [corpus] Weak - corpus doesn't contain specific evidence about BPE's mechanism
- Break condition: If the BPE vocabulary is too small or poorly optimized for the target languages, it may limit the model's ability to represent important morphological distinctions.

### Mechanism 3
- Claim: Orthographic word boundary conventions (disjunctive vs conjunctive) significantly impact cross-lingual transfer by affecting subword overlap between languages.
- Mechanism: Disjunctive orthographies (like Setswana) split linguistic words into multiple orthographic tokens, leading to different pre-tokenization before subword segmentation. This reduces subword overlap with languages that use conjunctive orthographies (like Siswati), hindering cross-lingual transfer.
- Core assumption: Pre-tokenization based on orthographic word boundaries affects the subsequent subword segmentation and overlap between languages.
- Evidence anchors:
  - [abstract] "differences in orthographic word boundary conventions... may impede cross-lingual transfer more significantly than linguistic unrelatedness"
  - [section] "Orthographic word boundaries determine the pre-tokenisation of text before subword segmenters are applied, so it could well affect aspects like segmentation granularity and overlap between the subword vocabularies of different languages"
  - [corpus] Weak - corpus doesn't contain specific evidence about orthographic effects
- Break condition: If the languages have similar morphological structures but different orthographic conventions, the morphological similarity may partially compensate for the orthographic differences.

## Foundational Learning

- Concept: Subword segmentation methods (BPE, ULM, SSMT, OBPE, XBPE)
  - Why needed here: The paper compares different subword segmentation methods and their effects on multilingual and cross-lingual MT performance.
  - Quick check question: What are the key differences between BPE and ULM in terms of how they generate subword vocabularies?

- Concept: Synergy and interference in multilingual modeling
  - Why needed here: The paper measures how multilingual models affect translation performance for individual languages (synergy if improved, interference if worsened).
  - Quick check question: How is synergy/interference quantified in the experiments?

- Concept: Cross-lingual transfer during finetuning
  - Why needed here: The paper examines how knowledge from high-resource languages can be transferred to low-resource languages during finetuning of pretrained models.
  - Quick check question: What is the difference between multilingual modeling and cross-lingual finetuning?

## Architecture Onboarding

- Component map: Data preprocessing (tokenization, subword segmentation) -> Bilingual/trilingual model training -> Cross-lingual finetuning -> Evaluation (chrF++)

- Critical path:
  1. Preprocess data (tokenization, subword segmentation)
  2. Train bilingual models for each language pair
  3. Train trilingual models for synergy/interference analysis
  4. Finetune bilingual models on new languages for cross-lingual transfer analysis
  5. Evaluate all models using chrF++

- Design tradeoffs:
  - Subword vocabulary size: Larger vocabularies can represent more morphological variants but increase model size and may lead to data sparsity
  - Subword regularization: ULM's probabilistic segmentation improves synergy but may hinder cross-lingual transfer during finetuning
  - Language sampling: Balancing exposure to high-resource and low-resource languages during training

- Failure signatures:
  - Low chrF++ scores on target languages
  - Negative synergy values (interference) for certain language pairs
  - Inconsistent segmentation patterns across languages

- First 3 experiments:
  1. Train bilingual models for en→ss, en→xh, en→ts, and en→af using BPE
  2. Train trilingual models for en→ss+xh, en→ss+af, and en→ss+ts using ULM
  3. Finetune the en→xh model on en→ss using the finetuned XBPE vocabulary

## Open Questions the Paper Calls Out

- Question: How do orthographic word boundary conventions affect subword segmentation granularity and cross-lingual transfer beyond the four South African languages studied?
- Basis in paper: [explicit] The authors note that orthographic word boundary conventions (disjunctive vs conjunctive) may impede cross-lingual transfer more than linguistic unrelatedness, and acknowledge their study is limited to four South African languages
- Why unresolved: The study only examined four specific languages with particular orthographic systems. The interaction between orthography and subword segmentation could vary significantly across different language families and writing systems.
- What evidence would resolve it: Systematic experiments across multiple language families with varying orthographic conventions (e.g., logographic, abjad, abugida, syllabary systems) would reveal whether the orthographic effects observed generalize or are specific to the studied languages.

## Limitations

- The impact of orthographic conventions on cross-lingual transfer remains speculative without ablation studies isolating the pre-tokenization step
- The choice of ULM hyperparameters (α=0.5) was not systematically explored, leaving open the possibility that different regularization strengths could yield different synergy/interference trade-offs
- The analysis of SSMT and XBPE methods is less comprehensive than for BPE and ULM, with fewer experimental conditions reported

## Confidence

- **High confidence**: The general finding that subword segmentation choice significantly affects multilingual MT performance, and that ULM improves synergy while BPE facilitates cross-lingual finetuning
- **Medium confidence**: The specific claim that orthographic word boundary conventions impact transfer more than linguistic relatedness, given the correlational nature of the evidence
- **Medium confidence**: The proposed mechanisms linking segmentation methods to synergy/interference, as the paper provides conceptual arguments but limited empirical validation of the underlying processes

## Next Checks

1. Conduct ablation experiments that isolate pre-tokenization effects by applying the same subword segmentation method to text with different orthographic conventions, to verify that orthographic differences are the causal factor in transfer differences
2. Systematically vary the ULM regularization strength (α parameter) across a wider range and measure its impact on the synergy-interference trade-off, to identify optimal settings for different multilingual scenarios
3. Implement the SSMT method with the same experimental rigor as BPE and ULM, including cross-lingual finetuning experiments, to provide a more complete comparison across all five subword segmentation methods