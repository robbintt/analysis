---
ver: rpa2
title: Estimation of multiple mean vectors in high dimension
arxiv_id: '2403.15038'
source_url: https://arxiv.org/abs/2403.15038
tags: []
core_contribution: This work addresses the problem of estimating multiple high-dimensional
  mean vectors based on independent samples from different probability distributions.
  The core idea is to form estimators as convex combinations of empirical means, using
  data-dependent weights found via either a testing procedure to identify neighboring
  means with low variance or by minimizing an upper confidence bound on the quadratic
  risk.
---

# Estimation of multiple mean vectors in high dimension

## Quick Facts
- arXiv ID: 2403.15038
- Source URL: https://arxiv.org/abs/2403.15038
- Authors: Gilles Blanchard; Jean-Baptiste Fermanian; Hannah Marienwald
- Reference count: 40
- One-line primary result: Data-dependent convex combination of empirical means asymptotically approaches oracle improvement as effective dimension increases.

## Executive Summary
This paper addresses the problem of estimating multiple high-dimensional mean vectors from independent samples, proposing two data-dependent methods that form convex combinations of empirical means. The methods leverage either a testing procedure to identify neighboring means or Q-aggregation to minimize an upper confidence bound on the quadratic risk. Theoretical analysis shows that both approaches asymptotically achieve oracle (minimax) improvement as the effective dimension of the data increases, with empirical validation demonstrating up to 50% improvement over naive estimators in kernel mean embedding estimation.

## Method Summary
The paper proposes two methods for estimating multiple high-dimensional mean vectors. The first is a testing approach that identifies neighboring tasks with low variance using U-statistics and forms estimators as convex combinations with weights based on these neighborhoods. The second is a Q-aggregation approach that directly minimizes an upper confidence bound on the quadratic risk over all possible convex combinations without requiring neighbor detection. Both methods are applied to kernel mean embedding estimation, where the goal is to estimate the mean embedding of probability distributions in reproducing kernel Hilbert spaces. The methods are validated on synthetic Gaussian data and real-world flow cytometry data.

## Key Results
- Both testing and Q-aggregation methods asymptotically approach oracle (minimax) improvement as effective dimension increases
- Up to 50% improvement over naive estimators in kernel mean embedding estimation on real-world flow cytometry data
- Theoretical guarantees hold under Gaussian assumptions with bounded sample sizes and effective dimension scaling
- Methods particularly effective when effective dimensionality is high and sample sizes are large enough

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Convex combination of empirical means improves estimation by exploiting task similarity without prior knowledge.
- Mechanism: The estimator forms as a convex combination of individual empirical means, with weights determined by either testing for task similarity (low-variance neighbors) or minimizing an upper confidence bound on the quadratic risk. This allows borrowing statistical strength from related tasks while controlling bias.
- Core assumption: Task similarity can be detected empirically through U-statistics and distances in effective dimensionality without explicit prior structure.
- Evidence anchors:
  - [abstract] "Our approach involves forming estimators through convex combinations of empirical means derived from these samples."
  - [section 3] "Our first approach explicitly controls the bias. We aim at identifying a subset of neighbour tasks whose means are sufficiently close to the target task."
  - [corpus] Weak evidence - corpus neighbors do not directly discuss multiple mean estimation or convex combinations.
- Break condition: If effective dimensionality is low or task means are uniformly dissimilar, the improvement over naive estimation vanishes.

### Mechanism 2
- Claim: Effective dimensionality governs asymptotic performance; higher effective dimension enables better relative risk improvement.
- Mechanism: As effective dimension grows, the relative risk of the proposed estimators approaches the oracle (minimax) improvement. This "blessing of dimensionality" occurs because testing and aggregation become more reliable relative to estimation error in high dimensions.
- Core assumption: Effective dimension (d* or de) captures the intrinsic structure of covariance matrices, and grows large relative to sample sizes and log(B).
- Evidence anchors:
  - [abstract] "showing that our methods asymptotically approach an oracle (minimax) improvement as the effective dimension of the data increases."
  - [section 2.2] "we will focus on high-dimensional asymptotics where the dimension grows large...the effective dimension of a task will be defined from spectral quantities related to its covariance matrix."
  - [section 3.5] "For fixed values of τ, ς, B, (Nk)k∈JBK, the bound on the relative risk of eω♯ becomes arbitrarily close to the oracle bound in the high-dimensional asymptotics d*1 → ∞."
- Break condition: If sample sizes do not scale with effective dimension (e.g., Nk ≲ log⁴ B), the asymptotic guarantees fail.

### Mechanism 3
- Claim: Q-aggregation adapts to unknown task structure without sample splitting, achieving oracle-consistency over any grouping.
- Mechanism: Direct minimization of an upper confidence bound on the risk over all convex weights, without pre-selecting neighbors, allows the method to adapt to the most favorable grouping structure of the true means, including varying group sizes and diameters.
- Core assumption: The penalization term can accurately account for uncertainty in bias when combining distant tasks, and the calibration constant u0 can be set appropriately.
- Evidence anchors:
  - [abstract] "determining weights via minimization of an upper confidence bound on the quadratic risk."
  - [section 4.1] "The parameter u is a calibration constant. Compared to the testing approach, one advantage is that it is not necessary to choose the parameters τ and ς."
  - [section 4.2] "the relative risk of the Q-aggregation method qualitatively enjoys the same asymptotic guarantees as the testing approach with optimally selected τ and ς."
- Break condition: If task means lie on a low-dimensional subspace but are very far apart, the method cannot significantly reduce compound risk.

## Foundational Learning

- Concept: Effective dimension (d* and de) as spectral measures of covariance matrices.
  - Why needed here: These quantities replace ambient dimension in high-dimensional asymptotics and determine the scaling of estimation error and the reliability of testing procedures.
  - Quick check question: How does the effective dimension d* relate to the ratio of trace and Frobenius norms of the covariance matrix?

- Concept: U-statistics for distance estimation between mean vectors.
  - Why needed here: U-statistics provide unbiased, high-probability estimates of squared distances between means, which are essential for both the testing approach and for constructing the penalization term in Q-aggregation.
  - Quick check question: What is the variance scaling of the U-statistic estimator for ∥∆k∥² in terms of the covariance norms and sample sizes?

- Concept: Oracle improvement and relative risk in multi-task estimation.
  - Why needed here: The performance metric is the ratio of estimator risk to naive risk, and the goal is to approach the best possible improvement given partial information about task similarity.
  - Quick check question: How does the oracle risk bound B(τ, ν) decompose into bias and variance components?

## Architecture Onboarding

- Component map: Estimator -> Weight selection (test-based or Q-aggregation) -> Neighbor detection / covariance estimation -> Kernel mean embedding (if applicable)
- Critical path: Sample collection -> Covariance/similarity estimation -> Weight computation -> Final convex combination
- Design tradeoffs: Testing approach requires sample splitting but is more robust to non-Gaussian data; Q-aggregation avoids splitting but needs careful calibration and is less robust to heavy tails
- Failure signatures: Poor improvement when effective dimension is low; instability when sample sizes are too small relative to dimension; failure to detect true neighbors when task means are very close but sample sizes differ greatly
- First 3 experiments:
  1. Synthetic Gaussian data with varying effective dimension and known similarity structure to validate oracle improvement bounds
  2. Flow cytometry data to test real-world performance and neighbor detection in imbalanced bag sizes
  3. High-dimensional data with heavy-tailed distributions to assess robustness beyond Gaussian assumptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed methods scale when the number of tasks B is much larger than the effective dimension of the data?
- Basis in paper: [inferred] The paper discusses high-dimensional asymptotics and the effect of the effective dimension on the performance of the methods, but does not explicitly analyze the regime where B >> effective dimension.
- Why unresolved: The paper focuses on the effective dimension growing large, but does not explore the case where the number of tasks significantly exceeds the effective dimension.
- What evidence would resolve it: Experimental results or theoretical analysis showing the performance of the methods as B grows while the effective dimension remains fixed or grows slowly.

### Open Question 2
- Question: Can the Q-aggregation approach be extended to handle non-Gaussian and heavy-tailed distributions beyond the bounded setting?
- Basis in paper: [explicit] The paper mentions that the testing approach can be extended to bounded and heavy-tailed distributions, but the Q-aggregation method is only discussed for Gaussian and bounded settings.
- Why unresolved: The paper does not provide a theoretical analysis or experimental results for the Q-aggregation method under non-Gaussian or heavy-tailed distributions.
- What evidence would resolve it: A theoretical analysis of the Q-aggregation method's performance under non-Gaussian or heavy-tailed distributions, along with experimental results demonstrating its effectiveness in such settings.

### Open Question 3
- Question: How sensitive are the proposed methods to the choice of kernel in the kernel mean embedding setting?
- Basis in paper: [explicit] The paper applies the methods to kernel mean embedding estimation and mentions that the choice of kernel can affect the constant ϕ, but does not explore the sensitivity to different kernel choices.
- Why unresolved: The paper does not provide a systematic analysis of how different kernel choices impact the performance of the methods in the kernel mean embedding setting.
- What evidence would resolve it: Experimental results comparing the performance of the methods using different kernels on various datasets, along with an analysis of the relationship between kernel properties and method performance.

## Limitations
- Theoretical guarantees rely heavily on high-dimensional asymptotics and assume effective dimension grows large relative to sample sizes
- Testing approach requires careful parameter tuning and sample splitting, which can be problematic in small sample regimes
- Q-aggregation is less robust to non-Gaussian or heavy-tailed data compared to the testing approach
- Some algorithmic details, especially for Tr(Σ²) approximation, are left as pseudocode, introducing potential implementation variability

## Confidence
- **High**: The asymptotic oracle improvement as effective dimension grows, and the basic mechanism of convex combination of empirical means
- **Medium**: The robustness of Q-aggregation to non-Gaussian data, and the practical performance on real-world flow cytometry data
- **Low**: The stability of the methods in very low-dimensional or highly imbalanced sample size regimes, and the sensitivity to parameter choices in non-standard settings

## Next Checks
1. Generate synthetic data with varying effective dimension and known similarity structure to empirically verify the asymptotic oracle improvement bounds and identify parameter regimes where the methods succeed or fail
2. Apply the methods to non-Gaussian and heavy-tailed distributions (e.g., Laplace, t-distributed) to quantify robustness beyond the Gaussian assumptions in theory
3. Systematically vary the key hyperparameters (τ, ς, u0) across a grid of settings to map out the stability and sensitivity of both the testing and Q-aggregation approaches