---
ver: rpa2
title: Decentralized Federated Learning with Model Caching on Mobile Agents
arxiv_id: '2408.14001'
source_url: https://arxiv.org/abs/2408.14001
tags:
- cache
- agents
- data
- local
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cached-DFL, a decentralized federated learning
  framework that leverages model caching on mobile agents to accelerate model convergence.
  By storing and exchanging not only their own models but also cached models from
  past encounters, agents can disseminate local models more efficiently and aggregate
  more information, improving convergence especially under non-i.i.d data distributions.
---

# Decentralized Federated Learning with Model Caching on Mobile Agents

## Quick Facts
- arXiv ID: 2408.14001
- Source URL: https://arxiv.org/abs/2408.14001
- Authors: Xiaoyu Wang; Guojun Xiong; Houwei Cao; Jian Li; Yong Liu
- Reference count: 40
- Key outcome: Introduces Cached-DFL, a decentralized federated learning framework that leverages model caching on mobile agents to accelerate model convergence, especially under non-i.i.d data distributions.

## Executive Summary
This paper proposes Cached-DFL, a decentralized federated learning framework that improves convergence speed and accuracy by enabling mobile agents to cache and exchange models from past encounters. The approach is particularly effective in non-i.i.d data scenarios common in mobile networks, where traditional DFL struggles due to limited model diversity. The authors theoretically analyze convergence with model staleness and compare different caching algorithms including LRU and group-based approaches through extensive experiments on MNIST, FashionMNIST, and CIFAR-10 datasets.

## Method Summary
Cached-DFL operates on mobile agents that perform local SGD updates and exchange models during encounters. Each agent maintains a cache storing models from recently encountered agents. During encounters, agents exchange both their current models and cached models, then aggregate all received models for the next training round. The framework uses LRU or group-based caching policies with configurable cache size and staleness tolerance τmax. Experiments use synthetic Manhattan mobility traces with 100 agents, comparing Cached-DFL against standard DFL across various data distributions and network conditions.

## Key Results
- Cached-DFL achieves faster convergence and higher accuracy than standard DFL, particularly in non-i.i.d scenarios
- LRU caching with cache size 10 and τmax=10 provides optimal balance between speed and accuracy
- Group-based caching outperforms LRU when agents naturally cluster into mobility or data-distribution groups
- Model staleness tolerance τmax must be carefully tuned - larger values speed initial convergence but may reduce final accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Caching stale models enables faster model spreading across a sparse mobile D2D network.
- Mechanism: Agents exchange not only their own latest models but also cached models from past encounters. This DTN-like approach increases the probability that any given agent receives a diverse set of local models from other agents in the network, even when direct pairwise encounters are infrequent.
- Core assumption: Mobility patterns ensure that cached models will eventually reach all agents via multi-hop relay through the physical movement of agents.
- Evidence anchors:
  - [abstract] "Each agent stores not only its own model, but also models of agents encountered in the recent past...When two agents meet, they exchange their own models as well as the cached models."
  - [section] "A similar problem was studied in the context of Mobile Ad hoc Network (MANET)...the efficiency of data dissemination in MANET can be significantly improved by Delay-Tolerant Networking (DTN)"
- Break condition: If mobility is too random or sparse, cached models may not reach all agents, limiting convergence.

### Mechanism 2
- Claim: Aggregating multiple cached models (including stale ones) can improve convergence in non-i.i.d. data scenarios.
- Mechanism: By using cached models in aggregation, an agent effectively simulates participation in a larger, more globally representative training process, even if some models are stale. This reduces the impact of data heterogeneity.
- Core assumption: The staleness introduced by caching is bounded and does not significantly distort the global gradient direction.
- Evidence anchors:
  - [abstract] "While DFL model caching sounds promising, it also faces a new challenge of model staleness: a cached model from an agent is not the current model on that agent, with the staleness determined by the mobility patterns, as well as the model spreading and caching algorithms."
  - [section] "We theoretically analyze the convergence of aggregation with cached models, explicitly taking into account the model staleness."
- Break condition: If τmax is too large or mobility is too slow, excessive staleness can prevent convergence.

### Mechanism 3
- Claim: Group-based caching prioritizes model diversity over recency when mobility patterns and data distributions are naturally clustered.
- Mechanism: Instead of purely LRU caching, agents maintain balanced representation of models from different geographic or data-distribution groups, improving coverage in heterogeneous environments.
- Core assumption: Agents within the same mobility group have similar data distributions, so a slightly older model from a different group is more valuable than a fresh model from the same group.
- Evidence anchors:
  - [section] "In practice, vehicle mobility patterns and local data distributions may naturally form groups...A fresh model of a vehicle in the same area is not as valuable as a slightly older model of a vehicle from another area."
  - [section] "So model caching should not just consider model freshness, but should also take into account the coverage of group-based data distributions."
- Break condition: If group structure is weak or data distributions overlap heavily, group-based caching offers no advantage.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: DFL is a decentralized variant of FL; understanding the centralized FL baseline is essential to appreciate the communication and computation trade-offs.
  - Quick check question: What is the main difference between FL and DFL in terms of communication topology?

- Concept: Non-i.i.d. data distribution
  - Why needed here: The paper emphasizes that caching helps most when data is non-i.i.d., so understanding why this is challenging is key.
  - Quick check question: Why does non-i.i.d. data make convergence harder in federated learning?

- Concept: Delay-Tolerant Networking (DTN)
  - Why needed here: The caching strategy is directly inspired by DTN principles; knowing how DTN works in MANETs explains the intuition.
  - Quick check question: How does DTN improve data dissemination in sparse mobile networks?

## Architecture Onboarding

- Component map: Agents -> Cache -> Mobility engine -> Communication layer -> Aggregation module -> Evaluation harness
- Critical path:
  1. Initialize models and caches
  2. Simulate agent mobility and encounters
  3. Exchange models (own + cached)
  4. Update local cache (LRU or group-based)
  5. Aggregate all cached models
  6. Perform local SGD updates
  7. Repeat until convergence

- Design tradeoffs:
  - Cache size vs. staleness: Larger caches reduce staleness but increase communication overhead
  - τmax vs. convergence: Higher τmax allows more stale models, potentially hurting convergence
  - Mobility speed vs. model spreading: Faster mobility improves dissemination but reduces local training time
  - Group-based vs. LRU: Group-based caching better handles clustered data but requires group knowledge

- Failure signatures:
  - Convergence stalls or slows dramatically in non-i.i.d. scenarios without caching
  - Accuracy plateaus below centralized FL when τmax is too large
  - Cache churn increases with very high mobility, reducing effective model coverage

- First 3 experiments:
  1. Compare DFL with and without LRU caching on MNIST non-i.i.d. data
  2. Sweep cache size from 1 to 30 and measure impact on convergence
  3. Vary τmax and measure trade-off between speed and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of model staleness threshold τmax affect the trade-off between convergence speed and final accuracy in different data heterogeneity scenarios?
- Basis in paper: [explicit] The authors experimentally evaluate different τmax values and observe that larger τmax speeds up initial convergence but reduces final accuracy, particularly in i.i.d scenarios.
- Why unresolved: The paper doesn't provide a theoretical framework to determine the optimal τmax for different levels of data heterogeneity.
- What evidence would resolve it: A theoretical analysis or empirical study mapping data heterogeneity metrics to optimal τmax values.

### Open Question 2
- Question: What is the impact of different caching algorithms (e.g., LRU, FIFO, LFU) on model convergence in DFL with non-i.i.d data distributions?
- Basis in paper: [explicit] The authors compare LRU and group-based caching algorithms but don't explore other common caching strategies.
- Why unresolved: The paper only evaluates a limited set of caching algorithms, leaving the question of optimal caching strategies open.
- What evidence would resolve it: A comprehensive comparison of various caching algorithms' performance in DFL with different data heterogeneity levels.

### Open Question 3
- Question: How does the frequency of agent encounters affect the performance of Cached-DFL in different mobility patterns?
- Basis in paper: [inferred] The authors discuss the impact of agent mobility on model convergence but don't explicitly analyze the relationship between encounter frequency and performance.
- Why unresolved: The paper focuses on speed variations but doesn't systematically vary encounter frequencies or explore different mobility patterns.
- What evidence would resolve it: Experiments varying agent encounter frequencies and mobility patterns to quantify their impact on convergence and accuracy.

## Limitations

- Theoretical analysis assumes bounded staleness and sufficient mixing via mobility, but quantitative thresholds are not provided
- Group-based caching requires prior knowledge of agent groupings, which may not be available in practice
- Manhattan mobility model may not generalize to all mobile agent scenarios beyond vehicular networks

## Confidence

- Mechanism 1 (DTN-like model spreading): High - well-established in MANET literature, directly applied here
- Mechanism 2 (stale models improving non-i.i.d. convergence): Medium - theoretically justified but sensitive to staleness bounds
- Mechanism 3 (group-based caching): Low-Medium - intuitive but requires group structure assumptions

## Next Checks

1. Test convergence sensitivity to τmax by running experiments with varying staleness thresholds (τmax = 5, 10, 20) on non-i.i.d. data
2. Evaluate Cached-DFL performance under alternative mobility models (random waypoint, community-based) to assess generalizability
3. Compare LRU vs. group-based caching when group structure is unknown or weak to measure robustness