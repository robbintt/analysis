---
ver: rpa2
title: 'ChatShop: Interactive Information Seeking with Language Agents'
arxiv_id: '2404.09911'
source_url: https://arxiv.org/abs/2404.09911
tags:
- agent
- product
- shopper
- agents
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The ChatShop task exposes a fundamental gap in language agents'
  interactive information-seeking capabilities. While agents excel at retrieval-style
  tasks with full product details, they struggle significantly when required to strategically
  gather information through multi-turn dialogue.
---

# ChatShop: Interactive Information Seeking with Language Agents

## Quick Facts
- arXiv ID: 2404.09911
- Source URL: https://arxiv.org/abs/2404.09911
- Authors: Sanxing Chen; Sam Wiseman; Bhuwan Dhingra
- Reference count: 20
- Key result: LLM agents achieve only ~63% average rewards in interactive shopping vs ~80% with full product details

## Executive Summary
ChatShop introduces a novel task that evaluates language agents' ability to gather information through multi-turn dialogue in a shopping scenario. The task exposes a fundamental gap in current LLM capabilities: while agents excel at retrieval-style tasks with complete product information, they struggle significantly when required to strategically explore and accumulate information through interactive communication. Through experiments with both simulated and human shoppers, the study reveals that even advanced prompting strategies like ReAct provide limited improvement, with top agents achieving only ~63% rewards compared to ~80% with full information available. The findings demonstrate that current language agents lack robust capabilities for the type of interactive, exploratory dialogue needed in realistic shopping scenarios.

## Method Summary
The ChatShop task builds on the WebShop dataset but reformulates goal instructions to be intentionally ambiguous, requiring agents to ask questions and gather information through multi-turn dialogue with shoppers. Agents interact with either simulated LLM shoppers or human participants through a web interface with BM25 search engine and product database. The task limits agents to 5 questions per session and evaluates performance based on reward calculation from product similarity to shopper needs. The study implements zero-shot prompting with conversation history compression and tests advanced strategies like ReAct, comparing interactive performance against non-interactive baselines with full product details available.

## Key Results
- Top agents (GPT-4) achieve only ~63% average rewards in interactive setting vs ~80% with full product details
- LLM-simulated shoppers serve as effective proxies for human shoppers, showing consistent performance patterns
- Advanced prompting strategies like ReAct provide limited improvement in bridging the interactive vs non-interactive performance gap
- Automatic analysis reveals common failure patterns: information tracking across long contexts, insufficient information gathering, and strategic question selection issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatShop reveals a fundamental gap in LLM agents' interactive information-seeking capabilities
- Mechanism: By removing full product details from goal instructions and requiring multi-turn dialogue, ChatShop forces agents to strategically gather information rather than relying on retrieval-style solutions
- Core assumption: The interaction challenge in the original WebShop task is artificial and can be solved through single-turn retrieval
- Evidence anchors:
  - [abstract] "The ChatShop task exposes a fundamental gap in language agents' interactive information-seeking capabilities"
  - [section 3] "We then demonstrate that the task can be reformulated and solved as a retrieval task without a requirement of interactive information seeking"
  - [corpus] Weak - corpus only shows related work on strategic information gathering but doesn't directly support this mechanism
- Break condition: If agents can achieve high rewards in ChatShop through single-turn retrieval or without meaningful interaction with shoppers

### Mechanism 2
- Claim: LLM-simulated shoppers serve as effective proxies for human shoppers in evaluating agent performance
- Mechanism: Simulated shoppers with constrained response lengths and instructions to avoid revealing target products create realistic interaction conditions that expose similar failure patterns as human shoppers
- Core assumption: Limiting response length and providing target product knowledge to shoppers creates sufficient realism for evaluation
- Evidence anchors:
  - [abstract] "we show that LLM-simulated shoppers serve as a good proxy for real human shoppers, revealing similar error patterns in agents"
  - [section 4.2] "we find that the LLM agents performance with the simulated shopper and the human shopper are consistent"
  - [corpus] Weak - corpus mentions related work on human-AI collaboration but doesn't directly support this mechanism
- Break condition: If agents perform significantly differently with simulated vs human shoppers, or if error patterns diverge substantially

### Mechanism 3
- Claim: Advanced prompting strategies like ReAct provide limited improvement in interactive information-seeking tasks
- Mechanism: While ReAct helps agents ask more questions in interactive settings, it doesn't bridge the fundamental gap between interactive and full-information performance
- Core assumption: ReAct's reasoning and action interleaving helps with strategic information gathering but cannot compensate for core limitations
- Evidence anchors:
  - [abstract] "even advanced prompting strategies like ReAct provide limited improvement in this interactive setting"
  - [section 4.1] "Although advanced prompting strategies further incentivize the agents, the gap between the best agent and the no-interactive full information baseline remains significant"
  - [corpus] Weak - corpus mentions related work on ReAct but doesn't directly support this mechanism
- Break condition: If ReAct or similar strategies can close the performance gap between interactive and full-information settings

## Foundational Learning

- Concept: Interactive information seeking vs retrieval
  - Why needed here: Understanding the difference between gathering information through dialogue vs accessing static knowledge sources is crucial for grasping ChatShop's design rationale
  - Quick check question: What key difference makes ChatShop more challenging than the original WebShop task?

- Concept: Task ambiguity and strategic exploration
  - Why needed here: ChatShop's core challenge comes from ambiguous goal instructions requiring agents to explore and accumulate information through interaction
  - Quick check question: How does task ambiguity in ChatShop differ from the clarity of instructions in WebShop?

- Concept: Long context modeling limitations
  - Why needed here: The paper identifies failure patterns related to information tracking across long contexts, which is a fundamental LLM limitation
  - Quick check question: Why might current LLM agents struggle with tracking information across multiple interaction turns?

## Architecture Onboarding

- Component map: Goal instruction → Agent question → Shopper response → Agent decision (search/select) → Product selection → Reward calculation
- Critical path: Goal instruction → Agent question → Shopper response → Agent decision (search/select) → Product selection → Reward calculation
- Design tradeoffs:
  - Response length limitation (5 words, 10 tokens) vs information richness
  - Question budget (5 questions) vs exploration depth
  - Open-ended vs instance-based communication channels
  - Simulated vs human shopper realism
- Failure signatures:
  - Reversion errors: Agent loses track of requirements across turns
  - Misinterpretation errors: Agent misunderstands shopper specifications
  - Insufficient information gathering: Agent doesn't ask enough questions
  - Repeated actions: Agent inefficiently repeats questions or searches
- First 3 experiments:
  1. Run baseline non-interactive agent with full product details to establish upper bound
  2. Run interactive agent with simulated shopper using different prompting strategies
  3. Compare performance with human shopper to validate simulated shopper effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which agents lose track of shopper-specified requirements across long contexts?
- Basis in paper: [explicit] "The first three error types are widespread, indicating current LLM agents' lack of strategic information seeking and robust long context modeling."
- Why unresolved: The paper identifies "reversion" as a common error type but does not explain the specific technical reasons why agents struggle with information tracking across long contexts.
- What evidence would resolve it: Detailed analysis of attention patterns in failed trajectories showing where and why information is lost during context processing.

### Open Question 2
- Question: How do different prompting strategies compare when applied to other web shopping domains beyond the current product categories?
- Basis in paper: [inferred] "We implement three prompting strategies with action enforcing" but only test on a fixed set of products.
- Why unresolved: The study only benchmarks agents on the existing WebShop dataset without testing generalization to new product domains or shopping scenarios.
- What evidence would resolve it: Systematic evaluation of prompting strategies across diverse product categories and shopping contexts.

### Open Question 3
- Question: What is the optimal balance between question budget and information gathering efficiency in the ChatShop task?
- Basis in paper: [explicit] "We limit the maximum number of questions the agent can ask in each session" but don't explore different budget allocations.
- Why unresolved: The study uses a fixed 5-question budget without investigating how different budget sizes affect agent performance and information gathering strategies.
- What evidence would resolve it: Comparative analysis of agent performance across varying question budgets to identify optimal resource allocation.

## Limitations

- The response length constraint of 5 words may artificially limit information richness and exaggerate interactive difficulty
- Reliance on LLM-simulated shoppers as human proxies may not fully capture human decision-making complexity
- Focus on shopping scenarios limits generalizability to other interactive information-seeking domains
- Evaluation metrics based on product similarity may not capture quality of interactive dialogue process

## Confidence

- High confidence: The fundamental gap between interactive and non-interactive performance is real and measurable
- Medium confidence: The effectiveness of LLM-simulated shoppers as human proxies, given the validation results
- Low confidence: The specific numerical performance thresholds (63% vs 80%) may be sensitive to experimental conditions

## Next Checks

1. **Response Length Variation Study**: Systematically vary the shopper response length constraint to determine its impact on agent performance and identify the threshold at which interactive communication becomes viable.

2. **Cross-Domain Transfer**: Apply the ChatShop evaluation framework to non-shopping domains (e.g., technical support, travel planning) to test the generalizability of the identified interactive information-seeking challenges.

3. **Human-in-the-Loop Fine-tuning**: Implement a small-scale human-in-the-loop training regime where agents learn from interactions with actual shoppers, then measure whether this improves performance relative to zero-shot approaches.