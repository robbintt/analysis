---
ver: rpa2
title: Automatic Combination of Sample Selection Strategies for Few-Shot Learning
arxiv_id: '2402.03038'
source_url: https://arxiv.org/abs/2402.03038
tags:
- selection
- learning
- strategies
- few-shot
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of 20 sample selection strategies
  on 5 few-shot learning approaches (meta-learning, fine-tuning, in-context learning)
  across 8 image and 6 text datasets. The study finds that sample selection has a
  strong impact on few-shot learning performance, especially for lower numbers of
  shots.
---

# Automatic Combination of Sample Selection Strategies for Few-Shot Learning

## Quick Facts
- arXiv ID: 2402.03038
- Source URL: https://arxiv.org/abs/2402.03038
- Authors: Branislav Pecher; Ivan Srba; Maria Bielikova; Joaquin Vanschoren
- Reference count: 40
- Key outcome: Sample selection strategies significantly impact few-shot learning performance, with ACSESS combining complementary strategies to achieve up to 3 percentage points improvement for gradient-based few-shot learning and 5 percentage points for in-context learning

## Executive Summary
This paper addresses the critical challenge of sample selection in few-shot learning, where the choice of which examples to include in the support set can dramatically affect model performance. The authors systematically evaluate 20 different sample selection strategies across 5 few-shot learning approaches (meta-learning, fine-tuning, and in-context learning) on 14 datasets. Their findings reveal that learnability is a stronger indicator of sample quality than informativeness or representativeness, and that sample selection impact diminishes as the number of shots increases. Most notably, the paper introduces ACSESS, an automatic method that combines complementary sample selection strategies using dataset-specific weights, consistently outperforming individual strategies.

## Method Summary
The study evaluates 20 sample selection strategies across 5 few-shot learning approaches on 14 datasets (8 image and 6 text). These strategies are categorized into learnability (measuring how easily samples can be learned), representativeness (capturing the data distribution), and informativeness (providing maximal information for model training). The authors propose ACSESS, which automatically combines multiple complementary sample selection strategies by identifying subsets of strategies that perform well on specific datasets and learning paradigms, then applying dataset-specific weights to these combinations. The method uses a validation set to determine optimal strategy combinations and weights for each scenario.

## Key Results
- Sample selection strategies have strong impact on few-shot learning performance, especially with lower numbers of shots
- ACSESS achieves up to 3 percentage points improvement for gradient-based few-shot learning and 5 percentage points for in-context learning compared to best individual strategy
- Learnability is identified as a stronger indicator of sample quality than informativeness or representativeness
- Impact of sample selection diminishes with higher numbers of shots, regressing to random selection at 30-40 shots

## Why This Works (Mechanism)
The mechanism behind ACSESS's success lies in its ability to identify and combine complementary sample selection strategies that capture different aspects of what makes a "good" sample for few-shot learning. By recognizing that different strategies excel in different scenarios and that samples near decision boundaries often provide unique value, ACSESS can construct more robust support sets. The dataset-specific weighting allows the method to adapt to the characteristics of each task, while the combination of strategies ensures that multiple quality dimensions (learnability, representativeness, informativeness) are simultaneously addressed.

## Foundational Learning
- **Few-shot learning paradigms**: Understanding meta-learning, fine-tuning, and in-context learning is essential for contextualizing the experiments. Quick check: Can you explain the key difference between how these three approaches handle new tasks?
- **Sample selection strategies**: Familiarity with learnability, representativeness, and informativeness metrics is crucial. Quick check: What is the fundamental difference between learnability and informativeness metrics?
- **Validation set methodology**: Understanding how validation sets are used to evaluate and combine strategies is key. Quick check: How does the validation set help determine optimal strategy combinations?
- **Decision boundary concepts**: Knowledge of how samples near decision boundaries affect model learning is important. Quick check: Why might samples near decision boundaries be particularly valuable for few-shot learning?
- **Dataset diversity**: Understanding the characteristics of the 14 datasets used helps interpret generalizability. Quick check: What are the key differences between the image and text datasets used in the evaluation?

## Architecture Onboarding

**Component Map**: Input datasets -> Sample selection strategies (20) -> ACSESS combination layer -> Validation set evaluation -> Optimized support set -> Few-shot learning model -> Performance metrics

**Critical Path**: The core pipeline processes datasets through multiple sample selection strategies, evaluates them using a validation set, and applies ACSESS to combine the most effective strategies with optimal weights before training the few-shot learning model.

**Design Tradeoffs**: The method trades computational complexity (evaluating 20 strategies and combining them) for improved performance. This overhead is justified by the consistent performance gains, though the exact computational cost isn't detailed in the paper.

**Failure Signatures**: The method may underperform if the validation set is not representative of the test distribution, or if the chosen sample selection strategies are not well-suited to the specific domain or task characteristics.

**First Experiments**: 1) Run individual sample selection strategies on a small dataset to observe baseline performance differences. 2) Test ACSESS on a single dataset with one learning paradigm to verify the combination mechanism. 3) Compare performance degradation as shot count increases to verify the diminishing returns observation.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation restricted to 20 specific sample selection strategies, limiting generalizability to other strategy types
- Focus on 5 few-shot learning approaches and 14 datasets may not capture full diversity of real-world applications
- Computational overhead of implementing ACSESS in production environments is not addressed
- Performance with noisy or adversarial data is not explored
- The exact threshold where random selection becomes optimal (30-40 shots) needs more precise characterization

## Confidence

**Major Claim Clusters Confidence:**
- **Sample selection strategies significantly impact few-shot learning performance (High)**: Supported by consistent improvements across multiple datasets and learning paradigms
- **ACSESS outperforms individual strategies (High)**: Demonstrated through controlled experiments with statistical significance
- **Learnability is a stronger indicator than informativeness or representativeness (Medium)**: While the results show this trend, the underlying reasons and theoretical justification could be more thoroughly explored
- **Impact of sample selection diminishes with more shots (Medium)**: The regression to random selection is observed but the precise relationship and mechanisms are not fully explained

## Next Checks

1. Test ACSESS on datasets from completely different domains (e.g., medical imaging, scientific literature) to verify cross-domain generalization
2. Conduct experiments with 50+ shot scenarios to precisely characterize when random selection becomes optimal and identify practical thresholds
3. Evaluate the computational overhead of ACSESS compared to baseline methods in production-scale settings with real-time constraints