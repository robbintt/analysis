---
ver: rpa2
title: 'Neural Sampling from Boltzmann Densities: Fisher-Rao Curves in the Wasserstein
  Geometry'
arxiv_id: '2410.03282'
source_url: https://arxiv.org/abs/2410.03282
tags:
- interpolation
- field
- linear
- gradient
- curve
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of sampling from an unnormalized
  Boltzmann density by learning a curve of Boltzmann densities starting from a simple
  reference density. The authors examine conditions under which Fisher-Rao flows are
  absolutely continuous in the Wasserstein geometry and analyze different interpolation
  schemes for learning the energy path and associated velocity fields.
---

# Neural Sampling from Boltzmann Densities: Fisher-Rao Curves in the Wasserstein Geometry

## Quick Facts
- arXiv ID: 2410.03282
- Source URL: https://arxiv.org/abs/2410.03282
- Reference count: 40
- Key outcome: Proposes gradient flow interpolation for sampling from unnormalized Boltzmann densities, demonstrating well-behaved flows that outperform linear and learned interpolation methods on Gaussian mixture models and many-well distributions.

## Executive Summary
This paper addresses the problem of sampling from unnormalized Boltzmann densities by learning a curve of Boltzmann densities starting from a simple reference density. The authors examine conditions under which Fisher-Rao flows are absolutely continuous in the Wasserstein geometry and analyze three interpolation schemes: linear interpolation (learns velocity field), learned interpolation (learns both energy and velocity), and gradient flow interpolation (uses fixed velocity field and learns only energy). The gradient flow approach corresponds to the Wasserstein gradient flow of the Kullback-Leibler divergence related to Langevin dynamics. Numerical experiments demonstrate that the gradient flow interpolation provides well-behaved flows and outperforms the other methods on Gaussian mixture models and many-well distributions.

## Method Summary
The paper proposes sampling from unnormalized Boltzmann densities by learning a curve of Boltzmann densities starting from a simple reference density. Three interpolation methods are compared: linear interpolation (parameterizing the vector field directly), learned interpolation (learning both energy and velocity), and gradient flow interpolation (fixing the velocity field and only learning the energy). The gradient flow interpolation uses the formula ft = (T-t)/T fD + tψt with fixed velocity vt = ∇(ft - fZ), corresponding to a Wasserstein gradient flow of the KL divergence. The methods are implemented using standard MLPs with swish activation functions and trained using the loss function E(θ, x, t) = |∂tf θ1 t - C θ2 t + ⟨∇f θ1 t, ∇(f θ1 t - fZ)⟩ - ∆(f θ1 t - fZ)|2.

## Key Results
- Proves that under mild conditions, Boltzmann density curves are absolutely continuous in the Wasserstein geometry
- Demonstrates analytically that linear interpolation suffers from velocity field explosion when target and reference distributions have modes far apart
- Shows numerically that gradient flow interpolation provides well-behaved flows with ESS ≈ 0.99-0.992, NLL ≈ 6.86-6.99, and energy distances ≈ 0.002-0.0021 on GMM and many-well tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The gradient flow interpolation provides a well-behaved velocity field by fixing the velocity as the gradient of the energy difference and only learning the time-dependent perturbation.
- Mechanism: By parameterizing ft = (T-t)/T fD + tψt and fixing vt = ∇(ft - fZ), the resulting PDE is the Fokker-Planck equation related to Langevin dynamics. This corresponds to a Wasserstein gradient flow of the Kullback-Leibler divergence, which is known to have stable solutions when the reference distribution is Gaussian.
- Core assumption: The reference distribution ρZ is Gaussian (or at least satisfies the drift condition), ensuring the PDE has a well-behaved solution.
- Evidence anchors: [abstract] states "This corresponds to the Wasserstein gradient flow of the Kullback-Leibler divergence related to Langevin dynamics." [section 3.2] notes "Equation (18) is well-known in connection with Wasserstein gradient flows... The existence and uniqueness of a solution of (20) is shown in (Ambrosio & Savar´e, 2007, Theorem 6.6)."

### Mechanism 2
- Claim: Linear interpolation suffers from "teleportation-of-mass" because the velocity field norm explodes as t approaches 1.
- Mechanism: For the linear interpolation ft = (1-t)fZ + tfD, the optimal velocity field required to satisfy the continuity equation develops singularities at late times when the target and reference distributions have modes far apart. The paper provides an analytic example showing ∥vt∥L2(ρt) grows exponentially with the distance between modes.
- Core assumption: The target and reference distributions have modes that are asymmetrically distributed in space.
- Evidence anchors: [abstract] states "we give an analytical example, where we can precisely measure the explosion of the velocity field." [section 3.1] provides "an analytical example, where we can precisely measure the explosion of the velocity field." [section A.4] shows detailed analytic calculation of the explosion.

### Mechanism 3
- Claim: The proposed gradient flow interpolation successfully solves the sampling task by learning a well-behaved flow field that outperforms linear and learned interpolations.
- Mechanism: By fixing the velocity field to ∇(ft - fZ) and only learning ψt, the method avoids the instability of learning both components simultaneously. The resulting flow is smoother and more stable, as demonstrated by numerical experiments.
- Core assumption: The learned ψt can approximate the necessary perturbation to transform the target distribution back to the reference distribution.
- Evidence anchors: [abstract] states "We demonstrate by numerical examples that our model provides a well-behaved flow field which successfully solves the above sampling task." [section 4] shows ESS ≈ 0.99-0.992, NLL ≈ 6.86-6.99, and energy distances ≈ 0.002-0.0021 for the gradient flow approach.

## Foundational Learning

- Concept: Wasserstein geometry and absolutely continuous curves
  - Why needed here: The paper relies on the theory of Wasserstein gradient flows to establish that certain Fisher-Rao curves are absolutely continuous in the Wasserstein space, which is crucial for the sampling approach.
  - Quick check question: What is the relationship between a Fisher-Rao curve and a Wasserstein absolutely continuous curve?

- Concept: Continuity equation and velocity fields
  - Why needed here: The continuity equation ∂tρt + ∇·(ρtvt) = 0 is fundamental to the sampling approach, as it describes how probability mass evolves under the velocity field vt.
  - Quick check question: How does the continuity equation relate to the Fokker-Planck equation in the context of Langevin dynamics?

- Concept: Kullback-Leibler divergence and gradient flows
  - Why needed here: The gradient flow of the KL divergence with respect to the Wasserstein metric leads to the Fokker-Planck equation, which is the basis for the proposed sampling method.
  - Quick check question: What is the gradient of the KL divergence in the Wasserstein space, and how does it relate to the Langevin dynamics?

## Architecture Onboarding

- Component map:
  - Energy functions: ft = (T-t)/T fD + tψt (learned) or ft = (1-t)fZ + tfD (linear)
  - Velocity fields: vt = ∇(ft - fZ) (gradient flow) or learned directly (linear/learned interpolation)
  - ODE solver: torchdiffeq with dopri5 (Runge-Kutta adaptive step size)
  - Loss function: E(θ, x, t) = |∂tf θ1 t - C θ2 t + ⟨∇f θ1 t, ∇(f θ1 t - fZ)⟩ - ∆(f θ1 t - fZ)|2

- Critical path:
  1. Initialize energy function and velocity field
  2. Sample points from uniform distribution and latent distribution
  3. Compute loss and backpropagate gradients
  4. Update parameters using optimizer
  5. Simulate ODE to generate samples from target distribution

- Design tradeoffs:
  - Learning both ft and vt (learned interpolation) vs. fixing vt and only learning ft (gradient flow): The latter is more stable but may be less expressive
  - Choice of latent distribution: Affects performance of linear and learned interpolations but not gradient flow
  - Time scheduling: Linear vs. learned scheduling function g(t) for gradient flow interpolation

- Failure signatures:
  - Exploding gradients during training (indicates instability in velocity field)
  - Poor ESS or high NLL on evaluation (indicates failure to learn the target distribution)
  - NaN errors during ODE simulation (indicates particles entering regions of very low density)

- First 3 experiments:
  1. Implement gradient flow interpolation on a simple 2D Gaussian mixture model with 2-3 modes
  2. Compare linear interpolation with different latent distribution standard deviations on the same task
  3. Implement learned interpolation and compare performance with gradient flow on a more complex many-well distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what precise conditions on the energy functions f₀ and f₁ does the linear interpolation lead to a velocity field with bounded L²(ρt) norm throughout the entire interpolation path?
- Basis in paper: [explicit] The paper proves conditions under which Fisher-Rao curves are absolutely continuous in Wasserstein geometry (Theorem 1 and 2), but the authors note that "it is not clear if Boltzmann densities stay Boltzmann densities" and demonstrate through an analytic example that the velocity field norm can explode for linear interpolation
- Why unresolved: While the paper provides sufficient conditions (drift condition, linear growth) for PPI and thus Wasserstein absolute continuity, it doesn't characterize when the velocity field remains bounded. The authors show an example where it explodes but don't provide a complete characterization of when this doesn't happen.
- What evidence would resolve it: A complete characterization of the class of energy functions f₀ and f₁ for which the linear interpolation produces bounded velocity fields, potentially through additional regularity conditions or constraints on the relationship between f₀ and f₁.

### Open Question 2
- Question: How does the performance of the gradient flow interpolation compare to other sampling methods like score-based diffusion models when both are implemented with the same computational budget?
- Basis in paper: [inferred] The paper demonstrates that gradient flow interpolation outperforms linear and learned interpolation on specific tasks, but doesn't compare against the broader class of score-based diffusion methods that inspired their approach
- Why unresolved: The paper focuses on comparing the three interpolation schemes it proposes, but doesn't benchmark against existing score-based diffusion methods that use similar mathematical foundations (Ornstein-Uhlenbeck processes, KL divergence minimization)
- What evidence would resolve it: Direct numerical comparisons of gradient flow interpolation against state-of-the-art score-based diffusion models on the same sampling tasks, controlling for computational resources and implementation details.

### Open Question 3
- Question: What is the theoretical relationship between the Fisher-Rao action and the Wasserstein-2 distance for the gradient flow interpolation path, and how does this compare to the linear interpolation?
- Basis in paper: [explicit] The paper establishes that for the linear interpolation, the Fisher-Rao action provides an upper bound for the Wasserstein-2 distance via the PPI, but notes that for gradient flow interpolation "it is not clear if Boltzmann densities stay Boltzmann densities"
- Why unresolved: While the paper shows that linear interpolation satisfies this relationship through PPI, it doesn't establish whether the gradient flow interpolation (which corresponds to Langevin dynamics) maintains this connection, particularly since it starts from the target distribution and flows toward the prior
- What evidence would resolve it: A rigorous proof establishing whether the gradient flow path maintains the Fisher-Rao action-Wasserstein distance relationship, potentially through analysis of the associated Fokker-Planck equation or Ornstein-Uhlenbeck process properties.

## Limitations

- Focuses on relatively low-dimensional problems (2D GMMs and 8D many-well distributions) without addressing scalability to higher dimensions
- Assumes well-behaved reference distributions, but real-world applications may involve more complex energy landscapes
- The choice of time discretization (4096 steps) and latent sampling strategy could significantly impact performance in practice

## Confidence

- High: Velocity field explosion in linear interpolation (Mechanism 2)
- Medium: Gradient flow stability and numerical performance (Mechanism 1 and 3)
- Low: Scalability claims and generalization to high-dimensional problems

## Next Checks

1. Test the gradient flow approach on higher-dimensional distributions (e.g., 20-50 dimensions) to assess scalability
2. Implement ablation studies comparing different latent distributions and time schedules for the gradient flow method
3. Compare the proposed method with established MCMC approaches (e.g., Hamiltonian Monte Carlo) on benchmark posterior inference tasks