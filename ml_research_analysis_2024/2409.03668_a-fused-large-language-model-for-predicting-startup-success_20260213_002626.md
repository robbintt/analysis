---
ver: rpa2
title: A Fused Large Language Model for Predicting Startup Success
arxiv_id: '2409.03668'
source_url: https://arxiv.org/abs/2409.03668
tags:
- startup
- language
- large
- success
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops a fused large language model to predict startup
  success using both fundamental variables and textual self-descriptions from online
  venture capital platforms. The model combines a pre-trained BERT language model
  for text embeddings with structured data and a neural network classifier.
---

# A Fused Large Language Model for Predicting Startup Success

## Quick Facts
- arXiv ID: 2409.03668
- Source URL: https://arxiv.org/abs/2409.03668
- Reference count: 40
- Primary result: Fused LLM achieves 74.33% balanced accuracy and 82.78% AUROC, outperforming baselines using only fundamental variables (72.00% balanced accuracy)

## Executive Summary
This study develops a fused large language model to predict startup success using both fundamental variables and textual self-descriptions from online venture capital platforms. The model combines pre-trained BERT language model for text embeddings with structured data and a neural network classifier. Evaluated on 20,172 Crunchbase profiles, the approach demonstrates that including textual self-descriptions significantly improves prediction performance and return on investment by 52.25 percentage points compared to using fundamental variables alone.

## Method Summary
The study employs a fused large language model architecture that concatenates fundamental startup variables (age, funding, industry, etc.) with document embeddings from a pre-trained BERT model. The approach uses 20,172 Crunchbase profiles from US startups founded 2013-2015, with a 5-year forecast horizon. The model is trained using 80/20 train/test splits across 5 random partitions, with hyperparameter tuning via 10-fold cross-validation. The BERT embeddings are generated from textual self-descriptions without fine-tuning, then concatenated with structured variables and passed through a neural network classifier.

## Key Results
- Fused model achieves 74.33% balanced accuracy and 82.78% AUROC on test data
- Including textual self-descriptions increases AUROC by 2.18 percentage points over fundamental variables alone
- ROI improvement of 52.25 percentage points when including textual information
- No performance improvement observed when fine-tuning BERT embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fusing structured data with learned text embeddings captures complementary information that improves predictive performance.
- Mechanism: The model concatenates fundamental variables with document embeddings from BERT, allowing the classifier to leverage both structured metrics and semantic content about business models.
- Core assumption: Textual self-descriptions contain predictive signals not captured by structured variables alone.
- Evidence anchors: The inclusion of textual self-descriptions significantly improves prediction performance and ROI by 52.25 percentage points; the inclusion of textual self-descriptions within our fused large language model performs best and increases the AUROC by 2.18 percentage points.
- Break condition: If textual self-descriptions contain redundant information already captured by structured variables, fusion provides no gain.

### Mechanism 2
- Claim: Large language models (BERT) capture semantic relationships and context that traditional bag-of-words approaches miss.
- Mechanism: BERT maps text to dense embeddings that encode meaning and relationships between words, allowing the model to understand nuanced descriptions of business models and innovations beyond simple word frequencies.
- Core assumption: Semantic understanding of text improves prediction accuracy over syntactic analysis.
- Evidence anchors: Traditional approaches from machine learning for making predictions from online descriptions (e.g., bag-of-words) will likely struggle with the complexity of the underlying task; compared to our fused large language model, the baselines are inferior, with all performance improvements attributed to the better model architecture.
- Break condition: If the textual descriptions are too short or formulaic, semantic richness may not translate to better predictions.

### Mechanism 3
- Claim: Pre-trained language models provide effective feature representations without extensive fine-tuning for domain-specific tasks.
- Mechanism: The study uses BERT's pre-trained embeddings directly, finding that fine-tuning does not improve performance, suggesting that general language understanding transfers well to startup success prediction.
- Core assumption: Pre-trained language models capture transferable features relevant to startup descriptions.
- Evidence anchors: Overall, we do not observe any performance improvement when fine-tuning our fused-large language model, underlining that increasing the number of trainable or fine-tunable parameters does not necessarily guarantee performance improvements.
- Break condition: If startup-specific language patterns differ significantly from pre-training corpus, fine-tuning might become necessary.

## Foundational Learning

- Concept: Large language models and their embedding generation
  - Why needed here: Understanding how BERT transforms text into dense representations is critical for implementing the fusion architecture.
  - Quick check question: What is the dimensionality of BERT embeddings used in this study, and how are they generated from text?

- Concept: Machine learning classifier selection and hyperparameter tuning
  - Why needed here: The study tests multiple classifiers (logistic regression, elastic net, random forest, neural network) and tunes hyperparameters via grid search.
  - Quick check question: Which classifier achieved the highest AUROC in the fused model, and what was the performance metric?

- Concept: Evaluation metrics for binary classification (AUROC, balanced accuracy, F1-score)
  - Why needed here: Understanding these metrics is essential for interpreting model performance and comparing baselines.
  - Quick check question: What was the AUROC improvement when adding textual self-descriptions to the fused model compared to using only fundamental variables?

## Architecture Onboarding

- Component map: Data ingestion -> Text processing (BERT tokenization and embedding generation) -> Feature fusion (concatenation of structured variables with BERT embeddings) -> Classification (neural network) -> Evaluation (cross-validation with multiple performance metrics)

- Critical path:
  1. Extract and preprocess Crunchbase data
  2. Generate BERT embeddings for textual descriptions
  3. Concatenate with structured variables
  4. Train classifier with hyperparameter tuning
  5. Evaluate on held-out test set

- Design tradeoffs:
  - Using pre-trained BERT vs. training from scratch: Pre-trained offers faster development but may miss domain-specific patterns
  - Neural network vs. simpler classifiers: Neural networks capture complex relationships but require more data and tuning
  - Fine-tuning BERT vs. using frozen embeddings: Fine-tuning didn't help here but might for other domains

- Failure signatures:
  - Poor performance with only textual data suggests embeddings don't capture relevant patterns
  - No improvement from fine-tuning suggests pre-trained embeddings already capture needed information
  - Large variance across folds indicates data sparsity or class imbalance issues

- First 3 experiments:
  1. Train baseline classifier using only fundamental variables (no text)
  2. Train classifier using only BERT embeddings (no structured variables)
  3. Train fused model and compare performance gains over both baselines

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Reliance on Crunchbase data may introduce selection bias and doesn't represent the full population of startups
- 5-year forecast horizon provides limited temporal validation without examining longer-term prediction accuracy
- Finding that fine-tuning BERT doesn't improve performance is unexpected and warrants further investigation

## Confidence

- High confidence: Fusion architecture's superior performance over single-modality approaches, supported by clear statistical improvements (74.33% vs 72.00% balanced accuracy) and consistent across multiple evaluation metrics
- Medium confidence: Claim that semantic embeddings specifically drive the improvement, as the study doesn't directly compare BERT embeddings against other neural text representations
- Low confidence: Generalizability of the fine-tuning results, given the lack of ablation studies on different pre-training domains or parameter freezing strategies

## Next Checks

1. Replicate the experiment with a different startup dataset (e.g., AngelList or PitchBook) to test generalizability across data sources and assess potential Crunchbase-specific biases

2. Conduct an ablation study comparing BERT embeddings against other neural text representations (e.g., Sentence-BERT, RoBERTa) to isolate whether semantic understanding or simply dense representations drive the performance gains

3. Test the model's predictive accuracy at multiple time horizons (1, 3, 5, and 7 years) to understand temporal degradation patterns and establish the practical limits of venture capital prediction windows