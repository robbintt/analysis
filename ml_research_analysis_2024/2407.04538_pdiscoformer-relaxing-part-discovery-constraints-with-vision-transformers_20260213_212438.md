---
ver: rpa2
title: 'PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers'
arxiv_id: '2407.04538'
source_url: https://arxiv.org/abs/2407.04538
tags:
- part
- parts
- image
- discovery
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses unsupervised part discovery for fine-grained
  image classification. Previous methods imposed strong geometric priors, requiring
  compact, small parts, which limited their generalization to irregular part shapes.
---

# PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers

## Quick Facts
- arXiv ID: 2407.04538
- Source URL: https://arxiv.org/abs/2407.04538
- Reference count: 40
- One-line primary result: Uses TV regularization and Gumbel-Softmax to discover interpretable, irregularly shaped parts without compactness priors, improving fine-grained classification across CUB, PartImageNet, and Oxford Flowers.

## Executive Summary
This paper addresses unsupervised part discovery for fine-grained image classification by relaxing the strong geometric priors that previously constrained part shapes to be compact and small. The authors leverage self-supervised vision transformers and total variation regularization to discover interpretable parts without constraining their shape or size. The approach combines total variation loss with entropy and background losses, along with Gumbel-Softmax for part assignment, achieving consistent improvements over prior work on multiple datasets.

## Method Summary
The method uses a pre-trained DinoV2 ViT-B backbone with frozen layers except for tokens, computing attention maps via Gumbel-Softmax over K+1 prototypes. Part embeddings are derived through weighted patch averaging, layer-normalized per part, and modulated before classification via per-part linear layers. The model is trained with cross-entropy plus multiple loss terms including Lp0 (background), Lp1 (presence), Lent (entropy), Ltv (total variation), Leq (equivariance), and L⊥ (orthogonality), using Adam with specific learning rates for different components.

## Key Results
- Total variation regularization enables detection of irregularly shaped parts without enforcing compactness
- Gumbel-Softmax ensures each patch token is assigned to exactly one part, reducing ambiguity
- Layer normalization per part embedding enables discriminative, distinct part representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Total variation regularization enables detection of irregularly shaped parts without enforcing compactness
- Mechanism: TV loss penalizes high-frequency spatial variations in attention maps, encouraging piece-wise constant regions that can be connected or disconnected while preserving shape flexibility
- Core assumption: Strong inductive biases from self-supervised ViT representations can be leveraged when geometric constraints are relaxed
- Evidence anchors:
  - [abstract] "total variation (TV) prior, which allows for multiple connected components of any size, substantially outperforms previous work"
  - [section] "we find that a total variation (TV) prior, which allows for multiple connected components of any size, substantially outperforms previous work"
- Break condition: If ViT backbone is not pre-trained self-supervised, TV alone leads to background leakage and inconsistent part maps

### Mechanism 2
- Claim: Gumbel-Softmax ensures each patch token is assigned to exactly one part, reducing ambiguity
- Mechanism: Stochastic relaxation of discrete part assignment during training forces attention maps to have low entropy, making part boundaries sharper
- Core assumption: Each patch token should belong to exactly one part to prevent information leakage
- Evidence anchors:
  - [abstract] "Gumbel-Softmax for part assignment"
  - [section] "ensure that each patch token is assigned to a unique part"
- Break condition: Without Gumbel-Softmax, parts become blurry and overlap, hurting downstream classification

### Mechanism 3
- Claim: Layer normalization per part embedding enables discriminative, distinct part representations
- Mechanism: Normalizing across feature and prototype dimensions before modulation makes each part embedding distinct, preventing parts from collapsing into similar features
- Core assumption: Modulated embeddings need to be decorrelated for effective classification
- Evidence anchors:
  - [section] "These modulated embedding vectors are then linearly projected to obtain a vector of class scores conditioned on the part embedding"
  - [section] "ensures that each embedding vector distinct for the classifier"
- Break condition: Without per-part normalization, parts may converge to similar prototypes, reducing interpretability

## Foundational Learning

- Concept: Total variation as spatial regularization
  - Why needed here: Provides piece-wise constant constraint without compactness prior, allowing irregular shapes
  - Quick check question: What happens to the total variation loss if part attention maps become perfectly uniform?

- Concept: Gumbel-Softmax for differentiable sampling
  - Why needed here: Enables gradient-based training for discrete part assignments without hard argmax
  - Quick check question: What would the attention maps look like if we replaced Gumbel-Softmax with standard softmax?

- Concept: Self-supervised pre-training in vision transformers
  - Why needed here: Provides strong semantic representations that can be fine-tuned for part discovery without needing strict shape priors
  - Quick check question: Why does the method fail when using a ResNet backbone with TV loss alone?

## Architecture Onboarding

- Component map: Image → ViT backbone (frozen except tokens) → Patch features → K+1 prototypes → Gumbel-Softmax attention maps → Weighted patch averaging → Layer-normalized part embeddings → Per-part classification → Mean pooling → Final classification
- Critical path: ViT features → Attention maps → Part embeddings → Classification
- Design tradeoffs: Relaxed shape priors improve generalization but require stronger semantic backbone; more parts increase granularity but may cause oversegmentation
- Failure signatures: Parts collapsing to background, parts overlapping significantly, classification accuracy dropping with more parts
- First 3 experiments:
  1. Ablation: Remove TV loss, observe part maps becoming noisy and background leaking into foreground
  2. Ablation: Remove Gumbel-Softmax, observe entropy in attention maps increasing and parts becoming ambiguous
  3. Ablation: Replace ViT with ResNet, observe need for compactness loss to maintain consistent part maps

## Open Questions the Paper Calls Out

- Question: How does the performance of PDiscoFormer scale when trained on larger and more diverse datasets beyond CUB, PartImageNet, and Oxford Flowers?
- Basis in paper: [explicit] The paper mentions that exploring training on larger and more diverse datasets could provide valuable insights and further validate performance in real-world scenarios.
- Why unresolved: The study focused on datasets with part annotations or FG-BG masks available for the test set, limiting the exploration of the model's generalization to larger datasets.
- What evidence would resolve it: Conducting experiments on larger datasets with varying object appearances and poses to assess the model's generalization capabilities.

- Question: Can the number of parts K be automatically estimated while preserving interpretability in the discovered semantic parts?
- Basis in paper: [explicit] The paper discusses the impact of the hyper-parameter K on the granularity of discovered parts and suggests exploring ways to automatically estimate this value.
- Why unresolved: Manual selection of K is currently required, and the paper does not provide a method for automatic estimation.
- What evidence would resolve it: Developing an algorithm or method to automatically determine the optimal number of parts K for a given dataset.

- Question: How would integrating PDiscoFormer with unsupervised object discovery methods improve part discovery in complex scenes?
- Basis in paper: [explicit] The paper suggests that integrating PDiscoFormer with unsupervised object discovery research could enable part discovery within each detected object, improving capabilities in complex scenes.
- Why unresolved: The paper does not provide experimental results or evidence of such integration.
- What evidence would resolve it: Conducting experiments to combine PDiscoFormer with unsupervised object discovery methods and evaluating the performance on complex scenes.

## Limitations
- The method relies heavily on a self-supervised ViT backbone, limiting generalization to other architectures
- Performance on datasets with significant occlusion or viewpoint variation remains untested
- Computational overhead from Gumbel-Softmax sampling and multiple loss terms may limit scalability

## Confidence
- **High Confidence**: The mechanism of total variation regularization enabling irregular part shapes (supported by consistent quantitative improvements across three diverse datasets)
- **Medium Confidence**: The necessity of Gumbel-Softmax for discrete part assignment (supported by ablation studies, but implementation details are underspecified)
- **Medium Confidence**: The role of layer normalization per part embedding in maintaining discriminative features (described but not extensively validated through ablation)

## Next Checks
1. **Cross-Architecture Validation**: Replace the ViT backbone with a ResNet and evaluate whether the method still discovers meaningful parts without the geometric compactness prior, quantifying the drop in performance.
2. **Shape Diversity Analysis**: On CUB and Flowers, measure the actual geometric diversity of discovered parts (e.g., using bounding box aspect ratios or convex hull areas) to confirm the claim of detecting irregular shapes.
3. **Robustness to Occlusion**: Create a synthetic test set by occluding 30-50% of images in CUB with random patches and evaluate whether part discovery and classification accuracy degrade significantly compared to baselines.