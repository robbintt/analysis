---
ver: rpa2
title: Efficient data selection employing Semantic Similarity-based Graph Structures
  for model training
arxiv_id: '2402.14888'
source_url: https://arxiv.org/abs/2402.14888
tags:
- data
- graph
- performance
- points
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SeSaME, a novel graph-based approach to efficient
  data selection for training ASR models. SeSaME leverages semantic similarity between
  textual utterances to predict ASR performance and sample salient data points for
  fine-tuning.
---

# Efficient data selection employing Semantic Similarity-based Graph Structures for model training

## Quick Facts
- arXiv ID: 2402.14888
- Source URL: https://arxiv.org/abs/2402.14888
- Reference count: 15
- Primary result: SeSaME achieves 93% accuracy increase in predicting ASR performance and improves fine-tuning with 7% WER drop on difficult datasets

## Executive Summary
SeSaME introduces a novel graph-based approach for efficient data selection in ASR model training. The method leverages semantic similarity between textual utterances to predict ASR performance and select salient data points for fine-tuning. By constructing a graph where nodes represent utterances and edges connect semantically similar nodes, SeSaME uses a GNN to predict WER labels. Experiments on Common Voice English data demonstrate significant improvements in ASR performance, particularly when combined with non-local aggregation (GAT).

## Method Summary
The method constructs a semantic similarity graph using BERT embeddings of textual utterances and their corresponding WER labels. Utterances are discretized into WER buckets and used as node labels. A GNN (GCN, GIN, SAGE, or GAT) is trained to predict these labels from node features. For fine-tuning, incoming utterances are added to the graph, their predicted labels guide data selection, and the ASR model is fine-tuned on the most difficult samples. The approach is evaluated on Common Voice English dataset with XLS-R-300M wav2vec2 variant.

## Key Results
- SeSaME achieves 93% accuracy increase in predicting ASR performance compared to random predictions
- GAT with SeSaME yields 7% WER drop on highly difficult datasets and 1.8% WER drop on semantically similar datasets
- Non-local aggregation (GAT) outperforms local aggregation methods for ASR performance prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SeSaME can predict ASR performance on unseen textual utterances without audio processing by leveraging semantic similarity graphs.
- Mechanism: The graph encodes utterances as nodes with BERT embeddings and connects them via cosine similarity. A GNN learns to map these embeddings to discretized WER buckets, effectively learning which textual patterns correlate with recognition difficulty.
- Core assumption: Semantic similarity between textual utterances is predictive of ASR difficulty, even without audio features.
- Evidence anchors:
  - [abstract]: "SeSaME learns to categorize new incoming data points into speech recognition difficulty buckets by employing semantic similarity-based graph structures and discrete ASR information from homophilous neighbourhoods through message passing."
  - [section 3.1]: "Our intuition is that we can infer the model’s performance on a new dataset Dholdout using the observed WER and the semantic similarity of Dholdout with Dtrain."
  - [corpus]: Weak - corpus shows related work but no direct evidence that semantic similarity alone suffices for ASR difficulty prediction.
- Break condition: If the semantic similarity structure does not correlate with WER (low homophily), the GNN cannot learn meaningful mappings.

### Mechanism 2
- Claim: Discretizing WER into ordinal buckets improves GNN learning compared to treating WER as continuous.
- Mechanism: Continuous WER values are grouped into 7 buckets (0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 1). Each bucket is treated as an ordinal class, and label encoding assigns binary vectors indicating all lower classes. Binary cross-entropy loss is used, enabling ordinal regression.
- Core assumption: WER discretization preserves meaningful difficulty ordering and reduces regression complexity to classification.
- Evidence anchors:
  - [section 3.1]: "However, WER has one nontrivial disadvantage when using it in an ordinal regression task: it is a continuous variable. To mitigate this issue, we discretise WER into k buckets..."
  - [section 4]: "We divide WER into seven classes by grouping all utterances with a WER xi ≤ Xc where Xc = [0 .05, 0.1, 0.15, 0.2, 0.3, 0.5, 1]."
  - [corpus]: Weak - no corpus evidence directly supports discretization benefit.
- Break condition: If bucket boundaries are poorly chosen, the ordinal structure may misrepresent true difficulty, hurting GNN performance.

### Mechanism 3
- Claim: Non-local aggregation (GAT) outperforms local aggregation (GCN, GIN, SAGE) for ASR performance prediction due to higher-order semantic dependencies.
- Mechanism: GAT uses masked self-attention to weight neighbor contributions dynamically, capturing long-range semantic relationships better than fixed local aggregation rules.
- Core assumption: ASR difficulty is influenced by broader semantic context beyond immediate neighbors.
- Evidence anchors:
  - [abstract]: "...7% WER drop with non-local aggregation when evaluating against a highly difficult dataset..."
  - [section 5]: "GAT shows significantly better performance, with a 17% decrease in test loss compared to random sampling. GAT also achieves an impressive WER decrease of 37.9%..."
  - [corpus]: Weak - corpus shows related speech tokenization and emotion recognition work but not direct GAT vs local aggregation comparison.
- Break condition: If semantic dependencies are primarily local, non-local aggregation adds unnecessary complexity without benefit.

## Foundational Learning

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: GNNs propagate semantic similarity information through the graph to infer WER labels for unseen nodes.
  - Quick check question: How does message passing aggregate neighbor information, and what aggregation functions are used in GCN vs GAT?

- Concept: Semantic Similarity and Embeddings
  - Why needed here: BERT embeddings represent utterances, and cosine similarity defines graph edges.
  - Quick check question: Why are BERT embeddings suitable for capturing semantic similarity, and how does approximate nearest neighbor search scale this?

- Concept: Ordinal Regression vs Classification
  - Why needed here: WER is continuous; discretizing into ordinal buckets allows ordinal regression, which is more informative than plain classification.
  - Quick check question: What is the difference between ordinal regression and classification, and how does binary cross-entropy support ordinal targets?

## Architecture Onboarding

- Component map: Common Voice English dataset -> Preprocessing (WER bucketing, BERT embeddings) -> Semantic similarity graph construction -> GNN training (label recovery) -> ASR fine-tuning with GNN-selected samples -> WER evaluation
- Critical path: Graph construction -> GNN training -> Fine-tuning ASR with GNN-selected samples -> WER evaluation
- Design tradeoffs:
  - Local vs non-local aggregation: GAT adds attention overhead but captures richer semantics; local GNNs are faster but may miss long-range dependencies.
  - Bucket granularity: More buckets increase resolution but reduce samples per class; fewer buckets increase class size but coarsen difficulty resolution.
  - Embedding dimensionality: Higher dims may capture more nuance but increase computation; lower dims speed training but risk information loss.
- Failure signatures:
  - GNN validation accuracy plateaus near random baseline (~14.3%) -> semantic similarity not predictive of WER.
  - WER fine-tuning shows no improvement over random sampling -> graph structure not transferring to ASR gains.
  - High homophily but poor ASR performance -> semantic similarity correlates with something other than ASR difficulty.
- First 3 experiments:
  1. Train GNN on graph constructed from Dtrain, evaluate validation accuracy and OFA; compare against random predictions.
  2. Fine-tune ASR with samples selected by GNN (most difficult bucket) vs random sample; measure WER on held-out test set.
  3. Repeat experiment 2 using GAT vs GCN/GIN/SAGE to isolate impact of non-local aggregation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of WER bucketing thresholds impact the performance of SeSaME, and what is the optimal number of buckets for different ASR tasks?
- Basis in paper: [explicit] The paper discretizes WER into seven buckets, but does not explore the impact of different bucketing strategies.
- Why unresolved: The paper does not provide a systematic analysis of how the choice of WER bucketing affects the performance of SeSaME. It is unclear whether the chosen seven buckets are optimal or if different ASR tasks would benefit from different bucketing strategies.
- What evidence would resolve it: Experiments comparing the performance of SeSaME with different WER bucketing strategies (e.g., varying the number of buckets, adjusting the threshold values) on various ASR tasks would provide insights into the optimal bucketing approach.

### Open Question 2
- Question: How does the choice of graph neural network architecture (e.g., GCN, GIN, GraphSAGE, GAT) impact the performance of SeSaME, and are there other GNN architectures that could further improve results?
- Basis in paper: [explicit] The paper compares four GNN architectures (GCN, GIN, GraphSAGE, GAT) but does not provide a comprehensive analysis of their relative performance or explore other GNN architectures.
- Why unresolved: While the paper shows that all four GNN architectures improve upon random predictions, it does not provide a detailed comparison of their performance or investigate whether other GNN architectures could yield better results.
- What evidence would resolve it: Extensive experiments comparing the performance of SeSaME with different GNN architectures (e.g., deeper GNNs, GNNs with different aggregation functions) on various ASR tasks would help identify the most suitable architecture for SeSaME.

### Open Question 3
- Question: How does the choice of BERT model (e.g., base, large, multilingual) impact the performance of SeSaME, and are there other contextual embeddings that could further improve results?
- Basis in paper: [explicit] The paper uses BERT base uncased model for generating contextualized word embeddings, but does not explore the impact of different BERT models or other contextual embeddings.
- Why unresolved: The paper does not provide insights into how the choice of BERT model or other contextual embeddings affects the performance of SeSaME. It is unclear whether using a larger or multilingual BERT model, or alternative contextual embeddings, could lead to better results.
- What evidence would resolve it: Experiments comparing the performance of SeSaME with different BERT models (e.g., base, large, multilingual) and alternative contextual embeddings (e.g., RoBERTa, XLNet) on various ASR tasks would help determine the optimal choice of contextual embeddings for SeSaME.

### Open Question 4
- Question: How does the choice of semantic similarity metric (e.g., cosine similarity, Euclidean distance) impact the performance of SeSaME, and are there other similarity metrics that could further improve results?
- Basis in paper: [explicit] The paper uses cosine similarity as the semantic similarity metric for constructing the graph, but does not explore the impact of different similarity metrics.
- Why unresolved: The paper does not provide insights into how the choice of semantic similarity metric affects the performance of SeSaME. It is unclear whether using a different similarity metric, such as Euclidean distance or other semantic similarity measures, could lead to better results.
- What evidence would resolve it: Experiments comparing the performance of SeSaME with different semantic similarity metrics (e.g., cosine similarity, Euclidean distance, other semantic similarity measures) on various ASR tasks would help determine the optimal choice of similarity metric for SeSaME.

## Limitations
- Unproven semantic-to-ASR transferability: Semantic similarity alone may not generalize across domains or languages
- Model-specific dependency: Results heavily depend on XLS-R-300M wav2vec2 variant performance
- Graph construction sensitivity: Effectiveness depends on edge creation parameters not specified in the paper
- Computational overhead trade-off: Additional preprocessing and GNN training may not justify WER gains

## Confidence

**High confidence**: GNN can learn to predict discretized WER buckets from BERT embeddings with ~93% accuracy vs random. This is directly measurable and demonstrated across multiple GNN architectures in the validation set.

**Medium confidence**: GAT outperforms local aggregation (GCN, GIN, SAGE) for this task. While the 17% test loss decrease and 37.9% WER drop are reported, these results depend on specific hyperparameters and may not generalize across different datasets or GNN implementations.

**Low confidence**: Semantic similarity is sufficient for predicting ASR difficulty without audio features. This core mechanism assumption lacks cross-domain validation and may break when tested on languages with different orthographic-to-phonetic mappings or specialized domains.

## Next Checks

1. **Cross-domain robustness test**: Apply SeSaME to a completely different ASR dataset (e.g., TED-LIUM, Librispeech, or accented speech) and measure whether semantic similarity still predicts WER accurately. Compare performance against audio-based difficulty predictors to quantify the semantic-only approach's limitations.

2. **Ablation study on graph construction**: Systematically vary the similarity threshold for edge creation and ANN search parameters. Measure how GNN validation accuracy and subsequent WER improvements change with graph density. This will reveal whether the reported results are robust to graph construction choices.

3. **Computational efficiency analysis**: Measure total wall-clock time and GPU hours for: (a) full training on random samples, (b) SeSaME preprocessing (graph construction + GNN training), and (c) fine-tuning with GNN-selected samples. Calculate the WER improvement per unit of computation to determine if the approach is truly "efficient" compared to simpler baselines.