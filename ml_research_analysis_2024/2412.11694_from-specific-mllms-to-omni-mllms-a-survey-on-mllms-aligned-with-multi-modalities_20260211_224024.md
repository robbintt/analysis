---
ver: rpa2
title: 'From Specific-MLLMs to Omni-MLLMs: A Survey on MLLMs Aligned with Multi-modalities'
arxiv_id: '2412.11694'
source_url: https://arxiv.org/abs/2412.11694
tags:
- understanding
- text
- wang
- zhang
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides the first comprehensive overview of Omni-MLLMs,
  which aim to achieve omni-modal understanding and generation by unifying multiple
  non-linguistic modalities within a single model. The paper introduces a meticulous
  taxonomy of Omni-MLLMs based on encoding and alignment methods, covering four core
  components: multi-modalities encoding, alignment, interaction, and generation.'
---

# From Specific-MLLMs to Omni-MLLMs: A Survey on MLLMs Aligned with Multi-modalities

## Quick Facts
- **arXiv ID:** 2412.11694
- **Source URL:** https://arxiv.org/abs/2412.11694
- **Reference count:** 40
- **Primary result:** This survey provides the first comprehensive overview of Omni-MLLMs, which aim to achieve omni-modal understanding and generation by unifying multiple non-linguistic modalities within a single model.

## Executive Summary
This survey presents the first comprehensive overview of Omni-MLLMs, which aim to achieve unified understanding and generation across multiple non-linguistic modalities. The paper introduces a detailed taxonomy based on encoding and alignment methods, covering four core components: multi-modalities encoding, alignment, interaction, and generation. It outlines the two-stage training process (alignment pre-training and instruction fine-tuning) and discusses training data construction and evaluation benchmarks. The survey identifies key challenges including efficient modality expansion, catastrophic forgetting mitigation, low-resource modality handling, and improving cross-modal capabilities like long-context processing and temporal alignment.

## Method Summary
The survey systematically categorizes Omni-MLLMs based on their encoding strategies (separate vs. unified encoders), alignment approaches (late, partial, or full fusion), and generation capabilities. It details a two-stage training methodology: first an alignment pre-training phase to learn cross-modal representations, followed by instruction fine-tuning for task-specific adaptation. The taxonomy organizes approaches by their treatment of the four core components and provides a framework for understanding how different models handle the complexity of multi-modal integration.

## Key Results
- Introduced first comprehensive taxonomy of Omni-MLLMs based on encoding and alignment methods
- Detailed two-stage training process (alignment pre-training and instruction fine-tuning)
- Identified key challenges including catastrophic forgetting, low-resource modalities, and cross-modal reasoning limitations
- Outlined future directions including real-time multi-modal interaction and world simulation applications

## Why This Works (Mechanism)
Omni-MLLMs work by unifying multiple non-linguistic modalities within a single model architecture through carefully designed encoding and alignment strategies. The two-stage training process first establishes cross-modal representations during alignment pre-training, then adapts these representations to specific tasks through instruction fine-tuning. This approach allows the model to develop shared understanding across modalities while maintaining task-specific capabilities.

## Foundational Learning
- **Multi-modal encoding strategies** - Why needed: Different modalities have different data structures requiring specialized processing; Quick check: Verify whether the approach uses separate or unified encoders for different modalities
- **Cross-modal alignment** - Why needed: To establish meaningful relationships between representations from different modalities; Quick check: Confirm the alignment occurs at feature level, token level, or both
- **Catastrophic forgetting mitigation** - Why needed: New modalities can overwrite previously learned representations; Quick check: Identify whether the approach uses regularization, rehearsal, or architectural solutions
- **Temporal alignment** - Why needed: For modalities with temporal components like audio and video; Quick check: Verify if the approach explicitly models temporal relationships between modalities
- **Instruction fine-tuning** - Why needed: To adapt pre-trained cross-modal representations to specific downstream tasks; Quick check: Confirm whether the approach uses task-specific or general instruction tuning

## Architecture Onboarding

**Component Map:** Input Modalities -> Encoding Stage -> Alignment Stage -> Interaction Stage -> Generation Stage -> Output

**Critical Path:** The most critical sequence is Encoding -> Alignment -> Interaction, as these stages establish the foundational cross-modal representations that enable generation capabilities.

**Design Tradeoffs:** Separate encoders provide modality-specific optimization but increase complexity, while unified encoders simplify architecture but may struggle with modality-specific nuances. Late fusion preserves modality independence but may miss cross-modal interactions, while early fusion enables stronger integration but risks modality interference.

**Failure Signatures:** Catastrophic forgetting manifests as degraded performance on previously learned modalities after training on new ones; poor cross-modal reasoning appears as inability to connect concepts across modalities; generation failures show as mode collapse or inconsistent outputs across modalities.

**First Experiments:**
1. Test cross-modal retrieval performance across all supported modalities
2. Evaluate generation consistency by prompting for outputs combining multiple modalities
3. Measure catastrophic forgetting by tracking performance on previously learned modalities after adding new ones

## Open Questions the Paper Calls Out
The survey does not explicitly identify specific open questions beyond those mentioned in the key outcome section regarding expanding modalities efficiently, mitigating catastrophic forgetting, handling low-resource modalities, and improving cross-modal capabilities.

## Limitations
- Taxonomy may become outdated as new architectures emerge that don't fit current framework
- Limited exploration of practical deployment challenges like latency and scalability
- No empirical comparisons between different Omni-MLLM approaches provided
- Some challenges identified (temporal alignment, world simulation) are speculative

## Confidence
- **Taxonomy completeness:** Medium - rapidly evolving field may produce new approaches
- **Training process description:** High - based on established literature and consistent with known architectures
- **Identified challenges:** Medium - some are speculative and depend on future research
- **Survey utility for beginners:** High - serves as comprehensive introduction
- **Utility for advanced practitioners:** Medium - limited empirical benchmarks and deployment considerations

## Next Checks
1. Conduct systematic review of post-survey literature to identify new Omni-MLLM architectures requiring taxonomy updates
2. Perform empirical benchmarking of key Omni-MLLM approaches across identified challenges
3. Evaluate scalability and latency of Omni-MLLMs in real-world deployment scenarios