---
ver: rpa2
title: Backward explanations via redefinition of predicates
arxiv_id: '2408.02606'
source_url: https://arxiv.org/abs/2408.02606
tags:
- state
- predicate
- agent
- action
- b-hxp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Backward-HXP method explains Reinforcement Learning agents
  by identifying important actions in long histories without approximating importance
  scores. Instead of forward analysis, it works backwards from the end of the history,
  iteratively studying subsequences and redefining intermediate predicates using locally-minimal
  Probabilistic Abductive eXplanations (PAXp).
---

# Backward explanations via redefinition of predicates

## Quick Facts
- arXiv ID: 2408.02606
- Source URL: https://arxiv.org/abs/2408.02606
- Reference count: 39
- Method explains RL agents by identifying important actions in long histories without approximating importance scores

## Executive Summary
This paper introduces Backward-HXP, a method for explaining Reinforcement Learning agents by identifying important actions in long histories without approximating importance scores. Unlike forward analysis methods that are #W[1]-hard, Backward-HXP works backwards from the end of the history, iteratively studying subsequences and redefining intermediate predicates using locally-minimal Probabilistic Abductive eXplanations (PAXp). The approach achieves computational tractability by reducing the search horizon from the full history length to a constant sub-sequence length, while still providing meaningful explanations for long histories.

## Method Summary
Backward-HXP is a method for explaining RL agent behavior that works by iteratively analyzing subsequences of a history in reverse order. For each subsequence, it identifies the most important action relative to achieving the current predicate, then uses a locally-minimal PAXp from that action's state to redefine the predicate for the next iteration. This process continues until reaching the beginning of the history or a utility threshold. The method relies on PAXpred to compute importance scores and findLmPAXp to generate locally-minimal PAXp approximations, achieving tractability by keeping the search horizon small while maintaining explanatory value.

## Key Results
- B-HXP successfully explains RL agent behavior in long histories without approximating importance scores
- The method is computationally tractable when search horizon l is kept small and predicates are sufficiently generic
- Experiments on Frozen Lake, Connect4, and Drone Coverage demonstrate meaningful explanations in reasonable computation time
- Intermediate predicates may become too specific to be useful for action evaluation, limiting practical applicability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backward-HXP avoids #W[1]-hardness by reducing the importance computation horizon from the full history length k to a constant sub-sequence length l.
- Mechanism: Instead of computing importance over all k steps at once, the algorithm iteratively examines short sub-sequences of length l (starting from the end), computes exact importance scores for these shorter horizons, then redefines the predicate for the next sub-sequence.
- Core assumption: The search horizon l is chosen small enough to make exhaustive importance score computation tractable, while still capturing meaningful action importance.
- Evidence anchors:
  - [abstract] "Instead of forward analysis, it works backwards from the end of the history, iteratively studying subsequences and redefining intermediate predicates using locally-minimal Probabilistic Abductive eXplanations (PAXp)."
  - [section] "The idea of B-HXP is to iteratively look for the most important action in the near past of the state that respects the predicate under study."
  - [corpus] Weak - the corpus doesn't directly address the computational complexity aspect of B-HXP.
- Break condition: If l is not small enough relative to the branching factor b, the computation becomes intractable again, defeating the purpose of the backward approach.

### Mechanism 2
- Claim: Locally-minimal PAXp (LmPAXp) generation provides a tractable approximation to full PAXp computation while maintaining explanatory value.
- Mechanism: Instead of computing all possible PAXp's (which could be exponential), the algorithm finds a locally-minimal one by iteratively removing features that don't break the PAXp property, using a #P-oracle to check weak PAXp membership.
- Core assumption: A single locally-minimal PAXp provides sufficient information to redefine the predicate in a way that preserves meaningful explanations.
- Evidence anchors:
  - [section] "One approximation to PAXpred is to calculate only one AXp (i.e. a PAXp with δ = 1). This reduces the problem to a more amenable co-NP problem... Instead, in order to provide sparser intermediate predicates, the considered approximation consists in computing one PAXp, or more precisely one locally-minimal PAXp."
  - [section] "Lemma 3. Given a state s, the computation of a locally-minimal PAXp is in FP #P (i.e. in polynomial time using a #P-oracle) and determining whether a given subset of features is a locally-minimal PAXp is #P-hard."
  - [corpus] Weak - corpus papers don't discuss PAXp or its computational properties.
- Break condition: If the locally-minimal PAXp is too specific (as seen in experiments), the resulting predicate may not generalize well and provide poor explanations.

### Mechanism 3
- Claim: The backward iterative process creates a chain of increasingly proximate causes that explain how the final predicate state was reached.
- Mechanism: Starting from the final state, the algorithm identifies the most important action, extracts a PAXp from its associated state, uses this to redefine the predicate, then repeats the process on the preceding sub-sequence, creating a backward chain of causes.
- Core assumption: The most important action in each sub-sequence is meaningfully connected to achieving the current predicate, and this chain of actions forms a coherent explanation.
- Evidence anchors:
  - [section] "The idea of B-HXP is to iteratively look for the most important action in the near past of the state that respects the predicate under study... When an important action is found, we look at its associated state s to define the new predicate to be studied."
  - [section] "Example 1 (Cont). For this history, δ is set to 1 and l is 4. The first sub-sequence studied is: [nap, eat, water the plants, read]. The most important action relative to the achievement of 'Bob is not hungry' is 'eat'... We extract a PAXp, which includes the features fridge and hungry set to 1... Now, we study the sub-sequence [work, shop, watch TV , nap], in which the most important action to achieve the new intermediate predicate is 'shop'."
  - [corpus] Weak - corpus papers don't discuss backward causal chains in RL explanation.
- Break condition: If the predicate becomes too specific at each step, the causal chain may lose interpretability and fail to provide useful explanations.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and their components (states, actions, transition functions, policies)
  - Why needed here: The entire explanation framework operates on MDPs, requiring understanding of how agents interact with environments through states and actions
  - Quick check question: In an MDP, what is the relationship between the transition function p and the policy π when computing the next state?

- Concept: Abductive reasoning and probabilistic explanations
  - Why needed here: PAXp is fundamentally an abductive explanation method that identifies feature subsets sufficient to explain a classifier's behavior with certain probability
  - Quick check question: How does a PAXp differ from a traditional explanation in terms of what it guarantees about the feature subset?

- Concept: Computational complexity classes (#P, #W[1], FPT)
  - Why needed here: Understanding why forward HXP is intractable and how B-HXP achieves tractability requires familiarity with these complexity classes and their relationships
  - Quick check question: What is the key difference between a problem being in #P versus #W[1]-hard, and why does this matter for the backward approach?

## Architecture Onboarding

- Component map: MDP model (states, actions, transition function, policy) -> Predicate language (boolean formulas over state features) -> Backward iteration (sub-sequence selection -> Importance computation -> PAXp generation -> Predicate redefinition) -> Explanation output formatter

- Critical path: MDP → Predicate → Backward iteration (sub-sequence selection → Importance computation → PAXp generation → Predicate redefinition) → Explanation

- Design tradeoffs:
  - Horizon length l vs. computational tractability vs. explanatory completeness
  - δ threshold vs. predicate specificity vs. action evaluation quality
  - Sampling size vs. predicate accuracy vs. computation time
  - Single PAXp vs. multiple PAXp's vs. computational complexity

- Failure signatures:
  - Importance scores close to zero for all actions (predicate too specific)
  - Exponential growth in computation time (l too large or b too high)
  - Uninterpretable predicates (δ too low or feature ordering poor)
  - Incomplete explanations (utility threshold reached too early)

- First 3 experiments:
  1. Implement on a simple deterministic MDP (like Frozen Lake) with known optimal policy to verify importance score correctness for small histories
  2. Test PAXp generation on a binary feature space with known perfect matchings to verify Lemma 3 reduction
  3. Run full B-HXP on Connect4 with varying l and δ values to observe tradeoff between explanation quality and computation time

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Intermediate predicates often become too specific (δ close to 1), making importance scores near zero and explanations meaningless
- Computational complexity remains high for large feature spaces despite avoiding #W[1]-hardness
- Method's effectiveness depends heavily on predicate genericity, which may not hold for many real-world applications

## Confidence
- Mechanism 1: Medium - While the backward approach clearly reduces the horizon for importance computation, the paper doesn't fully establish how the choice of l balances tractability against explanatory completeness
- Mechanism 2: Medium - The locally-minimal PAXp approximation is computationally justified, but the paper doesn't thoroughly explore how this approximation affects explanation quality
- Mechanism 3: Low-Medium - The backward causal chain concept is intuitive but the paper shows that intermediate predicates can become too specific to be useful

## Next Checks
1. Implement both forward and backward methods on problems with varying history lengths and branching factors to empirically verify the complexity reduction claims
2. Systematically vary δ thresholds and measure their impact on both predicate specificity and explanation quality across multiple domains
3. Compare the locally-minimal PAXp approach against other PAXp approximations to evaluate the trade-off between computational efficiency and explanation quality