---
ver: rpa2
title: Clinical Reasoning over Tabular Data and Text with Bayesian Networks
arxiv_id: '2403.09481'
source_url: https://arxiv.org/abs/2403.09481
tags:
- text
- clinical
- symptoms
- data
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies how to integrate neural text representations
  into Bayesian networks (BNs) for clinical reasoning over both structured tabular
  data and unstructured clinical text. The authors compare two approaches: a generative
  model that fits Gaussian distributions to text embeddings conditioned on the BN
  variables, and a discriminative model that uses separate neural classifiers for
  each variable given the text.'
---

# Clinical Reasoning over Tabular Data and Text with Bayesian Networks

## Quick Facts
- arXiv ID: 2403.09481
- Source URL: https://arxiv.org/abs/2403.09481
- Reference count: 40
- One-line primary result: Discriminative Bayesian networks with neural text integration outperform generative approaches for clinical reasoning over tabular and textual data.

## Executive Summary
This paper investigates methods for integrating neural text representations into Bayesian networks (BNs) for clinical reasoning tasks that combine structured tabular data with unstructured clinical text. The authors propose and compare two approaches: a generative model that fits Gaussian distributions to text embeddings conditioned on BN variables, and a discriminative model that uses separate neural classifiers for each variable given the text. Using a simulated pneumonia diagnosis use case, they demonstrate that including clinical notes significantly improves diagnostic accuracy, especially for rare conditions. The discriminative approach shows superior performance due to its modular architecture and better fit to text embedding distributions.

## Method Summary
The study implements two Bayesian network models that incorporate clinical text: BN-gen-text uses a generative approach fitting multivariate Gaussian distributions to text embeddings conditioned on symptoms and diagnoses, while BN-discr-text employs a discriminative approach with separate neural classifiers for each symptom and diagnosis given the text embedding. Both models are trained on a simulated pneumonia dataset where clinical notes are generated using GPT-3.5-turbo. The discriminative model uses BioLORD embeddings with two-layer MLPs for classification, while the generative model learns conditional distributions of embeddings. The methods are evaluated using average precision for pneumonia diagnosis under different evidence conditions, comparing performance with and without text integration.

## Key Results
- BN with text discriminator achieves 0.75 average precision for pneumonia diagnosis when using symptoms and text, compared to 0.09 for a BN without text
- The discriminative approach outperforms the generative one due to its modular architecture and better fit to text embeddings
- Text integration significantly improves diagnostic accuracy for rare conditions, particularly when symptoms are partially observed

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using a Bayesian network to model the joint distribution of symptoms, diagnoses, and text allows modular inference that can integrate structured and unstructured data.
- **Mechanism:** The BN structure defines conditional independence assumptions, enabling separate modeling of each variable given its parents. Text is incorporated either as a generative node (modeling text embeddings conditioned on symptoms/diagnoses) or as a discriminative node (modeling symptoms/diagnoses conditioned on text embeddings).
- **Core assumption:** The conditional independence structure encoded in the BN accurately reflects the true data-generating process, and text embeddings capture sufficient information about the underlying symptoms and diagnoses.
- **Evidence anchors:**
  - [abstract] "This paper compares and discusses strategies to augment Bayesian networks with neural text representations, both in a generative and discriminative manner."
  - [section] "The discriminative model for integrating text nodes into BNs in this paper corresponds to the neural predicates approach, since a BN can be seen as a special case of a probabilistic program."
  - [corpus] Weak evidence - the corpus neighbors discuss related topics (e.g., information extraction from EHRs, multimodal integration) but do not directly address BN integration with text.

### Mechanism 2
- **Claim:** Using separate neural classifiers for each symptom and diagnosis conditioned on text embeddings allows flexible modeling of the text-to-variable mapping without strong distributional assumptions.
- **Mechanism:** Each conditional distribution is modeled by a neural network that takes the text embedding as input and outputs a probability for the child variable given the parent values. This allows the model to learn complex non-linear relationships between text and variables.
- **Core assumption:** The text embedding space is sufficiently rich to capture the relevant information about symptoms and diagnoses, and the neural classifiers can learn the mapping from text to variables.
- **Evidence anchors:**
  - [abstract] "This paper compares and discusses strategies to augment Bayesian networks with neural text representations, both in a generative and discriminative manner."
  - [section] "The BN-discr-text model can benefit from the flexibility of the neural classifiers without requiring any assumptions on the distribution of the text embeddings."
  - [corpus] Weak evidence - the corpus neighbors discuss related topics (e.g., automated annotation of clinical text, multimodal integration) but do not directly address neural classifiers for BN integration.

### Mechanism 3
- **Claim:** Masking out certain symptoms in a subset of the training data forces the model to learn to extract information about those symptoms from the text, simulating the real-world scenario where not all symptoms are explicitly recorded.
- **Mechanism:** During training, the symptoms "fever" and "pain" are never observed in the tabular data, but are mentioned in the text. The model must learn to extract information about these symptoms from the text in order to accurately predict the diagnoses.
- **Core assumption:** The text contains sufficient information about the masked symptoms, and the model can learn to extract this information.
- **Evidence anchors:**
  - [abstract] "The discriminative approach outperforms the generative one due to its modular architecture and better fit to text embeddings."
  - [section] "By retaining the raw text and training the model to deal with this, the information extraction step is no longer required."
  - [corpus] Weak evidence - the corpus neighbors discuss related topics (e.g., patient-level information extraction, structured and unstructured medical records) but do not directly address the masking strategy.

## Foundational Learning

- **Concept: Bayesian networks**
  - Why needed here: BNs provide a principled way to model the joint distribution of symptoms, diagnoses, and text, and enable modular inference.
  - Quick check question: What is the key advantage of using a BN over a discriminative model for clinical reasoning?

- **Concept: Text embeddings**
  - Why needed here: Text embeddings provide a dense vector representation of the clinical notes that can be used as input to the neural classifiers.
  - Quick check question: What is the advantage of using a pre-trained language model like BioLORD for generating text embeddings?

- **Concept: Neural classifiers**
  - Why needed here: Neural classifiers provide a flexible way to model the mapping from text embeddings to symptoms and diagnoses, without requiring strong distributional assumptions.
  - Quick check question: What is the advantage of using separate neural classifiers for each symptom and diagnosis, rather than a single classifier?

## Architecture Onboarding

- **Component map:** Bayesian network -> Text embeddings -> Neural classifiers -> Inference engine
- **Critical path:**
  1. Generate text embeddings from clinical notes
  2. Feed text embeddings into neural classifiers
  3. Use classifier outputs as evidence in BN inference
  4. Compute posterior probabilities of diagnoses
- **Design tradeoffs:**
  - Generative vs. discriminative modeling of text: generative is more interpretable but requires strong distributional assumptions, discriminative is more flexible but less interpretable
  - Number of neural classifiers: more classifiers allows more flexibility but increases model complexity
  - Masking strategy: more aggressive masking forces more reliance on text but may hurt performance if text is not informative enough
- **Failure signatures:**
  - Poor performance on rare conditions: suggests the model is not effectively leveraging text information
  - Large discrepancy between generative and discriminative models: suggests the distributional assumptions of the generative model are not well-suited to the text embedding space
  - Sensitivity to hyperparameter choices: suggests the model may be overfitting or underfitting
- **First 3 experiments:**
  1. Compare performance of generative vs. discriminative text models on a held-out test set
  2. Vary the masking rate of symptoms in the training data and observe the impact on performance
  3. Visualize the learned text embeddings and classifier weights to gain insights into what information is being captured

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- The evaluation relies on a single simulated dataset, constraining generalizability to real clinical settings
- The generative text model's assumptions about Gaussian distributions in embedding space may not hold for diverse clinical text corpora
- The study does not explore robustness to text quality variations or domain shifts in real clinical settings

## Confidence
- **High Confidence:** The comparative performance advantage of discriminative over generative text integration is well-supported by the experimental results across multiple conditions.
- **Medium Confidence:** The general claim that text improves diagnostic accuracy for rare conditions, as this is demonstrated in a controlled simulation but not validated on real clinical data.
- **Medium Confidence:** The assertion that modular BN architecture with neural text integration enables better handling of partial observations, though the masking experiments provide supporting evidence.

## Next Checks
1. Test model performance on real clinical datasets with naturally occurring missing data to validate simulation results and assess robustness to text quality variations.
2. Conduct ablation studies comparing performance with different text embedding approaches (beyond BioLORD) to determine sensitivity to embedding quality and domain specificity.
3. Evaluate computational efficiency and inference latency differences between generative and discriminative approaches under realistic clinical workflow constraints.