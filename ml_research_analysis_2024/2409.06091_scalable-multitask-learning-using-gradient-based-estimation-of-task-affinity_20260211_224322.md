---
ver: rpa2
title: Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity
arxiv_id: '2409.06091'
source_url: https://arxiv.org/abs/2409.06091
tags:
- task
- tasks
- affinity
- learning
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Grad-TAG, an efficient algorithm for multitask
  learning that dramatically reduces the computational cost of estimating task affinity
  scores. The key innovation is Grad-TAE, which uses gradient-based linearization
  and random projection to approximate task affinities without full model training.
---

# Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity

## Quick Facts
- arXiv ID: 2409.06091
- Source URL: https://arxiv.org/abs/2409.06091
- Authors: Dongyue Li; Aneesh Sharma; Hongyang R. Zhang
- Reference count: 40
- Primary result: Achieves 97% reduction in computational cost for multitask learning through gradient-based task affinity estimation

## Executive Summary
This paper addresses the computational bottleneck in multitask learning by introducing Grad-TAG, a method that estimates task affinities without full model training. The approach uses gradient-based linearization and random projection to approximate how tasks interact, enabling efficient task grouping and learning. By leveraging a pretrained meta-initialization and applying logistic regression on projected gradients, Grad-TAG can estimate model parameters for specific task combinations with minimal computation.

The authors demonstrate that their method achieves task affinity estimates within 2.7% of true values while using only 3% of the FLOPs required for traditional full training. On a large-scale graph with 500 tasks, Grad-TAG estimates affinities within 5% accuracy using just 112 GPU hours compared to over 8000 hours for full training. The method consistently achieves Pareto-optimal tradeoffs between accuracy and computational cost across seven benchmark datasets.

## Method Summary
Grad-TAG introduces Grad-TAE, a gradient-based task affinity estimator that linearizes the relationship between task gradients and uses random projection to reduce dimensionality. The method starts with a pretrained meta-initialization and applies logistic regression on projected gradients to estimate model parameters for specific task combinations. For task grouping, the authors propose an SDP-based clustering algorithm that maximizes average cluster density. The approach leverages the fact that task gradients contain sufficient information about task relationships, allowing accurate affinity estimation without expensive full-model training.

## Key Results
- Achieves task affinity estimates within 2.7% of true values using only 3% of traditional training FLOPs
- On 500-task graph, estimates affinities within 5% accuracy using 112 GPU hours vs 8000+ for full training
- Consistently achieves Pareto-optimal tradeoffs between accuracy and computation across seven benchmark datasets

## Why This Works (Mechanism)
The method exploits the fact that task gradients encode rich information about task relationships. By linearizing the gradient-task affinity relationship and projecting onto lower dimensions, Grad-TAE can approximate these relationships efficiently. The pretrained meta-initialization provides a good starting point, reducing the need for extensive training. The SDP-based clustering algorithm then groups tasks based on these affinity estimates, enabling effective multitask learning with minimal computation.

## Foundational Learning
- **Gradient-based linearization**: Approximates non-linear task relationships through linear models of gradients. Why needed: Enables efficient computation by avoiding full model training. Quick check: Compare linearized vs non-linear affinity estimates on controlled datasets.
- **Random projection**: Reduces dimensionality of gradient space while preserving task relationships. Why needed: Makes affinity estimation computationally tractable for high-dimensional tasks. Quick check: Measure approximation accuracy vs projection dimension.
- **Meta-initialization**: Uses pretrained models as starting points for task affinity estimation. Why needed: Provides good initialization that reduces the need for extensive training. Quick check: Compare performance with random initialization.

## Architecture Onboarding
**Component map**: Input tasks -> Gradient computation -> Random projection -> Logistic regression -> Affinity matrix -> SDP clustering -> Task grouping -> Multitask learning

**Critical path**: Gradient computation → Random projection → Affinity estimation → Task grouping → Model training

**Design tradeoffs**: 
- Accuracy vs computational cost through projection dimension selection
- Linearization accuracy vs efficiency gains
- Cluster size vs task diversity within groups

**Failure signatures**: 
- Poor affinity estimates when task gradients are highly non-linear
- Suboptimal task grouping when tasks have complex interdependencies
- Reduced performance when meta-initialization is poorly aligned with target tasks

**3 first experiments**:
1. Verify gradient linearization accuracy on simple linear and non-linear task relationships
2. Test random projection preservation of task relationships across different dimensions
3. Validate SDP clustering effectiveness on synthetic task affinity matrices

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance scaling with dataset size, task diversity, and gradient dimensionality needs comprehensive evaluation
- Comparison with MetaMTL limited to single metric (AP), limiting generalizability assessment
- SDP-based clustering may face scalability issues for extremely large task spaces not explored in experiments
- Linearization assumption may break down for highly non-linear task relationships

## Confidence
**High confidence**: The core Grad-TAE approximation methodology and its computational efficiency gains are well-supported by presented evidence and theoretical framework.

**Medium confidence**: The effectiveness of the SDP-based clustering algorithm for task grouping is demonstrated but limited to specific datasets tested.

**Medium confidence**: The claim of achieving Pareto-optimal tradeoffs compared to existing methods is supported but based on limited comparisons.

## Next Checks
1. Perform systematic scaling experiments to evaluate Grad-TAE performance across datasets of varying sizes (10^3 to 10^6 samples) and task dimensionalities to identify break points in approximation accuracy.

2. Conduct ablation studies isolating the contributions of gradient linearization, random projection, and meta-initialization to quantify which components drive computational savings.

3. Test the method's robustness to task heterogeneity by creating synthetic datasets with controlled task affinity structures and measuring recovery accuracy across different correlation patterns.