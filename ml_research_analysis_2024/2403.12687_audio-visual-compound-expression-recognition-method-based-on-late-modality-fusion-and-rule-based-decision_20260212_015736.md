---
ver: rpa2
title: Audio-Visual Compound Expression Recognition Method based on Late Modality
  Fusion and Rule-based Decision
arxiv_id: '2403.12687'
source_url: https://arxiv.org/abs/2403.12687
tags:
- recognition
- surprised
- emotion
- emotions
- basic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a zero-shot audio-visual method for compound
  expression recognition (CER) that combines three models (static visual, dynamic
  visual, and audio) to predict emotion probabilities for basic emotions, which are
  then fused hierarchically and processed with two rules to determine compound expressions.
  The method achieves an F1-score of 22.01% on the C-EXPR-DB test subset without using
  any training data specific to the target task.
---

# Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision

## Quick Facts
- **arXiv ID**: 2403.12687
- **Source URL**: https://arxiv.org/abs/2403.12687
- **Reference count**: 36
- **Primary result**: Zero-shot compound expression recognition achieves 22.01% F1-score on C-EXPR-DB test subset

## Executive Summary
This paper presents a zero-shot method for compound expression recognition (CER) that combines three pre-trained models—static visual, dynamic visual, and audio—to predict basic emotion probabilities, which are then hierarchically fused and processed with rule-based decision making. The approach operates without any training data specific to the target task, instead relying on pre-trained models for basic emotion recognition and a two-rule system to derive compound expressions. The method demonstrates that different modalities are responsible for predicting specific compound expressions, with audio models handling Angry Surprised and Sadly Angry, static visual models predicting Happily Surprised, and dynamic visual models covering other expressions.

## Method Summary
The proposed method employs a hierarchical late modality fusion approach where three independent models (static visual, dynamic visual, and audio) predict probabilities for basic emotions. These predictions are first combined using a max rule within each modality, then fused across modalities using a weighted sum. A two-rule system processes these fused probabilities: Rule 1 identifies primary emotions and potential compound expressions, while Rule 2 resolves conflicts between predicted compound expressions. The entire pipeline operates in a zero-shot manner, requiring no training on the target compound expression dataset.

## Key Results
- Achieves F1-score of 22.01% on C-EXPR-DB test subset without task-specific training
- Audio model predicts Angry Surprised and Sadly Angry compound expressions
- Static visual model predicts Happily Surprised compound expressions
- Dynamic visual model predicts other compound expressions

## Why This Works (Mechanism)
The method leverages the complementary strengths of different modalities in capturing emotional expressions. Audio models excel at detecting anger-related emotions due to acoustic features like pitch and intensity, while visual models capture facial expressions and dynamic movements associated with surprise and happiness. The hierarchical fusion approach allows each modality to contribute according to its strengths, while the rule-based system provides interpretable decision-making for combining basic emotions into compound expressions.

## Foundational Learning
- **Zero-shot learning**: Learning without task-specific training data, needed to demonstrate generalization capabilities; quick check: verify no training on target dataset
- **Late modality fusion**: Combining predictions after individual modality processing, needed to preserve modality-specific information; quick check: confirm fusion occurs post-prediction
- **Hierarchical fusion**: Multi-level combination of predictions, needed for structured decision-making; quick check: verify two-stage fusion process
- **Rule-based decision systems**: Using predefined rules for complex decisions, needed for interpretable compound expression derivation; quick check: examine rule logic and application
- **Compound emotion recognition**: Identifying combinations of basic emotions, needed for realistic emotional expression modeling; quick check: validate compound expression definitions
- **Modality complementarity**: Leveraging different data types for enhanced recognition, needed to maximize information capture; quick check: assess modality-specific performance

## Architecture Onboarding

**Component Map**: Static Visual Model -> Dynamic Visual Model -> Audio Model -> Max Rule Fusion -> Weighted Sum Fusion -> Rule 1 -> Rule 2 -> Compound Expression Output

**Critical Path**: The core pipeline flows from three pre-trained models through two-stage fusion (max rule then weighted sum) followed by two sequential rule applications to produce final compound expression predictions.

**Design Tradeoffs**: The zero-shot approach sacrifices potential performance gains from task-specific fine-tuning in exchange for broad generalizability and reduced dependency on labeled compound expression data. The rule-based system trades flexibility for interpretability and ease of implementation.

**Failure Signatures**: Performance degradation may occur when basic emotion predictions are inaccurate, when modality fusion weights are suboptimal, or when compound expressions don't align well with the two-rule system's logic. The modest F1-score suggests limitations in handling complex or subtle compound expressions.

**First Experiments**:
1. Evaluate individual modality performance on basic emotion recognition to identify strengths and weaknesses
2. Test fusion strategy with different weighting schemes to optimize compound expression prediction
3. Apply the method to additional compound expression datasets to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Modest F1-score of 22.01% indicates limited effectiveness for complex compound expressions
- Reliance on pre-trained models without fine-tuning may constrain performance
- Rule-based system may not capture all possible emotional combinations or transitions
- Single dataset evaluation limits assessment of cross-dataset generalization

## Confidence
- **High confidence**: The methodological framework for combining static visual, dynamic visual, and audio models is clearly described and implemented as stated
- **Medium confidence**: The assignment of specific compound expressions to particular models (audio for Angry Surprised and Sadly Angry, static visual for Happily Surprised, dynamic visual for others) appears consistent with the experimental setup but may not generalize to all expression types
- **Medium confidence**: The zero-shot learning claim is valid given no training on the target task, though the reliance on pre-trained models for basic emotion recognition introduces indirect dependencies

## Next Checks
1. Test the method on additional audio-visual emotion datasets (e.g., AFEW-VA, OMG-Emotion) to evaluate cross-dataset performance and generalizability
2. Conduct ablation studies to quantify the contribution of each model component (static visual, dynamic visual, audio) and each fusion rule to overall performance
3. Implement a human evaluation study comparing model predictions against expert annotations to assess the practical utility of predicted compound expressions