---
ver: rpa2
title: '4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities'
arxiv_id: '2406.09406'
source_url: https://arxiv.org/abs/2406.09406
tags:
- modalities
- m-21
- input
- multimodal
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper expands upon the capabilities of multimodal multitask
  foundation models by training a single model on tens of highly diverse modalities,
  including semantic and geometric modalities, feature maps from recent state-of-the-art
  models, pseudo labels of specialist models, and new modalities like image metadata
  and color palettes. The authors achieve this by performing discrete tokenization
  on various modalities using modality-specific tokenizers and training a three billion
  parameter model on large-scale multimodal datasets and text corpora.
---

# 4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities

## Quick Facts
- arXiv ID: 2406.09406
- Source URL: https://arxiv.org/abs/2406.09406
- Reference count: 40
- This paper trains a single 3B parameter model on 21 diverse modalities, demonstrating it can solve 3x more tasks than existing models without performance degradation.

## Executive Summary
This paper presents 4M-21, a unified multimodal multitask foundation model that expands upon the capabilities of existing models by training on tens of highly diverse modalities including semantic, geometric, feature maps, pseudo labels, and new modalities like image metadata and color palettes. The authors achieve this through discrete tokenization of various modalities using modality-specific tokenizers, allowing a single transformer to perform any-to-any prediction across tasks without task-specific heads. The resulting model demonstrates strong out-of-the-box performance, often matching or outperforming specialist baselines while being a single model for all tasks.

## Method Summary
The 4M-21 model is trained using discrete tokenization to convert diverse modalities into sequences of tokens, which are then processed by a 3B parameter transformer encoder-decoder. The model is co-trained on large-scale multimodal datasets (CC12M, COYO700M) and text corpora (C4) using a diversified multimodal masking strategy. Modality-specific tokenizers (ViT-based VQ-VAE for image-like modalities, MLP-based VAEs for non-spatial modalities, WordPiece for text) convert inputs into discrete tokens. The transformer learns cross-modal representations through masked modeling, enabling any-to-any prediction where any subset of modalities can predict any other modality.

## Key Results
- 4M-21 demonstrates the possibility of training a single model on 21 diverse modalities without performance degradation
- The model achieves strong out-of-the-box performance, often matching or outperforming specialist baselines
- Enables multimodal generation, retrieval, and fine-grained control with cross-modal capabilities

## Why This Works (Mechanism)

### Mechanism 1
Discrete tokenization of diverse modalities into sequences of tokens allows a single transformer to perform any-to-any prediction across tasks without task-specific heads. By mapping each modality to a fixed-length sequence of discrete tokens using modality-specific VQ-VAE or MLP-based encoders, all inputs and outputs are unified into the same representation space. The transformer learns a shared cross-modal latent space through masked modeling, enabling it to predict any modality from any subset of others.

### Mechanism 2
Co-training on large-scale multimodal datasets with a diversified multimodal masking strategy enables the model to learn rich cross-modal representations without overfitting. By randomly sampling from a mixture of datasets and applying a diversified mixture of masking strategies (all-to-all, RGB-to-all, caption-biased), the model is exposed to a wide variety of modality combinations and contexts, preventing overfitting and encouraging learning of general cross-modal relationships.

### Mechanism 3
Scaling the model size to 3 billion parameters and the dataset to 0.5 billion samples enables the model to learn detailed representations for a large number of diverse modalities without a loss in performance. The increased model capacity and training data allow the model to maintain performance even as the number of modalities increases, providing sufficient parameters to learn complex cross-modal relationships and enough examples to learn from.

## Foundational Learning

- **Discrete tokenization and quantization (e.g., VQ-VAE, MLP-based VAEs)**: Needed to convert diverse modalities into a unified sequence of discrete tokens that can be processed by a transformer. Quick check: What is the difference between a VQ-VAE and an MLP-based VAE, and when would you use each for modality tokenization?

- **Masked multimodal modeling**: Needed to train the transformer to predict any modality from any subset of other modalities, enabling any-to-any prediction. Quick check: How does the choice of masking strategy (e.g., random vs. span masking) and the Dirichlet parameter Î± affect the learned cross-modal representations?

- **Co-training on multiple datasets with different modality compositions**: Needed to expose the model to a wide variety of modality combinations and contexts, preventing overfitting and encouraging learning of general cross-modal relationships. Quick check: How do you determine the optimal sampling weights for each dataset in the mixture, and how does this affect the learned representations?

## Architecture Onboarding

- **Component map**: Modality-specific tokenizers (ViT-based VQ-VAE for image-like modalities, MLP-based VAEs for non-spatial modalities, WordPiece for text) -> Transformer encoder-decoder with modality and positional embeddings -> Masking strategy (multimodal random and span masking with Dirichlet sampling) -> Dataset mixture (CC12M, COYO700M, C4) with sampling weights

- **Critical path**: 1. Tokenize all modalities in the dataset using modality-specific tokenizers 2. Sample a batch from the dataset mixture according to sampling weights 3. Apply the diversified masking strategy to the batch 4. Feed the masked input to the transformer and predict the target tokens 5. Compute the cross-entropy loss and update the model parameters

- **Design tradeoffs**: Tokenization: Higher vocabulary size and token count improve reconstruction quality but increase computational cost and memory usage; Masking strategy: More diverse masking strategies improve generalization but may slow down training convergence; Dataset mixture: More diverse datasets improve cross-modal learning but may introduce distribution shifts and require careful sampling weight tuning

- **Failure signatures**: Degraded performance on specific modalities indicates insufficient tokenization quality or model capacity for that modality; Overfitting to specific modality combinations indicates insufficient diversity in the masking strategy or dataset mixture; Slow convergence or unstable training indicates suboptimal hyperparameters or insufficient model capacity

- **First 3 experiments**: 1. Train a small model (100M parameters) on a subset of modalities (RGB, depth, normals) with a simple masking strategy to validate basic any-to-any prediction capability 2. Ablate the tokenization approach for SAM instances to find optimal tokenizer architecture and hyperparameters 3. Scale up model size and dataset size incrementally while monitoring performance on a held-out validation set to find optimal scaling point

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of 4M-21 scale with an even larger number of modalities beyond the current 21? The paper mentions that 4M-21 is trained on 21 diverse modalities and demonstrates strong performance without degradation, but does not explore the limits of how many modalities can be effectively incorporated.

### Open Question 2
What is the impact of using different tokenization schemes for various modalities on the overall performance of 4M-21? While the paper discusses using different tokenization approaches for different modalities and provides an ablation study on SAM instances, it does not extensively compare the impact of these choices on overall model performance across all modalities.

### Open Question 3
How does 4M-21 perform on tasks that are not explicitly included in its training modalities? The paper demonstrates good performance on out-of-the-box tasks like surface normal estimation, depth estimation, and semantic segmentation, but does not explore the model's performance on tasks that are not part of its training modalities.

## Limitations

- The claim of handling "3x more tasks/modalities than existing models" is difficult to verify given lack of standardized benchmarks for multimodal multitask models across such diverse modalities
- Limited empirical validation of information loss across the 21 diverse modalities, with no quantitative measurements of tokenization reconstruction quality across all modalities
- No systematic evaluation of zero-shot transfer to truly novel modality combinations or tasks not seen during training

## Confidence

- **High Confidence**: The basic architectural approach of using modality-specific tokenizers with a transformer decoder is sound and well-supported by prior work in multimodal modeling
- **Medium Confidence**: The specific scaling results (3B parameters, 0.5B samples) and performance claims are supported by reported experiments but lack sufficient ablation studies
- **Low Confidence**: The claim of handling "3x more tasks/modalities than existing models" is difficult to verify given the lack of standardized benchmarks

## Next Checks

1. **Tokenization Quality Audit**: Conduct systematic reconstruction quality evaluation for each modality (PSNR/SSIM for images, IoU for segmentation, L2 distance for features) across the full vocabulary range to identify potential information bottlenecks.

2. **Scaling Sensitivity Analysis**: Replicate the scaling experiments with controlled variations - first increase modalities while holding model size constant, then increase model size while holding modality count constant, to isolate the contributions of each factor.

3. **Cross-Modal Transfer Benchmark**: Design and execute a systematic evaluation of zero-shot transfer to novel modality combinations not present in the training data, measuring performance degradation as a function of modality novelty and combination complexity.