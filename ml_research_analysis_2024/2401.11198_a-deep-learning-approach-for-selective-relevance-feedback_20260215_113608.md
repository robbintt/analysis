---
ver: rpa2
title: A Deep Learning Approach for Selective Relevance Feedback
arxiv_id: '2401.11198'
source_url: https://arxiv.org/abs/2401.11198
tags:
- query
- feedback
- retrieval
- queries
- relevance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of pseudo-relevance feedback (PRF)
  drift, where expanding queries using top-ranked documents can hurt retrieval effectiveness
  for some queries. The authors propose a supervised, data-driven approach for selective
  PRF that uses a transformer-based bi-encoder architecture to learn whether to apply
  PRF for a given query.
---

# A Deep Learning Approach for Selective Relevance Feedback

## Quick Facts
- arXiv ID: 2401.11198
- Source URL: https://arxiv.org/abs/2401.11198
- Reference count: 40
- This paper proposes a supervised, data-driven approach for selective pseudo-relevance feedback (PRF) that uses a transformer-based bi-encoder architecture to predict whether applying PRF will improve retrieval effectiveness for a given query.

## Executive Summary
This paper addresses the problem of pseudo-relevance feedback (PRF) drift, where expanding queries using top-ranked documents can hurt retrieval effectiveness for some queries. The authors propose a supervised, data-driven approach for selective PRF that uses a transformer-based bi-encoder architecture to learn whether to apply PRF for a given query. The model predicts the usefulness of PRF by encoding interactions between the original and expanded queries with their respective top-retrieved documents. Additionally, the authors use the model's confidence estimates to combine results from original and expanded queries via rank fusion. Experiments show consistent improvements in retrieval effectiveness across multiple ranking and feedback models (sparse, dense, and generative) on the MS MARCO passage collection using TREC DL topics.

## Method Summary
The proposed approach uses a transformer-based bi-encoder architecture (BERT + LSTM) to encode interactions between original and expanded queries with their respective top-retrieved documents. The model is trained on ground-truth labels indicating whether PRF improves average precision compared to the original query. During inference, the model predicts whether to apply PRF for a given query, and the confidence estimates are used for rank fusion when combining results from original and expanded queries. The approach is evaluated on the MS MARCO passage collection using TREC DL topic sets.

## Key Results
- The selective PRF approach achieves up to 80% accuracy in PRF decisions
- Consistent improvements in retrieval effectiveness across multiple ranking and feedback models (sparse, dense, and generative)
- The approach approaches oracle performance on TREC DL topics
- The decision function trained on one PRF model generalizes well to other PRF models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A data-driven supervised model can predict whether PRF will improve retrieval effectiveness for a specific query.
- Mechanism: The model encodes interactions between the original query, its top-retrieved documents, the expanded query, and its top-retrieved documents using a transformer-based bi-encoder architecture. It then learns a decision function to predict the usefulness of PRF based on these encodings.
- Core assumption: The relevance assessments from training queries can be used to learn a generalizable decision function that captures when PRF is beneficial.
- Evidence anchors:
  - [abstract]: "The model predicts the usefulness of PRF by encoding interactions between the original and expanded queries with their respective top-retrieved documents."
  - [section 3.1]: "The training process itself makes use of a set of queries Qtrain for which ground-truth indicator labels are computed by evaluating the relative effectiveness obtained with the original query vs. the enriched query with the help of available relevance assessments."
  - [corpus]: Weak evidence. No directly relevant papers in the corpus neighbors list explicitly discuss supervised selective PRF.

### Mechanism 2
- Claim: Using the model's confidence estimates to combine results from original and expanded queries via rank fusion can further improve retrieval effectiveness.
- Mechanism: The predicted value of the decision function, which represents the probability of applying PRF, is used as weights to fuse the two different ranked lists. This allows for a soft combination of the initial and feedback lists, with more weight given to the list that the model is more confident in.
- Core assumption: The model's confidence estimates are correlated with the actual effectiveness gain from applying PRF, and combining the lists based on these estimates can lead to better results than a hard selection.
- Evidence anchors:
  - [abstract]: "Additionally, the authors use the model's confidence estimates to combine results from original and expanded queries via rank fusion."
  - [section 3.3]: "We propose a rank-fusion based method, where the fusion weights are obtained from the predictions of the PRF decision model Î¸(Q)."
  - [corpus]: Weak evidence. No directly relevant papers in the corpus neighbors list explicitly discuss confidence-based rank fusion for selective PRF.

### Mechanism 3
- Claim: The selective PRF decision function trained on one PRF model can generalize well to other PRF models.
- Mechanism: The decision function learns to identify queries that are amenable to PRF, regardless of the specific PRF model used. This is because the underlying characteristics of queries that benefit from PRF (e.g., underspecified information needs) are likely to be consistent across different PRF approaches.
- Core assumption: The queries that improve with one PRF model (e.g., RLM) are likely to improve with other PRF models (e.g., GRF, ColBERT-PRF) as well, as the decision function captures the fundamental query characteristics that make PRF beneficial.
- Evidence anchors:
  - [abstract]: "Experiments show consistent improvements in retrieval effectiveness across multiple ranking and feedback models (sparse, dense, and generative) on the MS MARCO passage collection using TREC DL topics."
  - [section 4.4]: "An interesting finding is that the SRF decision function trained on RLM on a set of queries generalises well not only for a different set of queries (the test set), but also across different feedback models."
  - [corpus]: Weak evidence. No directly relevant papers in the corpus neighbors list explicitly discuss the generalization of selective PRF across different PRF models.

## Foundational Learning

- Concept: Pseudo-Relevance Feedback (PRF)
  - Why needed here: Understanding the basics of PRF is crucial for grasping the problem of PRF drift and the motivation behind selective PRF.
  - Quick check question: What is the main goal of PRF, and what is the problem of PRF drift?

- Concept: Transformer-based architectures and bi-encoders
  - Why needed here: The proposed model relies on a transformer-based bi-encoder architecture for encoding query-document interactions and learning the decision function.
  - Quick check question: How do transformer-based bi-encoders work, and what are their advantages for this task?

- Concept: Rank fusion and confidence-based calibration
  - Why needed here: The proposed approach uses rank fusion with confidence-based calibration to combine results from original and expanded queries.
  - Quick check question: What is rank fusion, and how does confidence-based calibration work in this context?

## Architecture Onboarding

- Component map: Query -> Encoding (bi-encoder) -> Decision (sigmoid) -> Rank Fusion (if needed) -> Final ranked list
- Critical path: Query -> Encoding (bi-encoder) -> Decision (sigmoid) -> Rank Fusion (if needed) -> Final ranked list
- Design tradeoffs:
  - Using a supervised approach requires relevance assessments for training, but can lead to better generalization than unsupervised methods.
  - The bi-encoder architecture allows for efficient encoding of query-document interactions, but may not capture all relevant information.
  - Confidence-based rank fusion can improve results, but relies on well-calibrated confidence estimates.
- Failure signatures:
  - Poor decision accuracy (close to random) indicates the model is not learning the right decision function.
  - Rank fusion does not improve results, suggesting the confidence estimates are not well-calibrated.
  - The model does not generalize well to different PRF models, indicating the decision function is too specific to the training PRF model.
- First 3 experiments:
  1. Train the model on a small subset of queries and evaluate decision accuracy on a held-out set.
  2. Compare the effectiveness of the selective PRF approach with and without rank fusion on a test set.
  3. Evaluate the generalization of the decision function to different PRF models by training on one PRF model and testing on others.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we determine the necessity of the PRF step to reduce computational costs for queries where PRF is ultimately ignored?
- Basis in paper: [explicit] The paper mentions that "exploring techniques to determine the necessity of the PRF step could reduce computational costs for queries where PRF is ultimately ignored."
- Why unresolved: The paper proposes a method to predict whether PRF should be applied, but it still requires executing PRF to gauge result quality, which is computationally expensive.
- What evidence would resolve it: Developing a lightweight method to predict the potential benefit of PRF without executing it, or a method to estimate the computational savings of skipping PRF for certain queries.

### Open Question 2
- Question: How can we predict the optimal parameters for PRF, such as the number of relevant documents to use?
- Basis in paper: [explicit] The paper suggests that "further work could also examine strategies for predicting the parameters of PRF itself, such as the number of relevant documents."
- Why unresolved: The paper focuses on predicting whether to apply PRF but does not address how to optimize the parameters of PRF once it is decided to apply it.
- What evidence would resolve it: Developing a model that can predict the optimal number of relevant documents to use for PRF based on the query and initial retrieval results.

### Open Question 3
- Question: Can the selective PRF model be adapted to work effectively with generative PRF approaches like GRF?
- Basis in paper: [explicit] The paper mentions that "the SRF based decision function does not need to be trained for specific PRF approaches," suggesting that the model could potentially work with different PRF methods.
- Why unresolved: While the paper demonstrates that the model generalizes across different PRF approaches, it does not specifically address the effectiveness of the model with generative PRF approaches like GRF.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the selective PRF model with generative PRF approaches and comparing the results with other PRF methods.

## Limitations
- The approach requires relevance assessments for training, limiting applicability to domains with scarce labeled data.
- Generalization across different PRF models was only evaluated on three specific models (RLM, GRF, ColBERT-PRF).
- The effectiveness of rank fusion depends heavily on well-calibrated confidence estimates, which may vary with different datasets or retrieval configurations.

## Confidence
- **Selective PRF effectiveness**: High confidence - consistent improvements across multiple metrics and PRF models
- **Generalization across PRF models**: Medium confidence - promising results but limited to three PRF models
- **Rank fusion benefits**: Medium confidence - improvements observed but contingent on reliable confidence estimates

## Next Checks
1. **Dataset Transfer Validation**: Evaluate the selective PRF model on a completely different retrieval dataset (e.g., TREC CAR or Robust04) to assess generalization beyond MS MARCO/TREC DL.

2. **PRF Model Diversity Test**: Test the generalization claim by evaluating the decision function on additional PRF models not used in the original training (e.g., Rocchio-based feedback or more recent neural PRF approaches).

3. **Confidence Calibration Analysis**: Perform a detailed analysis of the model's confidence calibration using reliability diagrams and Expected Calibration Error (ECE) metrics to validate the assumptions underlying the rank fusion approach.