---
ver: rpa2
title: 'Graph Condensation: A Survey'
arxiv_id: '2401.11720'
source_url: https://arxiv.org/abs/2401.11720
tags:
- graph
- condensed
- methods
- condensation
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive overview of graph condensation
  (GC), a technique that synthesizes compact graphs to enable efficient training of
  graph neural networks (GNNs). The paper categorizes existing GC methods into five
  groups based on evaluation criteria: effectiveness, generalization, efficiency,
  fairness, and robustness.'
---

# Graph Condensation: A Survey

## Quick Facts
- arXiv ID: 2401.11720
- Source URL: https://arxiv.org/abs/2401.11720
- Reference count: 40
- Primary result: Comprehensive survey of graph condensation methods with five evaluation criteria: effectiveness, generalization, efficiency, fairness, and robustness

## Executive Summary
This survey provides a comprehensive overview of graph condensation (GC), a technique that synthesizes compact graphs to enable efficient training of graph neural networks (GNNs). The paper categorizes existing GC methods into five groups based on evaluation criteria: effectiveness, generalization, efficiency, fairness, and robustness. It discusses two essential components within GC: optimization strategies and condensed graph generation. The survey systematically reviews recent advancements, presents empirical comparisons across diverse evaluation criteria, and explores practical applications and available resources.

## Method Summary
Graph condensation aims to synthesize compact graphs that enable efficient training of GNNs with performance comparable to training on the original large graph. The method involves bi-level optimization using relay models to optimize condensed graphs based on strategies like gradient matching, trajectory matching, kernel ridge regression, or distribution matching. The inputs are large-scale graph datasets with node features, adjacency matrices, and labels, and the outputs are condensed graphs with significantly fewer nodes. The objective is to achieve high effectiveness, generalization across GNN architectures, efficiency in condensation, fairness across demographic groups, and robustness to noisy graphs.

## Key Results
- Trajectory matching-based methods show superior performance in effectiveness
- Distribution matching approaches offer efficiency gains in condensation
- Addressing fairness and robustness in condensed graphs is crucial for real-world applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph condensation achieves performance comparable to GNNs trained on the original graph by optimizing the condensed graph to align with the original graph's training dynamics.
- Mechanism: The core optimization strategy involves matching the training trajectories of models trained on the original and condensed graphs. By aligning gradients or full trajectories, the condensed graph is tuned to preserve the essential characteristics needed for downstream tasks.
- Core assumption: The downstream task performance is primarily determined by the training dynamics of the model, and matching these dynamics ensures comparable performance.
- Evidence anchors:
  - [abstract] "GC focuses on synthesizing a compact yet highly representative graph, enabling GNNs trained on it to achieve performance comparable to those trained on the original large graph."
  - [section] "Gradient matching... formulates the condensation objective... as matching the optimized parameters of the models trained on two datasets"
  - [corpus] Weak. Only 5/25 neighbors mention gradient or trajectory matching explicitly.

### Mechanism 2
- Claim: GC generalizes across different GNN architectures and tasks by preserving spectral and structural properties of the original graph.
- Mechanism: GC methods employ spectral decomposition or low-rank approximations to capture the essential eigenbasis of the original graph, ensuring that the condensed graph retains the key structural information needed for various GNNs.
- Core assumption: Different GNN architectures rely on different spectral properties, and preserving these properties ensures generalization.
- Evidence anchors:
  - [abstract] "A GC method with superior generalization is particularly beneficial in scenarios that demand the training of heterogeneous models or the handling of diverse tasks"
  - [section] "GDEM... directly generates the eigenbasis for the condensed graph, thereby eliminating the spectrum bias inherent in relay GNNs"
  - [corpus] Weak. Only 2/25 neighbors discuss spectral properties or eigenbasis matching explicitly.

### Mechanism 3
- Claim: GC improves efficiency by reducing the computational burden of training GNNs on large graphs through compact graph representation.
- Mechanism: By condensing the graph to a small size (e.g., 0.1% of original), GC significantly reduces the number of nodes and edges, leading to faster message-passing and lower memory requirements during GNN training.
- Core assumption: The majority of the computational cost in GNN training comes from processing large graphs, and a compact representation can capture the essential information for training.
- Evidence anchors:
  - [abstract] "enabling GNNs trained on it to achieve performance comparable to those trained on the original large graph"
  - [section] "GC... aims to synthesize a compact yet highly representative graph"
  - [corpus] Weak. Only 3/25 neighbors discuss efficiency gains or computational benefits explicitly.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs are the primary models trained on both the original and condensed graphs. Understanding their message-passing mechanism is crucial for grasping how GC preserves performance.
  - Quick check question: What are the two main components of a GNN layer in the message-passing framework?

- Concept: Spectral Graph Theory
  - Why needed here: Many GC methods rely on spectral properties of graphs (e.g., eigenvectors, eigenvalues) to preserve structural information. This concept is essential for understanding generalization mechanisms.
  - Quick check question: How do the eigenvectors of the graph Laplacian relate to the graph's structural properties?

- Concept: Bi-level Optimization
  - Why needed here: GC involves a nested optimization problem where the condensed graph is optimized based on the performance of models trained on it. This concept is key to understanding the optimization strategies.
  - Quick check question: In bi-level optimization, what is the relationship between the outer and inner optimization problems?

## Architecture Onboarding

- Component map: Original graph (nodes, edges, features, labels) -> Relay model (GNN or kernel-based) -> Optimization strategy (gradient matching, trajectory matching, kernel ridge regression, distribution matching) -> Condensed graph (node features, adjacency matrix) -> Downstream GNN training

- Critical path: Original graph → Relay model encoding → Optimization objective → Condensed graph generation → Downstream GNN training

- Design tradeoffs:
  - Graph structure vs. graphless: Modeling adjacency matrix explicitly vs. implicitly encoding structure in node features
  - Optimization strategy: Gradient matching (accurate but slow) vs. distribution matching (fast but less flexible)
  - Compression rate: Higher compression reduces efficiency but may lose information

- Failure signatures:
  - Poor downstream performance despite optimization
  - Slow convergence during condensation
  - Condensation process fails to preserve class distributions

- First 3 experiments:
  1. Train a simple GNN on a small graph and compare performance to the original graph.
  2. Implement gradient matching optimization and observe convergence behavior.
  3. Vary the compression rate and measure the impact on downstream performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can graph condensation be effectively extended to heterophilic graphs where connected nodes have different labels or features?
- Basis in paper: [inferred] The paper mentions that current GC methods focus on simple graphs and that heterophilic graphs present unique challenges due to their differing node characteristics.
- Why unresolved: Existing GC methods are primarily designed for homophilic graphs, and their effectiveness on heterophilic graphs is unknown. The structural differences in heterophilic graphs may require new condensation strategies.
- What evidence would resolve it: Empirical studies comparing the performance of GC methods on heterophilic versus homophilic graphs, and the development of novel condensation techniques specifically designed for heterophilic structures.

### Open Question 2
- Question: What are the most effective evaluation metrics for assessing the quality of condensed graphs across different applications and tasks?
- Basis in paper: [explicit] The paper highlights the lack of a systematic and well-established methodology for assessing condensed graph quality, which is critical for monitoring condensation progress and potentially streamlining optimization.
- Why unresolved: The concept of quality in condensed graphs is inherently complex and varies significantly across different applications, necessitating a multifaceted approach to evaluation. Current methods rely on downstream model performance, which adds computational overhead.
- What evidence would resolve it: Development and validation of new evaluation metrics that are efficient, task-agnostic, and capable of capturing the diverse aspects of condensed graph quality.

### Open Question 3
- Question: How can graph condensation be made secure against privacy threats and adversarial attacks, particularly in security-sensitive domains?
- Basis in paper: [explicit] The paper discusses the security vulnerabilities of GC to privacy threats and adversarial attacks, highlighting the need for secure, privacy-preserving GC algorithms in domains like finance and industrial controls.
- Why unresolved: Current GC methods do not adequately address security and privacy concerns, making them vulnerable to various attacks. The development of secure GC algorithms requires a deep understanding of the attack vectors and the integration of robust defense mechanisms.
- What evidence would resolve it: The creation of secure GC algorithms that can withstand known privacy threats and adversarial attacks, along with empirical evaluations demonstrating their effectiveness in security-sensitive applications.

## Limitations
- Limited empirical evidence across diverse GNN architectures for generalization claims
- Unknown impact of initialization strategies on condensation convergence speed
- Lack of systematic evaluation methodology for condensed graph quality

## Confidence

- Trajectory matching effectiveness: High confidence
- Generalization across GNN architectures: Medium confidence
- Efficiency gains: High confidence

## Next Checks

1. Benchmark condensation time vs. downstream training time across graph sizes (1k-1M nodes) to quantify efficiency claims
2. Test generalization across 5+ diverse GNN architectures with ablation studies on spectral vs structural preservation
3. Evaluate fairness metrics on datasets with explicit demographic attributes to validate claims about demographic parity and equal opportunity