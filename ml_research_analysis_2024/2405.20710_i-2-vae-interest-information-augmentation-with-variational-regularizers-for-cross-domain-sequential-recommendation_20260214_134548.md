---
ver: rpa2
title: 'i$^2$VAE: Interest Information Augmentation with Variational Regularizers
  for Cross-Domain Sequential Recommendation'
arxiv_id: '2405.20710'
source_url: https://arxiv.org/abs/2405.20710
tags:
- users
- cross-domain
- user
- information
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses cross-domain sequential recommendation (CDSR)
  challenges in real-world scenarios, where most users are either cold-start (with
  interactions in only one domain) or long-tailed (with sparse interaction histories).
  Existing CDSR methods rely heavily on overlapping users and struggle with these
  user groups.
---

# i$^2$VAE: Interest Information Augmentation with Variational Regularizers for Cross-Domain Sequential Recommendation

## Quick Facts
- arXiv ID: 2405.20710
- Source URL: https://arxiv.org/abs/2405.20710
- Authors: Xuying Ning; Wujiang Xu; Tianxin Wei; Xiaolei Liu
- Reference count: 40
- Key outcome: Achieves 5.00% improvement in HR@10 for long-tailed users and 5.95% improvement for cold-start users compared to second-best method on Cloth-Sport dataset

## Executive Summary
This paper addresses the challenge of cross-domain sequential recommendation (CDSR) for cold-start and long-tailed users who lack sufficient interaction histories in multiple domains. The proposed i2VAE framework uses a variational autoencoder enhanced with three mutual information-based regularizers to improve recommendation quality. The key innovations include a pseudo-sequence generator for augmenting sparse user histories, cross-domain and disentangling regularizers for extracting transferable features, and a denoising regularizer to filter noise from pseudo-sequences.

## Method Summary
i2VAE employs a pseudo-sequence generator using LightGCN to synthesize interactions for long-tailed users, creating augmented interaction sequences. These sequences are combined with real interactions and encoded using SASRec self-attention layers. The framework then applies a variational autoencoder with six latent variables representing domain-specific, cross-domain, and augmented interests. Three mutual information-based regularizers are applied during training: a cross-domain regularizer to extract transferable features for cold-start users, a disentangling regularizer to separate cross-domain from domain-specific interests, and a denoising regularizer to filter noise from pseudo-sequences while preserving meaningful signals.

## Key Results
- Achieves 5.00% improvement in HR@10 for long-tailed users on Cloth-Sport dataset
- Achieves 5.95% improvement in HR@10 for cold-start users on Cloth-Sport dataset
- Demonstrates consistent performance across varying user-item interaction densities and different numbers of overlapping users

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pseudo-sequence generator (PSG) augments sparse user interaction histories to improve learning for long-tailed users.
- Mechanism: LightGCN-based iterative recall synthesizes new interactions by expanding original sequences with high-scoring items not yet interacted with, creating pseudo-sequences that enrich user interest representations.
- Core assumption: Even imperfect recall models can provide meaningful pseudo-interactions that enhance user interest understanding when real data is sparse.
- Evidence anchors:
  - [abstract]: "a pseudo-sequence generator synthesizes interactions for long-tailed users, refined by a denoising regularizer to filter noise and preserve meaningful interest signals"
  - [section 3.1]: "The pseudo-sequence generation process consists of three steps: (1) Unified Item Set Construction... (2) Embedding Learning & Recall... (3) Pseudo-Sequence Generation"
  - [corpus]: Weak - no direct corpus evidence for PSG effectiveness in sequential recommendation specifically
- Break condition: If the recall model generates items with very low relevance to the user's actual interests, pseudo-sequences could introduce harmful noise that degrades recommendation quality.

### Mechanism 2
- Claim: The disentangling regularizer ensures cross-domain representations remain separate from domain-specific interests.
- Mechanism: The regularizer maximizes I(X; Z X , Z Y
t ) while minimizing I(X; Z X ) and I(X; Z Y
t |Y ), encouraging Z Y
t to capture only transferable cross-domain information without contamination from domain-specific preferences in Y.
- Core assumption: Cross-domain and domain-specific interests should be mutually exclusive to prevent negative transfer, especially for cold-start users.
- Evidence anchors:
  - [abstract]: "cross-domain and disentangling regularizers extract transferable features for cold-start users"
  - [section 3.4.2]: Proposition 1 shows that minimizing I(Z X ; Z Y
t ) is equivalent to the optimization objective that encourages distinct information encoding
  - [section 3.5.1]: "minimizing I(X; Z Y
t | Y ) aligns the auxiliary distribution ry(zy
t |y) with the cross-domain representation q(zy
t |x, y)"
- Break condition: If the disentangling assumption is violated and cross-domain interests inherently overlap significantly with domain-specific interests, this regularizer could actually harm performance by artificially separating related signals.

### Mechanism 3
- Claim: The denoising regularizer filters noise from pseudo-sequences while preserving meaningful interest signals.
- Mechanism: Maximizing I(Z X ; Z X
a ) reduces uncertainty of Z X given Z X
a , encouraging the augmented representation to capture true underlying information while eliminating irrelevant noise from the pseudo-sequence generation process.
- Core assumption: Pseudo-sequences contain both useful interest signals and noise, and the model can learn to distinguish between them through mutual information maximization.
- Evidence anchors:
  - [abstract]: "a denoising regularizer to filter noise and preserve meaningful interest signals"
  - [section 3.4.3]: "maximizing I(Z X ; Z X
a ) reduces the uncertainty of Z X given Z X
a , encouraging Z X
a to capture the true underlying information in Z X and eliminate irrelevant noise"
  - [section 3.5.1]: Derivation shows the tractable lower bound LI(ZX ;ZXa ) for optimizing this mutual information
- Break condition: If the noise level in pseudo-sequences is too high relative to signal, the denoising regularizer may fail to effectively separate them, leading to corrupted interest representations.

## Foundational Learning

- Concept: Mutual Information and Interaction Information
  - Why needed here: These information-theoretic measures quantify the shared information between variables and enable the design of regularizers that enforce desirable properties in the latent representations (e.g., cross-domain transferability, disentanglement, noise filtering).
  - Quick check question: What does maximizing I(X; Y ; Z Y
t ) achieve in terms of information flow between domains?

- Concept: Variational Autoencoder (VAE) Framework
  - Why needed here: The VAE provides the probabilistic framework for learning latent representations of user interests, with inference and generative procedures that can be regularized to capture cross-domain and augmented interests effectively.
  - Quick check question: How does the reparameterization trick enable gradient-based optimization of the VAE's evidence lower bound?

- Concept: LightGCN for Graph-Based Embedding Learning
  - Why needed here: LightGCN provides an efficient method for learning user and item embeddings from the bipartite graph structure, which are then used for iterative recall to generate pseudo-sequences.
  - Quick check question: What is the role of message passing in LightGCN, and how does it contribute to generating pseudo-interactions?

## Architecture Onboarding

- Component map:
  Input: User interaction sequences from two domains (SX, SY)
  -> PSG: LightGCN-based pseudo-sequence generator creating eSX, eSY
  -> Embedding Layer: SASRec self-attention layers producing SX, SY, SX
a, SY
a
  -> VAE Core: Six latent variables (Z X, Z Y, Z Y
t, Z X
t, Z X
a, Z Y
a) with domain-specific and cross-domain encoders
  -> Regularizers: Cross-domain (I(X; Y ; Z Y
t)), disentangling (-I(Z X ; Z Y
t)), denoising (I(Z X ; Z X
a))
  -> Output: Predicted next-item interactions in both domains

- Critical path:
  1. Generate pseudo-sequences via PSG
  2. Encode real and pseudo sequences into embeddings
  3. Infer latent variables through VAE encoders
  4. Apply interest-enhancing regularizers during training
  5. Generate predictions using VAE decoders

- Design tradeoffs:
  - PSG vs. Sequential Models: LightGCN is chosen over sequential models for PSG due to efficiency concerns, trading some precision for computational feasibility
  - Regularizer Weights: λa and λc balance denoising and cross-domain/disentangling effects against reconstruction loss
  - Sequence Length T: Longer sequences provide more context but increase computational cost and padding noise

- Failure signatures:
  - Poor performance on cold-start users: May indicate insufficient cross-domain information transfer or ineffective disentangling
  - Degraded performance with longer pseudo-sequences: Could signal the denoising regularizer is failing to filter noise effectively
  - Sensitivity to λc values: May indicate the cross-domain/disentangling balance is not optimal for the dataset characteristics

- First 3 experiments:
  1. Ablation study removing the pseudo-sequence generator to quantify its contribution to long-tailed user performance
  2. Sensitivity analysis varying the denoising weight λa to find the optimal balance between noise filtering and signal preservation
  3. Comparison with a baseline that uses random pseudo-sequences instead of PSG-generated ones to validate the importance of quality pseudo-interactions

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided. However, based on the methodology and experimental results, several implicit questions emerge regarding the framework's behavior under different conditions and its generalizability to more complex scenarios.

## Limitations
- The effectiveness of the pseudo-sequence generator relies heavily on the quality of the LightGCN recall model, which is not thoroughly evaluated across different quality levels.
- The disentangling assumption may not hold universally across all domain pairs, potentially harming performance when cross-domain interests naturally overlap.
- Computational overhead of generating pseudo-sequences for all users in training is not explicitly addressed, raising scalability concerns for large user bases.

## Confidence

**High Confidence**: The core VAE framework with mutual information regularizers is well-grounded in information theory. The experimental methodology (dataset splits, metrics) is clearly specified and reproducible.

**Medium Confidence**: The specific weightings of the regularizers (λa, λc) and their sensitivity analysis are presented, but the paper doesn't fully explore how optimal values might vary across different domain pairs or user distributions.

**Low Confidence**: The scalability claims for real-world deployment are not empirically validated. The paper doesn't provide runtime analysis or memory requirements for the full i2VAE system.

## Next Checks

1. **Pseudo-sequence Quality Analysis**: Systematically vary the quality of the recall model (e.g., by limiting LightGCN depth or training epochs) and measure the impact on i2VAE performance, particularly for long-tailed users.

2. **Disentangling Assumption Validation**: Design an experiment that measures actual overlap between cross-domain and domain-specific interests (e.g., using mutual information metrics) to verify when the disentangling regularizer helps vs. hurts.

3. **Scalability Benchmarking**: Measure training and inference time as a function of user count and sequence length, comparing i2VAE's overhead against baseline methods to validate practical deployment feasibility.