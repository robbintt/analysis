---
ver: rpa2
title: Proposal Report for the 2nd SciCAP Competition 2024
arxiv_id: '2407.01897'
source_url: https://arxiv.org/abs/2407.01897
tags:
- yang
- text
- information
- generation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a document summarization method that leverages
  OCR-extracted object information and selective paragraph filtering to improve summarization
  accuracy. The approach extracts high-quality OCR data from images and filters irrelevant
  text chunks before feeding them to enhanced text generation models.
---

# Proposal Report for the 2nd SciCAP Competition 2024

## Quick Facts
- arXiv ID: 2407.01897
- Source URL: https://arxiv.org/abs/2407.01897
- Reference count: 2
- Primary result: Achieved top scores of 4.33 and 4.66 in long and short caption tracks, ranking first in both categories

## Executive Summary
This paper presents a document summarization method for scientific figure captioning that combines OCR extraction, selective paragraph filtering, and ensemble inference. The approach addresses the challenge of generating accurate captions from scientific figures by improving OCR quality and reducing noise from irrelevant text. Using a two-stage training and inference process, the model achieved first place in both caption tracks of the 2024 SciCAP competition.

## Method Summary
The method employs a two-stage training and inference process that first filters paragraphs into chunks, then computes a relevance score by comparing generation probabilities with and without each chunk. Only chunks above a threshold are retained. The approach uses PaddleOCR to re-extract text from images, addressing inaccuracies in the original OCR data. Three models (Pegasus, Pegasus-X-Large, LLaMA2-13B) are trained and their weights from different epochs are used to generate 100 candidate captions per image, which are then selected through ensemble techniques based on pairwise ROUGE-2-normalized scores.

## Key Results
- Top scores of 4.33 and 4.66 in long and short caption tracks respectively
- Ranked first place in both categories of the 2024 SciCAP competition
- Successfully reduced noise interference from background information through paragraph chunking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage training and inference with paragraph chunking reduces noise and improves caption relevance.
- Mechanism: The model first filters paragraphs into chunks, then computes a relevance score by comparing generation probabilities with and without each chunk. Only chunks above a threshold are retained. This selective retention focuses the model on object-relevant text and minimizes background noise.
- Core assumption: Paragraphs contain redundant or irrelevant background information that dilutes caption quality.
- Evidence anchors:
  - [abstract] "Using a two-stage training and inference process, our model minimizes attention to text weakly related to the objects while reducing the noise interference from background information."
  - [section] "We designed a filtering method that divides paragraphs into multiple chunks, allowing the model to identify chunks that are useful for generating the correct answer."

### Mechanism 2
- Claim: Re-extracting OCR via PaddleOCR improves OCR accuracy and thus caption quality.
- Mechanism: The official dataset's OCR is noisy or incomplete; PaddleOCR re-extracts text from images, providing cleaner textual cues for the model.
- Core assumption: Better OCR text improves model input quality and downstream caption generation.
- Evidence anchors:
  - [section] "We found that OCR information in the images provided by the official dataset is not accurate... We have chosen to use the PaddleOCR to re-extract OCR information from the images."
  - [section] "Comparing OCR results from the original data with those extracted using PaddleOCR, the original OCR contains error information, while OCR captured by PaddleOCR corrects these errors."

### Mechanism 3
- Claim: Ensemble inference over multiple epochs and models improves caption quality by aggregating diverse outputs.
- Mechanism: Three models (Pegasus, Pegasus-X-Large, LLaMA2-13B) are run with weights from different epochs, producing 100 candidate captions. A scoring function based on pairwise ROUGE-2-normalized scores selects the best candidate.
- Core assumption: Diverse model outputs capture complementary aspects of the caption; ensemble selection finds the best combination.
- Evidence anchors:
  - [section] "We train the Pegasus, Pegasus-x-Large, and LLaMA2-13B models and obtain their weights at different epochs... For each image, we obtain 100 candidate summaries, from which we select the summary that best describes the image."
  - [section] "The score of this summary is the average of the cumulative scores of the ROUGE-2-normalized scores compared to the other summaries."

## Foundational Learning

- Concept: Text chunking and relevance scoring.
  - Why needed here: Allows selective filtering of background noise before caption generation.
  - Quick check question: How would you modify the threshold λ to trade off recall vs. precision of retained chunks?

- Concept: OCR extraction and text preprocessing.
  - Why needed here: Clean OCR text provides accurate object descriptions for the model.
  - Quick check question: What are common failure modes when OCR quality is poor, and how can you detect them automatically?

- Concept: Ensemble model selection and evaluation metrics.
  - Why needed here: Combining multiple models and epochs reduces variance and improves robustness.
  - Quick check question: How does ROUGE-2-normalized scoring differ from simple ROUGE-2, and why is normalization useful here?

## Architecture Onboarding

- Component map: OCR extraction -> PaddleOCR re-extraction -> paragraph chunking -> relevance filtering -> caption generation -> ensemble selection

- Critical path: OCR extraction → paragraph chunking → relevance filtering → caption generation → ensemble selection

- Design tradeoffs:
  - Higher λ reduces noise but risks dropping relevant content.
  - More ensemble candidates increase computational cost but improve selection robustness.
  - Using PaddleOCR improves OCR quality but adds preprocessing time.

- Failure signatures:
  - Low relevance scores across all chunks indicate OCR or filtering failure.
  - High variance among ensemble candidates may signal model instability.
  - Poor BLEU/ROUGE scores suggest input quality or model capacity issues.

- First 3 experiments:
  1. Run filtering with λ=0.1, 0.3, 0.5; measure impact on caption quality.
  2. Compare captions with original OCR vs. PaddleOCR-re-extracted OCR.
  3. Vary ensemble size (10, 50, 100 candidates) and evaluate score stability.

## Open Questions the Paper Calls Out
None

## Limitations
- The filtering mechanism relies on a manually tuned threshold λ that may not generalize well across different datasets or domains.
- The ensemble approach significantly increases computational cost, generating 100 candidates per image.
- The paper lacks ablation studies showing the individual contribution of each component to final performance.

## Confidence

**High Confidence**: The top ranking in both long and short caption tracks (4.33 and 4.66 scores) is a strong empirical validation of the overall approach. The mechanism of OCR re-extraction and the two-stage filtering process are clearly described and implemented.

**Medium Confidence**: The ensemble selection method using ROUGE-2-normalized scoring is reasonable but the paper does not provide evidence that this is optimal or that it outperforms simpler selection strategies.

**Low Confidence**: The claim about PaddleOCR providing "significant" improvement is based on qualitative comparison rather than systematic quantitative evaluation.

## Next Checks

1. **Ablation Study**: Systematically evaluate the contribution of each component by removing OCR re-extraction, filtering, and ensemble selection individually. Measure performance degradation to quantify each component's impact.

2. **Threshold Sensitivity Analysis**: Test the filtering mechanism with λ values ranging from 0.01 to 0.9 in increments of 0.1. Plot caption quality against threshold to identify optimal ranges and robustness to parameter choice.

3. **Cross-Dataset Generalization**: Apply the complete pipeline to at least two other scientific figure captioning datasets (e.g., FigureQA, VizWiz-Captions) without retraining. Compare performance to establish whether the approach generalizes beyond the competition dataset.