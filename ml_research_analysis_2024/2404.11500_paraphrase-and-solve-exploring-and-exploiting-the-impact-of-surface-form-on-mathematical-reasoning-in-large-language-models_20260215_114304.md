---
ver: rpa2
title: 'Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form
  on Mathematical Reasoning in Large Language Models'
arxiv_id: '2404.11500'
source_url: https://arxiv.org/abs/2404.11500
tags:
- problem
- solve
- language
- rate
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the relationship between the surface form of
  a mathematical problem and its solvability by large language models (LLMs). It finds
  that subtle alterations in the surface form can significantly impact the answer
  distribution and solve rate, exposing the LLM's lack of robustness and sensitivity
  to the surface form in reasoning through complex problems.
---

# Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form on Mathematical Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2404.11500
- Source URL: https://arxiv.org/abs/2404.11500
- Authors: Yue Zhou; Yada Zhu; Diego Antognini; Yoon Kim; Yang Zhang
- Reference count: 24
- This paper studies how surface form variations impact LLM mathematical reasoning performance and proposes SCoP to improve it.

## Executive Summary
This paper investigates how subtle alterations in the surface form of mathematical problems significantly impact the answer distribution and solve rate of large language models (LLMs). The authors demonstrate that LLMs exhibit substantial sensitivity to phrasing, with solve rates varying dramatically across different surface forms of the same problem. To address this sensitivity, they propose Self-Consistency-over-Paraphrases (SCoP), a method that generates multiple paraphrases of a mathematical problem and aggregates reasoning paths across these diverse surface forms. SCoP improves mathematical reasoning performance over vanilla self-consistency, particularly for problems initially deemed unsolvable, and is evaluated across four mathematics reasoning benchmarks and three LLM architectures.

## Method Summary
The paper proposes Self-Consistency-over-Paraphrases (SCoP), which enhances mathematical reasoning by generating multiple paraphrases of a problem to diversify reasoning paths. The approach consists of three steps: (1) paraphrase generation using either naive prompting or in-context exemplars to create semantically equivalent surface forms, (2) chain-of-thought reasoning path generation for each paraphrase, and (3) majority voting to aggregate answers across all reasoning paths. The method is evaluated against vanilla self-consistency on four mathematics reasoning benchmarks (GSM8K, AQuA, MATH, MMLU) using three LLMs (LLaMA-2-70b, GPT-3.5-turbo, GPT-4), showing improved accuracy particularly for initially unsolvable problems.

## Key Results
- SCoP improves mathematical reasoning performance over vanilla self-consistency across four benchmarks
- Solve rates increase dramatically when switching between effective and ineffective surface forms (5% to 100% on GSM8K)
- In-context exemplars significantly improve paraphrase quality compared to naive paraphrasing
- The aggregation mechanism successfully mitigates damage from "bad" paraphrases through majority voting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Surface form diversity reduces reasoning path bias by exposing the model to multiple semantically equivalent problem presentations.
- Mechanism: The language model generates multiple paraphrases of the same mathematical problem, each potentially triggering different reasoning paths due to sensitivity to phrasing. By aggregating over these diverse paths, the approach mitigates the risk of being stuck in poor reasoning trajectories from a single surface form.
- Core assumption: The language model's reasoning is sensitive to surface form variations but semantically consistent across paraphrases.
- Evidence anchors:
  - [abstract] "subtle alterations in the surface form can significantly impact the answer distribution and solve rate"
  - [section 2] "solve rate varies significantly across the surface forms" and "solve rate increases from 5% to 100%"
  - [corpus] Multiple related papers show surface form sensitivity in LLMs, supporting this mechanism.
- Break condition: If the model's reasoning becomes semantically inconsistent across paraphrases or if paraphrasing introduces noise that degrades understanding, the aggregation would fail to improve performance.

### Mechanism 2
- Claim: In-context exemplars improve paraphrase quality by guiding the model toward surface forms with higher solve rates.
- Mechanism: By providing the model with examples of paraphrases that previously improved solve rates, the model learns to generate new paraphrases more likely to yield correct answers. This leverages the model's ability to learn from demonstrations without explicit training.
- Core assumption: The language model can generalize from a small number of exemplar paraphrases to generate new effective paraphrases.
- Evidence anchors:
  - [section 4.1] "obtain Nshot 'good' paraphrases as the in-context exemplars...paraphrases that contribute to a solve rate improvement"
  - [section 3.2] "increase the chance of generating 'good' paraphrases" through in-context learning
  - [corpus] Evidence from prompt engineering literature supports in-context learning effectiveness.
- Break condition: If the exemplar set is too small or unrepresentative, the model may not learn effective paraphrasing strategies, or if the model fails to generalize from exemplars.

### Mechanism 3
- Claim: Aggregating over multiple paraphrases smooths out individual reasoning failures by leveraging majority voting.
- Mechanism: Even if some paraphrases lead to incorrect answers, the majority vote across all paraphrases tends to select the correct answer when most paraphrases yield correct reasoning paths. This statistical aggregation overcomes individual surface form weaknesses.
- Core assumption: The correct answer will be more frequent than incorrect ones across diverse paraphrases, making majority voting effective.
- Evidence anchors:
  - [section 2] "if a problem exhibits a low solve rate and ineffective reasoning paths due to its original surface form, introducing diversity in its surface forms would be beneficial"
  - [section 5] "the damage brought by the 'bad' paraphrases is small compared to the benefit brought by the 'good' paraphrases"
  - [corpus] Standard ensemble methods support this aggregation principle.
- Break condition: If most paraphrases consistently lead to incorrect answers, majority voting will fail; also if answer distribution becomes too uniform, voting loses effectiveness.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Mathematical problems require multi-step reasoning, and CoT prompting helps the model generate intermediate reasoning steps rather than jumping to conclusions.
  - Quick check question: What happens if you remove the "Let's think step by step" prompt when using SCoP?

- Concept: Self-consistency sampling
  - Why needed here: Instead of greedy decoding, sampling multiple reasoning paths captures the model's uncertainty and identifies the most consistent answer across samples.
  - Quick check question: How does increasing the number of sampled reasoning paths affect the solve rate?

- Concept: Paraphrasing and semantic equivalence
  - Why needed here: The approach relies on generating paraphrases that preserve problem semantics while varying surface form to trigger different reasoning paths.
  - Quick check question: Can you identify when a paraphrase might change the problem semantics versus just the surface form?

## Architecture Onboarding

- Component map: Original problem → Paraphrase generator (LLM) → K paraphrases → Answer generator (same LLM) → N/K reasoning paths per paraphrase → N total answers → Majority vote → Final answer
- Critical path: Original problem → K paraphrases generated → N/K reasoning paths per paraphrase → N total answers → majority vote for final answer
- Design tradeoffs: More paraphrases (higher K) increases diversity but may introduce noise; more reasoning paths per paraphrase increases confidence but reduces the number of paraphrases; exemplar-based paraphrasing improves quality but requires access to ground truth.
- Failure signatures: Solve rate degradation when paraphrases introduce semantic drift; poor performance when exemplar selection fails; saturation when increasing K or N provides no additional benefit.
- First 3 experiments:
  1. Run SCoP with K=1 (no paraphrasing) to establish baseline self-consistency performance.
  2. Run SCoP with K=2 and Naïve paraphrasing to test if simple paraphrasing improves performance.
  3. Run SCoP with K=2 and ICLpara to test if exemplar-guided paraphrasing provides additional benefit over Naïve.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: What factors contribute to a surface form being easier or harder for LLMs to solve?
- Basis in paper: Inferred
- Why unresolved: While the paper shows surface forms significantly impact solve rates, it doesn't identify what makes some paraphrases easier than others.
- What evidence would resolve it: Analysis of successful vs unsuccessful paraphrases to identify linguistic patterns or structural features that correlate with solve rates.

**Open Question 2**
- Question: How well does SCoP generalize across different languages and cultural contexts?
- Basis in paper: Inferred from "limitation" section
- Why unresolved: All experiments were conducted in English, leaving the cross-lingual generalizability unexplored.
- What evidence would resolve it: Testing SCoP on math problems in multiple languages and measuring performance across cultural contexts.

**Open Question 3**
- Question: What is the optimal mechanism for identifying or generating surface forms that are easier to solve than others?
- Basis in paper: Inferred from "limitations" section
- Why unresolved: Current approaches rely on random sampling or exemplar selection without a principled method for generating effective paraphrases.
- What evidence would resolve it: Development and validation of a systematic approach to generate or identify optimal surface forms.

## Limitations

- Surface Form Sensitivity Generalization: Findings may not generalize to other problem domains beyond mathematics or real-world problem distributions.
- Paraphrase Quality Assessment: The approach lacks explicit quality control mechanisms for generated paraphrases and may introduce semantic drift.
- Computational Resource Requirements: The approach potentially increases computational costs by a factor of K×N compared to vanilla self-consistency.

## Confidence

**High Confidence**: The core observation that surface form variations significantly impact LLM mathematical reasoning performance is well-supported by empirical evidence across multiple benchmarks and models.

**Medium Confidence**: The specific implementation details of the SCoP approach, particularly exemplar selection criteria and optimal values for hyperparameters K and N, are reasonable but may require tuning for different problem types or LLM architectures.

**Low Confidence**: Broader claims about general applicability to all reasoning tasks and consistent outperformance of specialized mathematical reasoning training are not fully substantiated by the limited evidence presented.

## Next Checks

1. **Ablation Study on Paraphrase Count**: Systematically vary the number of paraphrases K (e.g., K=1, 2, 4, 8, 16) and analyze the point of diminishing returns in solve rate improvement versus computational cost, to identify optimal resource allocation strategies.

2. **Cross-Domain Generalization Test**: Apply SCoP to non-mathematical reasoning benchmarks (such as logical reasoning or commonsense reasoning tasks) to validate whether surface form sensitivity and the benefits of paraphrase diversity extend beyond mathematical domains.

3. **Semantic Drift Detection**: Implement and evaluate explicit mechanisms for detecting paraphrases that introduce semantic drift or ambiguity, and measure the impact of filtering such paraphrases on overall performance compared to the current majority voting approach.