---
ver: rpa2
title: 'RAW: A Robust and Agile Plug-and-Play Watermark Framework for AI-Generated
  Images with Provable Guarantees'
arxiv_id: '2403.18774'
source_url: https://arxiv.org/abs/2403.18774
tags:
- image
- images
- watermark
- watermarks
- watermarking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RAW, a robust and agile watermark framework
  for AI-generated images, designed to address the problem of safeguarding intellectual
  property and preventing misuse. Unlike traditional methods that embed fixed binary
  codes into latent representations, RAW introduces learnable watermarks directly
  into the original image data and employs a jointly trained classifier for detection.
---

# RAW: A Robust and Agile Plug-and-Play Watermark Framework for AI-Generated Images with Provable Guarantees

## Quick Facts
- **arXiv ID**: 2403.18774
- **Source URL**: https://arxiv.org/abs/2403.18774
- **Reference count**: 40
- **Primary result**: RAW achieves AUROC improvement from 0.48 to 0.82 under adversarial attacks while maintaining image quality

## Executive Summary
RAW presents a novel watermark framework for AI-generated images that addresses intellectual property protection and misuse prevention. Unlike traditional approaches that embed fixed binary codes into latent representations, RAW introduces learnable watermarks directly into original image data in both frequency and spatial domains. The framework employs joint training of watermarks and classifiers, and integrates smoothing techniques to provide provable guarantees on false positive rates even under adversarial attacks. Experimental results on state-of-the-art diffusion models demonstrate significant performance enhancements compared to existing approaches.

## Method Summary
RAW is a plug-and-play watermark framework that introduces learnable watermarks directly into original image data rather than latent representations. The framework jointly trains watermarking and verification modules using a binary cross-entropy loss on combined datasets with data augmentations. The watermarking module applies FFT to add frequency-domain watermarks and spatial-domain watermarks to images. A CNN classifier is trained to distinguish watermarked from unwatermarked images, with randomized smoothing providing certified robustness to adversarial perturbations. The framework is compatible with various generative architectures and supports on-the-fly watermark injection after training.

## Key Results
- Significant AUROC improvement from 0.48 to 0.82 under adversarial attacks compared to state-of-the-art approaches
- Provable guarantees on false positive rate through integration of randomized smoothing techniques
- Maintains image quality while providing robust watermark detection against various manipulations
- Compatible with multiple diffusion models and supports on-the-fly watermark injection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAW introduces learnable watermarks directly into the original image data, both in frequency and spatial domains, instead of embedding fixed binary codes into latent representations.
- Mechanism: The watermarking module applies FFT to the image, adds frequency-domain watermark scaled by visibility parameter, then adds spatial-domain watermark. The classifier is jointly trained with these watermarks to distinguish watermarked from unwatermarked images.
- Core assumption: The joint training of watermarks and classifier enables the watermarks to adapt to specific data distributions, improving detection accuracy compared to fixed watermarks.
- Evidence anchors:
  - [abstract] "RAW introduces learnable watermarks directly into the original image data"
  - [section] "We propose to add two distinct watermarks into both frequency and spatial domains: Ew(X) = F −1(F(X) + c1 × v) + c2 × u"
  - [corpus] Weak evidence - corpus neighbors focus on attack/defense papers, not on the specific mechanism of combining frequency and spatial watermarks
- Break condition: If the watermark visibility parameters (c1, c2) are set too low, the watermarks become imperceptible but also undetectable, breaking the mechanism. If set too high, image quality degrades unacceptably.

### Mechanism 2
- Claim: RAW provides provable guarantees on false positive rate (FPR) even under adversarial attacks through integration of smoothing techniques.
- Mechanism: RAW uses randomized smoothing to create a smoothed version of the classifier Vθ, such that Ṽθ(X) = Φ−1(E_Z∼N(0,σ²I)[Vθ(X + Z)]). This creates a Lipschitz-continuous function that provides certified robustness to ℓ2-bounded perturbations.
- Core assumption: The adversarial transformation A satisfies ∥X − A(X)∥ ≤ γ for a small γ, meaning the attack cannot be excessively adversarial or it would overwrite original content.
- Evidence anchors:
  - [abstract] "By incorporating state-of-the-art smoothing techniques, we show that the framework also provides provable guarantees regarding the false positive rate"
  - [section] "The above result suggests that for any base verification module (classifier) Vθ, we can obtain a smoothed version with Ṽθ(X) = Φ−1(E_Z∼N(0,σ²I)[Vθ(X + Z)])"
  - [corpus] No direct evidence - corpus neighbors discuss attacks but not the specific smoothing-based certification mechanism
- Break condition: If the smoothing parameter σ is too small, the certified radius becomes negligible. If the adversarial attack exceeds the certified radius, the FPR guarantees no longer hold.

### Mechanism 3
- Claim: RAW achieves superior robustness to image manipulations and adversarial attacks while maintaining image quality, demonstrated by significant AUROC improvements (0.48 to 0.82) under attacks.
- Mechanism: The combination of joint training, dual-domain watermarking (frequency + spatial), and data augmentation during training creates a robust detection system. The spatial watermark specifically helps against Gaussian noise, while frequency watermark helps against geometric transformations.
- Core assumption: The adversarial attacks and image manipulations considered are within realistic bounds and don't completely destroy the image content.
- Evidence anchors:
  - [abstract] "our method demonstrates a notable increase in AUROC, from 0.48 to 0.82, when compared to state-of-the-art approaches in detecting watermarked images under adversarial attacks"
  - [section] "The experimental results consistently affirm the excellent performance of our approach, as evidenced by notable enhancements in AUROC from 0.48 to 0.82"
  - [corpus] Weak evidence - corpus neighbors discuss attacks but don't provide the specific performance metrics or comparison framework
- Break condition: If the image manipulations are too severe (e.g., extreme compression, complete cropping), the watermark detection performance degrades significantly, as evidenced by the ablation studies showing performance drops under strong manipulations.

## Foundational Learning

- **Concept**: Binary classification and hypothesis testing
  - Why needed here: RAW frames watermark detection as a binary classification problem (watermarked vs unwatermarked) with hypothesis testing framework
  - Quick check question: How would you formulate the watermark detection problem as a hypothesis test with H0 (watermarked) and H1 (unwatermarked)?

- **Concept**: Conformal prediction and distribution-free guarantees
  - Why needed here: RAW uses conformal prediction techniques to provide provable FPR guarantees without distributional assumptions
  - Quick check question: What is the key insight from conformal prediction that allows RAW to provide FPR guarantees without knowing the test data distribution?

- **Concept**: Randomized smoothing and Lipschitz continuity
  - Why needed here: RAW uses randomized smoothing to create a Lipschitz-continuous classifier that provides certified robustness to adversarial perturbations
  - Quick check question: How does randomized smoothing transform a base classifier into a Lipschitz-continuous function, and why is this property important for certified robustness?

## Architecture Onboarding

- **Component map**: Watermarking Module (FFT + additions + inverse FFT) -> Verification Module (CNN classifier) -> Randomized Smoothing -> Conformal Prediction
- **Critical path**: 1. Generate images with diffusion model 2. Apply watermarking module (FFT + additions + inverse FFT) 3. Train classifier to distinguish watermarked vs unwatermarked 4. Apply randomized smoothing for certified robustness 5. Set threshold using conformal prediction for desired FPR
- **Design tradeoffs**: Watermark visibility vs detectability (higher visibility improves detection but may degrade image quality), smoothing parameter σ vs certified radius (larger σ increases certified radius but may reduce clean accuracy), computational cost vs robustness (joint training is more expensive than fixed watermark approaches but provides better performance)
- **Failure signatures**: Low AUROC on clean data indicates poor watermark detection, high FPR on watermarked test data indicates threshold miscalibration, large performance drop under specific manipulations indicates vulnerability to that attack type
- **First 3 experiments**: 1. Test detection accuracy on clean watermarked vs unwatermarked images to verify basic functionality 2. Apply Gaussian noise to test images and measure detection performance to evaluate spatial watermark effectiveness 3. Apply rotation and cropping to test images and measure detection performance to evaluate frequency watermark effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the robustness of RAW be further improved against adversarial attacks targeting watermark removal?
- Basis in paper: [explicit] The paper discusses the integration of state-of-the-art smoothing techniques and mentions the potential for further enhancements through methods like adversarial training, contrastive learning, and label smoothing.
- Why unresolved: While the paper demonstrates improved robustness, it does not explore the full potential of these techniques or other advanced methods to enhance robustness against increasingly sophisticated adversarial attacks.
- What evidence would resolve it: Experimental results showing the performance of RAW with various advanced techniques and comparisons with other state-of-the-art methods under different adversarial attacks would provide insights into the potential for further improvements.

### Open Question 2
- Question: What is the maximum number of distinct watermarks that can be concurrently learned within a single training session using RAW?
- Basis in paper: [inferred] The paper introduces RAW as a framework for embedding learnable watermarks but does not explore the scalability of the framework in terms of the number of watermarks that can be effectively learned and detected simultaneously.
- Why unresolved: The scalability of RAW in handling multiple watermarks is not investigated, which is crucial for applications requiring the embedding of multiple watermarks for different purposes or entities.
- What evidence would resolve it: Experiments demonstrating the performance of RAW with varying numbers of watermarks, including detection accuracy and computational efficiency, would provide insights into the scalability limits of the framework.

### Open Question 3
- Question: How does the choice of watermark visibility parameters (c1, c2) affect the trade-off between watermark robustness and image quality?
- Basis in paper: [explicit] The paper mentions the use of visibility parameters (c1, c2) in the watermarking module but does not provide a detailed analysis of how these parameters influence the robustness of the watermark and the quality of the watermarked images.
- Why unresolved: Understanding the impact of visibility parameters on the trade-off between robustness and image quality is essential for optimizing the watermarking process for different applications and requirements.
- What evidence would resolve it: A comprehensive study analyzing the effects of varying c1 and c2 on the robustness of the watermark (e.g., under different adversarial attacks) and the quality of the watermarked images (e.g., using metrics like FID and CLIP scores) would provide valuable insights into the optimal parameter settings.

## Limitations
- The specific hyperparameters for training (learning rates, batch sizes, number of iterations) are not fully specified, making exact reproduction challenging
- The experimental setup relies heavily on diffusion model outputs with unclear generation parameters (sampling steps, guidance scale)
- Robustness claims depend on the assumption that adversarial attacks remain within a bounded ℓ2 norm, which is not fully explored when violated
- Comparison to baseline methods uses only AUROC as the primary metric, potentially missing other important aspects of watermark detection performance

## Confidence

**High Confidence**: The core mechanism of joint training between watermarks and classifier is well-established in the literature and the mathematical formulation is clearly presented. The theoretical framework for randomized smoothing and its application to provide FPR guarantees is sound and properly cited.

**Medium Confidence**: The experimental results showing AUROC improvement from 0.48 to 0.82 under adversarial attacks are impressive but depend heavily on the specific attack implementations and evaluation protocols, which are not fully detailed. The claim of "significant performance enhancements" relative to existing approaches requires careful verification since the baselines and comparison methodology are not exhaustively described.

**Low Confidence**: The scalability claims to various generative architectures and the assertion of "provable guarantees" require more rigorous validation across diverse scenarios and attack types beyond those presented in the paper.

## Next Checks
1. **Reproduce the baseline comparison**: Implement the exact baseline methods mentioned (DWT, DCT, Spread-spectrum) using the same datasets and attack scenarios to verify the claimed AUROC improvements from 0.48 to 0.82.

2. **Test breaking conditions**: Systematically evaluate performance degradation under extreme image manipulations (e.g., severe compression, heavy cropping, complete geometric transformations) to identify the limits of the robustness claims.

3. **Validate the smoothing parameter sensitivity**: Conduct ablation studies varying the smoothing parameter σ to determine the trade-off between certified radius and clean accuracy, and verify the sensitivity of FPR guarantees to this parameter choice.