---
ver: rpa2
title: 'WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents'
arxiv_id: '2404.05902'
source_url: https://arxiv.org/abs/2404.05902
tags:
- agent
- text
- demonstrations
- wilbur
- button
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WILBUR is a web agent that improves performance through adaptive
  in-context learning and intelligent backtracking. It addresses the challenge of
  generalizing across diverse websites by retrieving and synthesizing task demonstrations,
  both goal-conditioned and website-conditioned.
---

# WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents
## Quick Facts
- arXiv ID: 2404.05902
- Source URL: https://arxiv.org/abs/2404.05902
- Reference count: 27
- Achieves 53% success rate on WebVoyager benchmark

## Executive Summary
WILBUR is a web agent that achieves state-of-the-art performance through adaptive in-context learning and intelligent backtracking. The system retrieves and synthesizes task demonstrations to guide a large language model, enabling it to learn from both successful and failed attempts across diverse websites. WILBUR also features a reflection mechanism that learns from mistakes and backtracks when necessary. Trained on an auto-curriculum that generates goals and evaluates executions automatically, WILBUR requires no manual annotation and achieves 53% success rate on the WebVoyager benchmark, outperforming the previous text-only state-of-the-art by 8% and approaching a strong multimodal model.

## Method Summary
WILBUR uses adaptive in-context learning to improve web agent performance across diverse websites. The system retrieves and synthesizes task demonstrations (both goal-conditioned and website-conditioned) to populate the prompt for a large language model. A novel differentiable ranking model selects the most helpful demonstrations. WILBUR also features a reflection mechanism that learns from mistakes and backtracks when necessary. The agent is trained on an auto-curriculum that generates goals and evaluates executions automatically, requiring no manual annotation.

## Key Results
- Achieves 53% success rate on WebVoyager benchmark
- Outperforms previous text-only state-of-the-art by 8%
- Approaches performance of strong multimodal model while using only textual inputs

## Why This Works (Mechanism)
### Mechanism 1: Backtracking
- Claim: WILBUR's backtracking mechanism allows recovery from delayed mistakes that would otherwise cause failure.
- Mechanism: The agent maintains a trajectory graph and can revert to a previous URL state when the reflection module determines the current action did not contribute progress toward the goal.
- Core assumption: Web agent tasks can be modeled as graph exploration where states can be reverted to previous navigation points.
- Evidence anchors: [abstract], [section 3.2]

### Mechanism 2: Demonstration Retrieval and Synthesis
- Claim: The demonstration retrieval and synthesis system enables WILBUR to learn from diverse experiences and avoid previously discovered pitfalls.
- Mechanism: WILBUR maintains two demonstration banks - full trajectories and isolated actions. It retrieves both positive (successful) and negative (unsuccessful) demonstrations using cosine similarity and a trained ranking model.
- Core assumption: Including negative examples helps the agent avoid known failure patterns and improves generalization across websites.
- Evidence anchors: [abstract], [section 3.2]

### Mechanism 3: Auto-curriculum
- Claim: The auto-curriculum enables WILBUR to rapidly acquire knowledge of new websites and tasks without manual annotation.
- Mechanism: The system automatically generates plausible goals for websites using an LLM, executes the agent on these goals, and self-evaluates the results.
- Core assumption: LLM-generated goals are sufficiently representative of real-world use cases to create meaningful training data.
- Evidence anchors: [abstract], [section 3.3]

## Foundational Learning
- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: The web agent operates in an environment where the full state is not observable - the agent only sees the DOM and extracted text, not the backend state.
  - Quick check question: Why can't a standard Markov Decision Process be used for web agents instead of a POMDP?

- Concept: In-context learning and few-shot prompting
  - Why needed here: WILBUR uses demonstrations and synthesized instructions within the LLM prompt to guide behavior, rather than fine-tuning.
  - Quick check question: What's the difference between raw task demonstrations and synthesized instructions in WILBUR's approach?

- Concept: Auto-curriculum and self-play
  - Why needed here: The system generates its own training data by sampling goals, executing them, and evaluating success.
  - Quick check question: How does the auto-curriculum ensure diversity in the generated goals across different websites?

## Architecture Onboarding
- Component map: Goal → Retrieve Demonstrations → Synthesize Instructions → Actor Prediction → Execute → Reflect → Answer
- Critical path: Goal → Retrieve Demonstrations → Synthesize Instructions → Actor Prediction → Execute → Reflect → Answer
- Design tradeoffs:
  - Using raw demonstrations vs. synthesized instructions: Raw demonstrations provide specific examples but consume context window; synthesized instructions are more compact but lose some detail
  - Backtracking vs. retry: Backtracking can recover from delayed mistakes but requires state management; retry is simpler but cannot recover from mistakes that manifest later
  - Auto-curriculum vs. manual annotation: Auto-curriculum scales but may generate unrealistic goals; manual annotation is expensive but ensures quality
- Failure signatures:
  - Agent gets stuck in infinite loops: Check reflection module and backtracking logic
  - Agent selects wrong elements: Verify DOM formatting and embedding similarity thresholds
  - Agent fails to generalize: Examine demonstration retrieval and ranking model performance
  - Agent makes repeated mistakes: Check if negative examples are being properly included and synthesized
- First 3 experiments:
  1. Implement zero-shot baseline on a single website (e.g., Wikipedia) to establish baseline performance and identify basic failure modes
  2. Add backtracking mechanism to the zero-shot agent and measure improvement on tasks with delayed failure modes
  3. Implement demonstration retrieval and synthesis on a single website type, comparing performance with and without demonstrations on tasks that require learning website-specific patterns

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of WILBUR compare to a strong multimodal model on the WebVoyager benchmark, and what specific aspects of the multimodal model's performance could be further improved?
- Basis in paper: [explicit] The paper states that WILBUR is within 5% of a strong multimodal model despite only receiving textual inputs, and further analysis reveals a substantial number of failures are due to engineering challenges of operating the web.
- Why unresolved: The paper does not provide a detailed comparison of the performance of WILBUR and the multimodal model on individual tasks or websites, nor does it explore potential areas for improvement in the multimodal model's performance.
- What evidence would resolve it: A detailed comparison of the performance of WILBUR and the multimodal model on individual tasks or websites, along with an analysis of the specific aspects of the multimodal model's performance that could be improved.

### Open Question 2
- Question: What are the specific limitations of the current DSL and DOM representation used by WILBUR, and how could these limitations be addressed to improve the agent's performance?
- Basis in paper: [explicit] The paper mentions that the agent is built to operate only on the main DOM of the page, and cannot act inside frames or inside the shadow DOM used by web components. It also mentions issues with interacting with complex widgets like date selectors.
- Why unresolved: The paper does not provide a detailed analysis of the specific limitations of the current DSL and DOM representation, nor does it propose specific solutions to address these limitations.
- What evidence would resolve it: A detailed analysis of the specific limitations of the current DSL and DOM representation, along with proposed solutions to address these limitations.

### Open Question 3
- Question: How does the auto-curriculum strategy used by WILBUR impact the agent's ability to learn and adapt to new websites and tasks, and what are the potential limitations of this approach?
- Basis in paper: [explicit] The paper states that WILBUR uses an auto-curriculum strategy with an LLM scoring step to the web agent task, allowing it to obtain high quality training data without human feedback.
- Why unresolved: The paper does not provide a detailed analysis of the impact of the auto-curriculum strategy on the agent's ability to learn and adapt to new websites and tasks, nor does it explore the potential limitations of this approach.
- What evidence would resolve it: A detailed analysis of the impact of the auto-curriculum strategy on the agent's ability to learn and adapt to new websites and tasks, along with an exploration of the potential limitations of this approach.

## Limitations
- Performance claims are based on a single benchmark (WebVoyager) with 15 websites
- Auto-curriculum may generate goals that don't fully capture real-world user intent and complexity
- Backtracking mechanism is constrained to navigation states only

## Confidence
- **High Confidence**: The core architecture design (demonstration retrieval, synthesis, and reflection mechanism) is well-specified and logically sound
- **Medium Confidence**: The 8% improvement over state-of-the-art is credible given the systematic approach
- **Medium Confidence**: The auto-curriculum approach is theoretically sound but its practical effectiveness depends on implementation details

## Next Checks
1. Test WILBUR on additional web benchmarks beyond WebVoyager to assess performance across diverse website types and real-world scenarios
2. Analyze the diversity and realism of goals generated by the auto-curriculum by comparing them to actual user queries and tasks
3. Measure the impact of the backtracking mechanism on different types of failures (immediate vs. delayed) to determine if it provides meaningful improvement beyond simple retry mechanisms