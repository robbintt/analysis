---
ver: rpa2
title: Equivariant Offline Reinforcement Learning
arxiv_id: '2406.13961'
source_url: https://arxiv.org/abs/2406.13961
tags:
- learning
- equivariant
- offline
- policy
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether offline reinforcement learning (RL)
  algorithms can effectively learn robotic manipulation policies from small, sub-optimal
  datasets. It introduces SO(2)-equivariant versions of Conservative Q-Learning (CQL)
  and Implicit Q-Learning (IQL) to improve performance in low-data regimes.
---

# Equivariant Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.13961
- Source URL: https://arxiv.org/abs/2406.13961
- Reference count: 40
- Key outcome: SO(2)-equivariant CQL and IQL outperform non-equivariant counterparts on rotation-symmetric robotic manipulation tasks with small datasets

## Executive Summary
This paper introduces SO(2)-equivariant neural networks for offline reinforcement learning (RL) to improve sample efficiency in robotic manipulation tasks. The authors show that encoding rotational symmetry into the network architecture significantly enhances performance, especially when learning from small, sub-optimal datasets. The equivariant actor-critic methods consistently outperform standard approaches across drawer opening, block-in-bowl, and block-stacking tasks.

## Method Summary
The authors implement SO(2)-equivariant versions of CQL and IQL using the E2CNN library with 8-steerable convolutional layers. The actor network outputs a mixed representation (invariant and equivariant components) for continuous actions, while the critic estimates invariant Q-values. Training uses expectile regression (τ=0.8) for IQL, CQL weight α=1, and learning rate 1e-4 with 100K gradient updates. Data augmentation with random SO(2) rotations is applied during training. The method is tested on Pybullet manipulation tasks with 2-channel state representations (top-down depth image + gripper status) across datasets of varying quality and sizes.

## Key Results
- Equivariant CQL achieves 100% success rate on drawer opening task from 1000 transitions, compared to 30% for non-equivariant CQL
- Equivariant IQL shows consistent performance improvements across all tasks, especially in low-data regimes
- Invariant critics significantly improve CQL performance by reducing overestimation of out-of-distribution actions
- Equivariant actor improves policy generalization across rotated states

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Equivariant architectures reduce the amount of data needed to learn optimal policies in rotation-symmetric tasks.
- Mechanism: By encoding the SO(2) symmetry into the neural network structure, the model can generalize across rotated versions of states and actions, effectively multiplying the data coverage without requiring additional samples.
- Core assumption: The robotic manipulation task exhibits SO(2) symmetry, meaning the optimal policy and value functions are invariant or equivariant to rotations in the xy-plane.
- Evidence anchors:
  - [abstract]: "Given that many robotic manipulation tasks can be formulated as rotation-symmetric problems, we investigate the use of SO(2)-equivariant neural networks..."
  - [section 4.1]: "In this work, we focus on the robotic manipulation tasks in the two-dimensional plane. This class of problems is described by SO(2)-invariant MDPs..."
  - [corpus]: Weak. The corpus contains papers on equivariant methods, but none specifically analyze the data efficiency mechanism in offline RL.
- Break condition: The task loses rotational symmetry (e.g., includes gravity-dependent elements like pushing tasks where direction matters).

### Mechanism 2
- Claim: Invariant critics improve Q-value accuracy for out-of-distribution actions by generalizing from rotated in-distribution actions.
- Mechanism: The invariant critic ensures consistent Q-value estimates for rotated state-action pairs, reducing overestimation errors for unseen actions that are rotations of seen ones.
- Core assumption: The critic needs to evaluate actions not present in the dataset, and these actions can be rotated versions of in-distribution actions.
- Evidence anchors:
  - [section 5.4.2]: "The invariant critic plays a crucial role in accurately evaluating out-of-distribution actions in CQL by generalizing from rotated versions of these actions present in the dataset..."
  - [section D]: "The invariant critic ensures that Q-values for unseen actions are minimized, while higher Q-values are assigned to unseen optimal actions in rotated states."
  - [corpus]: Weak. The corpus mentions equivariant diffusion policies but doesn't address critic invariance in offline RL.
- Break condition: The dataset contains all necessary rotations of actions, making generalization from invariance unnecessary.

### Mechanism 3
- Claim: Equivariant actors improve policy generalization by producing actions that transform correctly under rotations.
- Mechanism: The equivariant actor outputs actions that, when the state is rotated, produce rotated actions, maintaining consistency across symmetric states.
- Core assumption: The policy needs to generalize to unseen states, and these states can be rotated versions of seen ones.
- Evidence anchors:
  - [section 5.4.2]: "IQL with an equivariant actor and a non-invariant critic performs comparably to fully-equivariant IQL, whereas IQL with a non-equivariant actor and an invariant critic performs worse..."
  - [section 4.3]: "The actor network πθ : S → A × Aσ estimates a conditional isotropic Gaussian distribution over the action space... g ∈ C8 acts on a ∈ A as follows: ga = (ρequi(g)aequi, ainv, aσ)"
  - [corpus]: Weak. The corpus has papers on equivariant policies but none specifically analyze actor generalization in offline RL.
- Break condition: The state space coverage is so complete that generalization is unnecessary, or the task loses rotational symmetry.

## Foundational Learning

- Concept: Group theory and symmetry in reinforcement learning
  - Why needed here: The entire approach relies on understanding how SO(2) symmetry can be encoded in neural networks and how it affects MDPs.
  - Quick check question: What is the difference between a group-invariant and group-equivariant function in the context of RL?

- Concept: Conservative Q-Learning (CQL) and Implicit Q-Learning (IQL)
  - Why needed here: The paper builds equivariant versions of these specific offline RL algorithms, so understanding their mechanisms is crucial.
  - Quick check question: How does CQL prevent overestimation of Q-values for out-of-distribution actions?

- Concept: Neural network equivariance and invariance
  - Why needed here: The performance improvements come from properly implementing equivariant/invariant architectures, which requires understanding the technical details.
  - Quick check question: How does a convolutional layer become SO(2)-equivariant?

## Architecture Onboarding

- Component map: 2-channel image → SO(2)-equivariant convolutional encoder → [Actor/Critic] → Action/Value
- Critical path: State → Encoder → [Actor/Critic] → Action/Value
  The encoder is the key component that introduces equivariance.
- Design tradeoffs:
  - Using equivariant networks increases computational complexity but reduces data requirements
  - Invariant critics help CQL but may be unnecessary for IQL due to its different sampling strategy
  - Mixed representation outputs handle the factored action space (invariant and equivariant components)
- Failure signatures:
  - Poor performance on tasks without rotational symmetry
  - Inconsistent Q-values for rotated state-action pairs (critic invariance broken)
  - Actions that don't transform correctly under rotation (actor equivariance broken)
- First 3 experiments:
  1. Train equivariant vs non-equivariant CQL on a simple rotation-symmetric task with small dataset
  2. Test critic invariance by querying Q-values for rotated versions of in-distribution state-action pairs
  3. Compare IQL with equivariant actor vs invariant actor on tasks with varying state space coverage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance improvement of equivariant methods persist with even larger datasets beyond 1000 transitions?
- Basis in paper: [explicit] The authors state that the performance gap between Equi-IQL and Non-Equi IQL narrows with larger datasets, but the gap remains significant for CQL.
- Why unresolved: The experiments only tested datasets up to 1000 transitions, so it's unclear how the methods scale with much larger datasets.
- What evidence would resolve it: Testing the equivariant and non-equivariant versions on datasets with significantly more transitions (e.g., 5000 or 10000) to see if the performance gap continues to narrow or remains stable.

### Open Question 2
- Question: How does the performance of equivariant methods change when the underlying MDP symmetry is not perfectly matched by the equivariant network (e.g., SO(2) symmetry but using SO(3) equivariant networks)?
- Basis in paper: [explicit] The authors mention that recent research has shown performance improvements even when the equivariant constraints do not perfectly match the domain symmetry.
- Why unresolved: The paper only tests SO(2) equivariant methods on SO(2) symmetric tasks, so it's unclear how well the methods perform when the symmetry is mismatched.
- What evidence would resolve it: Testing equivariant methods with different group symmetries (e.g., SO(3) on SO(2) tasks) and comparing their performance to non-equivariant and correctly matched equivariant methods.

### Open Question 3
- Question: What is the impact of the invariant critic on the performance of IQL compared to CQL?
- Basis in paper: [inferred] The authors note that the invariant critic does not significantly contribute to IQL's performance improvement, unlike for CQL.
- Why unresolved: The paper does not provide a detailed analysis of why the invariant critic is less important for IQL, despite both algorithms using a similar actor-critic architecture.
- What evidence would resolve it: A detailed ablation study comparing IQL with and without an invariant critic, and an analysis of the Q-value distributions for out-of-distribution actions in both cases.

## Limitations

- Equivariant approach is limited to SO(2)-symmetric tasks, reducing applicability to manipulation problems without rotational symmetry
- Performance may degrade when symmetry is latent (e.g., side-view images) rather than explicit in top-down representations
- Computational complexity increases due to equivariant network layers, with unclear trade-offs against data efficiency gains
- Theoretical analysis of why equivariant architectures improve offline RL is limited to empirical observations

## Confidence

**High confidence**: The empirical results showing equivariant CQL and IQL outperform their non-equivariant counterparts on rotation-symmetric tasks with small datasets. The performance improvements are consistent across multiple tasks and dataset qualities.

**Medium confidence**: The mechanism explanations for why equivariance improves performance - particularly the claims about invariant critics reducing overestimation and equivariant actors improving generalization. While supported by ablation studies, the theoretical foundations could be stronger.

**Low confidence**: The generalization of results to tasks without explicit SO(2) symmetry or to different state representations (e.g., side-view images). The paper doesn't extensively test these boundaries.

## Next Checks

1. **Symmetry boundary test**: Evaluate the equivariant methods on manipulation tasks that break rotational symmetry (e.g., gravity-dependent pushing tasks) to quantify performance degradation and identify the exact breaking point.

2. **Representation robustness test**: Train the same equivariant architectures on side-view RGB-D images where SO(2) symmetry is latent rather than explicit, comparing against non-equivariant baselines with data augmentation.

3. **Computational efficiency analysis**: Systematically compare the wall-clock training time and parameter count between equivariant and non-equivariant models, establishing the trade-off curve between data efficiency gains and computational overhead across different dataset sizes.