---
ver: rpa2
title: 'MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and
  Dynamic Distance Constraint'
arxiv_id: '2402.14244'
source_url: https://arxiv.org/abs/2402.14244
tags:
- uni00000013
- uni00000011
- learning
- reward
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of sparse rewards in hierarchical
  reinforcement learning (HRL) by proposing MENTOR, a framework that integrates human
  feedback and dynamic distance constraints. The core idea is to use human feedback
  to guide high-level policy learning for better subgoal generation, while employing
  a Dynamic Distance Constraint (DDC) mechanism to adjust subgoal difficulty according
  to the low-level policy's capabilities.
---

# MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint

## Quick Facts
- arXiv ID: 2402.14244
- Source URL: https://arxiv.org/abs/2402.14244
- Reference count: 40
- Introduces MENTOR framework combining human feedback with dynamic distance constraints to address sparse rewards in HRL

## Executive Summary
MENTOR is a hierarchical reinforcement learning framework that addresses the challenge of sparse rewards by integrating human feedback to guide high-level policy learning and employing a Dynamic Distance Constraint mechanism to adjust subgoal difficulty based on low-level policy capabilities. The framework also introduces an Exploration-Exploitation Decoupling approach to stabilize low-level policy training. Extensive evaluations across multiple domains including FetchPush, FetchPickAndPlace, FetchDraw, FetchObsPush, Pusher, and Four rooms demonstrate superior performance compared to baselines, achieving higher success rates and faster convergence with minimal human feedback.

## Method Summary
MENTOR operates through a two-level hierarchical structure where human feedback guides the high-level policy for better subgoal generation, while a Dynamic Distance Constraint (DDC) mechanism dynamically adjusts subgoal difficulty according to the low-level policy's capabilities. The framework incorporates an Exploration-Exploitation Decoupling (EED) approach to prevent non-stationarity in low-level policy training. The high-level policy learns to generate meaningful subgoals based on human feedback, while the DDC mechanism ensures these subgoals remain appropriately challenging but achievable, preventing the exploration-exploitation dilemma that often plagues hierarchical RL systems.

## Key Results
- Achieved higher success rates compared to baseline HRL methods across all tested environments
- Demonstrated faster convergence in training across FetchPush, FetchPickAndPlace, FetchDraw, FetchObsPush, Pusher, and Four rooms domains
- Successfully operated with only small amounts of human feedback while maintaining superior performance

## Why This Works (Mechanism)
The framework works by leveraging human feedback to provide meaningful supervision for the high-level policy, which generates subgoals that are then filtered and adjusted by the Dynamic Distance Constraint mechanism based on the low-level policy's current capabilities. The Exploration-Exploitation Decoupling approach prevents the low-level policy from becoming non-stationary by separating the exploration phase from exploitation, allowing for more stable and effective learning. This combination addresses the core challenges of sparse rewards in HRL by ensuring that subgoals are both meaningful (through human guidance) and appropriately challenging (through DDC), while maintaining stable low-level policy learning.

## Foundational Learning
- **Hierarchical Reinforcement Learning**: Why needed: Enables decomposition of complex tasks into manageable subtasks. Quick check: Verify subgoal hierarchy structure aligns with task complexity.
- **Human Feedback Integration**: Why needed: Provides meaningful supervision for high-level policy in sparse reward environments. Quick check: Ensure feedback quality and consistency across training episodes.
- **Dynamic Distance Constraint**: Why needed: Adjusts subgoal difficulty based on agent capability to prevent exploration-exploitation dilemma. Quick check: Monitor subgoal success rates and adjust constraints accordingly.
- **Exploration-Exploitation Decoupling**: Why needed: Stabilizes low-level policy training by separating exploration from exploitation phases. Quick check: Track policy performance stability across training iterations.
- **Sparse Reward Handling**: Why needed: Addresses the fundamental challenge of learning in environments with infrequent reward signals. Quick check: Evaluate success rate improvements compared to dense reward baselines.

## Architecture Onboarding

Component Map: Human Feedback -> High-Level Policy -> Dynamic Distance Constraint -> Low-Level Policy -> Environment

Critical Path: Human feedback provides subgoal guidance → High-level policy generates subgoals → DDC adjusts difficulty → Low-level policy attempts subgoals → Environment provides sparse rewards → Both policies update based on feedback

Design Tradeoffs: The framework trades off between human feedback dependency and autonomous learning capability. While human feedback provides meaningful guidance, it introduces potential scalability and consistency issues. The DDC mechanism adds computational overhead but enables more stable learning. EED increases training complexity but prevents non-stationarity issues.

Failure Signatures: 
- Poor subgoal generation despite good human feedback indicates high-level policy learning issues
- Consistently failed subgoals suggest DDC mechanism misconfiguration
- Unstable low-level policy performance points to EED mechanism problems
- Low overall success rates may indicate misalignment between high-level guidance and low-level execution capabilities

First 3 Experiments:
1. Test MENTOR performance with varying amounts of human feedback to establish the minimum effective feedback threshold
2. Evaluate DDC mechanism effectiveness by comparing performance with static distance constraints
3. Assess EED contribution by comparing against standard HRL baselines without exploration-exploitation decoupling

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on human feedback may not scale well to real-world applications with complex or ambiguous tasks
- Limited evaluation in simulated environments may not capture real-world challenges of feedback collection and environmental dynamics
- Lack of extensive ablation studies to isolate individual contributions of core mechanisms
- Insufficient validation of framework's robustness to noisy or inconsistent human feedback

## Confidence
- Performance claims in simulated environments: Medium
- Scalability to real-world applications: Low
- Robustness to feedback quality variations: Low
- Contribution of individual mechanisms: Medium

## Next Checks
1. Conduct extensive ablation studies to quantify the individual contributions of human feedback guidance, Dynamic Distance Constraint, and Exploration-Exploitation Decoupling to overall performance improvements.

2. Test MENTOR in real-world robotic manipulation tasks with actual human feedback collection to evaluate practical applicability and robustness to feedback quality variations.

3. Implement a noise injection study where human feedback is intentionally made inconsistent or erroneous to assess framework's resilience to suboptimal guidance and identify potential failure modes.