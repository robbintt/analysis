---
ver: rpa2
title: Dynamic Data Pruning for Automatic Speech Recognition
arxiv_id: '2406.18373'
source_url: https://arxiv.org/abs/2406.18373
tags:
- data
- pruning
- training
- dropping
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first investigation of dynamic data pruning
  for automatic speech recognition (ASR), demonstrating that pruning 30% of instances
  with a 30% chunk dropping rate maintains performance compared to full-data training.
  The proposed Dynamic Data Pruning for ASR (DDP-ASR) method incorporates instance-wise
  and fine-grained time-wise pruning strategies, achieving up to 1.6x training speedups
  with negligible performance loss.
---

# Dynamic Data Pruning for Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2406.18373
- Source URL: https://arxiv.org/abs/2406.18373
- Reference count: 0
- Key outcome: DDP-ASR maintains performance with 30% data pruning and achieves 1.6x speedups

## Executive Summary
This paper introduces Dynamic Data Pruning for Automatic Speech Recognition (DDP-ASR), the first investigation of dynamic data pruning specifically for ASR tasks. The method combines instance-wise pruning with time-wise dropping strategies to accelerate training while maintaining or improving performance. Using the Easy2hard scheduling method, the approach dynamically selects 70% of training instances and achieves up to 1.6x training speedups with negligible performance loss compared to full-data training.

## Method Summary
DDP-ASR employs a two-pronged pruning strategy: instance-wise pruning that removes entire sequences based on loss-based difficulty scores, and time-wise dropping that removes consecutive chunks of audio samples. The Easy2hard method dynamically schedules instance presentation during training, starting with easier examples and gradually incorporating harder ones. The approach uses a conformer-based ASR backbone with combined CTC and Cross-Entropy loss, trained with AdamW optimizer and cosine learning rate scheduler on Librispeech and LRS3 datasets.

## Key Results
- 1.6x training speedup achieved with 30% instance pruning and 30% chunk dropping
- 70% data retention maintains full-data performance using Easy2hard scheduling
- Time-wise dropping provides greater robustness to lower sampling rates (16kHz to 11kHz)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic Data Pruning (DDP) can maintain ASR performance by selecting 70% of the data using Easy2hard scheduling.
- Mechanism: Easy2hard dynamically adjusts the difficulty of training instances over time, starting with easier examples and gradually incorporating harder ones as training progresses, allowing the model to converge efficiently without seeing all data.
- Core assumption: The Easy2hard method correctly ranks instances by difficulty via loss values, and the gradual shift from easy to hard preserves generalization while reducing redundancy.
- Evidence anchors:
  - [abstract]: "we can reach the full-data performance by dynamically selecting 70% of data"
  - [section 2.2.1]: "Easy2hard... dynamically schedules the presentation of instances to the model during training"
  - [corpus]: Weak — corpus neighbors do not mention Easy2hard or curriculum learning.

### Mechanism 2
- Claim: Chunk-wise dropping preserves more temporal dependencies than point-wise dropping, improving robustness to lower sampling rates.
- Mechanism: By removing consecutive samples in fixed-size chunks rather than random individual samples, chunk dropping minimizes disruption to local temporal patterns crucial for ASR.
- Core assumption: Consecutive samples contain coherent temporal information; removing them in chunks causes less distortion than scattered removals.
- Evidence anchors:
  - [section 4.2]: "chunk dropping... can mitigate some of the performance declines... impact on performance is minimal"
  - [section 4.3]: "models trained using time-wise dropping exhibit greater robustness across different sampling rates"
  - [corpus]: Weak — corpus neighbors do not discuss time-wise pruning or sampling rate robustness.

### Mechanism 3
- Claim: Combining instance-wise and time-wise pruning enables greater speedups without sacrificing accuracy, especially when training data is limited.
- Mechanism: Instance-wise pruning removes entire sequences, saving compute per epoch; time-wise dropping removes portions of sequences, further reducing compute per batch; together they amplify efficiency gains.
- Core assumption: Both pruning methods operate independently and their effects are additive without interfering with each other's benefits.
- Evidence anchors:
  - [section 4.3]: "combining instance- and time-wise data pruning... leads to an observable reduction in WER"
  - [section 4.4]: "integrating both time-masking and time-dropping approaches... achieving enhanced efficiency and comparable performance"
  - [corpus]: Weak — corpus neighbors do not cover combined pruning strategies.

## Foundational Learning

- Concept: Curriculum learning and difficulty scheduling
  - Why needed here: Easy2hard is a curriculum learning variant that orders training data by difficulty to accelerate convergence while maintaining performance.
  - Quick check question: What metric does Easy2hard use to rank instances, and how does it change over training epochs?

- Concept: Temporal dependency preservation in audio
  - Why needed here: Chunk-wise dropping relies on preserving local temporal coherence; understanding how audio signals are structured is critical for pruning without degrading ASR.
  - Quick check question: Why does removing consecutive samples in chunks affect ASR less than removing random samples?

- Concept: Loss-based instance scoring
  - Why needed here: The pruning score H_t is based on loss values to determine which instances are "easy" or "hard" at each epoch.
  - Quick check question: How is the loss-based score updated dynamically during training, and why is it sufficient without extra computation?

## Architecture Onboarding

- Component map: Data loading -> instance scoring -> pruning selection -> chunk dropping -> model forward/backward pass -> loss computation
- Critical path: Data loading → instance scoring → pruning selection → chunk dropping → model forward/backward pass → loss computation. Any bottleneck here (e.g., slow scoring) directly impacts training speed.
- Design tradeoffs: Larger chunk sizes increase speed but risk removing more critical signal; stricter pruning ratios save time but may reduce accuracy; loss-based scoring is cheap but may not perfectly capture difficulty.
- Failure signatures: Sudden WER spikes after pruning updates; inconsistent training loss curves; model failing on long utterances; overfitting to easy instances; excessive GPU idle time due to data filtering.
- First 3 experiments:
  1. Train with Easy2hard instance-wise pruning at 70% kept ratio; measure WER vs baseline full-data training.
  2. Apply chunk dropping with 30% drop rate on top of Easy2hard; measure WER and epoch wall-clock time.
  3. Vary sampling rate from 16kHz to 11kHz; evaluate model robustness with and without time-wise dropping.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between instance-wise and time-wise pruning strategies across different ASR model architectures and dataset sizes?
- Basis in paper: [explicit] The paper demonstrates that combining both pruning strategies can yield performance benefits, but notes that the optimal balance depends on factors like dataset size and computational resources
- Why unresolved: The experiments only explored a limited set of combinations on specific datasets and model architectures, leaving open questions about generalizability to other scenarios
- What evidence would resolve it: Systematic ablation studies across multiple ASR architectures (transformer-based, RNN-based), dataset sizes, and pruning ratio combinations

### Open Question 2
- Question: How does dynamic data pruning affect the robustness of ASR models to real-world noise conditions and domain shifts?
- Basis in paper: [inferred] The paper mentions that time-wise pruned models show greater robustness to lower sampling rates, suggesting potential benefits for robustness, but does not comprehensively evaluate against real-world noise conditions
- Why unresolved: The experiments primarily focus on clean test sets and controlled sampling rate changes, without testing against actual noise conditions or domain-shifted data
- What evidence would resolve it: Comprehensive testing on noisy datasets (e.g., CHiME, Aurora) and domain-shifted data (different accents, recording conditions)

### Open Question 3
- Question: What are the long-term effects of dynamic data pruning on model convergence and final performance compared to full-data training?
- Basis in paper: [explicit] The paper demonstrates that models can reach full-data performance with 70% of data using Easy2hard, but doesn't investigate whether this holds across different training durations or with extended training
- Why unresolved: The experiments are limited to specific training durations (75 epochs) without exploring whether the observed benefits persist with longer training or different convergence behaviors
- What evidence would resolve it: Extended training experiments comparing convergence curves and final performance across various pruning strategies and training durations

## Limitations
- The Easy2hard method relies on loss-based difficulty scoring that may not generalize to diverse ASR datasets with different acoustic conditions
- Chunk dropping assumes fixed temporal dependencies that might not hold across all speech varieties or languages
- The study focuses on conformer-based architectures, leaving open questions about performance with transformer or RNN-based models

## Confidence
- **High confidence**: The 1.6x speedup claim with 30% data pruning is well-supported by controlled experiments showing consistent WER maintenance across multiple pruning ratios
- **Medium confidence**: The time-wise dropping robustness to lower sampling rates is demonstrated but only tested on a narrow range (16kHz to 11kHz) with limited ablation
- **Medium confidence**: The Easy2hard scheduling superiority over static pruning methods is shown, but the paper doesn't explore alternative curriculum learning approaches for comparison

## Next Checks
1. Test the Easy2hard method on a noisy, multi-accent ASR dataset (e.g., TED-LIUM or Common Voice) to assess generalization beyond clean Librispeech speech
2. Conduct a systematic ablation study varying chunk sizes (from 10ms to 100ms) to identify optimal temporal preservation across different speech rates and phoneme boundaries
3. Implement and compare against alternative curriculum learning strategies (e.g., self-paced learning or transfer-based difficulty ranking) to validate that Easy2hard is indeed optimal for ASR pruning