---
ver: rpa2
title: 'Mitigating Social Bias in Large Language Models: A Multi-Objective Approach
  within a Multi-Agent Framework'
arxiv_id: '2412.15504'
source_url: https://arxiv.org/abs/2412.15504
tags:
- bias
- moma
- social
- llms
- person
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of mitigating social bias in
  large language models (LLMs) while maintaining downstream performance. The proposed
  MOMA framework uses a multi-agent approach with causal intervention to strategically
  modify bias-related content in input prompts, breaking the connection between these
  contents and biased outputs.
---

# Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework

## Quick Facts
- arXiv ID: 2412.15504
- Source URL: https://arxiv.org/abs/2412.15504
- Reference count: 17
- Up to 87.7% bias reduction with only 6.8% performance degradation

## Executive Summary
This paper addresses the challenge of mitigating social bias in large language models (LLMs) while maintaining downstream performance. The proposed MOMA framework uses a multi-agent approach with causal intervention to strategically modify bias-related content in input prompts, breaking the connection between these contents and biased outputs. Through masking and balancing agents, MOMA achieves up to 87.7% reduction in bias scores on the BBQ dataset with only 6.8% performance degradation, and improves the icat metric by up to 58.1% on StereoSet. The method demonstrates superior performance compared to traditional debiasing techniques while requiring fewer computational resources than other multi-agent approaches.

## Method Summary
MOMA employs a hierarchical multi-agent framework consisting of masking and balancing agents that modify input prompts before task execution. The masking agent removes social group identifiers using causal intervention to break bias-inducing shortcuts, while the balancing agent restores necessary social group information with positive adjective augmentation. This approach achieves multi-objective optimization by separating task execution from value incorporation, minimizing the "alignment tax" typically seen in single-agent approaches. The framework requires only two additional model calls compared to six for traditional multi-agent methods.

## Key Results
- Achieved 87.7% reduction in BBQ bias score with only 6.8% accuracy degradation
- Improved icat metric by up to 58.1% on StereoSet dataset
- Demonstrated superior performance compared to CoT, ABP variants, SoM, and SC baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MOMA's masking agent reduces bias by breaking the shortcut connection between social group identifiers and biased outputs through causal intervention.
- Mechanism: The masking agent removes social group identifiers (Xsg) from the input prompt, creating a neutral context where the unobserved variable U (which induces bias) cannot directly influence the output. This is implemented as X' = X - X̃sg.
- Core assumption: Social group identifiers in prompts create direct pathways for bias to manifest in LLM outputs, and removing these identifiers while preserving task context is sufficient to reduce bias without harming performance.
- Evidence anchors: [abstract] "breaking the shortcut connection between these contents and the corresponding answers"; [section 3.1] "We manipulateX through the transformation function gθ to achieve a better Y denoted as Y ′ below"; [corpus] Weak evidence - related work on bias mitigation but no direct causal intervention studies
- Break condition: If social group identifiers are essential for task comprehension or if bias manifests through implicit rather than explicit social group references.

### Mechanism 2
- Claim: MOMA's balancing agent restores necessary social group information while mitigating bias through positive adjective augmentation.
- Mechanism: The balancing agent reintroduces masked social group attributes with carefully selected positive adjectives that enhance representation of underrepresented groups, implemented as X' = (X - X̃sg) + X̃'sg where X̃'sg contains adjective-modified social groups.
- Core assumption: Adding positive adjectives to social group representations can balance disparities between groups without introducing new biases or task-irrelevant information.
- Evidence anchors: [section 3.2] "The balancing agent strategically employs balancing words or counterfactual adjectives to foster a balanced representation"; [section 4.4] "We use positive adjectives to modify social groups' representations mainly because it creates the least ∆Xother"; [corpus] Weak evidence - limited corpus evidence for adjective-based balancing, mostly theoretical justification
- Break condition: If adjective selection introduces new stereotypes, if positive adjectives create unrealistic representations, or if the LLM overweights the adjectives in reasoning.

### Mechanism 3
- Claim: MOMA's hierarchical multi-agent framework achieves multi-objective optimization by separating task execution from value incorporation.
- Mechanism: Task agents execute operations in isolation from human values H, while assistant agents incorporate H to generate modified inputs X'. This separation minimizes "alignment tax" (performance degradation from value alignment).
- Core assumption: Separating the incorporation of human values from task execution reduces the negative impact on performance while maintaining bias reduction capabilities.
- Evidence anchors: [abstract] "MOMA substantially reduces bias while maintaining accuracy in downstream tasks"; [section 3.2] "This separation allows assistant agents to interact with H in a controllable manner, reducing the 'alignment tax'"; [section 4.2] "while LLMs can reduce bias with H, this often comes at the cost of significant performance drops"
- Break condition: If the separation between task and value agents introduces coordination overhead that outweighs benefits, or if assistant agent modifications are too extensive.

## Foundational Learning

- Concept: Causal inference theory
  - Why needed here: Understanding how unobserved variables U induce bias through input-output mappings in LLMs, and how interventions can break these causal pathways
  - Quick check question: How does Pearl's do-calculus apply to modifying LLM inputs to break bias-inducing causal pathways?

- Concept: Multi-objective optimization
  - Why needed here: Balancing the competing objectives of bias reduction and task performance requires understanding Pareto optimality and trade-off analysis
  - Quick check question: What distinguishes a Pareto superior solution from a Pareto optimal solution in the context of debiasing?

- Concept: Social bias taxonomy
  - Why needed here: Identifying and categorizing the nine bias dimensions (Age, Disability status, Gender identity, Nationality, Physical appearance, Race/ethnicity, Religion, Socioeconomic status, Sexual orientation) is essential for comprehensive bias mitigation
  - Quick check question: How do different bias dimensions manifest differently in LLM outputs, and why does this matter for intervention strategies?

## Architecture Onboarding

- Component map: Input → Masking Agent → Balancing Agent → Task Agent → LLM → Output
- Critical path: Input → Masking Agent → Balancing Agent → Task Agent → LLM → Output. The key is that masking and balancing occur before the main task execution, creating a linear thinking process with only two extra model calls.
- Design tradeoffs: Masking provides maximum bias reduction but loses social group information; balancing restores information but increases bias slightly; using multiple agents increases computational cost but improves multi-objective performance.
- Failure signatures: High bias scores indicate masking is insufficient or balancing is introducing new biases; low accuracy indicates over-aggressive masking or balancing that disrupts task comprehension; inconsistent results across different prompts suggest sensitivity to human values H.
- First 3 experiments:
  1. Test masking alone on BBQ dataset to establish baseline bias reduction (target: 80%+ bias score reduction)
  2. Test balancing alone with different adjective styles (neutral, unfair positive, fair positive, counterfactual) to identify optimal approach
  3. Test full MOMA pipeline comparing masking vs balancing variants on StereoSet to validate multi-objective performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would MOMA perform on tasks beyond question-answering, such as text generation or summarization, where bias might manifest differently?
- Basis in paper: [inferred] The paper acknowledges its focus on question-answering datasets and mentions that "bias is also present in other tasks (Gallegos et al. 2024a)".
- Why unresolved: The study deliberately limits itself to QA datasets to simplify analysis, but doesn't explore how the method would handle other NLP tasks where bias could manifest in more complex ways.
- What evidence would resolve it: Experiments applying MOMA to diverse NLP tasks like text generation, summarization, or sentiment analysis, with appropriate bias metrics for each task type.

### Open Question 2
- Question: What is the long-term stability of MOMA's debiasing effects when models are continuously fine-tuned or exposed to new data?
- Basis in paper: [inferred] The paper doesn't address how the debiasing effects would persist over time as models evolve or encounter new data distributions.
- Why unresolved: The experiments focus on static evaluations, but real-world deployment involves continuous learning and adaptation.
- What evidence would resolve it: Longitudinal studies tracking bias levels and performance metrics across multiple iterations of model updates, fine-tuning, or retraining with new data.

### Open Question 3
- Question: How does MOMA's performance compare to human-level debiasing, and could human feedback further improve the method?
- Basis in paper: [inferred] The paper mentions "human values" but doesn't directly compare MOMA's debiasing quality to human judgments or explore incorporating human feedback loops.
- Why unresolved: While the method uses human values as guidance, it doesn't establish a benchmark against human-level bias detection or leverage interactive human feedback.
- What evidence would resolve it: Comparative studies measuring MOMA's outputs against human-annotated bias-free responses, and experiments incorporating human-in-the-loop feedback to refine the debiasing process.

## Limitations

- The causal intervention approach may not account for implicit bias manifestations or context-dependent social cues that persist in language
- The effectiveness of positive adjective augmentation relies heavily on careful adjective selection with limited empirical validation across diverse social groups
- Computational efficiency claims compared to traditional multi-agent approaches lack detailed benchmarking of resource usage patterns

## Confidence

- High confidence: The reported bias reduction metrics (87.7% on BBQ, 58.1% icat improvement on StereoSet) are well-supported by the experimental results and comparison against multiple baselines
- Medium confidence: The causal intervention mechanism's theoretical foundation is sound, but the practical effectiveness depends heavily on implementation details not fully specified in the paper
- Medium confidence: The multi-objective optimization claims are supported by quantitative results, though the long-term stability and generalizability across different LLM architectures remain untested

## Next Checks

1. Test MOMA's performance on additional bias dimensions not covered in BBQ and StereoSet to verify generalizability across the nine identified social bias categories
2. Conduct ablation studies removing the causal intervention component to quantify its specific contribution versus standard masking techniques
3. Evaluate MOMA's behavior on out-of-distribution prompts and cross-cultural contexts to assess robustness beyond the training datasets