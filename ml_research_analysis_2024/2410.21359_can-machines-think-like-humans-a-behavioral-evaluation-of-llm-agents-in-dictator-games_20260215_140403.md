---
ver: rpa2
title: Can Machines Think Like Humans? A Behavioral Evaluation of LLM Agents in Dictator
  Games
arxiv_id: '2410.21359'
source_url: https://arxiv.org/abs/2410.21359
tags: []
core_contribution: This study investigates whether large language model (LLM) agents
  can replicate human prosocial behavior by examining their decision-making in dictator
  games under different personas and experimental framings. We evaluated 10 popular
  LLM models from four families (Llama3.1, Gemma2, Qwen2.5, Phi3) plus GPT-4o across
  10,000 trials each, manipulating demographic traits, personality types, social distance,
  and framing (Give vs.
---

# Can Machines Think Like Humans? A Behavioral Evaluation of LLM Agents in Dictator Games

## Quick Facts
- arXiv ID: 2410.21359
- Source URL: https://arxiv.org/abs/2410.21359
- Reference count: 40
- Primary result: LLM agents fail to replicate human prosocial behavior patterns in dictator games

## Executive Summary
This study investigates whether large language model (LLM) agents can replicate human prosocial behavior by examining their decision-making in dictator games under different personas and experimental framings. We evaluated 10 popular LLM models from four families (Llama3.1, Gemma2, Qwen2.5, Phi3) plus GPT-4o across 10,000 trials each, manipulating demographic traits, personality types, social distance, and framing (Give vs. Take). Results show LLM agents fail to consistently replicate human decision-making patternsâ€”most exhibit bimodal distributions (choosing 0 or half the stake) rather than the continuous distribution seen in humans.

The alignment with human behavior varies substantially across model architectures and prompt formulations without clear predictability. These findings suggest LLMs lack the underlying causal models and intuitive theories of human psychology needed to replicate nuanced prosocial decision-making, highlighting limitations for using LLMs as stand-ins for human participants in social research.

## Method Summary
The study evaluated 10 LLM models (Llama3.1, Gemma2, Qwen2.5, Phi3, and GPT-4o) across 10,000 trials each in dictator games. The experimental design manipulated demographic traits (age, gender, nationality), personality types (Big Five), social distance between participants, and framing conditions (Give vs. Take). Each model received prompts with these variables systematically varied, and responses were analyzed for distribution patterns compared to established human behavioral data.

## Key Results
- LLM agents consistently exhibit bimodal distributions (0 or half the stake) rather than the continuous distribution seen in human behavior
- Some models align with human behavior on specific factors like social distance effects, but no consistent patterns emerge across models or families
- Behavioral alignment varies substantially across model architectures and prompt formulations without clear predictability

## Why This Works (Mechanism)
Assumption: The bimodal distributions observed in LLM responses suggest these models lack the nuanced probabilistic reasoning underlying human prosocial decision-making. Instead of capturing the full spectrum of human generosity, LLMs appear to default to extreme or midpoint choices, possibly reflecting their training data distribution or the binary nature of many supervised learning objectives. The variability across models and prompt formulations indicates that different architectural choices and fine-tuning approaches significantly impact how these models approximate human social reasoning, but none successfully capture the continuous, context-dependent nature of human prosocial behavior.

## Foundational Learning
Unknown: The paper does not explicitly detail which specific human behavioral datasets or theoretical frameworks from behavioral economics were used as ground truth for comparison. The evaluation appears to rely on established human behavioral patterns from dictator game literature, but the exact sources and methodologies for human data collection are not specified in the available abstract and methodology description.

## Architecture Onboarding
Unknown: The report does not provide detailed information about the specific architectural differences between the evaluated models (Llama3.1, Gemma2, Qwen2.5, Phi3, GPT-4o) or how their distinct training approaches might influence their performance in dictator games. While the study mentions evaluating multiple model families, it does not explain how architectural variations contribute to the observed behavioral differences.

## Open Questions the Paper Calls Out
- Why do certain LLM models show better alignment with human behavior on specific factors like social distance, despite the overall lack of consistent patterns?
- Can fine-tuning on human behavioral datasets improve LLM alignment with nuanced prosocial decision-making patterns?
- Do different prompt engineering approaches or persona assignments significantly impact the ability of LLMs to replicate human-like distributions in dictator games?
- How would these models perform in more complex social scenarios that require deeper intuitive theories of human psychology beyond simple resource allocation?

## Limitations
- LLM agents fail to replicate nuanced, continuous distribution of human prosocial behavior
- Behavioral alignment varies substantially across models without clear predictability
- The study does not provide detailed information about the human behavioral datasets used for comparison
- Limited explanation of how specific architectural differences between models contribute to behavioral variations
- The evaluation focuses on a single type of social decision-making scenario (dictator games)

## Confidence
- High confidence that LLM agents cannot reliably simulate human prosocial decision-making based on the consistent observation of bimodal distributions across models
- Medium confidence in specific model-to-human alignment patterns due to substantial variability and lack of clear predictability across different models and prompt formulations
- Medium confidence in the interpretation of why these patterns emerge, given the limited architectural and training detail provided

## Next Checks
1. Test whether fine-tuning on human behavioral datasets can improve LLM alignment with human prosocial patterns
2. Examine whether different prompt engineering approaches can elicit more human-like distributions
3. Extend evaluation to more complex social scenarios beyond dictator games to assess performance in richer decision contexts