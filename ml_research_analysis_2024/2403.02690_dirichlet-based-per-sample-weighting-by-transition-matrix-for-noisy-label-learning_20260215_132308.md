---
ver: rpa2
title: Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning
arxiv_id: '2403.02690'
source_url: https://arxiv.org/abs/2403.02690
tags:
- rent
- label
- noisy
- matrix
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a Dirichlet-based per-sample weighting framework\
  \ for noisy label learning. It unifies reweighting and resampling under one formulation,\
  \ showing resampling can outperform reweighting when the shape parameter \u03B1\
  \ \u2192 0."
---

# Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning

## Quick Facts
- arXiv ID: 2403.02690
- Source URL: https://arxiv.org/abs/2403.02690
- Authors: HeeSun Bae; Seungjae Shin; Byeonghu Na; Il-Chul Moon
- Reference count: 40
- Introduces RENT, a resampling method that uses the noise transition matrix to select clean samples more effectively than existing transition matrix utilization methods

## Executive Summary
This paper presents a unified Dirichlet-based framework for per-sample weighting in noisy label learning that bridges reweighting and resampling approaches. The key insight is that resampling (α → 0) can outperform reweighting when leveraging transition matrices for noise correction. Based on this theoretical foundation, the authors propose RENT (Resampling with Noise Transition matrix), a method that uses the estimated noise transition matrix to identify and resample clean samples more effectively than previous approaches.

## Method Summary
The proposed framework formulates per-sample weighting through a Dirichlet distribution parameterized by α and the noise transition matrix T. When α approaches infinity, the method behaves like standard reweighting; when α approaches zero, it behaves like resampling. RENT implements the α → 0 case, using the transition matrix to compute the probability of each sample being clean and resampling accordingly. This approach is designed to work with any T-estimation method and requires minimal hyperparameter tuning while maintaining computational efficiency.

## Key Results
- RENT consistently outperforms existing transition matrix utilization methods (Forward, reweighting) across CIFAR-10/100 with symmetric and asymmetric noise
- Shows superior performance on real-world datasets (CIFAR-10N, Clothing1M) compared to state-of-the-art baselines
- Demonstrates robustness to class imbalance while maintaining computational efficiency
- Effectively filters noisy samples through its resampling mechanism

## Why This Works (Mechanism)
The method works by exploiting the relationship between the noise transition matrix and sample cleanliness. When α → 0, the Dirichlet distribution concentrates on the mode, allowing RENT to focus sampling on instances most likely to be correctly labeled according to the transition matrix. This creates a form of active learning where the model identifies and prioritizes samples that will provide the most reliable gradient updates for training.

## Foundational Learning
- Noise transition matrix estimation: Essential for understanding how labels are corrupted; quick check: verify T estimation accuracy on validation set
- Dirichlet distribution properties: Underpins the weighting framework; quick check: confirm α parameter controls concentration appropriately
- Resampling vs reweighting: Different strategies for handling noisy labels; quick check: compare gradient variance between approaches
- Sample selection bias: Risk of systematic exclusion of certain samples; quick check: monitor class distribution in resampled batches
- Computational complexity trade-offs: Resampling can be expensive; quick check: measure wall-clock time vs reweighting baseline

## Architecture Onboarding
Component map: Data loader -> T-estimation module -> RENT weighting module -> Loss computation -> Model update
Critical path: T estimation → RENT probability computation → Resampling → Model training
Design tradeoffs: Resampling provides cleaner gradients but increases computational overhead compared to reweighting
Failure signatures: Poor T estimation leads to incorrect sample selection; overly aggressive resampling (α too small) may exclude useful samples
First experiments: 1) Validate T estimation accuracy, 2) Compare α=1 vs α→0 behavior, 3) Test on small synthetic noise dataset before full experiments

## Open Questions the Paper Calls Out
None

## Limitations
- The Dirichlet formulation's theoretical unification of reweighting and resampling may not hold in all edge cases, particularly when α→0 might introduce sampling bias
- Performance superiority could be dataset-specific rather than demonstrating general robustness across diverse real-world scenarios
- Computational efficiency claims need validation on larger-scale datasets beyond CIFAR and Clothing1M, as resampling may become prohibitive with massive datasets

## Confidence
- High confidence in the mathematical derivation of the Dirichlet-based weighting framework and its relationship to reweighting/resampling
- Medium confidence in RENT's superiority claims, given the experiments are limited to standard benchmarks and may not capture all real-world scenarios
- Medium confidence in the computational efficiency claims without large-scale validation

## Next Checks
1. Test RENT on larger-scale datasets (e.g., ImageNet) to verify computational efficiency claims hold at scale
2. Conduct ablation studies varying α systematically to confirm the resampling advantage is consistent across different noise levels
3. Compare RENT's performance when paired with different T-estimation methods beyond those presented to assess general robustness