---
ver: rpa2
title: 'AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language
  Models'
arxiv_id: '2403.06574'
source_url: https://arxiv.org/abs/2403.06574
tags:
- chinese
- ancient
- language
- llms
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AC-EVAL is a benchmark for evaluating large language models' understanding
  of ancient Chinese. It consists of 3,245 multiple-choice questions across 13 subjects
  spanning historical knowledge, short text comprehension, and long text comprehension.
---

# AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models

## Quick Facts
- arXiv ID: 2403.06574
- Source URL: https://arxiv.org/abs/2403.06574
- Reference count: 21
- Key outcome: Benchmark reveals significant performance gaps in ancient Chinese comprehension, especially for English LLMs and long text understanding

## Executive Summary
AC-EVAL is a comprehensive benchmark designed to evaluate large language models' understanding of ancient Chinese language and historical knowledge. The benchmark comprises 3,245 multiple-choice questions across 13 subjects spanning from the Pre-Qin to Qing dynasties, covering historical facts, geography, social customs, art, philosophy, and classical literature. Through extensive evaluation of 17 top Chinese and English LLMs, the benchmark reveals that Chinese models significantly outperform English ones on ancient Chinese tasks, highlighting this as a low-resource area for models like GPT-4. The results show that while models perform reasonably well on general historical knowledge, they struggle considerably with long text comprehension tasks, suggesting substantial room for improvement in deep understanding of ancient Chinese texts.

## Method Summary
The AC-EVAL benchmark was constructed by collecting authoritative ancient Chinese texts from sources like Siku Quanshu, then designing questions spanning 13 subjects across three difficulty levels: general historical knowledge, short text understanding, and long text comprehension. The benchmark was evaluated using 17 top Chinese and English LLMs in zero-shot and few-shot settings, with and without chain-of-thought prompting. Models were assessed on their accuracy in answering multiple-choice questions about ancient Chinese content, with performance analyzed across different model sizes and training backgrounds.

## Key Results
- Chinese LLMs significantly outperform English LLMs on ancient Chinese comprehension tasks, with the best Chinese model achieving 63.91% accuracy versus 38.75% for the best English model
- Performance degrades predictably across difficulty levels: general knowledge (best 73.85%), short text (best 64.23%), and long text comprehension (best 51.11%)
- Larger models demonstrate advantages in zero-shot chain-of-thought reasoning for complex tasks, with 13B parameter models showing 12.82% improvement over 7B models in COT setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark's temporal coverage from Pre-Qin to Qing dynasties creates diverse linguistic patterns that models must generalize across
- Mechanism: By spanning thousands of years of language evolution, the benchmark forces models to capture both shallow lexical patterns and deep historical/cultural reasoning across multiple linguistic shifts
- Core assumption: Ancient Chinese language changed significantly enough across dynasties to require distinct comprehension capabilities
- Evidence anchors:
  - [abstract] "The benchmark comprises 13 tasks, spanning historical facts, geography, social customs, art, philosophy, classical poetry and prose, providing a comprehensive assessment framework."
  - [section] "It spans from the pre-Qin period to the Qing dynasty, offering a wide historical scope that covers thousands of years of evolution."
- Break condition: If ancient Chinese showed minimal linguistic variation across dynasties, this temporal diversity would not add significant evaluation value

### Mechanism 2
- Claim: Task difficulty stratification (general knowledge → short text → long text) reveals different model capability levels
- Mechanism: Progressive difficulty exposes whether models can handle increasingly complex reasoning demands, from simple fact retrieval to deep text comprehension and synthesis
- Core assumption: Model performance degrades predictably as task complexity increases, revealing capability gaps
- Evidence anchors:
  - [abstract] "AC-EVAL is structured across three levels of difficulty reflecting different facets of language comprehension: general historical knowledge, short text understanding, and long text comprehension."
  - [section] "The benchmark ranges from basic fragmented historical knowledge to complex tasks requiring the understanding of ancient Chinese texts of various lengths, providing a graded evaluation of model capabilities."
- Break condition: If models showed no performance difference across difficulty levels, the stratification would not effectively differentiate capabilities

### Mechanism 3
- Claim: Chinese LLMs significantly outperform English ones on this benchmark due to specialized training on ancient Chinese corpora
- Mechanism: Models trained on modern Chinese data can generalize better to ancient Chinese than models trained primarily on English, despite both facing low-resource challenges
- Core assumption: Ancient Chinese shares enough features with modern Chinese for transfer learning, but differs enough to challenge non-Chinese models
- Evidence anchors:
  - [abstract] "Chinese LLMs significantly outperform English ones on this benchmark, highlighting ancient Chinese as a low-resource area for models like GPT-4."
  - [section] "This distinction underscores the unique challenge that ancient Chinese as a low-resource area for models like GPT-4, despite their commendable performance on other Chinese benchmarks."
- Break condition: If English LLMs performed equally well, this would suggest ancient Chinese is not as low-resource as claimed or that modern Chinese training is insufficient for generalization

## Foundational Learning

- Concept: Temporal linguistic evolution
  - Why needed here: Understanding how language changes across dynasties is crucial for designing appropriate benchmark tasks and interpreting model performance
  - Quick check question: What are the key linguistic differences between Classical Chinese (pre-Qin) and Late Imperial Chinese (Qing dynasty)?

- Concept: Cultural-historical context integration
  - Why needed here: Ancient Chinese texts embed knowledge about historical events, social customs, and philosophical systems that models must reason about
  - Quick check question: How does knowledge of the Five Elements theory help in understanding classical Chinese poetry?

- Concept: Cross-lingual transfer limitations
  - Why needed here: Recognizing why models trained on one language family struggle with ancient Chinese informs benchmark interpretation
  - Quick check question: What linguistic features of ancient Chinese make it particularly challenging for models trained primarily on Indo-European languages?

## Architecture Onboarding

- Component map:
  - Data collection pipeline → Annotation system → Question bank → Evaluation framework → Leaderboard interface
  - Model evaluation → Multiple settings (zero-shot, few-shot, COT) → Category-level analysis

- Critical path:
  1. Collect authoritative ancient Chinese texts from sources like Siku Quanshu
  2. Design questions spanning 13 subjects across 3 difficulty levels
  3. Implement quality control with expert review and contamination mitigation
  4. Create evaluation framework with AO and COT settings
  5. Deploy automated evaluation with confidential test set

- Design tradeoffs:
  - Multiple-choice vs. open-ended: Multiple-choice enables standardized evaluation but may not capture generative capabilities
  - Temporal breadth vs. depth: Covering all dynasties provides diversity but may sacrifice detailed analysis of specific periods
  - Question quantity vs. quality: Large dataset ensures statistical significance but requires extensive quality control

- Failure signatures:
  - If model performance is uniformly high across all categories, the benchmark may not be sufficiently challenging
  - If few-shot learning consistently underperforms zero-shot, the benchmark may require too much specialized knowledge for few examples to be effective
  - If English and Chinese models show similar performance, the benchmark may not be effectively measuring ancient Chinese-specific capabilities

- First 3 experiments:
  1. Compare model performance on general historical knowledge vs. long text comprehension to validate difficulty stratification
  2. Test whether providing historical context in prompts improves model performance on poetry appreciation tasks
  3. Evaluate whether fine-tuning on modern Chinese corpora improves ancient Chinese comprehension across different model sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on AC-EVAL correlate with their performance on other Chinese benchmarks like C-Eval or CMMLU?
- Basis in paper: [explicit] The paper notes that GPT series models perform worse on AC-EVAL compared to Chinese LLMs, which is a divergence from their performance on other Chinese benchmarks
- Why unresolved: The paper does not provide a direct comparison of model performance across different benchmarks
- What evidence would resolve it: A comprehensive analysis comparing model rankings and accuracies on AC-EVAL with their rankings and accuracies on other established Chinese benchmarks

### Open Question 2
- Question: What is the impact of fine-tuning LLMs on ancient Chinese corpora versus using zero-shot or few-shot learning for ancient Chinese comprehension tasks?
- Basis in paper: [explicit] The paper discusses the performance of Xunzi-Qwen-Chat, which is fine-tuned on ancient Chinese texts, and compares it to the base Qwen-7B-Chat
- Why unresolved: The paper does not explore the broader implications or provide a systematic comparison of fine-tuning versus few-shot/zero-shot approaches
- What evidence would resolve it: Experimental results showing the performance differences between fine-tuned models and models using few-shot or zero-shot learning on AC-EVAL tasks

### Open Question 3
- Question: How do human experts in ancient Chinese literature perform on AC-EVAL compared to LLMs?
- Basis in paper: [inferred] The paper mentions the absence of a human baseline, which limits the evaluation of LLMs' depth of understanding and cultural acuity
- Why unresolved: The paper does not include human performance data, making it difficult to contextualize the LLMs' performance
- What evidence would resolve it: A study where human experts in ancient Chinese literature take the AC-EVAL benchmark and their scores are compared to those of the LLMs

## Limitations

- The benchmark relies on multiple-choice format which may not fully capture models' generative capabilities in ancient Chinese comprehension
- The temporal breadth from Pre-Qin to Qing dynasties may dilute the depth of evaluation for specific historical periods
- Quality control depends heavily on expert annotation which introduces potential subjectivity and scalability concerns

## Confidence

**High Confidence** (75-100%): The benchmark successfully covers a wide range of ancient Chinese subjects and historical periods, providing a comprehensive evaluation framework. The finding that Chinese LLMs significantly outperform English ones on ancient Chinese tasks is well-supported by the data. The observation that larger models show advantages in chain-of-thought reasoning for complex tasks is consistent with general LLM scaling trends.

**Medium Confidence** (50-75%): The claim that ancient Chinese represents a "low-resource area" for models like GPT-4 is plausible but could benefit from more direct evidence comparing resource availability across different language tasks. The effectiveness of the three-tier difficulty stratification in revealing model capabilities appears reasonable but would benefit from more granular analysis of which specific features distinguish each level.

**Low Confidence** (0-50%): The assertion that the benchmark reveals significant room for improvement, especially in long text comprehension, may be influenced by the particular evaluation setup rather than representing absolute model limitations. The claim that few-shot learning "introduces noise" for specialized tasks is based on observed performance patterns but lacks deeper analysis of why this occurs.

## Next Checks

1. **Temporal Generalization Test**: Evaluate model performance on ancient Chinese texts from specific dynasties (e.g., Tang vs. Song) separately to determine whether the observed performance patterns hold when analyzing narrower temporal slices rather than the full historical span.

2. **Generative Capability Assessment**: Supplement the multiple-choice evaluation with open-ended generation tasks where models must produce ancient Chinese text or explanations, then use expert human evaluation to compare with multiple-choice performance patterns.

3. **Cross-Lingual Transfer Analysis**: Systematically compare English LLMs fine-tuned on modern Chinese corpora versus those trained primarily on English to isolate whether the performance gap stems from language family differences or general ancient Chinese training data availability.