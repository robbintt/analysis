---
ver: rpa2
title: Part-based Quantitative Analysis for Heatmaps
arxiv_id: '2405.13264'
source_url: https://arxiv.org/abs/2405.13264
tags:
- pqah
- parts
- heatmaps
- heatmap
- body
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of quantitative, granular evaluation
  metrics for heatmap-based explainable AI (XAI) methods. The authors propose Part-based
  Quantitative Analysis of Heatmaps (PQAH), a novel approach that measures the overlap
  between heatmaps and semantic part segmentations of objects, using F1 scores to
  quantify the alignment between heatmap activations and annotated object parts.
---

# Part-based Quantitative Analysis for Heatmaps

## Quick Facts
- arXiv ID: 2405.13264
- Source URL: https://arxiv.org/abs/2405.13264
- Authors: Osman Tursun; Sinan Kalkan; Simon Denman; Sridha Sridharan; Clinton Fookes
- Reference count: 40
- Primary result: PQAH quantifies heatmap alignment with semantic part segmentations using F1 scores, validated across multiple networks and datasets.

## Executive Summary
This paper addresses the lack of quantitative, granular evaluation metrics for heatmap-based explainable AI (XAI) methods. The authors propose Part-based Quantitative Analysis of Heatmaps (PQAH), a novel approach that measures the overlap between heatmaps and semantic part segmentations of objects, using F1 scores to quantify the alignment between heatmap activations and annotated object parts. PQAH is validated across multiple backbone networks (ResNet-50, VGG-16, ViT) on PartImageNet and PASCAL-Part datasets, showing consistent alignment with conventional metrics like Top-1 error rate while providing detailed per-part and per-class insights. Experiments also demonstrate PQAH's effectiveness in evaluating data augmentation (Cutout, CutMix) and saliency enhancement methods (SESS, Puzzle-CAM), and its ability to guide model improvement, such as identifying position-based bias in medical X-ray classification and enabling targeted retraining. The method offers scalable, objective, and interpretable analysis for both heatmap evaluation and XAI reporting, especially when integrated with large language models for automated report generation.

## Method Summary
PQAH quantifies the alignment between heatmaps and semantic part segmentations using F1 scores. For each image, PQAH computes true positives, false positives, and false negatives between the heatmap and the ground truth part mask. It approximates Precision using the overall Precision of the object, then combines it with Recall to produce an F1 score per part. These scores aggregate into PHQ1-Q3 statistics. The method is validated on PartImageNet and PASCAL-Part datasets across ResNet-50, VGG-16, and ViT backbones, showing inverse correlation with Top-1 error rate and effectiveness in evaluating augmentation and saliency enhancement methods.

## Key Results
- PQAH shows inverse correlation with Top-1 error rate, validating it as a heatmap quality metric.
- PQAH effectively evaluates data augmentation (Cutout, CutMix) and saliency enhancement methods (SESS, Puzzle-CAM).
- LLM integration enables automated, interpretable XAI reports from PQAH statistics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PQAH quantifies the alignment between heatmaps and semantic part segmentations using F1 scores.
- Mechanism: For each image, PQAH computes true positives, false positives, and false negatives between the heatmap and the ground truth part mask. It approximates Precision using the overall Precision of the object, then combines it with Recall to produce an F1 score per part. These scores aggregate into PHQ1-Q3 statistics.
- Core assumption: The composite heatmap's overall Precision can approximate the Precision for each semantic part.
- Evidence anchors:
  - [abstract] "PQAH measures the overlap between heatmaps and semantic part segmentations of objects, using F1 scores to quantify the alignment between heatmap activations and annotated object parts."
  - [section] "We adopt the F1 Score as our evaluation measure... ^Precision for part p is not directly observable since H p is not available. Nevertheless, given that both Precision and Recall are normalized between 0 and 1, we can approximate the Precision for part p using the overall Precision for the object in image I to which part p belongs."
- Break condition: If the composite heatmap's Precision is not representative of part-level Precision (e.g., if the network focuses on only a subset of parts), the approximation fails and PH scores become unreliable.

### Mechanism 2
- Claim: PQAH results correlate inversely with Top-1 error rate, validating it as a heatmap quality metric.
- Mechanism: Higher PHQ1-Q3 scores indicate better heatmap-part overlap, meaning the network is attending to the correct object regions. This leads to more accurate predictions, reflected in lower Top-1 error.
- Core assumption: Heatmaps that align well with semantic parts reflect correct network reasoning.
- Evidence anchors:
  - [section] "An inverse relationship is observed, where higher PHQ1-Q3 values correlate with a reduced Top-1 error rate."
  - [section] "In conclusion, PQAH not only effectively captures subtle differences in heatmaps but also aligns with established metrics, such as the Top-1 Error Rate as given in Table 2."
- Break condition: If a model achieves low error through incorrect reasoning (e.g., relying on background or spurious correlations), PH scores may be low despite low Top-1 error.

### Mechanism 3
- Claim: LLM integration converts PQAH statistics into human-readable XAI reports, enabling non-expert interpretability.
- Mechanism: PQAH outputs JSON containing PHQ1-Q3 per part and category. This JSON is fed to GPT-4 with a prompt to analyze strengths, weaknesses, and improvement suggestions, producing a structured text report.
- Core assumption: GPT-4 can meaningfully interpret numerical PH scores and map them to actionable insights.
- Evidence anchors:
  - [abstract] "especially when integrated with large language models for automated report generation."
  - [section] "A large-language model like GPT-4 can not only distill PQAH's numerical data into a concise text-based XAI report but can also o ffer expert-level suggestions for potential enhancements."
- Break condition: If PHQ1-Q3 scores are too numerous or lack a clear reference, GPT-4 may produce overly general or less actionable suggestions.

## Foundational Learning

- Concept: Semantic segmentation and part-based annotation
  - Why needed here: PQAH relies on ground truth part masks to measure heatmap alignment. Understanding how semantic part segmentations are created and used is essential to interpret PQAH scores.
  - Quick check question: What is the difference between object segmentation and part segmentation, and why is part segmentation critical for PQAH?

- Concept: Evaluation metrics for segmentation (Dice coefficient, IoU, F1 score)
  - Why needed here: PQAH uses F1 score to quantify overlap between heatmaps and part masks. Knowing how these metrics work helps understand PQAH's reliability and limitations.
  - Quick check question: How does the F1 score balance Precision and Recall, and why is it chosen over Dice or IoU in PQAH?

- Concept: Large language model prompting for structured output
  - Why needed here: Generating XAI reports requires crafting prompts that guide GPT-4 to extract insights from numerical data and produce coherent, technical suggestions.
  - Quick check question: What key elements should a prompt include to ensure GPT-4 produces an actionable and structured XAI report from PQAH data?

## Architecture Onboarding

- Component map: Data layer (images + part annotations) -> Model layer (backbone networks) -> Heatmap extraction (GradCAM + SESS) -> PQAH engine (PH scores) -> LLM interface (GPT-4) -> Output (numerical summaries + text reports)

- Critical path:
  1. Load images and corresponding part masks.
  2. Extract heatmaps using a chosen method.
  3. Compute PH scores per part per image.
  4. Aggregate PH scores into PHQ1-Q3 statistics.
  5. (Optional) Generate XAI report via LLM.

- Design tradeoffs:
  - Part annotation cost vs. granularity: Manual annotations are accurate but costly; automated methods are cheaper but noisier.
  - Precision approximation: Using overall Precision per image simplifies computation but may misrepresent part-specific behavior.
  - LLM integration: Adds interpretability but introduces dependence on external API and potential generalization issues.

- Failure signatures:
  - Low PHQ1-Q3 with low Top-1 error: Possible spurious correlation or background bias.
  - High PHQ1-Q3 with high Top-1 error: Possible misalignment between heatmap interpretation and actual decision-making.
  - LLM report too general: Lack of standard reference or overly sparse PQAH input.

- First 3 experiments:
  1. Run PQAH on a small subset of PartImageNet using ResNet-50 and GradCAM, verify PHQ1-Q3 outputs.
  2. Compare PHQ1-Q3 against Top-1 error for multiple networks to confirm inverse correlation.
  3. Generate an XAI report using PQAH results for ResNet-50 via GPT-4, evaluate report quality and relevance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PQAH perform when applied to object detection datasets (e.g., COCO) with bounding box annotations instead of part segmentations?
- Basis in paper: [inferred] The paper evaluates PQAH on datasets with part-based annotations (PartImageNet and PASCAL-Part) but does not explore its applicability to standard object detection benchmarks.
- Why unresolved: The method's effectiveness on more general object detection tasks remains untested, limiting understanding of its broader applicability.
- What evidence would resolve it: Experiments applying PQAH to COCO-style bounding box annotations and comparing results with established object detection metrics like mAP.

### Open Question 2
- Question: What is the optimal threshold θ for binarizing heatmaps in PQAH across different backbone architectures and datasets?
- Basis in paper: [explicit] The paper uses a default threshold of 0.5 but does not systematically investigate how varying θ affects PQAH results.
- Why unresolved: Different architectures and datasets may require different thresholds for optimal heatmap binarization, affecting PQAH's sensitivity and accuracy.
- What evidence would resolve it: A parameter sensitivity analysis showing PQAH scores across different threshold values and their correlation with other evaluation metrics.

### Open Question 3
- Question: Can PQAH be extended to handle multi-modal inputs (e.g., images with text descriptions) for more comprehensive explainable AI?
- Basis in paper: [inferred] The paper focuses on image-based heatmaps but does not address multi-modal scenarios where textual explanations might complement visual heatmaps.
- Why unresolved: Modern AI systems often integrate multiple data modalities, and PQAH's effectiveness in such contexts is unexplored.
- What evidence would resolve it: Experimental validation of PQAH on multi-modal datasets (e.g., image-text pairs) and assessment of its ability to quantify alignment between visual and textual explanations.

## Limitations
- The approximation of Precision per semantic part using overall object Precision may introduce bias if the network attends selectively to only a subset of object parts.
- PQAH depends on the quality and granularity of part annotations, which can vary across datasets and may not generalize to domains with coarse or no part-level annotations.
- LLM-generated reports are contingent on the interpretability of numerical PQAH scores and the prompt quality; overly general or context-dependent outputs may limit actionable insights.

## Confidence

- High confidence in the mechanism and empirical validation of PQAH for measuring heatmap-part overlap and correlating with Top-1 error rate.
- Medium confidence in the scalability and generalizability of PQAH across diverse datasets and network architectures, pending further domain-specific validation.
- Medium confidence in the utility and clarity of LLM-integrated reports, as prompt quality and LLM performance may vary.

## Next Checks
1. Validate PQAH's approximation of part-level Precision by comparing against manually annotated part Precision in a subset of images, checking for systematic bias.
2. Apply PQAH to a dataset with coarse or incomplete part annotations (e.g., medical X-rays) and assess robustness, as well as the LLM's ability to generate meaningful reports with sparse input.
3. Test PQAH on models known to rely on spurious correlations or background cues (e.g., biased classifiers) to confirm its sensitivity to incorrect reasoning despite low Top-1 error.