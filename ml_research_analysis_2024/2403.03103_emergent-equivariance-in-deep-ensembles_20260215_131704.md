---
ver: rpa2
title: Emergent Equivariance in Deep Ensembles
arxiv_id: '2403.03103'
source_url: https://arxiv.org/abs/2403.03103
tags:
- ensemble
- data
- deep
- ensembles
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that deep ensembles become fully equivariant
  under data augmentation in the infinite-width limit, with predictions invariant
  under symmetry transformations for all inputs and training times. The emergent equivariance
  holds off-manifold and for any architecture, despite individual ensemble members
  not being equivariant.
---

# Emergent Equivariance in Deep Ensembles

## Quick Facts
- arXiv ID: 2403.03103
- Source URL: https://arxiv.org/abs/2403.03103
- Reference count: 40
- One-line primary result: Deep ensembles become fully equivariant under data augmentation in the infinite-width limit, with predictions invariant under symmetry transformations for all inputs and training times.

## Executive Summary
This paper proves that deep ensembles trained with full data augmentation achieve emergent equivariance in the infinite-width limit, with ensemble predictions being exactly equivariant even though individual members are not. Using neural tangent kernel (NTK) theory, the authors derive closed-form expressions showing that ensemble outputs follow Gaussian processes whose mean and covariance transform correctly under symmetry groups. The emergent equivariance holds off-manifold and for any architecture, despite individual ensemble members not being equivariant.

Empirically, the authors demonstrate significant invariance improvements in ensembles compared to individual members across multiple datasets including Ising models, FashionMNIST, and histological data, even with finite widths and ensemble sizes. The work provides theoretical justification for the widespread empirical success of data augmentation in improving model robustness and uncertainty quantification.

## Method Summary
The method involves training deep ensembles of neural networks with data augmentation using symmetry group transformations. The authors use NTK theory to analyze the infinite-width limit where ensemble outputs follow Gaussian processes. They derive closed-form expressions for the mean and covariance of these Gaussian processes, showing they transform correctly under group actions. Experiments validate the theory across multiple datasets: an Ising model energy function task, rotated FashionMNIST classification, and histological slice classification from the NCT-CRC-HE-100K dataset.

## Key Results
- Deep ensembles achieve emergent equivariance in the infinite-width limit even though individual members are not equivariant
- Emergent equivariance holds off-manifold and at all training times, not just on training data or at convergence
- Ensemble predictions show significantly improved invariance compared to individual members across multiple datasets and tasks
- The theoretical predictions match empirical results, with OSP variance converging to NTK predictions as ensemble size increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep ensembles trained with full data augmentation achieve emergent equivariance in the infinite-width limit, with ensemble predictions being exactly equivariant even though individual members are not.
- Mechanism: Neural Tangent Kernel (NTK) theory shows that in the infinite-width limit, ensemble outputs follow Gaussian processes whose mean and covariance transform correctly under symmetry groups. Data augmentation causes the NTK to transform under group actions, and ensemble averaging cancels out non-equivariant variations across members.
- Core assumption: Ensemble outputs can be modeled as Gaussian processes whose mean and covariance are analytically tractable via NTK theory.
- Evidence anchors:
  - [abstract] "Using neural tangent kernel theory, the authors derive closed-form expressions showing that ensemble outputs follow Gaussian processes whose mean and covariance transform correctly under symmetry groups."
  - [section] "In the infinite width limit, a deep ensemble follows a Gaussian distribution described by the neural tangent kernel (Jacot et al., 2018)"
- Break condition: Finite width, finite ensemble size, or incomplete data augmentation (for continuous groups) will break the emergent equivariance.

### Mechanism 2
- Claim: Data augmentation induces a permutation structure on the training set that, when combined with NTK transformation properties, enables ensemble equivariance.
- Mechanism: Data augmentation with finite group G creates permutations of training indices. The NTK transforms under group actions via ρK(g), and Lemma 5.2 shows these permutations commute with matrix functions of the NTK. This allows the ensemble mean to be equivariant even though individual members are not.
- Core assumption: Data augmentation creates a permutation structure on the training set indices that commutes with NTK transformations.
- Evidence anchors:
  - [abstract] "Data augmentation allows to rewrite the group action as a permutation... Combining this with the invariance of the MLP-kernels... we can shift a permutation from the first to the second index of the Gram matrix"
  - [section] "Data augmentation implies that the permutation group action Π commutes with any matrix-valued analytical function F involving the Gram matrices of the NNGP and NTK"
- Break condition: If data augmentation doesn't create complete permutations (e.g., continuous groups with finite samples), the mechanism breaks.

### Mechanism 3
- Claim: The emergent equivariance holds off-manifold and at all training times, not just on training data or at convergence.
- Mechanism: NTK theory provides closed-form expressions for the ensemble mean and covariance at any training time t, and these expressions transform correctly under group actions regardless of whether inputs are on or off the training manifold.
- Core assumption: NTK-based Gaussian process predictions are valid and equivariant for any input, not just training data.
- Evidence anchors:
  - [abstract] "crucially, equivariance holds off-manifold and for any architecture in the infinite width limit"
  - [section] "This result holds even off the data manifold, i.e., for out-of-distribution data, and in the early stages of training as well as at initialization"
- Break condition: If NTK theory breaks down (finite width) or if the Gaussian process approximation fails, this mechanism breaks.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) theory and its application to infinite-width neural networks
  - Why needed here: The entire proof of emergent equivariance relies on NTK theory providing closed-form expressions for ensemble outputs as Gaussian processes
  - Quick check question: What is the key property of NTK in the infinite-width limit that makes it useful for analyzing deep ensembles?

- Concept: Group representations and equivariance in neural networks
  - Why needed here: The paper proves that deep ensembles become equivariant with respect to specific group representations when using data augmentation
  - Quick check question: How does a group representation ρ(g) transform a vector space, and what does it mean for a function to be equivariant with respect to this representation?

- Concept: Data augmentation and its effect on training dynamics
  - Why needed here: The paper shows that data augmentation creates permutations that enable emergent equivariance, which is different from traditional architecture-based equivariance
  - Quick check question: How does data augmentation with a finite group G create permutations of the training set indices?

## Architecture Onboarding

- Component map: Neural network architecture -> Data augmentation pipeline -> Ensemble mechanism -> NTK analysis framework

- Critical path:
  1. Define the symmetry group G and its representations ρX, ρY
  2. Implement data augmentation by applying all group elements to training data
  3. Train multiple network instances with different random initializations
  4. Average predictions across ensemble members
  5. Verify equivariance empirically (e.g., using orbit same prediction metric)

- Design tradeoffs:
  - Complete vs. approximate data augmentation (finite vs. continuous groups)
  - Ensemble size vs. computational cost
  - Width of individual networks vs. closeness to NTK limit

- Failure signatures:
  - High orbit same prediction variance across ensemble members
  - Low test accuracy on augmented data
  - Poor generalization to out-of-distribution data

- First 3 experiments:
  1. Implement data augmentation with cyclic group C4 on FashionMNIST and verify that ensemble predictions are more invariant than individual members
  2. Vary ensemble size and measure the convergence of orbit same prediction to theoretical NTK predictions
  3. Test with a continuous symmetry group (e.g., SO(2)) using finite samples and measure the discretization error

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do finite-width corrections quantitatively affect the emergent equivariance observed in deep ensembles?
- Basis in paper: [explicit] The paper acknowledges that convergence to Gaussian distribution only holds in the infinite width limit and that finite-width corrections have been studied in prior work (Huang & Yau, 2020; Yaida, 2020; Halverson et al., 2021; Erbin et al., 2022).
- Why unresolved: The authors state that incorporating finite-width corrections to quantify violations of exact equivariance is of significant technical difficulty and beyond the scope of this work.
- What evidence would resolve it: Deriving explicit bounds or error estimates for equivariance violations as a function of network width, and experimentally validating these bounds across different architectures and tasks.

### Open Question 2
- Question: How does the degree of equivariance scale with the number of samples used to approximate continuous symmetry groups in data augmentation?
- Basis in paper: [explicit] The paper discusses the approximation of continuous groups with finite subgroups and provides a bound for the discretization error (Lemma 6.2).
- Why unresolved: While a theoretical bound is provided, the practical relationship between the number of samples, the actual equivariance achieved, and computational cost remains unexplored.
- What evidence would resolve it: Systematic experiments varying the number of samples used to approximate continuous groups, measuring equivariance metrics, and analyzing the trade-off with computational resources.

### Open Question 3
- Question: How do different data augmentation strategies (e.g., random vs. systematic sampling of group elements) affect the emergent equivariance of deep ensembles?
- Basis in paper: [inferred] The paper mentions that in exploratory analysis, they did not observe a notable difference between systematic and random sampling, but this is not the focus of the study.
- Why unresolved: The theoretical framework assumes full data augmentation, and the practical implications of different sampling strategies are not fully explored.
- What evidence would resolve it: Comparative experiments using different data augmentation strategies (random, systematic, importance sampling) and measuring their impact on equivariance and model performance.

### Open Question 4
- Question: Can the emergent equivariance property be extended to other ensemble methods beyond simple averaging, such as Bayesian model averaging or boosting?
- Basis in paper: [inferred] The paper focuses on deep ensembles as defined by averaging predictions of independently trained models, but does not explore other ensemble methods.
- Why unresolved: The theoretical analysis relies on properties specific to deep ensembles and the neural tangent kernel framework, which may not directly apply to other ensemble methods.
- What evidence would resolve it: Extending the theoretical analysis to other ensemble methods and conducting experiments to compare their emergent equivariance properties with deep ensembles.

## Limitations
- The theoretical results rely on the infinite-width limit, with finite-width corrections being technically difficult to analyze
- The computational cost of training large ensembles may be prohibitive compared to single equivariant architectures
- The paper doesn't fully explore how different data augmentation strategies affect emergent equivariance

## Confidence
- High Confidence: The mechanism that data augmentation creates permutation structures on training data that commute with NTK transformations is well-established theoretically and supported by the empirical results
- Medium Confidence: The claim that emergent equivariance holds off-manifold and at all training times is theoretically derived but relies on NTK approximations that may break down in practice
- Medium Confidence: The empirical improvements in OSP and RSD metrics are demonstrated across multiple datasets, but the statistical significance and practical impact on downstream tasks could be stronger

## Next Checks
1. Systematically measure how ensemble size and width parameters affect emergent equivariance metrics across different architectures to establish scaling laws and identify practical limits
2. Compare the computational cost and accuracy of deep ensembles with data augmentation versus single equivariant architectures across varying problem complexities and dataset sizes
3. Evaluate whether the emergent equivariance translates to improved performance on specific tasks that require symmetry preservation, such as molecular property prediction or medical image segmentation