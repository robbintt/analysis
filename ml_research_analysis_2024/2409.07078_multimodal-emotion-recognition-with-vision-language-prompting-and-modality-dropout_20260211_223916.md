---
ver: rpa2
title: Multimodal Emotion Recognition with Vision-language Prompting and Modality
  Dropout
arxiv_id: '2409.07078'
source_url: https://arxiv.org/abs/2409.07078
tags:
- emotion
- recognition
- modality
- clip
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a solution for the MER2024-SEMI multimodal
  emotion recognition challenge, achieving first place with 90.15% accuracy on the
  test set. The authors address the challenge of recognizing emotions from video data
  by proposing several key innovations: EmoVCLIP, a CLIP-based model fine-tuned with
  vision-language prompt learning for better emotional video feature extraction; modality
  dropout to reduce dependence between different input modalities (speech, image,
  text, video) and improve fusion robustness; GPT4-Baichuan, which enhances text feature
  extraction by using GPT-4 to provide emotional context to the Baichuan language
  model; and a self-training strategy to leverage unlabeled data through pseudo-labeling.'
---

# Multimodal Emotion Recognition with Vision-language Prompting and Modality Dropout

## Quick Facts
- arXiv ID: 2409.07078
- Source URL: https://arxiv.org/abs/2409.07078
- Reference count: 27
- Primary result: 90.15% accuracy on private test set, first place in MER2024-SEMI challenge

## Executive Summary
This paper presents a top-ranking solution for multimodal emotion recognition from video data in the MER2024-SEMI challenge. The authors propose a comprehensive approach that combines vision-language prompting, modality dropout, enhanced text feature extraction using GPT-4, and self-training with pseudo-labeling. Their system achieves 90.15% accuracy on a private test set, outperforming baseline approaches by 3%. The key innovations include EmoVCLIP for improved emotional video feature extraction, GPT4-Baichuan for contextualized text features, and a self-training strategy to leverage unlabeled data.

## Method Summary
The authors address multimodal emotion recognition by integrating multiple modalities (speech, image, text, video) through a carefully designed pipeline. They employ Hubert for speech processing, CLIP and a fine-tuned EmoVCLIP variant for vision, and a GPT4-Baichuan hybrid for text. A key innovation is the use of modality dropout during fusion to prevent over-reliance on any single modality. The system also leverages self-training by generating pseudo-labels for unlabeled data using the initial model predictions. Vision-language prompt learning is applied to CLIP to better capture emotional content in videos. The final model combines these components through weighted fusion of modality-specific predictions.

## Key Results
- Achieved 90.15% accuracy on private test set, ranking first in MER2024-SEMI challenge
- Outperformed baseline approaches by 3% absolute improvement
- Ablation studies demonstrate effectiveness of individual components (EmoVCLIP, modality dropout, GPT4-Baichuan enhancement)

## Why This Works (Mechanism)
The approach succeeds by addressing key challenges in multimodal emotion recognition: the need for rich emotional feature extraction, robustness to missing or noisy modalities, and effective utilization of limited labeled data. The vision-language prompting helps CLIP better capture emotional nuances in video content. Modality dropout prevents the model from becoming overly dependent on any single input source, improving generalization. The GPT4-Baichuan enhancement provides contextual understanding that improves text-based emotion detection. Self-training with pseudo-labels expands the effective training data without requiring additional manual annotation.

## Foundational Learning
- **Vision-Language Models (CLIP)**: Needed for bridging visual and textual representations; quick check: verify zero-shot classification capability
- **Modality Dropout**: Required to prevent overfitting to specific input channels; quick check: measure performance degradation when dropping individual modalities
- **Self-Training with Pseudo-labels**: Enables semi-supervised learning; quick check: track pseudo-label quality metrics (entropy, consistency)
- **Emotion Recognition from Multimodal Data**: Core task requiring temporal and cross-modal reasoning; quick check: evaluate per-emotion performance breakdown
- **Prompt Learning**: Allows fine-tuning of large models without catastrophic forgetting; quick check: compare prompt-based vs full fine-tuning performance
- **Weighted Fusion of Predictions**: Combines modality-specific outputs optimally; quick check: sensitivity analysis of fusion weights

## Architecture Onboarding

Component Map: Hubert -> Speech Features; CLIP/EmoVCLIP -> Vision Features; GPT4-Baichuan -> Text Features; Fusion Module (with Modality Dropout) -> Final Prediction

Critical Path: Input preprocessing → Individual modality encoders → Modality dropout application → Weighted fusion → Final emotion classification

Design Tradeoffs: Modality dropout improves robustness but may reduce performance when all modalities are reliable; GPT4 integration adds computational cost but improves text understanding; self-training risks propagating errors through pseudo-labels

Failure Signatures: Over-reliance on single modality (detected via modality ablation); poor generalization to underrepresented emotions; sensitivity to noise in specific input channels

First Experiments:
1. Ablation study removing modality dropout to measure robustness impact
2. Compare EmoVCLIP vs standard CLIP performance on emotion-specific subsets
3. Evaluate pseudo-label quality by measuring agreement with human annotations on validation samples

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Results based on private test set prevent independent verification of claimed 90.15% accuracy
- GPT-4 based pseudo-labeling may introduce domain-specific biases that are difficult to quantify
- Lack of detailed per-modality ablation studies makes it unclear which components drive improvements

## Confidence
- **High Confidence**: CLIP-based fine-tuning methodology and modality dropout implementation are well-documented and technically sound
- **Medium Confidence**: Self-training approach is plausible but effectiveness depends heavily on pseudo-label quality
- **Low Confidence**: Absolute performance numbers require independent validation due to reliance on private test data

## Next Checks
1. Replicate the complete pipeline on a publicly available multimodal emotion dataset (CMU-MOSEAS or IEMOCAP) to verify claimed improvements
2. Conduct controlled ablation studies isolating the contribution of each proposed component (EmoVCLIP, modality dropout, GPT4-Baichuan enhancement)
3. Perform robustness testing across different emotion categories to ensure the model doesn't overfit to specific emotion types or rely on spurious correlations