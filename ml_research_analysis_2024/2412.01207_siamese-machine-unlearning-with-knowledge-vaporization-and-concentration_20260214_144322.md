---
ver: rpa2
title: Siamese Machine Unlearning with Knowledge Vaporization and Concentration
arxiv_id: '2412.01207'
source_url: https://arxiv.org/abs/2412.01207
tags:
- class
- data
- unlearning
- forgetting
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Siamese Machine Unlearning with Knowledge
  Vaporization and Concentration, a novel method for removing learned knowledge from
  specific data points in trained models. The key idea is to leverage the observation
  that forgetting data produces dispersed logits across augmented views, while remaining
  data maintains concentrated logits.
---

# Siamese Machine Unlearning with Knowledge Vaporization and Concentration

## Quick Facts
- arXiv ID: 2412.01207
- Source URL: https://arxiv.org/abs/2412.01207
- Authors: Songjie Xie; Hengtao He; Shenghui Song; Jun Zhang; Khaled B. Letaief
- Reference count: 40
- Key outcome: Achieves perfect MIA scores (0.00) while maintaining high accuracy on remaining data (99.98% on CIFAR-10 full-class forgetting)

## Executive Summary
This paper introduces Siamese Machine Unlearning with Knowledge Vaporization and Concentration, a novel method for removing learned knowledge from specific data points in trained models. The key insight is that forgetting data produces dispersed logits across augmented views while remaining data maintains concentrated logits. This leads to two complementary processes: knowledge vaporization (dispersing logits for forgetting data) and knowledge concentration (maintaining concentrated logits for remaining data). Using Siamese networks with adaptively permuted labels for forgetting data and a few samples of remaining data, the method achieves efficient unlearning without additional memory overhead or full access to remaining data.

## Method Summary
The method uses Siamese networks to process pairs of augmented views of data, applying knowledge vaporization to disperse logits for forgetting data while concentrating logits for remaining data. A predictor MLP transforms output logits for comparison. The approach uses adaptively permuted labels for forgetting data to prevent excessive damage to model utility. Only a small subset of remaining data is needed, making the method memory-efficient. The framework achieves unlearning by minimizing distances between augmented views of remaining data while maximizing distances for forgetting data.

## Key Results
- Achieves perfect MIA scores (0.00) across all forgetting scenarios
- Maintains 99.98% accuracy on remaining data in CIFAR-10 full-class forgetting
- Effectively erases knowledge from forgetting data (0.01% accuracy on CIFAR-10 full-class forgetting)
- Outperforms baselines including Retrain, Finetune, NegGrad, Amnesiac, BadT, SCRUB, and SSD

## Why This Works (Mechanism)

### Mechanism 1
Knowledge vaporization disperses logits for forgetting data to erase learned knowledge by minimizing the distance between augmented views and a fixed target representation, pushing logits away from a stable representation. This reduces the model's ability to recognize and classify forgetting data. Break condition: If the model learns to ignore augmentation or if dispersion is insufficient to prevent classification.

### Mechanism 2
Knowledge concentration maintains concentrated logits for remaining data to preserve utility by maximizing similarity between augmented views, keeping logits clustered. This maintains model performance on remaining classes. Break condition: If concentration causes overfitting to remaining data or if the model cannot generalize.

### Mechanism 3
Adaptive label permutation prevents excessive damage to model utility on remaining data by permuting labels for forgetting data based on the ratio of unlearned data within each class, reducing the impact on remaining data. Break condition: If permutation is not effective in reducing damage or introduces bias.

## Foundational Learning

- Concept: Data Augmentation
  - Why needed here: Augmentation creates multiple views of the same data point, necessary for both knowledge vaporization and concentration
  - Quick check question: Why is data augmentation used in this method? (Answer: To create multiple views of the same data point for knowledge vaporization and concentration)

- Concept: Siamese Networks
  - Why needed here: Siamese networks process pairs of augmented views and compare their representations, enabling knowledge vaporization and concentration
  - Quick check question: What is the role of Siamese networks in this method? (Answer: To process pairs of augmented views and compare their representations)

- Concept: Knowledge Distillation
  - Why needed here: Knowledge distillation is a related concept used in some unlearning methods to transfer knowledge from teacher to student models
  - Quick check question: How is knowledge distillation related to this method? (Answer: It is a related concept used in some unlearning methods, but this method uses Siamese networks instead)

## Architecture Onboarding

- Component map: Original model (f) -> Predictor MLP (h) -> Siamese network -> Adaptive label permutation -> Knowledge vaporization loss (LKV) -> Knowledge concentration loss (LKC)

- Critical path:
  1. Pretrain original model on full dataset
  2. Select forgetting data and subset of remaining data
  3. Apply data augmentation to create pairs of views
  4. Process each pair through Siamese network
  5. Compute knowledge vaporization and concentration losses
  6. Update model parameters using backpropagation

- Design tradeoffs:
  - Memory efficiency vs. performance: Using Siamese networks reduces memory overhead but may slightly impact performance compared to teacher-student frameworks
  - Limited access to remaining data vs. unlearning effectiveness: Using only a subset of remaining data reduces memory requirements but may affect unlearning effectiveness

- Failure signatures:
  - High accuracy on forgetting data after unlearning: Indicates knowledge vaporization is not effective
  - Low accuracy on remaining data after unlearning: Indicates knowledge concentration is not effective
  - Slow convergence: Indicates optimization process is not working properly

- First 3 experiments:
  1. Full-class forgetting: Remove entire classes from dataset and evaluate unlearning performance
  2. Sub-class forgetting: Remove subset of data from a class and evaluate unlearning performance
  3. Random forgetting: Remove random subset of data and evaluate unlearning performance

## Open Questions the Paper Calls Out
- How does the choice of data augmentation strategy impact the efficiency and convergence speed of knowledge vaporization and concentration processes?
- What is the minimum number of remaining data samples required to achieve effective unlearning while maintaining model utility?
- How does the proposed method perform when unlearning data from non-IID distributions or data with temporal dependencies?

## Limitations
- Effectiveness of knowledge vaporization relies heavily on the assumption that dispersing logits will sufficiently erase learned representations
- Adaptive label permutation strategy's impact on unlearning effectiveness versus utility preservation requires more rigorous analysis
- Method's scalability to larger models and real-world applications remains unproven

## Confidence
- High confidence in the overall methodology and experimental design
- Medium confidence in the knowledge vaporization mechanism's effectiveness
- Low confidence in the method's scalability and real-world applicability

## Next Checks
1. Test the method on larger datasets (e.g., ImageNet) and more diverse model architectures to evaluate scalability and generalization
2. Conduct ablation studies to isolate contributions of knowledge vaporization and concentration mechanisms to overall performance
3. Implement and evaluate the method in a real-world scenario where data removal requests are frequent and diverse