---
ver: rpa2
title: Distributionally Generative Augmentation for Fair Facial Attribute Classification
arxiv_id: '2403.06606'
source_url: https://arxiv.org/abs/2403.06606
tags: []
core_contribution: The paper proposes Distributionally Generative Augmentation (DiGA),
  a two-stage framework to train fair facial attribute classification (FAC) models
  without requiring labels of spurious attributes. In the first stage, it identifies
  potential spurious attributes by combining biased semantic directions learned from
  generative models.
---

# Distributionally Generative Augmentation for Fair Facial Attribute Classification

## Quick Facts
- arXiv ID: 2403.06606
- Source URL: https://arxiv.org/abs/2403.06606
- Authors: Fengda Zhang; Qianpei He; Kun Kuang; Jiashuo Liu; Long Chen; Chao Wu; Jun Xiao; Hanwang Zhang
- Reference count: 40
- Primary result: DiGA improves fairness in facial attribute classification without requiring sensitive attribute labels, measured by equalized odds and worst-group accuracy

## Executive Summary
This paper introduces Distributionally Generative Augmentation (DiGA), a two-stage framework for training fair facial attribute classification (FAC) models without requiring labels of spurious attributes. The method identifies potential spurious attributes by combining biased semantic directions learned from generative models, then trains a fair FAC model by promoting invariance to distributionally generative augmentations of these spurious attributes. The approach effectively improves fairness while maintaining accuracy compared to state-of-the-art methods, and enhances interpretability by explicitly showing spurious attributes in image space.

## Method Summary
DiGA operates through a two-stage framework. First, it identifies potential spurious attributes by analyzing semantic directions in latent space using generative models like StyleGAN. This involves learning directions that correspond to biased correlations between target and spurious attributes. Second, it trains a fair FAC model by applying distributionally generative augmentations that modify these spurious attributes while preserving the target attribute, forcing the model to become invariant to such spurious variations. The framework is trained end-to-end and requires no labels for spurious attributes, making it applicable to real-world scenarios where such sensitive information is unavailable.

## Key Results
- DiGA effectively improves fairness metrics (equalized odds and worst-group accuracy) on CelebA, UTK-Face, and Dogs and Cats datasets
- The method maintains comparable or better classification accuracy compared to state-of-the-art fairness approaches
- DiGA demonstrates interpretability by explicitly showing generated spurious attribute variations in image space
- Performance improvements are achieved without requiring labels of spurious attributes

## Why This Works (Mechanism)
The method works by forcing the classification model to learn representations that are invariant to spurious correlations between target and non-target attributes. By generating augmentations that systematically vary spurious attributes while preserving target attributes, DiGA creates a training environment where the model cannot rely on these spurious correlations for classification. This distributional invariance is learned through the two-stage process: first identifying the relevant spurious directions, then enforcing invariance through targeted augmentations. The generative model provides a controlled way to manipulate attributes in image space, making the spurious correlations explicit and addressable.

## Foundational Learning
- **Spurious correlations**: Unintended associations between target and non-target attributes that can bias model predictions. Needed to understand fairness challenges in FAC; quick check: can you identify examples where background or other attributes correlate with the target attribute?
- **Semantic directions in latent space**: Learned directions in generative model latent space that correspond to specific attribute changes. Needed to manipulate images in a controlled way; quick check: can you explain how moving along a direction in latent space changes image attributes?
- **Distributional invariance**: Model behavior that remains consistent across different data distributions. Needed to ensure fair predictions across subgroups; quick check: can you describe why invariance to spurious attributes improves fairness?
- **Generative augmentations**: Using generative models to create synthetic variations of training data. Needed to expand training distribution with controlled attribute variations; quick check: can you explain how generated augmentations differ from traditional data augmentation?
- **Equalized odds**: Fairness metric requiring equal true positive and false positive rates across groups. Needed to evaluate fairness performance; quick check: can you write the mathematical definition of equalized odds?
- **Worst-group accuracy**: Performance metric focusing on the most disadvantaged group. Needed to measure fairness for minority subgroups; quick check: can you explain why worst-group accuracy is important for fairness evaluation?

## Architecture Onboarding

**Component Map**: Data -> DiGA Generator -> Distributionally Generative Augmentations -> Fair FAC Model -> Predictions

**Critical Path**: The core workflow follows: (1) Train generative model on dataset, (2) Identify spurious attribute directions in latent space, (3) Generate distributionally generative augmentations, (4) Train fair FAC model with invariance constraints.

**Design Tradeoffs**: The method trades computational cost of generating augmentations for improved fairness without requiring sensitive attribute labels. It balances between generating enough diverse augmentations to cover spurious variations while maintaining computational feasibility.

**Failure Signatures**: 
- Poor fairness performance if spurious attribute directions are not well-identified
- Decreased accuracy if augmentations are too aggressive or poorly controlled
- Limited effectiveness if generative model cannot adequately represent attribute variations
- Computational bottlenecks during augmentation generation phase

**First Experiments**:
1. Train DiGA on CelebA with hair color as target attribute and background as spurious attribute, measuring equalized odds improvement
2. Compare DiGA performance with baseline fairness methods on UTK-Face dataset using age as target and gender as spurious attribute
3. Evaluate DiGA's interpretability by visualizing generated augmentations for a specific target-spurious attribute pair

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the bias detection method perform when the spurious attributes are not linearly separable in the latent space?
- Basis in paper: The method relies on linear classifiers in latent space to identify spurious attributes, but this may not work for non-linear relationships.
- Why unresolved: The paper only evaluates the method on datasets where the spurious attributes are linearly separable in latent space.
- What evidence would resolve it: Experiments on datasets with non-linear relationships between target and spurious attributes, and comparison of performance to baseline methods.

### Open Question 2
- Question: What is the impact of using different generative models (e.g. VAEs, diffusion models) on the bias detection and mitigation performance?
- Basis in paper: The paper uses StyleGAN2 for generative modeling, but does not explore the impact of other generative models.
- Why unresolved: The paper does not compare the performance of different generative models for bias detection and mitigation.
- What evidence would resolve it: Experiments comparing the performance of different generative models on the same datasets and tasks.

### Open Question 3
- Question: How does the proposed method scale to datasets with a large number of target and spurious attributes?
- Basis in paper: The paper only evaluates the method on datasets with a small number of target and spurious attributes.
- Why unresolved: The paper does not discuss the scalability of the method to datasets with a large number of attributes.
- What evidence would resolve it: Experiments on datasets with a large number of target and spurious attributes, and analysis of the computational complexity of the method.

### Open Question 4
- Question: What is the impact of using different reference models (e.g. CLIP, human judgment) on the bias detection performance?
- Basis in paper: The paper uses CLIP as a reference model for bias detection, but does not explore the impact of other reference models.
- Why unresolved: The paper does not compare the performance of different reference models for bias detection.
- What evidence would resolve it: Experiments comparing the performance of different reference models on the same datasets and tasks.

### Open Question 5
- Question: How does the proposed method perform on datasets with continuous target or spurious attributes?
- Basis in paper: The paper only evaluates the method on datasets with binary target and spurious attributes.
- Why unresolved: The paper does not discuss the extension of the method to datasets with continuous attributes.
- What evidence would resolve it: Experiments on datasets with continuous target or spurious attributes, and analysis of the performance of the method in these settings.

## Limitations
- The method relies on generative models which may introduce their own biases or artifacts affecting fairness outcomes
- Identification of spurious attributes through semantic directions is heuristic and may miss some spurious correlations
- Experiments focus on balanced datasets and may not represent performance on highly imbalanced populations
- Computational cost of generating distributionally generative augmentations could be prohibitive for larger-scale applications

## Confidence
- **High Confidence**: The core methodology of using generative augmentations to promote invariance is sound and well-supported by experimental results
- **Medium Confidence**: Fairness improvements measured by equalized odds and worst-group accuracy are convincing but may not fully capture real-world fairness implications
- **Low Confidence**: Generalizability to other domains beyond facial attribute classification and long-term stability of learned invariances in dynamic environments

## Next Checks
1. Conduct experiments on additional diverse datasets with varying levels of spurious correlations to test the robustness of DiGA across different scenarios
2. Perform ablation studies to quantify the individual contributions of the two-stage framework components and determine if similar fairness gains could be achieved with simpler approaches
3. Evaluate the method's performance on out-of-distribution data and under domain shifts to assess the practical utility of the learned invariances in real-world deployment scenarios