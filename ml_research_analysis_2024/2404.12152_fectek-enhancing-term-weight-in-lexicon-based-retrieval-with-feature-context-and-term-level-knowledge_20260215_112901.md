---
ver: rpa2
title: 'FecTek: Enhancing Term Weight in Lexicon-Based Retrieval with Feature Context
  and Term-level Knowledge'
arxiv_id: '2404.12152'
source_url: https://arxiv.org/abs/2404.12152
tags:
- retrieval
- term
- context
- term-level
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes FecTek, a lexicon-based retrieval method that
  enhances term weight modeling by incorporating feature context representations and
  term-level knowledge guidance. The authors introduce two specialized modules: the
  Feature Context Module (FCM) that leverages BERT''s representations to determine
  dynamic weights for each embedding element, and the Term-level Knowledge Guidance
  Module (TKGM) that utilizes term-level knowledge to guide term weight modeling.'
---

# FecTek: Enhancing Term Weight in Lexicon-Based Retrieval with Feature Context and Term-level Knowledge

## Quick Facts
- arXiv ID: 2404.12152
- Source URL: https://arxiv.org/abs/2404.12152
- Reference count: 34
- Primary result: Achieves MRR@10 of 38.2% on MS Marco without distillation

## Executive Summary
This paper introduces FecTek, a lexicon-based retrieval method that enhances term weight modeling by incorporating feature context representations and term-level knowledge guidance. The approach combines a Feature Context Module (FCM) that leverages BERT's representations to determine dynamic weights for each embedding element, with a Term-level Knowledge Guidance Module (TKGM) that utilizes term-level knowledge to guide term weight modeling. Experimental results on the MS Marco benchmark demonstrate that FecTek outperforms previous state-of-the-art approaches, achieving an MRR@10 of 38.2% without distillation and 38.7% with distillation.

## Method Summary
FecTek uses a BERT-base backbone with two specialized branches: a text-level branch for learning term weights using FCM, and a term-level branch for binary classification of term importance using TKGM. The FCM module applies channel attention to BERT embeddings, learning dynamic element-wise weights through global average pooling and fully connected layers. The TKGM module creates binary labels for terms (1 if shared between query and passage, 0 otherwise) and trains a classifier to predict these labels, providing term-level supervision. The total loss combines contrastive loss from the text-level branch with binary cross-entropy from the term-level branch.

## Key Results
- FecTek achieves MRR@10 of 38.2% on MS Marco dev set without distillation
- With distillation, FecTek reaches MRR@10 of 38.7%
- Represents improvements of 3.0% and 0.7% over best existing sparse methods
- Ablation studies show FCM improves performance from 37.1 to 37.6, TKGM from 37.1 to 37.9, and combined from 37.1 to 38.2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature Context Module (FCM) enhances term weight modeling by learning dynamic element-wise weights for BERT embeddings using channel attention.
- Mechanism: FCM applies a transpose, global average pooling, two fully connected layers (with dimensionality reduction and expansion), and a second transpose to produce dynamic weights. These weights are multiplied element-wise with BERT embeddings to produce context-enhanced representations.
- Core assumption: Different embedding dimensions contribute differently to term importance, and these contributions can be learned adaptively.
- Evidence anchors:
  - [abstract] "which leverages the power of BERT's representation to determine dynamic weights for each element in the embedding"
  - [section] "The FCM module is proposed inspired by the remarkable gains achieved through the application of channel attention in CNN models [10]"
  - [corpus] Weak - no direct corpus evidence found for channel attention application to IR term weights

### Mechanism 2
- Claim: Term-level Knowledge Guidance Module (TKGM) improves term weight modeling by providing explicit binary supervision for term importance.
- Mechanism: TKGM creates binary labels (1 for shared terms between query and passage, 0 otherwise) and trains a binary classifier on BERT token representations to predict these labels, providing term-level supervision.
- Core assumption: Terms shared between query and passage are inherently more important for retrieval relevance than unshared terms.
- Evidence anchors:
  - [abstract] "develop a term-level knowledge guidance module (TKGM) for effectively utilizing term-level knowledge to intelligently guide the modeling process of term weight"
  - [section] "Terms found in both the query and passage are assigned a label of 1, while the remaining terms are labeled as 0"
  - [corpus] Weak - no direct corpus evidence found for binary term importance labeling in retrieval contexts

### Mechanism 3
- Claim: The combination of FCM and TKGM provides complementary improvements that are greater than either module alone.
- Mechanism: FCM provides feature context representations while TKGM provides term-level knowledge guidance; together they address both spatial context (via BERT) and feature context (via FCM) while incorporating explicit term importance knowledge (via TKGM).
- Core assumption: The two modules address orthogonal aspects of term weight modeling - one focuses on representation enhancement, the other on knowledge incorporation.
- Evidence anchors:
  - [section] "By introducing the FCM, the MRR@10 performance is improved from 37.1 to 37.6. The TKGM module increases the MRR@10 from 37.1 to 37.9. Finally, by using both, the performance can be significantly improved from 37.1 to 38.2"
  - [corpus] No direct corpus evidence found for synergistic effects of combined modules

## Foundational Learning

- Concept: Channel attention mechanisms in convolutional neural networks
  - Why needed here: FCM is inspired by CNN channel attention, so understanding how channel attention works is crucial for grasping FCM's operation
  - Quick check question: What is the key difference between spatial attention and channel attention in CNN architectures?

- Concept: Binary classification for term importance
  - Why needed here: TKGM uses binary classification to provide term-level supervision, so understanding binary classification fundamentals is essential
  - Quick check question: What are the typical loss functions used for binary classification tasks in neural networks?

- Concept: Contrastive learning for text retrieval
  - Why needed here: The text-level branch uses contrastive loss, so understanding contrastive learning principles is important for the overall architecture
  - Quick check question: In contrastive learning for retrieval, what is the typical relationship between positive and negative examples?

## Architecture Onboarding

- Component map:
  BERT backbone (spatial context representations) → FCM → Projector1 → term weights (primary path)
  BERT backbone → Indicator → Projector2 → term labels (auxiliary supervision path)

- Critical path: BERT → FCM → Projector1 → term weights (primary path for retrieval)
  BERT → Indicator → Projector2 → term labels (auxiliary supervision path)

- Design tradeoffs:
  - Added computational overhead from FCM (additional FC layers and operations)
  - Increased model complexity with dual branches requiring careful loss balancing
  - Potential overfitting risk with additional parameters in projector modules

- Failure signatures:
  - Training instability due to loss balancing between text-level and term-level objectives
  - Degradation in retrieval performance if FCM learns degenerate weights
  - Poor generalization if TKGM binary labels don't reflect true term importance

- First 3 experiments:
  1. Ablation test: Remove FCM and measure performance drop to verify its contribution
  2. Ablation test: Remove TKGM and measure performance drop to verify its contribution
  3. Loss balance test: Vary the weighting between text-level and term-level losses to find optimal balance

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Architecture specification gaps: Critical components like projector modules lack detailed specifications (layer sizes, activation functions)
- Binary label assumption: The assumption that shared terms are inherently more important may not hold for semantic matching scenarios
- Computational overhead: No comprehensive analysis of computational costs or trade-offs between performance gains and increased inference complexity

## Confidence
- High Confidence: FecTek achieves state-of-the-art performance on MS Marco with MRR@10 of 38.2% (without distillation) and 38.7% (with distillation)
- Medium Confidence: FCM and TKGM provide complementary improvements, though synergistic effects are not deeply analyzed
- Low Confidence: Binary term importance labeling is universally valid for retrieval tasks, as this assumption may break down in semantic matching scenarios

## Next Checks
1. Cross-dataset generalization: Test FecTek's performance on diverse retrieval datasets beyond MS Marco to validate universal applicability of binary labeling assumption
2. Ablation with different query types: Conduct detailed ablation studies focusing on different query categories to understand how FCM and TKGM performance varies
3. Computational efficiency analysis: Measure and compare inference time, memory usage, and parameter count against baseline methods to quantify trade-offs