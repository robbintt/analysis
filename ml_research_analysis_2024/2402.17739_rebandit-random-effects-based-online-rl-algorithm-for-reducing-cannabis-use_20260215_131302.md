---
ver: rpa2
title: 'reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use'
arxiv_id: '2402.17739'
source_url: https://arxiv.org/abs/2402.17739
tags:
- user
- prior
- data
- cannabis
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces reBandit, an online reinforcement learning
  algorithm designed to personalize mobile health interventions for reducing cannabis
  use among emerging adults. The algorithm addresses challenges such as limited data,
  delayed intervention effects, and the need for autonomy and explainability in clinical
  settings.
---

# reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use

## Quick Facts
- arXiv ID: 2402.17739
- Source URL: https://arxiv.org/abs/2402.17739
- Authors: Susobhan Ghosh; Yongyi Guo; Pei-Yao Hung; Lara Coughlin; Erin Bonar; Inbal Nahum-Shani; Maureen Walton; Susan Murphy
- Reference count: 40
- Primary result: reBandit outperforms common baseline algorithms in most scenarios for personalizing mobile health interventions to reduce cannabis use, with advantage increasing as population heterogeneity grows.

## Executive Summary
This paper introduces reBandit, an online reinforcement learning algorithm designed to personalize mobile health interventions for reducing cannabis use among emerging adults. The algorithm addresses challenges such as limited data, delayed intervention effects, and the need for autonomy and explainability in clinical settings. reBandit uses random effects to pool information across users while personalizing interventions, and employs empirical Bayes to update hyperparameters online. Evaluated in a simulation environment constructed from prior study data, reBandit outperforms common baseline algorithms in most scenarios, with its advantage increasing as population heterogeneity grows.

## Method Summary
reBandit is an online RL algorithm that uses random effects models and Bayesian inference to personalize mobile health interventions. It models each user's response as a linear function with both population-level (fixed) and user-specific (random) effects. The algorithm employs empirical Bayes to update hyperparameters online and uses smooth posterior sampling for action selection. To account for delayed intervention effects, reBandit engineers the reward by subtracting a cost term proportional to the standard deviation of observed rewards. The algorithm was evaluated in a simulation environment constructed from a prior mobile health study dataset.

## Key Results
- reBandit outperforms fully pooled (BLR) and random baseline algorithms in most simulation scenarios
- Performance advantage of reBandit increases as population heterogeneity increases
- reBandit demonstrates effective balance between pooling information across users and maintaining individual personalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: reBandit uses random effects to pool information across users while maintaining individual personalization.
- Mechanism: Random effects model splits each user's parameters into a population-level term and an individual-level deviation, allowing the algorithm to leverage shared structure across users but still adapt to individual differences.
- Core assumption: The underlying reward functions for users in the same population share enough structure that pooling is beneficial, but also differ enough that individual adaptation is needed.
- Evidence anchors:
  - [abstract] "reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments."
  - [section 4.1] "reBandit utilizes random effects to adaptively pool data across users depending on the degree of heterogeneity in the population."
  - [corpus] Weak. No direct corpus neighbor discusses random effects in reinforcement learning for mobile health.
- Break condition: If the population heterogeneity is extremely high (users have completely different reward structures), the benefit of pooling degrades, and the algorithm may perform worse than fully personalized approaches.

### Mechanism 2
- Claim: Smooth posterior sampling with action-centering improves after-study analysis and replicability.
- Mechanism: Uses a logistic function for action selection probability instead of a hard threshold, and clips probabilities to avoid extreme values, enabling off-policy evaluation and more stable replication across trials.
- Core assumption: A smooth decision rule concentrates action selection probabilities, making randomization reproducible across study repetitions.
- Evidence anchors:
  - [section 4.2] "Using a smooth function ensures that the randomization probabilities formed by the algorithm concentrate... Without concentration, the randomization probabilities might fluctuate greatly between repetitions of the study."
  - [section 4.2] "We choose ρ to be a generalized logistic function... We set the lower and upper clipping probabilities as Lmin = 0.2 and Lmax = 0.8 (i.e., 0.2 ≤ π(t) i ≤ 0.8)."
  - [corpus] Missing. No corpus neighbors discuss posterior sampling or clipping in RL for mobile health.
- Break condition: If the reward signal is too weak or the model is misspecified, smoothing may slow adaptation and reduce performance.

### Mechanism 3
- Claim: Reward engineering accounts for delayed intervention effects by penalizing immediate rewards when actions are taken.
- Mechanism: Subtracts a cost term proportional to the standard deviation of observed rewards from the reward when an action is taken, encouraging the algorithm to balance engagement with user burden over time.
- Core assumption: Sending intervention messages has a delayed negative effect (e.g., user habituation or burden) that should be accounted for in the reward used to update the model.
- Evidence anchors:
  - [section 4.3] "To account for delayed effects in the bandit framework, we engineer the reward for the RL algorithm... The engineered reward ˆR(t) i = R(t) i − a(t) i cost(a(t) i )"
  - [section 4.3] "cost(a(t) i ) = λ · σi,obs where σi,obs is the standard deviation of the observed rewards for a given user"
  - [corpus] Weak. No corpus neighbors discuss delayed effects or reward engineering in bandit algorithms for health interventions.
- Break condition: If the cost parameter λ is poorly tuned, the algorithm may under- or over-penalize interventions, harming performance.

## Foundational Learning

- Concept: Reinforcement Learning in Bandit Setting
  - Why needed here: The mobile health study operates in a sequential decision-making environment with binary actions and immediate feedback, which fits the contextual bandit framework.
  - Quick check question: In a contextual bandit, what is the difference between the policy and the value function?

- Concept: Random Effects Models in Statistics
  - Why needed here: Allows the algorithm to share statistical strength across users while still personalizing interventions, critical given limited data per user.
  - Quick check question: In a mixed-effects model, how are the fixed and random effects interpreted in terms of population and individual variation?

- Concept: Bayesian Inference and Posterior Sampling
  - Why needed here: Enables uncertainty-aware action selection and smooth, reproducible randomization probabilities, important for both performance and after-study analysis.
  - Quick check question: What is the advantage of using a smooth function for action selection probabilities in posterior sampling?

## Architecture Onboarding

- Component map: State representation -> Reward model (Bayesian mixed linear model with random effects) -> Hyperparameter updater (Empirical Bayes) -> Action selector (Smooth posterior sampling) -> Reward engineer (Penalty for delayed effects)
- Critical path:
  1. Observe state S(t)
  2. Compute posterior mean and variance for current user
  3. Sample action probability using smooth logistic function
  4. Execute action, collect reward
  5. Update posterior and hyperparameters at end of day/week
- Design tradeoffs:
  - Pooling vs personalization: Random effects balance between full pooling (BLR) and full personalization, trading off bias and variance.
  - Smooth vs hard action selection: Smooth sampling improves replicability but may slow adaptation.
  - Reward engineering: Penalizing actions for delayed effects adds realism but requires careful tuning of λ.
- Failure signatures:
  - If random effects variance shrinks to zero, algorithm degenerates to BLR (full pooling).
  - If noise variance is overestimated, action probabilities become too uniform.
  - If smoothing parameter b is too large, action probabilities concentrate too quickly, reducing exploration.
- First 3 experiments:
  1. Compare reBandit vs BLR and random policy on a simulated MiWaves-like environment with low heterogeneity.
  2. Vary the smoothing parameter b and observe effect on action selection variability and total reward.
  3. Test reBandit in a high heterogeneity environment to see if it outperforms full pooling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of reBandit compare to fully personalized approaches in terms of sample efficiency and reward maximization?
- Basis in paper: [explicit] The paper mentions that fully personalized approaches are not feasible for the MiWaves pilot study due to scarce data, but it doesn't directly compare reBandit's performance to fully personalized approaches.
- Why unresolved: The paper only compares reBandit to fully pooled (BLR) and random algorithms, leaving the comparison to fully personalized approaches unexplored.
- What evidence would resolve it: Experimental results comparing reBandit's performance to a fully personalized algorithm in a simulation environment with varying levels of data scarcity and user heterogeneity.

### Open Question 2
- Question: What is the impact of the choice of prior distribution on reBandit's performance, and how sensitive is the algorithm to the choice of hyperparameters?
- Basis in paper: [inferred] The paper mentions using informative Bayesian priors and updating hyperparameters online, but it doesn't explore the sensitivity of the algorithm to different prior choices or hyperparameter settings.
- Why unresolved: The paper only presents results using a specific prior distribution and hyperparameter settings, without exploring the robustness of the algorithm to different choices.
- What evidence would resolve it: Sensitivity analysis of reBandit's performance to different prior distributions and hyperparameter settings in various simulation environments.

### Open Question 3
- Question: How does reBandit's performance generalize to real-world mHealth data with different characteristics than the SARA dataset?
- Basis in paper: [explicit] The paper uses a simulation environment constructed from the SARA dataset to evaluate reBandit's performance, but it doesn't test the algorithm on real-world mHealth data.
- Why unresolved: The simulation environment, while useful for benchmarking, may not capture all the complexities and nuances of real-world mHealth data.
- What evidence would resolve it: Experimental results of reBandit's performance on real-world mHealth datasets with different characteristics (e.g., different user demographics, intervention types, and data collection methods).

## Limitations

- Empirical validation is limited to simulations using models trained on a single prior study dataset
- Key implementation details of smooth posterior sampling and empirical Bayes hyperparameter updates are not fully specified
- Choice of cost parameter λ for delayed effects is heuristic and not systematically evaluated

## Confidence

Our confidence in reBandit's core claims is **Medium**. The algorithm's theoretical grounding in random effects modeling and Bayesian inference is sound, but empirical validation is limited to simulations using models trained on a single prior study dataset. The paper does not report on a real-world clinical trial or independent validation dataset. Key uncertainties include: the exact implementation details of the smooth posterior sampling and empirical Bayes hyperparameter updates are not fully specified; the choice of cost parameter λ for delayed effects is heuristic and not systematically evaluated; and the simulation environment, while constructed from real data, is still a simplified abstraction of complex human behavior. The algorithm's performance advantage increases with population heterogeneity, but the range of heterogeneity tested is narrow. The lack of direct corpus neighbors for core methodological innovations (random effects in RL, reward engineering for delayed effects, smooth posterior sampling) means there are few external validation signals.

## Next Checks

1. **Reimplement and verify**: Independently reimplement reBandit and baseline algorithms, then reproduce the simulation results to confirm the reported performance advantage.
2. **Parameter sensitivity analysis**: Systematically vary the key hyperparameters (λ for delayed effects, smoothing parameter b, random effects variance) to map the robustness of reBandit's performance.
3. **Independent validation dataset**: Apply reBandit to a held-out subset of the SARA dataset or a new mobile health dataset to test generalization beyond the training simulation environment.