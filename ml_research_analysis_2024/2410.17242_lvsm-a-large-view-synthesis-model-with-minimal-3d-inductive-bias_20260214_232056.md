---
ver: rpa2
title: 'LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias'
arxiv_id: '2410.17242'
source_url: https://arxiv.org/abs/2410.17242
tags:
- lvsm
- view
- input
- tokens
- decoder-only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LVSM, a transformer-based approach for novel
  view synthesis from sparse input images. It proposes two architectures: an encoder-decoder
  model that learns a latent scene representation, and a decoder-only model that directly
  maps inputs to outputs, minimizing 3D inductive biases.'
---

# LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias

## Quick Facts
- arXiv ID: 2410.17242
- Source URL: https://arxiv.org/abs/2410.17242
- Authors: Haian Jin; Hanwen Jiang; Hao Tan; Kai Zhang; Sai Bi; Tianyuan Zhang; Fujun Luan; Noah Snavely; Zexiang Xu
- Reference count: 23
- Primary result: Decoder-only LVSM achieves state-of-the-art novel view synthesis with 1.5-3.5 dB PSNR improvement

## Executive Summary
This paper introduces LVSM, a transformer-based approach for novel view synthesis from sparse input images. It proposes two architectures: an encoder-decoder model that learns a latent scene representation, and a decoder-only model that directly maps inputs to outputs, minimizing 3D inductive biases. Both models achieve state-of-the-art results, with the decoder-only variant outperforming previous methods by 1.5 to 3.5 dB PSNR. Notably, LVSM demonstrates strong zero-shot generalization and scalability, even with limited computational resources.

## Method Summary
LVSM uses transformer architectures to perform novel view synthesis without traditional 3D inductive biases. The method tokenizes input images into patches, represents camera rays using Plücker embeddings, and uses self-attention transformers to synthesize novel views. Two variants are proposed: an encoder-decoder model that compresses inputs into latent tokens, and a decoder-only model that directly maps input tokens to output tokens. Both are trained with MSE and perceptual losses on datasets like Objaverse and LLFF.

## Key Results
- Decoder-only LVSM achieves 1.5-3.5 dB PSNR improvement over state-of-the-art methods
- Strong zero-shot generalization to varying numbers of input views (1-10+ views)
- Outperforms previous methods even when trained with only 1-2 GPUs versus 64 A100s
- Demonstrates scalability with improved performance when using more input views

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LVSM achieves high-quality novel view synthesis by replacing 3D inductive biases with large-scale transformer learning, enabling the model to learn rendering priors directly from data.
- **Mechanism**: The model bypasses traditional 3D representations (e.g., NeRF, 3DGS) and handcrafted structures (e.g., epipolar projections, plane sweeps). Instead, it uses self-attention transformers to map input tokens to target tokens without explicit geometric priors. This allows the model to learn implicit 3D geometry and appearance relationships directly from multi-view image data.
- **Core assumption**: The training data contains sufficient 3D geometric and photometric cues for the transformer to infer depth, occlusion, and view-dependent effects without explicit 3D priors.
- **Evidence anchors**:
  - [abstract] "Both models bypass the 3D inductive biases used in previous methods—from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps)—addressing novel view synthesis with a fully data-driven approach."
  - [section] "The decoder-only LVSM integrates the novel view synthesis process into a holistic data-driven framework, achieving scene reconstruction and rendering simultaneously in a fully implicit manner with minimal 3D inductive bias."
  - [corpus] Weak; no direct comparison to inductive bias removal in corpus neighbors.
- **Break condition**: If the training data lacks sufficient multi-view coverage or geometric diversity, the transformer cannot infer accurate 3D structure, leading to poor synthesis quality.

### Mechanism 2
- **Claim**: The decoder-only LVSM outperforms the encoder-decoder variant because it eliminates the lossy compression step inherent in latent representations, preserving more information for rendering.
- **Mechanism**: The encoder-decoder model compresses all input information into a fixed-length latent token sequence, which can lose detail. The decoder-only model directly maps the full input token sequence to the target output, avoiding compression artifacts and allowing richer information flow.
- **Core assumption**: The fixed latent token length is insufficient to capture all necessary scene information for high-quality synthesis, especially in complex geometries or materials.
- **Evidence anchors**:
  - [abstract] "the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR."
  - [section] "The compression process can result in information loss, as the fixed latent tokens length is usually smaller than the original image tokens length, which imposes an upper bound on performance."
  - [corpus] Weak; no explicit comparison of encoder-decoder vs decoder-only compression trade-offs.
- **Break condition**: If the latent token length is increased sufficiently, the performance gap may narrow, reducing the advantage of the decoder-only approach.

### Mechanism 3
- **Claim**: LVSM generalizes to unseen numbers of input views (including single-view) due to its minimal inductive bias, allowing it to learn flexible view synthesis mappings.
- **Mechanism**: By not encoding explicit 3D priors, the model learns a more general function mapping from any number of input views to the target view. This flexibility enables zero-shot generalization to different input view counts during inference.
- **Core assumption**: The training process exposes the model to sufficient variation in input view configurations to learn a robust, generalizable synthesis function.
- **Evidence anchors**:
  - [abstract] "our models, trained on 2-4 input views, demonstrate strong zero-shot generalization to an unseen number of views, ranging from a single input to more than 10."
  - [section] "our decoder-only LVSM shows increasingly better performance when using more input views, verifying the scalability of our model design at test time."
  - [corpus] Weak; no direct evidence of zero-shot generalization in corpus neighbors.
- **Break condition**: If the model overfits to specific view configurations during training, it may fail to generalize to unseen view counts or extreme baselines.

## Foundational Learning

- **Concept**: Plücker ray embeddings for representing camera poses and rays.
  - **Why needed here**: Plücker rays encode both the direction and position of a 3D ray in a compact 6D form, which is used to condition the transformer on the spatial relationship between input and target views.
  - **Quick check question**: How does a Plücker ray differ from a standard 3D ray representation, and why is it useful for view synthesis?

- **Concept**: Transformer self-attention and cross-attention mechanisms.
  - **Why needed here**: Self-attention allows the model to capture long-range dependencies between image patches and their corresponding rays, while cross-attention (in encoder-decoder) or direct mapping (in decoder-only) enables synthesis of the target view conditioned on inputs.
  - **Quick check question**: What is the difference between self-attention and cross-attention in transformers, and how are they applied in LVSM's architectures?

- **Concept**: Photometric loss functions (MSE + perceptual loss) for training.
  - **Why needed here**: These losses ensure the synthesized images are both pixel-accurate and perceptually realistic, capturing high-frequency details and structural similarity.
  - **Quick check question**: Why combine MSE and perceptual loss, and what aspects of image quality does each term address?

## Architecture Onboarding

- **Component map**: Input: Patchified images + Plücker ray embeddings → Input tokens; Target: Plücker ray embeddings → Query tokens; Model: Encoder-decoder or decoder-only transformer; Output: Updated query tokens → RGB patches → Synthesized image; Loss: MSE + perceptual loss

- **Critical path**: Tokenization → Transformer layers → Linear output → Sigmoid → Unpatchify → Final image

- **Design tradeoffs**:
  - Encoder-decoder: Faster inference (fixed latent size), but potential information loss; decoder-only: Higher quality and scalability, but slower with more inputs.
  - Model size vs. performance: Larger models (more layers) improve quality but increase training cost.
  - Training stability: Requires QK-Norm and gradient clipping to prevent divergence.

- **Failure signatures**:
  - Gradient explosion: Model crashes during training; fix with QK-Norm, gradient clipping.
  - Poor generalization: Blurry outputs or artifacts on unseen view counts; may need more diverse training data.
  - Aspect ratio mismatch: Blurred boundaries when input/output aspect ratios differ from training; center-cropping may be needed.

- **First 3 experiments**:
  1. Train a small decoder-only LVSM on a toy dataset (e.g., synthetic multi-view shapes) to verify basic synthesis works.
  2. Compare encoder-decoder vs decoder-only on a validation set to observe quality vs speed trade-offs.
  3. Test zero-shot generalization by evaluating on 1, 4, and 8 input views after training on 4 views.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Computational requirements are high (64 A100 GPUs), though the paper claims good performance with fewer resources
- Limited ablation studies on the impact of removing specific 3D inductive biases
- No open-source code available for reproducibility

## Confidence

- **High Confidence**: The empirical results showing LVSM outperforms previous methods by 1.5-3.5 dB PSNR; the general architecture description of using Plücker ray embeddings with transformers
- **Medium Confidence**: The claim that removing 3D inductive biases is the primary reason for performance gains; the explanation of why decoder-only outperforms encoder-decoder
- **Low Confidence**: The assertion that LVSM truly has "minimal" inductive bias (given it still uses 2D convolutions for patchification and Plücker rays); the scalability claims beyond the reported experiments

## Next Checks
1. Implement an ablation study comparing LVSM with and without Plücker ray embeddings to quantify the actual inductive bias contribution to performance gains
2. Replicate the zero-shot generalization experiments on a held-out dataset with varying numbers of input views (1, 4, 8, 16) to verify the claimed flexibility
3. Train a reduced computational budget version (e.g., 8 GPUs) and evaluate whether the reported performance can be achieved with more modest resources, as claimed in the paper