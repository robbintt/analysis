---
ver: rpa2
title: The Role of Syntactic Span Preferences in Post-Hoc Explanation Disagreement
arxiv_id: '2403.19424'
source_url: https://arxiv.org/abs/2403.19424
tags: []
core_contribution: This work analyzes why post-hoc explanation methods disagree by
  examining linguistic preferences. It finds that methods with similar word class
  preferences (e.g., targeting nouns vs.
---

# The Role of Syntactic Span Preferences in Post-Hoc Explanation Disagreement

## Quick Facts
- arXiv ID: 2403.19424
- Source URL: https://arxiv.org/abs/2403.19424
- Reference count: 0
- Key outcome: This work analyzes why post-hoc explanation methods disagree by examining linguistic preferences. It finds that methods with similar word class preferences (e.g., targeting nouns vs. determiners) tend to agree more, especially when compared at the span level rather than token level. Agreement improves using dynamic k estimation instead of fixed k, with the best results using a threshold of μ > 0. Token-level differences are smoothed when analyzing syntactic spans, and agreement increases when selecting spans dynamically rather than using a fixed subset size k. The findings suggest that disagreement arises from systematic differences in linguistic preferences across methods.

## Executive Summary
This study investigates why post-hoc explanation methods like LIME, SHAP, and Occlusion often disagree in their attributions. The researchers propose that systematic linguistic preferences drive these disagreements, with methods targeting different word classes (nouns, determiners, etc.) producing divergent results. By analyzing explanations at the syntactic span level rather than token level, and using dynamic k estimation instead of fixed k, the authors demonstrate improved agreement across methods. The findings suggest that syntactic structure and linguistic preferences are fundamental to understanding explanation method disagreements.

## Method Summary
The researchers analyzed post-hoc explanation methods (LIME, SHAP, Occlusion) on sentiment analysis and fact-checking tasks. They compared token-level versus syntactic span-level agreement, where spans are derived from constituency parse trees. To address the fixed k limitation in prior work, they introduced dynamic k estimation using a threshold μ > 0 to select the minimal subset of words that maximizes explanation agreement. The study measured agreement using both token-level metrics and span-level metrics, examining how different word class preferences (nouns, verbs, determiners, etc.) influenced method agreement.

## Key Results
- Methods with similar word class preferences (e.g., nouns vs. determiners) show higher agreement
- Span-level analysis smooths token-level disagreements and reveals systematic linguistic patterns
- Dynamic k estimation with threshold μ > 0 outperforms fixed k in maximizing agreement
- LIME and SHAP show high agreement when targeting similar word classes, while Occlusion shows distinct preferences

## Why This Works (Mechanism)
The disagreement between post-hoc explanation methods stems from their systematic preferences for different linguistic elements. When methods target similar word classes or syntactic structures, they produce more consistent explanations. Span-level analysis captures these preferences more effectively than token-level analysis by grouping related words into meaningful syntactic units. Dynamic k estimation allows each method to select its most confident attributions rather than forcing a fixed number of explanations, which accommodates their different confidence distributions across word classes.

## Foundational Learning
**Constituency Parsing**: Breaking sentences into hierarchical syntactic structures using grammar rules. Why needed: Provides the syntactic spans for analyzing linguistic preferences. Quick check: Verify parse trees correctly capture noun phrases, verb phrases, and other meaningful units.

**Post-hoc Explanation Methods**: Techniques like LIME, SHAP, and Occlusion that attribute model predictions to input features. Why needed: These are the explanation methods whose disagreements we're analyzing. Quick check: Ensure implementations match original papers' specifications.

**Agreement Metrics**: Statistical measures comparing explanation outputs across methods. Why needed: Quantifies the extent of disagreement or agreement between methods. Quick check: Test metrics on synthetic data where expected agreement is known.

**Dynamic k Estimation**: Adaptive selection of explanation subset size based on confidence thresholds rather than fixed k. Why needed: Addresses the limitation that different methods have different confidence distributions. Quick check: Verify dynamic k selects appropriate numbers of words for each method.

**Word Class Analysis**: Categorizing words by their grammatical function (noun, verb, determiner, etc.). Why needed: Reveals systematic preferences that drive explanation disagreements. Quick check: Cross-validate word class assignments with multiple taggers.

## Architecture Onboarding
**Component Map**: Input Text -> Constituency Parser -> Word Class Tagger -> Explanation Methods (LIME, SHAP, Occlusion) -> Token Attributions -> Span Aggregation -> Agreement Metrics -> Dynamic k Selection

**Critical Path**: Parser and tagger outputs feed into explanation methods, which generate token attributions. These are aggregated into spans, measured for agreement, and used to estimate dynamic k for the next iteration.

**Design Tradeoffs**: Token-level analysis offers fine-grained attribution but misses linguistic patterns; span-level analysis captures structure but may oversimplify. Fixed k is simple but arbitrary; dynamic k is adaptive but introduces hyperparameter tuning. Constituency parsing assumes grammatical structure is optimal, but dependency parsing might capture different linguistic relationships.

**Failure Signatures**: Parser errors propagate to span analysis; word class tagger mistakes affect preference identification; dynamic k threshold μ > 0 may be task-dependent and require tuning. Token-level agreement metrics may miss systematic linguistic disagreements visible only at span level.

**First Experiments**:
1. Test parser and tagger accuracy on validation dataset to ensure syntactic and word class analysis is reliable
2. Compare token-level vs. span-level agreement on synthetic examples with known linguistic patterns
3. Run ablation study on dynamic k threshold parameter across multiple datasets to establish sensitivity

## Open Questions the Paper Calls Out
None

## Limitations
- Findings may not generalize beyond sentiment analysis and fact-checking tasks to other NLP applications
- Analysis limited to three specific explanation methods, raising questions about generalizability to other post-hoc techniques
- Assumes constituency parse trees are optimal for explanation analysis, but alternative parsing strategies might yield different patterns
- Dynamic k estimation threshold μ > 0 may be arbitrary and require task-specific tuning

## Confidence
High confidence in claim that syntactic span preferences drive explanation disagreement for studied methods and tasks, given systematic analysis of word class targeting patterns. Medium confidence in finding that span-level analysis smooths token-level disagreements, as this depends on parse quality and the assumption that syntactic units better capture explanation meaning. Medium confidence in recommendation for dynamic k estimation over fixed k, since the optimal threshold may vary by task and explanation method.

## Next Checks
1. Test whether syntactic span preferences predict disagreement patterns in non-classification tasks (e.g., generation or sequence labeling) and across a broader set of explanation methods
2. Compare agreement scores when using alternative syntactic structures (dependency parses, semantic roles) versus constituency trees to determine if span-based analysis is parsing-method dependent
3. Conduct ablation studies on the μ threshold parameter in dynamic k estimation across multiple datasets to establish task-specific optimal values and test sensitivity to this hyperparameter