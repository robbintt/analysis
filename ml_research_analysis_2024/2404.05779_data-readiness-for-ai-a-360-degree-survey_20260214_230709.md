---
ver: rpa2
title: 'Data Readiness for AI: A 360-Degree Survey'
arxiv_id: '2404.05779'
source_url: https://arxiv.org/abs/2404.05779
tags:
- data
- quality
- metrics
- readiness
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey of data readiness metrics
  for AI, focusing on both structured and unstructured data. The authors examine over
  140 papers and propose a taxonomy of Data Readiness for AI (DRAI) metrics, covering
  dimensions such as completeness, outliers, mislabels, duplicity, feature relevancy,
  class imbalance, class separability, discrimination index, data split ratio, data
  point impact, correctness, timeliness, privacy leakage, sample size, and FAIRness
  score.
---

# Data Readiness for AI: A 360-Degree Survey

## Quick Facts
- arXiv ID: 2404.05779
- Source URL: https://arxiv.org/abs/2404.05779
- Authors: Kaveen Hiniduma; Suren Byna; Jean Luca Bez
- Reference count: 40
- Primary result: Comprehensive survey proposing taxonomy of 15+ data readiness dimensions for AI

## Executive Summary
This paper presents a comprehensive survey of data readiness metrics for AI, examining over 140 papers to propose a taxonomy of Data Readiness for AI (DRAI) metrics. The survey covers both structured and unstructured data, addressing dimensions such as completeness, outliers, mislabels, duplicity, feature relevancy, class imbalance, class separability, discrimination index, data split ratio, data point impact, correctness, timeliness, privacy leakage, sample size, and FAIRness score. The proposed taxonomy serves as a valuable resource for researchers and practitioners, offering insights into various metrics used to evaluate data readiness and highlighting the importance of addressing data quality issues in AI applications.

## Method Summary
The paper conducts a comprehensive literature review of data readiness metrics for AI, collecting papers from sources including ACM Digital Library, IEEE Xplore, journals like Nature and Springer, and online articles by prominent AI experts. The authors analyze over 140 papers to identify key dimensions of data readiness for AI and propose a taxonomy of DRAI metrics based on these dimensions. While the survey methodology is clearly described, the specific search queries and selection criteria are not explicitly provided, which may affect reproducibility.

## Key Results
- Taxonomy of 15+ DRAI metrics covering structured and unstructured data dimensions
- Identification of key data readiness dimensions including completeness, outliers, duplicity, class imbalance, and FAIRness
- Framework for evaluating data quality issues before model training begins
- Bridge between data quality assessment and AI model performance optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data readiness metrics enable early detection of quality issues before model training begins
- Mechanism: Applying structured metrics (completeness, outliers, duplicity) and unstructured metrics (lexical diversity, image quality) early in ML pipeline identifies problematic data patterns for correction, reducing downstream model failures
- Core assumption: Quality issues have measurable characteristics quantifiable before training
- Evidence anchors:
  - [abstract] "Evaluation of data readiness is a crucial step in improving the quality and appropriateness of data usage for AI"
  - [section] "Gupta et al. 's data quality toolkit introduces a class overlap metric, which identifies and quantifies overlapping regions among different classes"
  - [corpus] Weak - no direct corpus evidence supporting early detection claims
- Break condition: If data lacks measurable features or metrics cannot capture domain-specific nuances

### Mechanism 2
- Claim: Standardized metrics allow cross-dataset and cross-domain comparability
- Mechanism: Using consistent metrics (Mean Opinion Score for speech, PSNR for image) across datasets enables benchmarking and selection of best datasets for specific AI tasks
- Core assumption: Quality metrics are universally applicable or can be normalized across domains
- Evidence anchors:
  - [section] "PSNR, SSIM, VIF, and VSNR are image quality metrics... that can also be effectively applied to measure the quality of video data"
  - [section] "VMAF takes a multifaceted approach by combining traditional metrics like PSNR and SSIM with machine learning techniques"
  - [corpus] Weak - no direct corpus evidence supporting cross-dataset comparability claims
- Break condition: If domain-specific characteristics render standard metrics inapplicable

### Mechanism 3
- Claim: FAIRness metrics ensure long-term usability and interoperability of datasets
- Mechanism: Evaluating datasets against Findability, Accessibility, Interoperability, and Reusability criteria ensures datasets remain usable as AI systems evolve
- Core assumption: FAIR principles are essential for sustainable AI development
- Evidence anchors:
  - [section] "Wilkinson et al. [128] introduced a comprehensive FAIRness measurement framework, aligning with the four FAIR sub-principles"
  - [section] "DataONE(Data Observation Network for Earth) is a community-driven initiative that has adopted metrics to measure FAIRness of research data"
  - [corpus] Weak - no direct corpus evidence supporting FAIRness claims
- Break condition: If FAIR principles conflict with specific data governance requirements

## Foundational Learning

- Concept: Data quality dimensions (completeness, accuracy, consistency, timeliness)
  - Why needed here: These dimensions form the foundation for evaluating whether data is ready for AI training
  - Quick check question: What are the four primary dimensions of data quality used in most readiness assessments?

- Concept: Similarity metrics for duplicate detection (Levenshtein distance, Jaro distance)
  - Why needed here: Duplicate records can skew AI model training and must be identified and removed
  - Quick check question: Which similarity metric measures the number of operations needed to transform one string into another?

- Concept: Bias detection in textual data (word embeddings, cosine similarity)
  - Why needed here: Ensuring fairness in AI requires identifying and mitigating biases in training data
  - Quick check question: How does cosine similarity between word embeddings help detect bias in textual data?

## Architecture Onboarding

- Component map: Data Collection -> Preprocessing -> Quality Assessment -> Model Training -> Evaluation
- Critical path: Data Collection -> Preprocessing -> Quality Assessment -> Model Training
- Design tradeoffs: Comprehensive metrics vs. computational efficiency; domain-specific vs. universal metrics
- Failure signatures: High outlier rates, low lexical diversity, poor class separability
- First 3 experiments:
  1. Apply completeness and outlier detection metrics to a sample dataset
  2. Test lexical diversity measurement on a small text corpus
  3. Evaluate image quality using PSNR and SSIM on a sample image set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective and generalizable data readiness metrics for AI applications across diverse data types and domains?
- Basis in paper: [explicit] The paper highlights the lack of a unified framework for data readiness metrics and the need for adaptable metrics across various data types
- Why unresolved: Existing metrics are often tailored to specific data types or domains, making them less generalizable. The paper calls for a comprehensive framework but doesn't provide one
- What evidence would resolve it: Development and validation of a unified framework for data readiness metrics that can be applied across different data types (structured, unstructured, multimedia) and domains, with empirical evidence of its effectiveness

### Open Question 2
- Question: How can data readiness metrics effectively balance data quality and quantity to optimize AI model performance?
- Basis in paper: [explicit] The paper mentions the challenge of balancing data quality and quantity, emphasizing the need for sufficient data volumes for meaningful analysis and model training
- Why unresolved: Current metrics primarily focus on data quality, while the trade-off between quality and quantity remains underexplored. The paper highlights this as a crucial challenge but doesn't provide a solution
- What evidence would resolve it: Research on optimal data readiness thresholds that consider both quality and quantity, supported by empirical studies demonstrating improved AI model performance

### Open Question 3
- Question: What are the most effective methods for assessing and mitigating bias in unstructured data for AI applications?
- Basis in paper: [explicit] The paper discusses bias indicators for textual data but highlights the need for more comprehensive methods to address bias in unstructured data, including multimedia
- Why unresolved: Existing bias indicators are primarily designed for textual data, and their applicability to multimedia data is unclear. The paper emphasizes the importance of addressing bias but doesn't provide a solution for unstructured data
- What evidence would resolve it: Development and validation of bias assessment and mitigation methods specifically tailored for unstructured data (images, audio, video), with empirical evidence of their effectiveness in reducing bias in AI models

## Limitations
- Survey relies heavily on secondary literature review without empirical validation of proposed taxonomy
- Many claims about metric effectiveness supported by citations but lack direct corpus evidence
- Mechanism for cross-dataset comparability through standardized metrics remains largely theoretical
- FAIRness assessment framework shows weak corpus support for practical implementation in AI contexts

## Confidence

**Major Uncertainties and Limitations:**
- High confidence: The taxonomy structure and categorization of data readiness dimensions
- Medium confidence: The importance of data readiness assessment for AI quality improvement
- Low confidence: Claims about cross-dataset comparability and FAIRness implementation effectiveness

## Next Checks

1. Conduct a small-scale empirical study applying the DRAI taxonomy to three diverse datasets to test metric consistency and cross-dataset applicability

2. Perform a validation study with AI practitioners to assess the practical utility and completeness of the proposed taxonomy

3. Implement a subset of key metrics (completeness, outliers, class separability) on a real-world dataset to verify their effectiveness in detecting data quality issues before model training