---
ver: rpa2
title: Observations on Building RAG Systems for Technical Documents
arxiv_id: '2404.00657'
source_url: https://arxiv.org/abs/2404.00657
tags:
- group
- subfield
- indication
- service
- assignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates retrieval augmented generation (RAG) challenges
  in technical document QA, finding that embeddings poorly capture domain information.
  Experiments with IEEE wireless LAN specs and battery terminology show chunk length
  impacts retriever embeddings, similarity scores are unreliable for result comparison,
  and keyword position affects retrieval accuracy.
---

# Observations on Building RAG Systems for Technical Documents

## Quick Facts
- arXiv ID: 2404.00657
- Source URL: https://arxiv.org/abs/2404.00657
- Reference count: 11
- No single best method emerged for RAG systems on technical documents, though sentence-level retrieval with multiple paragraphs shows promise

## Executive Summary
This study investigates retrieval augmented generation (RAG) challenges in technical document question answering, finding that embeddings poorly capture domain-specific information. Through experiments with IEEE wireless LAN specifications and battery terminology documents, the authors demonstrate that chunk length impacts retriever embeddings, similarity scores are unreliable for result comparison, and keyword position affects retrieval accuracy. Sentence-based similarity retrieval consistently outperforms paragraph-based approaches, and generator performance benefits from broader context but struggles with acronyms and term definitions.

## Method Summary
The study uses IEEE Wireless LAN MAC/PHY specifications (2021) and IEEE Standard Glossary of Stationary Battery Terminology (2016) as technical document corpora. Documents are processed into term/definition pairs and full paragraphs, then embedded using MPNET. A Llama2-7b-chat LLM serves as the generator with specific prompts. The retrieval pipeline employs sentence-level similarity search to retrieve top-k distinct paragraphs. 42 representative queries are used to evaluate both retriever and generator performance across multiple hypotheses regarding chunk size, similarity scoring, and retrieval strategies.

## Key Results
- Chunk length significantly impacts retriever embeddings, with longer sentences producing spurious similarities
- Sentence-based similarity retrieval outperforms paragraph-based approaches for providing context to generators
- Generators benefit from broader context but struggle with acronym expansions and term definitions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sentence embeddings become unreliable with increasing chunk size
- Mechanism: As sentence length increases, the embedding model tends to produce higher cosine similarity scores between unrelated sentences, leading to spurious similarities
- Core assumption: MPNET embedding model's behavior changes with input length
- Evidence anchors:
  - [section] "Appendix B Fig. 1 shows the Kernel Density Estimate (KDE) plot of cosine similarity scores for various sentence lengths... The higher similarity distribution for larger lengths indicates spurious similarities which we manually validate for a few samples"
  - [section] "When both the query and queried document are over 200 words, the similarity distribution is bimodal"
- Break condition: If embedding models are modified to normalize similarity scores based on input length

### Mechanism 2
- Claim: Sentence-based similarity retrieval outperforms paragraph-based approaches
- Mechanism: Sentence-level similarity search retrieves more precise context paragraphs, providing better information to the generator
- Core assumption: Fine-grained retrieval provides better context than coarse-grained approaches
- Evidence anchors:
  - [section] "In the former, we retrieve the paragraph to which the sentence belongs and take top-k distinct paragraphs from top similar sentences... we observe that the results by sentence-based similarity search and paragraphs being used for generator provides better retriever and generator performance"
  - [section] "Table 2 shows sentence-based similarity and paragraph-based retrieval gives much better context to generator"
- Break condition: If generator architecture changes to better handle paragraph-level context

### Mechanism 3
- Claim: Generator performance is enhanced by broader context but struggles with acronyms and term definitions
- Mechanism: Multiple retrieved paragraphs provide sufficient context for the LLM to generate comprehensive answers, but acronym expansions often lack additional value
- Core assumption: LLM benefits from multiple context paragraphs but can't improve on acronym expansions
- Evidence anchors:
  - [section] "Better context gives better generated responses"
  - [section] "For acronyms and their expansions, the generator does not add any additional value"
  - [corpus] "Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications" (related work on telecom RAG challenges)
- Break condition: If generator is fine-tuned on domain-specific acronym usage patterns

## Foundational Learning

- Concept: Cosine similarity and embedding spaces
  - Why needed here: Understanding how similarity scores work is crucial for interpreting retrieval results
  - Quick check question: Why do longer sentences tend to have higher similarity scores even when unrelated?

- Concept: Chunking strategies for technical documents
  - Why needed here: Different chunking approaches (sentence vs paragraph) significantly impact retrieval quality
  - Quick check question: How does splitting definitions from terms affect retrieval performance?

- Concept: Large Language Model prompt engineering
  - Why needed here: Effective prompts are necessary to get useful answers from the generator
  - Quick check question: What information should be included in the prompt to help the LLM generate better answers?

## Architecture Onboarding

- Component map: Document processor -> MPNET embedding model -> Retriever (similarity search) -> Generator (Llama2-7b-chat) -> Query interface

- Critical path:
  1. Document processing and embedding generation
  2. Query embedding generation
  3. Similarity search for retrieval
  4. Context aggregation from retrieved results
  5. Answer generation using LLM

- Design tradeoffs:
  - Sentence vs paragraph chunking: Tradeoff between precision and context richness
  - Embedding model selection: Domain-specific vs general-purpose embeddings
  - Similarity threshold: Balance between recall and precision
  - Number of retrieved paragraphs: More context vs potential noise

- Failure signatures:
  - High similarity scores but incorrect answers
  - Low similarity scores but correct answers
  - Acronym expansions without additional context
  - Missing answers due to strict similarity thresholds

- First 3 experiments:
  1. Compare sentence vs paragraph chunking on a small set of test queries
  2. Test different similarity thresholds on retrieval performance
  3. Evaluate generator performance with 1, 3, and 5 retrieved paragraphs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does similarity thresholding for retrieved results improve or harm RAG performance for technical documents?
- Basis in paper: [explicit] Paper states "We find that retrieval by thresholding on similarity scores is not helpful" and shows inconsistent performance across experiments
- Why unresolved: While the paper shows thresholding is not helpful in their specific experiments, they don't provide systematic evaluation across different document types or similarity metrics to establish general principles
- What evidence would resolve it: Systematic experiments varying document domains, embedding models, and similarity metrics to determine conditions under which similarity thresholding helps or hurts RAG performance

### Open Question 2
- Question: What is the optimal chunk size for embedding technical documents in RAG systems?
- Basis in paper: [inferred] Paper shows that "sentence embeddings become unreliable with increasing chunk size" and observes "spurious similarities" in longer sentences
- Why unresolved: The paper demonstrates problems with both short and long chunks but doesn't provide clear guidance on optimal chunk boundaries or how this varies across technical domains
- What evidence would resolve it: Comparative studies measuring retrieval accuracy and embedding quality across different chunk sizes (sentence, paragraph, section levels) for various technical document types

### Open Question 3
- Question: How should technical documents with extensive acronyms and abbreviations be optimally embedded for RAG systems?
- Basis in paper: [explicit] Paper observes "Definitions with acronyms or words having acronyms don't perform well" and notes generators often "expand or provide abbreviations which is not helpful"
- Why unresolved: The paper identifies this as a challenge but doesn't propose or evaluate solutions for handling acronyms in technical documentation
- What evidence would resolve it: Evaluation of different preprocessing strategies (expanding acronyms, creating dual embeddings, or other approaches) for technical documents with varying acronym density

## Limitations

- The study is based on only 42 queries across two specific technical domains, limiting generalizability
- No single optimal retrieval method was identified, suggesting the approach may not scale well to diverse technical documents
- Bimodal similarity distributions for longer text chunks indicate fundamental limitations in using cosine similarity for retrieval

## Confidence

- High confidence: Chunk length impacts retriever embeddings; sentence-based retrieval outperforms paragraph-based approaches
- Medium confidence: Generator benefits from broader context; struggles with acronym expansions and term definitions
- Low confidence: Assertion that no single method emerges as best given limited experimental scope

## Next Checks

1. Expand Query Diversity: Test the retrieval and generation pipeline with a broader set of technical domains and query types to validate whether sentence-based retrieval consistently outperforms paragraph-based approaches across different technical contexts.

2. Similarity Score Calibration: Investigate methods to calibrate similarity scores based on chunk length, potentially normalizing scores to account for the observed spurious similarities in longer text segments.

3. Generator Fine-tuning: Evaluate whether fine-tuning the LLM on domain-specific terminology and acronym usage patterns can address the observed failures in acronym expansion and term definition generation.