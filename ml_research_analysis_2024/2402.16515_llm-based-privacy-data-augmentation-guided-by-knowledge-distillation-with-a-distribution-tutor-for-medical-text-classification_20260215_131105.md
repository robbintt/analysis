---
ver: rpa2
title: LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a
  Distribution Tutor for Medical Text Classification
arxiv_id: '2402.16515'
source_url: https://arxiv.org/abs/2402.16515
tags: []
core_contribution: This paper proposes a differential privacy (DP)-based data augmentation
  (DA) framework for private text classification. The method transfers the DP pseudo-sample
  generation task to a DP-based discrimination task, using a large language model
  (LLM) to generate public samples and a DP-based discriminator to select samples
  fitting the private domain.
---

# LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification

## Quick Facts
- arXiv ID: 2402.16515
- Source URL: https://arxiv.org/abs/2402.16515
- Reference count: 26
- Primary result: Achieves F1=0.34 and accuracy=0.37 with ε=4 privacy budget, outperforming non-private training

## Executive Summary
This paper proposes a differential privacy (DP)-based data augmentation framework for private text classification using LLMs. The method transfers the DP pseudo-sample generation task to a DP-based discrimination task, where an LLM generates public samples and a DP-based discriminator selects samples fitting the private domain. The discriminator is constructed via knowledge distillation with multiple teacher models and a student model, while a DP-based tutor controls the label distribution of generated samples. The framework achieves strong performance on medical text classification while providing theoretical privacy guarantees.

## Method Summary
The framework employs LLMs to generate public samples, then uses a knowledge distillation-based discriminator to select samples fitting the private domain distribution while satisfying DP. Multiple teacher models are trained on disjoint private data subsets, and their knowledge is distilled into a student discriminator with calibrated noise for DP guarantees. A distribution tutor maintains label distribution characteristics by adding DP noise to label frequencies. The final classifier is trained on the augmented data using ClinicalBERT.

## Key Results
- Achieves F1 score of 0.34 and accuracy of 0.37 with privacy budget ε=4
- Outperforms strong DP-based baselines on text classification in private domains
- Even outperforms non-private training on private data
- Successfully balances privacy protection with classification performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation with multiple teachers achieves differential privacy by bounding sensitivity
- Mechanism: Each teacher is trained on disjoint private data subsets, so any single sample affects only one teacher's output distribution. This bounds the L2 sensitivity to √2, enabling calibrated Gaussian noise addition for DP.
- Core assumption: Private data contains no duplicates and is equally divided among teachers
- Evidence anchors:
  - [abstract]: "We construct a knowledge distillation model as the DP-based discriminator: teacher models, accessing private data, teaches students how to select private samples with calibrated noise to achieve DP."
  - [section 4]: "We denote the output distribution of m-th teacher model is Pm(Y|x)... For neighboring datasets D and D′ differing only one sample, we denote i-th teacher is affected by the different sample in D and D′."
- Break condition: If private data contains duplicates, sensitivity increases by N times, requiring proportionally larger noise that harms performance.

### Mechanism 2
- Claim: The distribution tutor maintains label distribution characteristics while providing DP guarantees
- Mechanism: The tutor counts label frequencies across private data, adds calibrated Gaussian noise, and normalizes to maintain probability distribution properties while satisfying DP with sensitivity √2.
- Core assumption: The noisy label distribution remains close enough to original to preserve useful generation characteristics
- Evidence anchors:
  - [section 3.5]: "The tutor follows a statistic way to collect the noised label distribution of all private samples Plabel(C) with a very small privacy cost."
  - [section 4]: "The function g(D) = PN j Pj(C) is the distribution on whole datasets... The sensitivity ∆tutor in Lemma 4 is (See deductions in App. B)"
- Break condition: If privacy budget for tutor is too low (ε too small), noise overwhelms true distribution, making generation guidance ineffective.

### Mechanism 3
- Claim: LLM-generated samples can be transformed into privacy-preserving pseudo-private samples through DP-based filtering
- Mechanism: LLM generates public samples conditioned on labels, then DP-based discriminator filters samples to select those fitting private domain distribution, creating high-quality augmented data with theoretical privacy guarantees.
- Core assumption: LLM generation quality is sufficient to provide diverse candidate samples that include some fitting private domain characteristics
- Evidence anchors:
  - [abstract]: "we employ LLMs' generation ability PLLM(x) to generate public samples x and construct a DP-based discriminator Pdiscri(·|x) to select synthesized samples x fits for private domain c"
  - [section 3.6]: "The idea is (1) taking advantage of strong generation ability of LLMs to obtain high quality data. (2) accessing private data causes privacy cost but accessing privacy through the student (i.e. discriminator) and tutor satisfying DP would not brings in additional loss."
- Break condition: If LLM generation quality is poor or domain-specific vocabulary is missing, insufficient candidates pass discriminator filtering, reducing augmentation effectiveness.

## Foundational Learning

- Concept: Differential Privacy (DP) with Gaussian Mechanism
  - Why needed here: Provides theoretical privacy guarantees while allowing useful data augmentation
  - Quick check question: What is the relationship between privacy budget ε and required noise scale in Gaussian mechanism?

- Concept: Knowledge Distillation with Multiple Teachers
  - Why needed here: Enables DP learning by bounding sensitivity through disjoint data partitions
  - Quick check question: How does partitioning private data among teachers reduce sensitivity compared to a single teacher model?

- Concept: Label Distribution Preservation in DP
  - Why needed here: Ensures augmented samples maintain class balance representative of private data
  - Quick check question: Why is it important to preserve label distribution characteristics in privacy-preserving data augmentation?

## Architecture Onboarding

- Component map: LLM Generator -> Candidate Sample Pool -> DP-based Discriminator (Student-Teacher Framework) -> Quality Filter -> Label Distribution Tutor -> Distribution Guide -> Private Data Classifier -> Final Evaluation
- Critical path: LLM Generation -> Discriminator Filtering -> Sample Selection -> Classifier Training
- Design tradeoffs:
  - More teachers -> Better sensitivity bounds but higher computational cost
  - Higher ε -> Better utility but weaker privacy protection
  - Larger LLM -> Better generation quality but higher inference cost
- Failure signatures:
  - Poor downstream performance -> Likely discriminator filtering too strict/loose or insufficient LLM quality
  - Privacy budget exceeded -> Noise scale too low or teacher partitioning inadequate
  - Imbalanced augmented data -> Tutor noise overwhelming true distribution
- First 3 experiments:
  1. Verify DP bounds by measuring sensitivity with controlled data changes
  2. Test discriminator accuracy with varying teacher counts (3, 5, 10, 15)
  3. Evaluate downstream classifier performance with different ε values (1, 2, 4, 8)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed DP-based DA framework change when using more advanced LLMs like GPT-4 instead of GPT-3.5?
- Basis in paper: [explicit] The paper mentions that GPT-3.5 is used in the experiments due to cost considerations and for fair comparison with baselines, but notes that GPT-4 might offer better performance.
- Why unresolved: The paper does not conduct experiments with GPT-4, leaving the impact of using a more advanced LLM on the framework's performance unclear.
- What evidence would resolve it: Conducting experiments using GPT-4 and comparing the results with those obtained using GPT-3.5 would provide insights into the impact of LLM advancements on the framework's performance.

### Open Question 2
- Question: What is the impact of the number of teacher models on the performance of the DP-based discriminator in the proposed framework?
- Basis in paper: [explicit] The paper mentions that the number of teacher models is set to 15 by default and analyzes the impact of teacher number on the discriminator's performance.
- Why unresolved: While the paper provides some analysis, it does not explore the full range of teacher numbers or their impact on the overall performance of the framework.
- What evidence would resolve it: Conducting experiments with varying numbers of teacher models and analyzing their impact on the discriminator's performance and the overall framework's effectiveness would provide insights into the optimal number of teachers.

### Open Question 3
- Question: How does the proposed DP-based DA framework handle cases where the generated text quality is poor or the data is difficult to generate?
- Basis in paper: [explicit] The paper mentions that the framework relies on LLM-generated data and suggests that poor text quality or difficulty in generating data might limit the framework's advantages.
- Why unresolved: The paper does not provide specific strategies or solutions for handling cases with poor text quality or difficulty in generating data.
- What evidence would resolve it: Exploring and implementing strategies to handle poor text quality or difficulty in generating data, such as prompt tuning or alternative generation methods, and evaluating their impact on the framework's performance would provide insights into its robustness and limitations.

## Limitations
- Effectiveness depends heavily on assumption that private data contains no duplicates when partitioned among teacher models
- Reliance on LLM generation quality means framework performance may suffer if public LLM lacks domain-specific knowledge
- Experimental validation focuses on single medical domain with 5,000 samples across 40 specialties

## Confidence

**High Confidence:**
- The theoretical DP guarantees derived from the teacher partitioning approach (sensitivity bound of √2) are mathematically sound when assumptions hold
- The basic architecture design (LLM generation + DP-based filtering) is implementable and coherent
- The experimental results showing performance improvements over baseline DP methods are verifiable

**Medium Confidence:**
- The method's superiority over non-private training on private data (F1=0.34, accuracy=0.37) at ε=4 will generalize to other datasets and domains
- The specific parameter choices (15 teachers, σ values for noise) represent optimal tradeoffs for all scenarios
- The computational cost and inference latency will remain practical at scale

**Low Confidence:**
- The method will maintain privacy guarantees under all realistic data conditions (e.g., duplicate samples, non-iid distributions)
- The framework can be extended to other data modalities (images, structured data) without fundamental redesign
- The approach will remain effective as LLMs become more specialized and domain-adapted

## Next Checks
1. **Sensitivity Analysis Under Realistic Data Conditions**: Test the framework's DP guarantees and performance when private data contains duplicate samples, non-iid distributions, or small class imbalances that violate the theoretical assumptions.

2. **Cross-Domain Generalization Study**: Evaluate the method across multiple medical specialties and non-medical domains (legal, financial, customer service) to assess whether the F1=0.34 performance at ε=4 represents typical behavior or is specific to the tested medical transcriptions dataset.

3. **Ablation Study on Teacher Count and Noise Calibration**: Systematically vary the number of teacher models (3, 5, 10, 15, 20) and noise scales (different σ values) to determine the optimal configuration for different dataset sizes and ε values, identifying the point of diminishing returns.