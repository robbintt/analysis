---
ver: rpa2
title: Discover and Mitigate Multiple Biased Subgroups in Image Classifiers
arxiv_id: '2403.12777'
source_url: https://arxiv.org/abs/2403.12777
tags:
- subgroups
- subgroup
- biased
- image
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of discovering multiple biased
  subgroups in image classifiers, which is a more challenging but practical problem
  compared to previous works that assume a single biased subgroup. The proposed method,
  DIM (Decomposition, Interpretation, and Mitigation), decomposes image features into
  multiple subgroup components using a bilinear dimension reduction method called
  Partial Least Squares (PLS), guided by supervision from the image classifier.
---

# Discover and Mitigate Multiple Biased Subgroups in Image Classifiers

## Quick Facts
- **arXiv ID**: 2403.12777
- **Source URL**: https://arxiv.org/abs/2403.12777
- **Authors**: Zeliang Zhang; Mingqian Feng; Zhiheng Li; Chenliang Xu
- **Reference count**: 40
- **Primary result**: A method called DIM (Decomposition, Interpretation, and Mitigation) that discovers and mitigates multiple biased subgroups in image classifiers using PLS decomposition with classifier supervision.

## Executive Summary
This paper addresses the challenging problem of discovering multiple biased subgroups in image classifiers, which is more practical than previous single-bias approaches. The proposed DIM method decomposes image features into subgroup components using Partial Least Squares (PLS) guided by supervision from the classifier's training dynamics. The semantic meaning of each subgroup is interpreted using vision-language foundation models, and two strategies are proposed for simultaneous mitigation: data-centric (filtering external data) and model-centric (pseudo-labeling with existing bias mitigation methods). Experiments on CIFAR-100, Breeds, and Hard ImageNet demonstrate DIM's effectiveness in discovering and mitigating multiple biases while uncovering classifier failure modes.

## Method Summary
DIM discovers multiple biased subgroups through a three-stage process. First, it decomposes image features into subgroup-specific components using PLS regression guided by classifier supervision signals (correctness, logit, or loss). Second, it interprets each subgroup's semantic meaning through cross-modal embeddings using CLIP, retrieving images with descriptions that match subgroup characteristics. Finally, it mitigates biases using two strategies: data-centric filtering of external training data to balance subgroup representation, and model-centric pseudo-labeling that integrates discovered subgroups into existing supervised bias mitigation methods like groupDRO and DI. The method operates on CLIP-encoded image features and leverages vision-language models for interpretation.

## Key Results
- DIM successfully discovers multiple biased subgroups on CIFAR-100 and Breeds datasets, outperforming baseline methods in subgroup identification accuracy.
- The method demonstrates effectiveness in mitigating multiple biases simultaneously through both data-centric and model-centric strategies, improving worst-group performance.
- DIM uncovers failure modes of classifiers on Hard ImageNet, showcasing broader applicability to understanding model bias beyond standard benchmark datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PLS decomposition guided by model supervision extracts subgroup directions that align with learned biases in the classifier.
- Mechanism: Partial Least Squares (PLS) decomposes image features into components that maximize correlation with supervision signals from the classifier's training dynamics (loss, correctness, logit), creating directions in latent space that capture subgroup-specific patterns.
- Core assumption: The classifier's training dynamics contain sufficient signal to guide the discovery of multiple biased subgroups.
- Evidence anchors:
  - [abstract] "Our approach decomposes the image features into multiple components that represent multiple subgroups. This decomposition is achieved via a bilinear dimension reduction method, Partial Least Square (PLS), guided by useful supervision from the image classifier."
  - [section 4.1] "We integrate the model supervision into the partial least squares (PLS) [1] method to decompose the image features into multiple subgroup directions in the latent space."
- Break condition: If supervision signals become uncorrelated with subgroup membership, or if the classifier has not learned distinct subgroup patterns during training.

### Mechanism 2
- Claim: Cross-modal embeddings enable semantic interpretation of discovered subgroup directions.
- Mechanism: The method uses CLIP's cross-modal embedding space to map both image features and subgroup directions into a shared semantic space, then retrieves images with descriptions that match the subgroup characteristics.
- Core assumption: The cross-modal embedding space preserves semantic relationships between visual and textual representations of subgroups.
- Evidence anchors:
  - [abstract] "We further interpret the semantic meaning of each subgroup component by generating natural language descriptions using vision-language foundation models."
  - [section 4.2] "we leverage the retrieval approach to interpret the discovered subgroup... The ultimate goal is to generate natural language descriptions of biased subgroups."
- Break condition: If the cross-modal embedding space does not preserve subgroup semantics, or if retrieval fails to find relevant examples.

### Mechanism 3
- Claim: Pseudo-subgroup labeling enables targeted bias mitigation through both data-centric and model-centric strategies.
- Mechanism: The method generates pseudo-labels for training data based on discovered subgroup directions, then applies these labels to existing bias mitigation techniques (groupDRO, DI) or uses them to filter additional training data that represents underrepresented subgroups.
- Core assumption: Pseudo-subgroup labels accurately reflect the underlying subgroup structure that affects classifier performance.
- Evidence anchors:
  - [abstract] "Finally, DIM mitigates multiple biased subgroups simultaneously via two strategies, including the data- and model-centric strategies."
  - [section 4.3] "The model-centric strategy is proposed to leverage the discovered subgroups to annotate images in the training set with pseudo-subgroup labels and integrate those labels into existing supervised bias mitigation methods."
- Break condition: If pseudo-labels are noisy or incorrect, mitigation strategies may not effectively address the true biases.

## Foundational Learning

- **Concept: Partial Least Squares (PLS) regression**
  - Why needed here: PLS is used to decompose image features into subgroup-specific components that align with classifier supervision signals, which is more effective than unsupervised methods like PCA for discovering model-learned biases.
  - Quick check question: How does PLS differ from PCA in terms of what it optimizes for during dimensionality reduction?

- **Concept: Cross-modal embeddings**
  - Why needed here: Cross-modal embeddings (like CLIP) provide a shared semantic space where both visual features and textual descriptions can be compared, enabling interpretation of abstract subgroup directions.
  - Quick check question: Why is it beneficial to use a unified embedding space for both images and text when interpreting discovered subgroups?

- **Concept: Group distribution shifts and subgroup robustness**
  - Why needed here: Understanding how models perform differently on subgroups helps frame why discovering and mitigating subgroup biases is important for building robust classifiers.
  - Quick check question: What distinguishes a subgroup bias from a simple class imbalance in classification performance?

## Architecture Onboarding

- **Component map**: CLIP image encoder → PLS decomposition with classifier supervision → Pseudo-label generation → CLIP retrieval + LLM summarization → Data-centric/model-centric mitigation strategies
- **Critical path**: Extract image features → Apply PLS decomposition with supervision → Generate pseudo-labels → Interpret subgroups → Apply mitigation
- **Design tradeoffs**:
  - Using training dynamics as supervision captures learned biases but may miss structural biases the classifier hasn't learned to exploit yet
  - Soft labels provide more nuanced subgroup membership but complicate existing mitigation methods that expect hard labels
  - External data filtering for data-centric mitigation requires access to large datasets but can effectively rebalance underrepresented subgroups
- **Failure signatures**:
  - Low similarity scores between discovered and ground-truth subgroups indicate decomposition isn't capturing meaningful patterns
  - Poor performance improvement after mitigation suggests pseudo-labels don't accurately represent true subgroup structure
  - Inconsistent subgroup interpretations across runs may indicate instability in the decomposition process
- **First 3 experiments**:
  1. Verify PLS decomposition recovers known subgroups on CIFAR-100 with ground truth available
  2. Test different supervision signals (loss vs correctness vs logit) on their ability to guide subgroup discovery
  3. Apply discovered subgroups to a simple mitigation method (like oversampling) to confirm pseudo-labels are useful for bias reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal number of subgroups (G) be determined for each class in the absence of ground truth subgroup information?
- Basis in paper: [inferred] The paper assumes a constant number of subgroups across classes for simplicity but acknowledges this may not reflect real-world scenarios where the number of subgroups can vary.
- Why unresolved: Determining the optimal number of subgroups without ground truth information is a complex problem that requires further research. The paper does not provide a solution for this challenge.
- What evidence would resolve it: A method or framework that can accurately estimate the number of subgroups in each class without relying on ground truth information.

### Open Question 2
- Question: How can the selection of supervision signals be optimized to improve subgroup representation in the latent space decomposition?
- Basis in paper: [explicit] The paper discusses the use of training dynamics (correctness, logit, loss) as supervision but suggests that selecting optimal supervisory signals for enhanced subgroup representation remains an open area for future research.
- Why unresolved: While the paper demonstrates the importance of supervision, it does not explore how to choose the most effective supervisory signals for different scenarios or how to adapt the supervision based on the model's performance.
- What evidence would resolve it: Empirical studies comparing the effectiveness of different supervisory signals on various datasets and tasks, or a theoretical framework for selecting optimal supervision.

### Open Question 3
- Question: How can the proposed DIM method be extended to handle more complex bias scenarios, such as intersectional biases or biases involving multiple attributes?
- Basis in paper: [inferred] The paper focuses on discovering and mitigating single-attribute subgroup biases but does not address more complex bias scenarios that may involve multiple attributes or intersectional biases.
- Why unresolved: The current method is designed for single-attribute subgroup discovery and mitigation. Extending it to handle more complex bias scenarios requires further research and development.
- What evidence would resolve it: A modified version of DIM that can effectively discover and mitigate intersectional biases or biases involving multiple attributes, along with empirical evaluations demonstrating its effectiveness.

## Limitations

- The method relies heavily on the classifier's training dynamics containing sufficient signal to guide PLS decomposition toward meaningful subgroup discovery, which may not capture all types of biases.
- Cross-modal interpretation assumes that CLIP's embedding space preserves the semantic relationships needed to describe discovered subgroups, which may fail for abstract or highly specialized subgroup characteristics.
- Pseudo-label generation introduces uncertainty as noisy or incorrect subgroup assignments could lead to ineffective mitigation strategies, and the quality of these labels can vary significantly across datasets and classifier architectures.

## Confidence

- **High confidence**: The PLS decomposition framework and its integration with classifier supervision is well-specified and theoretically sound.
- **Medium confidence**: The effectiveness of cross-modal embeddings for semantic interpretation is supported by CLIP's general capabilities but hasn't been extensively validated specifically for bias subgroup interpretation.
- **Medium confidence**: The mitigation strategies show promise in experimental results, but their general applicability depends heavily on the quality of pseudo-subgroup labels.

## Next Checks

1. **Validate PLS decomposition stability**: Run DIM on CIFAR-100 with different random seeds and classifier training runs to assess the stability of discovered subgroups. Compare the consistency of subgroup directions and their associated pseudo-labels across runs.

2. **Test supervision signal sensitivity**: Systematically evaluate how different supervision signals (correctness, logit, loss) affect subgroup discovery quality. Measure the correlation between supervision signal choice and the semantic coherence of discovered subgroups.

3. **Benchmark against supervised baselines**: Compare DIM's performance on datasets with ground truth subgroup labels against existing supervised bias mitigation methods. This would establish whether the pseudo-label generation process provides comparable or superior subgroup assignments to expert-labeled alternatives.