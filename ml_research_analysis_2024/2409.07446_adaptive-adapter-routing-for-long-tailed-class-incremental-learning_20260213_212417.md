---
ver: rpa2
title: Adaptive Adapter Routing for Long-Tailed Class-Incremental Learning
arxiv_id: '2409.07446'
source_url: https://arxiv.org/abs/2409.07446
tags:
- uni00000013
- classes
- learning
- pool
- minority
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses long-tailed class-incremental learning (LTCIL),
  where new data arrives in a streaming manner with imbalanced class distributions.
  The proposed method, Apart, leverages pre-trained models and introduces adaptive
  adapter routing to overcome catastrophic forgetting and data imbalance.
---

# Adaptive Adapter Routing for Long-Tailed Class-Incremental Learning

## Quick Facts
- **arXiv ID:** 2409.07446
- **Source URL:** https://arxiv.org/abs/2409.07446
- **Reference count:** 7
- **Primary result:** Apart achieves up to 8-9% higher accuracy in shuffled and ordered LTCIL scenarios compared to state-of-the-art methods

## Executive Summary
This paper introduces Apart, a method for long-tailed class-incremental learning (LTCIL) that addresses catastrophic forgetting and data imbalance through adaptive adapter routing. The approach leverages pre-trained Vision Transformers with trainable adapters at each layer, using multiple adapter pools including an auxiliary pool for minority classes. By learning instance-specific routing weights, Apart dynamically combines these pools to create comprehensive representations of all classes. Extensive experiments on CIFAR100, ImageNet-R, and ObjectNet demonstrate significant improvements over existing methods, achieving superior performance without requiring exemplars.

## Method Summary
Apart uses pre-trained ViT models with inserted adapter modules at each transformer block, freezing most parameters to prevent catastrophic forgetting while enabling deeper adaptation through lightweight adapters. The method employs multiple adapter pools (A and Aaux) with learned instance-specific routing weights that determine how much each pool contributes to the final representation. The auxiliary pool is specifically trained on minority classes to compensate for data imbalance. During training, the model is optimized end-to-end using AdamW with a batch size of 48 for 10 epochs, dynamically selecting adapters based on instance and class information through the routing function.

## Key Results
- Achieves up to 8-9% higher accuracy than state-of-the-art methods on LTCIL benchmarks
- Demonstrates superior performance in both shuffled and ordered incremental learning scenarios
- Outperforms existing methods without relying on exemplar storage, making it suitable for memory-constrained applications
- Shows consistent improvements across CIFAR100, ImageNet-R, and ObjectNet datasets with different imbalance ratios

## Why This Works (Mechanism)

### Mechanism 1
Freezing most parameters of the pre-trained model and using trainable adapters at each layer enables deeper adaptation while preventing catastrophic forgetting. The frozen backbone preserves learned representations while lightweight adapters provide task-specific modifications, with multiple adapter groups allowing retrieval of the most suitable adapter for each instance.

### Mechanism 2
The auxiliary adapter pool specifically trained on minority classes compensates for data imbalance and improves representation of tail classes. By training a separate adapter pool only on minority classes (frequency ≤ θ), the model learns specialized features for underrepresented categories that are then dynamically combined with the main adapter pool via learned routing weights.

### Mechanism 3
Adaptive instance routing automatically learns to combine main and auxiliary adapter pools based on instance and class information, eliminating the need for manual threshold tuning. The routing function w(x,y) takes both instance embedding and class frequency information to produce a weight between 0 and 1, determining how much the auxiliary pool contributes to the final representation for each instance.

## Foundational Learning

- **Vision Transformers (ViT) and pre-training:** Understanding ViT architecture and large-scale pre-training is crucial since Apart relies on ViT as the frozen backbone for feature extraction.
  - *Quick check:* What is the dimension of the [CLS] token embedding in ViT-B/16-IN1K used in the experiments?

- **Adapter modules and parameter-efficient fine-tuning:** The core innovation involves inserting adapter modules at each transformer block, requiring understanding of how adapters work and their implementation for modifying the backbone.
  - *Quick check:* What is the bottleneck dimension (r) used in the adapter modules for the reported experiments?

- **Class-incremental learning and catastrophic forgetting:** Understanding the specific challenge of learning new classes over time without forgetting old ones is important for appreciating Apart's design choices.
  - *Quick check:* What is the main difference between rehearsal-based and rehearsal-free class-incremental learning approaches?

## Architecture Onboarding

- **Component map:** Input → Frozen backbone → Adapter selection → Adapter application → Classifier ensemble → Output
- **Critical path:** The most critical path is the adapter selection and routing, as it determines which adapters are used and how they're combined
- **Design tradeoffs:** Frozen backbone vs. fine-tuning (preserves knowledge vs. limits adaptation), multiple adapter pools vs. single pool (more capacity vs. increased complexity), adaptive routing vs. fixed threshold (flexible vs. requires learning additional network)
- **Failure signatures:** Poor performance on minority classes (inadequate auxiliary pool training or routing), catastrophic forgetting on older tasks (insufficient adapter capacity or poor selection), degraded performance on majority classes (routing weights too biased toward auxiliary pool)
- **First 3 experiments:**
  1. Implement basic adapter insertion with frozen backbone and test on single incremental task to verify adaptation mechanism
  2. Add auxiliary pool with fixed threshold routing and test on long-tailed dataset to verify minority class improvement
  3. Implement adaptive routing and test on same dataset to verify performance improvement over fixed threshold

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- The paper focuses exclusively on ViT architectures without validation on CNNs or other backbone types
- No comparison is made to other adapter-based methods, making it unclear whether improvements come from specific design choices or the adapter paradigm itself
- The paper doesn't report class-level performance breakdowns, making it difficult to assess whether improvements are evenly distributed across majority and minority classes

## Confidence
- **High confidence:** Basic adapter-based approach works better than fine-tuning for CIL; method improves performance on long-tailed data
- **Medium confidence:** Specific contribution of auxiliary pool and adaptive routing (lacks ablation studies isolating these components)
- **Low confidence:** Claim of "8-9% improvement" without clear comparison baselines or statistical significance testing

## Next Checks
1. Run an ablation study comparing Apart with: (a) standard adapter-based CIL, (b) Apart without auxiliary pool, and (c) Apart with fixed threshold routing instead of adaptive routing
2. Analyze the learned routing weights to verify they correlate with class frequencies as intended, and test robustness by introducing class frequency perturbations
3. Evaluate the method's performance when applied to a CNN backbone (e.g., ResNet-50) to test architecture independence