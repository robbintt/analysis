---
ver: rpa2
title: Leveraging Graph Structures to Detect Hallucinations in Large Language Models
arxiv_id: '2407.04485'
source_url: https://arxiv.org/abs/2407.04485
tags:
- graph
- hallucinations
- information
- dataset
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a graph-based method to detect hallucinations
  in large language model (LLM) generations. The core idea is to construct a graph
  where nodes represent sentences and edges connect semantically similar sentences
  in the embedding space.
---

# Leveraging Graph Structures to Detect Hallucinations in Large Language Models

## Quick Facts
- arXiv ID: 2407.04485
- Source URL: https://arxiv.org/abs/2407.04485
- Authors: Noa Nonkes; Sergei Agaronian; Evangelos Kanoulas; Roxana Petcu
- Reference count: 20
- Primary result: Graph-based method achieves 0.8244 recall vs 0.5069 recall for MLP baseline

## Executive Summary
This paper proposes a graph-based approach to detect hallucinations in LLM-generated content by constructing graphs where nodes represent sentences and edges connect semantically similar sentences. A Graph Attention Network (GAT) is trained on this graph structure to distinguish hallucinated from non-hallucinated statements. The method is evaluated on both generated datasets and benchmark datasets (FEVER and SelfCheckGPT), showing improved performance compared to baselines like MLPs and DeBERTa. Notably, the approach achieves competitive results without requiring external knowledge or additional LLM inference passes.

## Method Summary
The method constructs a graph where nodes are sentences and edges connect sentences with cosine similarity above a threshold τ. Sentence embeddings are obtained using BERT, enhanced through contrastive learning to improve discriminability, and dimensionality reduced before graph construction. A GAT is trained to classify sentences as hallucinated or non-hallucinated by leveraging the graph structure. The approach uses edge masking during training to prevent information leakage between data splits and relies on the assumption that hallucinated and non-hallucinated sentences form distinct structural patterns in the embedding space.

## Key Results
- GAT achieves 0.8244 recall vs 0.5069 recall for MLP baseline on training set with contrastive learning
- On SelfCheckGPT benchmark: 0.7412 recall vs 0.7166 for DeBERTa baseline
- Method performs competitively without requiring external knowledge or additional LLM inference passes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM hallucinations share structural patterns in the latent embedding space that can be captured by graph-based models.
- Mechanism: By constructing a graph where nodes represent sentences and edges connect semantically similar sentences, GATs can aggregate neighborhood information and learn discriminative patterns between hallucinated and non-hallucinated generations.
- Core assumption: Sentences with similar degrees of factuality cluster together in the embedding space, forming homophilic structures.
- Evidence anchors:
  - [abstract] "there exists a structure in the latent space that differentiates between hallucinated and non-hallucinated generations"
  - [section 3.1] "we can form a graph where only similar nodes will have a direct connection between them"
  - [corpus] Weak - corpus contains papers on graph-based hallucination detection but no direct evidence supporting latent space clustering claim
- Break condition: If semantic embeddings fail to capture factuality distinctions, or if hallucinated and true statements are uniformly distributed in embedding space.

### Mechanism 2
- Claim: Contrastive learning enhances the discriminative power of sentence embeddings for hallucination detection.
- Mechanism: CL training forces the model to learn embeddings where hallucinated and true statements are pushed apart in the embedding space, making graph structure more meaningful.
- Core assumption: Standard BERT embeddings contain contextual/semantic information but not sufficient discriminative information for factuality classification.
- Evidence anchors:
  - [section 5.2] "Initial experiments revealed that BERT embeddings are not discriminative enough for our task"
  - [section 5.2] "CL + GAT 0.8244 recall vs GAT 0.5069 recall (train set)"
  - [corpus] Weak - corpus mentions contrastive learning but not specifically for hallucination detection
- Break condition: If CL fails to improve embedding discriminability or if the learned embeddings become too specialized to training data.

### Mechanism 3
- Claim: Non-local aggregation through GAT message passing captures information beyond immediate local neighborhoods.
- Mechanism: GAT's attention mechanism assigns varying importance to neighbors based on their relevance, allowing information to propagate through the graph and capture global patterns.
- Core assumption: Local semantic similarity is insufficient; non-local structural relationships between sentences provide additional discriminative information.
- Evidence anchors:
  - [abstract] "non-local aggregation enhances these links"
  - [section 5.3] "GAT still trailed the performance of GAT, with approximately a 20% decrease in validation recall"
  - [corpus] Weak - corpus contains graph-based approaches but no direct evidence on non-local aggregation benefits
- Break condition: If attention weights become uniform or if graph connectivity prevents meaningful information propagation.

## Foundational Learning

- Concept: Graph Attention Networks (GATs)
  - Why needed here: GATs enable weighted message passing between semantically similar sentences, learning to identify hallucinated content through neighborhood patterns.
  - Quick check question: How does GAT's attention mechanism differ from standard graph convolution?

- Concept: Contrastive Learning
  - Why needed here: Standard embeddings lack discriminative power for factuality detection; CL forces the model to learn separable representations.
  - Quick check question: What is the key difference between supervised and unsupervised contrastive learning?

- Concept: Homophily in graphs
  - Why needed here: The assumption that similar sentences (in factuality) connect in the graph is fundamental to the approach.
  - Quick check question: How would heterophily (dissimilar nodes connecting) affect this method's performance?

## Architecture Onboarding

- Component map: BERT embedding layer → Contrastive learning MLP → Graph construction (cosine similarity thresholding) → GAT model → Classification layer

- Critical path:
  1. Generate embeddings for all sentences
  2. Construct graph with similarity threshold
  3. Train contrastive MLP to enhance embeddings
  4. Apply dimensionality reduction
  5. Train GAT with masked edges for training/validation separation
  6. Evaluate on test set

- Design tradeoffs:
  - Embedding threshold τ: Higher values create sparse graphs (better runtime) but may lose important connections
  - Contrastive batch size: Larger batches provide better negative sampling but increase memory usage
  - GAT layers: More layers capture longer-range dependencies but risk oversmoothing

- Failure signatures:
  - Low recall: Graph structure not capturing hallucinated patterns, or embeddings not discriminative
  - Low precision: Overfitting to training data, or edge masking not preventing information leakage
  - Random performance: CL not improving embeddings, or graph too sparse/too dense

- First 3 experiments:
  1. Baseline GAT without CL on controlled dataset (check if graph structure alone works)
  2. kNN classification on CL-enhanced embeddings (isolate embedding quality from graph benefits)
  3. GAT with varying similarity thresholds (find optimal graph connectivity)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method's performance compare to state-of-the-art retrieval-based methods on benchmark datasets like FEVER?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that their method performs similarly to evidence-based benchmarks without access to search-based methods, but does not provide a detailed comparison of performance metrics against specific state-of-the-art retrieval-based methods.
- What evidence would resolve it: A comprehensive comparison of the proposed method's performance metrics (e.g., recall, precision, AUC-PR) against those of leading retrieval-based methods on benchmark datasets like FEVER, using the same evaluation criteria.

### Open Question 2
- Question: How does the method scale with increasing graph size and complexity, particularly in terms of computational efficiency and memory usage?
- Basis in paper: Inferred
- Why unresolved: The paper acknowledges that the method is difficult to scale due to the exponential number of embedding comparisons required for adding edges in the graph, but does not provide empirical data on the method's performance as the graph size and complexity increase.
- What evidence would resolve it: Empirical results showing the method's computational time and memory usage as the number of nodes and edges in the graph increases, along with comparisons to alternative approaches for handling large-scale graph data.

### Open Question 3
- Question: How does the choice of graph construction parameters, such as the similarity threshold (τ) and the number of nearest neighbors, affect the method's performance?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that the choice of the similarity threshold (τ) is important for balancing graph connectivity and that the value of τ is dependent on the dataset, but does not provide a systematic analysis of how different parameter choices impact the method's performance.
- What evidence would resolve it: A sensitivity analysis of the method's performance across a range of similarity threshold values and numbers of nearest neighbors, with visualizations of the resulting graph structures and their effects on classification accuracy.

### Open Question 4
- Question: Can the method be extended to handle multi-class classification problems beyond the binary classification of hallucinated vs. non-hallucinated statements?
- Basis in paper: Inferred
- Why unresolved: The paper demonstrates the method's effectiveness for ordinal regression on a dataset with four classes of statements (hallucinated, true without context, true with context, and true statement), but does not explore its applicability to other multi-class classification problems.
- What evidence would resolve it: Empirical results showing the method's performance on datasets with different numbers of classes and varying levels of class imbalance, along with comparisons to other multi-class classification approaches.

### Open Question 5
- Question: How does the method's performance compare when using different sentence embedding models, such as RoBERTa or Sentence-BERT, instead of BERT?
- Basis in paper: Inferred
- Why unresolved: The paper uses BERT for sentence embeddings, but does not explore the impact of using alternative embedding models on the method's performance.
- What evidence would resolve it: Empirical results comparing the method's performance when using different sentence embedding models, with analyses of the effects on graph construction, message passing, and classification accuracy.

## Limitations

- The approach relies heavily on the assumption that hallucinated and non-hallucinated sentences form distinct structural patterns in the embedding space, which may not hold for all domains or generation tasks.
- The contrastive learning component requires careful tuning and additional computational overhead, with uncertainty about whether the specific implementation is optimal or necessary.
- The method's generalizability claim is limited by experiments using curated datasets with ordinal hallucination labels that may not reflect real-world deployment scenarios.

## Confidence

- High confidence: The experimental results showing GAT outperforms MLP baselines (0.6756 vs 0.5069 recall on validation set) are robust and well-documented.
- Medium confidence: The claim that graph structures provide unique value beyond just using embeddings directly, as the difference could partially stem from implementation details.
- Low confidence: The generalizability claim that this approach works "without relying on additional LLM inference passes or external knowledge" given the curated dataset nature.

## Next Checks

1. **Cross-domain robustness test**: Evaluate the method on a dataset from a different domain (e.g., medical vs general knowledge) to assess whether the structural patterns generalize beyond the original training distribution.

2. **Ablation on graph connectivity**: Systematically vary the similarity threshold τ and measure the tradeoff between graph density and classification performance to identify optimal graph construction parameters.

3. **Real-world deployment simulation**: Test the approach on LLM generations without ground truth labels, using only ordinal judgments from human raters to assess practical utility in deployed systems.