---
ver: rpa2
title: 'Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model'
arxiv_id: '2404.04167'
source_url: https://arxiv.org/abs/2404.04167
tags:
- chinese
- language
- data
- arxiv
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces CT-LLM, a 2-billion parameter Chinese-centric
  large language model trained from scratch on a 1,200-billion-token corpus with 800
  billion Chinese tokens, 300 billion English tokens, and 100 billion code tokens.
  Unlike prior models that adapt English-trained LLMs to Chinese, CT-LLM is pre-trained
  primarily on Chinese data, achieving superior Chinese language understanding while
  maintaining strong English proficiency through supervised fine-tuning.
---

# Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model

## Quick Facts
- arXiv ID: 2404.04167
- Source URL: https://arxiv.org/abs/2404.04167
- Reference count: 40
- Primary result: Introduces CT-LLM, a 2B parameter Chinese-centric LLM achieving superior Chinese language understanding while maintaining strong English proficiency through preference optimization

## Executive Summary
This paper presents CT-LLM, a 2-billion parameter Chinese-centric large language model trained from scratch on a 1,200-billion-token corpus with a Chinese-heavy composition (800B Chinese, 300B English, 100B code tokens). Unlike existing approaches that adapt English-trained models to Chinese, CT-LLM is pre-trained primarily on Chinese data, demonstrating exceptional performance in Chinese language tasks through a newly developed Chinese Hard Case Benchmark (CHC-Bench). The model is aligned using preference optimization techniques and introduces a high-quality Chinese pretraining dataset (MAP-CC). This work challenges the prevailing English-centric LLM training paradigm and contributes to more inclusive language model development.

## Method Summary
CT-LLM was pre-trained from scratch using a 1,200-billion-token corpus with 800 billion Chinese tokens, 300 billion English tokens, and 100 billion code tokens. The training approach differs from existing methods by prioritizing Chinese data during pretraining rather than adapting English-trained models. The model architecture follows standard transformer design with 2 billion parameters. After pretraining, CT-LLM underwent supervised fine-tuning to maintain English proficiency while enhancing Chinese capabilities. The model was aligned using preference optimization techniques, though specific implementation details are limited in the paper. The researchers developed a Chinese Hard Case Benchmark (CHC-Bench) for evaluation, focusing on challenging Chinese language understanding tasks.

## Key Results
- CT-LLM achieves superior Chinese language understanding while maintaining strong English proficiency on the newly developed CHC-Bench
- The model demonstrates exceptional performance in Chinese language tasks compared to baseline approaches
- Introduces MAP-CC, a high-quality Chinese pretraining dataset that addresses limitations of existing Chinese corpora
- Challenges the prevailing English-centric LLM training paradigm by demonstrating the effectiveness of Chinese-first pretraining

## Why This Works (Mechanism)
The success of CT-LLM stems from its fundamental shift in pretraining strategy - instead of adapting an English-centric model to Chinese, the model is trained primarily on Chinese data from scratch. This approach allows the model to develop deeper understanding of Chinese linguistic patterns, idiomatic expressions, and cultural context that may be lost when training predominantly on English data. The 800 billion Chinese tokens provide sufficient exposure to capture the nuances of Chinese language structure, while the 300 billion English tokens and 100 billion code tokens maintain the model's cross-lingual capabilities and programming knowledge. The preference optimization alignment further refines the model's ability to handle complex Chinese language tasks while maintaining balanced bilingual performance.

## Foundational Learning
- **Transformer Architecture**: Why needed - Forms the backbone of modern LLMs; quick check - Verify attention mechanisms and feed-forward layers function correctly
- **Tokenization for Chinese**: Why needed - Chinese lacks spaces between words, requiring specialized tokenization; quick check - Ensure subword units capture semantic meaning
- **Multilingual Training**: Why needed - Balancing Chinese and English performance; quick check - Monitor cross-entropy loss for both languages during training
- **Preference Optimization**: Why needed - Aligns model outputs with human preferences; quick check - Verify reward model provides meaningful gradients
- **Dataset Curation**: Why needed - Quality Chinese data is scarce compared to English; quick check - Validate data quality through perplexity measurements

## Architecture Onboarding

**Component Map:**
Input Tokens -> Embedding Layer -> Transformer Blocks -> Output Layer -> Preference Optimization Alignment

**Critical Path:**
The critical path is the transformer stack where most computation occurs. Each layer consists of multi-head self-attention followed by position-wise feed-forward networks. The attention mechanism is particularly crucial as it enables the model to capture long-range dependencies in Chinese text, which often requires understanding context across sentence boundaries.

**Design Tradeoffs:**
The primary tradeoff was dataset composition: 800B Chinese vs 300B English vs 100B code tokens. This heavy Chinese bias was chosen to maximize Chinese understanding but risks degrading English and code capabilities. The authors mitigated this through supervised fine-tuning and preference optimization. Another tradeoff was model size (2B parameters) - large enough for quality but small enough to be practical for pretraining and deployment.

**Failure Signatures:**
- Degraded English performance would manifest as increased perplexity on English benchmarks
- Poor Chinese understanding would show in inability to handle idioms, context-dependent meanings, or complex syntactic structures
- Alignment failures would result in outputs that don't match human preferences despite technical correctness
- Training instability could occur if the Chinese-to-English token ratio creates distribution shift

**First Experiments:**
1. Evaluate baseline perplexity on Chinese and English validation sets to establish pretraining quality
2. Test model on simple Chinese tasks (sentiment analysis, named entity recognition) before complex benchmarks
3. Conduct bilingual inference to verify the model maintains balanced performance across languages

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the research raises several implicit questions about scaling this approach to larger models, the optimal Chinese-to-English token ratio for different model sizes, and how this pretraining strategy generalizes to other non-English languages with different linguistic structures.

## Limitations
- Lack of direct comparisons with established Chinese-centric models like GLM-4 or InternLM makes performance claims difficult to verify
- Evaluation is limited to the newly developed CHC-Bench without benchmarking against established evaluation suites like C-Eval or CMMLU
- Limited details on preference optimization implementation reduce reproducibility of the alignment process
- The specific Chinese-to-English token ratio (800B:300B) may not be optimal and requires further investigation

## Confidence

**High:**
- The technical approach of Chinese-first pretraining is sound and theoretically justified

**Medium:**
- Chinese language performance claims based on CHC-Bench results
- English proficiency maintenance after Chinese-heavy pretraining
- Dataset quality assertions for MAP-CC

**Low:**
- Significance of paradigm shift compared to existing approaches
- Generalization of results to other non-English languages
- Long-term stability of bilingual balance

## Next Checks

1. Replicate the training setup on a smaller scale (e.g., 200M parameters) to verify dataset quality and training methodology before scaling to 2B parameters

2. Conduct head-to-head comparisons with established Chinese LLMs (GLM-4, InternLM) on both CHC-Bench and established benchmarks (C-Eval, CMMLU)

3. Perform ablation studies varying the Chinese-to-English token ratio to determine the optimal balance for bilingual performance and test the sensitivity of the results to dataset composition