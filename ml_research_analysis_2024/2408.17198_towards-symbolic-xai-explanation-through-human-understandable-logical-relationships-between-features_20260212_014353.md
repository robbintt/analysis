---
ver: rpa2
title: Towards Symbolic XAI -- Explanation Through Human Understandable Logical Relationships
  Between Features
arxiv_id: '2408.17198'
source_url: https://arxiv.org/abs/2408.17198
tags:
- relevance
- prediction
- features
- queries
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Symbolic XAI, a framework that attributes
  model relevance to logical formulas expressing feature relationships, rather than
  individual features. The method decomposes model predictions into multi-order terms
  using either propagation-based (e.g., GNN-LRP) or perturbation-based approaches,
  then computes query relevance by summing terms where logical formulas hold true.
---

# Towards Symbolic XAI -- Explanation Through Human Understandable Logical Relationships Between Features

## Quick Facts
- arXiv ID: 2408.17198
- Source URL: https://arxiv.org/abs/2408.17198
- Reference count: 40
- Key outcome: Introduces Symbolic XAI framework that attributes model relevance to logical formulas expressing feature relationships, enabling human-understandable explanations across NLP, vision, and quantum chemistry domains

## Executive Summary
This paper presents Symbolic XAI, a novel framework for explaining machine learning model predictions through human-understandable logical relationships between features rather than individual feature attributions. The method decomposes model predictions into multi-order terms using either propagation-based (e.g., GNN-LRP) or perturbation-based approaches, then computes query relevance by summing terms where logical formulas hold true. A search algorithm identifies the most expressive queries describing model decision strategies, enabling explanations that capture abstract reasoning patterns. The framework demonstrates effectiveness across diverse domains including sentiment analysis, facial expression recognition, and molecular dynamics prediction.

## Method Summary
The Symbolic XAI framework operates by first decomposing model predictions into multi-order relevance terms using either propagation-based methods (like GNN-LRP) or perturbation-based approximations. These terms represent relevance associated with logical relationships between features. The framework then computes query relevance by summing all terms where a given logical formula holds true for the input features. To generate explanations, a search algorithm incrementally builds logical queries starting from simple ones and adding features based on their contribution to explanation quality. The method can be applied to both model-based (propagation) and model-agnostic (perturbation) decompositions, providing flexibility in implementation while maintaining interpretability through logical formulas.

## Key Results
- Demonstrates ability to capture abstract reasoning patterns across NLP (sentiment analysis), vision (facial expression recognition), and quantum chemistry (molecular dynamics) domains
- Shows superior performance in input flipping tasks compared to traditional feature-based explanation methods
- Enables human-understandable explanations through logical formulas while maintaining flexibility for user customization
- Successfully identifies complex feature relationships that traditional methods miss, such as "head and shoulders should be upright" in facial expression recognition

## Why This Works (Mechanism)
The framework works by shifting from individual feature attribution to relationship-based attribution through logical formulas. By decomposing predictions into multi-order terms that capture interactions between features, the method can represent complex decision strategies that single-feature approaches cannot capture. The query relevance computation aggregates these multi-order terms where logical relationships hold, effectively attributing importance to patterns rather than isolated features. This approach aligns with how humans naturally reason about concepts - through relationships and patterns rather than individual components. The search algorithm's incremental approach ensures that explanations remain both interpretable and representative of the model's actual decision strategy.

## Foundational Learning

**Logical Formula Space Construction**
- Why needed: Provides the vocabulary for expressing feature relationships that the model uses for decisions
- Quick check: Verify that the formula space can represent all relevant patterns in the target domain through systematic coverage analysis

**Relevance Decomposition Methods**
- Why needed: Converts model predictions into interpretable components that can be associated with logical relationships
- Quick check: Compare propagation vs. perturbation results on simple models where ground truth is known

**Query Relevance Computation**
- Why needed: Enables aggregation of multi-order terms into meaningful explanations based on logical relationships
- Quick check: Validate that relevance sums to original prediction through numerical verification

**Greedy Search Algorithm**
- Why needed: Finds optimal explanations within the exponential search space of possible logical formulas
- Quick check: Test search completeness on synthetic problems with known optimal solutions

## Architecture Onboarding

**Component Map**
Input Features -> Decomposition Module -> Relevance Terms -> Query Relevance Engine -> Search Algorithm -> Logical Explanations

**Critical Path**
Feature extraction → Decomposition (propagation or perturbation) → Query relevance computation → Search algorithm → Explanation output

**Design Tradeoffs**
- Propagation-based decomposition offers exact results but requires model-specific implementations
- Perturbation-based decomposition is model-agnostic but introduces approximation errors
- Search algorithm complexity grows exponentially with formula complexity
- Balance between explanation simplicity and completeness

**Failure Signatures**
- Explanations that are too simple may miss important interactions
- Overly complex explanations may become uninterpretable
- Search algorithm may get stuck in local optima
- Relevance decomposition may fail to capture all relevant patterns

**3 First Experiments**
1. Apply framework to a simple linear model where ground truth explanations are known
2. Compare propagation vs. perturbation decomposition on a small neural network
3. Test search algorithm on synthetic data with controlled feature relationships

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity of searching through logical formula space grows exponentially with feature count and relationship complexity
- Perturbation-based decomposition relies on approximations that may introduce bias or inaccuracies in relevance attribution
- Greedy search algorithm may miss globally optimal explanations, especially when complex interactions are necessary
- Framework effectiveness depends on quality and expressiveness of logical formula space for the specific domain

## Confidence

**High confidence**: The general framework design and decomposition approach are mathematically sound and well-defined
**Medium confidence**: Empirical results across multiple domains show promise, though relative performance gains need further validation
**Medium confidence**: Search algorithm effectiveness in finding meaningful explanations, as greedy approach may have limitations

## Next Checks

1. Benchmark framework performance on larger, more complex datasets to evaluate scalability and computational efficiency
2. Conduct ablation studies to quantify impact of different decomposition methods (propagation vs. perturbation) on explanation quality
3. Perform user studies to assess whether logical formulas generated by framework actually improve human understanding compared to traditional feature-based explanations