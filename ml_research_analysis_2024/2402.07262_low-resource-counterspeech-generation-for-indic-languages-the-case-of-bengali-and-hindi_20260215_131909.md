---
ver: rpa2
title: 'Low-Resource Counterspeech Generation for Indic Languages: The Case of Bengali
  and Hindi'
arxiv_id: '2402.07262'
source_url: https://arxiv.org/abs/2402.07262
tags: []
core_contribution: This paper introduces a benchmark dataset of 5,062 abusive speech/counterspeech
  pairs in Bengali and Hindi, addressing the gap in low-resource languages for counterspeech
  generation. The authors implement several baseline models using various interlingual
  transfer mechanisms, including monolingual, joint training, and synthetic transfer.
---

# Low-Resource Counterspeech Generation for Indic Languages: The Case of Bengali and Hindi

## Quick Facts
- arXiv ID: 2402.07262
- Source URL: https://arxiv.org/abs/2402.07262
- Authors: Mithun Das; Saurabh Kumar Pandey; Shivansh Sethi; Punyajoy Saha; Animesh Mukherjee
- Reference count: 11
- Primary result: Introduces 5,062 abusive speech/counterspeech pairs for Bengali and Hindi; monolingual training outperforms transfer learning approaches

## Executive Summary
This paper addresses the challenge of counterspeech generation in low-resource Indic languages by introducing a benchmark dataset of 5,062 abusive speech/counterspeech pairs in Bengali and Hindi. The authors implement and compare several baseline models using monolingual, joint, and synthetic transfer learning approaches. Their experiments reveal that monolingual training yields the best performance across all setups, with language-specific models (BanglaT5 for Bengali, mT5-base for Hindi) outperforming general-purpose models. The study highlights the importance of developing language-specific datasets and models for effective counterspeech generation in resource-constrained settings.

## Method Summary
The authors collected 5,062 abusive speech/counterspeech pairs (2,460 Bengali, 2,602 Hindi) and implemented baseline models using GPT2 variants, mT5, BLOOM, and ChatGPT. They evaluated three training setups: monolingual (fine-tuning on target language only), joint (training on both languages simultaneously), and synthetic transfer (using translated English data). Models were fine-tuned using Adafactor optimizer with learning rate 2e-5 for up to 50 epochs with early stopping. Performance was assessed using automatic metrics (BLEU, METEOR, ROUGE, BERTScore, diversity, novelty, abusiveness) and human evaluation (suitability, specificity, grammaticality, choose-or-not).

## Key Results
- Monolingual training consistently outperforms joint and synthetic transfer approaches across all metrics
- BanglaT5 achieves the best performance for Bengali while mT5-base performs best for Hindi
- Synthetic transfer shows better effectiveness when transferring between languages of the same family (Bengali-Hindi) versus cross-family transfer (English-Hindi/Bengali)
- Few-shot fine-tuning with gold data after synthetic transfer provides steady improvements across metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monolingual training outperforms joint and synthetic transfer because the model learns language-specific nuances of abusive speech and counterspeech patterns.
- Mechanism: Fine-tuning a language model exclusively on gold-labeled data from the target language allows it to capture culturally and linguistically specific contexts, idioms, and abusive speech patterns unique to that language.
- Core assumption: The quality and representativeness of the monolingual dataset is sufficient to capture all relevant linguistic features.
- Evidence anchors:
  - [abstract]: "we observe that the monolingual setup yields the best performance"
  - [section]: "In Table 2, we report the performance in the monolingual setting. We observe that – For the Bengali language, BanglaT5 model performs the best across all the overlapping metrics"
  - [corpus]: Weak – no direct neighbor paper provides evidence; assumption based on internal results.
- Break condition: If the monolingual dataset is too small or lacks diversity, the model may overfit or miss important abusive speech patterns, reducing effectiveness.

### Mechanism 2
- Claim: Synthetic transfer is more effective between languages of the same family due to shared linguistic structures.
- Mechanism: Translating English (or another related low-resource language) into the target language and using it as silver data allows the model to learn general counterspeech patterns, which can be fine-tuned further with gold data.
- Core assumption: Languages in the same family share enough grammatical and semantic structure for effective knowledge transfer.
- Evidence anchors:
  - [abstract]: "specifically, we notice that transferability is better when languages belong to the same language family"
  - [section]: "Table 5 reveals that for the Bengali test set, the models trained with HI → BN translated synthetic dataset achieve better scores compared to the EN → BN translated synthetic dataset"
  - [corpus]: Weak – no direct neighbor paper provides evidence; assumption based on internal results.
- Break condition: If the source and target languages differ significantly in syntax or cultural context, translation artifacts may confuse the model, leading to poor performance.

### Mechanism 3
- Claim: Few-shot fine-tuning on gold data after synthetic transfer improves performance by correcting translation errors and aligning with target language norms.
- Mechanism: Adding a small set of gold AS-CS pairs to a model trained on synthetic data allows it to adapt to the nuances of the target language and correct any misalignment from the synthetic phase.
- Core assumption: The gold data added in the few-shot phase is representative and diverse enough to guide the model effectively.
- Evidence anchors:
  - [abstract]: "using synthetic transfer, language models can generate counterspeech to some extent; specifically, we notice that transferability is better when languages belong to the same language family"
  - [section]: "Table 7 shows the few-shot performance of the synthetic transfer where we add the actual gold AS-CS pairs to fine-tune the models further. Overall we observe adding gold AS-CS gives steady improvements in terms of different overlapping metrics."
  - [corpus]: Weak – no direct neighbor paper provides evidence; assumption based on internal results.
- Break condition: If the few-shot gold data is too small or biased, it may not sufficiently correct errors introduced during synthetic transfer, limiting improvement.

## Foundational Learning

- Concept: Interlingual transfer mechanisms
  - Why needed here: The paper compares monolingual, joint, and synthetic transfer setups to understand how knowledge can be transferred across languages for counterspeech generation.
  - Quick check question: What is the main difference between joint training and synthetic transfer in this context?

- Concept: Dataset quality and diversity
  - Why needed here: The performance of all models depends heavily on the quality, size, and diversity of the AS-CS pairs in each language.
  - Quick check question: How does the diversity score in Table 1 help assess the dataset quality?

- Concept: Evaluation metrics for text generation
  - Why needed here: BLEU, METEOR, ROUGE, BERTScore, and human evaluation metrics are used to assess the quality of generated counterspeech.
  - Quick check question: Why might BLEU scores be lower for low-resource languages compared to high-resource languages?

## Architecture Onboarding

- Component map: Data collection → Annotation → Preprocessing (regex cleaning, length limiting) → Train/validation/test split → Model fine-tuning → Generation → Automatic evaluation → Human evaluation
- Critical path: Load and preprocess dataset → Fine-tune selected model on training set → Generate counterspeech on test set → Evaluate using automatic and human metrics
- Design tradeoffs:
  - Monolingual: Best performance but requires large gold datasets per language
  - Joint: Generalizable but may lose language-specific nuances
  - Synthetic transfer: Cost-effective but depends on translation quality and relatedness of languages
- Failure signatures:
  - Low BLEU/ROUGE but high BERTScore: Model generates fluent but off-topic text
  - High diversity/novelty but low suitability: Model generates diverse but irrelevant responses
  - High abusiveness score: Model fails to neutralize tone in counterspeech
- First 3 experiments:
  1. Fine-tune mT5-base on monolingual Bengali data, evaluate on test set
  2. Fine-tune mT5-base on joint Bengali+Hindi data, evaluate on both test sets
  3. Translate English CONAN dataset to Bengali, fine-tune mT5-base (zero-shot), evaluate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of counterspeech generation models differ when using knowledge bases or external facts compared to models trained without such resources?
- Basis in paper: [inferred] The authors mention plans to explore methods for improving specificity by using various types of knowledge (e.g., facts, events, and named entities) from external resources in future work.
- Why unresolved: The paper focuses on evaluating baseline models without incorporating external knowledge, leaving the impact of knowledge bases unexplored.
- What evidence would resolve it: Comparative experiments between models with and without external knowledge integration, measuring metrics like BLEU, METEOR, and BERTScore.

### Open Question 2
- Question: To what extent does the cultural context of a language family influence the effectiveness of synthetic transfer in counterspeech generation?
- Basis in paper: [explicit] The authors observe that synthetic transfer works better between Bengali and Hindi, attributing this to their membership in the Indo-Aryan language family and socio-linguistic dissimilarity of English from Hindi and Bengali.
- Why unresolved: The study only examines two closely related languages, and the underlying mechanisms of cultural influence on transferability remain unexplored.
- What evidence would resolve it: Experiments involving languages from different families, measuring performance variations and identifying cultural factors that enhance or hinder transfer.

### Open Question 3
- Question: How does the length of generated counterspeech impact its effectiveness in mitigating abusive speech?
- Basis in paper: [inferred] The authors note that BLOOM and mT5-based models exhibit lower average lengths of counterspeech, making them more suitable for mitigating abusive speech. ChatGPT-generated CSs tend to be lengthy, requiring more edits during post-editing.
- Why unresolved: The study does not directly measure the impact of counterspeech length on its effectiveness, leaving the optimal length for counterspeech generation unexplored.
- What evidence would resolve it: Experiments varying the maximum length of generated counterspeech and evaluating its impact on human judgment metrics like suitability, specificity, and choose-or-not scores.

## Limitations
- Results may not generalize beyond Bengali and Hindi to other low-resource Indic languages with different typological features
- Limited investigation of how dataset size variations affect performance differences between monolingual and transfer learning approaches
- Human evaluation reliability is uncertain without reported inter-annotator agreement scores or detailed annotator background information

## Confidence
**High Confidence**: Monolingual training consistently outperforms joint and synthetic transfer across multiple metrics (BLEU, METEOR, ROUGE, BERTScore) for both Bengali and Hindi.

**Medium Confidence**: Synthetic transfer effectiveness within language families is supported by internal comparisons but lacks external validation against cross-family transfers.

**Low Confidence**: The specific contribution of few-shot fine-tuning after synthetic transfer is asserted but not isolated through ablation studies to determine its independent effect.

## Next Checks
1. **Cross-Family Transfer Experiment**: Test synthetic transfer from English to a Dravidian language (e.g., Tamil) versus an Indo-Aryan language (e.g., Hindi) to empirically validate whether language family membership affects transfer effectiveness.

2. **Dataset Size Sensitivity Analysis**: Systematically vary the size of monolingual gold data (e.g., 10%, 25%, 50%, 100%) to determine the minimum effective dataset size for monolingual training and compare against transfer learning approaches.

3. **Inter-Annotator Agreement Assessment**: Conduct a formal IAA calculation (e.g., Krippendorff's alpha or Cohen's kappa) on the human evaluation data to establish the reliability of human judgments and report the demographic and linguistic background of annotators.