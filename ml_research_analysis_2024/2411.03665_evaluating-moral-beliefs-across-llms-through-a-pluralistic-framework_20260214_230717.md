---
ver: rpa2
title: Evaluating Moral Beliefs across LLMs through a Pluralistic Framework
arxiv_id: '2411.03665'
source_url: https://arxiv.org/abs/2411.03665
tags:
- moral
- option
- choices
- debate
- choice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a three-module framework to evaluate moral
  beliefs in large language models (LLMs). It constructs a dataset of 472 Chinese
  moral scenarios derived from moral words and analyzes the decision-making of four
  prominent LLMs.
---

# Evaluating Moral Beliefs across LLMs through a Pluralistic Framework

## Quick Facts
- arXiv ID: 2411.03665
- Source URL: https://arxiv.org/abs/2411.03665
- Reference count: 40
- Four LLMs evaluated on 472 Chinese moral scenarios show cultural differences in moral reasoning

## Executive Summary
This study introduces a three-module framework to evaluate moral beliefs in large language models (LLMs). It constructs a dataset of 472 Chinese moral scenarios derived from moral words and analyzes the decision-making of four prominent LLMs. Results show that English models (ChatGPT and Gemini) closely mirror Chinese university students' moral decisions, demonstrate strong adherence to their choices, and prefer individualistic moral beliefs. In contrast, Chinese models (Ernie and ChatGLM) lean toward collectivist beliefs, exhibit ambiguity in moral choices, and show gender bias in their ethical prioritization. The study highlights cultural and demographic influences on LLM moral reasoning and proposes an innovative approach for assessing moral beliefs in AI systems.

## Method Summary
The study constructs a dataset of 472 Chinese moral scenarios from 514 negative moral words and their associated moral principles. Four LLMs (ChatGLM2-6B-32K, Ernie-Bot-turbo, Gemini Pro, GPT-3.5-turbo-16K) are evaluated through a three-module framework: moral choice collection, moral ranking using Best-Worst Scaling and Iterative Luce Spectral Ranking, and moral debate to assess choice firmness. Models make pairwise moral decisions, which are aggregated into ranked moral principle preferences, followed by debates between models to measure stability of moral convictions.

## Key Results
- English models (ChatGPT, Gemini) closely mirror Chinese university students' moral decisions and demonstrate strong adherence to choices
- Chinese models (Ernie, ChatGLM) favor collectivist moral beliefs and show less definitive choices
- Significant gender bias exists in moral reasoning, with Ernie being the most gender-neutral model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework maps moral beliefs to discrete choice patterns via scenario-based scoring.
- Mechanism: For each scenario, the model's choice is mapped to a pairwise preference between moral principles. These pairwise comparisons are then aggregated using Best-Worst Scaling and Iterative Luce Spectral Ranking to produce a ranked ordering of moral principles.
- Core assumption: Models express stable, comparable preferences for moral principles when confronted with scenario pairs.
- Evidence anchors:
  - [abstract] "By ranking these moral choices, we discern the varying moral beliefs held by different language models."
  - [section] "Following Best-Worst Scaling (BWS) (Louviere et al., 2015) and Iterative Luce Spectral Ranking (ILSR) (Maystre and Grossglauser, 2015), we obtained the pairs of options, their corresponding scores, and the overall ranking and corresponding weights based on the pairs of options."
- Break condition: If the model's choices become inconsistent across similar scenarios or if scenario design induces random choice, the ranking collapses.

### Mechanism 2
- Claim: Moral debate serves as a probe for the stability of moral choices.
- Mechanism: A second model challenges the first model's choice by arguing for the opposite option. The first model must defend its original choice, and afterward, its post-debate firmness score is measured. Changes in firmness score and choice reveal susceptibility to counter-arguments.
- Core assumption: Models can generate coherent counter-arguments and self-assess firmness in response to moral challenges.
- Evidence anchors:
  - [abstract] "Additionally, through moral debates, we investigate the firmness of these models to their moral choices."
  - [section] "Using prompts similar to those employed in prior research (Du et al., 2023), we paired the four models and initiated debates."
- Break condition: If models ignore the opponent's arguments or generate inconsistent responses across rounds, the probe loses diagnostic power.

### Mechanism 3
- Claim: Cultural background of training data influences moral belief structure.
- Mechanism: English models (ChatGPT, Gemini) trained on Western corpora favor individualistic principles, while Chinese models (ChatGLM, Ernie) trained on Chinese corpora favor collectivist principles. This is observed via differences in moral rankings and firmness distributions.
- Core assumption: The training corpus's cultural content leaves a measurable imprint on moral reasoning outputs.
- Evidence anchors:
  - [abstract] "Our findings indicate that English language models, namely ChatGPT and Gemini, closely mirror moral decisions of the sample of Chinese university students, demonstrating strong adherence to their choices and a preference for individualistic moral beliefs. In contrast, Chinese models such as Ernie and ChatGLM lean towards collectivist moral beliefs..."
  - [section] "This discrepancy may stem from the influence of cultural corpora during model training. Chinese culture tends to emphasize moderation and dialectics, leading to less definitive choices when faced with moral scenarios."
- Break condition: If cross-cultural variation is confounded by model size, architecture, or fine-tuning strategy, the cultural effect cannot be isolated.

## Foundational Learning

- Concept: Moral Foundations Theory
  - Why needed here: Provides the taxonomy of moral principles used to construct scenarios and interpret model choices.
  - Quick check question: Can you list the five moral foundations identified in Moral Foundations Theory and explain how each might manifest in a legal vs. family context?

- Concept: Kohlberg's Stages of Moral Development
  - Why needed here: Used to map moral principles to developmental stages, enabling analysis of how models' moral reasoning aligns with human developmental benchmarks.
  - Quick check question: How would a moral principle like "maintaining public safety" differ in justification between Stage 4 (law and order) and Stage 5 (social contract) of Kohlberg's theory?

- Concept: Luce Spectral Ranking and Best-Worst Scaling
  - Why needed here: These are the mathematical foundations for converting pairwise preference data into an overall ranking of moral principles.
  - Quick check question: What is the difference between a choice that is "best" vs. "worst" in BWS, and how does that influence the pairwise preference matrix fed into ILSR?

## Architecture Onboarding

- Component map:
  - Moral Scenario Generator → Pairwise Preference Extractor → BWS/ILSR Ranker → Firmness Probe (Debate Module)
- Critical path:
  - Generate scenario → Model selects A or B → Record choice and firmness → Map to principle pair → Update ranking matrix → Iterate until convergence
- Design tradeoffs:
  - Scenario complexity vs. model interpretability: Richer scenarios may elicit more nuanced choices but risk inconsistent outputs.
  - Debate rounds vs. computational cost: More rounds may yield stronger firmness estimates but increase latency and cost.
- Failure signatures:
  - Low consistency in pairwise preferences → Ranking unstable or uninformative
  - Post-debate choice change > threshold → Model lacks moral conviction
  - Firmness score bimodal clustering → Poor calibration of moral confidence
- First 3 experiments:
  1. Run a single scenario through all four models and manually verify that the principle mapping is consistent.
  2. Execute a small batch of scenarios and check that the BWS/ILSR pipeline produces a stable ranking without contradictions.
  3. Conduct a debate round between two models on a fixed scenario and record firmness score changes to validate the debate probe.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do moral beliefs of LLMs vary across different cultural contexts beyond the Chinese-Western binary examined in this study?
- Basis in paper: [explicit] The paper acknowledges that "the dichotomous categorization of Chinese and Western cultural backgrounds does not extend to all cultures beyond these two groups" and recognizes the importance of cultural diversity and complexity.
- Why unresolved: The study focused specifically on Chinese and English language models, limiting the generalizability of findings to other cultural contexts. The paper calls for including more annotators from diverse cultural backgrounds in future research.
- What evidence would resolve it: Comparative studies of moral beliefs across multiple diverse cultural contexts (e.g., Middle Eastern, African, Latin American) using the same three-module framework would reveal how cultural factors influence LLM moral reasoning across a broader spectrum.

### Open Question 2
- Question: What is the long-term impact of engaging LLMs in moral debates on their underlying moral beliefs and decision-making stability?
- Basis in paper: [explicit] The study observed varying susceptibility to opponents among different models during debates and noted that "future investigations will delve into these aspects to enhance our understanding." The paper mentions that models showed different tendencies to change choices after debates.
- Why unresolved: The study only conducted two rounds of debate and observed immediate changes, but did not examine whether repeated exposure to moral debates leads to persistent changes in moral beliefs or decision-making patterns over time.
- What evidence would resolve it: Longitudinal studies tracking LLM moral choices across multiple debate sessions over extended periods would reveal whether debate exposure leads to lasting changes in moral beliefs or merely temporary shifts in responses.

### Open Question 3
- Question: How does gender bias in LLM moral reasoning relate to real-world occupational gender segregation and societal stereotypes?
- Basis in paper: [explicit] The paper found "significant disparities in the prioritization of moral principles between men and women" across models and noted that "Erine stands out as a relatively gender-neutral language model." It also references prior research showing "the existence of a gender bias towards man as the norm, a tendency that is also reflected in language."
- Why unresolved: While the paper identifies gender bias in moral reasoning, it does not explore the relationship between these biases and real-world occupational gender patterns or societal stereotypes that may have influenced model training data.
- What evidence would resolve it: Correlational studies comparing LLM moral reasoning patterns with occupational gender segregation statistics and linguistic gender bias measures would reveal whether model biases reflect or amplify existing societal patterns.

## Limitations

- Cross-cultural comparison validity: The study's cultural conclusions may be confounded by model architecture, size, and fine-tuning differences rather than training data alone
- Firmness measurement reliability: The debate-based firmness scoring lacks validation and may reflect response consistency rather than true moral conviction
- Reproducibility concerns: Critical implementation details including exact prompt templates and temperature settings are not fully specified

## Confidence

- High confidence: The framework's general structure and dataset construction process are methodologically sound
- Medium confidence: The observation that different models exhibit different moral preferences is supported, though cultural interpretation may be overstated
- Low confidence: Specific claims about cultural influence on moral beliefs and English models mirroring Chinese students' choices require more careful interpretation

## Next Checks

1. **Cross-validation with neutral scenarios**: Test whether the same models produce different moral choices when presented with scenarios from their native language corpora versus the Chinese scenarios used in this study, controlling for other variables.

2. **Sensitivity analysis of firmness scores**: Conduct multiple debate rounds on the same scenarios with varying opponent arguments to determine whether firmness score changes are consistent and meaningful rather than artifacts of specific debate exchanges.

3. **Ablation study of ranking method**: Remove the BWS/ILSR ranking step and analyze raw pairwise choice frequencies to determine whether the complex ranking procedure adds meaningful information beyond simple majority preferences.