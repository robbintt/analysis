---
ver: rpa2
title: 'Verif.ai: Towards an Open-Source Scientific Generative Question-Answering
  System with Referenced and Verifiable Answers'
arxiv_id: '2402.18589'
source_url: https://arxiv.org/abs/2402.18589
tags: []
core_contribution: The Verif.ai project introduces an open-source scientific question-answering
  system that addresses hallucination issues in large language models by combining
  semantic and lexical search over PubMed, fine-tuned Mistral 7B for answer generation
  with references, and a verification engine using DeBERTa-large to detect hallucinations.
  The system achieves an F1-score of 0.88 for hallucination detection, outperforming
  GPT-4 in zero-shot regimes on the SciFact dataset.
---

# Verif.ai: Towards an Open-Source Scientific Generative Question-Answering System with Referenced and Verifiable Answers

## Quick Facts
- arXiv ID: 2402.18589
- Source URL: https://arxiv.org/abs/2402.18589
- Authors: Miloš Košprdić; Adela Ljajić; Bojana Bašaragin; Darija Medvecki; Nikola Milošević
- Reference count: 21
- Primary result: Achieves F1-score of 0.88 for hallucination detection, outperforming GPT-4 on SciFact dataset

## Executive Summary
Verif.ai introduces an open-source scientific question-answering system that addresses hallucination issues in large language models by combining semantic and lexical search over PubMed, fine-tuned Mistral 7B for answer generation with references, and a verification engine using DeBERTa-large to detect hallucinations. The system achieves an F1-score of 0.88 for hallucination detection, outperforming GPT-4 in zero-shot regimes on the SciFact dataset. By integrating user feedback and providing verifiable, referenced answers, Verif.ai aims to enhance trust and productivity in scientific research while mitigating misinformation risks.

## Method Summary
The system uses hybrid semantic and lexical search over PubMed articles to retrieve relevant scientific papers, then generates answers using fine-tuned Mistral 7B with reference annotations. Hallucination detection is performed by a separate DeBERTa-large model trained on the SciFact dataset to classify claims as supporting, contradicting, or unsupported relative to source abstracts. The entire pipeline is implemented as an open-source system with user feedback integration for continuous improvement.

## Key Results
- Achieves F1-score of 0.88 for hallucination detection using DeBERTa-large
- Outperforms GPT-4 in zero-shot regimes on SciFact dataset
- Successfully combines semantic and lexical search for improved retrieval quality
- Generates referenced answers with bracketed citation IDs mapped to specific papers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining semantic and lexical search improves retrieval quality by balancing exact matches with semantic relevance.
- Mechanism: Hybrid search uses lexical indexing for precise term matching while semantic search (via embeddings) captures meaning similarity, ensuring both exact matches and related content appear in results.
- Core assumption: Semantic search alone would deprioritize exact matches, while lexical search alone would miss paraphrased content.
- Evidence anchors:
  - [abstract] "This approach emphasizes direct matches while also finding semantically similar phrases and parts of the text where the text does not match."
  - [section] "We observed that lexical search may perform better when the search terms can be exactly matched in the documents, while semantic search works well with paraphrased text or synonymous terms."
  - [corpus] Corpus evidence shows related work uses hybrid approaches, but specific performance comparisons are missing.
- Break condition: If semantic embeddings become too dissimilar from lexical content, the hybrid system may prioritize irrelevant semantic matches over exact relevant terms.

### Mechanism 2
- Claim: Fine-tuning Mistral 7B with referenced Q&A data improves its ability to generate scientifically accurate answers with proper citations.
- Mechanism: The model learns to structure answers using retrieved abstracts as context, generating claims with bracketed reference IDs that map to specific papers.
- Core assumption: The fine-tuning dataset (10k PubMedQA questions with GPT-3.5-generated referenced answers) captures the reference-annotation patterns needed for scientific accuracy.
- Evidence anchors:
  - [abstract] "We have used Mistral 7B parameter model with instruction fine-tuning... The training was performed using a rescaled loss, a rank of 64, an alpha of 16, and a LoRA dropout of 0.1, resulting in 27,262,976 trainable parameters."
  - [section] "The fine-tuning of the Mistral 7B model improved the model's performance, making the generated answers comparable to those of much larger GPT-3.5 and GPT-4 models for the referenced question-answering task."
  - [corpus] Weak - corpus lacks direct performance comparisons between fine-tuned Mistral and baseline models on the same benchmark.
- Break condition: If the training data contains incorrect or biased references, the fine-tuned model may inherit these flaws and generate misleading citations.

### Mechanism 3
- Claim: Using DeBERTa-large for claim verification provides superior hallucination detection compared to the generation model itself.
- Mechanism: DeBERTa-large treats verification as natural language inference, classifying claims as supporting, contradicting, or unsupported relative to the referenced abstract.
- Core assumption: A model with different architecture than the generator (DeBERTa vs Mistral) can independently detect hallucinations the generator might miss.
- Evidence anchors:
  - [abstract] "The evaluation of the fine-tuned XLM-RoBERTa and DeBERTa model on the SciFact dataset that can be used for hallucination detection... DeBERTa-large model showed superior performance compared to the RoBERTa-large."
  - [section] "Therefore, our models outperformed GPT-4 model in zero-shot regime with carefully designed prompt for label prediction for the claims and abstracts in the SciFact dataset."
  - [corpus] Assumption: Corpus suggests NLI models are commonly used for fact verification, but lacks direct evidence of DeBERTa's superiority on this specific task.
- Break condition: If the claim involves subtle numerical differences or complex reasoning not well-handled by the NLI model, verification may fail even when hallucinations exist.

## Foundational Learning

- Concept: Information Retrieval (IR) - combining semantic and lexical search
  - Why needed here: Scientific QA requires both exact term matching (for precision) and semantic understanding (for recall) across PubMed's vast corpus
  - Quick check question: What happens to retrieval quality if you use only semantic search on a query with highly specific biomedical terminology?

- Concept: Natural Language Inference (NLI) for claim verification
  - Why needed here: Detecting hallucinations requires determining whether generated claims are supported by, contradict, or lack evidence in source abstracts
  - Quick check question: How would you handle a claim that is partially supported but contains one false detail according to the source abstract?

- Concept: LoRA (Low-Rank Adaptation) fine-tuning
  - Why needed here: Efficiently adapts large language models like Mistral 7B to specialized scientific QA tasks without full fine-tuning
  - Quick check question: What are the trade-offs between using LoRA versus full fine-tuning when adapting a 7B parameter model to a new domain?

## Architecture Onboarding

- Component map: User query -> Hybrid search (OpenSearch with lexical+semantic) -> Answer generation (fine-tuned Mistral 7B) -> Hallucination detection (DeBERTa-large) -> User output

- Critical path: User query → Hybrid search → Answer generation → Verification → User output

- Design tradeoffs:
  - Hybrid search adds complexity but improves retrieval quality vs. single-method approaches
  - Using a different architecture (DeBERTa) for verification provides independence but requires maintaining two models
  - Fine-tuning on automatically generated data (GPT-3.5 answers) is efficient but may propagate errors

- Failure signatures:
  - Poor retrieval: Users report missing relevant papers or getting irrelevant results
  - Hallucination leaks: Verification engine misses false claims that contradict source abstracts
  - Reference errors: Generated answers cite wrong papers or contain broken reference IDs

- First 3 experiments:
  1. Test hybrid search retrieval quality by comparing top-5 results from semantic-only, lexical-only, and hybrid approaches on 20 benchmark queries
  2. Evaluate fine-tuned Mistral's citation accuracy by generating 50 answers and manually checking reference mapping correctness
  3. Stress-test verification engine by creating adversarial claims (slight modifications of abstract content) to measure detection sensitivity

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope to biomedical literature, may not generalize to other scientific domains
- Reliance on automatically generated training data (GPT-3.5 answers) may propagate errors
- Lack of comprehensive quantitative comparisons for hybrid search effectiveness

## Confidence
- Hallucination detection performance: Medium - based on reported metrics but limited dataset scope
- Hybrid search effectiveness: Low-Medium - qualitative observations without comprehensive benchmarks
- Fine-tuning approach validity: Medium - efficient but potentially propagates errors from source data

## Next Checks
1. Conduct cross-domain evaluation of hallucination detection on non-biomedical scientific literature to assess generalizability
2. Perform ablation studies comparing hybrid search against pure semantic and pure lexical approaches across diverse query types
3. Implement human evaluation of reference accuracy in generated answers to validate citation quality beyond automated metrics