---
ver: rpa2
title: Prompt Design Matters for Computational Social Science Tasks but in Unpredictable
  Ways
arxiv_id: '2406.11980'
source_url: https://arxiv.org/abs/2406.11980
tags:
- your
- output
- provide
- class
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study tested how prompt design impacts large language models'
  (LLMs) compliance and accuracy on four computational social science tasks. Using
  a complete factorial design, it varied prompts along definition inclusion, output
  type, explanation, and length across three models (ChatGPT, PaLM2, Falcon7b) and
  four datasets.
---

# Prompt Design Matters for Computational Social Science Tasks but in Unpredictable Ways

## Quick Facts
- arXiv ID: 2406.11980
- Source URL: https://arxiv.org/abs/2406.11980
- Reference count: 40
- Primary result: Prompt design significantly impacts LLM compliance and accuracy for computational social science tasks, but effects vary unpredictably across models and tasks

## Executive Summary
This study systematically examines how prompt design affects large language models' performance on computational social science annotation tasks. Using a complete factorial design, researchers varied prompts across four dimensions (definition inclusion, output type, explanation, and length) while testing three models (ChatGPT, PaLM2, Falcon7b) on four datasets. Results demonstrate that prompt design has substantial and unpredictable effects on both compliance (following instructions) and accuracy (matching human annotations), with no single "best" approach working across all scenarios. The findings emphasize the need for careful prompt engineering and suggest using multiple prompting strategies to ensure robust results when using LLMs for social science annotation.

## Method Summary
The study employed a complete factorial design to test 16 different prompt variations across four computational social science tasks (toxicity, sentiment, rumor stance, news frames). Each prompt varied in definition inclusion, output type (label vs. score), explanation requirement, and length. Three LLM models (ChatGPT, PaLM2, Falcon7b) generated annotations for all datasets, which were then evaluated for compliance (whether outputs matched prompt instructions) and accuracy (compared to human annotations). The approach systematically isolates the impact of individual prompt design choices while controlling for task and model differences.

## Key Results
- Prompting for numerical scores instead of labels reduced both compliance and accuracy across all LLMs and tasks
- Concise prompts had mixed effects, improving efficiency for some tasks but reducing quality for others
- ChatGPT showed the highest compliance rates (up to 99%) while PaLM2 achieved the highest accuracy
- Minor prompt changes caused large shifts in label distributions, with ChatGPT labeling 34% more content as neutral when prompted to explain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt design significantly affects LLM compliance and accuracy, with effects varying unpredictably across models and tasks.
- Mechanism: The study shows that prompt variations in definition inclusion, output type, explanation, and length produce different levels of compliance and accuracy. For example, prompting for numerical scores instead of labels reduces compliance and accuracy across all LLMs and tasks. Concise prompts can improve efficiency but may reduce quality depending on the task and model.
- Core assumption: The study assumes that prompt design features (definition inclusion, output type, explanation, and length) are independent variables that can be manipulated to observe their impact on LLM performance.
- Evidence anchors:
  - [abstract]: "Our results show that LLM compliance and accuracy are highly prompt-dependent."
  - [section]: "For instance, prompting for numerical scores instead of labels reduces all LLMs' compliance and accuracy."
  - [corpus]: Weak evidence. The corpus contains related papers but no direct quantitative support for the mechanism's specific claims about compliance and accuracy variation.

### Mechanism 2
- Claim: Prompting for explanations increases LLM compliance but changes the distribution of generated labels.
- Mechanism: When LLMs are prompted to explain their outputs, they are more likely to mention the correct class labels, increasing compliance. However, this also leads to significant shifts in the distribution of generated labels, as seen in the sentiment and toxicity tasks where prompting for explanations resulted in more neutral or toxic labels, respectively.
- Core assumption: LLMs have the capacity to generate explanations and that these explanations influence their output in a predictable way.
- Evidence anchors:
  - [abstract]: "Prompting LLMs to explain their input improves their compliance with prompt instructions. However, this also changes the distribution of generated labels."
  - [section]: "ChatGPT annotates 34% more content as neutral when prompted to explain its output."
  - [corpus]: Weak evidence. The corpus does not provide direct support for the specific claim about explanations changing label distributions.

### Mechanism 3
- Claim: The impact of prompt design on accuracy is highly model and task dependent.
- Mechanism: The study finds that the effectiveness of different prompt designs varies significantly across models (ChatGPT, PaLM2, Falcon7b) and tasks (toxicity, sentiment, rumor stance, news frames). For example, prompting with definitions improves ChatGPT's accuracy but reduces compliance for PaLM2 and Falcon7b. Concise prompts can be efficient for some tasks but reduce accuracy for others.
- Core assumption: Different LLMs have different strengths and weaknesses, and the complexity of tasks affects how well they respond to prompt variations.
- Evidence anchors:
  - [abstract]: "The overall best prompting setup is task-dependent, and minor prompt changes can cause large changes in the distribution of generated labels."
  - [section]: "The impact of prompt design on accuracy is highly model and task dependent."
  - [corpus]: Weak evidence. The corpus contains related papers but does not provide direct support for the specific claim about model and task dependency.

## Foundational Learning

- Concept: Factorial design
  - Why needed here: The study uses a complete factorial design to systematically vary prompt features and observe their impact on LLM performance. Understanding factorial design is crucial for interpreting the results and designing similar experiments.
  - Quick check question: What is the purpose of using a factorial design in this study, and how does it help in understanding the impact of prompt variations?

- Concept: Compliance vs. Accuracy
  - Why needed here: The study distinguishes between compliance (whether the LLM follows the prompt instructions) and accuracy (how well the LLM's output matches human annotations). Understanding this distinction is essential for interpreting the results and evaluating LLM performance.
  - Quick check question: How does the study define and measure compliance and accuracy, and why is it important to consider both metrics?

- Concept: Zero-shot prompting
  - Why needed here: The study uses zero-shot prompting, where LLMs are given instructions without any examples. Understanding zero-shot prompting is crucial for interpreting the results and designing prompts for similar tasks.
  - Quick check question: What is zero-shot prompting, and how does it differ from other prompting techniques like few-shot or chain-of-thought prompting?

## Architecture Onboarding

- Component map: Datasets -> Prompt variations -> LLM models -> Output parsing -> Compliance/accuracy metrics
- Critical path:
  1. Design prompts based on task requirements and practical constraints
  2. Generate annotations using LLMs with different prompt variations
  3. Parse LLM output to extract labels and scores
  4. Measure compliance and accuracy by comparing LLM output with human annotations
  5. Analyze the impact of prompt variations on compliance and accuracy across models and tasks

- Design tradeoffs:
  - Prompt length vs. cost: Concise prompts can reduce costs but may affect accuracy
  - Output type (label vs. score): Labels are easier to parse but scores provide more nuanced information
  - Explanation vs. compliance: Prompting for explanations increases compliance but may change label distributions

- Failure signatures:
  - Low compliance: LLMs frequently ignore prompt instructions or generate invalid outputs
  - Inconsistent accuracy: LLM performance varies widely across different prompt variations or tasks
  - Label distribution shifts: Prompt variations cause significant changes in the distribution of generated labels

- First 3 experiments:
  1. Test the impact of definition inclusion on compliance and accuracy for a simple task (e.g., toxicity)
  2. Compare the effects of prompting for labels vs. scores on compliance and accuracy across different models
  3. Analyze the impact of prompting for explanations on compliance and label distribution for a multi-class task (e.g., sentiment)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt design elements interact with each other when used in combination (e.g., definition + explanation + concise)?
- Basis in paper: [explicit] The paper notes "Our results analyzed one prompt variation at a time, leaving open the possibility that different prompt variations may interact and produce different impacts."
- Why unresolved: The study used a complete factorial design but only analyzed individual prompt variations separately, not their interactions.
- What evidence would resolve it: Experiments testing specific combinations of prompt variations together and measuring their combined effects on compliance and accuracy.

### Open Question 2
- Question: Why do explanations cause such dramatic shifts in label distributions (e.g., ChatGPT labeling 34% more content as neutral when prompted to explain)?
- Basis in paper: [explicit] "Prompting LLMs to explain their output improves their compliance with prompt instructions. However, this also changes the distribution of generated labels."
- Why unresolved: The paper identifies this phenomenon but doesn't explain the underlying mechanism causing the distributional shifts.
- What evidence would resolve it: Analysis of model attention patterns or internal representations when generating explanations vs. non-explanations, or comparison of explanation quality across different label distributions.

### Open Question 3
- Question: Would combining multiple prompts per item (like crowdsourcing) improve overall annotation quality and robustness?
- Basis in paper: [inferred] The discussion suggests "Rather than using a single prompt unquestioningly, researchers should carefully evaluate several prompting strategies... and potentially use several prompts for robustness when making claims."
- Why unresolved: The paper doesn't test multi-prompt approaches experimentally.
- What evidence would resolve it: Head-to-head comparison of single-prompt vs. multi-prompt annotation strategies across various CSS tasks, measuring inter-annotator agreement and final task performance.

## Limitations
- The study only examined four prompt design features in isolation, without testing more complex prompt engineering techniques
- Results may not generalize to other types of social science annotation tasks beyond the four studied (toxicity, sentiment, rumor stance, news frames)
- The effects of prompt variations appear highly unpredictable across different models and tasks, making optimization challenging

## Confidence
- High confidence: The finding that prompting for numerical scores reduces both compliance and accuracy across all models and tasks
- Medium confidence: The general observation that prompt design significantly impacts LLM performance, though the specific effects are unpredictable
- Medium confidence: The claim that concise prompts can improve efficiency but may reduce quality depending on the task and model

## Next Checks
1. Test whether the observed prompt effects generalize to additional computational social science tasks beyond the four studied (e.g., topic classification, stance detection, or demographic inference)
2. Examine whether including few-shot examples or chain-of-thought prompting can mitigate the negative effects of concise prompts on accuracy
3. Validate whether the prompt design recommendations hold for additional LLM architectures beyond the three tested models, particularly open-source models with different pretraining approaches