---
ver: rpa2
title: 'BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing
  and Throughput-oriented Token Batching'
arxiv_id: '2412.03594'
source_url: https://arxiv.org/abs/2412.03594
tags:
- prefix
- batchllm
- tokens
- token
- requests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BatchLLM optimizes large-batch LLM inference for throughput-oriented
  tasks by explicitly identifying global prefix sharing, reordering requests, and
  batching tokens based on memory usage rather than request count. It uses a dynamic
  programming algorithm to maximize first-level prefix reuse, schedules requests by
  prefix-sharing groups, and fuses prefix-shared attention kernels to reduce overhead.
---

# BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching

## Quick Facts
- arXiv ID: 2412.03594
- Source URL: https://arxiv.org/abs/2412.03594
- Reference count: 40
- Achieves 1.3× to 10.8× speedup over vLLM and SGLang for large-batch LLM inference

## Executive Summary
BatchLLM addresses the challenge of optimizing large-batch LLM inference for throughput-oriented tasks by introducing three key innovations: explicit global prefix identification for optimal KV reuse, memory-centric token batching for increased GPU utilization, and horizontal fusion of prefix-shared attention kernels to reduce overhead. The system achieves significant performance improvements by reordering requests based on decoding-to-prompt length ratios and forming larger token-batches constrained by memory usage rather than request count. Evaluated across multiple GPU platforms with models like Llama-3.1 and Qwen-2.5, BatchLLM demonstrates token reuse ratios up to 85.2% and substantial reduction in token-batch "valleys."

## Method Summary
BatchLLM optimizes large-batch LLM inference through a three-pronged approach: (1) constructing a global prefix tree from all input prompts to identify shared prefixes, then using dynamic programming to maximize first-level prefix reuse and avoid premature KV context eviction; (2) reordering requests by their decoding-to-prompt length ratio and grouping requests with common prefixes to improve mixing of decoding and prefill tokens, while batching tokens based on KV memory usage rather than request count; (3) horizontally fusing prefix-shared attention computations into a single kernel to reduce tail effects and kernel launch overhead. The system integrates these optimizations with the vLLM framework while maintaining cross-platform compatibility through Triton implementation.

## Key Results
- Achieves 1.3× to 10.8× speedup over vLLM and SGLang on NVIDIA and AMD GPUs
- Increases token reuse ratio from 35.8% to 85.2% through global prefix identification
- Reduces token-batch "valleys" by reordering requests and memory-centric batching
- Maintains competitive performance across different GPU architectures (NVIDIA and AMD)

## Why This Works (Mechanism)

### Mechanism 1: Global Prefix Identification
BatchLLM explicitly identifies common prefixes across all input prompts using a compact prefix tree, then applies dynamic programming to maximize first-level prefix reuse. This approach prevents premature eviction of reusable KV contexts that occurs with LRU-based cache management in existing solutions. The global analysis enables optimal KV cache reuse, increasing token reuse ratio from 35.8% to 85.2% in microbenchmarks.

### Mechanism 2: Memory-Centric Token Batching
BatchLLM reorders requests by the ratio of decoding length to prompt length, schedules prefix-sharing groups together, and forms token-batches based on KV memory usage rather than request count. This throughput-oriented scheduling better mixes decoding tokens with prefill chunks and increases GPU utilization by forming larger batches when memory allows, reducing token-batch "valleys" and improving overall throughput.

### Mechanism 3: Horizontal Fused Attention Kernels
BatchLLM horizontally fuses partial attention calculations on prefix and distinct KV contexts into a single kernel with different tiling configurations for each part. This fusion reduces tail effects and kernel launch overhead compared to separate kernel implementations, achieving competitive performance while supporting multiple GPU platforms through Triton implementation.

## Foundational Learning

- Concept: Large language model inference phases (prefill and decoding)
  - Why needed here: Understanding phase characteristics is crucial for optimizing batching and scheduling strategies
  - Quick check question: Why is the decoding phase typically memory-bound while the prefill phase is compute-bound in LLM inference?

- Concept: Key-Value (KV) cache management and prefix sharing
  - Why needed here: BatchLLM's optimization heavily relies on efficient KV cache reuse across requests with shared prefixes
  - Quick check question: How does prefix sharing reduce KV cache memory consumption and computation in multi-request scenarios?

- Concept: Continuous batching and token-batch formation
  - Why needed here: BatchLLM's memory-centric token batching strategy builds upon continuous batching concepts
  - Quick check question: What are the advantages of batching at the token level rather than the request level in LLM inference?

## Architecture Onboarding

- Component map: Global prefix tree construction and DP optimization module -> Request grouping and reordering scheduler -> Memory-aware token batcher with three-queue system -> Horizontal fused prefix-shared attention kernel -> Integration layer with existing vLLM framework

- Critical path: Global prefix identification → Request reordering → Group scheduling → Memory-centric token batching → Fused attention execution

- Design tradeoffs:
  - Memory vs. throughput: Larger token-batches increase throughput but require more memory
  - Static vs. dynamic optimization: BatchLLM's static global analysis provides better optimization but lacks adaptability to dynamic workloads
  - Platform generality vs. peak performance: Triton-based implementation supports multiple platforms but may not match CUDA-specific optimizations

- Failure signatures:
  - Memory exhaustion: Indicates token-batching parameters need adjustment
  - Suboptimal prefix reuse: Suggests prefix tree construction or DP algorithm issues
  - High kernel launch overhead: May indicate problems with horizontal fusion implementation

- First 3 experiments:
  1. Measure token reuse ratio improvement with and without global prefix identification on a synthetic dataset with known shared prefixes
  2. Compare throughput with different request reordering strategies (ratio-based vs. random) on a microbenchmark
  3. Benchmark horizontal fused attention kernel performance against separate kernel implementation on both NVIDIA and AMD GPUs

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of BatchLLM's horizontal fused Attention kernel scale when handling extremely long shared prefixes compared to the current implementation? The paper mentions that for long-key-value cases, split-k could be a solution, but does not provide experimental data on how the current implementation performs with extremely long shared prefixes.

### Open Question 2
What is the optimal threshold for the KV memory size (M_threshold) in BatchLLM's memory-centric token batching strategy, and how does it vary across different GPU architectures and model sizes? While the paper explains the concept, it does not provide empirical data on optimal threshold values for different scenarios.

### Open Question 3
How does BatchLLM's performance compare to other state-of-the-art LLM inference systems when serving multi-turn conversations with context windows larger than those typically used in web search tasks? The paper's evaluation focuses on single-turn tasks with relatively short context windows, leaving questions about performance in scenarios requiring longer context management.

## Limitations

- Global prefix analysis requires all prompts to be known ahead-of-time, limiting effectiveness for dynamic or streaming workloads
- Memory-centric batching may cause excessive memory pressure or swapping when GPU memory is severely constrained
- Platform-agnostic Triton implementation may not achieve peak performance compared to platform-specific optimizations

## Confidence

**High confidence**: The core premise that prefix sharing can significantly improve KV cache reuse is well-established in related work. The 1.3× to 10.8× speedup claims are supported by the 85.2% token reuse ratio achieved through global prefix identification.

**Medium confidence**: The effectiveness of memory-centric token batching and throughput-oriented scheduling is supported by the reduction in token-batch "valleys," but the paper lacks detailed analysis of memory usage patterns and potential swapping behavior under different memory constraints.

**Low confidence**: The cross-platform performance claims (particularly for AMD GPUs) are based on Triton implementation, but the paper doesn't provide detailed comparison with platform-specific optimizations that could potentially yield better performance.

## Next Checks

1. **Dynamic Batch Validation**: Test BatchLLM's performance on workloads where prompts arrive incrementally rather than in a single batch, measuring how quickly the prefix tree adapts and whether performance degrades compared to the static case.

2. **Memory Pressure Analysis**: Run BatchLLM under progressively tighter memory constraints to identify the point where memory-centric batching begins to cause performance degradation through excessive swapping or reduced batch sizes.

3. **Platform-Specific Optimization Comparison**: Implement CUDA-specific optimizations for NVIDIA GPUs and ROCm-specific optimizations for AMD GPUs, then compare the performance of BatchLLM's platform-agnostic implementation against these optimized versions to quantify the performance gap.