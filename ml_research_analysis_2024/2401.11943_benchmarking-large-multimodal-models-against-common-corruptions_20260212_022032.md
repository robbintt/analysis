---
ver: rpa2
title: Benchmarking Large Multimodal Models against Common Corruptions
arxiv_id: '2401.11943'
source_url: https://arxiv.org/abs/2401.11943
tags:
- arxiv
- large
- corruptions
- text
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MMCBench, a comprehensive benchmark for
  evaluating the robustness of large multimodal models (LMMs) under common input corruptions
  across four key tasks: text-to-image, image-to-text, text-to-speech, and speech-to-text.
  The authors address the gap in existing benchmarks by focusing on the self-consistency
  of LMMs when their inputs are corrupted, using text as a semantic anchor for selecting
  challenging examples from large datasets like LAION and Common Voice.'
---

# Benchmarking Large Multimodal Models against Common Corruptions

## Quick Facts
- arXiv ID: 2401.11943
- Source URL: https://arxiv.org/abs/2401.11943
- Reference count: 40
- Primary result: Introduces MMCBench, a comprehensive benchmark for evaluating LMM robustness under common input corruptions across four key tasks

## Executive Summary
This paper addresses the critical need for robust evaluation of large multimodal models (LMMs) by introducing MMCBench, a comprehensive benchmark that tests model consistency under various input corruptions. The authors propose using text as a semantic anchor to select challenging examples from large datasets, then evaluate over 100 LMMs across more than 150 checkpoints using both cross-modality and output-only consistency metrics. The benchmark reveals that LMMs generally outperform non-LLM-based models on corrupted inputs, though larger LLMs don't always yield better consistency scores, highlighting important nuances in model robustness.

## Method Summary
The MMCBench framework evaluates LMM robustness by applying 23 text, 29 image, and 16 speech corruptions to carefully selected examples from LAION and Common Voice datasets. The selection process uses text similarity as a semantic anchor to identify challenging inputs that cause significant semantic changes. The evaluation measures consistency through cosine similarity between clean and corrupted outputs, using both cross-modality (comparing different output modalities) and output-only (comparing outputs within the same modality) metrics. Over 100 LMMs are tested across four tasks: text-to-image, image-to-text, text-to-speech, and speech-to-text.

## Key Results
- LMMs generally outperform non-LLM-based models when handling corrupted inputs
- Models tested on carefully selected challenging data perform worse than those tested on randomly sampled data, validating the selection strategy
- Larger LLMs do not always yield better consistency scores, particularly in image-to-text tasks
- Cross-modality and output-only consistency scores provide complementary insights into model robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text as semantic anchor allows robust cross-modal consistency evaluation.
- Mechanism: By mapping all modalities into text and computing cosine similarity, the benchmark creates a unified semantic space for measuring consistency across corrupted inputs.
- Core assumption: Text adequately captures semantic information across modalities and remains stable under corruption compared to other modalities.
- Evidence anchors:
  - [abstract] "we propose utilizing text as a semantic anchor... we hope to select examples with significant textual changes for evaluation"
  - [section 3] "We use consistency scores in the text modality for ranking data samples for data selection, which is applicable to all cross-modalities"
  - [corpus] Weak evidence - corpus lacks direct comparison of text vs other modality stability under corruption
- Break condition: If text corruption patterns differ fundamentally from other modalities, or if text fails to capture critical semantic aspects present in other modalities.

### Mechanism 2
- Claim: Selection based on textual inconsistency improves benchmark difficulty.
- Mechanism: By ranking samples based on text similarity drops under corruption, the benchmark focuses on inputs most likely to challenge model robustness.
- Core assumption: Inputs causing large text similarity drops will also challenge LMMs' cross-modal consistency.
- Evidence anchors:
  - [section 3] "We select them based on four scores: inconsistency score, readability score, syntax complexity score, and description score"
  - [section 3.1] "We hope to select examples with significant textual changes for evaluation by mapping all modalities into text and calculating similarity"
  - [corpus] Weak evidence - corpus lacks validation that textual inconsistency correlates with model performance drops
- Break condition: If the correlation between textual inconsistency and model difficulty breaks for certain corruption types or model architectures.

### Mechanism 3
- Claim: Dual metric approach (cross-modality + output-only) provides comprehensive robustness assessment.
- Mechanism: Using both cross-modality similarity and output-only consistency scores captures different aspects of model robustness and helps identify models with superficial consistency.
- Core assumption: Models with high cross-modality but low output-only scores are exploiting superficial patterns rather than genuine robustness.
- Evidence anchors:
  - [abstract] "we assess consistency within the output modality itself... if a model consistently outputs poor-quality captions, it may still achieve a high consistency score"
  - [section 1] "we present both cross-modality and output-only modality consistency scores for a comprehensive reference"
  - [corpus] Weak evidence - corpus lacks analysis of cases where cross-modality and output-only scores diverge significantly
- Break condition: If the two metrics become redundant or if one metric consistently dominates the other in predicting real-world performance.

## Foundational Learning

- Concept: Cosine similarity in semantic embedding space
  - Why needed here: Used to measure consistency between original and corrupted outputs across all tasks
  - Quick check question: If two text strings have cosine similarity of 0.8, what does this indicate about their semantic similarity?

- Concept: Cross-modal consistency evaluation
  - Why needed here: Core mechanism for assessing LMM robustness when input modalities are corrupted
  - Quick check question: Why might cross-modality consistency be more challenging than unimodal consistency for LMMs?

- Concept: Dataset selection based on complexity metrics
  - Why needed here: Ensures benchmark focuses on challenging examples that truly test model capabilities
  - Quick check question: What are the trade-offs between selecting hard vs random samples for robustness evaluation?

## Architecture Onboarding

- Component map: Data selection pipeline -> Corruption application -> Model inference -> Consistency measurement -> Result aggregation
- Critical path: Data selection -> Corruption application -> Model inference -> Cross-modality consistency measurement
- Design tradeoffs: Hard selection vs random selection balances benchmark difficulty with generalizability; greedy decoding vs sampling affects reproducibility vs optimality
- Failure signatures: Low variance across corruption types suggests insufficient challenge; high variance suggests unstable evaluation; correlation breakdown between selection scores and performance indicates selection method issues
- First 3 experiments:
  1. Run baseline evaluation with random selection to establish performance floor
  2. Test correlation between textual inconsistency scores and actual model performance drops
  3. Compare cross-modality vs output-only consistency scores to identify potential superficial consistency cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LMMs perform on robustness tasks involving video or other modalities beyond text, image, and speech?
- Basis in paper: [inferred] The paper focuses on text, image, and speech modalities and mentions future work on incorporating video.
- Why unresolved: The paper does not evaluate LMMs on video or other modalities, leaving a gap in understanding their robustness across all potential multimodal tasks.
- What evidence would resolve it: Experiments testing LMMs on video corruptions and other modalities using MMCBench or similar benchmarks.

### Open Question 2
- Question: Can cross-modality similarity metrics between speech and text be improved to better capture subtle differences in corrupted inputs?
- Basis in paper: [explicit] The authors note that current text similarity measures are less sensitive to minor character/word changes in speech-to-text tasks.
- Why unresolved: The paper acknowledges limitations in current similarity metrics for speech-text tasks but does not propose or test improved methods.
- What evidence would resolve it: Development and validation of new cross-modality similarity metrics that better detect subtle variations in speech transcriptions.

### Open Question 3
- Question: Does model size (LLM parameter count) correlate with robustness to input corruptions in LMMs?
- Basis in paper: [explicit] The authors observe that larger LLMs do not always yield better consistency scores in image-to-text tasks.
- Why unresolved: The paper does not systematically analyze the relationship between model size and robustness across all tasks.
- What evidence would resolve it: Comprehensive experiments varying LLM sizes while controlling for other factors to establish correlation with corruption robustness.

### Open Question 4
- Question: How do greedy decoding and other decoding strategies affect the measured robustness of LMMs to input corruptions?
- Basis in paper: [explicit] The authors use greedy decoding for reproducibility but acknowledge it may underestimate model performance.
- Why unresolved: The paper does not compare robustness scores across different decoding strategies.
- What evidence would resolve it: Experiments evaluating LMM robustness using beam search, sampling, or other decoding methods and comparing results to greedy decoding.

## Limitations

- The selection method based on text similarity lacks comprehensive validation across all corruption types and model architectures
- The paper doesn't provide detailed analysis of cases where cross-modality and output-only scores diverge significantly
- The representativeness of the selected challenging examples may introduce bias for certain corruption patterns

## Confidence

**High Confidence**: The core methodology of using text as a semantic anchor for cross-modal consistency evaluation is well-established in the literature. The technical implementation of cosine similarity calculations and consistency scoring is standard and reproducible.

**Medium Confidence**: The claim that LMMs generally outperform non-LLM-based models on corrupted inputs is supported by the results, but the analysis could be strengthened by more detailed comparisons across different corruption types and model sizes.

**Low Confidence**: The assertion that larger LLMs do not always yield better consistency scores requires more systematic investigation. The paper presents this as a key finding but doesn't provide sufficient analysis of when and why this pattern breaks down.

## Next Checks

1. **Correlation Analysis**: Conduct a detailed analysis of the correlation between textual inconsistency scores and actual model performance drops across different corruption types to validate the selection method's effectiveness.

2. **Metric Divergence Study**: Systematically identify and analyze cases where cross-modality and output-only consistency scores diverge significantly to understand what aspects of robustness each metric captures.

3. **Size-Performance Relationship**: Perform a more granular analysis of the relationship between model size and consistency scores across different tasks and corruption types to better understand when larger models provide robustness benefits.