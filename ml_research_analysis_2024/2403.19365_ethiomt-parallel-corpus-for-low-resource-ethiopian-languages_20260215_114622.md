---
ver: rpa2
title: 'EthioMT: Parallel Corpus for Low-resource Ethiopian Languages'
arxiv_id: '2403.19365'
source_url: https://arxiv.org/abs/2403.19365
tags: []
core_contribution: This paper introduces EthioMT, a new parallel corpus for 15 low-resource
  Ethiopian languages paired with English, along with a benchmark dataset for 23 Ethiopian
  languages. The authors collect religious texts and merge them with existing datasets
  to create larger, domain-mixed corpora.
---

# EthioMT: Parallel Corpus for Low-resource Ethiopian Languages

## Quick Facts
- arXiv ID: 2403.19365
- Source URL: https://arxiv.org/abs/2403.19365
- Reference count: 0
- New parallel corpus for 15 Ethiopian languages with English, benchmark dataset for 23 languages

## Executive Summary
This paper introduces EthioMT, a new parallel corpus for 15 low-resource Ethiopian languages paired with English, along with a benchmark dataset for 23 Ethiopian languages. The authors collect religious texts and merge them with existing datasets to create larger, domain-mixed corpora. They evaluate MT performance using a transformer model and a fine-tuned m2m100 multilingual model. Results show that fine-tuning the multilingual model outperforms training from scratch, especially for languages with larger corpora.

## Method Summary
The authors created EthioMT by collecting religious texts and merging them with existing datasets to create larger, domain-mixed corpora. They evaluated machine translation performance using both a transformer model trained from scratch and a fine-tuned m2m100 multilingual model. The evaluation focused on translation between English and 23 Ethiopian languages, with performance measured using spBLEU scores.

## Key Results
- Fine-tuned m2m100 multilingual model outperforms training from scratch
- Best results: Afaan Oromo (33.7 spBLEU) and Somali (63.9 spBLEU) from English
- Best results: Tigrinya (61.5 spBLEU) and Somali (61.50 spBLEU) to English

## Why This Works (Mechanism)
The fine-tuned m2m100 multilingual model likely benefits from pre-existing knowledge of multiple languages, allowing it to leverage cross-lingual transfer and handle low-resource languages more effectively than training from scratch. The domain mixing in the corpus may help the model generalize better across different text types.

## Foundational Learning
- Transformer architecture: Needed for sequence-to-sequence modeling in MT; Quick check: Verify attention mechanisms function correctly
- m2m100 multilingual model: Needed for cross-lingual transfer; Quick check: Confirm model can handle target language scripts
- Domain adaptation: Needed for handling mixed religious/non-religious text; Quick check: Test model on both domains

## Architecture Onboarding
Component map: m2m100 -> Fine-tuning -> spBLEU Evaluation

Critical path: Data collection → Corpus merging → Model training → Evaluation
Design tradeoffs: Training from scratch vs fine-tuning multilingual model
Failure signatures: Poor performance on low-resource languages
First experiments:
1. Test model on out-of-domain test sets
2. Conduct human evaluation of translations
3. Analyze corpus quality and noise levels

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions, but potential areas for further research could include:
1. How well does the model generalize to non-religious domains?
2. What is the impact of corpus size and quality on translation performance for different languages?
3. How does the model perform on low-resource languages with limited available data?

## Limitations
- BLEU/spBLEU scores may not reflect real-world performance due to religious domain focus
- Limited evaluation only on Bible domain may not generalize
- Focus on English as pivot language limits broader applicability

## Confidence
High: Fine-tuning multilingual model outperforms training from scratch
Medium: Specific BLEU/spBLEU scores and rankings across language pairs
Low: Generalization claims about broader impact on Ethiopian NLP research

## Next Checks
1. Test model performance on out-of-domain test sets to assess generalization beyond religious texts
2. Conduct human evaluation to verify automatic metric scores and assess translation quality across different domains
3. Analyze corpus quality and noise levels in the merged datasets to quantify their impact on model performance