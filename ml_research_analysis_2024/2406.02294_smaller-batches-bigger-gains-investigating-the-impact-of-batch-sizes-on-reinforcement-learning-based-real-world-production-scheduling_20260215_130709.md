---
ver: rpa2
title: Smaller Batches, Bigger Gains? Investigating the Impact of Batch Sizes on Reinforcement
  Learning Based Real-World Production Scheduling
arxiv_id: '2406.02294'
source_url: https://arxiv.org/abs/2406.02294
tags:
- batch
- size
- setup
- agent
- sizes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how varying batch sizes affect reinforcement
  learning (RL) performance in a two-stage production scheduling problem. Using a
  real-world household appliance production setup, the research explores the impact
  of batch sizes on both training dynamics and solution quality.
---

# Smaller Batches, Bigger Gains? Investigating the Impact of Batch Sizes on Reinforcement Learning Based Real-World Production Scheduling

## Quick Facts
- arXiv ID: 2406.02294
- Source URL: https://arxiv.org/abs/2406.02294
- Authors: Arthur MÃ¼ller; Felix Grumbach; Matthia Sabatelli
- Reference count: 14
- Primary result: RL agents trained with smaller batch sizes achieve lower setup efforts but require longer training times, with an optimal batch size minimizing total training duration

## Executive Summary
This study investigates how varying batch sizes affect reinforcement learning performance in a two-stage production scheduling problem. Using a real-world household appliance production setup, the research explores the impact of batch sizes on both training dynamics and solution quality. The RL agent sequences product batches to minimize setup efforts and idle times, with smaller batches showing reduced setup efforts but requiring longer training times due to increased sample complexity. The analysis reveals an optimal batch size that minimizes training duration, beyond which both sample complexity and setup efforts increase. To address training failures with very small batch sizes, two new curriculum learning strategies are introduced, resulting in improved setup efforts at the cost of significantly longer training times.

## Method Summary
The research employs reinforcement learning to solve a two-stage production scheduling problem where product batches must be sequenced to minimize setup efforts and idle times. The method uses PPO (Proximal Policy Optimization) with curriculum learning across three progressively challenging tasks. The MDP formulation includes states encoding product requirements, buffer levels, and last produced type; actions representing product type selection; and rewards combining criticality, safety time, and setup effort penalties. The training pipeline includes action masking to prevent invalid actions and curriculum strategies that gradually increase penalty weights for setup efforts.

## Key Results
- Smaller batch sizes consistently reduce setup efforts by increasing the relative impact of setup penalties in the reward function
- An optimal batch size exists that minimizes training duration, balancing opposing effects of policy space growth and agent flexibility loss
- Curriculum learning strategies successfully enable training with very small batch sizes but significantly increase training times
- The relationship between batch size and performance shows non-linear characteristics with specific batch sizes (e.g., 40 and 70) showing particularly favorable setup effort to sample complexity ratios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller batch sizes reduce setup effort because the penalty weight in the reward function has a larger relative impact per action, encouraging policies that minimize type changes.
- Mechanism: With fewer items per batch, the agent makes more decisions over the same planning horizon. Each setup decision thus contributes more significantly to the total reward, leading the agent to prioritize minimizing type switches to avoid large penalties.
- Core assumption: The agent correctly learns to trade off between minimizing setup effort and managing criticality/safety time penalties.
- Evidence anchors:
  - [abstract]: "The results demonstrate that it is possible to methodically identify reasonable boundaries for the batch size...by the decreasing flexibility of the agent when dealing with larger batch sizes."
  - [section]: "Another reason is that the penalties for setup efforts have a greater impact when batch sizes are smaller...At the beginning of task 3, there is a greater focus on avoiding setup effort for smaller batch sizes."
- Break condition: If the agent cannot balance setup effort minimization with criticality/safety time, it may fail to find valid solutions or drift to undesirable behaviors.

### Mechanism 2
- Claim: There exists an optimal batch size minimizing training duration due to opposing effects of policy space growth and agent flexibility loss.
- Mechanism: As batch size decreases, the number of time steps per episode increases, exponentially expanding the policy space. This makes the problem harder to solve, increasing sample complexity. Conversely, larger batch sizes reduce the agent's ability to flexibly respond to demand changes, increasing sample complexity again.
- Core assumption: The training algorithm (PPO) scales reasonably with changes in policy space size and problem flexibility.
- Evidence anchors:
  - [abstract]: "The analysis reveals an optimal batch size that minimizes training duration, beyond which both sample complexity and setup efforts increase."
  - [section]: "An increasing policy space with decreasing batch size and a decreasing agent flexibility with increasing batch size."
- Break condition: If the problem becomes too hard to solve (e.g., batch size too small) or too inflexible (e.g., batch size too large), training fails or results in suboptimal solutions.

### Mechanism 3
- Claim: Curriculum learning with gradual penalty increase prevents policy drift and enables training with smaller batch sizes.
- Mechanism: By incrementally increasing the setup effort penalty, the agent gradually adapts to the new objective without abruptly changing its policy. This prevents the agent from entering deadlocks or reducing episode length to avoid penalties.
- Core assumption: Smooth transitions in the curriculum allow the agent to maintain desired behavior while learning to minimize setup efforts.
- Evidence anchors:
  - [abstract]: "To address training failures with very small batch sizes, two new curriculum learning strategies are introduced, resulting in improved setup efforts at the cost of significantly longer training times."
  - [section]: "Curriculum B is designed to enable training with a batch size as low as 10...This gradual approach aims to prevent the policy drift observed in Fig. 5 by avoiding a large drop in the return."
- Break condition: If the curriculum steps are too large or the agent cannot learn the new objective, policy drift may still occur.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: The scheduling problem is modeled as an MDP, defining states, actions, rewards, and transitions for the RL agent.
  - Quick check question: What are the key components of an MDP, and how are they defined in this production scheduling problem?

- Concept: Curriculum learning
  - Why needed here: Curriculum learning is used to simplify the learning problem by gradually increasing task complexity, enabling the agent to learn effectively with smaller batch sizes.
  - Quick check question: How does curriculum learning help prevent policy drift when transitioning to more complex tasks?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is the RL algorithm used for training the agent, chosen for its robustness and hyperparameter sensitivity.
  - Quick check question: Why is PPO a suitable choice for this production scheduling problem compared to other RL algorithms?

## Architecture Onboarding

- Component map:
  MDP formulation -> Discrete-event simulator -> PPO agent -> Curriculum learning strategy -> Evaluation pipeline

- Critical path:
  1. Define MDP components based on production line constraints
  2. Train RL agent using PPO and curriculum learning
  3. Evaluate trained agent's performance on scheduling tasks
  4. Analyze results and adjust batch size or curriculum as needed

- Design tradeoffs:
  - Smaller batch sizes: Reduced setup effort, increased sample complexity, longer training times
  - Larger batch sizes: Reduced sample complexity, increased setup effort, potential loss of flexibility
  - Curriculum learning: Smoother transitions, longer training times, potential policy drift if not designed carefully

- Failure signatures:
  - Policy drift: Agent learns to reduce episode length or enter deadlocks to avoid penalties
  - Training failure: Agent cannot meet final criterion or generate zero-idle-time plans
  - High variance in setup effort: Agent discovers diverse policies with varying efficiency

- First 3 experiments:
  1. Train agents with different batch sizes (e.g., 10, 20, 30, 40, 50, 60, 70, 80, 90, 100) using Curriculum A to identify the optimal batch size and analyze training dynamics.
  2. Implement and train agents with Curriculum B and C for smaller batch sizes (e.g., 10, 12, 14, 16, 18, 20) to evaluate their effectiveness in preventing policy drift and reducing setup effort.
  3. Conduct sensitivity analysis on curriculum learning parameters (e.g., penalty increase steps, action mask aggressiveness) to optimize training performance and solution quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical explanation for why specific batch sizes (e.g., 40 and 70 in this study) show particularly low setup effort relative to sample complexity?
- Basis in paper: [inferred] The paper notes this pattern but does not explain it, stating "We suspect that this is related to the specific data set used. Further research is needed in this area to possibly identify these batch sizes a-priori."
- Why unresolved: The paper identifies an empirical pattern without establishing the underlying cause. The relationship between batch size, policy space, and setup effort optimization appears dataset-dependent.
- What evidence would resolve it: Systematic experiments across multiple datasets with varying problem characteristics, coupled with theoretical analysis of the interaction between batch size, policy space dimensionality, and setup effort optimization landscapes.

### Open Question 2
- Question: How can curriculum learning strategies be further optimized to prevent policy drift when training with very small batch sizes?
- Basis in paper: [explicit] The paper introduces two new curricula (B and C) but notes that "irreversible policy drifts occur in many cases at a certain point" despite smoother transitions.
- Why unresolved: The existing curricula reduce but do not eliminate the problem of policy drift during task transitions. The fundamental challenge of maintaining desired behavior while transitioning to setup effort optimization remains.
- What evidence would resolve it: Development and validation of curriculum strategies that successfully prevent policy drift across all batch sizes, potentially through adaptive penalty scheduling or alternative reward shaping approaches.

### Open Question 3
- Question: What is the generalizability of the optimal batch size findings to other production scheduling problems with different constraints and objectives?
- Basis in paper: [explicit] The authors state "We believe the results of our work may be applicable to similar scheduling problems, but additional research is required to validate this assumption."
- Why unresolved: The study focuses on a specific two-stage production system with particular constraints. The relationship between batch size, sample complexity, and setup effort may vary significantly with different problem structures.
- What evidence would resolve it: Empirical validation across diverse scheduling problems with varying numbers of stages, buffer constraints, and objective functions to establish generalizable principles for batch size selection.

## Limitations

- The optimal batch size relationship is highly problem-specific and may not generalize to different production configurations or scheduling objectives
- Curriculum learning strategies significantly increase training times, raising practical implementation concerns for real-world deployment
- The analysis focuses primarily on setup effort minimization without fully exploring trade-offs with other important objectives like production flexibility or maintenance scheduling

## Confidence

- **High confidence**: The observed relationship between batch size and sample complexity (Mechanism 2) is well-supported by training data and aligns with theoretical expectations about policy space growth.
- **Medium confidence**: The effectiveness of curriculum learning strategies (Mechanism 3) is demonstrated empirically, but results are sensitive to specific parameter choices and may not generalize across different problem domains.
- **Medium confidence**: The claim that smaller batch sizes reduce setup effort (Mechanism 1) is supported by results but depends on the specific reward function design and may not hold for alternative reward formulations.

## Next Checks

1. **Cross-configuration validation**: Test the optimal batch size identification process on production configurations with different numbers of product types (e.g., 4, 12, 16 types) and varying production line structures (parallel machines, job shops) to assess generalizability.

2. **Curriculum sensitivity analysis**: Systematically vary the penalty increase steps and action mask aggressiveness parameters across multiple problem instances to determine robust curriculum design principles and identify conditions where curriculum learning provides meaningful benefits versus standard training.

3. **Multi-objective extension**: Extend the analysis to include production flexibility metrics alongside setup effort, examining how batch size choices affect the Pareto-optimal frontier between efficiency and responsiveness to demand changes.