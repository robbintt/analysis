---
ver: rpa2
title: Incubating Text Classifiers Following User Instruction with Nothing but LLM
arxiv_id: '2404.10877'
source_url: https://arxiv.org/abs/2404.10877
tags:
- text
- incubator
- data
- label
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Incubator, a novel framework for generating
  training data for text classifiers based solely on user instructions. Incubator
  leverages a large language model (LLM) fine-tuned on instruction-to-data mappings
  from classification datasets and augmented with in-context learning.
---

# Incubating Text Classifiers Following User Instruction with Nothing but LLM

## Quick Facts
- **arXiv ID**: 2404.10877
- **Source URL**: https://arxiv.org/abs/2404.10877
- **Reference count**: 22
- **Primary result**: Introduces Incubator framework that generates training data for text classifiers from user instructions using a large language model

## Executive Summary
This paper introduces Incubator, a novel framework for generating training data for text classifiers based solely on user instructions. Incubator leverages a large language model (LLM) fine-tuned on instruction-to-data mappings from classification datasets and augmented with in-context learning. To improve data uniformity and diversity, Incubator employs a self-diversification technique using semantic text embeddings and clustering. Experiments demonstrate Incubator's ability to outperform strong baselines in traditional benchmarks, handle label interdependency and user preferences, and enable advanced text mining with logical conjunctions.

## Method Summary
The framework first collects 25 text classification datasets from HuggingFace, extracts descriptions and sample data formatted as Python dictionaries, then uses GPT-4 with in-context learning to augment these into 12K instruction-to-data pairs. This expanded dataset allows the LLM to learn how labels relate to each other and how to interpret user instructions holistically. The framework then generates 1024 samples per instruction, embeds them using E5 text embeddings, clusters the embeddings into 8 groups, and selects samples closest to each cluster center. These diverse samples are used to further instruction-tune the LLM, ensuring it generates more uniform and semantically diverse training data.

## Key Results
- Incubator outperforms strong baselines in traditional text classification benchmarks
- The framework successfully handles label interdependency and user preferences in classification tasks
- Incubator enables advanced text mining by combining multiple classifiers with logical conjunctions (AND, OR, NOT)
- The system achieves high precision in mining texts from unannotated corpora while showing robustness to instruction variations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Instruction-tuning with augmented datasets enables Incubator to capture label interdependency and user preferences
- **Mechanism**: The framework first collects 25 text classification datasets from HuggingFace, extracts descriptions and sample data formatted as Python dictionaries, then uses GPT-4 with in-context learning to augment these into 12K instruction-to-data pairs. This expanded dataset allows the LLM to learn how labels relate to each other and how to interpret user instructions holistically rather than treating each label independently.
- **Core assumption**: Large language models can effectively learn label interdependencies when trained on paired instruction-data samples that show complete classification tasks rather than isolated labels
- **Evidence anchors**:
  - [abstract]: "Our proposed Incubator is the first framework that can handle complicated and even mutually dependent classes"
  - [section 3.1]: "These data are beneficial for Incubator to learn label interdependency as the examples from different classes are presented jointly"
- **Break condition**: If the augmented dataset doesn't sufficiently represent complex label relationships, or if the LLM cannot generalize from the instruction-to-data mapping to unseen classification tasks

### Mechanism 2
- **Claim**: Self-diversification using semantic clustering improves generation uniformity and diversity
- **Mechanism**: The framework generates 1024 samples per instruction, embeds them using E5 text embeddings, clusters the embeddings into 8 groups, then selects samples closest to each cluster center. These diverse samples are used to further instruction-tune the LLM, ensuring it generates more uniform and semantically diverse training data.
- **Core assumption**: Clustering generated samples by semantic embeddings and using cluster centers as representatives will produce a more diverse and uniform training dataset
- **Evidence anchors**:
  - [abstract]: "We then refine Incubator by learning on the cluster centers of semantic textual embeddings to emphasize the uniformity and semantic diversity in generations"
  - [section 3.2]: "We incorporate these data in a batch of K and further instruction-tune the LLM on it"
- **Break condition**: If the clustering algorithm fails to capture true semantic diversity, or if the cluster centers don't represent the most diverse samples

### Mechanism 3
- **Claim**: Combining multiple incubated classifiers with logical conjunctions enables advanced text mining
- **Mechanism**: The framework incubates separate classifiers for individual labels, then combines their probability scores using logical operations (AND, OR, NOT) to create more complex classification queries. This allows users to mine texts based on compound criteria like "Positive and about food" or "Positive but not excited."
- **Core assumption**: Individual classifiers can be treated as independent probability distributions that can be combined using basic logical operations
- **Evidence anchors**:
  - [abstract]: "Experiments show Incubator is able to ... enable logical text mining by incubating multiple classifiers"
  - [section 4.7]: "We utilize the maneuverability of Incubator to incubate multiple text miners and combine their scores with logical probabilistic calculations"
- **Break condition**: If the independence assumption between classifiers is violated, or if the logical combinations don't accurately reflect the intended compound criteria

## Foundational Learning

- **Concept: Large Language Model Fine-tuning**
  - **Why needed here**: The framework relies on fine-tuning LLaMA-2 to create the Incubator, which requires understanding how LLMs learn from instruction-data pairs and how to optimize the fine-tuning process
  - **Quick check question**: What optimizer and learning rate scheduler are used for instruction-tuning the LLM?

- **Concept: Text Embeddings and Semantic Similarity**
  - **Why needed here**: The self-diversification technique uses E5 text embeddings to calculate semantic similarity between generated samples, which is crucial for the clustering approach
  - **Quick check question**: How are the generated Python dictionaries converted into embeddings for clustering?

- **Concept: Classification Metrics and Evaluation**
  - **Why needed here**: The framework evaluates performance using various metrics across different datasets, requiring understanding of classification evaluation methods
  - **Quick check question**: What evaluation metric is used for the text mining tasks with complicated class definitions?

## Architecture Onboarding

- **Component map**: Dataset collection → GPT-4 augmentation → LLM instruction-tuning → Self-diversification fine-tuning → Training data generation → Small classifier fine-tuning
- **Critical path**: Dataset collection → GPT-4 augmentation → LLM instruction-tuning → Self-diversification fine-tuning → Training data generation → Small classifier fine-tuning
- **Design tradeoffs**: Using open-source LLaMA-2 vs proprietary GPT-4, generating 1024 samples per instruction vs fewer samples, 8 clusters vs different numbers in self-diversification
- **Failure signatures**: Poor performance on label interdependency tasks, lack of diversity in generated training data, classifier performance degradation with increased logical complexity
- **First 3 experiments**:
  1. Test instruction-tuning without self-diversification on traditional benchmarks to establish baseline performance
  2. Test self-diversification alone by comparing generated sample diversity before and after clustering
  3. Test logical conjunction capability by incubating classifiers for individual labels and combining them on a simple compound task

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of Incubator compare when using different backbone LLMs (e.g., GPT-4 vs. LLaMA-2) for instruction tuning?
- **Basis in paper**: [explicit] The paper states that "Incubator w/ GPT-4: This is a variant of our Incubator that prompts GPT-4 with in-context examples from the Huggingface platform and the instruction to sample the training data."
- **Why unresolved**: While the paper mentions that Incubator w/ GPT-4 outperforms the one based on LLaMA-2, it doesn't provide a detailed comparison of the performance differences between different backbone LLMs.
- **What evidence would resolve it**: A comprehensive comparison of Incubator's performance using various backbone LLMs (e.g., GPT-4, LLaMA-2, BLOOM) on the same benchmark datasets.

### Open Question 2
- **Question**: How does the performance of Incubator change when using different clustering algorithms and parameters in the self-diversification stage?
- **Basis in paper**: [explicit] The paper mentions that "We run a K-means (We select K = 8) clustering algorithm on the embeddings and find the K samples with embeddings that are closest to the cluster centers."
- **Why unresolved**: The paper only tests K-means with K=8 for clustering, but it's unclear how other clustering algorithms (e.g., DBSCAN, hierarchical clustering) or different K values would affect Incubator's performance.
- **What evidence would resolve it**: An ablation study comparing Incubator's performance using different clustering algorithms and K values in the self-diversification stage.

### Open Question 3
- **Question**: How does the performance of Incubator scale with the size of the instruction-tuning dataset?
- **Basis in paper**: [explicit] The paper mentions that "We select 25 text classification datasets on the Huggingface platform to build the initial instruction-to-data dataset for instruction-tuning, such as financial news, counterfactual reviews, and toxic conversations."
- **Why unresolved**: The paper doesn't explore how the size of the instruction-tuning dataset affects Incubator's performance. It's unclear if there's a point of diminishing returns or if more data always leads to better performance.
- **What evidence would resolve it**: An analysis of Incubator's performance as the size of the instruction-tuning dataset increases, identifying any trends or optimal dataset sizes.

## Limitations
- The reliance on GPT-4 for data augmentation introduces potential reproducibility challenges and costs
- The clustering-based self-diversification technique lacks detailed implementation specifications and validation
- The paper doesn't address potential domain shift issues when generating data for concepts not well-represented in the training datasets

## Confidence
- **Mechanism 1 (Label Interdependency Learning)**: Medium - The augmented dataset approach is well-justified, but the actual effectiveness of learning complex label relationships from instruction-to-data pairs remains partially demonstrated
- **Mechanism 2 (Self-Diversification)**: Low - The clustering approach is described but lacks rigorous evaluation of whether it actually improves semantic diversity or just creates apparent variety
- **Mechanism 3 (Logical Conjunction Capability)**: Medium - While the logical operations are straightforward, the paper doesn't thoroughly validate whether the independence assumptions hold in practice or how performance degrades with increasing logical complexity

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary the number of clusters (3-15) in the self-diversification process and measure the impact on both diversity metrics and downstream classifier performance to determine optimal settings
2. **Cross-Domain Transfer Test**: Evaluate Incubator on classification tasks from domains not represented in the original 25 datasets (e.g., medical or legal text) to assess generalization beyond the training distribution
3. **Ablation Study on Label Dependency**: Compare Incubator's performance on tasks with known label interdependencies against a baseline that treats each label independently, using established datasets like MultiNLI or E2E-NLG to quantify the actual benefit of learning label relationships