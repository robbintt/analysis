---
ver: rpa2
title: What matters when building vision-language models?
arxiv_id: '2405.02246'
source_url: https://arxiv.org/abs/2405.02246
tags:
- language
- vision
- image
- conference
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper rigorously compares design choices in vision-language
  model (VLM) development through controlled experiments. It finds that the quality
  of the language model backbone has the highest impact on final VLM performance,
  and that the fully autoregressive architecture outperforms cross-attention when
  pre-trained backbones are fine-tuned using LoRA.
---

# What matters when building vision-language models?

## Quick Facts
- arXiv ID: 2405.02246
- Source URL: https://arxiv.org/abs/2405.02246
- Reference count: 40
- This paper rigorously compares design choices in VLM development and trains Idefics2, achieving state-of-the-art performance in its size category while being more efficient than larger models.

## Executive Summary
This paper systematically evaluates key design choices in vision-language model development through controlled experiments. The authors find that language model backbone quality has the greatest impact on VLM performance, and that fully autoregressive architecture with LoRA fine-tuning outperforms cross-attention approaches. They also identify efficiency optimizations including learned pooling for visual tokens and preserving original image aspect ratios. Based on these findings, they develop Idefics2, an 8-billion parameter VLM that achieves state-of-the-art performance while being more efficient than larger models.

## Method Summary
The study conducts controlled experiments comparing different VLM design choices including architecture types (cross-attention vs fully autoregressive), training methods (frozen vs fine-tuned backbones with LoRA), and efficiency optimizations (learned pooling, aspect ratio preservation). The authors train base models on OBELICS dataset and fine-tune on The Cauldron dataset mixture, evaluating on VQAv2, TextVQA, OKVQA, and COCO benchmarks. Idefics2 is built using SigLIP-SO400M vision backbone, Mistral-7B language model, and Perceiver resampler for learned pooling.

## Key Results
- Language model backbone quality has the highest impact on VLM performance, with 5.1-point improvement from Mistral-7B vs LLaMA-1-7B
- Fully autoregressive architecture with LoRA fine-tuning outperforms cross-attention, showing 12.9-point improvement
- Learned pooling to 64 visual tokens improves efficiency while maintaining performance
- Idefics2 achieves state-of-the-art results in 8B parameter category, matching models 4x larger on several tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Better pre-trained language model backbones yield larger performance gains than better vision backbones in VLMs.
- Mechanism: Language models provide primary reasoning and generation capability; vision backbones only provide input encoding. Higher-quality LM backbone directly improves interpretation, reasoning, and generation.
- Core assumption: Language model's reasoning ability is dominant factor in VLM performance across diverse tasks.
- Evidence anchors: Abstract states "quality of language model backbone has highest impact"; section 3.1 shows 5.1-point boost from Mistral-7B; related work focuses on architecture rather than backbone comparisons.
- Break condition: If VLM tasks shift to primarily vision-dependent reasoning without complex language generation.

### Mechanism 2
- Claim: Fully autoregressive architecture with LoRA training outperforms cross-attention architecture.
- Mechanism: Fully autoregressive allows end-to-end training with language model-like token prediction, enabling better modality fusion. LoRA enables stable fine-tuning of large pre-trained backbones while maintaining parameter efficiency.
- Core assumption: Training flexibility and end-to-end optimization are more important than cross-attention's parameter efficiency for final VLM performance.
- Evidence anchors: Abstract states "fully autoregressive architecture outperforms cross-attention when pre-trained backbones are fine-tuned using LoRA"; section 3.2 shows 12.9-point increase and outperforms despite fewer parameters.
- Break condition: If training stability issues with full fine-tuning cannot be resolved, or if task-specific architectures prove superior.

### Mechanism 3
- Claim: Learned pooling to reduce visual tokens and preserving original image aspect ratios improves VLM efficiency without sacrificing performance.
- Mechanism: Learned pooling with Perceiver resampler reduces computational load by decreasing visual tokens while maintaining task-relevant information. Preserving aspect ratios avoids distortion and allows flexible resolution handling, reducing preprocessing costs.
- Core assumption: Reducing visual tokens and avoiding resizing does not significantly impact model's ability to capture essential visual information.
- Evidence anchors: Abstract states "Efficiency gains come from learned pooling to reduce visual tokens and preserving original image aspect ratios"; section 3.3 shows significant compute efficiency improvements while improving performance.
- Break condition: If tasks require high-resolution details lost through pooling, or if models fail to adapt to varying aspect ratios.

## Foundational Learning

- Concept: Multimodal learning and cross-modal attention mechanisms
  - Why needed here: Understanding how vision and language modalities are fused is fundamental to VLM architecture design and performance.
  - Quick check question: What are the key differences between cross-attention and fully autoregressive architectures in VLMs, and how do they affect training and inference?

- Concept: Transfer learning and fine-tuning strategies
  - Why needed here: VLMs rely heavily on pre-trained backbones, and choice of fine-tuning method (full vs LoRA) significantly impacts stability and performance.
  - Quick check question: How does LoRA differ from full fine-tuning, and what are the trade-offs in terms of performance, stability, and computational cost?

- Concept: Data efficiency and dataset composition
  - Why needed here: Paper emphasizes importance of training data quality and diversity, including interleaved documents, image-text pairs, and OCR data.
  - Quick check question: How does composition and quality of training data influence VLM performance across different task types?

## Architecture Onboarding

- Component map: Vision backbone (SigLIP) → Modality projection → Perceiver resampler (learned pooling) → Concatenation with text tokens → LLM (Mistral) → Output generation

- Critical path: 1) Image encoding through vision backbone 2) Visual feature projection to LLM space 3) Learned pooling to reduce visual tokens 4) Concatenation with text embeddings 5) LLM processing and output generation

- Design tradeoffs:
  - Cross-attention vs fully autoregressive: Parameter count, training stability, performance
  - Visual token count: Computational cost vs information retention
  - Aspect ratio preservation: Preprocessing cost vs visual fidelity
  - Fine-tuning strategy: LoRA (efficient, stable) vs full fine-tuning (potentially better performance, less stable)

- Failure signatures:
  - Training divergence with fully autoregressive architecture (solved by LoRA)
  - Performance degradation with aggressive pooling or aspect ratio changes
  - Inefficiency with high visual token counts or fixed resizing

- First 3 experiments:
  1. Compare cross-attention vs fully autoregressive architectures with frozen vs fine-tuned backbones
  2. Ablate visual token count and pooling strategy impact on performance and efficiency
  3. Test aspect ratio preservation vs fixed resizing on downstream task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does quality of language model backbone compare to quality of vision backbone in determining final VLM performance?
- Basis in paper: Explicit - paper states "quality of language model backbone has higher impact on performance of final VLM than quality of vision backbone"
- Why unresolved: Paper provides experimental evidence but doesn't explore underlying reasons or magnitude across different architectures and tasks
- What evidence would resolve it: Comparative experiments varying both backbone qualities across multiple VLM architectures and tasks, with detailed analysis of relationship between backbone quality and downstream performance

### Open Question 2
- Question: What is optimal number of visual tokens per image for maximizing VLM performance and efficiency?
- Basis in paper: Explicit - paper finds 64 visual tokens optimal but doesn't explore full range or theoretical reasons
- Why unresolved: Paper only tests few values (64, 128, 729) without theoretical framework for understanding relationship between visual token count and performance
- What evidence would resolve it: Systematic experiments varying visual token count across wider range, combined with theoretical analysis of information content and computational efficiency

### Open Question 3
- Question: How does cross-attention architecture compare to fully autoregressive architecture in terms of performance, parameter count, and inference cost?
- Basis in paper: Explicit - paper compares these architectures and finds cross-attention performs better when backbones are frozen, but fully autoregressive outperforms when backbones are trained using LoRA
- Why unresolved: Paper doesn't explore full trade-offs including parameter count, inference cost, and potential hybrid approaches
- What evidence would resolve it: Comprehensive comparison across multiple dimensions (performance, parameters, inference cost) using consistent evaluation methods and potentially exploring hybrid approaches

## Limitations

- Architecture generalizability: Results may not directly translate to different model sizes, backbone combinations, or specialized VLM variants for specific domains
- Data composition bias: Findings about design choices may be influenced by dataset-specific characteristics rather than being universally applicable
- Efficiency metrics incompleteness: Lacks comprehensive measurements of training memory consumption, inference latency across hardware, and energy efficiency metrics

## Confidence

- High confidence: Language model backbone quality has highest impact on VLM performance (5.1-point improvement from Mistral-7B vs LLaMA-1-7B)
- Medium confidence: Superiority of fully autoregressive architecture with LoRA over cross-attention
- Medium confidence: Efficiency gains from learned pooling and aspect ratio preservation

## Next Checks

1. Replicate cross-attention vs fully autoregressive comparison across different model scales (3B, 13B parameters) and backbone combinations to test robustness of architecture findings

2. Conduct comprehensive efficiency measurements including training memory usage, inference latency on different hardware (GPU, CPU, mobile), and energy consumption to validate claimed efficiency gains

3. Compare LoRA fine-tuning with full fine-tuning and other parameter-efficient methods (prefix tuning, adapters) across same architecture to quantify trade-off between stability and maximum achievable performance