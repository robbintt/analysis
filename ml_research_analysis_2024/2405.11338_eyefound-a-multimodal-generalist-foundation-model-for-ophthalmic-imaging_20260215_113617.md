---
ver: rpa2
title: 'EyeFound: A Multimodal Generalist Foundation Model for Ophthalmic Imaging'
arxiv_id: '2405.11338'
source_url: https://arxiv.org/abs/2405.11338
tags: []
core_contribution: EyeFound is a multimodal foundation model for ophthalmic imaging
  that learns generalizable representations from unlabeled multimodal retinal images.
  Trained on 2.78 million images from 227 hospitals across 11 ophthalmic modalities,
  EyeFound enables efficient adaptation across multiple applications.
---

# EyeFound: A Multimodal Generalist Foundation Model for Ophthalmic Imaging

## Quick Facts
- arXiv ID: 2405.11338
- Source URL: https://arxiv.org/abs/2405.11338
- Authors: Danli Shi, Weiyi Zhang, Xiaolan Chen, Yexin Liu, Jiancheng Yang, Siyu Huang, Yih Chung Tham, Yingfeng Zheng, Mingguang He
- Reference count: 40
- Key outcome: EyeFound is a multimodal foundation model for ophthalmic imaging that learns generalizable representations from unlabeled multimodal retinal images. Trained on 2.78 million images from 227 hospitals across 11 ophthalmic modalities, EyeFound enables efficient adaptation across multiple applications. It outperforms previous work RETFound in diagnosing eye diseases, predicting systemic disease incidents, and zero-shot multimodal visual question answering.

## Executive Summary
EyeFound is a multimodal foundation model for ophthalmic imaging that learns generalizable representations from unlabeled multimodal retinal images. The model is trained on a massive dataset of 2.78 million images from 227 hospitals across China, covering 11 different ophthalmic imaging modalities. EyeFound demonstrates superior performance compared to previous work (RETFound) in various downstream tasks, including disease diagnosis, systemic disease prediction, and zero-shot multimodal visual question answering. The model provides a generalizable solution to improve model performance and reduce the annotation burden on experts, facilitating widespread clinical AI applications for retinal imaging.

## Method Summary
EyeFound uses a masked autoencoder (MAE) architecture with a ViT-large encoder and ViT-small decoder. The model is trained on 2.78 million unlabeled ophthalmic images from 227 hospitals across 11 modalities using an 80% mask ratio. During pretraining, the model learns to reconstruct masked regions, forcing it to capture cross-modal anatomical and pathological patterns. For downstream tasks, EyeFound can be fine-tuned with either an MLP classifier for classification tasks or concatenated with Vicuna for visual question answering. The model demonstrates faster convergence and superior performance compared to RETFound and self-supervised ImageNet pretraining.

## Key Results
- EyeFound outperforms RETFound in diagnosing eye diseases, predicting systemic disease incidents, and zero-shot multimodal visual question answering
- The model learns to reconstruct masked regions with an 80% masking ratio, demonstrating robust semantic feature learning
- EyeFound achieves faster convergence compared to RETFound and self-supervised ImageNet pretraining
- The model shows strong performance across multiple ophthalmic imaging modalities, enabling efficient adaptation to various clinical applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EyeFound's joint multimodal pretraining allows the model to learn shared representations across different ophthalmic imaging modalities, improving generalization.
- Mechanism: By training on 11 ophthalmic modalities simultaneously using a masked autoencoder, EyeFound learns to reconstruct masked image regions, forcing the model to capture cross-modal anatomical and pathological patterns rather than modality-specific artifacts.
- Core assumption: The anatomical structures relevant to ophthalmic diseases appear consistently across modalities, enabling shared feature learning.
- Evidence anchors:
  - [abstract] "EyeFound learns generalizable representations from unlabeled multimodal retinal images"
  - [section] "EyeFound performs comparably with RETFound for most datasets... occasionally outperforming it"
  - [corpus] Weak evidence - corpus neighbors focus on VQA and multimodal models but lack direct comparison of joint vs. separate modality training
- Break condition: If anatomical structures are modality-specific or if modality differences are too large to allow meaningful shared representation.

### Mechanism 2
- Claim: The large-scale, multi-hospital dataset provides sufficient diversity to learn robust, generalizable representations that transfer well to downstream tasks.
- Mechanism: Training on 2.78 million images from 227 hospitals across China exposes the model to diverse patient populations, equipment types, and disease presentations, reducing overfitting to any specific distribution.
- Core assumption: The pretraining dataset captures sufficient variation in disease presentation and imaging conditions to generalize across different clinical settings.
- Evidence anchors:
  - [abstract] "Trained on 2.78 million images from 227 hospitals across 11 ophthalmic modalities"
  - [section] "EyeFound generally converges faster than RETFound and self-supervised ImageNet, requiring fewer epochs"
  - [corpus] Weak evidence - corpus lacks studies on transfer performance from large-scale pretraining to diverse downstream tasks
- Break condition: If the pretraining data distribution is too narrow or biased, leading to poor performance on underrepresented populations or rare conditions.

### Mechanism 3
- Claim: The generative pretraining objective (masked reconstruction) forces the model to learn high-level semantic features rather than low-level pixel patterns.
- Mechanism: The 80% masking ratio requires the model to infer missing information based on context, encouraging learning of object parts, disease lesions, and anatomical relationships rather than memorizing local textures.
- Core assumption: The masking strategy and reconstruction objective effectively push the model to learn meaningful semantic representations rather than superficial patterns.
- Evidence anchors:
  - [abstract] "The model learns data distributions by predicting masked regions"
  - [section] "EyeFound learns to reconstruct the masking regions despite a large masking ratio of 0.8"
  - [corpus] Weak evidence - corpus focuses on VQA applications but lacks direct analysis of pretraining objective effectiveness
- Break condition: If the masking ratio is too high, preventing meaningful reconstruction, or if the model learns shortcuts that don't generalize.

## Foundational Learning

- Concept: Masked autoencoder pretraining
  - Why needed here: Enables learning from unlabeled data, addressing the annotation bottleneck in medical imaging while forcing the model to learn semantic representations
  - Quick check question: Does the model maintain reasonable reconstruction quality when 80% of patches are masked?

- Concept: Multimodal representation learning
  - Why needed here: Different ophthalmic modalities provide complementary information about eye structure and pathology, but training separate models for each modality misses potential synergies
  - Quick check question: Does performance improve on multimodal downstream tasks compared to modality-specific models?

- Concept: Transfer learning with foundation models
  - Why needed here: Pretraining on massive unlabeled datasets creates representations that can be efficiently adapted to specific tasks with limited labeled data
  - Quick check question: Does the model achieve good performance on downstream tasks with fewer fine-tuning epochs compared to models trained from scratch?

## Architecture Onboarding

- Component map:
  - Image preprocessing and augmentation -> Encoder (ViT-large) -> Decoder (ViT-small) -> Reconstruction loss (pretraining) -> MLP classifier or Vicuna (fine-tuning)

- Critical path:
  1. Image preprocessing and augmentation
  2. Encoder feature extraction
  3. Decoder reconstruction (pretraining only)
  4. Task-specific adaptation (fine-tuning)

- Design tradeoffs:
  - Large encoder vs. smaller decoder balances representation capacity with computational efficiency
  - High masking ratio (80%) forces semantic learning but may make reconstruction harder
  - Joint multimodal training vs. separate modality training trades potential modality interference for shared representation benefits

- Failure signatures:
  - Poor reconstruction quality indicates encoder isn't capturing meaningful features
  - Slow convergence or poor downstream performance suggests pretraining didn't learn generalizable representations
  - Large performance gap between modalities indicates joint training isn't beneficial

- First 3 experiments:
  1. Ablation study: Train separate RETFound-style models for each modality and compare to joint EyeFound on multimodal downstream tasks
  2. Masking ratio sensitivity: Vary masking ratio (60%, 80%, 90%) and measure reconstruction quality and downstream task performance
  3. Fine-tuning efficiency: Compare number of epochs needed to reach target performance on a downstream task between EyeFound, RETFound, and ImageNet-pretrained models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EyeFound's performance generalize to ophthalmic datasets from regions outside China, particularly for rare or unseen conditions?
- Basis in paper: [explicit] The authors acknowledge that EyeFound was trained on a large-scale dataset from China and note that incorporating data from other regions could enhance its diversity and representativeness. They also mention that the model's ability to capture all variations in ocular diseases and conditions may be limited, potentially affecting its generalizability to rare or unseen conditions.
- Why unresolved: The paper does not provide empirical evidence of EyeFound's performance on datasets from regions outside China or specifically address its ability to handle rare or unseen conditions.
- What evidence would resolve it: Testing EyeFound on diverse, international ophthalmic datasets, particularly those containing rare conditions, and comparing its performance to that of other models or to its performance on Chinese datasets would provide evidence of its generalizability.

### Open Question 2
- Question: What is the optimal way to combine and interpret information from different modalities in EyeFound?
- Basis in paper: [explicit] The authors state that while EyeFound has made strides in integrating multiple modalities into a single model, determining the optimal way to combine and interpret information from different modalities remains an open question for further exploration.
- Why unresolved: The paper does not provide a definitive answer on the best method for modality combination and interpretation in EyeFound. It only demonstrates that combining modalities improves performance compared to single-modality models.
- What evidence would resolve it: Systematic experiments comparing different methods of modality combination and interpretation in EyeFound, such as early fusion, late fusion, or attention mechanisms, and evaluating their impact on downstream task performance would help identify the optimal approach.

### Open Question 3
- Question: How can text labels or descriptions be effectively incorporated into EyeFound's pretraining process to maximize the use of all available data?
- Basis in paper: [explicit] The authors note that the current model focuses solely on image-based learning and does not fully exploit text labels or descriptions during pretraining, which could mean missing out on valuable information. They suggest that future research could investigate methods that combine self-supervised and supervised pretraining.
- Why unresolved: The paper does not explore methods for incorporating text information into EyeFound's pretraining process or demonstrate the potential benefits of doing so.
- What evidence would resolve it: Developing and testing variants of EyeFound that incorporate text information during pretraining, such as through contrastive language-image pretraining, and comparing their performance to the original model on downstream tasks would provide evidence of the benefits of incorporating text information.

## Limitations
- Dataset diversity limitations: The pretraining dataset, while large, comes from hospitals primarily in China, raising questions about performance on international populations
- Generalization across disease types: The paper focuses on common eye diseases and systemic conditions but doesn't evaluate performance on rare diseases or atypical presentations
- Modality-specific performance variations: The paper doesn't provide detailed analysis of how performance varies across the 11 different modalities

## Confidence
- Claims about superior performance: Medium confidence - incremental improvements over RETFound with limited comparative analysis
- Scalability claims: Medium confidence - large-scale pretraining appears effective but dataset composition details are lacking
- Generalizability across modalities: Medium confidence - joint training shows promise but ablation studies are missing
- Transfer learning efficiency: Medium confidence - faster convergence stated but specific metrics not provided

## Next Checks
1. **Ablation study on modality-specific vs. joint training**: Train separate models for each modality and compare their performance on multimodal downstream tasks against EyeFound to quantify the benefit of joint training.

2. **Cross-institutional validation**: Test EyeFound on ophthalmic datasets from hospitals outside China to assess generalizability across different populations, equipment, and clinical practices.

3. **Rare disease performance analysis**: Evaluate EyeFound's performance on datasets containing rare ophthalmic conditions to determine if the large-scale pretraining effectively captures these underrepresented disease patterns.