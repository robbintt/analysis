---
ver: rpa2
title: Using text embedding models as text classifiers with medical data
arxiv_id: '2402.16886'
source_url: https://arxiv.org/abs/2402.16886
tags:
- data
- vector
- text
- embedding
- database
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored using text embedding models and vector databases
  to classify medical text data without training a new model. The authors generated
  medical data using LLMs (gpt-3.5-turbo, LLaMA 2 70b-chat, and flan-t5-xl), embedded
  it with text embedding models (text-embedding-ada-002 and textembedding-gecko@001),
  and stored it in a vector database.
---

# Using text embedding models as text classifiers with medical data

## Quick Facts
- arXiv ID: 2402.16886
- Source URL: https://arxiv.org/abs/2402.16886
- Reference count: 13
- Primary result: Sparse query data with detailed knowledge base yielded optimal results, with misclassification rate as low as 3.63% and macro F1 score of 0.96

## Executive Summary
This study explored using text embedding models and vector databases to classify medical text data without training a new model. The authors generated synthetic medical data using LLMs, embedded it with text embedding models, and stored it in a vector database. They then queried the database with medical notes and compared the results to ground truth data. The approach achieved high classification accuracy while avoiding the need for clinicians to interact with LLMs directly.

## Method Summary
The study used synthetic medical data generated by LLMs (gpt-3.5-turbo, LLaMA 2 70b-chat, and flan-t5-xl) which was embedded using text embedding models (text-embedding-ada-002 and textembedding-gecko@001) and stored in a vector database. The authors then queried this database with medical notes and compared the results against ground truth data. The method focused on leveraging pre-trained embedding models and vector databases to perform classification tasks without the need for training new models.

## Key Results
- Higher embedding dimensions (1536 vs 768) led to better classification performance
- Sparse query data with detailed knowledge base yielded optimal results
- Misclassification rate as low as 3.63% with macro F1 score of 0.96

## Why This Works (Mechanism)
The approach works by leveraging the semantic understanding captured in text embedding models to represent medical text in a high-dimensional vector space. When these embeddings are stored in a vector database, similarity searches can effectively retrieve the most relevant medical categories or classifications. The use of sparse query data with a detailed knowledge base optimizes this process by reducing noise and focusing on the most relevant features for classification.

## Foundational Learning
- Text embedding models convert text into numerical vectors that capture semantic meaning
  - Why needed: To represent text in a format that enables similarity-based retrieval
  - Quick check: Verify embeddings preserve semantic relationships through nearest neighbor tests

- Vector databases enable efficient similarity search in high-dimensional spaces
  - Why needed: To quickly retrieve the most relevant stored embeddings based on query similarity
  - Quick check: Measure retrieval latency and accuracy with benchmark queries

- Sparse representations reduce dimensionality while preserving key information
  - Why needed: To minimize noise and computational overhead during classification
  - Quick check: Compare classification performance with dense vs sparse query representations

## Architecture Onboarding

Component map: LLM-generated data -> Text embedding model -> Vector database -> Query interface -> Classification output

Critical path: Medical note input -> Embedding generation -> Vector similarity search -> Classification result

Design tradeoffs: The study prioritized classification accuracy over computational efficiency by using higher-dimensional embeddings (1536 vs 768), which improved performance but increased storage and computation requirements.

Failure signatures: Poor classification results may indicate insufficient semantic capture in embeddings, inadequate knowledge base content, or inappropriate query formulation.

First experiments:
1. Test embedding quality by checking semantic similarity preservation
2. Validate vector database retrieval accuracy with known query-answer pairs
3. Compare classification performance across different embedding dimensions

## Open Questions the Paper Calls Out
None

## Limitations
- Use of synthetic medical data may not accurately represent real clinical scenarios
- Small, curated dataset limits generalizability to broader medical applications
- Evaluation focused primarily on classification accuracy without assessing clinical relevance or potential biases

## Confidence
- High: Higher embedding dimensions (1536 vs 768) improve classification performance
- Medium: Sparse query data with detailed knowledge base yields optimal results
- Low: Approach can be scaled up for broader medical applications without further validation

## Next Checks
1. Test the approach with real clinical notes from multiple institutions to assess generalizability and identify potential biases
2. Evaluate the system's performance in a clinical decision support context, measuring its impact on diagnostic accuracy and workflow efficiency
3. Conduct a head-to-head comparison with traditional supervised learning methods using the same medical datasets to benchmark relative performance