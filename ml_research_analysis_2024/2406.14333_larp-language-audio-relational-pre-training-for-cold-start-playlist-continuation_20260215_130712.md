---
ver: rpa2
title: 'LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation'
arxiv_id: '2406.14333'
source_url: https://arxiv.org/abs/2406.14333
tags:
- playlist
- loss
- tracks
- cold-start
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LARP, a multi-modal cold-start playlist
  continuation model, to address the cold-start problem in music recommendation. LARP
  is a three-stage contrastive learning framework that integrates both multi-modal
  and relational signals into its learned representations, using increasing stages
  of task-specific abstraction: within-track (language-audio) contrastive loss, track-track
  contrastive loss, and track-playlist contrastive loss.'
---

# LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation

## Quick Facts
- **arXiv ID**: 2406.14333
- **Source URL**: https://arxiv.org/abs/2406.14333
- **Authors**: Rebecca Salganik; Xiaohao Liu; Yunshan Ma; Jian Kang; Tat-Seng Chua
- **Reference count**: 40
- **Primary result**: LARP achieves Recall@10 of 0.0406 on Million Playlist Dataset and 0.0137 on Last-FM dataset for cold-start playlist continuation

## Executive Summary
This paper introduces LARP, a multi-modal cold-start playlist continuation model that addresses the cold-start problem in music recommendation through a three-stage contrastive learning framework. LARP integrates both multi-modal and relational signals into its learned representations using progressive task-specific abstraction: within-track (language-audio) contrastive loss, track-track contrastive loss, and track-playlist contrastive loss. Experimental results demonstrate LARP's efficacy over uni-modal and multi-modal models for playlist continuation in cold-start settings, achieving state-of-the-art performance on two publicly available datasets.

## Method Summary
LARP employs a three-stage contrastive learning framework for cold-start playlist continuation. It uses pre-trained HT-SAT (audio) and BERT (text) encoders to extract representations, then aligns these representations through progressive contrastive losses: WTC aligns audio and text within tracks, TTC aligns representations between tracks in the same playlists, and TPC aligns track representations with playlist-level representations using a self-attention fusion layer. The model uses a momentum queue for negative sampling and transfers learned weights between stages. For playlist continuation, track representations are aggregated into playlist embeddings and integrated with ItemKNN, DropoutNet, and CLCRec for final recommendations.

## Key Results
- LARP achieves Recall@10 of 0.0406 on Million Playlist Dataset and 0.0137 on Last-FM dataset
- Outperforms state-of-the-art methods on both datasets in cold-start playlist continuation
- Demonstrates effectiveness of multi-stage contrastive learning with increasing abstraction levels
- Shows self-attention fusion layer effectively handles playlist heterogeneity

## Why This Works (Mechanism)

### Mechanism 1
Relational pre-training directly improves cold-start performance by encoding playlist-track relationships into content representations. The three-stage contrastive learning framework progressively incorporates relational signals at increasing levels of abstraction, allowing the model to learn both cross-modal alignment and relational awareness simultaneously. Core assumption: High-quality content representations that encode relational information can substitute for collaborative filtering signals in cold-start settings. Evidence anchor: The paper demonstrates LARP's superior performance over methods that don't explicitly encode relational signals.

### Mechanism 2
Multi-stage contrastive learning with increasing abstraction levels enables effective cross-modal alignment while preserving relational structure. Within-track contrast aligns audio and text for individual tracks, track-track contrast aligns representations between tracks sharing playlists, and track-playlist contrast aligns track representations with playlist-level representations. Core assumption: Progressive refinement through staged training produces better representations than single-stage contrastive learning. Evidence anchor: LARP's staged training approach shows consistent performance improvements over ablations that skip stages.

### Mechanism 3
Self-attention fusion layer effectively handles playlist heterogeneity by weighting tracks based on their relevance to the playlist theme. The TPC-fusion module uses self-attention to dynamically weight track contributions when forming playlist representations, mitigating the impact of outlier tracks. Core assumption: Playlist tracks are not uniformly relevant to the playlist theme, and some tracks may be outliers that should be down-weighted. Evidence anchor: The paper shows that simple averaging can mislead optimization, while self-attention provides more robust playlist representations.

## Foundational Learning

- **Concept: Contrastive learning with temperature scaling and negative sampling**
  - Why needed here: LARP relies on contrastive losses at multiple stages to align representations across modalities and between related items
  - Quick check question: How does the temperature parameter τ affect the contrastive loss function and what happens when τ is too high or too low?

- **Concept: Multi-modal representation alignment**
  - Why needed here: LARP must align audio and text representations for the same track while preserving their individual characteristics
  - Quick check question: What are the potential failure modes when aligning audio and text representations, and how does LARP address them?

- **Concept: Self-attention mechanisms for set representation**
  - Why needed here: The TPC-fusion module uses self-attention to aggregate track representations into playlist representations while handling heterogeneity
  - Quick check question: How does self-attention differ from simple pooling when aggregating a set of representations, and what are the advantages in this context?

## Architecture Onboarding

- **Component map**: Audio encoder (HTS-AT) → Text encoder (BERT) → Cross-modal alignment layers → Momentum encoder queue → Three-stage contrastive loss modules → TPC-fusion self-attention layer → Track representation lookup tables
- **Critical path**: Audio/Text input → Uni-modal encoders → Cross-modal alignment → Contrastive losses (WTC → TTC → TPC) → Track representations → Playlist continuation
- **Design tradeoffs**: Using frozen pre-trained encoders (HTS-AT, BERT) versus fine-tuning them end-to-end; using momentum encoder for efficiency versus storing all representations
- **Failure signatures**: 
  - Poor cross-modal alignment: Audio and text embeddings for the same track are not close in representation space
  - Ineffective relational learning: Track representations don't cluster by playlist membership
  - Overfitting to training data: Poor generalization to held-out playlists
- **First 3 experiments**:
  1. Verify cross-modal alignment: Plot t-SNE of audio and text embeddings for same tracks to confirm they cluster together
  2. Test relational awareness: Check if track representations from same playlists are closer than random pairs
  3. Ablation study: Remove each loss component (WTC, TTC, TPC) to quantify their individual contributions to performance

## Open Questions the Paper Calls Out

### Open Question 1
Can the self-attention fusion layer in TPC loss be replaced with a learnable parametric aggregator (e.g., Transformer encoder, GNN) to improve performance on highly heterogeneous playlists? The paper acknowledges its limitations on diverse datasets like LFM, where TPC underperforms TTC, but only experiments with a single-layer self-attention.

### Open Question 2
What is the impact of incorporating sequential information into the TPC loss (e.g., using positional embeddings or order-aware aggregation) on playlist continuation tasks where track order matters? The paper explicitly states it ignores the sequential nature of playlists, simplifying them to unordered sets.

### Open Question 3
How does using different audio backbone architectures (e.g., PANN, AST) affect the overall performance of LARP, and how do architectural choices interact with dataset characteristics? The paper mentions trying AST and PANN but settling on HTS-AT due to computational constraints and performance issues.

## Limitations

- Limited evaluation to only two public datasets (MPD and Last-FM) without extensive ablation studies on critical framework components
- Temperature parameter τ=0.07 and momentum parameter m=0.995 not extensively validated through sensitivity analysis
- Claim that LARP can fully replace collaborative filtering in cold-start settings not rigorously tested across diverse cold-start scenarios beyond playlist continuation

## Confidence

- **High confidence**: Experimental results showing LARP outperforming baselines on tested datasets are reproducible and well-documented
- **Medium confidence**: Theoretical framework of progressive contrastive learning is sound, but specific architectural choices lack comparative analysis
- **Low confidence**: Claim that LARP can fully substitute for collaborative filtering in cold-start settings is not sufficiently validated across different types of cold-start scenarios

## Next Checks

1. **Cross-dataset generalization**: Validate LARP's performance on additional cold-start music recommendation datasets (e.g., Spotify, Deezer) to assess whether observed improvements generalize beyond the two tested datasets

2. **Component ablation study**: Systematically remove each stage of the three-stage contrastive learning framework (WTC, TTC, TPC) to quantify their individual contributions to final performance and determine if all three stages are necessary

3. **Robustness to playlist heterogeneity**: Test LARP's performance on playlists with varying degrees of thematic coherence (highly curated vs. randomly generated playlists) to verify that TPC-fusion module effectively handles outlier tracks as claimed