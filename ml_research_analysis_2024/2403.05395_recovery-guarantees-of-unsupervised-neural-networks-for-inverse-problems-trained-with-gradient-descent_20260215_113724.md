---
ver: rpa2
title: Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems trained
  with Gradient Descent
arxiv_id: '2403.05395'
source_url: https://arxiv.org/abs/2403.05395
tags:
- gradient
- network
- descent
- loss
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes gradient descent training of neural networks\
  \ for inverse problems, extending prior results from gradient flow. It proves convergence\
  \ and recovery guarantees for generic loss functions satisfying the Kurdyka-\u0141\
  ojasiewicz property when using gradient descent with appropriate step size."
---

# Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems trained with Gradient Descent

## Quick Facts
- arXiv ID: 2403.05395
- Source URL: https://arxiv.org/abs/2403.05395
- Authors: Nathan Buskulic; Jalal Fadili; Yvain Quéau
- Reference count: 40
- Key outcome: Proves convergence and recovery guarantees for gradient descent training of neural networks for inverse problems when loss satisfies KL property, with characterization of overparameterization needs for two-layer Deep Image Prior networks.

## Executive Summary
This paper extends prior gradient flow results to prove that gradient descent with appropriate step size inherits convergence and recovery guarantees for unsupervised neural networks in inverse problems. The main contribution is showing that under the Kurdyka-Łojasiewicz property and proper initialization, gradient descent converges to zero-loss solutions with rates dependent on the KL desingularizing function. The analysis also provides bounds on overparameterization required for two-layer Deep Image Prior networks to satisfy the restricted injectivity condition with high probability. Numerical experiments validate the theoretical findings, showing that the theoretical overparameterization bounds are conservative compared to practical requirements.

## Method Summary
The paper analyzes gradient descent training of neural networks for inverse problems by extending gradient flow convergence results. The key mechanism involves bounding the discretization error between gradient flow and gradient descent using proper initialization and step size selection. The analysis relies on the Kurdyka-Łojasiewicz property of the loss function and requires the initialization to satisfy a restricted injectivity condition with respect to the tangent cone of the parameter manifold. For two-layer networks, the paper characterizes the overparameterization level needed for the restricted injectivity condition to hold with high probability, showing that the discretization only affects the overparameterization bound by a constant factor.

## Key Results
- Gradient descent with step size γ ∈ (0, 1/L] converges to zero-loss solution when loss satisfies KL property and initialization satisfies restricted injectivity condition
- For two-layer Deep Image Prior networks, overparameterization bound k depends on operator condition number κ(A), dimensions, and KL desingularizing function
- Discretization error from gradient flow to gradient descent only affects overparameterization bound by constant factor for two-layer networks
- Theoretical overparameterization bounds are conservative compared to practical requirements observed in numerical experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent with appropriately chosen step size inherits convergence and recovery guarantees from gradient flow for unsupervised neural networks in inverse problems.
- Mechanism: The paper extends prior gradient flow results by showing that gradient descent iterates converge to a zero-loss solution when the loss function satisfies the Kurdyka-Łojasiewicz (KL) property. The key is bounding the discretization error between gradient flow and gradient descent by a constant factor through proper initialization and step size selection.
- Core assumption: The loss function satisfies the KL inequality, the network parameters remain bounded, and the initialization satisfies the restricted injectivity condition.
- Evidence anchors:
  - [abstract] "we extend these results by proving that these guarantees hold true when using gradient descent with an appropriately chosen step-size/learning rate"
  - [section 3.2] "There exists a constant L > 0 such that if γ ∈]0, 1/L] and if the initialization θθθ0 is such that σmin(Jg(0)) > 0 and R′ < R"
  - [corpus] Weak - neighboring papers discuss convergence guarantees but don't provide specific KL-based analysis for gradient descent discretization
- Break condition: Step size γ > 1/L or initialization violating R′ < R condition, causing loss of convergence guarantees

### Mechanism 2
- Claim: Overparameterization provides high-probability recovery guarantees for two-layer Deep Image Prior networks trained with gradient descent.
- Mechanism: The paper characterizes the overparameterization level needed for the restricted injectivity condition to hold with high probability. For a two-layer network, the bound on hidden units k depends on the operator condition number κ(A), problem dimensions, and the KL desingularizing function.
- Core assumption: The network architecture follows the specified form with random initialization, and the overparameterization bound is satisfied.
- Evidence anchors:
  - [section 3.4] "we give a bound on the overparametrization necessary for a two-layer DIP network to benefit from all these guarantees with high probability"
  - [section 3.4] "k ≥ C′σ−4A nψ(LL,0/2(C∥A∥pn log(d)+√m(∥Ax∥∞+∥ε∥∞))2)4"
  - [corpus] Weak - neighboring papers discuss overparameterization but focus on different architectures or supervised settings
- Break condition: Insufficient overparameterization leading to failure of restricted injectivity condition

### Mechanism 3
- Claim: The discretization error from gradient flow to gradient descent only affects the overparameterization bound by a constant factor for two-layer networks.
- Mechanism: The paper shows that while gradient descent requires bounded parameter sequences and careful step size selection (unlike gradient flow), the theoretical bounds on overparameterization needed for convergence remain essentially the same, differing only by constants related to the step size.
- Core assumption: The network architecture and initialization satisfy the conditions for gradient flow analysis to carry over to gradient descent.
- Evidence anchors:
  - [abstract] "we also show that the discretization only affects the overparametrization bound for a two-layer DIP network by a constant"
  - [section 3.2] "The price to be paid by gradient descent is in the requirement that the sequence of iterates a priori bounded, that γ is well-chosen, as well as by the constants ν1 and ν2"
  - [corpus] Weak - neighboring papers discuss discretization but not in the context of KL-based analysis
- Break condition: Large step sizes causing significant discretization error or poor initialization leading to parameter explosion

## Foundational Learning

- Concept: Kurdyka-Łojasiewicz (KL) inequality and its implications for non-convex optimization
  - Why needed here: The KL property provides a framework for analyzing convergence of gradient descent on non-convex loss functions, which is essential for proving the main theoretical results
  - Quick check question: What are the key conditions for a function to satisfy the KL inequality, and how does this property enable convergence guarantees?

- Concept: Restricted injectivity and tangent cone conditions in inverse problems
  - Why needed here: The recovery guarantees depend on the operator A satisfying a restricted injectivity condition with respect to the tangent cone of the network's parameter manifold, ensuring unique recovery of the original signal
  - Quick check question: How does the restricted injectivity condition relate to the expressivity of the network and the conditioning of the forward operator?

- Concept: Overparameterization and its role in neural network optimization
  - Why needed here: The theoretical analysis requires the network to be sufficiently overparameterized to ensure the restricted injectivity condition holds with high probability, connecting network architecture to recovery guarantees
  - Quick check question: What is the relationship between the number of hidden units, the operator condition number, and the probability of satisfying the required conditions?

## Architecture Onboarding

- Component map: Generator network g(u, θθθ) -> Forward operator A -> Loss function Ly -> Gradient descent optimization
- Critical path: Initialization → Parameter update via gradient descent → Loss evaluation → Convergence check → Recovery error assessment
- Design tradeoffs: More expressive networks (larger Σ) reduce reconstruction distance but may violate restricted injectivity; larger step sizes speed convergence but risk instability; higher overparameterization ensures theoretical guarantees but increases computational cost
- Failure signatures: Loss not decreasing monotonically, parameters diverging, recovery error not improving with iterations, theoretical bounds not matching empirical performance
- First 3 experiments:
  1. Verify that for a simple inverse problem (e.g., Gaussian matrix A, small n), the loss decreases monotonically with proper step size and initialization
  2. Test the effect of overparameterization on convergence probability by varying the number of hidden units k while monitoring the restricted injectivity condition
  3. Compare the empirical convergence rate with the theoretical bound involving the KL desingularizing function for different loss functions (MSE vs cross-entropy)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of gradient descent depend on the KL desingularizing function ψ and the step size γ for different loss functions?
- Basis in paper: [explicit] The paper states that the loss converges to 0 at a rate dependent on the mapping Ψ−1 (a primitive of −(ψ′)2) applied to an affine increasing function of τ, where ψ is the KL desingularizing function.
- Why unresolved: While the paper provides theoretical bounds, it does not explore how different choices of loss functions (and their associated ψ) affect the practical convergence speed of gradient descent in various scenarios.
- What evidence would resolve it: Numerical experiments comparing the convergence rates of gradient descent for different loss functions (e.g., MSE, cross-entropy) with varying KL desingularizing functions and step sizes on diverse inverse problems.

### Open Question 2
- Question: What is the optimal step size γ that balances convergence speed and the requirements of Theorem 3.1 for gradient descent training of neural networks in inverse problems?
- Basis in paper: [explicit] The paper discusses a trade-off between convergence speed and the requirements of the theorem, with ν2 being minimized when γ = 1/(2L), but this choice may incur a trade-off between convergence speed and the requirements of the theorem.
- Why unresolved: The paper does not provide a definitive answer on the optimal step size, as it depends on the specific problem and network architecture. Finding the optimal step size that balances these factors is an open problem.
- What evidence would resolve it: A systematic study of the relationship between step size, convergence speed, and the requirements of Theorem 3.1 across various inverse problems and network architectures.

### Open Question 3
- Question: How does the level of overparameterization k affect the recovery error bound and the practical performance of two-layer DIP networks in inverse problems?
- Basis in paper: [explicit] The paper provides a theoretical bound on the level of overparameterization k needed for a two-layer DIP network to benefit from the convergence and recovery guarantees, but it also notes that this bound is conservative compared to practical requirements.
- Why unresolved: While the paper establishes a theoretical bound, it does not fully explore the relationship between the level of overparameterization and the recovery error bound or the practical performance of the network in various scenarios.
- What evidence would resolve it: Extensive numerical experiments varying the level of overparameterization k for two-layer DIP networks on diverse inverse problems, comparing the theoretical bound with the actual performance and recovery error.

## Limitations

- The overparameterization bounds are conservative compared to practical requirements observed in numerical experiments
- Results depend critically on initialization satisfying restricted injectivity condition, which may be difficult to verify in practice
- Analysis is primarily asymptotic and does not fully characterize finite-time behavior or non-asymptotic convergence rates

## Confidence

- **High**: The KL-based convergence analysis for gradient descent with proper step sizes (Mechanism 1)
- **Medium**: The overparameterization bounds for two-layer networks (Mechanism 2) - theoretical but conservative in practice
- **Medium**: The discretization error analysis showing gradient flow and gradient descent bounds differ only by constants (Mechanism 3)

## Next Checks

1. Empirically test the restricted injectivity condition probability across different random initializations to validate the theoretical overparameterization bounds
2. Investigate the effect of different KL desingularizing functions on convergence rates for various loss functions and problem dimensions
3. Conduct experiments varying step size schedules to determine practical bounds beyond the theoretical constant factor guarantees