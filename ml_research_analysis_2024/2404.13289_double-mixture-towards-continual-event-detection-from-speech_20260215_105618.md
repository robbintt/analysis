---
ver: rpa2
title: 'Double Mixture: Towards Continual Event Detection from Speech'
arxiv_id: '2404.13289'
source_url: https://arxiv.org/abs/2404.13289
tags:
- speech
- event
- task
- learning
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces continual event detection from speech (CEDS),
  addressing catastrophic forgetting and semantic-acoustic event disentanglement.
  The proposed Double Mixture method combines a mixture of speech experts and a mixture
  of memory mechanisms to enhance adaptability and prevent forgetting.
---

# Double Mixture: Towards Continual Event Detection from Speech

## Quick Facts
- arXiv ID: 2404.13289
- Source URL: https://arxiv.org/abs/2404.13289
- Reference count: 40
- Primary result: Proposed method achieves 51.72% average accuracy and 34.33% forgetting rate on Speech ACE05, outperforming other continual learning methods

## Executive Summary
This paper introduces continual event detection from speech (CEDS), a challenging task requiring models to detect semantic events (words spoken) and acoustic events (background sounds and environmental context) across sequentially learned tasks without catastrophic forgetting. The authors propose Double Mixture, a method combining a mixture of speech experts with a mixture of memory mechanisms to enhance adaptability and prevent forgetting. Experiments on benchmark datasets demonstrate state-of-the-art performance, with the approach showing robustness across various continual learning sequences and improving the model's ability to process complex audio inputs.

## Method Summary
The proposed Double Mixture method addresses continual event detection from speech by combining two key mechanisms: a mixture of speech experts and a mixture of memory. The approach freezes the speech encoder (Whisper) to preserve feature extraction capabilities, while using adapter networks (experts) for each task that are dynamically routed through a gating network. A memory buffer stores mixed samples containing both semantic and acoustic events, which are replayed during training alongside new task data. The method fine-tunes only the decoder layer parameters while keeping the encoder frozen, allowing the model to adapt to new tasks without significantly altering the fundamental speech understanding capabilities.

## Key Results
- Achieved 51.72% average accuracy on Speech ACE05 benchmark, outperforming existing continual learning methods
- Demonstrated 34.33% forgetting rate, showing effective retention of previous task knowledge
- Showed robustness across multiple task sequences and benchmark datasets (Speech ACE05, Speech MAVEN, ESC-50)
- Outperformed baseline methods including Fine-tuning, Experience Replay, A-GEM, EWC, LwF, PB, and L2P

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The mixture of speech experts prevents catastrophic forgetting by assigning a dedicated expert to each task while maintaining a gating network to dynamically route inputs
- Mechanism: The gating network assigns incoming speech samples to the most relevant expert based on current input features, allowing each expert to specialize in a specific task. The mixture of memory mechanism stores mixed samples of semantic and acoustic events, which are replayed during training to reinforce previously learned knowledge
- Core assumption: Each task can be effectively handled by a specialized expert without significant interference from other experts, and the gating mechanism can accurately route inputs to the appropriate expert
- Evidence anchors: [abstract]: "This method merges speech expertise with robust memory mechanisms to enhance adaptability and prevent forgetting"; [section 3.1]: "We introduce the mixture of speech experts in our framework where each expert focuses on a single task in the task stream"

### Mechanism 2
- Claim: The mixture of memory mechanism enhances the model's ability to process complex audio inputs by storing and replaying mixed samples of semantic and acoustic events
- Mechanism: The memory buffer stores mixed samples containing both semantic events (words spoken) and acoustic events (background sounds and environmental context). During training, these stored samples are combined with new task data to form the training set, ensuring the model retains knowledge of both event types
- Core assumption: Storing mixed samples of semantic and acoustic events is more effective than storing separate samples for each event type, as it better reflects the complex interplay between these events in real-world speech data
- Evidence anchors: [abstract]: "This method merges speech expertise with robust memory mechanisms to enhance adaptability and prevent forgetting"; [section 3.2]: "We store mixed speech samples containing semantic events and acoustic events in memory, named mixed memory"

### Mechanism 3
- Claim: Freezing the speech encoder while fine-tuning only the mixture of speech experts in the decoder layer preserves the model's ability to understand speech features while adapting to new tasks
- Mechanism: By freezing the encoder, the model retains its ability to extract meaningful features from speech input. Fine-tuning only the decoder layer allows the model to adapt to new tasks without significantly altering the feature extraction process
- Core assumption: The speech encoder learns general features that are useful across all tasks, and fine-tuning only the decoder layer is sufficient to adapt to new tasks without forgetting previous knowledge
- Evidence anchors: [section 3.3]: "During the training phase, we freeze the parameters of the speech encoder to preserve its ability to understand speech features and only fine-tune the parameters of the mixture of speech experts in the decoding layer"

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: The paper aims to address the challenge of catastrophic forgetting in continual learning, where the model's performance on previous tasks degrades after learning new tasks
  - Quick check question: What is the main difference between catastrophic forgetting and overfitting in the context of continual learning?

- Concept: Event disentanglement
  - Why needed here: The paper proposes to disentangle semantic events (words spoken) from acoustic events (background sounds and environmental context) to improve the model's ability to handle rare or previously unseen event combinations
  - Quick check question: Why is it important to disentangle semantic and acoustic events in speech event detection?

- Concept: Mixture of experts
  - Why needed here: The paper introduces a mixture of speech experts to mitigate catastrophic forgetting by assigning a dedicated expert to each task while maintaining a gating network to dynamically route inputs
  - Quick check question: How does the mixture of experts approach differ from traditional fine-tuning methods in continual learning?

## Architecture Onboarding

- Component map: Speech input -> Speech encoder (frozen) -> Mixture of speech experts (via gating network) -> Text decoder -> Event prediction
- Critical path: Speech input → Speech encoder → Mixture of speech experts (via gating network) → Text decoder → Event prediction
- Design tradeoffs:
  - Freezing the speech encoder preserves its ability to understand speech features but may limit the model's ability to adapt to new tasks
  - Using a mixture of experts prevents catastrophic forgetting but increases the model's complexity and may require more training data
  - Storing mixed samples in memory improves the model's ability to process complex audio inputs but increases memory requirements
- Failure signatures:
  - If the gating network becomes confused or overloaded, the model may fail to assign inputs to the appropriate expert, leading to poor performance
  - If the memory buffer becomes too large or if the mixed samples are not representative of the true distribution of events, the model may fail to retain knowledge of previous tasks
  - If the speech encoder is not sufficiently general or if the tasks require significant changes to the feature extraction process, freezing the encoder may limit the model's ability to adapt to new tasks
- First 3 experiments:
  1. Train the model on a single task and evaluate its performance on that task to ensure the basic architecture is functioning correctly
  2. Train the model on two sequential tasks and evaluate its performance on both tasks to assess the model's ability to retain knowledge of the first task while learning the second task
  3. Train the model on multiple sequential tasks and evaluate its performance on all tasks to assess the model's ability to prevent catastrophic forgetting and maintain performance across all tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Double Mixture method perform on speech datasets with significantly longer audio durations compared to the current benchmarks?
- Basis in paper: [explicit] The paper mentions that the Speech ACE05 dataset has an average duration of 33.36 hours, but does not explore datasets with significantly longer durations
- Why unresolved: The paper does not provide results or analysis on datasets with longer audio durations, which could impact the model's performance and scalability
- What evidence would resolve it: Experimental results on datasets with longer audio durations, comparing the performance of the Double Mixture method with other continual learning methods

### Open Question 2
- Question: Can the Double Mixture method be extended to handle multilingual speech event detection tasks?
- Basis in paper: [inferred] The paper focuses on English speech datasets and does not explore multilingual scenarios, which are increasingly important in real-world applications
- Why unresolved: The current implementation and experiments are limited to English, and there is no discussion on adapting the method for multilingual settings
- What evidence would resolve it: Experimental results on multilingual speech datasets, demonstrating the model's ability to detect events across different languages and comparing its performance with monolingual approaches

### Open Question 3
- Question: What is the impact of varying the memory buffer size on the performance of the Double Mixture method?
- Basis in paper: [explicit] The paper mentions retaining 10% of data from each previous task but does not explore the impact of different memory buffer sizes
- Why unresolved: The paper does not provide a detailed analysis of how different memory buffer sizes affect the model's performance and forgetting rates
- What evidence would resolve it: Experiments with varying memory buffer sizes, analyzing the trade-offs between memory usage and model performance, and identifying the optimal buffer size for different task sequences

## Limitations

- The mixture of experts framework introduces significant architectural complexity and computational overhead
- The effectiveness of the memory mechanism depends heavily on the representativeness of stored mixed samples, which may degrade over long task sequences
- The approach assumes that semantic and acoustic events can be effectively disentangled through separate expert routing, which may not hold for complex, multi-modal speech events where these components are inherently intertwined

## Confidence

- **High Confidence**: The core mechanism of using frozen encoders with adapter layers (mixture of experts) is well-established in continual learning literature and the experimental results demonstrate clear improvements over baseline methods
- **Medium Confidence**: The specific implementation details of the gating mechanism and memory buffer management, while plausible, lack sufficient detail for exact reproduction and their optimal configuration may be dataset-dependent
- **Medium Confidence**: The claim that storing mixed samples is superior to storing separate semantic and acoustic samples is supported by results but lacks comparative ablation studies to validate this design choice

## Next Checks

1. Conduct ablation studies comparing the proposed mixed memory approach against separate memory buffers for semantic and acoustic events to empirically validate the claimed advantage of mixed storage

2. Systematically evaluate how task similarity affects expert routing performance by creating controlled task sequences with varying degrees of semantic and acoustic overlap, measuring both accuracy and forgetting rates

3. Evaluate the model's performance on extended task sequences (beyond the 5-task experiments) to assess whether the memory buffer remains effective and whether expert routing becomes unstable over time