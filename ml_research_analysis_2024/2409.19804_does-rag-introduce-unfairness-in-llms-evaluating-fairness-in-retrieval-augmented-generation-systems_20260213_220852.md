---
ver: rpa2
title: Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in Retrieval-Augmented
  Generation Systems
arxiv_id: '2409.19804'
source_url: https://arxiv.org/abs/2409.19804
tags:
- fairness
- group
- methods
- bias
- both
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic study of fairness in Retrieval-Augmented
  Generation (RAG) systems, revealing that while RAG methods improve utility (e.g.,
  exact match accuracy), fairness issues persist across retrieval and generation stages.
  The authors construct scenario-based QA benchmarks focusing on sensitive attributes
  like gender and geography, evaluating six RAG methods (Zero-Shot, Naive, Selective-Context,
  SKR, FLARE, Iter-RetGen) across multiple datasets (TREC 2022 Gender/Location, BBQ).
---

# Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in Retrieval-Augmented Generation Systems

## Quick Facts
- **arXiv ID**: 2409.19804
- **Source URL**: https://arxiv.org/abs/2409.19804
- **Reference count**: 36
- **Key outcome**: RAG methods improve utility but introduce fairness issues, with retrievers showing significant gender bias and fairness improving with more retrieved documents.

## Executive Summary
This paper presents the first systematic study of fairness in Retrieval-Augmented Generation (RAG) systems. The authors construct scenario-based QA benchmarks focusing on sensitive attributes like gender and geography, evaluating six RAG methods across multiple datasets. Results reveal that while RAG methods improve utility (e.g., exact match accuracy), fairness issues persist across retrieval and generation stages. The study demonstrates that adjusting document ranking (prioritizing protected group content) can effectively mitigate bias while maintaining or improving accuracy.

## Method Summary
The study constructs scenario-based QA benchmarks from TREC 2022 Fair Ranking Track and BBQ datasets, evaluating six RAG methods (Zero-Shot, Naive, Selective-Context, SKR, FLARE, Iter-RetGen) using E5-base-v2 and E5-large-v2 retrievers with retrieval numbers 1, 2, and 5. The researchers use Meta-Llama-3-8B-Instruct and Meta-Llama-3-70B-Instruct generators, measuring utility via Exact Match and ROUGE-1 scores, and fairness via Group Disparity and Equalized Odds metrics. The analysis examines component-wise fairness contributions across the RAG pipeline.

## Key Results
- Retrievers exhibit significant gender bias, with E5-large favoring males more than E5-base
- Fairness improves with more retrieved documents due to averaging effect
- Larger 70B generators consistently increase male bias compared to 8B models
- Document ranking adjustment (prioritizing protected group content) can mitigate bias while improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG systems exhibit fairness disparities due to retrieval stage bias that persists into generation
- Mechanism: The retriever (e.g., E5-large) systematically ranks documents from protected groups lower than non-protected groups, creating an initial bias that the generator amplifies
- Core assumption: Retrieval ranking reflects underlying document embeddings that encode demographic preferences
- Evidence anchors:
  - [abstract] "retrievers exhibit significant gender bias (E5-large favors males more than E5-base)"
  - [section] "E5-large is less effective in retrieving higher-ranked female-related golden documents"
  - [corpus] "Average neighbor FMR=0.526" (weak corpus support for retrieval bias)

### Mechanism 2
- Claim: Fairness improves with more retrieved documents due to averaging effect
- Mechanism: When more documents are retrieved, the representation of protected and non-protected groups becomes more balanced, diluting initial biases
- Core assumption: Each retrieved document carries independent bias signals that average out with larger sample sizes
- Evidence anchors:
  - [section] "retrieving more documents significantly improves fairness" and "High positive bias toward females when retrieving 1 document gradually balances out as more documents are retrieved"
  - [corpus] No direct corpus evidence, this is inferred from experimental results

### Mechanism 3
- Claim: Larger generators (70B vs 8B) consistently increase male bias due to parameter scaling effects
- Mechanism: Larger models have more capacity to encode and amplify subtle demographic patterns present in training data
- Core assumption: Model size correlates with bias amplification capability through increased parameter count
- Evidence anchors:
  - [section] "larger 70B model shows a consistent shift toward bias favoring males, while the 8B model exhibits more varied results"
  - [corpus] No direct corpus evidence, this is inferred from experimental results

## Foundational Learning

- **Group disparity metric calculation**: Understanding how fairness is quantified across demographic groups is essential for interpreting results
  - Quick check: If a system has 60% accuracy for females and 40% for males, what is the group disparity?

- **Retrieval-augmented generation pipeline architecture**: The modular nature of RAG components (retriever, refiner, judger, generator) determines where biases can enter and persist
  - Quick check: Which RAG component has the most significant influence on both fairness and exact match accuracy?

- **Question framing effects on bias measurement**: Positive vs negative question formulations can change how bias manifests and is measured
  - Quick check: Why do negatively phrased questions tend to result in greater bias toward females?

## Architecture Onboarding

- **Component map**: Retriever → Refiner → Judger → Generator → Output
  - Each component can introduce or mitigate bias independently
  - Retriever has the strongest influence on both fairness and accuracy

- **Critical path**: Retriever ranking → Document selection → Generator synthesis
  - The initial ranking determines which documents influence the final answer
  - Generator amplifies any bias present in retrieved documents

- **Design tradeoffs**: Utility vs fairness optimization
  - Models optimized for exact match accuracy often show worse fairness metrics
  - Increasing retrieved documents improves fairness but may reduce efficiency

- **Failure signatures**:
  - Consistently low MRR for protected group documents indicates retrieval bias
  - Systematic gender bias in generator outputs regardless of retrieval quality
  - Trade-off between exact match accuracy and fairness metrics

- **First 3 experiments**:
  1. Compare E5-base vs E5-large retrievers with identical generator to isolate retrieval bias
  2. Test fairness impact of retrieving 1, 2, and 5 documents with the same RAG method
  3. Evaluate the same RAG method with 8B vs 70B generator to measure size effects on bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the size of the generator model (e.g., 8B vs 70B parameters) consistently influence fairness metrics across different retrieval configurations, or is the observed bias in the 70B model specific to the E5-base retrieval setup?
- Basis in paper: Explicit - "We utilized different Llama-3-instruct models with varying parameter sizes (8B and 70B) to assess the influence of the LLM generator. As shown in Figure 6, across all RAG methods, EM remains roughly the same between the 8B and 70B models, but bias fluctuates significantly. The 70B model shows a consistent shift toward bias favoring males..."
- Why unresolved: The paper only tested one retriever (E5-base) with different generator sizes. It's unclear if the 70B model's male bias persists with other retrievers like BM25 or E5-large, or if retrieval quality moderates this effect.
- What evidence would resolve it: Replicating the generator size experiments using BM25 and E5-large retrievers while measuring fairness metrics (GD, EO) would clarify whether the 70B bias is generator-specific or interacts with retrieval configuration.

### Open Question 2
- Question: What is the relative contribution of each RAG component (retriever, refiner, judger, generator) to fairness degradation when using negative question framing, and do these contributions differ from positive question scenarios?
- Basis in paper: Explicit - "We constructed negative question forms to evaluate the utility and fairness between positive and negative question formats... negative question formulations may introduce new fairness concerns... negative phrasing in both S1/S5 and S2/S6 scenarios tends to contribute to biases toward females."
- Why unresolved: The paper analyzes component contributions only for positive questions in S1. The differential impact of each component on fairness when questions are negatively framed remains unexplored.
- What evidence would resolve it: Conducting the component ablation study (retriever vs refiner vs judger vs generator isolation) separately for negative question scenarios (S5, S6, S7, S8) would reveal whether certain components disproportionately amplify bias under negative phrasing.

### Open Question 3
- Question: Can the fairness improvements from increasing retrieved documents (e.g., 1→5 documents) be achieved through alternative strategies like document weighting or diversity-aware reranking without sacrificing EM performance?
- Basis in paper: Explicit - "retrieving more documents significantly improves fairness... High positive bias toward females when retrieving 1 document gradually balances out as more documents are retrieved... increasing the number of retrieved documents helps mitigate gender bias."
- Why unresolved: While the paper shows that retrieving more documents reduces bias, it doesn't explore whether targeted document selection strategies could achieve similar fairness gains with fewer documents, which would be more computationally efficient.
- What evidence would resolve it: Comparing fairness and EM metrics when using document reranking algorithms that prioritize protected group content against baseline approaches with fixed retrieval counts would determine if strategic selection can match the benefits of increased retrieval volume.

## Limitations

- The study focuses primarily on gender and geographic biases, leaving other demographic dimensions unexplored
- Corpus-based evidence for retrieval bias is weak (FMR=0.526), suggesting findings may not be fully generalizable
- The study uses relatively simple question-answering scenarios, and it's unclear how findings extend to more complex RAG applications

## Confidence

**High Confidence**: The finding that retrievers exhibit significant gender bias, particularly E5-large favoring males more than E5-base, is well-supported by direct experimental evidence showing lower MRR scores for female-related documents.

**Medium Confidence**: The claim that fairness improves with more retrieved documents is supported by experimental results, but the underlying mechanism (averaging effect) lacks direct corpus validation and may depend on specific dataset characteristics.

**Medium Confidence**: The assertion that larger generators increase male bias is based on comparative experiments between 8B and 70B models, but the causal mechanism linking model size to bias amplification is not empirically validated.

## Next Checks

1. **Cross-dataset validation**: Test the same RAG methods on additional datasets with different demographic attributes (e.g., age, ethnicity) to verify if the observed gender bias patterns generalize beyond the TREC 2022 dataset.

2. **Ablation study on retrieval ranking**: Systematically vary the ranking of documents with known protected group attributes while keeping all other RAG components constant to isolate the specific contribution of retriever ranking bias to final generation outputs.

3. **Bias mitigation effectiveness validation**: Implement and test the proposed document ranking adjustment strategy (prioritizing protected group content) across multiple RAG architectures to confirm that it consistently improves both fairness metrics and accuracy as claimed.