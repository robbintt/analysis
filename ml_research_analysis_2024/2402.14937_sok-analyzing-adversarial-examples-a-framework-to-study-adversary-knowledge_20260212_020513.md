---
ver: rpa2
title: 'SoK: Analyzing Adversarial Examples: A Framework to Study Adversary Knowledge'
arxiv_id: '2402.14937'
source_url: https://arxiv.org/abs/2402.14937
tags:
- adversarial
- information
- attack
- attacks
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes adversary knowledge in adversarial example
  attacks on image classifiers using information extraction oracles and Hasse diagrams.
  It presents an adversarial example game inspired by cryptographic games to standardize
  attack definitions and evaluations.
---

# SoK: Analyzing Adversarial Examples: A Framework to Study Adversary Knowledge

## Quick Facts
- arXiv ID: 2402.14937
- Source URL: https://arxiv.org/abs/2402.14937
- Reference count: 40
- Primary result: Formalizes adversary knowledge in adversarial example attacks using information extraction oracles and Hasse diagrams, providing a rigorous framework to analyze and compare attacks.

## Executive Summary
This paper introduces a formal framework to study adversary knowledge in adversarial example attacks on image classifiers. The authors propose information extraction oracles (IEOs) to categorize attacker knowledge and use Hasse diagrams to visualize the partial order between different threat models. They present an adversarial example game inspired by cryptographic games to standardize attack definitions and evaluations. Through a survey of recent attacks, the paper demonstrates that model information significantly impacts attack performance, while transferable attacks can achieve near-white-box effectiveness when given access to training data and information.

## Method Summary
The paper formalizes adversary knowledge using information extraction oracles (IEOs) for different categories (model, data, training, defense). It constructs Hasse diagrams to represent partial order relations between IEOs using a domination operator. The adversarial example game framework models attacker capabilities, defender responses, and success metrics. The authors survey recent attacks and classify their knowledge using this framework, then compare attack performance through a relative performance score metric.

## Key Results
- Model information access (white-box) has the strongest effect on attack performance through direct gradient optimization
- Transferable attacks can achieve near-white-box performance when given training data and process information
- The adversarial example game formalization provides a rigorous framework for attack evaluation beyond simple attack success rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model information access (white-box) has the strongest effect on attack performance.
- Mechanism: The adversary uses model parameters θM, architecture φM, or score query access to directly optimize adversarial perturbations using gradients or model outputs.
- Core assumption: The attacker can compute gradients or evaluate model outputs efficiently enough to craft effective perturbations.
- Evidence anchors:
  - [abstract] "Results confirm that model information has a strong effect on attack performance"
  - [section] "we find that there is a lack of study and formalization of adversary knowledge when mounting attacks"
  - [corpus] Weak - corpus contains no direct comparison of model information vs. other categories
- Break condition: Gradients become unavailable or too expensive to compute (e.g., in black-box settings).

### Mechanism 2
- Claim: Transferable attacks can achieve near-white-box performance when given access to training data and training information.
- Mechanism: The attacker trains surrogate models using available training data D and training function Train, then crafts adversarial examples on the surrogate to transfer to the target.
- Core assumption: Surrogate models trained on similar data/distribution approximate the target model's decision boundaries well enough for transfer.
- Evidence anchors:
  - [abstract] "transferable attacks using additional data/training information can achieve near-white-box performance"
  - [section] "a lack of information in that category, in the case of transferable attacks, can be compensated by the use of additional information"
  - [corpus] Weak - corpus lacks empirical comparison of transferable vs. white-box under same data/training info conditions
- Break condition: Surrogate models fail to capture target's decision boundaries (e.g., different architectures, data distributions).

### Mechanism 3
- Claim: The adversarial example game formalization provides a rigorous, comparable framework for attack evaluation.
- Mechanism: The game models attacker capabilities (via IEOs), defender responses, and success metrics (relative performance score) in a unified structure.
- Core assumption: All relevant attacker knowledge can be encoded as IEOs with proper domination relations.
- Evidence anchors:
  - [abstract] "We present an adversarial example game, inspired by cryptographic games, to standardize attacks"
  - [section] "We can now represent a salient situation as a distinguisher D : I → {0,1}..."
  - [corpus] Missing - corpus does not discuss cryptographic games or formal game-theoretic frameworks
- Break condition: Some attacker capabilities cannot be expressed as IEOs or domination relations fail to capture practical attack scenarios.

## Foundational Learning

- Concept: Information Extraction Oracles (IEOs) and domination relations
  - Why needed here: IEOs provide a formal way to encode attacker knowledge categories and compare threat models via ⊏ ordering.
  - Quick check question: Given two IEOs where O2 outputs a subset of O1's outputs for all queries, which IEO dominates the other?

- Concept: Hasse diagrams for partial order visualization
  - Why needed here: Hasse diagrams visually represent the ⊏ ordering between IEOs within each knowledge category, making threat model comparisons intuitive.
  - Quick check question: If O2 ⊏ O1 and O3 ⊏ O1, can we conclude O2 ⊏ O3 or O3 ⊏ O2?

- Concept: Adversarial example game with Evaluate functions
  - Why needed here: The game formalizes attack success beyond simple ESR by incorporating both attacker goals (targeted/untargeted) and defender defenses.
  - Quick check question: In a targeted attack, what does Evaluate1 return if the predicted label equals the target label?

## Architecture Onboarding

- Component map: IEO definitions (OM, OA, OS, OL, OD, OTrain, OFA, OPA, OSPD) -> Hasse diagram construction per category -> Adversarial example game implementation (AdvGen, Evaluate, Classify, Train functions) -> Survey data processing and comparison logic -> Relative performance score calculation

- Critical path: 1. Define IEOs for attacker knowledge 2. Build Hasse diagrams using ⊏ relations 3. Implement adversarial example game 4. Survey recent attacks and classify knowledge 5. Compare attack performance using relative score

- Design tradeoffs:
  - Granularity vs. usability of IEO definitions
  - Completeness of Hasse diagrams vs. readability
  - Precision of relative performance score vs. interpretability

- Failure signatures:
  - IEO domination relations not transitive or inconsistent
  - Hasse diagrams missing critical threat models
  - Relative score negative when attacks should succeed
  - Survey classification mismatches between papers

- First 3 experiments:
  1. Implement IEO domination checks for simple model architectures
  2. Build Hasse diagram for model information category and verify with paper examples
  3. Implement adversarial example game with PGD and verify against known results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much does access to training data improve the transferability of adversarial attacks compared to just model information?
- Basis in paper: [explicit] Results show transferable attacks using training data and process information achieve near-white-box performance.
- Why unresolved: Quantitative comparison between training data access and other information types missing.
- What evidence would resolve it: Systematic experiments varying access to training data while holding other information constant.

### Open Question 2
- Question: Are there fundamental limitations to the effectiveness of transferable attacks against defended models?
- Basis in paper: [inferred] Transferable attacks perform well against undefended models but results against defended models are mixed.
- Why unresolved: Limited evaluation of transferable attacks against defended models in survey.
- What evidence would resolve it: Comprehensive evaluation of transferable attacks against diverse defended models.

### Open Question 3
- Question: How does the type of model architecture information (exact vs. possible architectures) affect attack performance?
- Basis in paper: [explicit] Hasse diagrams show possible architectures is less specific than exact architecture.
- Why unresolved: Survey results don't clearly show performance difference between these cases.
- What evidence would resolve it: Controlled experiments comparing attacks with exact vs. possible architecture information.

## Limitations

- The paper lacks empirical validation of theoretical claims about transferable attacks achieving near-white-box performance
- The relative performance score metric may not capture all practical considerations in adversarial attack evaluation
- Limited evaluation of transferable attacks against defended models in the survey

## Confidence

- **High confidence**: The theoretical framework of IEOs and Hasse diagrams is internally consistent and provides a novel approach to formalizing adversary knowledge
- **Medium confidence**: The claim that transferable attacks can achieve near-white-box performance with sufficient training data is plausible but lacks direct empirical evidence
- **Low confidence**: The adversarial example game formalization's practical utility and completeness remain uncertain

## Next Checks

1. Conduct controlled experiments comparing the success rates of white-box attacks, black-box attacks, and transferable attacks on the same target model, ensuring equivalent access to training data and model information where applicable.

2. Implement automated checks for transitivity and consistency in the ⊏ ordering between IEOs across all knowledge categories, particularly focusing on edge cases where multiple IEOs partially dominate each other.

3. Test the relative performance score metric on a diverse set of attacks and defenses to verify that it correctly captures attack success in both targeted and untargeted scenarios, including cases where benign accuracy varies significantly.