---
ver: rpa2
title: Fine-Grained Prediction of Reading Comprehension from Eye Movements
arxiv_id: '2410.04484'
source_url: https://arxiv.org/abs/2410.04484
tags:
- reading
- comprehension
- movements
- word
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the task of predicting reading comprehension
  from eye movements at the level of a single question over a single paragraph. The
  authors use the OneStop dataset, which contains eye-tracking data from 360 participants
  reading news articles and answering multiple-choice questions.
---

# Fine-Grained Prediction of Reading Comprehension from Eye Movements

## Quick Facts
- arXiv ID: 2410.04484
- Source URL: https://arxiv.org/abs/2410.04484
- Authors: Omer Shubi; Yoav Meiri; Cfir Avraham Hadar; Yevgeni Berzak
- Reference count: 14
- Primary result: Models combining eye movements and text features achieve only modest improvements over text-only baselines for predicting reading comprehension at the single-question level

## Executive Summary
This paper tackles the challenging task of predicting reading comprehension from eye movements at the level of individual questions and paragraphs. Using the OneStop dataset with 360 participants reading news articles and answering multiple-choice questions, the authors introduce three transformer-based models that combine text and eye movement features: RoBERTa-QEye, MAG-QEye, and PostFusion-QEye. The models are evaluated across different generalization regimes (new participant, new item, and both) in ordinary reading and information-seeking contexts. While the best models outperform strong text-only baselines in some cases, the task proves highly challenging, and improvements are generally small. The study highlights the importance of benchmarking against text-only models and suggests that current modeling approaches may not fully capture the relationship between eye movements and fine-grained reading comprehension.

## Method Summary
The authors use the OneStop Eye Movements dataset containing eye-tracking data from 360 participants reading news articles and answering multiple-choice questions. They extract various eye movement features at different granularities (global, word-level, and fixation-level) and combine them with text embeddings from RoBERTa. Three transformer-based models are proposed with different fusion strategies: RoBERTa-QEye augments the input with eye movement features, MAG-QEye modifies hidden representations, and PostFusion-QEye uses cross-attention fusion. The models are evaluated using balanced accuracy across three generalization regimes (new participant, new item, and both) in ordinary reading and information-seeking contexts.

## Key Results
- The task of predicting reading comprehension from eye movements at the single-question level is highly challenging
- Best models achieve only modest improvements over strong text-only baselines
- MAG-QEye achieves 59.2% balanced accuracy in ordinary reading, while PostFusion-QEye achieves 58.0% in information seeking
- Limited generalization to new items and participants suggests the task's difficulty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Eye movements can be used to predict reading comprehension at the level of a single question over a single paragraph.
- Mechanism: The tight correspondence between eye movements and real-time language comprehension means that eye movement features can serve as real-time proxies for comprehension processes. Models that combine these features with text can capture this relationship.
- Core assumption: Eye movement patterns during reading are sufficiently informative to distinguish between correct and incorrect comprehension at a fine-grained level.
- Evidence anchors:
  - [abstract] "This line of work suggests that in some cases various aspects of reading comprehension can be predicted from eye movements with above-chance performance."
  - [section 2] "This line of work suggests that in some cases various aspects of reading comprehension can be predicted from eye movements with above-chance performance."
- Break condition: If eye movement patterns are too similar between correct and incorrect comprehension responses, or if comprehension processes are not reliably reflected in eye movements at the single-question level.

### Mechanism 2
- Claim: Combining eye movement features with linguistic properties of the text improves prediction of reading comprehension beyond text-only approaches.
- Mechanism: Eye movement behavior is systematically affected by linguistic properties of the text, and readers' linguistic proficiency influences these interactions. Models that incorporate both eye movements and linguistic features can capture these interactions.
- Core assumption: The interactions between eye movements and linguistic word properties contain information about reading comprehension that is not captured by text alone.
- Evidence anchors:
  - [abstract] "The importance of combining eye movements with attributes of the text is motivated by a large literature in the psychology of reading which points to systematic effects of linguistic properties of the text on reading times"
  - [section 5] "The strength of such interactions has been shown to be indicative of the readers' linguistic proficiency, which is directly related to reading comprehension."
- Break condition: If linguistic properties are already well-encoded in text embeddings, or if eye movement patterns do not vary systematically with linguistic properties in ways that predict comprehension.

### Mechanism 3
- Claim: Transformer-based models can effectively combine eye movement and text features to predict reading comprehension.
- Mechanism: Transformer encoders can process both eye movement features and text embeddings, learning complex relationships between them. Different fusion strategies allow the model to learn how eye movements relate to comprehension.
- Core assumption: The transformer architecture is capable of learning meaningful representations that combine eye movement patterns with textual information to predict comprehension outcomes.
- Evidence anchors:
  - [abstract] "In this work, we introduce three new multimodal language models that combine text and eye movement features: RoBERTa-QEye, MAG-QEye, and PostFusion-QEye."
  - [section 5] "Each of these models uses a different strategy for combining text with eye movements."
- Break condition: If the transformer architecture cannot effectively learn the relationship between eye movements and comprehension, or if simpler models perform as well.

## Foundational Learning

- Concept: Eye movement measures in reading (fixations, saccades, regression patterns)
  - Why needed here: Understanding what eye movement features are available and how they relate to reading behavior is crucial for feature engineering and model design.
  - Quick check question: What is the difference between a fixation and a saccade, and why are both important for understanding reading behavior?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The proposed models are based on transformer encoders, so understanding how transformers process sequential data and use attention is essential for understanding the model design.
  - Quick check question: How does the self-attention mechanism in transformers allow the model to focus on different parts of the input sequence?

- Concept: Evaluation of classification models with imbalanced data
  - Why needed here: The reading comprehension data is imbalanced (more correct than incorrect responses), so understanding how to properly evaluate models is important.
  - Quick check question: Why is balanced accuracy a more appropriate metric than regular accuracy for evaluating models on imbalanced datasets?

## Architecture Onboarding

- Component map:
  - Eye movement features + text embeddings -> Transformer-based model (RoBERTa-QEye, MAG-QEye, or PostFusion-QEye) -> Binary classification output

- Critical path:
  1. Extract eye movement features from raw gaze data
  2. Combine eye movement features with text embeddings
  3. Process combined features through transformer-based model
  4. Generate classification output
  5. Evaluate using balanced accuracy

- Design tradeoffs:
  - Word-level vs. fixation-level vs. global eye movement features: Finer granularity may capture more detail but increases complexity
  - Different fusion strategies: Augmenting input vs. modifying hidden representations vs. cross-attention have different strengths and weaknesses
  - Text-only vs. multimodal models: Text-only is simpler but may miss eye movement signals; multimodal is more complex but may capture additional information

- Failure signatures:
  - Poor performance in new participant regime: Model may be overfitting to individual participant patterns
  - Poor performance in new item regime: Model may be relying too heavily on specific text features rather than generalizable patterns
  - No improvement over text-only baseline: Eye movement features may not be adding predictive value, or model may not be effectively combining them with text

- First 3 experiments:
  1. Train and evaluate text-only RoBERTa baseline to establish performance without eye movements
  2. Train and evaluate RoBERTa-QEye (augment input) with word-level eye movement features
  3. Compare performance of RoBERTa-QEye, MAG-QEye, and PostFusion-QEye to determine which fusion strategy works best

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can eye movements reliably predict fine-grained reading comprehension at the level of individual questions and participants?
- Basis in paper: [explicit] The authors conclude that "the extent to which specific aspects of reading comprehension can be reliably decoded from eye movements signal remains an open question."
- Why unresolved: The models only achieve small improvements over text-only baselines, and the task proves highly challenging with limited generalization to new items and participants.
- What evidence would resolve it: Developing more effective modeling techniques that capture the relationship between eye movements and comprehension, or demonstrating consistent, substantial improvements over text-only baselines across diverse populations and reading tasks.

### Open Question 2
- Question: What specific aspects of reading comprehension, if any, are most predictable from eye movements?
- Basis in paper: [inferred] The authors note that eye movements might not contain sufficient information for decoding comprehension at high accuracy rates for the examined level of granularity, and that the imbalanced nature of the data may contribute to task difficulty.
- Why unresolved: The current models do not show strong predictive power, and the dataset's imbalance limits the ability to identify which comprehension aspects might be more predictable.
- What evidence would resolve it: Analyzing model performance on different question types or comprehension levels, or creating more balanced datasets with a wider range of comprehension outcomes.

### Open Question 3
- Question: How do linguistic word properties and eye movements interact to influence reading comprehension prediction?
- Basis in paper: [explicit] The authors perform ablation experiments showing that removing linguistic word properties or eye movement features does not substantially affect model performance, suggesting limited interaction effects.
- Why unresolved: The ablation results are inconclusive, and the role of linguistic features in enhancing eye movement-based predictions remains unclear.
- What evidence would resolve it: Conducting more extensive experiments with varied linguistic features and eye movement representations, or developing models that explicitly model the interaction between these two types of information.

## Limitations

- The task of predicting reading comprehension from eye movements at the single-question level is highly challenging, with only modest improvements over text-only baselines
- Limited generalization to new items and participants suggests current models may not capture robust patterns
- Results are based on a specific dataset (OneStop) and may not generalize to other reading contexts or populations

## Confidence

- High confidence: The task is challenging and improvements over text-only baselines are generally small
- Medium confidence: Specific performance numbers reported, given task complexity and potential variability across evaluation splits
- Low confidence: Generalizability of results to other reading comprehension datasets or contexts

## Next Checks

1. **Cross-dataset validation**: Evaluate the best-performing models on an independent eye-tracking dataset with reading comprehension tasks to assess generalizability beyond the OneStop dataset.

2. **Ablation study on eye movement features**: Systematically remove different types of eye movement features to determine which features contribute most to model performance and whether any specific features drive the improvements over text-only baselines.

3. **Human baseline comparison**: Compare model performance against human ability to predict comprehension from eye movement patterns alone, to establish whether the models are approaching human-level performance on this task.