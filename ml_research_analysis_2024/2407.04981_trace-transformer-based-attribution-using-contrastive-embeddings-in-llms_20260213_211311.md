---
ver: rpa2
title: 'TRACE: TRansformer-based Attribution using Contrastive Embeddings in LLMs'
arxiv_id: '2407.04981'
source_url: https://arxiv.org/abs/2407.04981
tags:
- data
- source
- trace
- attribution
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes TRACE, a framework for source attribution in
  large language models using contrastive learning. TRACE maps training data to embeddings
  and uses supervised contrastive learning to cluster embeddings by source.
---

# TRACE: TRansformer-based Attribution using Contrastive Embeddings in LLMs

## Quick Facts
- arXiv ID: 2407.04981
- Source URL: https://arxiv.org/abs/2407.04981
- Authors: Cheng Wang; Xinyang Lu; See-Kiong Ng; Bryan Kian Hsiang Low
- Reference count: 13
- Primary result: High source attribution accuracy (84.4–97.3%) and scalability across 10–100 data providers, though robustness drops under paraphrasing attacks.

## Executive Summary
TRACE introduces a framework for source attribution in large language models using contrastive embeddings. It maps training data to embeddings and uses supervised contrastive learning to cluster embeddings by source, enabling efficient inference via kNN or nearest centroid methods. Experiments on three datasets show high accuracy and scalability, with robustness challenges under paraphrasing attacks. The approach improves interpretability and source attribution in LLMs.

## Method Summary
TRACE uses TF-IDF to extract principal sentences from training data, which are then embedded using SBERT with a projection head. Supervised contrastive learning with NT-Xent loss clusters embeddings by source. At inference, generated responses are embedded and attributed to the nearest source cluster using kNN or nearest centroid methods. The framework is designed to be scalable and interpretable for source attribution in LLMs.

## Key Results
- High attribution accuracy: 84.4–97.3% across three datasets.
- Scalability: Effective across 10 to 100 data providers.
- Robustness: Paraphrasing attacks reduce accuracy more than deletion or synonym substitution.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TRACE uses contrastive learning to map training data from the same source into compact, well-separated clusters in the embedding space.
- Mechanism: TRACE assigns the same label to all data from a given source and minimizes NT-Xent loss, which pulls embeddings from the same source together while pushing embeddings from different sources apart.
- Core assumption: The NT-Xent loss with supervised labels reliably clusters embeddings by source when trained on representative sentences.
- Evidence anchors:
  - [abstract] "TRACE maps training data to embeddings and uses supervised contrastive learning to cluster embeddings by source."
  - [section] "Unlike the other contrastive learning frameworks in computer vision... TRACE aims to achieve source-coherent clustering... TRACE utilizes NT-Xent Loss (Sohn, 2016) for supervised contrastive learning."
  - [corpus] Weak - corpus neighbors are topically related but do not provide direct empirical support for the clustering claim.
- Break condition: If source data are too similar across providers, or if the sentence encoder cannot distinguish semantic differences, clusters may overlap and attribution accuracy will drop.

### Mechanism 2
- Claim: TRACE's kNN-based inference assigns a generated response to the nearest source cluster, with optional multi-source attribution via top-k ranking.
- Mechanism: After contrastive training, each source is represented by its cluster of embeddings. At inference, the response embedding is compared to all source embeddings; the nearest neighbor(s) determine the attributed source(s).
- Core assumption: The embedding space learned during contrastive training preserves meaningful source distinctions that can be captured by simple distance metrics.
- Evidence anchors:
  - [abstract] "For inference, it employs kNN or nearest centroid to attribute generated responses to the closest source."
  - [section] "At this stage, when a language model generates a response, we employ the k-Nearest Neighbor (kNN) algorithm to assign the response to the closest data source in the embedding space."
  - [corpus] Weak - corpus does not validate the inference mechanism empirically.
- Break condition: If inference embeddings are too close to multiple clusters (due to paraphrasing or semantic overlap), nearest-neighbor assignment becomes ambiguous and accuracy falls.

### Mechanism 3
- Claim: Selecting principal sentences via TF-IDF before contrastive training reduces computational cost while preserving discriminative power.
- Mechanism: TF-IDF identifies the most informative sentences in each document; only these are embedded and used for contrastive training, so the embedding space is built from high-value signals rather than all text.
- Core assumption: Principal sentences capture enough source-specific context to enable accurate clustering without embedding every sentence.
- Evidence anchors:
  - [section] "So, we propose to extract principal sentences from each source by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) which is effective for identifying significant sentences within documents."
  - [section] "Section 4.8 presents an ablation study examining the effect of different WINDOW_SIZEs on accuracy."
  - [corpus] Weak - corpus lacks quantitative results for the TF-IDF selection step.
- Break condition: If WINDOW_SIZE is too small, context is lost and clusters become noisy; if too large, computational gains vanish and embeddings become diluted.

## Foundational Learning

- Concept: Contrastive learning (NT-Xent loss)
  - Why needed here: Provides a principled way to push together embeddings from the same source and pull apart embeddings from different sources.
  - Quick check question: What is the role of the temperature parameter τ in NT-Xent loss?

- Concept: Sentence embedding models (e.g., SBERT)
  - Why needed here: Transforms variable-length text into fixed-size vectors suitable for distance-based source attribution.
  - Quick check question: How does SBERT differ from averaging word embeddings?

- Concept: k-Nearest Neighbor classification
  - Why needed here: Enables efficient inference by assigning a query embedding to the source whose cluster it is closest to in embedding space.
  - Quick check question: What is the difference between hard kNN and soft kNN in this context?

## Architecture Onboarding

- Component map: Data ingestion -> TF-IDF principal sentence extraction -> SBERT encoder (with projection head) -> Contrastive training (NT-Xent) -> Store source cluster embeddings -> Inference: embed response -> kNN or nearest centroid -> Output source attribution
- Critical path: Extract principal sentences -> Train contrastive encoder -> Store centroids -> Embed response -> kNN inference
- Design tradeoffs:
  - WINDOW_SIZE vs. accuracy/computation: larger windows increase context but cost more compute and risk dilution.
  - k value in kNN: larger k smooths decisions but may blur source boundaries; nearest centroid is faster but potentially less precise.
  - Single-source vs. multi-source attribution: single-source is simpler but may miss mixed influences; multi-source captures more nuance but is more complex.
- Failure signatures:
  - Low accuracy on held-out data -> clusters not well separated.
  - High variance across runs -> embedding space unstable, consider increasing training epochs or data.
  - Slow inference -> switch from kNN to nearest centroid or reduce number of stored embeddings.
- First 3 experiments:
  1. Run contrastive training with WINDOW_SIZE=30 and evaluate source clustering accuracy on a small held-out set.
  2. Compare hard kNN (k=1,10,20) vs. nearest centroid inference on the same set to identify speed-accuracy tradeoff.
  3. Test robustness by applying 5%, 10%, 15% deletion and synonym substitution to responses and measuring attribution drop.

## Open Questions the Paper Calls Out
None

## Limitations
- The mechanism by which NT-Xent loss reliably clusters embeddings by source is only weakly supported by the corpus, with no direct empirical validation of the clustering claim.
- The inference mechanism (kNN or nearest centroid) is asserted but not empirically validated in the corpus, leaving open questions about its robustness to ambiguous or overlapping embeddings.
- The TF-IDF principal sentence selection is described as reducing computational cost while preserving discriminative power, but lacks quantitative results in the corpus to substantiate this tradeoff.

## Confidence
- High Confidence: The overall framework design (TRACE uses contrastive learning to cluster source embeddings and assigns generated responses via kNN/nearest centroid) is clearly described and aligns with standard practices in contrastive learning and kNN classification.
- Medium Confidence: The accuracy claims (84.4–97.3%) and scalability across 10–100 data providers are supported by experiments, but the corpus does not provide enough detail to fully validate these results under varied conditions.
- Low Confidence: The robustness claims (especially regarding paraphrasing attacks) are based on limited attack types (deletion, synonym substitution) and lack broader validation against other adversarial scenarios.

## Next Checks
1. Validate Clustering Mechanism: Run contrastive training with a held-out set and evaluate source clustering accuracy to confirm that NT-Xent loss reliably separates embeddings by source in practice.
2. Test Inference Robustness: Apply a broader range of paraphrasing and semantic overlap scenarios to generated responses and measure attribution accuracy to assess the kNN/nearest centroid inference under realistic conditions.
3. Benchmark TF-IDF Selection: Compare attribution accuracy and computational cost with and without TF-IDF principal sentence selection across varying WINDOW_SIZE values to quantify its impact on performance and efficiency.