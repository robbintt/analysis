---
ver: rpa2
title: 'Statistics and explainability: a fruitful alliance'
arxiv_id: '2404.19301'
source_url: https://arxiv.org/abs/2404.19301
tags: []
core_contribution: The paper argues that standard statistical tools can address key
  foundational challenges in explainability research by providing proper mathematical
  definitions, theoretical guarantees, evaluation metrics, and uncertainty quantification
  for explanations. The core method idea is to interpret explanations as statistical
  quantities (e.g., variable importance measures) and leverage statistical estimators,
  convergence results, and resampling techniques to establish a rigorous framework.
---

# Statistics and explainability: a fruitful alliance

## Quick Facts
- arXiv ID: 2404.19301
- Source URL: https://arxiv.org/abs/2404.19301
- Authors: Valentina Ghidini
- Reference count: 23
- The paper proposes a statistical framework for XAI that provides mathematical definitions, theoretical guarantees, evaluation metrics, and uncertainty quantification for explanations

## Executive Summary
This paper addresses foundational challenges in explainability research by proposing a statistical framework for eXplainable Artificial Intelligence (XAI). The core insight is that explanations can be treated as statistical quantities, enabling the application of established statistical tools to define, evaluate, and quantify uncertainty in explanations. By leveraging statistical estimators, convergence results, and resampling techniques, the framework provides rigorous mathematical foundations for explainability that move beyond subjective human assessment. While not a complete solution, this approach offers a robust foundation for developing novel explainability methods with proper theoretical guarantees.

## Method Summary
The paper proposes treating explanations as statistical quantities, specifically variable importance measures, and applying standard statistical tools to establish a rigorous framework. The method involves defining explanations through statistical estimators, establishing convergence results using laws of large numbers and central limit theorems, and quantifying uncertainty through resampling techniques like bootstrap. This approach provides mathematically rigorous definitions, theoretical guarantees on explanation quality as sample size increases, and confidence intervals for explanations. The framework aims to address key challenges in XAI including the lack of proper definitions, theoretical guarantees, evaluation metrics, and uncertainty quantification.

## Key Results
- Statistical estimators can provide mathematically rigorous definitions for explanations
- Convergence results offer theoretical guarantees on explanation quality with increasing data
- Resampling techniques enable uncertainty quantification for explanations
- The framework circumvents subjective human assessment currently prevalent in explainability research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Statistical estimators provide mathematically rigorous definitions for explanations by treating them as well-defined quantities like variable importance measures
- Mechanism: By interpreting explanations as expected values of model output variations with respect to feature perturbations, standard statistical tools can define explanations in terms of probabilistic quantities that exist in well-defined probability spaces
- Core assumption: Explanations can be meaningfully represented as statistical quantities like variable importance measures that have theoretical foundations in statistics
- Evidence anchors:
  - [abstract] "leveraging statistical estimators allows for a proper definition of explanations"
  - [section] "explanations defined as in Equation (1) are just one (flexible) possibility showcased as example... statistics can serve as a valuable tool, offering a robust ground for the introduction of novel methods"
  - [corpus] Weak evidence - no direct citations about statistical definitions in corpus
- Break condition: If the relationship between feature perturbations and model behavior cannot be meaningfully quantified through expected values or if the perturbations themselves lack statistical properties

### Mechanism 2
- Claim: Statistical convergence results provide theoretical guarantees for explanation quality as sample size increases
- Mechanism: By using empirical estimators for explanations and applying laws of large numbers and central limit theorems, we can establish that estimated explanations converge to their true values as data increases, providing theoretical correctness guarantees
- Core assumption: The explanations defined as statistical quantities satisfy regularity conditions needed for convergence theorems to apply
- Evidence anchors:
  - [abstract] "enabling theoretical guarantees and the formulation of evaluation metrics"
  - [section] "it is natural to leverage standard statistical tools to establish convergence results... as the number of data points increases, the estimated explanations converge to their true, existing values"
  - [corpus] Weak evidence - no direct citations about convergence in corpus
- Break condition: If the regularity conditions for convergence theorems are violated or if the explanations are not well-defined statistical quantities

### Mechanism 3
- Claim: Statistical resampling techniques enable uncertainty quantification for explanations, making them more trustworthy
- Mechanism: By applying bootstrap or other resampling methods to explanation estimators, we can generate empirical distributions, confidence intervals, and variability measures that quantify the uncertainty in explanations
- Core assumption: The explanation estimators have sufficient statistical properties (e.g., finite variance, reasonable distribution shape) to make resampling meaningful
- Evidence anchors:
  - [abstract] "uncertainty quantification is essential for providing robust and trustworthy explanations, and it can be achieved in this framework through classical statistical procedures such as the bootstrap"
  - [section] "it is natural to incorporate uncertainty quantification into the resulting estimates, as is customary in any statistical analysis"
  - [corpus] Weak evidence - no direct citations about uncertainty quantification in corpus
- Break condition: If the resampling procedure is computationally prohibitive or if the explanation estimators have pathological statistical properties that make uncertainty quantification meaningless

## Foundational Learning

- Concept: Statistical estimators and their properties (bias, variance, consistency)
  - Why needed here: Understanding how statistical estimators work is fundamental to interpreting explanations as statistical quantities and establishing their theoretical properties
  - Quick check question: What conditions must a statistical estimator satisfy to be consistent, and why is consistency important for explanations?

- Concept: Resampling methods (bootstrap, permutation tests)
  - Why needed here: These methods are essential for quantifying uncertainty in explanations when analytical distributions are unavailable or intractable
  - Quick check question: How does the bootstrap procedure work, and what assumptions does it make about the underlying data distribution?

- Concept: Convergence theorems (law of large numbers, central limit theorem)
  - Why needed here: These theorems provide the theoretical foundation for establishing that explanation estimates improve with more data
  - Quick check question: Under what conditions does the central limit theorem apply to an estimator, and what does this tell us about the distribution of explanation estimates?

## Architecture Onboarding

- Component map:
  - Data preprocessing pipeline → Feature perturbation module → Model evaluation module → Statistical estimator → Uncertainty quantification module → Evaluation metrics

- Critical path:
  1. Define explanation as statistical quantity
  2. Implement estimator with convergence guarantees
  3. Apply resampling for uncertainty quantification
  4. Establish evaluation metrics based on true explanation values

- Design tradeoffs:
  - Computational cost vs. precision in resampling procedures
  - Model complexity vs. interpretability of explanations
  - Number of perturbations vs. statistical power of results

- Failure signatures:
  - Explanations show high variance across bootstrap samples
  - Convergence tests fail as sample size increases
  - Resampling distributions are highly skewed or multi-modal

- First 3 experiments:
  1. Implement a simple explanation estimator (e.g., permutation importance) and verify basic statistical properties
  2. Apply bootstrap to the estimator and analyze the distribution of resampled explanations
  3. Test convergence properties by varying sample size and measuring estimator stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we formally define a "purpose" for explanations that is applicable across different domains and applications?
- Basis in paper: [explicit] The paper identifies this as an open problem in section 5.1, noting that the purpose typically varies depending on the field and application.
- Why unresolved: The purpose of explanations is context-dependent and may vary based on whether the goal is fairness, model verification, understanding covariate dependence, or increasing practitioner trust. This makes it challenging to create a universal definition.
- What evidence would resolve it: Development of a framework that can adapt the definition of explanation purpose based on domain-specific requirements, possibly through a meta-analysis of use cases across different fields.

### Open Question 2
- Question: How can we ensure explanations are simple enough for users with diverse backgrounds while maintaining statistical rigor?
- Basis in paper: [explicit] The paper highlights in section 5.2 that simplicity is crucial for usability across a broad audience, yet maintaining statistical rigor is challenging.
- Why unresolved: Balancing interpretability with technical accuracy is difficult, as statistical measures may not be intuitive to non-statisticians, while oversimplification risks losing essential information.
- What evidence would resolve it: User studies comparing different explanation formats across various levels of statistical literacy, measuring comprehension and trust in the explanations.

### Open Question 3
- Question: How can we extend statistical frameworks to handle counterfactual and adversarial scenarios in explanations?
- Basis in paper: [explicit] The paper mentions in the conclusion that future research should define statistical counterparts for adversarial and counterfactual explanations.
- Why unresolved: Counterfactual and adversarial scenarios involve hypothetical changes that may not align with standard statistical inference frameworks, requiring new theoretical developments.
- What evidence would resolve it: Development of statistical methods that can quantify the probability or likelihood of counterfactual outcomes and assess the robustness of explanations under adversarial conditions.

## Limitations
- The statistical framework assumes explanations can be meaningfully represented as well-defined statistical quantities with regularity conditions for convergence theorems to apply
- The approach requires significant computational resources for resampling procedures, which could limit practical applicability for large-scale deployments
- The framework may not capture all types of explanations or model behaviors that cannot be quantified through statistical quantities

## Confidence

- **High Confidence**: The mathematical foundation of treating explanations as statistical estimators is sound and well-established
- **Medium Confidence**: The practical applicability across diverse XAI methods and model types remains to be extensively validated
- **Medium Confidence**: The computational feasibility for real-world applications needs empirical verification

## Next Checks
1. Test the framework on non-differentiable model architectures (e.g., tree ensembles) to verify estimator consistency across model types
2. Benchmark the computational overhead of bootstrap procedures against practical deployment constraints
3. Evaluate explanation stability under different perturbation schemes to assess robustness of the statistical assumptions