---
ver: rpa2
title: Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities
  through Open Large Language Models
arxiv_id: '2401.07115'
source_url: https://arxiv.org/abs/2401.07115
tags:
- personality
- they
- traits
- agents
- others
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the ability of open large language models (LLMs)
  to mimic human personalities. Using the Myers-Briggs Type Indicator (MBTI) test,
  we assessed both the intrinsic personality traits of 10 representative open LLMs
  and their capacity to emulate specific personalities through personality- and role-conditioning
  prompts.
---

# Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models

## Quick Facts
- arXiv ID: 2401.07115
- Source URL: https://arxiv.org/abs/2401.07115
- Reference count: 21
- Primary result: Most open LLMs exhibit intrinsic ENFJ personalities and struggle to mimic other personalities unless specifically tuned or combined with role conditioning.

## Executive Summary
This study investigates whether open large language models (LLMs) can mimic human personalities by assessing their intrinsic personality traits and their ability to emulate specific personalities through conditioning prompts. Using the Myers-Briggs Type Indicator (MBTI) test, the researchers evaluated 10 representative open LLMs across multiple experimental conditions, including unconditioned personality assessment, personality-conditioned prompting, and combined role-personality conditioning. The findings reveal that while each LLM exhibits distinct intrinsic personality traits, most demonstrate limited flexibility in personality mimicry, with only a few models successfully emulating imposed personalities. The study highlights the architectural constraints that contribute to these "closed-minded" behaviors and suggests that personality mimicry remains a significant challenge for open LLMs.

## Method Summary
The study employed a multi-stage assessment of 10 open LLM agents using the MBTI personality framework. First, models were evaluated for intrinsic personality traits through 60 MBTI questions presented individually. Next, personality-conditioning prompts were applied to assess mimicry capabilities, followed by combined role and personality conditioning experiments. Each condition involved multiple repetitions (N=10-30) to ensure statistical significance, with temperature settings varied (τ=0.01 and 0.7) to examine creativity effects. The assessment pipeline measured personality distribution patterns, accuracy of personality mimicry, and lexical/semantic similarity of generated personality descriptions against reference standards.

## Key Results
- Most open LLMs exhibit distinct intrinsic personalities, with ENFJ (Extraverted, Intuitive, Feeling, Judging) being the dominant type across models
- Personality-conditioned prompting produces limited effectiveness, with only SOLAR, Dolphin, and NeuralChat successfully mimicking imposed personalities
- Combining role and personality conditioning enhances personality mimicry, particularly for personalities associated with teaching roles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The personality of an LLM is strongly shaped by the intrinsic conditioning of its base architecture and training data, overriding system prompts unless the model is explicitly tuned for personality flexibility.
- **Mechanism:** The base architecture and pretraining data imprint latent personality traits that are expressed when the model is prompted without personality conditioning. Conditioning prompts can override these traits only if the model has sufficient architectural flexibility (e.g., SOLAR, Dolphin, NeuralChat) or if the temperature setting encourages exploration of alternative responses.
- **Core assumption:** Personality traits are embedded in the learned weights of the model and are not easily overwritten by simple prompting.
- **Evidence anchors:** [abstract] "each Open LLM agent showcases distinct human personalities; (ii) personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being “closed-minded” (i.e., they retain their intrinsic traits)"
- **Break condition:** If the model's base architecture lacks sufficient capacity for personality adaptation, or if the conditioning prompt is too generic, the model will default to its intrinsic personality traits.

### Mechanism 2
- **Claim:** Temperature settings modulate the degree of personality flexibility in LLMs, with higher temperatures enabling greater exploration of alternative personality traits.
- **Mechanism:** Higher temperature settings increase the randomness of the model's output, allowing it to explore a wider range of personality traits beyond its intrinsic ones. However, this increased exploration does not guarantee successful personality mimicry, as the model may still default to its base personality traits.
- **Core assumption:** Temperature settings influence the model's ability to deviate from its intrinsic personality traits.
- **Evidence anchors:** [abstract] "combining role and personality conditioning can enhance the agents' ability to mimic human personalities"
- **Break condition:** If the temperature is set too high, the model's output may become too random and incoherent, making it difficult to discern any consistent personality traits.

### Mechanism 3
- **Claim:** Role-conditioning, when combined with personality-conditioning, can enhance the model's ability to mimic specific human personalities, especially for personalities associated with the role of a teacher.
- **Mechanism:** Role-conditioning provides additional context and constraints that guide the model's personality mimicry. When the role is closely aligned with the target personality (e.g., teacher for ENFJ), the model is more likely to successfully emulate the desired personality traits.
- **Core assumption:** Role-conditioning provides relevant context that complements personality-conditioning.
- **Evidence anchors:** [abstract] "personalities typically associated with the role of teacher tend to be emulated with greater accuracy"
- **Break condition:** If the role-conditioning is not closely aligned with the target personality, or if the model lacks sufficient flexibility to incorporate the role context, the model may not successfully emulate the desired personality traits.

## Foundational Learning

- **Concept:** Myers-Briggs Type Indicator (MBTI) personality test
  - **Why needed here:** The MBTI test is used to assess the personality traits of the LLM agents and to condition them to specific personalities.
  - **Quick check question:** What are the four dichotomies used in the MBTI test to categorize personality types?

- **Concept:** System prompts and conditioning
  - **Why needed here:** System prompts are used to condition the LLM agents to specific personalities and roles, guiding their behavior and output.
  - **Quick check question:** How do system prompts influence the behavior of LLM agents, and what are the key components of an effective conditioning prompt?

- **Concept:** Temperature settings in LLM inference
  - **Why needed here:** Temperature settings control the randomness of the model's output, affecting its ability to explore alternative personality traits.
  - **Quick check question:** How does the temperature setting influence the model's output, and what are the trade-offs between creativity and coherence at different temperature levels?

## Architecture Onboarding

- **Component map:** LLM agents (Mixtral, Llama2 variants, SOLAR, etc.) -> System messages (conditioning prompts) -> Temperature settings -> MBTI test (assessment tool)
- **Critical path:** 1. Load and configure the LLM agents with their respective system messages 2. Administer the MBTI test to assess the agents' intrinsic personality traits 3. Apply personality-conditioning prompts to guide the agents' behavior 4. Optionally, apply role-conditioning prompts to further refine the agents' personality mimicry 5. Evaluate the agents' personality mimicry using the MBTI test
- **Design tradeoffs:** Flexibility vs. consistency: Higher temperature settings increase creativity but may reduce consistency in personality mimicry; Specificity vs. generality: More specific conditioning prompts may be more effective but may also limit the agents' ability to adapt to new situations; Computational cost vs. accuracy: More complex conditioning prompts or higher temperature settings may require more computational resources but may also improve accuracy
- **Failure signatures:** Inability to mimic the target personality: The agent consistently exhibits its intrinsic personality traits despite conditioning prompts; Inconsistent behavior: The agent's output is too random or incoherent, making it difficult to discern any consistent personality traits; Overfitting to the conditioning prompt: The agent's output is too closely aligned with the conditioning prompt, lacking flexibility or adaptability
- **First 3 experiments:** 1. Assess the intrinsic personality traits of the LLM agents using the MBTI test 2. Apply personality-conditioning prompts to guide the agents' behavior and evaluate their effectiveness 3. Experiment with different temperature settings to find the optimal balance between creativity and consistency in personality mimicry

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do model size and architecture specifically influence personality type and adaptability in open LLMs?
- Basis in paper: [inferred] The paper notes that model size doesn't seem to have a tangible impact on personality preferences, but some models with different architectures (like Mixtral's mixture-of-experts) exhibit distinct personalities. It also mentions that certain architectural choices (e.g., depth upscaling in SOLAR, fine-tuning datasets) lead to improved personality mimicking.
- Why unresolved: The paper provides some anecdotal evidence linking architecture to personality traits, but lacks a systematic analysis of how different architectural choices (size, type, training data, fine-tuning methods) affect personality emergence and adaptability.
- What evidence would resolve it: Controlled experiments varying architectural parameters (size, type, training data, fine-tuning methods) while measuring personality type and adaptability using the MBTI test.

### Open Question 2
- Question: Can combining personality and role conditioning be optimized to improve personality mimicry beyond the current best-performing models?
- Basis in paper: [explicit] The paper finds that combining personality and role conditioning can enhance personality mimicry, particularly for agents already demonstrating high flexibility. However, most models still struggle with this combined approach.
- Why unresolved: The paper only explores a limited set of roles and personalities. It doesn't investigate optimal combinations or conditioning strategies to maximize personality mimicry.
- What evidence would resolve it: Systematic exploration of different role-personality combinations and conditioning strategies, measuring their impact on personality mimicry accuracy.

### Open Question 3
- Question: How do personality traits in open LLMs affect their performance on downstream tasks and real-world applications?
- Basis in paper: [inferred] The paper focuses on understanding personality emergence and mimicry in open LLMs but doesn't explore the practical implications of these personality traits on task performance or real-world applications.
- Why unresolved: The paper doesn't investigate how personality traits influence LLM behavior in specific tasks or real-world scenarios, limiting the understanding of their practical impact.
- What evidence would resolve it: Experiments evaluating the performance of open LLMs with different personality traits on various downstream tasks and real-world applications, measuring the correlation between personality and task success.

## Limitations

- The study relies on the Myers-Briggs framework, which has ongoing psychometric validity debates and may not reflect genuine personality characteristics
- Most models demonstrate "closed-mindedness," retaining intrinsic personality traits despite conditioning, suggesting fundamental architectural constraints
- Temperature-dependent effects indicate personality mimicry may be more stochastic than controllable, raising reproducibility concerns

## Confidence

**High Confidence**: The observation that open LLMs exhibit distinct intrinsic personalities (ENFJ dominant) is well-supported by consistent assessment results across multiple models and the 60-question MBTI evaluation.

**Medium Confidence**: Claims about personality-conditioned prompting effectiveness are supported but show significant variation across models, requiring deeper architectural analysis to fully explain differential success.

**Low Confidence**: The assertion that "teacher-associated personalities" are emulated with greater accuracy needs additional validation, as it conflates role-conditioning effects with personality accuracy.

## Next Checks

1. **Architectural Dependency Analysis**: Systematically test whether personality mimicry success correlates with specific architectural features (attention mechanisms, training objectives, parameter counts) across the model family rather than treating models as independent samples.

2. **Psychometric Validation**: Replicate personality assessments using alternative validated frameworks (Big Five, HEXACO) to determine whether MBTI-specific findings generalize to other personality taxonomies.

3. **Prompt Engineering Robustness**: Conduct ablation studies varying conditioning prompt specificity, formatting, and contextual framing to establish whether observed "closed-mindedness" reflects architectural constraints or suboptimal prompt engineering.