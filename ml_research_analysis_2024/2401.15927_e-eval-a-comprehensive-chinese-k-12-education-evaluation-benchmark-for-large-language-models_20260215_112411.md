---
ver: rpa2
title: 'E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for Large
  Language Models'
arxiv_id: '2401.15927'
source_url: https://arxiv.org/abs/2401.15927
tags: []
core_contribution: This paper introduces E-EVAL, the first comprehensive evaluation
  benchmark designed specifically for the Chinese K-12 education field. The benchmark
  consists of 4,351 multiple-choice questions across primary, middle, and high school
  levels, covering subjects like Chinese, English, Physics, Chemistry, and Mathematics.
---

# E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2401.15927
- Source URL: https://arxiv.org/abs/2401.15927
- Reference count: 21
- First comprehensive Chinese K-12 education benchmark with 4,351 multiple-choice questions across primary, middle, and high school levels

## Executive Summary
This paper introduces E-EVAL, the first comprehensive evaluation benchmark specifically designed for Chinese K-12 education. The benchmark comprises 4,351 multiple-choice questions spanning primary, middle, and high school levels across subjects including Chinese, English, Physics, Chemistry, and Mathematics. The authors conducted evaluations on advanced LLMs, comparing English-dominant and Chinese-dominant models, and found that Chinese-dominant models performed competitively, with many scoring above GPT 4.0, though all models struggled significantly with complex mathematics questions. The study also revealed that Chain of Thought prompting was effective primarily for science subjects, while Few-shot prompting showed greater benefits for liberal arts subjects.

## Method Summary
The benchmark consists of 4,351 multiple-choice questions across K-12 education levels, covering core subjects including Chinese, English, Physics, Chemistry, and Mathematics. The evaluation framework tested both English-dominant and Chinese-dominant large language models using various prompting techniques including Chain of Thought and Few-shot prompting. Performance metrics were collected and analyzed across different subject domains and educational levels to identify model strengths and weaknesses in educational contexts.

## Key Results
- Chinese-dominant models performed well compared to English-dominant models, with many scoring above GPT 4.0
- All models performed poorly in complex subjects such as mathematics
- Chain of Thought technique was effective only for challenging science subjects, while Few-shot prompting was more beneficial for liberal arts subjects

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of Chinese K-12 curriculum across multiple subjects and difficulty levels, providing a standardized evaluation framework for educational AI applications. The comparative analysis between language-dominant models reveals how training data and language focus impact educational performance. The differential effectiveness of prompting techniques across subject domains suggests that reasoning strategies need to be tailored to subject-specific cognitive requirements.

## Foundational Learning

**Chinese K-12 Curriculum Structure**: Understanding the organization of Chinese primary, middle, and high school education systems, including subject progression and difficulty levels. Needed to contextualize benchmark question distribution and difficulty calibration. Quick check: Verify benchmark question distribution matches official curriculum standards.

**Large Language Model Evaluation Metrics**: Knowledge of standard LLM evaluation methodologies, including accuracy metrics, cross-model comparisons, and prompt engineering techniques. Needed to properly interpret performance results and benchmarking methodology. Quick check: Confirm evaluation metrics align with established LLM assessment practices.

**Prompt Engineering Techniques**: Understanding Chain of Thought reasoning and Few-shot prompting methods, including their implementation and expected effects on model performance. Needed to interpret why different prompting strategies work better for different subject domains. Quick check: Validate prompt implementation consistency across all evaluated models.

## Architecture Onboarding

**Component Map**: E-EVAL Benchmark (4,351 questions) -> LLM Models (Chinese/English-dominant) -> Prompting Techniques (CoT/Few-shot) -> Performance Metrics -> Subject-Domain Analysis

**Critical Path**: Benchmark questions are presented to models using specific prompting techniques, model responses are evaluated for accuracy, results are aggregated by subject domain, and performance patterns are analyzed to identify strengths and weaknesses across models and prompting strategies.

**Design Tradeoffs**: The benchmark prioritizes comprehensive subject coverage over individual question complexity, choosing breadth to represent the full K-12 curriculum. Prompting techniques balance between reasoning depth (CoT) and efficiency (Few-shot), with different strategies optimized for different subject cognitive demands.

**Failure Signatures**: Poor performance in mathematics indicates fundamental limitations in numerical reasoning and complex problem-solving. Ineffective CoT prompting in liberal arts suggests that step-by-step reasoning may not align with the intuitive or pattern-based thinking required for these subjects.

**First Experiments**:
1. Run baseline evaluations on both Chinese and English-dominant models without any prompting techniques to establish performance floors
2. Apply CoT prompting uniformly across all subjects to identify domains where step-by-step reasoning provides benefits
3. Implement Few-shot prompting with subject-specific examples to determine optimal sample sizes for different educational domains

## Open Questions the Paper Calls Out
None

## Limitations
- The 4,351 questions may not fully capture the breadth of Chinese K-12 curricula across all regions and difficulty levels
- The benchmark's construction methodology and question selection criteria are not detailed, raising questions about potential sampling bias
- Comparative analysis assumes comparable evaluation conditions, but translation quality and cultural context handling could significantly influence results

## Confidence
- E-EVAL as comprehensive Chinese K-12 benchmark: Medium confidence (benchmark existence confirmed, but comprehensiveness not independently verified)
- Model performance comparisons between language-dominant models: Medium confidence (results reported but methodology limitations acknowledged)
- CoT effectiveness in science subjects: Low confidence (limited experimental validation shown)
- Few-shot prompting benefits for liberal arts: Low confidence (minimal empirical support provided)

## Next Checks
1. Conduct inter-rater reliability assessment on a sample of benchmark questions to verify consistency in difficulty calibration and subject coverage across K-12 levels
2. Perform cross-lingual model evaluation with identical prompts in both Chinese and English to isolate language-specific performance differences
3. Design controlled experiments testing CoT and Few-shot prompting across identical question sets in both science and liberal arts domains to validate domain-specific effectiveness claims