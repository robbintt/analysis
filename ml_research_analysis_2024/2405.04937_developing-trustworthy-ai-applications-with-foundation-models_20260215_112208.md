---
ver: rpa2
title: Developing trustworthy AI applications with foundation models
arxiv_id: '2405.04937'
source_url: https://arxiv.org/abs/2405.04937
tags: []
core_contribution: This whitepaper presents a systematic approach to assess and ensure
  the trustworthiness of AI applications developed using foundation models. The methodology,
  based on the "AI Assessment Catalog" by Fraunhofer IAIS, adapts existing risk-based
  frameworks to account for the unique challenges posed by foundation models.
---

# Developing trustworthy AI applications with foundation models

## Quick Facts
- arXiv ID: 2405.04937
- Source URL: https://arxiv.org/abs/2405.04937
- Authors: Michael Mock; Sebastian Schmidt; Felix Müller; Rebekka Görge; Anna Schmitz; Elena Haedecke; Angelika Voss; Dirk Hecker; Maximillian Poretschkin
- Reference count: 17
- Primary result: Systematic approach to assess and ensure trustworthiness of AI applications using foundation models

## Executive Summary
This whitepaper presents a systematic methodology for developing trustworthy AI applications using foundation models, adapting the established risk-based approach from the Fraunhofer IAIS "AI Assessment Catalog" to the unique challenges posed by foundation models. The approach addresses six dimensions of trustworthiness—fairness, autonomy & control, transparency, reliability, safety & security, and data protection—through application-specific risk analysis and tailored evaluation metrics. The methodology provides practical guidance for foundation model selection, data usage, development, testing, and monitoring while contextualizing these practices within the emerging European AI Regulation framework.

## Method Summary
The methodology transfers a risk-based approach for testing and ensuring trustworthiness from conventional AI applications to foundation models by adapting existing trustworthiness frameworks and benchmarks. It involves conducting application-specific risk analysis to determine which trustworthiness dimensions are most relevant, selecting suitable foundation models based on benchmark performance, choosing appropriate data for development and testing, and implementing monitoring during operation. The approach leverages foundation models' capabilities for synthetic data generation and automated testing while acknowledging the challenges of comprehensive evaluation due to the models' general-purpose nature.

## Key Results
- Risk-based approach from conventional AI can be adapted to foundation models through application-specific analysis
- Foundation models offer new possibilities for testing and quality assurance through synthetic data generation and automated test case creation
- European AI Regulation requirements can be integrated into the trustworthiness assessment framework for foundation model applications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Risk-based, application-specific approach transfers successfully to foundation models by adapting existing trustworthiness frameworks
- **Mechanism**: Existing AI trustworthiness frameworks define risk dimensions and evaluation criteria that can be applied to foundation model-based applications by identifying application-specific risks
- **Core assumption**: Foundation model-based AI applications share the same trustworthiness dimensions as conventional AI applications
- **Evidence anchors**: [abstract], [section 6.1]
- **Break condition**: If risk dimensions for foundation model applications fundamentally differ from conventional AI applications

### Mechanism 2
- **Claim**: Foundation models can be evaluated using benchmarks and metrics to assess their suitability for specific applications
- **Mechanism**: Standardized benchmarks provide metrics for evaluating foundation models across various tasks and dimensions that can be used for model selection
- **Core assumption**: Publicly available benchmarks accurately reflect real-world performance and trustworthiness
- **Evidence anchors**: [section 6.3]
- **Break condition**: If benchmarks are not representative of real-world application performance

### Mechanism 3
- **Claim**: Foundation models offer new possibilities for testing and quality assurance during development and operation
- **Mechanism**: Foundation models can generate synthetic test data, automate test case generation, and support continuous monitoring
- **Core assumption**: Foundation models can generate high-quality synthetic data that accurately represents application domain
- **Evidence anchors**: [section 6.5], [section 6.6]
- **Break condition**: If synthetic data introduces new biases or fails to represent real-world scenarios

## Foundational Learning

- **Concept**: Risk-based approach to trustworthiness
  - Why needed here: Understanding how to assess and mitigate risks in AI applications is crucial for ensuring trustworthiness, especially with foundation models
  - Quick check question: Can you explain the difference between a risk-based approach and a technology-based approach to AI regulation?

- **Concept**: Foundation model capabilities and limitations
  - Why needed here: Developers need to understand unique properties of foundation models to effectively use them in AI applications and assess their trustworthiness
  - Quick check question: What are the key differences between supervised learning and self-supervised learning in the context of foundation models?

- **Concept**: European AI Regulation requirements
  - Why needed here: The AI Regulation will significantly impact development and deployment of AI applications including those based on foundation models
  - Quick check question: How does the AI Regulation classify AI applications based on risk, and what are the implications for foundation models?

## Architecture Onboarding

- **Component map**: Foundation model -> Application-specific adaptation -> Testing and validation -> External systems
- **Critical path**:
  1. Define target function and application domain
  2. Conduct application-specific risk analysis and determine metrics
  3. Select suitable foundation model based on benchmarks and metrics
  4. Develop and test AI application using foundation model capabilities
  5. Implement monitoring and quality assurance during operation

- **Design tradeoffs**:
  - Performance vs. trustworthiness: Optimizing for one dimension may negatively impact another
  - Data requirements: Zero-shot approaches require minimal data but may have lower performance
  - Complexity vs. interpretability: Complex models may achieve higher performance but are harder to interpret

- **Failure signatures**:
  - Hallucinations: Foundation models generating factually incorrect but plausible-sounding outputs
  - Bias and discrimination: Foundation models reflecting or amplifying biases present in training data
  - Data leakage: Foundation models inadvertently exposing sensitive information from training data

- **First 3 experiments**:
  1. Evaluate performance of different foundation models on a relevant benchmark task using available metrics
  2. Generate synthetic test data using a foundation model and compare its quality and coverage to real-world data
  3. Implement a simple prompt engineering approach to adapt a foundation model for a specific task and assess its performance and trustworthiness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can foundation models be systematically evaluated for trustworthiness across all potential application domains given their inherent incompleteness of assessment?
- Basis in paper: [explicit] "it is in principle impossible to provide sufficient and appropriate evidence for the overall assessment... it is just impossible to verify that a foundation model 'can do everything'"
- Why unresolved: Foundation models are designed to handle unlimited tasks across various domains, making comprehensive evaluation practically impossible
- What evidence would resolve it: Development of novel evaluation frameworks that can meaningfully assess foundation models without requiring exhaustive testing across all domains

### Open Question 2
- Question: What standardized benchmarks and metrics can effectively quantify trade-offs between different dimensions of trustworthiness in foundation models?
- Basis in paper: [explicit] "Optimizing an application for a single metric is therefore rarely the right approach... How trade-offs are evaluated and which trustworthiness dimensions are given particular weighting depends on the application"
- Why unresolved: Current benchmarks typically focus on individual metrics rather than providing a comprehensive framework for evaluating trade-offs
- What evidence would resolve it: Creation of multi-dimensional benchmarking frameworks that can simultaneously evaluate multiple trustworthiness metrics

### Open Question 3
- Question: How can the data coverage of foundation models' training data for specific application domains be technically verified given the challenges with unstructured data?
- Basis in paper: [explicit] "the challenge is that the coverage of the application domain by the training data cannot be easily examined in most cases... transferred to unstructured data... requires cost-intensive and time-consuming annotations"
- Why unresolved: Current methods for verifying data coverage are inadequate for unstructured data where traditional annotation approaches are too resource-intensive
- What evidence would resolve it: Development of automated or semi-automated methods for assessing data coverage that leverage foundation models themselves

## Limitations
- Lack of standardized metrics for many trustworthiness dimensions in foundation models
- Potential for benchmark overfitting and disconnect from real-world performance
- Challenge of validating synthetic test data quality and representativeness

## Confidence
- Risk-based transfer effectiveness: Medium-High
- Benchmark reliability for model selection: Medium
- Synthetic data quality for testing: Medium-Low

## Next Checks
1. Conduct empirical studies comparing effectiveness of synthetic test data generated by foundation models versus real-world data for detecting trustworthiness issues
2. Evaluate whether existing benchmarks accurately predict real-world performance and trustworthiness failures across different foundation model architectures
3. Test the framework's risk assessment methodology on actual foundation model deployments to identify gaps in dimension coverage and assessment criteria