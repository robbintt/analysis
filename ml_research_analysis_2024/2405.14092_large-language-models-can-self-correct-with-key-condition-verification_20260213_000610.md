---
ver: rpa2
title: Large Language Models Can Self-Correct with Key Condition Verification
arxiv_id: '2405.14092'
source_url: https://arxiv.org/abs/2405.14092
tags:
- answer
- question
- proco
- verification
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of large language models' inability
  to self-correct reasoning errors without external feedback. The authors propose
  PROCO, a method that identifies a key condition in a question, masks it, and constructs
  a verification question using the current answer as a new condition.
---

# Large Language Models Can Self-Correct with Key Condition Verification

## Quick Facts
- arXiv ID: 2405.14092
- Source URL: https://arxiv.org/abs/2405.14092
- Reference count: 31
- Primary result: PROCO achieves +6.8 EM, +14.1 accuracy, +9.6 accuracy over Self-Correct

## Executive Summary
This paper addresses the fundamental limitation of large language models in self-correcting reasoning errors without external feedback. The authors propose PROCO (Progressive Correction), a method that enables iterative verification and correction by identifying key conditions in questions, masking them, and constructing verification questions using the current answer as a new condition. PROCO achieves substantial improvements across multiple reasoning benchmarks compared to existing self-correction methods.

## Method Summary
PROCO introduces an iterative verify-then-correct framework that enables LLMs to self-correct reasoning errors. The method identifies a key condition in the original question, masks it, and constructs a verification question by adding the current answer as a new condition. This verification question is solved to produce a verified answer, which is then compared to the masked key condition to assess correctness. If incorrect, the answer is added to a set of potentially incorrect answers and used as feedback to guide future corrections. The process repeats for a fixed number of iterations, progressively refining the answer.

## Key Results
- +6.8 exact match improvement on open-domain QA datasets (NQ, TriviaQA, WebQ, HotpotQA)
- +14.1 accuracy improvement on arithmetic reasoning datasets (GSM8K, AQuA, MATH)
- +9.6 accuracy improvement on commonsense reasoning tasks (CSQA)
- PROCO outperforms Self-Correct, CoT, RAG, and GenRead across all tested benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mask-based substitute verification enables accurate self-correction without external feedback.
- Mechanism: The system masks a key condition in the question, adds the current answer as a new condition, and generates a verification question. Solving this verification question produces a verified answer that can be compared to the masked condition.
- Core assumption: The masked condition remains uniquely identifiable through the verification process and the LLM can accurately solve the constructed verification question.
- Evidence anchors:
  - [abstract]: "That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response."
  - [section 4.3]: "The verification phase uses substitute verification method to verify the correctness of the previous generated answer at−1. This phase encompasses several substeps... By replacing the key condition with a specific token 'X', we create a masked question."
- Break condition: The verification question construction fails if the key condition is ambiguous or if the LLM cannot accurately solve the constructed verification question.

### Mechanism 2
- Claim: Iterative verify-then-correct cycles progressively improve answer quality by preventing repetition of previous mistakes.
- Mechanism: After verification identifies an incorrect answer, it is added to a set of potentially incorrect answers. Future corrections use this set as feedback with the hint "the answer is likely not in {set}" to guide the LLM away from previous errors.
- Core assumption: LLMs can effectively use explicit feedback about incorrect answers to avoid repeating the same mistakes in subsequent attempts.
- Evidence anchors:
  - [abstract]: "PROCO employs the substitute verification method to verify the correctness of LLM-generated answers. If an answer is deemed incorrect, PROCO adds it to a set of potentially incorrect answers, which then serves as feedback to guide LLMs in correcting previous mistakes."
  - [section 4.3]: "For incorrect answers, following PRP (Wu et al., 2024a), we can use the Prompt A.4 to correct them."
- Break condition: The correction process fails if the LLM continues to generate answers from the set of known incorrect answers despite explicit feedback.

### Mechanism 3
- Claim: Proposition-based verification handles cases where verification questions have multiple valid answers.
- Mechanism: Instead of directly comparing the verified answer to the key condition, the system constructs a proposition asking whether "If the answer to verification question Q(v) is ck, then X could also be a(v)" and uses LLM judgment to verify this proposition.
- Core assumption: LLMs can accurately judge the logical equivalence of multiple valid answers to verification questions.
- Evidence anchors:
  - [section 4.3]: "For open-domain or commonsense questions, we propose a proposition-based verification method... We construct an answer verification prompt: 'Determine the correctness of the proposition: If the answer to question Q(v) is ck, then X could also be a(v)'."
  - [appendix A.4.2]: "We use the substitute verification method to verify the correctness of the previous generated answer... By checking if the answer for the verification question and the key condition are equivalent, we can assess the correctness of the previous generated answer."
- Break condition: The proposition-based verification fails if the LLM cannot accurately judge the logical relationship between multiple valid answers.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: PROCO builds on CoT by first generating an initial answer using reasoning paths, then iteratively verifying and correcting it
  - Quick check question: What is the primary purpose of appending "Let's think step by step" in CoT prompting?

- Concept: Key condition identification
  - Why needed here: The system requires accurate identification of key conditions (numerical values, entities, or concepts) that are crucial for problem-solving and verification
  - Quick check question: How does the system identify numerical values as key conditions in arithmetic reasoning tasks?

- Concept: Iterative refinement with feedback
  - Why needed here: The verify-then-correct loop requires understanding how to use feedback from previous iterations to improve subsequent answers
  - Quick check question: What happens to an answer that is verified as incorrect in the iterative correction process?

## Architecture Onboarding

- Component map: Initial Answer Generator -> Key Condition Identifier -> Verification Question Constructor -> Answer Verifier -> Answer Corrector -> Iteration Controller

- Critical path: Initial Answer → Key Condition Identification → Verification Question Construction → Answer Verification → (if incorrect) Answer Correction → Repeat until termination

- Design tradeoffs:
  - Maximum iterations vs. token efficiency: Setting T=3 balances accuracy gains against computational cost
  - Exact match vs. proposition-based verification: Different strategies for different reasoning types
  - External documents vs. self-correction: Trade-off between accuracy and token efficiency

- Failure signatures:
  - No improvement across iterations: Indicates the verification process may not be effectively identifying errors
  - Oscillation between answers: Suggests the correction mechanism isn't properly using feedback
  - High token consumption: May indicate excessive iterations or inefficient verification construction

- First 3 experiments:
  1. Test PROCO on GSM8K arithmetic reasoning with T=1 iteration to establish baseline improvement over CoT
  2. Run PROCO on NQ open-domain QA with both match-based and proposition-based verification to compare strategies
  3. Measure token consumption and accuracy trade-off by varying T from 1 to 5 iterations on CSQA commonsense reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PROCO's performance scale with increasingly longer and more complex reasoning problems beyond the 52.3 word average used in this study?
- Basis in paper: [inferred] The paper mentions that problems solved are generally short, averaging 52.3 words, with answers typically being numerical values or entities. The authors note that accurately solving much longer problems or those where answers are not numerical or entity-based is considered future research.
- Why unresolved: The paper only tested on problems of moderate length with specific answer types (numerical values or entities). There's no evidence of PROCO's effectiveness on longer, more complex problems or different answer formats.
- What evidence would resolve it: Experimental results showing PROCO's performance on datasets with significantly longer problems (e.g., 200+ words) and varied answer types (e.g., multi-sentence explanations, abstract concepts).

### Open Question 2
- Question: Can PROCO's key condition identification method be effectively adapted for non-English languages, particularly those with different grammatical structures?
- Basis in paper: [explicit] The authors state that the study focused exclusively on English reasoning tasks, with non-English tasks excluded from training and test data. They acknowledge this as a limitation and mention future research will explore solutions for multilingual complex reasoning tasks.
- Why unresolved: The current key condition identification relies on language-specific features like regular expressions for numerical values and entity/concept identification prompts that may not translate well to other languages.
- What evidence would resolve it: Successful implementation and testing of PROCO on multiple non-English language datasets, demonstrating comparable or improved performance to the English results.

### Open Question 3
- Question: What is the impact of PROCO's iterative verification process on computational resources and real-time application feasibility?
- Basis in paper: [inferred] While the paper mentions efficiency comparisons with Self-Correct and provides average token/time consumption data, it doesn't fully explore the resource implications of scaling PROCO for real-world applications or high-volume usage.
- Why unresolved: The paper provides some efficiency metrics but doesn't analyze the computational cost scaling with problem complexity or discuss practical limitations for deployment in resource-constrained environments.
- What evidence would resolve it: Comprehensive resource usage analysis including CPU/GPU utilization, memory requirements, and processing time across various problem scales, along with recommendations for optimizing PROCO for different deployment scenarios.

## Limitations

- Dependence on accurate key condition identification, where failure in this step causes the entire verification framework to fail
- Increased computational costs due to iterative verification process, creating trade-offs between accuracy and token efficiency
- Limited to English-language benchmarks, raising questions about generalizability to other languages or specialized domains

## Confidence

- **High confidence** in the core mechanism's theoretical soundness and the empirical improvements demonstrated on standard benchmarks
- **Medium confidence** in the method's robustness across diverse reasoning types, as the study shows performance variations between arithmetic, commonsense, and open-domain tasks
- **Low confidence** in the scalability of the approach to real-world applications where computational resources and response time are constrained

## Next Checks

1. Test PROCO's performance on multilingual datasets to evaluate cross-lingual generalizability and identify potential language-specific limitations in key condition identification

2. Conduct ablation studies systematically removing each component (key condition masking, verification construction, feedback integration) to quantify individual contribution to overall performance gains

3. Measure computational efficiency by tracking token consumption and response time across different iteration counts (T=1, 3, 5) on a representative subset of problems from each benchmark category