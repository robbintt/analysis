---
ver: rpa2
title: Improving Resistance to Noisy Label Fitting by Reweighting Gradient in SAM
arxiv_id: '2411.17132'
source_url: https://arxiv.org/abs/2411.17132
tags:
- noisy
- accuracy
- saner
- noise
- fitting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes Sharpness-Aware Minimization (SAM)'s gradient
  behavior in noisy label settings and identifies specific gradient components that
  contribute to SAM's resistance against overfitting noisy labels. The authors observe
  that certain gradient components in SAM are down-weighted compared to SGD, and these
  components have a stronger impact on noisy fitting than clean fitting.
---

# Improving Resistance to Noisy Label Fitting by Reweighting Gradient in SAM

## Quick Facts
- arXiv ID: 2411.17132
- Source URL: https://arxiv.org/abs/2411.17132
- Authors: Hoang-Chau Luong; Thuc Nguyen-Quang; Minh-Triet Tran
- Reference count: 40
- Key outcome: SANER achieves up to 8% improvement over SAM on CIFAR-100 with 50% label noise by explicitly reducing gradient components that contribute more to noisy fitting than clean fitting

## Executive Summary
This paper analyzes Sharpness-Aware Minimization's (SAM) behavior in noisy label settings and identifies specific gradient components that contribute to SAM's resistance against overfitting noisy labels. The authors observe that certain gradient components in SAM are down-weighted compared to SGD, and these components have a stronger impact on noisy fitting than clean fitting. Based on this insight, they propose SANER (Sharpness-Aware Noise-Explicit Reweighting), which explicitly reduces the magnitude of these components to enhance noise resistance. SANER achieves up to 8% improvement over SAM on CIFAR-100 with 50% label noise, demonstrates consistent improvements across various architectures and noise types, and shows superior performance in overfitting scenarios.

## Method Summary
SANER is a variant of SAM that enhances noise resistance by explicitly reweighting gradient components. The method analyzes the ratio between SAM and SGD gradients component-wise, identifying "Group B" components where this ratio is less than 1. These components are down-weighted in SAM compared to SGD. SANER applies an additional scaling factor α < 1 to these Group B components to further reduce their magnitude. A linear scheduler gradually decreases α from 1 to a target value over k epochs to prevent early-stage clean fitting degradation while maintaining noise resistance in later training stages.

## Key Results
- SANER achieves up to 8% improvement over SAM on CIFAR-100 with 50% label noise
- Consistently outperforms SAM across various architectures including ResNet18, ResNet34, DenseNet121, WideResNet40-2, and WideResNet28-10
- Demonstrates superior performance in overfitting scenarios including increased model width, training without data augmentation, and limited dataset sizes
- Shows robust performance across different noise types: symmetric, asymmetric, instance-dependent, and real-world noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAM implicitly down-weights specific gradient components (Group B) that contribute more to noisy fitting than clean fitting.
- Mechanism: During each iteration, SAM's gradient calculation naturally reduces the magnitude of certain components that have opposing signs between clean and noisy gradients. This reduction disproportionately affects noise-dominated parameters.
- Core assumption: The gradient components where clean and noisy gradients have opposite signs are primarily responsible for fitting noise.
- Evidence anchors:
  - [section 3] "Group B, where gradient components are down-weighted, covers around 30-40% of the parameters during most of the training phase"
  - [section 4] "pr value of Group B is less than 1 in the early stages of training. However, after 25 epochs, Group B consistently shows pr > 1"
- Break condition: If clean and noisy gradients align rather than oppose, the down-weighting would affect both equally and lose its noise-filtering advantage.

### Mechanism 2
- Claim: Further reducing Group B components through explicit reweighting enhances noise resistance without significantly harming clean fitting.
- Mechanism: By applying an additional scaling factor α < 1 to Group B components in SAM's gradient, SANER amplifies SAM's natural noise-filtering behavior, particularly effective in later training stages when noise overfitting dominates.
- Core assumption: The components most affected by this reduction are noise-dominated rather than clean-dominated, as evidenced by pr > 1 in later training.
- Evidence anchors:
  - [section 5.1] "SANER, a variant of SAM that has superior resistance to fitting noisy labels compared to SAM"
  - [section 5.1] "higher values of α (e.g., 2, 1.5, 1.25, 1.1) accelerate noisy fitting compared to SAM (α = 1)"
- Break condition: If the scheduler fails to adapt α appropriately, early training could be slowed by excessive reduction of clean-dominated components.

### Mechanism 3
- Claim: The scheduler that gradually reduces α from 1 to target value prevents early-stage clean fitting degradation while maintaining noise resistance.
- Mechanism: Since pr < 1 early in training (meaning Group B affects clean fitting more), keeping α = 1 initially allows normal learning. The linear decay to α < 1 in later epochs targets the phase where pr > 1, maximizing noise resistance when needed.
- Core assumption: The transition from pr < 1 to pr > 1 occurs predictably during training, allowing targeted intervention.
- Evidence anchors:
  - [section 5.1] "we propose a simple yet effective solution: a linear scheduler that gradually decreases α from 1 to a predetermined value over k epochs"
  - [section 5.1] "As shown in Figure 5(c), experiments on ResNet34 with CIFAR-100 under 50% label noise demonstrate that SANER without the α scheduler (k = 0) significantly harms clean accuracy, reducing it by approximately 15%"
- Break condition: If the transition timing varies significantly across architectures or datasets, fixed k values may be suboptimal.

## Foundational Learning

- Concept: Gradient component-wise analysis
  - Why needed here: The paper's key insight relies on decomposing gradients into individual parameter components rather than treating them as vectors, revealing how SAM differentially affects clean vs noisy fitting
  - Quick check question: Given a gradient vector [2, -1, 3] for three parameters, if SAM produces [-1, 0.5, 2], which parameters would be classified as Group B?

- Concept: Sharpness-aware minimization fundamentals
  - Why needed here: Understanding SAM's perturbation-based gradient calculation is essential to grasp why it produces different component-wise gradients than SGD
  - Quick check question: In SAM's formulation, what is the purpose of computing gradients at w + ρ(g/||g||) instead of just at w?

- Concept: Label noise types and their effects
  - Why needed here: The paper evaluates SANER across symmetric, asymmetric, instance-dependent, and real-world noise, requiring understanding of how these differ in their impact on learning
  - Quick check question: Which noise type would most likely create situations where clean and noisy gradients have opposing signs?

## Architecture Onboarding

- Component map:
  - Data pipeline -> Model -> Optimizer core -> SANER wrapper -> Scheduler -> Evaluation
  - Load CIFAR-10/100, apply RandomHorizontalFlip and RandomCrop, batch size 128
  - ResNet18 (base), plus variants like ResNet34, DenseNet121, WideResNet40-2, WideResNet28-10
  - SAM implementation with perturbation radius ρ = 0.1
  - Gradient component analysis and reweighting logic
  - Linear α decay from 1 to target over k epochs
  - Track noisy accuracy, clean-noisy gap, test accuracy, generalization gap

- Critical path:
  1. Forward pass on mini-batch
  2. Compute SGD gradient gSGD
  3. Compute SAM gradient gSAM via perturbation
  4. Calculate component-wise ratio r = gSAM/gSGD
  5. Create binary mask mB for Group B components
  6. Apply reweighting: gSANER = (1 - mB)·gSAM + α·mB·gSAM
  7. Update parameters with gSANER
  8. Update scheduler for α if needed

- Design tradeoffs:
  - Computational overhead: SANER adds negligible cost (no extra gradient computation vs SAM's already doubled cost)
  - Hyperparameter sensitivity: α and k values require tuning per dataset/noise level
  - Generalization: SANER improves noise robustness but may have minimal impact on clean data
  - Compatibility: SANER can be applied to SAM variants but requires understanding their specific gradient formulations

- Failure signatures:
  - Noisy accuracy increases rapidly in later epochs (indicates insufficient noise resistance)
  - Clean accuracy plateaus or drops early (indicates over-aggressive α or insufficient k)
  - Generalization gap widens (indicates overfitting to either clean or noisy data)
  - Performance matches SAM on clean data (indicates SANER's mechanism isn't activated without noise)

- First 3 experiments:
  1. Reproduce Figure 1: Compare SGD, SAM, and SANER on CIFAR-10 with 25% label noise, tracking noisy accuracy, clean-noisy gap, and test accuracy
  2. Validate Figure 5: Test SANER with different α values (0.25, 0.5, 0.75, 1.0) on CIFAR-100 with 50% noise to observe the relationship between α and noisy fitting rate
  3. Test scheduler impact: Compare SANER with k = 0, k = 25, k = 50, k = 75 on CIFAR-100 with 50% noise to demonstrate the effect on clean accuracy preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SANER's performance scale with larger, more complex datasets beyond CIFAR and Mini-WebVision, particularly on full ImageNet?
- Basis in paper: [inferred] The paper mentions computational resource limitations prevented testing on ImageNet and acknowledges this as a future research direction.
- Why unresolved: The paper only tested SANER on relatively small-scale datasets (CIFAR-10/100, Mini-WebVision) and acknowledges they couldn't test on larger datasets like ImageNet due to computational constraints.
- What evidence would resolve it: Comprehensive experiments testing SANER on full ImageNet or other large-scale real-world noisy datasets would demonstrate whether the performance gains observed on smaller datasets transfer to more challenging, realistic settings.

### Open Question 2
- Question: Can the Group B identification mechanism in SANER be refined to better isolate noise-dominated components from clean-dominated ones?
- Basis in paper: [explicit] The authors acknowledge that their design of Group B "does not fully separate noise-dominated components from clean-dominated ones" and this "may inadvertently affect the clean fitting rate."
- Why unresolved: The current SANER method uses a binary mask to identify Group B components but this doesn't perfectly distinguish between components dominated by clean versus noisy gradients, leading to potential trade-offs between noise resistance and clean data fitting.
- What evidence would resolve it: Developing and validating a more precise method for distinguishing noise-dominated from clean-dominated gradient components, followed by experimental demonstration that this refinement improves SANER's performance without harming clean data fitting.

### Open Question 3
- Question: What is the optimal strategy for determining the scheduler parameter k across different noise levels and architectures?
- Basis in paper: [inferred] While the paper shows k affects clean accuracy and mentions k=50 as stable in most cases, it doesn't provide a principled method for determining k and notes that "the performance is not overly sensitive to k."
- Why unresolved: The paper uses a heuristic linear scheduler that decreases α from 1 to 0.5 over k epochs, but doesn't establish how to optimally determine k values across different training scenarios, noise levels, or architectures.
- What evidence would resolve it: A systematic study identifying factors that influence optimal k values (noise rate, dataset size, architecture type, training stage) and developing a principled method for setting k would provide clearer guidance for SANER deployment.

## Limitations

- The paper's claims about gradient component down-weighting in SAM rely on specific interpretations of component-wise gradient ratios that weren't directly validated through ablation studies
- SANER's effectiveness depends on the assumption that components where clean and noisy gradients have opposite signs are primarily responsible for noise fitting, which isn't empirically proven
- The linear scheduler assumes the transition from pr < 1 to pr > 1 occurs predictably during training, which may vary across architectures

## Confidence

- **High Confidence:** SANER's implementation as a simple modification to SAM gradients and its computational efficiency claims
- **Medium Confidence:** The empirical improvements on CIFAR datasets with various noise types and the general relationship between α values and noise resistance
- **Low Confidence:** The theoretical mechanism explaining why SAM naturally down-weights specific gradient components and why this specifically targets noise fitting

## Next Checks

1. Conduct ablation studies to isolate the effect of Group B reduction from other SAM properties, testing whether the specific gradient component targeting is essential to SANER's performance
2. Test SANER across diverse architectures (CNNs, Transformers, MLPs) to verify that the pr < 1 to pr > 1 transition timing is consistent or requires adaptive scheduling
3. Evaluate SANER's performance on real-world noisy datasets beyond synthetic label noise to assess practical utility in realistic settings