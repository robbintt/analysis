---
ver: rpa2
title: Multi-Sample Dynamic Time Warping for Few-Shot Keyword Spotting
arxiv_id: '2404.14903'
source_url: https://arxiv.org/abs/2404.14903
tags:
- samples
- cost
- time
- query
- echet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-sample dynamic time warping is proposed to efficiently compute
  class-specific DTW similarity scores when multiple query samples per class are available.
  The method creates class-specific cost tensors that capture the variability of all
  query samples, which are then converted to cost matrices before applying DTW.
---

# Multi-Sample Dynamic Time Warping for Few-Shot Keyword Spotting

## Quick Facts
- arXiv ID: 2404.14903
- Source URL: https://arxiv.org/abs/2404.14903
- Reference count: 29
- Primary result: Multi-sample DTW achieves near-individual-sample accuracy with significantly faster runtime than using all samples in few-shot keyword spotting

## Executive Summary
This paper introduces multi-sample dynamic time warping (DTW) as an efficient method for few-shot keyword spotting when multiple query samples per class are available. The approach constructs class-specific cost tensors from multiple query samples and reduces them to cost matrices using element-wise minimum, enabling DTW paths to switch between samples during alignment. This maintains computational efficiency while capturing intra-class variability. Experiments on the KWS-DailyTalk dataset show the method achieves F-scores around 70% while being significantly faster than using all individual query samples.

## Method Summary
The method computes class-specific Fréchet means (standard and altered) from query samples, converts each query sample to reference template length via sub-sequence DTW alignment, constructs cost tensors by combining individual cost matrices, reduces them to cost matrices using element-wise minimum, and applies standard DTW for similarity scoring. The altered Fréchet mean uses a second DBA pass with non-standard steps to incorporate sample variability. This framework allows efficient computation of class-specific similarity scores while maintaining accuracy close to using all individual samples.

## Key Results
- Multi-sample DTW achieves F-scores around 70% on KWS-DailyTalk dataset
- Performance close to using all individual query samples while being significantly faster
- Runtime only slightly slower than using Fréchet means but much faster than all samples
- Method maintains computational efficiency while allowing DTW paths to switch between query samples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-sample DTW improves performance by allowing DTW paths to switch between multiple query samples during alignment.
- **Mechanism:** The method computes a class-specific cost tensor that captures the variability across all query samples. This tensor is then reduced to a cost matrix by taking the element-wise minimum across the sample dimension. During DTW, the path can now "choose" the best matching sample at each step, effectively aggregating information from all samples without explicitly running DTW on each one.
- **Core assumption:** The minimum operation across samples preserves the best alignment path while discarding worse matches, and that DTW paths will naturally follow the most discriminative sample at each step.
- **Evidence anchors:**
  - [abstract] "allows DTW paths to switch between different query samples while maintaining computational efficiency"
  - [section] "To obtain a single cost matrix describing the similarity between all converted query samples and a target sequence, first a three-dimensional cost-tensor is computed by combining the cost matrices between the modified samples and the target sequence. After that, the cost tensor is transformed into a standard cost matrix by taking the element-wise minimum over all cost matrices"
  - [corpus] Weak: No direct mention of this specific switching mechanism in neighboring papers; they focus on feature engineering and adapters rather than path-switching DTW variants.
- **Break condition:** If the variability across samples is not well-captured by the minimum operation (e.g., some samples are consistently worse but still influence the minimum), or if the DTW path becomes too fragmented by switching.

### Mechanism 2
- **Claim:** The altered Fréchet mean improves the reference template by incorporating variability from individual samples.
- **Mechanism:** After computing the standard Fréchet mean via DBA, a second DBA pass is run using sub-sequence DTW with non-standard steps (1,1), (1,2), (2,1). This allows the mean to "skip" entries, effectively mixing in entries from individual samples that differ from the mean, thereby increasing intra-class variability representation.
- **Core assumption:** Allowing the DBA to skip entries will pull in diverse sample features that better represent the class distribution than a pure average.
- **Evidence anchors:**
  - [section] "The resulting standard Fr\'echet means are altered by applying the DBA algorithm a second time with sub-sequence DTW and the non-standard steps (1, 1), (1, 2) and (2, 1). The cosine distance is used as the local distance measure. These steps allow ignoring of every other entry when matching two sequences. As a result, some variability of the query samples can be incorporated into the mean by alternating original entries with novel entries resulting from the second DBA algorithm."
  - [section] "Note that in general the algorithm for computing the Fr\'echet mean can be chosen arbitrarily when using multi-sample DTW and this particular choice was used as it significantly improved the performance (cf. Table I)."
- **Break condition:** If the variability introduced by skipping entries is too high, it may distort the mean and hurt alignment accuracy.

### Mechanism 3
- **Claim:** Converting all query samples to the same temporal length as the reference template enables fair per-position comparison.
- **Mechanism:** Each query sample is aligned to the reference template using sub-sequence DTW with cosine similarity and non-standard steps. The alignment mapping is then used to construct a new sample of identical length to the reference by replacing each reference entry with the mean of all query entries mapped to that position. This ensures all samples contribute equally to the cost tensor.
- **Core assumption:** The alignment mapping from sub-sequence DTW accurately reflects which portions of each sample correspond to each position in the reference, and that averaging aligned entries preserves discriminative information.
- **Evidence anchors:**
  - [section] "Second, all query samples are converted to have the same temporal dimension as the reference template of the class they belong to. The converted sample is created from the reference template by replacing each entry individually matching each query sample with its corresponding reference template and replacing each entry of the reference template with the arithmetic mean of all entries that are matched to this position."
- **Break condition:** If the alignment mapping is noisy or ambiguous (multiple query samples mapping to the same reference position with very different values), the averaged sample may lose discriminative features.

## Foundational Learning

- **Concept:** Dynamic Time Warping (DTW) and sub-sequence DTW
  - Why needed here: DTW is the core similarity measure used to align sequences of varying lengths and capture temporal deviations. Sub-sequence DTW is used to find the best matching subsequence within a longer target.
  - Quick check question: What is the computational complexity of DTW between two sequences of lengths N and M? (Answer: O(N·M))

- **Concept:** Fréchet mean / DBA (Dynamic Time Warping Barycenter Averaging)
  - Why needed here: The Fréchet mean provides a single representative template for a class. DBA is used to compute this mean when sequences are not aligned, by iteratively aligning and averaging.
  - Quick check question: Why is computing the Fréchet mean NP-complete? (Answer: Because it requires finding the sequence that minimizes the sum of DTW distances to all samples, which is a non-convex optimization over a discrete space.)

- **Concept:** Cost tensors and element-wise reduction
  - Why needed here: The cost tensor aggregates per-sample cost matrices across the sample dimension. Reducing it via element-wise minimum allows a single DTW pass to capture the best alignment across all samples.
  - Quick check question: If you have K samples each producing an NxM cost matrix, what is the shape of the resulting cost tensor? (Answer: NxM×K)

## Architecture Onboarding

- **Component map:** Input query samples -> Compute Fréchet means -> Convert samples to reference length -> Build cost tensor -> Reduce to cost matrix -> Apply DTW -> Output similarity scores

- **Critical path:** Step 3 (cost tensor construction + reduction) is the bottleneck for runtime, as it requires K×N×M distance computations and a parallelizable reduction.

- **Design tradeoffs:**
  - Using all individual samples: Highest accuracy, O(N·M·C·K) runtime
  - Using only Fréchet mean: Fastest, O(N·M·C) runtime, lower accuracy
  - Multi-sample DTW: Near-individual-sample accuracy, O(N·M·C·K) but with smaller constant, slower than Fréchet mean but much faster than all samples

- **Failure signatures:**
  - Runtime unexpectedly high: Likely due to unparallelized cost tensor reduction or large K
  - Performance drops vs individual samples: Cost tensor reduction may be discarding useful variability, or alignment mappings are noisy
  - Performance worse than Fréchet mean: Altered Fréchet mean step may be over-distorting the template

- **First 3 experiments:**
  1. **Baseline sanity check:** Run DTW with individual samples vs Fréchet mean on a small subset (e.g., 2 classes, K=2) and verify expected performance/runtime gap.
  2. **Ablation on cost tensor reduction:** Replace element-wise minimum operation with maximum or mean in Step 3 and observe impact on accuracy/runtime.
  3. **Parallelization test:** Implement parallel cost tensor reduction and measure speedup; confirm it matches theoretical expectations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of multi-sample DTW compare to other state-of-the-art few-shot learning methods beyond those tested in the paper?
- **Basis in paper:** [explicit] The paper compares multi-sample DTW to using individual samples and Fréchet means, but does not compare it to other few-shot learning approaches.
- **Why unresolved:** The paper focuses on comparing different DTW-based methods rather than benchmarking against broader few-shot learning literature.
- **What evidence would resolve it:** Empirical results showing multi-sample DTW performance against other few-shot learning techniques on standard benchmarks like Speech Commands or other KWS datasets.

### Open Question 2
- **Question:** What is the impact of using different distance metrics (beyond cosine similarity) in multi-sample DTW on detection performance and runtime?
- **Basis in paper:** [explicit] The paper uses cosine similarity as the distance metric but does not explore alternatives like Euclidean or Mahalanobis distance.
- **Why unresolved:** The choice of distance metric could affect both the quality of the cost tensors and the efficiency of DTW alignment.
- **What evidence would resolve it:** Comparative experiments using different distance metrics while measuring both F-score and computational time.

### Open Question 3
- **Question:** How does the proposed altered Fréchet mean algorithm affect the theoretical convergence properties of the DBA algorithm?
- **Basis in paper:** [explicit] The paper introduces an altered Fréchet mean using non-standard steps but does not analyze its convergence properties.
- **Why unresolved:** The paper demonstrates practical improvements but doesn't establish theoretical guarantees for the modified algorithm.
- **What evidence would resolve it:** Mathematical analysis proving convergence conditions or bounds for the altered Fréchet mean computation approach.

## Limitations
- The exact mechanism by which the altered Fréchet mean improves performance is not fully detailed, particularly how skipping entries enhances template representation.
- The method's performance relative to modern metric learning approaches for few-shot learning is not established.
- The element-wise minimum reduction assumes the best alignment path will naturally emerge, which may fail when query samples have conflicting alignment patterns.

## Confidence
- **High confidence:** The core mechanism of cost tensor construction and reduction, and the general framework of using multi-sample DTW for few-shot KWS. The performance claims relative to individual samples and Fréchet means are supported by the presented experimental results.
- **Medium confidence:** The effectiveness of the altered Fréchet mean with sub-sequence DTW steps. While the paper claims improvement, the mechanism is not fully detailed and could be sensitive to parameter choices.
- **Low confidence:** The generality of the method to other domains or datasets beyond KWS-DailyTalk, and the scalability to very large numbers of query samples per class.

## Next Checks
1. **Ablation study on cost reduction:** Systematically replace the element-wise minimum operation with mean or maximum, and measure the impact on F-score and runtime to understand the sensitivity of the reduction method.
2. **Baseline comparison with metric learning:** Compare multi-sample DTW against modern few-shot KWS methods that use metric learning (e.g., Prototypical Networks, Relation Networks) to establish relative performance.
3. **Robustness to sample variability:** Test the method on datasets with varying levels of inter-sample variability (e.g., controlled synthetic datasets) to quantify when the element-wise minimum assumption breaks down.