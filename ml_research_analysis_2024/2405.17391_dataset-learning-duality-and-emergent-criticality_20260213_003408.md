---
ver: rpa2
title: Dataset-learning duality and emergent criticality
arxiv_id: '2405.17391'
source_url: https://arxiv.org/abs/2405.17391
tags:
- variables
- trainable
- activation
- duality
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the emergence of criticality in artificial
  neural networks using a framework called dataset-learning duality. The authors propose
  that the activation dynamics of non-trainable variables (e.g., the dataset) is strongly
  coupled to the learning dynamics of trainable variables (e.g., weights and biases).
---

# Dataset-learning duality and emergent criticality

## Quick Facts
- arXiv ID: 2405.17391
- Source URL: https://arxiv.org/abs/2405.17391
- Reference count: 39
- Primary result: Emergence of criticality in neural networks through dataset-learning duality, producing power-law distributions in trainable variable fluctuations

## Executive Summary
This paper presents a novel framework called dataset-learning duality that explains the emergence of criticality in artificial neural networks. The authors propose that the dynamics of non-trainable variables (the dataset) are strongly coupled to the learning dynamics of trainable variables (weights and biases). By composing activation and learning maps, they establish a duality between dataset and trainable variable spaces that can produce power-law distributions indicative of criticality.

The key insight is that specific compositions of activation and loss functions can be engineered to produce desired power-law exponents in the distribution of trainable variable fluctuations. The authors demonstrate this through analytical derivations and numerical experiments on a toy model, showing that different activation-loss function combinations can yield power-law distributions with exponents k = 1, 0, 2/3, and 2.

## Method Summary
The authors develop an analytical framework based on dataset-learning duality, which treats the composition of activation dynamics (forward propagation) and learning dynamics (backward propagation) as a mapping between dataset variables and trainable variable fluctuations. They show that the probability distribution of fluctuations in trainable variables is determined by the Jacobian determinant of this mapping. By choosing specific activation and loss function compositions, they can engineer power-law distributions with controllable exponents. The framework is tested on a toy model with two trainable and two non-trainable variables, demonstrating good agreement between analytical predictions and numerical results.

## Key Results
- Identification of specific activation-loss function compositions that produce power-law distributions with different exponents (k = 1, 0, 2/3, 2)
- Analytical framework connecting dataset properties, activation functions, and loss functions to fluctuations in trainable variables
- Numerical validation of analytical predictions using a toy model with two trainable and two non-trainable variables
- Demonstration that dataset-learning duality can explain emergent criticality in neural networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The duality map between boundary neurons (dataset) and tangent space of trainable variables emerges from the composition of activation and learning dynamics.
- Mechanism: Forward propagation maps boundary neurons to bulk neurons, while backward propagation maps both bulk and boundary neurons to changes in trainable variables. Composing these two maps creates a non-linear mapping between dataset and learning dynamics.
- Core assumption: The system operates near learning equilibrium, allowing local linearization and simplification of the high-dimensional duality mapping.
- Evidence anchors:
  - [abstract] "During the activation pass, the boundary neurons (e.g., input neurons) are mapped to the bulk neurons (e.g., hidden neurons), and during the learning pass, both bulk and boundary neurons are mapped to changes in trainable variables (e.g., weights and biases)."
  - [section 3] "Composition of the activation (3.4) and learning (3.1) maps is a map from non-trainable degrees of freedom ( x∂, x /∂(t0)) at time t0 to changes of trainable degrees of freedom ˙q at time tL"
- Break condition: The assumption of local learning equilibrium breaks down when the system is far from equilibrium, making the duality mapping too complex to analyze with the simplified approach used here.

### Mechanism 2
- Claim: Power-law distributions of fluctuations in trainable variables emerge from the Jacobian determinant of the dataset-learning duality mapping.
- Mechanism: The probability distribution of fluctuations in trainable variables is determined by the Jacobian determinant of the transformation from dataset variables to trainable variable fluctuations. When this Jacobian follows a power-law form, it leads to power-law distributions in the trainable variables.
- Core assumption: The probability distribution function of tangent space variables is factorizable, allowing the multidimensional problem to be reduced to one-dimensional problems.
- Evidence anchors:
  - [section 4] "The Jacobian matrix can be expressed as" and subsequent derivation showing the probability distribution depends on the Jacobian
  - [section 6] "From conservation of probability in terms of ˙q′ and y we obtain" showing the relationship between probability distributions and the Jacobian
- Break condition: The assumption of factorizability breaks down when correlations between different trainable variables become significant, making the probability distribution non-separable.

### Mechanism 3
- Claim: Specific compositions of activation and loss functions can be engineered to produce desired power-law exponents in the distribution of trainable variable fluctuations.
- Mechanism: By choosing appropriate activation and loss functions, the composition H(f(y)) can be made to follow specific functional forms (exponential, power-law, logarithmic) that, through the duality mapping, produce power-law distributions with controllable exponents in the trainable variables.
- Core assumption: The analytical predictions for power-law exponents derived from the duality framework accurately match the observed distributions in numerical experiments.
- Evidence anchors:
  - [section 6] "In this section, we shall utilize the dataset-learning duality... to investigate the potential emergence of criticality" and subsequent derivation of conditions for different power-law exponents
  - [section 7] Numerical results confirming analytical predictions for k = 1, k = 0, k = 2/3, and k = 2 for different activation-loss function compositions
- Break condition: The analytical approximations break down when higher-order terms become significant or when the system deviates from the assumptions used in the derivation (e.g., local equilibrium, linear approximations).

## Foundational Learning

- Concept: Neural network architecture fundamentals (activation dynamics, learning dynamics, forward/backward propagation)
  - Why needed here: The paper builds its framework on understanding how neural networks operate, specifically the relationship between forward propagation (activation) and backward propagation (learning)
  - Quick check question: What are the two main types of dynamics in neural networks that the paper identifies as being coupled through the duality mapping?

- Concept: Statistical mechanics and phase transitions
  - Why needed here: The concept of criticality and power-law distributions comes from statistical physics, and the paper applies these concepts to neural network learning dynamics
  - Quick check question: How does the concept of criticality in physical systems relate to the emergence of power-law distributions in neural network trainable variables?

- Concept: Duality in physics and mathematics
  - Why needed here: The dataset-learning duality is central to the paper's framework, mapping between boundary (dataset) and bulk (trainable variables) spaces
  - Quick check question: What is the key idea behind physical dualities that the paper applies to the dataset-learning context?

## Architecture Onboarding

- Component map:
  - Dataset (boundary neurons) -> Neural network (forward propagation) -> Loss function -> Learning algorithm (backward propagation) -> Trainable variables (weights and biases)

- Critical path:
  1. Initialize network with weights and biases (trainable variables)
  2. Sample input data from dataset (boundary neurons)
  3. Forward propagate to compute predictions (activation dynamics)
  4. Compute loss between predictions and targets
  5. Backward propagate to compute gradients (learning dynamics)
  6. Update trainable variables using gradients
  7. Analyze fluctuations in trainable variables for criticality emergence

- Design tradeoffs:
  - Simplicity vs. generality: The toy model uses only 2 trainable and 2 non-trainable variables for analytical tractability, but the framework is meant to generalize to larger networks
  - Equilibrium assumption: The analysis assumes local learning equilibrium, which may not hold in all training scenarios
  - Dimensionality reduction: The framework reduces high-dimensional problems to lower-dimensional ones, potentially missing important multi-dimensional effects

- Failure signatures:
  - Non-power-law distributions of trainable variable fluctuations despite expected criticality
  - Poor agreement between analytical predictions and numerical results
  - Violation of local equilibrium assumptions during training
  - Factorizability assumption failing due to strong correlations between trainable variables

- First 3 experiments:
  1. Implement the toy model with sigmoid activation and mean squared loss, verify k = 1 power-law distribution of fluctuations
  2. Implement the toy model with ReLU activation and power-law loss, verify different k values (e.g., k = 0 for n = 2)
  3. Implement the toy model with piecewise linear activation and cross-entropy loss, verify k = 2 power-law distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dataset-learning duality framework be extended to non-equilibrium learning systems?
- Basis in paper: [explicit] The authors note that "the analysis of the learning efficiency and its relation to criticality must involve considerations of non-equilibrium systems, which is beyond the scope of the current paper."
- Why unresolved: The current framework focuses on local learning equilibrium states, but real-world learning systems are often far from equilibrium.
- What evidence would resolve it: Development of mathematical tools to analyze dataset-learning duality during transient learning phases and validation through experiments showing how criticality properties change during non-equilibrium states.

### Open Question 2
- Question: How does the choice of coordinate transformation in the tangent space affect the emergence of criticality?
- Basis in paper: [inferred] The authors note that "the transformation matrix Λ is not unique" and discuss freedom in choosing coordinate systems that "will not change the form of the equations."
- Why unresolved: While the paper discusses coordinate freedom, it doesn't systematically explore how different transformations might enhance or suppress critical behavior.
- What evidence would resolve it: Systematic numerical experiments comparing criticality measures across different coordinate transformations and identification of optimal transformations for specific learning tasks.

### Open Question 3
- Question: Can the dataset-learning duality explain criticality in more complex neural network architectures beyond simple feedforward networks?
- Basis in paper: [explicit] The authors state they "expect the same mechanisms to be responsible for the emergence of criticality in higher-dimensional problems" but limit their analysis to simple architectures.
- Why unresolved: The current analysis is limited to a toy model with minimal complexity, and the authors acknowledge this limitation.
- What evidence would resolve it: Application of the duality framework to recurrent networks, transformers, or other complex architectures with empirical validation of power-law distributions in trainable variable fluctuations.

## Limitations

- The framework relies heavily on local learning equilibrium assumptions that may not hold in all training scenarios
- The analysis is limited to simple toy models with minimal complexity, and generalization to larger networks remains to be tested
- The factorizability assumption for probability distributions may break down when correlations between trainable variables become significant

## Confidence

- Framework validity: High confidence - The mathematical consistency of the duality framework is well-established and supported by numerical validation in the toy model
- Power-law engineering: Medium confidence - Analytical predictions match numerical results in the toy model, but generalization to larger networks needs verification
- Equilibrium assumptions: Medium confidence - The local equilibrium approximation works for the toy model but may fail in more complex scenarios

## Next Checks

1. Test the framework on a larger neural network with multiple layers and neurons to verify if the duality mapping still produces the expected power-law distributions
2. Experiment with different learning rates and optimization algorithms to check the robustness of the local equilibrium assumption
3. Analyze the correlations between trainable variables to verify the validity of the factorizability assumption in more complex scenarios