---
ver: rpa2
title: 'Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance'
arxiv_id: '2403.12000'
source_url: https://arxiv.org/abs/2403.12000
tags:
- notochord
- midi
- events
- which
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Notochord is a deep probabilistic model designed for real-time
  MIDI performance, enabling low-latency, interpretable interventions in musical generation.
  It models sequences of structured MIDI events (pitch, velocity, time, instrument)
  as autoregressive distributions, allowing flexible querying and manipulation of
  sub-event attributes.
---

# Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance

## Quick Facts
- arXiv ID: 2403.12000
- Source URL: https://arxiv.org/abs/2403.12000
- Reference count: 40
- Notochord achieves sub-10ms latency for real-time MIDI performance with interpretable sub-event interventions

## Executive Summary
Notochord is a deep probabilistic model designed for real-time MIDI performance that enables low-latency, interpretable interventions in musical generation. The model processes sequences of structured MIDI events (pitch, velocity, time, instrument) as autoregressive distributions, allowing flexible querying and manipulation of sub-event attributes. Trained on the Lakh MIDI dataset, Notochord achieves sub-10ms latency and supports applications like steerable generation, harmonization, machine improvisation, and likelihood-based interfaces. Key innovations include order-agnostic sub-event prediction and continuous time/velocity modeling.

## Method Summary
Notochord processes structured MIDI events using a GRU-based autoregressive architecture that models each event as a tuple of sub-events. During training, each sub-event prediction is conditioned on a random subset of other sub-events, enabling any-order querying at inference time. Time and velocity are modeled as continuous values using discretized mixture of logistics distributions, while instrument and pitch use categorical distributions with look-up table embeddings. The model is trained on the Lakh MIDI Dataset with data augmentation including tempo changes, transposition, and velocity curve adjustments.

## Key Results
- Achieves sub-10ms latency (6ms event processing, 3ms sampling) suitable for real-time performance
- Enables any-order sub-event querying for flexible interventions in musical generation
- Successfully models polyphonic and multi-track MIDI sequences from Lakh MIDI Dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The autoregressive factorization allows low-latency, sub-event-level interventions in real-time MIDI performance
- Mechanism: By modeling each MIDI event as a tuple of sub-events and conditioning each sub-event prediction on a random subset of other sub-events, the model enables any-order querying. This means a user can fix one attribute (e.g., pitch = G#3) and query the model for the remaining attributes without waiting for the entire event to be generated
- Core assumption: The independence structure of musical events is sufficiently captured by conditioning on previous events and a subset of concurrent sub-events
- Evidence anchors:
  - [abstract]: "Our probabilistic formulation allows interpretable interventions at a sub-event level, which enables one model to act as a backbone for diverse interactive musical functions..."
  - [section 3.3]: "To enable a user to query in any desired order at inference time, our solution is to optimize over all permutations of sub-event parts. During training, each sub-event prediction is conditioned upon a random subset of the other sub-events"
- Break condition: If the random conditioning during training fails to capture the true statistical dependencies between sub-events, interventions may become meaningless or produce musically implausible results

### Mechanism 2
- Claim: The GRU-based architecture enables fixed, low-latency inference suitable for real-time musical interaction
- Mechanism: GRUs process each event sequentially with a constant computational cost, unlike Transformers which scale quadratically with sequence length. This ensures that the time to process each incoming MIDI event remains under 10ms, meeting the perceptual threshold for real-time performance
- Core assumption: The temporal dependencies in musical sequences are sufficiently modeled by the fixed-size hidden state of the GRU
- Evidence anchors:
  - [abstract]: "Notochord can generate polyphonic and multi-track MIDI, and respond to inputs with latency below ten milliseconds"
  - [section 3.5]: "To embed sub-events into vector space at the inputs to networks, we use a standard look-up table embedding for categorical variables... We implement the causal dependency between events using a gated recurrent unit (GRU) network..."
- Break condition: If musical sequences require long-range dependencies beyond the GRU's memory horizon, the model's predictions will degrade over longer interactions

### Mechanism 3
- Claim: Continuous modeling of time and velocity via discretized mixture of logistics improves generalization across different MIDI sources and tempos
- Mechanism: By representing time in seconds and velocity as continuous values dequantized from MIDI's 0-127 scale, the model can handle performances recorded at different tempos, with different MIDI tick resolutions, and even non-MIDI sensor data in the future. The mixture of logistics allows the model to represent both the quantized nature of MIDI and the continuous variations in real performances
- Core assumption: The continuous distributions learned from quantized MIDI data will generalize to truly continuous data sources
- Evidence anchors:
  - [section 3.1]: "In contrast to much previous work which represents time using quantized, tempo-relative units, we represent time in seconds, as a continuous quantity... We dequantize velocity and treat it similarly to time..."
  - [section 3.4]: "Time and velocity, however, are continuous in our model... A discretized mixture of logistics [33] is used to model the values of velocity and time"
- Break condition: If the dequantization process introduces noise that the model cannot filter, or if the continuous assumption is too strong for inherently discrete musical structures, performance will degrade

## Foundational Learning

- Concept: Autoregressive modeling and probability factorization
  - Why needed here: Notochord must model the joint distribution of complex MIDI sequences by breaking it down into a product of simpler conditional distributions, enabling both generation and intervention
  - Quick check question: Can you explain why P(x1, x2, x3) = P(x1)P(x2|x1)P(x3|x1,x2) is more tractable than modeling P(x1,x2,x3) directly?

- Concept: Recurrent neural networks and hidden state propagation
  - Why needed here: The GRU maintains a hidden state that summarizes all previous events, allowing each new event to be conditioned on the entire history without recomputing from scratch
  - Quick check question: What is the key difference between how a GRU and a Transformer handle sequential dependencies?

- Concept: Mixture density networks and continuous probability distributions
  - Why needed here: Musical attributes like timing and velocity are continuous in nature, and modeling them with simple Gaussians would be too restrictive; mixture densities allow multi-modal, flexible distributions
  - Quick check question: How does a mixture of logistics differ from a single logistic distribution, and why is this useful for modeling rhythm?

## Architecture Onboarding

- Component map: Input embeddings (categorical lookup + sinusoidal for continuous) -> GRU hidden state -> MLP per sub-event -> distribution parameters (softmax for categorical, mixture weights/locations/scales for continuous) -> sampling/intervention logic
- Critical path: Event embedding -> GRU update -> MLP conditioning on selected sub-events -> distribution parameter computation -> sampling or intervention
- Design tradeoffs: GRU offers low, fixed latency but limited long-range context vs. Transformer's better context but higher latency; continuous modeling increases flexibility but requires more complex training
- Failure signatures: High NLL on validation indicates poor modeling; slow inference times suggest architectural bottlenecks; musically implausible outputs indicate conditioning or distribution issues
- First 3 experiments:
  1. Feed a simple MIDI sequence and verify sub-10ms latency per event
  2. Fix pitch and query instrument/time/velocity to test any-order intervention
  3. Compare NLL when conditioning on different subsets of sub-events to validate the random conditioning strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the any-order sub-event factorization impact the quality and diversity of generated musical sequences compared to fixed-order models?
- Basis in paper: [explicit] The paper mentions that the any-order factorization allows for flexible interventions and supports diverse interactive functions, but does not provide empirical comparisons with fixed-order models
- Why unresolved: The paper does not conduct experiments comparing the any-order approach to fixed-order alternatives, leaving the impact on musical quality and diversity untested
- What evidence would resolve it: Comparative experiments evaluating the quality and diversity of musical sequences generated by Notochord (with any-order factorization) and a baseline model with fixed sub-event ordering

### Open Question 2
- Question: What is the impact of using continuous time and velocity representations on the model's ability to handle tempo changes and expressive dynamics compared to quantized representations?
- Basis in paper: [explicit] The paper states that continuous time allows the model to handle tempo changes and free timing, and continuous velocity enables finer dynamics, but does not provide quantitative comparisons with quantized approaches
- Why unresolved: The paper does not include experiments directly comparing the performance of continuous and quantized time/velocity representations on tasks like tempo change handling or dynamic expressiveness
- What evidence would resolve it: Quantitative experiments comparing Notochord's performance on tempo change adaptation and dynamic expressiveness against a quantized baseline model

### Open Question 3
- Question: How does the performance of Notochord scale with the number of concurrent instruments and the complexity of musical textures?
- Basis in paper: [inferred] The paper mentions that Notochord can handle up to 16 instruments simultaneously and does not discuss limitations on musical complexity, suggesting potential scalability issues are unexplored
- Why unresolved: The paper does not investigate the model's performance as the number of instruments or musical texture complexity increases, leaving scalability untested
- What evidence would resolve it: Experiments evaluating Notochord's performance (e.g., generation quality, latency) as the number of concurrent instruments and musical texture complexity are systematically varied

## Limitations

- Limited empirical validation beyond NLL metrics, lacking user studies or musical quality assessments
- No direct validation of continuous time/velocity modeling on truly continuous data sources
- Lack of theoretical grounding for random conditioning strategy during training
- Missing hyperparameter specifications making exact reproduction challenging

## Confidence

- **High confidence**: The sub-10ms latency claim is well-supported by the GRU architecture choice and timing measurements provided in the paper. The autoregressive factorization mechanism for enabling any-order querying is clearly explained and theoretically sound
- **Medium confidence**: The continuous modeling of time and velocity shows promise for future transfer learning applications, but the paper provides limited empirical evidence of its benefits beyond MIDI data. The random conditioning training strategy is innovative but lacks comparative analysis
- **Low confidence**: The musical quality and practical utility of the model are difficult to assess without user studies, listening tests, or qualitative evaluation of generated outputs. The paper focuses on technical metrics rather than musical outcomes

## Next Checks

1. **Musical Quality Assessment**: Conduct a listening test comparing Notochord-generated sequences against human performances and other generation models. Have musicians rate the naturalness, expressiveness, and musical coherence of generated passages to validate that low NLL translates to musically plausible outputs

2. **Long-Range Dependency Test**: Evaluate the model's performance on extended musical sequences (e.g., 2-5 minute compositions) to determine if the GRU's fixed hidden state size becomes a bottleneck. Measure whether musical structure and thematic development degrade over longer time horizons

3. **Cross-Modality Transfer Validation**: Test the continuous time/velocity modeling by training and evaluating on non-MIDI data sources such as audio-derived MIDI, sensor data from digital instruments, or live performance capture. This would validate the claimed advantage of continuous modeling for future transfer learning applications