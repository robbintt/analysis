---
ver: rpa2
title: Tokenisation is NP-Complete
arxiv_id: '2412.15210'
source_url: https://arxiv.org/abs/2412.15210
tags:
- tokenisation
- symbols
- which
- problem
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proves the NP-completeness of two variants of tokenisation,\
  \ defined as the problem of compressing a dataset to at most \u03B4 symbols. The\
  \ variants are direct tokenisation, which finds a vocabulary directly, and bottom-up\
  \ tokenisation, which selects a sequence of merge operations."
---

# Tokenisation is NP-Complete
## Quick Facts
- arXiv ID: 2412.15210
- Source URL: https://arxiv.org/abs/2412.15210
- Authors: Philip Whittington; Gregor Bachmann; Tiago Pimentel
- Reference count: 40
- Primary result: Proves NP-completeness of two tokenisation variants through reduction from max-2-SAT

## Executive Summary
This paper establishes the theoretical hardness of tokenisation by proving that two variants - direct tokenisation and bottom-up tokenisation - are NP-complete problems. The authors demonstrate this through a reduction from the max-2-SAT problem, showing that finding optimal tokenisers that compress a dataset to at most δ symbols is computationally intractable. This theoretical result has significant implications for the field of natural language processing, suggesting that practical tokenisation approaches must rely on approximation algorithms rather than exact solutions. The work bridges theoretical computer science with NLP practice by formalizing tokenisation as a computational problem and analyzing its complexity characteristics.

## Method Summary
The authors prove NP-completeness for two variants of tokenisation by reducing from the max-2-SAT problem. In direct tokenisation, the goal is to find a vocabulary that compresses a dataset to at most δ symbols. In bottom-up tokenisation, the task involves selecting a sequence of merge operations to achieve the same compression goal. The reduction demonstrates that if either tokenisation variant could be solved efficiently, it would imply an efficient solution to max-2-SAT, which is known to be NP-complete. The proof constructs instances of tokenisation problems that encode the structure of max-2-SAT instances, showing that solving the tokenisation problem would directly solve the satisfiability problem. This establishes both NP-hardness and membership in NP for these tokenisation variants.

## Key Results
- Direct tokenisation and bottom-up tokenisation are proven NP-complete through reduction from max-2-SAT
- The results establish that finding optimal tokenisers is computationally intractable, implying that approximation algorithms should be prioritized
- The paper identifies key theoretical differences between tokenisation and compression algorithms, highlighting how tokenisation constraints create additional computational complexity

## Why This Works (Mechanism)
The NP-completeness proof works by encoding the structure of max-2-SAT instances into tokenisation problems. In max-2-SAT, the goal is to maximize the number of satisfied clauses in a Boolean formula where each clause has exactly two literals. The reduction constructs tokenisation instances where finding an optimal vocabulary or merge sequence corresponds to finding an assignment that satisfies the maximum number of clauses. The compression constraint δ serves as a mechanism to encode the satisfaction conditions - only tokenisers that correctly "solve" the underlying satisfiability problem can achieve the required compression. This encoding ensures that any efficient algorithm for tokenisation would immediately yield an efficient algorithm for max-2-SAT, establishing NP-hardness. The reduction is carefully constructed to preserve the combinatorial structure of the satisfiability problem within the tokenisation framework.

## Foundational Learning
- **Computational Complexity Theory**: Understanding of P, NP, NP-complete, and NP-hard classes
  - *Why needed*: To grasp the significance of proving tokenisation is NP-complete
  - *Quick check*: Can explain why NP-completeness implies no efficient exact algorithm is likely

- **Reduction Techniques**: Method of reducing one problem to another to prove complexity
  - *Why needed*: The core proof technique used to establish NP-hardness
  - *Quick check*: Can describe how max-2-SAT is transformed into a tokenisation instance

- **Max-2-SAT Problem**: Optimization variant of Boolean satisfiability with 2-literal clauses
  - *Why needed*: Serves as the source problem for the complexity reduction
  - *Quick check*: Can explain why maximizing satisfied clauses is NP-hard

- **Tokenisation Algorithms**: BPE, WordPiece, and other practical tokenisation approaches
  - *Why needed*: To understand the practical context and implications of theoretical results
  - *Quick check*: Can compare how existing tokenisers handle vocabulary size constraints

- **Compression Theory**: Relationship between tokenisation and data compression
  - *Why needed*: To understand the connection between tokenisation and achieving compression bounds
  - *Quick check*: Can explain how compression ratio relates to vocabulary size in tokenisation

## Architecture Onboarding
- **Component Map**: Input text -> Tokenisation problem instance -> Vocabulary/merge sequence -> Compressed output
- **Critical Path**: Problem formulation → Reduction construction → NP-completeness proof → Implications for practice
- **Design Tradeoffs**: Exact vs. approximate algorithms; theoretical guarantees vs. practical performance; vocabulary size vs. compression quality
- **Failure Signatures**: Inability to achieve optimal compression; computational intractability for large vocabularies; approximation errors in practical implementations
- **First Experiments**: 1) Implement a greedy approximation algorithm for direct tokenisation and measure performance on standard benchmarks 2) Apply the reduction construction to create tokenisation instances from max-2-SAT problems and verify the encoding 3) Test existing tokenisers under varying δ constraints to observe how performance degrades as theoretical limits are approached

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical nature of the results without empirical validation leaves uncertainty about practical impact
- The connection between abstract max-2-SAT reduction and real-world tokeniser performance remains unexplored
- The paper does not investigate how well existing approximation algorithms perform relative to theoretical bounds

## Confidence
- NP-completeness proofs: High
- Practical relevance of theoretical results: Medium
- Impact on existing tokeniser design: Medium

## Next Checks
1. Implement and benchmark one of the proposed approximation algorithms on standard tokenisation benchmarks to assess practical impact of the theoretical hardness results
2. Test whether existing tokenisers (like BPE or WordPiece) exhibit behavior consistent with the theoretical constraints outlined
3. Conduct experiments varying δ to empirically verify how the hardness scales with vocabulary size constraints