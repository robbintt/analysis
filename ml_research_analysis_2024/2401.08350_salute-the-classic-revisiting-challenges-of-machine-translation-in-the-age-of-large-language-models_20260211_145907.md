---
ver: rpa2
title: 'Salute the Classic: Revisiting Challenges of Machine Translation in the Age
  of Large Language Models'
arxiv_id: '2401.08350'
source_url: https://arxiv.org/abs/2401.08350
tags:
- translation
- data
- llms
- language
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper revisits six classical challenges in machine translation
  through the lens of large language models (LLMs). It finds that LLMs reduce the
  need for extensive parallel data for high-resource languages and improve long sentence
  translation, even at the document level.
---

# Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models

## Quick Facts
- arXiv ID: 2401.08350
- Source URL: https://arxiv.org/abs/2401.08350
- Reference count: 40
- Key outcome: The paper revisits six classical challenges in machine translation through the lens of large language models (LLMs). It finds that LLMs reduce the need for extensive parallel data for high-resource languages and improve long sentence translation, even at the document level. However, they still struggle with domain mismatch and rare word prediction. New challenges for LLMs include inference efficiency, low-resource language translation during pretraining, and human-aligned evaluation.

## Executive Summary
This paper revisits six classical challenges in machine translation (MT) through the lens of large language models (LLMs), finding that LLMs have largely mitigated issues around data requirements for high-resource languages and long sentence translation. While LLMs demonstrate superior performance in document-level translation, they still struggle with domain mismatch and rare word prediction. The study identifies new challenges specific to LLMs, including inference efficiency and the need for better evaluation metrics aligned with human preferences.

## Method Summary
The study evaluates LLM performance on six classical MT challenges using multiple language pairs and datasets. For parallel data requirements, experiments use varying amounts of training data from 1K to 10M sentence pairs across 7 languages. Long sentence translation is tested on the FLORES-101 benchmark. Document-level translation evaluation uses English-to-Chinese and English-to-Japanese news datasets with human judgments. Domain mismatch is examined through legal and medical domains using TED Talks and clinical datasets. Rare word prediction analysis involves data-constrained translation with constrained decoding. The study employs metrics including COMET, BLEU, chrF, and human evaluation.

## Key Results
- LLMs significantly reduce parallel data requirements for high-resource languages, achieving comparable performance with 10-1000x less data
- LLMs demonstrate strong performance on long sentences (up to 256 tokens) and document-level translation
- Domain mismatch remains a challenge, with LLMs struggling more with target-side domain adaptation
- Rare word prediction continues to be difficult, particularly for words appearing fewer than 5 times in training data

## Why This Works (Mechanism)
LLMs leverage their extensive pretraining on diverse text corpora to develop strong language understanding and generation capabilities. This pretraining enables better handling of long-range dependencies and context, which improves translation quality even with limited parallel data. The attention mechanisms in transformer architectures allow LLMs to maintain coherence across longer sentences and document-level contexts. However, the model's performance still degrades when encountering out-of-distribution domain-specific terminology or truly rare words that fall below the frequency threshold in training data.

## Foundational Learning
- Large Language Models (LLMs): Foundation models trained on massive text corpora that can be adapted for translation tasks. Needed because they provide strong language understanding and generation capabilities that improve translation quality.
- Parallel Data Requirements: The amount of aligned sentence pairs needed for effective translation model training. Quick check: Compare model performance across different data sizes (1K, 10K, 100K, 1M, 10M sentences).
- Domain Mismatch: When training and test data come from different domains, affecting translation quality. Quick check: Evaluate on in-domain vs out-of-domain test sets using the same model.
- Document-Level Translation: Translating across sentence boundaries to maintain coherence and context. Quick check: Assess consistency of pronouns, terminology, and style across multiple sentences.
- Rare Word Prediction: The ability to correctly translate words that appear infrequently in training data. Quick check: Analyze translation accuracy for words with different frequency thresholds (1-5, 5-10, 10+ occurrences).
- Human-Aligned Evaluation: Assessment methods that better reflect human translation preferences beyond traditional metrics. Quick check: Use comparative human judgments to rate translation quality.

## Architecture Onboarding

**Component Map:**
Pretrained LLM -> Fine-tuning/Adaptation -> Translation Generation -> Evaluation

**Critical Path:**
Data Input → LLM Processing → Output Generation → Evaluation Metrics → Human Assessment

**Design Tradeoffs:**
- Model Size vs Inference Efficiency: Larger models achieve better translation quality but require more computational resources
- Pretraining vs Fine-tuning: Balance between general language understanding and domain-specific adaptation
- Evaluation Metrics vs Human Judgment: Traditional metrics may not fully capture translation quality nuances

**Failure Signatures:**
- Degradation in low-resource language translation quality
- Inconsistent terminology across document-level translations
- Poor handling of domain-specific terminology
- Inefficient inference requiring excessive computational resources

**3 First Experiments:**
1. Compare translation quality across different data sizes (1K to 10M sentences) for high-resource languages
2. Evaluate long sentence translation performance up to 256 tokens
3. Test document-level translation consistency across multi-sentence passages

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding LLM-based translation systems: How to improve low-resource language translation during the pretraining phase? What are the most effective ways to evaluate translation quality that align with human preferences? How can we address the efficiency challenges of LLM inference for practical deployment? What strategies can better handle domain adaptation when the target domain differs significantly from pretraining data?

## Limitations
- Human evaluation methodology lacks specification of rater qualifications and inter-annotator agreement metrics
- Domain mismatch analysis focuses primarily on legal and medical domains without systematic coverage of other specialized domains
- Rare word prediction analysis depends on frequency thresholds without examining morphological complexity or out-of-vocabulary patterns

## Confidence
- High confidence in findings about parallel data requirements for high-resource languages
- Medium confidence in long sentence and document-level translation improvements
- Low confidence in rare word prediction challenges

## Next Checks
1. Conduct inter-annotator agreement studies with detailed rater qualifications for human evaluation of low-resource language translations
2. Expand domain mismatch analysis to include technical, scientific, and social media domains with systematic error categorization
3. Perform morphological and frequency analysis of rare words to distinguish between true lexical gaps and morphological variation challenges across different language families