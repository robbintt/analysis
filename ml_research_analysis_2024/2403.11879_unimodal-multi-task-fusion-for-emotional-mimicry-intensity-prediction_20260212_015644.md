---
ver: rpa2
title: Unimodal Multi-Task Fusion for Emotional Mimicry Intensity Prediction
arxiv_id: '2403.11879'
source_url: https://arxiv.org/abs/2403.11879
tags:
- emotional
- data
- mimicry
- features
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unimodal audio-only approach for predicting
  emotional mimicry intensity, achieving second place in the 6th Affective Behavior
  Analysis in-the-wild (ABAW) workshop. The method leverages a Wav2Vec 2.0 model fine-tuned
  for Valence-Arousal-Dominance (VAD) regression to extract audio features, which
  are then fused with global mean vectors and processed by an LSTM network.
---

# Unimodal Multi-Task Fusion for Emotional Mimicry Intensity Prediction

## Quick Facts
- arXiv ID: 2403.11879
- Source URL: https://arxiv.org/abs/2403.11879
- Reference count: 40
- Achieved second place in 6th ABAW Emotional Mimicry Challenge with Pearson correlation coefficient of 0.554

## Executive Summary
This paper presents a unimodal audio-only approach for predicting emotional mimicry intensity that achieved second place in the 6th Affective Behavior Analysis in-the-wild (ABAW) workshop. The method leverages a Wav2Vec 2.0 model fine-tuned for Valence-Arousal-Dominance (VAD) regression to extract audio features, which are then fused with global mean vectors and processed by an LSTM network. A multi-task fusion strategy incorporates the VAD predictions to refine the emotion intensity prediction. The approach demonstrates that focusing solely on audio data can be effective, avoiding the computational complexity and data quality issues associated with multimodal integration.

## Method Summary
The method uses Wav2Vec 2.0-large fine-tuned for VAD regression to extract 1024-dimensional audio features from 16kHz mono recordings. These features are combined with VAD predictions through global mean pooling and processed by an LSTM with dropout. The model predicts emotional mimicry intensity for six categories (Admiration, Amusement, Determination, Empathic Pain, Excitement, Joy) using a regression head with tanh activation. Training uses MSE loss with learning rate 1e-4 and cosine decay over 30 epochs, with early stopping based on validation performance.

## Key Results
- Achieved second place in 6th ABAW Emotional Mimicry Challenge
- Pearson correlation coefficient of 0.554 on test set
- Outperformed baseline approaches on emotional mimicry intensity prediction
- Demonstrated effectiveness of audio-only modality in naturalistic settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audio-only modality outperforms multimodal fusion due to variable video quality and inconsistent frame rates
- Mechanism: Eliminating unreliable visual data avoids downstream failures from occlusions, poor illumination, and inconsistent temporal alignment
- Core assumption: Visual data introduces more noise than signal in naturalistic mimicry datasets
- Evidence anchors:
  - [section] "While our approach does not leverage the vision data... sound consistently emerging as the most indicative modality for emotional analysis."
  - [section] "By exclusively utilising audio data, we conducted evaluations of the given challenge dataset and achieved competitive results."
  - [corpus] Weak evidence for visual modality effectiveness; related papers still use multimodal approaches despite similar findings
- Break condition: Dataset has high-quality, consistently aligned visual data with minimal occlusions

### Mechanism 2
- Claim: Task-specific fine-tuning of Wav2Vec 2.0 significantly improves emotional feature extraction over generic ASR pretraining
- Mechanism: Domain adaptation aligns acoustic representations with emotional rather than linguistic patterns
- Core assumption: Pre-trained Wav2Vec 2.0 captures general acoustic features that need task-specific refinement for emotion recognition
- Evidence anchors:
  - [section] "Our results... demonstrate the incremental impact of different architectural elements... Foundational experiment utilised the fine-tuning of a Wav2Vec 2.0-large model... yielding a ρV AL of 0.017 and 0.021."
  - [section] "This initial outcome highlighted the limitations of using models pre-trained on standard Automatic Speech Recognition (ASR) tasks... for complex emotional recognition tasks."
  - [corpus] Related work confirms task-specific fine-tuning improves emotion recognition performance
- Break condition: Model architecture becomes too specialized, losing generalization ability across emotional categories

### Mechanism 3
- Claim: Multi-task learning with VAD regression provides richer contextual understanding than single-task prediction
- Mechanism: Joint optimization of emotion mimicry intensity with valence, arousal, and dominance creates complementary feature learning
- Core assumption: Emotional mimicry intensity correlates with underlying VAD dimensions, enabling knowledge transfer
- Evidence anchors:
  - [abstract] "A key aspect of our approach is the multi-task fusion strategy that not only leverages these features but also incorporates a pre-trained Valence-Arousal-Dominance (VAD) model."
  - [section] "By integrating these dimensions, we aim to encapsulate a more comprehensive emotional spectrum, thereby providing a richer contextual basis for predicting specific emotional mimicry intensities."
  - [corpus] Limited evidence; multi-task learning benefits are mentioned but not extensively validated in related work
- Break condition: VAD predictions become unreliable or introduce conflicting gradients during training

## Foundational Learning

- Concept: Wav2Vec 2.0 architecture and self-supervised speech representation learning
  - Why needed here: Core feature extractor for audio emotional analysis
  - Quick check question: How does Wav2Vec 2.0 differ from traditional speech recognition models in feature extraction?

- Concept: Valence-Arousal-Dominance (VAD) emotional dimensions and their relationships
  - Why needed here: Multi-task learning component that enriches emotional understanding
  - Quick check question: How do VAD dimensions relate to specific emotional mimicry categories like "Admiration" and "Joy"?

- Concept: Long Short-Term Memory (LSTM) networks for temporal sequence modeling
  - Why needed here: Processes variable-length audio features while capturing temporal dependencies
  - Quick check question: Why choose LSTM over Transformer for temporal fusion in this audio-focused architecture?

## Architecture Onboarding

- Component map: Wav2Vec 2.0 → VAD predictions + Global vector → LSTM → Dense layers → Emotion intensity prediction
- Critical path: Audio preprocessing → Wav2Vec 2.0 → VAD features + Global vector → LSTM → Dense layers → Emotion intensity prediction
- Design tradeoffs:
  - Audio-only vs multimodal: Simplicity and computational efficiency vs potential loss of visual emotional cues
  - Fine-tuning vs frozen features: Better task-specific performance vs risk of overfitting
  - LSTM vs Transformer: Lower computational cost vs potentially better long-range dependencies
- Failure signatures:
  - Training loss plateaus early: Likely VAD module not providing useful gradients
  - Validation correlation drops during training: Overfitting to training data
  - Extremely slow convergence: Learning rate too low or model too complex for dataset size
- First 3 experiments:
  1. Test Wav2Vec 2.0 feature extraction quality on validation set before adding any additional components
  2. Compare fine-tuned Wav2Vec 2.0 vs frozen features on a small subset of data
  3. Evaluate VAD auxiliary task performance independently before integrating into multi-task setup

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance change when incorporating textual data from the audio signals alongside the acoustic features?
- Basis in paper: [explicit] The authors mention that an intriguing avenue for future research could be the development of multimodal models that not only process audio signals but also interpret the textual content within these signals, enriching the emotional analysis.
- Why unresolved: The study focused solely on audio data, avoiding the computational complexity of multimodal integration. The impact of adding textual data to the audio features has not been explored.
- What evidence would resolve it: Conducting experiments that integrate textual features extracted from the audio signals with the current audio-only model and comparing the performance metrics (e.g., Pearson correlation coefficient) to the unimodal audio model.

### Open Question 2
- Question: What is the impact of fine-tuning Wav2Vec 2.0 on task-specific datasets versus using pre-trained weights from general ASR tasks?
- Basis in paper: [explicit] The authors highlight the necessity for task-specific adjustments, demonstrating that the Wav2Vec 2.0 model, originally trained for automatic speech recognition (ASR), requires fine-tuning to better align with the nuances of emotion recognition tasks.
- Why unresolved: While the paper discusses the importance of fine-tuning, it does not provide a direct comparison between models fine-tuned on task-specific datasets and those using pre-trained weights from general ASR tasks.
- What evidence would resolve it: Conducting a comparative study where models are trained with and without task-specific fine-tuning on the same dataset, measuring performance differences in emotion recognition tasks.

### Open Question 3
- Question: How does the model handle long-range dependencies in audio data, and what is the effect of varying the maximum video length on performance?
- Basis in paper: [inferred] The authors mention the variability in video lengths and the decision to cut videos to a maximum length of 12 seconds to balance batch processing and data retention. This implies a consideration of how video length affects model performance.
- Why unresolved: The paper does not explore the impact of different maximum video lengths on the model's ability to capture long-range dependencies in the audio data.
- What evidence would resolve it: Experimenting with different maximum video lengths and analyzing the model's performance on emotion recognition tasks, particularly focusing on how well it captures long-range dependencies in the audio data.

## Limitations

- Relies exclusively on audio data without empirically validating potential gains from multimodal fusion despite available visual data
- Pearson correlation of 0.554 measured only on test set without comprehensive ablation studies comparing audio-only versus multimodal approaches
- Multi-task learning benefits depend heavily on VAD prediction quality, which is not independently validated
- Label distribution imbalance (55.3%-89.9% of labels near zero) raises concerns about model robustness across less frequent emotional expressions

## Confidence

- **High Confidence**: The architectural choices (Wav2Vec 2.0 fine-tuning, LSTM temporal modeling) are technically sound and well-supported by existing literature
- **Medium Confidence**: The audio-only approach's superiority over multimodal methods is claimed but not rigorously tested against the same model with visual inputs
- **Low Confidence**: The multi-task learning benefits are asserted without sufficient empirical evidence showing VAD predictions improve emotion intensity prediction

## Next Checks

1. Replicate the study on a subset of data with high-quality visual annotations to empirically compare audio-only versus multimodal performance
2. Conduct ablation studies measuring the independent contribution of VAD predictions to emotion intensity accuracy
3. Test model robustness across different emotional expression frequencies by stratified validation to ensure performance isn't driven by dominant zero-label samples