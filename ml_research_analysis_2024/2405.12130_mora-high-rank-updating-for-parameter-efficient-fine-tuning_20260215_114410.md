---
ver: rpa2
title: 'MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning'
arxiv_id: '2405.12130'
source_url: https://arxiv.org/abs/2405.12130
tags:
- lora
- arxiv
- tasks
- mora
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of low-rank adaptation (LoRA)
  in parameter-efficient fine-tuning of large language models, particularly for memory-intensive
  tasks. The authors propose MoRA, a method that uses a square matrix instead of low-rank
  matrices to achieve high-rank updating while maintaining the same number of trainable
  parameters.
---

# MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning

## Quick Facts
- **arXiv ID**: 2405.12130
- **Source URL**: https://arxiv.org/abs/2405.12130
- **Reference count**: 25
- **Primary result**: MoRA uses square matrices instead of low-rank matrices for high-rank updating in parameter-efficient fine-tuning, outperforming LoRA on memory-intensive tasks while maintaining comparable performance on other tasks.

## Executive Summary
This paper addresses the limitations of low-rank adaptation (LoRA) in parameter-efficient fine-tuning of large language models, particularly for memory-intensive tasks. The authors propose MoRA, a method that uses a square matrix instead of low-rank matrices to achieve high-rank updating while maintaining the same number of trainable parameters. MoRA employs non-parameterized operators to reduce input dimension and increase output dimension for the square matrix. The method is evaluated across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory, and pretraining. MoRA outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks, demonstrating the effectiveness of high-rank updating.

## Method Summary
MoRA introduces a novel approach to parameter-efficient fine-tuning by replacing the low-rank decomposition used in LoRA with a square matrix. The key innovation lies in using non-parameterized operators to first reduce the input dimension before applying the square matrix, then increasing the output dimension. This design maintains the same number of trainable parameters as LoRA while enabling high-rank updates to the model. The method is implemented as adapters that can be inserted into transformer blocks, specifically targeting the attention and feed-forward network components. The architecture is designed to be compatible with existing fine-tuning pipelines while providing enhanced representational capacity for memory-intensive tasks.

## Key Results
- MoRA outperforms LoRA on memory-intensive tasks including continual pretraining for biomedical and financial domains
- Achieves similar performance to FFT in instruction tuning and mathematical reasoning tasks
- Demonstrates effectiveness of high-rank updating while maintaining parameter efficiency

## Why This Works (Mechanism)
The paper argues that LoRA's low-rank constraint limits the model's ability to capture complex task-specific patterns, particularly in memory-intensive scenarios. By using a square matrix with dimension reduction/expansion operators, MoRA enables full-rank updates that can better represent the task-specific information. The non-parameterized operators serve as efficient dimensionality transformations that preserve computational efficiency while enabling richer representations. This approach addresses the fundamental limitation of low-rank methods that struggle to capture the full complexity of task-specific adaptations, especially when the target task requires significant deviation from the pretrained distribution.

## Foundational Learning

**Parameter-efficient fine-tuning (PEFT)**: Methods that update only a small subset of model parameters during adaptation to reduce computational cost and memory usage. Why needed: Large language models are expensive to fine-tune in full; PEFT enables efficient adaptation. Quick check: Verify the number of trainable parameters relative to full model size.

**Low-rank adaptation (LoRA)**: PEFT method that approximates weight updates using low-rank decomposition (A × B). Why needed: Provides parameter efficiency by constraining updates to a low-dimensional subspace. Quick check: Confirm rank selection and its impact on performance.

**Transformer architecture**: Neural network architecture using self-attention mechanisms for sequence processing. Why needed: Understanding where adapters are inserted and how they interact with attention/FFN layers. Quick check: Map adapter placement relative to original transformer components.

**Continual pretraining**: Process of further pretraining a model on domain-specific data to adapt to specialized knowledge. Why needed: One of the primary tasks where MoRA shows significant improvements over LoRA. Quick check: Compare pretraining objectives and datasets across methods.

## Architecture Onboarding

**Component map**: Input → Dimension Reduction Operator → Square Matrix → Dimension Expansion Operator → Output

**Critical path**: The dimension reduction operator reduces the input to a smaller dimension, the square matrix performs the primary transformation, and the dimension expansion operator restores the original dimension. This path is critical because it determines the computational efficiency and representational capacity of the adapter.

**Design tradeoffs**: Square matrix provides full-rank updates but requires dimension reduction/expansion operators to maintain parameter efficiency. The non-parameterized nature of these operators ensures no additional trainable parameters while enabling the high-rank transformation. This trades some computational overhead for improved representational capacity.

**Failure signatures**: Poor performance on tasks requiring complex adaptations, instability during training due to dimension mismatch, or inability to converge when the dimension reduction ratio is too aggressive. These failures typically manifest as degraded performance relative to baseline LoRA or training divergence.

**First experiments**:
1. Compare MoRA performance against LoRA on a simple classification task to establish baseline effectiveness
2. Test different dimension reduction ratios to find optimal balance between efficiency and performance
3. Evaluate training stability across multiple random seeds to assess robustness

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Scalability concerns for MoRA's non-parameterized dimension-reduction operators across extremely large model architectures beyond those tested
- Limited task diversity within each category evaluated, with only one continual pretraining task per domain tested
- Lack of statistical significance testing or variance measurements across multiple runs to establish confidence in performance claims

## Confidence
- **Scalability claims**: Medium - Limited evaluation to LLaMA-7B raises questions about performance on larger models
- **Performance comparisons**: Medium - Absence of statistical significance testing makes it difficult to determine meaningful differences
- **Method generalizability**: Medium - No analysis of how MoRA performs with different initialization schemes or on non-standard architectures

## Next Checks
1. Conduct ablation studies testing MoRA with various initialization schemes (Xavier, Kaiming, orthogonal) to determine optimal settings for different task types and model scales
2. Perform statistical significance testing across at least 5 independent runs for each task to establish confidence intervals and determine if performance differences are meaningful
3. Evaluate MoRA on models with non-standard architectures (sparse attention, MoE) to assess generalizability beyond dense transformer models