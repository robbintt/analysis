---
ver: rpa2
title: 'Breaking Free: How to Hack Safety Guardrails in Black-Box Diffusion Models!'
arxiv_id: '2402.04699'
source_url: https://arxiv.org/abs/2402.04699
tags:
- adversarial
- images
- diffusion
- evoseed
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EvoSeed is a black-box framework that uses CMA-ES to find adversarial\
  \ initial seed vectors for diffusion models, producing natural adversarial images\
  \ that maintain photorealism while deceiving classifiers. It operates with auxiliary\
  \ conditional diffusion and classifier models, searching within an L\u221E constraint\
  \ around random seed vectors."
---

# Breaking Free: How to Hack Safety Guardrails in Black-Box Diffusion Models!

## Quick Facts
- arXiv ID: 2402.04699
- Source URL: https://arxiv.org/abs/2402.04699
- Reference count: 40
- Key outcome: Black-box framework generates natural adversarial examples in diffusion models, achieving over 92% attack success rate while maintaining photorealism.

## Executive Summary
EvoSeed introduces a novel black-box framework for generating adversarial examples using diffusion models. By leveraging CMA-ES to optimize initial seed vectors within L∞ constraints, the method produces high-quality adversarial images that fool safety, ethnicity, and gender classifiers while remaining imperceptible to human observers. The approach demonstrates effectiveness across multiple diffusion models and classifier architectures without requiring white-box access or model modifications.

## Method Summary
The framework uses CMA-ES optimization to search for adversarial initial seed vectors that, when processed through conditional diffusion models, generate images capable of deceiving target classifiers. The optimization operates within L∞ constraints around random seed vectors, maintaining compatibility between diffusion and classifier models. The method employs auxiliary conditional diffusion models (SD-Turbo, SDXL-Turbo, PhotoReal 2.0, EDM-VP, EDM-VE) and classifier models (ViT-L/14, ResNet-50, Q16, NudeNet-v2, DeepFace) to generate and evaluate adversarial examples. Key metrics include Attack Success Rate (ASR), FID, IS, SSIM, and Clip-IQA.

## Key Results
- Achieves over 92% attack success rate on CIFAR-10-like data while maintaining high image quality
- Outperforms random search and matches or exceeds white-box approaches like SD-NAE
- Successfully fools safety, ethnicity, and gender classifiers without altering human perception
- Demonstrates strong transferability to L2 robust classifiers

## Why This Works (Mechanism)
The framework exploits the optimization capabilities of CMA-ES to find adversarial seed vectors that generate misclassifiable images when processed through diffusion models. The L∞ constraint ensures perturbations remain imperceptible while maintaining photorealism. The compatibility between diffusion and classifier models enables effective optimization without requiring white-box access.

## Foundational Learning
- **CMA-ES optimization**: Evolutionary strategy for continuous black-box optimization; needed for finding adversarial seed vectors without gradient information
- **L∞ constraint**: Maximum perturbation bound; needed to ensure adversarial changes remain imperceptible
- **Conditional diffusion models**: Generate images conditioned on specific inputs; needed to produce high-quality, targeted adversarial examples
- **Classifier compatibility**: Ensuring diffusion and classifier models work together; needed for effective optimization and evaluation

## Architecture Onboarding

**Component map**: Seed vectors -> CMA-ES optimization -> Conditional diffusion models -> Generated images -> Classifier evaluation -> Fitness update

**Critical path**: Initial seed vector optimization through CMA-ES -> Image generation via diffusion models -> Classifier evaluation -> Iterative optimization

**Design tradeoffs**: 
- Black-box vs white-box access: Black-box preserves privacy but may limit optimization precision
- Image quality vs attack success: Higher quality images may reduce attack effectiveness
- Computational cost vs optimization speed: More iterations improve results but increase runtime

**Failure signatures**:
- Poor image quality: Indicates issues with L∞ constraint or diffusion model compatibility
- Low attack success rate: Suggests problems with CMA-ES initialization or classifier compatibility
- Inconsistent results: May indicate instability in the optimization process

**First experiments**:
1. Verify basic image generation capability with random seed vectors
2. Test classifier evaluation pipeline with non-adversarial images
3. Run small-scale CMA-ES optimization with simplified constraints

## Open Questions the Paper Calls Out
None

## Limitations
- Lacks specific hyperparameter settings for CMA-ES beyond population size and L∞ constraint
- Exact implementation details for model compatibility testing are not fully specified
- Limited validation across diverse classifier architectures and datasets

## Confidence
- High: Core methodology of using CMA-ES for black-box adversarial example generation is well-founded
- Medium: Claims of high attack success rates and photorealism are supported by experimental results
- Low: Assertions about outperforming white-box approaches require broader validation

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary CMA-ES hyperparameters to assess their impact on attack success rate and image quality
2. **Cross-model generalization test**: Evaluate effectiveness against a broader range of classifiers not used in original experiments
3. **Real-world deployment simulation**: Test framework in simulated black-box environment with unknown target model architecture