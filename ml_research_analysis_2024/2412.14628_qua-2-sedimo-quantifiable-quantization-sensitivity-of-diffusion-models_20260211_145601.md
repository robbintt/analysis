---
ver: rpa2
title: 'Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models'
arxiv_id: '2412.14628'
source_url: https://arxiv.org/abs/2412.14628
tags:
- quantization
- weight
- layers
- pixart
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Qua2SeDiMo is a Post-Training Quantization (PTQ) framework that
  quantifies the sensitivity of different weight layers, operations, and architectures
  in diffusion models to quantization-induced performance degradation. It leverages
  graph neural networks to attribute end-to-end task performance directly to individual
  layers and block structures, bypassing costly calibration datasets.
---

# Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models

## Quick Facts
- arXiv ID: 2412.14628
- Source URL: https://arxiv.org/abs/2412.14628
- Reference count: 40
- Sub-4-bit weight quantization (3.4-3.9 bits) without significant image quality loss

## Executive Summary
Qua$^2$SeDiMo introduces a Post-Training Quantization (PTQ) framework that quantifies quantization sensitivity in diffusion models by attributing end-to-end task performance to individual layers and block structures. The framework leverages graph neural networks to identify which components are most sensitive to quantization-induced performance degradation, enabling efficient mixed-precision configurations without requiring calibration datasets. Experimental results on PixArt-α, PixArt-Σ, Hunyuan-DiT, and SDXL demonstrate that Qua$^2$SeDiMo achieves sub-4-bit weight quantization while maintaining high image quality, outperforming existing PTQ methods in both FID and CLIP scores.

## Method Summary
Qua$^2$SeDiMo addresses the challenge of quantizing diffusion models by systematically identifying which layers, operations, and architectural components are most sensitive to quantization-induced performance degradation. The framework uses graph neural networks to attribute end-to-end task performance directly to individual components, bypassing the need for costly calibration datasets. By quantifying the sensitivity of different weight layers and block structures, Qua$^2$SeDiMo builds efficient mixed-precision configurations that preserve image quality even with aggressive quantization levels. The approach is particularly valuable for diffusion models, which present unique challenges due to their complex architectures and the need to maintain high-fidelity image generation.

## Key Results
- Achieves sub-4-bit weight quantization (3.4-3.9 bits) on PixArt-α, PixArt-Σ, Hunyuan-DiT, and SDXL
- Outperforms existing PTQ methods in both FID and CLIP scores
- Maintains high-fidelity image generation with low-bit activation quantization
- Eliminates need for calibration datasets through GNN-based sensitivity attribution

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to directly attribute task performance degradation to specific model components through graph neural networks. This approach captures the complex interactions between quantized layers and operations, identifying which components have the most significant impact on final image quality. By focusing quantization precision on sensitive components while aggressively quantizing less sensitive ones, Qua$^2$SeDiMo achieves optimal trade-offs between compression efficiency and performance preservation. The elimination of calibration datasets through sensitivity attribution represents a significant efficiency gain over traditional PTQ methods.

## Foundational Learning

**Graph Neural Networks for Sensitivity Attribution**: Why needed - To capture complex interactions between quantized components; Quick check - Verify GNN can accurately predict sensitivity patterns across different model architectures.

**Diffusion Model Architecture**: Why needed - Understanding how different blocks contribute to image generation quality; Quick check - Map sensitivity patterns to specific architectural components.

**Mixed-Precision Quantization**: Why needed - To balance compression efficiency with performance preservation; Quick check - Validate that mixed-precision configurations outperform uniform quantization.

## Architecture Onboarding

**Component Map**: Diffusion model blocks (U-Net, transformer, etc.) -> Graph Neural Network -> Sensitivity Attribution -> Mixed-precision Configuration

**Critical Path**: Input image embedding -> Forward pass through quantized blocks -> Output generation -> Quality evaluation -> Sensitivity attribution feedback

**Design Tradeoffs**: Accuracy vs compression ratio vs computational overhead; Per-channel vs per-tensor quantization granularity; Sensitivity attribution accuracy vs real-world performance.

**Failure Signatures**: Performance degradation concentrated in specific architectural blocks; Inconsistent sensitivity patterns across different input types; GNN attribution errors leading to suboptimal mixed-precision configurations.

**Three First Experiments**: 1) Quantize single architectural block to minimum precision and measure quality impact. 2) Compare GNN-based sensitivity attribution against empirical measurements. 3) Test mixed-precision configurations against uniform quantization baselines.

## Open Questions the Paper Calls Out
None

## Limitations
- GNN-based sensitivity attribution may not fully capture real-world deployment interactions
- Evaluation focuses primarily on FID and CLIP scores, with limited discussion of other quality metrics
- Ablation studies could be more extensive for extreme quantization levels and cross-architecture patterns

## Confidence

**Methodology Validity**: High - Strong quantitative improvements and visual quality results
**Generalization Across Architectures**: Medium - Covers reasonable but finite set of model variants
**Universal Transferability Claims**: Low - Limited testing beyond text-to-image generation

## Next Checks
1. Test Qua$^2$SeDiMo on diffusion models for domains like medical imaging or scientific visualization where quantization artifacts have higher stakes.
2. Conduct ablation studies with varying quantization granularity (per-channel vs per-tensor) to validate sensitivity attribution's robustness.
3. Implement real-time deployment benchmarks to verify that theoretically optimal mixed-precision configurations translate to practical performance gains on hardware accelerators.