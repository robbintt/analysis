---
ver: rpa2
title: Towards Consistent Natural-Language Explanations via Explanation-Consistency
  Finetuning
arxiv_id: '2401.13986'
source_url: https://arxiv.org/abs/2401.13986
tags: []
core_contribution: Explanation-consistency finetuning (EC-finetuning) is a method
  to improve the consistency of natural-language explanations generated by large language
  models (LLMs). The method finetunes LLMs on synthetic data containing consistent
  explanations, where follow-up questions are generated and answered based on an initial
  explanation.
---

# Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning

## Quick Facts
- arXiv ID: 2401.13986
- Source URL: https://arxiv.org/abs/2401.13986
- Reference count: 16
- Key outcome: EC-finetuning improves explanation consistency by 10.0% relative on finetuning datasets and 4.5% relative on out-of-distribution datasets

## Executive Summary
This paper introduces explanation-consistency finetuning (EC-finetuning), a method to improve the consistency of natural-language explanations generated by large language models (LLMs). The approach synthetically augments training data with follow-up questions that must be answered consistently with initial explanations. When applied to LLaMA2-13B, EC-finetuning demonstrates consistent improvements in explanation consistency across multiple datasets without sacrificing accuracy, addressing a key limitation where LLMs often generate inconsistent explanations for related questions.

## Method Summary
EC-finetuning synthetically augments training examples with follow-up questions generated by LLMs, where answers must be consistent with the initial explanation. The method uses different LLMs for data generation (GPT-4 for questions, Claude-2 for answers) and finetuning (LLaMA2-13B) to avoid favoring the generator's outputs. The approach is evaluated on four finetuning datasets (StrategyQA, MedQA-Diff, MedQA-Sim, MedMCQA) and tested for generalization on seven out-of-distribution datasets (BoolQ, NQ, MS-Marco, OBQA, MMLU-Med, PubMedQA, ARC-Easy).

## Key Results
- EC-finetuning improves explanation consistency by 10.0% relative on four finetuning datasets
- The method generalizes to seven out-of-distribution datasets with a 4.5% relative improvement
- EC-finetuning maintains or improves prediction accuracy while enhancing consistency
- Performance gains are consistent across different types of consistency errors (overgeneralization and prediction mismatch)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EC-finetuning improves explanation consistency by forcing the LLM to answer related questions in a way that matches the initial explanation
- Core assumption: LLMs can learn to generate more consistent explanations if trained on data that explicitly enforces consistency
- Evidence anchors: [abstract] "EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations" and [section] "EC-finetuning synthetically augments the examples in a dataset using LLMs"

### Mechanism 2
- Claim: EC-finetuning improves explanation consistency by teaching the model to be more precise and avoid overgeneralization
- Core assumption: Overgeneralized explanations are a major cause of inconsistency
- Evidence anchors: [section] "In the first example, EC-finetuning encourages the model to generate more precise explanations that are not overgeneralized/vague"

### Mechanism 3
- Claim: EC-finetuning improves explanation consistency by teaching the model to change its predictions on related questions to be more consistent with the initial explanation
- Core assumption: Inconsistency often arises because the model's predictions on related questions do not match its explanations
- Evidence anchors: [section] "On the other hand, in the second example, EC-finetuning does not change the explanation the model generates for the initial question, but instead changes the modelâ€™s predictions on related questions to be more consistent with the explanation on the initial question"

## Foundational Learning

- Concept: Synthetic data generation
  - Why needed here: EC-finetuning requires generating synthetic data with consistent explanations to finetune the LLM
  - Quick check question: How does EC-finetuning generate synthetic data with consistent explanations?

- Concept: Consistency metrics
  - Why needed here: Evaluating the effectiveness of EC-finetuning requires measuring the consistency of the LLM's explanations
  - Quick check question: How is explanation consistency measured in EC-finetuning?

- Concept: Finetuning
  - Why needed here: EC-finetuning involves finetuning an LLM on the synthetic data with consistent explanations
  - Quick check question: What is the role of finetuning in EC-finetuning?

## Architecture Onboarding

- Component map: Synthetic data generation -> LLM finetuning -> Consistency evaluation
- Critical path: Generate synthetic data with consistent explanations -> Finetune LLM on synthetic data -> Evaluate explanation consistency
- Design tradeoffs: Using different LLMs for data generation versus finetuning to avoid bias, versus using the same LLM for simplicity
- Failure signatures: If the LLM fails to generate consistent explanations for synthetic data during finetuning, the method will not improve consistency; if consistency metrics are unreliable, evaluation will be meaningless
- First 3 experiments:
  1. Run EC-finetuning on a small dataset and evaluate the consistency improvement
  2. Compare EC-finetuning to baseline finetuning without the consistency constraint
  3. Test EC-finetuning on out-of-distribution datasets to evaluate generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EC-finetuning performance scale with model size, and what is the optimal model size for balancing explanation consistency and accuracy?
- Basis in paper: Explicit - The paper mentions exploring EC-finetuning on LLaMA-2 13-billion parameter model and a simplified setting using only the LLaMA-2 13-billion parameter model for both synthetic data generation and explanation finetuning
- Why unresolved: The paper only tests EC-finetuning on one model size (13B parameters) and briefly mentions testing a simplified setting with the same model
- What evidence would resolve it: Running EC-finetuning experiments on a range of model sizes (e.g., 7B, 13B, 33B, 65B parameters) and comparing the trade-off between explanation consistency improvements and accuracy degradation across different model sizes

### Open Question 2
- Question: Can EC-finetuning be effectively applied to multimodal tasks, such as image-text question answering or video-text reasoning?
- Basis in paper: Explicit - The paper mentions "exploring EC-finetuning on more complicated tasks (e.g., multimodal understanding)" as part of future work
- Why unresolved: The current implementation of EC-finetuning is only tested on text-based question answering datasets
- What evidence would resolve it: Applying EC-finetuning to multimodal datasets like VQA (Visual Question Answering) or ActivityNet-QA (Video Question Answering) and measuring the improvement in explanation consistency for multimodal reasoning tasks

### Open Question 3
- Question: How does EC-finetuning affect the internal representation of consistency in LLMs, and can we develop interpretability methods to visualize these changes?
- Basis in paper: Explicit - The paper suggests studying "how LLMs represent consistency in their parameters and if EC-finetuning improves this representation" as part of future work
- Why unresolved: The paper does not investigate the internal changes in LLM representations due to EC-finetuning or propose methods to visualize these changes
- What evidence would resolve it: Analyzing the internal representations of LLMs before and after EC-finetuning using techniques like probing classifiers, representation similarity analysis, or attention visualization to understand how consistency is represented and improved

## Limitations

- The synthetic data generation process relies heavily on other LLMs (GPT-4 and Claude-2), raising questions about reproducibility and potential biases
- The paper focuses on LLaMA2-13B and may not generalize to other model architectures or scales
- The core assumption that consistency is a primary desideratum for explanations is not fully validated - there may be cases where inconsistent explanations are actually more informative

## Confidence

- **High confidence**: The EC-finetuning methodology is clearly described and the experimental setup is rigorous, with appropriate controls and evaluation metrics
- **Medium confidence**: The relative improvements in consistency are statistically significant and reproducible across multiple datasets, though the absolute gains are modest
- **Medium confidence**: The claim that EC-finetuning improves consistency without sacrificing accuracy is supported, though the tradeoff space could be more thoroughly explored

## Next Checks

1. **Ablation study on synthetic data quality**: Systematically vary the quality of synthetic data (using different prompting strategies or models) to quantify how much of the consistency improvement comes from the finetuning approach versus the quality of the training data itself

2. **Human evaluation of explanation utility**: Conduct user studies to determine whether the increased consistency actually improves human understanding and trust in the explanations, versus simply making them more uniform

3. **Cross-architecture validation**: Apply EC-finetuning to different LLM families (e.g., GPT, Mistral) and sizes (from 7B to 70B parameters) to test the robustness and generalizability of the approach across the LLM landscape