---
ver: rpa2
title: 'BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented
  LLMs via Key Supporting Evidence'
arxiv_id: '2402.12174'
source_url: https://arxiv.org/abs/2402.12174
tags:
- knowledge
- retrieval
- information
- bider
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of knowledge inconsistency in
  retrieval-augmented large language models (LLMs), where retrieved documents often
  contain noise and irrelevant information that negatively impacts answer quality.
  The authors propose BIDER, a method that refines retrieval documents into Key Supporting
  Evidence (KSE) through a three-stage process: knowledge synthesis, supervised fine-tuning
  (SFT), and preference alignment using reinforcement learning.'
---

# BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence

## Quick Facts
- arXiv ID: 2402.12174
- Source URL: https://arxiv.org/abs/2402.12174
- Authors: Jiajie Jin; Yutao Zhu; Yujia Zhou; Zhicheng Dou
- Reference count: 11
- Key result: Achieves 7% improvement in LLM answer quality while reducing input document length by 80%

## Executive Summary
This paper addresses knowledge inconsistency in retrieval-augmented large language models (RAG), where retrieved documents often contain noise and irrelevant information that negatively impacts answer quality. The authors propose BIDER, a method that refines retrieval documents into Key Supporting Evidence (KSE) through a three-stage process: knowledge synthesis, supervised fine-tuning (SFT), and preference alignment using reinforcement learning. BIDER achieves significant improvements in answer quality while dramatically reducing the amount of information the LLM needs to process, making it more efficient than traditional RAG approaches.

## Method Summary
BIDER operates through a three-stage pipeline: First, it synthesizes KSE from retrieved documents using a three-step process (nugget extraction, refinement, and cleaning) that identifies the most relevant sentences. Second, it trains a refiner model using supervised learning with the synthesized KSE as targets. Third, it fine-tunes the refiner using reinforcement learning with preference alignment, incorporating feedback from the downstream LLM to align the refined documents with the LLM's information acquisition preferences. The method models the refinement task as a seq2seq problem rather than a ranking problem, allowing more flexible generation of content that adapts to the generator's input preferences.

## Key Results
- Achieves 7% improvement in LLM answer quality compared to standard RAG approaches
- Reduces input document length by 80% through effective knowledge distillation
- Outperforms existing methods on five datasets across three knowledge-intensive tasks (open-domain QA, dialogue generation, and fact verification)
- Demonstrates effectiveness on both extractive and generative RAG scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The three-stage synthesis process effectively identifies and isolates Key Supporting Evidence (KSE) that bridges the knowledge gap between retrieved documents and the LLM's internal knowledge.
- Mechanism: The synthesis process works through three steps: nugget extraction (sentence-level retrieval based on semantic similarity to question-answer fact), nugget refinement (iterative selection based on gain score and NLI support degree), and nugget cleaning (removal of nuggets that don't improve answer generation probability).
- Core assumption: The assumption is that sentences most similar to the question-answer fact and those that improve answer generation probability are the most valuable for answering the question.
- Evidence anchors:
  - [abstract] "BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, supervised fine-tuning (SFT), and preference alignment."
  - [section 3.2] "We design a three-step method to gradually synthesize authentic KSE" and detailed descriptions of nugget extraction, refinement, and cleaning.
  - [corpus] Weak - only 1 of 8 related papers mentions similar KSE concepts, suggesting this is a novel approach.
- Break condition: If the nugget cleaning step removes too much information (over-filtering), or if the NLI model incorrectly assesses support degree, leading to loss of essential information.

### Mechanism 2
- Claim: Modeling the refinement task as a seq2seq problem rather than a ranking problem allows more flexible and effective generation of content that adapts to the generator's input preferences.
- Mechanism: The seq2seq model learns to map from the concatenation of question and retrieved documents to the synthesized KSE, enabling the model to generate new content rather than just selecting from existing text.
- Core assumption: The assumption is that generating new, synthesized content is more effective than selecting existing content for bridging the knowledge gap.
- Evidence anchors:
  - [section 3.3] "We model the task as a seq2seq task, which is similar to the idea of pointer network... This method ensures the flexibility of refinement while enhancing the potential of the generation model in expression."
  - [section 5.1] Comparison with extractive methods shows BIDER outperforms them, supporting the effectiveness of the seq2seq approach.
  - [corpus] Weak - only 1 of 8 related papers mentions similar seq2seq approaches, suggesting this is a novel approach.
- Break condition: If the seq2seq model fails to generate coherent or relevant content, or if the generated content is too far from the original retrieval documents, leading to information loss.

### Mechanism 3
- Claim: Incorporating reinforcement learning with preference alignment aligns the refined documents with the LLM's information acquisition preferences, leading to improved answer quality.
- Mechanism: The RL stage uses a segmented reward function that only provides rewards when the model generates the end-of-sentence token, comparing the F1 score of answers generated from refined versus original documents.
- Core assumption: The assumption is that the LLM's preference for information density and position can be effectively learned through reinforcement learning with the designed reward function.
- Evidence anchors:
  - [section 3.4] "We further enhance the adaptability of BIDER by incorporating feedback from a downstream LLM... We use the CLIP version of the PPO algorithm for optimization."
  - [section 5.4] "after preference alignment training, the proportion of golden answers in the model output increased by 3%-4%, and their position in the output text moved closer to the beginning."
  - [corpus] Weak - only 1 of 8 related papers mentions similar RL approaches, suggesting this is a novel approach.
- Break condition: If the reward function doesn't accurately capture the LLM's preferences, or if the RL training leads to unstable updates or collapse to trivial solutions.

## Foundational Learning

- Concept: Semantic similarity and embedding models (E5 model)
  - Why needed here: To identify nuggets in the retrieved documents that are semantically similar to the question-answer fact during the nugget extraction step.
  - Quick check question: How does the E5 model calculate semantic similarity between sentences and the question-answer fact?

- Concept: Natural Language Inference (NLI) models
  - Why needed here: To assess the support degree of the candidate nugget pool during the nugget refinement step, determining how well the nuggets support answering the question.
  - Quick check question: What is the threshold value for the NLI model's support degree, and how does it affect the termination of the iterative selection process?

- Concept: Reinforcement learning and advantage estimation (GAE)
  - Why needed here: To optimize the refiner model based on the LLM's feedback during the preference alignment stage, aligning the refined documents with the LLM's information acquisition preferences.
  - Quick check question: How does the Generalized Advantage Estimation (GAE) algorithm calculate the advantage function for each step in the RL training process?

## Architecture Onboarding

- Component map: Retriever (BM25 + SimLM reranker) -> Refiner (T5-XXL NLI model + BART-Large seq2seq model) -> Generator (Llama2-7B)
- Critical path: 1. Retrieve documents using BM25 + SimLM 2. Synthesize KSE through nugget extraction, refinement, and cleaning 3. Train refiner model using seq2seq supervised learning 4. Fine-tune refiner model using RL with preference alignment 5. Use refined documents as input to generator
- Design tradeoffs:
  - Nugget granularity: Sentences vs. passages vs. phrases - sentences provide robust and consistent results
  - KSE synthesis complexity: Three-step process vs. simpler approaches - three-step process provides better results but is more complex
  - RL vs. supervised learning: RL provides better alignment with LLM preferences but is more complex and potentially unstable
- Failure signatures:
  - Over-filtering: Loss of essential information due to aggressive nugget cleaning
  - Information loss: Generated content is too far from original retrieval documents
  - Unstable training: RL training leads to unstable updates or collapse to trivial solutions
  - Poor performance on complex tasks: BIDER performs less effectively on complex datasets like HotpotQA
- First 3 experiments:
  1. Ablation study: Compare BIDER with and without each of the three knowledge synthesis steps to validate their effectiveness
  2. RL impact study: Compare BIDER with and without the preference alignment stage to assess the impact of reinforcement learning
  3. Retrieval quality study: Test BIDER with different retriever qualities (BM25 vs. BM25+SimLM) to demonstrate generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BIDER vary when using different granularity levels for nuggets (e.g., phrases, sentences, paragraphs) during the nugget extraction phase?
- Basis in paper: explicit
- Why unresolved: The paper mentions that sentences are used as nuggets because they yield robust and consistent results, but future work is planned to explore approaches with different nugget granularities.
- What evidence would resolve it: Comparative experimental results showing performance metrics for BIDER using different nugget granularities on the same datasets.

### Open Question 2
- Question: What is the impact of using BIDER on tasks with complex multi-hop reasoning requirements, such as HotpotQA, compared to simpler single-hop tasks like NQ and TQA?
- Basis in paper: explicit
- Why unresolved: The paper acknowledges that BIDER performs less effectively on complex datasets like HotpotQA, suggesting that additional factors need to be considered for complex tasks.
- What evidence would resolve it: Detailed analysis and performance comparisons of BIDER on multi-hop versus single-hop tasks, identifying specific challenges and potential improvements.

### Open Question 3
- Question: How does the performance of BIDER change when applied to real-world RAG applications involving diverse sources with varied writing styles, as opposed to the Wikipedia-based datasets used in the experiments?
- Basis in paper: inferred
- Why unresolved: The paper notes that the datasets are based solely on Wikipedia, while real-world RAG applications involve diverse sources with varied writing styles, indicating a need for further optimization.
- What evidence would resolve it: Experimental results demonstrating BIDER's performance on diverse, real-world datasets with varied writing styles, compared to the controlled Wikipedia-based datasets.

## Limitations
- The three-stage synthesis process and RL fine-tuning add significant computational overhead compared to baseline RAG approaches
- Performance on complex multi-hop reasoning tasks like HotpotQA remains limited, suggesting the approach may struggle with tasks requiring synthesis of information from multiple disparate sources
- The method's effectiveness across different LLM architectures beyond Llama2-7B has not been thoroughly validated

## Confidence

**High Confidence**: The core mechanism of knowledge inconsistency in RAG systems is well-established, and the general approach of document refinement to address this issue is sound. The reported 7% improvement in answer quality is statistically significant and the 80% reduction in input document length provides clear efficiency benefits.

**Medium Confidence**: The three-stage synthesis process (nugget extraction, refinement, and cleaning) shows strong theoretical justification and empirical support, but the exact thresholds and hyperparameters (like NLI support degree thresholds) may not generalize well across different domains or question types. The seq2seq modeling approach is reasonable but could potentially generate less faithful content than extractive methods in some scenarios.

**Low Confidence**: The reinforcement learning component's long-term stability and effectiveness across diverse LLM architectures remains uncertain. The preference alignment may be overfit to the specific Llama2-7B generator used in experiments, and the reward function design might not capture all relevant aspects of answer quality.

## Next Checks
1. **Ablation Study on Knowledge Synthesis Steps**: Systematically remove each of the three synthesis steps (extraction, refinement, cleaning) to quantify their individual contributions and identify potential over-filtering issues.

2. **Cross-LLM Validation**: Test BIDER with different generator models (e.g., GPT-3.5, Claude, Mistral) to verify that the learned KSE format and RL preferences generalize beyond Llama2-7B.

3. **Robustness Testing on Complex Reasoning**: Evaluate BIDER on multi-hop reasoning datasets with progressively increasing complexity to identify the breaking point where the refinement approach fails to capture necessary information.