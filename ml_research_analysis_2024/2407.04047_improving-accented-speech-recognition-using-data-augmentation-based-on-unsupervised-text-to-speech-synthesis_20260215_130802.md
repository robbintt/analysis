---
ver: rpa2
title: Improving Accented Speech Recognition using Data Augmentation based on Unsupervised
  Text-to-Speech Synthesis
arxiv_id: '2407.04047'
source_url: https://arxiv.org/abs/2407.04047
tags: []
core_contribution: Unsupervised text-to-speech synthesis is used as a data augmentation
  method for accented speech recognition. TTS systems are trained with accented speech
  training data and their pseudo-labels generated by decoding the data using a baseline
  ASR model.
---

# Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis

## Quick Facts
- arXiv ID: 2407.04047
- Source URL: https://arxiv.org/abs/2407.04047
- Reference count: 32
- Unsupervised TTS synthesis with pseudo-labels yields up to 6.1% relative WER reduction on accented speech

## Executive Summary
This paper addresses the challenge of accented speech recognition by using unsupervised text-to-speech synthesis as a data augmentation method. The approach trains VITS TTS systems on accented speech data paired with pseudo-labels generated by decoding the same data with a baseline ASR model. The synthetic accented speech data is then combined with non-accented speech data to fine-tune Wav2vec2.0 models. Experiments show that this unsupervised approach achieves competitive performance with supervised TTS methods, yielding up to 6.1% relative word error rate reductions on the EdAcc evaluation set.

## Method Summary
The method employs unsupervised TTS synthesis for data augmentation in accented speech recognition. First, a baseline Wav2vec2.0 model is fine-tuned on Librispeech 960h non-accented speech data. Then, accented speech from L2-ARCTIC and British Isles corpora is decoded using this baseline model to generate pseudo-labels. A VITS TTS system is trained on the accented speech paired with these pseudo-labels, and used to generate 250 hours of synthetic accented speech from text prompts. Finally, Wav2vec2.0 models are fine-tuned on the combination of Librispeech data and synthetic accented speech data, demonstrating improved performance on accented speech evaluation sets.

## Key Results
- Wav2vec2.0 models fine-tuned with synthetic accented speech data achieve up to 6.1% relative WER reduction
- Unsupervised TTS with pseudo-labels performs comparably to supervised TTS with manual transcriptions
- Performance gains are particularly notable when there is overlap between first languages of speakers in training and evaluation data

## Why This Works (Mechanism)
The approach works by leveraging unsupervised TTS to generate large quantities of synthetic accented speech data, which helps bridge the domain gap between non-accented training data and accented evaluation data. By training TTS systems on accented speech paired with pseudo-labels, the method creates diverse synthetic samples that capture accent characteristics without requiring manual transcription. The combination of non-accented and synthetic accented data provides a more representative training distribution, improving the ASR model's robustness to accent variations.

## Foundational Learning
- Wav2vec2.0: Self-supervised speech representation learning model pre-trained on large-scale unsupervised speech data, needed to provide strong baseline representations for fine-tuning
- VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech): End-to-end TTS model that generates high-quality speech from text, needed to produce synthetic accented speech
- Pseudo-labels: ASR-generated transcriptions used as training targets when manual transcriptions are unavailable, needed to enable unsupervised TTS training
- Data augmentation: Technique of expanding training data through synthetic generation, needed to improve model robustness to accent variations
- Domain adaptation: Process of adapting models trained on one data distribution to perform well on another, needed to handle the mismatch between non-accented and accented speech
- Word Error Rate (WER): Standard metric for ASR evaluation measuring the edit distance between reference and hypothesis transcriptions, needed to quantify recognition accuracy

## Architecture Onboarding

Component map: Wav2vec2.0 (pre-trained) -> Pseudo-label generation -> VITS TTS -> Synthetic speech generation -> Wav2vec2.0 (fine-tuned)

Critical path: The most critical sequence is baseline ASR model quality → pseudo-label generation accuracy → TTS system training → synthetic data quality → final ASR fine-tuning. Any degradation in the pseudo-label generation step directly impacts the quality of the synthetic speech data and consequently the final ASR performance.

Design tradeoffs: The method trades manual transcription effort for potential quality loss in pseudo-labels. While supervised TTS with manual transcriptions could provide higher-quality training data, the unsupervised approach significantly reduces annotation costs while maintaining competitive performance. The choice of 250 hours of synthetic data represents a balance between computational cost and potential performance gains.

Failure signatures: High WER on pseudo-label generation (>11%) indicates poor baseline ASR model performance, which would propagate through the entire pipeline. Poor TTS quality manifests as unnatural synthetic speech that fails to capture accent characteristics effectively. Insufficient diversity in the synthetic data generation process may lead to overfitting to specific accent patterns.

Three first experiments:
1. Generate pseudo-labels for L2-ARCTIC corpus using baseline Wav2vec2.0 and verify WER is below 11%
2. Train VITS TTS on accented speech + pseudo-labels and evaluate sample quality using MOS or other perceptual metrics
3. Fine-tune Wav2vec2.0 on Librispeech + synthetic accented data and verify WER improvements on EdAcc dev set exceed 5% relative to Librispeech-only baseline

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of unsupervised TTS-based data augmentation compare to supervised TTS-based augmentation when the amount of supervised accented speech data is gradually increased?
- Basis in paper: The paper shows that supervised TTS with manual transcriptions achieves up to 6.1% relative WER reduction, while unsupervised TTS with pseudo-labels achieves similar gains, suggesting the potential for comparable performance with less supervision.
- Why unresolved: The paper does not explore scenarios where the amount of supervised data is varied, leaving the relationship between supervision level and performance gains unclear.
- What evidence would resolve it: Experimental results comparing WER reductions across different amounts of supervised accented speech data, from zero (unsupervised) to full supervision, would clarify the trade-off between supervision and performance.

### Open Question 2
- Question: What is the impact of using synthetic accented speech data generated from speakers with first languages not present in the evaluation data?
- Basis in paper: The paper notes that gains are particularly notable when there is overlap between the first languages of speakers in the training and evaluation data, implying that mismatches might reduce effectiveness.
- Why unresolved: The study does not test scenarios where the synthetic data's speaker accents differ significantly from those in the evaluation set, leaving the generalizability of the approach uncertain.
- What evidence would resolve it: Experiments using synthetic data from accents not represented in the evaluation data, with corresponding WER measurements, would reveal the limits of the approach's applicability.

### Open Question 3
- Question: How does the quality of pseudo-labels affect the performance of unsupervised TTS-based data augmentation?
- Basis in paper: The paper mentions that pseudo-labels are generated by decoding accented speech with a baseline ASR model, and their WERs are 10.7% and 10.2% for L2-ARCTIC and British Isles corpora, respectively, suggesting potential inaccuracies.
- Why unresolved: The study does not investigate how variations in pseudo-label quality influence the effectiveness of the synthetic data, leaving the robustness of the approach to label noise unexplored.
- What evidence would resolve it: Controlled experiments where the quality of pseudo-labels is systematically varied (e.g., by using models of different accuracy) and their impact on WERs is measured would clarify the sensitivity to label quality.

## Limitations
- Performance depends heavily on baseline ASR model quality for pseudo-label generation
- The approach may not generalize well to accents not represented in the training data
- Computational cost of generating large quantities of synthetic speech data

## Confidence
High: Core methodology is well-established and results are plausible
Medium: Exact reproducibility depends on specific model checkpoints and configurations
Low: Long-term effectiveness across diverse accent scenarios remains to be validated

## Next Checks
1. Generate pseudo-labels for the L2-ARCTIC corpus using the baseline Wav2vec2.0 model and verify WER is below 11%
2. Train the VITS TTS system on accented speech + pseudo-labels and evaluate sample quality using MOS or other perceptual metrics
3. Fine-tune Wav2vec2.0 on Librispeech + synthetic accented data and verify WER improvements on the EdAcc dev set exceed 5% relative to the Librispeech-only baseline