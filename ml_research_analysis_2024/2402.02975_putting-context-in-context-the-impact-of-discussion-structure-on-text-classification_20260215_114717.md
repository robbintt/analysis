---
ver: rpa2
title: 'Putting Context in Context: the Impact of Discussion Structure on Text Classification'
arxiv_id: '2402.02975'
source_url: https://arxiv.org/abs/2402.02975
tags:
- dataset
- discussion
- each
- context
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using transformer-based models to incorporate
  contextual information for text classification tasks. Specifically, it models linguistic,
  temporal, and structural context using natural language and feeds it as input to
  a transformer-based model.
---

# Putting Context in Context: the Impact of Discussion Structure on Text Classification

## Quick Facts
- arXiv ID: 2402.02975
- Source URL: https://arxiv.org/abs/2402.02975
- Authors: Nicol√≤ Penzo; Antonio Longa; Bruno Lepri; Sara Tonelli; Marco Guerini
- Reference count: 32
- Key outcome: Contextual information, especially structural context, can significantly improve stance detection when sufficient training data (24k-49k examples) is available.

## Executive Summary
This paper explores how transformer-based models can leverage contextual information for text classification tasks, specifically focusing on stance detection in online discussions. The authors propose encoding linguistic, temporal, and structural context as natural language input to RoBERTa, using special tokens to delimit different context types. Experiments on a large Kialo dataset demonstrate that contextual models outperform text-only approaches when sufficient training data is available, with structural context being particularly beneficial for complex discussion networks. The findings highlight both the potential and limitations of context-aware classification models.

## Method Summary
The approach uses RoBERTa with an MLP classifier to incorporate contextual information for text classification. Four contextual input formats are tested: textual context (TC), textual+temporal (TC+T), textual+user (TC+U), and textual+temporal+user (TC+U+T). The model adds special tokens (`<o>`, `</o>`, `<t>`, `</t>`, `[SEP]`) to encode user interaction sequences and time intervals between claims. The [CLS] embedding from RoBERTa is passed through a 3-layer MLP classifier for stance prediction. Training involves Optuna for hyperparameter optimization, with evaluation using macro-F1, weighted F1, and class-specific metrics across 10 random seeds.

## Key Results
- Structural context (TC+U+T) achieved the best performance with 71.7% macro-F1, outperforming text-only models by 0.5%.
- Contextual models required 24k-49k training examples to match text-only model performance.
- Structural context was particularly beneficial for discussions with longer user interaction chains and more complex local discussion networks.
- Temporal context showed limited benefit in the Kialo dataset due to uniform posting intervals from moderation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structural context encoded as user interaction sequences improves stance detection by revealing conversational flow patterns.
- Mechanism: The model learns to map sequential user participation patterns (via `<o>` and `</o>` tokens) into implicit structural features, effectively capturing the argumentative flow without explicit graph modeling.
- Core assumption: User interaction patterns are predictive of stance, and transformer attention can learn these patterns from natural language input.
- Evidence anchors:
  - [abstract] "structural information can be highly beneficial to text classification but only under certain circumstances"
  - [section 5] "the local user ID of ui... makes it possible to reconstruct the structure of the LDN among the users"
  - [corpus] Weak evidence - related papers focus on hypergraph-aware layers and node classification, not stance detection via user sequences.
- Break condition: If user interactions become too sparse or if multiple users participate simultaneously without clear sequential order, the structural signal degrades.

### Mechanism 2
- Claim: Combining textual context with temporal information enhances performance by providing a richer semantic context window.
- Mechanism: Time prefixes (`<t>` and `</t>`) allow the model to learn temporal dynamics between claims, enabling it to detect "waves" of discussion or response urgency.
- Core assumption: The order and timing of claims contain predictive information beyond the text itself.
- Evidence anchors:
  - [abstract] "temporal information allows us to identify peaks or 'waves' of comments"
  - [section 5] "To model the temporal context, we add at the beginning of each ci the time ti passed between the publication of the initial claim and of ci"
  - [corpus] Weak evidence - related work on time series and LLMs focuses on token-level alignment, not discussion temporal patterns.
- Break condition: If discussion platform has uniform posting intervals (like Kialo's moderated environment), temporal signals become less discriminative.

### Mechanism 3
- Claim: Dataset size is a prerequisite for learning contextual representations - insufficient data prevents the model from learning what special tokens mean.
- Mechanism: The transformer must see enough examples to understand that `<o>`, `</o>`, `<t>`, `</t>` tokens delimit specific types of contextual information, requiring extensive exposure to varied discussion structures.
- Core assumption: Contextual models require more training examples than text-only models to learn the same task because they must learn additional mapping functions.
- Evidence anchors:
  - [abstract] "the benefits of context are only observed when sufficient training data is available, with 24k-49k examples needed"
  - [section 8] "these results show that CONTEXTUAL models need between 20% and 40% of the training data (i.e., from 24 thousand to 49 thousand training examples)"
  - [corpus] Weak evidence - no direct citations about dataset size thresholds for contextual learning.
- Break condition: If dataset size falls below ~20k examples, contextual models underperform text-only baselines consistently.

## Foundational Learning

- Concept: Transformer attention mechanisms and positional encoding
  - Why needed here: The model uses RoBERTa, which relies on self-attention to process sequential input including special context tokens. Understanding how attention weights interact with custom tokens is crucial.
  - Quick check question: How does adding special tokens like `<o>` and `<t>` affect the attention distribution compared to standard text tokens?

- Concept: Graph neural networks vs. natural language modeling
  - Why needed here: The paper contrasts GNN-based approaches with natural language encoding of structural information. Knowing when each approach is appropriate helps evaluate the design choice.
  - Quick check question: What are the computational and data efficiency trade-offs between encoding graph structure in natural language vs. using GNNs?

- Concept: Privacy-preserving user representation techniques
  - Why needed here: The approach uses local user IDs to avoid global profiling. Understanding privacy implications and technical alternatives is important for ethical implementation.
  - Quick check question: How does using local discussion IDs instead of global user IDs prevent user profiling while maintaining structural information?

## Architecture Onboarding

- Component map: Raw text with special tokens -> RoBERTa encoder -> [CLS] embedding -> MLP classifier -> Softmax probabilities
- Critical path:
  1. Preprocess discussion chain into textual, temporal, and structural formats
  2. Tokenize with RoBERTa tokenizer, handle truncation deterministically
  3. Pass through RoBERTa to get [CLS] embedding
  4. Feed [CLS] to MLP for classification
  5. Apply softmax and compute loss
- Design tradeoffs:
  - Using natural language for context vs. GNNs: simpler implementation but may lose some structural precision
  - Local vs. global user IDs: better privacy but loses cross-discussion user behavior patterns
  - Deterministic truncation: reproducible but may bias results toward shorter discussions
- Failure signatures:
  - High variance across runs suggests initialization sensitivity or insufficient data
  - Performance degradation when chain length exceeds max input indicates truncation issues
  - No improvement over text-only baseline suggests context tokens aren't being properly interpreted
- First 3 experiments:
  1. Run SINGLE baseline to establish text-only performance floor
  2. Test TC model to measure impact of textual context alone
  3. Evaluate TC + U to assess structural context contribution with minimal complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal amount of training data required for contextual models to consistently outperform text-only models across different text classification tasks and datasets?
- Basis in paper: [explicit] The paper found that contextual models needed between 24,000 and 49,000 examples to achieve comparable results to text-only models on the Kialo dataset. However, this range may vary depending on the task and dataset characteristics.
- Why unresolved: The optimal training data amount likely depends on factors such as the complexity of the task, the quality of the contextual information, and the inherent structure of the dataset. More research is needed to establish generalizable guidelines.
- What evidence would resolve it: Conducting extensive experiments on diverse text classification tasks and datasets with varying sizes and characteristics could help determine the relationship between training data size and contextual model performance.

### Open Question 2
- Question: How does the type of contextual information (linguistic, temporal, or structural) impact classification performance in different text classification tasks and domains?
- Basis in paper: [explicit] The paper found that structural context was particularly beneficial for complex discussion networks, while temporal context provided some improvement but was less relevant in the Kialo dataset.
- Why unresolved: The relative importance of different contextual information types may vary depending on the task, domain, and characteristics of the dataset. Further investigation is needed to understand the general patterns.
- What evidence would resolve it: Conducting controlled experiments that isolate and compare the impact of different contextual information types across various text classification tasks and domains could shed light on their relative importance.

### Open Question 3
- Question: Can the proposed approach of using natural language to represent contextual information be effectively applied to other types of contextual information beyond linguistic, temporal, and structural context?
- Basis in paper: [explicit] The paper focused on using natural language to represent linguistic, temporal, and structural context. It did not explore other types of contextual information.
- Why unresolved: There may be other types of contextual information that could be valuable for text classification tasks, such as social context, domain-specific knowledge, or user preferences. The applicability of the proposed approach to these types of context remains unexplored.
- What evidence would resolve it: Conducting experiments that apply the proposed approach to other types of contextual information and evaluating their impact on classification performance could determine the generalizability of the method.

## Limitations

- The paper relies on a single proprietary dataset (Kialo SDK), raising concerns about generalizability to other discussion platforms or classification tasks.
- Statistical significance tests show modest effect sizes (0.5% improvement) that may not be practically meaningful despite being statistically significant.
- The approach uses local user IDs instead of global identifiers, limiting the model's ability to capture cross-discussion user behavior patterns.

## Confidence

- High Confidence (8/10): The core claim that contextual information can improve text classification performance is well-supported by experimental results.
- Medium Confidence (6/10): The claim that structural context is particularly beneficial has mixed support, with relatively modest improvements over other context types.
- Low Confidence (4/10): The assertion about specific dataset size requirements (24k-49k examples) appears approximate rather than precisely determined.

## Next Checks

- Check 1: Reproduce the dataset preprocessing pipeline - Implement the exact procedure for adding TIME and USER prefixes to discussion chains, including how to handle truncation when the input exceeds max length.
- Check 2: Conduct cross-platform validation - Test the contextual models on alternative datasets beyond Kialo to validate whether benefits generalize beyond the specific discussion format.
- Check 3: Analyze attention patterns for special tokens - Examine the self-attention weights in the RoBERTa encoder to determine how the model actually uses the special context tokens and whether they're being interpreted as intended.