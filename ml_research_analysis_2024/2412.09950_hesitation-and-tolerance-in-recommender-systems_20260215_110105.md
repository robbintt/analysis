---
ver: rpa2
title: Hesitation and Tolerance in Recommender Systems
arxiv_id: '2412.09950'
source_url: https://arxiv.org/abs/2412.09950
tags:
- user
- tolerance
- recommender
- systems
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates user hesitation and tolerance behaviors
  in recommender systems through large-scale surveys and online A/B experiments. The
  surveys with 6,644 and 3,864 responses respectively reveal that hesitation is a
  common phenomenon where users deliberate over recommended items before making decisions,
  and this often leads to tolerance - negative emotions when users spend time on items
  they ultimately find uninteresting.
---

# Hesitation and Tolerance in Recommender Systems

## Quick Facts
- **arXiv ID**: 2412.09950
- **Source URL**: https://arxiv.org/abs/2412.09950
- **Reference count**: 40
- **Primary result**: Incorporating tolerance signals into recommender systems improves user retention (0.67% and 0.36% Day-2 retention increase) with minimal computational cost.

## Executive Summary
This paper investigates user hesitation and tolerance behaviors in recommender systems through large-scale surveys and online A/B experiments. The research reveals that tolerance—negative emotions when users spend time on items they ultimately find uninteresting—is a common phenomenon that correlates with decreased user activity. Through analysis of survey data and offline datasets, the authors identify tolerance behavior signals and demonstrate their impact on user engagement. Online experiments on a major short-video platform show that incorporating tolerance signals into recommendation models significantly improves user retention while requiring minimal computational overhead.

## Method Summary
The research combines large-scale user surveys (6,644 and 3,864 responses) with offline dataset analysis and online A/B experiments. The surveys captured user behavior patterns and emotional responses to recommended items in online shopping and content browsing scenarios. Offline analysis of Taobao and Kuaishou datasets identified tolerance behaviors and their impact on engagement. Online experiments on Youku App involved 80,000 real users per experimental group, testing two strategies: treating tolerance samples as negatives or as weak positives with discounting. The ranking model was modified to incorporate tolerance signals using a hierarchical treatment with a discounting mechanism.

## Key Results
- Incorporating tolerance signals into recommendation models improved Day-2 retention by 0.67% and 0.36% in online experiments
- Over 60% of survey participants reported that increased tolerance would lead to decreased interest in the platform
- Offline analysis showed that increased tolerance behavior correlates with decreased user activity
- The approach achieved significant improvements with minimal computational costs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Tolerance signals act as weak positive or negative samples during model training, improving retention by better capturing nuanced user preferences.
- **Mechanism:** Traditional recommender systems treat clicks as positive samples, but hesitation and tolerance behaviors reveal that clicks don't always indicate strong interest. By distinguishing between deep engagement (positive), tolerance (weak positive/negative), and non-engagement (negative), the model can better align recommendations with actual user satisfaction.
- **Core assumption:** User behavior sequences can be reliably segmented into interest → hesitation → tolerance or disinterest patterns, and these patterns correlate with retention outcomes.
- **Evidence anchors:**
  - [abstract] "incorporating tolerance signals into recommendation models significantly improves user retention"
  - [section] "We integrate these insights into the training process of a recommender system for a major short-video platform"
  - [corpus] Weak - no direct corpus papers on tolerance signals specifically, but related papers on multi-behavior modeling exist
- **Break condition:** If user behavior patterns become too noisy or individual differences in tolerance behavior are too large to model reliably.

### Mechanism 2
- **Claim:** Reducing tolerance behavior increases user satisfaction by minimizing time wasted on uninteresting content, leading to higher retention.
- **Mechanism:** Tolerance behavior involves users spending time on content they ultimately find uninteresting. By identifying and reducing such recommendations, users spend more time on genuinely interesting content, improving their overall experience and likelihood of returning.
- **Core assumption:** User satisfaction is directly correlated with the ratio of time spent on interesting vs. uninteresting content.
- **Evidence anchors:**
  - [abstract] "increased tolerance behavior and decreased user activity"
  - [section] "over 60% of participants reported that an increase in tolerance would lead to a decline in interest"
  - [corpus] Weak - general relationship between engagement quality and retention is established, but specific tolerance link not in corpus
- **Break condition:** If reducing tolerance behavior leads to overly conservative recommendations that miss potentially interesting content.

### Mechanism 3
- **Claim:** The hierarchical treatment of signals (positive > tolerance > negative) allows the model to learn more nuanced user preferences without discarding potentially useful weak signals.
- **Mechanism:** Instead of discarding tolerance signals entirely, they're down-weighted using a discounting factor β, preserving some learning signal while acknowledging their weaker strength. This prevents the model from overfitting to pure positive signals and improves generalization.
- **Core assumption:** A continuous spectrum of user interest can be effectively modeled through signal weighting rather than binary classification.
- **Evidence anchors:**
  - [section] "We establish a clear precedence: 'positive signal' > 'tolerance signal' > 'negative signal'"
  - [section] "employ a discounting mechanism: L_T = -∑(positive) - β×∑(tolerance) - ∑(negative)"
  - [corpus] Weak - no direct corpus evidence for this specific hierarchical signal treatment
- **Break condition:** If the discounting factor β cannot be reliably estimated or varies too much across user segments.

## Foundational Learning

- **Concept:** User behavior modeling in recommender systems
  - Why needed here: The entire approach hinges on accurately identifying and categorizing different types of user behaviors (clicks, views, tolerance patterns)
  - Quick check question: Can you explain the difference between a click that indicates interest vs. a click that leads to tolerance behavior?

- **Concept:** A/B testing methodology
  - Why needed here: The validation of the approach relies on online A/B experiments to measure retention impact
  - Quick check question: What metrics would you track in an A/B test for a recommender system optimization targeting user retention?

- **Concept:** Multi-task learning and signal weighting
  - Why needed here: The approach uses weighted loss functions to handle different signal strengths, requiring understanding of how to balance multiple objectives
  - Quick check question: How would you choose the discounting factor β for tolerance signals in practice?

## Architecture Onboarding

- **Component map:** Data collection pipeline -> Behavior classification module -> Recommendation model -> A/B testing framework
- **Critical path:**
  1. User interacts with recommended content
  2. System captures interaction data (clicks, watch time, completion ratios)
  3. Behavior classification determines sample type (positive/tolerance/negative)
  4. Recommendation model updates using weighted loss function
  5. A/B testing framework measures retention impact
- **Design tradeoffs:**
  - Granularity vs. noise: More granular behavior classification may capture more nuances but could introduce noise
  - Signal weighting vs. simplicity: Complex weighting schemes may improve performance but increase model complexity
  - Short-term vs. long-term metrics: Optimizing for retention may conflict with short-term engagement metrics
- **Failure signatures:**
  - No improvement in retention despite successful implementation
  - Increased user churn due to overly conservative recommendations
  - Model instability due to poorly estimated β parameters
- **First 3 experiments:**
  1. **Offline validation:** Test tolerance classification accuracy on historical data before online deployment
  2. **Controlled A/B test:** Implement with small user segment to validate retention impact without major risk
  3. **Parameter sensitivity:** Test different β values to find optimal tolerance signal weighting

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the tolerance behavior phenomenon generalize beyond e-commerce and short-video platforms to other recommendation domains like news, music, or movies?
- **Basis in paper:** Explicit - The paper explicitly acknowledges this limitation, stating "Whether the findings generalize to more recommendation settings remains unclear and more research effort is needed."
- **Why unresolved:** The surveys and experiments were conducted only on e-commerce and short-video platforms, so there is no empirical evidence about other domains.
- **What evidence would resolve it:** Conducting similar surveys and A/B experiments on platforms like news readers, music streaming services, or movie recommendation systems to test if tolerance behavior occurs and affects retention similarly.

### Open Question 2
- **Question:** What is the optimal discounting factor (β) for treating tolerance samples as weak positives in the loss function?
- **Basis in paper:** Explicit - The paper mentions that "β is dynamically determined based on the user's content viewing completion rate against the average user completion rate, and then normalized to the interval [0, 1]" but does not specify the exact formula or optimal value.
- **Why unresolved:** The paper only mentions that β is dynamically determined but doesn't provide the specific methodology or optimal value for different scenarios.
- **What evidence would resolve it:** A systematic study varying β values and measuring their impact on retention rates across different user segments and content types to determine optimal values.

### Open Question 3
- **Question:** How do different user personality types (e.g., patient vs. impatient) affect the threshold for identifying tolerance behavior?
- **Basis in paper:** Inferred - The paper mentions that "individual differences, such as browsing habits and patience levels, add complexity" and gives examples of how patient users might engage with up to 80% of content they find uninteresting while impatient users might disengage after only 20%.
- **Why unresolved:** The paper acknowledges this variability but doesn't provide a methodology for personalizing tolerance detection based on user personality.
- **What evidence would resolve it:** A study that clusters users by engagement patterns and determines optimal tolerance thresholds for each personality cluster, then measures the impact on retention rates.

## Limitations
- The online experiments were conducted exclusively on a Chinese short-video platform, limiting generalizability to other domains and markets
- The tolerance classification mechanism relies on viewing ratios and watch times, which may not capture all forms of user hesitation and tolerance
- Individual differences in tolerance behavior (such as patience levels and browsing habits) add complexity that the current approach may not fully address

## Confidence

- **High confidence**: The correlation between tolerance behavior and decreased user activity (supported by both surveys and offline dataset analysis)
- **Medium confidence**: The effectiveness of tolerance signal integration in improving retention (based on single-platform online experiments)
- **Low confidence**: The generalizability of the tolerance classification mechanism across different content types and platforms

## Next Checks

1. **Cross-platform validation**: Replicate the online experiments on multiple platforms (e.g., e-commerce, news, and different video platforms) to verify robustness across domains
2. **Longitudinal impact assessment**: Track user behavior beyond Day-2 retention to evaluate whether tolerance signal optimization affects long-term user engagement and satisfaction
3. **Tolerance signal calibration**: Conduct controlled experiments to optimize the discounting factor β and validate the hierarchical signal treatment across different user segments and content categories