---
ver: rpa2
title: 'Fortify the Guardian, Not the Treasure: Resilient Adversarial Detectors'
arxiv_id: '2404.12120'
source_url: https://arxiv.org/abs/2404.12120
tags:
- adversarial
- detector
- attacks
- training
- detectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RADAR, a novel approach for adversarial training
  of adversarial detectors to improve their robustness against adaptive attacks. The
  key idea is to adversarially train the detector using adversarial examples optimized
  to fool both the classifier and detector, enabling the detector to learn and adapt
  to potential attack scenarios.
---

# Fortify the Guardian, Not the Treasure: Resilient Adversarial Detectors

## Quick Facts
- arXiv ID: 2404.12120
- Source URL: https://arxiv.org/abs/2404.12120
- Authors: Raz Lapid; Almog Dubin; Moshe Sipper
- Reference count: 15
- One-line primary result: RADAR significantly improves adversarial detector robustness without sacrificing clean classifier accuracy

## Executive Summary
This paper introduces RADAR, a novel adversarial training approach that fortifies adversarial detectors rather than classifiers. The key insight is that instead of training classifiers to be robust, we should train detectors to better identify adaptive attacks. RADAR achieves this by adversarially training detectors using examples optimized to simultaneously fool both the classifier and detector, exposing the detector to realistic attack scenarios.

The method demonstrates substantial improvements in detecting adaptive adversarial attacks across CIFAR-10, SVHN, and a 50-class ImageNet subset, achieving near-perfect ROC-AUC scores and low SR@5 values. Notably, RADAR maintains clean classifier accuracy while significantly enhancing detector robustness, addressing a critical tradeoff in existing adversarial training approaches.

## Method Summary
RADAR employs adversarial training to improve detector robustness by iteratively updating detector parameters using adversarial examples crafted to maximize both classification loss (fooling the classifier) and detection loss (fooling the detector). The classifier remains frozen during training, and selective/orthogonal gradient strategies (SPGD/OPGD) are used to resolve potential conflicts between classifier and detector objectives. The detector is trained to minimize binary cross-entropy loss on mixed batches of clean and adversarial examples, with the adversarial examples generated using SPGD/OPGD attacks that optimize against both classifier and detector losses simultaneously.

## Key Results
- Achieves near-perfect ROC-AUC scores across CIFAR-10, SVHN, and 50-class ImageNet datasets
- Maintains clean classifier accuracy while significantly improving detector robustness
- Demonstrates strong generalization, with detectors trained on one classifier architecture effectively detecting attacks on unseen classifiers
- Outperforms standard adversarial training baselines in detecting adaptive attacks (OPGD, SPGD)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: RADAR strengthens adversarial detectors by adversarially training them on examples optimized to fool both classifier and detector simultaneously.
- **Mechanism**: The detector is updated to minimize binary cross-entropy loss on adversarial examples that are specifically crafted to maximize both classification loss (fooling the classifier) and detection loss (fooling the detector). This exposes the detector to realistic attack scenarios where both objectives are simultaneously violated.
- **Core assumption**: Optimizing the detector against combined loss from classifier and detector leads to improved robustness without harming clean accuracy.
- **Evidence anchors**: [abstract]: "Our proposed method leverages adversarial training to reinforce the ability to detect attacks, without compromising clean accuracy." [section 3.2]: "The attacker's second goal is: 2) To cause the detector to predict that the image is benign... while also maintaining the lp norm constraint."

### Mechanism 2
- **Claim**: RADAR avoids trade-offs between clean accuracy and robustness by focusing optimization solely on the detector rather than the classifier.
- **Mechanism**: The classifier is held fixed (no learning), so the adversarial training loop only updates the detector's parameters. This isolates detector robustness gains from any potential clean accuracy degradation in the classifier.
- **Core assumption**: Classifier accuracy is unaffected by detector training when classifier weights are frozen.
- **Evidence anchors**: [abstract]: "Our proposed method leverages adversarial training to reinforce the ability to detect attacks, without compromising clean accuracy." [section 3.2]: "Our main objective is to improve the robustness of detector using adversarial training, by iteratively updating ϕ based on adversarial examples."

### Mechanism 3
- **Claim**: RADAR uses selective and orthogonal gradient strategies to resolve conflicts between classifier and detector loss objectives during adversarial training.
- **Mechanism**: Selective Projected Gradient Descent (SPGD) and Orthogonal Projected Gradient Descent (OPGD) ensure that gradient updates target only unsatisfied constraints, preventing contradictory optimization directions between classifier accuracy and detector robustness.
- **Core assumption**: SPGD/OPGD can effectively disentangle the optimization of classifier and detector objectives without sacrificing overall adversarial detection performance.
- **Evidence anchors**: [section 3.2]: "We employ the selective and orthogonal approaches proposed by Bryniarski et al. (2021)." and detailed description of SPGD and OPGD.

## Foundational Learning

- **Concept**: Adversarial examples and their generation via Projected Gradient Descent (PGD)
  - **Why needed here**: RADAR relies on generating adversarial examples that fool both classifier and detector; understanding PGD is essential to grasp how these examples are crafted.
  - **Quick check question**: What is the role of the step size α and epsilon ε in the PGD attack used by RADAR?

- **Concept**: Binary cross-entropy loss and its role in adversarial detection
  - **Why needed here**: The detector is trained using binary cross-entropy to distinguish benign from adversarial inputs; knowing how this loss works is key to understanding detector training dynamics.
  - **Quick check question**: How does binary cross-entropy loss guide the detector to improve its ability to flag adversarial inputs?

- **Concept**: ROC-AUC and SR@5 as evaluation metrics for adversarial detectors
  - **Why needed here**: RADAR's performance is evaluated using ROC-AUC and SR@5; understanding these metrics is necessary to interpret the reported results and robustness claims.
  - **Quick check question**: What does a high ROC-AUC score and low SR@5 value indicate about the detector's performance?

## Architecture Onboarding

- **Component map**: Clean data -> Classifier (fθ) -> Prediction; Attacker (internal) -> Adversarial examples; Mixed batch (clean + adversarial) -> Detector (gϕ) -> Benign/Adversarial probability -> Binary cross-entropy loss -> Detector parameter update

- **Critical path**: 1) Generate adversarial examples using SPGD/OPGD with combined classifier/detector loss; 2) Mix benign and adversarial examples; 3) Update detector parameters to minimize binary cross-entropy loss on the mixed batch; 4) Repeat for multiple epochs

- **Design tradeoffs**: Freezing classifier vs. joint training: avoids clean accuracy loss but may limit detector adaptability; SPGD vs. OPGD: SPGD optimizes unsatisfied constraints; OPGD projects gradients orthogonally to avoid violating constraints—tradeoff is computational complexity vs. theoretical guarantees; Batch size and learning rate: larger batch size can improve stability but may reduce diversity; learning rate must be tuned to avoid divergence or slow convergence

- **Failure signatures**: Detector converges to trivial solutions (always benign or always adversarial); Loss plateaus at high values, indicating difficulty in optimization; ROC-AUC remains low despite training, suggesting the detector fails to generalize; Clean accuracy drops, indicating interference between classifier and detector training

- **First 3 experiments**: 1) Train detector with clean data only; evaluate ROC-AUC and SR@5 on PGD attacks; 2) Train detector with RADAR (SPGD/OPGD adversarial fine-tuning); compare ROC-AUC and SR@5 against PGD, OPGD, SPGD attacks; 3) Ablation: vary epsilon, step size, batch size; measure impact on detector robustness and clean accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the fundamental theoretical limits of adversarial detector robustness under adaptive attacks, and can these limits be characterized mathematically?
- **Basis in paper**: [explicit] The paper acknowledges that adversarial detectors can be fooled by tailored attacks, leading to a "cat-and-mouse" cycle. The authors suggest that understanding the interplay between adversarial-training strategies and detector characteristics is crucial, but do not provide a theoretical framework.
- **Why unresolved**: Existing research focuses on empirical evaluations and practical improvements, but lacks a rigorous theoretical foundation to characterize the fundamental limits of detector robustness against adaptive attacks.
- **What evidence would resolve it**: A mathematical framework that establishes bounds on detector performance against adaptive attacks, considering factors like attack budget, detector architecture, and training methodology.

### Open Question 2
- **Question**: How does the performance of adversarial detectors vary across different adversarial attack algorithms and threat models, and can we develop a unified framework for evaluating detector robustness?
- **Basis in paper**: [explicit] The paper evaluates RADAR against several attack algorithms (PGD, OPGD, SPGD) and datasets, but acknowledges that there may be other attack strategies not considered. The authors emphasize the need for a more realistic evaluation paradigm that accounts for attacker knowledge.
- **Why unresolved**: Current evaluation methods are often limited to specific attack algorithms and threat models, making it difficult to compare detector performance across different studies and generalize findings to real-world scenarios.
- **What evidence would resolve it**: A comprehensive evaluation framework that incorporates a diverse set of attack algorithms, threat models, and evaluation metrics, allowing for a fair and standardized comparison of detector robustness.

### Open Question 3
- **Question**: Can we develop adversarial detectors that are not only robust against existing attack methods but also have the ability to generalize to unseen and potentially more sophisticated attacks?
- **Basis in paper**: [explicit] The paper highlights the importance of developing detectors that can withstand adaptive attacks and mentions the potential for transferability of adversarial examples across different classifiers and architectures. However, the authors do not explore the generalization capabilities of their proposed method beyond the evaluated attack algorithms.
- **Why unresolved**: Most adversarial detection methods are designed to counter specific attack algorithms or threat models, making them vulnerable to novel and more sophisticated attacks that exploit different vulnerabilities.
- **What evidence would resolve it**: Empirical studies demonstrating the generalization capabilities of adversarial detectors against a wide range of attack algorithms, including those not used during training, and theoretical insights into the factors that contribute to detector generalization.

## Limitations

- The experimental setup uses only a subset of ImageNet (50 classes), which may not fully represent the challenges of large-scale datasets.
- The reliance on OPGD and SPGD attacks for both training and evaluation may not capture the full spectrum of adaptive attacks that could be developed in response to RADAR.
- The claim that clean accuracy is unaffected by detector training is not independently verified; only the detector's ability to generalize to different classifiers is tested, not whether the classifier's accuracy changes post-detector training.
- The robustness gains against white-box attacks are not explored, leaving a gap in understanding RADAR's effectiveness against more sophisticated adversaries.

## Confidence

- **Claim Cluster 1 (RADAR improves detector robustness without clean accuracy loss)**: Medium
- **Claim Cluster 2 (SPGD/OPGD effectively resolve optimization conflicts)**: Low (limited independent validation)
- **Claim Cluster 3 (RADAR generalizes to unseen classifiers)**: High (supported by ablation results)

## Next Checks

1. **Cross-dataset generalization**: Test RADAR-trained detectors on classifiers trained on entirely different datasets (e.g., ImageNet detectors on CIFAR-10 classifiers) to verify true domain generalization.

2. **White-box attack evaluation**: Implement and evaluate against white-box attacks that have full knowledge of the detector architecture and parameters to assess RADAR's robustness against more sophisticated adversaries.

3. **Clean accuracy verification**: Independently measure classifier accuracy before and after detector training to confirm that RADAR truly preserves clean accuracy as claimed.