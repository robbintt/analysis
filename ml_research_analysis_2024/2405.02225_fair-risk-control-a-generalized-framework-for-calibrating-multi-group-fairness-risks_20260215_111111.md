---
ver: rpa2
title: 'Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness
  Risks'
arxiv_id: '2405.02225'
source_url: https://arxiv.org/abs/2405.02225
tags:
- function
- prediction
- algorithm
- output
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a generalized framework for post-processing\
  \ machine learning models to satisfy multi-group fairness guarantees. The proposed\
  \ (s, G, \u03B1)-GMC (Generalized Multi-Dimensional Multicalibration) extends existing\
  \ multicalibration notions to handle multi-dimensional outputs and more flexible\
  \ functional mappings."
---

# Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks

## Quick Facts
- arXiv ID: 2405.02225
- Source URL: https://arxiv.org/abs/2405.02225
- Reference count: 40
- Primary result: Generalized framework for post-processing ML models to satisfy multi-group fairness guarantees

## Executive Summary
This paper introduces the (s, G, α)-GMC framework, a generalized approach for calibrating multi-group fairness risks in machine learning models. The framework extends existing multicalibration notions to handle multi-dimensional outputs and flexible functional mappings. It provides both population and finite-sample algorithms with convergence guarantees under KL-smoothness assumptions. The framework is demonstrated on three diverse applications: de-biased text generation, prediction-set conditional coverage in hierarchical classification, and fair FNR control in image segmentation.

## Method Summary
The (s, G, α)-GMC framework achieves multi-group fairness by iteratively projecting onto constraint sets until fairness violations fall below threshold α. It uses a potential functional L(f, h, y, D) whose gradient with respect to f equals the fairness violation s(f, x, h, y, D). The algorithm updates the scoring function f in the direction of constraint violations using a constraint class G that encodes the groups. The framework provides convergence guarantees under KL-smoothness assumptions and can be implemented with either population-level updates or finite-sample algorithms using uniform convergence bounds.

## Key Results
- Proposes (s, G, α)-GMC framework generalizing multicalibration to multi-dimensional outputs
- Demonstrates applications to de-biased text generation, hierarchical classification, and image segmentation
- Provides both population and finite-sample algorithms with convergence guarantees
- Shows framework can achieve fair false negative rate control and prediction set coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative projection onto constraint sets reduces fairness violations below threshold α
- Mechanism: KL-smoothness of potential functional L ensures gradient updates decrease L by at least ηα - η²KLEx[∥g(f(t)(x), x)∥²]/2 per iteration
- Core assumption: KL-smoothness of L with respect to f
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If L is not KL-smooth or gradient s is not computable

### Mechanism 2
- Claim: Finite-sample algorithm achieves fairness guarantees using uniform convergence
- Mechanism: Sample size m ≥ C₁(d(G)+log(1/δ))/α² ensures empirical violations upper bound true violations with probability 1-δ
- Core assumption: Finite dimension d(G) of constraint class G
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If d(G) is infinite or too large

### Mechanism 3
- Claim: Framework generalizes multiple fairness notions through appropriate choice of s and G
- Mechanism: Different fairness problems expressed as (s, G, α)-GMC by defining s to capture violation and G to encode groups
- Core assumption: Fairness violation expressible as ⟨s(f, x, h, y, D), g(f(x), x)⟩
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If violation cannot be expressed as bilinear form ⟨s, g⟩

## Foundational Learning

- Concept: Multicalibration
  - Why needed here: Framework extends multicalibration to multi-dimensional outputs; essential for understanding group fairness
  - Quick check question: What is the difference between multicalibration and multi-accuracy? (Hint: conditioning event)

- Concept: Convex optimization and projection
  - Why needed here: Algorithm uses iterative projection onto convex sets; crucial for understanding convergence
  - Quick check question: In convex optimization, what property ensures that projecting onto a convex set decreases the distance to the set?

- Concept: Uniform convergence and VC dimension
  - Why needed here: Finite-sample algorithm relies on uniform convergence over G; essential for determining sample complexity
  - Quick check question: What is the relationship between the VC dimension of a function class and the sample complexity needed for uniform convergence?

## Architecture Onboarding

- Component map: Potential functional L(f, h, y, D) -> Gradient mapping s(f, x, h, y, D) -> Constraint class G -> Iterative update algorithm -> Calibration dataset (+ validation dataset for finite-sample)

- Critical path: 1) Define fairness violation s, 2) Choose constraint class G, 3) Verify KL-smoothness of L, 4) Implement iterative update algorithm, 5) Ensure sufficient sample size for uniform convergence

- Design tradeoffs: KL-smoothness vs. stronger conditions for faster convergence; larger G increases sample complexity but allows more expressive constraints; randomization can help satisfy technical conditions

- Failure signatures: Algorithm fails to converge (violations remain above α), poor accuracy despite fairness (overly conservative updates), high sample complexity making method impractical, inability to express fairness violation in required ⟨s, g⟩ form

- First 3 experiments:
  1. Implement algorithm for binary classification with two sensitive groups, verify convergence to equal false positive rates
  2. Test finite-sample version on small synthetic dataset, verify uniform convergence with theoretical sample complexity
  3. Apply framework to multi-class classification, defining s to capture multi-group fairness violations across all classes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can (s, G, α)-GMC framework be extended to handle non-convex constraint sets F and multi-dimensional label spaces Y?
- Basis in paper: [explicit] Current assumptions rely on convex F and don't address non-convex or higher-dimensional Y
- Why unresolved: Convergence proofs and assumptions rely on convexity of F and don't address complexity of non-convex sets or higher-dimensional labels
- What evidence would resolve it: Developing convergence results for non-convex F or extending framework to multi-dimensional Y with appropriate modifications

### Open Question 2
- Question: What are the computational trade-offs between finite-sample and distributional versions of (s, G, α)-GMC algorithm?
- Basis in paper: [explicit] Both versions presented but computational efficiency not compared
- Why unresolved: No detailed analysis of computational complexity or empirical performance differences between versions
- What evidence would resolve it: Empirical studies comparing runtime and accuracy of both versions on various datasets and applications

### Open Question 3
- Question: Can (s, G, α)-GMC framework be applied to fairness metrics beyond false negative rates?
- Basis in paper: [explicit] Focuses on FNR and doesn't explore other fairness metrics
- Why unresolved: Framework tailored to specific metrics, extending to others requires additional theoretical and empirical work
- What evidence would resolve it: Formulating and proving applicability to other fairness metrics with experimental validation

## Limitations
- KL-smoothness assumption is abstract and may be difficult to verify for complex fairness constraints
- Finite-sample results may require exponentially large sample sizes for complex constraint classes
- Framework's ability to handle non-linear fairness constraints or interdependent group fairness is not fully explored

## Confidence
- High confidence: Framework's ability to generalize existing fairness notions to general settings
- Medium confidence: Convergence guarantees dependent on KL-smoothness assumption
- Low confidence: Practical sample complexity for real-world applications may be overly conservative

## Next Checks
1. Verify KL-smoothness for broader class of fairness violations beyond presented applications
2. Conduct empirical studies on sample complexity across different constraint classes G
3. Test framework's ability to handle interdependent group fairness constraints