---
ver: rpa2
title: Are You Doubtful? Oh, It Might Be Difficult Then! Exploring the Use of Model
  Uncertainty for Question Difficulty Estimation
arxiv_id: '2412.11831'
source_url: https://arxiv.org/abs/2412.11831
tags:
- difficulty
- question
- uncertainty
- features
- choice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for estimating the difficulty of
  multiple-choice questions using model uncertainty from large language models. The
  approach combines textual features and uncertainty metrics from multiple LLMs in
  a Random Forest regressor to predict question difficulty, measured by the proportion
  of students answering correctly.
---

# Are You Doubtful? Oh, It Might Be Difficult Then! Exploring the Use of Model Uncertainty for Question Difficulty Estimation

## Quick Facts
- arXiv ID: 2412.11831
- Source URL: https://arxiv.org/abs/2412.11831
- Authors: Leonidas Zotos; Hedderik van Rijn; Malvina Nissim
- Reference count: 40
- Primary result: Model uncertainty features significantly improve multiple-choice question difficulty prediction, achieving state-of-the-art results on USMLE and CMCQRD datasets

## Executive Summary
This paper introduces a novel approach for estimating multiple-choice question difficulty using model uncertainty from large language models. The method combines textual features with uncertainty metrics from multiple LLMs in a Random Forest regressor to predict question difficulty, measured by the proportion of students answering correctly. Experiments on three datasets (Biopsychology, USMLE, and CMCQRD) demonstrate that model uncertainty serves as a useful proxy for item difficulty, with the approach achieving state-of-the-art results on the USMLE and CMCQRD datasets.

## Method Summary
The approach estimates question difficulty by combining textual features (TF-IDF and BERT embeddings) with uncertainty metrics from multiple LLMs. The uncertainty metrics include 1st token probability and choice order sensitivity, which capture the model's confidence in selecting correct answers. A Random Forest regressor learns to combine these features to predict difficulty scores. The method was evaluated on three multiple-choice question datasets, comparing performance with and without uncertainty features to demonstrate their contribution to difficulty estimation.

## Key Results
- Model uncertainty features significantly improve difficulty prediction across all three datasets
- Combining 1st token probability and choice order sensitivity uncertainty metrics yields the best results
- State-of-the-art RMSE scores of 0.286 on USMLE and 8.28 on CMCQRD datasets
- SHAP analysis shows higher model certainty correlates with higher predicted student performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM uncertainty metrics serve as proxies for question difficulty
- Mechanism: When questions are difficult, model probability distributions become more uniform, indicating higher uncertainty
- Core assumption: Model uncertainty correlates with student performance on the same questions
- Evidence anchors:
  - [abstract]: "model uncertainty features significantly improve difficulty prediction"
  - [section 5.3]: SHAP analysis shows higher certainty correlates with correct answers
  - [corpus]: Weak evidence - related papers focus on different uncertainty contexts
- Break condition: When LLMs become too capable and answer all questions with high confidence

### Mechanism 2
- Claim: Combining multiple LLM uncertainty signals creates a more robust difficulty estimator
- Mechanism: Different models capture different aspects of difficulty through diverse uncertainty patterns
- Core assumption: Ensemble of diverse LLMs provides complementary uncertainty signals
- Evidence anchors:
  - [abstract]: "combining textual features and uncertainty metrics from multiple LLMs"
  - [section 4.4]: Experiments with both default precision and 4-bit quantized models
  - [section 5.1]: Combining both uncertainty metrics yields best results
- Break condition: When model diversity decreases or uncertainty patterns converge

### Mechanism 3
- Claim: Random Forest regressor effectively learns to weight and combine uncertainty features
- Mechanism: Ensemble learning captures non-linear relationships between uncertainty signals and difficulty
- Core assumption: Random Forest can effectively learn complex relationships between model uncertainty and question difficulty
- Evidence anchors:
  - [abstract]: "By using both model uncertainty features as well as textual features in a Random Forest regressor"
  - [section 4]: Random Forest chosen for interpretability
  - [section 5.3]: SHAP analysis demonstrates feature importance patterns
- Break condition: When relationships become non-linear in ways Random Forest cannot capture

## Foundational Learning

- Concept: Multiple-choice question structure and difficulty metrics
  - Why needed here: Understanding how MCQs are constructed and how difficulty is measured (p-value) is crucial for interpreting predictions
  - Quick check question: What does a p-value of 0.8 mean for a multiple-choice question?

- Concept: Model uncertainty and calibration
  - Why needed here: The approach relies on measuring and interpreting model uncertainty
  - Quick check question: How does 1st token probability relate to model confidence?

- Concept: Feature engineering and vector representation
  - Why needed here: The system combines multiple feature types that need proper encoding
  - Quick check question: Why might TF-IDF scores be preferred over raw word counts for text features?

## Architecture Onboarding

- Component map: MCQ text -> Text feature extractor (TF-IDF + BERT) -> Uncertainty generator (Multiple LLMs) -> Feature combiner (Vector concatenation) -> Model (Random Forest regressor) -> Output (Difficulty prediction)

- Critical path:
  1. Process MCQ text through text feature extractors
  2. Generate uncertainty features from LLMs
  3. Concatenate all features
  4. Random Forest prediction
  5. Output difficulty score

- Design tradeoffs:
  - Simple Random Forest vs. complex neural networks: Chosen for interpretability
  - Multiple small LLMs vs. single large LLM: Ensemble approach provides robustness
  - Default vs. quantized models: Balance between performance and efficiency

- Failure signatures:
  - High RMSE on validation set: Check feature extraction or model training
  - Random Forest overfits: Check feature correlation or increase regularization
  - Uncertainty features not contributing: Verify LLM responses or feature scaling

- First 3 experiments:
  1. Baseline: Text features only (TF-IDF + BERT) → Establish baseline performance
  2. Uncertainty only: 1st token probabilities from single LLM → Validate uncertainty mechanism
  3. Combined: Text + uncertainty features from all LLMs → Test full system performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can calibrated language models be developed and integrated into the uncertainty-based difficulty estimation framework to improve performance across different educational levels?
- Basis in paper: Explicit - The paper discusses using calibrated LLMs as a potential solution to address the limitation of relying on uncalibrated models.
- Why unresolved: The paper only hypothesizes about the potential benefits of calibrated models without exploring their implementation or effectiveness.
- What evidence would resolve it: Empirical results comparing difficulty estimation performance using calibrated versus uncalibrated models across multiple educational datasets, demonstrating whether calibration consistently improves prediction accuracy.

### Open Question 2
- Question: To what extent do linguistic features contribute independently to question difficulty estimation beyond what is captured by model uncertainty and basic text encodings?
- Basis in paper: Inferred - The paper shows that linguistic features combined with other features can marginally improve results, but their independent contribution remains unclear.
- Why unresolved: The experiments primarily focus on combinations of features rather than isolating the unique contribution of linguistic features.
- What evidence would resolve it: Ablation studies systematically removing linguistic features from the model while controlling for uncertainty and text features, showing whether their contribution is significant and consistent across different question types.

### Open Question 3
- Question: Can model uncertainty-based difficulty estimation be effectively extended to non-multiple-choice question formats such as mathematical reasoning or open-ended questions?
- Basis in paper: Explicit - The paper explicitly states that their approach has not been evaluated on non-MCQ formats and questions whether it would be effective for mathematical reasoning.
- Why unresolved: The methodology was only tested on factual knowledge and reading comprehension MCQs, with no exploration of other question types.
- What evidence would resolve it: Experimental results applying the uncertainty-based approach to mathematical reasoning questions and open-ended questions, demonstrating whether model uncertainty remains a useful signal for these formats.

## Limitations

- Generalizability across domains is uncertain since all tested datasets focus on medical or psychology education
- Computational cost of running multiple LLMs may limit scalability for large question banks
- Relies on the assumption that LLM uncertainty correlates with human difficulty, which may break down as models improve

## Confidence

- **High Confidence**: The core finding that model uncertainty features improve difficulty prediction is well-supported by experimental results
- **Medium Confidence**: The claim that ensemble uncertainty outperforms single model uncertainty is supported but could benefit from more systematic ablation studies
- **Medium Confidence**: The assertion of state-of-the-art performance is based on comparisons with unspecified baselines

## Next Checks

1. **Cross-domain validation**: Test the model on question datasets from non-medical domains (e.g., mathematics, humanities) to assess generalizability beyond current educational contexts

2. **Human correlation study**: Conduct a controlled experiment comparing LLM uncertainty scores with human expert difficulty ratings on the same question sets to validate the assumed correlation

3. **Temporal robustness test**: Evaluate model performance on questions from different time periods to assess whether LLM uncertainty remains a reliable proxy as models are updated or question patterns evolve