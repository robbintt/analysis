---
ver: rpa2
title: Is this the real life? Is this just fantasy? The Misleading Success of Simulating
  Social Interactions With LLMs
arxiv_id: '2403.05020'
source_url: https://arxiv.org/abs/2403.05020
tags:
- social
- agents
- script
- mode
- simulations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares large language model (LLM) social simulations
  under omniscient (SCRIPT) versus information-asymmetric (AGENTS) settings. While
  omniscient simulations achieve higher goal completion and naturalness, they fail
  to reflect realistic human interaction and introduce harmful biases that hinder
  learning for realistic settings.
---

# Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs

## Quick Facts
- arXiv ID: 2403.05020
- Source URL: https://arxiv.org/abs/2403.05020
- Reference count: 28
- LLMs in omniscient settings achieve higher performance but introduce harmful biases that don't generalize to realistic information-asymmetric scenarios

## Executive Summary
This study examines the limitations of large language model (LLM) social simulations by comparing omniscient (SCRIPT) and information-asymmetric (AGENTS) settings. While omniscient simulations achieve higher goal completion and naturalness, they fail to reflect realistic human interaction patterns and introduce harmful biases that hinder learning for realistic settings. The research reveals that finetuning on omniscient data improves naturalness but not cooperative goal achievement, and imparts overly agreeable or information-leaking strategies that do not generalize. These findings highlight the critical need for more transparent reporting and better evaluation frameworks for LLM-based social simulations.

## Method Summary
The researchers conducted experiments using both omniscient and information-asymmetric settings for social simulations. They compared goal completion rates, naturalness scores, and cooperative behavior across different finetuning approaches. The study employed controlled simulation environments with multiple scenarios to evaluate LLM performance under varying information access conditions. Data was collected and analyzed to measure the impact of training data source (omniscient vs. information-asymmetric) on downstream task performance and behavior.

## Key Results
- Omniscient simulations achieve higher goal completion and naturalness scores but fail to reflect realistic human interaction patterns
- Finetuning on omniscient data improves naturalness but impairs cooperative goal achievement in information-asymmetric settings
- Omniscient training imparts harmful biases including overly agreeable behavior and excessive information leakage that don't generalize to realistic scenarios

## Why This Works (Mechanism)
The study demonstrates that LLMs trained on omniscient data learn strategies optimized for perfect information scenarios, which break down when information is limited. These models develop overly cooperative behaviors that work well when all parties have complete information but fail when realistic constraints apply. The mechanism involves the model internalizing patterns that exploit full information availability, leading to strategies that are ineffective or counterproductive in asymmetric information settings where agents must navigate uncertainty and incomplete knowledge.

## Foundational Learning
- Information asymmetry in communication: Understanding how limited information access affects decision-making and interaction strategies (why needed: central to distinguishing realistic from unrealistic simulation scenarios)
- Social simulation evaluation metrics: Goal completion rates, naturalness scores, and cooperative behavior measures (why needed: to quantitatively compare different training approaches)
- Bias propagation in LLM finetuning: How training data characteristics influence learned behaviors (why needed: to understand why omniscient training creates harmful biases)

## Architecture Onboarding

**Component Map:**
LLM base model -> Finetuning data source (omniscient/information-asymmetric) -> Performance evaluation (goal completion/naturalness) -> Behavioral analysis

**Critical Path:**
Training data selection → Finetuning process → Performance evaluation → Bias assessment

**Design Tradeoffs:**
Omniscient training offers higher performance metrics but creates unrealistic behaviors; information-asymmetric training is more realistic but achieves lower baseline performance. The tradeoff involves balancing simulation realism against measurable success metrics.

**Failure Signatures:**
Decreased goal achievement in information-asymmetric settings despite improved naturalness scores; overly agreeable responses; excessive information sharing in scenarios requiring strategic withholding.

**Three First Experiments:**
1. Compare goal completion rates between models trained on omniscient vs. information-asymmetric data across multiple social scenarios
2. Evaluate naturalness scores while measuring information leakage frequency in cooperative tasks
3. Test transfer learning from omniscient to information-asymmetric settings to measure generalization capability

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Reliance on controlled simulation environments that may not fully capture real-world social complexity
- Evaluation metrics may not comprehensively capture all dimensions of successful social interaction
- Limited scope of scenarios examined may affect generalizability of findings

## Confidence

**High Confidence:**
- Findings regarding detrimental effects of omniscient finetuning on information-asymmetric performance
- Quantitative results showing decreased goal achievement despite improved naturalness scores

**Medium Confidence:**
- Characterization of learned biases as "harmful" (context-dependent assessment)
- Sample size and scope of scenarios providing solid but not comprehensive foundation

## Next Checks

1. Conduct experiments with real human participants interacting with both omniscient and information-asymmetric LLM agents to validate simulation findings in actual social contexts

2. Test the transferability of findings across multiple distinct social tasks beyond the current scenario to assess generalizability

3. Implement and evaluate alternative finetuning approaches that explicitly penalize information leakage while maintaining naturalness to determine if more balanced performance is achievable