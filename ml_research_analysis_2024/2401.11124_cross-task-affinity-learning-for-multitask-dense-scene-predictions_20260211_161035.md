---
ver: rpa2
title: Cross-Task Affinity Learning for Multitask Dense Scene Predictions
arxiv_id: '2401.11124'
source_url: https://arxiv.org/abs/2401.11124
tags:
- task
- distillation
- tasks
- features
- cross-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cross-Task Affinity Learning (CTAL), a lightweight
  multitask learning module that improves dense scene predictions by modeling local
  and long-range dependencies across tasks using optimized affinity matrices and grouped
  convolutions. CTAL outperforms existing task-prediction distillation methods in
  both CNN and transformer backbones, achieving state-of-the-art performance with
  fewer parameters than single-task learning.
---

# Cross-Task Affinity Learning for Multitask Dense Scene Predictions

## Quick Facts
- arXiv ID: 2401.11124
- Source URL: https://arxiv.org/abs/2401.11124
- Authors: Dimitrios Sinodinos; Narges Armanfard
- Reference count: 40
- Key outcome: Introduces CTAL, a lightweight multitask learning module that improves dense scene predictions by modeling local and long-range dependencies across tasks using optimized affinity matrices and grouped convolutions. CTAL outperforms existing task-prediction distillation methods in both CNN and transformer backbones, achieving state-of-the-art performance with fewer parameters than single-task learning.

## Executive Summary
This paper introduces Cross-Task Affinity Learning (CTAL), a lightweight multitask learning module that improves dense scene predictions by modeling local and long-range dependencies across tasks using optimized affinity matrices and grouped convolutions. CTAL outperforms existing task-prediction distillation methods in both CNN and transformer backbones, achieving state-of-the-art performance with fewer parameters than single-task learning. The method demonstrates robust performance across multiple datasets and task combinations while maintaining computational efficiency.

## Method Summary
CTAL is a multitask learning module that enhances dense scene predictions by capturing cross-task dependencies through affinity matrices and grouped convolutions. The module learns to model both local and long-range spatial relationships between tasks, allowing for improved information sharing while maintaining task-specific features. By optimizing affinity matrices that represent task relationships, CTAL can dynamically adjust how information flows between tasks during training. The lightweight design makes it compatible with various backbone architectures, including both CNNs and transformers.

## Key Results
- Achieved MTL gain of +2.64% (SS) and +4.76% (MS) on NYUv2 with significantly fewer parameters than baselines
- Outperformed task-prediction distillation methods across CNN and transformer backbones
- Demonstrated state-of-the-art performance while maintaining fewer parameters than single-task learning approaches
- Showed robust generalization across different datasets and task combinations

## Why This Works (Mechanism)
CTAL works by explicitly modeling the dependencies between different dense prediction tasks through optimized affinity matrices. These matrices capture both local and long-range spatial relationships, allowing the model to identify which features from one task are beneficial for another. The grouped convolution mechanism then enables selective information sharing while preserving task-specific representations. This approach addresses the challenge of balancing shared learning with task-specific optimization, which is particularly important in dense prediction tasks where spatial context varies significantly across tasks.

## Foundational Learning
- Multitask Learning: Learning multiple related tasks simultaneously to improve generalization
  - Why needed: Dense scene understanding requires coordinated predictions across tasks
  - Quick check: Compare single-task vs. multitask performance on benchmark datasets
- Dense Prediction: Generating pixel-level predictions for tasks like segmentation and depth estimation
  - Why needed: Scene understanding requires fine-grained spatial predictions
  - Quick check: Evaluate pixel-wise accuracy metrics (IoU, RMSE)
- Affinity Matrices: Learnable matrices that capture relationships between tasks
  - Why needed: Explicitly model cross-task dependencies for better information sharing
  - Quick check: Analyze learned affinity values across different task pairs
- Grouped Convolutions: Convolution operations that process feature groups separately
  - Why needed: Enable selective information sharing while maintaining task specificity
  - Quick check: Measure parameter efficiency compared to standard convolutions

## Architecture Onboarding
- Component map: Input features -> Affinity matrix optimization -> Grouped convolutions -> Task-specific heads -> Output predictions
- Critical path: Feature extraction → Affinity learning → Cross-task feature fusion → Task-specific prediction
- Design tradeoffs: Balances between shared learning (parameter efficiency) and task-specific optimization (performance)
- Failure signatures: Performance degradation when tasks have minimal semantic overlap; overfitting when affinity matrices become too task-specific
- First experiments: 1) Ablation study on affinity matrix dimensions 2) Comparison with task-specific vs. shared backbones 3) Cross-dataset generalization testing

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on indoor scene understanding datasets, with limited testing on outdoor or medical imaging tasks
- Claims of superiority over task-prediction distillation methods need verification across diverse backbone architectures
- Assumption that affinity matrices effectively capture task dependencies may not hold for all task combinations

## Confidence
- Claims about state-of-the-art performance on NYUv2: High
- Generalization claims to new tasks/datasets: Medium
- Parameter and FLOPs efficiency claims: Medium
- Scalability across backbone architectures: Low

## Next Checks
1. Test CTAL on outdoor scene understanding datasets (Cityscapes, KITTI) to verify cross-domain generalization
2. Evaluate the method with diverse transformer backbones (Swin, ConvNeXt, ViT) to confirm architectural robustness
3. Conduct ablation studies on task affinity matrices to determine their contribution versus the grouped convolution mechanism alone