---
ver: rpa2
title: Feature Protection For Out-of-distribution Generalization
arxiv_id: '2405.16027'
source_url: https://arxiv.org/abs/2405.16027
tags: []
core_contribution: This paper investigates why fine-tuning large pre-trained models
  like CLIP often leads to worse out-of-distribution (OOD) performance despite improved
  in-distribution accuracy. The authors demonstrate that fine-tuning causes significant
  distortion of pre-trained semantic features, leading to overfitting on the fine-tuning
  dataset and poor generalization to OOD data.
---

# Feature Protection For Out-of-distribution Generalization

## Quick Facts
- arXiv ID: 2405.16027
- Source URL: https://arxiv.org/abs/2405.16027
- Reference count: 9
- Primary result: Fine-tuning pre-trained models causes feature distortion leading to OOD performance degradation; feature protection methods like WiSE-FT improve OOD accuracy

## Executive Summary
This paper investigates the degradation of out-of-distribution (OOD) performance when fine-tuning large pre-trained models like CLIP. The authors demonstrate that fine-tuning causes significant distortion of pre-trained semantic features, leading to overfitting on the fine-tuning dataset and poor generalization to OOD data. Through systematic evaluation of regularization methods including L1/L2 penalties, knowledge distillation, LoRA, and model averaging (WiSE-FT), they show that protecting pre-trained features is crucial for maintaining OOD generalization. WiSE-FT achieves the best OOD accuracy of 63.5% on ImageNet variants, outperforming both standard fine-tuning and zero-shot inference.

## Method Summary
The authors fine-tune CLIP ViT-B/16 on ImageNet using standard training procedures (AdamW optimizer, learning rate 3e-5, batch size 512, cosine schedule with warmup). They evaluate several feature protection methods: L1/L2 regularization penalties on weights (λ=1e-4 to 1e-0), knowledge distillation using L2 penalty between fine-tuned and pre-trained features (λ=1e-3 to 5e-3), LoRA parameter-efficient fine-tuning (ranks 2-32), and WiSE-FT model averaging (α=0.2-0.8). Performance is measured through in-distribution accuracy on ImageNet and OOD accuracy on five ImageNet variants plus DomainNet other domains. Linear probing accuracy is used to assess the effectiveness of preserved features.

## Key Results
- Fine-tuning CLIP causes feature distortion that degrades OOD performance despite improving in-distribution accuracy
- WiSE-FT achieves 63.5% OOD accuracy on ImageNet variants, outperforming standard fine-tuning and zero-shot inference
- L1/L2 regularization and knowledge distillation effectively preserve pre-trained features and improve OOD generalization
- Feature protection methods prevent overfitting on fine-tuning data while maintaining semantic feature quality

## Why This Works (Mechanism)
Feature protection methods work by constraining the fine-tuning process to preserve the semantic structure learned during pre-training. When fine-tuning is unconstrained, the model adapts to fit the fine-tuning data at the expense of the broader semantic representations that enable OOD generalization. Regularization methods like L1/L2 penalties and knowledge distillation penalize deviations from the pre-trained feature space, preventing the model from overfitting to the specific distribution of the fine-tuning dataset. WiSE-FT explicitly combines the pre-trained model with the fine-tuned model, allowing the model to benefit from task-specific adaptation while maintaining access to the original semantic features.

## Foundational Learning
- **Feature forgetting**: The degradation of pre-trained semantic features during fine-tuning that reduces OOD generalization. *Why needed*: Understanding this phenomenon is central to explaining why fine-tuning can hurt OOD performance. *Quick check*: Compare linear probing accuracy before and after fine-tuning to measure feature quality changes.
- **Knowledge distillation**: Training method that uses a pre-trained model's outputs as targets for the fine-tuned model. *Why needed*: Provides a way to constrain fine-tuning to preserve pre-trained features. *Quick check*: Verify that KD loss between fθ(x) and fθ0(x) decreases during training.
- **Parameter-efficient fine-tuning**: Methods like LoRA that add small trainable matrices to existing layers rather than updating all parameters. *Why needed*: Offers a way to adapt models with fewer parameters, potentially reducing feature distortion. *Quick check*: Confirm that LoRA matrices (A initialized Gaussian, B zeroed) are properly integrated into the model.
- **Model averaging (WiSE-FT)**: Technique that combines pre-trained and fine-tuned models using weighted averaging. *Why needed*: Directly preserves pre-trained features while allowing task adaptation. *Quick check*: Test different averaging coefficients (α) to find optimal balance.
- **Linear probing**: Evaluating feature quality by training a linear classifier on frozen features. *Why needed*: Provides a measure of semantic feature effectiveness independent of fine-tuning. *Quick check*: Compare linear probing accuracy across different feature protection methods.

## Architecture Onboarding

### Component Map
Pre-trained CLIP ViT-B/16 -> Fine-tuning process (with feature protection) -> Evaluation on ID and OOD datasets -> Linear probing for feature quality assessment

### Critical Path
The critical path involves: (1) loading pre-trained CLIP model, (2) applying fine-tuning with chosen protection method, (3) evaluating on ID and OOD datasets, (4) computing linear probing accuracy. The feature protection method directly impacts steps 2-4, with WiSE-FT showing the strongest OOD performance.

### Design Tradeoffs
The main tradeoff is between in-distribution accuracy and out-of-distribution generalization. Strong feature protection (high λ values) preserves OOD performance but may limit ID accuracy gains. WiSE-FT balances this by explicitly maintaining the pre-trained model while allowing task adaptation. LoRA offers computational efficiency but may be less effective at preserving features compared to explicit regularization.

### Failure Signatures
- Over-regularization: High λ values cause poor ID accuracy and large train/validation gaps
- Under-regularization: Insufficient feature protection leads to feature forgetting and poor OOD performance
- LoRA rank too low: Fails to capture necessary adaptation, performing worse than full fine-tuning
- WiSE-FT coefficient issues: Incorrect α values either under-utilize fine-tuning benefits or fail to preserve features adequately

### First Experiments to Run
1. Fine-tune CLIP ViT-B/16 on ImageNet with varying L1/L2 regularization strengths (λ=1e-4 to 1e-0) and measure ID vs OOD performance tradeoff
2. Implement WiSE-FT with different averaging coefficients (α=0.2,0.4,0.6,0.8) to find optimal balance between adaptation and feature preservation
3. Compare LoRA with different ranks (2,4,8,16,32) against full fine-tuning and regularization methods on both ID and OOD accuracy

## Open Questions the Paper Calls Out

### Open Question 1
Does the effectiveness of feature protection methods like WiSE-FT, L1/L2 regularization, and KD vary significantly across different model architectures (e.g., ResNet vs. ViT) or scales (e.g., CLIP-ViT-B/16 vs. CLIP-ViT-L/14)?

### Open Question 2
Can feature protection methods be effectively combined with other OOD generalization techniques like domain adversarial training or self-supervised pre-training to achieve even better performance?

### Open Question 3
How does the forgetting of pre-trained features manifest at different layers of the network, and do different feature protection methods differentially preserve features at various depths?

## Limitations
- The paper only evaluates feature protection methods on CLIP ViT-B/16, limiting generalizability to other architectures
- Analysis focuses on specific datasets without establishing causal mechanisms linking feature modifications to OOD performance
- Does not explore combinations of feature protection with other established OOD generalization techniques
- Limited analysis of layer-wise feature preservation and how different methods affect various network depths

## Confidence
- **High confidence**: Empirical observation that fine-tuning degrades OOD performance
- **Medium confidence**: Proposed mechanism of feature distortion causing overfitting
- **Medium confidence**: Effectiveness of regularization methods as generalizable solutions

## Next Checks
1. Conduct ablation studies varying regularization strength (λ) across a wider range to identify optimal trade-offs between ID and OOD performance for each method
2. Test feature protection methods on non-CLIP architectures (e.g., ResNet-based models, vision transformers with different pre-training objectives) to assess architectural generality
3. Perform controlled experiments isolating specific feature layers to determine which layers contribute most to OOD degradation and whether targeted protection is more effective than global regularization