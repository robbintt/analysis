---
ver: rpa2
title: Which Model to Transfer? A Survey on Transferability Estimation
arxiv_id: '2402.15231'
source_url: https://arxiv.org/abs/2402.15231
tags:
- transferability
- target
- source
- learning
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides the first comprehensive survey of model transferability
  estimation (MTE), which aims to efficiently select the most suitable pre-trained
  models for downstream tasks without individually training them. The authors categorize
  MTE methods into two realms: source-free (SF-MTE) and source-dependent (SD-MTE),
  based on whether source data is required.'
---

# Which Model to Transfer? A Survey on Transferability Estimation

## Quick Facts
- arXiv ID: 2402.15231
- Source URL: https://arxiv.org/abs/2402.15231
- Authors: Yuhe Ding; Bo Jiang; Aijing Yu; Aihua Zheng; Jian Liang
- Reference count: 14
- Primary result: First comprehensive survey of model transferability estimation (MTE) methods for selecting optimal pre-trained models for downstream tasks

## Executive Summary
This paper provides the first comprehensive survey of model transferability estimation (MTE) methods, which aim to efficiently select the most suitable pre-trained models for downstream tasks without individually training them. The authors categorize MTE methods into source-free (SF-MTE) and source-dependent (SD-MTE) approaches based on whether source data is required. SF-MTE methods are further divided into static approaches (feature structure-based, Bayesian statistic-based, information theory-based, matrix analysis-based) and dynamic approaches (energy models, linear frameworks, model vectorization). SD-MTE methods leverage source data for distribution matching or gradient information. The paper evaluates MTE methods using correlation coefficients between predicted transferability scores and actual transferred accuracy.

## Method Summary
MTE methods predict transferability scores for candidate pre-trained models that should correlate with their actual transferred accuracy on target tasks. Source-free methods analyze only target data and pre-trained models, while source-dependent methods use both source and target data. Static methods compute scores directly from statistical information like features and logits, while dynamic methods involve optimization procedures. The methods are evaluated using Pearson, Kendall, and weighted Kendall correlation coefficients between predicted scores and actual fine-tuned accuracy.

## Key Results
- MTE methods can predict model performance on target tasks without individual fine-tuning
- Source-free methods offer privacy benefits but may be less accurate than source-dependent approaches
- Current MTE methods show sensitivity to experimental settings and lack unified benchmarks
- Foundation models represent an emerging area where transferability estimation remains unexplored

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transferability estimation methods can predict which pre-trained model will perform best on a target task without individual training.
- Mechanism: These methods compute statistical or structural features from pre-trained models and target data, then use correlation-based metrics to rank models without fine-tuning.
- Core assumption: The computed features or statistical properties correlate strongly with actual transfer performance across diverse tasks and model architectures.
- Evidence anchors: [abstract] "MTE methods predict a transferability score for each candidate model and these scores should be highly correlated with the transferred accuracy" [section] "An efficient MTE metric should predict a score for the candidate source model which is highly correlated with their transferred accuracy"
- Break condition: If correlation between predicted scores and actual accuracy drops significantly across different experimental settings or model types.

### Mechanism 2
- Claim: Static methods can estimate transferability by analyzing feature structure without requiring source data.
- Mechanism: These methods calculate metrics based on intra-class variance, inter-class separation, or feature redundancy using only target data and pre-trained model outputs.
- Core assumption: Good transfer performance correlates with specific feature space properties like class separability or low intra-class variance.
- Evidence anchors: [section] "Static methods refer to those that calculate scores directly based on some statistical information, such as features and logits computed on the target dataset" [section] "A fast and general method TMI [Xu and Kang, 2023] measures the transferability by the intra-class feature variance"
- Break condition: When feature structure assumptions break down for complex tasks or when source data distribution differs drastically from target.

### Mechanism 3
- Claim: Distribution matching methods can estimate transferability by comparing source and target data distributions.
- Mechanism: These methods use optimal transport, Wasserstein distance, or kernel mean embeddings to quantify domain gaps between source and target distributions.
- Core assumption: Smaller distribution distance between source and target implies better transferability.
- Evidence anchors: [section] "Since source data is available, it is possible to assess the domain gap using distribution distance metrics directly" [section] "Optimal transport (OT), also known as the Wasserstein distance... is a mathematical framework used to measure the minimum cost or distance required to transport a set of objects from one distribution to another"
- Break condition: When distribution distances don't capture task similarity or when high-dimensional feature spaces make distance calculations unreliable.

## Foundational Learning

- Concept: Correlation coefficients (Pearson, Kendall, weighted Kendall)
  - Why needed here: These metrics quantify how well predicted transferability scores match actual transfer performance
  - Quick check question: What's the difference between Pearson correlation (measuring linear relationships) and Kendall correlation (measuring rank order)?

- Concept: Transfer learning paradigms (pre-training and fine-tuning)
  - Why needed here: Understanding the baseline transfer learning approach is essential for evaluating MTE methods
  - Quick check question: Why is it computationally prohibitive to individually train each candidate model on the target task?

- Concept: Feature space analysis and statistical metrics
  - Why needed here: Most MTE methods rely on analyzing features extracted from pre-trained models
  - Quick check question: How does intra-class variance relate to a model's generalization ability on a target task?

## Architecture Onboarding

- Component map: Model hub (pre-trained models) -> Feature extraction -> Statistical analysis -> Transferability score -> Correlation evaluation
- Critical path: Extract features from pre-trained models on target data -> Compute statistical metrics -> Generate transferability scores -> Compare with actual fine-tuned accuracy
- Design tradeoffs: Source-free vs source-dependent methods (data privacy vs accuracy), static vs dynamic methods (speed vs adaptability), computational cost vs estimation quality
- Failure signatures: Low correlation coefficients across diverse tasks, sensitivity to hyperparameter changes, poor performance on novel model architectures
- First 3 experiments:
  1. Implement TMI method and test on a small image classification benchmark with 5 pre-trained models
  2. Compare H-score and LogME on a model selection task using the same dataset and measure correlation coefficients
  3. Test distribution matching methods using optimal transport on a cross-domain adaptation task and analyze sensitivity to hyperparameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can transferability estimation methods be made robust across different experimental settings and model hubs?
- Basis in paper: [explicit] The paper identifies robustness to experimental settings as a key challenge, noting that minor variations can lead to different conclusions about metric superiority
- Why unresolved: Current methods show sensitivity to changes in experimental setups, model architectures, and source datasets, leading to inconsistent results
- What evidence would resolve it: Systematic evaluation across diverse model hubs, transfer learning algorithms, and target tasks showing consistent performance rankings of transferability metrics

### Open Question 2
- Question: How can transferability estimation be extended to foundation models like GPT and CLIP families?
- Basis in paper: [explicit] The paper identifies foundation models as an emerging area where transferability remains unexplored, noting these models have more parameters and higher training costs
- Why unresolved: Foundation models differ significantly from traditional models in scale, training objectives, and architecture, requiring new estimation approaches
- What evidence would resolve it: Development and validation of transferability metrics specifically designed for foundation models that correlate well with downstream task performance

### Open Question 3
- Question: What is the relationship between transferability estimation and various transfer learning paradigms beyond pre-training and fine-tuning?
- Basis in paper: [explicit] The paper notes that most methods rely on pre-training then fine-tuning paradigm and questions how different transfer learning algorithms relate to model transferability
- Why unresolved: Current research has not explored how source-free domain adaptation, test-time adaptation, and other paradigms affect transferability predictions
- What evidence would resolve it: Comparative studies showing how transferability metrics perform across different transfer learning paradigms and identifying which metrics generalize best

## Limitations

- Current MTE methods lack comprehensive evaluation across diverse experimental settings and model architectures
- Some MTE methods require substantial computational resources that may negate benefits of avoiding individual model training
- Correlation-based evaluation framework may not guarantee optimal model selection in practical scenarios with class imbalance

## Confidence

**High Confidence (8/10):** The categorization of MTE methods into source-free and source-dependent realms is well-supported by the literature and represents a fundamental distinction in the field.

**Medium Confidence (6/10):** Claims about the effectiveness of specific static methods are supported by individual papers but lack comprehensive comparative studies across diverse scenarios.

**Low Confidence (4/10):** Claims about the robustness of dynamic methods and their superiority over static approaches are largely based on limited experimental evidence.

## Next Checks

1. **Cross-Domain Robustness Test:** Evaluate the top 5 MTE methods (TMI, H-score, LogME, and two distribution matching methods) across at least 10 different domain adaptation scenarios with varying domain gaps, using the same evaluation protocol to measure consistency of correlation coefficients.

2. **Foundation Model Transferability:** Test the applicability of traditional MTE methods on large-scale vision foundation models (e.g., CLIP, DINOv2) for fine-grained classification tasks, measuring both correlation performance and computational overhead compared to individual fine-tuning.

3. **Hyperparameter Sensitivity Analysis:** Systematically vary key hyperparameters (e.g., number of PCA components in LogME, kernel parameters in distribution matching) across 5 different datasets and model combinations to quantify the stability of transferability predictions and identify methods that are robust to hyperparameter changes.