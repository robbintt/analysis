---
ver: rpa2
title: Label Propagation for Zero-shot Classification with Vision-Language Models
arxiv_id: '2404.04072'
source_url: https://arxiv.org/abs/2404.04072
tags:
- zlap
- clip
- inmap
- classification
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vision-Language Models (VLMs) achieve impressive zero-shot classification
  using only class names. This paper addresses zero-shot classification with access
  to unlabeled data from the target distribution.
---

# Label Propagation for Zero-shot Classification with Vision-Language Models

## Quick Facts
- arXiv ID: 2404.04072
- Source URL: https://arxiv.org/abs/2404.04072
- Authors: Vladan Stojnić; Yannis Kalantidis; Giorgos Tolias
- Reference count: 40
- Primary result: ZLaP improves CLIP's average accuracy by +3.4% and +2.9% for transductive and inductive inference, respectively, on 14 datasets

## Executive Summary
This paper introduces ZLaP, a label propagation method for zero-shot classification using vision-language models (VLMs) when unlabeled data from the target distribution is available. The approach extends traditional label propagation by incorporating both text and image features from VLMs, using geodesic distances and per-modality neighbor search to handle the bimodality of the data. Extensive experiments demonstrate significant improvements over state-of-the-art methods, with ZLaP achieving superior performance on 14 common datasets.

## Method Summary
ZLaP extends label propagation to handle graphs containing both text and image features from VLMs. The method introduces per-modality neighbor search and contribution balancing to handle the bimodality of the data. It uses geodesic distances for classification and proposes an efficient inductive inference approach using a dual solution and sparsification. The approach is evaluated extensively on 14 common datasets, showing significant improvements over the latest related works.

## Key Results
- ZLaP improves CLIP's average accuracy by +3.4% for transductive inference across 14 datasets
- ZLaP improves CLIP's average accuracy by +2.9% for inductive inference across 14 datasets
- Incorporating InMaP's class proxies further boosts performance to achieve state-of-the-art results

## Why This Works (Mechanism)
Assumption: The effectiveness of ZLaP stems from its ability to leverage unlabeled data from the target distribution while handling the bimodality of VLMs through per-modality neighbor search and contribution balancing. The use of geodesic distances for classification likely captures more meaningful relationships between data points in the feature space compared to Euclidean distances.

## Foundational Learning

- **Label Propagation**: A semi-supervised learning method that propagates labels through a graph structure. Why needed: To leverage unlabeled data in the target distribution. Quick check: Verify that label propagation converges and produces reasonable initial classifications.

- **Geodesic Distance**: The shortest path between nodes in a graph. Why needed: To measure distances between features in the label propagation graph. Quick check: Confirm that geodesic distances capture meaningful relationships between data points.

- **Vision-Language Models**: Models that can process both visual and textual information. Why needed: To generate feature representations for both images and class names. Quick check: Validate that VLM features capture relevant semantic information.

## Architecture Onboarding

**Component Map**: Image Features ↔ Graph Construction ↔ Label Propagation ↔ Classification
Text Features → Neighbor Search → Contribution Balancing → Label Propagation

**Critical Path**: Image/Text feature extraction → Graph construction → Label propagation → Classification

**Design Tradeoffs**: 
- Uses geodesic distances for classification (crucial but limited ablation)
- Employs dual solution approach for efficiency (potential scalability concerns)
- Balances contributions from text and image modalities

**Failure Signatures**: 
- Poor performance if VLM features don't capture relevant semantics
- Scalability issues with very large datasets due to dual solution approach
- Suboptimal results if unlabeled data distribution differs significantly from test distribution

**First Experiments**:
1. Validate VLM feature quality on a small subset of data
2. Test label propagation convergence on a simple graph
3. Evaluate per-modality neighbor search effectiveness

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Limited ablation study on the impact of geodesic distance formulation
- Potential scalability concerns with the dual solution approach for very large datasets
- Generalizability to domains outside the 14 tested datasets
- Reliance on unlabeled data from the target distribution may introduce practical constraints

## Confidence
- High confidence in the technical validity of the label propagation framework and its core algorithmic components
- Medium confidence in the absolute performance numbers, given the comprehensive experimental setup but limited details on hyperparameter tuning
- Low confidence in the scalability claims for very large-scale applications, as the paper focuses on moderate-sized datasets

## Next Checks
1. Ablation study isolating the impact of the geodesic distance formulation versus alternative distance metrics
2. Scalability testing on datasets 10x larger than those used in the paper to verify computational efficiency claims
3. Cross-domain validation testing on datasets from substantially different domains than the 14 used in the current experiments