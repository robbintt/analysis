---
ver: rpa2
title: Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations
arxiv_id: '2402.12598'
source_url: https://arxiv.org/abs/2402.12598
tags:
- data
- locations
- graph
- ggnet
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of virtual sensing when sensor
  coverage is sparse, making physical proximity less useful for interpolation. The
  authors propose a graph-based deep learning method, GgNet, which leverages dependencies
  between target variables and correlated variables (covariates) at each location.
---

# Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations

## Quick Facts
- arXiv ID: 2402.12598
- Source URL: https://arxiv.org/abs/2402.12598
- Authors: Giovanni De Felice; Andrea Cini; Daniele Zambon; Vladimir V. Gusev; Cesare Alippi
- Reference count: 40
- Primary result: GgNet achieves higher reconstruction accuracy than state-of-the-art methods in sparse sensor settings by learning inter-location graph structures based on functional similarity rather than physical proximity.

## Executive Summary
This paper addresses virtual sensing in scenarios with sparse sensor coverage, where traditional physical proximity-based interpolation fails. The authors propose GgNet, a graph-based deep learning architecture that leverages both spatial dependencies between locations and variable correlations within each location. By learning a nested graph structure composed of inter-location and intra-location graphs, GgNet can effectively propagate information across physically distant but functionally similar locations. The method demonstrates superior performance compared to existing approaches on climatic and photovoltaic energy production datasets, particularly in sparse settings where other methods fail.

## Method Summary
GgNet uses a nested graph architecture to model dependencies in multivariate spatio-temporal data. The inter-location graph propagates information across different spatial locations based on learned functional similarity, while the intra-location graph models dependencies between different variables at the same location. The model is trained with masked channels and a weighted loss function that emphasizes reconstruction of masked portions, encouraging learning of the virtual sensing task rather than simple imputation. The approach leverages both target variables and correlated covariates at each location to improve reconstruction accuracy in sparse sensor settings.

## Key Results
- GgNet outperforms state-of-the-art methods (GRIN, SAITS, BRITS, KNN) on climatic and photovoltaic datasets
- Higher reconstruction accuracy particularly in sparse settings where other approaches fail
- The method successfully leverages functional similarity rather than physical proximity when sensors are sparse
- Ablation studies validate the importance of both inter-location and intra-location graph components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learned inter-location graph structure enables virtual sensing in sparse sensor coverage by modeling functional similarity rather than physical proximity.
- Mechanism: The model learns node embeddings for each location that capture latent functional relationships. These embeddings are transformed through MLPs to produce edge weights in the inter-location graph, allowing information propagation between physically distant but functionally similar locations.
- Core assumption: Functional similarity between locations can be learned from data and is more useful than physical proximity when sensors are sparse.
- Evidence anchors:
  - [abstract]: "we model T →Y dependencies in a data-driven fashion, learning a similarity score for each pair of locations"
  - [section 4.2]: "we learn the graph topology...from the data to account for dependencies among sensors that can be far apart in space"
  - [corpus]: Weak - neighbors discuss PDEs and physical field estimation but don't directly address learned graph structures for virtual sensing

### Mechanism 2
- Claim: Nested graph architecture with inter-location and intra-location graphs enables simultaneous modeling of spatial dependencies and variable correlations.
- Mechanism: The inter-location graph propagates information across different spatial locations, while the intra-location graph models dependencies between different variables at the same location. This dual propagation allows the model to leverage both spatial and variable correlations.
- Core assumption: Both spatial dependencies between locations and variable correlations at each location are important for accurate virtual sensing reconstruction.
- Evidence anchors:
  - [abstract]: "We introduce a novel graph-based methodology to exploit such relationships and design a graph deep learning architecture, named GgNet, implementing the framework"
  - [section 4.2]: "To take advantage of both T →Y and C →Y relations, we learn a nested graph structure composed of an inter-location graph and an intra-location graph"
  - [corpus]: Weak - neighbors discuss spatio-temporal patterns but don't specifically address nested graph architectures for multivariate virtual sensing

### Mechanism 3
- Claim: Training with masked channels and weighted loss encourages learning of virtual sensing task rather than simple imputation.
- Mechanism: During training, a fraction of available channels are masked out and the model is trained to reconstruct these masked channels with higher weight in the loss function. This forces the model to learn how to infer missing variables rather than just reproduce observed ones.
- Core assumption: Explicitly training on the virtual sensing task (reconstructing missing channels) is more effective than training on full reconstruction.
- Evidence anchors:
  - [section 4.4]: "we mask out a fraction of the available channels for each training batch and train the model to reconstruct the input data, giving a higher weight to the masked out observations"
  - [section 5]: "we mask out a fraction of the available training channels (entirely for all timestamps in a batch) and train the models to reconstruct such masked portions of data"
  - [corpus]: Missing - neighbors don't discuss this specific training strategy for virtual sensing

## Foundational Learning

- Concept: Graph neural networks and message passing
  - Why needed here: The entire architecture relies on propagating information through learned graph structures (both inter-location and intra-location graphs)
  - Quick check question: Can you explain how a graph convolution layer aggregates information from neighboring nodes?

- Concept: Multivariate time series analysis and imputation
  - Why needed here: The problem involves reconstructing missing channels in multivariate spatio-temporal data, requiring understanding of both temporal patterns and multivariate dependencies
  - Quick check question: What are the key challenges in multivariate time series imputation compared to univariate?

- Concept: Virtual sensing and kriging
  - Why needed here: The task is to infer values at unobserved locations, which is the core of virtual sensing, with the twist that traditional spatial proximity approaches don't work in sparse settings
  - Quick check question: How does virtual sensing differ from traditional spatial interpolation methods like kriging?

## Architecture Onboarding

- Component map: Data → Input encoder (with embeddings) → Temporal convolutions → G-convolutions (spatial) → g-convolutions (channel-wise) → Readout MLPs → Predictions
- Critical path: The critical path flows through the nested graph structure, with inter-location graph convolutions propagating spatial information followed by intra-location graph convolutions modeling variable dependencies.
- Design tradeoffs: The nested graph approach adds complexity but enables modeling both spatial and variable dependencies. The transductive nature limits scalability but improves performance on known locations. The masking training strategy improves virtual sensing performance but may slow convergence.
- Failure signatures: Poor performance on geographically isolated locations (where T→Y dependencies are weak), failure to capture variable correlations (where C→Y dependencies are weak), or sensitivity to hyperparameter choices in the masking strategy.
- First 3 experiments:
  1. Test on the daily climatic dataset with N=235 locations, comparing against GRIN and SAITS baselines
  2. Test on the photovoltaic dataset with varying numbers of locations (10, 50, 200) to assess scalability
  3. Perform ablation study removing components (temporal convolutions, G-convolutions, g-convolutions) to validate architectural design choices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GgNet perform on datasets with highly irregular temporal sampling patterns compared to regular intervals?
- Basis in paper: [inferred] The paper evaluates GgNet on daily and hourly climatic datasets with regular sampling. It mentions potential future work on "collections of asynchronous time series, typically found in healthcare applications."
- Why unresolved: The current experiments use regularly sampled data, and the authors do not test GgNet's performance on irregularly sampled time series.
- What evidence would resolve it: Experiments comparing GgNet's performance on regularly sampled vs. irregularly sampled datasets of the same underlying phenomenon.

### Open Question 2
- Question: What is the impact of varying the number of edges per node in the inter-location graph on GgNet's performance?
- Basis in paper: [explicit] The paper mentions using a thresholded Gaussian kernel to compute the adjacency matrix for GRIN, setting the threshold to achieve an average of 10 edges per node for large datasets.
- Why unresolved: The paper does not explore how different numbers of edges per node affect GgNet's performance or whether 10 edges per node is optimal.
- What evidence would resolve it: Systematic experiments varying the average number of edges per node and measuring the impact on reconstruction accuracy.

### Open Question 3
- Question: How does GgNet's performance scale with the number of channels (D) compared to the number of locations (N)?
- Basis in paper: [explicit] The paper states that GgNet's complexity is O(N²TD) + O(NTD²), reducing to O(N²TD) when D² ≪ N².
- Why unresolved: While the complexity analysis is provided, the paper does not empirically test how GgNet's performance scales with increasing D relative to N.
- What evidence would resolve it: Experiments systematically varying D and N while keeping other factors constant, and measuring both performance and computational requirements.

### Open Question 4
- Question: How robust is GgNet to noise in the covariate variables?
- Basis in paper: [inferred] The paper evaluates GgNet's robustness to varying fractions of masked training channels but does not specifically test robustness to noise in the covariates.
- Why unresolved: The paper does not explore how GgNet performs when the covariate variables contain measurement errors or other forms of noise.
- What evidence would resolve it: Experiments adding different levels and types of noise to the covariate variables and measuring the impact on reconstruction accuracy.

## Limitations

- The nested graph architecture may not generalize well to extremely sparse settings where functional relationships between locations are weak or non-existent.
- The transductive nature of the approach limits its applicability to new, unseen locations.
- The effectiveness of the masking training strategy may be dataset-dependent and could lead to overfitting in some scenarios.

## Confidence

- **Mechanism 1 (Learned graph structures)**: Medium confidence. The theoretical framework is sound, but the paper lacks ablation studies isolating the contribution of learned vs. fixed graph structures.
- **Mechanism 2 (Nested graph architecture)**: High confidence. The dual propagation approach is well-justified and supported by strong empirical results.
- **Mechanism 3 (Masked training strategy)**: Low confidence. While the strategy is explained, the paper doesn't provide sufficient analysis of its effectiveness compared to standard training approaches.

## Next Checks

1. **Ablation on graph learning**: Implement a variant of GgNet with fixed physical proximity graphs instead of learned structures, and compare performance across varying levels of sensor sparsity to quantify the value of learned graph topologies.

2. **Transferability analysis**: Evaluate GgNet's performance when applied to a new location not seen during training (inductive setting) to assess the practical limitations of its transductive design.

3. **Variable correlation sensitivity**: Systematically remove covariates with varying degrees of correlation to the target variables and measure performance degradation, establishing the minimum correlation threshold required for effective virtual sensing.