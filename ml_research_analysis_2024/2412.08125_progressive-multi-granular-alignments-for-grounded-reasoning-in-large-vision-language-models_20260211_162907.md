---
ver: rpa2
title: Progressive Multi-granular Alignments for Grounded Reasoning in Large Vision-Language
  Models
arxiv_id: '2412.08125'
source_url: https://arxiv.org/abs/2412.08125
tags:
- promvil
- visual
- compositional
- dataset
- lvlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new framework for compositional visual reasoning
  in large vision-language models. Existing methods either process whole images/texts
  without object-level details, or align simple phrases with single objects without
  capturing complex relationships.
---

# Progressive Multi-granular Alignments for Grounded Reasoning in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2412.08125
- Source URL: https://arxiv.org/abs/2412.08125
- Authors: Quang-Hung Le; Long Hoang Dang; Ngan Le; Truyen Tran; Thao Minh Le
- Reference count: 6
- Key outcome: 9.0 point improvement on compositional visual reasoning benchmark

## Executive Summary
This paper addresses the challenge of compositional visual reasoning in large vision-language models by introducing a hierarchical alignment framework called PromViL. Existing approaches either lack object-level detail or fail to capture complex relationships between objects. The authors propose a progressive multi-granular alignment method that constructs hierarchical associations from simple to complex concepts, progressively aligning textual descriptions with corresponding visual regions. This approach significantly improves performance on compositional reasoning tasks while using parameter-efficient fine-tuning.

## Method Summary
The method introduces Progressive Multi-granular Vision-Language alignments (PromViL) that constructs hierarchical associations between textual descriptions and visual regions. The framework creates a new CompoVL dataset from Visual Genome annotations, containing nested compositional vision-language pairs with varying complexity levels. The approach uses LoRA fine-tuning on Kosmos-2 models, progressively training from simple expressions to complex compositional phrases. During inference, the model follows a hierarchical reasoning process that mirrors the training structure, enabling it to handle both simple grounding and complex compositional reasoning tasks effectively.

## Key Results
- 9.0 points increase on the proposed CompoVL benchmark
- Up to 5.5 points improvement on zero-shot grounding tasks
- Nearly 5 points increase in accuracy and 10 points in validity on compositional reasoning tasks
- Outperforms larger models (CoVLM 2.4B, Pink 7B) on grounding tasks while fine-tuning only 4.9% of parameters

## Why This Works (Mechanism)
The hierarchical approach works by progressively building compositional understanding from simple to complex structures. By training on nested subsequences that mirror the compositional structure of visual scenes, the model learns to reason about relationships between objects in a structured way. The progressive alignment ensures that the model first establishes basic object-level understanding before tackling more complex compositional expressions, preventing the loss of detailed information that occurs in global approaches.

## Foundational Learning
- **Compositional Reasoning**: Understanding how objects relate to each other in visual scenes - needed to solve complex visual queries involving multiple objects and their relationships
- **Multi-granular Alignment**: Aligning text at different levels of abstraction (words, phrases, sentences) with corresponding visual regions - needed to capture both simple and complex relationships
- **Hierarchical Training**: Progressive training from simple to complex expressions - needed to build compositional understanding incrementally
- **Nested Subsequence Extraction**: Identifying hierarchical textual structures that correspond to visual compositions - needed to create training data that mirrors compositional complexity
- **LoRA Fine-tuning**: Parameter-efficient adaptation of large models - needed to achieve good performance without full fine-tuning
- **Visual Genome Annotations**: Structured object and relationship annotations in images - needed to create ground truth for compositional alignments

## Architecture Onboarding

Component Map:
Visual Genome Dataset -> Data Generation Pipeline -> CompoVL Dataset -> LoRA Fine-tuning -> Kosmos-2 Model -> Progressive Inference Engine -> Performance Evaluation

Critical Path:
Data Generation -> Hierarchical Fine-tuning -> Progressive Inference -> Evaluation

Design Tradeoffs:
- Parameter efficiency (LoRA) vs. full fine-tuning capacity
- Training complexity (hierarchical) vs. simplicity (flat training)
- Dataset size (60K samples) vs. coverage of compositional diversity
- Annotation quality (Visual Genome) vs. potential noise in automatically generated data

Failure Signatures:
- Poor performance on complex compositional expressions indicates inadequate nested subsequence extraction
- Overfitting on CompoVL dataset manifests as degraded zero-shot task performance
- Inconsistent grounding results suggest issues with visual region alignment during training

First Experiments:
1. Evaluate baseline Kosmos-2 performance on simple grounding tasks before fine-tuning
2. Test model performance on nested compositional expressions with varying complexity levels
3. Compare zero-shot generalization on RefCOCOg before and after LoRA fine-tuning

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Performance heavily depends on Visual Genome annotation quality, which may contain noise
- 60K training samples may not capture full diversity of real-world compositional reasoning
- LoRA fine-tuning may have limited capacity compared to full fine-tuning for complex patterns
- Automatic text parsing and entity extraction can introduce errors in compositional structure identification

## Confidence

**High Confidence:**
- Reported performance improvements on CompoVL benchmark
- Zero-shot grounding task improvements

**Medium Confidence:**
- Scalability claims when comparing with larger models
- Generalization to out-of-distribution data on GQA-OOD

## Next Checks
1. Validate hierarchical data generation pipeline by manually inspecting 100 randomly sampled compositional pairs
2. Test model performance degradation when trained on progressively smaller subsets of the 60K samples
3. Evaluate zero-shot performance on additional grounding benchmarks beyond RefCOCOg