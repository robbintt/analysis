---
ver: rpa2
title: Improving robustness to corruptions with multiplicative weight perturbations
arxiv_id: '2406.16540'
source_url: https://arxiv.org/abs/2406.16540
tags:
- damp
- training
- data
- corruption
- corruptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving deep neural network
  robustness to input corruptions without harming clean-image accuracy. The core method,
  DAMP, introduces multiplicative weight perturbations during training to simulate
  input corruptions in the weight space, thereby enhancing generalization to various
  distortions.
---

# Improving robustness to corruptions with multiplicative weight perturbations

## Quick Facts
- **arXiv ID**: 2406.16540
- **Source URL**: https://arxiv.org/abs/2406.16540
- **Reference count**: 40
- **Primary result**: DAMP improves robustness to input corruptions without harming clean accuracy across multiple datasets

## Executive Summary
This paper introduces DAMP (Dropout via Additive and Multiplicative Perturbations), a training method that enhances deep neural network robustness to input corruptions by simulating perturbations in the weight space. DAMP applies multiplicative weight perturbations during training, which helps models generalize better to various input distortions. The method establishes a theoretical connection to Adaptive Sharpness-Aware Minimization (ASAM) and demonstrates consistent performance improvements over standard techniques like Dropout and SAM across CIFAR-10/100, TinyImageNet, and ImageNet datasets. Notably, DAMP enables training Vision Transformers from scratch on ImageNet with performance comparable to ResNet50 without requiring extensive data augmentations.

## Method Summary
DAMP introduces multiplicative weight perturbations during training to simulate input corruptions in the weight space. The method applies random multiplicative noise to model weights, creating a regularization effect that enhances robustness to various input distortions. This approach differs from traditional dropout by operating directly on weights rather than activations. The paper establishes a theoretical connection between DAMP's multiplicative perturbations and Adaptive Sharpness-Aware Minimization (ASAM), suggesting they share optimization characteristics. During training, DAMP samples weight perturbations and optimizes the model to maintain performance under these perturbations, effectively simulating corruption robustness in the parameter space.

## Key Results
- DAMP consistently outperforms standard methods like Dropout and SAM across CIFAR-10/100, TinyImageNet, and ImageNet datasets
- Improves corruption robustness without sacrificing clean accuracy
- Enables training a ViT-S/16 on ImageNet from scratch with top-1 error of 23.7% without extensive data augmentations

## Why This Works (Mechanism)
DAMP works by introducing multiplicative weight perturbations during training, which simulates the effect of input corruptions in the weight space. This regularization forces the model to learn representations that are robust to variations in the input, effectively creating a form of implicit data augmentation. The multiplicative nature of the perturbations is particularly effective because it can model a wide range of corruption types, from noise to structural changes. By optimizing under these perturbed conditions, the model develops flatter minima in the loss landscape, which correlates with better generalization to unseen corruptions. The connection to ASAM suggests that DAMP also encourages smoother optimization trajectories, further contributing to robustness.

## Foundational Learning
- **Multiplicative perturbations**: Random scaling of weights during training to simulate input variations; needed for robustness generalization; quick check: observe weight variance during training
- **Sharpness-Aware Minimization (SAM)**: Optimization technique that encourages flat minima; needed for understanding DAMP's theoretical connection; quick check: compare loss landscape flatness with/without DAMP
- **Adversarial training**: Training on perturbed inputs to improve robustness; needed as conceptual foundation; quick check: measure robustness to adversarial examples
- **Weight-space regularization**: Regularizing the parameter space rather than input space; needed for DAMP's unique approach; quick check: analyze weight distribution statistics
- **Implicit data augmentation**: Regularization techniques that simulate additional training data; needed to understand DAMP's efficiency; quick check: compare performance with explicit augmentation

## Architecture Onboarding

**Component Map**: Input -> DAMP Perturbations -> Model Weights -> Loss Function -> Optimization

**Critical Path**: Weight initialization → DAMP perturbation sampling → Forward pass under perturbation → Loss computation → Backward pass → Weight update

**Design Tradeoffs**: 
- Benefits: Improved robustness without explicit corruption augmentation, enables ViT training from scratch
- Drawbacks: Additional computational overhead during training, potential instability in optimization
- Alternatives: Traditional dropout, explicit corruption augmentation, ASAM

**Failure Signatures**: 
- Degraded clean accuracy despite improved corruption robustness
- Training instability or slow convergence
- Minimal improvement over baseline methods on corruption benchmarks

**First Experiments**:
1. Compare clean accuracy vs corruption robustness trade-off curve with and without DAMP
2. Measure training stability and convergence speed compared to standard training
3. Analyze the effect of perturbation magnitude on both clean and corrupted performance

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Scalability to larger architectures and real-world scenarios remains untested
- Performance on naturally occurring perturbations beyond synthetic corruptions is unknown
- Theoretical connection to ASAM lacks extensive validation and practical implications are not fully explored

## Confidence
- **CIFAR-10/100 and TinyImageNet results**: High
- **ImageNet experiments**: Medium (fewer ablation studies)
- **Theoretical contributions**: Low (lack extensive validation)

## Next Checks
1. Test DAMP on naturally corrupted datasets (e.g., real-world noise, compression artifacts) beyond synthetic distortions
2. Compare DAMP's optimization dynamics with standard ASAM through gradient norm analysis and convergence curves
3. Evaluate DAMP on larger architectures (e.g., Swin Transformer, ConvNeXt) and more diverse datasets (e.g., JFT, COCO) to assess scalability limits