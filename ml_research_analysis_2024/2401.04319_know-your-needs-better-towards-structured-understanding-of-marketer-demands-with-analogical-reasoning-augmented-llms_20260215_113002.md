---
ver: rpa2
title: 'Know Your Needs Better: Towards Structured Understanding of Marketer Demands
  with Analogical Reasoning Augmented LLMs'
arxiv_id: '2401.04319'
source_url: https://arxiv.org/abs/2401.04319
tags:
- reasoning
- llms
- analogical
- language
- belongs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel user targeting approach that leverages
  large language models (LLMs) to understand marketer demands in natural language
  and transform them into structured logical expressions. The proposed ARALLM framework
  combines analogical reasoning based prompting with reasoning-augmented multi-task
  model distillation to achieve high-quality structured understanding.
---

# Keep Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs

## Quick Facts
- arXiv ID: 2401.04319
- Source URL: https://arxiv.org/abs/2401.04319
- Reference count: 35
- Primary result: ARALLM framework achieves 7% improvement in structure accuracy and 62.2 S-BLEU score, surpassing ChatGPT

## Executive Summary
This paper introduces a novel approach for understanding marketer demands by transforming natural language requests into structured logical expressions using large language models enhanced with analogical reasoning. The proposed ARALLM framework combines analogical reasoning based prompting with reasoning-augmented multi-task model distillation to achieve high-quality structured understanding. The system has been successfully deployed online, significantly reducing operation time while improving user satisfaction compared to previous methods.

## Method Summary
The method involves constructing a reasoning library from 50 real-world demands with expert verification, then using analogical reasoning retrieval to find similar examples when processing new demands. The system employs knowledge distillation from ChatGPT to smaller, more deployable models (Baichuan2-13B-Chat or ChatGLM2-6B-32K) using a multi-task training strategy that incorporates both reasoning steps and final answers. The analogical reasoning prompting method retrieves contextually similar reasoning examples from the library to guide the LLM toward better structured output generation.

## Key Results
- Analogical reasoning prompting outperforms baseline methods with 7% improvement in structure accuracy
- Distilled model (Baichuan2-13B-Chat) achieves 62.2 S-BLEU score, surpassing teacher model ChatGPT (61.2)
- Online deployment reduces operation time by 4-10 times and increases user satisfaction by 1.3 times

## Why This Works (Mechanism)

### Mechanism 1
Analogical reasoning prompting retrieves contextually similar reasoning examples from a reasoning library, outperforming fixed few-shot examples. When a new demand is input, the system finds similar past demands and their reasoning steps to guide the LLM. The core assumption is that similar marketing demands follow similar reasoning steps. Evidence shows 7% improvement in structure accuracy, though performance may degrade if demands are too dissimilar or the library lacks relevant examples.

### Mechanism 2
Reasoning-augmented multi-task model distillation improves performance by training on both reasoning steps and final answers. The model learns reasoning patterns through a multi-task loss function. The distilled Baichuan2-13B-Chat model achieves 62.2 S-BLEU score, surpassing ChatGPT. Performance could degrade if reasoning steps are noisy or incorrect.

### Mechanism 3
Knowledge distillation from large models to smaller models enables practical deployment while maintaining performance. A large teacher model generates training data used to fine-tune smaller models. The Baichuan2-13B-Chat model surpasses ChatGPT in S-BLEU score, though distillation could introduce noise or fail if the smaller model lacks capacity.

## Foundational Learning

- **Chain-of-Thought (CoT) reasoning**: Helps LLMs break down complex reasoning tasks into manageable steps for transforming natural language demands into structured logical expressions. Quick check: How does CoT differ from standard prompting in handling complex reasoning tasks?

- **Analogical reasoning**: Allows leveraging similar past examples to guide reasoning on new demands, improving performance on diverse marketer requests. Quick check: What is the core assumption behind using analogical reasoning for marketing demand translation?

- **Knowledge distillation**: Enables transfer of capabilities from large, resource-intensive models to smaller, deployable models while maintaining performance. Quick check: Why is knowledge distillation particularly important for industrial deployment of LLMs?

## Architecture Onboarding

- **Component map**: Marketer input -> Retriever (finds similar examples) -> Analogical Reasoning Prompt Generator -> Fine-tuned LLM -> Parser (converts SELL to visual card) -> Marketer output

- **Critical path**: 1) Marketer inputs natural language demand, 2) Retriever finds similar examples from reasoning library, 3) Analogical reasoning prompt generator constructs prompt, 4) Fine-tuned LLM generates SELL expression, 5) Parser converts SELL to visual card, 6) Marketer reviews and exports target users

- **Design tradeoffs**: Larger reasoning library improves retrieval quality but increases storage and retrieval time; more detailed reasoning steps improve performance but increase annotation costs; larger student models perform better but are more expensive to deploy

- **Failure signatures**: Poor retrieval results (low-quality or irrelevant examples), model generation errors (syntax errors or illogical structures), parser failures (visual cards don't accurately represent SELL expression), deployment issues (latency or crashes)

- **First 3 experiments**: 1) Compare analogical reasoning prompting vs. fixed few-shot examples on small test set, 2) Evaluate impact of including reasoning steps in training vs. only using final answers, 3) Test different student model sizes to find optimal balance between performance and deployability

## Open Questions the Paper Calls Out

The paper identifies several open questions: How does analogical reasoning perform on other tasks beyond NL2SELL? What is the exact cost of reasoning library construction? How sensitive is the approach to reasoning library quality? Can answer-to-demand distillation automate reasoning library construction? How does the approach scale to larger reasoning libraries?

## Limitations

- Limited reasoning library diversity (50 examples) may not cover full range of marketer demands
- Performance metrics based on internal deployment without independent verification
- Single comparison of distillation results may be sensitive to dataset composition or evaluation methodology

## Confidence

- Analogical reasoning improves prompt quality: Medium confidence (7% improvement in structure accuracy, limited by small sample size)
- Multi-task training with reasoning augmentation improves performance: Medium confidence (Baichuan2-13B-Chat shows better performance, but attribution difficult due to different architectures)
- Online system effectiveness: Low confidence (internal deployment data without peer review)

## Next Checks

1. Conduct ablation study on reasoning library size by systematically varying library size (25, 50, 100, 200 examples) to determine optimal size for generalization

2. Replicate knowledge distillation results using different teacher model (e.g., Claude or LLaMA) to verify generalizability across model combinations

3. Implement long-term deployment monitoring over 3-6 months to track performance metrics and identify degradation patterns or emerging failure modes