---
ver: rpa2
title: 'ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems'
arxiv_id: '2403.12660'
source_url: https://arxiv.org/abs/2403.12660
tags:
- feature
- selection
- methods
- features
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ERASE benchmarks feature selection methods for deep recommender
  systems (DRS), addressing issues of unfair comparisons, limited dataset scope, and
  lack of robustness assessment. It evaluates eleven methods across four public datasets,
  a private industrial dataset, and an online platform.
---

# ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems

## Quick Facts
- arXiv ID: 2403.12660
- Source URL: https://arxiv.org/abs/2403.12660
- Authors: Pengyue Jia; Yejing Wang; Zhaocheng Du; Xiangyu Zhao; Yichao Wang; Bo Chen; Wanyu Wang; Huifeng Guo; Ruiming Tang
- Reference count: 40
- Primary result: Benchmark of eleven feature selection methods for deep recommender systems across four public datasets, one industrial dataset, and online deployment

## Executive Summary
ERASE addresses critical gaps in evaluating feature selection methods for deep recommender systems by providing a comprehensive benchmark that tackles unfair comparisons, limited dataset scope, and lack of robustness assessment. The study evaluates eleven different feature selection approaches across multiple real-world scenarios, including public datasets, industrial applications, and online deployment. Key findings demonstrate that gate-based methods generally outperform alternatives, with AutoField showing exceptional robustness by maintaining 99.91% AUC while using only 10% of memory on the Criteo dataset.

The research introduces a systematic framework for comparing feature selection methods under consistent conditions, revealing that traditional approaches like SHAP, while interpretable, often underperform in practical deployment scenarios. The online deployment results showing 20% latency reduction without effectiveness loss validate the practical significance of the benchmark, while the industrial dataset evaluation provides evidence that extends beyond academic datasets to real-world e-commerce environments.

## Method Summary
The study establishes a standardized evaluation framework for feature selection methods in deep recommender systems, testing eleven approaches across four public datasets (Criteo, Avazu, MovieLens, and another undisclosed dataset), one private industrial dataset, and an online A/B testing platform. The benchmark evaluates methods based on multiple criteria including effectiveness (measured through AUC), efficiency (memory usage and latency), and robustness (performance across varying feature counts). Gate-based methods, AutoField, SHARK, and SFS are identified as top performers, with particular emphasis on their ability to maintain performance while reducing computational overhead. The evaluation protocol includes consistent preprocessing, hyperparameter tuning, and metric calculation to ensure fair comparisons across different methodological approaches.

## Key Results
- Gate-based methods generally outperform other feature selection approaches across multiple datasets
- AutoField achieves 99.91% AUC with only 10% memory usage on Criteo dataset
- Online deployment reduces latency by 20% without effectiveness loss
- SHARK and SFS demonstrate best robustness across varying feature counts

## Why This Works (Mechanism)
The effectiveness of gate-based feature selection methods stems from their ability to learn adaptive importance weights during model training, allowing the network to dynamically determine which features contribute most to prediction accuracy. These methods integrate feature selection directly into the model architecture, enabling end-to-end optimization where the selection process is guided by the same loss function used for final predictions. AutoField's success can be attributed to its automated field-aware mechanism that learns feature interactions while simultaneously identifying relevant features, creating a more efficient representation without sacrificing predictive power. The robustness of SHARK and SFS across varying feature counts indicates their ability to maintain stable selection criteria even when the feature space changes, likely due to their statistical foundations that are less sensitive to specific dataset characteristics.

## Foundational Learning

**Feature Importance in Deep Learning**: Understanding how neural networks assign importance to different input features is crucial for effective feature selection. Quick check: Verify that feature importance scores correlate with actual contribution to model predictions through ablation studies.

**Tabular Data Representation**: Deep recommender systems often rely on tabular features that require careful encoding and normalization. Quick check: Ensure feature scaling and encoding methods are consistent across all tested methods to avoid bias in comparisons.

**Model Compression Techniques**: Feature selection is fundamentally a model compression problem where the goal is to reduce dimensionality while maintaining performance. Quick check: Compare the compression ratios achieved by different methods against theoretical limits.

## Architecture Onboarding

**Component Map**: Input Features -> Feature Selection Method -> Deep Recommender Model -> Output Predictions

**Critical Path**: Raw feature vectors → Feature selection gating mechanism → Compressed feature representation → Deep neural network layers → Prediction layer

**Design Tradeoffs**: The study reveals a fundamental tradeoff between interpretability and performance, where methods like SHAP provide clear explanations but underperform compared to gate-based approaches that sacrifice interpretability for superior accuracy and efficiency.

**Failure Signatures**: Methods that rely heavily on statistical independence assumptions may fail when features exhibit complex interactions. Gate-based methods may overfit when training data is limited, while wrapper methods can become computationally prohibitive with large feature sets.

**First 3 Experiments**:
1. Replicate the AUC comparison across all eleven methods on the Criteo dataset to verify the 99.91% benchmark
2. Test the 20% latency reduction claim through controlled A/B testing with identical user cohorts
3. Evaluate feature selection stability by randomly removing 20% of features and measuring performance degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses primarily on tabular features, potentially limiting applicability to sequential or graph-based features
- Experimental scope may not capture all real-world deployment scenarios, especially those with extreme class imbalance
- Results from single industrial dataset may not generalize to all e-commerce or recommendation contexts

## Confidence
- Gate-based methods outperforming others: High confidence based on consistent cross-dataset performance
- AutoField's 99.91% AUC with 10% memory usage: Medium confidence due to potential deployment environment variations
- 20% latency reduction claim: Medium confidence requiring longer-term seasonal validation

## Next Checks
1. Test gate-based methods on graph-based feature selection tasks to verify cross-domain applicability
2. Conduct longer-term A/B testing across multiple seasons to validate latency reduction claims
3. Evaluate feature selection methods on datasets with extreme class imbalance to assess robustness limits