---
ver: rpa2
title: 'MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and
  Dynamic Distance Constraint'
arxiv_id: '2402.14244'
source_url: https://arxiv.org/abs/2402.14244
tags:
- uni00000013
- uni00000011
- learning
- reward
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of sparse rewards in hierarchical
  reinforcement learning (HRL) by proposing MENTOR, a framework that integrates human
  feedback and dynamic distance constraints. The core idea is to use human feedback
  to guide high-level policy learning for better subgoal generation, while employing
  a Dynamic Distance Constraint (DDC) mechanism to adjust subgoal difficulty according
  to the low-level policy's capabilities.
---

# MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint

## Quick Facts
- arXiv ID: 2402.14244
- Source URL: https://arxiv.org/abs/2402.14244
- Authors: Xinglin Zhou; Yifu Yuan; Shaofu Yang; Jianye Hao
- Reference count: 40
- Key outcome: MENTOR achieved higher success rates and faster convergence in sparse reward environments using human feedback and dynamic distance constraints

## Executive Summary
MENTOR addresses the challenge of sparse rewards in hierarchical reinforcement learning by integrating human feedback with dynamic distance constraints. The framework uses human preferences to guide high-level policy subgoal generation while employing a Dynamic Distance Constraint mechanism to adjust subgoal difficulty based on the low-level policy's capabilities. Additionally, an Exploration-Exploitation Decoupling approach stabilizes low-level policy training through separate exploration and exploitation policies.

## Method Summary
MENTOR combines hierarchical reinforcement learning with human feedback and dynamic distance constraints. The high-level policy generates subgoals guided by a reward model trained on human pairwise comparisons, while the Dynamic Distance Constraint mechanism adjusts subgoal difficulty based on the low-level policy's current capabilities. The low-level policy uses an Exploration-Exploitation Decoupling approach with separate policies for exploration and exploitation, stabilized by a noise injection mechanism and hindsight relabeling.

## Key Results
- Achieved higher success rates across multiple domains including FetchPush, FetchPickAndPlace, FetchDraw, FetchObsPush, Pusher, and Four rooms
- Demonstrated faster convergence compared to baselines including HER and HhP
- Showed effectiveness using only small amounts of human feedback while addressing sparse rewards and non-stationarity in HRL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF guides high-level policy to generate subgoals that are preferred by humans and aligned with task objectives
- Mechanism: The reward model learns from pairwise human comparisons of subgoals, capturing human preferences which then directly guide the high-level policy in selecting effective subgoals
- Core assumption: Human feedback can effectively capture task-relevant preferences and the reward model can generalize from these preferences to unseen states
- Break condition: If human feedback is noisy or inconsistent, or if the reward model fails to generalize, the high-level policy may generate suboptimal subgoals

### Mechanism 2
- Claim: Dynamic Distance Constraint (DDC) ensures that subgoals are appropriately challenging for the current low-level policy capabilities
- Mechanism: DDC dynamically adjusts the range of acceptable subgoals based on the distance between the proposed subgoal and the current achieved state, ensuring subgoals are neither too easy nor too difficult
- Core assumption: The distance model accurately reflects the difficulty of achieving subgoals and the balancing coefficient can effectively trade-off between human guidance and distance constraints
- Break condition: If the distance model is inaccurate or the balancing coefficient is poorly tuned, DDC may fail to appropriately constrain subgoal difficulty

### Mechanism 3
- Claim: Exploration-Exploitation Decoupling (EED) stabilizes low-level policy training by separating exploration and exploitation
- Mechanism: EED uses two policies: an exploration policy that focuses on discovering new states and an exploitation policy that learns from the exploration policy's experiences to achieve subgoals efficiently
- Core assumption: The exploration policy can effectively discover new states and the exploitation policy can learn from these experiences without being overwhelmed by exploration noise
- Break condition: If the exploration policy fails to discover useful states or the exploitation policy cannot learn effectively from the exploration experiences, EED may not improve training stability

## Foundational Learning

- Concept: Hierarchical Reinforcement Learning (HRL)
  - Why needed here: HRL provides the foundational framework for decomposing complex tasks into manageable subgoals, essential for addressing sparse rewards and long-horizon tasks
  - Quick check question: What are the key components of an HRL framework, and how do they interact to solve complex tasks?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF provides a way to incorporate human preferences into the learning process, guiding the high-level policy to generate more effective subgoals
  - Quick check question: How does RLHF differ from traditional RL, and what are the advantages and disadvantages of using human feedback in reinforcement learning?

- Concept: Hindsight Relabeling
  - Why needed here: Hindsight relabeling is a technique for augmenting training data by reinterpreting failed transitions as successes towards different goals, crucial for learning from sparse rewards
  - Quick check question: How does hindsight relabeling work, and what are the different strategies for selecting new goals during relabeling?

## Architecture Onboarding

- Component map:
  - State → High-level policy → Subgoal → Low-level policy → Action → Environment → Next state → Subgoal achieved?
  - High-level policy (πhi): Generates subgoals based on current state and environment goal, guided by RLHF and DDC
  - Low-level policy (πlo): Executes actions to achieve subgoals, trained using EED
  - Reward model (rhf): Learns from human feedback to evaluate subgoals
  - Distance model (d): Estimates the difficulty of achieving subgoals
  - RND model (frnd): Encourages exploration by rewarding novel states
  - Exploration policy (πe_lo): Explores the environment to discover new states
  - Exploitation policy (πlo): Learns from exploration experiences to achieve subgoals efficiently

- Critical path: State → High-level policy → Subgoal → Low-level policy → Action → Environment → Next state → Subgoal achieved?

- Design tradeoffs:
  - RLHF vs. automatic subgoal generation: RLHF provides human guidance but requires human feedback, while automatic methods are more scalable but may be less effective
  - DDC vs. fixed subgoal difficulty: DDC adapts to the low-level policy's capabilities but requires a distance model and balancing coefficient, while fixed difficulty is simpler but may be suboptimal
  - EED vs. single policy: EED stabilizes training but requires two policies, while a single policy is simpler but may be less stable

- Failure signatures:
  - High-level policy generates irrelevant or ineffective subgoals: RLHF or DDC may be malfunctioning
  - Low-level policy fails to achieve subgoals or training is unstable: EED may be malfunctioning
  - Overall performance is poor: One or more components may be poorly tuned or implemented incorrectly

- First 3 experiments:
  1. Evaluate the performance of MENTOR on a simple task with sparse rewards, comparing it to HER and HhP
  2. Analyze the impact of DDC on subgoal difficulty and low-level policy learning
  3. Assess the effectiveness of EED in stabilizing low-level policy training

## Open Questions the Paper Calls Out
None

## Limitations
- The quality and consistency of human feedback are not thoroughly validated, which could impact the effectiveness of the RLHF component
- The distance model's accuracy in estimating subgoal difficulty is critical for DDC but lacks detailed performance analysis
- The EED approach introduces additional complexity with dual policies, but computational overhead and practical implementation challenges are not discussed

## Confidence

**High confidence**: The core architecture combining RLHF with HRL is well-grounded and the experimental results show clear improvements over baselines.

**Medium confidence**: The DDC mechanism's effectiveness is demonstrated, but relies on assumptions about the distance model's accuracy that are not fully validated.

**Medium confidence**: The EED approach shows promise for stabilizing training, but the benefits versus added complexity tradeoff is not fully explored.

## Next Checks

1. Conduct ablation studies to isolate the contributions of RLHF, DDC, and EED to overall performance, including analysis of sample efficiency and computational overhead.

2. Test the framework's robustness to noisy or inconsistent human feedback through controlled experiments with varying feedback quality.

3. Evaluate the distance model's accuracy across different task types and analyze its impact on DDC performance when the model is imperfect.