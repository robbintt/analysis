---
ver: rpa2
title: Training Green AI Models Using Elite Samples
arxiv_id: '2402.12010'
source_url: https://arxiv.org/abs/2402.12010
tags:
- training
- energy
- samples
- data
- elite
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework that uses an evolutionary algorithm
  to select elite training samples from datasets, aiming to reduce AI model training
  energy consumption while maintaining or improving model performance. The method
  selects 10% of the most informative training samples, achieving up to 98% energy
  savings compared to standard 70% training splits.
---

# Training Green AI Models Using Elite Samples

## Quick Facts
- arXiv ID: 2402.12010
- Source URL: https://arxiv.org/abs/2402.12010
- Reference count: 40
- Key outcome: Up to 98% energy savings while improving accuracy by up to 50% using 10% elite training samples

## Executive Summary
This paper introduces a framework that uses an evolutionary algorithm (Differential Evolution) to select elite training samples from datasets, aiming to reduce AI model training energy consumption while maintaining or improving model performance. The method achieves remarkable energy savings of up to 98% compared to standard 70% training splits by using only 10% of the most informative training samples. Using these elite samples, model accuracy improved by up to 50% on average across 8 classification models and 25 datasets. The approach ensures generalizability, as models trained on elite samples performed well on larger test sets, offering a scalable path to greener AI through data-centric optimization.

## Method Summary
The method employs a Differential Evolution algorithm to optimize the selection of elite training samples from the original dataset. The framework iteratively evolves a population of candidate training subsets, evaluating each based on classification accuracy to identify the fittest (most informative) subsets. These elite samples, representing only 10% of the original dataset, are then used to train machine learning models. The approach includes energy consumption measurement for both the sampling framework and the training process using the codecarbon library, comparing results against traditional 70% training splits across 25 datasets and 8 classification models.

## Key Results
- Achieved up to 98% energy savings compared to standard 70% training splits
- Model accuracy improved by up to 50% on average across 8 classification models and 25 datasets
- Elite samples maintained or improved model generalizability when tested on larger test sets

## Why This Works (Mechanism)

### Mechanism 1
Selecting elite training samples using an evolutionary algorithm reduces both training data size and energy consumption while maintaining or improving model performance. The Differential Evolution algorithm iteratively optimizes candidate training subsets, retaining the fittest subsets (elite samples) and reducing the dataset to 10% of its original size. Core assumption: A small subset of training data can capture the essential information needed for accurate model learning.

### Mechanism 2
The computational overhead of finding elite samples is offset by the energy savings during model training. The energy consumed by the sampling framework (DE algorithm) is included in the total energy calculation and compared to the energy saved by training on 10% of the data instead of 70%. Core assumption: The energy saved during training outweighs the energy used by the sampling process.

### Mechanism 3
Elite training samples improve model generalizability by exposing the model to a broader range of test samples. After training on the elite samples, the model is tested on the remaining 90% of the data, ensuring it has learned representative patterns and not overfit to a small training set. Core assumption: The elite samples are representative of the entire dataset, allowing the model to generalize well to unseen data.

## Foundational Learning

- **Evolutionary Algorithms (EAs)**: Used to optimize the selection of elite training samples as this is an NP-hard problem. Quick check: What is the primary advantage of using EAs over traditional instance selection methods in this context?

- **Instance Selection (IS)**: The core technique used to reduce the training dataset size while maintaining model performance. Quick check: How does IS differ from feature selection in the context of data reduction?

- **Green AI**: The paper's goal is to reduce the environmental impact of AI model training through energy-efficient practices. Quick check: What are the key principles of Green AI, and how does this paper contribute to them?

## Architecture Onboarding

- **Component map**: Differential Evolution algorithm for elite sample selection → Machine learning model training pipeline → Energy consumption measurement module
- **Critical path**: The DE algorithm selects elite samples, which are then used to train the ML model. The energy consumption of both the DE algorithm and the training process is measured and compared to the baseline.
- **Design tradeoffs**: The choice of the DE algorithm and its parameters (population size, mutation factor, crossover probability) affects the quality of the elite samples and the computational overhead. The sample size (10% in this case) is a tradeoff between energy savings and model performance.
- **Failure signatures**: If the model's performance significantly degrades on the test set, it indicates that the elite samples are not representative. If the energy savings are minimal, it suggests that the DE algorithm's overhead is too high or the sample size is not optimized.
- **First 3 experiments**:
  1. Run the DE algorithm with default parameters on a small dataset and evaluate the quality of the elite samples based on model performance.
  2. Compare the energy consumption of training on the elite samples versus the full dataset for a single ML model.
  3. Test the generalizability of the model trained on elite samples by evaluating its performance on a larger test set.

## Open Questions the Paper Calls Out

### Open Question 1
How does the energy consumption of the differential evolution algorithm itself compare to the energy savings achieved by using elite training samples? The paper discusses the energy consumption of the sampling framework and the training process but does not provide a direct comparison of the energy consumed by the DE algorithm to the overall energy savings.

### Open Question 2
How do different evolutionary algorithms (EAs) compare in terms of their effectiveness in finding elite training samples and their energy efficiency? The paper uses the DE algorithm as an example and mentions that other EAs exist, but does not compare their performance or energy efficiency.

### Open Question 3
How does the size of the elite training sample set (q) affect the performance and energy efficiency of the models? The paper uses a fixed q value of 10% but mentions that this is a parameter that needs further tuning.

## Limitations

- The paper does not specify critical DE algorithm parameters (mutation factor F, crossover probability, population size), making exact reproduction challenging.
- While codecarbon is mentioned for energy measurement, exact implementation details for ensuring consistent measurements are not provided.
- The claim of improved generalizability is based on testing elite-trained models on larger test sets, but the methodology for ensuring these test sets are truly representative is not detailed.

## Confidence

- **High Confidence**: The core concept of using evolutionary algorithms for instance selection and the general energy-saving claims (up to 98% reduction) are well-supported by the experimental results presented.
- **Medium Confidence**: The mechanism explaining how elite samples improve both performance and generalizability is plausible but relies on several assumptions about data representativeness and the DE algorithm's effectiveness that are not fully validated.
- **Low Confidence**: The specific claim of achieving up to 50% accuracy improvement is highly dependent on the unknown DE parameters and may not be consistently reproducible without careful tuning.

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary DE algorithm parameters (mutation factor F, crossover probability, population size) across a range of values to identify optimal settings and understand their impact on elite sample quality and energy savings.

2. **Cross-Dataset Generalizability**: Apply the elite sampling framework to datasets from different domains (e.g., image, text, time series) to verify if the 98% energy savings and performance improvements generalize beyond the UCI and KEEL repositories.

3. **Ablation Study on Sample Size**: Conduct experiments with different elite sample percentages (5%, 10%, 15%, 20%) to determine the optimal balance between energy savings and model performance, validating the 10% choice presented in the paper.