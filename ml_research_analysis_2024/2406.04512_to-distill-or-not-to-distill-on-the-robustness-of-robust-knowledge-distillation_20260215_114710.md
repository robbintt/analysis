---
ver: rpa2
title: To Distill or Not to Distill? On the Robustness of Robust Knowledge Distillation
arxiv_id: '2406.04512'
source_url: https://arxiv.org/abs/2406.04512
tags:
- speech
- data
- arabic
- arxiv
- whisper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We evaluate multilingual ASR models on a diverse set of Arabic
  datasets, including a new human-annotated dataset covering five under-represented
  dialects. We also distill knowledge from large teacher models into smaller, more
  efficient student models for Arabic speech recognition.
---

# To Distill or Not to Distill? On the Robustness of Robust Knowledge Distillation

## Quick Facts
- **arXiv ID**: 2406.04512
- **Source URL**: https://arxiv.org/abs/2406.04512
- **Reference count**: 26
- **Primary result**: Best distilled model achieves 45.0% WER, outperforming Whisper-large-v2 teacher (55.1% WER) and SeamlessM4T-large-v2 (47.0% WER)

## Executive Summary
This paper investigates knowledge distillation for Arabic automatic speech recognition, focusing on transferring knowledge from large Whisper models to smaller, more efficient student models. The authors evaluate their approach on a diverse set of Arabic datasets including five novel human-annotated dialectal datasets. Through systematic experimentation with distillation parameters and architecture choices, they demonstrate that distilled models can outperform both their teacher models and larger state-of-the-art models, particularly on dialectal data where robustness is critical.

## Method Summary
The authors employ knowledge distillation from Whisper-large-v2 to smaller student models using a combination of dense representation and token-level probability distribution losses. Pseudo-labels are generated from the teacher model on a mixture of Arabic speech corpora, then filtered using a WER threshold (80%) to remove low-quality examples. Students are trained on this filtered data and evaluated across 10 Arabic datasets including MSA and five dialectal varieties (ALG, JOR, PAL, UAE, YEM). The approach balances efficiency and performance, with distilled models achieving 25-75% compute efficiency while maintaining competitive WER scores.

## Key Results
- Best distilled model (DW-16-16) achieves 45.0% WER, outperforming Whisper-large-v2 teacher (55.1% WER) and SeamlessM4T-large-v2 (47.0% WER)
- Distilled models show superior performance on dialectal data, with best average WER of 56.9% on the five novel dialects
- WER filtering at 80% threshold improves results by removing ~28% of low-quality pseudo-labels while maintaining sufficient training data
- Orthographic vs normalized+no-diacritics evaluation shows significant WER variation across datasets (Whisper-large-v2: 44.3% vs 55.1%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation transfers teacher model's generalization ability to smaller student models.
- Mechanism: The student model is trained to mimic both the dense representations and token-level probability distributions of the teacher model using a weighted sum of distillation losses.
- Core assumption: The teacher model has learned robust features that can be transferred to a smaller model without significant performance degradation.
- Evidence anchors:
  - [abstract] "we distill knowledge from large teacher models into smaller student variants that are more efficient"
  - [section 3] "The student model is trained to mimic the behavior of the teacher model both at the dense representation level and the sequence level"
  - [corpus] Weak - related papers focus on data-free distillation but don't directly address speech recognition

### Mechanism 2
- Claim: Filtering pseudo-labels by WER threshold improves distillation quality by removing low-quality examples.
- Mechanism: High WER threshold (80%) filters out pseudo-labels where teacher model performs poorly, ensuring student trains on higher quality examples.
- Core assumption: Filtering based on WER correlates with pseudo-label quality and improves student model performance.
- Evidence anchors:
  - [section 4.3] "we use a value of 80% for the WER threshold λ to filter-out low-quality transcription from pseudo-labels"
  - [section 5] "we find that discarding examples where the WER is above 80% (amounting to about 28% of the total examples) results in the best overall performance"
  - [corpus] Weak - related work mentions data-free distillation but doesn't specifically address WER-based filtering

### Mechanism 3
- Claim: Mixing diverse Arabic datasets in distillation improves model robustness to dialectal variations.
- Mechanism: Training on a mixture of MSA, dialectal, and varied accent data helps student model learn features that generalize across linguistic variations.
- Core assumption: Exposure to diverse linguistic patterns during distillation enables better handling of unseen dialects.
- Evidence anchors:
  - [section 4.2.3] "We randomly sample 100K and 500K segments from a mixture of MGB2, MGB3, FLEURS, CommonVoice 15.0, QASR, Arabic Speech Corpus, and Massive Arabic Speech Corpus"
  - [section 5] "Our distilled models are 25-75% compute efficient while maintaining the same performance as big models" and "Our best-performing distilled models yield the best results in terms of WER on four out of ten evaluated datasets"
  - [corpus] Weak - related work focuses on multilingual distillation but doesn't specifically address Arabic dialect diversity

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The paper's core contribution relies on transferring knowledge from large Whisper models to smaller, more efficient student models for Arabic ASR
  - Quick check question: What are the two types of losses used in the distillation objective (equation 3)?

- Concept: Word Error Rate (WER) and Character Error Rate (CER)
  - Why needed here: These are the primary evaluation metrics used to assess model performance across different Arabic datasets and dialects
  - Quick check question: How does normalization and diacritic removal affect WER scores on different datasets?

- Concept: Dialectal Arabic vs Modern Standard Arabic
  - Why needed here: The paper addresses the challenge of Arabic ASR across different varieties, with significant performance differences between MSA and dialectal data
  - Quick check question: Which Arabic dialects are included in the novel in-house dataset and which show the poorest performance across models?

## Architecture Onboarding

- Component map: Teacher model (Whisper-large-v2) → Pseudo-label generation → WER-based filtering → Student model training → Evaluation
- Critical path: Teacher model inference → Pseudo-label generation → WER filtering → Student model training → Evaluation
- Design tradeoffs: Smaller student models are more efficient but may lose some teacher performance; higher WER thresholds improve quality but reduce training data
- Failure signatures: High WER on dialectal data, hallucinations in output, incomplete transcriptions, empty predictions
- First 3 experiments:
  1. Train DW-16-16 on 100K segments with 80% WER threshold and evaluate on all datasets
  2. Compare orthographic vs normalized + no-diacritics WER on Whisper-large-v2
  3. Vary WER threshold (10%, 20%, 40%, 80%) on DW-16-16 and DW-32-16 to find optimal filtering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise impact of different pseudo-labeling WER thresholds on the final distilled model's performance across various Arabic dialects?
- Basis in paper: [explicit] Section 5 discusses experimenting with different WER thresholds (10, 20, 40, 80, none) for filtering pseudo-labels during distillation and their effect on model performance.
- Why unresolved: The paper shows results for specific threshold values but doesn't systematically analyze the optimal threshold for different dialectal characteristics or the trade-off between data quantity and quality.
- What evidence would resolve it: A comprehensive study showing WER performance on each dialect at different WER thresholds, including statistical analysis of the relationship between threshold, dialect complexity, and model performance.

### Open Question 2
- Question: How does the size of distillation training data (from 100K to 1M segments) affect the model's ability to generalize to previously unseen Arabic dialects?
- Basis in paper: [explicit] Section 5 mentions scaling data from 100K to 500K and 1M segments, observing performance improvements, but doesn't explore the limits or optimal data size for dialectal generalization.
- Why unresolved: The paper only explores a limited range of data sizes and doesn't analyze whether there's a point of diminishing returns or specific data requirements for different dialect types.
- What evidence would resolve it: A systematic study mapping data size to dialectal performance across all five tested dialects, identifying optimal data requirements for each dialect type.

### Open Question 3
- Question: What are the specific linguistic features (phonetic, lexical, syntactic) that cause the largest performance drops when models transition from Modern Standard Arabic to dialectal varieties?
- Basis in paper: [explicit] Section 5 discusses performance degradation from MSA to dialects and Section 6 provides error analysis categories including dialectal inaccuracies.
- Why unresolved: While error types are identified, the paper doesn't conduct a detailed linguistic analysis of which specific features cause the most problems across different dialect pairs.
- What evidence would resolve it: A detailed linguistic analysis comparing error patterns across dialect pairs, identifying specific phonetic, lexical, and syntactic features that consistently cause recognition failures.

## Limitations
- Lack of ablation studies to isolate contribution of each distillation component (loss types, WER filtering, architecture choices)
- 80% WER filtering threshold lacks theoretical justification and no analysis of optimal threshold balancing quality vs diversity
- Novel in-house dialect datasets are not independently verified, limiting reproducibility and generalizability claims

## Confidence
- **High confidence**: Basic knowledge distillation mechanism is well-established and implementation follows standard practices
- **Medium confidence**: WER filtering approach shows empirical success but lacks theoretical justification for 80% threshold
- **Low confidence**: Claim about diverse dataset mixing improving dialectal robustness is based on aggregate performance without dialect-specific statistical validation

## Next Checks
1. **Ablation on WER threshold**: Systematically vary the WER filtering threshold (10%, 20%, 40%, 60%, 80%) and measure not just overall WER but also dialect-specific performance degradation/gains to determine optimal filtering

2. **Dialect-specific performance analysis**: Compute statistical significance of performance differences across the five dialects for each model, and identify whether certain dialects consistently benefit from distillation versus others

3. **Teacher-student representation analysis**: Compare dense representation similarity between teacher and student models across MSA vs dialectal data to verify whether the claimed generalization transfer actually occurs at the feature level