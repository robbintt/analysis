---
ver: rpa2
title: On Speculative Decoding for Multimodal Large Language Models
arxiv_id: '2404.08856'
source_url: https://arxiv.org/abs/2404.08856
tags:
- draft
- image
- language
- decoding
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies speculative decoding to multimodal large language
  models (MLLMs) to accelerate inference. The authors show that a language-only model
  can serve as an effective draft model for speculative decoding with LLaVA 7B, bypassing
  the need for image tokens and their associated processing components.
---

# On Speculative Decoding for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2404.08856
- Source URL: https://arxiv.org/abs/2404.08856
- Reference count: 27
- Language-only draft models can serve as effective draft models for MLLMs, bypassing the need for image tokens and achieving up to 2.37x speedup.

## Executive Summary
This paper introduces speculative decoding to multimodal large language models (MLLMs) to accelerate inference. The authors demonstrate that language-only models can serve as effective draft models for MLLMs like LLaVA 7B, eliminating the need for image encoders and associated processing components. Through experiments on three tasks, they achieve memory-bound speedup of up to 2.37x using a 115M parameter language model trained from scratch. Additionally, they introduce a compact LLaVA draft model with an image adapter that shows marginal performance gains in image captioning while maintaining comparable results in other tasks.

## Method Summary
The authors apply speculative decoding to MLLMs by using a smaller draft model to generate draft tokens that are verified in parallel by the target MLLM. They train a 115M parameter language model from scratch and use it as a draft model for LLaVA 7B. The draft model generates tokens based only on text prompts, bypassing the need for image embeddings. These draft tokens are then verified by the target MLLM which uses both image and text embeddings. They also create a small LLaVA draft model with an image adapter to compare performance. The approach is evaluated on LLaVA Instruct 150K dataset, COCO dataset for image captioning, and ScienceQA dataset.

## Key Results
- Speculative decoding with language-only draft models achieves up to 2.37x memory-bound speedup on MLLMs
- Language-only draft models outperform LLaVA draft models on token rate while maintaining comparable output quality
- The LLaVA draft model with image adapter provides marginal performance gains specifically for image captioning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-only draft models can serve as effective draft models for MLLMs without requiring image encoders or adapters.
- Mechanism: The draft model generates text tokens based only on the input text prompt, bypassing the need for image embeddings. These draft tokens are then verified by the target MLLM which uses both image and text embeddings. Since speculative decoding only requires the draft model to predict the next text token, the absence of image information in the draft model does not prevent it from generating plausible text continuations that can be verified by the target model.
- Core assumption: A significant portion of the text generation in MLLM tasks consists of common words, prepositions, and word completions that do not require image-specific knowledge.
- Evidence anchors:
  - [abstract] "We show that a language-only model can serve as a good draft model for speculative decoding with LLaVA 7B, bypassing the need for image tokens and their associated processing components from the draft model."
  - [section 3] "The draft model generates draft tokens autoregressively for block-size number of iterations followed by parallel evaluation by the target language model which also uses image features."
  - [corpus] "Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups (<1.5x)."

### Mechanism 2
- Claim: Speculative decoding with MLLMs achieves memory-bound speedup by parallelizing verification of draft tokens.
- Mechanism: The draft model generates a block of draft tokens auto-regressively. These draft tokens are then verified in parallel by the target MLLM through rejection sampling. This parallel verification allows the target model to process multiple tokens simultaneously, reducing the overall generation time. The speedup is memory-bound because the bottleneck shifts from sequential token generation to memory bandwidth during parallel verification.
- Core assumption: The parallel verification of draft tokens by the target MLLM is faster than sequential auto-regressive generation, even when considering the overhead of rejection sampling.
- Evidence anchors:
  - [abstract] "Our experiments across three different tasks show that speculative decoding can achieve a memory-bound speedup of up to 2.37× using a 115M parameter language model that we trained from scratch."
  - [section 2.1] "Speculative decoding (SPD) [3, 9] involves a smaller draft model generating multiple tokens which are verified in parallel by the target LLM."
  - [corpus] "Speculative decoding is a widely adopted technique to speed up inference for Large Language Models (LLMs) without sacrificing quality."

### Mechanism 3
- Claim: Training a smaller LLaVA draft model with an image adapter provides marginal performance gains in image-specific tasks while maintaining comparable results in other tasks.
- Mechanism: The smaller LLaVA draft model includes both a language model and an image adapter, allowing it to generate draft tokens conditioned on both text and image embeddings. This additional image information enables the draft model to make more informed predictions for tasks that heavily rely on image understanding, such as image captioning. However, for tasks where text generation does not require extensive image knowledge, the performance is comparable to that of language-only draft models.
- Core assumption: The additional image information in the draft model improves its ability to predict text tokens that are closely related to the image content, especially in tasks like image captioning.
- Evidence anchors:
  - [abstract] "Additionally, we introduce a compact LLaVA draft model incorporating an image adapter, which shows marginal performance gains in image captioning while maintaining comparable results in other tasks."
  - [section 3] "We also create a small LLaVA draft model which consists of an image adapter along with our trained language model and show that it improves the performance slightly on COCO captioning task and ScienceQA task while performing similar to language-model-only draft models on the other tasks."
  - [corpus] "This gap is increasingly significant as multimodal capabilities become more prevalent in language models."

## Foundational Learning

- Concept: Speculative decoding
  - Why needed here: Speculative decoding is the core technique used to accelerate MLLM inference by generating draft tokens in parallel and verifying them with the target model.
  - Quick check question: How does speculative decoding achieve speedup compared to traditional auto-regressive generation?

- Concept: Multimodal Large Language Models (MLLMs)
  - Why needed here: Understanding the architecture of MLLMs, including their vision encoder, image adapter, and language model backbone, is crucial for implementing speculative decoding with MLLMs.
  - Quick check question: What are the main components of an MLLM, and how do they interact during inference?

- Concept: Rejection sampling
  - Why needed here: Rejection sampling is used in speculative decoding to verify draft tokens generated by the draft model, ensuring that the output distribution matches that of the target model.
  - Quick check question: How does rejection sampling work in the context of speculative decoding, and what is its role in maintaining output quality?

## Architecture Onboarding

- Component map:
  - Input image and text prompt → Vision encoder → Image adapter → Language model backbone → Output tokens
  - Input text prompt → Draft model → Draft tokens → Target model (parallel verification) → Output tokens

- Critical path:
  - Input image and text prompt → Vision encoder → Image adapter → Language model backbone → Output tokens
  - Input text prompt → Draft model → Draft tokens → Target model (parallel verification) → Output tokens

- Design tradeoffs:
  - Language-only draft model vs. LLaVA draft model: Language-only draft models are more efficient as they don't require image processing, but LLaVA draft models may provide better draft tokens for image-specific tasks.
  - Block size: Larger block sizes can potentially provide higher speedup but may also increase the overhead of rejection sampling and parallel verification.

- Failure signatures:
  - Low block efficiency: Indicates that the draft model is not generating accurate draft tokens, leading to frequent rejections during verification.
  - Slow token rate: Suggests that the overhead of rejection sampling and parallel verification is outweighing the benefits of parallel processing.
  - Poor performance on image-specific tasks: May indicate that the language-only draft model is not generating accurate draft tokens for tasks that heavily rely on image understanding.

- First 3 experiments:
  1. Implement speculative decoding with a language-only draft model and measure the block efficiency and token rate on a simple text generation task.
  2. Compare the performance of a language-only draft model and an LLaVA draft model on an image captioning task.
  3. Vary the block size and measure its impact on the speedup and token rate to find the optimal block size for the given MLLM and draft model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of speculative decoding change when using different block sizes (draft lengths) beyond the tested values of 3 and 5?
- Basis in paper: [explicit] The authors mention that they tested block sizes of 3 and 5, and observed that block size 3 generally gave better token rate than block size 5. However, they did not explore a wider range of block sizes.
- Why unresolved: The paper only provides results for two specific block sizes, limiting the understanding of how block size affects performance across a broader range of values.
- What evidence would resolve it: Experiments testing a wider range of sizes (e.g., 1, 2, 4, 6, 10) would provide insights into the optimal block size for different tasks and draft models.

### Open Question 2
- Question: How does the use of sampling-based decoding methods (e.g., varying temperature, top-p, top-k) impact the performance of speculative decoding for MLLMs?
- Basis in paper: [explicit] The authors mention that they used greedy decoding for all experiments and leave the exploration of sampling-based decoding as future work.
- Why unresolved: The paper only evaluates greedy decoding, which may not capture the full potential of speculative decoding when combined with sampling-based methods.
- What evidence would resolve it: Experiments comparing greedy decoding with various sampling-based decoding strategies would reveal the impact of these methods on speculative decoding performance.

### Open Question 3
- Question: How does the performance of speculative decoding change when using different MLLM architectures (e.g., BLIP-2, MiniGPT-4, OpenFlamingo) as the target model?
- Basis in paper: [explicit] The authors mention that their work can be extended to other MLLM architectures but do not provide experimental results.
- Why unresolved: The paper only evaluates speculative decoding with LLaVA 7B, limiting the generalizability of the findings to other MLLM architectures.
- What evidence would resolve it: Experiments applying speculative decoding to various MLLM architectures would demonstrate the effectiveness of the approach across different models.

## Limitations

- The study evaluates only three specific tasks with a single target model (LLaVA 7B), limiting generalizability to other MLLM architectures and vision-language tasks.
- Critical implementation details for the 115M parameter language model training are missing, including learning rate schedules, optimizer choice, and exact dataset composition.
- The paper doesn't thoroughly analyze the trade-offs between draft model size, image adapter complexity, and performance across diverse task types.

## Confidence

*High Confidence* claims:
- Language-only draft models can serve as effective draft models for MLLMs
- Speculative decoding achieves memory-bound speedup through parallel verification
- The 115M parameter language model achieves up to 2.37x speedup on tested tasks

*Medium Confidence* claims:
- Language-only draft models outperform LLaVA draft models on token rate
- The LLaVA draft model with image adapter provides marginal performance gains specifically for image captioning
- Block size optimization significantly impacts overall speedup

*Low Confidence* claims:
- Generalizability to other MLLM architectures beyond LLaVA 7B
- Performance on tasks outside the three evaluated domains
- Scalability to larger MLLM models (13B, 30B+ parameters)

## Next Checks

1. **Cross-architecture validation**: Test the proposed approach with other MLLM architectures (e.g., MiniGPT-4, LLaVA-1.5, Qwen-VL) to assess generalizability beyond LLaVA 7B.

2. **Parameter scaling study**: Evaluate draft model performance across a range of parameter sizes (30M, 115M, 300M) to determine the optimal draft model size for different target MLLM scales.

3. **Extended task evaluation**: Test on additional vision-language tasks including VQA, visual reasoning, and cross-modal retrieval to comprehensively assess the approach's effectiveness across diverse MLLM use cases.