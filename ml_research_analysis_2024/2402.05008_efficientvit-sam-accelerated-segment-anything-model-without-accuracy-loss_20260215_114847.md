---
ver: rpa2
title: 'EfficientViT-SAM: Accelerated Segment Anything Model Without Accuracy Loss'
arxiv_id: '2402.05008'
source_url: https://arxiv.org/abs/2402.05008
tags: []
core_contribution: This paper proposes EfficientViT-SAM, which accelerates the Segment
  Anything Model (SAM) by replacing SAM's heavy image encoder with EfficientViT. The
  authors retain SAM's prompt encoder and mask decoder, and perform knowledge distillation
  from SAM-ViT-H to EfficientViT, followed by end-to-end training on the SA-1B dataset.
---

# EfficientViT-SAM: Accelerated Segment Anything Model Without Accuracy Loss

## Quick Facts
- arXiv ID: 2402.05008
- Source URL: https://arxiv.org/abs/2402.05008
- Authors: Zhuoyang Zhang, Han Cai, Song Han
- Reference count: 25
- Key outcome: 48.9x speedup on A100 GPU over SAM-ViT-H without accuracy loss, achieving 47.8% COCO zero-shot instance segmentation mAP at 182 images/s

## Executive Summary
EfficientViT-SAM accelerates the Segment Anything Model (SAM) by replacing its heavy ViT-H image encoder with the more efficient EfficientViT architecture while retaining SAM's prompt encoder and mask decoder. The model is trained using knowledge distillation from SAM-ViT-H followed by end-to-end training on the SA-1B dataset. This approach achieves 48.9x measured speedup on A100 GPU over SAM-ViT-H without sacrificing performance, delivering 47.8% COCO zero-shot instance segmentation mAP with 182 images/s throughput compared to SAM-ViT-H's 46.5% mAP at 11 images/s.

## Method Summary
The method replaces SAM's ViT-H image encoder with EfficientViT while keeping the prompt encoder and mask decoder unchanged. Training occurs in two phases: first, knowledge distillation transfers representations from SAM-ViT-H to EfficientViT using L2 loss; second, the full model is fine-tuned end-to-end on SA-1B using focal and dice losses (20:1 ratio). The approach leverages EfficientViT's linear attention mechanism to reduce computational complexity while maintaining global receptive field, and employs multi-scale learning with hardware-efficient operations.

## Key Results
- 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H
- 47.8% COCO zero-shot instance segmentation mAP (vs. SAM-ViT-H's 46.5%)
- 182 images/s throughput (vs. SAM-ViT-H's 11 images/s)
- Superior performance compared to other SAM acceleration methods

## Why This Works (Mechanism)

### Mechanism 1: EfficientViT Linear Attention
Replacing SAM's heavy ViT-H image encoder with EfficientViT maintains accuracy while reducing computation through linear attention with ReLU activation and local convolutions, reducing complexity from O(n²) to O(n) while preserving global receptive field. Core assumption: EfficientViT's architecture can capture equivalent image features as ViT-H for SAM's downstream tasks.

### Mechanism 2: Knowledge Distillation Initialization
Knowledge distillation from SAM-ViT-H to EfficientViT provides effective initialization for downstream training through L2 loss between image embeddings, creating feature space alignment that transfers learned representations. Core assumption: SAM-ViT-H's image embeddings contain generalizable features that EfficientViT can approximate effectively.

### Mechanism 3: Component Compatibility
Retaining SAM's prompt encoder and mask decoder while only replacing the image encoder preserves model behavior by leveraging pre-trained weights to maintain consistent prompt-response mapping. Core assumption: SAM's prompt encoder and mask decoder are sufficiently general to work with any compatible image embedding space.

## Foundational Learning

- **Knowledge Distillation**: Transfers learned representations from large SAM-ViT-H to smaller EfficientViT without full retraining. *Quick check*: What loss function is used for knowledge distillation between image encoders in EfficientViT-SAM?

- **Linear Attention Mechanisms**: Reduces computational complexity while maintaining global receptive field needed for segmentation. *Quick check*: How does EfficientViT's linear attention complexity compare to standard softmax attention?

- **Multi-task Training with Focal and Dice Loss**: Balances handling class imbalance (focal) with overlap accuracy (dice) for robust mask prediction. *Quick check*: What is the ratio of focal loss to dice loss used in EfficientViT-SAM training?

## Architecture Onboarding

- **Component map**: Image → EfficientViT encoder → Neck (F-MBConv blocks) → SAM head → Mask output
- **Critical path**: Image → EfficientViT encoder → Neck (F-MBConv blocks) → SAM head → Mask output
- **Design tradeoffs**: Smaller EfficientViT variants trade accuracy for speed; distillation vs. random initialization; end-to-end vs. stage-wise training
- **Failure signatures**: Accuracy drop despite speed gain (check EfficientViT feature representation quality); training instability (verify distillation initialization); hardware bottlenecks (profile tensor operations)
- **First 3 experiments**: 1) Verify distillation by comparing SAM-ViT-H and EfficientViT embeddings on SA-1B validation set; 2) Measure throughput on target hardware with TensorRT; 3) Test prompt compatibility by validating prompt encoder outputs with EfficientViT features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the long-term performance trend of EfficientViT-SAM compared to other SAM acceleration methods as the model scales up?
- Basis in paper: The paper shows that EfficientViT-SAM outperforms other SAM acceleration methods in terms of COCO zero-shot instance segmentation mAP and throughput, but it does not provide long-term performance trends as the model scales up.
- Why unresolved: The paper does not provide information on how EfficientViT-SAM's performance compares to other methods as the model scales up over time.
- What evidence would resolve it: Long-term studies comparing EfficientViT-SAM's performance with other SAM acceleration methods as the model scales up, including metrics such as mAP, throughput, and parameter count.

### Open Question 2
- Question: How does EfficientViT-SAM perform on datasets other than COCO and LVIS, especially those with different object scales and complexities?
- Basis in paper: The paper evaluates EfficientViT-SAM on COCO and LVIS datasets, but it does not provide information on its performance on other datasets with different object scales and complexities.
- Why unresolved: The paper does not provide evidence of EfficientViT-SAM's performance on datasets other than COCO and LVIS, limiting the understanding of its generalizability.
- What evidence would resolve it: Experiments evaluating EfficientViT-SAM's performance on a diverse set of datasets with varying object scales and complexities, including metrics such as mAP, mIoU, and inference time.

### Open Question 3
- Question: How does EfficientViT-SAM's performance change when using different object detectors, such as YOLOv8 and GroundingDINO, as box prompts?
- Basis in paper: The paper mentions that EfficientViT-SAM achieves superior performance compared to SAM when using YOLOv8 and GroundingDINO as object detectors, but it does not provide a detailed comparison of its performance with different detectors.
- Why unresolved: The paper does not provide a comprehensive comparison of EfficientViT-SAM's performance when using different object detectors, limiting the understanding of its adaptability to different detection methods.
- What evidence would resolve it: Experiments comparing EfficientViT-SAM's performance when using different object detectors, such as YOLOv8 and GroundingDINO, as box prompts, including metrics such as mAP, mIoU, and inference time.

## Limitations
- Knowledge distillation effectiveness lacks direct experimental validation in the corpus
- Architectural modifications lack comparative studies showing equivalent feature representation quality
- Compatibility of SAM's prompt encoder and mask decoder with EfficientViT features remains empirically unproven

## Confidence
- **High confidence**: Computational efficiency gains from replacing ViT-H with EfficientViT (mechanism is well-established in EfficientViT literature)
- **Medium confidence**: Knowledge distillation effectiveness (reasonable approach but limited direct evidence)
- **Medium confidence**: Prompt encoder/decoder compatibility (theoretically plausible but untested)

## Next Checks
1. Verify distillation quality by comparing SAM-ViT-H and EfficientViT image embeddings on SA-1B validation set using both L2 loss and downstream segmentation performance metrics
2. Conduct controlled experiments freezing SAM's prompt encoder and mask decoder while varying the image encoder architecture to quantify performance degradation
3. Profile TensorRT FP16 conversion on A100 GPU to identify any precision mismatches or unsupported operations that could explain the claimed 48.9x speedup