---
ver: rpa2
title: 'MISS: A Generative Pretraining and Finetuning Approach for Med-VQA'
arxiv_id: '2401.05163'
source_url: https://arxiv.org/abs/2401.05163
tags:
- medical
- image
- encoder
- images
- image-text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of Medical Visual Question Answering
  (Med-VQA), where most methods treat the task as answer classification, limiting
  their applicability in real-world scenarios. The proposed method, MISS, treats Med-VQA
  as a generative task, enabling direct application in practical settings.
---

# MISS: A Generative Pretraining and Finetuning Approach for Med-VQA
## Quick Facts
- arXiv ID: 2401.05163
- Source URL: https://arxiv.org/abs/2401.05163
- Authors: Jiawei Chen; Dingkang Yang; Yue Jiang; Yuxuan Lei; Lihua Zhang
- Reference count: 31
- Primary result: Generative Med-VQA approach achieving excellent results with fewer multimodal datasets

## Executive Summary
This paper introduces MISS, a novel generative approach for Medical Visual Question Answering (Med-VQA) that addresses the limitations of traditional classification-based methods. Unlike existing approaches that treat Med-VQA as answer classification, MISS frames it as a generative task, enabling direct application in real-world clinical settings. The key innovation is the Joint Text-Multimodal (JTM) encoder, which unifies text and multimodal encoders to learn joint feature representations through multi-task learning. The method also introduces Transfer-and-Caption (TransCap) to extend feature spaces of single-modal image datasets using LLM-generated captions, allowing traditional medical vision task data to be utilized for Vision-Language Pre-training.

## Method Summary
MISS proposes a generative pretraining and finetuning approach for Med-VQA that treats the task as direct answer generation rather than classification. The core innovation is the Joint Text-Multimodal (JTM) encoder, which integrates text and multimodal encoders to learn unified feature representations through multi-task learning. Additionally, the Transfer-and-Caption (TransCap) method leverages Large Language Models to generate descriptive captions for single-modal medical image datasets, effectively extending their feature space for Vision-Language Pre-training. This approach enables the utilization of traditional medical vision datasets that lack textual question-answer pairs, significantly expanding the available training data for Med-VQA models.

## Key Results
- Achieves superior performance on Med-VQA benchmarks compared to classification-based methods
- Demonstrates effectiveness with fewer multimodal datasets through innovative data augmentation
- Shows promising results for direct application in practical clinical settings

## Why This Works (Mechanism)
The generative approach allows for more flexible and natural answer generation compared to fixed answer classification. By unifying text and multimodal encoders through the JTM architecture, the model learns richer joint representations that capture both visual and textual information more effectively. The TransCap method addresses the data scarcity problem by leveraging LLM-generated captions to transform existing single-modal medical image datasets into usable multimodal training data.

## Foundational Learning
- **Medical Visual Question Answering**: Understanding the task of answering questions about medical images using AI models
  - Why needed: Core problem domain that MISS addresses
  - Quick check: Verify understanding of medical imaging modalities and question-answer formats
- **Vision-Language Pre-training**: Joint training of vision and language models using paired image-text data
  - Why needed: Foundation for multimodal understanding in medical contexts
  - Quick check: Review standard VLP architectures and their limitations
- **Multi-task Learning**: Training models on multiple related tasks simultaneously to improve generalization
  - Why needed: Enables JTM encoder to learn robust joint representations
  - Quick check: Examine how task supervision signals influence feature learning
- **Large Language Models**: Advanced text generation models used for caption generation
  - Why needed: Powers the TransCap data augmentation method
  - Quick check: Understand LLM capabilities and limitations in medical contexts
- **Generative vs. Classification Models**: Different approaches to producing model outputs
  - Why needed: Key architectural decision in MISS framework
  - Quick check: Compare performance and applicability of each approach in clinical settings
- **Medical Image Datasets**: Curated collections of medical images with annotations
  - Why needed: Training data foundation for Med-VQA systems
  - Quick check: Review common medical imaging modalities and their characteristics

## Architecture Onboarding
**Component Map**: Image Encoder -> JTM Encoder -> Text Encoder -> LLM Caption Generator -> Training Pipeline
**Critical Path**: Input Image → JTM Encoder → Joint Features → Answer Generation
**Design Tradeoffs**: Generative approach offers flexibility but requires more computational resources; JTM encoder complexity vs. performance gains; TransCap augmentation quality vs. data quantity
**Failure Signatures**: Inaccurate medical image understanding, poor text-vision alignment, generation of clinically irrelevant answers, over-reliance on text cues
**First Experiments**: 1) Ablation study of JTM encoder components, 2) Comparison of TransCap-generated captions vs. human annotations, 3) Performance analysis across different medical imaging modalities

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Real-world clinical applicability not thoroughly validated across diverse settings
- Potential bias introduced by LLM-generated captions for data augmentation
- Limited comprehensive comparison against state-of-the-art classification methods
- Unclear computational efficiency compared to standard approaches

## Confidence
- **High confidence**: Technical feasibility of JTM encoder architecture and TransCap methodology
- **Medium confidence**: Claimed performance improvements over existing Med-VQA approaches
- **Low confidence**: Real-world clinical applicability and generalization across diverse medical imaging scenarios

## Next Checks
1. Conduct cross-institutional validation using diverse medical imaging datasets from different clinical environments to assess generalizability
2. Perform comprehensive error analysis focusing on clinically critical failure cases and their impact on diagnostic accuracy
3. Benchmark the computational overhead and inference latency compared to standard classification-based approaches in clinical deployment scenarios