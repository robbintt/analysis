---
ver: rpa2
title: Mitigating Catastrophic Forgetting in Multi-domain Chinese Spelling Correction
  by Multi-stage Knowledge Transfer Framework
arxiv_id: '2402.11422'
source_url: https://arxiv.org/abs/2402.11422
tags:
- knowledge
- chinese
- forgetting
- domain
- catastrophic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in multi-domain Chinese
  spelling correction (CSC), where models tend to lose previously learned domain-specific
  knowledge when adapting to new domains. The authors propose a model-agnostic Multi-stage
  Knowledge Transfer (MKT) framework based on continual learning, which uses a dynamically
  evolving teacher model to transfer accumulated knowledge at each training stage.
---

# Mitigating Catastrophic Forgetting in Multi-domain Chinese Spelling Correction by Multi-stage Knowledge Transfer Framework

## Quick Facts
- arXiv ID: 2402.11422
- Source URL: https://arxiv.org/abs/2402.11422
- Reference count: 20
- Multi-stage knowledge transfer framework reduces catastrophic forgetting in multi-domain Chinese spelling correction, improving performance across all domains

## Executive Summary
This paper addresses catastrophic forgetting in multi-domain Chinese spelling correction (CSC), where models tend to lose previously learned domain-specific knowledge when adapting to new domains. The authors propose a model-agnostic Multi-stage Knowledge Transfer (MKT) framework based on continual learning, which uses a dynamically evolving teacher model to transfer accumulated knowledge at each training stage. Experiments on four domains (General, Car, Medical, Legal) show that MKT improves performance across all domains compared to baseline CSC models (BERT, Soft-Masked BERT, REALISE). The framework also significantly reduces catastrophic forgetting, as demonstrated by smoother performance degradation in the General domain when incrementally trained on other domains.

## Method Summary
The MKT framework employs a dynamic teacher model strategy where a frozen copy of the student model from the previous training stage serves as the teacher. During training on a new domain, knowledge distillation loss is calculated between the student and teacher models, combined with the task-specific cross-entropy loss. The loss function is weighted by hyperparameter λ, which controls the balance between preserving old knowledge and learning new domain-specific knowledge. The framework is trained sequentially across domains, with the student model's parameters updated while the teacher model remains frozen until the next stage.

## Key Results
- REALISE+MKT achieves 55.28 F1 score on Medical domain compared to 53.33 for baseline REALISE
- MKT framework reduces catastrophic forgetting, with smoother performance degradation in General domain during incremental training
- Parameter λ set to 0.01 (ratio of general to special domain data) provides optimal performance balance across all domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dynamically evolving teacher model preserves and transfers knowledge from previous domains to prevent catastrophic forgetting.
- Mechanism: The teacher model is a frozen copy of the student model from the previous training stage. During training on a new domain, it provides knowledge distillation loss that encourages the student to maintain performance on previously learned domains while adapting to the new one.
- Core assumption: Knowledge distillation loss effectively preserves learned representations across domain shifts when weighted appropriately against the new domain loss.
- Evidence anchors:
  - [abstract] "which utilizes a continuously evolving teacher model for knowledge transfer in each domain"
  - [section] "Our framework employs a dynamic teacher model strategy... This teacher model acts as a comprehensive knowledge repository, effectively serving as a backup of the student model from the previous stage to calculate the distillation loss for the current stage's student model"
- Break condition: If λ is set too low, the distillation loss becomes negligible and catastrophic forgetting resumes. If λ is too high, the model cannot adapt to new domains effectively.

### Mechanism 2
- Claim: The MKT framework's model-agnostic design allows it to improve performance across diverse CSC architectures.
- Mechanism: By adding knowledge distillation as an auxiliary loss term that works with any base CSC model, MKT provides domain knowledge retention without requiring architectural modifications to the underlying model.
- Core assumption: Different CSC architectures can benefit from knowledge distillation in the same way, regardless of their specific detection and correction mechanisms.
- Evidence anchors:
  - [abstract] "we propose a novel model-agnostic Multi-stage Knowledge Transfer (MKT) framework"
  - [section] "Experiments prove the effectiveness of our proposed method, and further analyses demonstrate the importance of overcoming catastrophic forgetting for improving the model performance"
- Break condition: If the base model's architecture fundamentally conflicts with knowledge distillation (e.g., non-differentiable components), MKT may not be applicable.

### Mechanism 3
- Claim: The weighted loss formulation balances preserving old knowledge with learning new domain-specific knowledge.
- Mechanism: The loss function combines knowledge distillation loss (preserving old knowledge) and cross-entropy loss (learning new knowledge) with hyperparameter λ controlling the balance between them.
- Core assumption: There exists an optimal λ value that balances these competing objectives based on the relative size of training data across domains.
- Evidence anchors:
  - [section] "We think that the main reason for this phenomenon is that the amount of training data in each special domain accounts for approximately 1% of the amount of general training data"
  - [section] "Parameter Study... settings λ between 0.005 and 0.02 stably bring improvements over the baseline. Particularly, setting λ at 0.01 performs best in all domains"
- Break condition: If domain data distributions change dramatically, the optimal λ may shift, requiring re-tuning.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding this phenomenon is crucial because the paper's core contribution is addressing catastrophic forgetting in multi-domain CSC
  - Quick check question: What happens to a model's performance on previously learned domains when it's trained on new domains without any mechanism to preserve old knowledge?

- Concept: Knowledge distillation
  - Why needed here: The MKT framework relies on knowledge distillation to transfer knowledge from the teacher model to the student model
  - Quick check question: How does knowledge distillation help a student model learn from a teacher model without directly copying its parameters?

- Concept: Multi-stage training and incremental learning
  - Why needed here: The MKT framework trains models sequentially across multiple domains, requiring understanding of how knowledge accumulates and degrades across stages
  - Quick check question: In a multi-stage training scenario, how does the model's ability to generalize to earlier domains typically change as it learns more domains?

## Architecture Onboarding

- Component map: Student model -> Teacher model -> Data loader -> Training loop (combines distillation + task loss) -> Backpropagation through student
- Critical path: Student model forward pass → Teacher model forward pass → Compute combined loss (distillation + task loss) → Backpropagation through student only → Parameter update → Save student as new teacher for next stage
- Design tradeoffs: Higher λ values better preserve old knowledge but may hinder adaptation to new domains; lower λ values allow better adaptation but risk catastrophic forgetting. The model-agnostic design trades potential for domain-specific optimization for broader applicability.
- Failure signatures: Performance degradation on previously learned domains during incremental training, inability to correct domain-specific errors from earlier stages, or poor adaptation to new domains if λ is set too high.
- First 3 experiments:
  1. Baseline comparison: Train each CSC model (BERT, Soft-Masked BERT, REALISE) sequentially across domains without MKT to establish catastrophic forgetting baseline
  2. Parameter sensitivity: Train BERT+MKT with varying λ values (0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 0.8) to identify optimal value
  3. Domain-specific evaluation: After training on all domains, test each model's performance on each individual domain to measure catastrophic forgetting quantitatively

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MKT framework scale with the number of domains and their diversity?
- Basis in paper: [inferred] The paper only experiments with four domains (General, Car, Medical, Legal) and does not explore scenarios with more domains or domains with varying degrees of similarity.
- Why unresolved: The paper does not provide evidence or analysis on how the framework would perform with a larger number of domains or domains that are more similar or dissimilar to each other.
- What evidence would resolve it: Experiments testing the MKT framework with a varying number of domains and domains with different levels of similarity would provide insights into its scalability and adaptability.

### Open Question 2
- Question: Can the MKT framework be extended to other languages beyond Chinese, and what modifications would be necessary?
- Basis in paper: [explicit] The paper mentions that the approach focuses on Chinese scenarios but suggests that other languages, such as English, could also benefit from the methodology.
- Why unresolved: The paper does not provide any experiments or detailed analysis on applying the MKT framework to other languages, nor does it discuss the specific challenges or modifications needed for different languages.
- What evidence would resolve it: Implementing the MKT framework on datasets from other languages and comparing its performance with existing methods would demonstrate its applicability and effectiveness across languages.

### Open Question 3
- Question: How does the choice of the teacher model (e.g., using a more complex model than the student) affect the performance of the MKT framework?
- Basis in paper: [inferred] The paper uses the student model from the previous stage as the teacher model but does not explore the impact of using different types of models as teachers.
- Why unresolved: The paper does not provide experiments or analysis on how using a more complex or simpler model as the teacher compared to the student would affect the knowledge transfer and overall performance.
- What evidence would resolve it: Experiments comparing the performance of the MKT framework when using different types of models as teachers (e.g., a more complex model or a model trained with different objectives) would shed light on the optimal choice of the teacher model.

## Limitations

- Domain Coverage and Generalization: The framework was evaluated on only four domains with specific data distributions, and the optimal λ value may not generalize to domains with different characteristics.
- Architectural Constraints: The knowledge distillation approach assumes the base CSC model is trainable with backpropagation, potentially limiting compatibility with non-differentiable or reinforcement learning-based models.
- Computational Overhead: The framework requires maintaining multiple model checkpoints, doubling storage requirements and increasing training time due to additional teacher model forward passes.

## Confidence

**High Confidence** (supported by multiple experiments and ablation studies):
- The MKT framework effectively mitigates catastrophic forgetting across all tested CSC architectures
- Setting λ to 0.01 provides optimal performance balance for the tested domain distributions
- Knowledge distillation loss is the primary mechanism for preserving cross-domain knowledge

**Medium Confidence** (based on limited experiments or domain-specific observations):
- The model-agnostic nature of MKT extends to CSC architectures beyond those tested
- The performance improvements scale proportionally with domain diversity and complexity
- The optimal λ value generalizes to other domain combinations with different data distributions

**Low Confidence** (requires additional validation):
- The framework's effectiveness on domains with significantly different error patterns
- Performance on languages other than Chinese with different orthographic and grammatical structures
- Scalability to dozens or hundreds of domains without performance degradation

## Next Checks

1. **Cross-linguistic Validation**: Implement MKT on English or multilingual CSC datasets to verify the framework's effectiveness across different linguistic structures and error patterns.

2. **Scalability Testing**: Evaluate MKT performance when training on 10+ diverse domains to assess whether the framework maintains effectiveness as the number of domains increases and domain similarities become more complex.

3. **Alternative Distillation Methods**: Compare MKT's knowledge distillation approach against other continual learning methods (e.g., elastic weight consolidation, experience replay) to quantify the relative contribution of knowledge distillation versus other potential mechanisms for preventing catastrophic forgetting.