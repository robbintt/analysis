---
ver: rpa2
title: 'GeneSUM: Large Language Model-based Gene Summary Extraction'
arxiv_id: '2412.18154'
source_url: https://arxiv.org/abs/2412.18154
tags:
- gene
- summary
- information
- sentences
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GeneSUM, a two-stage automated gene summary
  extraction framework leveraging large language models (LLMs) to address the challenge
  of efficiently navigating and synthesizing the rapidly expanding biomedical literature
  on gene functions. The first stage retrieves and filters relevant gene literature
  using signature term filtering and Gene Ontology (GO) annotation expansion, while
  the second stage fine-tunes an LLM (Gemma-7B) to generate concise gene summaries.
---

# GeneSUM: Large Language Model-based Gene Summary Extraction

## Quick Facts
- arXiv ID: 2412.18154
- Source URL: https://arxiv.org/abs/2412.18154
- Authors: Zhijian Chen; Chuan Hu; Min Wu; Qingqing Long; Xuezhi Wang; Yuanchun Zhou; Meng Xiao
- Reference count: 36
- One-line primary result: GeneSUM achieves significant improvements in ROUGE metrics (ROUGE-1: 0.3874, ROUGE-2: 0.1856, ROUGE-L: 0.3681), outperforming baselines by at least fourfold in ROUGE-2 for gene summary extraction.

## Executive Summary
GeneSUM addresses the challenge of efficiently navigating and synthesizing rapidly expanding biomedical literature on gene functions by introducing a two-stage automated gene summary extraction framework. The approach leverages large language models (LLMs) to retrieve and filter relevant gene literature using signature term filtering and Gene Ontology (GO) annotation expansion, followed by fine-tuning an LLM (Gemma-7B) to generate concise gene summaries. Experiments on 8,887 human genes demonstrate significant improvements in ROUGE metrics compared to baselines, with case studies showing generated summaries closely align with expert-written descriptions while maintaining traceability to source literature.

## Method Summary
GeneSUM employs a two-stage framework for automated gene summary extraction. The first stage retrieves and filters relevant gene literature using signature term filtering based on Pearson's chi-square test to identify gene-specific terms, combined with GO annotation expansion using ChatGPT to generate structured gene function descriptions. The second stage fine-tunes the Gemma-7B LLM with LoRA (rank 32) to generate concise summaries from the filtered and structured literature. The system uses BioBERT for semantic vectorization, K-means clustering with Calinski-Harabasz optimization for key sentence selection, and cosine similarity to align selected sentences with GO descriptions.

## Key Results
- GeneSUM achieves ROUGE-1: 0.3874, ROUGE-2: 0.1856, and ROUGE-L: 0.3681 on 8,887 human genes
- Outperforms baselines by at least fourfold in ROUGE-2 metric
- Generated summaries maintain traceability to source literature, addressing LLM hallucination concerns
- Case studies demonstrate close alignment with expert-written gene descriptions

## Why This Works (Mechanism)

### Mechanism 1
Signature term filtering using Pearson's chi-square test effectively reduces noise by retaining only sentences with high-frequency gene-specific terms. The chi-square test compares term frequencies in gene reference summaries versus random biomedical literature, selecting terms with large X² values as signatures. Sentences with fewer than three signature terms are filtered out, leaving only literature relevant to the target gene.

### Mechanism 2
GO annotation expansion via LLM rewriting provides structured, multi-angle gene function descriptions that guide sentence selection. ChatGPT generates concise sentences for each GO term (molecular function, biological process, cellular component) using a prompt with examples and constraints (≤10 words per sentence). These descriptions serve as reference vectors for clustering and similarity comparison.

### Mechanism 3
K-means clustering with Calinski-Harabasz optimization effectively groups semantically similar sentences, allowing key sentence selection based on cosine similarity to GO descriptions. After vectorizing all sentences and GO descriptions with BioBERT, K-means clusters them into 3-10 groups. For each GO description, the most similar sentence within its cluster is selected as a key sentence.

## Foundational Learning

- **BioBERT embeddings**
  - Why needed here: Provides domain-specific semantic representations for biomedical text, improving clustering and similarity calculations compared to general-purpose embeddings.
  - Quick check question: What makes BioBERT embeddings more suitable than standard BERT for gene literature summarization?

- **K-means clustering and Calinski-Harabasz score**
  - Why needed here: Groups semantically similar sentences, and CH score objectively selects optimal cluster number for key sentence extraction.
  - Quick check question: How does the Calinski-Harabasz score determine the optimal number of clusters for gene summary extraction?

- **Pearson's chi-square test for feature selection**
  - Why needed here: Identifies signature terms that distinguish gene-specific literature from general biomedical text, enabling effective initial filtering.
  - Quick check question: What null hypothesis does Pearson's chi-square test evaluate when selecting signature terms for gene literature?

## Architecture Onboarding

- **Component map**: Literature retrieval (PubMed IDs from Entrez Gene) → Signature filtering (chi-square test, 3710 terms) → GO rewriting (ChatGPT prompt, ≤10 words per sentence) → Vectorization (BioBERT embeddings) → Clustering (K-means with CH score optimization) → Key sentence selection (cosine similarity to GO descriptions) → LoRA fine-tuning (Gemma-7B, rank 32) → Summary generation (Prompt #2 with key sentences)

- **Critical path**: PubMed IDs → Signature filtering → GO rewriting → Vectorization → Clustering → Key sentence selection → LoRA fine-tuning → Summary generation

- **Design tradeoffs**:
  - BioBERT vs GPT-3.5 embeddings: BioBERT offers better domain specificity but slightly lower performance than GPT-3.5 despite larger parameter count
  - K-means vs other clustering: K-means provides interpretability but may struggle with non-spherical clusters
  - ChatGPT vs rule-based GO expansion: ChatGPT offers flexibility but introduces potential variability

- **Failure signatures**:
  - Low ROUGE-2 scores indicate poor capture of multi-word biomedical concepts
  - High variance in summary length suggests inconsistent key sentence selection
  - Low CH scores across all k values indicate poor clustering structure

- **First 3 experiments**:
  1. Run signature filtering on a small gene dataset and manually verify filtered sentences contain gene-specific terms
  2. Test GO expansion with ChatGPT using the provided prompt and check sentence length and aspect coverage
  3. Perform clustering with k=3,5,7,10 and plot CH scores to verify optimization works as expected

## Open Questions the Paper Calls Out

### Open Question 1
How do different clustering algorithms (e.g., hierarchical clustering, DBSCAN) compare to K-means in terms of performance for selecting key sentences in GeneSUM? The paper mentions using K-means for clustering but does not explore alternative clustering methods or compare their performance. Experimental results comparing K-means with other clustering algorithms on the same dataset would resolve this question.

### Open Question 2
How does GeneSUM perform when applied to genes from non-human species or different biological domains? The paper explicitly states that the dataset consists of 8,887 human genes, but does not test the framework on genes from other species or biological domains. Applying GeneSUM to genes from model organisms (e.g., mice, fruit flies) or plants would demonstrate generalizability.

### Open Question 3
How does GeneSUM handle genes with limited or no associated literature, and what is the minimum threshold of literature required for reliable summarization? The paper does not address scenarios where genes have sparse or no literature, which could limit the framework's applicability. Systematic evaluation of GeneSUM on genes with varying levels of literature availability would identify performance degradation points.

## Limitations
- Evaluation relies solely on ROUGE metrics against NCBI Entrez Gene summaries, which may not capture full semantic quality or factual accuracy
- Lacks ablation studies comparing alternative filtering methods, embedding models, or clustering approaches
- Signature term filtering effectiveness depends on assumption that chi-square-selected terms effectively discriminate gene-specific literature - this assumption remains unverified
- Use of ChatGPT for GO expansion introduces variability that could affect reproducibility

## Confidence
- **ROUGE performance claims**: Medium confidence - improvements are quantified but evaluated only against a single reference dataset without human validation or alternative baselines
- **Hallucination mitigation**: Low confidence - while the paper claims traceable summaries through source sentence selection, there's no systematic evaluation of factual accuracy or comparison with other LLM approaches
- **Generalizability**: Low confidence - the framework is tested only on human genes with existing Entrez Gene summaries, limiting claims about applicability to other species or genes without curated references

## Next Checks
1. **Manual evaluation of summary quality**: Have domain experts assess 50 randomly selected GeneSUM summaries against reference descriptions for factual accuracy, completeness, and coherence, focusing on multi-word biomedical concepts that ROUGE-2 is designed to capture
2. **Ablation study of filtering mechanisms**: Compare GeneSUM performance with alternative filtering approaches (TF-IDF, mutual information) and embedding models (GPT-3.5, PubMedBERT) to isolate the contribution of each component to the final ROUGE scores
3. **Cross-species validation**: Apply GeneSUM to non-human genes (mouse, rat) and evaluate whether the signature term filtering and GO expansion mechanisms maintain effectiveness when applied to literature with different domain characteristics and reference summary styles