---
ver: rpa2
title: Few-Shot Adversarial Prompt Learning on Vision-Language Models
arxiv_id: '2403.14774'
source_url: https://arxiv.org/abs/2403.14774
tags:
- adversarial
- prompt
- natural
- learning
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of adversarial robustness in
  vision-language models by proposing a few-shot adversarial prompt learning framework.
  The core idea is to learn adversarially correlated text supervision end-to-end from
  adversarial examples, rather than relying on static hand-crafted prompts.
---

# Few-Shot Adversarial Prompt Learning on Vision-Language Models

## Quick Facts
- arXiv ID: 2403.14774
- Source URL: https://arxiv.org/abs/2403.14774
- Authors: Yiwei Zhou; Xiaobo Xia; Zhiwei Lin; Bo Han; Tongliang Liu
- Reference count: 40
- Primary result: Achieves 38.05% adversarial accuracy on base classes and 21.86% on new classes using only 1% of training data

## Executive Summary
This paper addresses adversarial robustness in vision-language models (VLMs) through a novel few-shot adversarial prompt learning framework. The key innovation lies in learning adversarially correlated text supervision end-to-end from adversarial examples, rather than relying on static hand-crafted prompts. By introducing a training objective that encourages consistency of multi-modal features while differentiating between natural and adversarial examples, the method significantly improves robustness. The approach matches state-of-the-art zero-shot performance while requiring only 1% of the training data typically needed.

## Method Summary
The proposed framework learns adversarially correlated text supervision by jointly optimizing over both natural and adversarial examples. The method introduces a multi-modal consistency objective that encourages feature alignment between image and text representations while explicitly accounting for adversarial perturbations. During training, the model generates adversarial examples and uses them to learn robust prompt embeddings that maintain consistency across modalities even under attack. This end-to-end learning approach replaces static prompt engineering with dynamically learned text supervision that is inherently robust to adversarial examples.

## Key Results
- Achieves 38.05% average adversarial accuracy on base classes in base-to-new generalization setting
- Reaches 21.86% adversarial accuracy on new classes, significantly outperforming previous methods
- Matches state-of-the-art zero-shot performance while using only 1% of training data

## Why This Works (Mechanism)
The method works by learning text supervision that is intrinsically correlated with adversarial examples rather than learning to resist attacks through standard adversarial training. By generating adversarial examples during training and using them to shape the learned prompt embeddings, the model develops text representations that maintain multi-modal consistency even when images are perturbed. This approach addresses the fundamental challenge that standard VLMs rely on hand-crafted prompts that lack adversarial robustness, by making the text supervision itself adaptive to adversarial conditions.

## Foundational Learning

**Adversarial examples**: Input data deliberately modified to fool machine learning models while appearing normal to humans. Needed to understand the threat model and evaluate robustness; quick check: verify attacks maintain perceptual similarity to clean inputs.

**Vision-Language Models**: AI systems that process both visual and textual information to perform tasks like image classification. Needed as the target architecture for robustness improvements; quick check: confirm model can perform zero-shot classification before adding adversarial training.

**Prompt engineering**: The practice of designing text inputs to guide model behavior. Needed to understand why static prompts fail under adversarial conditions; quick check: test model performance drop when using different hand-crafted prompts.

## Architecture Onboarding

**Component map**: Input images -> Adversarial attack generator -> Vision encoder -> Text prompt generator -> Cross-modal attention -> Consistency loss -> Text encoder -> Final classification

**Critical path**: The adversarial prompt learning relies on the feedback loop between generated adversarial examples and the learned text supervision. The model must balance feature consistency across modalities while maintaining discriminative power between natural and adversarial examples.

**Design tradeoffs**: The method trades computational overhead during training (generating adversarial examples) for improved inference-time robustness. The few-shot approach sacrifices some accuracy for efficiency, though results show this tradeoff is favorable.

**Failure signatures**: The model may overfit to specific attack patterns if training data is limited, or fail to generalize to unseen attack types. Performance degradation may occur when the ratio of natural to adversarial examples shifts significantly from training conditions.

**First experiments**: 1) Test baseline VLM performance on clean and adversarial examples to establish vulnerability; 2) Evaluate learned prompts on out-of-distribution images to check generalization; 3) Measure feature consistency between natural and adversarial examples before and after prompt learning.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, focusing instead on demonstrating the effectiveness of the proposed method.

## Limitations

The experimental evaluation focuses primarily on base-to-new generalization settings, leaving open questions about performance in other transfer learning scenarios. The reliance on specific adversarial attack methods (BIM and FGSM) may limit generalizability to other attack types or real-world threat models. The efficiency claims (1% training data) lack thorough analysis of sensitivity to different few-shot training set sizes.

## Confidence

- High confidence in the core methodological contribution of learning adversarially correlated text supervision end-to-end
- Medium confidence in the claimed performance improvements given strong results but limited comparison with emerging robust VLMs
- Medium confidence in the efficiency claims due to lack of sensitivity analysis

## Next Checks

1. Evaluate the method's robustness against adaptive attacks specifically designed to circumvent the learned adversarial prompts
2. Conduct ablation studies to isolate the contribution of each component in the multi-modal consistency objective
3. Test the method on additional vision-language benchmarks beyond the current dataset to assess generalizability claims