---
ver: rpa2
title: Long-Term Fair Decision Making through Deep Generative Models
arxiv_id: '2401.11288'
source_url: https://arxiv.org/abs/2401.11288
tags:
- time
- fairness
- data
- decision
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses long-term fairness in sequential decision-making
  systems by proposing a novel three-phase learning framework. The core idea is to
  use a deep generative model to simulate the system dynamics and generate both observational
  and interventional data distributions.
---

# Long-Term Fair Decision Making through Deep Generative Models

## Quick Facts
- arXiv ID: 2401.11288
- Source URL: https://arxiv.org/abs/2401.11288
- Reference count: 6
- Primary result: A three-phase learning framework using deep generative models achieves better balance between long-term fairness, local fairness, and utility compared to traditional fairness methods.

## Executive Summary
This paper addresses the challenge of long-term fairness in sequential decision-making systems where current decisions affect future outcomes. The authors propose a novel three-phase framework that uses a recurrent conditional generative adversarial network (RCGAN) to simulate system dynamics and generate both observational and interventional data distributions. By training a decision model on this generated data using a regularized objective, the approach optimizes long-term fairness (measured as 1-Wasserstein distance between demographic groups at future time steps) while maintaining utility and local fairness.

## Method Summary
The proposed method consists of three phases: (1) Train a base classifier hω on real data to approximate the decision-making mechanism, (2) Train an RCGAN to generate high-fidelity time series data based on the causal structure, and (3) Train a fair decision model hθ using performative risk minimization with repeated gradient descent on the generated interventional data. The approach formulates the optimization as a performative risk minimization problem, where the decision model parameters affect the data distribution, which in turn affects the loss.

## Key Results
- The DeepLF method achieves a more effective balance between long-term fairness, local fairness, and utility compared to methods based on traditional fairness notions
- Experiments on both synthetic (SimLoan) and semi-synthetic (Taiwan) datasets show improved performance across all three metrics
- The approach successfully reconciles demographic parity and equal opportunity through the 1-Wasserstein distance metric

## Why This Works (Mechanism)

### Mechanism 1
Generative models trained on causal structure can produce interventional distributions that balance long-term fairness and utility. By modeling system dynamics with a recurrent conditional GAN whose architecture mirrors the temporal causal graph, the generator learns to simulate how decisions affect future features. This allows training on both observational and interventional data, enabling optimization of long-term fairness while maintaining utility. The core assumption is that the causal structure is correctly specified and the RCGAN can approximate the true data generation mechanism.

### Mechanism 2
Repeated gradient descent (RGD) can solve the performative risk minimization problem in long-term fairness optimization. RGD iteratively updates the model by generating data using current parameters, computing loss on that data, and updating parameters. This process continues until convergence, addressing the dependency between model and data. The core assumption is that the performative risk is convex or well-behaved enough for RGD to converge to a good solution.

### Mechanism 3
The 1-Wasserstein distance metric enables reconciliation between demographic parity and equal opportunity in long-term fairness. This metric measures the difference between interventional distributions of different groups at a future time step. Minimizing this distance while constraining local fairness can lead to both demographic parity and equal opportunity being satisfied at the future time step. The core assumption is that the decision model is Lipschitz continuous and satisfies the equal base rate condition.

## Foundational Learning

- **Temporal Causal Graphs**: Needed to model sequential decision-making processes and feedback loops between decisions and features over time. Quick check: What is the key difference between a temporal causal graph and a standard causal graph?

- **Structural Causal Models (SCM)**: Provides formal framework for defining interventions and counterfactuals in sequential decision-making systems. Quick check: What is the difference between a hard intervention and a soft intervention in an SCM?

- **Performative Risk Minimization**: Addresses optimization problems where the decision model affects the data distribution, which affects the loss. Quick check: How does performative risk minimization differ from traditional empirical risk minimization?

## Architecture Onboarding

- **Component map**: Real data -> Base classifier hω -> RCGAN (generator + discriminator) -> Generated data -> Decision model hθ -> Performative risk minimization with RGD

- **Critical path**: 1. Train classifier hω on real data, 2. Train RCGAN using hω as part of generator, 3. Replace hω with hθ in RCGAN, 4. Use RGD to optimize hθ on generated data

- **Design tradeoffs**: Model complexity vs. training stability (more complex models may better capture data distribution but be harder to train), balancing fairness metrics (long-term fairness vs. local fairness vs. utility), data generation tradeoff between high-fidelity data and computational cost

- **Failure signatures**: RCGAN mode collapse (generator produces limited variety), RGD divergence (loss increases or oscillates), poor long-term fairness (1-Wasserstein distance remains high despite optimization)

- **First 3 experiments**: 1. Train RCGAN on synthetic data with known causal structure; evaluate data fidelity using MMD, 2. Train hθ using RGD on generated data; measure convergence and loss stability, 3. Evaluate long-term fairness of hθ on test data; compare 1-Wasserstein distance to baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
How can the feasibility of achieving long-term fairness be theoretically analyzed? The paper acknowledges that long-term fairness may not always be achievable only through updating the decision model, but does not provide theoretical analysis of when it is feasible. A mathematical proof or counterexample demonstrating conditions for achievability would resolve this.

### Open Question 2
How does the performance of the proposed method compare to other methods that use traditional fairness notions like demographic parity and equal opportunity? While the paper mentions improved balance, it lacks detailed experimental results or analysis of performance differences with traditional fairness methods.

### Open Question 3
How does the proposed method handle the trade-off between long-term fairness, local fairness, and utility in real-world scenarios? The paper mentions utilizing the learning algorithm to achieve balance but does not provide detailed analysis of how it handles this trade-off in real-world scenarios.

## Limitations

- Heavy reliance on synthetic and semi-synthetic datasets where ground truth classifiers are available, limiting generalizability to real-world scenarios
- Performance metrics depend on 1-Wasserstein distance at specific time step T without rigorous justification for the choice of T
- Computational complexity of the three-phase approach, particularly RGD algorithm, is not discussed in detail

## Confidence

- **High confidence**: Theoretical framework for performative risk minimization and RCGAN architecture design
- **Medium confidence**: Experimental results showing improved balance on synthetic datasets
- **Low confidence**: Claims about real-world applicability based on semi-synthetic experiments

## Next Checks

1. Implement the method on a real-world sequential decision-making dataset (e.g., criminal justice or lending data) without access to ground truth classifiers to assess practical applicability
2. Conduct ablation studies to determine sensitivity of results to the choice of time step T for measuring long-term fairness
3. Evaluate computational efficiency of RGD algorithm compared to standard gradient descent in terms of training time and convergence stability