---
ver: rpa2
title: 'MedHalu: Hallucinations in Responses to Healthcare Queries by Large Language
  Models'
arxiv_id: '2409.19492'
source_url: https://arxiv.org/abs/2409.19492
tags:
- hallucination
- answer
- llms
- medical
- healthcare
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MED HALU, the first benchmark to study hallucinations
  in LLM-generated responses to real-world healthcare queries. The authors create
  this dataset by generating hallucinated answers to questions from three public healthcare
  datasets using GPT-3.5, with detailed annotation of hallucination types and text
  spans.
---

# MedHalu: Hallucinations in Responses to Healthcare Queries by Large Language Models

## Quick Facts
- arXiv ID: 2409.19492
- Source URL: https://arxiv.org/abs/2409.19492
- Reference count: 40
- First benchmark for studying hallucinations in LLM-generated healthcare responses

## Executive Summary
This paper introduces MED HALU, the first benchmark to study hallucinations in LLM-generated responses to real-world healthcare queries. The authors create this dataset by generating hallucinated answers to questions from three public healthcare datasets using GPT-3.5, with detailed annotation of hallucination types and text spans. They then evaluate the hallucination detection capabilities of three groups: medical experts, LLMs, and laypeople. Results show that LLMs perform significantly worse than medical experts and only as well as laypeople in detecting hallucinations. To improve LLM performance, the authors propose an expert-in-the-loop approach that infuses expert reasoning into prompts, achieving a 6.3% macro-F1 improvement for GPT-4.

## Method Summary
The authors create the MED HALU dataset by generating hallucinated answers to healthcare questions from three public datasets (HealthQA, LiveQA, MedicationQA) using GPT-3.5. They generate three types of hallucinations: input-conflicting, context-conflicting, and fact-conflicting. The dataset is annotated by medical experts and laypeople to identify hallucinated text spans. The authors then evaluate three LLM models (LLaMA-2, GPT-3.5, GPT-4) for hallucination detection using prompt-based evaluation. They propose an expert-in-the-loop approach that improves detection by infusing expert reasoning into the prompts, achieving a 6.3% macro-F1 improvement for GPT-4.

## Key Results
- LLMs detect hallucinations with 46.8% macro-F1, significantly worse than medical experts (76.8%) but similar to laypeople (46.5%)
- Expert-in-the-loop approach improves GPT-4 hallucination detection by 6.3% macro-F1
- Context-conflicting hallucinations are easiest to detect (77.6% accuracy) while input-conflicting are hardest (47.3% accuracy)
- Medical experts and LLMs perform significantly worse on LiveQA dataset compared to HealthQA and MedicationQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert-in-the-loop approach improves LLM hallucination detection by infusing domain knowledge and reasoning.
- Mechanism: When expert reasoning is provided alongside healthcare queries and responses, LLMs can better detect hallucinations by cross-referencing the expert's analysis with their own understanding.
- Core assumption: Expert reasoning captures the key indicators of hallucinations that LLMs miss without such guidance.
- Evidence anchors:
  - [abstract]: "We propose an expert-in-the-loop approach to improve hallucination detection through LLMs by infusing expert reasoning into the prompt."
  - [section]: "We observe that LLMs are much worse than the experts... To fill this gap, we propose expert-in-the-loop approach to improve medical hallucination detection through LLMs by infusing expert reasoning into the prompt."
  - [corpus]: Found related work on "CHECK" framework which also uses expert knowledge for hallucination detection in medical domain.
- Break condition: If expert reasoning becomes too complex or verbose, it may overwhelm the LLM's context window, reducing effectiveness.

### Mechanism 2
- Claim: Hallucination detection performance varies significantly across different hallucination types.
- Mechanism: LLMs can detect certain hallucination types (like context-conflicting) more easily than others because they involve self-contradictions that are detectable through pattern matching.
- Core assumption: The nature of hallucination types creates varying levels of difficulty for detection algorithms.
- Evidence anchors:
  - [section]: "Table 7 shows the results for each hallucination type... GPT-3.5 and GPT-4 give clear indications of detecting context-conflicting hallucination the best for all the 3 subsets."
  - [section]: "Intuitively, it makes sense as well since it is easier to detect self-conflicts in context-conflicting hallucinations just by looking at the LLM-generated healthcare responses."
  - [corpus]: Found related work on "MedHallBench" which specifically assesses different types of hallucinations in medical LLMs.
- Break condition: If hallucination patterns become more subtle or sophisticated, the detection performance gap between types may narrow or reverse.

### Mechanism 3
- Claim: Healthcare query complexity affects hallucination detection difficulty for all evaluators.
- Mechanism: Real-world healthcare queries contain ambiguity and incomplete information that increases hallucination risk and detection difficulty.
- Core assumption: The complexity and ambiguity in real-world queries create conditions where hallucinations are more likely and harder to detect.
- Evidence anchors:
  - [abstract]: "Interactions with real-world users often involve ambiguous or incomplete queries, requiring the model to interpret the missing context, which increases the risk of generating hallucinated information."
  - [section]: "The questions exhibit characteristics such as 1) ambiguity: open-ended or vague queries that require inference of additional context; 2) incomplete information: questions lacking necessary details."
  - [corpus]: Found related work on "Retrieval-Augmented Generation" which addresses similar challenges in healthcare queries.
- Break condition: If queries become overly simple or standardized, the complexity effect may diminish, making detection easier across the board.

## Foundational Learning

- Concept: Binary classification with macro-averaged metrics
  - Why needed here: Hallucination detection is framed as a binary classification task (hallucination present or not), and macro-averaging ensures fair evaluation across all hallucination types.
  - Quick check question: What is the difference between macro-F1 and weighted-F1 scores, and why is macro-F1 more appropriate for this study?

- Concept: Edit distance for span similarity measurement
  - Why needed here: To evaluate how well LLMs identify specific hallucinated text spans compared to expert annotations.
  - Quick check question: How does edit distance handle cases where the LLM identifies a related but not identical span of text as hallucinated?

- Concept: Stratified sampling for dataset creation
  - Why needed here: To ensure the sampled dataset maintains the same distribution of hallucination types as the full dataset.
  - Quick check question: What would happen to the evaluation results if simple random sampling were used instead of stratified sampling?

## Architecture Onboarding

- Component map:
  Healthcare query -> Hallucination generation -> Expert annotation -> LLM detection -> Expert-in-the-loop enhancement -> Performance evaluation

- Critical path: Healthcare query → Hallucination generation → Expert annotation → LLM detection → Expert-in-the-loop enhancement → Performance evaluation

- Design tradeoffs:
  - Using GPT-3.5 for hallucination generation vs. more advanced models: Simpler model may produce more obvious hallucinations, making detection easier
  - Sampling 5% for human evaluation vs. full dataset: Balances cost with statistical significance
  - Expert-in-the-loop approach vs. fine-tuning: Prompt engineering is more flexible but may have lower performance ceiling

- Failure signatures:
  - Low agreement between LLM detections and expert annotations (high edit distance)
  - Similar performance between laypeople and LLMs indicating lack of domain knowledge utilization
  - Poor performance on LiveQA dataset suggesting difficulty with real-world queries

- First 3 experiments:
  1. Compare hallucination detection performance across the three hallucination types (input-conflicting, context-conflicting, fact-conflicting) to identify which type is easiest/hardest for LLMs.
  2. Test different prompt engineering strategies for the expert-in-the-loop approach to optimize hallucination detection performance.
  3. Evaluate whether providing multiple expert reasons per query improves LLM detection compared to a single expert reason.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs in detecting hallucinations vary when exposed to multilingual healthcare queries beyond English?
- Basis in paper: [inferred] The paper focuses on English medical queries in textual formats, with a suggestion for future research to explore non-English languages and alternative modalities.
- Why unresolved: The study's scope is limited to English healthcare queries, and there is no empirical data on how LLMs perform with multilingual inputs or different modalities such as medical videos.
- What evidence would resolve it: Conducting experiments with multilingual healthcare queries and multimodal data to compare LLM performance across different languages and formats would provide insights into their generalizability.

### Open Question 2
- Question: What are the long-term effects of incorporating expert-in-the-loop feedback on the hallucination detection capabilities of LLMs?
- Basis in paper: [explicit] The paper proposes an expert-in-the-loop approach to improve hallucination detection, noting significant performance gains, but does not explore the long-term impact of this approach.
- Why unresolved: The study provides initial evidence of improvement but does not track changes over time or the sustainability of these enhancements.
- What evidence would resolve it: Longitudinal studies tracking LLM performance in hallucination detection over extended periods with continuous expert feedback would clarify the long-term effects and potential for sustained improvement.

### Open Question 3
- Question: How do different types of hallucinations (input-conflicting, context-conflicting, fact-conflicting) impact the trust levels of laypeople when using LLM-generated healthcare information?
- Basis in paper: [explicit] The paper categorizes hallucinations into three types and notes that laypeople are more vulnerable to hallucinated healthcare responses, but does not measure trust levels.
- Why unresolved: While the paper identifies hallucination types and their detection challenges, it does not investigate the psychological impact on user trust.
- What evidence would resolve it: Surveys or experiments measuring layperson trust levels when exposed to different types of hallucinations would provide insights into the psychological impact and help prioritize which hallucination types to address.

## Limitations

- The MED HALU dataset was generated using GPT-3.5, which may not represent hallucination patterns in more advanced models like GPT-4 or Claude
- Limited details about human evaluator qualifications and inter-rater reliability scores
- Expert-in-the-loop approach relies on human experts, raising scalability concerns for real-time clinical applications
- Macro-F1 and edit distance metrics may not fully capture the clinical impact of hallucinations

## Confidence

- **High Confidence**: Medical experts significantly outperform both LLMs and laypeople in hallucination detection
- **Medium Confidence**: Expert-in-the-loop approach shows 6.3% improvement for GPT-4, but long-term scalability remains uncertain
- **Low Confidence**: Generalizability to real-world clinical settings due to synthetic hallucination generation process

## Next Checks

1. **External Validation with Real Clinical Data**: Test the hallucination detection framework using naturally occurring LLM outputs from actual clinical interactions rather than synthetically generated hallucinations to validate real-world applicability.

2. **Cross-Specialty Evaluation**: Expand expert evaluation to include diverse medical specialties to assess whether hallucination detection performance varies by domain and whether the expert-in-the-loop approach needs specialization.

3. **Longitudinal Performance Assessment**: Conduct a longitudinal study to evaluate whether LLMs show improved hallucination detection capabilities over time with continued expert feedback exposure.