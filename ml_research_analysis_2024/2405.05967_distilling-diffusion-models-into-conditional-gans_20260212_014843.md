---
ver: rpa2
title: Distilling Diffusion Models into Conditional GANs
arxiv_id: '2405.05967'
source_url: https://arxiv.org/abs/2405.05967
tags:
- diffusion
- distillation
- image
- conference
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Diffusion2GAN, a method to distill a complex
  multi-step diffusion model into a single-step conditional GAN, dramatically accelerating
  inference while preserving image quality. The approach interprets diffusion distillation
  as a paired image-to-image translation task, using noise-to-image pairs from the
  diffusion model's ODE trajectory.
---

# Distilling Diffusion Models into Conditional GANs

## Quick Facts
- arXiv ID: 2405.05967
- Source URL: https://arxiv.org/abs/2405.05967
- Authors: Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, Taesung Park
- Reference count: 40
- One-line primary result: Introduces Diffusion2GAN, a method to distill multi-step diffusion models into single-step conditional GANs, outperforming one-step diffusion distillation models on COCO benchmark

## Executive Summary
This paper introduces Diffusion2GAN, a method to distill a complex multi-step diffusion model into a single-step conditional GAN, dramatically accelerating inference while preserving image quality. The approach interprets diffusion distillation as a paired image-to-image translation task, using noise-to-image pairs from the diffusion model's ODE trajectory. A key contribution is E-LatentLPIPS, a perceptual loss operating directly in the diffusion model's latent space, utilizing an ensemble of augmentations for efficient regression loss computation. Additionally, the method adapts a diffusion model to construct a multi-scale discriminator with a text alignment loss, building an effective conditional GAN-based formulation.

## Method Summary
Diffusion2GAN distills a pre-trained diffusion model into a single-step conditional GAN by first generating noise-to-image pairs through ODE trajectory simulation. The generator is trained using E-LatentLPIPS, a perceptual loss function operating in the diffusion model's latent space, which is calibrated using the BAPPS dataset. A multi-scale conditional discriminator is built by adapting the pre-trained diffusion U-Net with noise conditioning and multiple prediction heads. The training follows a two-stage approach: first optimizing the regression loss with E-LatentLPIPS, then fine-tuning with both E-LatentLPIPS and GAN losses. The method is demonstrated on Stable Diffusion 1.5 and SDXL-Base-1.0, showing superior performance on zero-shot COCO benchmarks compared to other one-step diffusion distillation models.

## Key Results
- One-step generator outperforms cutting-edge one-step diffusion distillation models (DMD, SDXL-Turbo, and SDXL-Lightning) on the zero-shot COCO benchmark
- Achieves lower FID and higher CLIP scores while maintaining diversity
- E-LatentLPIPS converges more efficiently than many existing distillation methods, even accounting for dataset construction costs
- Multi-scale conditional discriminator with pre-trained diffusion weights improves realism and stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interpreting diffusion distillation as paired image-to-image translation enables use of perceptual losses and conditional GANs.
- Mechanism: By simulating ODE trajectories of a pre-trained diffusion model to obtain noise-to-image pairs, the problem becomes supervised (paired) instead of unsupervised (unpaired), allowing leverage of established image translation tools.
- Core assumption: The noise-image pairs from ODE simulation are sufficiently aligned with the teacher model's distribution to enable effective learning.
- Evidence anchors:
  - [abstract] "Our approach interprets diffusion distillation as a paired image-to-image translation task, using noise-to-image pairs of the diffusion model's ODE trajectory."
  - [section] "Our key idea is to tackle the above tasks one by one...We first find the correspondence between noises and images by simulating the ODE solver with a pre-trained diffusion model."
- Break condition: If noise-image pairs are not well-aligned (e.g., from a poor ODE simulation or stochastic sampling), the paired learning assumption breaks down.

### Mechanism 2
- Claim: E-LatentLPIPS enables perceptual loss computation directly in latent space, improving efficiency.
- Mechanism: A VGG network is trained in the latent space of Stable Diffusion, calibrated using the BAPPS dataset to match human perceptual responses, allowing LPIPS-style loss without expensive pixel-space decoding.
- Core assumption: Perceptual properties of LPIPS can transfer from pixel space to latent space despite compression.
- Evidence anchors:
  - [section] "We hypothesize that the same perceptual properties of LPIPS can hold for a function directly computed on latent space...Learning LatentLPIPS...This successfully yields a function that operates in the latent space"
  - [corpus] Weak - no corpus papers directly address latent-space perceptual losses, though E-LPIPS [39] provides ensembling inspiration.
- Break condition: If latent-space features lose too much perceptually relevant information, the calibration will fail to match human judgments.

### Mechanism 3
- Claim: Multi-scale conditional discriminator with pre-trained diffusion weights improves realism and stability.
- Mechanism: Reusing the pre-trained U-Net weights from the teacher diffusion model as a discriminator backbone, augmented with multi-scale inputs/outputs and noise conditioning, provides strong gradients and efficient training.
- Core assumption: Pre-trained diffusion weights contain useful features for discrimination and can be effectively adapted for GAN training.
- Evidence anchors:
  - [section] "We demonstrate that initializing the discriminator weights with a pre-trained diffusion model is effective for diffusion distillation...Using a pre-trained Stable Diffusion 1.5 U-Net...and fine-tuning the model as the discriminator in the latent space results in superior FID"
  - [section] "The new design enforces that all U-Net layers participate in the final prediction, ranging from shallow skip connections to deep middle blocks. This design enhances low-frequency structural consistency and significantly increases FIDs"
- Break condition: If the pre-trained features are too specialized to denoising and not general enough for discrimination, adaptation may fail.

## Foundational Learning

- Concept: ODE trajectory simulation in diffusion models
  - Why needed here: The paper's core innovation relies on simulating ODE trajectories to generate noise-to-image pairs for distillation. Understanding how DDIM or other solvers approximate the continuous-time limit of diffusion is essential.
  - Quick check question: What is the difference between a deterministic ODE solver like DDIM and a stochastic sampler like Euler-Maruyama in terms of the noise-image pairs they produce?

- Concept: Perceptual loss functions (LPIPS)
  - Why needed here: E-LatentLPIPS is built on the principle of LPIPS, which uses deep features to measure perceptual similarity. Understanding how LPIPS is calibrated and why it works better than L2 is crucial.
  - Quick check question: Why does LPIPS tend to preserve high-frequency details better than L2 loss, and how is this property used in the paper?

- Concept: Conditional GAN training dynamics
  - Why needed here: The paper uses a conditional GAN loss alongside regression loss. Understanding how conditional GANs work, the role of conditioning, and the minimax objective is necessary.
  - Quick check question: How does conditioning on both text and noise in the discriminator affect the gradients compared to unconditional GAN training?

## Architecture Onboarding

- Component map:
  Generator (G) -> E-LatentLPIPS Loss -> Multi-scale Conditional Discriminator (D) -> ODE Pair Generator -> Decoder (only for evaluation)

- Critical path:
  1. Pre-train teacher diffusion model (assumed given)
  2. Generate ODE dataset (noise-latent pairs) using DDIM
  3. Train generator with E-LatentLPIPS loss (first stage)
  4. Fine-tune with E-LatentLPIPS + GAN loss (second stage)
  5. Evaluate with decoder and standard metrics

- Design tradeoffs:
  - Using latent space vs pixel space: Latent space is faster but may lose some perceptual information; the paper addresses this with ensembling augmentations.
  - Two-stage training: First stage stabilizes learning, second stage adds realism; could be merged but may hurt stability.
  - Pre-trained discriminator: Reuses teacher weights for efficiency but may bias learning toward teacher style.

- Failure signatures:
  - High FID but low CLIP: Generator produces realistic images but poor text alignment
  - Low FID but high variance across runs: Discriminator training unstable (missing R1 regularization)
  - Wavy/patchy artifacts: LatentLPIPS not converging (missing augmentations)
  - Diversity collapse: GAN loss too strong relative to regression loss

- First 3 experiments:
  1. Train generator with only E-LatentLPIPS on small dataset (e.g., CIFAR10) to verify convergence
  2. Add single-scale discriminator (no pre-trained weights) to see if adversarial loss helps
  3. Implement multi-scale discriminator with pre-trained weights and compare FID improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can E-LatentLPIPS be further improved by exploring alternative augmentation strategies or by learning a more optimal latent space representation?
- Basis in paper: [explicit] The paper mentions using differentiable augmentations like geometric transformations and cutout, and notes that adding color-related augmentations further improves performance on larger datasets. It also shows that LatentLPIPS achieves competitive perceptual scores despite lower ImageNet classification accuracy compared to pixel-space LPIPS.
- Why unresolved: The paper uses a fixed set of augmentations and does not explore the full design space of potential improvements. The choice of augmentations and their impact on different datasets and model scales could be further optimized.
- What evidence would resolve it: Experiments comparing different augmentation strategies (e.g., different combinations, learned augmentations) and their impact on FID, CLIP-score, and perceptual metrics across various datasets and model scales.

### Open Question 2
- Question: How does the diversity of generated images scale with model size, and can this be mitigated without sacrificing image quality?
- Basis in paper: [inferred] The paper mentions that diversity drop still occurs as models scale up, and that increasing the conditional GAN loss weight improves image fidelity but decreases diversity. It also introduces a new diversity metric (DreamDiv) to quantify image variation.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between model size and diversity, nor does it explore potential solutions to maintain diversity at larger scales.
- What evidence would resolve it: Systematic experiments varying model size and analyzing the trade-off between image quality (FID, CLIP-score) and diversity (DreamDiv, other diversity metrics) across different scales. Exploration of techniques to improve diversity without sacrificing quality, such as adaptive loss weighting or novel diversity-promoting objectives.

### Open Question 3
- Question: Can Diffusion2GAN be extended to support multi-step generation, and what would be the benefits and challenges of such an extension?
- Basis in paper: [explicit] The paper mentions that Diffusion2GAN currently only supports one-step image synthesis and that extending it to multi-step generation could result in future performance improvement.
- Why unresolved: The paper does not explore the feasibility or potential benefits of multi-step generation with Diffusion2GAN. It is unclear how the current framework would need to be modified and what trade-offs would be involved.
- What evidence would resolve it: Experiments implementing and evaluating Diffusion2GAN with multi-step generation, comparing performance (FID, CLIP-score, diversity) and inference speed to the one-step version and other multi-step models. Analysis of the challenges and potential solutions for adapting the current framework to multi-step generation.

## Limitations
- The paper's reliance on a two-stage training process introduces complexity that may not generalize well to all diffusion architectures
- E-LatentLPIPS method requires careful calibration of the VGG network in latent space, with incomplete implementation details
- The cost of generating ODE datasets (12M pairs for SDXL) is substantial and not fully accounted for in efficiency comparisons

## Confidence

**High confidence**: The core mechanism of interpreting diffusion distillation as paired image-to-image translation is well-supported and theoretically sound. The improvement over baseline methods on COCO benchmarks is substantial and well-documented.

**Medium confidence**: The E-LatentLPIPS implementation details are partially specified, and while the concept is clear, exact hyperparameter choices could significantly impact results. The calibration process on BAPPS dataset is described but lacks complete implementation details.

**Low confidence**: The paper claims computational efficiency advantages over existing methods, but the cost of generating ODE datasets (12M pairs for SDXL) is substantial and not fully accounted for in efficiency comparisons.

## Next Checks

1. Replicate E-LatentLPIPS calibration: Train the latent-space VGG network on a held-out subset of the ODE dataset and evaluate its BAPPS scores against vanilla LPIPS to verify perceptual alignment claims.

2. Ablation study on discriminator conditioning: Train with unconditional discriminator, text-only conditional discriminator, and full noise+text conditioning to isolate the contribution of each conditioning signal to final image quality.

3. Dataset generation cost analysis: Measure wall-clock time for generating ODE datasets at different scales (1M, 3M, 12M pairs) and compare total training time including dataset generation against existing distillation methods to verify efficiency claims.