---
ver: rpa2
title: 'Latxa: An Open Language Model and Evaluation Suite for Basque'
arxiv_id: '2403.20266'
source_url: https://arxiv.org/abs/2403.20266
tags:
- language
- latxa
- basque
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Latxa addresses the challenge of building effective language models
  for low-resource languages like Basque by continuing pretraining of Llama 2 models
  using a carefully curated Basque corpus of 4.3M documents and 4.2B tokens. The approach
  combines existing and newly collected data sources, applying thorough deduplication
  and quality filtering.
---

# Latxa: An Open Language Model and Evaluation Suite for Basque

## Quick Facts
- arXiv ID: 2403.20266
- Source URL: https://arxiv.org/abs/2403.20266
- Reference count: 19
- Primary result: Latxa 70B achieves 61.08 average accuracy on Basque benchmarks, outperforming Llama 2 by 25.18 points

## Executive Summary
Latxa addresses the challenge of building effective language models for low-resource languages like Basque by continuing pretraining of Llama 2 models using a carefully curated Basque corpus of 4.3M documents and 4.2B tokens. The approach combines existing and newly collected data sources, applying thorough deduplication and quality filtering. The resulting Latxa family (7B, 13B, 70B parameters) outperforms all previous open models on multiple-choice benchmarks by a large margin, with the 70B variant achieving 61.08 average accuracy and outperforming Llama 2 by 25.18 points. Notably, Latxa excels in Basque language proficiency tasks, surpassing even GPT-4 Turbo in this domain, demonstrating that language model capabilities in a specific language are not solely determined by linguistic competence.

## Method Summary
The authors developed Latxa by continuing pretraining Llama 2 models with a carefully curated Basque corpus. They combined existing web-crawled Basque data with newly collected sources including legal documents, newspapers, and books. The corpus underwent extensive deduplication and quality filtering before being used to continue pretraining Llama 2 models of different sizes (7B, 13B, 70B parameters). The training process leveraged the established pretraining methodology while adapting it to the Basque language context, focusing on maximizing linguistic competence in Basque while maintaining general capabilities.

## Key Results
- Latxa 70B achieves 61.08 average accuracy on multiple-choice benchmarks, outperforming Llama 2 by 25.18 points
- Latxa family (7B, 13B, 70B) outperforms all previous open models on Basque benchmarks by a large margin
- Latxa surpasses GPT-4 Turbo in Basque language proficiency tasks, demonstrating superior Basque-specific capabilities

## Why This Works (Mechanism)
Latxa succeeds by leveraging continued pretraining on a large, high-quality Basque corpus. By building upon the strong foundation of Llama 2 and specializing it with extensive Basque data, the model develops deep linguistic competence in Basque while maintaining general language capabilities. The combination of diverse data sources (web, legal, news, books) and rigorous quality filtering ensures comprehensive coverage of Basque language use cases. The scale of the corpus (4.3M documents, 4.2B tokens) provides sufficient training signal for the model to learn nuanced Basque patterns that smaller or lower-quality datasets cannot capture.

## Foundational Learning
- **Continued Pretraining**: Adapting an existing pretrained model to a new domain/task by further training on specialized data
  - Why needed: More efficient than training from scratch, leverages existing capabilities while adding domain expertise
  - Quick check: Verify baseline model performance on target language before continued pretraining

- **Corpus Curation**: The process of collecting, cleaning, and filtering text data for training language models
  - Why needed: Quality data is critical for model performance, especially for low-resource languages
  - Quick check: Analyze token distribution and vocabulary coverage across different data sources

- **Deduplication**: Removing duplicate or near-duplicate content from training corpora
  - Why needed: Prevents overfitting to repeated content and improves training efficiency
  - Quick check: Compare vocabulary overlap between different data sources before and after deduplication

- **Multilingual Model Adaptation**: Fine-tuning or continuing pretraining multilingual models for specific languages
  - Why needed: Leverages existing multilingual knowledge while specializing for target language
  - Quick check: Measure performance improvement on target language versus original multilingual model

- **Benchmark Design**: Creating evaluation suites that accurately measure model capabilities in specific domains
  - Why needed: Ensures fair comparison and identifies genuine capability improvements
  - Quick check: Validate benchmark difficulty and coverage through human evaluation

- **Parameter Scaling**: Understanding how model size affects performance for specific tasks/languages
  - Why needed: Balances computational cost with performance gains
  - Quick check: Plot performance curves across different model sizes to identify optimal scaling

## Architecture Onboarding

**Component Map**: Llama 2 base model -> Basque corpus (4.3M docs, 4.2B tokens) -> Continued pretraining -> Latxa family (7B, 13B, 70B)

**Critical Path**: Data collection and curation -> Corpus preprocessing (deduplication, filtering) -> Continued pretraining Llama 2 -> Evaluation on Basque benchmarks

**Design Tradeoffs**: The authors chose to continue pretraining an existing strong model rather than training from scratch, trading initial development time for faster specialization. They balanced corpus diversity (web, legal, news, books) against quality, accepting computational costs of larger models (70B) for maximum performance.

**Failure Signatures**: Poor Basque performance despite large corpus size would indicate issues with data quality or filtering. Underperformance relative to baseline Llama 2 on general tasks would suggest over-specialization. Benchmark scores below expected ranges would indicate training instability or insufficient Basque exposure.

**3 First Experiments**:
1. Evaluate baseline Llama 2 performance on Basque benchmarks to establish pretraining starting point
2. Test continued pretraining on subsets of the corpus (e.g., only web data vs. full corpus) to validate curation decisions
3. Compare Latxa performance against other Basque models on individual benchmark components to identify specific capability strengths

## Open Questions the Paper Calls Out
None

## Limitations
- Data composition uncertainties exist regarding the specific makeup of newly collected documents and filtering impact on performance
- Benchmark representativeness may be limited by the scope of 9 multiple-choice evaluations
- Generalization claims lack supporting evidence from cross-linguistic transfer experiments or ablation studies

## Confidence
- Data Composition Uncertainties: Low Confidence
- Benchmark Representativeness: Medium Confidence
- Generalization Claims: Medium Confidence

## Next Checks
1. Conduct ablation studies comparing Latxa's performance when trained with different proportions of web-scraped versus curated Basque data to isolate the impact of data quality on language-specific performance.

2. Perform cross-linguistic evaluation by fine-tuning Latxa on other low-resource languages to assess whether its architecture and training approach generalizes beyond Basque.

3. Implement human evaluation studies across diverse Basque language tasks (creative writing, technical documentation, conversational dialogue) to complement the multiple-choice benchmark results and validate real-world utility.