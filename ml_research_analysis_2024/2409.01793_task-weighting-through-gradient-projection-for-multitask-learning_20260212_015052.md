---
ver: rpa2
title: Task Weighting through Gradient Projection for Multitask Learning
arxiv_id: '2409.01793'
source_url: https://arxiv.org/abs/2409.01793
tags:
- task
- tasks
- gradient
- weighting
- projection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a method for task weighting in multi-task learning
  that integrates task prioritization into the gradient projection algorithm (PCGrad).
  Their approach assigns a probability distribution to tasks and, during conflicts,
  leaves the gradient of a higher-priority task unmodified while projecting others
  onto it.
---

# Task Weighting through Gradient Projection for Multitask Learning

## Quick Facts
- arXiv ID: 2409.01793
- Source URL: https://arxiv.org/abs/2409.01793
- Reference count: 37
- Method improves multitask learning by assigning task priorities in gradient conflicts, achieving up to 4.6% NDS and 3.2% mIoU gains on nuScenes

## Executive Summary
This paper introduces a novel approach to task weighting in multitask learning by integrating task prioritization into the gradient projection algorithm (PCGrad). Unlike traditional methods that apply uniform scaling across all tasks, this approach dynamically adjusts task weights only during gradient conflicts, leaving higher-priority task gradients unmodified while projecting lower-priority ones onto them. The method is evaluated across three datasets (nuScenes, CelebA, CIFAR-100) and demonstrates consistent performance improvements over standard PCGrad.

## Method Summary
The proposed method modifies PCGrad by incorporating a priority-based weighting scheme that operates selectively during gradient conflicts. Each task is assigned a probability distribution, and when conflicts arise, the gradient of the higher-priority task remains unchanged while gradients from other tasks are projected onto it. This selective weighting strategy differs fundamentally from uniform loss scaling approaches, as it applies adjustments only in conflict scenarios rather than across all training steps. The implementation maintains compatibility with existing PCGrad frameworks while adding minimal computational overhead for priority management.

## Key Results
- nuScenes dataset: BEVFormer-Small achieves up to 4.6% NDS and 3.2% mIoU improvement over PCGrad
- CelebA dataset: Accuracy improves by 0.6% compared to baseline
- CIFAR-100 dataset: Classification accuracy increases by 0.9% relative to PCGrad

## Why This Works (Mechanism)
The method works by recognizing that not all tasks contribute equally to overall performance and that gradient conflicts are the primary source of interference in multitask learning. By preserving higher-priority task gradients during conflicts while projecting others onto them, the approach maintains task-specific optimization quality where it matters most. This selective intervention is more efficient than uniform scaling because it only modifies gradients when interference actually occurs, reducing unnecessary perturbations to well-aligned task gradients.

## Foundational Learning

### Gradient Projection in Multitask Learning
- **Why needed**: Gradients from different tasks often point in conflicting directions, causing the model to oscillate between objectives without making meaningful progress on any
- **Quick check**: Verify that conflicting gradients exist by computing cosine similarity between task gradients - values near -1 indicate severe conflicts requiring projection

### Task Prioritization in Optimization
- **Why needed**: Real-world applications often have task hierarchies where certain objectives (e.g., safety in autonomous driving) must be prioritized over others
- **Quick check**: Establish task priorities based on domain requirements or performance analysis of individual task baselines

### Probabilistic Task Weighting
- **Why needed**: Provides a flexible framework for expressing relative task importance while maintaining differentiability for gradient-based optimization
- **Quick check**: Ensure probability distributions sum to 1 and reflect actual task importance through ablation studies

## Architecture Onboarding

### Component Map
Task Gradients -> Priority Comparator -> Gradient Projector -> Model Optimizer

### Critical Path
1. Compute individual task gradients
2. Detect gradient conflicts through cosine similarity
3. Apply priority-based projection rules
4. Update model parameters with projected gradients

### Design Tradeoffs
The method trades computational simplicity for potential sensitivity to priority assignment errors. While it avoids the complexity of adaptive weighting schemes, incorrect priority ordering could lead to suboptimal performance on lower-priority tasks. The selective nature of the approach also means it may be less effective when tasks are naturally well-aligned.

### Failure Signatures
- Performance degradation on lower-priority tasks when priorities are misassigned
- Increased variance in training loss for tasks with conflicting gradients
- Potential convergence issues when multiple high-priority tasks conflict

### First Experiments
1. Run baseline PCGrad with uniform task weights on a simple multitask classification problem
2. Implement priority assignment based on individual task performance metrics
3. Compare training dynamics and final performance across different priority configurations

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions beyond the need for further validation across diverse domains and architectures.

## Limitations
- Performance gains are relatively modest (0.6-4.6%) and may not justify added complexity in all scenarios
- The method's effectiveness is primarily demonstrated on detection and classification tasks, with unclear benefits for regression-heavy problems
- Priority assignment introduces potential brittleness, as incorrect orderings could degrade performance more severely than uniform PCGrad

## Confidence
- Performance improvements on tested datasets: High
- Generalizability to other domains and architectures: Medium
- Computational efficiency compared to baselines: Low
- Theoretical properties of the gradient projection mechanism: Low

## Next Checks
1. Test the method on regression-dominant multitask problems (e.g., robotics control + perception) to evaluate its effectiveness beyond classification/detection tasks.
2. Conduct ablation studies systematically varying task priority assignments to quantify sensitivity to ordering choices.
3. Measure and report wall-clock training time overhead compared to standard PCGrad across different batch sizes and model scales.