---
ver: rpa2
title: 'Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective
  from Circuits Competition'
arxiv_id: '2402.15175'
source_url: https://arxiv.org/abs/2402.15175
tags:
- training
- size
- grokking
- data
- memorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a unified framework that explains three key
  phenomena in deep learning: grokking, double descent, and emergent abilities in
  large language models. The framework centers on the competition between memorization
  and generalization circuits within neural models.'
---

# Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition

## Quick Facts
- **arXiv ID**: 2402.15175
- **Source URL**: https://arxiv.org/abs/2402.15175
- **Reference count**: 30
- **Primary result**: A unified framework explaining grokking, double descent, and emergent abilities through competition between memorization and generalization circuits in neural models.

## Executive Summary
This paper presents a unified framework that explains three key phenomena in deep learning: grokking, double descent, and emergent abilities in large language models. The framework centers on the competition between memorization and generalization circuits within neural models. By analyzing how model size and training data quantity affect this competition, the authors identify four distinct training dynamics: progression, ungrokking, grokking, and semi-grokking. Using this framework, they provide a detailed explanation of the double descent phenomenon, predicting and experimentally verifying its occurrence. Furthermore, they extend the framework to multi-task learning, demonstrating how combining algorithm tasks with pure memorization tasks can lead to emergent abilities, offering new insights into the mechanisms behind emergent abilities in large language models.

## Method Summary
The authors use a simplified decoder-only transformer architecture (1-layer, 4 attention heads) to study the competition between memorization and generalization circuits. They conduct experiments on synthetic tasks including modular addition and subtraction, varying model size (hidden dimensions from 32-1024) and training data quantity (2000-4000 samples). The training procedure employs cross-entropy loss with AdamW optimizer, and they analyze parameter norm changes during training to identify different dynamics. For emergent abilities, they combine algorithm tasks with pure memorization tasks to observe when generalization circuits develop.

## Key Results
- Larger models exhibit grokking with less training data due to an inverse relationship between model size and critical dataset size
- Double descent occurs when training data volume is below the intersection point of memorization capacity and critical dataset size curves
- Emergent abilities in LLMs can be explained by competition between memorization and generalization circuits in multi-task learning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Larger models exhibit grokking with less training data due to an inverse relationship between model size and critical dataset size.
- **Mechanism**: The critical dataset size (DM_crit) is negatively correlated with model size, meaning larger models require less data to transition from memorization to generalization circuits.
- **Core assumption**: The efficiency of memorization circuits decreases with increasing training data size, while generalization circuit efficiency remains stable.
- **Evidence anchors**: 
  - [abstract]: "Utilizing this framework, we provide a detailed analysis of the double descent phenomenon and establish a predictive method for identifying instances of double descent occurrence."
  - [section]: "From the analysis presented in Figure 3, it is evident that models with larger hidden sizes tend to exhibit grokking with smaller datasets."
- **Break condition**: If empirical results show DM_crit increasing with model size instead of decreasing, this mechanism fails.

### Mechanism 2
- **Claim**: The double descent phenomenon occurs when training data volume is below the intersection point of memorization capacity and critical dataset size curves.
- **Mechanism**: As model size increases, the model transitions through progression (increasing performance), ungrokking (decreasing/zero performance), semi-grokking (moderate performance), and grokking (increasing performance), creating the double descent curve.
- **Core assumption**: The training dynamics can be categorized into four distinct types based on the relationship between training data size and model capacity.
- **Evidence anchors**:
  - [abstract]: "Analyzing the depicted graph, we anticipate that the function representing final validation performance with model size will exhibit varying trends, contingent upon the quantity of training data."
  - [section]: "This pattern exemplifies the double descent phenomenon (Belkin et al., 2019; Nakkiran et al., 2020)."
- **Break condition**: If models don't show the predicted progression → ungrokking → semi-grokking → grokking sequence, this mechanism fails.

### Mechanism 3
- **Claim**: Emergent abilities in large language models can be explained by the competition between memorization and generalization circuits in multi-task learning scenarios.
- **Mechanism**: When algorithm tasks are combined with pure memorization tasks, smaller models cannot develop generalization circuits for the algorithm task until reaching a sufficiently large size, creating emergent behavior.
- **Core assumption**: The presence of pure memorization tasks prevents models from transitioning to generalization circuits for algorithm tasks until they have excess capacity.
- **Evidence anchors**:
  - [abstract]: "Moreover, we expand our framework to the multi-task learning paradigm, demonstrating how algorithm tasks can be turned into emergent abilities."
  - [section]: "This phenomenon reminds us of the emergent abilities in Large Language Models (Wei et al., 2022a)."
- **Break condition**: If emergent abilities occur at smaller model sizes or don't correlate with the presence of memorization tasks, this mechanism fails.

## Foundational Learning

- **Concept**: Neural circuit competition theory
  - **Why needed here**: The framework is built on the competition between memorization and generalization circuits within neural models
  - **Quick check question**: What are the two types of circuits competing in this framework, and how do their efficiencies differ with training data size?

- **Concept**: Double descent phenomenon
  - **Why needed here**: The framework provides a unified explanation for double descent by analyzing how model size affects the training dynamics
  - **Quick check question**: What four stages do models go through in the double descent phenomenon according to this framework?

- **Concept**: Emergent abilities in large language models
  - **Why needed here**: The framework extends to explain emergent abilities by showing how multi-task learning can create emergent behavior
  - **Quick check question**: How does combining algorithm tasks with pure memorization tasks lead to emergent abilities according to this framework?

## Architecture Onboarding

- **Component map**: Memorization circuits <-> Generalization circuits
- **Critical path**: 1) Identify model size and training data volume 2) Determine the relationship between memorization capacity and critical dataset size 3) Predict the training dynamics (progression, ungrokking, grokking, or semi-grokking) 4) Analyze the resulting validation performance curve
- **Design tradeoffs**: Larger models have lower critical dataset sizes but higher memorization capacity, while smaller models have higher critical dataset sizes but lower memorization capacity. The framework must balance these tradeoffs to predict training dynamics accurately.
- **Failure signatures**: If the predicted training dynamics don't match empirical results, or if the double descent phenomenon doesn't occur as predicted, the framework may be failing. Additionally, if emergent abilities don't correlate with the presence of memorization tasks, the framework may be incomplete.
- **First 3 experiments**:
  1. Test the relationship between model size and critical dataset size using different model configurations on the modular addition task
  2. Verify the four training dynamics (progression, ungrokking, grokking, semi-grokking) by varying training data size and model size
  3. Investigate emergent abilities by combining algorithm tasks with pure memorization tasks and observing the emergence threshold in terms of model size

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do model architectures beyond 1-layer Transformers affect the competition between memorization and generalization circuits?
- **Basis in paper**: [inferred] The paper focuses on 1-layer Transformers and suggests that emergent abilities and double descent are related to the competition between memorization and generalization circuits. It notes that further experiments with more realistic tasks and models are needed.
- **Why unresolved**: The study only explores a specific model architecture (1-layer Transformer). Other architectures might exhibit different dynamics in the competition between memorization and generalization circuits.
- **What evidence would resolve it**: Experimental results comparing different model architectures (e.g., deeper Transformers, MLPs, LSTMs) on the same tasks would show whether the observed phenomena generalize across architectures.

### Open Question 2
- **Question**: What is the precise mechanism by which the efficiency of memorization circuits varies inversely with training data size?
- **Basis in paper**: [explicit] The paper states that the efficiency of the memorization circuit is inversely proportional to the volume of training data, while the efficiency of the generalization circuit remains stable regardless of training data size.
- **Why unresolved**: While the paper asserts this relationship, it doesn't provide a detailed explanation of the underlying mechanism causing this inverse relationship.
- **What evidence would resolve it**: Theoretical analysis or empirical studies that trace how memorization circuits utilize model capacity and how this utilization changes with varying amounts of training data would clarify the mechanism.

### Open Question 3
- **Question**: Can the proposed framework be extended to explain emergent abilities in tasks other than algorithmic and pure memorization tasks?
- **Basis in paper**: [explicit] The paper extends the framework to multi-task learning with algorithm tasks and pure memorization tasks, demonstrating how this combination can lead to emergent abilities. It suggests this offers a novel perspective on emergent abilities in LLMs.
- **Why unresolved**: The paper only demonstrates the framework's application to specific types of tasks. Its applicability to other types of tasks, especially those involving complex reasoning or natural language understanding, remains untested.
- **What evidence would resolve it**: Applying the framework to a diverse set of tasks (e.g., natural language inference, visual reasoning, multi-step arithmetic) and observing whether the same principles of memorization-generalization competition predict emergent abilities would validate its broader applicability.

## Limitations
- The framework's applicability beyond synthetic tasks like modular addition remains untested, raising questions about generalization to real-world datasets.
- The proposed mechanisms rely heavily on the specific architecture used (simplified decoder-only transformer) without exploring whether results hold for other architectures like MLPs or CNNs.
- The explanation for emergent abilities assumes a specific relationship between memorization and algorithm tasks that may not hold across all task combinations or domains.

## Confidence
- **High Confidence**: The explanation of grokking through circuit competition and the inverse relationship between model size and critical dataset size are well-supported by experimental evidence in synthetic settings. The double descent phenomenon analysis shows clear empirical validation with the proposed four-stage progression.
- **Medium Confidence**: The extension to emergent abilities in multi-task learning is conceptually compelling but lacks extensive empirical validation across diverse task combinations. The framework's predictions about how memorization tasks enable emergent behavior are supported by limited experiments.
- **Low Confidence**: The theoretical foundations connecting these phenomena through a unified circuit competition framework are still developing. The mathematical rigor of the proposed explanations needs strengthening, particularly for the double descent analysis.

## Next Checks
1. **Architecture Transfer Test**: Validate the framework across different neural architectures (MLPs, CNNs, RNNs) using the same synthetic tasks to determine if the circuit competition mechanism is architecture-agnostic or specific to the transformer-based setup used in the paper.
2. **Real-World Dataset Validation**: Apply the framework to natural language processing tasks like sentiment analysis or machine translation to test whether the proposed mechanisms explain performance patterns in practical scenarios, particularly examining whether the inverse relationship between model size and critical dataset size holds.
3. **Multi-Task Ablation Study**: Systematically vary the proportion of memorization versus algorithm tasks in mixed-task scenarios to empirically map the emergence threshold across different task combinations, testing whether the framework's predictions about emergent abilities scale linearly or follow a more complex relationship.