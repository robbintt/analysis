---
ver: rpa2
title: 'Leave No Knowledge Behind During Knowledge Distillation: Towards Practical
  and Effective Knowledge Distillation for Code-Switching ASR Using Realistic Data'
arxiv_id: '2407.10603'
source_url: https://arxiv.org/abs/2407.10603
tags:
- data
- speech
- knowledge
- distillation
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes K\xB2D, a framework for efficient code-switching\
  \ ASR via knowledge distillation on realistic speech-only data. K\xB2D improves\
  \ distillation by using a small auxiliary model to filter out erroneous pseudo-labels\
  \ from a large teacher model."
---

# Leave No Knowledge Behind During Knowledge Distillation: Towards Practical and Effective Knowledge Distillation for Code-Switching ASR Using Realistic Data

## Quick Facts
- arXiv ID: 2407.10603
- Source URL: https://arxiv.org/abs/2407.10603
- Reference count: 0
- Key outcome: K²D yields 2× smaller, 5× faster models that outperform teacher and baselines, achieving over 17% and 30% MER reduction for in-domain and out-of-domain testing respectively

## Executive Summary
This paper addresses the challenge of efficient code-switching automatic speech recognition (CS-ASR) by proposing K²D, a framework that leverages knowledge distillation on realistic unlabeled speech data. The key innovation is using a small auxiliary model to filter out erroneous pseudo-labels generated by a large teacher model, thereby improving distillation quality. The method achieves state-of-the-art performance on both in-domain and out-of-domain datasets while significantly reducing model size and inference time.

## Method Summary
K²D employs a three-stage framework: realistic pseudo-labeling, data pre-filtering, and knowledge distillation. The approach uses Whisper Large-v2 as the teacher model to generate pseudo-labels on 60,000 hours of unlabeled Mandarin-English code-switched speech, then employs Whisper Base as a small auxiliary validator to filter out low-quality transcriptions using cross-model validation with MER, PER, or composite distance metrics. The validated data is used to train a smaller student model (32 encoder + 2 decoder layers) via a combined cross-entropy and KL divergence loss. The method includes chunking long-form audio into ≤30s segments based on teacher timestamps and filtering out repetitive hallucinations.

## Key Results
- Achieves over 17% MER reduction on in-domain datasets (COOLTEST and NTUML2021)
- Delivers over 30% MER reduction on out-of-domain datasets (CommonVoice zh-TW and ASCEND)
- Produces student models that are 2× smaller and 5× faster than the teacher while outperforming it
- Filtering validated 55-74% of the original 60k-hour dataset depending on distance metric choice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The auxiliary small model improves pseudo-label quality by filtering out repetitive hallucinations that the teacher model produces.
- Mechanism: Cross-model validation compares teacher vs. validator outputs using n-gram repetition detection and error rate thresholds to discard low-quality pseudo-labels.
- Core assumption: The small validator model is sensitive enough to detect repetitive patterns that indicate hallucination while still agreeing closely with the teacher on valid labels.
- Evidence anchors:
  - [abstract] mentions "filter out erroneous pseudo-labels from a large teacher model"
  - [section] describes detecting repetitive generation via n-gram counting
  - [corpus] provides weak evidence; no explicit hallucination detection data in neighbors
- Break condition: If the validator model is too weak, it may fail to detect hallucinations or incorrectly filter valid labels, reducing recall.

### Mechanism 2
- Claim: Using multiple distance metrics (MER, PER, composite) allows more robust filtering across different types of errors in code-switched speech.
- Mechanism: Distance metrics quantify transcription mismatch; MER captures overall word-level accuracy, PER focuses on phoneme-level errors, and the composite metric blends both with hallucination detection.
- Core assumption: Code-switched speech errors manifest differently at word vs. phoneme levels; combining metrics covers more error types.
- Evidence anchors:
  - [section] details three distance metrics and their roles
  - [section] states "Using the direct distance metrics... gives performance improvements"
  - [corpus] lacks direct neighbor evidence for multi-metric filtering efficacy
- Break condition: If the distance thresholds are set too conservatively, useful data may be discarded; too lenient, and noisy labels remain.

### Mechanism 3
- Claim: The chunked segmentation strategy improves distillation quality by creating manageable, timestamp-aligned training segments.
- Mechanism: Long-form audio is split into ≤30s chunks based on teacher-generated timestamps, ensuring segments are natural speech units with aligned transcriptions.
- Core assumption: Teacher timestamps are accurate and natural segment boundaries improve model learning compared to random chunking.
- Evidence anchors:
  - [section] explains chunking based on timestamps and maximum duration
  - [section] notes this "facilitate segmentation behavior learning during knowledge distillation"
  - [corpus] does not contain explicit segmentation strategy comparisons
- Break condition: If timestamps are inaccurate, segments may cut off words or include silence, degrading training quality.

## Foundational Learning

- Concept: Code-switching ASR challenges
  - Why needed here: The method targets realistic code-switched data, requiring understanding of bilingual speech dynamics and evaluation metrics like MER.
  - Quick check question: What evaluation metric combines character error rate for Mandarin and word error rate for English in code-switching ASR?

- Concept: Knowledge distillation fundamentals
  - Why needed here: K²D builds on Distil-Whisper, using teacher predictions as soft labels and combining cross-entropy with KL divergence losses.
  - Quick check question: In knowledge distillation, what two types of losses are combined to train the student model?

- Concept: Semi-supervised learning with pseudo-labels
  - Why needed here: The framework generates and filters pseudo-labels from unlabeled realistic speech to enable effective distillation.
  - Quick check question: Why might directly using all pseudo-labels without filtering degrade model performance?

## Architecture Onboarding

- Component map: Teacher model (Whisper Large-v2) -> Auxiliary validator (Whisper Base) -> Student model (32 encoder, 2 decoder) -> Evaluation datasets
- Critical path:
  1. Teacher model generates pseudo-labels on unlabeled realistic data
  2. Small validator model generates validation transcriptions
  3. Cross-model validation filters out low-quality pseudo-labels using distance metrics
  4. Student model trained on validated data using combined CE and KL losses
- Design tradeoffs:
  - Smaller validator model reduces computation but may be less accurate; larger models increase filtering recall but are slower
  - Distance metric choice balances recall and precision; MER and PER differ in error sensitivity
  - Chunk size limits affect training efficiency and context modeling
- Failure signatures:
  - High MER on validation sets may indicate ineffective filtering or poor teacher predictions
  - Low data usage suggests overly aggressive filtering; high usage may mean insufficient filtering
  - Slow validation generation points to inefficient validator model selection
- First 3 experiments:
  1. Compare MER/PER/composite metrics on a small subset to tune α threshold
  2. Test validator models of different sizes to balance recall and speed
  3. Evaluate chunk size impact on downstream distillation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of K²D vary with different sizes of the auxiliary validation model, particularly when using models much smaller than Whisper Base?
- Basis in paper: [explicit] The paper uses Whisper Base as the validation model and compares it with Whisper Small and Medium, noting efficiency and robustness.
- Why unresolved: The analysis focuses on Whisper Base, Small, and Medium, but does not explore the performance with significantly smaller models or the trade-offs involved.
- What evidence would resolve it: Experiments comparing K²D performance using various smaller models (e.g., Tiny, Mini) and analyzing the impact on accuracy, efficiency, and robustness.

### Open Question 2
- Question: What is the impact of the choice of distance metric (MER, PER, composite) on the quality of the distilled model across different languages and code-switching scenarios?
- Basis in paper: [explicit] The paper introduces three distance metrics (MER, PER, composite) and shows varying performance, but does not deeply analyze their impact across different languages.
- Why unresolved: The paper provides results for specific datasets but lacks a comprehensive analysis of how each metric performs across diverse linguistic contexts and code-switching patterns.
- What evidence would resolve it: A systematic evaluation of each distance metric's effectiveness across multiple languages and code-switching scenarios, including qualitative analysis of errors.

### Open Question 3
- Question: How does K²D handle code-switching points in speech, and can it be optimized to better recognize transitions between languages?
- Basis in paper: [explicit] The paper focuses on improving CS-ASR but does not specifically address the handling of code-switching points or transitions.
- Why unresolved: The methodology improves overall ASR performance but does not explicitly target the recognition of language transitions, which is crucial in CS-ASR.
- What evidence would resolve it: Detailed analysis of K²D's performance at code-switching points, including metrics for transition accuracy and strategies to enhance language boundary detection.

## Limitations

- Dataset representativeness uncertainty: The 60,000-hour dataset composition is only partially specified, with only six subjects mentioned, limiting generalizability assessment.
- Validation filtering trade-off ambiguity: The optimal filtering threshold (α=0.4) appears somewhat arbitrary, and the relationship between filtering aggressiveness and final performance isn't thoroughly characterized.
- Out-of-domain generalization concerns: Substantial MER improvements on out-of-domain datasets are impressive but these datasets differ in multiple dimensions beyond just domain shift.

## Confidence

**High confidence**: The core distillation methodology (combining CE and KL losses, using validated pseudo-labels) is technically sound and well-established in the literature. The reported improvements in model efficiency (2× smaller, 5× faster) with maintained or improved accuracy are supported by standard evaluation protocols.

**Medium confidence**: The specific filtering mechanism using cross-model validation shows promise, but the optimal configuration appears dataset-dependent. The claim that "filtering can significantly improve distillation performance" is supported by results, but the precise contribution of each distance metric and the filtering threshold selection process warrant further investigation.

**Low confidence**: The claim that the approach "generalizes well to out-of-domain datasets" is based on limited testing across two specific datasets with different characteristics. Without systematic ablation studies isolating the effect of training data realism versus distillation methodology, it's difficult to determine whether improvements would transfer to arbitrary out-of-domain scenarios.

## Next Checks

**Validation check 1**: Perform ablation studies systematically varying the filtering threshold α across a wider range (0.2 to 0.8) on a held-out validation set to establish the sensitivity of final MER to filtering aggressiveness and identify optimal thresholds for different evaluation datasets.

**Validation check 2**: Test the approach on a truly held-out in-domain dataset from a different subject area not represented in the 60k-hour training set to isolate whether improvements stem from the distillation methodology versus domain-specific training data characteristics.

**Validation check 3**: Compare the cross-model validation filtering approach against simpler baselines such as n-gram repetition filtering alone, confidence thresholding, or no filtering to quantify the marginal benefit of the auxiliary validator model and distance metric computation.