---
ver: rpa2
title: Dual-domain Collaborative Denoising for Social Recommendation
arxiv_id: '2405.04942'
source_url: https://arxiv.org/abs/2405.04942
tags:
- social
- denoising
- interaction
- network
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of noise in both social networks
  and user-item interaction data in social recommendation systems. The authors propose
  a novel model called Dual-domain Collaborative Denoising for Social Recommendation
  (DCDSR) to tackle this issue.
---

# Dual-domain Collaborative Denoising for Social Recommendation

## Quick Facts
- arXiv ID: 2405.04942
- Source URL: https://arxiv.org/abs/2405.04942
- Reference count: 40
- One-line primary result: DCDSR achieves >10% average improvement over state-of-the-art methods on denoising social recommendation

## Executive Summary
This paper addresses the pervasive problem of noise in both social networks and user-item interaction data in social recommendation systems. The authors propose DCDSR, a dual-domain collaborative denoising approach that simultaneously cleans structural noise through mutual guidance between interaction graphs and social networks, and addresses cross-domain noise diffusion through contrastive learning with embedding perturbation. The model demonstrates significant performance gains over existing methods while showing strong noise-resistance under varying degrees of external noise attacks.

## Method Summary
DCDSR consists of two main modules: a structure-level collaborative denoising module that uses preference-guided social network denoising and social-enhanced interaction graph denoising to clean structural noise through mutual cyclic reconstruction, and an embedding-space collaborative denoising module that employs contrastive learning with dual-domain embedding collaborative perturbation to address noise cross-domain diffusion. The model is trained jointly using BPR loss for recommendation and Anchor-InfoNCE loss for contrastive learning, with experiments demonstrating over 10% average improvement on multiple real-world datasets.

## Key Results
- DCDSR outperforms state-of-the-art social recommendation methods by over 10% on average across various metrics
- The model demonstrates strong noise-resistance when facing varying degrees of external noise interaction attacks
- Experimental results on three real-world datasets (Douban-book, Yelp, Douban-movie) validate the effectiveness of the dual-domain approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The structure-level collaborative denoising module reduces structural noise by enabling the interaction graph and social network to mutually guide each other's cyclic reconstruction.
- Mechanism: The module uses preference-guided social network denoising to remove noisy social edges, then uses the denoised social network to enhance user embeddings and perform social-enhanced interaction graph denoising to remove noisy interactions. This cycle repeats to progressively clean both graphs.
- Core assumption: Social relations with high preference consistency are more likely to be true signals, and noisy interactions can be identified by checking if interacted items are compatible with the user's social-enhanced preferences.
- Evidence anchors:
  - [abstract] "In the structure-level collaborative denoising module, information from interaction domain is first employed to guide social network denoising. Subsequently, the denoised social network is used to supervise the denoising for interaction data."
  - [section] "The structural-level collaborative denoising module is used to perform denoising on original social network and interaction graph to minimize the impact of structural noise on GNN learning user/item embeddings."
  - [corpus] Weak - The corpus neighbors discuss denoising approaches but do not specifically address the dual-domain collaborative structure described here.
- Break condition: If either the interaction graph or social network contains very little signal relative to noise, the mutual guidance may reinforce incorrect patterns.

### Mechanism 2
- Claim: The embedding-space collaborative denoising module alleviates noise cross-domain diffusion through contrastive learning with dual-domain embedding collaborative perturbation.
- Mechanism: The module generates noise that simulates the knowledge distribution of the opposite domain (interaction noise for social embeddings and vice versa), subtracts it from the noisy embeddings to simulate clean embeddings, and uses these with contrastive learning to denoise.
- Core assumption: Noise from one domain propagates to the other during the structure-level denoising process, and this cross-domain noise can be modeled and removed by embedding perturbation.
- Evidence anchors:
  - [abstract] "The embedding-space collaborative denoising module devotes to resisting the noise cross-domain diffusion problem through contrastive learning with dual-domain embedding collaborative perturbation."
  - [section] "After obtaining the denoised social network G′ S and interaction graph G′ R, we use parallel GNN encoders to learn the embeddings... However, PR U , PR I and PS U are containing noise. This problem arises from the following reason: The original interaction graph GR and social network GS contain noise while manual structure-level denoising process cannot completely eliminate the noise."
  - [corpus] Weak - The corpus does not provide direct evidence about cross-domain noise diffusion or the specific embedding perturbation approach.
- Break condition: If the cross-domain noise diffusion is not the dominant source of error, or if the perturbation fails to accurately simulate the noise, the module may not improve denoising.

### Mechanism 3
- Claim: The Anchor-InfoNCE loss function improves contrastive learning by reducing gradient bias and increasing positive sample count.
- Mechanism: Anchor-InfoNCE sets the original embedding as an anchor and pulls both positive samples towards it simultaneously, while pushing apart different users/items. This avoids the gradient bias issue of InfoNCE where one positive sample may dominate.
- Core assumption: The uncertainty in data augmentation and negative sample distribution in InfoNCE causes gradient bias, and using the original embedding as an anchor mitigates this.
- Evidence anchors:
  - [abstract] "Additionally, a novel contrastive learning strategy, named Anchor-InfoNCE, is introduced to better harness the denoising capability of contrastive learning."
  - [section] "AC-InfoNCE is proposed to alleviate the gradient bias issue due to the uncertainty of embedding perturbation and the distribution of negative samples... It can, to a certain extent, mitigate the noise cross-domain diffusion, thereby enabling the perturbed embeddings to maximally simulate the raw embeddings unaffected by noise diffusion."
  - [corpus] Weak - The corpus neighbors do not discuss this specific contrastive learning loss function.
- Break condition: If the gradient bias is not a significant issue in the specific embedding space, or if the anchor-based approach introduces new biases, performance may not improve.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message passing mechanism
  - Why needed here: GNNs are the core architecture used to encode both the social network and interaction graph, and their ability to propagate information is central to both the denoising modules and the overall recommendation task.
  - Quick check question: Can you explain how a simple GCN layer aggregates information from a node's neighbors?

- Concept: Contrastive Learning and the InfoNCE loss
  - Why needed here: Contrastive learning is used in the embedding-space denoising module to pull positive samples (augmented versions of the same embedding) closer together while pushing apart negative samples. Understanding InfoNCE is crucial for grasping the improvement offered by Anchor-InfoNCE.
  - Quick check question: What is the key difference between contrastive learning and traditional supervised learning in terms of the signals used for training?

- Concept: Social Homophily Theory
  - Why needed here: The fundamental assumption that users connected in a social network tend to have similar preferences is the basis for using social information in recommendation. The denoising modules work to identify and remove the social relations that violate this assumption.
  - Quick check question: Can you give an example of a social relation that might exist but not follow the social homophily theory?

## Architecture Onboarding

- Component map: Input graphs (GR, GS) -> Structure-level Denoising (GS, then GR) -> GNN Encoding -> Embedding-space Denoising -> Joint Training (BPR + CL losses) -> Output embeddings

- Critical path: Input graphs → Structure-level Denoising (GS, then GR) → GNN Encoding → Embedding-space Denoising → Joint Training (BPR + CL losses) → Output embeddings

- Design tradeoffs:
  - Complexity vs. Performance: The dual-domain approach is more complex than single-domain denoising but offers better noise resistance.
  - Interpretability vs. Augmentation: The embedding perturbation is more interpretable than random augmentation but may be less flexible.
  - Hyperparameter Sensitivity: The model has several key hyperparameters (βs, βr, λ1/2/3, ϵ) that require tuning.

- Failure signatures:
  - If βs is too high, too many social edges are removed, losing useful signal.
  - If βr is too low, too much noise remains in the interaction graph.
  - If the perturbation magnitude ϵ is too high, the augmented samples may be too far from the original, breaking the assumption of them being "positive" samples.
  - If the CL loss weights (λ1/2/3) are too high, the recommendation task may be under-optimized.

- First 3 experiments:
  1. Ablation study: Compare performance with and without the structure-level collaborative denoising module to verify its impact.
  2. Ablation study: Compare performance with and without the embedding-space collaborative denoising module to verify its impact.
  3. Hyperparameter sweep: Test different values of βs and βr to find the optimal thresholds for denoising.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we quantify and model the noise cross-domain diffusion process in social recommendation more precisely?
- Basis in paper: [explicit] The paper discusses noise cross-domain diffusion and proposes methods to mitigate it, but does not provide a quantitative model.
- Why unresolved: While the paper acknowledges the problem and proposes solutions, it lacks a detailed quantitative analysis of how noise propagates across domains.
- What evidence would resolve it: A mathematical model that quantifies noise propagation between social and interaction domains, validated through empirical studies.

### Open Question 2
- Question: What are the optimal hyperparameters (e.g., βs, βr, λ1, λ2, λ3) for DCDSR across different types of social recommendation datasets?
- Basis in paper: [explicit] The paper mentions tuning hyperparameters but does not provide a systematic approach to determine optimal values across different datasets.
- Why unresolved: The paper demonstrates performance improvements with DCDSR but does not offer a method to determine the best hyperparameters for various datasets.
- What evidence would resolve it: A comprehensive study that tests DCDSR across multiple datasets with different characteristics to establish optimal hyperparameter settings.

### Open Question 3
- Question: How does the performance of DCDSR compare to other denoising methods in real-world scenarios with varying degrees of noise?
- Basis in paper: [explicit] The paper discusses noise resistance but does not provide a direct comparison with other denoising methods in real-world scenarios.
- Why unresolved: While the paper mentions noise resistance, it does not offer a detailed comparison of DCDSR's performance against other denoising methods in practical applications.
- What evidence would resolve it: Empirical studies comparing DCDSR's performance to other denoising methods in real-world social recommendation scenarios with different noise levels.

## Limitations

- The exact implementation details of the social-enhanced interaction graph denoising AGG function are not specified, which could affect reproducibility
- The cross-domain noise diffusion mechanism is theoretically sound but relies on assumptions about noise propagation patterns that may vary across datasets
- The Anchor-InfoNCE loss improvement is proposed but the specific gradient bias issues it addresses are not quantified in the paper

## Confidence

- **High**: The overall dual-domain architecture and its ability to outperform state-of-the-art methods (verified by experimental results)
- **Medium**: The specific mechanisms of cross-domain noise diffusion and the effectiveness of dual-domain embedding perturbation
- **Low**: The quantitative impact of the Anchor-InfoNCE loss improvement over standard InfoNCE

## Next Checks

1. **Ablation Study on Denoising Modules**: Test DCDSR performance with only structure-level denoising, only embedding-space denoising, and the full dual-domain approach to quantify the contribution of each component
2. **Noise Injection Analysis**: Systematically inject varying degrees of noise into the social network and interaction graph separately, then measure how well each denoising module recovers performance
3. **Hyperparameter Sensitivity Analysis**: Conduct a grid search over βs, βr, and the CL loss weights (λ1/2/3) to identify optimal values and assess model robustness to hyperparameter changes