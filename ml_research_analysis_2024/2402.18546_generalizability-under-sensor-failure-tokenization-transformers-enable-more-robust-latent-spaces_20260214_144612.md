---
ver: rpa2
title: 'Generalizability Under Sensor Failure: Tokenization + Transformers Enable
  More Robust Latent Spaces'
arxiv_id: '2402.18546'
source_url: https://arxiv.org/abs/2402.18546
tags:
- session
- sensor
- totem
- across
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates generalizability of neural data models under
  sensor failure, a critical issue in neuroscience experiments. The authors compare
  two models - EEGNet (CNN) and TOTEM (tokenization + transformers) - across various
  generalizability cases including within session, cross session, cross subject, and
  sensor failure.
---

# Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces

## Quick Facts
- **arXiv ID**: 2402.18546
- **Source URL**: https://arxiv.org/abs/2402.18546
- **Reference count**: 4
- **Primary result**: TOTEM (tokenization + transformers) outperforms or matches EEGNet (CNN) across all generalizability cases including sensor failure scenarios.

## Executive Summary
This paper addresses the critical challenge of generalizability in neural data models under sensor failure conditions. The authors introduce TOTEM, a novel architecture that combines tokenization via VQ-VAE with transformer classifiers, and compare it against the established EEGNet CNN architecture. Through experiments on a rich EEG dataset with 128 electrodes and 600 trials per session across two subjects, TOTEM demonstrates superior or comparable performance across multiple generalizability scenarios: within-session, cross-session, cross-subject, and particularly under sensor failure conditions. The analysis reveals that tokenization enables the model to learn more robust latent representations that generalize better across different recording conditions.

## Method Summary
The study compares two neural decoding architectures: EEGNet, a CNN-based approach with temporal and spatial convolutional layers, and TOTEM, which uses a VQ-VAE to learn a sensor-agnostic discrete codebook followed by a transformer classifier. TOTEM first learns discrete tokens from EEG signals through self-supervised pretraining, then uses these tokens as input to the transformer. The EEG dataset consists of 128 electrodes with 600 trials per session across four sessions (A1, A2, B1, B2) for two subjects. Both models are evaluated across multiple generalizability cases including baseline within-session performance, cross-session transfer, cross-subject transfer, and sensor failure scenarios ranging from 0% to 100% electrode dropout.

## Key Results
- TOTEM consistently outperforms or matches EEGNet across all generalizability cases including sensor failure.
- TOTEM shows particular advantage in cross-subject and cross-session generalization tasks.
- Analysis of TOTEM's latent codebook reveals high similarity between codewords across different subjects/sessions, indicating learned domain-invariant features.
- The tokenization approach enables robust performance even with significant sensor dropout (up to 100% failure).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokenization enables more generalizable latent representations across different subjects and sessions.
- Mechanism: TOTEM learns a sensor-agnostic latent codebook via VQ-VAE that captures temporal patterns in EEG data. These tokens act as a learned vocabulary that can reconstruct signals with low MSE across varying recording conditions.
- Core assumption: The temporal patterns in EEG signals are sufficiently consistent across subjects and sessions to be captured by a shared discrete codebook.
- Evidence anchors:
  - [abstract] "Finally through analysis of TOTEM's latent codebook we observe that tokenization enables generalization."
  - [section] "TOTEM first learns a sensor-agnostic set of discrete tokens via a self-supervised vector quantized variational autoencoder, then uses these pretrained tokens as the input to a transformer classifier."
  - [corpus] Weak - corpus neighbors focus on different applications (driving, sEMG, fMRI synthesis) with no direct evidence about EEG tokenization generalization.
- Break condition: If temporal patterns vary too widely between subjects (e.g., due to fundamental neuroanatomical differences), the shared codebook would fail to capture meaningful patterns for cross-subject generalization.

### Mechanism 2
- Claim: Transformers maintain performance under sensor failure better than CNNs due to sequence length flexibility.
- Mechanism: TOTEM's transformer architecture can handle variable input lengths and sensor configurations by operating on tokenized sequences, while EEGNet's fixed-size CNN architecture requires zero-padding of failed sensors.
- Core assumption: The tokenized representation preserves enough information about spatial relationships even when some sensors are missing.
- Evidence anchors:
  - [abstract] "We find that TOTEM outperforms or matches EEGNet across all generalizability cases."
  - [section] "These transformers are able to operate on different sequence lengths (Vaswani et al. (2017)), providing benefits for modeling trials with different lengths and sensor availabilities."
  - [corpus] Weak - corpus neighbors don't provide direct evidence about transformer performance under sensor failure in EEG contexts.
- Break condition: If the spatial relationships between sensors are critical for decoding and tokenization fails to preserve this information adequately.

### Mechanism 3
- Claim: The learned codebook creates a common representation space that enables zero-shot generalization.
- Mechanism: Codewords across different codebooks (trained on different subjects/sessions) show high similarity (low MSE), indicating the tokenization space captures domain-invariant features.
- Core assumption: The similarity in codewords reflects true functional similarity in the underlying neural signals rather than coincidental patterns.
- Evidence anchors:
  - [abstract] "Finally through analysis of TOTEM's latent codebook we observe that tokenization enables generalization."
  - [section] "Indeed, when we plot the MSE between codewords across different codebooks...we find that the codebook similarity matrices are qualitatively comparable to within code-book."
  - [corpus] Weak - corpus neighbors don't provide direct evidence about codebook similarity analysis for generalization.
- Break condition: If the low MSE between codewords is due to overfitting to specific noise patterns rather than capturing true signal structure.

## Foundational Learning

- Concept: Vector Quantized Variational Autoencoders (VQ-VAE)
  - Why needed here: VQ-VAE is the core mechanism for learning the discrete codebook that enables tokenization. Understanding how it learns discrete representations is crucial for understanding TOTEM's generalization.
  - Quick check question: How does VQ-VAE differ from standard VAE in terms of the latent space it learns?

- Concept: Transformer architectures for sequence modeling
  - Why needed here: TOTEM uses transformers after tokenization. Understanding self-attention mechanisms and how transformers handle variable sequence lengths is essential for grasping the model's flexibility.
  - Quick check question: What architectural feature of transformers allows them to handle variable sequence lengths, unlike CNNs?

- Concept: Generalization in machine learning
  - Why needed here: The paper's central contribution is about model generalizability. Understanding different types of generalization (within-distribution, cross-distribution, zero-shot) and how to measure them is crucial.
  - Quick check question: What is the difference between cross-session and cross-subject generalization in the context of EEG decoding?

## Architecture Onboarding

- Component map: Data preprocessing → VQ-VAE tokenizer → Discrete codebook → Tokenized input → Transformer classifier → Output predictions
- Critical path: VQ-VAE training → Codebook freezing → Tokenization → Transformer training/inference
  - The VQ-VAE must converge first to provide meaningful tokens for the downstream transformer
- Design tradeoffs:
  - TOTEM: More complex pipeline (tokenization + transformer) but better generalization; requires VQ-VAE training overhead
  - EEGNet: Simpler pipeline but less robust to sensor failure and cross-dataset scenarios
  - TOTEM sacrifices some within-session performance for better cross-session/cross-subject performance
- Failure signatures:
  - TOTEM: Poor performance if VQ-VAE fails to learn meaningful tokens (codebook similarity analysis would show high MSE between codebooks)
  - EEGNet: Performance degrades linearly with sensor failure; no recovery mechanism
  - Both: Overfitting if validation performance is much higher than test performance
- First 3 experiments:
  1. Train TOTEM and EEGNet on A1 within-session baseline, compare decoding accuracy
  2. Test both models on cross-session (A1→A2) and cross-subject (A1→B1) with 0% sensor failure
  3. Test both models under increasing sensor failure (0%, 10%, 30%, 70%, 100%) on cross-subject generalization task

## Open Questions the Paper Calls Out
None

## Limitations
- Small subject pool (2 subjects) despite high trial count limits generalizability to broader populations
- Results based on controlled laboratory conditions may not reflect real-world EEG variability
- Limited quantitative analysis of latent space geometry and its relationship to neural signals
- Comparison only with EEGNet, not other deep learning approaches for EEG (e.g., graph neural networks)

## Confidence

**High Confidence**: TOTEM's superior performance across generalizability cases (within-session, cross-session, cross-subject, sensor failure) is well-supported by the experimental results, with clear quantitative comparisons showing consistent advantages over EEGNet.

**Medium Confidence**: The claim that TOTEM creates "more robust latent spaces" is supported by performance metrics but would benefit from additional analysis of the latent space geometry and its relationship to underlying neural signals.

**Low Confidence**: The broader claim about tokenization + transformers being generally applicable to "time series neural data" extends beyond the EEG-specific results presented.

## Next Checks

1. **Cross-Modality Validation**: Test TOTEM on non-EEG neural time series data (e.g., fMRI, MEG, or ECoG) to evaluate whether the tokenization + transformer approach generalizes beyond scalp EEG recordings.

2. **Codebook Similarity Quantification**: Implement quantitative metrics for measuring codebook similarity (e.g., Procrustes distance, mutual information) and correlate these with classification performance to establish a more rigorous link between representation similarity and generalization capability.

3. **Ablation Study on Tokenization**: Create variants of TOTEM that use alternative tokenization methods (e.g., clustering, PCA-based discretization) or remove tokenization entirely to isolate its specific contribution to the observed performance gains.