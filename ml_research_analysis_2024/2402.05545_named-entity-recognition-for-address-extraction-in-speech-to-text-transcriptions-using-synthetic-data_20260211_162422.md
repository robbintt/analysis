---
ver: rpa2
title: Named Entity Recognition for Address Extraction in Speech-to-Text Transcriptions
  Using Synthetic Data
arxiv_id: '2402.05545'
source_url: https://arxiv.org/abs/2402.05545
tags:
- data
- dataset
- address
- municipality
- slovak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Named Entity Recognition (NER) model for
  extracting address components from speech-to-text transcriptions using the SlovakBERT
  architecture. Due to the scarcity of real data, the authors generated a synthetic
  dataset using GPT API, emphasizing the importance of mimicking spoken language variability.
---

# Named Entity Recognition for Address Extraction in Speech-to-Text Transcriptions Using Synthetic Data

## Quick Facts
- arXiv ID: 2402.05545
- Source URL: https://arxiv.org/abs/2402.05545
- Reference count: 5
- Primary result: 93.06% accuracy on real test dataset using synthetic-only training

## Executive Summary
This paper presents a Named Entity Recognition (NER) model for extracting address components from speech-to-text transcriptions in Slovak. Due to the scarcity of real training data, the authors generated a synthetic dataset using GPT API, emphasizing the importance of mimicking spoken language variability. The NER model was trained exclusively on synthetic data and evaluated on a small real test dataset, achieving 93.06% predictive accuracy. The approach involved iterative improvements in data generation, fine-tuning the SlovakBERT model, and hyperparameter optimization.

## Method Summary
The method involves generating synthetic training data using GPT API with prompts focused on address components and BIO annotation scheme, fine-tuning the SlovakBERT model on this synthetic dataset with a token classification layer for 9 entity types, and evaluating the model on a real test dataset of 69 instances. The approach is iterative, with error analysis on the real test set informing subsequent synthetic data generation to cover identified failure modes.

## Key Results
- Model achieved 93.06% predictive accuracy on real test dataset
- Synthetic data generation successfully compensated for real-world data scarcity
- Fine-tuned SlovakBERT with token classification head effectively extracted address components
- Iterative data generation and error analysis improved model performance over time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data generation compensates for real-world data scarcity in Slovak NER.
- Mechanism: Iterative prompt engineering via GPT API produces realistic address-like sentences, which are then cleaned and labeled using BIO scheme, allowing model training without real data.
- Core assumption: Synthetic sentences mimic real-world speech-to-text variability sufficiently to generalize to unseen real examples.
- Evidence anchors:
  - [abstract] "The model achieved a predictive accuracy of 93.06% on the real test dataset, demonstrating the potential of using synthetic data for training NER models..."
  - [section 2.1] "Artificial generation of training dataset occurred as the only, but still viable option to tackle the problem of data shortage."
  - [corpus] Weak evidence; corpus contains papers on synthetic data for NER but not specific to Slovak or address parsing.
- Break condition: Real-world test set shows poor generalization; synthetic data lacks realistic noise patterns (hesitations, transcription glitches) found in speech-to-text.

### Mechanism 2
- Claim: Fine-tuning SlovakBERT with token classification head enables effective NER for address components.
- Mechanism: SlovakBERT pre-trained on Slovak corpus is adapted by adding 9-class classification layer (B/I/O for Street, Housenumber, Municipality, Postcode), then trained on synthetic data.
- Core assumption: Pre-trained language understanding transfers well to address entity extraction when fine-tuned with sufficient synthetic data.
- Evidence anchors:
  - [section 2.2] "We modified both models by adding a token classification layer, obtaining in both cases models suitable for NER tasks."
  - [section 3] "Model's predictive accuracy kept increasing with systematic data generation... accuracy of 93.06%."
  - [corpus] Weak; corpus has NER with Transformers but no SlovakBERT or address extraction.
- Break condition: Model overfits synthetic data; real-world addresses contain unseen patterns or formats.

### Mechanism 3
- Claim: Iterative data generation improves model performance by addressing error patterns.
- Mechanism: Each iteration involves training, analyzing errors on real test set, then generating new synthetic data to cover identified failure modes (e.g., municipality before street).
- Core assumption: Error analysis on small real test set reveals generalizable patterns that synthetic data can be engineered to cover.
- Evidence anchors:
  - [section 2.3] "Each iteration can be split into the following steps: generate data, train a model, visualize obtained prediction errors... Based on these insights we generated new data..."
  - [section 3] "With each newly expanded dataset both of our models were trained... Eventually, the whole dataset was duplicated, with the duplicities being in uppercase/lowercase."
  - [corpus] Weak; corpus lacks iterative synthetic data approaches for NER.
- Break condition: Error patterns in real data are too diverse or context-dependent to be captured by synthetic generation.

## Foundational Learning

- Concept: BIO (Beginning-Inside-Outside) annotation scheme
  - Why needed here: Provides fine-grained token-level labels for nested and adjacent address entities in free-form text.
  - Quick check question: How would you label "Bratislava HlavnÃ¡ 1" using BIO tags?

- Concept: Knowledge distillation and model size trade-offs
  - Why needed here: Decision to use SlovakBERT over DistilSlovakBERT based on higher accuracy despite larger size.
  - Quick check question: What percentage of language understanding does DistilSlovakBERT retain compared to SlovakBERT?

- Concept: Hyperparameter tuning and overfitting prevention
  - Why needed here: Small real dataset and synthetic-only training require careful regularization to avoid memorization.
  - Quick check question: Which techniques were used to prevent overfitting during training?

## Architecture Onboarding

- Component map: GPT API -> synthetic sentence generator -> Synthetic dataset (10k+ sentences) -> training/validation/test splits -> SlovakBERT -> base transformer encoder -> Token classification head (9 neurons) -> entity label predictor -> Real test set (69 instances) -> final evaluation -> HPC (NVidia A100) -> training infrastructure

- Critical path: Synthetic data generation -> model fine-tuning -> error analysis -> iterative data expansion -> final evaluation

- Design tradeoffs:
  - Synthetic vs. real data: Speed and scale vs. authenticity
  - SlovakBERT vs. DistilSlovakBERT: Accuracy vs. efficiency
  - Case sensitivity: Retains linguistic nuance vs. robustness to input variation

- Failure signatures:
  - High confusion between municipality and street when municipality precedes street
  - Misclassification of non-entity tokens resembling entity subwords (e.g., "Kal" -> B-Municipality)
  - Overfitting: Perfect synthetic test performance but lower real test accuracy

- First 3 experiments:
  1. Train baseline SlovakBERT on initial synthetic dataset; evaluate on real test set.
  2. Analyze confusion matrix; generate new synthetic data covering most frequent error patterns.
  3. Retrain with expanded dataset; compare accuracy and inspect remaining failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance generalize to longer and more complex address structures beyond the tested examples?
- Basis in paper: [inferred] The paper mentions that the real dataset contained only 69 instances and that the synthetic data was iteratively expanded. However, it doesn't explicitly test the model's ability to handle significantly longer or more complex address formats.
- Why unresolved: The paper focuses on achieving high accuracy on the limited real test dataset but doesn't explore the model's robustness to variations in address complexity or length.
- What evidence would resolve it: Testing the model on a larger and more diverse dataset with varied address structures, including longer addresses, addresses with multiple street names, or addresses with additional components like apartment numbers.

### Open Question 2
- Question: How does the model perform when encountering out-of-vocabulary words or rare street/municipality names not present in the training data?
- Basis in paper: [explicit] The paper mentions that the synthetic data was generated using placeholders for street and municipality names, which were then replaced with randomly chosen names from a list. It also notes that the final model still had some errors, particularly in identifying tokens that should have been tagged as outside but were misclassified as municipality.
- Why unresolved: While the paper addresses data scarcity by using synthetic data, it doesn't explicitly evaluate the model's performance on rare or unseen entities.
- What evidence would resolve it: Evaluating the model on a dataset containing rare or unseen street and municipality names, or conducting an analysis of the model's predictions on out-of-vocabulary words in the test set.

### Open Question 3
- Question: How does the model's performance compare to other NER models or approaches when applied to Slovak address extraction?
- Basis in paper: [inferred] The paper mentions that there are several publicly available NER models for the Slovak language, but they don't support the specific entities relevant to address extraction. It also states that using popular LLMs like GPT is not an option due to privacy concerns and time delays.
- Why unresolved: The paper doesn't provide a direct comparison of the proposed model's performance to other NER models or approaches for Slovak address extraction.
- What evidence would resolve it: Conducting a comparative study of the proposed model against other NER models or approaches, including those based on different architectures or trained on different datasets, using the same test set or a standardized benchmark.

## Limitations
- Reliance on synthetic data without extensive validation against larger real-world datasets
- Only 69 real test instances may not generalize well to diverse address formats
- Iterative synthetic data generation process lacks transparency in prompt engineering

## Confidence

| Claim | Evidence | Confidence |
|-------|----------|------------|
| 93.06% accuracy on real test set | Reported in abstract and results | High |
| Synthetic data compensates for real data scarcity | Demonstrated by successful training and evaluation | High |
| Iterative data generation improves performance | Documented in methodology and results | Medium |
| BIO annotation scheme effectively labels address components | Used in training and evaluation | High |

## Next Checks
1. Evaluate model performance on a larger and more diverse real-world dataset with varied address structures
2. Test model's ability to handle out-of-vocabulary words and rare street/municipality names
3. Conduct a comparative study against other NER models or approaches for Slovak address extraction