---
ver: rpa2
title: Analytical results for uncertainty propagation through trained machine learning
  regression models
arxiv_id: '2404.11224'
source_url: https://arxiv.org/abs/2404.11224
tags:
- uncertainty
- input
- data
- which
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of uncertainty propagation through
  trained/fixed machine learning (ML) regression models. Analytical expressions for
  the mean and variance of the model output are obtained for certain input data distributions
  and for a variety of ML models.
---

# Analytical results for uncertainty propagation through trained machine learning regression models

## Quick Facts
- arXiv ID: 2404.11224
- Source URL: https://arxiv.org/abs/2404.11224
- Reference count: 30
- Primary result: Analytical expressions for uncertainty propagation through trained ML regression models are derived and validated, offering computational efficiency advantages over Monte Carlo sampling for certain input distributions and model sizes.

## Executive Summary
This paper presents analytical methods for propagating uncertainty through trained machine learning regression models, focusing on metrology applications. The authors derive exact expressions for the mean and variance of model outputs under specific input distributions for linear models, penalized linear regression, kernel ridge regression, Gaussian Processes, support vector machines, and relevance vector machines. The work demonstrates that these analytical approaches can be more computationally efficient than Monte Carlo sampling when the number of training samples exceeds the number of Monte Carlo trials, while providing exact results without sampling error.

## Method Summary
The authors develop analytical uncertainty propagation formulas for various ML models under specific input distributions (multivariate Gaussian, independent uniform, and independent triangular). For linear models, they derive closed-form expressions involving the input covariance matrix. For kernel-based models like GPs, they use the "kernel trick" to express results in terms of kernel evaluations between training points and input distribution moments. The analytical expressions are validated against Monte Carlo sampling on both synthetic and real-world metrology data (lithium-ion cell state-of-health estimation from Electrical Impedance Spectroscopy measurements).

## Key Results
- Analytical uncertainty propagation provides exact mean and variance for supported input distributions, eliminating Monte Carlo sampling error
- Computational efficiency advantage when training sample count n exceeds Monte Carlo trial count T, with O(n²) vs O(nT) complexity
- Analytical methods validated on real metrology data show competitive accuracy to Monte Carlo while being computationally faster for typical sample sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Analytical formulas reduce computational cost compared to Monte Carlo when n > T
- Mechanism: Analytical expressions require O(n²) kernel evaluations vs O(nT) for Monte Carlo; when n > T, analytical is faster
- Core assumption: Dominant cost is kernel evaluations and n >> T
- Evidence anchors:
  - [section] "We discovered that, for a given model, there exists an accuracy threshold below which a Monte Carlo sampling approach becomes more computationally expensive."
  - [section] "We found that as the number of Monte Carlo trials is increased beyond roughly the number of training samples, it becomes the more computationally expensive method."
- Break condition: If n is small or T is very large, Monte Carlo may be competitive

### Mechanism 2
- Claim: Analytical expressions give exact mean and variance, avoiding Monte Carlo sampling error
- Mechanism: By integrating over known input distribution analytically, expressions for E[Y*] and Var(Y*) are exact for supported distributions
- Core assumption: Input distribution accurately modeled by supported distributions and model is fixed
- Evidence anchors:
  - [abstract] "Analytical expressions for the mean and variance of the model output are obtained for certain input data distributions..."
  - [section] "The analytical expressions, on the other hand, are exact."
- Break condition: If input uncertainty cannot be well modeled by supported distributions

### Mechanism 3
- Claim: For linear models, analytical propagation is always computationally more efficient than Monte Carlo
- Mechanism: Linear model predictions y* = β^T x* require only matrix-vector products (O(m²)), while Monte Carlo requires sampling scaling with T
- Core assumption: Model is linear in parameters and input covariance is precomputed
- Evidence anchors:
  - [section] "We observe that, as expected, the analytical approach is faster even than the Monte Carlo approach with just 10^2 samples."
  - [section] "For linear models, we conclude that the analytical approach is always likely to be faster."
- Break condition: If linear model is extremely high-dimensional (m >> n) with dense Γ

## Foundational Learning

- Concept: Kernel trick and feature space transformation
  - Why needed here: Kernel-based models rely on inner products in transformed feature space; essential for deriving analytical uncertainty propagation formulas
  - Quick check question: How does the RBF kernel k(x, x') = exp(-||x-x'||^2/(2λ^2)) implicitly compute inner products in an infinite-dimensional space?

- Concept: Multivariate Gaussian distribution properties
  - Why needed here: Theorems 4 and related results require integrating RBF kernels against Gaussian inputs
  - Quick check question: Given X ~ N(μ, Σ), what is E[exp(-||X-x'||^2/(2λ^2))] in closed form?

- Concept: Numerical stability in matrix computations
  - Why needed here: Analytical formulas involve matrix inversions and determinants; poor conditioning can lead to inaccurate results
  - Quick check question: What numerical techniques can improve stability when inverting near-singular kernel matrices?

## Architecture Onboarding

- Component map: Input uncertainty modeling → Analytical mean/variance computation (Theorems 3-7) → Output uncertainty characterization. For Monte Carlo: Input sampling → Forward model evaluation → Empirical mean/variance estimation.
- Critical path: For analytical: compute lll (n kernel evals) → compute L (n^2 kernel evals) → evaluate α^T lll and α^T L α → return mean and variance. For Monte Carlo: sample T inputs → evaluate model T times → compute sample mean and variance.
- Design tradeoffs: Analytical: exact for supported distributions but restricted to certain models; Monte Carlo: flexible for any distribution/model but incurs sampling error and higher computational cost.
- Failure signatures: Analytical: inaccurate results if input distribution mis-specified or kernel matrix ill-conditioned; Monte Carlo: high variance in estimates if T is too small.
- First 3 experiments:
  1. Implement and validate Theorem 1 on simple linear regression with synthetic multivariate Gaussian inputs; compare analytical to Monte Carlo estimates.
  2. Implement Theorem 4 for GP with RBF kernel on synthetic 1D data; verify analytical mean matches model prediction at μ and variance decreases as input covariance shrinks.
  3. Benchmark analytical vs Monte Carlo propagation for GP on small dataset (n=50) with varying T; confirm analytical is faster when T > n and accuracy improves with T.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can numerical stability of analytical expressions for kernel-based methods be further improved beyond current order of 10^-7?
- Basis in paper: [explicit] Authors mention improving numerical precision is left as future work, suggesting Taylor series expansions and floating-point algorithms.
- Why unresolved: Paper doesn't provide specific methods or experiments to improve numerical stability beyond current level.
- What evidence would resolve it: Study demonstrating improved numerical precision through specific algorithmic modifications.

### Open Question 2
- Question: Can analytical uncertainty propagation methods be extended to tree-based models like random forests and neural networks?
- Basis in paper: [inferred] Authors mention tree-based models and neural networks are not covered and obtaining analytical results would be interesting future challenge.
- Why unresolved: Paper focuses on linear models and kernel-based methods, which are more amenable to analytical uncertainty quantification.
- What evidence would resolve it: Development and validation of analytical uncertainty propagation methods for tree-based models and neural networks.

### Open Question 3
- Question: How can uncertainty propagation framework be extended to incorporate model uncertainty in addition to input data uncertainty?
- Basis in paper: [explicit] Authors mention results presented are simplification of existing results that also take into account model uncertainty.
- Why unresolved: Paper focuses on uncertainty propagation through fixed models and doesn't explore methods for incorporating model uncertainty.
- What evidence would resolve it: Study demonstrating how to incorporate model uncertainty into analytical uncertainty propagation framework.

## Limitations
- Analytical results are exact only for specific input distributions (multivariate Gaussian, independent uniform, independent triangular)
- Computational efficiency advantage contingent on n > T threshold, with threshold around n ≈ T
- Approach requires careful numerical implementation to avoid instability in matrix inversions and determinant calculations

## Confidence

**High confidence**: Theoretical derivations for linear models and computational complexity analysis showing O(n²) vs O(nT) scaling; exactness of analytical expressions for supported input distributions is mathematically proven.

**Medium confidence**: Empirical validation showing analytical methods outperform Monte Carlo for n > T based on specific datasets and models tested; temperature sensitivity analysis for EIS metrology application relies on assumptions about sensitivity coefficient form.

**Low confidence**: Generalizability to arbitrary input distributions or model architectures beyond those explicitly covered (e.g., neural networks); numerical stability for high-dimensional inputs or ill-conditioned kernel matrices.

## Next Checks
1. **Numerical Stability Testing**: Implement analytical uncertainty propagation for GP with RBF kernel on synthetic 1D data; systematically test numerical stability as input covariance Σ varies, particularly for near-singular kernel matrices.

2. **Threshold Validation**: Design experiments to empirically determine n ≈ T threshold where analytical methods become more efficient than Monte Carlo; test across range of model sizes (n = 10 to 10,000) and Monte Carlo sample counts (T = 10 to 100,000).

3. **Distribution Robustness**: Test analytical approach with input distributions that deviate from supported cases (e.g., mixture distributions or heavy-tailed distributions) to assess robustness and identify failure modes where Monte Carlo methods may be preferable.