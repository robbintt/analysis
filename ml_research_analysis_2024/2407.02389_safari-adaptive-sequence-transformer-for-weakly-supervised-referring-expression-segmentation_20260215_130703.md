---
ver: rpa2
title: SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring Expression
  Segmentation
arxiv_id: '2407.02389'
source_url: https://arxiv.org/abs/2407.02389
tags:
- referring
- mask
- segmentation
- masks
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SafaRi addresses weakly supervised referring expression segmentation
  (WSRES) with limited mask and box annotations. It introduces a novel Cross-modal
  Fusion with Attention Consistency (X-FACt) module to improve image-text region-level
  alignment and spatial localization.
---

# SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation

## Quick Facts
- arXiv ID: 2407.02389
- Source URL: https://arxiv.org/abs/2407.02389
- Reference count: 40
- Key outcome: With 30% annotations, achieves 59.31 and 48.26 mIoUs on RefCOCO+@testA and testB respectively, outperforming fully-supervised SeqTR baseline

## Executive Summary
SafaRi introduces an adaptive sequence transformer approach for weakly supervised referring expression segmentation that requires only 30% of available mask and box annotations. The method combines cross-modal fusion with attention consistency regularization, bootstrapping with iterative retraining, and zero-shot spatial reasoning for proposal scoring. By leveraging limited annotations and generating high-quality pseudo-labels, SafaRi achieves state-of-the-art performance on benchmark datasets while demonstrating strong generalization to unseen tasks like referring video object segmentation.

## Method Summary
SafaRi employs a Swin transformer and RoBERTa encoder with gated cross-attention fusion layers to improve image-text region-level alignment. The Cross-modal Fusion with Attention Consistency (X-FACt) module introduces attention mask consistency regularization to constrain cross-attention within object boundaries. A bootstrapping strategy iteratively trains the model, generates pseudo-labels, filters them using zero-shot spatial reasoning (SpARC), and retrains with weighted combinations of ground truth and pseudo-masks. The SpARC module uses CLIP with visual prompting and rule-based spatial reasoning to generate bounding boxes without requiring box annotations.

## Key Results
- Achieves 59.31 and 48.26 mIoUs on RefCOCO+@testA and testB with only 30% annotations
- Outperforms fully-supervised SeqTR baseline while using significantly fewer annotations
- Demonstrates strong generalization to referring video object segmentation task
- Shows effective performance across multiple annotation percentages (10%, 20%, 30%)

## Why This Works (Mechanism)

### Mechanism 1
Cross-modal attention with attention consistency regularization improves region-level alignment in weakly supervised settings. The X-FACt module introduces normalized gated cross-attention into feature extractor layers, allowing the model to learn weighted representations that combine self-attention features with cross-modal interactions. The Attention Mask Consistency Regularization (AMCR) loss ensures cross-attention maps are spatially constrained within the object mask, preventing background attention. Core assumption: Cross-attention maps benefit from explicit regularization to focus on the referred object when ground truth annotations are limited.

### Mechanism 2
Weak supervision with γ-scheduling enables effective bootstrapping from limited annotations. The system starts with x% labeled data, trains initial model, generates pseudo-labels, filters them using zero-shot spatial reasoning (SpARC), then retrains with weighted combination of ground truth and pseudo-masks. The γ parameter increases over iterations, giving more weight to pseudo-masks as model confidence grows. Core assumption: Pseudo-masks generated by an initial model trained on limited data can be validated and used to improve the model iteratively.

### Mechanism 3
Zero-shot spatial reasoning with visual prompting improves proposal selection without requiring box annotations. The SpARC module uses CLIP with red-box visual prompts (Gaussian-blurred background with red border) to score proposals, then applies rule-based spatial reasoning to handle relationships like "left of" or "between". This generates bounding boxes without using ground truth boxes. Core assumption: CLIP combined with spatial reasoning can effectively ground referring expressions to object proposals without explicit supervision.

## Foundational Learning

- Concept: Cross-modal attention mechanisms
  - Why needed here: The task requires aligning image regions with textual descriptions, which is fundamentally a cross-modal alignment problem.
  - Quick check question: What is the difference between self-attention and cross-attention in multimodal transformers?

- Concept: Bootstrapping and semi-supervised learning
  - Why needed here: The approach starts with limited annotations and iteratively improves through self-labeling, which is a classic semi-supervised learning scenario.
  - Quick check question: How does γ-scheduling differ from standard self-training approaches?

- Concept: Vision-Language Pretrained models (VLP)
  - Why needed here: The method leverages CLIP for zero-shot proposal scoring and relies on understanding how VLPs handle spatial relationships.
  - Quick check question: What are the limitations of CLIP for spatial reasoning tasks?

## Architecture Onboarding

- Component map: Image + Text → Fused Features (X-FACt) → Sequence Transformer → Contour Points → Mask → Pseudo-label filtering (SpARC) → Retraining

- Critical path: Image encoder → Text encoder → X-FACt fusion → Sequence transformer decoder → Contour point prediction → Mask generation → SpARC filtering → Iterative retraining

- Design tradeoffs:
  - Parameter efficiency vs performance: Using gated cross-attention in existing layers vs adding separate fusion layers
  - Complexity vs effectiveness: Rule-based spatial reasoning vs learned spatial modules
  - Iteration count vs convergence: More bootstrapping iterations may improve quality but increase training time

- Failure signatures:
  - Poor cross-attention maps indicate X-FACt/AMCR issues
  - Noisy pseudo-labels suggest SpARC filtering problems
  - No improvement over iterations indicates bootstrapping instability

- First 3 experiments:
  1. Validate X-FACt improves cross-attention localization without AMCR
  2. Test SpARC proposal scoring with different visual prompts
  3. Verify γ-scheduling improves performance over fixed γ during retraining

## Open Questions the Paper Calls Out

None identified in the provided material.

## Limitations

- Performance may degrade significantly with lower annotation percentages or noisier initial training data
- SpARC's rule-based spatial reasoning may struggle with complex relational expressions or ambiguous spatial descriptions
- X-FACt introduces additional computational overhead through gated cross-attention layers without runtime comparisons

## Confidence

**High Confidence (4/5):**
- X-FACt module improves cross-modal alignment through attention consistency regularization
- The bootstrapping strategy with iterative retraining provides performance gains over single-stage training
- Zero-shot SpARC proposal scoring achieves reasonable results without box supervision

**Medium Confidence (3/5):**
- The 30% annotation threshold represents the optimal tradeoff between annotation effort and performance
- Generalization to unseen tasks (like referring video object segmentation) scales with the same architecture
- The specific γ-scheduling strategy outperforms alternative self-training approaches

**Low Confidence (2/5):**
- Performance gains would scale linearly with additional bootstrapping iterations
- The method's effectiveness extends equally well to other visual grounding tasks beyond referring expression segmentation
- The attention consistency regularization contributes more to performance than other architectural changes

## Next Checks

1. **Annotation Efficiency Analysis**: Conduct experiments with 10%, 20%, 30%, 50%, and 100% labeled data to precisely quantify the relationship between annotation percentage and performance, including statistical significance testing across multiple random seeds.

2. **SpARC Module Ablation**: Test SpARC performance with alternative visual prompting strategies (different shapes, colors, or no visual prompts) and compare rule-based spatial reasoning against learned spatial modules on a held-out test set of complex relational expressions.

3. **Bootstrapping Stability Test**: Run the iterative retraining process with different initial model checkpoints and varying numbers of iterations (1-5) to assess convergence behavior and identify the point of diminishing returns or potential performance degradation.