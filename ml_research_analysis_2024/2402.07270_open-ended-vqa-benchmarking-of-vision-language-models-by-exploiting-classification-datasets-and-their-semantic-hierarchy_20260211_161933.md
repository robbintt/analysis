---
ver: rpa2
title: Open-ended VQA benchmarking of Vision-Language models by exploiting Classification
  datasets and their semantic hierarchy
arxiv_id: '2402.07270'
source_url: https://arxiv.org/abs/2402.07270
tags:
- question
- what
- object
- output
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of evaluating text-generative
  vision-language models in open-ended Visual Question Answering (VQA). The authors
  propose a novel VQA benchmark based on well-known visual classification datasets
  (ImageNet, COCO, ActivityNet, OV AD) to enable granular evaluation and comparison
  with discriminative models.
---

# Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy

## Quick Facts
- arXiv ID: 2402.07270
- Source URL: https://arxiv.org/abs/2402.07270
- Reference count: 39
- Primary result: Novel benchmark for evaluating text-generative vision-language models using classification datasets and semantic hierarchies

## Executive Summary
This paper introduces a novel approach to benchmarking vision-language models for open-ended Visual Question Answering (VQA) tasks. The authors leverage well-established classification datasets (ImageNet, COCO, ActivityNet, OV AD) with their inherent semantic hierarchies to create a granular evaluation framework. By formulating VQA tasks based on classification objectives, they enable direct comparison between text-generative models and traditional discriminative models. The key innovation is the use of follow-up questions guided by semantic hierarchies to improve evaluation of coarse answers on fine-grained tasks.

## Method Summary
The authors propose a benchmark that repurposes classification datasets for open-ended VQA evaluation. They formulate questions that map to classification objectives (object, action, attribute) and use the semantic hierarchies within these datasets to generate follow-up questions when initial answers are too coarse. The evaluation framework compares traditional NLP metrics with LLM-based metrics against human judgments to determine the most reliable evaluation method. The benchmark is applied to a suite of vision-language models to assess their performance across different task granularities and types.

## Key Results
- Follow-up questions guided by semantic hierarchies significantly improve evaluation fairness for coarse answers on fine-grained tasks
- LLM-based metrics show promise in evaluating model predictions against ground truth answers when validated against human judgments
- Vision-language models exhibit varying performance across task types, with notable weaknesses in fine-grained attribute classification

## Why This Works (Mechanism)
The approach works by exploiting the structured nature of classification datasets and their semantic hierarchies. Classification datasets inherently contain fine-grained categories with parent-child relationships, which can be used to generate both initial questions and follow-up probes. This structure allows for systematic evaluation of whether models can provide sufficiently detailed answers and, when they don't, whether they can be guided to more precise responses through semantically relevant follow-up questions.

## Foundational Learning

### Visual Question Answering (VQA)
**Why needed**: Understanding the fundamental challenge of answering questions about images
**Quick check**: Can you explain the difference between discriminative and generative approaches to VQA?

### Semantic Hierarchies
**Why needed**: These hierarchies enable structured question generation and follow-up probing
**Quick check**: Can you identify the parent-child relationships in a sample ImageNet synset hierarchy?

### Classification Datasets as Evaluation Resources
**Why needed**: Repurposing existing datasets provides standardized evaluation without creating new data
**Quick check**: Can you map a COCO object category to a corresponding VQA question?

## Architecture Onboarding

### Component Map
Classification Dataset (ImageNet/COCO) -> Question Generation Engine -> Vision-Language Model -> Answer Generation -> Evaluation Metrics (NLP/LLM) -> Human Evaluation Benchmark

### Critical Path
The critical path flows from question generation through the vision-language model to answer evaluation, with semantic hierarchies enabling follow-up questions when initial answers are insufficiently detailed.

### Design Tradeoffs
The approach trades the flexibility of arbitrary VQA questions for the consistency and comparability of classification-based evaluation. This enables direct comparison with discriminative models but may not capture all aspects of real-world VQA scenarios.

### Failure Signatures
Model failures typically manifest as overly coarse answers that cannot be refined through follow-up questions, or as answers that fall outside the semantic hierarchy constraints of the classification dataset.

### First 3 Experiments to Run
1. Generate a sample set of initial questions and follow-up questions from ImageNet's semantic hierarchy
2. Evaluate a vision-language model's performance on coarse vs. fine-grained attribute classification tasks
3. Compare LLM-based metrics against human judgments for answer evaluation on a subset of questions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond those related to its specific methodology and results.

## Limitations
- The benchmark relies on classification datasets which may not fully capture the complexity of real-world VQA scenarios
- Effectiveness of follow-up questions depends on the quality and coverage of semantic hierarchies
- The comparison between evaluation metrics is limited by the specific human evaluation methodology employed

## Confidence
- Major claims: Medium
- Methodology soundness: Medium
- Evaluation scope: Medium
- Results generalizability: Medium

## Next Checks
1. Expand the benchmark to include more diverse VQA datasets that require multi-step reasoning
2. Conduct an independent validation of the semantic hierarchies used for follow-up questions
3. Test the evaluation metrics across a wider range of vision-language models and comparison with additional human evaluation studies