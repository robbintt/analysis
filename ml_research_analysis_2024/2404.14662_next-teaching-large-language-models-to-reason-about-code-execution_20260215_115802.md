---
ver: rpa2
title: 'NExT: Teaching Large Language Models to Reason about Code Execution'
arxiv_id: '2404.14662'
source_url: https://arxiv.org/abs/2404.14662
tags:
- execution
- program
- code
- next
- trace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NExT teaches large language models to reason about program execution
  using chain-of-thought rationales. It uses self-training to bootstrap synthetic
  training data by sampling execution-aware rationales that lead to correct code fixes,
  verified by unit tests.
---

# NExT: Teaching Large Language Models to Reason about Code Execution

## Quick Facts
- arXiv ID: 2404.14662
- Source URL: https://arxiv.org/abs/2404.14662
- Reference count: 40
- Primary result: 26.1% and 14.3% fix rate improvements on program repair benchmarks

## Executive Summary
NExT (NEgative data eXclusion and Training) is a self-training method that teaches large language models to reason about program execution through chain-of-thought rationales. The approach generates synthetic training data by sampling execution-aware rationales that lead to correct code fixes, verified by unit tests. By focusing on rationales that connect program behavior to code changes, NExT improves the reasoning capabilities of models like PaLM 2 on program repair tasks.

## Method Summary
NExT employs a self-training loop where the model first generates chain-of-thought rationales for code fixes, then uses these rationales to guide the selection of training examples. The method filters out incorrect rationales and focuses on execution-aware reasoning patterns that successfully identify bugs and propose fixes. Synthetic data is created by sampling from this filtered pool of rationales, creating a curriculum that progressively improves the model's ability to reason about program execution and generate correct fixes.

## Key Results
- PaLM 2's fix rate improved by 26.1% on one benchmark and 14.3% on another
- Significantly improved rationale quality for code reasoning tasks
- Method generalizes to scenarios without execution traces at test time

## Why This Works (Mechanism)
The method works by teaching models to explicitly reason about program execution through chain-of-thought rationales. By self-training on rationales that successfully lead to correct fixes, the model learns to identify execution patterns that cause bugs and develop systematic approaches to repair them. The unit test verification ensures that only rationales leading to functional fixes are retained, creating a quality feedback loop that progressively improves reasoning capabilities.

## Foundational Learning

- **Chain-of-thought reasoning**: Models generate step-by-step explanations linking program execution to code changes; needed to create interpretable debugging traces; quick check: verify rationales contain logical execution flow
- **Self-training loops**: Model iteratively improves by training on its own high-quality outputs; needed to bootstrap reasoning capabilities without human-labeled data; quick check: confirm training data quality improves over iterations
- **Unit test verification**: Automated validation of proposed fixes against test suites; needed to ensure only correct fixes are used for training; quick check: measure test pass rate of generated fixes
- **Execution-aware sampling**: Selecting rationales that explicitly reference program behavior; needed to focus learning on execution-relevant patterns; quick check: count rationales mentioning execution states or variable values

## Architecture Onboarding

**Component Map:**
PaLM 2 model -> Chain-of-thought generator -> Fix generator -> Unit test verifier -> Training data selector -> Synthetic data generator

**Critical Path:**
Generate rationales → Propose fixes → Verify with unit tests → Select successful patterns → Create synthetic training data → Fine-tune model

**Design Tradeoffs:**
- Self-training vs. human-labeled data: Self-training enables scalability but may amplify model biases
- Rationale quality vs. quantity: Strict filtering improves quality but reduces training data volume
- Execution traces vs. generalization: Training with traces improves performance but raises question about test-time applicability

**Failure Signatures:**
- Low unit test pass rates indicating poor fix quality
- Rationales lacking execution-specific details or containing logical inconsistencies
- Plateau in rationale quality improvements across training iterations

**First 3 Experiments:**
1. Generate rationales for known buggy programs and verify logical consistency
2. Test fix generation on small programs with complete test coverage
3. Measure rationale quality improvement across self-training iterations

## Open Questions the Paper Calls Out
None

## Limitations
- Improvements measured only on PaLM 2 models, unclear if transferable to other architectures
- Relies heavily on unit test availability, which may not hold for real-world codebases
- Evaluation limited to programs under 300 lines, scalability to larger systems unknown

## Confidence

**Confidence labels:**
- Fix rate improvements: High
- Rationale quality improvements: Medium  
- Generalization to trace-free scenarios: Medium

## Next Checks

1. Evaluate NExT on additional model families (GPT-4, CodeLlama, StarCoder) to assess architecture independence
2. Test performance on programs exceeding 300 lines to evaluate scalability limits
3. Measure effectiveness on codebases with sparse or missing unit tests to validate real-world applicability