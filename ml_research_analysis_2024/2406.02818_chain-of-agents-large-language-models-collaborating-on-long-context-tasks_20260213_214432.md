---
ver: rpa2
title: 'Chain of Agents: Large Language Models Collaborating on Long-Context Tasks'
arxiv_id: '2406.02818'
source_url: https://arxiv.org/abs/2406.02818
tags:
- arxiv
- context
- long
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chain-of-Agents (CoA), a novel framework
  for addressing long-context challenges in large language models (LLMs). CoA leverages
  multi-agent collaboration to process long texts by dividing them into chunks, with
  each agent handling a specific segment.
---

# Chain of Agents: Large Language Models Collaborating on Long-Context Tasks

## Quick Facts
- arXiv ID: 2406.02818
- Source URL: https://arxiv.org/abs/2406.02818
- Reference count: 40
- Key outcome: CoA framework achieves up to 10% performance improvement on long-context tasks by using sequential multi-agent communication

## Executive Summary
This paper introduces Chain-of-Agents (CoA), a novel framework for addressing long-context challenges in large language models (LLMs). CoA leverages multi-agent collaboration to process long texts by dividing them into chunks, with each agent handling a specific segment. The agents communicate sequentially, passing information to build a comprehensive understanding of the entire text. A manager agent then synthesizes the accumulated knowledge to generate the final output. The framework effectively mitigates the "lost-in-the-middle" issue common in long-context tasks by assigning each agent a short context, ensuring focused processing.

## Method Summary
Chain-of-Agents processes long-context inputs by dividing them into chunks that fit within the LLM's context window. Multiple worker agents process these chunks sequentially, with each worker receiving the communication unit from the previous agent before producing its own output. This creates a chain of information flow where each agent builds upon accumulated context. After all workers complete their processing, a manager agent synthesizes the final output from the last communication unit. The framework is training-free, task-agnostic, and highly interpretable, making it applicable to diverse long-context tasks including question answering, summarization, and code completion.

## Key Results
- Achieves up to 10% performance improvement over strong baselines like RAG and full-context approaches
- Effectively mitigates the "lost-in-the-middle" problem common in long-context tasks
- Demonstrates task-agnostic effectiveness across question answering, summarization, and code completion datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential agent communication preserves information across chunks better than parallel approaches.
- Mechanism: Each worker agent reads its chunk, then receives and integrates the communication unit from the previous agent before producing its own output. This creates a chain where each agent builds upon the accumulated context.
- Core assumption: Natural reading order (left-to-right) optimizes information flow for comprehension tasks.
- Evidence anchors: [abstract] "CoA consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text"; [section] "The left side of Figure 1 underscores the necessity of collaborative communication among workers to effectively address complex, long-context reasoning tasks."

### Mechanism 2
- Claim: Assigning each agent a short context window mitigates the "lost-in-the-middle" problem.
- Mechanism: By limiting each worker to process only its assigned chunk, the model maintains high attention density within that segment, avoiding the dilution of focus that occurs with long context windows.
- Core assumption: LLMs have attention mechanisms that perform better on shorter sequences due to computational constraints.
- Evidence anchors: [abstract] "CoA processes the entire input by interleaving reading and reasoning, and it mitigates long context focus issues by assigning each agent a short context."; [section] "As shown in Figure 2, CoA can outperform the vanilla baseline by a large margin on various source lengths."

### Mechanism 3
- Claim: Separating worker and manager agents creates a cleaner abstraction for long-context processing.
- Mechanism: Workers focus on information extraction and chunk comprehension, while the manager agent synthesizes the accumulated knowledge into the final output. This separation of concerns allows each type of agent to specialize.
- Core assumption: Different cognitive tasks (extraction vs. synthesis) benefit from different prompting strategies and model configurations.
- Evidence anchors: [abstract] "followed by a manager agent who synthesizes these contributions into a coherent final output."; [section] "While worker agents extract relevant information in a long-context source, the manager agent synthesizes relevant information accumulated by the end of 'worker-agent-chain' to generate the final answer."

## Foundational Learning

- Concept: Attention mechanism limitations in transformer architectures
  - Why needed here: Understanding why long context windows cause performance degradation requires knowledge of how transformers handle attention across sequences
  - Quick check question: Why does attention computation scale quadratically with sequence length, and how does this affect performance?

- Concept: Retrieval-augmented generation (RAG) limitations
  - Why needed here: CoA is positioned as an alternative to RAG, so understanding RAG's weaknesses (incomplete retrieval, ranking issues) is crucial
  - Quick check question: What are the two main failure modes of RAG when handling long-context questions?

- Concept: Multi-agent systems and communication patterns
  - Why needed here: CoA's effectiveness depends on the specific communication pattern (sequential vs. parallel), which is a fundamental concept in distributed systems
  - Quick check question: How does sequential communication differ from parallel communication in terms of information flow and potential bottlenecks?

## Architecture Onboarding

- Component map: Input splitter -> W1 -> W2 -> ... -> Wl -> Manager agent
- Critical path:
  1. Split input into chunks
  2. W1 processes chunk 1 + empty CU → produces CU1
  3. W2 processes chunk 2 + CU1 → produces CU2
  4. ...continue through all workers
  5. Manager processes final CU + query → produces final answer

- Design tradeoffs:
  - Sequential vs. parallel processing: Sequential preserves context better but increases latency
  - Fixed chunk size vs. dynamic sizing: Fixed simplifies implementation but may not optimize for content boundaries
  - Shared vs. separate LLM instances: Shared reduces cost but may introduce state management complexity

- Failure signatures:
  - Performance degradation with very long input: May indicate chunk size needs adjustment
  - Inconsistent outputs across runs: Could suggest temperature or randomness issues
  - High latency: May indicate need for parallelization or model optimization

- First 3 experiments:
  1. Ablation test: Remove manager agent and compare performance to full CoA
  2. Chunk size sensitivity: Test with different chunk sizes (2k, 4k, 8k, 16k) on a single dataset
  3. Communication pattern comparison: Implement parallel worker approach and compare to sequential

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Chain-of-Agents scale with the number of worker agents used, and is there an optimal number that balances performance and computational efficiency?
- Basis in paper: [inferred] The paper mentions adjusting the number of worker agents to handle inputs of different lengths, but does not explore the relationship between the number of agents and performance.
- Why unresolved: The paper does not provide experiments or analysis on how the number of worker agents affects the performance and efficiency of the Chain-of-Agents framework.
- What evidence would resolve it: Experiments varying the number of worker agents on the same dataset and analyzing the trade-off between performance gains and computational costs would provide insights into the optimal number of agents.

### Open Question 2
- Question: Can the Chain-of-Agents framework be extended to handle tasks beyond question answering, summarization, and code completion, such as dialogue generation or machine translation?
- Basis in paper: [explicit] The paper states that Chain-of-Agents is task-agnostic and demonstrates effectiveness on diverse tasks, but does not explore its applicability to other NLP tasks.
- Why unresolved: The experiments in the paper focus on three specific task types, leaving the potential of Chain-of-Agents for other NLP tasks unexplored.
- What evidence would resolve it: Applying Chain-of-Agents to other NLP tasks, such as dialogue generation or machine translation, and evaluating its performance compared to existing methods would demonstrate its versatility and potential for broader applications.

### Open Question 3
- Question: How does the choice of communication strategy between worker agents affect the performance of Chain-of-Agents, and are there more effective strategies than the sequential communication used in the paper?
- Basis in paper: [explicit] The paper mentions that the effectiveness of Chain-of-Agents relies on the communication between worker agents but does not explore different communication strategies.
- Why unresolved: The paper only evaluates the sequential communication strategy and does not investigate alternative strategies, such as parallel communication or hierarchical communication.
- What evidence would resolve it: Experiments comparing the performance of Chain-of-Agents using different communication strategies, such as parallel or hierarchical communication, would reveal the impact of communication strategies on the framework's effectiveness.

### Open Question 4
- Question: Can the Chain-of-Agents framework be combined with other techniques, such as few-shot learning or meta-learning, to further improve its performance on long-context tasks?
- Basis in paper: [inferred] The paper focuses on the basic Chain-of-Agents framework without exploring its integration with other techniques.
- Why unresolved: The paper does not investigate the potential benefits of combining Chain-of-Agents with other techniques, such as few-shot learning or meta-learning, which could enhance its performance on long-context tasks.
- What evidence would resolve it: Experiments combining Chain-of-Agents with techniques like few-shot learning or meta-learning and evaluating their impact on performance would provide insights into the potential for further improvements.

## Limitations

- Performance comparison limited to existing baselines without testing against latest state-of-the-art long-context models
- Sequential communication creates inherent latency that wasn't thoroughly characterized across different task types
- Assumes equal-sized chunks are optimal without exploring content-aware splitting strategies

## Confidence

**High Confidence (8/10):** The core mechanism of sequential agent communication with a manager synthesis agent is well-supported by ablation studies and multiple task types. The 10% performance improvement over strong baselines is statistically significant across the tested datasets.

**Medium Confidence (6/10):** The generalizability of CoA to diverse task types beyond the tested question answering, summarization, and code completion domains. The framework shows promise but hasn't been validated on tasks requiring real-time processing or those with different information structures.

**Low Confidence (4/10):** The scalability analysis for extremely long contexts (100K+ tokens) and the cost-effectiveness comparison with emerging single-model long-context solutions. The paper provides limited data on how performance scales with context length beyond the tested ranges.

## Next Checks

1. **Ablation on Communication Patterns**: Implement a parallel worker variant where all agents process their chunks independently before a manager agent synthesizes the results. Compare performance and latency against the sequential approach to quantify the information preservation benefits versus processing overhead.

2. **Chunk Boundary Sensitivity**: Design experiments that systematically vary chunk boundaries (sliding window approach) to measure how sensitive the framework is to where chunks are split. This would reveal whether semantic boundaries or equal-length chunks are more effective for different task types.

3. **Cross-Model Generalization**: Test CoA with different underlying LLM architectures (not just GPT-4) including smaller open-source models to evaluate whether the framework's benefits are model-dependent or represent a more general solution to long-context processing.