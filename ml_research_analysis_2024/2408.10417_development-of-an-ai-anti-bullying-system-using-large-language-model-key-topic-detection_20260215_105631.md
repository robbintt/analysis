---
ver: rpa2
title: Development of an AI Anti-Bullying System Using Large Language Model Key Topic
  Detection
arxiv_id: '2408.10417'
source_url: https://arxiv.org/abs/2408.10417
tags:
- prompt
- response
- respond
- only
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: An AI anti-bullying system (AABS) was developed to identify, characterize,
  and respond to coordinated cyberbullying campaigns via social media. The system
  uses a large language model (LLM) to process content and populate a Blackboard Architecture-based
  network model, then generates reports and remediation strategies for first responders.
---

# Development of an AI Anti-Bullying System Using Large Language Model Key Topic Detection

## Quick Facts
- arXiv ID: 2408.10417
- Source URL: https://arxiv.org/abs/2408.10417
- Authors: Matthew Tassava; Cameron Kolodjski; Jordan Milbrath; Adorah Bishop; Nathan Flanders; Robbie Fetsch; Danielle Hanson; Jeremy Straub
- Reference count: 40
- Primary result: AI anti-bullying system (AABS) achieves 30% full accuracy on 21 test cases using LLM for key topic detection

## Executive Summary
This paper presents the development of an AI anti-bullying system (AABS) that leverages large language models (LLMs) to identify, characterize, and respond to coordinated cyberbullying campaigns through social media. The system processes content using an LLM to populate a Blackboard Architecture-based network model, enabling analysis and generation of remediation strategies for first responders. Testing revealed that while the current model creation technology is not fully reliable, it can provide useful associations between concepts and shows potential for detecting harmful content and supporting law enforcement applications.

## Method Summary
The system employs Llama 3.1 Instruct LLM to process social media content, identify bullying relevance, extract key elements (subjects, objects, actions), and populate a Blackboard Architecture model. The workflow involves: content acquisition from social media, LLM processing to split complex sentences and extract SVO structures, model development to populate the Blackboard network with containers and links, and response generation including protected speech identification and remediation strategy creation. The system was tested on 21 cases using commerce and travel topics as surrogates for actual bullying content due to LLM content filtering constraints.

## Key Results
- The system achieved only 30% full accuracy in correctly identifying and extracting subjects, objects, and actions from test text
- The system successfully identified at least part of the relevant elements in 70% of test cases
- Current LLM technology shows potential for useful concept associations but requires significant improvement for reliable anti-bullying detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system identifies key elements (subjects, objects, actions) from bullying-related text by splitting sentences into simple clauses and mapping them into a Blackboard Architecture model.
- Mechanism: LLM processes each sentence through a pipeline—detecting complexity, splitting complex sentences, determining topic relevance, and extracting SVO (subject-verb-object) phrases. These elements populate containers and links in the Blackboard network for analysis.
- Core assumption: The LLM can accurately parse and extract SVO structures even when sentences are complex or contain pronouns that need resolution.
- Evidence anchors:
  - [abstract] "The system uses a large language model (LLM) to process content and populate a Blackboard Architecture-based network model"
  - [section 3.4] "To determine if the sentence relates to the key topic, it is passed into the LLM... The subject, object, action and other key components of the text are identified"
  - [corpus] Weak: No direct corpus evidence on LLM SVO extraction accuracy; relies on internal testing data.
- Break condition: If the LLM fails to correctly split complex sentences or replace pronouns, the model will misrepresent relationships and actions.

### Mechanism 2
- Claim: The Blackboard Architecture supports explainable analysis by storing bullying elements as linked containers, allowing pattern detection and context generation.
- Mechanism: After SVO extraction, each unique subject/object becomes a container node; links connect them with action verbs. This structured model enables trend identification and remediation suggestions.
- Core assumption: The Blackboard network can reliably store and link elements such that patterns (e.g., repeated links to a container) indicate relevant bullying trends.
- Evidence anchors:
  - [abstract] "This facilitates analysis and remediation activity – such as generating report messages to social media companies"
  - [section 3.3] "The combination of all of the data collected by the model development component produces the single target bullying attack model (STBAM)"
  - [corpus] Moderate: The neighbor paper "The Use of a Large Language Model for Cyberbullying Detection" supports LLM-based detection but not specifically Blackboard integration.
- Break condition: If container-link mapping is inconsistent or containers are not unique, trend detection and remediation generation will fail.

### Mechanism 3
- Claim: The system can identify and annotate protected speech within the model, enabling lawful and accurate reporting.
- Mechanism: After building the STBAM, the response component flags speech that is identified, non-threatening, and non-defamatory as protected. This content remains in the model but is marked to distinguish actionable from non-actionable elements.
- Core assumption: The LLM can reliably determine whether content meets protected speech criteria without misclassifying harmful content.
- Evidence anchors:
  - [abstract] "Protected Speech Identification and Model Alteration" in the workflow diagram
  - [section 3.3] "Protected speech is not actionable... it can demonstrate motive and intent and provide context for unprotected speech"
  - [corpus] Weak: No corpus evidence on protected speech classification accuracy.
- Break condition: If protected speech is misclassified, either actionable content may be ignored or non-actionable content may trigger false remediation.

## Foundational Learning

- Concept: Large Language Model (LLM) natural language processing
  - Why needed here: The system depends on LLMs for sentence splitting, topic relevance detection, and SVO extraction. Without understanding LLM behavior, the model creation pipeline cannot be tuned or debugged.
  - Quick check question: What is the difference between using a general LLM versus a domain-specific LLM for cyberbullying detection?

- Concept: Blackboard Architecture for knowledge representation
  - Why needed here: The Blackboard model is the backbone for storing and linking bullying elements. Understanding how containers, links, and rules interact is essential for extending or debugging the system.
  - Quick check question: How does the Blackboard Architecture differ from a simple graph database in terms of rule application?

- Concept: Protected speech legal framework
  - Why needed here: Correctly identifying protected speech determines what remediation actions are legally permissible. Misunderstanding this could lead to improper reporting or legal exposure.
  - Quick check question: What are the key legal distinctions between protected and unprotected speech in the context of cyberbullying?

## Architecture Onboarding

- Component map: Search Component -> Model Development Component -> Response & Remediation Component
- Critical path: Content acquisition -> LLM processing -> Blackboard model creation -> Report generation
- Design tradeoffs:
  - Using a general LLM (Meta Llama 3.1) offers broad capability but may misclassify domain-specific content; a specialized LLM could improve accuracy but requires additional training.
  - Including protected speech in the model provides context but increases model complexity; excluding it simplifies the model but loses investigative leads.
- Failure signatures:
  - Model creation errors -> incorrect or missing SVO extraction; LLM guardrails blocking topic processing.
  - Protected speech misclassification -> improper remediation triggers or missed actionable content.
  - Incomplete content acquisition -> model missing key bullying elements.
- First 3 experiments:
  1. Test LLM SVO extraction accuracy on a set of labeled bullying sentences; measure precision/recall of subject, object, action identification.
  2. Validate Blackboard container-link mapping by feeding known SVO sets and checking structural consistency.
  3. Simulate protected speech annotation by running the classification step on a mixed dataset of protected vs. unprotected content and measuring accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific improvements to the LLM prompts or training would increase the accuracy of key topic detection beyond the current 30% success rate?
- Basis in paper: [explicit] The paper discusses the need for prompt refinement and notes that the current LLM struggles with pronoun replacement, proper noun expansion, and action verb identification.
- Why unresolved: The authors acknowledge that prompt engineering could improve results but do not specify which modifications would be most effective or provide concrete evidence of their impact.
- What evidence would resolve it: Systematic testing of different prompt variations with measurable accuracy improvements, or a comparison of performance between a generic LLM and one fine-tuned specifically for key topic detection.

### Open Question 2
- Question: How would the system perform when processing real bullying content versus the surrogate topics (commerce and travel) used in testing?
- Basis in paper: [inferred] The authors explicitly state they used surrogate topics due to potential LLM content filtering, but acknowledge this limitation prevents validation of the system's core purpose.
- Why unresolved: The paper does not present results using actual bullying-related content, leaving uncertainty about whether the same accuracy and reliability issues would persist.
- What evidence would resolve it: Testing the complete system with real-world bullying data and comparing the accuracy, error rates, and model creation success to the surrogate topic results.

### Open Question 3
- Question: What is the optimal balance between automated analysis and human review in the proposed system to ensure both efficiency and accuracy?
- Basis in paper: [explicit] The authors mention that manual response activities are part of the workflow and that evidentiary integrity and cybersecurity will be important considerations, but do not define specific human involvement thresholds.
- Why unresolved: The paper does not provide guidelines for when human intervention should occur, how to verify automated results, or what proportion of cases require manual review.
- What evidence would resolve it: A study comparing system performance with different levels of human oversight, including metrics on false positives/negatives, response time, and resource allocation.

## Limitations

- Limited evaluation scope with only 21 test cases and 30% full accuracy makes it difficult to generalize performance to real-world cyberbullying scenarios
- No empirical evidence provided for protected speech classification accuracy or system effectiveness in actual coordinated cyberbullying campaign detection
- Testing used surrogate topics (commerce and travel) rather than real bullying content due to LLM content filtering constraints

## Confidence

- **Low confidence**: Full system accuracy and real-world effectiveness claims (only 30% accuracy on limited test set)
- **Medium confidence**: Individual component capabilities (SVO extraction, Blackboard modeling) based on controlled test cases
- **Medium confidence**: Potential for broader security applications and law enforcement support (conceptual, not empirically validated)

## Next Checks

1. Conduct comprehensive testing on a diverse, real-world dataset of cyberbullying incidents to measure system accuracy across different types of bullying content and coordination patterns.
2. Validate protected speech classification by testing the system on a balanced dataset containing both protected and unprotected content, measuring false positive and false negative rates.
3. Implement and test the complete remediation workflow by simulating actual first responder scenarios and measuring the accuracy and utility of generated remediation strategies.