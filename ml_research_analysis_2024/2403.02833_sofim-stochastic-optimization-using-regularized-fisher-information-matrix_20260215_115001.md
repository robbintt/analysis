---
ver: rpa2
title: 'SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix'
arxiv_id: '2403.02833'
source_url: https://arxiv.org/abs/2403.02833
tags:
- sofim
- matrix
- hessian
- optimization
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SOFIM is a novel stochastic optimization method that utilizes a
  regularized Fisher information matrix (FIM) to approximate the Hessian matrix for
  efficient Newton-style updates in large-scale machine learning optimization. It
  addresses the computational challenges of natural gradient descent (NGD) by using
  regularized FIM and Sherman-Morrison matrix inversion to directly compute the update
  direction, while also incorporating momentum-like first moment estimation similar
  to Adam to handle non-stationary objectives across mini-batches.
---

# SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix

## Quick Facts
- arXiv ID: 2403.02833
- Source URL: https://arxiv.org/abs/2403.02833
- Reference count: 26
- Primary result: Achieves faster convergence than SGD with momentum and state-of-the-art Newton methods while maintaining O(d) time and space complexity

## Executive Summary
SOFIM introduces a novel stochastic optimization method that approximates the Hessian matrix using a regularized Fisher Information Matrix (FIM) for efficient Newton-style updates. The method combines the benefits of second-order optimization with the computational efficiency of first-order methods by using the Sherman-Morrison formula to directly compute update directions without explicitly inverting matrices. By incorporating first-moment estimation similar to Adam, SOFIM effectively handles non-stationary objectives across mini-batches. Extensive experiments on CIFAR10, CIFAR100, and SVHN datasets demonstrate superior performance compared to SGD with momentum and other state-of-the-art Newton methods.

## Method Summary
SOFIM is a stochastic optimization algorithm that approximates the Hessian matrix using a regularized Fisher Information Matrix (FIM) to enable efficient Newton-style updates in large-scale machine learning. The method constructs Ft = cMcMᵀ + ρI from the bias-corrected first moment estimate cMt, where cM is the bias-corrected running average of gradients. Using the Sherman-Morrison formula, SOFIM directly computes the inverse Ft⁻¹cM without storing or explicitly inverting the full matrix, maintaining O(d) time and space complexity. The algorithm also incorporates momentum-like first moment estimation with bias correction to handle non-stationary objectives across mini-batches. Experiments demonstrate that SOFIM achieves faster convergence and better generalization than SGD with momentum and state-of-the-art Newton methods on image classification tasks using deep learning models like LeNet5, ResNet9, and ResNet18.

## Key Results
- Outperforms SGD with momentum, Nyström-SGD, L-BFGS, and AdaHessian in convergence speed on CIFAR10, CIFAR100, and SVHN datasets
- Achieves lower training and test losses with higher test accuracy across all tested deep learning models
- Maintains O(d) time and space complexity while providing second-order optimization benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regularized FIM approximation maintains SGD with momentum's complexity while approximating Hessian
- Mechanism: Uses Ft = cMcMᵀ + ρI where cMt is bias-corrected first moment, enabling Sherman-Morrison inversion in O(d) time/space
- Core assumption: Regularization parameter ρ balances approximation accuracy with numerical stability
- Evidence anchors: Abstract claims improved convergence with same complexities; section confirms O(d) time/space for Sherman-Morrison inversion
- Break condition: Numerical instability when ρ too small; poor approximation when ρ too large

### Mechanism 2
- Claim: First moment estimation with bias correction handles non-stationary objectives across mini-batches
- Mechanism: Maintains Mt = βMt₋₁ + (1-β)gt with bias-corrected estimate cMt = Mt/(1-βᵗ)
- Core assumption: Non-stationarity primarily due to heterogeneous data distribution across mini-batches
- Evidence anchors: Abstract mentions addressing non-stationary objectives; section describes bias-corrected first moment usage
- Break condition: Convergence slows when β close to 1; loses adaptive property when β close to 0

### Mechanism 3
- Claim: Sherman-Morrison formula enables direct Newton update computation without full matrix inversion
- Mechanism: Computes Ft⁻¹cM = cM/ρ - cM(cMᵀcM)/(ρ²(1 + cMᵀcM/ρ)) avoiding O(d³) inversion
- Core assumption: Sherman-Morrison formula is numerically stable for this matrix structure
- Evidence anchors: Section describes using Sherman-Morrison formula eq. 6 for direct update direction computation
- Break condition: Numerical instability when cMᵀcM/ρ approaches -1

## Foundational Learning

- Concept: Fisher Information Matrix
  - Why needed here: Serves as natural Hessian approximation in probabilistic models for second-order optimization
  - Quick check question: What is the relationship between Fisher Information Matrix and the Hessian for probabilistic models?

- Concept: Sherman-Morrison Formula
  - Why needed here: Enables efficient matrix inverse computation for rank-one updates, avoiding O(d³) complexity
  - Quick check question: What is the computational complexity of matrix inversion using Sherman-Morrison compared to standard methods?

- Concept: Momentum-based optimization
  - Why needed here: First moment estimation helps handle non-stationarity of objectives across mini-batches
  - Quick check question: How does the bias correction in Adam/SOFIM affect the initial phase of optimization?

## Architecture Onboarding

- Component map: Gradient computation → First moment estimation → FIM construction → Sherman-Morrison inversion → Parameter update
- Critical path: Gradient computation → First moment estimation → FIM construction → Sherman-Morrison inversion → Parameter update
- Design tradeoffs:
  - Regularization parameter ρ: Balances numerical stability vs approximation accuracy
  - Momentum parameter β: Affects convergence speed vs adaptability to non-stationarity
  - Mini-batch size: Impacts gradient variance vs computational efficiency
- Failure signatures:
  - Divergence: Likely indicates ρ too small or learning rate too large
  - Slow convergence: May indicate ρ too large or β too close to 1
  - Numerical instability: Check if cMᵀcM/ρ approaches -1
- First 3 experiments:
  1. Verify gradient computation correctness by comparing with standard SGD gradients
  2. Test Sherman-Morrison inversion by comparing with direct matrix inversion for small d
  3. Validate convergence behavior on simple convex problem with known optimum

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of the FIM regularization parameter ρ for different types of deep learning models and datasets?
- Basis in paper: The paper tests limited ρ values (1, 0.5, 0.1) on small set of models/datasets, finding ρ = 0.5 works well for LeNet5 on CIFAR10 and ResNet9 on CIFAR100/SVHN, but ρ < 0 leads to poor performance
- Why unresolved: Only tests limited range of ρ values on small set of models and datasets; optimal value may vary with model and dataset characteristics
- What evidence would resolve it: Extensive experiments with wider range of ρ values on diverse models and datasets

### Open Question 2
- Question: How does SOFIM perform on other types of learning tasks beyond image classification, such as natural language processing or reinforcement learning?
- Basis in paper: Paper mentions plans to apply SOFIM to "versatile learning tasks" but provides no results beyond image classification
- Why unresolved: Only evaluates on image classification tasks; unclear how well it performs on other task types with different characteristics
- What evidence would resolve it: Apply SOFIM to various learning tasks beyond image classification and compare performance to other optimization methods

### Open Question 3
- Question: How sensitive is SOFIM to the choice of other hyperparameters, such as learning rate η, momentum β, and mini-batch size?
- Basis in paper: Paper uses specific hyperparameter values but doesn't explore sensitivity to these parameters
- Why unresolved: No comprehensive analysis of how hyperparameter changes affect SOFIM performance; different settings might lead to significantly different results
- What evidence would resolve it: Systematic study of hyperparameter impact through grid search or random search

## Limitations

- Theoretical analysis assumes sufficiently large mini-batch sizes for accurate FIM approximation
- Regularization parameter ρ requires careful tuning with unclear selection strategies across different domains
- While claiming O(d) complexity, constant factors in Sherman-Morrison computation may become significant for very high-dimensional problems

## Confidence

- High confidence in core mathematical formulation and O(d) complexity claims
- Medium confidence in empirical superiority over baselines given results primarily shown on image classification tasks
- Low confidence in generalization to non-vision domains and other model architectures without further validation

## Next Checks

1. Test the method on non-convex optimization problems beyond image classification (e.g., NLP tasks, reinforcement learning) to verify broad applicability
2. Perform ablation studies varying the regularization parameter ρ across different orders of magnitude to understand its impact on convergence and stability
3. Compare computational wall-clock time against baselines to verify that theoretical O(d) complexity translates to practical efficiency gains