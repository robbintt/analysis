---
ver: rpa2
title: 'Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs'
arxiv_id: '2404.06025'
source_url: https://arxiv.org/abs/2404.06025
tags:
- greedy-dim
- morphing
- search
- attacks
- face
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Greedy-DiM proposes greedy optimization of the diffusion process
  for face morphing attacks. Instead of using fixed or searched blend parameters,
  it uses an identity-based heuristic to guide the sampling at each diffusion step.
---

# Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs

## Quick Facts
- **arXiv ID**: 2404.06025
- **Source URL**: https://arxiv.org/abs/2404.06025
- **Reference count**: 40
- **Primary result**: Greedy-DiM achieves 100% MMPMR across three tested FR systems, outperforming 10+ existing morphing algorithms

## Executive Summary
Greedy-DiM introduces a novel approach to face morphing attacks by optimizing the diffusion process at each step rather than using fixed blend parameters. The method uses an identity-based heuristic to guide gradient descent on noise predictions during the ODE solver's sampling process, directly optimizing morphs toward maximizing similarity to both input faces. This greedy optimization strategy achieves perfect attack success rates on ArcFace, AdaFace, and ElasticFace while reducing computational overhead compared to state-of-the-art methods like Morph-PIPE.

## Method Summary
Greedy-DiM treats face morphing as a greedy optimization problem over the diffusion process rather than searching for optimal blend parameters. At each timestep during the ODE solver, the algorithm performs gradient descent on the predicted noise to minimize an identity-based heuristic loss based on the FR system's embeddings. This directly optimizes the morph toward fooling the FR system at every iteration. The method initializes with either a slerp of encoded faces or random noise, then iteratively refines the image through nopt optimization steps per timestep. Unlike previous approaches that search over discrete blend values, Greedy-DiM performs continuous optimization over the entire image space, theoretically guaranteeing access to the optimal solution.

## Key Results
- Achieves 100% MMPMR across ArcFace, AdaFace, and ElasticFace FR systems
- Outperforms 10+ existing morphing algorithms including Morph-PIPE, FaceFusion, and Poisson blending
- Reduces computational overhead from 2350 NFE (Morph-PIPE) to 270 NFE
- Demonstrates first representation-based morphing attack to consistently outperform landmark-based methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Greedy-DiM achieves 100% MMPMR by optimizing noise predictions at each diffusion step rather than blending pre-generated candidates
- **Mechanism**: At each timestep during the ODE solver, gradient descent minimizes identity loss on predicted noise, directly optimizing morphs to maximize similarity to both input faces
- **Core assumption**: The noise prediction network can be optimized to produce noise that results in morphs maximizing identity similarity to both input faces
- **Evidence anchors**: [abstract] identity-based heuristic guides sampling at each diffusion step; [section 3.2] directly optimize noise prediction equivalent to optimizing ODE solver steps
- **Break condition**: If heuristic becomes saturated or gradient descent fails to find improvements, optimization could stall or diverge

### Mechanism 2
- **Claim**: The search space for Greedy-DiM* is the full image space, making it more likely to contain the optimal solution than discrete search strategies
- **Mechanism**: Performs continuous optimization over entire image space at each timestep, allowing exploration of solutions outside linear/spherical interpolation paths between input faces
- **Core assumption**: Optimal morph may not lie on interpolation path between two input faces, requiring full image space exploration
- **Evidence anchors**: [section 3.3] search space is X^N vs discrete alternatives; Theorem 3.2 proves probability optimal solution is in search space is 1 vs 0 for others
- **Break condition**: If gradient descent gets stuck in local minima or heuristic becomes uninformative, algorithm may fail to find global optimum

### Mechanism 3
- **Claim**: Greedy-DiM* makes globally optimal decisions at each timestep, not just locally optimal ones
- **Mechanism**: Due to optimal substructure property of PF-ODE, locally optimal noise predictions at time t remain optimal at all earlier times s < t, ensuring global optimality
- **Core assumption**: Probability flow ODE has optimal substructure such that optimal solutions at later timesteps remain optimal at earlier timesteps
- **Evidence anchors**: [section 3.3] Theorem 3.1 proves locally optimal solution at time tn is globally optimal; optimal predictions at time t remain optimal at time s < t
- **Break condition**: If ODE solver introduces numerical errors or score network predictions become unreliable, optimal substructure may not hold in practice

## Foundational Learning

- **Concept**: Diffusion models and score matching
  - **Why needed**: Understanding diffusion model mechanics is essential to grasp why optimizing noise predictions at each step improves morph quality
  - **Quick check**: What is the relationship between the noise prediction network and the score function in diffusion models?

- **Concept**: Face recognition embeddings and similarity metrics
  - **Why needed**: Identity-based heuristic relies on FR system embeddings and cosine similarity to guide optimization process
  - **Quick check**: How does the ArcFace additive angular margin loss encourage discriminative feature embeddings?

- **Concept**: Greedy algorithms and optimal substructure
  - **Why needed**: Theoretical justification relies on understanding when greedy strategies produce globally optimal solutions
  - **Quick check**: What property must a problem have for a greedy algorithm to guarantee global optimality?

## Architecture Onboarding

- **Component map**: Face images → Conditional encoder → ODE solver (DDIM/DPM++) → Heuristic function (ArcFace loss) → Gradient descent optimizer (RAdam) → Denoised morph

- **Critical path**: 
  1. Encode both bona fide images to get conditional embeddings
  2. Initialize with morphed initial noise (slerp of encoded faces or random noise)
  3. For each timestep from T to 0:
     - Predict noise conditioned on current timestep and embeddings
     - Perform gradient descent to minimize identity loss
     - Update image using ODE solver with optimized noise
  4. Return final denoised image

- **Design tradeoffs**:
  - Number of optimization steps (nopt) vs computation time
  - Choice of ODE solver (DDIM vs DPM++ 2M) affecting stability
  - Initial noise choice (slerp vs random) impacting convergence
  - Optimizer parameters (learning rate, momentum) affecting optimization quality

- **Failure signatures**:
  - Saturation artifacts indicate clipping or numerical instability
  - High-frequency noise patterns suggest adversarial perturbation generation
  - Poor FR system performance indicates optimization not finding good solutions
  - Detectability by MAD systems suggests morphs follow recognizable patterns

- **First 3 experiments**:
  1. Compare MMPMR with different nopt values (10, 50, 100) to find optimal tradeoff
  2. Test different initial noise strategies (slerp vs random) on convergence and final quality
  3. Evaluate impact of different ODE solvers on both quality and computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the theoretical limit of the Greedy-DiM* algorithm's performance on other face recognition systems beyond the three tested (ArcFace, AdaFace, ElasticFace)?
- **Basis**: Paper states Greedy-DiM* achieves 100% MMPMR on three FR systems but does not explore performance on other FR systems
- **Why unresolved**: Only tests three FR systems, leaving performance on other FR systems unknown
- **What evidence would resolve it**: Testing Greedy-DiM* on diverse set of additional FR systems and comparing performance to three tested systems

### Open Question 2
- **Question**: How does computational overhead of Greedy-DiM* compare to other morphing algorithms when scaling to larger datasets or higher resolution images?
- **Basis**: Paper mentions Greedy-DiM* reduces computational overhead compared to Morph-PIPE but does not explore scalability to larger datasets or higher resolution images
- **Why unresolved**: Only evaluates on SYN-MAD 2022 dataset, not exploring performance on larger datasets or higher resolution images
- **What evidence would resolve it**: Evaluating Greedy-DiM* on larger datasets and higher resolution images and comparing computational overhead to other morphing algorithms

### Open Question 3
- **Question**: What is the impact of different interpolation functions (e.g., slerp, lerp) on the performance of Greedy-DiM*?
- **Basis**: Paper uses slerp as interpolation function but does not explore impact of other interpolation functions on performance
- **Why unresolved**: Only tests with slerp, leaving impact of other interpolation functions unknown
- **What evidence would resolve it**: Evaluating Greedy-DiM* with different interpolation functions and comparing performance to original implementation

### Open Question 4
- **Question**: How does detectability of Greedy-DiM* compare to other morphing algorithms when using different single-image morphing attack detection (S-MAD) algorithms?
- **Basis**: Paper shows Greedy-DiM* is slightly easier to detect than other DiM variants using specific S-MAD algorithm but does not explore detectability with other S-MAD algorithms
- **Why unresolved**: Only tests detectability with specific S-MAD algorithm, leaving detectability with other S-MAD algorithms unknown
- **What evidence would resolve it**: Evaluating detectability of Greedy-DiM* with different S-MAD algorithms and comparing performance to other morphing algorithms

## Limitations
- Theoretical guarantees about global optimality rely on assumptions about PF-ODE optimal substructure that may not fully translate to practical implementations
- Identity-based heuristic may be vulnerable to adaptive attacks that manipulate FR system's embedding space
- Method's performance against unseen MAD systems remains untested, raising questions about potential overfitting to specific FR architectures

## Confidence
- **High Confidence**: Empirical MMPMR results (100% across three FR systems), computational efficiency improvements (270 NFE vs 2350 NFE)
- **Medium Confidence**: Theoretical claims about global optimality and search space advantages, practical effectiveness against MAD systems
- **Low Confidence**: Generalization to unseen FR systems and MAD approaches, robustness to adaptive countermeasures

## Next Checks
1. Test Greedy-DiM* against adaptive MAD systems that incorporate knowledge of the identity-based heuristic, measuring whether adversaries can detect the optimization patterns
2. Evaluate cross-FR system generalization by testing morphs generated to fool ArcFace against AdaFace and ElasticFace without retraining, quantifying transferability
3. Conduct ablation studies varying the number of optimization steps (nopt) and ODE solver parameters to identify practical limits of the greedy optimization approach