---
ver: rpa2
title: Map-based Modular Approach for Zero-shot Embodied Question Answering
arxiv_id: '2405.16559'
source_url: https://arxiv.org/abs/2405.16559
tags:
- object
- agent
- navigation
- question
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a map-based modular approach for zero-shot
  Embodied Question Answering (EQA), enabling real-world robots to explore and map
  unknown environments to answer natural language questions. The method combines object-goal
  navigation with Visual Question Answering (VQA), using frontier-based exploration
  and semantic mapping to locate target objects.
---

# Map-based Modular Approach for Zero-shot Embodied Question Answering
## Quick Facts
- arXiv ID: 2405.16559
- Source URL: https://arxiv.org/abs/2405.16559
- Authors: Koya Sakamoto; Daichi Azuma; Taiki Miyanishi; Shuhei Kurita; Motoaki Kawanabe
- Reference count: 40
- One-line primary result: Proposed map-based modular approach achieves VQA top-1 accuracy of 0.43 on MP3D-EQA dataset and demonstrates successful real-world navigation in two houses.

## Executive Summary
This paper introduces a map-based modular approach for zero-shot Embodied Question Answering (EQA) that enables real-world robots to explore unknown environments and answer natural language questions. The method combines object-goal navigation with Visual Question Answering (VQA), using frontier-based exploration and semantic mapping to locate target objects. By leveraging pre-trained vision-language models (BLIP2, CLIP, LLaVA), the system can handle open vocabulary questions without retraining. The approach was evaluated on the Matterport3D-EQA dataset and in real-world settings, demonstrating competitive performance compared to end-to-end methods.

## Method Summary
The method combines object-goal navigation with VQA using a modular architecture. It employs frontier-based exploration to navigate and map unknown environments, using semantic mapping with object detection (Detic) to locate objects. When a target object is detected, image-text matching with BLIP2-MSCOCO verifies it by comparing images to declarative sentences derived from questions. VQA modules (BLIP, LLaVA) then provide answers based on collected images. The approach uses pre-trained vision-language models to enable zero-shot operation on diverse questions.

## Key Results
- VQA top-1 accuracy of approximately 0.43 on MP3D-EQA dataset
- Successful real-world navigation in two houses
- Competitive performance compared to end-to-end methods
- Robust handling of open vocabulary and diverse questions

## Why This Works (Mechanism)
### Mechanism 1
Frontier-based exploration reduces domain gap between simulation and real-world navigation by selecting the closest unexplored region as the long-term goal without requiring training, avoiding overfitting to simulated environments.

### Mechanism 2
Image-text matching with vision-language models (BLIP2-MSCOCO) enables accurate identification of target objects among multiple candidates by comparing images to declarative sentences derived from questions.

### Mechanism 3
Modular combination of object-goal navigation and VQA enables handling of open vocabulary questions without retraining by extracting target object categories and using pre-trained vision-language models for VQA.

## Foundational Learning
- Vision-language pre-training: Foundation models like BLIP2 and LLaVA are pre-trained on large datasets to understand both visual and textual information, enabling zero-shot question answering.
- Semantic mapping with object detection: The agent needs to create a map of the environment to plan navigation paths, requiring object detection to identify and locate objects in the scene.
- Frontier-based exploration: This exploration strategy allows the agent to systematically discover unknown areas without requiring extensive training data.

## Architecture Onboarding
- Component map: Language Understanding (GPT-4 Turbo) → Object Goal Extraction → Perception (Detic + Depth Projection) → Semantic Map → Planner (Frontier-based Exploration + A*) → Navigation Actions → Image-Text Matching (BLIP2-MSCOCO) → Target Object Verification → VQA (LLaVA/BLIP2) → Final Answer Generation
- Critical path: Language Understanding → Object Detection → Frontier-based Exploration → Image-Text Matching → VQA → Answer
- Design tradeoffs: Navigation precision vs. exploration efficiency (lower thresholds lead to more detections but more false positives), Real-time performance vs. accuracy (using pre-trained models vs. fine-tuning), Complexity of question handling vs. system robustness (open vocabulary vs. controlled vocabulary)
- Failure signatures: Navigation failures (agent cannot find target within step limit or collides with obstacles), Image-text matching failures (agent stops at wrong object or fails to recognize target), VQA failures (incorrect answers despite correct image selection)
- First 3 experiments: 1) Run navigation-only test: Agent explores with no VQA to verify frontier-based exploration works, 2) Test image-text matching alone: Feed images of target and non-target objects to verify BLIP2-MSCOCO scores, 3) Test VQA on collected images: Verify LLaVA can answer questions about objects without navigation component

## Open Questions the Paper Calls Out
### Open Question 1
How does the performance of the proposed map-based modular approach for zero-shot EQA compare to end-to-end methods when deployed in real-world environments? The paper mentions comparable performance in simulation but lacks direct real-world comparison.

### Open Question 2
What is the impact of using different vision-language models (e.g., BLIP2, CLIP) for image-text matching on the overall EQA performance? The paper compares models but doesn't provide comprehensive analysis of impact on overall performance.

### Open Question 3
How does the proposed method handle complex reasoning tasks, such as counting objects across multiple rooms or rooms with similar layouts? The paper mentions struggles with counting but lacks detailed analysis of limitations and solutions.

## Limitations
- Reliance on foundation models introduces potential brittleness in novel environments
- Performance bounded by step limit (500 steps) which may be insufficient for very large environments
- Struggles with multi-hop reasoning and counting tasks requiring information aggregation across multiple locations

## Confidence
- High confidence: Modular architecture design and integration of existing components is technically sound and well-documented
- Medium confidence: Claim about frontier-based exploration reducing domain gap lacks systematic ablation studies
- Low confidence: Real-world evaluation in only two houses provides insufficient evidence for robust generalization

## Next Checks
1. Conduct ablation study comparing frontier-based exploration against learned navigation policies in both simulated and real-world environments
2. Systematically characterize failure cases across different question types to identify architectural bottlenecks
3. Test system in 5-10 diverse real-world environments with varying layouts and object distributions to validate zero-shot claims beyond two-house demonstration