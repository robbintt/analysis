---
ver: rpa2
title: A Minimaximalist Approach to Reinforcement Learning from Human Feedback
arxiv_id: '2401.04056'
source_url: https://arxiv.org/abs/2401.04056
tags:
- learning
- preference
- reward
- preferences
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Self-Play Preference Optimization (SPO), a
  new method for reinforcement learning from human feedback that avoids the need for
  reward modeling and unstable adversarial training. The key idea is to frame RLHF
  as a two-player zero-sum game and then leverage the symmetry of the game to show
  that a single agent can be trained against itself to compute the Minimax Winner.
---

# A Minimaximalist Approach to Reinforcement Learning from Human Feedback

## Quick Facts
- arXiv ID: 2401.04056
- Source URL: https://arxiv.org/abs/2401.04056
- Authors: Gokul Swamy; Christoph Dann; Rahul Kidambi; Zhiwei Steven Wu; Alekh Agarwal
- Reference count: 40
- One-line primary result: SPO achieves faster and more robust RLHF by avoiding reward modeling and leveraging game-theoretic self-play

## Executive Summary
This paper introduces Self-Play Preference Optimization (SPO), a novel approach to Reinforcement Learning from Human Feedback (RLHF) that frames the problem as a two-player zero-sum game and leverages its symmetry to enable a single agent to learn against itself. By avoiding the need for explicit reward modeling and unstable adversarial training, SPO achieves faster convergence and improved robustness to various preference structures including intransitive, non-Markovian, and stochastic preferences. The method provides theoretical guarantees for convergence to the Minimax Winner while demonstrating superior sample efficiency compared to reward-model-based approaches on continuous control tasks.

## Method Summary
SPO frames RLHF as a two-player zero-sum game where policies compete against each other based on human preference feedback. The key innovation is leveraging the game's symmetry to show that a single agent can play against itself while maintaining convergence guarantees to the Minimax Winner. The algorithm maintains a queue of recent trajectories, compares new trajectories against this queue using a preference model, and uses the win rate as a reward signal to update the policy. This approach eliminates the need for explicit reward modeling and unstable adversarial training while providing theoretical convergence guarantees.

## Key Results
- SPO achieves faster convergence to optimal policies (O(1/T) rate) when underlying reward functions exist, matching UCB-style methods
- SPO demonstrates superior sample efficiency compared to reward-model-based approaches on continuous control tasks
- The method provably handles non-Markovian, intransitive, and stochastic preferences while maintaining robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SPO converges to the Minimax Winner by leveraging the symmetry of the zero-sum game payoff matrix
- Mechanism: The preference function's anti-symmetry (P(ξ1, ξ2) = -P(ξ2, ξ1)) allows a single agent to play against itself while maintaining convergence guarantees. This is equivalent to solving a two-player zero-sum game with only one player.
- Core assumption: The preference function is anti-symmetric and the policy class is convex and compact.
- Evidence anchors:
  - [abstract]: "By leveraging the symmetry of the game, we prove that rather than using the traditional technique of dueling two policies to compute the MW, we can simply have a single agent play against itself while maintaining strong convergence guarantees."
  - [section]: "Lemma 2.1. ∃(ˆp, ˆq) ∈ MW(P) s.t. ˆp = ˆq."
- Break condition: If the preference function loses anti-symmetry (e.g., contains noise that breaks the P(ξ1, ξ2) = -P(ξ2, ξ1) relationship), the single-agent reduction may fail.

### Mechanism 2
- Claim: SPO achieves faster convergence to optimal policies when underlying reward functions exist compared to reward-model-based approaches
- Mechanism: When preferences are explained by a reward function, SPO converges at O(1/T) rate matching UCB-style methods, while reward-model approaches typically converge at O(1/√T).
- Core assumption: There exists a clearly optimal policy π* where P(π*, π) > Δ for all π ≠ π*.
- Evidence anchors:
  - [abstract]: "we prove that when an underlying reward function does exist, our approach converges to the optimal policy at a fast rate that matches that of standard techniques."
  - [section]: "Corollary 2.5 (Informal). Suppose that there exists π* ∈ Π such that P(π*, π) > Δ for all π ≠ π*..."
- Break condition: If the gap condition fails (no clear optimal policy exists), SPO reverts to the slower O(1/√T) convergence rate.

### Mechanism 3
- Claim: SPO handles non-Markovian preferences by learning Markovian policies that exhibit non-Markovian behavior through trajectory-level feedback
- Mechanism: By optimizing trajectory-level preference feedback without requiring Markovian rewards, SPO can learn policies that behave non-Markovially (e.g., changing behavior mid-episode based on cumulative reward) even when optimizing over Markovian policy classes.
- Core assumption: The preference function can capture non-Markovian constraints through trajectory-level comparisons.
- Evidence anchors:
  - [abstract]: "provably handles non-Markovian, intransitive, and stochastic preferences"
  - [section]: "Can SPO handle Non-Markovian preferences? We consider a challenging situation where we want the agent to maximize their cumulative reward as much as possible subject to the constraint that their total reward in the last quarter of a trajectory is below a threshold rmax."
- Break condition: If the preference function cannot distinguish between policies based on non-Markovian behavior (e.g., if it only compares final states), SPO cannot learn the desired non-Markovian behavior.

## Foundational Learning

- Concept: Minimax Winner (MW) from social choice theory
  - Why needed here: Provides the solution concept for aggregating intransitive and noisy preferences without requiring a total order
  - Quick check question: What is the key difference between a Copeland Winner and a Minimax Winner when dealing with intransitive preferences?

- Concept: Zero-sum game theory and Nash equilibria
  - Why needed here: Frames RLHF as a game between policies, enabling the use of game-theoretic solution concepts
  - Quick check question: How does the anti-symmetry of the preference function relate to the payoff matrix in a zero-sum game?

- Concept: No-regret online learning algorithms
  - Why needed here: Provides the algorithmic framework for computing Minimax Winners efficiently
  - Quick check question: What is the relationship between the regret bound of the underlying no-regret algorithm and the convergence rate of SPO?

## Architecture Onboarding

- Component map: Trajectory sampler -> Preference oracle -> Queue management -> Policy optimizer -> Trajectory sampler

- Critical path: Sample trajectories → Compare against queue → Compute win rates → Update policy → Repeat

- Design tradeoffs:
  - Queue size vs. computational efficiency: Larger queues provide more stable win rates but increase computation
  - Preference model vs. human feedback: Preference models enable online querying but may introduce modeling errors
  - Credit assignment granularity: Trajectory-level vs. timestep-level rewards affects learning efficiency

- Failure signatures:
  - Policy collapse to deterministic behavior: Indicates preference function may be too deterministic or queue management issues
  - Oscillation in policy updates: Suggests win rate estimates are too noisy or learning rate is too high
  - Slow convergence: May indicate queue size is too small or preference function is too noisy

- First 3 experiments:
  1. Implement SPO on a simple 3-armed bandit with intransitive preferences to verify convergence to MW
  2. Compare SPO vs. reward-model-based approach on a continuous control task with ground-truth reward preferences
  3. Test SPO's robustness to noisy preferences by adding Bernoulli noise to preference labels

## Open Questions the Paper Calls Out

## Limitations

- Theoretical analysis relies heavily on idealized assumptions about preference function anti-symmetry and policy space convexity
- Empirical validation is limited to continuous control tasks and does not address high-dimensional state spaces typical in real-world applications
- Impact of systematic biases in the preference oracle is not thoroughly explored

## Confidence

- **High confidence**: The core game-theoretic framing and the self-play mechanism for computing the Minimax Winner are mathematically sound and well-established in the literature.
- **Medium confidence**: The convergence rate claims for SPO under specific reward structure assumptions are theoretically proven but have not been extensively validated empirically across diverse environments.
- **Medium confidence**: The robustness claims to non-Markovian and intransitive preferences are theoretically supported but rely on strong assumptions about the preference oracle's behavior.

## Next Checks

1. **Robustness to Preference Noise**: Systematically evaluate SPO's performance degradation as the noise level in the preference oracle increases, comparing against baseline reward-model-based approaches.

2. **Scalability Analysis**: Test SPO on high-dimensional state spaces (e.g., robotic manipulation tasks) to assess its scalability limitations and identify potential bottlenecks in the preference comparison mechanism.

3. **Credit Assignment Granularity**: Experiment with timestep-level preference feedback to determine if SPO can benefit from more granular reward signals, potentially improving learning efficiency.