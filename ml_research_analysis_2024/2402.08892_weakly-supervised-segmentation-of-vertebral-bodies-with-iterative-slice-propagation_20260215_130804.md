---
ver: rpa2
title: Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation
arxiv_id: '2402.08892'
source_url: https://arxiv.org/abs/2402.08892
tags:
- segmentation
- dataset
- wiss
- training
- slices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a weakly supervised iterative spinal segmentation
  (WISS) method for vertebral body (VB) segmentation from CT images using only four
  corner landmark weak labels on a single sagittal slice. WISS first segments VBs
  on an annotated slice in an iterative self-training manner, refining labels using
  confident prediction selection and CRF.
---

# Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation

## Quick Facts
- arXiv ID: 2402.08892
- Source URL: https://arxiv.org/abs/2402.08892
- Authors: Shiqi Peng; Bolin Lai; Guangyu Yao; Xiaoyun Zhang; Ya Zhang; Yan-Feng Wang; Hui Zhao
- Reference count: 29
- Key outcome: Achieves 91.7% dice coefficient for mid-sagittal slices and 83.7% for 3D volumes using only four corner landmark weak labels on a single sagittal slice

## Executive Summary
This paper introduces a weakly supervised iterative spinal segmentation (WISS) method that performs vertebral body segmentation using minimal annotation - only four corner landmarks on a single mid-sagittal CT slice. The approach employs iterative self-training with confident prediction selection and CRF refinement to progressively improve segmentation from coarse quadrilateral labels. It then extends the segmentation to full 3D volumes through slice-by-slice propagation. Experiments on spinal metastases and lumbar CT datasets demonstrate competitive performance compared to strongly supervised methods while significantly reducing annotation burden.

## Method Summary
WISS uses Mask R-CNN with edge loss as the backbone for segmentation. The method first trains on quadrilateral labels derived from four corner landmarks on a mid-sagittal slice, then iteratively refines the training data using confident prediction selection and CRF boundary refinement. After convergence on the mid-sagittal slice, the model propagates segmentation slice-by-slice through the volume, using confident predictions as pseudo-labels to train on adjacent slices. This approach achieves volumetric segmentation while only requiring annotation on a single slice.

## Key Results
- Dice coefficient of 91.7% for mid-sagittal slices (comparable to strongly supervised methods)
- Dice coefficient of 83.7% for full 3D volumetric segmentation
- Robust to weak and noisy supervision, maintaining performance even with reduced landmark annotation
- Significant reduction in labeling costs compared to traditional strong supervision approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative self-training with confident prediction selection and CRF refinement progressively improves segmentation accuracy from coarse quadrilateral labels.
- Mechanism: The model trains on imperfect quadrilateral labels, then iteratively refines training data by selecting high-confidence predictions and using CRF to recover boundaries, leading to convergence on better segmentations.
- Core assumption: Initial coarse labels contain sufficient structural information to bootstrap learning, and confident predictions are reliable enough to serve as training data.
- Evidence anchors: [abstract] "This self-training method alternates between training and refining labels in the training set"; [section] "We propose a confident prediction selection method to avoid passing errors to the next iteration"

### Mechanism 2
- Claim: Slice-by-slice propagation generalizes mid-sagittal segmentation to full volumetric segmentation by leveraging spatial continuity in CT volumes.
- Mechanism: After convergence on mid-sagittal slice, the model processes adjacent slices using confident predictions as pseudo-labels, iteratively expanding segmentation across the entire volume.
- Core assumption: Vertebrae maintain consistent appearance and spatial relationships across adjacent slices, making slice-wise propagation feasible.
- Evidence anchors: [abstract] "WISS proceeds to segment the whole VBs slice by slice with a slice-propagation method"; [section] "We train the segmentation model in a slice-propagated manner"

### Mechanism 3
- Claim: Using Mask R-CNN with edge loss preserves vertebral boundaries better than U-Net, improving segmentation quality.
- Mechanism: Mask R-CNN's instance segmentation capability combined with explicit edge loss helps maintain sharp boundaries between adjacent vertebrae.
- Core assumption: Boundary preservation is critical for medical diagnosis and can be improved through architectural choices and loss function design.
- Evidence anchors: [section] "We adopt weights pre-trained on MSCOCO dataset" and "edge loss is added to the loss function"; [section] "Ledge can be considered as an attention of the possible edge areas"

## Foundational Learning

- Concept: Self-training and iterative refinement in machine learning
  - Why needed here: The method relies on generating better training data from model predictions
  - Quick check question: What are the risks of error amplification in self-training loops, and how does this paper attempt to mitigate them?

- Concept: Conditional Random Fields (CRFs) for boundary refinement
  - Why needed here: CRF is used to recover object boundaries and correct oversegmentation/undersegmentation errors
  - Quick check question: How does the fully connected CRF differ from traditional local-range CRFs, and why is this difference important for medical image segmentation?

- Concept: Instance segmentation vs. semantic segmentation
  - Why needed here: The paper uses Mask R-CNN (instance segmentation) rather than U-Net (semantic segmentation)
  - Quick check question: What are the key differences between instance and semantic segmentation, and why might instance segmentation be more appropriate for vertebral body segmentation?

## Architecture Onboarding

- Component map: CT volume with single annotated sagittal slice → Mask R-CNN with edge loss → self-training with confident prediction selection → CRF refinement → slice propagation → volumetric output
- Critical path: Mid-sagittal self-training → model convergence → slice propagation → volumetric output
- Design tradeoffs: Weak supervision (4 landmarks) vs. segmentation accuracy; iterative refinement vs. computational cost; instance segmentation (Mask R-CNN) vs. simpler architectures
- Failure signatures: Error amplification in self-training loop; poor boundary recovery; propagation failures at slice boundaries or morphological changes
- First 3 experiments:
  1. Verify self-training convergence on mid-sagittal slice with synthetic quadrilateral labels
  2. Test CRF boundary refinement effectiveness by comparing with and without CRF on validation set
  3. Validate slice propagation by comparing dice scores at mid-sagittal vs. peripheral slices

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications emerge from the limitations section and methodology choices.

## Limitations
- Lack of implementation details for confident prediction selection algorithm and CRF configuration parameters
- Limited validation of generalizability from spinal metastases to lumbar datasets
- No discussion of computational cost for iterative self-training with slice propagation
- Performance with different initial annotation qualities remains unexplored

## Confidence

- Mechanism 1 (Self-training refinement): Medium - Iterative approach is sound but depends on underspecified implementation details
- Mechanism 2 (Slice propagation): Medium - Spatial continuity assumption is reasonable but not empirically validated across full volume
- Mechanism 3 (Mask R-CNN + edge loss): Low - Architectural comparison lacks ablation studies and edge loss implementation details are missing

## Next Checks

1. Implement a synthetic test with known ground truth to verify self-training convergence behavior and error amplification risks
2. Conduct ablation studies comparing Dice scores with and without CRF refinement across different vertebra positions in the volume
3. Test the method's robustness to initial annotation quality by systematically degrading the four corner landmarks and measuring segmentation degradation