---
ver: rpa2
title: Zero-Shot Mono-to-Binaural Speech Synthesis
arxiv_id: '2412.08356'
source_url: https://arxiv.org/abs/2412.08356
tags:
- binaural
- audio
- phase
- error
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZeroBAS, the first zero-shot neural method
  for mono-to-binaural speech synthesis that requires no binaural training data. The
  approach uses geometric time warping and amplitude scaling based on source position,
  followed by iterative refinement with a pretrained monaural denoising vocoder.
---

# Zero-Shot Mono-to-Binaural Speech Synthesis

## Quick Facts
- arXiv ID: 2412.08356
- Source URL: https://arxiv.org/abs/2412.08356
- Reference count: 40
- First zero-shot neural method for mono-to-binaural speech synthesis requiring no binaural training data

## Executive Summary
ZeroBAS introduces the first zero-shot neural method for converting monaural speech to binaural audio without requiring any binaural training data. The approach uses geometric time warping and amplitude scaling based on source position to create an initial binaural approximation, which is then iteratively refined using a pretrained monaural denoising vocoder. The method achieves perceptual quality on par with supervised approaches on in-distribution data while significantly outperforming them on out-of-distribution room conditions, demonstrating superior generalization across acoustic environments.

## Method Summary
ZeroBAS converts mono audio to binaural through a three-stage pipeline: geometric time warping (GTW) applies interaural time delays based on source and ear positions, amplitude scaling (AS) adjusts channel levels using inverse square law distance cues, and a pretrained WaveFit denoising vocoder iteratively refines each channel independently using log-mel spectrogram conditioning. The method requires no binaural training data, instead leveraging the vocoder's learned speech priors from 60k hours of monaural audiobook data to reconstruct natural-sounding binaural speech from the geometrically warped approximation.

## Key Results
- Achieves MOS scores of 4.21-4.32 on in-distribution Binaural Speech dataset, matching supervised method performance
- Significantly outperforms supervised methods on TUT Mono-to-Binaural out-of-distribution dataset (MOS 4.24 vs 3.54-3.74)
- Objective metrics show Wave ℓ2 of 0.150-0.155 and Amplitude ℓ2 of 0.025-0.027 on in-distribution data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ZeroBAS achieves zero-shot binaural synthesis by using geometric time warping and amplitude scaling to create an initial binaural approximation, which is then iteratively refined by a pretrained monaural denoising vocoder.
- Mechanism: The geometric time warping (GTW) step applies interaural time delays (ITDs) to the mono signal based on source and listener ear positions, while amplitude scaling (AS) adjusts channel amplitudes according to inverse square law distance cues. These operations create a rough binaural signal that mimics spatial cues without needing binaural training data. The WaveFit denoising vocoder then iteratively refines each channel independently using log-mel spectrogram conditioning, effectively "cleaning" the rough binaural output.
- Core assumption: That a monaural denoising vocoder trained on tens of thousands of hours of speech can generalize to reconstruct natural binaural speech when conditioned on spectrograms derived from a geometrically warped signal.
- Evidence anchors:
  - [abstract]: "Specifically, we show that a parameter-free geometric time warping and amplitude scaling based on source location suffices to get an initial binaural synthesis that can be refined by iteratively applying a pretrained denoising vocoder."
  - [section 3.3]: "We use a WaveFit neural vocoder [Koizumi et al., 2022a] as our denoising vocoder model... The pretrained weights we use were trained on the 60k-hour LibriLight audiobook dataset [Kahn et al., 2020] using 128 v3 TPUs for 500k steps."
  - [corpus]: Weak evidence - no corpus neighbors directly address zero-shot monaural-to-binaural synthesis using geometric warping + denoising vocoders.

### Mechanism 2
- Claim: ZeroBAS generalizes well to out-of-distribution room conditions because it never learns room-specific impulse responses or head-related transfer functions, instead relying on the vocoder's learned priors.
- Mechanism: By avoiding explicit modeling of room acoustics (RIRs) and HRTFs, ZeroBAS does not overfit to the specific acoustic characteristics of the training environment. Instead, it produces binaural audio that imputes a generalized low-RIR room and implicit HRTF, regularized by the vast amount of data the vocoder was trained on. This allows it to perform better than supervised methods when evaluated on rooms and acoustic conditions not seen during training.
- Core assumption: That the WaveFit vocoder's training on diverse speech data captures enough acoustic variability to handle unseen room conditions when used in a zero-shot manner.
- Evidence anchors:
  - [abstract]: "Furthermore, we find this leads to generalization across room conditions, which we measure by introducing a new dataset, TUT Mono-to-Binaural, to evaluate state-of-the-art monaural-to-binaural synthesis methods on unseen conditions."
  - [section 5.4]: "Table 5 demonstrates that our zero-shot method, ZeroBAS, significantly outperforms all supervised methods on the TUT mono to binaural dataset."
  - [section 7]: "ZeroBAS uses an off-the-shelf neural vocoder which is conditioned on (log-mel) spectrogram features and no positional information which makes it difficult to condition towards a target phase spectrum. For this reason, our method struggles to directly and accurately process the phase information in binaural audio, leading to high Phase ℓ2 error. Furthermore, our method does not encode or use any room or head shape information. We hypothesize that this fact helps our method generalize to new rooms and acoustic environments, while still not being able to match the performance of supervised methods which are trained on a specific room and acoustic environment."

### Mechanism 3
- Claim: The iterative refinement process with WaveFit effectively denoises and enhances the geometrically warped binaural signal, compensating for the rough approximation created by GTW and AS.
- Mechanism: The WaveFit model is applied iteratively (N times) to each channel separately, starting from the geometrically warped and amplitude-scaled signal as the "noisy" input. Each iteration progressively denoises the signal while conditioning on the log-mel spectrogram of the current estimate and a low noise level timestep. This process effectively refines the rough binaural approximation into natural-sounding speech by leveraging the vocoder's learned mapping from degraded to clean speech.
- Core assumption: That the iterative denoising process with a fixed low noise level timestep can effectively transform the geometrically warped signal into natural binaural speech without introducing artifacts or losing spatial cues.
- Evidence anchors:
  - [section 3.3]: "The temporal sequences of conditioning vectors cl, cr are obtained by extracting the log-mel features of ˆxℓ, ˆxr. A low noise level k is also conditioned on, to reflect that we are emulating an input that is 'close' to a true binaural sample... This sampling is repeated for N iterations."
  - [section 6]: "Increasing the number of iterations until 3 improves the objective metrics Waveℓ2, Amplitude ℓ2 and Phase ℓ2 and improves MOS. After 3 iterations, quality is constant but there is no need to continue iterating."

## Foundational Learning

- Concept: Geometric Time Warping (GTW) and Interaural Time Differences (ITDs)
  - Why needed here: GTW is the first step in ZeroBAS that creates the initial binaural approximation by applying time delays to the mono signal based on the relative positions of the sound source and the listener's ears. Understanding ITDs is crucial because they are the primary spatial cue used in this step.
  - Quick check question: How does GTW calculate the warpfield for each ear, and what physical phenomenon does this warpfield approximate?

- Concept: Amplitude Scaling and Inverse Square Law
  - Why needed here: After GTW, ZeroBAS applies amplitude scaling to adjust the channel amplitudes based on the inverse square law distance cues. This step enhances the spatial perception by mimicking how sound intensity decreases with distance.
  - Quick check question: How does ZeroBAS determine which channel to attenuate and by how much, based on the source and ear positions?

- Concept: Denoising Diffusion Probabilistic Models (DDPMs) and Iterative Refinement
  - Why needed here: ZeroBAS uses WaveFit, a denoising vocoder based on DDPM principles, to iteratively refine the geometrically warped and amplitude-scaled signal. Understanding how DDPMs work and how iterative refinement can progressively denoise a signal is essential for grasping the core of ZeroBAS.
  - Quick check question: How does WaveFit's iterative denoising process differ from traditional vocoder approaches, and why is it suitable for ZeroBAS's zero-shot approach?

## Architecture Onboarding

- Component map: Input (Mono waveform, source position, listener ear positions) → GTW (Applies interaural time delays to create left and right channels) → AS (Adjusts channel amplitudes based on inverse square law) → WaveFit (Pretrained monaural denoising vocoder) → Iterative Refinement (N iterations of WaveFit denoising per channel) → Output (Binaural waveform)

- Critical path: Input → GTW → AS → WaveFit iterations (N) → Output
  The geometric time warping and amplitude scaling steps are critical for creating the initial binaural approximation, while the WaveFit iterations are crucial for refining this approximation into natural-sounding speech.

- Design tradeoffs:
  - Zero-shot vs. supervised: ZeroBAS sacrifices the potential for perfect room-specific binaural rendering in exchange for generalization across unseen conditions.
  - Monoaural vocoder vs. binaural model: Using a monaural vocoder simplifies the architecture but may limit the ability to capture certain binaural-specific artifacts.
  - Number of iterations: More iterations may improve quality but increase computational cost and risk of over-smoothing.

- Failure signatures:
  - Poor spatial perception: If GTW or AS is implemented incorrectly, the initial binaural approximation may lack proper ITDs or ILDs, leading to unnatural spatial cues.
  - Artifacts or distortion: If WaveFit is not well-suited for the degraded input or if too many iterations are used, the output may contain artifacts or lose naturalness.
  - Lack of generalization: If the WaveFit vocoder's learned priors are too specific to certain conditions, ZeroBAS may not generalize well to truly unseen environments.

- First 3 experiments:
  1. Verify GTW and AS implementation: Generate binaural audio using only GTW and AS (without WaveFit) and check if the output has the expected ITDs and ILDs. Compare against ground truth to quantify initial approximation quality.
  2. Test WaveFit refinement with known degraded inputs: Take clean binaural audio, apply a known degradation (e.g., add noise, time-warp), and use ZeroBAS to reconstruct it. Measure the quality improvement to verify WaveFit's denoising capability.
  3. Evaluate generalization on out-of-distribution data: Use ZeroBAS to generate binaural audio from mono speech in a room/acoustic condition not seen during WaveFit's training. Compare against supervised methods to quantify generalization benefits.

## Open Questions the Paper Calls Out

- Open Question 1: Does ZeroBAS maintain performance across diverse languages and speaker characteristics beyond French?
- Open Question 2: What is the impact of room size and acoustic complexity on ZeroBAS performance compared to supervised methods?
- Open Question 3: Can the amplitude scaling (AS) component be further optimized or learned rather than using the fixed inverse-square law?
- Open Question 4: What is the computational overhead of ZeroBAS compared to real-time DSP approaches and how does it scale with iteration count?

## Limitations

- Generalization claims rest on a single out-of-distribution dataset (TUT Mono-to-Binaural), limiting confidence in real-world robustness
- WaveFit vocoder was pretrained on monaural audiobooks, not spatial audio, creating uncertainty about transfer effectiveness
- No ablations are provided showing individual contributions of GTW, AS, and iterative refinement to overall performance
- Study only evaluates single static source positions, not dynamic movement or multiple concurrent sources

## Confidence

- High confidence: The geometric time warping and amplitude scaling mechanisms are well-established physical principles
- Medium confidence: The iterative refinement approach with WaveFit shows promising results, but effectiveness depends on vocoder's ability to generalize to degraded inputs
- Low confidence: Generalization across room conditions is demonstrated on a single dataset, requiring more extensive testing to confirm real-world applicability

## Next Checks

1. Conduct controlled experiments comparing ZeroBAS with GTW/AS only (no WaveFit) to quantify contribution of iterative refinement
2. Test ZeroBAS on multiple out-of-distribution datasets with varying acoustic properties to validate generalization claims
3. Perform perceptual studies specifically targeting spatial quality (ITD/ILD perception) rather than just naturalness to better understand spatial fidelity limitations