---
ver: rpa2
title: 'EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions'
arxiv_id: '2409.18042'
source_url: https://arxiv.org/abs/2409.18042
tags:
- speech
- arxiv
- data
- omni-modal
- emova
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EMOVA is the first omni-modal LLM that achieves state-of-the-art
  performance on both vision-language and speech benchmarks simultaneously. It enables
  end-to-end speech understanding and generation with vivid emotions by using a semantic-acoustic
  disentangled speech tokenizer and a lightweight style module.
---

# EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions

## Quick Facts
- **arXiv ID**: 2409.18042
- **Source URL**: https://arxiv.org/abs/2409.18042
- **Reference count**: 40
- **One-line primary result**: EMOVA is the first omni-modal LLM achieving state-of-the-art performance on both vision-language and speech benchmarks simultaneously.

## Executive Summary
EMOVA is a pioneering omni-modal language model that integrates vision, speech understanding, and speech generation capabilities with emotional expressiveness. By leveraging a semantic-acoustic disentangled speech tokenizer and a lightweight style module, EMOVA achieves significant improvements over existing bi-modal baselines and competitive models like GPT-4o/4V and Gemini Pro 1.5. The model demonstrates superior performance across 11 of 15 vision-language benchmarks and sets new state-of-the-art results in Librispeech ASR, while enabling emotional spoken dialogue with high controllability.

## Method Summary
EMOVA employs a three-stage training approach to achieve omni-modal alignment and instruction tuning. The architecture integrates a continuous vision encoder (QwenViT) with a projector, a semantic-acoustic disentangled speech tokenizer (SPIRAL-based), and a unified LLM (Qwen-2.5 7B) extended to include speech units. Training proceeds through vision-language pre-alignment, omni-modal text-centric alignment using 7.4M bi-modal samples, and omni-modal instruction tuning on 4.4M samples. Joint training of image-text and speech-text alignment enables end-to-end speech understanding and generation with vivid emotions.

## Key Results
- Outperforms GPT-4o/4V and Gemini Pro 1.5 on 11 of 15 vision-language benchmarks
- Sets new state-of-the-art results in Librispeech ASR
- Achieves high controllability in emotion and pitch recognition for emotional spoken dialogue
- Demonstrates strong performance in both speech understanding (ASR) and speech generation (TTS)

## Why This Works (Mechanism)
The success of EMOVA stems from its semantic-acoustic disentangled speech tokenization approach, which separates speech content from emotional style information. This enables the model to understand and generate speech with rich emotional content while maintaining semantic accuracy. The three-stage training process allows for effective omni-modal alignment without requiring large-scale omni-modal datasets, leveraging instead publicly available bi-modal data. The lightweight style module provides fine-grained control over emotional expression without significantly increasing computational overhead.

## Foundational Learning

**Semantic-Acoustic Disentanglement**
- *Why needed*: To separate speech content from emotional style for better control and understanding
- *Quick check*: Verify speech units maintain semantic meaning while style information is isolated in separate embeddings

**SPIRAL Architecture**
- *Why needed*: To encode speech into discrete units while preserving both semantic and acoustic information
- *Quick check*: Ensure encoder produces consistent units for same phonetic content across different emotional styles

**FSQ Quantization**
- *Why needed*: To discretize continuous speech representations into learnable speech units
- *Quick check*: Confirm quantization preserves speech quality while enabling efficient LLM processing

**Three-Stage Training**
- *Why needed*: To progressively build capabilities from vision-language alignment to full omni-modal instruction following
- *Quick check*: Validate performance improvements at each stage through benchmark evaluation

## Architecture Onboarding

**Component Map**
QwenViT Encoder -> Vision Projector -> LLM
Speech Encoder (SPIRAL) -> FSQ Quantizer -> LLM
Text Tokenizer -> LLM

**Critical Path**
Speech/audio input → SPIRAL encoder → FSQ quantizer → LLM → Text output (for understanding)
Text input → LLM → U2S detokenizer → Speech output (for generation)

**Design Tradeoffs**
- Using publicly available bi-modal data instead of large omni-modal datasets reduces data collection costs but may limit cross-modal reasoning capabilities
- Semantic-acoustic disentanglement improves emotional control but adds complexity to the speech processing pipeline
- Lightweight style module minimizes computational overhead but may have limited expressiveness compared to more complex approaches

**Failure Signatures**
- Poor vision-language performance indicates issues with projector training or vision encoder alignment
- Degraded ASR/TTS quality suggests problems with speech tokenization or detokenization
- Inconsistent emotional expression points to style module or disentanglement issues

**First Experiments**
1. Evaluate vision-language performance on MME and TextVQA after Stage 1 and Stage 2
2. Test ASR performance on LibriSpeech and TTS quality after Stage 2
3. Assess emotional controllability on style classification accuracy after Stage 3

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused primarily on controlled datasets, limiting assessment of real-world generalization
- Claims about emotional controllability and style module utility lack extensive validation across diverse scenarios
- Computational efficiency trade-offs of the lightweight style module versus potential performance gains require further exploration

## Confidence
- **Technical Architecture**: High - Well-documented design with clear three-stage training methodology
- **Benchmark Performance**: High - Results on established vision-language and speech benchmarks are well-documented
- **Emotional Controllability**: Medium - Less extensive validation across diverse emotional contexts
- **Practical Utility**: Medium - Real-world applicability beyond controlled datasets needs further assessment

## Next Checks
1. Test EMOVA's emotional speech generation capabilities on out-of-domain emotional contexts (e.g., spontaneous conversation or storytelling) to assess generalization
2. Conduct ablation studies to quantify the specific contribution of the semantic-acoustic disentanglement versus alternative speech tokenization approaches
3. Evaluate computational efficiency and latency trade-offs compared to existing bi-modal and omni-modal models across different hardware configurations