---
ver: rpa2
title: 'LLMs May Not Be Human-Level Players, But They Can Be Testers: Measuring Game
  Difficulty with LLM Agents'
arxiv_id: '2410.02829'
source_url: https://arxiv.org/abs/2410.02829
tags:
- game
- human
- difficulty
- llms
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether large language models (LLMs) can serve
  as effective testers for measuring game difficulty in game development. The authors
  propose a general framework where LLM agents play games and their performance metrics
  are used as proxies for difficulty, with the aim of correlating with human-perceived
  difficulty.
---

# LLMs May Not Be Human-Level Players, But They Can Be Testers: Measuring Game Difficulty with LLM Agents

## Quick Facts
- arXiv ID: 2410.02829
- Source URL: https://arxiv.org/abs/2410.02829
- Authors: Chang Xiao; Brenda Z. Yang
- Reference count: 40
- Primary result: LLM performance metrics correlate strongly with human-indicated game difficulty

## Executive Summary
This paper explores whether large language models (LLMs) can serve as effective testers for measuring game difficulty in game development. The authors propose a general framework where LLM agents play games and their performance metrics are used as proxies for difficulty, with the aim of correlating with human-perceived difficulty. They implement and test this framework on two widely played strategy games—Wordle and Slay the Spire—using GPT-3.5 and GPT-4 models with various prompting techniques including Chain-of-Thought reasoning. The results show that while LLMs generally perform below average human players, their performance demonstrates a statistically significant and strong correlation with human-indicated difficulty.

## Method Summary
The researchers implemented a framework where LLM agents interact with games through text-based interfaces, receiving game state information as natural language and generating actions based on prompting techniques. They tested this on Wordle (529 puzzles) and Slay the Spire (boss battles) using GPT-3.5 and GPT-4 with zero-shot, Chain-of-Thought, and Chain-of-Thought+ prompting. Performance metrics included average guesses in Wordle and remaining HP in Slay the Spire, which were then correlated with human performance data. No model fine-tuning was performed—only prompting techniques were varied to guide LLM behavior.

## Key Results
- GPT-4 with Chain-of-Thought prompting achieved correlations up to 0.871 with human difficulty assessments
- LLM performance correlates significantly with human difficulty perception even when LLMs play worse than average humans
- Chain-of-Thought prompting significantly improves both LLM performance and correlation with human behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can serve as effective proxies for human difficulty assessment even when they play worse than average humans.
- Mechanism: The LLM's performance metrics correlate strongly with human-indicated difficulty, meaning that when the LLM struggles more on a challenge, humans also find that challenge harder.
- Core assumption: The correlation between LLM performance and human difficulty perception is sufficient for difficulty assessment, even if absolute performance differs.
- Evidence anchors:
  - [abstract] "their performance, when guided by simple, generic prompting techniques, shows a statistically significant and strong correlation with difficulty indicated by human players"
  - [section] "puzzles hard for human players are similarly difficult for LLM agents" and "GPT-4 CoT shows the strongest correlation...up to 0.871"
- Break condition: If the correlation between LLM performance and human difficulty perception becomes weak or statistically insignificant, the mechanism fails.

### Mechanism 2
- Claim: Chain-of-Thought (CoT) prompting significantly improves LLM performance and its correlation with human difficulty assessment.
- Mechanism: CoT prompting encourages LLMs to reason step-by-step through game decisions, leading to more human-like strategic thinking and better alignment with human performance patterns.
- Core assumption: The reasoning process induced by CoT prompting mirrors human decision-making processes in games.
- Evidence anchors:
  - [abstract] "when guided by simple, generic prompting techniques" and "GPT-4 with Chain-of-Thought prompting achieved correlations up to 0.871"
  - [section] "the implementation of advanced prompting techniques such as CoT reasoning...largely enhance the LLM agents' gameplay performance" and "more advanced LLMs, such as GPT-4, perform better...when CoT reasoning and external strategies are applied"
- Break condition: If CoT prompting does not improve correlation or actually decreases it, the mechanism fails.

### Mechanism 3
- Claim: LLM agents better approximate human difficulty assessment than heuristic rule-based AI systems.
- Mechanism: LLMs can generate human-like reasoning and decision-making patterns that align more closely with human players, while rule-based systems optimize for different metrics that don't match human experience.
- Core assumption: Human-like reasoning is necessary for accurate difficulty assessment, not just optimal performance.
- Evidence anchors:
  - [abstract] "LLMs may not perform as well as the average human player, their performance...shows a statistically significant and strong correlation with difficulty indicated by human players"
  - [section] "the Wordle Solver, despite its near-optimal gameplay, exhibits a very weak correlation with human performance" and "LLMs are better than heuristic AIs at reasoning and acting like humans"
- Break condition: If heuristic rule-based systems show equal or better correlation with human difficulty than LLMs, the mechanism fails.

## Foundational Learning

- Concept: Correlation vs. Causation
  - Why needed here: Understanding that strong correlation between LLM performance and human difficulty doesn't mean LLMs are playing "correctly" or that their reasoning matches human reasoning exactly
  - Quick check question: If an LLM shows 0.9 correlation with human difficulty but plays completely differently, does this invalidate the difficulty assessment approach?

- Concept: Prompt Engineering Techniques
  - Why needed here: Different prompting strategies (zero-shot, CoT, CoT+) have dramatically different impacts on both performance and correlation with human behavior
  - Quick check question: What happens to correlation if you use zero-shot prompting versus CoT prompting on the same LLM model?

- Concept: Text Representation for Game State
  - Why needed here: The format and structure of game information fed to LLMs significantly affects their ability to understand and respond appropriately
  - Quick check question: Why did reformating Wordle words from plain text to lists like "[A, P, P, L, E]" improve LLM performance?

## Architecture Onboarding

- Component map:
  Game I/O Component -> Instruction Component -> LLM Agent -> Action Generator

- Critical path:
  1. Extract game state through APIs
  2. Convert game information to natural language text
  3. Combine with game rules/strategies using prompting techniques
  4. Send to LLM for action generation
  5. Parse LLM response into executable game actions
  6. Execute action and repeat until game completion

- Design tradeoffs:
  - Using more advanced LLM models (GPT-4 vs GPT-3.5) improves correlation but increases computational cost
  - Adding compensation mechanisms for LLM performance limitations helps gather more data but risks distorting difficulty assessment
  - Text-only representation limits applicability to visually intensive games

- Failure signatures:
  - Low or statistically insignificant correlation between LLM performance metrics and human difficulty indicators
  - LLM consistently fails challenges or performs at ceiling/floor levels across all difficulties
  - Significant discrepancies between LLM performance trends and human performance trends

- First 3 experiments:
  1. Implement zero-shot prompting on GPT-3.5 for Wordle and measure correlation with human data
  2. Add CoT prompting to the same setup and compare correlation improvements
  3. Test GPT-4 with CoT prompting on Slay the Spire and measure correlation with human win rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LLM-based difficulty testing remain effective when games include cumulative effects where player resources or abilities carry over between challenges?
- Basis in paper: [inferred] Future Work section mentions this as an area needing exploration
- Why unresolved: The paper only tested isolated challenges in Wordle and Slay the Spire, not sequences where performance in one challenge affects subsequent ones
- What evidence would resolve it: Testing the framework on games with progressive difficulty curves where early performance impacts later challenges, measuring correlation with human difficulty perception

### Open Question 2
- Question: How does LLM difficulty assessment change when agents are allowed to learn and improve through multiple attempts at the same challenge, simulating human skill development?
- Basis in paper: [inferred] Future Work section suggests exploring dynamic learning through repeated trials
- Why unresolved: The paper tested LLMs as static agents without learning mechanisms, while human players typically improve with practice
- What evidence would resolve it: Comparing difficulty assessments from static vs. learning LLM agents across multiple trials of identical challenges

### Open Question 3
- Question: Can LLM agents effectively evaluate non-difficulty aspects of game quality such as bug detection, gameplay balance, or user experience factors?
- Basis in paper: [explicit] Future Work section explicitly mentions this as an open area
- Why unresolved: The paper focused exclusively on difficulty measurement, not other quality assurance aspects
- What evidence would resolve it: Testing LLM agents on identifying bugs, detecting balance issues, or assessing user experience in games, and comparing their findings with human testers

## Limitations
- Results based on text-based games may not generalize to visually intensive or action-based games
- The compensation mechanisms used to improve LLM performance could potentially introduce bias in difficulty assessment
- Correlation does not prove that LLMs are reasoning identically to humans, only that their performance patterns align

## Confidence
- High Confidence: The core finding that LLM performance correlates with human difficulty assessment (p < 0.05 across all tested configurations)
- Medium Confidence: The superiority of CoT prompting over zero-shot approaches for difficulty measurement
- Medium Confidence: The claim that LLMs better approximate human difficulty than heuristic AI systems

## Next Checks
1. Test the framework on visually intensive games requiring image inputs rather than text descriptions to evaluate generalizability beyond text-based games
2. Compare LLM-based difficulty assessment with human playtesting on new game designs that weren't in the original dataset
3. Evaluate whether the correlation holds when using smaller, less capable LLM models that would be more practical for commercial game development