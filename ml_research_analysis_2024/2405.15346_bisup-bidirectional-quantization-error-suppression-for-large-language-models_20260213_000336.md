---
ver: rpa2
title: 'BiSup: Bidirectional Quantization Error Suppression for Large Language Models'
arxiv_id: '2405.15346'
source_url: https://arxiv.org/abs/2405.15346
tags:
- quantization
- bisup
- atom
- uni00000013
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of bidirectional quantization
  error propagation in large language models (LLMs) under weight-activation quantization.
  The proposed BiSup method constructs optimizable parameter spaces and employs quantization-aware
  parameter-efficient fine-tuning to suppress vertical error accumulation, while introducing
  a prompt mixed-precision quantization strategy to mitigate horizontal error diffusion
  across tokens.
---

# BiSup: Bidirectional Quantization Error Suppression for Large Language Models

## Quick Facts
- arXiv ID: 2405.15346
- Source URL: https://arxiv.org/abs/2405.15346
- Reference count: 40
- Reduces WikiText2 perplexity from 13.26 to 9.41 for Atom and from 14.33 to 7.85 for QuaRot under W3A3-g128 configuration

## Executive Summary
This paper addresses the critical problem of bidirectional quantization error propagation in large language models under weight-activation quantization. The authors identify that quantization errors accumulate vertically within tokens through layers and diffuse horizontally across tokens via self-attention mechanisms, degrading model performance. Their proposed BiSup method introduces optimizable parameter spaces and quantization-aware parameter-efficient fine-tuning to suppress vertical error accumulation, while employing a prompt mixed-precision quantization strategy to mitigate horizontal error diffusion.

The method demonstrates significant improvements over state-of-the-art quantization techniques, achieving lower perplexity scores on WikiText2 and C4 benchmarks while maintaining zero-shot accuracy on multiple tasks. By keeping system prompts at high precision and optimizing quantization parameters layer-wise, BiSup effectively addresses the unique challenges of quantizing large language models without requiring extensive retraining or calibration data.

## Method Summary
BiSup constructs optimizable parameter spaces for fine-grained weight-activation clipping, soft-constrained weight-activation smoothing, and stabilized low-rank error compensation. The method employs quantization-aware parameter-efficient fine-tuning with layer-wise optimization to suppress vertical error accumulation. A prompt mixed-precision quantization strategy preserves high-precision system prompts to minimize horizontal error diffusion across tokens. The approach uses a small calibration dataset (128 samples) and AdamW optimizer with learning rate 0.005 for 5 epochs, achieving effective quantization while maintaining model performance.

## Key Results
- Reduces WikiText2 perplexity from 13.26 to 9.41 for Atom under W3A3-g128 configuration
- Reduces WikiText2 perplexity from 14.33 to 7.85 for QuaRot under W3A3-g128 configuration
- Maintains zero-shot accuracy on ARC, BoolQ, HellaSwag, PIQA, and Winogrande tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Bidirectional quantization error suppression is needed because quantization errors accumulate vertically within the same token through layers and diffuse horizontally across different tokens due to self-attention mechanisms.
- **Mechanism**: The method constructs optimizable parameter spaces and employs quantization-aware parameter-efficient fine-tuning to suppress vertical error accumulation, while introducing prompt mixed-precision quantization to mitigate horizontal error diffusion.
- **Core assumption**: The bidirectional propagation of quantization errors significantly degrades LLM performance and can be effectively suppressed through targeted optimization strategies.
- **Evidence anchors**: Weak evidence. While related papers discuss mixed-precision quantization, none explicitly address bidirectional error propagation in the manner described.

### Mechanism 2
- **Claim**: Soft-constrained weight-activation smoothing effectively migrates quantization difficulty from activations to weights, improving overall quantization accuracy.
- **Mechanism**: By scaling down activations and scaling up weights while maintaining computational invariance, the method reduces outlier impact on quantization.
- **Core assumption**: Activations are more difficult to quantize than weights due to outliers, and migrating this difficulty to weights improves quantization performance.
- **Evidence anchors**: Moderate evidence. Papers like SmoothQuant and OmniQuant discuss similar smoothing mechanisms, providing indirect support.

### Mechanism 3
- **Claim**: Prompt mixed-precision quantization strategy effectively reduces errors in important token interactions by retaining high-precision system prompts.
- **Mechanism**: By analyzing attention maps, the method identifies that LLMs have a strong dependency on the first token, which is part of system prompts, and preserves its high precision.
- **Core assumption**: The first token in instruction-tuned LLMs is crucial for maintaining interaction patterns and can be effectively identified and preserved at high precision.
- **Evidence anchors**: Weak evidence. While the concept of prompt caching is mentioned in related papers, none specifically discuss the mixed-precision quantization of system prompts.

## Foundational Learning

- **Concept**: Quantization-Aware Training (QAT)
  - **Why needed here**: QAT is mentioned as requiring substantial data and computational resources, contrasting with the proposed method's approach of using a small amount of data for quantization-aware parameter-efficient fine-tuning.
  - **Quick check question**: What are the key differences between QAT and Post-Training Quantization (PTQ) in terms of data and computational requirements?

- **Concept**: Post-Training Quantization (PTQ)
  - **Why needed here**: PTQ is discussed as requiring only a small amount of calibration data and computational resources, which aligns with the proposed method's approach.
  - **Quick check question**: How does PTQ typically achieve quantization without the need for extensive retraining?

- **Concept**: Mixed-Precision Quantization
  - **Why needed here**: Mixed-precision quantization is mentioned as a direction explored by existing methods, which the proposed method builds upon by introducing prompt mixed-precision quantization.
  - **Quick check question**: What are the benefits and challenges of using different precision levels for different parts of the model?

## Architecture Onboarding

- **Component map**: Optimizable parameter spaces (Θ1, Θ2, Θ3) → fine-grained weight-activation clipping → soft-constrained weight-activation smoothing → stabilized low-rank error compensation → prompt mixed-precision quantization strategy
- **Critical path**: Preprocess model → calculate and store high-precision system prompts → initialize learnable parameters → perform gradient optimization → quantize transformer layers
- **Design tradeoffs**: Trades off between accuracy and memory consumption by adopting layer-wise optimization strategy and using small calibration dataset
- **Failure signatures**: Poor perplexity results compared to baseline methods; persistent errors in important token interactions
- **First 3 experiments**:
  1. Evaluate quantized models using language generation tasks (WikiText2 and C4) to measure perplexity improvements
  2. Conduct ablation studies to validate effectiveness of each technique used in proposed method
  3. Analyze error propagation of different quantization methods to verify suppression of bidirectional quantization errors

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal layer-wise optimization strategy that correlates with final cross-entropy loss rather than mean square error?
- **Basis in paper**: [explicit] The paper explicitly identifies that layer-wise optimization based on MSE does not adequately correlate with final results and tends to fall into local optima under simpler quantization configurations.
- **Why unresolved**: The paper identifies this as a limitation but does not propose or test specific alternative optimization strategies.
- **What evidence would resolve it**: Comparative experiments showing improved perplexity and accuracy when using cross-layer optimization or weighted MSE loss that accounts for token importance.

### Open Question 2
- **Question**: How does the performance of BiSup scale with different calibration dataset sizes and qualities?
- **Basis in paper**: [explicit] The paper notes that BiSup can produce better results with more samples but uses only 128 samples for experiments, and mentions exploring high-quality data as a potential improvement.
- **Why unresolved**: The paper only tests with a fixed small calibration dataset size without exploring the relationship between dataset size/quality and performance.
- **What evidence would resolve it**: Systematic experiments varying calibration dataset sizes and qualities while measuring performance trade-offs.

### Open Question 3
- **Question**: What is the optimal prompt mixed-precision quantization strategy for user prompts beyond system prompts?
- **Basis in paper**: [explicit] The paper mentions that important tokens also exist in user prompts that could be dynamically identified and preserved at high precision during inference.
- **Why unresolved**: The paper only implements prompt mixed-precision quantization for system prompts and does not explore dynamic identification of important tokens in user prompts.
- **What evidence would resolve it**: Implementation and evaluation of dynamic importance-aware mixed precision quantization for both system and user prompts showing performance improvements.

## Limitations

- Experimental validation uses only 128 calibration samples from WikiText2, potentially limiting generalizability
- Layer-wise optimization strategy lacks theoretical justification for correlation with final performance
- Prompt mixed-precision quantization assumes system prompts are always the first token, which may not generalize across all LLM architectures
- Method's effectiveness on models beyond Llama and Qwen families remains unverified

## Confidence

- **Mechanism 1 (Bidirectional Error Propagation)**: Medium confidence - The conceptual framework is sound, but empirical evidence for the magnitude of both vertical and horizontal error propagation is limited to specific architectures.
- **Mechanism 2 (Soft-constrained Smoothing)**: Medium confidence - The smoothing approach is well-supported by related work, but the claim about migrating quantization difficulty from activations to weights needs more rigorous validation across different outlier distributions.
- **Mechanism 3 (Prompt Mixed-Precision)**: Low confidence - The reliance on system prompts being the first token is a strong assumption that may not hold universally, and the attention map analysis appears specific to certain model families.

## Next Checks

1. **Error Propagation Quantification**: Conduct ablation studies measuring quantization error accumulation rates across different numbers of layers and attention heads to empirically verify the claimed vertical and horizontal error propagation patterns across multiple model architectures.

2. **Generalization Across Model Families**: Test BiSup on diverse LLM architectures including OPT, BLOOM, and GPT-style models to validate whether the prompt mixed-precision strategy and error suppression mechanisms generalize beyond Llama and Qwen families.

3. **Calibration Data Sensitivity Analysis**: Systematically vary the calibration dataset size (from 16 to 1024 samples) and composition to determine the minimum effective calibration requirements and assess whether the 128-sample choice is optimal or merely sufficient.