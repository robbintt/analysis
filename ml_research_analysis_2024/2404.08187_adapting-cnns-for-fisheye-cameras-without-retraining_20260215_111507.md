---
ver: rpa2
title: Adapting CNNs for Fisheye Cameras without Retraining
arxiv_id: '2404.08187'
source_url: https://arxiv.org/abs/2404.08187
tags: []
core_contribution: The paper addresses the challenge of adapting pre-trained convolutional
  neural networks (CNNs) for use with fisheye cameras without requiring retraining.
  The core method, Rectified Convolutions (RectConv), modifies the convolutional layers
  of pre-trained networks to handle the distortion inherent in fisheye images.
---

# Adapting CNNs for Fisheye Cameras without Retraining

## Quick Facts
- arXiv ID: 2404.08187
- Source URL: https://arxiv.org/abs/2404.08187
- Authors: Ryan Griffiths; Donald G. Dansereau
- Reference count: 0
- Primary result: Rectified Convolutions (RectConv) enable pre-trained CNNs to process fisheye imagery without retraining, achieving up to 31.68% MIOU improvement on Woodscape and up to 76.91% precision improvement on PIROPO for segmentation.

## Executive Summary
This paper addresses the challenge of adapting pre-trained convolutional neural networks (CNNs) for use with fisheye cameras without requiring retraining. The core method, Rectified Convolutions (RectConv), modifies the convolutional layers of pre-trained networks to handle the distortion inherent in fisheye images. Instead of adapting the image to the network, RectConv adjusts the kernel shape to match the local deformation at each image location, allowing the network to process fisheye imagery directly.

The primary results demonstrate that RectConv significantly improves performance compared to naive application of pre-trained networks or pre-rectification methods. On the Woodscape dataset, RectConv achieved mean intersection over union (MIOU) improvements of up to 31.68% and pixel accuracy improvements of up to 89.12% for segmentation tasks. For the PIROPO dataset, RectConv showed precision improvements of up to 76.91% and recall improvements of up to 48.05% for segmentation, and precision improvements of up to 86.67% and recall improvements of up to 65.66% for object detection. While RectConv introduces a computational overhead, it outperforms patch-based methods in terms of both accuracy and inference time.

## Method Summary
The Rectified Convolutions (RectConv) method modifies pre-trained CNN convolutional layers to handle fisheye distortion without retraining. The key innovation is adapting the convolution kernel shape to match local image deformation at each spatial location, rather than warping the entire image or retraining the network. This is achieved by sampling the input image according to the fisheye projection model, computing the deformation at each pixel, and adjusting the kernel sampling coordinates accordingly. The method maintains the pre-trained weights while changing how they interact with the distorted input. The approach works by creating location-specific kernel deformations that compensate for the radial distortion, allowing standard CNNs to process fisheye imagery with minimal modification to the network architecture.

## Key Results
- On Woodscape dataset: up to 31.68% MIOU improvement and 89.12% pixel accuracy improvement for segmentation tasks
- On PIROPO dataset: up to 76.91% precision and 48.05% recall improvement for segmentation, with 86.67% precision and 65.66% recall improvement for object detection
- RectConv outperforms both naive application of pre-trained networks and pre-rectification methods across all tested metrics
- Computational overhead is lower than patch-based methods while maintaining superior accuracy

## Why This Works (Mechanism)
RectConv works by directly addressing the fundamental mismatch between rectilinear CNN architectures and fisheye projection geometry. Standard CNNs assume uniform sampling patterns, but fisheye lenses create non-uniform spatial relationships where pixel distances vary dramatically with radius from the image center. By deforming the convolution kernel at each location to match the local fisheye distortion, RectConv ensures that the network's learned features (optimized for rectilinear views) are applied to appropriately transformed image regions. This kernel-level adaptation preserves the semantic knowledge captured during pre-training while accommodating the geometric differences, effectively bridging the domain gap between standard and fisheye imagery without the need for expensive retraining or complete network redesign.

## Foundational Learning

**Fisheye Projection Models** - Understanding how fisheye lenses map 3D points to 2D images is essential for computing the distortion at each pixel location. Quick check: Can you derive the projection equation for a common fisheye model like equidistant or equisolid angle?

**Convolutional Neural Networks** - Knowledge of how standard convolution operations work, including kernel sampling patterns and weight application, is necessary to understand how RectConv modifies these operations. Quick check: Can you explain how a standard 3x3 convolution samples its input?

**Camera Calibration** - Accurate intrinsic parameters (focal length, distortion coefficients) are required to compute the local deformation at each pixel. Quick check: What camera parameters are needed to fully describe a fisheye lens distortion model?

## Architecture Onboarding

**Component Map:** Input Image -> Rectified Convolution Layers -> Standard CNN Backbone -> Task-specific Head
**Critical Path:** Fisheye image → Local distortion computation → Kernel deformation → Standard convolution → Feature extraction → Output prediction
**Design Tradeoffs:** Accuracy vs computational overhead (kernel deformation adds cost but avoids retraining), generality vs specificity (works with any pre-trained CNN but requires accurate calibration)
**Failure Signatures:** Poor performance when camera calibration is inaccurate, degraded results with extreme distortion beyond model assumptions, computational bottlenecks at high resolutions
**First Experiments:** 1) Apply RectConv to a simple classification task with synthetic fisheye distortion, 2) Compare performance against pre-rectification baseline on a small segmentation dataset, 3) Measure computational overhead at different input resolutions

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead may be prohibitive for real-time applications, especially at higher resolutions
- Performance depends on accurate camera calibration, which may not always be available in practice
- Evaluation is primarily focused on automotive and indoor scenarios, with limited testing across diverse fisheye camera models and varying distortion profiles

## Confidence
- Core RectConv methodology and effectiveness: **High**
- Performance improvements over baseline methods: **High**
- Computational efficiency compared to patch-based methods: **Medium** (based on specific hardware and implementation details not fully disclosed)

## Next Checks
1. Evaluate RectConv performance across a broader range of fisheye camera models and distortion profiles to assess generalizability
2. Conduct ablation studies to quantify the impact of camera calibration accuracy on final performance
3. Benchmark real-time performance on embedded systems to determine practical deployment feasibility