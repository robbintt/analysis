---
ver: rpa2
title: The Influence of Faulty Labels in Data Sets on Human Pose Estimation
arxiv_id: '2409.03887'
source_url: https://arxiv.org/abs/2409.03887
tags:
- data
- coco
- sets
- mpii
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how faulty labels in widely-used human pose
  estimation datasets impact model performance. Authors analyzed two main datasets
  (COCO and MPII) and found approximately 2% positional and left-right swap errors,
  along with false annotations.
---

# The Influence of Faulty Labels in Data Sets on Human Pose Estimation

## Quick Facts
- arXiv ID: 2409.03887
- Source URL: https://arxiv.org/abs/2409.03887
- Authors: Arnold Schwarz; Levente Hernadi; Felix Bießmann; Kristian Hildebrand
- Reference count: 40
- One-line primary result: Cleaning faulty labels in COCO and MPII datasets improves HPE metrics by 0.9-5.2 percentage points using a multi-model outlier detection heuristic

## Executive Summary
This study investigates how faulty labels in widely-used human pose estimation datasets impact model performance. The authors analyze COCO and MPII datasets and find approximately 2% positional and left-right swap errors. They develop a heuristic using multiple models to detect label errors by comparing prediction-annotation distances. When applying their cleaning method to validation data, they observed improvements in standard metrics: COCO AP increased from 76.6% to 77.5% and MPII PCKh@0.5 from 90.0% to 95.2%. The impact was more pronounced when cleaning training data. The heuristic detected errors with 15-41% precision and 22-33% recall.

## Method Summary
The authors developed a heuristic to detect faulty labels by running five different top-down pose estimation models (ResNet50, ResNeSt, SE-ResNet50, SCNet50, HRNet_w32) on COCO and MPII datasets. They computed prediction-annotation distances for each keypoint and used Isolation Forest to identify outliers as potential faulty labels. The method was applied to both validation and training data, with cleaned datasets then used to retrain models and evaluate performance improvements. Manual verification of detected errors was performed on 100 instances per dataset to assess the heuristic's precision.

## Key Results
- COCO AP improved from 76.6% to 77.5% and MPII PCKh@0.5 from 90.0% to 95.2% after cleaning validation data
- Faulty labels were found in approximately 2% of annotations in both COCO and MPII datasets
- Cleaning training data had more pronounced impact on model performance than cleaning validation data
- The heuristic detected errors with 15-41% precision and 22-33% recall

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heuristic outlier detection improves HPE metrics by removing faulty annotations that cause model confusion.
- Mechanism: The heuristic compares model predictions to annotations, flags large deviations as outliers, and removes them. Since most annotations are correct, models learn cleaner patterns when faulty ones are removed.
- Core assumption: HPE models learn accurate predictions for correctly annotated keypoints, and large prediction-annotation discrepancies indicate faulty labels.
- Evidence anchors:
  - [abstract] "When applying their cleaning method to validation data, they observed improvements in standard metrics: COCO AP increased from 76.6% to 77.5% and MPII PCKh@0.5 from 90.0% to 95.2%."
  - [section] "We use the distance δm between the predictions ˆym of model m ∈ {1, . . . , M} and the ground truth y to create a feature vector ∆ = [δ1, δ2, . . . , δM]."
  - [corpus] Weak evidence - corpus doesn't contain direct support for outlier detection mechanism, only related HPE papers.
- Break condition: If the assumption that models learn accurate predictions fails (e.g., under severe label noise), the heuristic may incorrectly flag correct annotations as faulty.

### Mechanism 2
- Claim: Cleaning training data has larger impact on model performance than cleaning validation data.
- Mechanism: Faulty training annotations act as noise during learning, causing models to learn incorrect patterns. Removing them allows cleaner gradient signals and better generalization.
- Core assumption: Label noise in training data negatively impacts model learning, while validation data noise only affects evaluation metrics.
- Evidence anchors:
  - [abstract] "The impact was more pronounced when cleaning training data."
  - [section] "This effect can be seen for the model trained on cleaned MPII data for almost all body parts except the hips."
  - [corpus] Weak evidence - corpus contains related HPE papers but no direct evidence about training vs validation cleaning impact.
- Break condition: If the model is robust to label noise or if the faulty annotations are distributed similarly to correct ones, cleaning may have minimal impact.

### Mechanism 3
- Claim: Human annotators struggle with consistency, making some "hard" poses appear as faulty annotations to the heuristic.
- Mechanism: Poses that are difficult for humans to annotate (requiring assumptions or being time-consuming) create label noise. The heuristic flags these as outliers, conflating annotation difficulty with annotation error.
- Core assumption: Hard poses for humans are more likely to be incorrectly annotated, and models find these poses difficult to predict accurately.
- Evidence anchors:
  - [section] "The evaluation shows that our heuristic indeed has the tendency to discard hard poses (see Figure 6a), but we can also see that hard poses are generally more prone to incorrect annotations."
  - [section] "We assume that similar difficulties exist for human annotators to detect hip joints correctly and therefore ground truth positions scatter a lot."
  - [corpus] Weak evidence - corpus doesn't directly address the relationship between annotation difficulty and error rates.
- Break condition: If hard poses are systematically annotated correctly or if models learn to handle these poses well, the heuristic may unnecessarily remove useful training data.

## Foundational Learning

- Concept: Outlier detection using Isolation Forest
  - Why needed here: The paper uses Isolation Forest to identify annotation errors by treating large prediction-annotation distances as outliers.
  - Quick check question: How does Isolation Forest differ from other outlier detection methods like LOF or DBSCAN in handling high-dimensional feature spaces?

- Concept: Human Pose Estimation metrics (OKS, PCKh)
  - Why needed here: The paper evaluates HPE performance using COCO's OKS metric and MPII's PCKh@0.5 metric, which have different formulations and sensitivities to annotation errors.
  - Quick check question: What is the key difference between OKS and PCKh in terms of how they handle keypoint visibility and scale?

- Concept: Label noise and its impact on model training
  - Why needed here: The paper investigates how faulty labels in training data affect model learning and generalization performance.
  - Quick check question: How does label noise in training data typically affect model convergence and final performance compared to clean data?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model inference -> Distance calculation -> Outlier detection -> Data cleaning -> Model retraining -> Evaluation

- Critical path: Model inference → Distance calculation → Outlier detection → Data cleaning → Model retraining → Evaluation
- Design tradeoffs:
  - Using multiple models for detection increases robustness but requires more computation
  - Isolation Forest provides non-parametric outlier detection but requires threshold calibration
  - Removing hard poses may improve metrics but could reduce model robustness
- Failure signatures:
  - If outlier detection threshold is too low: Valid annotations are incorrectly removed
  - If threshold is too high: Faulty annotations remain in the dataset
  - If models used for detection are poorly calibrated: The heuristic becomes unreliable
- First 3 experiments:
  1. Run the heuristic on a small subset of COCO validation data and manually verify flagged annotations
  2. Compare HPE metrics before and after cleaning validation data for a single model
  3. Retrain a model on cleaned training data and evaluate on both original and cleaned validation sets

## Open Questions the Paper Calls Out
- The paper doesn't explicitly call out open questions, but several remain implicit in the discussion:
  - How to improve the heuristic's precision and recall to make it more reliable
  - Whether the cleaning methodology can be applied to other HPE datasets beyond COCO and MPII
  - How to distinguish between genuinely faulty annotations and difficult poses that models struggle with
  - The optimal threshold for outlier detection that balances removing errors while preserving useful data

## Limitations
- The heuristic has moderate precision (15-41%) and recall (22-33%), meaning it may miss many faulty annotations while potentially flagging some correct ones
- Analysis is limited to only two datasets (COCO and MPII), which may not generalize to other HPE datasets or domains
- Manual verification of detected errors was limited to only 100 instances per dataset, which may not be representative of the overall error distribution

## Confidence
- **High Confidence**: The observed metric improvements (COCO AP from 76.6% to 77.5% and MPII PCKh@0.5 from 90.0% to 95.2%) are well-supported by experimental results and demonstrate the practical impact of cleaning faulty labels.
- **Medium Confidence**: The claim that faulty labels negatively impact model performance is supported by the metric improvements, but the exact magnitude of this impact may vary depending on the specific model architecture and training procedure.
- **Low Confidence**: The assertion that hard poses are systematically more prone to incorrect annotations lacks strong empirical support beyond the limited manual verification and requires further investigation.

## Next Checks
1. Apply the heuristic to additional HPE datasets (e.g., LSP, CrowdPose) to evaluate whether the 2% error rate and cleaning benefits generalize beyond COCO and MPII.
2. Manually categorize and quantify the types of annotation errors detected by the heuristic to understand its limitations and potential biases.
3. Train models with varying degrees of label noise to quantify how robust different architectures are to faulty annotations and whether the heuristic's benefits depend on model choice.