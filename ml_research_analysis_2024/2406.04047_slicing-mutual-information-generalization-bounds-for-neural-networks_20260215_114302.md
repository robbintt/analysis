---
ver: rpa2
title: Slicing Mutual Information Generalization Bounds for Neural Networks
arxiv_id: '2406.04047'
source_url: https://arxiv.org/abs/2406.04047
tags:
- generalization
- bounds
- error
- neural
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces new information-theoretic generalization bounds
  tailored for learning algorithms trained on random lower-dimensional subspaces,
  motivated by recent findings on the compressibility of neural networks. The proposed
  bounds rely on disintegrated mutual information and k-sliced mutual information,
  offering significant computational and statistical advantages over standard mutual
  information bounds.
---

# Slicing Mutual Information Generalization Bounds for Neural Networks

## Quick Facts
- arXiv ID: 2406.04047
- Source URL: https://arxiv.org/abs/2406.04047
- Reference count: 40
- Introduces information-theoretic generalization bounds for neural networks trained on random lower-dimensional subspaces, achieving non-vacuous bounds for compressible models

## Executive Summary
This paper addresses the challenge of computing non-vacuous information-theoretic generalization bounds for neural networks by leveraging recent insights about their compressibility. The authors propose bounds based on disintegrated mutual information and k-sliced mutual information, which are computationally more tractable than standard mutual information bounds. By training networks on random d-dimensional subspaces and incorporating model compressibility through rate-distortion theory, the proposed bounds demonstrate tighter characterizations of generalization error. Empirically, the approach achieves non-vacuous bounds on MNIST and CIFAR-10 datasets, marking a significant advance in information-theoretic generalization analysis for neural networks.

## Method Summary
The method involves training neural networks on random lower-dimensional subspaces by projecting parameters onto d-dimensional spaces using random projection matrices (Θ). The disintegrated mutual information IΘ(W′; Sn) is estimated using the MINE estimator with a neural network architecture consisting of two 100-unit layers with ELU activations. For compressible models, rate-distortion theory is incorporated by measuring the distortion between original weights and their subspace projections, with a regularization term controlling this distortion during training. The approach is validated on MNIST and CIFAR-10 datasets using feedforward and ResNet architectures with various projection methods including SVD-based, sparse, and Kronecker product projectors.

## Key Results
- Introduces disintegrated and k-sliced mutual information bounds that are tighter and more computationally tractable than standard MI bounds
- Achieves non-vacuous information-theoretic generalization bounds for neural networks on MNIST and CIFAR-10
- Demonstrates that model compressibility through subspace projection directly correlates with improved generalization bounds
- Provides empirical validation showing the distortion-generalization tradeoff can be controlled through regularization

## Why This Works (Mechanism)

### Mechanism 1
Random slicing of neural network parameters to lower-dimensional subspaces improves generalization by reducing the effective hypothesis space dimensionality. Training on random d-dimensional subspaces (W_Θ,d) restricts the parameter space from RD to a lower-dimensional manifold, reducing the mutual information between training data and learned hypothesis. Core assumption: The performance degradation from restricting to random subspaces is minimal for sufficiently large d relative to intrinsic dimensionality. Evidence anchors: [abstract] "We consider algorithms that operate by slicing the parameter space, i.e., trained on random lower-dimensional subspaces"; [section 3.2] "Li et al. (2018) found that restricting W ∈ RD during training to lie in a d-dimensional subspace spanned by a random matrix (with d ≪ D) not only provides computational advantages, but does not meaningfully damage the performance". Break condition: If d is chosen too small relative to the intrinsic dimensionality, performance degrades significantly and generalization bounds become vacuous.

### Mechanism 2
Disintegrating mutual information by conditioning on the random projection matrix yields tighter generalization bounds. By applying Jensen's inequality to the disintegrated MI (conditioned on Θ), we obtain bounds that are tighter than standard MI bounds that treat Θ and W as a single high-dimensional variable. Core assumption: The loss function is σΘ-sub-Gaussian conditioned on Θ for all parameter vectors in the subspace. Evidence anchors: [section 4.1] "Our bound entails notable advantages over Xu & Raginsky (2017). First, (3) is tighter than (2), since EPΘ[qσ²ΘIΘ(W′; Sn)] ≤ √I(ΘW′; Sn)"; [section 4.1] "common estimators for IΘ(W′; Sn) exhibit faster convergence rates than I(ΘW′; Sn) because W′ has a lower dimension than ΘW′ (d ≪ D)". Break condition: If the sub-Gaussian condition fails or if W′ is a deterministic function of Sn given Θ, the disintegrated MI becomes infinite.

### Mechanism 3
Rate-distortion theory connects model compressibility to generalization error through a trade-off between distortion and information-theoretic complexity. By measuring how closely weights can be approximated by their projections onto random subspaces (distortion term), and combining this with sliced MI bounds, we obtain generalization bounds that reflect compressibility. Core assumption: The loss is Lipschitz continuous in the parameter space, allowing control of the generalization gap through the distortion term. Evidence anchors: [section 5] "This strategy yields generalization bounds that incorporate a distortion term measuring model compressibility under slicing"; [section 5] "The proof of Theorem 5.1 consists in considering two models A : Z^n → RD and A′ : Z^n → WΘ,d such that A(Sn) = W may depend on Θ ~ PΘ, and A′(Sn) = ΘΘ⊤W". Break condition: If the Lipschitz condition fails or if the distortion cannot be controlled through regularization, the bound becomes loose.

## Foundational Learning

- Concept: Mutual Information (MI) as a measure of dependence between random variables
  - Why needed here: MI forms the basis of information-theoretic generalization bounds by quantifying how much information the learned hypothesis contains about the training data
  - Quick check question: What is the relationship between MI and the generalization error according to Xu & Raginsky (2017)?

- Concept: Rate-Distortion Theory and its application to model compression
  - Why needed here: Rate-distortion provides the theoretical framework for connecting model compressibility (how well weights can be approximated by compressed representations) to generalization performance
  - Quick check question: How does the distortion term in rate-distortion bounds relate to the compressibility of neural network weights?

- Concept: Disintegration of probability measures and its application to conditional MI
  - Why needed here: Disintegration allows us to condition on the random projection matrix Θ, yielding tighter bounds by reducing the dimensionality of the variables involved in MI estimation
  - Quick check question: How does conditioning on Θ in the disintegrated MI IΘ(W′; Sn) differ from the standard MI I(ΘW′; Sn)?

## Architecture Onboarding

- Component map: Random projection generator -> Neural network training on projected subspaces -> Mutual information estimation (MINE estimator) -> Quantization module for weight compression -> Regularization layer for distortion control

- Critical path: 1. Generate random projection matrix Θ 2. Project initial weights to subspace: w' = Θ⊤w 3. Train network on projected weights 4. Quantize weights if needed 5. Estimate MI terms using MINE 6. Compute generalization bounds 7. Apply regularization if optimizing for rate-distortion bounds

- Design tradeoffs:
  - d vs. performance: Smaller d yields tighter bounds but may hurt test performance
  - MI estimation vs. accuracy: More samples improve MI estimates but increase computational cost
  - Quantization level vs. compression: Higher quantization levels preserve accuracy but reduce compressibility benefits
  - Regularization strength vs. distortion: Stronger regularization reduces distortion but may increase training error

- Failure signatures:
  - NaN values in MI estimation indicate vacuous bounds (often when d is too small)
  - Poor test performance suggests d is below the intrinsic dimensionality
  - Bounds that don't improve with more data suggest estimation issues
  - High distortion values indicate poor compressibility despite regularization

- First 3 experiments:
  1. Train a simple logistic regression on Gaussian data with varying d to validate the d-dependent bound behavior
  2. Apply the sliced MI bounds to a small feedforward network on MNIST with d = 100, 1000, 5000
  3. Test the rate-distortion bound by varying λ (regularization strength) on a 3-layer network and measuring the distortion-MI tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on sub-Gaussian loss assumptions may not hold for neural networks with non-smooth activations or noisy labels
- Computational complexity of MI estimation remains a bottleneck, particularly for high-dimensional subspaces
- Theoretical guarantees for rate-distortion bounds assume Lipschitz continuity, which may not be strictly satisfied in practice due to optimization dynamics

## Confidence

**High confidence**: The mathematical derivation of disintegrated MI bounds and their computational advantages over standard MI bounds (Section 4.1)

**Medium confidence**: The empirical validation of non-vacuous bounds for neural networks on MNIST and CIFAR-10, given the computational constraints of MI estimation

**Medium confidence**: The extension to compressible models via rate-distortion theory, pending further empirical validation on larger networks

## Next Checks

1. Test the disintegrated MI bounds on a logistic regression model with synthetic data where the true MI can be computed analytically to verify the tightness claims

2. Validate the rate-distortion bound on a ResNet architecture with structured pruning to assess real-world compressibility benefits

3. Conduct ablation studies varying d and the regularization strength λ on CIFAR-10 to map the distortion-generalization tradeoff empirically