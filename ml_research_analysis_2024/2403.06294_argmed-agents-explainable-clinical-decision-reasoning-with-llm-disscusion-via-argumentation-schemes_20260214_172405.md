---
ver: rpa2
title: 'ArgMed-Agents: Explainable Clinical Decision Reasoning with LLM Disscusion
  via Argumentation Schemes'
arxiv_id: '2403.06294'
source_url: https://arxiv.org/abs/2403.06294
tags:
- decision
- reasoning
- clinical
- argumentation
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ArgMed-Agents addresses two barriers to LLM use in clinical reasoning:
  poor performance on complex reasoning and lack of interpretable decision-making.
  It introduces a multi-agent framework using Argumentation Schemes for Clinical Decision
  (ASCD) to enable explainable clinical reasoning.'
---

# ArgMed-Agents: Explainable Clinical Decision Reasoning with LLM Disscusion via Argumentation Schemes

## Quick Facts
- arXiv ID: 2403.06294
- Source URL: https://arxiv.org/abs/2403.06294
- Reference count: 23
- Achieves 62.1% accuracy on MedQA and 78.3% on PubMedQA with GPT-3.5-turbo

## Executive Summary
ArgMed-Agents addresses the dual challenges of poor LLM performance on complex clinical reasoning and lack of interpretable decision-making by introducing a multi-agent framework that leverages Argumentation Schemes for Clinical Decision (ASCD). The framework employs Generator, Verifier, and Reasoner agents that iteratively debate and construct argumentation graphs to support clinical decisions. Experiments demonstrate superior accuracy and explainability compared to direct generation and Chain-of-Thought methods, with human evaluation showing 91% predictability versus 63% for CoT.

## Method Summary
ArgMed-Agents is a multi-agent framework that uses ASCD to guide clinical decision reasoning. Three LLM agents (Generator, Verifier, Reasoner) work iteratively: Generator creates arguments based on ASCD, Verifier applies critical questions to validate or reject arguments, and Reasoner uses symbolic solvers to identify coherent argument sets supporting decisions. The system operates in a zero-shot setting using GPT-3.5-turbo and GPT-4 without fine-tuning, evaluated on MedQA and PubMedQA datasets.

## Key Results
- Achieves 62.1% accuracy on MedQA and 78.3% on PubMedQA with GPT-3.5-turbo
- GPT-4 implementation reaches 83.3% on MedQA and 81.6% on PubMedQA
- Outperforms direct generation and Chain-of-Thought methods on both accuracy and explainability
- Human evaluation shows 91% predictability of decisions versus 63% for CoT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive argumentation iterations allow LLMs to refine reasoning through critical questioning.
- Mechanism: Generator agents propose arguments based on ASCD, Verifier agents apply critical questions (CQs), and rejected arguments trigger new argumentation or self-attack, creating a feedback loop that refines reasoning.
- Core assumption: LLMs can generate meaningful arguments and recognize their own reasoning limitations when prompted with ASCD-based critical questions.
- Evidence anchors:
  - [abstract] "ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Decision (a reasoning mechanism for modeling cognitive processes in clinical reasoning)"
  - [section] "ArgMed-Agents uses generation as a assumption to recursively prompt the model with critical questions, identifying conflicting and erroneous arguments in an iterative process"
  - [corpus] Weak - related works on argumentative LLMs exist but specific validation of this recursive refinement mechanism is not directly cited
- Break condition: When the argumentation framework becomes too complex or the Verifier repeatedly fails to reject invalid arguments, the iteration may enter infinite loops or produce no coherent decision support.

### Mechanism 2
- Claim: Multi-agent interaction with role specialization improves explainability by separating reasoning steps.
- Mechanism: Different agents (Generator, Verifier, Reasoner) each handle distinct tasks - argument generation, validity checking, and final decision selection via symbolic solver - creating a structured reasoning pipeline.
- Core assumption: Specialized roles reduce cognitive load on individual agents and create more interpretable reasoning traces.
- Evidence anchors:
  - [abstract] "There are three types of agents in ArgMed-Agents: the Generator, the Verifier, and the Reasoner"
  - [section] "These two types of arguments play different roles; arguments in support of decisions build on beliefs and goals and try to justify choices, while arguments in support of beliefs always try to undermine decision arguments"
  - [corpus] Moderate - argumentation frameworks in AI are well-established, but specific multi-agent LLM implementations for clinical reasoning are emerging
- Break condition: If agents fail to coordinate properly or the Verifier's rejections are inconsistent, the reasoning path may become fragmented and explanations may lose coherence.

### Mechanism 3
- Claim: Symbolic solvers applied to argumentation graphs ensure logical consistency and eliminate contradictory reasoning paths.
- Mechanism: After iterative argumentation, the Reasoner agent uses abstract argumentation frameworks and preferred semantics to identify coherent argument sets that support clinical decisions.
- Core assumption: Symbolic solvers can effectively identify acceptable arguments from the generated argumentation framework.
- Evidence anchors:
  - [abstract] "Reasoner (a symbolic solver) identify a series of rational and coherent arguments to support decision"
  - [section] "Definition 3. Given an AA framework for ASCD⟨A, R⟩, E ⊆ E is a set of preferred extension such that Argsb(E) is the support decision for E"
  - [corpus] Strong - abstract argumentation frameworks and Dung's semantics are well-established in computational argumentation literature
- Break condition: When the argumentation graph contains cycles or the solver cannot identify any acceptable decision, the system must fail gracefully rather than produce unsupported conclusions.

## Foundational Learning

- Concept: Argumentation Schemes for Clinical Decision (ASCD)
  - Why needed here: Provides the formal reasoning structure that guides how clinical decisions should be justified through premises, conclusions, and critical questions
  - Quick check question: What are the three core argumentation schemes in ASCD and how do they relate to each other?
- Concept: Abstract Argumentation Frameworks (AAF)
  - Why needed here: Enables the formal representation of argument attack relations and the application of semantics to identify acceptable arguments
  - Quick check question: How does Dung's preferred semantics differ from grounded semantics in handling conflict in argumentation frameworks?
- Concept: Critical Questions (CQs) in argumentation
- Why needed here: Serve as the mechanism for Verifier agents to challenge argument validity and trigger refinement or rejection
  - Quick check question: What happens when a CQ is rejected - does the argument get rejected or does it trigger a new argumentation scheme?

## Architecture Onboarding

- Component map:
  - Question input -> Generator produces initial ASD argument -> Verifier applies CQs -> if rejected, triggers new argumentation or self-attack -> Iterate until no new arguments generated -> Reasoner solves AAF to identify preferred extension -> Output decision with supporting arguments
- Critical path:
  1. Question input → Generator produces initial ASD argument
  2. Verifier applies CQs → if rejected, triggers new argumentation or self-attack
  3. Iterate until no new arguments generated
  4. Reasoner solves AAF to identify preferred extension
  5. Output decision with supporting arguments
- Design tradeoffs:
  - More agents → better reasoning but higher complexity and potential coordination overhead
  - More CQs → more thorough validation but longer reasoning times
  - Strict AAF semantics → more reliable decisions but may reject valid reasoning in edge cases
- Failure signatures:
  - No acceptable decision found (Argsd(E) = ∅ in preferred extension)
  - Infinite argumentation loops between agents
  - Verifier repeatedly accepts invalid arguments
  - Reasoner cannot resolve conflicts in the argumentation framework
- First 3 experiments:
  1. Run ArgMed-Agents on a simple MedQA question and verify the argumentation framework structure matches expectations
  2. Test Verifier behavior by providing arguments with known flaws and confirming they are rejected
  3. Validate Reasoner output by manually checking if preferred extension correctly identifies the decision from the argumentation graph

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would ArgMed-Agents perform on clinical reasoning tasks that require numerical calculations or probabilistic reasoning, which are outside the scope of abstract argumentation frameworks?
- Basis in paper: [inferred] The authors mention that abstract arguments alone may struggle with clinical reasoning tasks requiring numerical calculations or probabilistic reasoning, and that future research will explore integrating value-based or probabilistic argumentation techniques.
- Why unresolved: The paper only identifies this as a future research direction without providing experimental results or theoretical analysis of how ArgMed-Agents would handle such tasks.
- What evidence would resolve it: Experiments testing ArgMed-Agents on clinical datasets requiring numerical calculations (e.g., dosage calculations, risk assessments) or probabilistic reasoning (e.g., Bayesian diagnosis), comparing performance against baseline methods.

### Open Question 2
- Question: What is the theoretical upper bound on the number of agents and iterations in ArgMed-Agents before performance degrades or the framework becomes computationally intractable?
- Basis in paper: [inferred] The authors set a dialogue limit of 4 for agents to prevent getting into loops, suggesting concerns about iteration limits, but do not analyze the theoretical bounds or provide complexity analysis.
- Why unresolved: The paper does not provide theoretical analysis of the maximum number of agents or iterations before performance degradation, nor does it explore the computational complexity of the argumentation framework as it scales.
- What evidence would resolve it: Mathematical proofs or empirical studies showing the relationship between the number of agents, iterations, and computational complexity, including identification of the point where additional agents/iterations no longer improve accuracy.

### Open Question 3
- Question: How does the performance of ArgMed-Agents vary with different LLMs (e.g., open-source models like Llama or Claude) and different temperature settings?
- Basis in paper: [explicit] The authors implemented Generator and Verifier using GPT-3.5-turbo and GPT-4 with temperature set to 0.0, but do not explore performance variations with other LLMs or temperature settings.
- Why unresolved: The paper only tests with OpenAI's proprietary models and a fixed temperature setting, leaving questions about the framework's generalizability to other LLMs and parameter settings.
- What evidence would resolve it: Comparative experiments testing ArgMed-Agents with multiple LLMs (open-source and proprietary) and various temperature settings, measuring accuracy and explainability metrics across different configurations.

## Limitations
- The framework depends heavily on the quality of ASCD prompt engineering, and performance may degrade when critical questions fail to trigger meaningful refinement.
- Multi-agent coordination introduces complexity that could lead to inconsistent reasoning paths and longer processing times.
- The symbolic solver's effectiveness is limited by the quality of the argumentation graph, and edge cases with conflicting arguments may not resolve cleanly.

## Confidence
- **High**: The overall framework architecture and multi-agent design are well-grounded in established argumentation theory and computational argumentation frameworks.
- **Medium**: The specific implementation details of ASCD prompt strategies and their effectiveness in triggering meaningful reasoning refinements require further validation.
- **Medium**: The human evaluation methodology for explainability, while promising, needs more rigorous validation to ensure consistency across different evaluators and clinical scenarios.

## Next Checks
1. **Mechanistic Validation**: Conduct ablation studies to isolate the contribution of each component (ASCD, multi-agent coordination, symbolic solver) to overall performance.
2. **Clinical Expert Review**: Have practicing clinicians evaluate the quality and safety of ArgMed-Agents' reasoning outputs on complex clinical scenarios beyond the benchmark datasets.
3. **Scalability Testing**: Test the framework's performance and reasoning quality as the number of agents increases and as the complexity of clinical scenarios grows.