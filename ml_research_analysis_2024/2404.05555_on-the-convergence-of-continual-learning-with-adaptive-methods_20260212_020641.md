---
ver: rpa2
title: On the Convergence of Continual Learning with Adaptive Methods
arxiv_id: '2404.05555'
source_url: https://arxiv.org/abs/2404.05555
tags:
- learning
- continual
- memory
- forgetting
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a convergence analysis of memory-based continual
  learning under non-convex optimization, revealing that training on current tasks
  causes cumulative degradation of previous tasks through a defined "catastrophic
  forgetting term." The authors propose Non-convex Continual Learning (NCCL), an adaptive
  method that adjusts step sizes for previous and current tasks based on gradient
  information. They prove NCCL can achieve the same convergence rate as SGD when the
  forgetting term is suppressed at each iteration.
---

# On the Convergence of Continual Learning with Adaptive Methods

## Quick Facts
- arXiv ID: 2404.05555
- Source URL: https://arxiv.org/abs/2404.05555
- Reference count: 40
- Primary result: Proposed NCCL method achieves 0.028 forgetting on split-CIFAR100 vs 0.09 for ER-Reservoir with the same memory size of 1-5 examples per class per task.

## Executive Summary
This paper provides a convergence analysis of memory-based continual learning under non-convex optimization, revealing that training on current tasks causes cumulative degradation of previous tasks through a defined "catastrophic forgetting term." The authors propose Non-convex Continual Learning (NCCL), an adaptive method that adjusts step sizes for previous and current tasks based on gradient information. They prove NCCL can achieve the same convergence rate as SGD when the forgetting term is suppressed at each iteration. Experiments on image classification tasks show NCCL outperforms existing methods, particularly in reducing forgetting, with memory size of 1-5 examples per class per task.

## Method Summary
The paper introduces Non-convex Continual Learning (NCCL), an adaptive method for continual learning that adjusts step sizes for previous and current tasks based on gradient information. The algorithm maintains a memory buffer of examples from previous tasks and computes adaptive step sizes αHt and βHt by analyzing the inner product between gradients of previous and current tasks. When interference is detected (negative inner product), the method reduces the step size for current tasks; when transfer is detected (positive inner product), it increases the step size. The convergence analysis decomposes the optimization problem and shows that NCCL can achieve the same convergence rate as SGD when the catastrophic forgetting term Γt is suppressed at each iteration.

## Key Results
- NCCL achieves 0.028 forgetting on split-CIFAR100 with memory size of 1-5 examples per class per task
- Outperforms ER-Reservoir which achieves 0.09 forgetting under the same conditions
- The theoretical analysis proves NCCL can match SGD's convergence rate when catastrophic forgetting is suppressed
- Memory selection schemes (episodic vs reservoir) significantly affect the variance of convergence rates

## Why This Works (Mechanism)

### Mechanism 1
Training on current tasks causes cumulative degradation of previous tasks through a "catastrophic forgetting term" (Γt). The convergence analysis decomposes the finite-sum problem into previous and current tasks. During optimization, the gradient update on current tasks introduces a term Γt that accumulates over time and degrades performance on previous tasks. The inner product between gradients determines interference (negative) vs transfer (positive) between tasks.

### Mechanism 2
NCCL achieves the same convergence rate as SGD when the catastrophic forgetting term is suppressed at each iteration. The method adjusts step sizes for previous and current tasks based on gradient information. By ensuring βHt < αHt and adapting αHt based on whether interference or transfer occurs, the algorithm can suppress the Γt term that causes forgetting.

### Mechanism 3
Memory-based methods mitigate forgetting, but previous works do not fully exploit gradient information of memory. NCCL uses adaptive step sizes that respond to gradient information, allowing it to better balance learning current tasks while preserving performance on previous tasks compared to fixed-step methods like A-GEM.

## Foundational Learning

- **Non-convex optimization and convergence analysis**
  - Why needed here: The paper analyzes continual learning as a non-convex optimization problem, requiring understanding of convergence rates and stationary points in non-convex settings.
  - Quick check question: What is the difference between convergence to a global minimum versus a stationary point in non-convex optimization?

- **Stochastic gradient descent with variance reduction**
  - Why needed here: The analysis builds on SGD methods but extends them to the continual learning setting with memory buffers and non-i.i.d. data streams.
  - Quick check question: How does the variance of gradient estimates affect convergence rates in non-convex optimization?

- **Catastrophic forgetting in neural networks**
  - Why needed here: The paper specifically addresses the phenomenon where learning new tasks causes degradation of performance on previously learned tasks.
  - Quick check question: What are the main mechanisms by which neural networks experience catastrophic forgetting during sequential task learning?

## Architecture Onboarding

- **Component map:**
  - Memory buffer (Mt) -> Current task data stream (C) -> Adaptive step size computation -> Gradient computation module -> Update rule

- **Critical path:**
  1. Sample minibatches It from memory Mt and Jt from current task C
  2. Compute gradients ∇fIt(xt) and ∇gJt(xt)
  3. Calculate step sizes αHt and βHt based on gradient information and interference/transfer detection
  4. Update model parameters: xt+1 = xt - αHt∇fIt(xt) - βHt∇gJt(xt)
  5. Update memory buffer with new data

- **Design tradeoffs:**
  - Memory size vs. forgetting: Larger memory buffers reduce forgetting but increase computational cost
  - Step size adaptation vs. stability: More aggressive adaptation can improve learning but may cause instability
  - Memory update strategy (episodic vs reservoir) affects the variance of gradient estimates

- **Failure signatures:**
  - If forgetting increases despite using NCCL, check if βHt ≥ αHt is being violated
  - If convergence is slow, verify that the memory buffer is being properly maintained and sampled
  - If performance is unstable, examine the variance of gradient estimates from the memory buffer

- **First 3 experiments:**
  1. Compare NCCL with fixed-step methods (A-GEM, ER-Reservoir) on split-CIFAR100 with memory size 1-5 examples per class per task
  2. Test the effect of memory buffer size on forgetting and convergence rate across different datasets (Permuted-MNIST, Split-MNIST)
  3. Evaluate the impact of different memory update strategies (episodic vs reservoir) on the variance of convergence rates

## Open Questions the Paper Calls Out

### Open Question 1
How does the memory selection scheme affect the variance of convergence rate among trials? The paper mentions that memory schemes matter for variance but doesn't quantify or compare different schemes.

### Open Question 2
What is the exact relationship between the overfitting bias term Bt and catastrophic forgetting term Γt in terms of their relative impact on forgetting? While the paper shows Bt has minimal empirical effect, it doesn't provide a quantitative comparison of their relative contributions to forgetting.

### Open Question 3
How does the proposed theoretical framework extend to non-memory based continual learning methods like EWC or SupSup? The paper focuses on memory-based methods but mentions non-memory approaches exist without providing theoretical analysis for them.

### Open Question 4
What is the optimal value of δ in the NCCL algorithm that balances forgetting reduction and current task performance? The paper introduces δ as a hyperparameter but doesn't provide guidance on its optimal selection or sensitivity analysis.

### Open Question 5
How does the convergence rate change when considering the heavy-tailed noise distribution in gradient updates as discussed in related work? The paper assumes standard smooth nonconvex optimization but doesn't analyze how heavy-tailed gradient noise affects the theoretical bounds.

## Limitations
- Theoretical analysis assumes specific conditions (βHt < αHt) that may not always hold in practice
- Empirical validation focuses on image classification tasks with relatively simple neural network architectures
- Computational overhead of adaptive step size computation is not thoroughly analyzed

## Confidence
- **High confidence** in the theoretical framework and convergence guarantees
- **Medium confidence** in the practical effectiveness of NCCL
- **Low confidence** in the generalizability of results to non-image domains or scenarios with significantly larger task sequences

## Next Checks
1. Test NCCL on sequential regression tasks or reinforcement learning environments to evaluate performance beyond image classification benchmarks
2. Conduct ablation studies varying memory buffer sizes systematically to determine the minimum effective memory size for different dataset complexities
3. Compare NCCL's computational efficiency against baseline methods in terms of training time per iteration and memory requirements for the adaptive step size computation