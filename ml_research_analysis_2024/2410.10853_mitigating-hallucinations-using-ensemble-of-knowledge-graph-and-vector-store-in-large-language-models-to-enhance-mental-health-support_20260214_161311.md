---
ver: rpa2
title: Mitigating Hallucinations Using Ensemble of Knowledge Graph and Vector Store
  in Large Language Models to Enhance Mental Health Support
arxiv_id: '2410.10853'
source_url: https://arxiv.org/abs/2410.10853
tags:
- score
- knowledge
- vector
- graph
- store
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses hallucinations in Large Language Models
  (LLMs) within mental health support by introducing an ensemble retriever framework
  that integrates knowledge graph retrieval and vector store retrieval. The system
  combines semantic richness from vector embeddings with factual accuracy from structured
  knowledge graphs to generate reliable, contextually relevant responses.
---

# Mitigating Hallucinations Using Ensemble of Knowledge Graph and Vector Store in Large Language Models to Enhance Mental Health Support

## Quick Facts
- arXiv ID: 2410.10853
- Source URL: https://arxiv.org/abs/2410.10853
- Authors: Abdul Muqtadir; Hafiz Syed Muhammad Bilal; Ayesha Yousaf; Hafiz Farooq Ahmed; Jamil Hussain
- Reference count: 26
- Key outcome: Ensemble retriever framework combining knowledge graph and vector store retrieval reduces hallucination rates from 22% to 5% in mental health support applications

## Executive Summary
This research addresses the critical challenge of hallucinations in Large Language Models when applied to mental health support applications. The proposed solution implements an ensemble retriever framework that integrates knowledge graph retrieval with vector store retrieval to generate more reliable and contextually appropriate responses. By leveraging both semantic richness from vector embeddings and factual accuracy from structured knowledge graphs, the system demonstrates significant improvements in response quality while maintaining factual consistency in sensitive healthcare contexts.

## Method Summary
The ensemble retriever framework combines two complementary retrieval approaches: knowledge graph retrieval provides structured factual information from curated medical knowledge bases, while vector store retrieval offers semantic similarity matching through dense embeddings. The system processes user queries through both retrievers in parallel, then synthesizes the retrieved information using weighted aggregation based on retrieval confidence scores. The retrieved context is passed to fine-tuned LLMs (Google Gemma, Mistral, and Zephyr) for response generation. The framework employs a dynamic confidence scoring mechanism that prioritizes knowledge graph outputs for factual queries and vector store outputs for semantic queries, with adaptive weighting based on query type and context.

## Key Results
- Hallucination rates reduced from 22% to 5% compared to single-retriever approaches
- ROUGE-1, ROUGE-2, and METEOR scores improved significantly across all tested models
- User satisfaction and trust metrics showed substantial increases in perceived response reliability
- The ensemble approach maintained consistent performance across diverse mental health scenarios

## Why This Works (Mechanism)
The ensemble retriever leverages complementary strengths of structured and unstructured knowledge sources. Knowledge graphs provide precise, curated factual information with clear provenance, while vector stores capture nuanced semantic relationships through learned embeddings. By combining these approaches, the system can verify factual claims against structured data while maintaining contextual relevance through semantic matching. The weighted aggregation mechanism dynamically balances precision and recall based on query characteristics, ensuring appropriate emphasis on either factual accuracy or semantic relevance as needed.

## Foundational Learning
- Knowledge Graph Retrieval: Structured data querying from curated medical ontologies - needed for factual accuracy and provenance tracking - quick check: verify entity relationships match clinical knowledge bases
- Vector Store Embeddings: Dense semantic representations for context matching - needed for capturing nuanced language patterns - quick check: ensure embedding dimensionality matches retrieval requirements
- LLM Fine-tuning: Domain adaptation for mental health contexts - needed for specialized terminology and sensitive response generation - quick check: validate model outputs against clinical guidelines
- Confidence Scoring: Dynamic weighting of retrieval sources - needed for adaptive query processing - quick check: test confidence threshold sensitivity across query types
- Ensemble Fusion: Weighted combination of multiple retrieval streams - needed for balanced accuracy and relevance - quick check: verify fusion weights optimize for domain-specific metrics
- Hallucination Detection: Automated identification of factual inconsistencies - needed for measuring system performance - quick check: test detection accuracy across hallucination types

## Architecture Onboarding

Component Map: User Query -> Knowledge Graph Retriever -> Vector Store Retriever -> Confidence Scorer -> Weighted Aggregator -> LLM Generator -> Response Output

Critical Path: User Query → Dual Retrieval (KG + Vector) → Confidence Scoring → Weighted Context Fusion → LLM Generation → Response Output

Design Tradeoffs: The system balances between precision (favoring knowledge graphs) and recall (favoring vector stores), with the tradeoff being computational overhead from dual retrieval and complexity in confidence scoring. The ensemble approach adds latency but provides superior accuracy compared to single-retriever systems.

Failure Signatures: Knowledge graph failures manifest as missing contextual information; vector store failures appear as semantic drift or irrelevant matches. Combined failures can lead to low-confidence outputs or fallback to default responses. The system should monitor retrieval confidence scores and implement graceful degradation when both sources fail.

First Experiments:
1. A/B test comparing single retriever (KG only) vs single retriever (vector only) vs ensemble retriever on identical mental health queries
2. Ablation study removing confidence scoring to measure impact on hallucination rates
3. Cross-domain validation testing the framework on chronic disease management scenarios

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The evaluation methodology for hallucination detection lacks transparency regarding human versus automated assessment
- Performance metrics focus on surface-level text similarity rather than clinical accuracy or appropriateness
- Model configurations and fine-tuning details for mental health applications are insufficiently specified
- The framework's generalizability to other healthcare domains beyond mental health remains unproven
- User satisfaction metrics lack detail on sample size, demographic diversity, and evaluation methodology

## Confidence
- Hallucination rate reduction (22% to 5%): Low - methodology and evaluation criteria unclear
- ROUGE and METEOR improvements: Medium - metrics assess text similarity but not clinical validity
- Model selection and configuration: Low - insufficient detail on fine-tuning and domain adaptation
- Cross-domain generalizability: Low - no validation beyond tested mental health scenarios
- User satisfaction and trust metrics: Low - evaluation methodology and sample details missing

## Next Checks
1. Conduct blind human evaluation with mental health professionals to independently verify hallucination rates and clinical appropriateness of responses across diverse mental health scenarios
2. Perform ablation studies to isolate the contribution of knowledge graph versus vector store components to overall performance improvements and determine optimal weighting strategies
3. Test the ensemble retriever framework on additional healthcare domains (e.g., chronic disease management, medication guidance) to assess generalizability and identify domain-specific limitations