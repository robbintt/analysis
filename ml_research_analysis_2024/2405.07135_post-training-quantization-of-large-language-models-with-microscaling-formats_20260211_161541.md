---
ver: rpa2
title: Post Training Quantization of Large Language Models with Microscaling Formats
arxiv_id: '2405.07135'
source_url: https://arxiv.org/abs/2405.07135
tags:
- quantization
- gptq
- smoothquant
- formats
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the interaction of three post-training quantization\
  \ (PTQ) algorithms\u2014SmoothQuant, AWQ, and GPTQ\u2014applied to large language\
  \ models (LLMs). The authors extend these algorithms to support microscaling (MX)\
  \ formats, which use shared scaling factors across blocks of values, improving memory\
  \ efficiency over fixed-point formats."
---

# Post Training Quantization of Large Language Models with Microscaling Formats
## Quick Facts
- arXiv ID: 2405.07135
- Source URL: https://arxiv.org/abs/2405.07135
- Reference count: 40
- One-line primary result: Combining AWQ and GPTQ algorithms with MXINT microscaling formats enables 4-bit LLM quantization with minimal accuracy loss

## Executive Summary
This paper investigates the interaction of three post-training quantization algorithms—SmoothQuant, AWQ, and GPTQ—when applied to large language models. The authors extend these algorithms to support microscaling formats, which use shared scaling factors across blocks of values, improving memory efficiency over fixed-point formats. They systematically evaluate these methods on Llama2, Llama3.1, and Qwen2 models across multiple datasets and tasks. Results demonstrate that combining AWQ and GPTQ algorithms is particularly effective for aggressive quantization (4-bit and 3-bit), while microscaling formats outperform fixed-point formats at the same bit-width.

## Method Summary
The authors evaluate three PTQ algorithms (SmoothQuant, AWQ, and GPTQ) extended to support microscaling (MX) formats on Llama2, Llama3.1, and Qwen2 models. They systematically test various combinations of algorithms and formats across different bit-widths (4-bit, 3-bit, and 2-bit). The evaluation uses perplexity measurements on WikiText-2 and C4 datasets, along with accuracy on eight zero-shot commonsense reasoning tasks. The microscaling format implementation uses block sizes of 256 for weights and 64 for activations, with per-block scaling factors that improve memory efficiency compared to traditional fixed-point quantization.

## Key Results
- Combining AWQ and GPTQ algorithms shows synergistic effects, particularly for aggressive quantization (4-bit and 3-bit)
- MXINT4-AWQ-GPTQ achieves perplexity of 5.53 on Llama2-7B, close to the baseline
- MXINT formats outperform fixed-point formats at the same bit-width across all tested configurations
- Best results achieved with MXINT4-AWQ-GPTQ: 4-bit weights and 8-bit activations with minimal accuracy loss

## Why This Works (Mechanism)
The paper demonstrates that different PTQ algorithms address distinct aspects of quantization: SmoothQuant redistributes quantization error across weight and activation dimensions, AWQ focuses on minimizing quantization error in weight matrices, and GPTQ optimizes for layer-wise reconstruction error. When combined, these algorithms complement each other by addressing different sources of quantization error. The microscaling format further improves efficiency by using shared scaling factors within blocks of values, reducing memory overhead while maintaining precision where needed.

## Foundational Learning
**Post-training quantization (PTQ)**: Quantization performed after model training without requiring retraining. Needed because it's faster and simpler than quantization-aware training. Quick check: Model can be quantized without access to training data or computational resources for fine-tuning.

**Microscaling formats**: Use shared scaling factors across blocks of values rather than per-value scaling. Needed to reduce memory overhead while maintaining precision. Quick check: Scaling factors are shared across 256-value blocks for weights and 64-value blocks for activations.

**Perplexity**: Measure of how well a probability model predicts a sample. Lower perplexity indicates better model performance. Needed for evaluating language model quality. Quick check: Baseline perplexity should be established before applying quantization.

**Zero-shot commonsense reasoning**: Evaluating models on tasks requiring understanding without task-specific training. Needed to assess general reasoning capabilities after quantization. Quick check: Tasks should be diverse and representative of real-world reasoning challenges.

## Architecture Onboarding
**Component map**: PTQ algorithms (SmoothQuant, AWQ, GPTQ) -> Microscaling formats (MXINT) -> LLM architectures (Llama2, Llama3.1, Qwen2) -> Evaluation (perplexity + accuracy)

**Critical path**: Quantization algorithm selection → Microscaling format application → Model quantization → Performance evaluation

**Design tradeoffs**: Memory efficiency vs. accuracy trade-off is central, with microscaling formats providing better memory efficiency at the cost of slightly increased computation. Algorithm combinations provide accuracy improvements but increase complexity.

**Failure signatures**: Significant perplexity degradation (>20% from baseline), accuracy drops on reasoning tasks (>10% absolute), or failure to converge during quantization process.

**First experiments**: 1) Apply MXINT4 format to a single layer to verify implementation correctness, 2) Compare baseline perplexity with AWQ-only quantization, 3) Test GPTQ algorithm on a small model before scaling to larger models.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Limited scope of datasets (WikiText-2 and C4) and tasks (8 zero-shot reasoning tasks) may not capture diverse real-world performance
- Focus on specific LLM architectures (Llama2, Llama3.1, Qwen2) limits generalizability to other model families
- No evaluation of runtime performance implications (inference speed, memory usage) across different hardware accelerators
- Limited comparative analysis with other quantization methods beyond the three PTQ algorithms studied

## Confidence
High confidence: Effectiveness of combining AWQ and GPTQ algorithms for aggressive quantization (4-bit and 3-bit)
Medium confidence: Superiority of MXINT formats over fixed-point formats at the same bit-width
Low confidence: Broader applicability to other LLM architectures and real-world deployment scenarios

## Next Checks
1. Evaluate the proposed quantization methods on a broader range of datasets and tasks, including those that reflect real-world applications, to assess generalizability
2. Conduct a comparative analysis with other quantization techniques, such as dynamic quantization or quantization-aware training, to establish the relative advantages of the studied methods
3. Investigate the runtime performance implications, including inference speed and memory usage, of the proposed quantization methods across different hardware accelerators to validate their practical utility