---
ver: rpa2
title: Trustworthy Automated Driving through Qualitative Scene Understanding and Explanations
arxiv_id: '2403.09668'
source_url: https://arxiv.org/abs/2403.09668
tags:
- qualitative
- scene
- driving
- action
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Qualitative Explainable Graph (QXG),
  a unified symbolic and qualitative representation for scene understanding in automated
  driving. QXG leverages spatio-temporal graphs and qualitative constraints to extract
  scene semantics from raw sensor inputs, enabling interpretable machine learning
  models for action explanations.
---

# Trustworthy Automated Driving through Qualitative Scene Understanding and Explanations

## Quick Facts
- arXiv ID: 2403.09668
- Source URL: https://arxiv.org/abs/2403.09668
- Authors: Nassim Belmecheri; Arnaud Gotlieb; Nadjib Lazaar; Helge Spieker
- Reference count: 14
- Key outcome: QXG achieves 89.8% precision and recall in explaining vehicle actions using qualitative scene representations

## Executive Summary
This paper introduces the Qualitative Explainable Graph (QXG), a unified symbolic and qualitative representation for scene understanding in automated driving. QXG leverages spatio-temporal graphs and qualitative constraints to extract scene semantics from raw sensor inputs, enabling interpretable machine learning models for action explanations. The approach uses qualitative calculi to capture spatial relationships and temporal dynamics between objects. Experiments on the nuScenes dataset demonstrate that QXG can be efficiently constructed in real-time and supports accurate action explanations.

## Method Summary
The method constructs QXGs incrementally from LiDAR data using four qualitative calculi (QDC, RA, QT Cb, ST AR4) to encode spatial and temporal relations between objects. Object-relation chains are extracted from recent QXG frames and used to train one-class random forest classifiers for explaining vehicle actions like cruising, acceleration, and stopping. The framework processes raw sensor inputs into interpretable symbolic representations that support real-time decision-making and explanation generation.

## Key Results
- QXG construction remains efficient (<50ms) even with scenes containing up to 160 objects
- One-class classifiers trained on QXG features achieve 89.8% precision and recall for action explanations
- The approach successfully explains vehicle actions including cruising, acceleration, and stopping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QXG enables real-time incremental construction of symbolic scene representations
- Mechanism: The QXG is built frame-by-frame using LiDAR data, leveraging spatial and temporal qualitative calculi to update object relationships incrementally without recomputing the entire graph
- Core assumption: LiDAR provides sufficient spatial resolution to infer object bounding boxes and trajectories for qualitative reasoning
- Evidence anchors:
  - [abstract] "QXG can be incrementally constructed in real-time, making it a versatile tool for in-vehicle explanations and real-time decision-making"
  - [section] "The QXG is built incrementally, frame-by-frame from the top LiDAR view, which is computationally efficient. Remarkably, even for frames with the maximum number of 160 objects, construction remains efficient, taking less than 50 milliseconds"
- Break condition: LiDAR data becomes too noisy or sparse to maintain reliable object tracking and bounding box extraction

### Mechanism 2
- Claim: Qualitative calculi enable interpretable reasoning about spatial and temporal object relationships
- Mechanism: Four qualitative calculi (QDC, RA, QT Cb, ST AR4) encode relative positions, distances, orientations, and trajectories between objects without requiring precise metric measurements
- Core assumption: Qualitative descriptions are sufficient for downstream action explanation tasks, even without exact numerical values
- Evidence anchors:
  - [section] "We rely on four qualitative calculi [5] for all spatial aspects: 1. Qualitative Distance Calculus (QDC) [11] 2. Rectangle Algebra (RA) [11] 3. Basic Qualitative Trajectory Calculus (QT Cb) [5] 4. Star Calculus [11]"
  - [section] "The QXG captures the relation between two objects over the temporal course of a scene through spatial relations and their changes"
- Break condition: Scene complexity exceeds the expressiveness of the chosen qualitative calculi

### Mechanism 3
- Claim: One-class classifiers trained on QXG representations can explain vehicle actions
- Mechanism: Random forests are trained to classify object-relation chains as explanations for specific actions, using feature vectors extracted from recent QXG frames
- Core assumption: The patterns of object relationships that precede an action are consistent enough to be captured by interpretable classifiers
- Evidence anchors:
  - [section] "Random forests are trained as the interpretable action explainability classifiers on 595 scenes"
  - [section] "Our evaluation results on 255 held-out scenes, summarised in Table 2b, employ Precision and Recall as key metrics, gauging prediction correctness and sensitivity, respectively"
  - [section] "Notably, Precision and Recall exhibit identical values, as we are dealing with a one-class classification scenario"
- Break condition: Training data lacks sufficient diversity in object-relation patterns for certain actions

## Foundational Learning

- Concept: Qualitative spatial reasoning
  - Why needed here: QXG relies on qualitative calculi to represent object relationships without exact measurements
  - Quick check question: What qualitative calculi are used in QXG and what aspects of scene understanding do they capture?

- Concept: Spatio-temporal graph construction
  - Why needed here: QXG represents scenes as graphs where nodes are objects and edges encode spatial-temporal relationships
  - Quick check question: How does QXG incrementally update its graph representation frame-by-frame?

- Concept: One-class classification for action explanation
  - Why needed here: The method trains classifiers to identify which object relationships explain specific vehicle actions
  - Quick check question: How are object-relation chains extracted from QXG to train action explanation classifiers?

## Architecture Onboarding

- Component map: LiDAR preprocessing → object detection → qualitative calculi application → graph update → classifier inference → action explanation
- Critical path: LiDAR data → object detection → QXG construction → classifier inference → action explanation
- Design tradeoffs: Qualitative vs. quantitative representations (interpretability vs. precision), one-class vs. multi-class classification (simplicity vs. flexibility)
- Failure signatures: QXG construction time exceeds real-time constraints, classifier precision/recall drops significantly, explanations don't align with expected action causes
- First 3 experiments:
  1. Measure QXG construction time with varying numbers of objects (baseline timing validation)
  2. Evaluate classifier performance on held-out scenes with different threshold values
  3. Test explanation accuracy by comparing generated explanations against human-labeled ground truth for sample actions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different qualitative calculi combinations affect the expressiveness and efficiency of the QXG representation for various driving scenarios?
- Basis in paper: [inferred] The paper mentions that the selection of spatial calculi is a parameter of the QXG and alternative calculi may be considered depending on use case requirements
- Why unresolved: The paper only evaluates one specific combination of four qualitative calculi (QDC, RA, QT Cb, STAR4) on the nuScenes dataset, without exploring other possible combinations or their impact on performance
- What evidence would resolve it: Systematic evaluation of different qualitative calculi combinations on diverse driving scenarios and datasets, comparing their expressiveness, computational efficiency, and action explanation accuracy

### Open Question 2
- Question: How does the choice of classifier model (beyond tree-based models) impact the quality and interpretability of action explanations in QXG?
- Basis in paper: [explicit] The paper states that "by employing interpretable classifiers like tree-based models, we can provide decision paths as supplementary context for the highest classification scores"
- Why unresolved: The paper only experiments with random forests as the interpretable classifier, leaving open the question of how other interpretable models (e.g., rule-based systems, neural-symbolic models) might perform
- What evidence would resolve it: Comparative evaluation of various interpretable classifier models on the same QXG dataset, measuring both explanation quality (precision/recall) and interpretability metrics

### Open Question 3
- Question: How can the QXG framework be extended to handle more complex multi-agent interactions and emergent behaviors in automated driving scenarios?
- Basis in paper: [inferred] The paper mentions future work on "vehicle-to-vehicle scene understanding" and "advanced message-passing techniques to enhance the action explanation process"
- Why unresolved: The current QXG representation and explanation method are primarily demonstrated on simple object-object relationships, without addressing complex multi-agent dynamics or emergent behaviors
- What evidence would resolve it: Extension of the QXG framework to incorporate higher-order relationships between multiple agents, along with evaluation on scenarios involving complex multi-agent interactions and emergent behaviors

## Limitations
- The approach relies on sufficient LiDAR spatial resolution, which may be compromised in adverse weather conditions
- One-class classification may struggle with rare or complex driving maneuvers not well-represented in training data
- Evaluation metrics focus on precision and recall without assessing the actual interpretability or usefulness of explanations to human drivers

## Confidence
- **High Confidence**: QXG construction is efficient and supports real-time incremental updates (supported by timing experiments showing <50ms for scenes with up to 160 objects)
- **Medium Confidence**: Qualitative calculi effectively capture spatial and temporal relationships for action explanation (theoretical justification is strong, but empirical validation of qualitative reasoning quality is limited)
- **Medium Confidence**: One-class classifiers trained on QXG features achieve 89.8% precision and recall (results are reported but lack comparison to alternative approaches or detailed analysis of failure cases)

## Next Checks
1. Test QXG construction and classifier performance on driving scenarios with adverse weather conditions or degraded sensor data to assess robustness limits
2. Conduct a human factors study to evaluate whether the generated explanations are actually interpretable and useful for human drivers, not just quantitatively accurate
3. Compare the one-class classification approach against multi-class alternatives and baseline methods to establish whether the claimed performance benefits are significant and meaningful