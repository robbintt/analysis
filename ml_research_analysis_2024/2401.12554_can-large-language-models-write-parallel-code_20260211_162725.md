---
ver: rpa2
title: Can Large Language Models Write Parallel Code?
arxiv_id: '2401.12554'
source_url: https://arxiv.org/abs/2401.12554
tags:
- code
- parallel
- llms
- pass
- openmp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ParEval, a benchmark for evaluating large language
  models (LLMs) on parallel code generation tasks. ParEval consists of 420 prompts
  covering 12 problem types and 7 execution models (serial, OpenMP, MPI, Kokkos, CUDA,
  HIP).
---

# Can Large Language Models Write Parallel Code?

## Quick Facts
- arXiv ID: 2401.12554
- Source URL: https://arxiv.org/abs/2401.12554
- Authors: Daniel Nichols; Joshua H. Davis; Zhaojun Xie; Arjun Rajaram; Abhinav Bhatele
- Reference count: 40
- One-line primary result: GPT-3.5 achieves 76.0% pass@1 for serial and 39.6% for parallel code generation on the ParEval benchmark

## Executive Summary
This paper presents ParEval, a comprehensive benchmark for evaluating large language models on parallel code generation tasks. The benchmark includes 420 prompts across 12 problem types and 7 execution models (serial, OpenMP, MPI, Kokkos, CUDA, HIP). The authors introduce novel metrics for assessing both correctness and performance of generated code, revealing that LLMs perform significantly worse on parallel code generation compared to serial code. GPT-3.5 shows the best performance among evaluated models, but even it struggles with MPI code and sparse, unstructured problems. The study demonstrates that while LLMs can benefit from correct implementations in other execution models, they often generate inefficient parallel code.

## Method Summary
The authors developed ParEval, a benchmark consisting of 420 prompts covering 12 problem types (transform, blur, n-body, heat, linear solver, sort, prefix sum, search, sparse matrix vector multiply, merge, histogram, black-scholes) across 7 execution models. For each prompt, they generate N samples (typically 20) and evaluate them for correctness. They introduce novel metrics including pass@k (probability of generating at least one correct solution in k attempts) and speedup@k (expected best speedup relative to sequential baseline). The benchmark tests several state-of-the-art LLMs including GPT-3.5, GPT-4, CodeLlama, and others, measuring both correctness and performance of generated parallel code.

## Key Results
- GPT-3.5 achieves the highest performance with 76.0% pass@1 for serial and 39.6% for parallel code generation
- All evaluated LLMs perform significantly worse on parallel code generation compared to serial code
- LLMs struggle most with MPI code and sparse, unstructured problems
- Generated parallel code is often inefficient, with poor scaling behavior
- Providing correct implementations in one execution model helps improve code generation in another model but doesn't necessarily enhance performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pass@1 metric effectively measures LLM code generation correctness by sampling multiple outputs and evaluating them against a single correct solution.
- Mechanism: For each prompt, the model generates N samples. The number of correct samples (c_p) determines the probability that at least one correct solution appears in k attempts using the formula: pass@k = 1 - (N - c_p choose k) / (N choose k).
- Core assumption: The N generated samples are independent and uniformly distributed across all possible solutions.
- Evidence anchors: [abstract] "We introduce novel metrics for evaluating the performance of generated code, and use them to explore how well each large language model performs for 12 different computational problem types and six different parallel programming models." [section] "To estimate the pass@k over a set of prompts, we first generate N samples for each prompt using the model, where N > k. These samples are then evaluated for correctness."

### Mechanism 2
- Claim: The speedupð‘›@ð‘˜ metric accurately estimates the expected best speedup relative to a sequential baseline when the model is given k attempts.
- Mechanism: The metric calculates the expected maximum speedup across k samples by weighting each sample's speedup by the probability that it would be selected as the best among k attempts.
- Core assumption: All size k permutations of generated samples are equally likely when selecting the best k attempts.
- Evidence anchors: [abstract] "We introduce novel metrics for evaluating the performance of generated code, and use them to explore how well each large language model performs for 12 different computational problem types and six different parallel programming models." [section] "E[max(T_p,s1,n, ..., T_p,sk,n)] = Î£(N choose j-1)/(N choose k) Â· T_p,j,n" shows the mathematical formulation of expected maximum speedup.

### Mechanism 3
- Claim: Providing correct implementations in one execution model improves LLM code generation in another execution model through translation tasks.
- Mechanism: When given a correct implementation in one model (e.g., serial), the LLM can leverage the structural understanding to generate correct code in another model (e.g., OpenMP) by translating the algorithmic structure rather than reasoning from scratch.
- Core assumption: The LLM can recognize and transfer algorithmic patterns between different execution models when provided with a reference implementation.
- Evidence anchors: [abstract] "We show that providing LLMs with correct implementations in one execution model helps improve code generation in another execution model but doesn't necessarily enhance performance." [section] "All of the LLMs score better when given a correct example implementation in a different execution model i.e. translation."

## Foundational Learning

- Concept: Probability theory and combinatorics
  - Why needed here: Understanding how pass@k and speedup metrics work requires knowledge of probability distributions and combinatorial calculations
  - Quick check question: If an LLM generates 20 samples for a prompt and 5 are correct, what is the probability that at least one correct solution appears in 5 attempts?

- Concept: Parallel programming models and their characteristics
  - Why needed here: Evaluating LLM performance requires understanding the differences between serial, OpenMP, MPI, CUDA, HIP, and Kokkos programming models
  - Quick check question: What is the key difference between shared memory (OpenMP) and distributed memory (MPI) programming models?

- Concept: Performance benchmarking and measurement
  - Why needed here: The speedup and efficiency metrics require understanding of performance measurement techniques and hardware dependencies
  - Quick check question: Why is it important to measure performance on the same hardware when comparing different LLMs?

## Architecture Onboarding

- Component map: LLM inference -> Code compilation and execution -> Test harness evaluation -> Metric calculation -> Results aggregation
- Critical path: LLM generates code â†’ code is compiled and executed â†’ results are collected and evaluated â†’ metrics are calculated â†’ conclusions are drawn about LLM capabilities
- Design tradeoffs: Using larger models provides better performance but increases computational cost; using more samples improves metric accuracy but increases evaluation time; testing on diverse hardware provides broader insights but introduces variability
- Failure signatures: If generated code fails to compile, the LLM likely misunderstood the prompt or lacks knowledge of required syntax; if code executes but produces incorrect results, the LLM may have logical errors in the algorithm; if code executes correctly but performs poorly, the LLM may not understand parallel optimization techniques
- First 3 experiments:
  1. Generate 20 samples for a single prompt using GPT-3.5 with temperature 0.2 and verify that pass@1 calculation works correctly
  2. Compile and execute generated code for a simple problem type (e.g., transform) across all execution models to verify the test harness functionality
  3. Compare speedup and efficiency metrics for a single problem type across different LLMs to verify performance measurement accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper bound of LLM performance on parallel code generation, and how much improvement can be achieved through fine-tuning on HPC-specific datasets?
- Basis in paper: [explicit] The authors note that "further fine-tuning of the open-source code models can improve their performance on parallel code generation" and observe that Phind-CodeLlama-V2, which was fine-tuned on 1.5 billion tokens of code data, performs significantly better than base CodeLlama models.
- Why unresolved: The paper only compares a limited set of models and does not explore the full potential of fine-tuning on HPC-specific datasets or architectural modifications for parallel code generation.
- What evidence would resolve it: Systematic evaluation of various fine-tuning strategies on HPC-specific datasets, including architectural modifications and curriculum learning approaches, with comparison to the performance of GPT-4 and other state-of-the-art models.

### Open Question 2
- Question: How do different prompt engineering strategies affect the performance of LLMs on parallel code generation tasks?
- Basis in paper: [explicit] The authors mention that "we evaluated several prompting formats for translation" and found that "the format in Listing 2 to be the most effective," suggesting that prompt engineering significantly impacts performance.
- Why unresolved: The paper only explores a limited set of prompt engineering techniques and does not systematically investigate the impact of different strategies on parallel code generation performance.
- What evidence would resolve it: Comprehensive evaluation of various prompt engineering techniques, including few-shot learning, chain-of-thought prompting, and task-specific instructions, with comparison to the performance of different LLMs on the ParEval benchmark.

### Open Question 3
- Question: How do the performance and scalability of LLM-generated parallel code compare to human-written code across different problem sizes and architectures?
- Basis in paper: [inferred] The authors introduce novel metrics for evaluating the performance and scaling of LLM-generated code, but do not compare the results to human-written code across different problem sizes and architectures.
- Why unresolved: The paper only evaluates the performance of LLM-generated code relative to a sequential baseline, without comparing it to human-written code across different problem sizes and architectures.
- What evidence would resolve it: Systematic comparison of the performance and scalability of LLM-generated parallel code to human-written code across different problem sizes, architectures, and execution models, using the ParEval benchmark and the proposed performance metrics.

## Limitations

- The evaluation framework relies heavily on the correctness of ground-truth solutions and the comprehensiveness of test harnesses
- Performance measurements may be influenced by hardware-specific optimizations that don't generalize across different computing environments
- The study only evaluates a limited set of models and does not explore the full potential of fine-tuning on HPC-specific datasets

## Confidence

- **High confidence**: The core finding that LLMs perform significantly worse on parallel code generation compared to serial code is well-supported by the systematic evaluation across 420 prompts and 7 execution models. The statistical significance is demonstrated through multiple samples per prompt.
- **Medium confidence**: The novel metrics (speedup@k and efficiency@k) show promise but require further validation. The mathematical formulations are sound, but their practical utility in comparing LLMs across different problem domains needs more extensive testing.
- **Medium confidence**: The observation that providing correct implementations in one execution model helps with code generation in another model