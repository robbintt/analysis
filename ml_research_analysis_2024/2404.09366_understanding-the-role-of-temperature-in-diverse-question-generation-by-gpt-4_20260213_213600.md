---
ver: rpa2
title: Understanding the Role of Temperature in Diverse Question Generation by GPT-4
arxiv_id: '2404.09366'
source_url: https://arxiv.org/abs/2404.09366
tags:
- temperature
- questions
- generation
- question
- bloom
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We investigated how GPT-4's temperature parameter affects the diversity
  of generated multiple choice questions across different levels of Bloom's Taxonomy.
  Using temperature values of 0.2, 1.0, and 1.2 with 52 learning objectives spanning
  introductory CS topics, we found that higher temperatures (1.0-1.2) produced significantly
  more diverse question sets compared to lower temperature (0.2).
---

# Understanding the Role of Temperature in Diverse Question Generation by GPT-4

## Quick Facts
- arXiv ID: 2404.09366
- Source URL: https://arxiv.org/abs/2404.09366
- Reference count: 7
- Primary result: Higher temperatures (1.0-1.2) produce significantly more diverse question sets than lower temperature (0.2)

## Executive Summary
This study investigates how GPT-4's temperature parameter affects the diversity of generated multiple choice questions across different levels of Bloom's Taxonomy. Using 52 learning objectives from introductory CS topics and temperature values of 0.2, 1.0, and 1.2, the research found that higher temperatures significantly increase question diversity while maintaining quality. The findings reveal that lower Bloom's levels (Remember/Understand) are more challenging to generate diverse questions for compared to higher levels (Create/Analyze). These results provide practical guidance for using temperature between 1.0-1.2 when generating diverse MCQs, with particular attention needed for lower cognitive levels.

## Method Summary
The study generated three multiple choice questions per learning objective using GPT-4 with three different temperature settings (0.2, 1.0, 1.2). Researchers used 52 learning objectives spanning introductory CS topics and randomly selected question types from five categories. Four human instructors evaluated the question sets for distinctness (Q1-distinct) and completeness (Q2-complete) using established criteria. Inter-rater agreement was measured using Fleiss's kappa, and statistical analysis was performed using chi-squared tests to compare distributions across temperature settings.

## Key Results
- Higher temperatures (1.0-1.2) produced significantly more diverse question sets than lower temperature (0.2)
- At temperature 0.2, 46% of question sets had 2-3 similar questions, dropping to 17-20% at temperatures 1.0-1.2
- Lower Bloom's levels (Remember/Understand) showed higher similarity rates (67%/53%) compared to higher levels (Create/Analyze at 32%/54%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Higher temperature values (1.0-1.2) lead to more diverse question sets by increasing the randomness in token selection during generation.
- Mechanism: Temperature in language models controls the softmax distribution over next-token probabilities. Higher values flatten this distribution, allowing the model to sample from a wider range of tokens rather than always picking the most likely ones. This creates variation in question structure, wording, and approach while maintaining semantic coherence.
- Core assumption: The underlying question generation model has sufficient knowledge to generate multiple valid question formulations for a given learning objective, and higher temperature doesn't push it into generating invalid or nonsensical content.
- Evidence anchors:
  - [abstract] "we found that higher temperatures (1.0-1.2) produced significantly more diverse question sets compared to lower temperature (0.2)"
  - [section] "When temperature is set at 1.0 and 1.2, we tend to generate more sets of questions where all three questions are distinct"
  - [corpus] Weak evidence - corpus neighbors discuss temperature effects generally but don't specifically validate this mechanism for educational question generation
- Break condition: Temperature values above 1.2 lead to "generally unusable questions" as mentioned in the methodology, suggesting the mechanism breaks when randomness overwhelms the model's learned coherence.

### Mechanism 2
- Claim: Temperature affects the types of similarities between generated question sets, exposing different patterns of redundancy.
- Mechanism: Different temperature settings create distinct patterns in how questions become similar. Lower temperatures (0.2) tend to produce questions that are near-duplicates or minor rephrasings, while higher temperatures create more varied but potentially still related questions that share conceptual similarities without direct duplication.
- Core assumption: The temperature parameter influences not just diversity but the specific nature of similarity relationships between generated items.
- Evidence anchors:
  - [abstract] "with different temperatures exposing different types of similarity between generated sets of questions"
  - [section] "When temperature is set to 0.2, we tend to generate more sets of questions where either two of the questions were too similar or all three questions were too similar"
  - [corpus] Weak evidence - corpus doesn't provide specific validation of this temperature-similarity relationship pattern
- Break condition: If the model lacks sufficient variation in its training data or the learning objectives are too narrow, even higher temperatures may only produce variations on the same theme rather than truly diverse approaches.

### Mechanism 3
- Claim: Temperature effects vary by Bloom's Taxonomy level, with lower cognitive levels showing higher similarity rates.
- Mechanism: The model's ability to generate diverse questions is constrained by the inherent complexity and ambiguity of the cognitive level being targeted. Lower Bloom's levels (Remember/Understand) have more straightforward question patterns, making it easier for the model to generate similar questions. Higher levels (Create/Analyze) have more varied question structures and approaches, allowing temperature to have more pronounced diversity effects.
- Core assumption: The diversity challenge is not primarily about the learning objectives themselves but about the generation process's ability to vary question patterns at different cognitive complexity levels.
- Evidence anchors:
  - [abstract] "Question diversity varied by Bloom's level, with lower levels (Remember/Understand) showing higher similarity rates (67%/53%) compared to higher levels (Create/Analyze at 32%/54%)"
  - [section] "If we look only at the level of Bloom's Taxonomy and look at the percentage of MCQ sets where Q1-distinct identified multiple distinct questions... as we go to higher levels of Bloom's Taxonomy, instructors generally view the questions as having fewer duplicates"
  - [corpus] Weak evidence - corpus neighbors don't specifically address Bloom's Taxonomy level effects on temperature-driven diversity
- Break condition: If the model's training data doesn't adequately represent diverse question patterns at lower cognitive levels, temperature adjustments may not overcome this fundamental limitation.

## Foundational Learning

- Concept: Temperature parameter in language models
  - Why needed here: Understanding how temperature controls randomness in token generation is essential to grasp why higher values produce more diverse questions
  - Quick check question: What happens to the probability distribution over tokens when temperature is increased from 0.2 to 1.2?

- Concept: Bloom's Taxonomy cognitive levels
  - Why needed here: The study shows that question diversity varies significantly by cognitive level, so understanding the hierarchy and characteristics of each level is crucial
  - Quick check question: Which Bloom's Taxonomy levels showed the highest similarity rates in generated questions?

- Concept: Inter-rater reliability and Fleiss's kappa
  - Why needed here: The study reports a Fleiss's kappa of 0.30472, indicating fair agreement, which affects how we interpret the results
  - Quick check question: What does a Fleiss's kappa value of 0.30472 indicate about instructor agreement?

## Architecture Onboarding

- Component map: Learning Objective → Question Type Selection → GPT-4 Generation (with Temperature Parameter) → Human Evaluation (Q1-distinct and Q2-complete) → Analysis of Diversity Metrics

- Critical path: Learning Objective → Question Type Selection → GPT-4 Generation (with Temperature Parameter) → Human Evaluation (Q1-distinct and Q2-complete) → Analysis of Diversity Metrics

- Design tradeoffs: Higher temperatures increase diversity but risk generating unusable questions; lower temperatures ensure coherence but reduce diversity. The sweet spot appears to be 1.0-1.2 where diversity is maximized without sacrificing quality.

- Failure signatures: Questions becoming nonsensical or off-topic (temperature too high), questions being near-duplicates or minor rephrasings (temperature too low), or systematic failure to generate diverse questions at lower Bloom's levels regardless of temperature.

- First 3 experiments:
  1. Generate 10 question sets at temperature 0.2, 1.0, and 1.2 for the same learning objective and compare diversity scores
  2. Test the same learning objective across all five question types at temperature 1.0 to assess type-specific diversity effects
  3. Generate questions for learning objectives at different Bloom's levels (Remember vs. Create) at temperature 1.0 to validate taxonomy-level diversity differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do other LLM parameters like frequency_penalty affect question diversity compared to temperature?
- Basis in paper: [explicit] The authors mention wanting to experiment with other parameters like frequency_penalty in future work
- Why unresolved: This study focused exclusively on temperature parameter effects, leaving other parameters unexplored
- What evidence would resolve it: Comparative analysis of MCQ diversity metrics across different frequency_penalty values using the same experimental methodology

### Open Question 2
- Question: Why are lower Bloom's Taxonomy levels (Remember/Understand) particularly challenging for diverse question generation?
- Basis in paper: [explicit] The authors found lower taxonomy levels had higher duplicate rates (67%/53%) compared to higher levels
- Why unresolved: The study identified this pattern but didn't investigate the underlying causes
- What evidence would resolve it: Qualitative analysis of question structures and content differences across taxonomy levels to identify systematic generation challenges

### Open Question 3
- Question: Does question type (Fill-in-the-blank, Code Tracing, etc.) interact with temperature effects on diversity?
- Basis in paper: [explicit] The authors randomly chose question types but didn't analyze type-specific temperature effects
- Why unresolved: Question types were controlled for but not analyzed as a variable in the diversity results
- What evidence would resolve it: Temperature-diversity relationship analysis stratified by question type to identify any type-specific optimal temperature ranges

## Limitations

- The study only tested three discrete temperature values (0.2, 1.0, 1.2), leaving uncertainty about the optimal range between these points
- The human evaluation relied on random sampling of question sets rather than complete annotation, potentially introducing selection bias
- The reported Fleiss's kappa of 0.30472 indicates only fair inter-rater agreement, suggesting criteria for "distinct" questions may not have been consistently interpreted

## Confidence

**High Confidence:** The finding that higher temperatures (1.0-1.2) produce significantly more diverse question sets compared to lower temperature (0.2) is well-supported by the data and analysis. The chi-squared test results provide statistical validation of this relationship.

**Medium Confidence:** The observation that temperature effects vary by Bloom's Taxonomy level is supported by the data but may be influenced by the limited number of learning objectives at higher cognitive levels. The specific percentage differences (67%/53% vs 32%/54%) should be interpreted with caution given the fair inter-rater agreement.

**Low Confidence:** The claim about "different types of similarity" exposed by different temperature settings is based on qualitative observations rather than systematic analysis. The mechanism by which temperature creates these different similarity patterns requires further investigation.

## Next Checks

1. **Replicate with expanded temperature range:** Generate questions using temperature values at 0.4, 0.6, 0.8, 1.4, and 1.6 to identify the precise optimal range and determine if the observed effects continue beyond the tested values.

2. **Cross-domain validation:** Apply the same temperature experimentation to learning objectives from non-CS domains (e.g., humanities, social sciences) to test generalizability and identify domain-specific effects.

3. **Automated diversity metrics:** Develop and validate automated measures of question diversity (e.g., semantic similarity scores, structural variation metrics) to supplement human evaluation and enable larger-scale testing without annotator burden.