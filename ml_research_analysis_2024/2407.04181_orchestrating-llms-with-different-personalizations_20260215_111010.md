---
ver: rpa2
title: Orchestrating LLMs with Different Personalizations
arxiv_id: '2407.04181'
source_url: https://arxiv.org/abs/2407.04181
tags:
- preference
- like
- your
- have
- mope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning large language models
  with individual user preferences, known as Reinforcement Learning from Personalized
  Human Feedback (RLPHF). The core method, Mixture of Preference Experts (MoPE), dynamically
  merges the outputs of specialized expert LLMs using a lightweight Preference Control
  Model (PCM) that computes context-dependent weights for each token based on user
  preferences.
---

# Orchestrating LLMs with Different Personalizations

## Quick Facts
- arXiv ID: 2407.04181
- Source URL: https://arxiv.org/abs/2407.04181
- Reference count: 40
- Average pairwise win rate: 62.00% against baselines

## Executive Summary
This paper addresses the challenge of aligning large language models with individual user preferences through Reinforcement Learning from Personalized Human Feedback (RLPHF). The authors propose Mixture of Preference Experts (MoPE), a method that dynamically merges outputs from specialized expert LLMs using a lightweight Preference Control Model (PCM) that computes context-dependent weights for each token based on user preferences. Unlike prior approaches that merge model parameters, MoPE operates as a black-box method without requiring access to expert model weights, making it more scalable and practical for real-world deployment.

## Method Summary
MoPE works by training a small PCM (160M parameter LLaMA-based model) to output context-dependent weights for merging next-token probabilities from six expert LLMs, each specialized for different preference dimensions (elementary, knowledgeable, concise, informative, friendly, unfriendly). The PCM learns these weights through REBEL or PPO online RL to maximize combined Bradley-Terry rewards across preference dimensions. During inference, MoPE generates tokens by computing PCM weights for each token, merging the expert distributions, and sampling from the combined distribution. This approach avoids the computational cost and complexity of fine-tuning individual models for each user while maintaining high personalization quality.

## Key Results
- MoPE achieves an average pairwise win rate of 62.00% compared to baselines
- Outperforms Personalized Soup (55.90%) and Multi-Objective RL (60.93%) baselines
- Demonstrates effective personalization across eight combined preference profiles

## Why This Works (Mechanism)
MoPE's effectiveness stems from its ability to dynamically adjust the contribution of different expert models based on context and user preferences, rather than relying on static parameter merging. The PCM learns to predict optimal weighting schemes that maximize preference satisfaction across multiple dimensions simultaneously, capturing complex preference interactions that simpler approaches miss.

## Foundational Learning
- **Reinforcement Learning from Human Feedback (RLHF)**: Why needed: Provides framework for aligning models with human preferences; Quick check: Verify reward models are properly trained on pairwise comparisons
- **Bradley-Terry Model**: Why needed: Enables probabilistic comparison of preference satisfaction; Quick check: Ensure Bradley-Terry rewards are correctly computed from multiple dimensions
- **Preference Control Model (PCM)**: Why needed: Learns context-dependent expert weighting; Quick check: Monitor PCM weight entropy during training
- **Mixture of Experts**: Why needed: Enables specialization while maintaining unified capability; Quick check: Verify expert diversity through KL divergence analysis

## Architecture Onboarding

Component map: Data -> Reward Models -> Expert LLMs -> PCM -> MoPE Inference

Critical path: User preference context → PCM → Expert weights → Token distribution → Generated text

Design tradeoffs: Black-box vs parameter access (MoPE avoids weight access but needs expert availability), dynamic vs static weighting (MoPE adapts per token vs fixed schemes), single vs multiple experts (MoPE handles multiple dimensions vs single-purpose models)

Failure signatures: Degenerate PCM weights (all mass on one expert), merged distribution collapse (too peaked or flat), reward misalignment (PCM optimizes wrong objective)

First experiments:
1. Train reward models on pairwise comparison data and validate with held-out pairs
2. Implement PCM with REBEL training and monitor weight entropy
3. Run MoPE inference on sample prompts and compare with individual expert outputs

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown pairwise comparison data format and preprocessing for reward model training
- Unspecified REBEL hyperparameters (η, learning rate, batch size) and training combinations
- Evaluation limited to 50 instances from Koala dataset, may not reflect real-world diversity

## Confidence
- High confidence in methodological contribution and black-box advantage
- Medium confidence in empirical superiority due to limited evaluation details
- Low confidence in reproducibility without specification of training data and hyperparameters

## Next Checks
1. Replicate PCM training with specified REBEL hyperparameters and monitor weight entropy
2. Conduct ablation studies comparing MoPE with different expert combinations
3. Extend evaluation to larger, more diverse datasets beyond the 50-instance Koala subset