---
ver: rpa2
title: Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters
arxiv_id: '2403.02677'
source_url: https://arxiv.org/abs/2403.02677
tags:
- data
- filtering
- image-text
- image
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving image-text data filtering
  for vision-language models. The core method is to fine-tune multimodal language
  models on quality scoring tasks to create effective data filters that outperform
  traditional methods like CLIPScore.
---

# Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters

## Quick Facts
- **arXiv ID:** 2403.02677
- **Source URL:** https://arxiv.org/abs/2403.02677
- **Reference count:** 34
- **Primary result:** MLM filter achieves 1.7% better average performance than CLIPScore across 38 downstream tasks

## Executive Summary
This paper addresses the challenge of improving image-text data filtering for vision-language models. The authors propose fine-tuning multimodal language models on quality scoring tasks to create more effective data filters. The MLM (Masked Language Model) filter demonstrates significant improvements over traditional methods like CLIPScore, showing strong correlation with human scoring while serving as a drop-in replacement for CLIPScore in various vision-language architectures.

## Method Summary
The authors fine-tune multimodal language models using a quality scoring task, where the model learns to predict whether an image-text pair is of high quality. The MLM filter is trained on a curated dataset and evaluated against CLIPScore across multiple vision-language benchmarks. The approach leverages the multimodal capabilities of language models to capture both visual and textual coherence, enabling more nuanced quality assessments than single-modality scoring methods.

## Key Results
- MLM filter achieves 1.7% better average performance than CLIPScore across 38 downstream tasks on the DATACOMP benchmark
- Strong correlation with human scoring for image-text quality assessment
- Can be used as a drop-in replacement for CLIPScore in vision-language model architectures
- Demonstrates consistent improvements across multiple vision-language tasks

## Why This Works (Mechanism)
The MLM filter leverages the combined understanding of both visual and textual information through fine-tuning. By training on quality scoring tasks, the model learns to identify subtle correlations between image and text that indicate high quality. This dual-modality approach captures relationships that single-modality methods like CLIPScore miss, leading to more accurate quality assessments.

## Foundational Learning
- **Multimodal fusion**: Why needed - to combine visual and textual information; Quick check - verify both modalities contribute to final score
- **Quality scoring task**: Why needed - to train model on distinguishing good vs bad image-text pairs; Quick check - confirm model learns to rank pairs by quality
- **Fine-tuning process**: Why needed - to adapt pre-trained models to specific quality assessment task; Quick check - validate performance improvement after fine-tuning
- **Human correlation metrics**: Why needed - to ensure model aligns with human judgment; Quick check - measure Spearman correlation with human scores
- **Vision-language benchmarks**: Why needed - to evaluate performance across diverse tasks; Quick check - test on multiple established benchmarks
- **Drop-in replacement compatibility**: Why needed - to ensure easy integration into existing pipelines; Quick check - verify interchangeability with CLIPScore

## Architecture Onboarding
- **Component map:** Pre-trained multimodal model → Fine-tuning on quality scoring → MLM filter
- **Critical path:** Input image-text pair → Multimodal encoding → Quality prediction → Output score
- **Design tradeoffs:** Fine-tuned MLM offers better accuracy but requires more computational resources than CLIPScore
- **Failure signatures:** Poor performance on domain-specific content, potential overfitting to training dataset distribution
- **3 first experiments:**
  1. Compare MLM filter vs CLIPScore on a small subset of DATACOMP tasks
  2. Measure correlation between MLM filter scores and human ratings on sample pairs
  3. Test MLM filter as CLIPScore replacement in a vision-language model pipeline

## Open Questions the Paper Calls Out
None

## Limitations
- Performance advantage may not generalize to all vision-language applications or datasets
- Evaluation focuses primarily on English-language image-text pairs, limiting multilingual applicability
- Computational overhead of fine-tuning and inference compared to simpler scoring methods is not fully characterized

## Confidence
- **High:** MLM filter's superior performance over CLIPScore on DATACOMP benchmark
- **Medium:** Claims about human correlation and real-world applicability based on proxy measures
- **Low:** Generalization to non-English or highly specialized domains due to limited evaluation scope

## Next Checks
1. Test the MLM filter's performance on multilingual image-text datasets to assess cross-language generalization
2. Compare computational efficiency and inference speed against CLIPScore in production-scale settings
3. Evaluate performance on domain-specific datasets (medical, scientific, or technical imagery) to establish applicability beyond general-purpose vision-language tasks