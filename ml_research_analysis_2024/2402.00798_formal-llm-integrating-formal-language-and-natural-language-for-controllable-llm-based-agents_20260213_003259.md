---
ver: rpa2
title: 'Formal-LLM: Integrating Formal Language and Natural Language for Controllable
  LLM-based Agents'
arxiv_id: '2402.00798'
source_url: https://arxiv.org/abs/2402.00798
tags:
- plan
- language
- task
- plans
- automaton
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel "Formal-LLM" framework for LLM-based
  agents by integrating the expressiveness of natural language and the precision of
  formal language. Specifically, the framework allows agent developers to express
  their requirements or constraints for the planning process as an automaton.
---

# Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents

## Quick Facts
- arXiv ID: 2402.00798
- Source URL: https://arxiv.org/abs/2402.00798
- Reference count: 40
- Primary result: Novel framework integrating formal and natural language to guide LLM plan generation with over 50% performance improvement

## Executive Summary
This paper introduces Formal-LLM, a framework that combines the expressiveness of natural language with the precision of formal language to create controllable LLM-based agents. The approach translates task constraints into context-free grammars and pushdown automata, which then guide the LLM's plan generation process through a stack-based mechanism. The framework includes a backtracking mechanism to handle dead-ends and can be integrated with reinforcement learning from task feedback (RLTF) for further improvement. Experiments demonstrate significant performance gains across benchmark and practical tasks.

## Method Summary
The Formal-LLM framework operates by first translating task constraints into a context-free grammar (CFG), which is then converted into a pushdown automaton (PDA). During plan generation, the LLM receives prompts that include the current task state, progress, and feasible choices based on the PDA's current state and stack configuration. The framework employs a backtracking mechanism to handle dead-ends by returning to previous steps and exploring alternative branches. The valid plans generated can be used with RLTF to fine-tune the LLM's parameters based on plan performance. The approach is evaluated on both benchmark tasks (OpenAGI and TravelPlanner) and practical real-life tasks.

## Key Results
- Achieved over 50% overall performance increase compared to baseline methods
- Generated plans that consistently satisfy formal constraints defined by PDAs
- Demonstrated effectiveness on both benchmark and real-life practical tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Formal-LLM framework guarantees plan validity by constraining the LLM generation process with a PDA that enforces formal language rules.
- Mechanism: The framework translates task constraints into a context-free grammar (CFG), then into a PDA. The LLM is prompted to generate plans that follow the PDA's state transitions, ensuring that generated plans are syntactically valid and executable.
- Core assumption: The PDA accurately encodes all constraints needed for valid plans, and the LLM reliably follows the PDA's state transitions when prompted.
- Evidence anchors:
  - [abstract] "A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable."
  - [section 4.3] "The automaton initiates the plan generation process from the start state... When the PDA in Figure 3 is in state q1 with the symbol I at the top of the stack, three viable transits... are included in the {feasible choices} of the prompt."
  - [corpus] Weak evidence; related works focus on safety and planning but not on CFG/PDA-based LLM plan generation.
- Break condition: If the PDA does not fully capture all constraints, or if the LLM ignores the PDA's state transitions, invalid plans could be generated.

### Mechanism 2
- Claim: The backtracking mechanism increases the probability of finding valid plans by allowing the planning process to return to previous steps when a dead end is reached.
- Mechanism: When the automaton encounters a dead end (no valid transitions available), the framework backtracks to the previous step, records the automaton's details, and eliminates the dead-end branch from the choice list. This process continues until a valid plan is found or all possibilities are exhausted.
- Core assumption: The backtracking mechanism can systematically explore all possible valid plans, and the LLM can effectively generate plans within the constrained choice space.
- Evidence anchors:
  - [section 4.4] "To address this challenge, we propose to document the automaton's details at each step... When the automaton confronts a dead-end, we initiate backtracking to the preceding steps until there is an unexplored branch."
  - [abstract] "Furthermore, we introduce the backtracking mechanism in the Formal-LLM to increase the probability of finding valid plans for the agent, which enables the planning process to return to the previous step when a dead end is reached on the automaton."
  - [corpus] Weak evidence; related works focus on LLM planning but not on backtracking mechanisms for plan validity.
- Break condition: If the search space is too large or the backtracking mechanism is not efficient, the framework may fail to find a valid plan within a reasonable time.

### Mechanism 3
- Claim: Integrating Formal-LLM with RLTF improves plan performance by increasing the number of valid rewards for LLM fine-tuning.
- Mechanism: The Formal-LLM framework ensures that only valid plans are generated, which are then executed on benchmark data to assess performance. The performance is used as a reward for RL to update the parameters of the LLM, leading to improved plan performance over time.
- Core assumption: The valid plans generated by Formal-LLM are informative for RLTF, and the RL algorithm can effectively learn from these rewards to improve the LLM's planning capabilities.
- Evidence anchors:
  - [abstract] "Our Formal-LLM approach guarantees that invalid plans are excluded during the agent's plan generation. As a result, our approach helps to increase the amount of valid rewards for LLM fine-tuning, which improves the performance of fine-tuned LLM-based agents."
  - [section 4.4] "Our framework ensures that invalid plans are always excluded during the LLM-based agent's plan generation. Therefore, the number of valid rewards for LLM fine-tuning increases, and our approach can improve the performance of fine-tuned LLM-based agents."
  - [corpus] Weak evidence; related works focus on LLM planning and RL but not on the specific combination of Formal-LLM and RLTF.
- Break condition: If the valid plans are not diverse enough or the RL algorithm is not effective, the performance improvement may be limited.

## Foundational Learning

- Concept: Context-Free Grammar (CFG)
  - Why needed here: CFG is used to represent the constraints for the agent's planning process in a formal language.
  - Quick check question: What are the four components of a CFG, and how do they define the structure of a language?
- Concept: Pushdown Automaton (PDA)
  - Why needed here: PDA is used to guide the LLM's plan generation process by enforcing the constraints defined in the CFG.
  - Quick check question: How does a PDA use a stack to keep track of the current state and determine valid transitions?
- Concept: Reinforcement Learning from Task Feedback (RLTF)
  - Why needed here: RLTF is used to fine-tune the LLM's parameters based on the performance of the generated plans, improving the LLM's planning capabilities over time.
  - Quick check question: How does RLTF use the performance of the generated plans as a reward to update the LLM's parameters?

## Architecture Onboarding

- Component map: Natural Language Prompt -> Pushdown Automaton (PDA) -> LLM -> Backtracking Mechanism -> RLTF
- Critical path:
  1. Translate task constraints into CFG
  2. Convert CFG into PDA
  3. Generate natural language prompt based on PDA's state
  4. LLM generates plan based on prompt and PDA's constraints
  5. If dead end, backtrack to previous step and try alternative choices
  6. Execute valid plan and assess performance
  7. Use performance as reward for RLTF to fine-tune LLM
- Design tradeoffs:
  - Using PDA for plan generation ensures validity but may limit the LLM's creativity and flexibility
  - Backtracking increases the probability of finding valid plans but may also increase the planning time
  - Integrating RLTF improves plan performance but requires additional computational resources and training data
- Failure signatures:
  - LLM generates plans that do not follow the PDA's constraints
  - Backtracking mechanism fails to find a valid plan within a reasonable time
  - RLTF does not improve the LLM's planning capabilities despite valid rewards
- First 3 experiments:
  1. Implement the Formal-LLM framework on a simple benchmark task with known constraints
  2. Test the backtracking mechanism on a task with multiple possible valid plans
  3. Integrate RLTF with Formal-LLM and evaluate the performance improvement on a complex benchmark task

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Lack of detailed implementation specifications for CFG construction and backtracking mechanism
- Experimental validation relies on proprietary benchmarks and self-reported metrics
- Limited discussion of computational overhead and efficiency considerations
- Integration with RLTF mentioned but not thoroughly evaluated

## Confidence
- High confidence in the theoretical framework and mechanism design
- Medium confidence in the experimental results due to limited replication details
- Low confidence in the scalability and practical deployment aspects

## Next Checks
1. Implement the Formal-LLM framework on a simple benchmark task with known constraints to verify the CFG-to-PDA translation and plan generation process
2. Test the backtracking mechanism on a task with multiple possible valid plans to evaluate its effectiveness in handling dead-ends
3. Integrate RLTF with Formal-LLM and evaluate the performance improvement on a complex benchmark task, comparing results with state-of-the-art baselines