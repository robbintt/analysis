---
ver: rpa2
title: 'Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code
  Rewriting'
arxiv_id: '2405.16133'
source_url: https://arxiv.org/abs/2405.16133
tags:
- code
- synthetic
- detection
- similarity
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting code generated
  by Large Language Models (LLMs) by proposing a novel zero-shot synthetic code detector
  based on code rewriting and similarity measurement. The method leverages the observation
  that differences between LLM-rewritten and original code tend to be smaller when
  the original code is synthetic.
---

# Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting

## Quick Facts
- arXiv ID: 2405.16133
- Source URL: https://arxiv.org/abs/2405.16133
- Reference count: 8
- Key outcome: Zero-shot synthetic code detector using code rewriting and similarity measurement achieves 20.5% and 29.1% AUROC improvements on APPS and MBPP benchmarks respectively.

## Executive Summary
This paper introduces a novel zero-shot approach for detecting code generated by Large Language Models (LLMs). The method leverages the observation that differences between LLM-rewritten and original code tend to be smaller when the original code is synthetic. By rewriting input code using an LLM and measuring similarity between original and rewritten versions, the detector can identify synthetic code without requiring labeled training data. The approach demonstrates significant improvements over existing state-of-the-art synthetic content detectors across two major benchmarks.

## Method Summary
The proposed zero-shot synthetic code detector operates through a three-step process: first, it rewrites the input code using an LLM; second, it measures the similarity between the original and rewritten code using a self-supervised contrastive learning model (SCScore); and third, it estimates the expected similarity score by sampling multiple rewritten codes. The core insight is that synthetic code tends to have smaller differences when rewritten compared to human-written code, allowing for detection without traditional supervised training.

## Key Results
- Achieved 20.5% AUROC improvement on the APPS benchmark compared to existing detectors
- Achieved 29.1% AUROC improvement on the MBPP benchmark
- Demonstrated effectiveness of zero-shot approach without requiring labeled training data

## Why This Works (Mechanism)
The method works because synthetic code generated by LLMs tends to have lower entropy and more predictable patterns than human-written code. When such code is rewritten by another LLM, the changes are minimal since the rewriting process encounters less ambiguity. In contrast, human-written code often contains unique patterns, comments, and stylistic choices that lead to more substantial differences when rewritten. This difference in rewrite behavior creates a measurable signal that can distinguish synthetic from human-generated code.

## Foundational Learning

**Code Similarity Measurement**
- Why needed: To quantify differences between original and rewritten code
- Quick check: SCScore model should maintain consistent performance across different code domains

**Contrastive Learning**
- Why needed: To learn effective code representations for similarity comparison
- Quick check: Model should converge within reasonable training iterations on code datasets

**LLM-based Code Rewriting**
- Why needed: To generate perturbed versions of input code for comparison
- Quick check: Rewriting should preserve functionality while introducing stylistic variations

## Architecture Onboarding

**Component Map**
LLM Rewriter -> SCScore Similarity Model -> Expected Score Estimator -> Decision Threshold

**Critical Path**
Input Code -> Multiple LLM Rewritings -> Similarity Measurements -> Score Aggregation -> Classification

**Design Tradeoffs**
- Multiple rewrites improve reliability but increase computational cost
- Choice of LLM affects both rewriting quality and detection performance
- Similarity threshold selection balances precision and recall

**Failure Signatures**
- Low similarity scores for both synthetic and human code indicate ineffective rewriting
- High variance in similarity scores suggests unstable rewriting or similarity measurement
- Poor performance on one benchmark may indicate domain-specific limitations

**First 3 Experiments**
1. Vary the number of rewrites (1, 5, 10) to assess impact on detection accuracy
2. Test different LLM models for rewriting to evaluate sensitivity
3. Apply the method to code in different programming languages to test generalizability

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Requires access to an LLM for code rewriting, limiting deployment scenarios
- Effectiveness of SCScore model and expected score estimation needs broader validation
- Experimental evaluation limited to APPS and MBPP benchmarks; generalizability to other code types unclear

## Confidence

**High confidence**: Core methodology of using code rewriting and similarity measurement is sound and well-explained, with compelling performance improvements over existing detectors.

**Medium confidence**: Generalizability to other code datasets and programming languages not fully explored; effectiveness of SCScore model and expected score estimation warrant further investigation.

**Low confidence**: No discussion of potential adversarial attacks or countermeasures against the proposed method.

## Next Checks

1. Evaluate performance on additional code datasets and programming languages to assess generalizability
2. Conduct ablation studies to quantify impact of different components (SCScore model, expected score estimation) on overall performance
3. Investigate potential adversarial attacks and develop countermeasures to enhance method robustness