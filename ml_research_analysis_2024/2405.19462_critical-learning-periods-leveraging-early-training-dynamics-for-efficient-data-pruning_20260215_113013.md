---
ver: rpa2
title: 'Critical Learning Periods: Leveraging Early Training Dynamics for Efficient
  Data Pruning'
arxiv_id: '2405.19462'
source_url: https://arxiv.org/abs/2405.19462
tags:
- data
- pruning
- translation
- association
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Checkpoints Across Time (CAT), a data pruning
  technique for neural machine translation that leverages early training dynamics
  to identify high-value data points. CAT uses the variability in perplexity across
  early checkpoints to rank and select data points most relevant to model performance.
---

# Critical Learning Periods: Leveraging Early Training Dynamics for Efficient Data Pruning

## Quick Facts
- **arXiv ID**: 2405.19462
- **Source URL**: https://arxiv.org/abs/2405.19462
- **Reference count**: 40
- **Primary result**: CAT achieves comparable performance to full dataset while pruning up to 50% of training data for English-German, English-French, and English-Swahili translation tasks.

## Executive Summary
This paper introduces Checkpoints Across Time (CAT), a novel data pruning technique for neural machine translation that leverages early training dynamics to identify high-value data points. The method computes perplexity differences across early checkpoints to rank and select the most relevant examples, achieving performance comparable to using the full dataset while reducing training data by up to 50%. CAT outperforms random pruning and embedding-based quality estimation techniques, particularly for Indo-European languages, and shows particular promise for pruning noisy or automatically aligned datasets.

## Method Summary
CAT uses perplexity variability across early training checkpoints to rank data points by their learning importance. The method trains an initial model on the full dataset for a few epochs, computes perplexity scores at multiple checkpoints, and ranks examples based on either the difference (CAT-DIFF) or variance (CAT-VAR) of these scores. The top-ranked examples are retained while the rest are pruned. The pruned dataset is then used to train a final NMT model. The approach is evaluated on English-German, English-French, and English-Swahili translation tasks using the fairseq Transformer architecture.

## Key Results
- CAT-DIFF(1,5) maintains 92-99% of full dataset performance when pruning 50% of training data across three language pairs
- CAT outperforms random pruning and embedding-based quality estimators (LaBSE, LASER) on high-quality datasets
- CAT tends to select longer sentences and those with unique or rare words, suggesting it captures more complex training examples
- On noisy datasets like Swahili, translation quality estimators perform better than CAT due to their ability to filter low-quality examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early training dynamics reveal which data points are most valuable for generalization.
- Mechanism: The method leverages the observation that neural networks learn easier features earlier in training and harder features later. By computing perplexity differences across early checkpoints (e.g., epochs 1 and 5), it identifies examples with the largest variability in learning progress, which indicates high-value data points.
- Core assumption: Data points with high perplexity variability across early training stages are more critical for model performance.
- Evidence anchors:
  - [abstract] "CAT uses the variability in perplexity across early checkpoints to rank and select data points most relevant to model performance."
  - [section 3] "We propose a novel method, Checkpoints Across Time (CAT), that leverages perplexity variation to prune of MT datasets with significant improvements over existing methods."
  - [corpus] Weak. Related work focuses on layer pruning and early-bird tickets, not on data pruning via perplexity dynamics.
- Break condition: If early training dynamics don't correlate with example difficulty or if perplexity doesn't capture learning progress effectively.

### Mechanism 2
- Claim: Removing low-value data points while preserving performance reduces computational cost.
- Mechanism: By ranking data points based on their perplexity variability (CAT-DIFF) or variance (CAT-VAR) and pruning the bottom percentile, the method creates a smaller, more efficient training set that maintains model performance.
- Core assumption: The selected subset of data points is sufficient to maintain model performance.
- Evidence anchors:
  - [abstract] "CAT achieves comparable performance to using the full dataset, while pruning up to 50% of training data."
  - [section 5] "Using our best CAT-based method on English-German, English-French and English-Swahili translation tasks, we are able to achieve comparable performance while pruning up to 50% of the training data."
  - [corpus] Weak. Related work focuses on model compression, not data pruning efficiency.
- Break condition: If the pruned dataset lacks sufficient diversity or if the ranking method fails to identify truly valuable examples.

### Mechanism 3
- Claim: Translation quality estimators are less effective for high-quality datasets but perform well on noisy or automatically aligned data.
- Mechanism: Methods like LaBSE and LASER work better on datasets with more noise (like Swahili) because they can filter out low-quality examples, but they are less effective on cleaner datasets (like German) where most examples are already of high quality.
- Core assumption: Translation quality estimators are better at identifying and removing noise in datasets.
- Evidence anchors:
  - [section 5.2] "We find translation quality estimators to be less effective in pruning German than Swahili."
  - [section 5.2] "Pruning Swahili data with LaBSE and LASER yields superior performance compared to training on the full dataset, particularly for the 50% pruning level for the FLORES test set."
  - [corpus] Weak. Related work focuses on filtering methods, not the specific behavior on high vs. low-quality datasets.
- Break condition: If the dataset quality is uniformly high or if the estimators cannot distinguish between subtle quality differences.

## Foundational Learning

- Concept: Perplexity as a measure of model uncertainty.
  - Why needed here: CAT uses perplexity to quantify how well the model predicts data points at different training stages.
  - Quick check question: What does a high perplexity score indicate about a model's prediction on a given example?

- Concept: Neural network training dynamics and critical learning periods.
  - Why needed here: The method relies on the idea that different features are learned at different stages of training.
  - Quick check question: Why might examples with high variability in early training be more valuable for model generalization?

- Concept: Data pruning and its impact on model performance.
  - Why needed here: Understanding how removing data points affects model training and performance is crucial for evaluating CAT.
  - Quick check question: What are the potential risks of pruning too much data from a training set?

## Architecture Onboarding

- Component map:
  NMT model (e.g., fairseq Transformer) -> SentencePiece tokenizer -> CAT algorithm (perplexity computation, ranking, pruning) -> Evaluation metrics (BLEU, chrF++, COMET)

- Critical path:
  1. Train NMT model on full dataset for few epochs
  2. Compute perplexity scores at early checkpoints
  3. Rank data points and prune based on CAT method
  4. Retrain model on pruned dataset
  5. Evaluate performance

- Design tradeoffs:
  - Early checkpoint selection (tradeoff between computational cost and ranking accuracy)
  - Pruning percentage (tradeoff between efficiency and performance)
  - CAT variant choice (CAT-DIFF vs CAT-VAR)

- Failure signatures:
  - Performance degradation after pruning (indicates too much data removed or poor ranking)
  - Inconsistent results across runs (indicates sensitivity to random seeds or dataset splits)

- First 3 experiments:
  1. Compare CAT-DIFF(1,5) vs random pruning on a small subset of the English-German dataset
  2. Test different pruning percentages (50%, 70%, 90%) with CAT-DIFF(1,5) on English-French
  3. Evaluate CAT-VAR vs CAT-DIFF on the English-Swahili dataset to understand language-specific performance

## Open Questions the Paper Calls Out
None

## Limitations
- CAT's effectiveness on non-Indo-European languages remains uncertain, with Swahili results suggesting potential language-specific dependencies
- The claim that CAT outperforms embedding-based quality estimators on high-quality datasets needs validation across more language pairs
- The correlation between CAT-selected examples (long sentences, rare words) and model performance could be coincidental rather than causal

## Confidence

- **High confidence**: CAT-DIFF(1,5) achieves 92-99% performance at 50% pruning on en-de, en-fr, en-sw - multiple evaluation metrics and test sets support this
- **Medium confidence**: CAT's superior performance over LaBSE/LASER on high-quality datasets - based on single language pair comparisons that may not generalize
- **Low confidence**: CAT's ability to identify "complex" examples (long sentences, rare words) as more valuable - the correlation between these features and model performance could be coincidental

## Next Checks

1. **Cross-linguistic robustness test**: Apply CAT to language pairs with different typological properties (e.g., Japanese-English, Arabic-English) to verify performance gains aren't language-specific

2. **Ablation study on checkpoint selection**: Systematically vary early checkpoint intervals (e.g., epochs 1-3, 1-4, 2-5) to determine optimal window for perplexity-based ranking

3. **Dataset quality variation**: Test CAT on datasets with controlled levels of noise (e.g., synthetic errors) to quantify its effectiveness at filtering vs. high-quality data where selection is harder