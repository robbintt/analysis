---
ver: rpa2
title: 'Self-play with Execution Feedback: Improving Instruction-following Capabilities
  of Large Language Models'
arxiv_id: '2406.13542'
source_url: https://arxiv.org/abs/2406.13542
tags:
- data
- instructions
- instruction
- response
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AutoIF, the first automated method for generating
  high-quality instruction-following data without manual annotation. The method uses
  code-based verification to ensure instruction adherence, generating verification
  functions and unit tests for each instruction.
---

# Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2406.13542
- Source URL: https://arxiv.org/abs/2406.13542
- Reference count: 29
- One-line result: AutoIF achieves over 90% loose instruction accuracy on IFEval and improves FollowBench SSR metrics by more than 5% through automated instruction-following data generation.

## Executive Summary
This paper introduces AutoIF, the first automated method for generating high-quality instruction-following data without manual annotation. The method uses code-based verification to ensure instruction adherence, generating verification functions and unit tests for each instruction. Through execution feedback-based rejection sampling, AutoIF produces data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) training. Applied to Qwen2 and LLaMA3 models, AutoIF achieved over 90% loose instruction accuracy on IFEval and improved FollowBench SSR metrics by more than 5%, demonstrating its effectiveness in enhancing instruction-following capabilities.

## Method Summary
AutoIF is an automated framework that generates instruction-following data through self-play with execution feedback. The method transforms instruction-following validation into code verification by generating verification functions for each instruction. It employs a three-stage process: instruction augmentation and verification, query augmentation and verification, and training using generated data. The approach uses self-instruct for seed instruction generation, creates verification functions that can be executed to check response quality, and applies back-translation with NLI models to ensure semantic consistency. Through execution feedback-based rejection sampling, AutoIF generates data for both SFT and DPO training, significantly improving instruction-following performance without manual annotation.

## Key Results
- AutoIF achieves over 90% loose instruction accuracy on IFEval benchmark
- Improves FollowBench SSR metrics by more than 5% compared to baseline models
- Maintains or improves performance on general benchmarks (C-Eval, MMLU, GSM8k, HumanEval) while enhancing instruction-following capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AutoIF transforms instruction-following validation into code verification, enabling scalable automated data generation.
- Mechanism: By requiring LLMs to generate both instructions and corresponding verification functions, the method creates a self-validating loop where execution feedback confirms instruction adherence.
- Core assumption: Instructions that can be verified by code constitute a significant and useful subset of all possible instructions.
- Evidence anchors:
  - [abstract]: "transforms the validation of instruction-following data quality into code verification"
  - [section]: "verification function fI such that fI(y) returns true when the model's response y correctly follows the instruction"
  - [corpus]: Weak - corpus shows related work on code-based verification but no direct evidence this approach is effective at scale.
- Break condition: If verification functions become too complex to generate reliably, or if instructions cannot be meaningfully verified through code.

### Mechanism 2
- Claim: Execution feedback-based rejection sampling improves data quality through iterative refinement.
- Mechanism: Responses that pass verification functions are retained while failing responses are discarded, creating a self-improving dataset generation process.
- Core assumption: Models can learn to distinguish between passing and failing responses when given clear feedback.
- Evidence anchors:
  - [abstract]: "execution feedback-based rejection sampling can generate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) training"
  - [section]: "employ LLMs to generate responses that either pass or fail the verification code using execution feedback-based rejection sampling"
  - [corpus]: Moderate - corpus shows rejection sampling is used in related works, but no specific evidence for this particular application.
- Break condition: If the verification function is too permissive or too strict, leading to poor-quality data selection.

### Mechanism 3
- Claim: Back-translation verification ensures consistency between instructions and verification functions.
- Mechanism: Verification functions are translated back into instructions and compared using NLI models to ensure semantic alignment.
- Core assumption: Semantic consistency between original and back-translated instructions indicates reliable verification function generation.
- Evidence anchors:
  - [abstract]: "back-translates to the original instruction is retained"
  - [section]: "treat I as the premise and the back-translated instruction If as the hypothesis. Using the NLI model, we identify the semantic relationship"
  - [corpus]: Weak - corpus shows NLI is used in related works but no direct evidence this specific application works well.
- Break condition: If NLI models cannot reliably detect semantic inconsistencies, or if back-translation introduces errors.

## Foundational Learning

- Concept: Code-based verification functions
  - Why needed here: Forms the foundation of AutoIF's automated quality control system
  - Quick check question: Can you write a simple Python function that verifies whether a response contains exactly 20 words?

- Concept: Rejection sampling with execution feedback
  - Why needed here: Enables iterative improvement of generated data without manual intervention
  - Quick check question: How would you implement a rejection sampling loop that generates responses until one passes a verification function?

- Concept: Back-translation and NLI for semantic consistency
  - Why needed here: Ensures the verification functions accurately represent the original instructions
  - Quick check question: What NLI model would you use to compare semantic similarity between two instructions?

## Architecture Onboarding

- Component map: Seed instructions → Self-Instruct → Verification Function Generation → Back-translation → Query Augmentation → Execution Feedback → SFT/DPO Data
- Critical path: Seed instructions → Self-Instruct → Verification Function Generation → Execution Feedback (data quality depends most on this path)
- Design tradeoffs: Balance between instruction complexity and verifiability vs. coverage of general instruction-following capabilities
- Failure signatures: Poor instruction-following performance despite high data generation volume suggests verification function generation issues
- First 3 experiments:
  1. Generate 10 simple seed instructions and verify they produce working verification functions
  2. Test rejection sampling on a small set of instructions to confirm data quality improvement
  3. Validate back-translation consistency on generated verification functions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AutoIF be extended to handle cross-instructions and compositional instructions that involve combining multiple atomic constraints?
- Basis in paper: Explicit
- Why unresolved: The paper acknowledges that AutoIF focuses on atomic instructions and mentions that combining multiple simple instructions into cross-instructions is a key direction for future research, but does not explore this in the current work.
- What evidence would resolve it: Demonstrating that AutoIF can effectively generate and verify cross-instructions, and showing improved performance on benchmarks that include compositional instructions.

### Open Question 2
- Question: What are the limitations of using code-based verification for instruction-following, and how can AutoIF be adapted to handle instructions that cannot be easily verified by simple code snippets?
- Basis in paper: Inferred
- Why unresolved: The paper focuses on verifiable instructions but acknowledges that many real-world instructions are more complex and unverifiable with simple code. It does not explore methods for handling such instructions.
- What evidence would resolve it: Developing new techniques to generate and verify complex instructions, and demonstrating improved performance on benchmarks that include a wider range of instruction types.

### Open Question 3
- Question: How does the choice of supervision model (e.g., Qwen2-72B vs. GPT-4) impact the quality and diversity of the generated instruction-following data, and what are the trade-offs involved?
- Basis in paper: Explicit
- Why unresolved: The paper shows that GPT-4 generates higher-quality data than Qwen2-72B, but does not explore the reasons for this difference or the potential trade-offs involved in using more powerful models.
- What evidence would resolve it: Analyzing the characteristics of data generated by different supervision models, and conducting experiments to determine the optimal balance between model capability and data quality.

### Open Question 4
- Question: How can AutoIF be used to improve the instruction-following capabilities of language models in specific domains, such as medicine, law, or engineering, where specialized knowledge and constraints are involved?
- Basis in paper: Inferred
- Why unresolved: The paper focuses on general instruction-following capabilities and does not explore domain-specific applications. Adapting AutoIF to handle domain-specific instructions and constraints would require additional research.
- What evidence would resolve it: Developing domain-specific instruction datasets and verification functions, and demonstrating improved performance on benchmarks that assess domain-specific instruction-following.

### Open Question 5
- Question: What are the potential ethical implications of using AutoIF to generate instruction-following data, and how can these risks be mitigated?
- Basis in paper: Explicit
- Why unresolved: The paper acknowledges the potential for malicious prompts to generate harmful outputs and mentions the need for safeguards, but does not provide specific solutions or guidelines.
- What evidence would resolve it: Conducting a thorough analysis of the ethical risks associated with AutoIF, and developing concrete strategies to prevent the generation of harmful or biased data.

## Limitations

- The approach is limited to instructions that can be verified through code, potentially excluding complex or subjective instructions
- The paper uses relatively small models (7B-8B parameters) for fine-tuning, raising questions about scalability to larger models
- Implementation details such as prompt templates and hyperparameters are not fully specified, affecting reproducibility

## Confidence

**High Confidence**: The core mechanism of using code-based verification for automated data generation is well-established and the paper provides clear implementation details for this aspect. The demonstration of improved performance on IFEval and FollowBench provides strong evidence for the effectiveness of AutoIF in these specific benchmarks.

**Medium Confidence**: The paper's claims about the scalability and generalizability of AutoIF to more complex instruction types and larger models are plausible but not fully validated. The paper provides some evidence through ablation studies and comparison with existing methods, but more extensive testing would be needed to confirm these claims.

**Low Confidence**: The long-term impact of AutoIF on real-world instruction-following tasks is uncertain. The paper does not provide extensive testing on more open-ended or complex instruction types, and the reliance on code-based verification may limit the method's applicability to certain domains.

## Next Checks

1. **Verification Function Robustness Test**: Generate a diverse set of 100 instructions spanning different domains and complexity levels. For each instruction, generate a verification function and test whether it correctly identifies passing and failing responses across 10 different response variations. Measure the accuracy and consistency of verification functions.

2. **Scalability Evaluation**: Apply AutoIF to generate data for a larger model (e.g., LLaMA3-70B or Qwen2-72B) and evaluate the performance on a broader set of instruction-following benchmarks, including those that require more open-ended responses. Compare the results with those obtained using smaller models.

3. **Generalizability Assessment**: Test AutoIF on a set of instructions that cannot be easily verified through code, such as those requiring subjective judgment or open-ended responses. Evaluate whether the method can be adapted to handle these types of instructions or if it is fundamentally limited to code-verifiable tasks.