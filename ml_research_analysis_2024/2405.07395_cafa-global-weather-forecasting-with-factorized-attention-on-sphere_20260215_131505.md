---
ver: rpa2
title: 'CaFA: Global Weather Forecasting with Factorized Attention on Sphere'
arxiv_id: '2405.07395'
source_url: https://arxiv.org/abs/2405.07395
tags:
- attention
- weather
- learning
- prediction
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CaFA, a global weather forecasting model
  that uses factorized attention on a sphere to address the computational challenges
  of applying standard Transformers to high-resolution weather data. The proposed
  model employs multi-dimensional factorized kernels that convolve over different
  axes, reducing computational complexity from quadratic in the overall resolution
  to quadratic in the axial resolution.
---

# CaFA: Global Weather Forecasting with Factorized Attention on Sphere

## Quick Facts
- arXiv ID: 2405.07395
- Source URL: https://arxiv.org/abs/2405.07395
- Reference count: 40
- This paper introduces CaFA, a global weather forecasting model that uses factorized attention on a sphere to address the computational challenges of applying standard Transformers to high-resolution weather data.

## Executive Summary
This paper presents CaFA, a novel approach to global weather forecasting that employs factorized attention mechanisms on spherical domains. The key innovation lies in using multi-dimensional factorized kernels that convolve over different axes, reducing computational complexity from quadratic in overall resolution to quadratic in axial resolution. This addresses a fundamental challenge in applying standard Transformers to high-resolution weather data, where quadratic scaling becomes computationally prohibitive.

The proposed model achieves deterministic forecasting accuracy comparable to state-of-the-art purely data-driven MLWP models for 1.5° resolution and 0-7 days lead time. Beyond accuracy, CaFA demonstrates potential to improve the accuracy-efficiency Pareto front for Transformer weather models, achieving better accuracy with less computational cost compared to models with standard attention mechanisms.

## Method Summary
CaFA introduces a factorized attention mechanism specifically designed for spherical weather data. The core innovation involves decomposing the attention computation using multi-dimensional factorized kernels that operate along different axes of the spherical coordinate system. This factorization reduces the computational complexity from O(N²) to O(N), where N represents the number of grid points, by exploiting the structure of spherical data. The model maintains the expressiveness of standard attention while making global weather forecasting computationally tractable at high resolutions.

## Key Results
- Achieves deterministic forecasting accuracy on par with state-of-the-art MLWP models for 1.5° resolution and 0-7 days lead time
- Demonstrates improved accuracy-efficiency Pareto front compared to standard attention Transformers
- Reduces computational complexity from quadratic in overall resolution to quadratic in axial resolution

## Why This Works (Mechanism)
CaFA works by factorizing the attention computation across different dimensions of the spherical coordinate system. Standard attention mechanisms require computing similarity scores between all pairs of positions, resulting in quadratic complexity. CaFA instead decomposes this computation using factorized kernels that operate along latitude and longitude dimensions separately, then combines these results efficiently. This factorization exploits the geometric structure of spherical data while maintaining the ability to capture long-range dependencies essential for weather forecasting.

## Foundational Learning

- **Spherical geometry and coordinate systems**: Understanding how weather data is structured on a sphere is crucial for implementing the factorized attention mechanism correctly.
  - *Why needed*: The model's architecture is specifically designed to exploit the structure of spherical data.
  - *Quick check*: Verify understanding of latitude/longitude grids and how they differ from planar coordinates.

- **Attention mechanisms in Transformers**: Core understanding of how standard attention works and why it scales quadratically.
  - *Why needed*: The paper builds on standard attention concepts but modifies them for efficiency.
  - *Quick check*: Can you explain the difference between standard attention and factorized attention in terms of computational complexity?

- **Factorization techniques in machine learning**: Understanding how to decompose complex operations into simpler, more efficient components.
  - *Why needed*: The key innovation is factorizing attention computation across dimensions.
  - *Quick check*: Can you describe how tensor factorization reduces computational complexity in other contexts?

## Architecture Onboarding

**Component map**: Input data -> Spherical coordinate transformation -> Factorized attention blocks -> Output predictions

**Critical path**: Weather data (temperature, pressure, wind) → Spherical coordinate embedding → Multi-head factorized attention layers → Feed-forward networks → Forecast output

**Design tradeoffs**: 
- Standard attention: High expressiveness but quadratic complexity
- Factorized attention: Reduced complexity but requires careful design to maintain accuracy
- CaFA balances these by factorizing attention across spherical dimensions while preserving essential interaction patterns

**Failure signatures**: 
- Performance degradation at very high resolutions where axial factorization may lose important cross-axis interactions
- Potential loss of fine-grained local features due to factorization
- Accuracy loss for very short-term forecasts where local dynamics dominate

**3 first experiments**:
1. Reproduce accuracy results at 1.5° resolution for 0-7 day forecasts against MLWP baseline
2. Measure computational complexity empirically by benchmarking training/inference time at different resolutions
3. Evaluate performance degradation when using standard (non-factorized) attention as an ablation study

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on deterministic forecasting at a single resolution (1.5°), with limited exploration of probabilistic forecasting capabilities
- Computational efficiency claims are supported by theoretical analysis but lack comprehensive empirical runtime comparisons across different hardware configurations
- Limited validation of model performance at resolutions other than 1.5°, raising questions about generalizability

## Confidence
- Claims about forecasting accuracy compared to MLWP models: High
- Computational complexity reduction claims: Medium (theoretical analysis provided, but practical implementation details could be more comprehensive)
- Generalizability to other resolutions and forecasting horizons: Low (limited experimental validation beyond the 1.5° resolution case)

## Next Checks
1. Evaluate CaFA's performance at multiple spatial resolutions (e.g., 0.25°, 1.0°, 2.0°) to assess scalability and resolution-dependent accuracy patterns
2. Conduct runtime benchmarking on different hardware platforms (CPU, GPU, TPU) with varying batch sizes to validate practical computational efficiency claims
3. Test the model's ability to produce reliable probabilistic forecasts through ensemble generation and uncertainty quantification, comparing against deterministic-only metrics used in the current evaluation