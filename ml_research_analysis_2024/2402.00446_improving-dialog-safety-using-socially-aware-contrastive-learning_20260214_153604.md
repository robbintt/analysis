---
ver: rpa2
title: Improving Dialog Safety using Socially Aware Contrastive Learning
arxiv_id: '2402.00446'
source_url: https://arxiv.org/abs/2402.00446
tags:
- base
- prosocial
- dialog
- dataset
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies prosociality in dialog generation, finding that
  language models often produce unsafe content even after adversarial fine-tuning.
  To address this, it proposes a dual-stage fine-tuning framework using a socially
  aware n-pair contrastive loss, trained first on adversarial data (MIC, ProsocialDialog)
  and then on casual dialog datasets (DailyDialog, PersonaChat, EmpatheticDialogues,
  BlendedSkillTalk).
---

# Improving Dialog Safety using Socially Aware Contrastive Learning

## Quick Facts
- **arXiv ID**: 2402.00446
- **Source URL**: https://arxiv.org/abs/2402.00446
- **Reference count**: 21
- **Primary result**: Dual-stage fine-tuning with socially aware contrastive loss improves dialog safety and prosociality across multiple datasets, outperforming baselines including GPT-4.

## Executive Summary
This paper addresses the persistent issue of unsafe content generation in language models, even after adversarial fine-tuning. The authors propose a dual-stage fine-tuning framework that leverages a socially aware n-pair contrastive loss to improve prosocial dialog generation. The approach combines adversarial datasets (MIC, ProsocialDialog) with casual dialog datasets (DailyDialog, PersonaChat, EmpatheticDialogues, BlendedSkillTalk) to train models that can better distinguish between safe and unsafe responses.

The method introduces a novel re-ranking mechanism that evaluates candidate responses based on both fluency and prosociality, guided by generated rules-of-thumb. Extensive experiments demonstrate significant improvements in generating socially appropriate responses while reducing unsafe content requiring intervention. The approach outperforms strong baselines including Prost and DEXPERTS, with human evaluations confirming superior prosociality and overall quality compared to GPT-4 and COSMO.

## Method Summary
The proposed framework employs dual-stage fine-tuning using a socially aware n-pair contrastive loss. First, the model is trained on adversarial datasets (MIC, ProsocialDialog) to learn safety boundaries. Then, it's fine-tuned on casual dialog datasets (DailyDialog, PersonaChat, EmpatheticDialogues, BlendedSkillTalk) to maintain natural conversation flow. The contrastive loss encourages the model to distinguish between safe and unsafe response pairs, while the re-ranking mechanism evaluates candidates based on both fluency and prosociality metrics. Generated rules-of-thumb provide additional guidance during response selection, creating a comprehensive approach to improving dialog safety.

## Key Results
- Significant improvements in prosocial dialog generation across multiple datasets
- Reductions in unsafe content requiring intervention compared to baseline approaches
- Outperforms established baselines including Prost and DEXPERTS
- Superior performance in human evaluations for both prosociality and overall quality compared to GPT-4 and COSMO

## Why This Works (Mechanism)
The socially aware contrastive learning framework works by explicitly teaching the model to distinguish between safe and unsafe responses through paired examples. The dual-stage fine-tuning approach first establishes strong safety boundaries using adversarial examples, then refines the model's ability to generate natural, prosocial responses in casual conversation contexts. The n-pair contrastive loss creates a more nuanced understanding of safety boundaries by comparing multiple response candidates simultaneously, rather than binary safe/unsafe classification. The re-ranking mechanism ensures that safety considerations are balanced with fluency requirements, preventing over-conservative responses while maintaining prosocial behavior.

## Foundational Learning
- **Adversarial Fine-tuning**: Training models on deliberately challenging examples to improve robustness and safety. Needed to expose models to edge cases and harmful content patterns. Quick check: Evaluate model performance on held-out adversarial examples.
- **Contrastive Learning**: Learning by comparing similar and dissimilar pairs of data. Needed to create nuanced distinctions between safe and unsafe responses. Quick check: Measure margin between safe and unsafe response embeddings.
- **Multi-stage Fine-tuning**: Progressive training on different dataset types. Needed to balance safety learning with natural conversation ability. Quick check: Compare single vs. dual-stage performance on safety metrics.
- **Human Evaluation Protocols**: Systematic assessment of model outputs by human raters. Needed to capture subjective aspects of prosociality and safety. Quick check: Inter-rater reliability scores and consistency across evaluators.
- **Rule-of-thumb Generation**: Creating heuristic guidelines for response selection. Needed to provide interpretable decision criteria for the re-ranking mechanism. Quick check: Rule coverage and effectiveness in improving safety metrics.

## Architecture Onboarding

**Component Map**: Encoder -> Contrastive Loss Layer -> Re-ranking Module -> Output Generator

**Critical Path**: Input dialog context → Encoder representation → Contrastive loss computation → Re-ranking of candidate responses → Final output generation

**Design Tradeoffs**: The dual-stage approach trades increased training complexity for better safety-performance balance. The contrastive loss adds computational overhead but enables more nuanced safety distinctions compared to binary classification approaches.

**Failure Signatures**: Over-conservative responses indicating excessive safety prioritization, generation of generic or non-committal outputs, failure to maintain conversational flow while prioritizing safety, inconsistent safety behavior across different conversation topics.

**First Experiments**:
1. Ablation study removing the contrastive loss to measure its contribution to safety improvements
2. Single-stage vs. dual-stage fine-tuning comparison on safety metrics
3. Analysis of re-ranking effectiveness by comparing top-1 vs. re-ranked outputs

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Robustness to adversarial examples outside training distribution remains uncertain
- Heavy reliance on human judgments introduces potential subjectivity and scale limitations
- Focus on English dialog data limits generalization to other languages and cultural contexts
- Lack of extensive ablation studies to isolate contributions of individual components
- Unproven effectiveness in open-domain, real-world conversational systems with dynamic user inputs

## Confidence

**High confidence**: The core finding that adversarial fine-tuning on prosocial dialog datasets improves safety metrics and outperforms baselines like Prost and DEXPERTS is well-supported by experimental results. The dual-stage fine-tuning framework and n-pair contrastive loss are clearly described and implemented.

**Medium confidence**: The claim of superiority over strong baselines including GPT-4 and COSMO is based on human evaluation, which is inherently subjective. While the results are promising, the evaluation methodology and scale could benefit from further clarification and validation.

**Low confidence**: Generalization claims to other languages, cultures, or dynamic conversational scenarios are not substantiated by the current experimental setup.

## Next Checks
1. Conduct adversarial testing with examples outside the training distribution to assess robustness
2. Perform ablation studies to isolate the contribution of each training stage and the contrastive loss to the observed improvements
3. Evaluate the model's performance on multilingual and multicultural dialog datasets to test generalization claims