---
ver: rpa2
title: 'COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization'
arxiv_id: '2403.07134'
source_url: https://arxiv.org/abs/2403.07134
tags:
- quantization
- comq
- neural
- accuracy
- post-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COMQ introduces a backpropagation-free post-training quantization
  (PTQ) algorithm for neural networks, including vision transformers and convolutional
  models. The method employs coordinate-wise minimization of layer-wise reconstruction
  errors by decomposing quantized weights into a shared floating-point scalar and
  integer bit-codes, updating one parameter at a time using a carefully designed greedy
  order to improve quantization accuracy.
---

# COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization

## Quick Facts
- arXiv ID: 2403.07134
- Source URL: https://arxiv.org/abs/2403.07134
- Authors: Aozhong Zhang; Zi Yang; Naigang Wang; Yingyong Qi; Jack Xin; Xin Li; Penghang Yin
- Reference count: 40
- Primary result: Backpropagation-free PTQ achieving <1% accuracy loss in 4-bit quantization for vision transformers

## Executive Summary
COMQ introduces a novel backpropagation-free post-training quantization algorithm that achieves state-of-the-art performance for neural network weight quantization. The method uses coordinate-wise minimization of layer-wise reconstruction errors through a greedy optimization approach, decomposing quantized weights into shared floating-point scalars and integer bit-codes. By updating parameters sequentially based on reconstruction error gradients, COMQ avoids hyper-parameter tuning while maintaining computational efficiency through simple operations like dot products and rounding.

## Method Summary
COMQ employs a coordinate-wise minimization strategy where quantized weights are decomposed into a shared floating-point scalar and integer bit-codes. The algorithm updates one parameter at a time using a carefully designed greedy order based on reconstruction error gradients. This greedy approach allows the method to avoid backpropagation while still achieving effective quantization. The optimization process relies solely on dot products and rounding operations, making it computationally efficient and avoiding the need for hyper-parameter tuning typically required in other PTQ methods.

## Key Results
- Achieves state-of-the-art 4-bit weight quantization for vision transformers with less than 1% accuracy loss
- Near-lossless accuracy for convolutional networks with only 0.3% drop in performance
- Strong performance in lower bit-widths, achieving 96.7% accuracy for 3-bit and 90.9% for 2-bit quantization on ResNet-18
- Supports both vision transformers and convolutional models without architecture-specific modifications

## Why This Works (Mechanism)
The coordinate-wise greedy optimization approach works by decomposing the quantization problem into simpler subproblems that can be solved efficiently. By focusing on minimizing layer-wise reconstruction errors and using a carefully designed parameter update order, the algorithm can effectively navigate the discrete optimization space without requiring gradient backpropagation. The decomposition into shared scalars and integer bit-codes reduces the complexity of the optimization problem while maintaining quantization quality.

## Foundational Learning
- **Coordinate-wise minimization**: Needed to break down complex quantization into simpler subproblems; quick check: verify convergence properties across different initialization schemes
- **Greedy optimization order**: Required for efficient parameter updates without backpropagation; quick check: analyze sensitivity to different ordering strategies
- **Reconstruction error minimization**: Core objective function driving quantization quality; quick check: compare with alternative objective functions
- **Weight decomposition**: Technique for separating shared parameters from discrete codes; quick check: validate impact on quantization accuracy

## Architecture Onboarding
- **Component map**: Input weights -> Decomposition layer -> Greedy coordinate optimization -> Quantized weights
- **Critical path**: Weight decomposition → Parameter update order determination → Sequential coordinate updates → Reconstruction error evaluation
- **Design tradeoffs**: Greedy optimization trades theoretical guarantees for computational efficiency and simplicity
- **Failure signatures**: Poor initialization may lead to suboptimal local minima; aggressive quantization may cause significant accuracy drops
- **First experiments**: 1) Verify convergence behavior across different random seeds, 2) Test on additional datasets beyond ImageNet-1K, 3) Compare computational efficiency with existing PTQ methods

## Open Questions the Paper Calls Out
None

## Limitations
- Exclusively focuses on weight quantization without addressing activation quantization
- Evaluation limited to ImageNet-1K and small subsets of CIFAR datasets
- No theoretical guarantees for global convergence of the greedy optimization approach
- Does not address computational overhead during the quantization process

## Confidence
- **High Confidence**: Core algorithmic framework and basic implementation details appear sound and reproducible
- **Medium Confidence**: Claims of state-of-the-art performance for 4-bit quantization on vision transformers are supported by experimental results, though limited to a single benchmark
- **Medium Confidence**: Computational efficiency claims based on dot products and rounding operations are plausible but lack comprehensive timing analysis

## Next Checks
1. Verify convergence properties and quantization quality across diverse initialization schemes and random seeds
2. Evaluate performance on additional datasets and real-world deployment scenarios beyond ImageNet-1K
3. Conduct comprehensive computational efficiency analysis including wall-clock time and memory usage during quantization