---
ver: rpa2
title: 'UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer'
arxiv_id: '2401.06426'
source_url: https://arxiv.org/abs/2401.06426
tags:
- pruning
- subnet
- training
- block
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified progressive depth pruning (UPDP)
  method for CNN and vision transformer models. Traditional pruning methods struggle
  with efficient models like MobileNetV2 and ConvNeXtV1 due to their depth-wise convolutions
  and normalization layers.
---

# UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer

## Quick Facts
- arXiv ID: 2401.06426
- Source URL: https://arxiv.org/abs/2401.06426
- Reference count: 19
- Primary result: Unified depth pruning method for CNNs and vision transformers with 1.26× speedup on DeiT-Tiny with only 1.9% accuracy drop

## Executive Summary
This paper introduces a unified progressive depth pruning (UPDP) method that addresses the limitations of traditional pruning approaches on modern CNN and vision transformer architectures. Existing pruning methods struggle with efficient models like MobileNetV2 and ConvNeXtV1 due to their depth-wise convolutions and normalization layers. UPDP overcomes these challenges through a novel block pruning strategy and progressive training method that gradually transfers model structure and weights from baseline to pruned subnet, enabling better utilization of original weights. The approach demonstrates consistent superiority over existing depth pruning methods on ImageNet-1K classification tasks.

## Method Summary
UPDP employs a unified progressive training strategy that bridges the gap between traditional progressive pruning and direct pruning methods. The key innovation lies in its gradual weight and structure transfer mechanism, where the model structure and weights are progressively shifted from the baseline model to the pruned subnetwork during training. This approach effectively handles depth-wise convolutions and various normalization layers (BatchNorm, LayerNorm, GroupNorm) that typically pose challenges for conventional pruning methods. The method uses a block pruning strategy that groups layers into pruning units, allowing for more coherent structure reduction while maintaining performance. For CNNs, UPDP achieves better results than existing methods on architectures like MobileNetV2 and ConvNeXtV1, while for vision transformers, it demonstrates state-of-the-art pruning results with significant speedup and minimal accuracy loss.

## Key Results
- Pruned ConvNeXtV1 models outperform most state-of-the-art efficient models while maintaining comparable inference speed
- Achieves 1.26× speedup on DeiT-Tiny with only 1.9% accuracy drop on ImageNet-1K
- Consistently outperforms existing depth pruning methods across multiple CNN and transformer architectures
- Successfully handles various normalization layers including BatchNorm, LayerNorm, and GroupNorm

## Why This Works (Mechanism)
UPDP's effectiveness stems from its progressive weight and structure transfer mechanism. Unlike direct pruning that abruptly removes layers, UPDP gradually shifts the model's representation from the baseline to the pruned subnetwork. This progressive approach allows the model to adapt its weights incrementally, preventing the catastrophic performance drop typically seen in direct pruning methods. The block pruning strategy groups related layers together, maintaining structural coherence during pruning. The method's ability to handle depth-wise convolutions and various normalization layers makes it particularly effective for modern efficient architectures. By allowing the pruned subnetwork to gradually inherit and refine the baseline model's knowledge, UPDP achieves superior performance compared to methods that force immediate adaptation to a drastically reduced architecture.

## Foundational Learning
**Depth-wise Convolutions** - Specialized convolution operation that applies a single filter per input channel, commonly used in efficient CNN architectures. *Why needed*: Traditional pruning methods struggle with depth-wise convolutions due to their unique computational patterns and gradient flow. *Quick check*: Verify the model contains depth-wise separable convolutions in the architecture diagram.

**Normalization Layers** - Components like BatchNorm, LayerNorm, and GroupNorm that stabilize training by normalizing activations. *Why needed*: Different normalization types require distinct handling during pruning to maintain training stability. *Quick check*: Confirm which normalization layers are present in the baseline architecture.

**Progressive Training** - Training approach where model parameters are gradually updated over multiple stages rather than in a single step. *Why needed*: Enables smooth transition from full model to pruned subnetwork, reducing catastrophic forgetting. *Quick check*: Verify the training schedule includes gradual weight transfer phases.

**Block Pruning Strategy** - Approach that groups multiple layers into pruning units rather than pruning individual layers. *Why needed*: Maintains structural coherence and prevents isolated layer removal that can disrupt feature flow. *Quick check*: Examine how layers are grouped into pruning blocks in the methodology.

## Architecture Onboarding
**Component Map**: Input -> Feature Extraction Blocks -> Pooling -> Classification Head
**Critical Path**: The progressive training mechanism where weights flow from baseline to pruned subnetwork through gradual adaptation phases
**Design Tradeoffs**: Balances pruning aggressiveness with performance retention through progressive transfer vs. direct pruning approaches
**Failure Signatures**: Performance collapse when progressive transfer is too aggressive; suboptimal speedup when transfer is too conservative
**First Experiments**: 1) Baseline performance on full model, 2) Progressive pruning on ConvNeXtV1 with varying block sizes, 3) Speed-accuracy tradeoff analysis on DeiT-Tiny

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Focuses only on depth pruning without addressing width/channel pruning, limiting applicability to certain efficiency scenarios
- Experimental validation restricted to image classification on ImageNet-1K, with no testing on downstream tasks like detection or segmentation
- Progressive training approach may introduce additional computational overhead during training, though not explicitly quantified

## Confidence
- **High Confidence**: Method's effectiveness on CNNs and transformers for image classification (based on comprehensive ImageNet-1K experiments)
- **Medium Confidence**: Claims about superiority over existing methods (given controlled experimental setup but potential for implementation differences)
- **Medium Confidence**: Generalizability to other vision tasks (limited empirical evidence)

## Next Checks
1. Evaluate UPDP on downstream tasks including object detection and semantic segmentation to verify performance retention
2. Test the method on extremely deep architectures (500+ layers) to assess scalability limits
3. Compare training overhead of the progressive method against standard pruning approaches in terms of total FLOPs and wall-clock time