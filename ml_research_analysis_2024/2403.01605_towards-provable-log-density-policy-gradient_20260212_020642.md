---
ver: rpa2
title: Towards Provable Log Density Policy Gradient
arxiv_id: '2403.01605'
source_url: https://arxiv.org/abs/2403.01605
tags:
- gradient
- policy
- density
- equation
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of residual error in policy
  gradient estimation, which can lead to suboptimal solutions in reinforcement learning.
  The authors propose a novel approach called log density gradient, which estimates
  policy gradients using the state-action discounted distributional formulation.
---

# Towards Provable Log Density Policy Gradient

## Quick Facts
- arXiv ID: 2403.01605
- Source URL: https://arxiv.org/abs/2403.01605
- Authors: Pulkit Katdare; Anant Joshi; Katherine Driggs-Campbell
- Reference count: 40
- Primary result: Proposed log density gradient method corrects residual error in policy gradient estimation, achieving O(m^(-1/2)) sample complexity

## Executive Summary
This paper addresses a fundamental limitation in policy gradient methods: the residual error introduced when estimating gradients from samples. The authors propose a novel approach using log density gradients that corrects this error through a state-action discounted distributional formulation. The method provides provable convergence guarantees and improved sample efficiency compared to classical approaches like REINForce, as demonstrated on gridworld environments.

## Method Summary
The method estimates policy gradients using the log density of the discounted stationary distribution, correcting for the residual term commonly ignored in actor-critic implementations. Three estimation approaches are provided: a model-based method for exact calculation in tabular MDPs, a temporal difference method under reversibility assumptions, and a min-max optimization approach that uses only on-policy samples with O(m^(-1/2)) sample complexity. The min-max formulation transforms the problem into a Fenchel-dual form, enabling function approximation via linear or neural network methods.

## Key Results
- Proved residual error in classical policy gradient methods is significant and can be corrected
- Established O(m^(-1/2)) sample complexity for the min-max optimization approach
- Demonstrated improved sample efficiency over REINFORCE and theoretical policy gradient on 3×3 and 5×5 gridworld environments
- Provided provable convergence and uniqueness guarantees under linear function approximation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Log density gradient corrects residual error ignored by classical policy gradient methods
- Mechanism: By estimating the policy gradient through the log density of the discounted stationary distribution, the method inherently accounts for the term (1-γ)E[∇logd(s)·Vγ(s)] that is omitted in classical actor-critic implementations
- Core assumption: The ergodicity assumption holds for the Markov chain induced by the policy
- Evidence anchors:
  - [abstract] "Modern policy gradient methods, although successful, introduce a residual error in gradient estimation"
  - [section 2] "We argue that this residual term is significant and correcting for it could potentially improve sample-complexity"
  - [corpus] Weak - no direct supporting evidence found in neighbor papers
- Break condition: If the Markov chain is non-ergodic, the state-action distribution may have zero entries, making log density undefined

### Mechanism 2
- Claim: Temporal difference method estimates log density gradient without requiring backward samples
- Mechanism: Uses TD(0) update rule that iteratively contracts to the true log density gradient through operator Yγ, which is a contraction for γ ∈ [0,1)
- Core assumption: Feature matrix Ψ has linearly independent rows, and A is non-singular or regularizer λ > 0
- Evidence anchors:
  - [section 4.2] "We first generalize algorithm of Morimura et al. (2010), who do it only for γ=1, to estimate log density gradient for all discounting factor γ∈[0,1]"
  - [section 8.7] "Under Assumption 2 and the fact that the learning rate satisfies the Robbins Monroe condition then the TD update equation 20 converges in probability"
  - [corpus] Weak - no direct supporting evidence found in neighbor papers
- Break condition: If features are linearly dependent or A is singular with λ=0, convergence may fail

### Mechanism 3
- Claim: Min-max optimization estimates log density gradient using only on-policy samples with O(m^(-1/2)) sample complexity
- Mechanism: Transforms the log density estimation problem into a Fenchel-dual min-max form, allowing function approximation via linear or neural network methods, and converges under linear function approximation
- Core assumption: The matrix A = Ψ(I-γPπ)DπΨ^T is non-singular or λ > 0
- Evidence anchors:
  - [section 5] "We propose a min-max optimization that estimates log density gradient using just on-policy samples"
  - [section 8.9] "The proof of this Theorem is almost similar to the proof of Zhang et al. (2020b)"
  - [corpus] Moderate - neighbor paper "Policy Gradient Methods for Risk-Sensitive Distributional Reinforcement Learning with Provable Convergence" shares similar convergence analysis framework
- Break condition: If feature matrix Ψ has linearly dependent columns, the optimization may not have a unique solution

## Foundational Learning

- Concept: Ergodic Markov chains
  - Why needed here: The method assumes the state-action distribution is positive for all pairs, which only holds for ergodic chains
  - Quick check question: What happens to the log density gradient method if the Markov chain is periodic?

- Concept: Bellman flow equation
  - Why needed here: The log density gradient estimation relies on the flow conservation property expressed by this equation
  - Quick check question: How does the Bellman flow equation for the state-action distribution differ from the Bellman equation for value functions?

- Concept: Fenchel duality
  - Why needed here: The min-max formulation of log density gradient estimation uses this duality to transform the problem
  - Quick check question: In what way does Fenchel duality enable the transformation from a regularized loss to a min-max optimization?

## Architecture Onboarding

- Component map: Policy parameterization (πθ) -> Feature extraction (Φ) -> Min-max optimization solver -> TD estimator -> Environment interaction

- Critical path:
  1. Initialize policy parameters θ and optimization variables
  2. Collect on-policy samples from environment
  3. Extract features Φ(st, at) for each sample
  4. Update min-max optimization variables using projected gradients
  5. Compute log density gradient estimate
  6. Update policy parameters using policy gradient theorem

- Design tradeoffs:
  - Linear vs. non-linear function approximation: Linear offers theoretical guarantees but may underfit; non-linear can better approximate complex log densities but loses convergence proofs
  - Projection set size: Larger sets allow better approximation but increase computational cost and may require more samples
  - Learning rate scheduling: Constant rates are simpler but adaptive rates may improve convergence

- Failure signatures:
  - Divergence in min-max updates: Likely indicates learning rate too high or feature matrix ill-conditioned
  - Poor performance vs. REINFORCE: May indicate insufficient samples or poor feature representation
  - Numerical instability in log calculations: Likely indicates zero entries in state-action distribution

- First 3 experiments:
  1. 3x3 gridworld with linear features: Verify method converges and outperforms REINFORCE on simple task
  2. 5x5 gridworld with varying λ values: Test sensitivity to regularization parameter
  3. Gridworld with non-linear features (small neural network): Assess benefit of non-linear approximation while monitoring for instability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed log density gradient method scale to high-dimensional state-action spaces and complex environments beyond gridworld?
- Basis in paper: [inferred] The paper acknowledges scalability challenges for large state-action spaces and proposes linear function approximation as a solution, but does not provide empirical results beyond simple gridworld environments.
- Why unresolved: The experiments are limited to small gridworld environments (3x3 and 5x5), and the paper does not demonstrate performance on more complex benchmarks like Atari or continuous control tasks.
- What evidence would resolve it: Empirical evaluation on standard RL benchmarks with high-dimensional state spaces and comparison against state-of-the-art policy gradient methods.

### Open Question 2
- Question: What is the impact of the reversibility assumption on the TD method's performance in non-reversible Markov chains?
- Basis in paper: [explicit] The paper explicitly states that the TD method requires reversibility assumptions and proposes the min-max optimization to overcome this limitation.
- Why unresolved: The paper does not quantify the performance degradation of the TD method when the reversibility assumption is violated or provide empirical comparisons between the TD and min-max approaches.
- What evidence would resolve it: Experiments comparing the TD and min-max methods on environments with varying degrees of reversibility, including non-reversible cases.

### Open Question 3
- Question: How sensitive is the min-max optimization approach to the choice of function approximation architecture and regularization parameters?
- Basis in paper: [inferred] The paper proposes using linear function approximation and a regularization term λ, but does not provide sensitivity analysis or comparisons with other architectures like neural networks.
- Why unresolved: The paper does not explore the impact of different function classes or regularization strengths on the method's performance and convergence properties.
- What evidence would resolve it: Systematic experiments varying the function approximation architecture (e.g., neural networks, kernels) and regularization parameter λ, and analysis of their impact on performance and sample complexity.

## Limitations

- Strong ergodicity assumptions limit applicability to non-ergodic environments
- Theoretical guarantees rely on linear function approximation, with unknown behavior for non-linear approximators
- Limited experimental validation beyond simple gridworld environments
- Missing ablation studies to isolate the contribution of log density correction versus other algorithmic choices

## Confidence

- Theoretical convergence guarantees: High
- Residual error significance: Medium
- Sample complexity bound: Medium
- Practical performance gains: Medium

## Next Checks

1. Test the method on a gridworld with known periodic structure to verify behavior when ergodicity assumption is violated
2. Implement ablation study comparing log density gradient with and without the residual correction term to isolate its contribution
3. Evaluate the method with neural network function approximation to assess gap between theory and practice