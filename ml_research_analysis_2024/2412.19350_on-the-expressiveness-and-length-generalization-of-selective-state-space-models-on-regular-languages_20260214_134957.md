---
ver: rpa2
title: On the Expressiveness and Length Generalization of Selective State-Space Models
  on Regular Languages
arxiv_id: '2412.19350'
source_url: https://arxiv.org/abs/2412.19350
tags:
- length
- state
- selective
- ssms
- diagonal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Selective state-space models (SSMs) have shown promise in sequence
  modeling but lack formal analysis of their expressiveness and length generalization,
  particularly on regular language tasks such as finite-state automaton (FSA) emulation.
  This paper addresses this gap by introducing the Selective Dense State-Space Model
  (SD-SSM), which uses a dictionary of dense transition matrices, softmax-based selection,
  and a linear readout to achieve perfect length generalization (99.9% accuracy) on
  diverse FSA tasks with a single layer.
---

# On the Expressiveness and Length Generalization of Selective State-Space Models on Regular Languages

## Quick Facts
- **arXiv ID**: 2412.19350
- **Source URL**: https://arxiv.org/abs/2412.19350
- **Reference count**: 15
- **Primary result**: Selective state-space models achieve perfect length generalization (>99.9% accuracy) on finite-state automaton tasks using a novel SD-SSM architecture with dense transition matrices and linear readout.

## Executive Summary
This paper analyzes the expressiveness and length generalization capabilities of selective state-space models (SSMs) on regular language tasks, particularly finite-state automaton (FSA) emulation. The authors identify that standard diagonal selective SSMs are theoretically limited to commutative automata, explaining their poor performance on non-commutative tasks. To address this limitation, they introduce the Selective Dense State-Space Model (SD-SSM), which uses a dictionary of dense transition matrices with softmax-based selection and a linear readout. SD-SSM achieves perfect length generalization on diverse FSA tasks, outperforming standard SSM variants and recurrent models in both accuracy and length efficiency.

## Method Summary
The study evaluates sequence models on FSA emulation tasks, training on sequences up to length 40 and testing on sequences up to length 500. The primary contribution is SD-SSM, which maintains a dictionary of k dense transition matrices, applies softmax-based selection to create convex combinations at each time step, and uses operator normalization for stability. The readout consists of layer normalization followed by a linear map. The paper also provides theoretical analysis showing that single-layer diagonal selective SSMs are restricted to commutative automata due to simultaneous diagonalizability constraints.

## Key Results
- SD-SSM achieves >99.9% accuracy on all seven FSA tasks with perfect length generalization
- Diagonal selective SSMs are theoretically limited to commutative automata, explaining poor performance on non-commutative tasks
- Nonlinear readouts in SD-SSM significantly degrade length generalization performance compared to linear readouts
- SD-SSM outperforms Mamba, S4D, and recurrent models on length generalization metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SD-SSM achieves perfect length generalization on FSA emulation by using a dictionary of dense transition matrices with softmax-based selection.
- Mechanism: At each time step, the input is transformed through a linear layer and softmax to produce weights over a fixed dictionary of dense transition matrices. These weights form a convex combination of the matrices, which is then normalized via operator normalization to stabilize eigenvalues. This combined matrix is used in the recurrence. The readout uses layer normalization followed by a linear map, avoiding the detrimental effects of nonlinear readouts observed in other architectures.
- Core assumption: The convex combination of dense matrices allows sufficient expressiveness to model arbitrary FSA dynamics, while operator normalization maintains numerical stability across long sequences.
- Evidence anchors:
  - [abstract] "It utilizes a dictionary of dense transition matrices, a softmax selection mechanism that creates a convex combination of dictionary matrices at each time step, and a readout consisting of layer normalization followed by a linear map."
  - [section] "In the first phase, the transition matrices A(ut) are generated for each input ut in parallel... The outputs of the softmax are used to weigh a dictionary of k-many trainable dense transition matrices labeled A1, ..., Ak."
  - [corpus] Weak - no direct corpus evidence for this specific mechanism.

### Mechanism 2
- Claim: Diagonal selective SSMs are limited to commutative automata due to the simultaneous diagonalizability constraint.
- Mechanism: When transition matrices are simultaneously diagonalizable, they can be simultaneously transformed into diagonal form via a single matrix W. This transformation commutes, meaning the order of matrix applications is irrelevant. Consequently, the FSA dynamics must be commutative. If the FSA is non-commutative, the model cannot capture the state transitions correctly.
- Core assumption: The mapping from FSA to selective SSM sets x0 = enc(qinit), b(ut) = 0, and requires the transition matrices to be simultaneously diagonalizable.
- Evidence anchors:
  - [abstract] "We explain the experimental results with theoretical considerations... single-layer selective diagonal SSMs are restricted to emulating commutative automata."
  - [section] "Under the described mapping of FSA to a single-layer selective SSMs which sets x0 = enc(qinit), b(ut) = 0 , and whose transition matrices are simultaneously diagonalizable, the selective SSM can only emulate commutative automata."
  - [corpus] Weak - no direct corpus evidence for this specific theoretical limitation.

### Mechanism 3
- Claim: The presence of a nonlinear readout in SD-SSM degrades length generalization performance.
- Mechanism: When the linear readout in SD-SSM is replaced with a two-layer MLP with ReLU activation, the model's ability to generalize to longer sequences than seen during training is significantly reduced. This suggests that the added nonlinearity interferes with the model's ability to maintain a clean state representation across long sequences.
- Core assumption: The layer normalization followed by a linear map readout preserves the state information in a way that is conducive to length generalization.
- Evidence anchors:
  - [abstract] "We identify that a common design choice, the presence of a nonlinear readout, prevents SD-SSM from achieving full accuracy on a challenging FSA emulation task."
  - [section] "On the Arithmetic task, we conducted extensive experiments in which the linear layer in SD-SSM’s readout was replaced by a standard two-layer MLP with the ReLU non-linearity... The best accuracy was achieved by using weight decay. The model achieves an accuracy of 71.9%, significantly below 99.9% achieved by the linear readout SD-SSM."
  - [corpus] Weak - no direct corpus evidence for this specific observation.

## Foundational Learning

- **Concept**: Finite-State Automata (FSA)
  - Why needed here: The paper evaluates model performance on FSA emulation tasks, so understanding FSAs is fundamental to grasping the problem setup and results.
  - Quick check question: What is the difference between a deterministic FSA and a semi-automaton?

- **Concept**: State-Space Models (SSM)
  - Why needed here: SSMs are the core model family being analyzed, and understanding their basic structure (A, B, C, D matrices) is crucial for following the discussion of diagonal vs. dense variants.
  - Quick check question: How does a diagonal SSM differ from a dense SSM in terms of computational efficiency?

- **Concept**: Simultaneous Diagonalizability
  - Why needed here: This property is central to the theoretical analysis of why diagonal selective SSMs are limited to commutative automata.
  - Quick check question: What does it mean for a set of matrices to be simultaneously diagonalizable, and why is this a restrictive condition?

## Architecture Onboarding

- **Component map**: Input embedding layer -> Linear layer (S) -> Softmax layer -> Dictionary of dense transition matrices -> Operator normalization -> Linear recurrence -> Layer normalization -> Linear readout

- **Critical path**:
  1. Input ut → Linear layer (S) → Softmax → Weights
  2. Weights + Dictionary matrices → Convex combination → OpNorm → A(ut)
  3. A(ut) + state xt → Recurrence → New state xt+1
  4. New state xt+1 → LayerNorm → Linear layer → Output yt

- **Design tradeoffs**:
  - Dictionary size (k): Larger k increases expressiveness but also parameter count and computational cost.
  - Operator normalization: Ensures stability but may limit the range of representable dynamics.
  - Readout design: Linear readout preserves state information for length generalization; nonlinear readouts can hurt performance.

- **Failure signatures**:
  - Poor in-domain accuracy: Likely due to insufficient expressiveness (k too small) or learning instability.
  - Good in-domain accuracy but poor length generalization: May indicate that the model memorizes short sequences but fails to capture the underlying FSA dynamics.
  - Numerical instability: Could be caused by eigenvalues of transition matrices growing too large; check operator normalization.

- **First 3 experiments**:
  1. Train SD-SSM on the Parity FSA task with k=5 transition matrices and varying learning rates to find the optimal setting.
  2. Replace the linear readout in SD-SSM with a two-layer MLP (as described in the paper) and evaluate the impact on length generalization on the Arithmetic task.
  3. Train a diagonal selective SSM (as described in the paper) on the C2 × C30 task and evaluate its performance compared to SD-SSM.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SD-SSM model perform on tasks involving context-free languages compared to regular languages?
- Basis in paper: [inferred] The paper focuses on regular languages and finite-state automaton (FSA) emulation, achieving perfect length generalization with SD-SSM on these tasks.
- Why unresolved: The paper does not explore tasks beyond regular languages, leaving the model's effectiveness on more complex language tasks unexamined.
- What evidence would resolve it: Experimental results showing SD-SSM's performance on context-free language tasks, such as balanced parentheses or nested structures, compared to existing models.

### Open Question 2
- Question: Can the SD-SSM model's length generalization capabilities be maintained when scaling to larger state sizes and more complex automata?
- Basis in paper: [explicit] The paper demonstrates perfect length generalization with a state size of 64 on FSA tasks but does not explore scaling effects.
- Why unresolved: The study does not investigate how increasing the state size or complexity of automata affects the model's generalization performance.
- What evidence would resolve it: Experimental results evaluating SD-SSM's performance with varying state sizes and more complex automata, such as those with hundreds of states.

### Open Question 3
- Question: What is the impact of using different activation functions in the SD-SSM model's softmax selection mechanism on its performance and interpretability?
- Basis in paper: [inferred] The paper uses softmax for transition matrix selection but does not explore alternative activation functions or their effects on model interpretability.
- Why unresolved: The choice of softmax is not experimentally justified, and the potential benefits of other activation functions remain unexplored.
- What evidence would resolve it: Comparative experiments using different activation functions (e.g., sigmoid, tanh) in the selection mechanism, measuring both performance and interpretability metrics.

## Limitations

- The theoretical analysis of diagonal selective SSMs relies on specific FSA-to-SSM mapping assumptions that may not capture all implementation strategies
- Operator normalization mechanism lacks theoretical guarantees about its impact on the representable function class
- The study focuses exclusively on regular languages, leaving performance on context-free and more complex languages unexamined

## Confidence

- **High confidence**: The experimental observation that SD-SSM achieves near-perfect length generalization on FSA tasks with linear readout. The empirical results are clearly demonstrated across multiple tasks with consistent performance metrics.
- **Medium confidence**: The theoretical claim that diagonal selective SSMs are restricted to commutative automata. While the mathematical reasoning is sound, it depends on specific architectural assumptions that may not hold in all implementations.
- **Medium confidence**: The claim that nonlinear readouts prevent length generalization in SD-SSM. The empirical evidence is strong for the specific tasks tested, but the underlying mechanism requires further investigation to determine if it generalizes across different problem domains.

## Next Checks

1. **Theoretical validation**: Derive explicit bounds on the expressive power of SD-SSM as a function of dictionary size k and operator normalization strength, and compare these bounds to the requirements for emulating specific FSA classes.

2. **Architectural ablation**: Systematically vary the operator normalization (different lp-norms, scaling strategies) and readout architectures (residual connections, gated readouts) to identify which components are essential for length generalization versus in-domain performance.

3. **Generalization scope**: Test SD-SSM on non-regular language tasks (context-free grammars, hierarchical structures) to determine whether the length generalization capability extends beyond finite-state patterns, and analyze failure cases to identify fundamental limitations.