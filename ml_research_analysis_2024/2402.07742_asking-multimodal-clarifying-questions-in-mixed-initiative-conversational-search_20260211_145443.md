---
ver: rpa2
title: Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search
arxiv_id: '2402.07742'
source_url: https://arxiv.org/abs/2402.07742
tags:
- questions
- retrieval
- multimodal
- clarifying
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal query clarification (MQC) task
  to improve conversational search by adding relevant images to clarifying questions.
  The authors collect a new dataset Melon containing 4k multimodal clarifying questions
  and 14k images, and propose a model Marto based on the VLT5 architecture.
---

# Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search

## Quick Facts
- **arXiv ID**: 2402.07742
- **Source URL**: https://arxiv.org/abs/2402.07742
- **Reference count**: 40
- **Key outcome**: Proposes MQC task with Melon dataset (4k questions, 14k images) and Marto model, achieving up to 90% improvement in retrieval performance

## Executive Summary
This paper addresses the challenge of query ambiguity in conversational search by introducing multimodal query clarification (MQC). The authors propose enhancing clarifying questions with relevant images to help users disambiguate their information needs more effectively. They present a novel dataset called Melon containing 4,000 multimodal clarifying questions paired with 14,000 relevant images, along with a comprehensive annotation framework. The Marto model, built on the VLT5 architecture, uses a multi-task fine-tuning strategy to simultaneously perform question classification, image selection, and document retrieval. Experimental results demonstrate that incorporating images into clarifying questions significantly improves retrieval performance by up to 90% compared to text-only approaches.

## Method Summary
The authors tackle the problem of query ambiguity in conversational search through multimodal clarification. They first collect the Melon dataset, which includes real user queries, generated clarifying questions, and manually annotated relevant images. The Marto model employs the VLT5 architecture and is trained using a multi-task fine-tuning approach that simultaneously learns to classify the type of clarification needed, select appropriate images, and retrieve relevant documents. The model processes both text and image inputs to generate more effective clarifying questions. The experimental setup compares Marto against unimodal baselines and other multimodal approaches across various retrieval metrics, demonstrating significant improvements when images are incorporated into the clarification process.

## Key Results
- Marto outperforms both unimodal and multimodal baselines in retrieval effectiveness
- Adding images to clarifying questions improves retrieval performance by up to 90%
- The multi-task fine-tuning strategy enables efficient learning across all three tasks
- The Melon dataset provides a comprehensive resource for future research in multimodal conversational search

## Why This Works (Mechanism)
The multimodal approach works because visual information can convey contextual cues that text alone may miss, particularly for ambiguous queries. Images in clarifying questions provide users with concrete examples that help them refine their information needs more precisely. The VLT5 architecture's ability to process both text and images in a unified framework allows the model to learn meaningful representations that capture the relationship between visual and textual information. The multi-task fine-tuning strategy is effective because the three tasks (question classification, image selection, and document retrieval) are inherently related and benefit from shared representations. This approach mimics how humans naturally use both visual and textual information when seeking clarification, making the system more intuitive and effective for users.

## Foundational Learning

**Multimodal Learning**: Why needed: To process and integrate information from multiple modalities (text and images) effectively; Quick check: Verify that the model can handle different input combinations and produce coherent outputs.

**Query Clarification**: Why needed: To address ambiguity in user queries and improve search accuracy; Quick check: Ensure the clarification questions are relevant and helpful for disambiguation.

**Conversational Search**: Why needed: To understand the context and history of user interactions in a conversational setting; Quick check: Validate that the system maintains context across multiple turns.

**VLT5 Architecture**: Why needed: To provide a unified framework for processing both text and images; Quick check: Confirm that the architecture can handle the specific requirements of the MQC task.

**Multi-task Learning**: Why needed: To leverage shared representations across related tasks and improve overall performance; Quick check: Verify that the multi-task approach outperforms single-task training.

## Architecture Onboarding

**Component Map**: User Query -> VLT5 Encoder (Text + Image) -> Multi-task Head (Classification + Image Selection + Retrieval) -> Output

**Critical Path**: The critical path involves encoding the user query along with candidate images through the VLT5 architecture, then using the multi-task head to simultaneously determine the clarification type, select relevant images, and retrieve documents. This integrated approach ensures that all components work together to produce the most effective clarifying question.

**Design Tradeoffs**: The primary tradeoff is between model complexity and performance - using VLT5 allows for rich multimodal representations but increases computational requirements. The multi-task approach reduces training time compared to separate models but may introduce interference between tasks. The dataset collection required significant manual effort but ensures high-quality annotations. The choice of retrieval metrics focuses on effectiveness but doesn't fully capture user satisfaction with the clarifying questions.

**Failure Signatures**: Potential failures include selecting irrelevant or confusing images that don't help disambiguate the query, generating clarifying questions that don't match the user's actual information need, or failing to maintain conversational context across turns. The model may also struggle with queries that have complex or subtle ambiguities that require deep domain knowledge.

**3 First Experiments**:
1. Test the model's ability to classify clarification types accurately on a held-out validation set
2. Evaluate image selection quality by measuring the relevance of selected images to the query
3. Measure retrieval performance improvements when using different numbers of clarifying images

## Open Questions the Paper Calls Out
None

## Limitations
- The claimed 90% improvement figure needs clarification regarding whether it represents relative or absolute gains
- The comparison primarily focuses on retrieval metrics without thoroughly examining the quality and relevance of selected images
- The approach doesn't address potential biases in the Melon dataset or generalization to other domains
- Computational efficiency claims lack sufficient validation for real-time deployment scenarios

## Confidence

**High Confidence**: The dataset collection methodology and basic experimental setup are sound and well-documented.

**Medium Confidence**: The retrieval performance improvements are demonstrated but could benefit from additional validation across different domains and query types.

**Low Confidence**: The claimed 90% improvement figure needs clarification regarding whether it represents relative or absolute gains, and the computational efficiency claims lack sufficient validation.

## Next Checks

1. Conduct ablation studies to isolate the specific contribution of image features versus text-based features in the Marto model's performance.

2. Test the model's generalization by evaluating its performance on datasets from different domains or languages not represented in the original Melon dataset.

3. Perform a user study to assess whether the selected images actually improve user comprehension and search satisfaction, beyond the measured retrieval metrics.