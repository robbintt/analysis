---
ver: rpa2
title: 'EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models'
arxiv_id: '2402.10866'
source_url: https://arxiv.org/abs/2402.10866
tags:
- passages
- budget
- llms
- arxiv
- ecorank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EcoRank addresses budget-constrained text re-ranking using large
  language models (LLMs). The method optimizes prompt choices, LLM API selection,
  and budget allocation across two stages: initial pointwise filtering with a high-accuracy,
  expensive LLM followed by pairwise re-ranking with a cheaper LLM.'
---

# EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models

## Quick Facts
- arXiv ID: 2402.10866
- Source URL: https://arxiv.org/abs/2402.10866
- Authors: Muhammad Shihab Rashid; Jannat Ara Meem; Yue Dong; Vagelis Hristidis
- Reference count: 26
- Key outcome: EcoRank achieves 14% improvement in MRR and R@1 ranking accuracy compared to baselines while operating within budget constraints.

## Executive Summary
EcoRank addresses the challenge of text re-ranking using large language models (LLMs) under strict budget constraints. The method employs a two-stage pipeline that first filters irrelevant passages using an expensive, high-accuracy LLM, then performs fine-grained pairwise comparisons using a cheaper LLM on the filtered set. By optimizing prompt choices, LLM API selection, and budget allocation across these stages, EcoRank balances ranking quality with cost efficiency. Experimental results on four QA and passage reranking datasets demonstrate significant performance improvements over existing baselines while adhering to predefined budget limits.

## Method Summary
EcoRank is a budget-constrained LLM-based text re-ranking pipeline that jointly optimizes prompt designs, LLM API selection, and budget allocation. The method uses a two-stage approach: Stage 1 employs a high-accuracy LLM with pointwise classification to filter irrelevant passages, while Stage 2 uses a cheaper LLM with pairwise comparisons to rank the filtered relevant passages. The budget is split equally between the two stages, with each stage using specialized prompts optimized for their respective tasks. This architecture allows for efficient use of the budget by focusing expensive comparisons only on passages likely to be relevant, achieving superior ranking performance within cost constraints.

## Key Results
- EcoRank achieves 14% improvement in MRR and R@1 ranking accuracy compared to baselines
- Two-stage pipeline with equal budget split (x=0.5, y=0.5) provides optimal trade-off
- Performance remains stable across three budget categories (B1, B2, B3)
- Outperforms monoT5, monoBERT, and InPars baselines on NQ, WQ, TREC DL19, and TREC DL20 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EcoRank improves ranking accuracy by first filtering irrelevant passages with an expensive, high-accuracy LLM before applying a cheaper LLM for fine-grained pairwise comparisons.
- Mechanism: Stage 1 uses a high-accuracy LLM to classify passages as "Yes" or "No" for coarse relevance filtering. Stage 2 applies a cheaper LLM to perform pairwise comparisons only on the filtered relevant passages, concentrating the budget on valuable content.
- Core assumption: High-accuracy LLMs excel at coarse relevance classification, and pairwise comparisons on smaller relevant sets yield better rankings than applying pairwise comparisons to all passages.
- Evidence anchors: [abstract] "Our most efficient method, called EcoRank, is a two-layered pipeline that jointly optimizes decisions regarding budget allocation across prompt strategies and LLM APIs." [section] "First stage. Only the first few passages can be compared by pairwise with a limited budget... Thus in the first stage, we intelligently pick the passages to do pairwise comparison on."
- Break condition: If initial LLM fails to accurately filter irrelevant passages, Stage 2 wastes budget on irrelevant content, reducing overall performance.

### Mechanism 2
- Claim: Using a hybrid of pointwise and pairwise prompting strategies within a fixed budget optimizes both quality and quantity of ranking decisions.
- Mechanism: Pointwise methods (binary/Likert classification) use fewer tokens per passage for coarse relevance evaluation, allowing more passages to be processed. Pairwise methods provide fine-grained relative ordering but are expensive. Combining both maximizes relevant passages evaluated coarsely and then ranked finely.
- Core assumption: Coarse-grained relevance filtering with pointwise methods is effective enough to justify spending budget before applying more expensive pairwise comparisons.
- Evidence anchors: [abstract] "Our most efficient method, which we refer as EcoRank, is a budget-constrained LLM-based text re-ranking pipeline, that jointly optimizes several objectives: 1) which prompt designs to deploy, 2) which LLM APIs to call, and 3) how to split budget between multiple prompts and LLMs." [section] "We use a combination of pointwise and pairwise methods... the performance decreases as only half budget is used."
- Break condition: If pointwise filtering is too coarse, important passages may be incorrectly filtered out, leading to poor final rankings despite fine-grained pairwise stage.

### Mechanism 3
- Claim: Splitting the budget equally between two stages (x = 0.5, y = 0.5) optimizes the trade-off between filtering quality and comparison quantity.
- Mechanism: Equal budget split ensures neither filtering stage nor comparison stage is under-resourced. This balance allows sufficient initial filtering to improve quality of passages entering pairwise stage while providing enough budget for meaningful comparisons.
- Core assumption: Both stages contribute equally to final ranking quality, so equal budget split is optimal.
- Evidence anchors: [abstract] "We further propose a novel two-layer cascading pipeline EcoRank which optimizes the budget usage." [section] "We hypothesize that both stages contribute equally to the pipeline and choose an equal split of x and y. We experiment on various splits on two datasets and confirm our hypothesis in Section 5."
- Break condition: If one stage is more critical than the other for specific dataset or query type, equal split may be suboptimal, leading to wasted budget or insufficient processing.

## Foundational Learning

- Concept: Budget-constrained optimization in machine learning
  - Why needed here: EcoRank must maximize ranking performance within fixed financial budget, requiring careful resource allocation across different stages and APIs
  - Quick check question: If a model has a budget of $10 and two stages costing $6 and $4 respectively, what happens if you reallocate $2 from stage 2 to stage 1?

- Concept: Prompt engineering for large language models
  - Why needed here: Different prompt strategies (pointwise, pairwise, listwise) have varying costs and effectiveness, and choosing right combination is critical for performance within budget
  - Quick check question: Why might a pointwise prompt be more budget-friendly than a pairwise prompt when processing 50 passages?

- Concept: Trade-offs between model accuracy and cost
  - Why needed here: EcoRank uses high-accuracy, expensive LLM for initial filtering and cheaper LLM for detailed comparisons, balancing cost and performance
  - Quick check question: What is the risk of using only the cheapest LLM for all stages in a budget-constrained re-ranking task?

## Architecture Onboarding

- Component map: Query + Passage List -> Stage 1 (Expensive LLM + Pointwise) -> Filtered Passages -> Stage 2 (Cheaper LLM + Pairwise) -> Final Ranked List

- Critical path:
  1. Receive query and initial passage list
  2. Allocate first half of budget to Stage 1
  3. Use expensive LLM with binary classification to filter passages
  4. Pass filtered passages to Stage 2
  5. Allocate second half of budget to Stage 2
  6. Use cheaper LLM with pairwise comparisons to rank filtered passages
  7. Output final ranked list

- Design tradeoffs:
  - Accuracy vs. Cost: More expensive LLM increases accuracy but reduces number of passages that can be processed
  - Coarse vs. Fine-grained: Pointwise filtering is cheaper but less precise; pairwise ranking is expensive but provides better ordering
  - Equal vs. Unequal Budget Split: Equal split balances both stages but may not be optimal for all datasets

- Failure signatures:
  - Stage 1 produces too many "Yes" classifications → Stage 2 exceeds budget
  - Stage 1 produces too few "Yes" classifications → Stage 2 has too few passages to rank meaningfully
  - Pairwise stage fails to complete → Final ranking relies too heavily on initial ordering
  - Budget manager miscalculates token usage → API calls fail or are cut off

- First 3 experiments:
  1. Test EcoRank with equal budget split (x=0.5, y=0.5) on small dataset to verify basic functionality
  2. Vary budget split (e.g., x=0.7, y=0.3) to observe impact on ranking performance
  3. Replace expensive LLM in Stage 1 with cheaper one to measure degradation in accuracy and effect on overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for automatically selecting the appropriate LLM for each stage of EcoRank based on question difficulty and other factors?
- Basis in paper: [inferred] The paper mentions opportunity to automatically choose appropriate LLM and presents two initial experiments (EcoRank-Auto1 and EcoRank-Auto2) that do not surpass EcoRank but show potential for future automation
- Why unresolved: Current experiments show automated approaches do not yet outperform manually chosen LLMs, indicating more research needed to develop effective automatic selection method
- What evidence would resolve it: Comparative studies evaluating different automatic LLM selection methods against EcoRank and other baselines across multiple datasets and question types

### Open Question 2
- Question: How does performance of EcoRank change when considering larger number of passages per query (e.g., 1000 instead of 50)?
- Basis in paper: [explicit] Paper mentions experimenting with 50 passages per query and setting lower budget categories due to high cost, but notes more passages could be considered with budget limits increasing proportionally
- Why unresolved: Paper does not provide results for larger number of passages, so unclear how EcoRank's performance scales with increased passage numbers
- What evidence would resolve it: Conducting experiments with larger number of passages per query and varying budget limits would demonstrate how EcoRank's performance is affected by increased complexity and computational requirements

### Open Question 3
- Question: What is the impact of optimizing for R@10 instead of R@1 on performance of different ranking strategies in budget-constrained scenarios?
- Basis in paper: [explicit] Paper mentions optimizing for R@1 as in QA tasks typically care about top few re-ranked passages, but also shows R@10 results in Table 8. They note that pointwise methods score higher than pairwise in all budget categories in R@10
- Why unresolved: Paper does not explore trade-offs between optimizing for R@1 and R@10, nor investigate how different ranking strategies perform when R@10 is primary metric
- What evidence would resolve it: Experiments comparing performance of various ranking strategies when optimizing for R@10 instead of R@1, across different budget categories and datasets

## Limitations

- Budget Parameter Sensitivity: Equal 50/50 budget split may not be optimal across all datasets or query distributions; comprehensive sensitivity analysis is lacking
- API Cost Volatility: Method relies on specific LLM APIs with particular token costs; pricing changes or API unavailability would require recalibration
- Dataset Dependency: Evaluation limited to four specific QA and passage reranking datasets; generalizability to other domains remains uncertain

## Confidence

- High Confidence: Core claim that two-stage pipeline combining coarse relevance filtering with fine-grained pairwise ranking improves performance within budget constraints
- Medium Confidence: Claim that equal budget split (x=0.5, y=0.5) is optimal; justification is somewhat heuristic rather than theoretically grounded
- Low Confidence: Claim about generalizability across different LLM APIs and budget ranges; experiments limited to specific T5 model variants and three budget categories

## Next Checks

1. **Budget Split Sensitivity Analysis**: Systematically vary budget split ratio (e.g., 0.3/0.7, 0.4/0.6, 0.6/0.4, 0.7/0.3) across all four datasets to quantify performance degradation and identify optimal splits for different query types

2. **API Substitution Experiment**: Replace expensive T5-XL in Stage 1 with cheaper model (like T5-base) while keeping T5-L in Stage 2, then measure trade-off between cost savings and ranking accuracy to determine robustness to API changes

3. **Domain Transfer Validation**: Apply EcoRank to non-QA domain (such as conversational dialogue ranking or code search) using same budget constraints to test generalizability beyond evaluated datasets