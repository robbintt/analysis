---
ver: rpa2
title: 'Don''t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM
  Collaboration'
arxiv_id: '2402.00367'
source_url: https://arxiv.org/abs/2402.00367
tags:
- abstain
- answer
- knowledge
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how to identify knowledge gaps in large
  language models (LLMs) and abstain from answering when such gaps exist. The authors
  first adapt 11 existing approaches from calibration, training, prompting, and self-consistency
  categories to enable abstention functionality, but observe that these often require
  held-out sets and rely on self-reflection, which can be unreliable.
---

# Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration

## Quick Facts
- arXiv ID: 2402.00367
- Source URL: https://arxiv.org/abs/2402.00367
- Reference count: 40
- Primary result: Multi-LLM collaboration approaches COOPERATE and COMPETE outperform adapted baseline methods in 9 of 12 settings with up to 19.3% improvements in abstain accuracy

## Executive Summary
This paper addresses the critical challenge of identifying knowledge gaps in large language models (LLMs) to enable abstention from answering when confidence is low. The authors evaluate 11 existing approaches adapted for abstention but find they require held-out sets or rely on unreliable self-reflection. To overcome these limitations, they propose two novel multi-LLM collaboration frameworks: COOPERATE, where multiple LLMs provide feedback on proposed answers and a judge LLM synthesizes abstain decisions, and COMPETE, where an LLM is challenged by conflicting knowledge from other LLMs. Extensive experiments demonstrate that these approaches significantly outperform baselines in identifying when LLMs should abstain, with the ability to detect retrieval failures and localize knowledge gaps in multi-hop reasoning tasks.

## Method Summary
The paper introduces two multi-LLM collaboration approaches for knowledge gap identification. COOPERATE employs multiple LLMs to provide feedback on proposed answers, with a judge LLM synthesizing an abstain decision based on this collective input. COMPETE challenges an LLM by exposing it to conflicting knowledge from other LLMs, determining whether it should maintain its answer or abstain. Both frameworks are evaluated against 11 adapted baseline methods across three LLMs on four QA tasks. The approaches leverage the diversity of multiple LLMs to improve abstention accuracy beyond what single models can achieve through self-reflection or calibration-based methods.

## Key Results
- COOPERATE and COMPETE outperform all baseline methods in 9 of 12 experimental settings
- Up to 19.3% improvements in abstain accuracy compared to the best baseline methods
- The approaches successfully identify retrieval failures and localize knowledge gaps in multi-hop reasoning (StrategyQA task)
- Performance gains demonstrate the value of leveraging multiple LLMs rather than relying on single-model self-reflection

## Why This Works (Mechanism)
The core insight is that single LLMs struggle to accurately identify their own knowledge gaps through self-reflection, which can be unreliable and biased. By leveraging multiple LLMs with potentially diverse knowledge representations and reasoning patterns, the collaboration frameworks create a more robust mechanism for detecting uncertainty. COOPERATE benefits from collective wisdom where multiple perspectives help identify weaknesses that any single model might miss. COMPETE exploits the tension between conflicting knowledge sources to force models to critically evaluate their confidence. The diversity among LLMs serves as a natural check against overconfidence and hallucination, particularly valuable when individual models have incomplete or conflicting knowledge about specific topics.

## Foundational Learning

**Calibration Methods** - Techniques for adjusting model confidence scores to better reflect true accuracy; needed to establish baseline performance for abstention, quick check: compare calibration accuracy before/after adjustment

**Self-Consistency Approaches** - Methods where models generate multiple answers and select based on agreement; needed to understand limitations of single-model uncertainty estimation, quick check: measure agreement rates across multiple generations

**Multi-LLM Diversity** - The variation in knowledge and reasoning patterns across different LLM architectures and training data; needed to justify the collaboration approach, quick check: measure disagreement rates between model pairs

**Knowledge Gap Localization** - The ability to identify specific points where reasoning fails or knowledge is missing; needed for practical applications requiring precise error diagnosis, quick check: analyze failure points in multi-hop reasoning chains

**Abstention Thresholds** - Decision boundaries for when models should refuse to answer; needed to optimize the trade-off between coverage and reliability, quick check: sweep threshold values to find optimal precision-recall balance

## Architecture Onboarding

**Component Map**: User Query -> COOPERATE/COMPETE Framework -> Multiple LLM Calls -> Judge LLM (COOPERATE) or Answer Decision (COMPETE) -> Abstain/Answer Output

**Critical Path**: The sequence of LLM calls and decision synthesis that determines whether to abstain or provide an answer, with latency dominated by parallel LLM inference in the collaboration phase

**Design Tradeoffs**: Computational overhead from multiple LLM calls versus improved abstention accuracy; complexity of coordinating multiple models versus simplicity of single-model approaches; potential for emergent behaviors from LLM interactions

**Failure Signatures**: False abstentions occur when models are overly conservative; false answers occur when models fail to detect genuine knowledge gaps; performance degrades when LLMs have similar biases or training distributions

**First Experiments**: 1) Baseline ablation comparing single LLM abstention to multi-LLM collaboration on simple QA tasks, 2) Latency measurement across different numbers of collaborating models, 3) Cross-domain generalization testing to evaluate robustness across knowledge domains

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily on multiple-choice QA tasks, limiting generalizability to open-ended scenarios
- Adapted baseline methods were not originally designed for abstention, potentially inflating relative performance gains
- Knowledge gap localization demonstrated on only one multi-hop reasoning task (StrategyQA)
- Computational overhead from multiple LLM calls not thoroughly analyzed for practical deployment constraints
- Assumption of reliable LLM disagreement may not hold across all domains or similar training distributions

## Confidence

**High confidence** in the observation that existing abstention methods have limitations requiring held-out data or self-reflection

**Medium confidence** in the relative performance improvements of COOPERATE and COMPETE, given the adapted baseline methods

**Medium confidence** in the knowledge gap localization results, limited to single task demonstration

## Next Checks

1. Evaluate the approaches on open-ended QA tasks and compare against abstention methods specifically designed for such formats rather than adapted baselines

2. Conduct a comprehensive ablation study measuring the computational overhead (latency and cost) of multi-LLM collaboration across different model sizes and API constraints

3. Test the frameworks' effectiveness across diverse domains with varying knowledge distributions to assess whether LLM disagreement patterns remain reliable indicators of knowledge gaps