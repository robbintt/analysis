---
ver: rpa2
title: 'Investigating the Performance of Language Models for Completing Code in Functional
  Programming Languages: a Haskell Case Study'
arxiv_id: '2403.15185'
source_url: https://arxiv.org/abs/2403.15185
tags:
- code
- haskell
- languages
- completion
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of language models for code
  completion in Haskell, a functional programming language underrepresented in existing
  research. The authors fine-tune two models, CodeGPT and UniXcoder, on a dataset
  of Haskell functions and manually translate the HumanEval dataset to Haskell for
  evaluation.
---

# Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study

## Quick Facts
- arXiv ID: 2403.15185
- Source URL: https://arxiv.org/abs/2403.15185
- Reference count: 40
- Primary result: Fine-tuning enables feasible Haskell code completion despite poor transfer from imperative language pre-training

## Executive Summary
This study evaluates language model performance for code completion in Haskell, a functional programming language underrepresented in existing research. The authors fine-tune CodeGPT and UniXcoder models on Haskell functions and translate the HumanEval dataset to Haskell for evaluation. Their results show that models pre-trained on imperative languages do not transfer well to functional languages, but fine-tuning enables feasible code completion on Haskell. Manual analysis reveals distinct error patterns between models, with CodeGPT generating more empty predictions and unnecessary comments, while UniXcoder produces more incomplete, wrong syntax, and undefined predictions.

## Method Summary
The authors fine-tune two language models (CodeGPT and UniXcoder) on a dataset of Haskell functions and manually translate the HumanEval dataset to Haskell for evaluation. They employ both automatic evaluation metrics and manual analysis of model outputs across three code completion tasks. The study releases the translated HumanEval dataset, fine-tuned models, and code to reproduce experiments, providing a foundation for future research in functional language code completion.

## Key Results
- Models pre-trained on imperative languages show poor transfer to functional languages
- Fine-tuning enables feasible code completion performance on Haskell
- CodeGPT generates more empty predictions and unnecessary comments; UniXcoder produces more syntax errors and undefined predictions

## Why This Works (Mechanism)
The study demonstrates that language models require functional language-specific training to perform well on functional programming languages. Pre-training on imperative languages provides limited transfer learning benefits for functional paradigms, but fine-tuning on Haskell-specific code enables the models to learn functional programming patterns, type systems, and idioms necessary for accurate code completion.

## Foundational Learning
1. **Functional Programming Paradigms** - Understanding pure functions, immutability, and higher-order functions; needed to evaluate Haskell code generation quality; quick check: verify generated code follows functional principles
2. **Code Completion Metrics** - Familiarity with pass@k, exact match, and semantic equivalence metrics; needed to interpret automatic evaluation results; quick check: compare multiple evaluation metrics
3. **Model Fine-tuning Process** - Understanding how language models are adapted to specific programming languages; needed to contextualize performance differences; quick check: examine training data characteristics
4. **Haskell Type System** - Knowledge of Haskell's static typing and type inference; needed to assess code correctness; quick check: verify type signatures in generated code
5. **HumanEval Benchmark** - Understanding the original Python-based coding benchmark; needed to evaluate translated dataset validity; quick check: compare problem difficulty across languages
6. **Error Analysis Categories** - Familiarity with code completion error types (empty, incomplete, syntax errors, undefined); needed to interpret manual analysis results; quick check: categorize model outputs using defined error taxonomy

## Architecture Onboarding

**Component Map:**
Dataset Translation -> Model Fine-tuning -> Automatic Evaluation -> Manual Analysis -> Result Synthesis

**Critical Path:**
Haskell dataset creation → model fine-tuning → automated evaluation → manual error analysis → performance comparison

**Design Tradeoffs:**
- Translation vs native benchmarks: Used translated HumanEval for consistency but may miss Haskell-specific patterns
- Automatic vs manual evaluation: Combined metrics to address functional language evaluation challenges
- Model selection: Focused on two architectures to manage scope but limits generalizability

**Failure Signatures:**
- Poor performance on imperative-pretrained models indicates paradigm mismatch
- Distinct error patterns between models suggest architectural differences in handling functional code
- Limited manual sample size reduces confidence in comparative performance claims

**First 3 Experiments to Run:**
1. Evaluate fine-tuned models on larger native Haskell benchmarks to assess real-world performance
2. Compare additional model architectures to determine if patterns generalize beyond CodeGPT and UniXcoder
3. Test different fine-tuning strategies (learning rates, epochs, data augmentation) to optimize Haskell performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on a small manually translated HumanEval dataset rather than native Haskell benchmarks
- Automatic evaluation methodology has known limitations for functional language equivalence checking
- Manual analysis covers only 50 samples, limiting statistical power for detecting performance differences

## Confidence

**High confidence:** Models pre-trained on imperative languages show poor transfer to functional languages, and fine-tuning enables reasonable Haskell performance

**Medium confidence:** Comparative analysis showing CodeGPT produces more empty and comment-heavy predictions while UniXcoder generates more syntax errors and undefined predictions

**Low confidence:** Claims about absolute performance levels or general transferability patterns beyond the specific models examined

## Next Checks
1. Evaluate model performance on larger, native Haskell benchmarks rather than translated Python code
2. Conduct larger-scale manual analysis (minimum 200 samples) to improve statistical reliability
3. Test additional model architectures and fine-tuning strategies to determine generalizability of results