---
ver: rpa2
title: 'LDPKiT: Superimposing Remote Queries for Privacy-Preserving Local Model Training'
arxiv_id: '2405.16361'
source_url: https://arxiv.org/abs/2405.16361
tags:
- ldpkit
- sidp
- data
- noise
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the privacy-utility tradeoff in cloud-based
  machine learning inference by introducing LDPKiT, a framework that leverages local
  differential privacy (LDP) to protect user data while recovering utility lost due
  to noise injection. LDPKiT uses a novel superimposition technique to train a local
  model on noisy queries and their corresponding noisy labels from a remote model,
  enabling effective knowledge transfer under LDP.
---

# LDPKiT: Superimposing Remote Queries for Privacy-Preserving Local Model Training

## Quick Facts
- **arXiv ID:** 2405.16361
- **Source URL:** https://arxiv.org/abs/2405.16361
- **Authors:** Kexin Li; Aastha Mehta; David Lie
- **Reference count:** 40
- **Primary result:** LDPKiT improves utility of local model inference under LDP noise by training on noisy queries and labels from remote model.

## Executive Summary
This paper introduces LDPKiT, a framework that addresses the privacy-utility tradeoff in cloud-based machine learning inference. LDPKiT uses local differential privacy (LDP) to protect user data while leveraging a novel superimposition technique to train a local model on noisy queries and their corresponding noisy labels from a remote model. The framework demonstrates that knowledge available in noisy labels from cloud model inference can be aggregated to recover correct labels, effectively recovering utility lost due to noise injection. Experiments on Fashion-MNIST, SVHN, and PathMNIST show LDPKiT consistently improves utility while maintaining privacy, with benefits more pronounced at stronger noise levels.

## Method Summary
LDPKiT works by having users query a remote cloud model with LDP-noised inputs, then training a local model on the resulting noisy inputs paired with noisy labels from the cloud model. This "noise²" data enables the local model to learn patterns that help denoise predictions. The framework uses Laplacian noise for image datasets and UMLDP for text sanitization. The local model is trained using the collected noisy queries and labels, then used to infer labels on the original sensitive data. Performance is evaluated by comparing accuracy on the original data against standard LDP schemes.

## Key Results
- LDPKiT achieves nearly the same inference accuracy at ε=1.25 as at ε=2.0, yielding stronger privacy guarantees with less than 2% accuracy reduction
- On SVHN, LDPKiT consistently outperforms standard LDP across all epsilon values tested
- LDPKiT does not constitute a model extraction attack, with Zest distances > 1 indicating the local model does not replicate the cloud model's performance
- The framework shows diminishing returns at large dataset sizes, suggesting an optimal stopping point for querying

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training a local model on noisy inputs and noisy labels from the cloud model recovers utility lost due to LDP noise injection.
- Mechanism: The cloud model's noisy predictions on noised inputs still contain partial information about the true labels. By training a local model on this "noise²" data (noisy inputs + noisy labels), the local model learns to denoise the predictions, effectively recovering the utility of the original model.
- Core assumption: The noisy labels from the cloud model still retain enough information about the true labels to enable the local model to learn the underlying pattern despite the added noise.
- Evidence anchors:
  - [abstract]: "Our key insight is that knowledge available in the noisy labels returned from performing inference on noisy inputs can be aggregated and used to recover the correct labels."
  - [section 3.3]: "Our intuition is that the repeated responses that the cloud model returns to the user should similarly leak more information about the 'true label' that the cloud model would have predicted in the absence of LDP noise."
  - [corpus]: Weak evidence - the corpus neighbors discuss privacy-preserving LLM interactions and data poisoning attacks, which are related but don't directly support this specific mechanism.
- Break condition: If the LDP noise level is too high (very low epsilon), the noisy labels may contain insufficient information about the true labels, making it impossible for the local model to learn effectively.

### Mechanism 2
- Claim: The local model trained with LDPKiT does not constitute a model extraction attack because it does not achieve similar performance to the cloud model.
- Mechanism: Model extraction attacks aim to replicate the victim model's high performance with minimal queries. LDPKiT, however, queries inputs with LDP noise added, which are not in the distribution the cloud model is trained on. Additionally, the local model's performance remains significantly below the cloud model's, especially at stronger privacy levels.
- Core assumption: The Zest distance metric effectively measures the similarity between models and can distinguish between legitimate knowledge transfer and adversarial model extraction.
- Evidence anchors:
  - [section 4.4]: "We answer RQ3 quantitatively using Zest distances [22] as our evaluation metrics, which are the distances between two models computed based on LIME's model-agnostic explanations [47]."
  - [section 4.4]: "As explained in Appendix G, an adversarial model extraction attack occurs when Dz < 1. Table 1 shows that LDPKiT does not contribute to model theft at any noise level since all Dz > 1."
  - [corpus]: Weak evidence - the corpus neighbors discuss privacy-preserving machine learning and data poisoning attacks, which are related but don't directly support this specific mechanism.
- Break condition: If the local model's performance approaches that of the cloud model, or if the Zest distance metric fails to accurately distinguish between legitimate knowledge transfer and model extraction.

### Mechanism 3
- Claim: LDPKiT provides stronger privacy guarantees than standard LDP schemes while maintaining comparable utility.
- Mechanism: Standard LDP schemes add noise to each query independently, degrading utility. LDPKiT leverages the collective knowledge from a batch of noisy queries and their corresponding noisy labels to train a local model, which can then provide more accurate predictions on the original data. This allows LDPKiT to achieve similar utility with stronger privacy (lower epsilon).
- Core assumption: The privacy loss from querying the cloud model with LDP noise is bounded by the LDP guarantee, and the additional privacy loss from training the local model is negligible compared to the gains in utility.
- Evidence anchors:
  - [abstract]: "LDPKiT achieves nearly the same inference accuracy at ε=1.25 as at ε=2.0, yielding stronger privacy guarantees with less than a 2% accuracy reduction."
  - [section 4.2]: "Our experiments show that LDPKiT provides better privacy with essentially no loss of accuracy compared to SIDP. For instance, on CIFAR-10 with ResNet-18, SIDP provides an average accuracy of 84.78% at ε = 15, while LDPKiT is able to provide a roughly equivalent accuracy of 83.27% for a much stronger level of privacy at ε = 5."
  - [corpus]: Weak evidence - the corpus neighbors discuss privacy-preserving machine learning and data poisoning attacks, which are related but don't directly support this specific mechanism.
- Break condition: If the privacy loss from querying the cloud model or training the local model exceeds the LDP guarantee, or if the utility gains are not sufficient to justify the increased privacy risk.

## Foundational Learning

- Concept: Local Differential Privacy (LDP)
  - Why needed here: LDP is the core privacy mechanism used in LDPKiT to protect user queries before sending them to the cloud model. Understanding LDP is crucial for grasping how LDPKiT preserves privacy while recovering utility.
  - Quick check question: What is the key difference between local differential privacy (LDP) and global differential privacy (GDP)?

- Concept: Knowledge Distillation
  - Why needed here: LDPKiT uses knowledge distillation techniques to transfer knowledge from the noisy cloud model to the local model. Understanding knowledge distillation is important for understanding how LDPKiT recovers utility from noisy labels.
  - Quick check question: What is the main goal of knowledge distillation, and how does it differ from model extraction?

- Concept: Model Extraction Attacks
  - Why needed here: LDPKiT is designed to be different from model extraction attacks. Understanding model extraction attacks is crucial for grasping how LDPKiT ensures that the local model does not violate the cloud model's privacy or terms of use.
  - Quick check question: What are the key characteristics of a model extraction attack, and how does LDPKiT differ from such attacks?

## Architecture Onboarding

- Component map: User's sensitive dataset (Dpriv) -> LDP noise injection -> Cloud-hosted model (MR) -> Noisy labels -> Local model (ML) training -> Validation dataset (Dval)

- Critical path:
  1. User prepares sensitive dataset (Dpriv)
  2. LDP noise is added to Dpriv
  3. Noisy queries are sent to cloud model (MR)
  4. MR returns noisy labels
  5. Local model (ML) is trained on noisy data and labels
  6. ML is used to infer labels on original (noise-free) data in Dpriv
  7. ML's performance is evaluated on Dval

- Design tradeoffs:
  - Privacy vs. Utility: Stronger privacy (lower epsilon) leads to more noise and lower utility, but LDPKiT can recover some of the lost utility.
  - Number of queries: More queries lead to better utility recovery but also increase privacy risk.
  - Model complexity: More complex local models may achieve better utility recovery but also require more computational resources.

- Failure signatures:
  - Low accuracy on Dval: Indicates that the local model is not generalizing well to unseen data.
  - High Zest distance between MR and ML: Suggests that the local model is not effectively learning from the noisy labels.
  - Accuracy on Dpriv plateaus early: Indicates that the local model has reached its learning limit with the given number of queries.

- First 3 experiments:
  1. Compare the accuracy of LDPKiT with standard LDP on a small dataset (e.g., Fashion-MNIST) at different epsilon values.
  2. Measure the Zest distance between MR and ML at different epsilon values to verify that LDPKiT does not constitute a model extraction attack.
  3. Evaluate the effect of the number of queries on LDPKiT's performance by gradually increasing the size of Dpriv and measuring the accuracy on Dval.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal stopping point for querying the cloud model to prevent privacy leakage while maintaining high utility in LDPKiT?
- Basis in paper: [explicit] The paper discusses the trade-off between privacy and utility, mentioning that LDPKiT has diminishing returns at large |Dpriv| and suggests monitoring accuracy on a small labeled Dval to determine when to stop querying.
- Why unresolved: The paper provides a general guideline but does not specify a concrete method or threshold for determining the stopping point, which can vary depending on the dataset and privacy requirements.
- What evidence would resolve it: Empirical studies showing the relationship between accuracy improvement, privacy leakage, and dataset size for different datasets and noise levels, along with a proposed stopping criterion based on these findings.

### Open Question 2
- Question: How does LDPKiT perform on regression tasks or unsupervised clustering tasks compared to its performance on classification tasks?
- Basis in paper: [inferred] The paper focuses on supervised learning classification tasks and mentions the possibility of extending LDPKiT to regression or unsupervised tasks as a future direction.
- Why unresolved: The current evaluation is limited to classification tasks, and the effectiveness of LDPKiT in other types of machine learning tasks remains unexplored.
- What evidence would resolve it: Experimental results demonstrating the performance of LDPKiT on regression and unsupervised clustering tasks, comparing it to standard LDP methods and non-private baselines.

### Open Question 3
- Question: What advanced training strategies, such as active learning, can be integrated with LDPKiT to improve its efficiency and effectiveness?
- Basis in paper: [explicit] The paper mentions that LDPKiT is compatible with advanced training strategies like active learning and core-set strategies, but does not explore their effects in detail.
- Why unresolved: While the compatibility is acknowledged, the specific impact of these strategies on LDPKiT's performance, privacy, and utility has not been investigated.
- What evidence would resolve it: Experimental results showing the impact of various advanced training strategies on LDPKiT's performance, including comparisons with standard LDPKiT and traditional active learning approaches.

### Open Question 4
- Question: How does the choice of privacy-preserving query generation method (e.g., synthetic queries vs. LDP noise) affect the performance and privacy guarantees of LDPKiT?
- Basis in paper: [explicit] The paper discusses using LDP noise for privacy-preserving queries but mentions the possibility of using synthetic queries or selecting similar queries from public datasets as future directions.
- Why unresolved: The current implementation uses LDP noise, and the effects of alternative query generation methods on LDPKiT's performance and privacy have not been explored.
- What evidence would resolve it: Comparative studies evaluating LDPKiT's performance and privacy guarantees when using different query generation methods, including synthetic queries and public dataset selection, against the current LDP noise approach.

## Limitations
- The effectiveness of the superimposition technique depends critically on the cloud model's ability to provide meaningful noisy labels even after LDP perturbation.
- The privacy analysis focuses primarily on the LDP noise injection mechanism, but the potential privacy leakage from training a local model on the noisy cloud predictions warrants further investigation.
- At very low epsilon values, the noisy labels may contain insufficient information for effective knowledge transfer, potentially limiting the framework's utility recovery capabilities.

## Confidence
- **High:** The core mechanism of using noisy queries and noisy labels for local model training is well-established in knowledge distillation literature. The empirical results showing improved utility over standard LDP schemes are supported by concrete experimental evidence.
- **Medium:** The claim that LDPKiT does not constitute a model extraction attack is supported by Zest distance measurements, but the sensitivity of this metric to different types of attacks and the robustness of the conclusions require further validation.
- **Low:** The theoretical analysis of the latent space representations and the exact mechanism by which utility is recovered through superimposition could benefit from more rigorous mathematical proofs and ablation studies.

## Next Checks
1. Conduct experiments with extremely low epsilon values (e.g., ε < 1) to determine the practical limits of LDPKiT's utility recovery capabilities and identify the threshold beyond which the framework fails to provide meaningful improvements.
2. Perform comprehensive privacy analysis including membership inference attacks on the trained local model to quantify any additional privacy risks introduced by the superimposition training process.
3. Design ablation studies that systematically vary the number of queries, model architectures, and noise levels to better understand the relationship between these factors and the framework's performance, providing clearer guidelines for practical deployment.