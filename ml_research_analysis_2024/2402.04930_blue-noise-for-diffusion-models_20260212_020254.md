---
ver: rpa2
title: Blue noise for diffusion models
arxiv_id: '2402.04930'
source_url: https://arxiv.org/abs/2402.04930
tags:
- noise
- blue
- gaussian
- diffusion
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new diffusion model that incorporates correlated
  noise into the training process to improve image generation quality. The key idea
  is to use time-varying noise, specifically blue noise, which has high-frequency
  characteristics and no low-frequency components.
---

# Blue noise for diffusion models

## Quick Facts
- arXiv ID: 2402.04930
- Source URL: https://arxiv.org/abs/2402.04930
- Authors: Xingchang Huang; Corentin SalaÃ¼n; Cristina Vasconcelos; Christian Theobalt; Cengiz Ã–ztireli; Gurprit Singh
- Reference count: 16
- Primary result: Proposed diffusion model with blue noise achieves FID scores of 3.16 on AFHQ-Cat (642), outperforming IADB (3.86) and DDIM (3.84)

## Executive Summary
This paper introduces a novel diffusion model that incorporates blue noise instead of traditional Gaussian white noise during the denoising process. The key innovation is a time-varying noise framework that interpolates between white and blue noise across diffusion steps, preserving high-frequency image details while allowing coarse structure reconstruction. The method demonstrates significant improvements in image generation quality across multiple datasets including AFHQ-Cat, CelebA, and LSUN-Church, while maintaining efficient generation times.

## Method Summary
The proposed method uses time-varying noise that transitions from Gaussian white noise to Gaussian blue noise throughout the diffusion process. Blue noise is generated efficiently using pre-computed lower triangular matrices and matrix-vector multiplication. The network is modified to output both x0 - LtÎµ and Î±(t-1)(LbÎµ - LwÎµ) terms, with the interpolation controlled by a gamma scheduler. An optional rectified mapping mechanism improves gradient flow by pairing similar noise-image combinations within mini-batches. The framework is built upon deterministic diffusion models and achieves state-of-the-art results while maintaining comparable generation speeds.

## Key Results
- Achieves FID of 3.16 on AFHQ-Cat (642) compared to IADB (3.86) and DDIM (3.84)
- Outperforms existing methods on CelebA and LSUN-Church datasets
- Demonstrates effective conditional image generation for super-resolution tasks
- Maintains similar generation speed to baseline deterministic models
- Shows improved precision and recall metrics across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Blue noise preserves high-frequency image details during diffusion by preventing the destruction of fine-grained structure in early denoising steps.
- **Mechanism:** Blue noise masks have no low-frequency energy, so when added to corrupted images they preserve coarse structure while allowing high-frequency content to survive through later denoising steps. This matches the diffusion model's coarse-to-fine reconstruction behavior.
- **Core assumption:** The network's denoising capacity aligns with the frequency content of the noise used at each time step.
- **Evidence anchors:**
  - [abstract] "blue noise, which has high-frequency characteristics and no low-frequency components"
  - [section] "Blue noise is characterized by a power spectrum with no energy in its low-frequency region"
  - [corpus] Weak evidence; no direct citations for this mechanism in related papers

### Mechanism 2
- **Claim:** Time-varying noise schedules interpolate between white and blue noise, allowing coarse structure reconstruction first and fine detail refinement later.
- **Mechanism:** The gamma scheduler linearly interpolates from Gaussian (white) noise at early steps to Gaussian blue noise at later steps. This lets the network first recover low-frequency structure with white noise, then refine high-frequency details with blue noise.
- **Core assumption:** Different frequency noise types are beneficial at different stages of denoising.
- **Evidence anchors:**
  - [section] "Gaussian noise and Gaussian blue noise are interpolated based on the time stepğ‘¡"
  - [section] "Using Gaussian blue noise in our framework failed to recover the fine details due to its low-frequency property"
  - [corpus] No direct evidence; this is a novel architectural claim

### Mechanism 3
- **Claim:** Correlated batch mapping improves gradient flow by pairing each image with the most similar noise, reducing trajectory variance.
- **Mechanism:** For each noise sample in a mini-batch, the closest unpaired image (by L2 distance) is selected. This creates consistent noise-image trajectories across training iterations, improving gradient stability.
- **Core assumption:** Consistent noise-image pairing improves optimization by reducing trajectory variance.
- **Evidence anchors:**
  - [section] "For each b, the x0 with the shortest distance that has not yet been used is selected"
  - [section] "This improved mapping ensures that a specific image will consistently be associated with the same type of noise"
  - [corpus] No direct evidence; this is a novel training technique

## Foundational Learning

- **Concept: Diffusion model forward/backward processes**
  - Why needed here: Understanding how noise corrupts and how denoising reconstructs is essential for implementing the time-varying noise framework.
  - Quick check question: In the forward process, what happens to the image at each time step, and how is the backward process defined?

- **Concept: Frequency domain and power spectra**
  - Why needed here: Blue noise's high-frequency characteristic is defined in the frequency domain; understanding this is crucial for implementing and debugging the noise generation.
  - Quick check question: What is the key difference between the power spectrum of white noise and blue noise?

- **Concept: Cholesky decomposition and matrix-vector multiplication**
  - Why needed here: Efficient blue noise generation requires precomputing a lower triangular matrix L and multiplying it with Gaussian noise.
  - Quick check question: How does the Cholesky decomposition enable efficient generation of correlated Gaussian noise from uncorrelated noise?

## Architecture Onboarding

- **Component map:** Noise generation -> Gamma scheduler -> U-Net backbone -> Rectified mapping

- **Critical path:**
  1. Generate blue noise mask via matrix multiplication.
  2. Apply time-varying noise according to gamma scheduler.
  3. Train U-Net to predict both denoising terms.
  4. Apply rectified mapping (optional) for improved gradient flow.

- **Design tradeoffs:**
  - Precomputing L matrices vs. on-the-fly computation: Precomputing is faster but memory-intensive for high resolutions.
  - Padding method for higher resolutions: Efficient but may introduce seams; trade-off between speed and artifact visibility.
  - Single vs. dual network outputs: Single network with 6-channel output is more efficient than two separate networks.

- **Failure signatures:**
  - Visual artifacts at seams in high-resolution images: Likely due to padding method.
  - Degraded image quality when using only blue noise: Indicates mismatch between noise frequency and denoising capacity at early steps.
  - Training instability with rectified mapping: May occur if pairing becomes too rigid or computation overhead is too high.

- **First 3 experiments:**
  1. Implement basic blue noise generation and verify frequency spectrum matches expected blue noise profile.
  2. Integrate time-varying noise into a simple diffusion model and verify smooth interpolation between white and blue noise over time steps.
  3. Compare image quality with and without rectified mapping on a small dataset to quantify gradient flow improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal time-varying schedule (ğ›¾-scheduler) for different image resolutions and datasets?
- Basis in paper: [explicit] The paper mentions that they fix ğœ based on image resolution (ğœ = 0.2 for 1282, ğœ = 1000 for 642) but note that parameter tuning depends on image resolution and is computationally intensive.
- Why unresolved: The paper acknowledges that the choice of ğ›¾-scheduler parameters (ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡, ğ‘’ğ‘›ğ‘‘, ğœ) significantly impacts performance but doesn't provide a systematic method for determining optimal values across different resolutions and datasets.
- What evidence would resolve it: A comprehensive ablation study showing FID scores across different ğœ values for various resolutions and datasets, along with an analysis of the convergence behavior of the ğ›¾-scheduler parameters during training.

### Open Question 2
- Question: How does the blue noise property (specifically the absence of low-frequency components) affect the ability to generate complex, low-frequency structures in images?
- Basis in paper: [explicit] The paper discusses how using only Gaussian blue noise leads to degraded quality because it restricts the diffusion process to a limited range of directions, making it difficult to refine intermediately generated content in later time steps.
- Why unresolved: While the paper demonstrates the effectiveness of time-varying noise that incorporates blue noise in later steps, it doesn't fully explore how the inherent properties of blue noise impact the generation of low-frequency structures like overall shapes and compositions.
- What evidence would resolve it: A detailed analysis comparing the frequency spectra of generated images using different noise types (Gaussian, blue, red) across various stages of the diffusion process, and correlating these with human perceptual studies of image quality.

### Open Question 3
- Question: Can the time-varying noise framework be effectively extended to stochastic diffusion models and fewer-step models?
- Basis in paper: [explicit] The paper mentions that extending their framework to stochastic models [Ho et al. 2020; Song et al. 2021b] and fewer-step models [Karras et al. 2022; Song et al. 2023; Luo et al. 2023] would be an interesting future direction.
- Why unresolved: The paper focuses on deterministic diffusion models (IADB) and doesn't explore how their time-varying noise concept would integrate with the inherent stochasticity of other diffusion models or the efficiency requirements of fewer-step models.
- What evidence would resolve it: Implementation and evaluation of the time-varying noise framework on a stochastic diffusion model (e.g., DDPM) and a fewer-step model (e.g., Consistency Models), comparing FID scores and generation speed with their respective baseline models.

## Limitations
- Only tested on three specific datasets (AFHQ-Cat, CelebA, LSUN-Church), limiting generalizability
- Computational overhead from rectified mapping may not scale well to larger batch sizes
- Time-varying noise schedule requires careful hyperparameter tuning with no systematic method provided

## Confidence
- **High confidence**: The mathematical formulation of blue noise generation using Cholesky decomposition and the implementation of time-varying noise interpolation between white and blue noise are technically sound and well-specified.
- **Medium confidence**: The empirical improvements in FID scores over existing methods (IADB, DDIM) are convincing, though the absolute improvements are relatively modest (0.7-1.2 points on AFHQ-Cat) and may not justify the added complexity for all applications.
- **Low confidence**: The theoretical justification for why blue noise specifically improves image quality is not fully developed, and the mechanism claims lack direct supporting evidence from related work or ablation studies.

## Next Checks
1. Perform frequency-domain analysis of generated images to verify that blue noise preserves high-frequency details compared to white noise, using power spectrum plots and comparing low vs. high-frequency energy preservation.
2. Conduct ablation studies on the gamma scheduler parameters (sstart, send, Ï„) across different datasets to determine the sensitivity of performance to these hyperparameters and identify optimal settings.
3. Test the rectified mapping mechanism with larger batch sizes to evaluate computational overhead and determine if the gradient flow improvements scale or diminish with increased batch size.