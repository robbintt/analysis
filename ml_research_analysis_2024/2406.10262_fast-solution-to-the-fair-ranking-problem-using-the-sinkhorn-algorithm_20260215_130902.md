---
ver: rpa2
title: Fast solution to the fair ranking problem using the Sinkhorn algorithm
arxiv_id: '2406.10262'
source_url: https://arxiv.org/abs/2406.10262
tags: []
core_contribution: The paper addresses the problem of fair ranking in two-sided marketplaces,
  where the goal is to balance consumer satisfaction and fairness among items. The
  authors propose a fast solution to the impact-based fair ranking problem, which
  maximizes the Nash social welfare based on fair division.
---

# Fast solution to the fair ranking problem using the Sinkhorn algorithm

## Quick Facts
- arXiv ID: 2406.10262
- Source URL: https://arxiv.org/abs/2406.10262
- Authors: Yuki Uehara; Shunnosuke Ikeda; Naoki Nishimura; Koya Ohashi; Yilin Li; Jie Yang; Deddy Jobson; Xingxia Zha; Takeshi Matsumoto; Noriyoshi Sukegawa; Yuichi Takano
- Reference count: 11
- One-line primary result: Proposed algorithm provides fair rankings of high quality and is about 1000 times faster than commercial optimization software

## Executive Summary
This paper addresses the problem of fair ranking in two-sided marketplaces, where the goal is to balance consumer satisfaction and fairness among items. The authors propose a fast solution to the impact-based fair ranking problem by transforming it into an unconstrained optimization problem and using a gradient ascent method that repeatedly executes the Sinkhorn algorithm. Experimental results on synthetic and real-world datasets demonstrate that the proposed algorithm provides fair rankings of high quality while significantly reducing computation time compared to commercial optimization software.

## Method Summary
The paper proposes a fast solution to the impact-based fair ranking problem by first transforming it into an unconstrained optimization problem. The authors design a gradient ascent method that repeatedly executes the Sinkhorn algorithm to solve the transformed problem. The algorithm searches for transport costs such that the corresponding Sinkhorn solution yields an optimal fair ranking. The method is implemented using PyTorch, leveraging automatic differentiation and GPU acceleration for efficient computation.

## Key Results
- The proposed algorithm is about 1000 times faster than applying commercial optimization software (Mosek) to the impact-based fair ranking problem.
- The algorithm achieves comparable results to other methods in terms of user utility, mean max envy, and the proportion of items better or worse off.
- The method demonstrates high-quality fair rankings on both synthetic and real-world datasets (Delicious dataset).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transforming the constrained fair ranking problem into an unconstrained optimization problem enables the use of efficient gradient-based methods.
- Mechanism: By reformulating the problem to minimize the negative Nash social welfare with respect to transport cost matrices, the algorithm can leverage gradient ascent, which is computationally more efficient than solving the original constrained problem.
- Core assumption: The optimal doubly stochastic matrix for any transport cost matrix can be computed using the Sinkhorn algorithm.
- Evidence anchors:
  - [abstract]: "We first transform the fair ranking problem into an unconstrained optimization problem..."
  - [section]: "Theorem 1. For any Xu satisfying Eqs. (1)–(3), there exists Cu such that Xu = X ⋆ u(Cu)."
- Break condition: If the Sinkhorn algorithm fails to find an optimal solution for some transport cost matrices, the transformation would be invalid.

### Mechanism 2
- Claim: Using the Sinkhorn algorithm with entropic regularization provides fast and parallelizable computation of optimal transport matrices.
- Mechanism: The Sinkhorn algorithm efficiently computes the optimal doubly stochastic matrix for a given transport cost matrix by iteratively updating scaling factors, and it is compatible with GPU parallel computing.
- Core assumption: The entropic regularization parameter ε is chosen appropriately to balance computational speed and solution accuracy.
- Evidence anchors:
  - [abstract]: "...repeats the Sinkhorn algorithm."
  - [section]: "This optimal transport problem can be solved efficiently using the Sinkhorn algorithm [4]."
- Break condition: If the regularization parameter is too small, the algorithm may become slow; if too large, the solution may deviate significantly from the true optimum.

### Mechanism 3
- Claim: Gradient ascent on the transport cost matrices converges to a solution that maximizes the Nash social welfare.
- Mechanism: By iteratively updating the transport cost matrices in the direction that increases the Nash social welfare, the algorithm searches for cost matrices that yield optimal fair rankings.
- Core assumption: The gradient of the Nash social welfare with respect to the transport cost matrices can be accurately computed and used for updates.
- Evidence anchors:
  - [abstract]: "...design a gradient ascent method that repeatedly executes the Sinkhorn algorithm."
  - [section]: "We now consider searching for transport costs C := (Cu)u∈U such that the corresponding Sinkhorn solution... will be optimal for the fair ranking problem (6)."
- Break condition: If the gradient landscape is too flat or has many local optima, the gradient ascent may converge slowly or to suboptimal solutions.

## Foundational Learning

- Concept: Optimal Transport and the Sinkhorn Algorithm
  - Why needed here: The Sinkhorn algorithm is used to efficiently compute optimal transport matrices, which are central to transforming and solving the fair ranking problem.
  - Quick check question: What is the role of the entropic regularization term in the Sinkhorn algorithm?

- Concept: Doubly Stochastic Matrices
  - Why needed here: The ranking policy is represented by doubly stochastic matrices, ensuring that each consumer ranks all items exactly once and each position is assigned to exactly one item.
  - Quick check question: What are the constraints that a matrix must satisfy to be doubly stochastic?

- Concept: Nash Social Welfare (NSW)
  - Why needed here: The NSW is the objective function being maximized, balancing consumer utility and fairness among items.
  - Quick check question: How is the Nash social welfare defined in terms of item impacts?

## Architecture Onboarding

- Component map: Input -> Gradient Ascent Loop -> Sinkhorn Algorithm -> Output (Doubly Stochastic Matrices)
- Critical path:
  1. Initialize transport cost matrices
  2. For each consumer, compute optimal transport matrix using Sinkhorn
  3. Compute gradient of Nash social welfare w.r.t. transport costs
  4. Update transport cost matrices using gradient ascent
  5. Repeat until convergence

- Design tradeoffs:
  - Entropic regularization parameter ε: Larger values speed up computation but may reduce solution accuracy
  - Batch size in gradient ascent: Larger batches provide more stable gradients but increase computation time
  - GPU vs CPU: GPU significantly accelerates Sinkhorn iterations but requires compatible hardware

- Failure signatures:
  - Slow convergence: May indicate poor choice of learning rate or ε
  - Numerical instability: Can occur if relevance scores are extreme or ε is too small
  - Non-convergence: Could suggest issues with the gradient computation or ill-conditioned problem

- First 3 experiments:
  1. Verify that the algorithm produces doubly stochastic matrices for simple test cases
  2. Compare user utility and fairness metrics against the naive MaxRele baseline
  3. Measure computation time scaling with the number of consumers and items

## Open Questions the Paper Calls Out
- How does the proposed method perform when extended to other optimization problems involving doubly stochastic matrices?
- What is the impact of different regularization parameters (ε) on the solution quality and computation time?
- How does the proposed method handle cases where the number of positions (m) is significantly smaller than the number of items (|I|)?
- Can the proposed method be extended to handle more complex fairness constraints, such as group fairness or individual fairness beyond envy-freeness?

## Limitations
- The specific methodology for generating synthetic datasets is not detailed, which may affect reproducibility and generalizability of results.
- The impact of hyperparameters such as the entropic regularization parameter ε and the Adam optimizer's learning rate on the algorithm's performance is not thoroughly explored.
- While the algorithm is shown to be fast on the tested datasets, its performance on significantly larger datasets with millions of consumers and items remains unverified.

## Confidence
- High Confidence: The theoretical foundation of transforming the fair ranking problem into an unconstrained optimization problem and using the Sinkhorn algorithm for efficient computation.
- Medium Confidence: The experimental results demonstrating improved computation time and comparable fairness metrics compared to other methods.
- Low Confidence: The generalizability of the results to different types of datasets and the impact of hyperparameter choices on the algorithm's performance.

## Next Checks
1. Implement the algorithm using the provided pseudocode and verify that it produces doubly stochastic matrices and converges to a solution on simple test cases.
2. Conduct experiments varying the entropic regularization parameter ε and the learning rate of the Adam optimizer to assess their impact on convergence speed and solution quality.
3. Test the algorithm on a larger synthetic dataset (e.g., 10,000 consumers and 5,000 items) to evaluate its performance and scalability.