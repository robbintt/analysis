---
ver: rpa2
title: 'CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model'
arxiv_id: '2403.05034'
source_url: https://arxiv.org/abs/2403.05034
tags:
- images
- arxiv
- diffusion
- mesh
- reconstruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CRM, a convolutional reconstruction model
  for high-fidelity 3D textured mesh generation from single images in 10 seconds.
  CRM leverages the spatial correspondence between triplanes and six orthographic
  input images, using a U-Net to generate high-resolution triplanes and Flexicubes
  for direct mesh optimization.
---

# CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model

## Quick Facts
- arXiv ID: 2403.05034
- Source URL: https://arxiv.org/abs/2403.05034
- Reference count: 40
- Primary result: Achieves state-of-the-art 3D textured mesh generation from single images in 10 seconds with superior geometric and texture quality

## Executive Summary
CRM introduces a convolutional reconstruction model that generates high-fidelity 3D textured meshes from single images in approximately 10 seconds. The approach leverages spatial correspondence between six orthographic input images and triplane structure, using a U-Net to generate high-resolution triplanes that are then decoded into meshes via Flexicubes. This convolutional approach achieves superior geometric and texture quality compared to transformer-based methods while significantly reducing training costs and time. CRM demonstrates state-of-the-art performance on geometry metrics like Chamfer Distance and F-Score, as well as texture metrics like PSNR and SSIM.

## Method Summary
CRM builds on the observation that triplane visualization exhibits spatial correspondence with six orthographic input images. The method uses a convolutional U-Net to generate high-resolution triplanes from orthographic RGB images and Canonical Coordinate Maps (CCMs), then decodes these into textured meshes using Flexicubes. The pipeline includes multi-view diffusion models to generate orthographic views and CCMs from a single input image, followed by CRM's U-Net architecture that processes concatenated RGB and CCM inputs to produce triplanes. These triplanes are decoded into SDF values, colors, deformations, and weights stored in an 80³ Flexicubes grid, from which textured meshes are extracted via dual marching cubes. The model is trained end-to-end with MSE and LPIPS losses for RGB, plus depth, mask, and mesh-quality regularization.

## Key Results
- Achieves state-of-the-art Chamfer Distance of 0.108 on GSO dataset, outperforming transformer-based LRM (0.125) and LGM (0.158)
- Reaches PSNR of 27.28 and SSIM of 0.883 on GSO, demonstrating superior texture quality
- Generates 3D textured meshes in approximately 10 seconds per image, significantly faster than competing methods
- Reduces training cost to 1/8 of transformer-based approaches by using batch size 32 versus 1024

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spatial correspondence between six orthographic images and triplane structure allows a convolutional U-Net to generate high-fidelity triplanes more efficiently than transformers.
- Mechanism: The triplane representation inherently projects geometry onto three orthogonal 2D planes (xy, xz, yz). When visualized, each plane shows strong alignment with the corresponding orthographic input views (left/right, front/back, top/bottom). This alignment means pixel-level features in the input images map directly to spatial regions in the triplane, which a convolutional network can exploit via local receptive fields and spatial coherence.
- Core assumption: The geometric structure of the triplane preserves spatial alignment with the input orthographic views, and this alignment is consistent across diverse 3D shapes.
- Evidence anchors:
  - [abstract] "CRM builds on the key observation that the visualization of triplane exhibits spatial correspondence of six orthographic images."
  - [section 3.2] "Our key insight is that the triplane is spatially aligned with the input six orthographic images and CCMs..."
  - [corpus] No direct evidence found in corpus; inference based on paper claims.
- Break condition: If the triplane visualization does not preserve spatial correspondence for certain shapes (e.g., non-axis-aligned or highly symmetric objects), the convolutional mapping would fail to capture accurate geometry.

### Mechanism 2
- Claim: Flexicubes as geometry representation enables end-to-end textured mesh optimization without extra conversion steps.
- Mechanism: Flexicubes stores SDF values, deformation fields, and per-voxel weights in a regular 3D grid. During training, dual marching cubes extracts meshes directly from these features, bypassing the need for NeRF volume rendering followed by Marching Cubes. This allows gradients to flow directly to mesh vertices and textures, improving both geometric detail and texture fidelity.
- Core assumption: The Flexicubes grid resolution (80³ in experiments) is sufficient to capture fine geometric details while remaining computationally tractable.
- Evidence anchors:
  - [section 3.2] "We use Flexicubes [46] as our geometry representation...enables us to train our reconstruction model with textured mesh as the final output in an end-to-end manner."
  - [corpus] No direct evidence found in corpus; inference based on paper claims.
- Break condition: If the grid resolution is too coarse, geometric details will be lost; if too fine, memory and compute costs become prohibitive.

### Mechanism 3
- Claim: Adding Canonical Coordinate Maps (CCM) to the input provides geometry priors that stabilize triplane prediction, especially for complex shapes.
- Mechanism: CCM encodes each point's position in canonical space (values in [0,1] per axis). When concatenated with RGB orthographic images, CCM supplies explicit 3D spatial context that RGB alone cannot provide. This additional signal helps the U-Net disambiguate geometry from texture, reducing artifacts and improving mesh quality.
- Core assumption: CCMs can be reliably generated by a diffusion model conditioned on the orthographic images, and their spatial encoding is consistent with the triplane's internal representation.
- Evidence anchors:
  - [section 3.2] "We also add CCM as input [22], which contains extra geometry information...CCM is the coordinates of each point in canonical space."
  - [section 4.3] "It can be seen that the results of the geometry degrade a lot without CCM input."
  - [corpus] No direct evidence found in corpus; inference based on paper claims.
- Break condition: If CCM generation is noisy or misaligned, it could introduce conflicting geometry signals and degrade performance.

## Foundational Learning

- Concept: Triplane representation and its spatial projection properties.
  - Why needed here: CRM relies on the triplane's property that each 2D plane corresponds to an orthographic view. Understanding this projection is essential to grasp why the convolutional U-Net can map images to triplanes effectively.
  - Quick check question: How does the triplane represent 3D geometry using three orthogonal 2D planes, and why does this preserve spatial correspondence with orthographic views?

- Concept: Flexicubes and dual marching cubes for mesh extraction.
  - Why needed here: Flexicubes replaces NeRF + Marching Cubes with a single differentiable representation. Knowing how SDF, deformation, and weights are decoded into meshes is key to understanding the end-to-end training pipeline.
  - Quick check question: What are the three feature types stored in Flexicubes, and how does dual marching cubes use them to produce a textured mesh?

- Concept: Canonical Coordinate Maps and their role in geometry encoding.
  - Why needed here: CCMs supply explicit 3D spatial context that RGB images lack. Recognizing their format ([0,1] per axis) and how they complement RGB is necessary to appreciate their impact on reconstruction quality.
  - Quick check question: What information does a CCM contain, and how does concatenating it with RGB images improve geometry prediction?

## Architecture Onboarding

- Component map: Single image → Multi-view diffusion (6 orthographic views + CCMs) → CRM U-Net (12-channel input) → Triplane (256×768×32) → Flexicubes features (80³) → Dual marching cubes → Textured mesh

- Critical path: Single image → multi-view diffusion (6 views + CCMs) → CRM U-Net → triplane → Flexicubes features → textured mesh. Training passes gradients from rendered images/masks/depth back through this path.

- Design tradeoffs:
  - Grid size 80³ vs. higher resolution: balances detail vs. memory/compute
  - 6 orthographic views vs. arbitrary poses: ensures consistent spatial alignment but may miss view-dependent details
  - CCM inclusion vs. RGB-only: improves geometry but adds generation cost and potential noise

- Failure signatures:
  - Geometry degradation without CCM: mesh becomes smooth but loses fine structure
  - Inconsistent multi-view diffusion outputs: causes artifacts in triplane, visible as cracks or misalignments in mesh
  - Grid too coarse: loss of small geometric features, visible as blocky surfaces

- First 3 experiments:
  1. Train CRM without CCM input; compare Chamfer Distance and PSNR to full model on GSO test set
  2. Vary Flexicubes grid size (40³, 80³, 120³); measure geometry quality vs. memory usage
  3. Replace U-Net with a small transformer encoder; measure convergence speed and final mesh quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CRM's training speed advantage be maintained when scaling to larger datasets or more complex 3D models?
- Basis in paper: [explicit] The paper states that CRM can be trained with a much smaller batch size (32) compared to transformer-based LRM (batch size 1024), resulting in 1/8 the training cost, and mentions fast convergence even with early iterations
- Why unresolved: The paper only reports results on a filtered version of Objaverse (~376k objects) and doesn't explore performance on larger datasets or more complex 3D geometries that might challenge the U-Net architecture
- What evidence would resolve it: Experiments training CRM on datasets an order of magnitude larger than Objaverse, or on datasets with significantly more complex geometries (e.g., high-poly models, intricate details, or diverse topological structures)

### Open Question 2
- Question: How does CRM perform on 3D objects with highly irregular or asymmetric shapes compared to symmetrical objects?
- Basis in paper: [inferred] The paper demonstrates good performance on standard datasets but doesn't specifically analyze performance variations across different object categories or shape complexities. The reliance on six orthographic views assumes some regularity in object structure
- Why unresolved: The paper focuses on overall metrics but doesn't break down performance by object type, symmetry, or geometric complexity, leaving questions about robustness to irregular shapes
- What evidence would resolve it: Systematic evaluation of CRM on diverse object categories (furniture, vehicles, organic shapes, abstract forms) with quantitative metrics segmented by symmetry, complexity, and topological features

### Open Question 3
- Question: What is the impact of the limited Flexicubes grid size (80) on the geometric detail achievable by CRM, and how does it compare to higher-resolution alternatives?
- Basis in paper: [explicit] The paper acknowledges that "the Flexicubes grid size is only 80 owing to the limited computing resource, which cannot represent very detailed geometry"
- Why unresolved: The paper doesn't explore the trade-off between grid resolution and quality, or compare CRM's geometric fidelity to methods using higher-resolution triplanes or different representations like dense neural fields
- What evidence would resolve it: Experiments varying the Flexicubes grid size (e.g., 80, 120, 160) on the same dataset, measuring geometric quality metrics (Chamfer Distance, F-score) and comparing to methods using higher-resolution triplanes or different representations

## Limitations
- The method relies on six orthographic views with fixed spatial alignment, limiting generalization to arbitrary camera poses
- Flexicubes grid resolution (80³) cannot represent very detailed geometry due to computational constraints
- Spatial correspondence assumption between triplane and orthographic views is never empirically verified across diverse shape categories

## Confidence
- **High confidence**: The convolutional architecture is faster than transformers; CRM achieves state-of-the-art Chamfer Distance and PSNR scores on reported benchmarks.
- **Medium confidence**: Spatial correspondence enables efficient convolutional mapping; CCMs improve geometry quality; Flexicubes enables end-to-end mesh optimization.
- **Low confidence**: Generalization to non-orthographic views; training stability across different object categories; scalability to higher grid resolutions.

## Next Checks
1. **Ablation study with metrics**: Remove CCM input and measure quantitative degradation in Chamfer Distance, PSNR, and SSIM on GSO test set to verify claimed geometry improvement.
2. **Pose generalization experiment**: Replace 6 orthographic views with 6 randomly sampled views (not orthographic) and measure geometry/texture quality degradation compared to baseline.
3. **Training stability analysis**: Train CRM with varying batch sizes and learning rates, plot convergence curves for geometry and texture losses, and report final metric variance across multiple seeds.