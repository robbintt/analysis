---
ver: rpa2
title: '"Give Me an Example Like This": Episodic Active Reinforcement Learning from
  Demonstrations'
arxiv_id: '2406.03069'
source_url: https://arxiv.org/abs/2406.03069
tags:
- learning
- demonstrations
- agent
- policy
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EARLY is an episodic active reinforcement learning algorithm that
  enables an agent to actively query episodic expert demonstrations in a trajectory-based
  feature space. By employing a trajectory-based uncertainty measurement based on
  temporal difference errors, EARLY determines optimal timing and content for feature-based
  queries, requesting complete demonstrations from initial to terminal states rather
  than isolated state-action pairs.
---

# "Give Me an Example Like This": Episodic Active Reinforcement Learning from Demonstrations

## Quick Facts
- arXiv ID: 2406.03069
- Source URL: https://arxiv.org/abs/2406.03069
- Reference count: 34
- Key outcome: Episodic Active Reinforcement Learning from Demonstrations (EARLY) achieves over 50% faster convergence to expert-level performance compared to baseline methods across three navigation tasks

## Executive Summary
EARLY is an episodic active reinforcement learning algorithm that enables an agent to actively query episodic expert demonstrations based on trajectory-level uncertainty measurements. The algorithm uses temporal difference errors to identify uncertain trajectories and requests complete demonstrations from initial to terminal states rather than isolated state-action pairs. Experiments show EARLY converges to expert-level performance over 50% faster than baseline methods when demonstrations are generated by simulated oracle policies, and pilot user studies demonstrate significantly faster convergence with human demonstrators while improving perceived task load.

## Method Summary
EARLY builds upon the Soft Actor-Critic algorithm and introduces a trajectory-based uncertainty measurement using temporal difference errors. For each episodic roll-out, the algorithm calculates average TD errors along the trajectory as a measure of uncertainty. An adaptive threshold based on recent uncertainty history determines when to query demonstrations. Instead of requesting isolated state-action pairs, EARLY queries complete episodic demonstrations matching the feature values of uncertain trajectories. The expert demonstrations are then added to the replay buffer for combined training with agent roll-outs.

## Key Results
- EARLY achieves over 50% faster convergence to expert-level performance compared to I-ARLD and IL baselines across three navigation tasks
- Pilot user study (N=18) shows EARLY reduces human time consumption and improves perceived task load while maintaining learning performance
- The algorithm successfully queries demonstrations only when uncertainty exceeds adaptive thresholds, optimizing the use of limited demonstration budgets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trajectory-based uncertainty measurement using temporal difference errors identifies the most beneficial feature points for querying demonstrations
- Mechanism: The algorithm evaluates the absolute temporal difference error along each episodic roll-out trajectory. Higher average TD errors indicate greater uncertainty in the agent's policy for that trajectory. By querying demonstrations that match the feature values of these uncertain trajectories, the agent receives targeted guidance in areas where its policy is least confident.
- Core assumption: Temporal difference errors are a reliable proxy for policy uncertainty in trajectory space.
- Evidence anchors:
  - [abstract]: "Based on a trajectory-level estimate of uncertainty in the agent's current policy, EARLY determines the optimized timing and content for feature-based queries"
  - [section]: "For a given episodic roll-out trajectory ğœ‰ğ‘–ğœ‹ under the policy ğœ‹, we define its uncertainty ğ‘¢ as: ğ‘¢ (ğœ‰ğ‘– ğœ‹ ) = E(ğ‘ ğ‘– ğ‘¡ ,ğ‘ğ‘– ğ‘¡ ) âˆˆğœ‰ğ‘–ğœ‹ [ |ğ›¿ğ‘– ğ‘¡ | ]"
- Break condition: If the agent's policy encounters systematic TD errors due to function approximation errors rather than true uncertainty, the measurement becomes unreliable.

### Mechanism 2
- Claim: Querying episodic demonstrations (rather than isolated state-action pairs) improves learning efficiency by providing context-rich guidance
- Mechanism: Instead of asking for individual state-action pairs with frequent environment resets, the agent requests complete trajectories from initial to terminal states. This reduces cognitive load on human demonstrators and eliminates the time cost of repeated context switches, while providing the agent with complete demonstrations of how to navigate from specific starting conditions to goals.
- Core assumption: Complete trajectories provide more learning value than isolated state-action pairs for sequential decision-making tasks.
- Evidence anchors:
  - [abstract]: "By querying episodic demonstrations as opposed to isolated state-action pairs, EARLY improves the human teaching experience and achieves better learning performance"
  - [section]: "we present a method that enables an RL agent to actively request episodic expert demonstrations (i.e., starting from an initial state till a terminal state) for better learning performance and improved user experience"
- Break condition: If the environment requires immediate intervention at specific states rather than complete trajectories, the episodic approach becomes less effective.

### Mechanism 3
- Claim: Dynamic adaptive uncertainty threshold based on recent history optimizes query timing
- Mechanism: The algorithm maintains a sliding window of recent feature points and their corresponding uncertainty values. When the current trajectory's uncertainty exceeds the threshold (defined as the top rğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦ percentile of recent uncertainties), the agent queries an episodic demonstration for that feature. This prevents over-querying while ensuring the agent still requests demonstrations when genuinely uncertain.
- Core assumption: Recent uncertainty patterns are indicative of current policy needs.
- Evidence anchors:
  - [section]: "After the shifting recent history grows to its full length ğ‘â„, an adaptive uncertainty threshold will be determined via a ratio threshold ğ‘Ÿğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦ âˆˆ [ 0, 1]"
  - [section]: "Whenever the current uncertainty value ğ‘¢ğ‘– is among the top ğ‘Ÿğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦ of the shifting recent history of uncertainty and the demonstration query budget ğ‘ğ‘‘ has not been used up, the learning agent will decide to make a query"
- Break condition: If the policy's uncertainty distribution changes rapidly, historical patterns may become stale and the adaptive threshold may miss critical query opportunities.

## Foundational Learning

- Concept: Temporal Difference Error
  - Why needed here: TD errors form the basis of the uncertainty measurement used to identify when to query demonstrations.
  - Quick check question: What does a high temporal difference error indicate about the agent's value function estimates?

- Concept: Soft Actor-Critic Algorithm
  - Why needed here: EARLY builds upon SAC as its underlying RL algorithm, so understanding its actor-critic structure and entropy maximization is essential.
  - Quick check question: How does SAC's entropy maximization objective differ from traditional actor-critic methods?

- Concept: Active Learning from Demonstrations
  - Why needed here: EARLY is a specific implementation of active learning from demonstrations, so understanding the general paradigm is crucial.
  - Quick check question: What is the key difference between passive RLED and active learning from demonstrations?

## Architecture Onboarding

- Component map: SAC algorithm (Q-networks, V-network, policy network) -> Trajectory-based uncertainty measurement module -> Query strategy module -> Expert demonstration collection -> Replay buffer
- Critical path: 1) Agent performs roll-out -> 2) Uncertainty is calculated from TD errors -> 3) Uncertainty is compared to adaptive threshold -> 4) If threshold exceeded, feature is queried -> 5) Expert demonstration is received and added to replay buffer -> 6) SAC updates from combined buffer of agent roll-outs and demonstrations
- Design tradeoffs: The choice between querying isolated state-action pairs vs. episodic demonstrations involves a tradeoff between granularity of guidance and efficiency of demonstration collection. Similarly, using recent history for adaptive thresholds trades off responsiveness to new uncertainty patterns against stability of query decisions.
- Failure signatures: The system may fail if the agent policy encounters systematic TD errors not related to true uncertainty, if the feature space representation is inadequate (e.g., using only initial states when other features would be more informative), or if the demonstration budget is exhausted before convergence.
- First 3 experiments:
  1. Test the uncertainty measurement on a simple grid world to verify that TD errors correlate with actual policy uncertainty.
  2. Compare convergence speed when using initial state as feature vs. more complex trajectory features.
  3. Validate that the adaptive threshold mechanism prevents over-querying by testing with varying ratio thresholds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of EARLY change when using different trajectory-based features beyond initial states, such as cumulative rewards or state entropy?
- Basis in paper: [explicit] The paper states "In this work, we chose the initial state ğ‘ 0 as the feature ğœ‘ğ‘– of an episodic roll-out trajectory" and mentions this is a limitation in Section 5.3
- Why unresolved: The paper only evaluates EARLY using initial state as the feature, leaving the question of how other feature choices might affect performance unanswered
- What evidence would resolve it: Comparative experiments testing EARLY with different trajectory-based features (e.g., cumulative rewards, state entropy, or combinations) across the same navigation tasks

### Open Question 2
- Question: How does EARLY's performance degrade when expert demonstrations have feature values that differ from the queried feature values, as would be expected with human demonstrators?
- Basis in paper: [explicit] Section 5.3 states "we assumed that the expert will always be able to provide a demonstration whose feature value is exactly equal to ğœ‘ğ‘˜. In practice, especially in the cases of human experts, the feature ğœ‘ğ‘Ÿğ‘’ğ‘ğ‘™ of the obtained expert demonstrations may follow an unknown distribution that is related to ğœ‘ğ‘˜"
- Why unresolved: The pilot user study results suggest performance degrades with human demonstrators, but the paper doesn't specifically analyze how feature-value discrepancies affect learning
- What evidence would resolve it: Controlled experiments where the feature-value discrepancy between queries and demonstrations is systematically varied, measuring the impact on convergence speed and final performance

### Open Question 3
- Question: What is the optimal ratio threshold ğ‘Ÿğ‘ğ‘¢ğ‘’ğ‘Ÿ ğ‘¦ for different task complexities and how does it interact with the demonstration budget?
- Basis in paper: [explicit] The paper mentions "we chose the ratio threshold ğ‘Ÿğ‘ğ‘¢ğ‘’ğ‘Ÿ ğ‘¦ as 0.35, 0.4, and 0.55 for three navigation tasks respectively" but doesn't provide analysis of why these values were chosen or how they affect performance
- Why unresolved: The paper only reports that different values were used for different tasks without explaining the methodology for selecting these values or analyzing their impact on learning efficiency
- What evidence would resolve it: Systematic experiments varying ğ‘Ÿğ‘ğ‘¢ğ‘’ğ‘Ÿ ğ‘¦ across tasks of different complexities and with different demonstration budgets, analyzing the trade-off between query frequency and learning performance

### Open Question 4
- Question: How does EARLY's trajectory-based uncertainty measurement compare to state-based uncertainty measurements in terms of identifying truly beneficial demonstrations?
- Basis in paper: [explicit] The paper states "Different from [3], we choose to query the most uncertain feature point" and uses trajectory-level uncertainty rather than state-level uncertainty
- Why unresolved: While the paper demonstrates EARLY outperforms I-ARLD (which uses state-based uncertainty), it doesn't directly compare the effectiveness of trajectory-based vs state-based uncertainty measurements in identifying which demonstrations are most beneficial
- What evidence would resolve it: Ablation studies where EARLY's trajectory-based uncertainty is replaced with state-based uncertainty while keeping all other components constant, measuring the difference in learning performance and demonstration efficiency

## Limitations
- The temporal difference error-based uncertainty measurement may not reliably indicate policy uncertainty when value function approximation introduces systematic errors
- The effectiveness of using initial states as features in trajectory-based querying has not been validated across diverse environment types
- The study uses only three navigation tasks and a small pilot user study (N=18), limiting generalizability to other domains

## Confidence
- Mechanism 1 (TD-based uncertainty): Medium - The theoretical foundation is sound but assumes a strong correlation between TD errors and policy uncertainty that may not hold in all cases
- Mechanism 2 (Episodic demonstrations): High - The contextual benefits of complete trajectories are well-established, though the specific implementation details need more validation
- Mechanism 3 (Adaptive threshold): Medium - The approach is reasonable but depends heavily on the assumption that recent uncertainty patterns predict current needs

## Next Checks
1. Test the TD error uncertainty measurement on environments with known approximation errors to determine when it fails to correlate with true policy uncertainty
2. Evaluate performance using alternative feature representations beyond initial states (e.g., mid-trajectory states, feature combinations) to assess robustness
3. Conduct a larger-scale user study with diverse tasks and multiple demonstrator skill levels to validate the human teaching benefits across different scenarios