---
ver: rpa2
title: 'Modular Growth of Hierarchical Networks: Efficient, General, and Robust Curriculum
  Learning'
arxiv_id: '2406.06262'
source_url: https://arxiv.org/abs/2406.06262
tags:
- networks
- modular
- network
- recurrent
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a modular RNN architecture trained on progressively
  complex memory tasks using a curriculum learning approach. The key innovation is
  iteratively growing the network by adding small RNN modules and dedicated readout
  heads for each new task complexity level.
---

# Modular Growth of Hierarchical Networks: Efficient, General, and Robust Curriculum Learning

## Quick Facts
- arXiv ID: 2406.06262
- Source URL: https://arxiv.org/abs/2406.06262
- Reference count: 11
- Modular RNN architecture trained on progressively complex memory tasks achieves better performance, faster training, improved generalization, and greater robustness than non-modular RNNs

## Executive Summary
This paper introduces a modular RNN architecture trained on progressively complex memory tasks using a curriculum learning approach. The key innovation is iteratively growing the network by adding small RNN modules and dedicated readout heads for each new task complexity level. Compared to non-modular RNNs of equivalent parameter count, the modular architecture achieves better performance (higher task difficulty solved), faster training (solving new tasks within a single epoch), improved generalization, and greater robustness to connectivity perturbations. Analysis reveals that feedforward connections between modules are more sensitive to perturbations and more task-specific than recurrent connections within modules, which remain relatively conserved across tasks.

## Method Summary
The method involves a modular RNN architecture trained on N-parity tasks of increasing complexity (N=2,3,4,...). At each curriculum step, a new RNN module is added with feedforward connections from the previous module and a dedicated readout head. The network is trained using backpropagation through time with SGD optimizer and cross-entropy loss. Key features include weight duplication from previous modules to initialize new modules, freezing of all connections except those in the most recently added module, and maintaining performance on all previously learned tasks while learning new ones. The approach leverages the functional conservation of recurrent connections across tasks while allowing feedforward connections to specialize.

## Key Results
- Modular architecture solves tasks with fewer parameters than non-modular networks
- Modular networks train faster, solving new tasks within a single epoch
- Modular networks show better generalization to more complex tasks
- Modular networks are more robust to connectivity perturbations than non-modular networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular growth allows each new task complexity level to be solved with minimal retraining of the entire network.
- Mechanism: When a new module is added, it starts with duplicated weights from the previous module, then only the new module's weights are trained. This exploits the functional conservation of recurrent connections across tasks while allowing feedforward connections to specialize.
- Core assumption: Recurrent connections within modules maintain functional relevance across tasks and can be reused without significant retraining.
- Evidence anchors:
  - [abstract] "We then demonstrate that the inductive bias introduced by the modular topology is strong enough for the network to perform well even when the connectivity within modules is fixed and only the connections between modules are trained."
  - [section] "At each curriculum step, we freeze all network connections for all modules except the last and train only the recurrent and feedforward connections (input from sequence and input from previous module) of the last module."
- Break condition: If recurrent connections become task-specific rather than generic functional units, duplication would fail to provide the claimed advantage.

### Mechanism 2
- Claim: Modular architecture provides better generalization from trained tasks to more complex tasks.
- Mechanism: Each module becomes specialized for a specific task complexity level, but the feedforward connections between modules create a compositional pathway that enables transfer of learned representations to more complex tasks.
- Core assumption: The modular decomposition of the problem space aligns with the underlying structure of the task complexity progression.
- Evidence anchors:
  - [abstract] "modular architecture can solve a task with fewer parameters than a non-modular network."
  - [section] "We also explore the impact of a limited budget for learnable parameters as an alternative evaluation of the cost-benefit trade-off between two architectures."
- Break condition: If task complexity progression doesn't align with modular decomposition, the compositional pathway would not generalize effectively.

### Mechanism 3
- Claim: Modular networks are more robust to connectivity perturbations than non-modular networks.
- Mechanism: The modular structure isolates perturbations to specific modules and their immediate feedforward connections, limiting the cascade of errors through the network. Recurrent connections within modules are more conserved and thus more stable.
- Core assumption: The isolation provided by modular boundaries limits error propagation compared to a fully interconnected network.
- Evidence anchors:
  - [abstract] "modular networks are more robust against connectivity perturbations (Fig. 3a)."
  - [section] "Figure 3 shows the degradation of performance as a function of the size of the perturbation on the connections... Modular networks not only have better initial performance but are also more robust against connectivity perturbations."
- Break condition: If perturbations affect multiple modules simultaneously or if error propagation mechanisms bypass modular boundaries, the robustness advantage would diminish.

## Foundational Learning

- Concept: Curriculum learning - progressively increasing task difficulty during training
  - Why needed here: The N-parity task complexity increases with N, requiring networks to build on previous learning rather than learning all complexities simultaneously
  - Quick check question: What would happen if we trained the network on all N-parity tasks simultaneously from the start?

- Concept: Backpropagation through time (BPTT) for recurrent networks
  - Why needed here: The N-parity task requires maintaining memory over sequences, which is a fundamental capability of recurrent networks trained with BPTT
  - Quick check question: How does the gradient flow differ between modular and non-modular networks during BPTT?

- Concept: Timescale estimation from neural activity
  - Why needed here: Understanding how effective timescales emerge from connectivity patterns is crucial for explaining the network's memory capabilities
  - Quick check question: How would you estimate the effective timescale of a neuron if you only had access to its activity trace?

## Architecture Onboarding

- Component map:
  - Input layer: Receives binary sequence
  - Module structure: Each module contains recurrent neurons with internal connectivity
  - Feedforward connections: Connect modules sequentially
  - Readout heads: Linear classifiers for each task complexity level
  - Output layer: Binary predictions for each N-parity task

- Critical path: Input → Module 1 → Module 2 → ... → Module N → Readout Head N → Output

- Design tradeoffs:
  - Module size vs. number of modules: Smaller modules require more modules but each has fewer parameters
  - Freezing vs. training weights: Freezing recurrent weights reduces training time but may limit adaptation
  - Duplication vs. random initialization: Duplication provides better starting points but may trap in local optima

- Failure signatures:
  - Plateauing performance: May indicate insufficient module size or need for more modules
  - Catastrophic forgetting: Suggests inadequate freezing strategy or insufficient separation between tasks
  - Sensitivity to perturbations: Could indicate over-reliance on specific weight configurations

- First 3 experiments:
  1. Compare training curves of modular vs. non-modular networks on N=2 to N=10 parity tasks
  2. Test generalization by training on N=10 and evaluating on N=11, N=12, etc.
  3. Apply connectivity perturbations to both architectures and measure performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of modular networks scale with larger numbers of modules (e.g., N > 200) and different task types beyond N-parity?
- Basis in paper: [explicit] The authors note that they were able to reach N = 200 in one network before ending the simulation, with even greater capabilities being theoretically possible given sufficient training time. However, they only tested N-parity tasks.
- Why unresolved: The paper does not explore the scalability limits of the modular architecture or its performance on other types of sequential or memory tasks.
- What evidence would resolve it: Empirical results showing the performance of modular networks on a variety of tasks (e.g., sequence prediction, language modeling) with varying numbers of modules and task complexities.

### Open Question 2
- Question: What is the impact of the number of neurons per module on the trade-off between performance and computational efficiency, and is there an optimal module size?
- Basis in paper: [inferred] The paper compares modular networks with different module sizes (Mm = 5, 10, 15, 20) but does not explicitly analyze the optimal module size for balancing performance and efficiency.
- Why unresolved: The authors observe that smaller modular networks (Mm = 5) are more sensitive to perturbations, but they do not provide a systematic analysis of the optimal module size for different task complexities.
- What evidence would resolve it: A comprehensive study varying the number of neurons per module and task complexity, quantifying the trade-off between performance and computational cost.

### Open Question 3
- Question: How does the modular architecture compare to other biologically-inspired neural network structures (e.g., hierarchical, compositional) in terms of performance, efficiency, and generalization?
- Basis in paper: [explicit] The authors mention that hierarchical structures in the cortex are associated with a gradual increase in intrinsic timescales and have been linked to the computational requirements of tasks. However, they only compare modular networks to non-modular RNNs.
- Why unresolved: The paper does not provide a direct comparison between the modular architecture and other biologically-inspired network structures that could offer similar benefits.
- What evidence would resolve it: Empirical results comparing the performance, efficiency, and generalization of modular networks to hierarchical, compositional, and other biologically-inspired network architectures on a variety of tasks.

## Limitations

- Evaluation is limited to synthetic N-parity tasks, which may not represent real-world complexity
- Performance on noisy, high-dimensional, or non-stationary data remains unverified
- Computational overhead of managing multiple modules may offset efficiency gains in some scenarios

## Confidence

- High confidence: The modular architecture's ability to solve tasks with fewer parameters than non-modular networks is well-supported by direct comparisons.
- Medium confidence: The robustness to connectivity perturbations is demonstrated, but the mechanism's effectiveness in more complex architectures needs validation.
- Medium confidence: The generalization claims are supported by initial experiments but require testing across more diverse task distributions.

## Next Checks

1. Test the modular architecture on real-world sequential learning tasks (e.g., language modeling or time series prediction) to verify scalability beyond synthetic parity problems.
2. Evaluate the impact of different weight initialization strategies for new modules to determine if duplication is optimal or if alternative approaches yield better performance.
3. Investigate the sensitivity of the modular approach to the curriculum design itself - test what happens when task complexity progression is non-linear or when tasks are presented out of order.