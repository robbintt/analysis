---
ver: rpa2
title: Sharing Parameter by Conjugation for Knowledge Graph Embeddings in Complex
  Space
arxiv_id: '2404.11809'
source_url: https://arxiv.org/abs/2404.11809
tags:
- complex
- time
- parameters
- conjugate
- compl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a parameter-sharing method for complex number
  KGE models by using conjugate parameters. The method reduces relation embedding
  memory usage by 50% while maintaining comparable performance to baseline models.
---

# Sharing Parameter by Conjugation for Knowledge Graph Embeddings in Complex Space

## Quick Facts
- **arXiv ID**: 2404.11809
- **Source URL**: https://arxiv.org/abs/2404.11809
- **Reference count**: 24
- **Primary result**: 50% reduction in relation embedding memory usage while maintaining comparable performance to baseline models

## Executive Summary
This paper introduces a parameter-sharing method for complex number knowledge graph embedding (KGE) models using conjugate parameters. The approach reduces relation embedding memory usage by 50% while maintaining competitive performance with baseline models. The authors demonstrate their method on two specific models (ComplEx and 5⋆E) and show state-of-the-art results on five benchmark datasets, with additional training time improvements of 31% for the 5⋆ϵ model.

## Method Summary
The proposed method leverages conjugate parameters to share relation embeddings in complex space KGE models. By utilizing the mathematical properties of complex conjugates, the approach effectively halves the number of unique relation parameters that need to be stored. This is achieved by defining a relation embedding as a combination of a base parameter and its conjugate, thereby reducing memory requirements while preserving the expressive power of complex number embeddings. The method is designed to be easily generalizable across various complex space KGE models.

## Key Results
- 50% reduction in relation embedding memory usage through conjugate parameter sharing
- State-of-the-art performance on five benchmark datasets with ComplEx and 5⋆E models
- 31% average reduction in training time for the 5⋆ϵ model compared to baseline
- Maintained comparable performance to baseline models despite significant parameter reduction

## Why This Works (Mechanism)
The method works by exploiting the mathematical properties of complex conjugates. In complex space KGE models, relations are typically represented as complex vectors. By defining a relation embedding as a combination of a base parameter and its conjugate, the approach effectively reduces the number of unique parameters while maintaining the same representational capacity. This is possible because complex conjugates preserve the geometric relationships necessary for scoring functions in KGE models. The conjugate parameter sharing exploits the inherent symmetry in complex space operations, allowing for efficient parameter reuse without sacrificing model expressiveness.

## Foundational Learning

**Complex number KGE models**: Knowledge graph embedding models that use complex numbers for relation and entity representations, offering richer expressiveness than real-valued embeddings due to their two-dimensional nature.
*Why needed*: Complex number embeddings can capture more nuanced relationships in knowledge graphs compared to real-valued embeddings.
*Quick check*: Verify that the scoring functions in the KGE models being used are compatible with complex arithmetic operations.

**Conjugate parameters**: Complex numbers paired with their conjugates, where the conjugate of a + bi is a - bi, preserving magnitude while reflecting across the real axis.
*Why needed*: Conjugate pairs maintain the mathematical properties required for scoring functions while enabling parameter sharing.
*Quick check*: Ensure that the conjugate operation preserves the essential geometric relationships needed for the KGE scoring functions.

**Parameter sharing in neural networks**: A technique where multiple parts of a model use the same parameters, reducing memory usage and potentially improving generalization.
*Why needed*: Parameter sharing reduces model complexity and memory footprint, which is crucial for large-scale knowledge graphs.
*Quick check*: Verify that the parameter sharing doesn't lead to loss of essential information or model capacity.

## Architecture Onboarding

**Component map**: Complex KGE model -> Conjugate parameter sharing layer -> Reduced parameter set -> Scoring function
Critical path: Relation embedding generation → Conjugate parameter application → Scoring function computation → Loss calculation
Design tradeoffs: Memory reduction vs. potential loss of unique relation representations; training time vs. model complexity
Failure signatures: Degraded performance on specific relation types; numerical instability in conjugate operations; unexpected interactions between shared parameters
First experiments: 1) Test memory reduction on a small benchmark dataset; 2) Compare performance of conjugate-sharing vs. standard complex embeddings on a single relation type; 3) Measure training time impact on a subset of the full dataset

## Open Questions the Paper Calls Out
None

## Limitations
- The practical implementation details and edge cases where conjugate parameter sharing might break down are not fully explored
- Performance gains are reported against specific baseline models, with unclear generalizability to other complex space KGE models
- Potential numerical stability issues in scenarios with highly complex or noisy knowledge graphs are not adequately addressed

## Confidence

**Memory Reduction Claims**: Medium - The theoretical basis is solid, but practical implementation details need more thorough investigation
**Performance Claims**: Medium - Results are promising but limited to specific baseline models
**Training Time Claims**: Medium - Reduction appears significant but hardware and implementation specifics could affect results

## Next Checks
1. Test the conjugate parameter sharing approach on additional complex space KGE models not included in the original study to verify generalizability
2. Conduct extensive numerical stability analysis across different knowledge graph sizes and complexity levels to identify potential edge cases
3. Perform cross-platform performance testing on different hardware configurations to validate the reported training time improvements and identify any platform-specific limitations