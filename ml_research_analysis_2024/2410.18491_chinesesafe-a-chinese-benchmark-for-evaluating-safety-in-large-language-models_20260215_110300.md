---
ver: rpa2
title: 'ChineseSafe: A Chinese Benchmark for Evaluating Safety in Large Language Models'
arxiv_id: '2410.18491'
source_url: https://arxiv.org/abs/2410.18491
tags: []
core_contribution: 'This paper introduces ChineseSafe, a comprehensive Chinese safety
  benchmark for evaluating large language models (LLMs). The benchmark contains 205,034
  examples across 4 classes and 10 sub-classes of safety issues, including three new
  categories unique to Chinese contexts: political sensitivity, pornography, and variant/homophonic
  words.'
---

# ChineseSafe: A Chinese Benchmark for Evaluating Safety in Large Language Models

## Quick Facts
- arXiv ID: 2410.18491
- Source URL: https://arxiv.org/abs/2410.18491
- Reference count: 40
- Primary result: ChineseSafe benchmark contains 205,034 examples across 4 classes and 10 sub-classes, with generation-based evaluation outperforming perplexity-based evaluation for LLM safety assessment

## Executive Summary
This paper introduces ChineseSafe, a comprehensive Chinese safety benchmark designed to evaluate large language models (LLMs) across diverse safety dimensions. The benchmark addresses the gap in existing Chinese safety evaluation tools by covering 205,034 examples across 4 main classes and 10 sub-classes of safety issues, including three categories unique to Chinese contexts: political sensitivity, pornography, and variant/homophonic words. The authors evaluated 26 LLMs using both generation-based and perplexity-based methods, finding that the generation method was more effective at detecting unsafe content. The benchmark provides a valuable tool for developers and researchers to improve LLM safety in Chinese contexts, with GPT-4 series and DeepSeek series showing superior safety performance compared to other models.

## Method Summary
The ChineseSafe benchmark was constructed through data collection from open-sourced datasets and internet resources, followed by data cleaning and deduplication to create a balanced dataset of 205,034 examples. The authors employed two evaluation methods: generation-based using the Outlines framework to generate responses and evaluate safety, and perplexity-based by selecting the label with lowest perplexity. They evaluated 26 LLMs (4 APIs and 22 open-source models) using both methods, calculating overall accuracy, precision, and recall metrics for safe and unsafe content with standard deviation across random seeds. The evaluation covered 10 safety sub-categories including physical safety, mental health, discrimination, privacy, political sensitivity, pornography, violence, drug use, cheating, and variant/homophonic words.

## Key Results
- ChineseSafe benchmark contains 205,034 examples across 4 classes and 10 sub-classes of safety issues
- Generation-based evaluation method outperformed perplexity-based method in detecting unsafe content
- GPT-4 series and DeepSeek series achieved superior safety performance compared to other evaluated models
- LLMs showed vulnerabilities in specific categories like physical health and mental health, with accuracy dropping significantly

## Why This Works (Mechanism)
ChineseSafe works effectively because it provides a comprehensive evaluation framework specifically designed for Chinese language contexts, addressing the unique safety challenges that arise in Chinese linguistic and cultural settings. The benchmark's strength lies in its balanced dataset construction and the use of generation-based evaluation, which better captures the nuanced nature of safety violations compared to simpler perplexity-based methods. By including three Chinese-specific safety categories (political sensitivity, pornography, and variant/homophonic words), the benchmark provides more relevant and contextually appropriate safety assessment for Chinese language models.

## Foundational Learning
- **Chinese language safety evaluation**: Why needed - Standard English-focused safety benchmarks don't capture Chinese-specific safety concerns; Quick check - Verify benchmark includes categories unique to Chinese contexts
- **Generation-based vs perplexity-based evaluation**: Why needed - Different evaluation methods capture different aspects of model safety; Quick check - Compare results from both methods across multiple models
- **Balanced dataset construction**: Why needed - Ensures fair evaluation across safety categories; Quick check - Verify class distribution is approximately equal in test set
- **LLM safety vulnerability analysis**: Why needed - Identifies specific areas where models need improvement; Quick check - Compare performance across different safety sub-categories
- **Multi-method evaluation framework**: Why needed - Provides comprehensive safety assessment; Quick check - Ensure both evaluation methods are properly implemented and compared

## Architecture Onboarding

**Component Map:**
ChineseSafe Dataset -> Data Preprocessing -> Balanced Test Set -> Evaluation Methods (Generation-based, Perplexity-based) -> LLM Models -> Performance Metrics

**Critical Path:**
Data collection → Data cleaning → Balanced test set creation → Generation-based evaluation → Perplexity-based evaluation → Performance comparison → Safety analysis

**Design Tradeoffs:**
The paper chose generation-based evaluation over simpler methods despite higher computational cost, as it provides more accurate safety assessment. The tradeoff between comprehensive coverage (10 safety categories) and evaluation efficiency was addressed by focusing on the most critical safety concerns for Chinese contexts.

**Failure Signatures:**
- Models showing poor performance on perplexity-based evaluation compared to generation-based, particularly for models like Llama3-ChatQA-1.5-70B
- Models exhibiting vulnerabilities in specific categories like physical health and mental health, with accuracy dropping significantly compared to overall performance
- Models failing to properly handle Chinese-specific safety categories like political sensitivity and variant/homophonic words

**First 3 Experiments to Run:**
1. Evaluate a small subset of models using both generation-based and perplexity-based methods to verify the superiority of generation-based evaluation
2. Test model performance across individual safety categories to identify specific vulnerability areas
3. Compare ChineseSafe results with existing English safety benchmarks to assess cross-linguistic safety consistency

## Open Questions the Paper Calls Out
None

## Limitations
- The paper lacks detailed implementation specifics for both evaluation methods that would enable full reproducibility
- The benchmark's generalizability to other Chinese contexts remains uncertain without broader validation
- The paper does not provide comprehensive error analysis or discussion of potential biases in the data collection process

## Confidence
- **High confidence**: Benchmark construction process and general finding that generation-based evaluation outperforms perplexity-based evaluation
- **Medium confidence**: Comparative performance rankings of different LLM families and overall methodology
- **Low confidence**: Specific numerical results and generalizability to other Chinese contexts

## Next Checks
1. Reconstruct the balanced test set sampling process using the 205,034 examples with the 0.1 ratio mentioned, ensuring proper stratification across all 10 safety categories to verify the reported evaluation results.
2. Implement both evaluation methods independently using the Outlines framework for generation-based evaluation and a standardized perplexity calculation approach to confirm the superiority of generation-based evaluation.
3. Conduct ablation studies by removing each of the three Chinese-specific safety categories (political sensitivity, pornography, and variant/homophonic words) to assess their impact on overall model performance and safety boundary identification.