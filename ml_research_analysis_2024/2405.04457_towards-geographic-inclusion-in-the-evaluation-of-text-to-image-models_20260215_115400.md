---
ver: rpa2
title: Towards Geographic Inclusion in the Evaluation of Text-to-Image Models
arxiv_id: '2405.04457'
source_url: https://arxiv.org/abs/2405.04457
tags:
- images
- image
- annotators
- object
- geographic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study finds that human perceptions of geographic representation,
  visual appeal, and object consistency in text-to-image models vary notably across
  geographic locations. Annotators in Africa, Europe, and Southeast Asia often disagree
  on whether stereotypical or exaggerated depictions are geographically representative.
---

# Towards Geographic Inclusion in the Evaluation of Text-to-Image Models

## Quick Facts
- **arXiv ID:** 2405.04457
- **Source URL:** https://arxiv.org/abs/2405.04457
- **Reference count:** 40
- **Primary result:** Human perceptions of geographic representation, visual appeal, and object consistency in text-to-image models vary notably across geographic locations, with automatic metrics failing to fully capture this variation.

## Executive Summary
This study examines how human perceptions of geographic representation, visual appeal, and object consistency in text-to-image models vary across geographic locations. The research reveals significant differences between in-region and out-of-region annotators, with out-of-region annotators more likely to favor stereotypical depictions. The study evaluates multiple automatic metrics and feature extractors, finding that CLIP and DINOv2 align better with human similarity judgments than Inceptionv3. The authors recommend collecting annotations from both in-region and out-of-region annotators, using modern feature extractors, and clarifying evaluation criteria definitions to improve both human and automatic evaluations.

## Method Summary
The study collected 65,000 image annotations and 20 survey responses from annotators in Africa, Europe, and Southeast Asia. Researchers used real images from the GeoDE dataset and generated images from DM w/ CLIP and LDM 2.1 models for six objects (bag, car, cooking pot, dog, plate of food, storefront) across three regions. Two annotation tasks were employed: Task 1 involved image comparison and object consistency, while Task 2 focused on geographic representation and object consistency. Annotators selected similar objects, visually appealing images, and indicated geographic representation. The study compared human judgments against automatic metrics including Region-CLIPScore, precision, recall, and various feature extractors.

## Key Results
- Annotators in Africa, Europe, and Southeast Asia often disagree on whether stereotypical or exaggerated depictions are geographically representative
- Out-of-region annotators tend to favor stereotypical imagery more than in-region annotators
- Automatic metrics like Region-CLIPScore and precision do not fully capture cross-annotator variation and may favor stereotypical representations over realistic ones
- CLIP and DINOv2 feature extractors align better with human similarity judgments than Inceptionv3

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Geographic representation evaluation depends on annotator location and whether they are inside or outside the region of interest.
- Mechanism: Annotators use different visual cues—out-of-region annotators focus on exaggerated, stereotypical identifiers while in-region annotators favor realistic, subtle, everyday features.
- Core assumption: Visual cues used to judge geographic representation are culturally conditioned and not universal.
- Evidence anchors:
  - [abstract] "Annotators in Africa, Europe, and Southeast Asia often disagree on whether stereotypical or exaggerated depictions are geographically representative."
  - [section 3.1.1] "In-region annotators often consider objects and backgrounds with few geographically-distinguishing features as representative more than out-of-region annotators."
- Break condition: If evaluation instructions explicitly require use of specific definitions (e.g., "use local expert definitions"), geographic cue bias would be reduced.

### Mechanism 2
- Claim: Automatic metrics like Region-CLIPScore fail to capture cross-annotator variation in geographic representation.
- Mechanism: CLIPScore measures cosine similarity to a region-specific text embedding, but these embeddings are trained on web data that may over-represent stereotypes, causing the metric to favor exaggerated depictions.
- Core assumption: The training data for CLIP contains more stereotypical than realistic depictions of certain regions.
- Evidence anchors:
  - [abstract] "Automatic metrics like Region-CLIPScore and precision do not fully capture this variation and may favor stereotypical representations over realistic ones."
  - [section 3.1.3] "Region-CLIPScore has a larger disparity in its approximation of geographic representation across annotator locations when used for identifying Southeast Asia representation than Africa and Europe."
- Break condition: If the reference embeddings are constructed from balanced, regionally diverse datasets rather than stereotypical web content.

### Mechanism 3
- Claim: Feature extractor choice matters: CLIP and DINOv2 better align with human similarity judgments than Inceptionv3.
- Mechanism: CLIP and DINOv2 are trained on larger, more diverse datasets and capture semantic object similarity more reliably, while Inceptionv3 relies on narrower ImageNet-based features that can conflate unrelated visual patterns.
- Core assumption: The feature space of a model trained on a broader dataset more closely matches human perception of similarity.
- Evidence anchors:
  - [abstract] "CLIP and DINOv2 feature extractors align better with human similarity judgments than Inceptionv3."
  - [section 3.1.4] "Both DINOv2 and CLIP features correlate significantly better with human judgement than InceptionV3, with DINOv2 features are slightly more associated with human judgment than CLIP features at low distance values."
- Break condition: If human annotators rely heavily on non-semantic features (e.g., color palette) that Inceptionv3 happens to encode well.

## Foundational Learning

- Concept: Cross-cultural perception of geographic representation
  - Why needed here: The paper hinges on understanding that "realistic" vs "stereotypical" imagery is interpreted differently by in-region vs out-of-region annotators.
  - Quick check question: If an annotator from outside Africa sees a rural village scene with mud huts, will they more likely rate it as "representative" than an in-region annotator?

- Concept: Semantic similarity vs. perceptual similarity in feature spaces
  - Why needed here: The paper contrasts how different feature extractors (Inceptionv3, CLIP, DINOv2) align with human similarity judgments.
  - Quick check question: If CLIP and DINOv2 correlate better with human similarity ratings, what does that say about the semantic breadth of their training data compared to Inceptionv3?

- Concept: Bias amplification in automated metrics
  - Why needed here: The paper shows that metrics like Region-CLIPScore can inadvertently reinforce stereotypes by favoring exaggerated depictions.
  - Quick check question: If a metric scores a stereotypical depiction higher than a realistic one, what aspect of its design is likely causing that bias?

## Architecture Onboarding

- Component map: Image generation -> Triplet construction (distance sampling) -> Annotator assignment (one per region per job) -> Quality filtering -> Statistical analysis -> Recommendation drafting
- Critical path: Image generation → Triplet construction → Annotator assignment → Quality filtering → Statistical analysis → Recommendation drafting
- Design tradeoffs: (1) Using real images as ground truth for object consistency vs. allowing subjective geographic interpretation; (2) Balancing cost of multi-annotator coverage vs. single-annotator speed; (3) Choosing older Inceptionv3 for comparability vs. newer CLIP/DINOv2 for better human alignment
- Failure signatures: (1) Low annotator agreement on object presence → likely ambiguous image or unclear instructions; (2) CLIPScore variance unexplained by annotator location → metric may not capture regional nuance; (3) High agreement on similarity but low on appeal → appeal may depend on unstated cultural aesthetics
- First 3 experiments:
  1. Rerun similarity judgments using only CLIP and DINOv2 features to confirm improved alignment without Inceptionv3 confounding
  2. Collect additional in-region annotators for a subset of objects to measure intra-region variability
  3. Construct a "balanced" reference dataset (equal stereotypical vs realistic images) and test whether Region-CLIPScore ranking changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do intra-region variations in object definitions and interpretations affect the reliability of geographic representation annotations?
- Basis in paper: [explicit] The paper discusses how in-region annotators can disagree about objects in real images collected from people living in that region, suggesting possible intra-region variations in the definition of objects.
- Why unresolved: The study primarily focuses on inter-region differences, and while it acknowledges intra-region variations, it does not deeply explore how these variations impact annotation reliability or how to account for them.
- What evidence would resolve it: Collecting annotations from multiple annotators within the same region and analyzing the consistency of their responses for the same images would provide evidence of intra-region variations and their impact on reliability.

### Open Question 2
- Question: To what extent do annotators' personal experiences and external resources introduce biases that affect their perception of geographic representation in images?
- Basis in paper: [explicit] The paper notes that some annotators reported using external references like Google Images, which can introduce additional biases from online web data, and that annotators frequently refer to common stereotypes when explaining their judgments.
- Why unresolved: While the paper acknowledges the role of personal experience and external resources, it does not quantify the extent of their influence or explore how these factors might skew annotations.
- What evidence would resolve it: Conducting a controlled study where annotators are explicitly instructed to either use or avoid external resources, and analyzing the differences in their annotations, would help quantify the impact of these factors.

### Open Question 3
- Question: How do the interpretations of visual appeal vary across different demographic groups within the same geographic region, and how does this impact the evaluation of text-to-image models?
- Basis in paper: [inferred] The paper discusses variations in visual appeal perceptions across geographic locations but does not delve into variations within the same region based on demographic factors such as age, gender, or socioeconomic status.
- Why unresolved: The study focuses on geographic location as a primary factor but does not explore how demographic subgroups within the same region might interpret visual appeal differently.
- What evidence would resolve it: Collecting annotations from a diverse set of annotators within the same region, categorized by demographic factors, and analyzing the consistency of their responses for visual appeal would provide insights into intra-region demographic variations.

## Limitations
- The study relies on single-annotator-per-region design, which limits our ability to quantify intra-regional variability in geographic perception
- The mechanism of why out-of-region annotators favor stereotypical depictions more than in-region annotators is inferred from observed patterns but not explicitly tested
- Automatic metric limitations are identified but not systematically tested with alternative training data or reference sets

## Confidence

- High confidence: The observed geographic variation in human annotation patterns is robust and well-documented
- Medium confidence: The mechanism explaining why out-of-region annotators prefer stereotypical depictions (cultural conditioning of visual cues)
- Medium confidence: The finding that automatic metrics like Region-CLIPScore may favor stereotypical representations due to training data bias
- Medium confidence: The superior alignment of CLIP and DINOv2 with human similarity judgments compared to Inceptionv3

## Next Checks

1. Conduct within-region annotation variability study by having multiple annotators from the same region evaluate the same images to establish baseline intra-regional agreement
2. Test the stereotype bias hypothesis by constructing a balanced reference dataset with equal stereotypical and realistic depictions and measuring how automatic metrics rank them
3. Run a controlled experiment comparing annotation patterns when explicit definitions of "geographic representation" are provided versus when they are left open to interpretation