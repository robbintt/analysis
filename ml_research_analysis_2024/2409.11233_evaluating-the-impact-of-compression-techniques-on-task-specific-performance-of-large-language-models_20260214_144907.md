---
ver: rpa2
title: Evaluating the Impact of Compression Techniques on Task-Specific Performance
  of Large Language Models
arxiv_id: '2409.11233'
source_url: https://arxiv.org/abs/2409.11233
tags:
- performance
- compression
- divergence
- perplexity
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of popular compression
  techniques (Magnitude Pruning, SparseGPT, and Wanda) on the LLaMA-2-7B model, focusing
  on the trade-offs between model size reduction and downstream task performance.
  While SparseGPT and Wanda preserve perplexity at 50% sparsity, they suffer significant
  degradation on downstream tasks, revealing the inadequacy of perplexity as the sole
  evaluation metric.
---

# Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models

## Quick Facts
- arXiv ID: 2409.11233
- Source URL: https://arxiv.org/abs/2409.11233
- Reference count: 35
- This study investigates compression techniques (Magnitude Pruning, SparseGPT, Wanda) on LLaMA-2-7B, revealing perplexity's inadequacy as a sole evaluation metric for downstream task performance.

## Executive Summary
This paper examines how popular compression techniques affect the task-specific performance of large language models. The authors compress LLaMA-2-7B using Magnitude Pruning, SparseGPT, and Wanda at 50% sparsity, evaluating both traditional metrics like perplexity and downstream task performance. Their key finding is that while SparseGPT and Wanda maintain perplexity scores even at high compression levels, they suffer significant degradation on downstream tasks. To address this limitation, they introduce Jensen-Shannon Divergence as a more comprehensive metric that better captures the nuanced changes in model behavior post-compression. The study also demonstrates that task-specific calibration data significantly enhances the downstream performance of compressed models compared to general calibration data.

## Method Summary
The researchers compress LLaMA-2-7B using three techniques (Magnitude Pruning, SparseGPT, Wanda) at 50% sparsity, calibrating with 128 random samples from the C4 dataset. They evaluate compressed models using perplexity, JS Divergence, and downstream task metrics (EM, F1, ROUGE-1) on the Unnatural dataset. Performance is compared against GPT-4 and GPT-4o evaluations to validate JS Divergence as a comprehensive metric. The study also examines the impact of calibration data by comparing models calibrated with general C4 data versus task-specific Alpaca data.

## Key Results
- SparseGPT and Wanda preserve perplexity at 50% sparsity but show significant degradation on downstream tasks
- JS Divergence better captures the impact of compression on model behavior compared to perplexity alone
- Task-specific calibration data (Alpaca) significantly improves downstream performance compared to general calibration data (C4)
- Magnitude Pruning at 50% sparsity preserves downstream performance more effectively than SparseGPT and Wanda

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity is an insufficient metric for evaluating compressed LLM performance on downstream tasks because it only measures next-token prediction confidence, not task-specific output quality.
- Mechanism: Perplexity evaluates how well a model predicts the next token in a sequence, but this does not capture the nuanced changes in model behavior that affect specific downstream tasks like instruction-following or code generation.
- Core assumption: Task-specific performance depends on more than just the model's ability to predict the next token; it requires preserving the model's capability to generate contextually appropriate and accurate outputs for specific tasks.
- Evidence anchors:
  - [abstract] "Our findings reveal that while SparseGPT and Wanda preserve perplexity even at 50% sparsity, they suffer significant degradation on downstream tasks, highlighting the inadequacy of perplexity as the sole evaluation metric."
  - [section] "This indicates that Perplexity does not provide a clear picture of the true impact of compression on the model's usefulness for specific tasks [7]."
- Break condition: If downstream task performance metrics (like EM, F1, ROUGE-1) do not correlate with JS Divergence but do correlate with perplexity, this mechanism would be invalid.

### Mechanism 2
- Claim: Jensen-Shannon (JS) Divergence is a more comprehensive metric for evaluating LLM compression because it quantifies the overall similarity between the output distributions of the original and compressed models.
- Mechanism: JS Divergence measures the symmetrized and bounded version of KL Divergence between the probability distributions of outputs from the base model and compressed models, capturing nuanced changes in the entire output distribution rather than just next-token prediction.
- Core assumption: The similarity between output distributions of the original and compressed models is a good indicator of how well the compressed model preserves the original model's capabilities on downstream tasks.
- Evidence anchors:
  - [abstract] "To address this, we introduce Jensen-Shannon (JS) Divergence as a more comprehensive metric that captures nuanced changes in model behavior post-compression."
  - [section] "JS Divergence more effectively captures the impact of compression and may serve as a superior metric for evaluating a compressed model's support for downstream tasks."
- Break condition: If JS Divergence does not correlate with GPT-4-based evaluations or downstream task performance metrics, this mechanism would be invalid.

### Mechanism 3
- Claim: Task-specific calibration data significantly enhances the downstream performance of compressed models compared to general calibration data.
- Mechanism: Calibration data is used during the compression process to guide which parameters to prune or update. Using task-specific calibration data (like the Alpaca dataset) ensures that the compression process preserves the model's capabilities on specific downstream tasks, whereas general calibration data (like C4) may not.
- Core assumption: The characteristics of the calibration data influence which parameters are considered important during compression, and task-specific data ensures that parameters important for specific tasks are preserved.
- Evidence anchors:
  - [abstract] "We further demonstrate that task-specific calibration data significantly enhances the downstream performance of compressed models compared to general calibration data."
  - [section] "Our experiments reveal that calibration data significantly influences the effectiveness of model compression, with models calibrated using the Alpaca dataset generally outperforming those calibrated with C4 across various metrics."
- Break condition: If models compressed with task-specific and general calibration data show no significant difference in downstream task performance, this mechanism would be invalid.

## Foundational Learning

- Concept: Probability distributions and KL Divergence
  - Why needed here: JS Divergence is based on KL Divergence, which measures the difference between two probability distributions. Understanding this is crucial for grasping why JS Divergence is a suitable metric for comparing model outputs.
  - Quick check question: What is the main difference between KL Divergence and JS Divergence, and why is this difference important for comparing model outputs?

- Concept: Model compression techniques (pruning, quantization, knowledge distillation)
  - Why needed here: The paper evaluates different compression techniques (Magnitude Pruning, SparseGPT, Wanda) and their effects on model performance. Understanding these techniques is essential for interpreting the results and implications of the study.
  - Quick check question: How does Magnitude Pruning differ from SparseGPT and Wanda in terms of the compression process and the role of calibration data?

- Concept: Evaluation metrics for NLP tasks (EM, F1, ROUGE)
  - Why needed here: The paper uses Exact Match (EM), F1 Score, and ROUGE-1 to evaluate downstream task performance. Understanding these metrics is necessary to interpret the results and assess the impact of compression on specific tasks.
  - Quick check question: What is the key difference between EM and F1 Score in the context of evaluating instruction-following tasks?

## Architecture Onboarding

- Component map: LLaMA-2-7B -> Compression Techniques (Magnitude Pruning, SparseGPT, Wanda) -> Calibration Data (C4, Alpaca) -> Evaluation (Perplexity, JS Divergence, EM, F1, ROUGE-1, GPT-4)

- Critical path: 1) Select base model and compression techniques, 2) Prepare calibration and evaluation datasets, 3) Compress base model using different techniques and calibration data, 4) Evaluate compressed models using various metrics, 5) Compare results to assess effectiveness

- Design tradeoffs:
  - Compression ratio vs. performance: Higher compression ratios may lead to better efficiency but worse performance on downstream tasks
  - General vs. task-specific calibration data: General calibration data may lead to better preservation of general capabilities, while task-specific data may better preserve task-specific performance
  - Evaluation metrics: Using a single metric (like perplexity) may not capture the full impact of compression, while using multiple metrics (like JS Divergence and task-specific metrics) provides a more comprehensive evaluation but increases complexity

- Failure signatures:
  - Low perplexity but poor downstream task performance: Indicates that perplexity is not a sufficient metric for evaluating compression effectiveness
  - High JS Divergence but good downstream task performance: Suggests that JS Divergence may not always be a reliable indicator of task-specific performance
  - No significant difference between models compressed with general and task-specific calibration data: Implies that the choice of calibration data may not be as important as expected

- First 3 experiments:
  1. Compress the base model using Magnitude Pruning at different sparsity levels and evaluate using perplexity, JS Divergence, and downstream task metrics
  2. Compress the base model using SparseGPT and Wanda with general (C4) and task-specific (Alpaca) calibration data, then evaluate using the same metrics as in experiment 1
  3. Compare the results of experiments 1 and 2 to assess the effectiveness of different compression techniques and the role of calibration data in preserving downstream task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of fine-tuning with compression techniques impact the overall performance and complexity of large language models on downstream tasks?
- Basis in paper: Explicit - The authors propose investigating how fine-tuning can be effectively combined with compression methods to enhance task-specific performance while maintaining model efficiency.
- Why unresolved: The paper discusses the potential benefits of integrating fine-tuning with compression but does not provide empirical evidence or results from such experiments.
- What evidence would resolve it: Conducting experiments that compare the performance of models that are both compressed and fine-tuned versus those that are only compressed or only fine-tuned, using the same evaluation metrics (EM, F1, ROUGE-1, and JS Divergence) and calibration data (C4 and Alpaca datasets).

### Open Question 2
- Question: What is the optimal balance between model sparsity and performance for different compression techniques when evaluated on various downstream tasks?
- Basis in paper: Explicit - The paper evaluates the performance of LLaMA-2-7B compressed with Magnitude Pruning, SparseGPT, and Wanda at 50% sparsity, but does not explore the effects of different sparsity levels.
- Why unresolved: The study focuses on a fixed sparsity level (50%) and does not investigate how varying the sparsity impacts model performance across different tasks.
- What evidence would resolve it: Conducting a systematic study that evaluates the performance of compressed models at multiple sparsity levels (e.g., 10%, 30%, 50%, 70%) on various downstream tasks, using the same evaluation metrics and datasets.

### Open Question 3
- Question: How does the choice of evaluation metrics influence the perceived effectiveness of compression techniques on large language models?
- Basis in paper: Explicit - The paper highlights the limitations of using perplexity as the sole evaluation metric and introduces JS Divergence as a more comprehensive alternative.
- Why unresolved: While the paper compares JS Divergence with perplexity and GPT-4 evaluations, it does not explore how other potential metrics might influence the assessment of compression techniques.
- What evidence would resolve it: Conducting a comparative study that evaluates the same compressed models using a diverse set of metrics (e.g., human evaluations, task-specific benchmarks, and other divergence measures) to determine which metrics best capture the true impact of compression on model performance.

## Limitations

- The study's findings are based on a single model architecture (LLaMA-2-7B) and may not generalize to other base models or compression techniques
- The evaluation focuses on a fixed 50% sparsity level, leaving questions about performance at other compression ratios
- The calibration data findings are based on a single task (Alpaca instruction-following) and may not generalize to other downstream applications

## Confidence

- High Confidence: The inadequacy of perplexity as a sole evaluation metric (supported by clear degradation in downstream tasks despite stable perplexity scores)
- Medium Confidence: The superiority of JS Divergence as a comprehensive metric (supported by correlation with GPT-4 evaluations but limited to single model/task)
- Medium Confidence: The importance of task-specific calibration data (demonstrated through comparative results but with potential dataset-specific effects)

## Next Checks

1. **Cross-model validation**: Test whether JS Divergence correlates with downstream task performance across different base models (e.g., LLaMA-1, Mistral) and compression techniques not examined in this study.

2. **Calibration data generalization**: Evaluate the impact of task-specific calibration data on diverse downstream tasks beyond instruction-following, including code generation, reasoning, and multilingual tasks.

3. **Compression ratio sensitivity**: Systematically examine the relationship between compression ratio (10%-90% sparsity) and the effectiveness of different evaluation metrics, particularly JS Divergence's sensitivity across the compression spectrum.