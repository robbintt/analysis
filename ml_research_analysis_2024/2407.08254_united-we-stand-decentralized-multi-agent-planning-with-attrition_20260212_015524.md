---
ver: rpa2
title: 'United We Stand: Decentralized Multi-Agent Planning With Attrition'
arxiv_id: '2407.08254'
source_url: https://arxiv.org/abs/2407.08254
tags:
- agents
- agent
- utility
- action
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of decentralized multi-agent
  planning in the presence of agent attrition (failures) during information-gathering
  missions. The authors propose Attritable MCTS (A-MCTS), a novel algorithm that combines
  Monte Carlo Tree Search with regret matching to adapt quickly to agent failures
  while maintaining efficient coordination.
---

# United We Stand: Decentralized Multi-Agent Planning With Attrition

## Quick Facts
- arXiv ID: 2407.08254
- Source URL: https://arxiv.org/abs/2407.08254
- Authors: Nhat Nguyen; Duong Nguyen; Gianluca Rizzo; Hung Nguyen
- Reference count: 40
- One-line primary result: Novel algorithm combining MCTS with regret matching achieves superior performance in decentralized multi-agent planning under agent attrition

## Executive Summary
This paper addresses the critical challenge of decentralized multi-agent planning in environments where agents can fail during mission execution. The authors propose Attritable MCTS (A-MCTS), a novel algorithm that integrates Monte Carlo Tree Search with regret matching to enable agents to adapt quickly to failures while maintaining efficient coordination. The approach is particularly relevant for information-gathering missions in hostile environments where communication is restricted and agent attrition is likely.

## Method Summary
The method combines Monte Carlo Tree Search with regret matching to handle agent attrition in decentralized multi-agent planning. Agents use global reward functions for local utility estimation, enabling immediate recognition of global reward reductions when failures occur. Regret matching coordinates actions between agents, guaranteeing convergence to a Nash equilibrium. The algorithm adapts to agent failures by updating plans based on observed global utility changes while maintaining coordination through the regret matching mechanism.

## Key Results
- A-MCTS significantly outperforms existing approaches (Dec-MCTS, Dec-MCTS-Reset, Dec-MCTS-Global, and Greedy-MCTS) in terms of global utility and scalability
- Maintains superior performance even with up to 50% communication disruption in hostile environments
- Theoretical proof shows convergence to pure-strategy Nash equilibrium under submodular utility functions
- Demonstrates robust adaptation to agent failures while maintaining efficient coordination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using global reward functions for estimating local contributions enables agents to immediately recognize global reward reductions when failures occur.
- Mechanism: Each agent optimizes its local actions using the global utility Ug directly instead of marginal contribution utility Un. This allows agents to detect utility drops when others fail.
- Core assumption: Submodular utility functions ensure that local utility changes reflect global changes.
- Evidence anchors:
  - [abstract]: "Using global reward functions for estimating local contributions, enabling agents to immediately recognize global reward reductions when failures occur."
  - [section 4.1]: "The key idea of our proposed algorithm is to have the search trees of every agent be guided by the same utility of the joint action sequences."
  - [corpus]: Weak evidence - the corpus papers focus on resilience and safety rather than utility function design.
- Break condition: If the utility function is not submodular, local utility changes may not accurately reflect global changes.

### Mechanism 2
- Claim: Regret matching coordinates actions between agents and guarantees convergence to a Nash equilibrium.
- Mechanism: Agents independently construct matrix games using shared action sequences and apply regret matching to compute best response joint decisions.
- Core assumption: The game has a pure-strategy Nash equilibrium that can be approximated through self-play.
- Evidence anchors:
  - [abstract]: "Employing regret matching to coordinate actions between agents, guaranteeing convergence to a Nash equilibrium."
  - [section 4.2]: "We employ a computational-effective game-based technique to coordinate agents, enabling adaptive decision-making in the presence of peer failures while ensuring fast convergence."
  - [corpus]: Weak evidence - the corpus papers discuss related multi-agent planning but not regret matching specifically.
- Break condition: If the game is not a finite coordination game or the utility functions are not properly aligned, regret matching may not converge to a Nash equilibrium.

### Mechanism 3
- Claim: Combining Monte Carlo Tree Search with regret matching enables efficient adaptation to agent failures while maintaining coordination.
- Mechanism: A-MCTS uses global utility to guide MCTS tree growth and regret matching for coordination, allowing agents to adapt plans when failures occur.
- Core assumption: Agents can effectively communicate and share action sequences to coordinate their plans.
- Evidence anchors:
  - [abstract]: "We propose Attritable MCTS (A-MCTS), a novel algorithm that combines Monte Carlo Tree Search with regret matching to adapt quickly to agent failures while maintaining efficient coordination."
  - [section 4.1]: "Our approach is based on MCTS and the use of the global reward instead of the local one in the estimation of each agent's local contribution."
  - [corpus]: Weak evidence - the corpus papers focus on different aspects of multi-agent systems rather than combining MCTS with regret matching.
- Break condition: If communication between agents is severely disrupted or the environment is highly dynamic, the combination of MCTS and regret matching may not be sufficient.

## Foundational Learning

- Concept: Submodular functions
  - Why needed here: The convergence proof relies on the diminishing returns property of submodular functions.
  - Quick check question: What is the defining property of a submodular function?

- Concept: Nash equilibrium
  - Why needed here: The algorithm guarantees convergence to a Nash equilibrium for coordination.
  - Quick check question: What conditions must be met for a joint action profile to be a Nash equilibrium?

- Concept: Monte Carlo Tree Search
  - Why needed here: MCTS is the core search algorithm used for planning in the decentralized setting.
  - Quick check question: What are the four main steps in the MCTS algorithm?

## Architecture Onboarding

- Component map:
  - MCTS tree for each agent
  - Regret matching coordination module
  - Communication interface for sharing action sequences
  - Utility function module (global reward)

- Critical path:
  1. Initialize MCTS trees
  2. Grow trees using global utility and best response policy
  3. Periodically compute best response joint policy using regret matching
  4. Communicate and synchronize best response policies
  5. Execute first action and observe changes
  6. Repeat until budget expires

- Design tradeoffs:
  - Global vs. local utility: Using global utility allows immediate detection of failures but may introduce variance due to sampling.
  - Communication frequency: More frequent communication can improve coordination but increases computational overhead.
  - Planning horizon: Longer horizons may lead to better plans but increase computational complexity.

- Failure signatures:
  - High variance in global utility estimates due to sampling other agents' actions
  - Slow convergence to optimal policies when using marginal contribution utility
  - Poor adaptation to agent failures when using local utility functions

- First 3 experiments:
  1. Test A-MCTS performance with no agent failures to establish baseline
  2. Introduce agent failures at different stages of the mission and measure adaptation
  3. Vary the communication failure probability and observe impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of A-MCTS compare to other methods when agent failures are not uniformly distributed throughout the mission, but instead follow a non-uniform or time-varying failure distribution?
- Basis in paper: [inferred] The paper mentions that the time of agent failures is "distributed uniformly at random throughout the mission duration" in the experimental evaluation. It would be valuable to know how A-MCTS performs under more realistic, non-uniform failure patterns.
- Why unresolved: The current experimental setup assumes a uniform failure distribution, which may not reflect real-world scenarios where failures are more likely to occur at certain times or under specific conditions.
- What evidence would resolve it: Conducting experiments with various non-uniform failure distributions (e.g., increasing failure rate over time, failures clustered in certain mission phases) and comparing the performance of A-MCTS and other methods under these conditions.

### Open Question 2
- Question: What is the impact of communication delays or asynchrony on the performance of A-MCTS, and how can the algorithm be adapted to handle such communication imperfections?
- Basis in paper: [explicit] The paper states that agents can exchange information instantaneously and reliably, but acknowledges in the introduction that communication may be restricted in hostile environments. It also mentions in Section 5.2 that inter-agent communication can be unreliable and intermittent in practical settings.
- Why unresolved: The current implementation assumes perfect and instantaneous communication, which is not always the case in real-world scenarios. Understanding how communication delays affect performance and developing strategies to mitigate their impact is crucial for practical deployment.
- What evidence would resolve it: Simulating scenarios with various communication delay models (e.g., constant delays, random delays, message loss) and evaluating the performance of A-MCTS under these conditions. Developing and testing modifications to the algorithm to handle communication delays, such as incorporating communication delay predictions or adjusting the regret matching procedure.

### Open Question 3
- Question: How does the performance of A-MCTS scale with the size of the state space and the number of agents, and what are the limitations of the algorithm in terms of computational complexity and communication overhead?
- Basis in paper: [inferred] The paper mentions that the cardinality of the action space can be very large and grows exponentially, and that the number of paths exchanged between agents influences system complexity. It also evaluates the impact of the number of agents on performance.
- Why unresolved: While the paper provides some insights into scalability, a more comprehensive analysis of the algorithm's performance as the state space and number of agents grow is needed to understand its limitations and potential bottlenecks.
- What evidence would resolve it: Conducting experiments with increasingly large state spaces and numbers of agents, measuring the computational time and communication overhead of A-MCTS. Analyzing the relationship between these factors and the algorithm's performance, and identifying the points at which scalability becomes a significant issue.

## Limitations

- Theoretical convergence proof assumes submodular utility functions, which may not hold in all real-world scenarios
- Experimental validation focuses primarily on underwater sensor network applications, limiting generalizability to other domains
- Does not extensively address computational overhead introduced by regret matching coordination in large action spaces

## Confidence

- High confidence: The core mechanism of using global reward functions for immediate failure detection is well-supported by both theoretical analysis and experimental results
- Medium confidence: The regret matching coordination approach shows promise, but its scalability and performance under various failure patterns need further validation
- Medium confidence: The theoretical convergence proof is sound for the submodular case, but real-world utility functions may deviate from this assumption

## Next Checks

1. **Utility Function Validation**: Test A-MCTS performance with non-submodular utility functions to assess the algorithm's robustness when the theoretical assumptions are violated
2. **Scalability Assessment**: Evaluate the computational overhead of regret matching coordination in larger multi-agent systems with more complex action spaces
3. **Domain Transferability**: Apply A-MCTS to different multi-agent planning domains (e.g., warehouse robotics, traffic management) to verify its generalizability beyond underwater sensor networks