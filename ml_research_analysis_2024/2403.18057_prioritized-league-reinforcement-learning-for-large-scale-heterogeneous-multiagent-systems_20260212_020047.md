---
ver: rpa2
title: Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent
  Systems
arxiv_id: '2403.18057'
source_url: https://arxiv.org/abs/2403.18057
tags:
- heterogeneous
- policy
- agents
- learning
- league
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses challenges in large-scale heterogeneous multiagent
  systems, specifically non-stationarity and imbalanced agent numbers, by proposing
  Prioritized Heterogeneous League Reinforcement Learning (PHLRL). The method maintains
  a league of diverse policies and uses prioritized policy gradients to compensate
  for differences in agent types.
---

# Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent Systems

## Quick Facts
- arXiv ID: 2403.18057
- Source URL: https://arxiv.org/abs/2403.18057
- Reference count: 7
- One-line primary result: PHLRL outperforms state-of-the-art MARL methods on a large-scale heterogeneous multiagent benchmark, achieving win rates above 90% within 60k episodes.

## Executive Summary
This paper addresses challenges in large-scale heterogeneous multiagent systems, specifically non-stationarity and imbalanced agent numbers, by proposing Prioritized Heterogeneous League Reinforcement Learning (PHLRL). The method maintains a league of diverse policies and uses prioritized policy gradients to compensate for differences in agent types. The authors design a new benchmark, Large-Scale Multiagent Operation (LSMO), using Unreal Engine, which is a complex two-team competition scenario. Experiments show that PHLRL outperforms state-of-the-art methods like QTRAN and QPLEX in LSMO, achieving win rates above 90% in most experiments within 60k episodes.

## Method Summary
The method maintains a league of frozen policy groups to introduce diversity and mitigate non-stationarity, and uses a mixed policy sampler to combine the frontier policy for most agent types with a frozen league policy for one randomly chosen type during training. Prioritized policy gradients are introduced to compensate for the imbalance in agent numbers by weighting updates according to relative performance. The policy network is a hypernetwork that receives both the agent's observation and a policy identity vector encoding the current policy group combination and league policy performance. The method is trained using PPO with a dual-clip variant.

## Key Results
- PHLRL achieves win rates above 90% in most experiments within 60k episodes on LSMO.
- PHLRL outperforms state-of-the-art methods like QTRAN and QPLEX on the LSMO benchmark.
- The method demonstrates superior performance in addressing complex heterogeneous multiagent cooperation in large-scale scenarios.

## Why This Works (Mechanism)

### Mechanism 1
Heterogeneous league training mitigates non-stationarity by exposing each agent type to a diverse set of teammate policies. The method samples a mixed policy combination for each episode, combining the frontier policy for most agent types with a frozen league policy for one randomly chosen type. This forces agents to adapt their cooperation strategies across different teammate behaviors, stabilizing learning.

### Mechanism 2
Prioritized policy gradients correct for sample imbalance between agent types by weighting updates according to relative performance. For each trajectory, an advantage prioritization factor is computed as the ratio of the overall team performance to the performance of the specific agent type in that trajectory. Trajectories from underrepresented or higher-variance agent types receive higher weight in the gradient update.

### Mechanism 3
Hypernetwork-based policy architecture enables adaptive collaboration by encoding policy identity and agent type information. The policy network receives both the agent's observation and a policy identity vector that encodes which policy group combination is active and the performance of the league policy for that agent type. This allows the network to adjust its behavior based on teammate identity and policy version.

## Foundational Learning

- Concept: Non-stationarity in multiagent reinforcement learning
  - Why needed here: Large-scale heterogeneous systems suffer from changing teammate policies, which destabilizes learning if not addressed.
  - Quick check question: Why does the policy of one agent type make the environment non-stationary for another type?

- Concept: Credit assignment in cooperative multiagent tasks
  - Why needed here: Sparse team-based rewards make it hard to attribute success or failure to individual agents, especially when agent types are imbalanced.
  - Quick check question: How does the prioritization factor in PHLRL relate to credit assignment?

- Concept: Hypernetworks and conditional policy generation
  - Why needed here: Agents must adapt their behavior based on the identity and performance of their teammates, requiring conditional policy outputs.
  - Quick check question: What information does the policy identity vector encode, and why is it important?

## Architecture Onboarding

- Component map:
  - League container -> Mixed policy sampler -> Hypernetwork policy/critic -> Prioritized policy gradient module -> Unreal Engine benchmark

- Critical path:
  1. Initialize empty league and random frontier policies.
  2. For each iteration: sample mixed policies, run episodes, calculate priorities, update frontier policies, periodically add new league members.
  3. Evaluate performance against baseline or SOTA methods.

- Design tradeoffs:
  - League size vs. computational cost: Larger leagues increase diversity but slow training.
  - Hypernetwork complexity vs. adaptability: More expressive hypernetworks may better adapt but require more data and tuning.
  - Sparse rewards vs. learning speed: Sparse rewards promote emergent behavior but slow convergence.

- Failure signatures:
  - Win rate plateaus below 50%: Possible league homogeneity or poor prioritization.
  - High variance in agent performance: Possible imbalance in sample weighting or insufficient adaptation.
  - Slow learning: Possible overly sparse rewards or insufficient policy diversity.

- First 3 experiments:
  1. Train PHLRL on LSOP-66x2 against the expert baseline; measure win rate and policy diversity.
  2. Compare PHLRL win rate and sample efficiency against QTRAN, QPLEX, and VDN on the same task.
  3. Test scalability by training on LSOP-102x2 and 80vs80; measure win rate and adaptation speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Prioritized Heterogeneous League Reinforcement Learning (PHLRL) method scale when the number of agent types increases significantly beyond the current experimental setup?
- Basis in paper: [explicit] The paper mentions that in large-scale heterogeneous systems, the number of agent types M is usually significantly smaller than the total agent number N, and M â‰ª N. It also tests scalability by changing the number of agents in both teams.
- Why unresolved: The paper tests scalability by increasing the number of agents but does not explicitly test scenarios with a significantly higher number of agent types, which could present different challenges.
- What evidence would resolve it: Experiments demonstrating PHLRL's performance with a large number of agent types, showing how the method handles increased complexity in agent diversity.

### Open Question 2
- Question: What are the specific impacts of the prioritized policy gradient on the learning stability and convergence speed in different types of heterogeneous multiagent environments?
- Basis in paper: [explicit] The paper introduces prioritized policy gradients to compensate for the gap caused by differences in the number of different types of intelligent agents and mentions learning robust cooperation policies.
- Why unresolved: While the paper shows that PHLRL outperforms other methods, it does not provide a detailed analysis of how the prioritized policy gradient specifically affects learning stability and convergence speed across various heterogeneous environments.
- What evidence would resolve it: Comparative studies showing learning curves and stability metrics for PHLRL with and without prioritized policy gradients across different heterogeneous environments.

### Open Question 3
- Question: How does the performance of PHLRL change when applied to non-competitive, cooperative-only tasks in heterogeneous multiagent systems?
- Basis in paper: [inferred] The paper focuses on competitive scenarios (e.g., LSOP, a two-team competition) and does not explore cooperative-only tasks, which could present different challenges and dynamics.
- Why unresolved: The paper does not provide evidence of PHLRL's effectiveness in purely cooperative scenarios, leaving its adaptability to such tasks untested.
- What evidence would resolve it: Experiments applying PHLRL to cooperative-only tasks, demonstrating its performance and adaptability in non-competitive environments.

## Limitations
- The paper does not provide a detailed analysis of the computational complexity of PHLRL, which is crucial for understanding its scalability.
- The implementation details of the hypernetwork-based policy architecture and the prioritized policy gradient are not fully specified, which may hinder reproducibility.
- The evaluation is limited to a single benchmark (LSMO), and the performance on other tasks or domains is not explored.

## Confidence
- High: The core mechanism of heterogeneous league training and prioritized policy gradients is well-supported by the experimental results.
- Medium: The scalability of PHLRL to larger agent counts is demonstrated, but the computational complexity is not fully analyzed.
- Low: The long-term stability and generalization of PHLRL to other tasks or domains is not thoroughly investigated.

## Next Checks
1. Implement a comprehensive analysis of the computational complexity of PHLRL, including the memory and time requirements for maintaining the league and training the hypernetwork-based policies.
2. Evaluate PHLRL on additional benchmarks or domains to assess its generalization and robustness.
3. Investigate the impact of different hyperparameters, such as league size and prioritization factors, on the performance and stability of PHLRL.