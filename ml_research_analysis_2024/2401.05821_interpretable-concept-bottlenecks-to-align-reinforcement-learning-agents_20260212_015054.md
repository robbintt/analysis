---
ver: rpa2
title: Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents
arxiv_id: '2401.05821'
source_url: https://arxiv.org/abs/2401.05821
tags:
- scobots
- agents
- learning
- reward
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Successive Concept Bottleneck Agents (SCoBots) introduce interpretable,
  human-understandable RL agents that decompose the policy into object detection,
  relational concept extraction, and decision tree-based action selection. By explicitly
  modeling both object properties and their relations, SCoBots enable multi-level
  inspection of the decision process and allow expert-guided concept pruning and reward
  shaping.
---

# Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents

## Quick Facts
- arXiv ID: 2401.05821
- Source URL: https://arxiv.org/abs/2401.05821
- Reference count: 40
- Primary result: SCoBots achieve human-normalized scores on par with or better than deep RL agents on 9 Atari games while providing interpretable concept-based policies

## Executive Summary
Successive Concept Bottleneck Agents (SCoBots) introduce interpretable, human-understandable RL agents that decompose the policy into object detection, relational concept extraction, and decision tree-based action selection. By explicitly modeling both object properties and their relations, SCoBots enable multi-level inspection of the decision process and allow expert-guided concept pruning and reward shaping. Experiments on 9 Atari games show SCoBots achieve human-normalized scores on par with or better than deep RL agents, while also revealing previously unknown goal misalignment in Pong (where agents relied on enemy position instead of ball position).

## Method Summary
SCoBots extend the concept bottleneck framework to reinforcement learning by introducing a three-stage architecture: object detection extracts game elements and their properties, relational concept extraction computes relationships between objects using predefined functions, and decision tree action selection maps these concepts to actions. The method preserves the Markov Decision Process framework while replacing the standard policy with interpretable components. Training uses Proximal Policy Optimization (PPO) on 20M frames, with the object detector pretrained on game screenshots. The approach enables human guidance through concept pruning and expert reward signals based on extracted concepts.

## Key Results
- SCoBots achieve human-normalized scores comparable to or better than deep RL agents on 9 Atari games
- Concept inspection revealed goal misalignment in Pong, where agents relied on enemy position rather than ball position
- Expert-guided concept pruning and reward shaping enabled desired behaviors like completing levels in Kangaroo and learning with sparse feedback in Pong

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SCoBots decompose RL policy into interpretable concept bottlenecks, enabling human inspection and correction of agent behavior
- Mechanism: Raw game states are processed through three successive bottleneck layers - object detection, relational concept extraction, and decision tree action selection. This decomposition allows inspection at each level (objects, relations, actions) and enables targeted intervention
- Core assumption: Human-understandable concepts can be extracted from game states and used to make competitive RL decisions
- Evidence anchors:
  - [abstract] "SCoBots introduce interpretable, human-understandable RL agents that decompose the policy into object detection, relational concept extraction, and decision tree-based action selection"
  - [section 2.1] "SCoBots thus represent inherently explainable RL agents that, in comparison to deep RL agents, allow for inspecting and revising their learned decision policies at multiple levels"
  - [corpus] Weak - related work focuses on concept-based RL but doesn't directly address the successive bottleneck architecture
- Break condition: If extracted concepts don't capture sufficient information for competitive performance, or if decision trees become too complex to interpret

### Mechanism 2
- Claim: Relational concepts extracted from object properties enable SCoBots to perform relational reasoning required for many RL tasks
- Mechanism: After initial object detection, SCoBots automatically extract relational concepts using predefined functions (distance, speed, etc.) applied to object combinations. These relations capture the dynamic interactions between game elements
- Core assumption: Relational reasoning is crucial for RL tasks and can be effectively captured through predefined relational functions
- Evidence anchors:
  - [abstract] "In contrast to current CB models, SCoBots do not just represent concepts as properties of individual objects, but also as relations between objects which is crucial for many RL tasks"
  - [section 2.1] "RL tasks thus often require relational reasoning, as they involve understanding the relationships between instances that evolve through time and interact with another"
  - [corpus] Moderate - related work on concept-based RL exists but the emphasis on relational concepts is unique to SCoBots
- Break condition: If relational functions don't capture relevant game dynamics, or if the set of predefined relations is insufficient for complex environments

### Mechanism 3
- Claim: Human-guided concept pruning and reward shaping enable SCoBots to correct misaligned behaviors and address RL-specific challenges
- Mechanism: Domain experts can prune irrelevant concepts from the concept space and define expert reward signals based on extracted concepts. This allows correcting shortcut learning and guiding agents toward desired behaviors
- Core assumption: Domain experts can identify relevant concepts and define effective reward signals based on the interpretable concept representations
- Evidence anchors:
  - [abstract] "SCoBots demonstrate that interpretable, concept-based RL can achieve strong performance while providing transparency and control"
  - [section 2.2] "The inspectable nature of SCoBots not only brings the benefit of improved human-understandability, but importantly allows for targeted human-machine interactions"
  - [section 3] "We showcase SCoBots' ability to provide valuable explanations and the potential of mitigating a variety of RL specific issues, from reward sparsity to misalignment problems"
  - [corpus] Strong - related work on concept-based RL with human guidance supports this mechanism
- Break condition: If human guidance introduces bias or if experts cannot effectively identify relevant concepts for complex tasks

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation of RL
  - Why needed here: SCoBots preserve the MDP framework while decomposing the policy into interpretable components
  - Quick check question: What are the four components of an MDP and how does SCoBots' architecture relate to each?

- Concept: Concept Bottleneck Models (CBMs) in supervised learning
  - Why needed here: SCoBots extend CBMs from supervised classification to RL by adding relational reasoning and decision tree action selection
  - Quick check question: How do traditional CBMs differ from SCoBots in terms of input processing and output generation?

- Concept: Decision tree distillation from neural networks
  - Why needed here: SCoBots use decision trees for interpretable action selection, requiring knowledge of how to distill neural policies into tree structures
  - Quick check question: What are the advantages and limitations of using decision trees versus neural networks for action selection in RL?

## Architecture Onboarding

- Component map:
  Object Detector (ω) -> Relation Extractor (µ) -> Action Selector (ρ) -> Decision

- Critical path:
  1. Game state → Object detector → Set of object representations
  2. Object representations → Relation extractor → Set of relational concepts
  3. Relational concepts → Decision tree → Action selection
  4. (Optional) Human guidance → Concept pruning/reward shaping → Modified concept space

- Design tradeoffs:
  - Interpretability vs. performance: Decision trees are interpretable but may be less expressive than neural networks
  - Predefined vs. learned relations: Using predefined functions ensures interpretability but may miss complex relationships
  - Granularity of concepts: More detailed concepts enable better inspection but increase complexity

- Failure signatures:
  - Poor performance despite interpretable concepts → Concepts don't capture sufficient information
  - Complex decision trees → Too many concepts or poorly defined relations
  - Misalignment issues persist → Guidance interface not effectively used or concepts not properly pruned

- First 3 experiments:
  1. Train SCoBot on Pong without guidance to verify competitive performance
  2. Apply concept pruning to remove enemy position and observe effect on NoEnemy Pong performance
  3. Add expert reward signal for ball proximity and measure effect on Pong success rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SCoBots scale effectively to more complex environments requiring maze navigation or pathfinding capabilities beyond object-centric representations?
- Basis in paper: [explicit] The paper acknowledges this limitation in section 4, noting "Other environments require additional information extraction processes. E.g. in MsPacman, an agent must navigate a maze. Extending the concept representations to cover such concepts in maze or platform environments is an important step for future work."
- Why unresolved: The current implementation focuses on object properties and relations but doesn't demonstrate performance on environments requiring more complex spatial reasoning beyond simple object interactions.
- What evidence would resolve it: Experimental results showing SCoBots successfully learning policies in maze-like environments (e.g., MsPacman, Montezuma's Revenge) with comparable performance to deep RL agents.

### Open Question 2
- Question: How do SCoBots perform when the object extraction quality degrades beyond the 5% misdetection rate tested in the paper?
- Basis in paper: [inferred] The paper tests with 5% misdetection probability and Gaussian noise, showing "NN-SCoBots learn comparable policies on all games but Kangaroo," but doesn't explore performance degradation at higher error rates.
- Why unresolved: The evaluation only tests one level of noise, leaving uncertainty about the robustness threshold and whether performance collapses at higher noise levels.
- What evidence would resolve it: Systematic evaluation across multiple noise levels (1%, 10%, 25%, 50%) showing performance curves and identifying the point where SCoBots can no longer learn viable policies.

### Open Question 3
- Question: Can the relational concept extraction process in SCoBots be made differentiable to enable end-to-end training rather than the current sequential approach?
- Basis in paper: [inferred] The paper uses a two-stage training process where object detection is pretrained separately, then relational concepts are extracted using fixed functions, and finally a decision tree is distilled from a neural network. This sequential approach is acknowledged as potentially limiting.
- Why unresolved: The paper maintains the sequential training approach for interpretability but doesn't explore whether end-to-end differentiable training could improve performance while maintaining transparency.
- What evidence would resolve it: Comparative experiments showing performance differences between the current sequential approach and an end-to-end trainable version of SCoBots, along with analysis of interpretability trade-offs.

## Limitations

- Limited evaluation scope: Only 9 Atari games tested, limiting generalizability to other RL domains
- Scalability concerns: Decision tree approach may become unwieldy for environments with many concepts
- Human guidance dependency: Effectiveness relies on experts identifying relevant concepts and defining useful reward signals

## Confidence

Medium confidence in the interpretability claims due to limited validation across diverse failure modes. Medium confidence in performance claims based on 9 game evaluations. Low confidence in the scalability of the approach to more complex environments.

## Next Checks

1. **Scalability Test**: Evaluate SCoBots on more complex Atari games with larger state spaces to assess whether the decision tree architecture remains interpretable while maintaining performance.

2. **Generalization Analysis**: Test whether concept pruning and reward shaping generalize beyond the specific examples shown, particularly in games with different dynamics and objectives.

3. **Human Study**: Conduct user studies to verify that domain experts can effectively identify relevant concepts and define useful reward signals, measuring both the quality of guidance and the learning outcomes.