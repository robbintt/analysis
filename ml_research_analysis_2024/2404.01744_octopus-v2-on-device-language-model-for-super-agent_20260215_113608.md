---
ver: rpa2
title: 'Octopus v2: On-device language model for super agent'
arxiv_id: '2404.01744'
source_url: https://arxiv.org/abs/2404.01744
tags:
- function
- arxiv
- language
- training
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Octopus v2, an on-device language model that
  enables efficient function calling on edge devices. The authors propose a method
  that assigns unique functional tokens to represent functions, transforming function
  selection into a single-token classification problem.
---

# Octopus v2: On-device language model for super agent

## Quick Facts
- arXiv ID: 2404.01744
- Source URL: https://arxiv.org/abs/2404.01744
- Authors: Wei Chen; Zhiyuan Li
- Reference count: 12
- Primary result: 2B parameter on-device model achieves 99.524% accuracy and 0.38s latency for Android function calls

## Executive Summary
This paper introduces Octopus v2, an on-device language model that enables efficient function calling on edge devices through a novel functional token approach. The method assigns unique tokens to each function, transforming function selection into a single-token classification problem rather than full sequence generation. Combined with fine-tuning using function descriptions and early stopping with a special <nexa_end> token, Octopus v2 achieves state-of-the-art performance, surpassing GPT-4 in both accuracy and latency while reducing context length by 95%. The 2B parameter model can be deployed on mobile devices with quantization, enabling function calls within 1.1-1.7 seconds.

## Method Summary
Octopus v2 uses a 2B parameter model (Gemma-2B) fine-tuned with functional tokens where each function is assigned a unique special token (<nexa_0>, <nexa_1>, etc.). The model is trained on a dataset where queries are mapped to these functional tokens along with function descriptions, enabling it to learn the semantic relationships. A weighted cross-entropy loss emphasizes the special tokens during training. For inference, the model uses early stopping with <nexa_end> to minimize context length. The approach is evaluated using full model training and LoRA fine-tuning (rank=16, alpha=32) for deployment flexibility.

## Key Results
- Achieves 99.524% accuracy and 0.38s latency for Android function calls
- Outperforms GPT-4 in both accuracy and latency metrics
- Reduces context length by 95% compared to RAG-based approaches
- Enables mobile deployment with inference times of 1.1-1.7 seconds after quantization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Functional tokens transform function calling into a single-token classification problem
- Mechanism: By mapping each available function to a unique special token (e.g., <nexa_0>, <nexa_1>), the model only needs to select the correct token instead of generating a full function name, drastically reducing error probability and context length
- Core assumption: The model can learn the semantic mapping between functional tokens and their descriptions during fine-tuning
- Evidence anchors:
  - [abstract] "This approach, combined with fine-tuning the model with function descriptions, significantly reduces latency and context length."
  - [section 3.1] "We propose designating functions as unique functional tokens... transforms the prediction task for function names into a single-token classification among the N functional tokens"
  - [corpus] Weak: No direct corpus evidence comparing functional tokens vs string generation

### Mechanism 2
- Claim: Early stopping with <nexa_end> token eliminates need for processing function descriptions during inference
- Mechanism: After selecting the functional token, the model can immediately stop generation upon encountering <nexa_end>, avoiding the need to retrieve or process descriptions of all candidate functions
- Core assumption: The model reliably learns to generate <nexa_end> after completing parameter generation
- Evidence anchors:
  - [section 3.1] "After the model is fine-tuned to understand the significance of functional tokens, it can conduct inference by employing the added special token, <nexa_end>, as the early stopping criterion"
  - [section 4.1] "the Octopus model has already learned to mapping functional tokens to corresponding function descriptions, thereby conserving a significant number of tokens for processing"
  - [corpus] Weak: No corpus evidence on inference time savings from early stopping

### Mechanism 3
- Claim: Fine-tuning with functional tokens enables the model to learn function descriptions and their relationships
- Mechanism: During training, the model is presented with queries, function descriptions, and the expected functional token output, allowing it to associate semantic meaning with each token
- Core assumption: The model can effectively learn from the limited dataset of 100-1000 samples per API
- Evidence anchors:
  - [section 3.1] "We decided to incorporate the function descriptions into the training dataset, enabling the model to learn the importance of these specialized tokens"
  - [section 4.1] "the Octopus model has already learned to mapping functional tokens to corresponding function descriptions"
  - [corpus] Weak: No corpus evidence on dataset size vs performance tradeoffs

## Foundational Learning

- Concept: Single-token classification vs sequence generation
  - Why needed here: Understanding why functional tokens reduce error probability compared to generating full function names
  - Quick check question: If a model must choose from 20 functions, what is the probability of error for single-token classification versus generating a 3-token function name?

- Concept: RAG-based function calling vs functional token approach
  - Why needed here: Appreciating the latency and context length advantages of the proposed method
  - Quick check question: How many tokens are typically required for RAG-based function calling compared to the functional token approach?

- Concept: LoRA fine-tuning vs full fine-tuning
  - Why needed here: Understanding the tradeoffs between model performance and deployment flexibility
  - Quick check question: What are the key architectural differences between LoRA and full fine-tuning, and how do they affect deployment?

## Architecture Onboarding

- Component map:
  Tokenizer with special functional tokens (<nexa_0> to <nexa_N-1>, <nexa_end>) -> Modified language model head with expanded vocabulary -> Training dataset generator with query-to-function mapping -> Inference engine with early stopping logic

- Critical path:
  1. User query input
  2. Model processes query and selects functional token
  3. Model generates parameters and <nexa_end>
  4. System executes function call with generated parameters

- Design tradeoffs:
  - Token vocabulary size vs model head complexity
  - Dataset size vs accuracy (100-1000 samples per API recommended)
  - LoRA rank and alpha parameters vs fine-tuning speed and quality

- Failure signatures:
  - Incorrect functional token selection (accuracy drops)
  - Missing or malformed <nexa_end> (context length increases)
  - Parameter generation errors (function call failures)

- First 3 experiments:
  1. Benchmark functional token approach vs RAG-based method on latency and accuracy
  2. Test early stopping with <nexa_end> on context length reduction
  3. Compare LoRA vs full fine-tuning on a subset of functions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Octopus v2 model's performance compare to other specialized function calling models beyond Llama-7B and GPT-4?
- Basis in paper: [explicit] The paper compares Octopus v2 to Llama-7B and GPT-4, but mentions that other models like NexusRaven, Toolformer, ToolAlpaca, Gorrila, ToolLlama, and Taskmatrix have demonstrated similar capabilities.
- Why unresolved: The paper does not provide a comprehensive comparison with all these models, focusing mainly on Llama-7B and GPT-4.
- What evidence would resolve it: Benchmarking Octopus v2 against a wider range of function calling models would provide a more complete picture of its performance relative to the state of the art.

### Open Question 2
- Question: What are the potential privacy and security implications of using the Octopus v2 model for on-device function calling?
- Basis in paper: [explicit] The paper mentions privacy concerns associated with cloud-based models and highlights the advantage of on-device models in this regard.
- Why unresolved: While the paper acknowledges privacy benefits, it does not delve into specific privacy or security considerations related to the Octopus v2 model or on-device function calling in general.
- What evidence would resolve it: A thorough analysis of potential privacy risks and security vulnerabilities associated with on-device function calling using Octopus v2 would provide valuable insights for users and developers.

### Open Question 3
- Question: How does the Octopus v2 model handle complex, multi-step function calling scenarios?
- Basis in paper: [inferred] The paper focuses on single-function calling, but mentions the potential for extending the model to handle reasoning tasks.
- Why unresolved: The paper does not provide information on the model's performance in scenarios requiring multiple function calls or complex reasoning.
- What evidence would resolve it: Evaluating the model's ability to handle multi-step tasks and its performance in scenarios requiring chaining multiple function calls would demonstrate its practical applicability in real-world situations.

## Limitations

- Dataset construction methodology lacks transparency, affecting validity of reported accuracy metrics
- Comparative analysis with GPT-4 and Llama-7B+RAG missing critical implementation details for independent verification
- Scalability concerns for large function spaces not addressed, with no discussion of performance degradation or handling function hierarchies

## Confidence

**High Confidence Claims:**
- The functional token approach fundamentally transforms function selection into single-token classification
- Early stopping with <nexa_end> reduces context length during inference
- LoRA fine-tuning with rank=16 and alpha=32 is technically feasible for deployment
- Quantized models can run on mobile devices with acceptable latency

**Medium Confidence Claims:**
- The 99.524% accuracy figure for Android function calls
- The 35-fold latency improvement over Llama-7B+RAG
- The 95% reduction in context length
- The cost-effectiveness of training with 100-1000 samples per API

**Low Confidence Claims:**
- The claim of surpassing GPT-4 in both accuracy and latency without controlled comparison details
- The scalability of the approach to large function spaces
- The robustness of the method across diverse application domains beyond the tested examples

## Next Checks

1. **Independent Dataset Creation and Benchmarking**: Recreate the evaluation dataset using a different LLM (e.g., Claude or another Gemini variant) to generate queries and function arguments, then benchmark Octopus v2 against GPT-4 and Llama-7B+RAG under identical conditions. This will validate whether the claimed performance advantages are reproducible.

2. **Scalability Testing**: Test the functional token approach with progressively larger function sets (50, 100, 500 functions) to measure accuracy degradation and latency impact. This will reveal the practical limits of the single-token classification approach and inform when alternative strategies might be needed.

3. **Cross-Domain Generalization**: Apply the Octopus v2 methodology to a completely different domain (e.g., web APIs, database queries, or IoT device controls) to assess whether the functional token approach generalizes beyond Android and vehicle functions. This will validate the claim that the method is suitable "for various edge devices and applications."