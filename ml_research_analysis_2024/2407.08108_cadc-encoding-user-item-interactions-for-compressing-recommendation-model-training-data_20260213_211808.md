---
ver: rpa2
title: 'CADC: Encoding User-Item Interactions for Compressing Recommendation Model
  Training Data'
arxiv_id: '2407.08108'
source_url: https://arxiv.org/abs/2407.08108
tags:
- data
- training
- dataset
- cadc
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CADC, a method to compress recommendation model
  training data while preserving accuracy. The key idea is to use matrix factorization
  to pre-train user and item embeddings that capture collaborative information from
  the full dataset.
---

# CADC: Encoding User-Item Interactions for Compressing Recommendation Model Training Data

## Quick Facts
- arXiv ID: 2407.08108
- Source URL: https://arxiv.org/abs/2407.08108
- Reference count: 40
- Achieves HR@10 of 6.57 and NDCG@10 of 3.28 on MovieLens 1M with just 5.1% and 3.2% degradation using only 10% of training data

## Executive Summary
This paper addresses the challenge of compressing recommendation model training data while preserving accuracy. The proposed CADC method uses matrix factorization to pre-train user and item embeddings that capture collaborative information from the full dataset. These enriched embeddings are then frozen and integrated into the recommendation model, allowing it to maintain accuracy even when trained on a much smaller, randomly sampled dataset. Experiments on MovieLens and Epinions datasets show that CADC achieves similar performance to training on the full dataset while using only 10% of the data and reducing training time by over 90%.

## Method Summary
CADC employs a two-step approach: first, matrix factorization (MF) is used to pre-train user and item embeddings on the full interaction matrix, capturing collaborative information efficiently. These embeddings are then frozen and injected into a Two Tower Neural Network (TTNN) recommendation model. The model is trained on a small, randomly sampled subset of the original training data (as little as 10%). The frozen MF embeddings compensate for the reduced training data, allowing the model to maintain high accuracy in terms of Hit Rate and NDCG metrics while significantly reducing training time and computational resources.

## Key Results
- Achieves HR@10 of 6.57 and NDCG@10 of 3.28 on MovieLens 1M with only 5.1% and 3.2% degradation using 10% of training data
- Reduces training time by over 90% compared to training on full dataset
- Maintains accuracy even with aggressive data reduction (up to 90% fewer training examples)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained embeddings from matrix factorization capture collaborative information that compensates for data reduction.
- Mechanism: MF learns latent representations from full user-item interaction matrix. These embeddings are frozen and injected into the recommendation model, allowing it to maintain accuracy even with 90% fewer training examples.
- Core assumption: The latent space learned by MF preserves the essential collaborative structure needed for recommendation.
- Evidence anchors:
  - [abstract] "use matrix factorization of the user-item interaction matrix to create a novel embedding representation"
  - [section] "we employ MF, a well-regarded and computationally efficient method... to generate pre-trained embeddings"
  - [corpus] No direct evidence found; corpus neighbors focus on LLM-based approaches rather than MF-based compression
- Break condition: If the interaction matrix has very low rank or the collaborative signal is weak, MF embeddings may not capture enough information to compensate for data loss.

### Mechanism 2
- Claim: Freezing pre-trained embeddings prevents noisy gradient updates that could degrade the captured collaborative signal.
- Mechanism: By freezing MF embeddings in the recommendation model, the model avoids corrupting the learned collaborative structure with noisy gradients from limited training data.
- Core assumption: Early training gradients in deep models are unstable and could degrade high-quality pre-trained embeddings.
- Evidence anchors:
  - [section] "The empirical results... indicate that Init-Frz... achieves superior outcomes... allowing embeddings to be updated in the Init method reduces accuracy"
  - [section] "freezing the embeddings... simplifies the training process as it does not require updating the large user and item ID embedding tables"
  - [corpus] No direct evidence found; corpus neighbors don't discuss gradient stability in embedding initialization
- Break condition: If the recommendation model architecture requires fine-tuning embeddings for domain-specific adaptation, freezing could limit performance gains.

### Mechanism 3
- Claim: Random sampling of interaction data is sufficient when collaborative information is pre-encoded in embeddings.
- Mechanism: Since MF embeddings capture the entire collaborative structure, uniform random sampling can reduce dataset size without sophisticated selection strategies.
- Core assumption: The pre-trained embeddings contain all necessary collaborative information, making sophisticated sampling unnecessary.
- Evidence anchors:
  - [section] "Since our pre-training captures the interaction history efficiently we do not need to explore any complex compression schemes"
  - [section] "the approach then applies uniform random sampling of the training dataset to drastically reduce the training dataset size"
  - [corpus] Weak evidence; corpus neighbors discuss sampling strategies but not in the context of pre-encoded collaborative information
- Break condition: If the recommendation task requires capturing recent trends or temporal dynamics, random sampling without considering recency could degrade performance.

## Foundational Learning

- Concept: Matrix Factorization for collaborative filtering
  - Why needed here: MF is the foundation for extracting collaborative information from user-item interactions
  - Quick check question: What are the two primary components learned by MF in collaborative filtering?

- Concept: Embedding initialization and freezing
  - Why needed here: Understanding when and why to freeze pre-trained embeddings is critical for CADC's success
  - Quick check question: What happens to gradient updates when embeddings are frozen during training?

- Concept: Hit Rate and NDCG metrics
  - Why needed here: These are the primary evaluation metrics used to measure recommendation quality
  - Quick check question: How does NDCG differ from Hit Rate in evaluating ranked recommendation lists?

## Architecture Onboarding

- Component map:
  - MF pre-training component (learns user/item embeddings from full interaction matrix)
  - Embedding injection layer (replaces original ID embeddings in recommendation model)
  - Frozen embedding layer (non-trainable user/item embeddings)
  - Recommendation model (Two Tower Neural Network processing frozen embeddings)
  - Random sampling component (selects subset of interactions for final training)

- Critical path:
  1. Train MF on full dataset to learn embeddings
  2. Freeze MF embeddings
  3. Initialize recommendation model with frozen embeddings
  4. Randomly sample training data subset
  5. Train recommendation model on sampled data

- Design tradeoffs:
  - Embedding size vs. computational efficiency (96 dimensions chosen)
  - Degree of data reduction vs. accuracy retention
  - Training time vs. embedding quality (100 epochs for MF pre-training)
  - Model complexity vs. simplicity of random sampling

- Failure signatures:
  - Significant drop in HR@10 or NDCG@10 compared to full training
  - Increased training instability or divergence
  - Poor performance on long-tail items
  - Sensitivity to initial embedding quality

- First 3 experiments:
  1. Train MF on full dataset and inspect embedding quality (correlation with user preferences)
  2. Compare HR@10 with frozen vs. trainable embeddings on 10% data
  3. Measure training time reduction when using 10% vs. 100% of data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of embedding size (e.g., 95 vs 96) impact the performance of CADC in terms of HR@10 and NDCG@10?
- Basis in paper: [explicit] The paper mentions that the embedding size for MF is set to 95, which aligns with the TTNN's effective size when incorporating additional bias terms. However, it does not explore the impact of varying the embedding size.
- Why unresolved: The paper does not provide experiments or analysis on how different embedding sizes affect the model's performance.
- What evidence would resolve it: Experiments comparing the performance of CADC with different embedding sizes (e.g., 95, 96, 128) on the same datasets would provide insights into the optimal embedding size for balancing model accuracy and computational efficiency.

### Open Question 2
- Question: How does CADC perform when applied to datasets with different characteristics, such as different sparsity levels or domain-specific features?
- Basis in paper: [inferred] The paper evaluates CADC on MovieLens and Epinions datasets, which are both user-item interaction datasets. However, it does not explore how CADC performs on datasets with different characteristics, such as higher sparsity or domain-specific features.
- Why unresolved: The paper does not provide experiments or analysis on the performance of CADC on diverse datasets.
- What evidence would resolve it: Experiments applying CADC to datasets with varying sparsity levels, domain-specific features, or different interaction patterns would provide insights into the generalizability and robustness of the method.

### Open Question 3
- Question: How does the computational efficiency of CADC compare to other data compression techniques when applied to extremely large-scale datasets?
- Basis in paper: [explicit] The paper demonstrates that CADC reduces training time significantly compared to the gold standard. However, it does not compare the computational efficiency of CADC to other data compression techniques, such as coresets or data distillation, when applied to extremely large-scale datasets.
- Why unresolved: The paper does not provide a comprehensive comparison of CADC's computational efficiency with other data compression techniques on large-scale datasets.
- What evidence would resolve it: Experiments comparing the training time and memory usage of CADC with other data compression techniques (e.g., coresets, data distillation) on extremely large-scale datasets would provide insights into the scalability and efficiency of CADC.

## Limitations

- No ablation studies on embedding dimensions or MF training epochs, leaving uncertainty about hyperparameter robustness
- Limited evaluation on datasets with different sparsity levels or domain-specific characteristics
- The claim that random sampling is sufficient lacks theoretical justification or exploration of alternative sampling strategies

## Confidence

- High confidence: The two-step compression approach (MF pre-training + random sampling) is clearly described and empirically validated
- Medium confidence: The superiority of frozen embeddings over trainable ones, as the mechanism is demonstrated but the underlying reasons aren't deeply explored
- Medium confidence: The claim that CADC generalizes to different recommendation architectures, as only Two Tower Neural Networks are tested

## Next Checks

1. Conduct ablation studies varying embedding dimensions (32, 64, 128) and MF training epochs to determine sensitivity to these hyperparameters
2. Test CADC on a highly sparse dataset (e.g., BookCrossing) to verify performance across different data density regimes
3. Implement and compare against a stratified sampling strategy to empirically validate the claim that random sampling is sufficient when using pre-trained MF embeddings