---
ver: rpa2
title: Can Large Language Models Understand You Better? An MBTI Personality Detection
  Dataset Aligned with Population Traits
arxiv_id: '2412.12510'
source_url: https://arxiv.org/abs/2412.12510
tags:
- labels
- personality
- soft
- mbti
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the over-optimism in MBTI personality detection
  by creating MBTIBench, the first manually annotated high-quality MBTI personality
  detection dataset with soft labels. The authors resolve data quality issues (29.58%
  incorrect labeling) and capture population trait distributions by estimating soft
  labels reflecting polarity tendencies.
---

# Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits

## Quick Facts
- arXiv ID: 2412.12510
- Source URL: https://arxiv.org/abs/2412.12510
- Reference count: 40
- Key outcome: Soft labels better align with population traits than hard labels for MBTI personality detection

## Executive Summary
This paper addresses the over-optimism in MBTI personality detection by creating MBTIBench, the first manually annotated high-quality MBTI personality detection dataset with soft labels. The authors resolve data quality issues (29.58% incorrect labeling) and capture population trait distributions by estimating soft labels reflecting polarity tendencies. Experiments show that soft labels better align with population traits than hard labels and improve performance on related psychological tasks. The study highlights polarized predictions and biases in LLMs as key directions for future research, demonstrating that soft labels provide more benefits than hard labels for psychological tasks.

## Method Summary
The study creates MBTIBench by first filtering existing MBTI datasets to remove label leakage and noise issues, then conducting manual annotation with trained annotators following specific guidelines. Soft labels are estimated using an EM algorithm that integrates annotator variability to produce polarity tendencies and realistic rankings for each sample. Six backbone models (GPT-4 variants, Qwen2, Llama3.1) are evaluated using four prompting methods (Zero-shot, Step-by-step, Few-shot, PsyCoT) with performance measured by Segmented RMSE (S-RMSE) and Segmented MAE (S-MAE) for soft labels, and Accuracy and F1 for hard labels.

## Key Results
- Soft labels exhibit a smooth distribution from 0 to 1 with more non-extreme samples than extreme ones, aligning with population distributions
- Zero-shot prompting outperforms other prompting methods across all six backbone models
- Soft labels outperform hard labels in stress detection tasks, demonstrating their benefits for psychological applications

## Why This Works (Mechanism)

### Mechanism 1
Soft labels better align with population traits than hard labels by capturing the polarity tendency of samples using EM algorithm that integrates annotator variability and produces realistic rankings for each sample. The core assumption is that personality is not binary but complex, nuanced, and highly individualized with more people having non-extreme traits. Evidence shows soft labels exhibit smooth distribution from 0 to 1 with more non-extreme samples than extreme ones. Break condition: If the population distribution of personality traits differs significantly from the assumed smooth distribution with more non-extreme cases.

### Mechanism 2
Zero-shot prompting outperforms other prompting methods for soft label prediction because direct task description without additional reasoning steps allows models to leverage their pre-trained knowledge more effectively for personality assessment. The core assumption is that models have sufficient pre-trained knowledge about personality traits and can apply this directly without guided reasoning. Evidence shows zero-shot method demonstrates superior overall performance across all six backbones and achieves the lowest average S-MAE. Break condition: If models lack sufficient pre-trained knowledge about personality traits or require structured reasoning to make accurate assessments.

### Mechanism 3
Data quality issues significantly impact model performance because filtering removes label leakage and useless noise that compromise the factual accuracy of personality detection. The core assumption is that self-reported labels contain errors and social media posts contain irrelevant content that interferes with personality inference. Evidence shows datasets exhibit varying degrees of quality issues before refinement that impact model performance, with 29.58% incorrect labeling identified. Break condition: If the proportion of label leakage and noise issues is lower than the reported 29.58% or if filtering guidelines are not comprehensive.

## Foundational Learning

- Concept: Myers-Briggs Type Indicator (MBTI) personality theory
  - Why needed here: Understanding the four dimensions (E/I, S/N, T/F, J/P) and their definitions is essential for creating annotation guidelines and interpreting results
  - Quick check question: What are the four dimensions of MBTI and what does each dimension measure?

- Concept: Soft label estimation using EM algorithm
  - Why needed here: The EM algorithm processes annotator labels to derive polarity tendencies and create continuous personality scores
  - Quick check question: How does the EM algorithm account for annotator variability when estimating soft labels?

- Concept: Dataset quality assessment in NLP
  - Why needed here: Understanding data filtering, annotation guidelines, and quality metrics is crucial for evaluating the dataset construction methodology
  - Quick check question: What are the key components of a comprehensive data quality assessment for personality detection datasets?

## Architecture Onboarding

- Component map: Data collection -> Preprocessing (filtering) -> Annotation training -> Formal annotation -> Quality assurance -> Soft label estimation -> Model evaluation
- Critical path: Dataset construction (filtering + annotation) -> Soft label generation -> Model experimentation -> Performance analysis
- Design tradeoffs: Manual annotation ensures quality but limits scalability; soft labels capture nuance but require complex estimation; multiple backbones provide robustness but increase complexity
- Failure signatures: Poor Fleiss' Kappa scores indicate annotation inconsistency; high RMSE/MAE indicates model struggles with personality prediction; significant label distribution shifts indicate data quality issues
- First 3 experiments:
  1. Compare model performance on filtered vs unfiltered datasets to validate data quality impact
  2. Test different prompting methods on a subset of the dataset to optimize inference approach
  3. Evaluate soft label predictions against hard label baselines to demonstrate soft label advantages

## Open Questions the Paper Calls Out

### Open Question 1
How do the soft labels compare to hard labels in terms of predicting personality-related psychological outcomes (e.g., stress, anxiety, or job performance)? The paper shows that soft labels outperform hard labels in stress detection, but it does not explore other psychological outcomes. This remains unresolved because the study focuses on stress detection as a single case study. Experiments applying soft labels to predict other psychological outcomes and comparing their performance to hard labels would resolve this question.

### Open Question 2
How does the quality of the soft labels (e.g., reliability, validity) compare to other methods of personality assessment (e.g., self-report questionnaires, observer ratings)? The paper mentions that soft labels are estimated using an EM algorithm, but it does not compare their quality to other methods of personality assessment. This remains unresolved because the study focuses on developing and evaluating soft labels but does not compare their quality to other established methods. Studies comparing the reliability and validity of soft labels to other methods would resolve this question.

### Open Question 3
How do different types of text data (e.g., social media posts, essays, interviews) influence the accuracy of soft label predictions? The paper uses social media data for training and evaluation, but it does not explore how different types of text data might influence the accuracy of soft label predictions. This remains unresolved because the study focuses on social media data. Experiments training and evaluating soft label models on different types of text data and comparing their performance would resolve this question.

## Limitations

- EM algorithm implementation details for soft label estimation were not fully specified, creating uncertainty about exact reproducibility
- The dataset focuses on MBTI detection from social media posts, which may limit generalizability to other text domains or personality frameworks
- The assumed population distribution of personality traits comes from psychological literature rather than direct corpus evidence

## Confidence

- Soft label alignment with population traits: High confidence
- Zero-shot prompting superiority: Medium confidence
- Data quality improvement through filtering: High confidence
- Soft label benefits for psychological tasks: Medium confidence

## Next Checks

1. Implement the EM algorithm for soft label estimation using the provided annotation data and compare the resulting polarity tendencies and distributions with those reported in the paper to verify the soft label generation process.

2. Test the soft label approach and zero-shot prompting method on personality detection datasets from different domains (e.g., academic writing, clinical interviews) to assess generalizability beyond social media posts.

3. Conduct a separate analysis of the actual distribution of MBTI traits in the population using the annotated dataset, comparing the observed distribution with the assumed smooth distribution to validate the core assumption about non-extreme personality traits being more common.