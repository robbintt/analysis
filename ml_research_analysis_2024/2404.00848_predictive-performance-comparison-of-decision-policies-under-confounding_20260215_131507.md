---
ver: rpa2
title: Predictive Performance Comparison of Decision Policies Under Confounding
arxiv_id: '2404.00848'
source_url: https://arxiv.org/abs/2404.00848
tags: []
core_contribution: This paper proposes a framework for comparing predictive performance
  of decision-making policies under confounding. The key insight is that uncertainty
  cancellation can tighten regret bounds by isolating comparison-relevant uncertainty,
  ignoring redundant uncertainty in agreement regions of policy comparisons.
---

# Predictive Performance Comparison of Decision Policies Under Confounding

## Quick Facts
- **arXiv ID**: 2404.00848
- **Source URL**: https://arxiv.org/abs/2404.00848
- **Reference count**: 40
- **Key outcome**: This paper proposes a framework for comparing predictive performance of decision-making policies under confounding.

## Executive Summary
This paper introduces a novel framework for comparing the predictive performance of decision policies under confounding. The key insight is that uncertainty cancellation can tighten regret bounds by isolating comparison-relevant uncertainty and ignoring redundant uncertainty in agreement regions. The authors develop methods for finite-sample estimation of regret intervals under no parametric assumptions on the status quo policy, and show their approach is compatible with various modern identification approaches from causal inference and off-policy evaluation literature.

## Method Summary
The paper develops a framework for comparing decision policies by estimating partially identified policy regret intervals. It leverages uncertainty cancellation in policy comparisons to tighten regret bounds, and uses doubly robust estimation to achieve fast convergence rates. The approach works with various causal assumptions (MSM, IV, Rosenbaum's Γ) to construct bounding functions that constrain the uncertainty set. The method provides finite-sample estimation of regret intervals without parametric assumptions on the status quo policy.

## Key Results
- Uncertainty cancellation isolates comparison-relevant uncertainty, ignoring redundant uncertainty in agreement regions
- Doubly robust estimation achieves fast convergence rates even when nuisance functions are estimated at slow nonparametric rates
- The framework is compatible with various modern identification approaches from causal inference and off-policy evaluation literature
- Applied to healthcare enrollment policies, the method yields tighter regret bounds than baseline approaches, particularly when there is high uncertainty in the cancellation term

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uncertainty cancellation tightens regret bounds by isolating comparison-relevant uncertainty and ignoring redundant uncertainty in agreement regions.
- Mechanism: The policy comparison decomposes into v-statistics. When comparing two policies, terms in the agreement region (where both policies make the same decision) cancel out, leaving only the disagreement region uncertainty to contribute to the regret bound. This reduces the overall uncertainty interval.
- Core assumption: The key v-statistics that cancel are those corresponding to the agreement region where both policies select the same action.
- Evidence anchors:
  - [abstract] "Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison."
  - [section 4] "Our approach leverages the key insight that we can eliminate redundant uncertainty which does not contribute to differential policy performance (Figure 1)."
- Break condition: If the agreement region contributes significant uncertainty (e.g., high uncertainty in v0(0,0)), then the improvement from cancellation may be minimal.

### Mechanism 2
- Claim: Pointwise bounding functions enable tighter regret bounds by constraining the uncertainty set around partially identified v-statistics.
- Mechanism: Bounding functions τ(x) and τ̄(x) restrict the possible values of the unobserved outcome regression µ0(x), which in turn bounds the partially identified v-statistics. This shrinks the uncertainty set V(p; τ) compared to the unconstrained set V(p).
- Core assumption: The bounding functions are implied by common causal assumptions like the marginal sensitivity model (MSM), instrumental variable (IV), or Rosenbaum's Γ sensitivity model.
- Evidence anchors:
  - [section 5] "We show that this assumption is implied by a range of causal assumptions studied in prior OPE literature (e.g., MSM, Rosenbaum's Γ, IV) in Appendix B."
  - [appendix B.4] "Assumption B.8. (Treatment confounding bridge)... Lemma B.9. (Ghassami et al. (2023)) Let Assumptions 3.2, B.7 and B.8 hold..."
- Break condition: If the bounding functions are misspecified or too loose, the constrained uncertainty set may not provide significant improvement over the unconstrained set.

### Mechanism 3
- Claim: Doubly robust estimation enables fast convergence rates even when nuisance functions are estimated at slow nonparametric rates.
- Mechanism: The doubly robust estimator has an error term that is a product of nuisance function errors. This allows it to achieve fast convergence rates (n^-1/2) even when the nuisance functions are estimated at slower rates (n^-1/4).
- Core assumption: The nuisance functions can be estimated at a rate of n^-1/4 using nonparametric machine learning methods.
- Evidence anchors:
  - [section 6.2] "Because doubly robust estimators have an error term which is a product of nuisance function errors, they attain fast convergence rates, even when estimating nuisance functions at slow nonparametric rates (Kennedy et al., 2021)."
  - [appendix C.2] "Theorem C.1. The doubly-robust estimator satisfies... when ∥ϕ − ˆϕ∥ = op(1)."
- Break condition: If the nuisance functions cannot be estimated at the required rate, or if the doubly robust estimator is misspecified, the convergence rate guarantees may not hold.

## Foundational Learning

- Concept: Off-policy evaluation (OPE) in contextual bandits
  - Why needed here: The paper addresses comparing decision policies under confounding, which is a form of OPE in the confounded offline contextual bandits setting.
  - Quick check question: What is the difference between on-policy and off-policy evaluation in reinforcement learning?

- Concept: Partial identification and confidence intervals
  - Why needed here: The paper deals with partially identified policy regret intervals due to unmeasured confounding, and develops methods to estimate these intervals from finite samples.
  - Quick check question: How does partial identification differ from point identification in causal inference?

- Concept: Causal inference assumptions (e.g., MSM, IV, Rosenbaum's Γ)
  - Why needed here: The paper leverages various causal assumptions to derive bounding functions that constrain the uncertainty set and tighten regret bounds.
  - Quick check question: What is the key difference between the marginal sensitivity model and the instrumental variable assumption?

## Architecture Onboarding

- Component map: Data preprocessing -> Nuisance function estimation -> Partial identification -> Regret bound estimation -> Policy comparison

- Critical path:
  1. Estimate nuisance functions (propensity scores, outcome regressions, bounding functions)
  2. Construct uncertainty sets using the estimated nuisance functions and bounding functions
  3. Compute δ-regret and baseline regret intervals
  4. Compare policies based on the estimated regret intervals

- Design tradeoffs:
  - Computational efficiency vs. statistical efficiency: Plug-in estimators are computationally simpler but may have slower convergence rates, while doubly robust estimators are more complex but can achieve faster rates.
  - Flexibility vs. tightness of bounds: The unconstrained uncertainty set V(p) is more flexible but yields wider bounds, while the constrained set V(p; τ) is tighter but requires specifying bounding functions.
  - Asymptotic vs. finite-sample performance: The paper provides asymptotic regret bounds, but finite-sample performance depends on the quality of nuisance function estimates.

- Failure signatures:
  - Wide regret intervals that include zero: Indicates high uncertainty in the policy comparison, potentially due to confounding or finite sample error
  - Violation of coverage guarantees: Suggests misspecification of nuisance functions or bounding functions
  - Slow convergence rates: May indicate that nuisance functions are not estimated at the required rate

- First 3 experiments:
  1. Synthetic data experiment: Generate data under known data-generating mechanisms (MSM or IV) and compare the δ-regret and baseline regret intervals across various policy performance measures.
  2. Assumption robustness experiment: Vary the strength of the causal assumptions (e.g., MSM sensitivity parameter Λ) and assess the impact on regret interval coverage and tightness.
  3. Real-world application: Apply the framework to compare alternative healthcare enrollment policies using the Obermeyer et al. (2019) dataset, and interpret the results in the context of the policy comparison.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework relies heavily on correctly specifying nuisance functions and bounding functions
- Theoretical guarantees are asymptotic, with finite-sample performance depending on nuisance function estimation quality
- Bounding functions derived from causal assumptions may not always capture true uncertainty in the data

## Confidence

- **High Confidence**: The insight that uncertainty cancellation can tighten regret bounds by isolating comparison-relevant uncertainty and ignoring redundant uncertainty in agreement regions.
- **Medium Confidence**: The doubly robust estimation approach for fast convergence rates, assuming the nuisance functions are estimated at the required rate.
- **Medium Confidence**: The compatibility of the framework with various modern identification approaches from causal inference and off-policy evaluation literature, given correct specification of nuisance functions and bounding functions.

## Next Checks

1. **Synthetic Data Experiment**: Generate synthetic data under known data-generating mechanisms (MSM or IV) and compare the δ-regret and baseline regret intervals across various policy performance measures to validate the framework's performance under controlled conditions.

2. **Assumption Robustness Analysis**: Vary the strength of the causal assumptions (e.g., MSM sensitivity parameter Λ) and assess the impact on regret interval coverage and tightness to understand the framework's sensitivity to assumption violations.

3. **Real-World Application**: Apply the framework to compare alternative healthcare enrollment policies using the Obermeyer et al. (2019) dataset, and interpret the results in the context of the policy comparison to validate the framework's practical utility.