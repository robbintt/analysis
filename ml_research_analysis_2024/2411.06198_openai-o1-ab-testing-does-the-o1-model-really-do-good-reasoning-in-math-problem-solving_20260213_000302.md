---
ver: rpa2
title: 'OpenAI-o1 AB Testing: Does the o1 model really do good reasoning in math problem
  solving?'
arxiv_id: '2411.06198'
source_url: https://arxiv.org/abs/2411.06198
tags:
- reasoning
- column
- monster
- o1-mini
- integer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates whether OpenAI''s o1 model''s strong mathematical
  reasoning abilities stem from genuine reasoning or memorization of training data.
  The authors conduct an A/B test using two datasets: 60 International Mathematical
  Olympiad (IMO) problems with high public accessibility and 60 Chinese National Team
  Training camp (CNT) problems with similar difficulty but lower accessibility.'
---

# OpenAI-o1 AB Testing: Does the o1 model really do good reasoning in math problem solving?

## Quick Facts
- arXiv ID: 2411.06198
- Source URL: https://arxiv.org/abs/2411.06198
- Reference count: 5
- Key outcome: o1-mini shows no significant performance difference between IMO and CNT problems, suggesting reasoning rather than memorization

## Executive Summary
This paper investigates whether OpenAI's o1 model's mathematical reasoning abilities stem from genuine reasoning or memorization of training data. The authors conduct an A/B test using two datasets of similar difficulty but different public accessibility: 60 International Mathematical Olympiad (IMO) problems and 60 Chinese National Team Training camp (CNT) problems. By comparing performance across these datasets, they aim to determine if the model is leveraging memorized solutions or demonstrating authentic problem-solving capabilities.

The results show no statistically significant difference between the two datasets (t-statistics close to 0), indicating that o1-mini's performance is not driven by memorization. Case studies reveal the model can provide correct intuitions and identify patterns, but often lacks rigorous proof steps and struggles to justify why alternative solutions don't exist. This suggests that while o1-mini demonstrates some reasoning capabilities, its mathematical problem-solving still has room for improvement in terms of formal proof construction.

## Method Summary
The study employs an A/B testing framework where two datasets with similar difficulty but different public accessibility are used. The researchers selected 60 IMO problems with high public accessibility and 60 CNT problems with similar difficulty but lower accessibility. They evaluate the model's performance on both "search" and "solve" type problems, measuring accuracy rates. The t-statistics are calculated to determine if there are significant differences between the datasets. Case studies are conducted to examine the model's reasoning process, including its ability to provide correct intuitions, identify patterns, and construct rigorous proofs.

## Key Results
- No statistically significant difference between IMO and CNT problem performance (t-statistics close to 0)
- o1-mini can provide correct intuitions and identify patterns in mathematical problems
- Model often lacks rigorous proof steps and struggles to justify why alternative solutions don't exist

## Why This Works (Mechanism)
The study's design leverages the difference in public accessibility between IMO and CNT problems to create a natural experiment. By using problems of similar difficulty but different availability in training data, the authors can test whether performance differences arise from memorization or genuine reasoning. The t-statistic analysis provides statistical validation of the null hypothesis that there is no significant performance difference between the two datasets, supporting the conclusion that the model's abilities are not solely based on memorization.

## Foundational Learning
The findings suggest that o1-mini has learned some fundamental mathematical reasoning patterns that transfer across different problem sets, regardless of their public accessibility. This indicates the model has developed generalizable problem-solving strategies rather than simply memorizing solutions. The ability to identify patterns and provide correct intuitions, even when formal proofs are lacking, points to an underlying understanding of mathematical concepts that goes beyond rote learning.

## Architecture Onboarding
The results imply that the o1 architecture has been successfully trained to develop reasoning capabilities that generalize across different mathematical domains. The model's performance on both search and solve-type problems suggests it can handle various problem structures and reasoning requirements. However, the limitations in proof construction indicate that while the architecture can support reasoning, it may need further refinement to produce more rigorous mathematical arguments.

## Open Questions the Paper Calls Out
- How can the model's reasoning capabilities be further improved to produce more rigorous proofs?
- What specific aspects of the o1 architecture enable it to develop generalizable reasoning skills?
- How do the reasoning capabilities of o1-mini compare to other model variants like o1-preview in more complex mathematical domains?

## Limitations
- Sample size of 60 problems per dataset may be insufficient to detect subtle performance differences
- Study only evaluates two specific model variants (o1-mini and o1-preview), limiting generalizability
- Classification of problems as "search" or "solve" types relies on subjective human judgment
- The analysis focuses on binary correctness rather than the quality of reasoning process
- Case studies provide limited insight into the model's internal reasoning mechanisms

## Confidence
High confidence in: The conclusion that o1-mini's performance does not show statistically significant differences between IMO and CNT problems (t-statistics close to 0).

Medium confidence in: The broader claim that o1-mini's performance is "not driven by memorization" based on the current evidence.

Low confidence in: The characterization of o1-mini's reasoning abilities based on case studies, given the limited scope and potential subjectivity in analysis.

## Next Checks
1. Conduct power analysis to determine if the current sample size (60 problems per dataset) is sufficient to detect meaningful performance differences, and if not, scale up the experiment accordingly.

2. Design and implement a systematic rubric to quantify reasoning quality beyond binary correctness, including measures of proof rigor, justification quality, and error analysis for both correct and incorrect solutions.

3. Test additional model variants (including other LLM families) and problem types to assess whether the observed patterns hold across a broader range of mathematical reasoning tasks and model architectures.