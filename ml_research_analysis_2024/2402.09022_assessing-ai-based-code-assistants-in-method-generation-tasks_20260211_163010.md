---
ver: rpa2
title: Assessing AI-Based Code Assistants in Method Generation Tasks
arxiv_id: '2402.09022'
source_url: https://arxiv.org/abs/2402.09022
tags:
- code
- assistants
- generated
- ai-based
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates four AI-based code assistants\u2014GitHub\
  \ Copilot, Tabnine, ChatGPT, and Google Bard\u2014on method generation tasks using\
  \ 100 Java methods from real-world projects. Methods were generated using method\
  \ comments and signatures as prompts, then evaluated for correctness, complexity,\
  \ efficiency, size, and similarity to developer-written code."
---

# Assessing AI-Based Code Assistants in Method Generation Tasks
## Quick Facts
- arXiv ID: 2402.09022
- Source URL: https://arxiv.org/abs/2402.09022
- Reference count: 11
- Primary result: AI assistants rarely produce ready-to-use correct code in method generation tasks

## Executive Summary
This study evaluates four AI-based code assistants—GitHub Copilot, Tabnine, ChatGPT, and Google Bard—on method generation tasks using 100 Java methods from real-world projects. Methods were generated using method comments and signatures as prompts, then evaluated for correctness, complexity, efficiency, size, and similarity to developer-written code. Results show Copilot achieved the highest correctness rate (32%), followed by ChatGPT (23%), Bard (15%), and Tabnine (13%). All assistants produced code with complexity and size comparable to developers, and in some cases, more efficient code. However, generated code rarely matched developer implementations exactly and struggled with inter-class dependencies.

The study demonstrates that while AI code assistants show promise in generating syntactically correct and appropriately sized code, they still fall short of producing production-ready implementations. The assistants performed well on simpler methods but struggled with more complex scenarios involving multiple classes and dependencies. This highlights the current limitations of AI code generation tools and suggests they remain best suited as aids to human developers rather than replacements for human coding expertise.

## Method Summary
The study evaluated AI code assistants by presenting them with 100 Java methods from real-world projects, using method comments and signatures as prompts. Each assistant generated code for these methods, which was then evaluated against developer-written implementations across five dimensions: correctness, complexity, efficiency, size, and similarity. Correctness was determined through developer verification, while other metrics were measured objectively. The evaluation focused specifically on method generation tasks, excluding other programming activities to maintain scope and consistency in assessment.

## Key Results
- Copilot achieved highest correctness rate at 32%, followed by ChatGPT (23%), Bard (15%), and Tabnine (13%)
- All assistants produced code with complexity and size comparable to human developers
- Generated code rarely matched developer implementations exactly and struggled with inter-class dependencies

## Why This Works (Mechanism)
AI code generation tools function by leveraging pattern matching from their training data to produce syntactically correct implementations. The models identify common coding patterns, structures, and solutions from their training corpus and apply these patterns to new prompts. When provided with method signatures and comments, the assistants generate code that matches the expected structure and purpose, drawing on learned programming patterns. This pattern-based approach explains why generated code tends to have similar complexity and size to human-written code, as these metrics are often determined by established coding conventions and patterns.

## Foundational Learning
- AI code generation relies on pattern matching from training data to generate syntactically correct implementations
- Code complexity metrics (cyclomatic, halstead) provide objective measures of implementation difficulty
- Runtime efficiency can sometimes be improved by AI assistants through optimized implementations
- Method signature and comment prompts serve as critical inputs for guiding AI code generation
- Inter-class dependencies require understanding of larger code context beyond individual methods

## Architecture Onboarding
- Component map: User Prompt -> AI Model -> Code Generation -> Evaluation Metrics
- Critical path: Method comment + signature → AI assistant → generated code → correctness verification
- Design tradeoffs: Focused evaluation on method generation limits scope but enables consistent comparison
- Failure signatures: Difficulty with multi-class dependencies, incorrect logic despite syntactic correctness
- First experiments: 1) Test single-method generation accuracy across assistants, 2) Compare complexity metrics between generated and human code, 3) Evaluate dependency handling in multi-class scenarios

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit questions emerge from the study's findings. These include: How would AI assistants perform on larger codebases beyond individual methods? What are the long-term effects of using AI-generated code on code maintainability and technical debt? How do AI assistants handle domain-specific knowledge and specialized coding patterns not well-represented in training data?

## Limitations
- Evaluation used only 100 Java methods from limited real-world projects
- Correctness metric relies on subjective developer verification
- Study focused solely on method generation, excluding other programming activities
- No assessment of runtime behavior or edge case handling
- Limited to four specific AI assistants without comparison to other generation tools
- Results may not generalize to other programming languages or development contexts

## Confidence
- Method generation correctness rates (High): Standardized prompts and developer verification provide reliable comparative results
- Complexity and efficiency comparisons (Medium): Small sample size limits generalizability of these findings
- Dependency handling assessment (Low): Identified as weakness but not systematically evaluated
- Cross-assistant comparison (Medium): Limited to four tools with different underlying models and training approaches

## Next Checks
1. Replicate the study with a larger corpus of methods across multiple programming languages and project types to assess generalizability
2. Conduct systematic testing of generated code against comprehensive test suites to evaluate actual runtime correctness and edge case handling
3. Perform longitudinal studies tracking how developers modify and integrate AI-generated code in real development workflows
4. Investigate the impact of different prompt engineering techniques on generation quality
5. Study the effects of AI-generated code on long-term code maintainability and technical debt