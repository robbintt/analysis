---
ver: rpa2
title: Large Language Models Perform on Par with Experts Identifying Mental Health
  Factors in Adolescent Online Forums
arxiv_id: '2404.16461'
source_url: https://arxiv.org/abs/2404.16461
tags:
- data
- health
- mental
- synthetic
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the performance of large language models
  (LLMs) in extracting mental health factors from adolescent social media posts. A
  new dataset of Reddit posts was created and annotated by expert psychiatrists for
  six categories: TRAUMA, PRECARITY, CONDITION, SYMPTOMS, SUICIDALITY, and TREATMENT.'
---

# Large Language Models Perform on Par with Experts Identifying Mental Health Factors in Adolescent Online Forums

## Quick Facts
- **arXiv ID**: 2404.16461
- **Source URL**: https://arxiv.org/abs/2404.16461
- **Reference count**: 40
- **Primary result**: GPT4 performs on par with expert human annotators in extracting mental health factors from adolescent social media posts, with some advantages in recall.

## Executive Summary
This study investigates the performance of large language models (LLMs) in extracting mental health factors from adolescent social media posts. A new dataset of Reddit posts was created and annotated by expert psychiatrists for six categories: TRAUMA, PRECARITY, CONDITION, SYMPTOMS, SUICIDALITY, and TREATMENT. The performance of GPT3.5 and GPT4 was compared to human annotations on both real and synthetic data. GPT4 performed on par with human annotators, with some advantages in recall. However, both models still exhibited errors in negation and factuality. Synthetic data, while easier to annotate, lacked the diversity of real data. The study concludes that GPT4 shows promise for mental health monitoring applications, but further research is needed to address its limitations.

## Method Summary
The study collected 1000 Reddit posts from mental health-related subreddits, filtered to include only posts from adolescents aged 12-19, resulting in 6500 sentences. Expert psychiatrists annotated these sentences with six categories of mental health factors: TRAUMA, PRECARITY, CONDITION, SYMPTOMS, SUICIDALITY, and TREATMENT. The performance of GPT3.5 and GPT4 was compared to human annotations on both real and synthetic data. Synthetic datasets of 500 sentences each were generated by prompting GPT3.5 and GPT4 to create and label Reddit-like posts. Performance was evaluated using precision, recall, F1-score at category level, and accuracy at subcategory level.

## Key Results
- GPT4 performs on par with expert human annotators in extracting mental health factors from adolescent social media posts.
- GPT4 shows higher recall than human annotators, potentially catching annotations that experts miss.
- Synthetic data, while easier to annotate, lacks the diversity of real data and may not be suitable for training robust models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT4 performs on par with expert human annotators in extracting mental health factors from adolescent social media posts.
- Mechanism: GPT4 leverages its large-scale training on diverse text to recognize patterns in mental health-related language and extract relevant categories without explicit fine-tuning.
- Core assumption: The patterns and language used by adolescents discussing mental health are sufficiently represented in GPT4's training data.
- Evidence anchors:
  - [abstract] "We find GPT4 to be on par with human inter-annotator agreement"
  - [section] "GPT4 displays a substantially higher accuracy at the subcategory level (0.47 vs 0.35)"
  - [corpus] Weak - no direct evidence of training data composition for adolescent mental health language.
- Break condition: If the language patterns used by adolescents discussing mental health are underrepresented in GPT4's training data, performance will degrade.

### Mechanism 2
- Claim: GPT4 shows higher recall than human annotators, potentially catching annotations that experts miss.
- Mechanism: GPT4's pattern recognition capabilities allow it to identify subtle or less obvious indicators of mental health factors that human annotators might overlook.
- Core assumption: GPT4's pattern recognition is more comprehensive than human annotators for this specific task.
- Evidence anchors:
  - [abstract] "GPT4 performed on par with human annotators, with some advantages in recall"
  - [section] "error analysis and manual examination of annotations suggest the LLMs potentially outperform human annotators in terms of recall (sensitivity)"
  - [corpus] No direct evidence - this is inferred from the comparison results.
- Break condition: If the task requires nuanced understanding of context or sarcasm that GPT4 struggles with, recall advantage may disappear.

### Mechanism 3
- Claim: Synthetic data generated by LLMs is easier to annotate, leading to higher performance for both LLMs and humans.
- Mechanism: LLM-generated text tends to be more homogeneous and less diverse than real data, making patterns more consistent and easier to identify.
- Core assumption: The reduced complexity and diversity in synthetic data makes the annotation task simpler.
- Evidence anchors:
  - [abstract] "performance on synthetic data to be substantially higher"
  - [section] "higher performance on synthetic data is driven by greater complexity of real data rather than inherent advantage"
  - [corpus] Weak - no direct evidence of synthetic data generation process or comparison of diversity metrics.
- Break condition: If synthetic data becomes too homogeneous and loses important nuances present in real data, it may not be suitable for training robust models.

## Foundational Learning

- Concept: Open information extraction
  - Why needed here: The task requires extracting any relevant mental health factors from text without a predetermined set of answers.
  - Quick check question: How does open information extraction differ from tasks with closed sets of answers?

- Concept: Negation handling in NLP
  - Why needed here: The study specifically tests LLMs' ability to handle both positive and negative annotations of mental health factors.
  - Quick check question: Why is negation handling particularly challenging for language models?

- Concept: Inter-annotator agreement
  - Why needed here: The study uses Cohen's Kappa to measure the upper bound of human performance for comparison with LLM performance.
  - Quick check question: What does high inter-annotator agreement indicate about the quality and consistency of annotations?

## Architecture Onboarding

- Component map: Data collection (Reddit posts) -> Annotation (expert psychiatrists) -> LLM annotation (GPT3.5 and GPT4) -> Performance comparison modules
- Critical path: Data collection → Annotation → LLM processing → Performance comparison → Error analysis
- Design tradeoffs: Using synthetic data for easier annotation vs. real data for diversity and authenticity.
- Failure signatures: LLMs producing infelicitous negations, incorrect factuality, or labeling irrelevant factors.
- First 3 experiments:
  1. Compare GPT4 performance on real vs. synthetic data to confirm the complexity hypothesis.
  2. Test GPT4 with varied prompts to see if instruction clarity affects negation and factuality errors.
  3. Create a human-in-the-loop annotation system where LLM suggestions are reviewed by experts to improve accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs consistently distinguish between positive and negative annotations in mental health contexts, particularly regarding sensitive topics like suicidality and trauma?
- Basis in paper: [explicit] The paper mentions that both GPT3.5 and GPT4 produce infelicitous negations, such as negative annotations that seem irrelevant to humans. It also notes that LLMs often confuse positive and negative annotations, outputting negative annotations for positive ones.
- Why unresolved: While the study identifies these issues, it doesn't provide a comprehensive analysis of the extent to which LLMs can reliably handle negation and factuality in mental health contexts. The paper suggests that GPT4 still makes errors in negation and factuality, but the frequency and impact of these errors are not fully explored.
- What evidence would resolve it: A detailed error analysis comparing the frequency and types of negation and factuality errors made by LLMs versus human annotators, along with an assessment of the clinical significance of these errors in mental health contexts.

### Open Question 2
- Question: How does the diversity and complexity of synthetic data compare to real-world adolescent mental health data, and what are the implications for training and evaluating mental health monitoring models?
- Basis in paper: [explicit] The paper finds that synthetic data, while easier to annotate, lacks the diversity of real data. It also notes that the distribution of subcategories in synthetic datasets is more homogenous, potentially due to the LLMs using given examples in a more uniform manner.
- Why unresolved: The study provides initial insights into the differences between synthetic and real data but doesn't fully explore the implications of these differences for model performance and generalizability. It suggests that the better performance on synthetic data is due to its lower complexity and diversity, but the broader implications for mental health monitoring applications are not thoroughly discussed.
- What evidence would resolve it: Comparative analyses of model performance on synthetic and real data across various mental health monitoring tasks, along with assessments of the clinical relevance and generalizability of insights gained from synthetic data.

### Open Question 3
- Question: What are the specific factors contributing to GPT4's superior performance compared to GPT3.5 in extracting mental health factors from adolescent social media posts?
- Basis in paper: [explicit] The paper finds that GPT4 outperforms GPT3.5 on both real and synthetic data, with better precision, recall, and F1-scores. However, it doesn't delve into the specific reasons for this performance difference.
- Why unresolved: While the study establishes that GPT4 performs better, it doesn't investigate the underlying factors contributing to this improvement. The paper mentions that GPT4 is more consistent and catches more annotations than human annotators, but the specific aspects of its architecture or training that lead to these advantages are not explored.
- What evidence would resolve it: Comparative analyses of the architectures, training processes, and fine-tuning approaches used for GPT3.5 and GPT4, along with targeted experiments to isolate the specific features or techniques responsible for GPT4's improved performance in mental health factor extraction.

## Limitations

- The study uses a relatively small dataset of 1000 posts (6500 sentences) annotated by three expert psychiatrists, which may underestimate the upper bound for human performance.
- Both GPT3.5 and GPT4 exhibit errors in negation and factuality, which could be problematic for clinical applications.
- Synthetic data, while easier to annotate, lacks the diversity of real data and may not be suitable for training robust models.

## Confidence

- **GPT4 performance on par with human annotators**: Medium
  - Comparable performance metrics, particularly in recall, but influenced by data complexity and model limitations in negation and factuality.
- **Synthetic data advantages**: Low
  - Performance gains on synthetic data likely due to reduced complexity and diversity rather than genuine model capability.
- **Clinical applicability**: Low
  - Errors in negation and factuality, along with limited dataset size, raise concerns about reliability for clinical applications.

## Next Checks

1. **External Validation**: Test GPT4 on an independent dataset of adolescent social media posts from different platforms (e.g., Twitter, Instagram) to assess generalization beyond Reddit.

2. **Clinical Expert Review**: Have clinical psychologists independently review a sample of LLM annotations to assess clinical validity and potential safety concerns, particularly for high-risk categories like SUICIDALITY.

3. **Longitudinal Performance**: Evaluate GPT4's consistency over time by analyzing the same posts on multiple occasions to assess stability and identify potential hallucination or factuality issues.