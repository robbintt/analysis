---
ver: rpa2
title: 'Chasing Progress, Not Perfection: Revisiting Strategies for End-to-End LLM
  Plan Generation'
arxiv_id: '2412.10675'
source_url: https://arxiv.org/abs/2412.10675
tags:
- block
- plan
- place
- goal
- truck
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how well large language models (LLMs) can
  generate plans by revisiting several recent training strategies. The authors train
  an LLM on natural language descriptions of planning problems and then test its performance
  on in-distribution, longer, unseen, and obfuscated domains.
---

# Chasing Progress, Not Perfection: Revisiting Strategies for End-to-End LLM Plan Generation

## Quick Facts
- arXiv ID: 2412.10675
- Source URL: https://arxiv.org/abs/2412.10675
- Authors: Sukai Huang; Trevor Cohn; Nir Lipovetzky
- Reference count: 38
- Key outcome: Reinforcement learning with LCCS reward most effective strategy for improving both plan validity and executability on longer planning problems

## Executive Summary
This paper evaluates how well large language models (LLMs) can generate plans by revisiting several recent training strategies. The authors train an LLM on natural language descriptions of planning problems and then test its performance on in-distribution, longer, unseen, and obfuscated domains. They find that fine-tuning alone leads to good performance only on in-distribution data, with significant drops on out-of-distribution tasks. Among the strategies tested—permutation augmentation, chain-of-thought (CoT), self-correction, and reinforcement learning (RL)—RL with a longest contiguous common subsequence reward is the most effective, improving both plan executability and validity on longer problems. CoT and permutation help improve executability but not validity, and self-correction helps identify errors but not correct them. The study concludes that future efforts should target both executability and validity to improve LLM planning.

## Method Summary
The authors fine-tune QWEN 2-7B-INSTRUCT on extended PlanBench dataset using next-token prediction, then apply four strategies: permutation augmentation, chain-of-thought, self-correction, and reinforcement learning with LCCS reward. They evaluate on in-distribution, longer, unseen, and obfuscated test sets measuring validity rate, executability rate, and goal satisfiability rate.

## Key Results
- Reinforcement learning with LCCS reward is most effective, improving both plan validity and executability on longer problems
- Permutation augmentation significantly improves executability on unseen domains (75.5% vs 20.1% baseline)
- Self-correction enables error detection but fails to correct errors effectively
- Chain-of-thought strategies improve executability but only within training distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement Learning with LCCS reward improves both plan validity and executability by providing granular feedback beyond binary validity signals.
- Mechanism: LCCS measures longest contiguous subsequence between generated and reference plans, offering partial credit for correct action sequences even when overall plan is invalid. This smoother reward signal helps the model learn incremental improvements in planning ability.
- Core assumption: LCCS captures meaningful structural similarity between plans that correlates with planning quality.
- Evidence anchors:
  - [abstract] "reinforcement learning with our novel 'Longest Contiguous Common Subsequence' reward emerged as the most effective, contributing to both plan validity and executability"
  - [section 3.2] "When the plan is invalid, a supplementary reward is also provided based on the length of the LCCS between Pg and Pr"
- Break condition: If LCCS reward fails to correlate with actual plan quality improvements, the reinforcement signal becomes misleading.

### Mechanism 2
- Claim: Permutation augmentation enhances executability by preventing the model from overfitting to superficial positional patterns in action descriptions.
- Mechanism: Random reordering of action descriptions, condition/effect descriptions, and atoms in initial/goal statements forces the model to learn underlying semantic relationships rather than relying on positional cues. This improves the quality of token embeddings through better self-attention.
- Core assumption: Positional cues are a significant source of spurious correlations that hurt generalization.
- Evidence anchors:
  - [section 3.2] "permutation augmentation – randomly rearranging sentences in a query – significantly improved Transformer models for question-answering tasks"
  - [section 4.2] "we observe a remarkable 75.5% score in 'unseen' test set, while the vanilla model only got 20.1%"
- Break condition: If semantic relationships are sufficiently captured without permutation, the augmentation provides no additional benefit.

### Mechanism 3
- Claim: Chain-of-thought strategies improve executability by explicitly modeling intermediate reasoning steps, but only within the plan length distribution seen during training.
- Mechanism: Goal CoT prompts model to estimate remaining steps to goal, while State CoT prompts for grounded preconditions and effects. This helps the model learn world dynamics and state transitions, improving local consistency of generated plans.
- Core assumption: Explicit intermediate reasoning helps the model maintain state consistency during generation.
- Evidence anchors:
  - [section 3.2] "training LLMs to predict these details before deciding on an action would help the model learn world dynamics better"
  - [section 4.4] "State CoT does not improve plan executability within the 'long' test set, yet it significantly enhances performance within the 'unseen' test set"
- Break condition: If the model cannot effectively learn from intermediate reasoning steps, CoT provides no benefit and may even harm performance by adding complexity.

## Foundational Learning

- Concept: Next-token prediction in autoregressive language models
  - Why needed here: The paper's evaluation of LLM planning capabilities relies on understanding how next-token prediction models sequential reasoning tasks
  - Quick check question: How does next-token prediction differ from other autoregressive modeling approaches?

- Concept: Reinforcement learning with sequence-level rewards
  - Why needed here: RL with LCCS reward is the most effective strategy for improving LLM planning, requiring understanding of how sequence-level optimization differs from token-level approaches
  - Quick check question: Why is sequence-level reward more appropriate for planning tasks than token-level rewards?

- Concept: Plan executability vs. validity
  - Why needed here: The paper introduces executability as a complementary metric to validity, which is crucial for understanding incremental improvements in plan quality
  - Quick check question: What's the relationship between plan executability and validity according to the paper's definitions?

## Architecture Onboarding

- Component map: QWEN 2-7B-INSTRUCT model -> Extended PlanBench dataset (8 domains) -> Four strategies (permutation, CoT, self-correction, RL) -> LCCS reward model -> Evaluation metrics (validity, executability, goal satisfiability)

- Critical path:
  1. Fine-tune LLM on PlanBench corpus using next-token prediction
  2. Apply selected strategy (permutation, CoT, self-correction, or RL)
  3. Evaluate on in-distribution, longer, unseen, and obfuscated test sets
  4. Measure validity and executability rates

- Design tradeoffs:
  - Permutation vs. semantic understanding: Permutation improves executability but doesn't directly improve validity
  - CoT complexity vs. performance: Goal CoT adds complexity that can hurt performance, while State CoT only helps within training distribution
  - Self-correction detection vs. correction: Model can identify errors but struggles to correct them effectively
  - RL data efficiency vs. performance: RL shows better gains than SFT despite using less training data

- Failure signatures:
  - Strategy produces high executability but low validity (e.g., permutation alone)
  - Strategy improves performance on in-distribution but not OOD data (e.g., vanilla fine-tuning)
  - Strategy introduces complexity without performance gains (e.g., Goal CoT on OOD data)
  - Model generates actions from training domains when faced with obfuscated inputs

- First 3 experiments:
  1. Fine-tune LLM on extended PlanBench and evaluate baseline performance across all test sets
  2. Apply permutation augmentation and compare executability improvements on unseen domains
  3. Implement RL with LCCS reward and measure validity improvements on long planning problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does self-correction learning enable the model to recognize errors but fail to correct them effectively?
- Basis in paper: [explicit] The authors observe that the model achieves high precision and recall in identifying mistakes during mistake identification probing tests, but the detected errors do not lead to effective correction.
- Why unresolved: The paper does not provide a clear explanation for why the model's error detection capability does not translate into successful error correction. The authors suggest that external expert feedback might help, but this is speculative.
- What evidence would resolve it: Experiments that provide detailed feedback on why an action is incorrect and measure if this improves the model's ability to correct errors.

### Open Question 2
- Question: Why does the goal CoT strategy hinder planning performance in out-of-distribution scenarios?
- Basis in paper: [explicit] The authors observe that Goal CoT does not improve validity rates in OOD cases and attribute this to the complexity paradox and poor generalization, where the model struggles to estimate goal distances outside the training distribution.
- Why unresolved: The paper does not provide empirical evidence to support the claim that the complexity of estimating goal distance or the model's bias towards familiar plan lengths are the root causes of Goal CoT's failure.
- What evidence would resolve it: Experiments that isolate the effects of goal distance estimation and plan length distribution on model performance, such as training on variable plan lengths or using a different heuristic estimation method.

### Open Question 3
- Question: Why does RL with LCCS reward outperform supervised fine-tuning (SFT) in improving validity and executability rates?
- Basis in paper: [explicit] The authors find that RL with LCCS reward improves validity and executability rates more than SFT, even when both use the same training data.
- Why unresolved: The paper does not explain the underlying mechanisms that make RL more effective than SFT for this task. It suggests that RL fosters more comprehensive planning skills, but this is not substantiated.
- What evidence would resolve it: Comparative analysis of the learned representations or strategies between RL and SFT models, or ablation studies on different reward functions and their impact on plan quality.

## Limitations

- Evaluation limited to 8 domains from PlanBench with performance varying significantly across domain types
- Permutation augmentation strategy shows large improvements on unseen domains but exact implementation details remain underspecified
- Self-correction strategy can identify errors but cannot effectively correct them, suggesting fundamental limitations in LLM ability to revise generated content
- Study focuses on single LLM architecture (QWEN 2-7B-INSTRUCT), leaving open questions about strategy effectiveness across different model sizes

## Confidence

**High Confidence:** The finding that vanilla fine-tuning leads to significant performance drops on out-of-distribution data (in-distribution validity rate: 80.4% vs. unseen: 20.1%) is well-supported by systematic evaluation across multiple test sets. The observation that RL with LCCS reward improves both validity and executability rates on longer problems is also strongly evidenced through comparative analysis.

**Medium Confidence:** The effectiveness of permutation augmentation and CoT strategies shows more variability across domains and test conditions. While permutation clearly improves executability on unseen domains, its impact on validity is less consistent. Similarly, CoT improves executability but the benefits are limited to specific test conditions and domain types.

**Low Confidence:** The scalability of these strategies to larger models and more complex planning domains remains untested. The paper does not explore how these strategies perform with models significantly larger or smaller than 7B parameters, nor does it test on domains with substantially more complex action schemas or larger state spaces.

## Next Checks

1. **Implement permutation augmentation with full specification**: Recreate the exact shuffling mechanism for action descriptions, conditions, and effects to verify the 75.5% executability improvement on unseen domains reported in the paper.

2. **Cross-domain robustness testing**: Evaluate the trained models on completely new planning domains not included in PlanBench to assess whether improvements generalize beyond the studied domain set.

3. **Error correction capability assessment**: Design targeted experiments to test whether self-correction can be improved through alternative prompting strategies or whether the inability to correct errors represents a fundamental limitation of current LLM architectures for planning tasks.