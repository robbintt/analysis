---
ver: rpa2
title: 'IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities
  of Language Models in E-commerce'
arxiv_id: '2406.10173'
source_url: https://arxiv.org/abs/2406.10173
tags:
- intention
- product
- intentions
- products
- because
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces IntentionQA, a benchmark for evaluating
  language models'' ability to understand purchase intentions in e-commerce. The benchmark
  consists of two tasks: inferring customer intentions from purchased products and
  predicting additional purchases based on those intentions.'
---

# IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce

## Quick Facts
- **arXiv ID:** 2406.10173
- **Source URL:** https://arxiv.org/abs/2406.10173
- **Reference count:** 36
- **Primary result:** Language models struggle with understanding products and intentions accurately, falling far behind human performance on the IntentionQA benchmark.

## Executive Summary
This paper introduces IntentionQA, a benchmark designed to evaluate language models' ability to comprehend purchase intentions in e-commerce settings. The benchmark consists of two tasks: inferring customer intentions from purchased products and predicting additional purchases based on those intentions. IntentionQA contains 4,360 multiple-choice questions across three difficulty levels, constructed using an automated pipeline that leverages ASER, a large-scale eventuality knowledge graph, for context augmentation and similarity computation. Experiments across 19 language models demonstrate that they still struggle with understanding products and intentions accurately, jointly reasoning with products and intentions, and other aspects of intention comprehension.

## Method Summary
The IntentionQA benchmark is constructed through an automated pipeline that transforms intention assertions from FolkScope (a human-annotated knowledge base) into multiple-choice questions. The pipeline uses ASER knowledge graph to compute semantic context embeddings for products and intentions, enabling similarity-based distractor sampling. Products are conceptualized into categories using ChatGPT to expand coverage beyond the original intention assertions. The benchmark evaluates language models on two tasks: IntentUnderstand (inferring intentions from products) and IntentUtilize (predicting additional purchases based on intentions). Human evaluation validates the quality of generated questions, showing low false-negative rates of 2.56% and 1.67% for the two tasks respectively.

## Key Results
- Language models show significantly lower accuracy than humans on both IntentUnderstand and IntentUtilize tasks
- Models struggle with understanding products and intentions accurately, jointly reasoning with products and intentions, and utilizing purchase intentions
- The benchmark successfully differentiates between model capabilities across different difficulty levels
- Automated construction using ASER knowledge graph produces high-quality questions with controlled difficulty

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The benchmark's two-task design effectively isolates distinct aspects of purchase intention comprehension.
- **Mechanism:** By separating intention understanding (inferring intention from products) and intention utilization (predicting additional purchases), the benchmark can diagnose specific failure modes in LM reasoning.
- **Core assumption:** Intention comprehension can be meaningfully decomposed into these two separate cognitive processes.
- **Evidence anchors:**
  - [abstract] The paper explicitly defines Task 1 as "infer intentions based on purchased products" and Task 2 as "predict additional purchases based on those intentions."
  - [section 3.1] The authors state that "Together, they make up the entire process of intention comprehension."
- **Break condition:** If the two tasks are not truly independent (e.g., if intention utilization always requires understanding the original intention), then the decomposition loses diagnostic value.

### Mechanism 2
- **Claim:** Automated construction using ASER knowledge graph enables scalable, consistent benchmarking.
- **Mechanism:** ASER provides semantic context for products and intentions, enabling similarity-based distractor sampling without human intervention.
- **Core assumption:** Product and intention similarity in ASER correlates with semantic similarity relevant to purchase intentions.
- **Evidence anchors:**
  - [section 3.2] The paper describes using ASER to compute "context embeddings" and similarity scores for both products and intentions.
  - [section 3.3] Similarity metrics are explicitly defined based on these embeddings.
- **Break condition:** If ASER's coverage is insufficient for the product domain, or if the similarity metrics don't capture relevant semantic relationships, the automated construction will fail.

### Mechanism 3
- **Claim:** Multiple-choice format with carefully sampled distractors enables consistent evaluation across diverse models.
- **Mechanism:** By providing four options (1 correct, 3 distractors) with controlled similarity to the correct answer, the benchmark can use accuracy as a reliable metric across different model types and sizes.
- **Core assumption:** The distractor sampling strategy (similarity thresholds between 0.6-0.9) effectively balances difficulty and prevents false negatives.
- **Evidence anchors:**
  - [section 3.4] The paper describes the distractor sampling strategy and similarity thresholds for both tasks.
  - [section 4.2.2] Human evaluation shows low false-negative rates (2.56% and 1.67% for the two tasks).
- **Break condition:** If the similarity thresholds are poorly calibrated, the benchmark may become too easy (trivial distractors) or too hard (false negatives).

## Foundational Learning

- **Concept: Knowledge graph utilization**
  - Why needed here: Understanding how to leverage structured knowledge (ASER) for semantic similarity computation is crucial for grasping the benchmark construction method.
  - Quick check question: How does the paper use ASER to compute product similarity? (Answer: By averaging Sentence-BERT embeddings of one-hop neighborhood edges)

- **Concept: Intention modeling in e-commerce**
  - Why needed here: Comprehending the distinction between purchase intention and product features is essential for understanding the benchmark's objectives.
  - Quick check question: What's the difference between product similarity and intention similarity in this context? (Answer: Product similarity is based on co-purchase contexts, intention similarity is based on the minimum product similarity across intention's associated products)

- **Concept: Multiple-choice question design**
  - Why needed here: Understanding the principles of distractor selection and difficulty calibration is key to evaluating the benchmark's quality.
  - Quick check question: How are difficulty levels determined for the questions? (Answer: Based on product similarity scores between co-buy products in the original intention assertion)

## Architecture Onboarding

- **Component map:**
  FolkScope -> ASER knowledge graph -> Product conceptualization module -> Similarity computation module -> Distractor sampling module -> Difficulty labeling module -> Human evaluation module

- **Critical path:**
  1. Extract intention assertions from FolkScope
  2. Conceptualize products and align with ASER
  3. Compute context embeddings and similarities
  4. Sample distractors based on similarity thresholds
  5. Label difficulty and generate final questions
  6. Human evaluation for quality control

- **Design tradeoffs:**
  - Automated vs. human-curated construction: Automation enables scalability but may introduce quality issues
  - Similarity thresholds: Tighter thresholds reduce false negatives but may create trivial distractors
  - Product conceptualization: Using ChatGPT adds semantic richness but introduces potential bias

- **Failure signatures:**
  - High false-negative rates in human evaluation
  - Low correlation between product similarity and question difficulty
  - Inconsistent performance across model categories
  - Poor generalization to new product domains

- **First 3 experiments:**
  1. Test similarity computation with a