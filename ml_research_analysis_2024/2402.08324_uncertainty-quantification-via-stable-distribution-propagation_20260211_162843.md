---
ver: rpa2
title: Uncertainty Quantification via Stable Distribution Propagation
arxiv_id: '2402.08324'
source_url: https://arxiv.org/abs/2402.08324
tags:
- distribution
- gaussian
- network
- marginal
- distributions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Stable Distribution Propagation (SDP), a
  method for propagating Gaussian and Cauchy input uncertainties through neural networks
  to quantify output uncertainties. The core idea is to use local linearization for
  non-linear layers, which is shown to be optimal in terms of total variation distance
  for the ReLU non-linearity.
---

# Uncertainty Quantification via Stable Distribution Propagation

## Quick Facts
- arXiv ID: 2402.08324
- Source URL: https://arxiv.org/abs/2402.08324
- Reference count: 40
- Method propagates Gaussian and Cauchy uncertainties through neural networks using local linearization

## Executive Summary
This paper introduces Stable Distribution Propagation (SDP), a method for propagating input uncertainties through neural networks to quantify output uncertainties. The approach uses local linearization for non-linear layers, claiming optimality in total variation distance for ReLU non-linearity. SDP is demonstrated on UCI regression tasks and selective prediction on MNIST with EMNIST letters as out-of-distribution data, showing competitive performance against baseline methods.

## Method Summary
SDP uses local linearization to propagate Gaussian and Cauchy input uncertainties through neural networks. For non-linear layers, the method linearizes the activation function around the mean input, allowing efficient propagation of covariances and correlated scales. The authors claim this approach is optimal in terms of total variation distance for ReLU non-linearity. The method is applied to predict calibrated confidence intervals and perform selective prediction on out-of-distribution data.

## Key Results
- On UCI regression tasks, SDP achieves competitive or better performance than baselines in prediction interval coverage probability and mean prediction interval width
- For selective prediction on MNIST with EMNIST letters as out-of-distribution data, SDP achieves an AUC of 18.3%, outperforming baselines
- The method demonstrates broad applicability and advantages over moment matching approaches

## Why This Works (Mechanism)
The method works by approximating non-linear activation functions with local linearizations around the mean input. This allows the propagation of uncertainty measures (mean and covariance for Gaussian, scale for Cauchy) through the network in a computationally efficient manner. The local linearization is claimed to be optimal in terms of total variation distance for ReLU, providing a theoretically grounded approach to uncertainty propagation.

## Foundational Learning
- **Total Variation Distance**: A measure of the difference between probability distributions. Needed to quantify the optimality of the local linearization approximation. Quick check: Verify that the TV distance between the true and approximated distributions is minimized.
- **Gaussian and Cauchy Distributions**: Stable distributions used to model input uncertainties. Required for the method's application to different types of input noise. Quick check: Ensure the input data can be reasonably approximated by these distributions.
- **Local Linearization**: Approximation technique for non-linear functions around a point. Core to the method's efficiency in propagating uncertainties. Quick check: Confirm the linearization error is acceptable for the chosen activation functions.

## Architecture Onboarding

Component Map:
Input -> Local Linearization Layer -> Linear Propagation -> Output

Critical Path:
The critical path involves the local linearization of non-linear activation functions and the subsequent propagation of uncertainty measures through the network.

Design Tradeoffs:
- Accuracy vs. Computational Efficiency: Local linearization provides a good balance between accurate uncertainty propagation and computational efficiency compared to sampling-based methods.
- Theoretical Optimality vs. Practical Performance: While the method claims optimality in TV distance for ReLU, its performance on other activation functions and architectures needs further validation.

Failure Signatures:
- Poor performance on highly non-linear activation functions where local linearization is a poor approximation
- Degradation in performance for very deep networks due to error accumulation in the linearization process

First Experiments:
1. Apply SDP to a simple fully connected network on UCI regression tasks and compare prediction interval coverage with Monte Carlo dropout.
2. Test the method's performance on selective prediction for MNIST with EMNIST letters, comparing AUC scores with moment matching approaches.
3. Evaluate the impact of network depth on SDP's performance by testing on deeper architectures (e.g., adding more layers to a baseline network).

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical foundation, particularly optimality claims for ReLU, lacks extensive empirical validation
- Evaluation is limited to specific datasets and scenarios, not capturing the full landscape of uncertainty quantification techniques
- Applicability to deeper networks and more complex architectures (e.g., CNNs, transformers) remains unexplored

## Confidence
- Theoretical claims: Medium
- Empirical performance: Medium
- Scalability to complex architectures: Low

## Next Checks
1. Evaluate SDP on deeper networks (e.g., ResNet architectures) and compare performance and computational efficiency with Monte Carlo dropout and deep ensembles.
2. Test the method's robustness to different types of input uncertainties (e.g., multimodal distributions) and assess its behavior on out-of-distribution detection tasks beyond the MNIST-EMNIST setup.
3. Conduct a thorough ablation study to isolate the impact of the local linearization assumption on final uncertainty estimates, particularly for non-linear activation functions beyond ReLU.