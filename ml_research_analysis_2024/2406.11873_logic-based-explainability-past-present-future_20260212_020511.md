---
ver: rpa2
title: 'Logic-Based Explainability: Past, Present & Future'
arxiv_id: '2406.11873'
source_url: https://arxiv.org/abs/2406.11873
tags:
- explanations
- https
- feature
- marques-silva
- logic-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a technical survey of logic-based explainability
  (XAI), a rigorous approach to explainable AI that aims to provide human decision-makers
  with understandable explanations for the predictions made by ML models. The paper
  covers the origins, current research topics, and emerging future directions of logic-based
  XAI.
---

# Logic-Based Explainability: Past, Present & Future

## Quick Facts
- arXiv ID: 2406.11873
- Source URL: https://arxiv.org/abs/2406.11873
- Reference count: 40
- Primary result: Technical survey of logic-based explainability (XAI) covering formal definitions, algorithms, complexity results, and emerging directions

## Executive Summary
This paper provides a comprehensive survey of logic-based explainability, a rigorous approach to explainable AI that uses formal logic and automated reasoning to compute human-understandable explanations for ML model predictions. The work covers formal definitions of abductive and contrastive explanations, presents tractable algorithms for certain classifier families, and identifies complexity limitations for others. The survey also addresses misconceptions in non-rigorous XAI methods like SHAP and introduces distance-restricted explanations to improve scalability.

## Method Summary
The paper synthesizes various approaches to logic-based XAI, focusing on encoding ML models into formal logic (SAT, SMT, MILP) and using automated reasoning tools to compute explanations. For classification and regression models, it defines abductive and contrastive explanations based on similarity predicates, and presents algorithms for computing these explanations for specific model families like decision trees and naive Bayes. The work also introduces distance-restricted explanations to handle computational complexity issues when explaining large ML models.

## Key Results
- Formal definitions of abductive and contrastive explanations for both classification and regression models
- Tractable algorithms for computing explanations for decision trees and naive Bayes classifiers
- Complexity results showing intractability for random forests and neural networks
- Introduction of distance-restricted explanations to scale logic-based XAI to large models

## Why This Works (Mechanism)

### Mechanism 1
Logic-based XAI provides mathematically rigorous explanations that can be formally verified, unlike heuristic approaches like SHAP or LIME. By encoding ML models into formal logic (SAT/SMT/MILP) and using established automated reasoning tools, explanations can be computed with guaranteed correctness and certified results. The core assumption is that the ML model can be accurately encoded into a formal logic representation without loss of essential behavior.

### Mechanism 2
Distance-restricted explanations trade global validity for computational tractability while maintaining localized correctness guarantees. Instead of requiring explanations to hold for all possible inputs, distance-restricted explanations only need to be valid within a specified distance threshold from the target sample, reducing computational complexity while still providing useful local insights. The core assumption is that localized explanations within a bounded distance provide meaningful insights for decision-making.

### Mechanism 3
The connection between feature selection (AXp/CXp) and feature attribution (SHAP scores) can be formalized to create rigorous alternatives to current heuristic attribution methods. AXp/CXp provide mathematically rigorous feature importance through set-theoretic necessity and sufficiency, while SHAP scores attempt to capture similar information through game-theoretic Shapley values, but with known inadequacies. The core assumption is that the mathematical properties of AXp/CXp can be mapped to feature importance measures that address SHAP's shortcomings.

## Foundational Learning

- Concept: Propositional logic and SAT solving fundamentals
  - Why needed here: The core of logic-based XAI relies on encoding ML models into propositional logic and using SAT solvers to find explanations
  - Quick check question: Can you explain the difference between a CNF formula and a DNF formula, and why CNF is typically used in SAT solving?

- Concept: First-order logic and SMT solving basics
  - Why needed here: For more complex ML models, first-order logic and SMT solvers are needed to handle features with richer domain structures
  - Quick check question: What is the difference between a quantifier-free theory and a quantified theory in SMT solving?

- Concept: Game theory and Shapley value computation
  - Why needed here: Understanding why SHAP scores can be inadequate requires knowledge of how Shapley values are computed and their limitations
  - Quick check question: Can you compute the Shapley value for a feature in a simple 3-feature classifier where each feature contributes differently to the final prediction?

## Architecture Onboarding

- Component map: ML Model Encoder -> Automated Reasoner Interface -> Explanation Validator -> Distance Calculator -> Feature Attribution Mapper
- Critical path: Model encoding → Automated reasoning → Explanation validation → Result presentation
- Design tradeoffs:
  - Precision vs. Performance: More precise logic encodings lead to better explanations but require more computational resources
  - Global vs. Local: Distance-restricted explanations are faster but only valid locally
  - Expressivity vs. Tractability: More expressive logic theories enable richer explanations but may become intractable
- Failure signatures:
  - Unsatisfiable encodings: The ML model cannot be accurately represented in the chosen logic framework
  - Solver timeouts: The computational complexity exceeds available resources
  - Inconsistent explanations: The computed explanations contradict known model behavior
- First 3 experiments:
  1. Encode a simple decision tree into propositional logic and verify it produces the same predictions as the original model
  2. Compute AXp/CXp for a small random forest and compare with SHAP scores for the same predictions
  3. Implement distance-restricted explanations for a neural network and test different distance thresholds to find the optimal balance between tractability and informativeness

## Open Questions the Paper Calls Out

### Open Question 1
Can logic-based XAI methods scale to explain large neural networks with millions of parameters? While distance-restricted explanations show promise, further improvements are needed to fully explain very large neural networks efficiently. Evidence would include demonstrations of logic-based XAI successfully explaining predictions of large neural networks with reasonable computational resources.

### Open Question 2
How can we certify the correctness of computed explanations for non-monotonic ML models? Current certification methods are limited to monotonic classifiers and more research is needed for general cases. Evidence would include development and validation of formal methods to certify correctness for arbitrary non-monotonic ML models.

### Open Question 3
What is the relationship between rigorous feature attribution methods and logic-based feature selection in practice? While theoretical connections are established, further research is expected to understand practical implications. Evidence would include comparative studies and benchmarks showing practical differences and advantages of using logic-based feature selection versus traditional feature attribution methods.

## Limitations

- Limited empirical validation of complexity results and scalability claims
- Theoretical connections between AXp/CXp and feature attribution remain largely unproven
- No comprehensive user studies comparing logic-based explanations with existing XAI methods
- Weak corpus evidence supporting the paper's claims about superiority over heuristic methods

## Confidence

- High confidence: Theoretical foundations of logic-based XAI are well-established in formal methods literature
- Medium confidence: Complexity results for classifier families appear sound but lack experimental verification
- Low confidence: Practical impact of proposed connections and distance-restricted explanations remains unproven

## Next Checks

1. Implement and benchmark logic encodings for decision trees, naive Bayes, and random forests to verify claimed complexity results experimentally
2. Conduct controlled user study comparing logic-based explanations with SHAP scores and LIME for real-world decision-making tasks
3. Develop and test proposed feature attribution mapping algorithm to validate theoretical connection between AXp/CXp and Shapley values