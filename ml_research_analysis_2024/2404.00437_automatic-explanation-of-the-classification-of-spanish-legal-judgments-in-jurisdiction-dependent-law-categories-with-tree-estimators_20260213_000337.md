---
ver: rpa2
title: Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent
  law categories with tree estimators
arxiv_id: '2404.00437'
source_url: https://arxiv.org/abs/2404.00437
tags:
- legal
- jurisdiction
- test
- social
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a system combining Natural Language Processing
  with Machine Learning to classify Spanish legal texts and automatically explain
  its decisions. The system extracts features from the input text and uses decision
  tree-based estimators to classify it into a law category within each jurisdiction.
---

# Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators

## Quick Facts
- arXiv ID: 2404.00437
- Source URL: https://arxiv.org/abs/2404.00437
- Reference count: 11
- Primary result: System achieves >90% accuracy in classifying Spanish legal judgments and generates expert-validated explanations

## Executive Summary
This paper presents a system that combines NLP with interpretable ML to classify Spanish legal judgments into jurisdiction-dependent law categories while automatically generating explanations for its decisions. The approach uses decision tree-based estimators to extract features from legal text, classify judgments, and trace decision paths to identify relevant features. These features are then mapped to legal terms and validated by legal experts to produce natural language explanations. The system demonstrates high accuracy (>90%) and produces explanations that are understandable to both legal experts and non-expert users.

## Method Summary
The system preprocesses Spanish legal judgments by removing stop words and lemmatizing text, then generates char-grams (3-7 characters) and word-grams (unigrams/bi-grams) as features. Features are selected using chi-squared scoring with frequency thresholds (5-50%). The system trains parallel classifiers per jurisdiction using Random Forests, Decision Trees, SVM, and Gradient Boosting, with optimized hyperparameters via GridSearchCV. For explicability, it extracts decision paths from tree structures, identifies relevant features based on bifurcation thresholds, reconstructs terms, and validates them through legal expert review to generate natural language explanations.

## Key Results
- Achieved classification accuracy well above 90% on an annotated dataset of 96,163 Spanish legal judgments
- Generated explanations validated by legal experts as accurate and relevant
- Demonstrated that explanations are easily understandable to non-expert users in pilot testing
- Showed superior performance compared to pre-trained language models adapted for legal domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decision tree-based classifiers can produce interpretable decision paths that reveal which features drive a classification.
- Mechanism: The system extracts features from text, trains a decision tree, and traces the decision path to identify features meeting bifurcation thresholds. These features map to legal terms for explanations.
- Core assumption: Features satisfying "greater than threshold" conditions in decision paths are most relevant for classification.
- Evidence anchors:
  - [abstract] "We analyze the features involved in the decision and the threshold bifurcation values of the decision paths of tree structures and present this information to the users in natural language."
  - [section 3.3] "We consider that a feature in a decision path is relevant if the logical routing condition ' >' is fulfilled for that feature at some node in the path."

### Mechanism 2
- Claim: Char-gram and word-gram features capture sufficient textual patterns for accurate jurisdiction-dependent law category classification.
- Mechanism: The system generates n-grams (char-grams for character sequences, word-grams for word sequences), selects relevant ones via Chi-squared scoring, and uses them as classifier features.
- Core assumption: Distribution of n-grams in legal judgments correlates with law category and jurisdiction.
- Evidence anchors:
  - [section 3.2.1] "We employ two types of n-grams, char-grams, and word-grams... A word-gram is any sequence of n words in the text."
  - [section 4.3] "We tried different ranges of values for these parameters and checked the resulting model accuracies for different combinations of values."

### Mechanism 3
- Claim: Human-in-the-loop validation of relevant terms ensures explanations are meaningful to legal experts.
- Mechanism: Legal experts review top 50 most frequent relevant features for each (jurisdiction, law category) pair and mark truly relevant ones. This validated set forms the "expert-in-the-loop" dictionary used in explanations.
- Core assumption: Legal experts can reliably identify which features are legally meaningful for each category.
- Evidence anchors:
  - [section 4.5] "The expert layer was presented with two questions on each expansion of a selected relevant feature for law category A and jurisdiction B."
  - [section 4.5] "This produced an 'expert-in-the-loop' dictionary per (jurisdiction, law category) pair with up to 50 expanded terms."

## Foundational Learning

- Concept: Feature engineering with n-grams
  - Why needed here: Legal texts contain specific terminology and patterns that can be captured by character and word sequences; these become the input for ML classification.
  - Quick check question: What is the difference between a char-gram and a word-gram, and why might both be useful for legal text?

- Concept: Decision tree interpretability
  - Why needed here: The system relies on decision trees (or forests) to provide transparent decision paths that can be translated into human-readable explanations.
  - Quick check question: How does a decision tree determine which feature to split on at each node, and how is that used to explain a classification?

- Concept: Human-in-the-loop validation
  - Why needed here: Ensures automatically extracted features are truly meaningful in legal context, not just statistically significant.
  - Quick check question: Why is it important to validate the relevance of features with legal experts rather than relying solely on automated feature selection?

## Architecture Onboarding

- Component map: Raw text → preprocessing → feature engineering → classification → explicability → explanation
- Critical path: Raw text → preprocessing (stop word removal, lemmatization, jurisdiction sorting) → feature engineering (char-gram/word-gram generation, feature selection) → classification (parallel classifiers per jurisdiction) → explicability (decision path extraction, feature relevance check, term reconstruction) → explanation
- Design tradeoffs:
  - Accuracy vs interpretability: Random Forests give better accuracy than Decision Trees but are slightly less interpretable
  - Feature granularity vs noise: Char-grams can capture subtle patterns but may introduce noise if too short
  - Expert validation vs automation: Expert-in-the-loop improves quality but adds cost and time
- Failure signatures:
  - Low accuracy on test #2 (real-world proportions) suggests overfitting to training distribution
  - Explanations containing meaningless terms indicate term reconstruction or validation failure
  - High variance between jurisdictions may signal insufficient training data for some categories
- First 3 experiments:
  1. Run classification with only word-grams (no char-grams) to measure impact on accuracy and explanation quality
  2. Test the system with a single jurisdiction to verify end-to-end pipeline before scaling to all jurisdictions
  3. Compare explanations generated with and without the expert-in-the-loop dictionary to quantify expert contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the system handle the classification of judgments with multiple law categories across different jurisdictions?
- Basis in paper: [explicit] The paper mentions that 59.78% of judgments have a single law category label, 32.68% have one alternative law category label, and 7.54% have two alternative law category labels.
- Why unresolved: The paper does not provide details on how the system handles the classification of judgments with multiple law categories across different jurisdictions, especially when the labels are distributed across different jurisdictions.
- What evidence would resolve it: Experimental results showing the system's performance in classifying judgments with multiple law categories across different jurisdictions, including metrics such as precision, recall, and F1-score for each jurisdiction and law category.

### Open Question 2
- Question: How does the system's performance compare to other state-of-the-art methods for legal text classification, particularly those using transformer-based language models?
- Basis in paper: [inferred] The paper mentions that most pre-trained language models' embeddings are not adapted to the legal domain and tend to underperform compared to traditional ML algorithms like random forests.
- Why unresolved: The paper does not provide a direct comparison of the system's performance to other state-of-the-art methods, particularly those using transformer-based language models, which have shown promising results in various NLP tasks.
- What evidence would resolve it: Experimental results comparing the system's performance to other state-of-the-art methods, including transformer-based language models, on the same dataset or a similar dataset, using the same evaluation metrics.

### Open Question 3
- Question: How does the system handle the classification of judgments in jurisdictions with a small number of samples or highly imbalanced data?
- Basis in paper: [explicit] The paper mentions that some jurisdictions have highly imbalanced data, with some law categories having significantly more samples than others.
- Why unresolved: The paper does not provide details on how the system handles the classification of judgments in jurisdictions with a small number of samples or highly imbalanced data, which can affect the model's performance and generalizability.
- What evidence would resolve it: Experimental results showing the system's performance in classifying judgments in jurisdictions with a small number of samples or highly imbalanced data, including metrics such as precision, recall, and F1-score for each jurisdiction and law category, as well as techniques used to address the imbalance, such as oversampling or undersampling.

## Limitations
- The proprietary nature of the E4Legal Analytics dataset prevents independent verification of results
- Limited analysis of explanation quality beyond expert validation, with small-scale user study (8 non-experts)
- High accuracy scores may be influenced by dataset characteristics not fully disclosed

## Confidence
- High confidence: The general framework combining NLP with interpretable ML for legal text classification is sound and well-established
- Medium confidence: The claim that explanations are "easily understandable even to non-expert users" based on a small pilot test (8 non-experts) is promising but requires broader validation
- Medium confidence: The high accuracy scores (>90%) are impressive but may be influenced by dataset characteristics that are not fully disclosed

## Next Checks
1. Conduct a larger-scale user study with diverse participants (legal experts, non-experts, and intermediate users) to evaluate explanation comprehensibility and usefulness
2. Perform ablation studies to quantify the individual contributions of char-grams vs word-grams, expert validation, and different classifier types to overall performance
3. Test the system on an external, publicly available Spanish legal corpus to assess generalizability and robustness to different data distributions