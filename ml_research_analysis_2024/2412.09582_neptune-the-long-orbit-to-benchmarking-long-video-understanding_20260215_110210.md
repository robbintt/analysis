---
ver: rpa2
title: 'Neptune: The Long Orbit to Benchmarking Long Video Understanding'
arxiv_id: '2412.09582'
source_url: https://arxiv.org/abs/2412.09582
tags: []
core_contribution: This paper introduces Neptune, a new benchmark for long video understanding
  that requires reasoning over extended time horizons and across multiple modalities.
  To address the challenge of creating high-quality annotations for long videos, the
  authors propose a scalable pipeline that leverages large vision-language models
  (VLMs) and large language models (LLMs) to automatically generate dense, time-aligned
  video captions and complex question-answer-decoy sets for videos up to 15 minutes
  long.
---

# Neptune: The Long Orbit to Benchmarking Long Video Understanding

## Quick Facts
- arXiv ID: 2412.09582
- Source URL: https://arxiv.org/abs/2412.09582
- Reference count: 40
- New benchmark for long video understanding with reasoning over extended time horizons

## Executive Summary
Neptune is a novel benchmark designed to advance long video understanding by requiring models to reason over extended time horizons and across multiple modalities. The dataset comprises 3,268 questions across 2,405 videos, with annotations generated through an innovative pipeline leveraging large vision-language models and large language models. Current open-source long video models perform poorly on Neptune, particularly on temporal ordering, counting, and state changes, highlighting the benchmark's challenging nature and its potential to drive model development in this domain.

## Method Summary
The Neptune benchmark addresses the challenge of creating high-quality annotations for long videos through a scalable pipeline that automatically generates dense, time-aligned video captions and complex question-answer-decoy sets for videos up to 15 minutes long. This approach leverages large vision-language models (VLMs) and large language models (LLMs) to create the annotations. The dataset covers diverse long video reasoning abilities and introduces a new open-source model-based metric called GEM for scoring open-ended responses, aiming to provide a comprehensive evaluation framework for long video understanding.

## Key Results
- Current open-source long video models perform poorly on Neptune, especially on temporal ordering, counting, and state changes
- Neptune consists of 3,268 questions across 2,405 videos, testing diverse long video reasoning abilities
- The proposed VLM and LLM-based annotation pipeline enables scalable creation of high-quality annotations for long videos

## Why This Works (Mechanism)
Neptune works by providing a challenging benchmark that requires models to understand and reason over extended video sequences, going beyond simple frame-level analysis. The automatic annotation pipeline enables the creation of a large, diverse dataset that tests various aspects of long video understanding, including temporal reasoning, counting, and state changes. The introduction of the GEM metric allows for more nuanced evaluation of open-ended responses, capturing the complexity of long video understanding tasks.

## Foundational Learning
- Long video understanding: The ability to comprehend and reason over extended video sequences, requiring models to track objects, actions, and relationships over time.
  - Why needed: Traditional video understanding benchmarks focus on short clips, failing to capture the challenges of long-form video content.
  - Quick check: Evaluate model performance on tasks requiring temporal reasoning over sequences longer than 5 minutes.

- Multi-modal reasoning: The integration of visual and textual information to answer complex questions about video content.
  - Why needed: Long videos often contain rich visual and auditory information that must be combined for comprehensive understanding.
  - Quick check: Test model's ability to answer questions requiring both visual observation and language comprehension.

- Automatic annotation generation: The use of VLMs and LLMs to create high-quality, time-aligned annotations for long videos at scale.
  - Why needed: Manual annotation of long videos is time-consuming and expensive, limiting dataset size and diversity.
  - Quick check: Compare VLM/LLM-generated annotations with human annotations on a subset of videos to assess quality and consistency.

## Architecture Onboarding

Component map: Video input -> VLM for captioning -> LLM for question generation -> Question-answer-decoy set -> GEM scoring metric

Critical path: The critical path involves processing the input video through the VLM to generate dense captions, then using the LLM to create complex questions and answer sets based on these captions. The GEM metric is then applied to evaluate open-ended responses to these questions.

Design tradeoffs: The pipeline trades manual annotation quality for scalability and diversity. While VLM and LLM-generated annotations may not be perfect, they enable the creation of a much larger and more diverse dataset than would be possible with human annotation alone.

Failure signatures: Potential failures include VLM caption errors propagating to question generation, LLM producing ambiguous or misleading questions, and GEM metric misalignment with human judgment of answer quality.

First experiments:
1. Evaluate VLM caption quality on a subset of videos by comparing with human-generated captions
2. Test LLM question generation diversity and complexity by having human raters assess question quality
3. Validate GEM metric effectiveness by comparing its scores with those of human raters on a set of open-ended responses

## Open Questions the Paper Calls Out
None

## Limitations
- Confidence in model performance claims is Medium due to limited details on evaluation methodology
- Validation of annotation quality and potential biases in the automatic generation process is unclear
- Dataset size may limit statistical significance for certain evaluation tasks

## Confidence
- Model performance claims: Medium
- Annotation quality and reliability: Medium
- GEM metric effectiveness: Low

## Next Checks
1. Conduct inter-annotator agreement studies comparing VLM/LLM-generated annotations against human annotations on a representative subset of Neptune videos
2. Perform ablation studies to quantify how specific reasoning abilities (temporal ordering, counting, state changes) contribute to overall model performance differences
3. Test GEM metric consistency by having multiple expert raters score the same open-ended responses and comparing against GEM outputs to establish correlation and reliability metrics