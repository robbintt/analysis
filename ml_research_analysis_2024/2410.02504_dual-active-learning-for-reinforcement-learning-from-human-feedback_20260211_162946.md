---
ver: rpa2
title: Dual Active Learning for Reinforcement Learning from Human Feedback
arxiv_id: '2410.02504'
source_url: https://arxiv.org/abs/2410.02504
tags:
- learning
- reward
- have
- policy
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning large language models
  with human preferences in reinforcement learning from human feedback (RLHF). The
  core problem is that human feedback is costly and time-consuming, while teachers
  have varying levels of expertise that impact the quality of their feedback.
---

# Dual Active Learning for Reinforcement Learning from Human Feedback

## Quick Facts
- arXiv ID: 2410.02504
- Source URL: https://arxiv.org/abs/2410.02504
- Authors: Pangpang Liu, Chengchun Shi, Will Wei Sun
- Reference count: 40
- Primary result: 1.77%-9.06% improvement in reward accuracy over state-of-the-art methods

## Executive Summary
This paper addresses the challenge of aligning large language models with human preferences in reinforcement learning from human feedback (RLHF) by proposing a dual active learning algorithm. The core innovation lies in simultaneously selecting both conversations and teachers for feedback collection, using D-optimal design principles to maximize reward estimator accuracy. The method also incorporates pessimistic reinforcement learning to handle distributional shifts between behavior and optimal policies.

## Method Summary
The paper proposes a dual active learning algorithm that strategically selects both which conversations to collect feedback on and which teachers to provide that feedback. The approach uses D-optimal design to maximize the accuracy of the reward estimator by choosing the most informative samples and teachers. It employs pessimistic reinforcement learning to handle distributional shifts between the behavior policy and the optimal policy. The theoretical analysis shows that the proposed reward estimator achieves minimal generalized variance asymptotically, with sub-optimality scaling as O(1/√T) with sample budget T.

## Key Results
- Achieves 1.77%-9.06% improvement in reward accuracy compared to state-of-the-art methods
- Theoretical guarantee of minimal generalized variance asymptotically for the reward estimator
- Sub-optimality of learned policy scales as O(1/√T) with sample budget T
- Demonstrated effectiveness on public LLM datasets

## Why This Works (Mechanism)
The dual active learning framework works by simultaneously optimizing sample selection and teacher selection through D-optimal design principles. This allows the algorithm to strategically collect the most informative feedback from the most reliable teachers, maximizing reward estimator accuracy while minimizing the number of expensive human feedback queries. The pessimistic reinforcement learning component provides robustness against distributional shifts that occur when the learned policy diverges from the behavior policy.

## Foundational Learning

1. **D-optimal Design**
   - Why needed: Maximizes determinant of Fisher information matrix to achieve most efficient parameter estimation
   - Quick check: Verify design efficiency through A-optimality and E-optimality metrics

2. **Pessimistic Reinforcement Learning**
   - Why needed: Handles distributional shifts between behavior and optimal policies
   - Quick check: Validate pessimism parameter through performance bounds

3. **Generalized Variance Minimization**
   - Why needed: Achieves most efficient covariance structure for reward estimator
   - Quick check: Confirm asymptotic efficiency through trace and determinant analysis

## Architecture Onboarding

**Component Map**: Data Generator -> Reward Estimator -> Policy Optimizer -> Teacher Selector -> Sample Selector

**Critical Path**: Conversation Selection → Teacher Assignment → Feedback Collection → Reward Update → Policy Update

**Design Tradeoffs**: The dual selection framework trades computational complexity for reduced human feedback costs. The D-optimal design requires matrix operations that scale with sample size, but this is offset by more efficient feedback utilization.

**Failure Signatures**: 
- Poor teacher selection leading to high variance estimates
- Suboptimal sample selection causing slow convergence
- Distributional shift exceeding pessimistic bounds
- Numerical instability in matrix inversions for D-optimal design

**First 3 Experiments**:
1. Test basic reward estimation accuracy on synthetic preference data with known ground truth
2. Validate teacher selection accuracy under varying expertise distributions
3. Measure distributional shift handling by comparing with and without pessimistic RL

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations

**Major Theoretical Limitations**:
- Theoretical guarantees rely on asymptotic assumptions that may not hold in practical RLHF settings
- O(1/√T) sub-optimality scaling requires validation under non-stationary reward functions
- D-optimal design framework assumes linear reward models, which may be restrictive

**Empirical Validation Limitations**:
- Improvements shown on public datasets only, not real-world deployment scenarios
- Lack of analysis on interaction between pessimistic RL and active sampling strategy
- No validation under heterogeneous teacher expertise and selection costs

## Confidence

*High confidence*: The dual active learning framework is well-grounded in established active learning theory and the general approach to simultaneous sample and teacher selection is methodologically sound.

*Medium confidence*: The theoretical bounds and their practical applicability, particularly the asymptotic results and O(1/√T) scaling, require additional empirical validation under realistic RLHF conditions.

*Medium confidence*: The empirical improvements reported are based on public datasets; performance in production environments with diverse teacher populations remains to be validated.

## Next Checks

1. Conduct experiments with varying teacher expertise distributions and selection costs to validate the dual selection framework under realistic heterogeneity.

2. Test the algorithm's performance when reward functions are non-linear and when distributional shifts are more severe than the controlled experimental conditions.

3. Evaluate the interaction between pessimistic RL and active sampling by comparing against baselines that use alternative distributional shift handling methods.