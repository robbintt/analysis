---
ver: rpa2
title: 'IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with
  Multimodal Capabilities'
arxiv_id: '2408.12902'
source_url: https://arxiv.org/abs/2408.12902
tags:
- language
- multimodal
- arxiv
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Inner-Adaptor Architecture (IAA) to enable
  frozen large language models (LLMs) to acquire multimodal capabilities without fine-tuning
  the LLM itself. Previous approaches either require unfreezing the LLM, which degrades
  NLP performance, or use frozen LLMs that fail to achieve strong multimodal results.
---

# IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities

## Quick Facts
- arXiv ID: 2408.12902
- Source URL: https://arxiv.org/abs/2408.12902
- Authors: Bin Wang; Chunyu Xie; Dawei Leng; Yuhui Yin
- Reference count: 12
- Enables frozen LLMs to acquire multimodal capabilities while preserving NLP performance

## Executive Summary
This paper introduces the Inner-Adaptor Architecture (IAA) to enable frozen large language models to achieve multimodal capabilities without fine-tuning the LLM itself. Previous approaches either required unfreezing the LLM, which degraded NLP performance, or used frozen LLMs that failed to achieve strong multimodal results. IAA addresses this by inserting trainable adaptors at multiple depths within the frozen LLM, allowing direct interaction between multimodal features and text-oriented transformer layers. The architecture achieves state-of-the-art performance on general multimodal and visual grounding benchmarks while preserving NLP ability.

## Method Summary
IAA employs a two-stage pre-training process that first aligns vision and language components, followed by instruction and grounding fine-tuning. The core innovation involves inserting trainable adaptors at multiple depths within the frozen LLM, creating direct pathways for multimodal features to interact with text-oriented transformer layers. This architecture enables the frozen LLM to process multimodal inputs while maintaining its original text-only capabilities, overcoming the limitations of previous approaches that either degraded NLP performance when unfreezing the LLM or failed to achieve strong multimodal results with frozen LLMs.

## Key Results
- Achieves 74.9, 70.5, and 39.9 on MME, MMBench, and MMMU multimodal benchmarks respectively
- Maintains 68.4 and 51.3 on text-only benchmarks (MMLU, C-Eval)
- Outperforms LLaVA-Llama3 on both multimodal tasks and text-only evaluations
- Preserves NLP ability while achieving state-of-the-art multimodal performance

## Why This Works (Mechanism)
IAA works by inserting trainable adaptors at multiple depths within the frozen LLM architecture, creating multiple interaction points between multimodal features and text-oriented transformer layers. This multi-depth adaptor insertion allows the model to progressively integrate visual information throughout the transformer hierarchy rather than attempting to inject it only at the input or output layers. The two-stage pre-training process first establishes alignment between vision and language components, creating a foundation for the adaptors to build upon during the instruction and grounding fine-tuning phase.

## Foundational Learning

**Multimodal Feature Integration**: Understanding how visual and textual features can be combined within transformer architectures - needed to create effective cross-modal representations; quick check: verify feature fusion occurs at multiple transformer depths

**Adaptor Layer Design**: Knowledge of how small trainable modules can modify frozen model behavior - needed to enable capability extension without full fine-tuning; quick check: confirm adaptor parameters remain trainable while LLM weights freeze

**Cross-Modal Alignment**: Principles of aligning different input modalities during pre-training - needed to establish foundation for multimodal understanding; quick check: verify alignment quality before adaptor insertion

**Transformer Depth Progression**: Understanding how information flows through multiple transformer layers - needed to determine optimal adaptor insertion points; quick check: trace feature interaction across adaptor positions

## Architecture Onboarding

**Component Map**: Vision Encoder -> Multimodal Projector -> IAA Adaptors (multiple positions) -> Frozen LLM -> Output Generator

**Critical Path**: Visual input flows through vision encoder, projects to text space, passes through multiple IAA adaptors at different transformer depths, reaches frozen LLM core, and generates multimodal output

**Design Tradeoffs**: Multiple adaptor positions increase multimodal integration capability but add computational overhead; freezing LLM preserves NLP performance but limits full fine-tuning potential; two-stage pre-training balances alignment quality with training efficiency

**Failure Signatures**: Poor multimodal performance suggests adaptor insertion points are suboptimal; degraded NLP results indicate adaptor interference with frozen LLM; alignment issues manifest as incoherent cross-modal representations

**3 First Experiments**:
1. Test single adaptor position versus multiple positions to verify depth-progressive integration benefit
2. Evaluate adaptor-frozen ratio to find optimal balance between new capability and preserved performance
3. Compare two-stage pre-training versus direct fine-tuning to isolate pre-training contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to larger LLM backbones beyond Llama-3-8B requires further investigation
- Computational overhead of multiple adaptor layers across transformer depths needs evaluation
- Performance metrics may not fully capture real-world deployment scenarios with diverse input complexities

## Confidence

**High Confidence**: Preservation of NLP performance when freezing LLM core, demonstrated through consistent text-only benchmark results across multiple datasets

**Medium Confidence**: Generalizability of IAA's performance gains to other frozen LLM architectures beyond Llama-3, given architecture-specific nature of adaptor integration

**Medium Confidence**: Long-term stability of multimodal capabilities after training, particularly under continuous deployment conditions with diverse input distributions

## Next Checks
1. Test IAA with alternative frozen LLM backbones (e.g., Mistral, Claude) to verify architectural independence and performance transferability
2. Conduct ablation studies removing the two-stage pre-training to isolate its contribution to final performance versus the adaptor architecture itself
3. Evaluate computational efficiency during inference, specifically measuring latency and memory overhead introduced by multiple adaptor layers across transformer depths