---
ver: rpa2
title: Fast On-device LLM Inference with NPUs
arxiv_id: '2407.05858'
source_url: https://arxiv.org/abs/2407.05858
tags:
- mllm-npu
- mobile
- execution
- inference
- npus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of high inference latency for
  on-device large language models (LLMs), particularly the prefill stage, which is
  a bottleneck for tasks like screen UI understanding and personalized content generation.
  The authors present mllm-NPU, the first LLM inference system that efficiently utilizes
  on-device Neural Processing Units (NPUs) to reduce prefill latency.
---

# Fast On-device LLM Inference with NPUs

## Quick Facts
- arXiv ID: 2407.05858
- Source URL: https://arxiv.org/abs/2407.05858
- Reference count: 40
- Achieves 22.4x faster prefill speed and 30.7x energy savings for on-device LLM inference

## Executive Summary
This paper addresses the critical challenge of high inference latency for on-device large language models, particularly during the prefill stage which is essential for tasks like screen UI understanding and personalized content generation. The authors introduce mllm-NPU, the first LLM inference system that efficiently leverages on-device Neural Processing Units (NPUs) to accelerate inference while maintaining accuracy. By reconstructing prompts and model operations at three levels (prompt, tensor, and block), the system achieves significant performance improvements for billion-sized models on mobile devices.

## Method Summary
mllm-NPU addresses the prefill bottleneck in on-device LLM inference by strategically distributing operations between NPUs, CPUs, and GPUs. The system reconstructs the inference pipeline at three levels: prompt-level chunking divides variable-length prompts into fixed-sized segments for efficient NPU processing; tensor-level outlier detection identifies significant numerical values requiring floating-point precision and routes them to CPU/GPU; and block-level scheduling assigns Transformer blocks to hardware based on their computational characteristics and accuracy requirements. This hybrid approach maximizes NPU utilization for integer operations while preserving accuracy-critical floating-point computations on traditional processors.

## Key Results
- Achieves 22.4x faster prefill speed compared to baseline implementations
- Delivers 30.7x energy savings during inference operations
- Reaches over 1,000 tokens/second prefilling rate for billion-sized models for the first time

## Why This Works (Mechanism)
The system works by exploiting the computational strengths of different hardware components: NPUs excel at integer operations and can be maximally utilized for the bulk of LLM computations, while CPUs and GPUs handle floating-point operations that are accuracy-critical. The three-level reconstruction approach ensures that each hardware component processes operations it can handle most efficiently, creating a synergistic acceleration effect that dramatically reduces prefill latency without sacrificing model accuracy.

## Foundational Learning

**Neural Processing Units (NPUs)** - Specialized hardware accelerators designed for machine learning workloads, particularly efficient at integer operations and matrix computations. Needed because NPUs provide superior performance-per-watt compared to general-purpose processors for AI inference tasks. Quick check: Verify NPU capabilities support INT8/INT4 operations commonly used in quantized LLMs.

**Transformer Block Scheduling** - Strategic assignment of different Transformer layers to appropriate hardware based on their computational intensity and accuracy sensitivity. Needed because not all layers benefit equally from acceleration, and some require higher precision to maintain model performance. Quick check: Profile individual Transformer blocks to identify those most sensitive to quantization or requiring floating-point precision.

**Outlier Detection in Tensors** - Identification of numerically significant values within activation tensors that require special handling to preserve accuracy. Needed because LLMs often contain extreme values that are crucial for maintaining model behavior when using quantization techniques. Quick check: Implement statistical analysis of activation distributions to establish outlier thresholds.

## Architecture Onboarding

Component Map: Prompt Chunks -> Tensor Processing -> Block Scheduling -> NPU/CPU/GPU Execution -> Output Generation

Critical Path: Input prompt → Chunking → Outlier extraction → Hardware scheduling → NPU acceleration → CPU/GPU refinement → Output tokens

Design Tradeoffs: The system balances NPU utilization against accuracy preservation by selectively keeping floating-point operations on CPU/GPU. This creates a performance-accuracy trade-off where maximum acceleration requires accepting some precision loss, while maintaining full accuracy limits NPU utilization.

Failure Signatures: Performance degradation occurs when outlier detection is too aggressive (excessive CPU/GPU load) or too conservative (accuracy loss). Energy inefficiency manifests when operations are misallocated to less efficient hardware. Model accuracy drops when critical floating-point operations are incorrectly assigned to integer-only NPUs.

Three First Experiments:
1. Profile baseline prefill latency and energy consumption on target NPU hardware
2. Implement and test the three-level reconstruction approach with synthetic prompts
3. Conduct accuracy validation comparing quantized NPU execution against full-precision CPU execution

## Open Questions the Paper Calls Out

None identified in the source material.

## Limitations

- Performance gains heavily depend on specific NPU architectures and quantization strategies not fully detailed
- Trade-offs between accuracy loss from floating-point operations on CPU/GPU and performance gains are not quantitatively analyzed
- Results focus primarily on prefill optimization without extensive discussion of decode-phase performance or overall end-to-end improvements

## Confidence

High: Novel reconstruction methodology at three distinct levels (prompt, tensor, block)
Medium: Reported performance gains (22.4x speedup, 30.7x energy savings) and achievement of 1,000+ tokens/sec for billion-sized models
Medium: Generalizability across diverse hardware platforms and model architectures

## Next Checks

1. Test mllm-NPU on multiple NPU architectures beyond the specific hardware used in the study to verify cross-platform performance consistency
2. Conduct a comprehensive accuracy-latency trade-off analysis across different quantization levels and floating-point precision configurations
3. Evaluate the system's performance and energy efficiency on decode-phase operations to assess overall end-to-end inference improvements