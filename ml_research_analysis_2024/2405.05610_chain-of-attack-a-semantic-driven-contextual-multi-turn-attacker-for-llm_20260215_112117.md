---
ver: rpa2
title: 'Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM'
arxiv_id: '2405.05610'
source_url: https://arxiv.org/abs/2405.05610
tags:
- attack
- prompt
- target
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CoA (Chain of Attack), a semantic-driven contextual
  multi-turn attack method for large language models (LLMs) in dialogue systems. CoA
  adaptively adjusts attack strategies based on contextual feedback and semantic relevance
  during multi-turn dialogue, aiming to expose vulnerabilities by guiding models to
  produce unreasonable or harmful content.
---

# Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM

## Quick Facts
- arXiv ID: 2405.05610
- Source URL: https://arxiv.org/abs/2405.05610
- Reference count: 18
- Most successful attacks occur within 3 rounds with average 20 queries

## Executive Summary
This paper introduces Chain of Attack (CoA), a semantic-driven contextual multi-turn attack method designed to expose vulnerabilities in large language models (LLMs) used in dialogue systems. The approach adaptively adjusts attack strategies based on contextual feedback and semantic relevance during multi-turn dialogue, guiding models to produce unreasonable or harmful content. CoA operates through a three-step process: seed attack chain generation, attack chain execution, and context-driven attack policy updates. Experiments demonstrate high attack success rates across multiple open-source and closed-source models, with most successful attacks completing within 3 rounds and an average of 20 queries per attack.

## Method Summary
CoA is a semantic-driven contextual multi-turn attack method for LLMs in dialogue systems that adaptively adjusts attack policy through contextual feedback and semantic relevance during multi-turn dialogue. The method uses three main steps: seed attack chain generation, attack chain execution, and context-driven attack policy updates. It leverages semantic correlation between model responses and target queries, ensuring each turn increases relevance until alignment is breached. The approach tests on PAIR and GCG50 datasets across multiple LLMs including Vicuna-13b, Llama2-7b-chat, Chatglm2-6b, Baichuan2-7b-chat, and GPT-3.5-turbo, with GPT-3.5-turbo serving as the evaluator model.

## Key Results
- High attack success rates achieved across multiple models and datasets
- Most successful attacks occur within 3 dialogue rounds
- Average of 20 queries per successful attack
- Effectiveness demonstrated through semantic correlation-based policy selection and context-aware prompt refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic relevance increases gradually guide models from safe to unsafe content
- Mechanism: The attack uses semantic correlation between model responses and target queries, ensuring each turn increases relevance until alignment is breached
- Core assumption: Models will follow semantic gradients even when they lead to unsafe content
- Evidence anchors:
  - [abstract] "CoA is a semantic-driven contextual multi-turn attack method that adaptively adjusts the attack policy through contextual feedback and semantic relevance during multi-turn of dialogue"
  - [section 3] "We posit a hypothesis: the semantic correlation between the target model's responses and the designated task will progressively enhance as the rounds increase"
  - [corpus] Weak - related papers focus on structural vulnerabilities but don't explicitly discuss semantic gradient following
- Break condition: If model implements semantic discontinuity detection or refuses to follow decreasing relevance paths

### Mechanism 2
- Claim: Multi-turn context allows circumvention of single-turn safety mechanisms
- Mechanism: By building conversation context over multiple turns, attackers can guide models through intermediate safe topics toward unsafe content
- Core assumption: Single-turn alignment mechanisms don't effectively track semantic trajectories across multiple turns
- Evidence anchors:
  - [abstract] "CoA adaptively adjusts the attack policy through contextual feedback and semantic relevance during multi-turn of dialogue"
  - [section 2] "multi round conversations where large models are more easily guided by contextual content, resulting in harmful or biased responses"
  - [corpus] Moderate - Pattern Enhanced Multi-Turn Jailbreaking discusses exploiting conversational context, supporting this mechanism
- Break condition: If models implement cross-turn safety monitoring or conversation-level alignment tracking

### Mechanism 3
- Claim: Context-driven prompt refinement adapts attacks based on model responses
- Mechanism: The attacker model analyzes previous responses and refines prompts to better align with attack objectives
- Core assumption: Models can be systematically guided by prompt modifications based on their own outputs
- Evidence anchors:
  - [abstract] "CoA adaptively adjusts the attack policy through contextual feedback and semantic relevance during multi-turn of dialogue"
  - [section 4.3.2] "The attack prompt is refined based on the response received from the target model"
  - [corpus] Moderate - Chain-of-Lure discusses synthetic narratives for attacks, supporting adaptive prompting approaches
- Break condition: If models detect and resist iterative prompt manipulation patterns

## Foundational Learning

- Concept: Semantic correlation and embedding similarity
  - Why needed here: The attack relies on measuring semantic correlation between responses and target queries using embedding similarity
  - Quick check question: How would you calculate the semantic correlation between two texts using sentence embeddings?

- Concept: Multi-turn dialogue context modeling
  - Why needed here: Understanding how conversational context influences model behavior across multiple turns
  - Quick check question: What mechanisms do LLMs use to maintain context across dialogue turns?

- Concept: Prompt engineering and adversarial prompting
  - Why needed here: The attack requires sophisticated prompt engineering to gradually guide models toward unsafe content
  - Quick check question: What are common techniques for creating adversarial prompts that bypass safety mechanisms?

## Architecture Onboarding

- Component map: Seed Generator → Executor → Updater → Evaluator → (repeat or terminate)
- Critical path: Seed Generator → Executor → Updater → Evaluator → (repeat or terminate)
- Design tradeoffs:
  - Query efficiency vs. attack success rate: More rounds increase success but also resource usage
  - Semantic relevance measurement vs. computational overhead: SIMCSE embeddings add latency
  - Prompt specificity vs. generality: More targeted prompts may be more effective but less transferable
- Failure signatures:
  - Early termination due to model refusal
  - Semantic correlation plateauing or decreasing
  - Model generating off-topic responses
  - Evaluator model flagging false positives
- First 3 experiments:
  1. Test semantic correlation calculation on simple examples to verify embedding similarity works as expected
  2. Run single-turn attack with seed generator to verify basic prompt creation functionality
  3. Execute complete 3-round attack chain on a simple, well-understood target task to verify full pipeline operation

## Open Questions the Paper Calls Out
None

## Limitations

- Unknown implementation details: The paper does not provide specific threshold values for semantic correlation (SEM) that trigger different attack policies, making exact behavior and effectiveness unclear.
- Evaluation methodology concerns: Using GPT-3.5-turbo as an evaluator for detecting unsafe content introduces potential bias and uncertainty.
- Limited dataset scope: Testing on only PAIR and GCG50 datasets (50 toxic samples) may not capture the full range of potential attack scenarios or model vulnerabilities.

## Confidence

- High confidence: The core claim that multi-turn contextual attacks can expose LLM vulnerabilities through semantic gradient following is theoretically sound
- Medium confidence: The reported attack success rates and their significance depend on evaluator model behavior and undisclosed threshold choices
- Medium confidence: The practical applicability is suggested by average 20 queries per successful attack, but real-world deployment considerations are not explored

## Next Checks

1. **Threshold sensitivity analysis**: Systematically vary the semantic correlation thresholds for policy selection to determine their impact on attack success rates and identify optimal parameter ranges.

2. **Evaluator model ablation study**: Test the attack method with multiple evaluator models (including human evaluation) to assess the reliability and consistency of attack success determination.

3. **Cross-dataset generalization test**: Evaluate the method's performance on additional datasets beyond PAIR and GCG50 to assess its robustness across diverse attack scenarios and model types.