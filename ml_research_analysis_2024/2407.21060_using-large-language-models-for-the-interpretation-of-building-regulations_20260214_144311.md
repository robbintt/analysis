---
ver: rpa2
title: Using Large Language Models for the Interpretation of Building Regulations
arxiv_id: '2407.21060'
source_url: https://arxiv.org/abs/2407.21060
tags:
- exemplars
- building
- lrml
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of large language models (LLMs) for
  translating building regulations into a formal representation suitable for automated
  compliance checking. By leveraging few-shot learning and contextual prompts, GPT-3.5
  successfully translates complex regulatory text into LegalRuleML with competitive
  performance compared to supervised models.
---

# Using Large Language Models for the Interpretation of Building Regulations

## Quick Facts
- arXiv ID: 2407.21060
- Source URL: https://arxiv.org/abs/2407.21060
- Authors: Stefan Fuchs; Michael Witbrock; Johannes Dimyadi; Robert Amor
- Reference count: 0
- One-line primary result: LLMs can translate building regulations into LegalRuleML with F1-scores up to 70.3% using few-shot learning and contextual prompts

## Executive Summary
This paper explores the use of large language models for translating building regulations into a formal representation suitable for automated compliance checking. The authors leverage few-shot learning with GPT-3.5 to generate LegalRuleML rules without explicit training, achieving competitive performance compared to supervised models. By providing exemplars and contextual prompts, the model learns to map natural language clauses to formal representations. The approach significantly reduces the effort required to automate the conversion of regulatory text into machine-readable formats, with promising results for practical applications.

## Method Summary
The paper employs few-shot learning with GPT-3.5-turbo to translate building regulations from the New Zealand Building Code into LegalRuleML format. The method uses representative sampling strategies to select exemplars for each target clause, contextual prompts to guide the model's reasoning, and evaluates translations using F1-scores and BLEU metrics. The approach is compared against a T5 model fine-tuned with synthetic data, demonstrating competitive performance while requiring less training effort.

## Key Results
- GPT-3.5 achieves F1-scores up to 70.3% when translating building regulations into LegalRuleML using few-shot learning
- Contextual prompts and representative exemplar selection significantly improve translation accuracy
- The LLM approach requires minimal training effort compared to supervised models while maintaining competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot learning with GPT-3.5 can generate LegalRuleML rules with reasonable accuracy using only a handful of exemplars.
- Mechanism: The model learns the structural mapping between natural language clauses and LegalRuleML through in-context exemplars, enabling translation without explicit training.
- Core assumption: The model has been pre-trained on enough diverse data to recognize patterns in legal and regulatory language, and that a small number of exemplars is sufficient for the model to infer the mapping structure.
- Evidence anchors:
  - [abstract] "By providing GPT-3.5 with only a few example translations, it can learn the basic structure of the format."
  - [section] "Few-shot learning setup... By providing GPT-3.5 with only a few example translations, it can learn the basic structure of the format."
  - [corpus] Weak - related papers mention "rule generation" but not few-shot learning in this specific context.
- Break condition: If the regulatory text contains highly specialized jargon or novel constructs not present in the exemplars, the model may fail to generalize correctly.

### Mechanism 2
- Claim: Contextualization through system prompts improves translation accuracy by guiding the model's reasoning process.
- Mechanism: Providing explicit task descriptions, syntax specifications, and common terms helps the model understand the expected output format and reduce ambiguity.
- Core assumption: The model can use the additional context to better interpret the exemplars and generate more accurate translations.
- Evidence anchors:
  - [section] "We further specify the LegalRuleML representation and explore the existence of expert domain knowledge in the model."
  - [section] "To support the model further with the translation, we provide a description of the translation task, with the relevant syntax, special cases, and the most common relations and entities."
  - [corpus] Weak - no direct evidence from corpus neighbors about contextualization improving accuracy.
- Break condition: If the contextualization is too verbose or contains conflicting information, it may confuse the model and degrade performance.

### Mechanism 3
- Claim: Representative sampling strategies improve translation quality by selecting exemplars that closely match the target clause.
- Mechanism: By choosing exemplars that are semantically similar to the target clause (e.g., via n-gram overlap or semantic clustering), the model can better learn the mapping for that specific type of regulation.
- Core assumption: The similarity between exemplars and target clauses is a strong predictor of translation accuracy.
- Evidence anchors:
  - [section] "By using sophisticated methods to select the most appropriate exemplars for each clause to translate."
  - [section] "We propose a strategy similar to Drozdov et al. (2022)... We generate multiple predictions with different samples and sampling strategies."
  - [corpus] Moderate - related papers mention "rule generation" but not sampling strategies.
- Break condition: If the training data lacks exemplars similar to the target clause, the sampling strategy cannot find good matches, limiting its effectiveness.

## Foundational Learning

- Concept: Semantic parsing and formal representations (e.g., LegalRuleML)
  - Why needed here: Understanding how natural language regulations are converted into machine-readable formats is essential for grasping the translation task.
  - Quick check question: What is the difference between "ConstitutiveStatement" and "PrescriptiveStatement" in LegalRuleML?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: GPT models are based on transformers, and understanding self-attention is key to understanding how they process input text.
  - Quick check question: How does the decoder in a transformer generate output while paying attention to specific parts of the input?

- Concept: In-context learning and few-shot learning
  - Why needed here: The paper relies on few-shot learning, where the model learns from a small number of exemplars provided in the prompt.
  - Quick check question: What is the difference between in-context learning and traditional fine-tuning?

## Architecture Onboarding

- Component map:
  GPT-3.5 API -> Prompt engineering -> Sampling strategy -> Evaluation pipeline -> T5 model (for comparison)

- Critical path:
  1. Select exemplars using sampling strategy.
  2. Construct prompt with exemplars, context, and target clause.
  3. Send prompt to GPT-3.5 API.
  4. Parse and evaluate the generated LegalRuleML rule.
  5. Iterate on prompt engineering and sampling strategy to improve accuracy.

- Design tradeoffs:
  - Exemplar selection vs. context length: More exemplars improve accuracy but may exceed context limits.
  - Contextualization vs. exemplar count: Adding context improves accuracy but reduces the number of exemplars that fit.
  - Sampling strategy vs. computational cost: Sophisticated sampling (e.g., semantic clustering) may improve accuracy but increase computational overhead.

- Failure signatures:
  - Low F1-scores: Indicate poor translation quality, possibly due to inadequate exemplars or context.
  - Incomplete outputs: Suggest the model ran out of context or failed to generate the full rule.
  - Logical errors: Indicate the model misunderstood the regulatory text or made incorrect inferences.

- First 3 experiments:
  1. Random sampling with varying numbers of exemplars to establish baseline performance.
  2. Manual exemplar selection to test human judgment in exemplar selection.
  3. Contextualization with task descriptions to evaluate the impact of additional context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can retrieval-augmented LLMs or knowledge base querying improve the accuracy of building regulation translations by providing better access to domain-specific vocabulary and classification systems?
- Basis in paper: [inferred] The paper mentions that LLMs struggle with large vocabulary sizes and suggests that using retrieval-augmented LLMs or pre-training on the LRML vocabulary could help.
- Why unresolved: The paper only proposes these approaches as future work and does not test them.
- What evidence would resolve it: Empirical testing comparing translation accuracy with and without retrieval-augmented LLMs or knowledge base querying.

### Open Question 2
- Question: How effective is human-in-the-loop ranking of different LLM translations for the same clause in improving the quality of generated training data for supervised models?
- Basis in paper: [inferred] The paper suggests that human ranking of translations could be a valuable strategy to improve the quality of generated data, especially since discrimination is easier than generation.
- Why unresolved: The paper does not test this approach.
- What evidence would resolve it: Experimental comparison of supervised model performance trained on LLM-generated data with and without human ranking.

### Open Question 3
- Question: Can LLMs be effectively integrated into the regulatory drafting process to translate regulations into formal representations during publication, and what tool support would enhance this process?
- Basis in paper: [explicit] The paper proposes exploring the feasibility of involving regulators in translating regulations into formal representations during regulatory drafting and publishing.
- Why unresolved: This is proposed as future work and has not been tested.
- What evidence would resolve it: Case studies or pilot projects demonstrating the effectiveness and challenges of integrating LLMs into the regulatory drafting process.

## Limitations

- The method's performance is highly dependent on the quality and relevance of the few-shot exemplars provided to the model.
- The approach relies on a single dataset (New Zealand Building Code), limiting generalizability to other regulatory domains or jurisdictions.
- While F1-scores up to 70.3% are promising, they still indicate significant room for improvement before practical deployment.

## Confidence

- **High Confidence:** The core mechanism of using few-shot learning with LLMs for translating regulatory text into formal representations is well-supported by the experimental results and aligns with established practices in in-context learning.
- **Medium Confidence:** The claim that contextualization improves translation accuracy is supported by the methodology but lacks extensive comparative analysis against non-contextualized approaches.
- **Medium Confidence:** The assertion that representative sampling strategies enhance translation quality is plausible given the results, but the paper does not provide exhaustive comparisons across all sampling strategies or rule out confounding factors.

## Next Checks

1. Test the approach on building regulations from different countries or domains to assess cross-domain generalization.
2. Conduct experiments varying the number of exemplars and their semantic similarity to determine the optimal balance between exemplar count and translation accuracy.
3. Implement a human oversight pipeline where domain experts review and correct LLM-generated LegalRuleML rules, then measure how much human intervention is needed to achieve production-level accuracy.