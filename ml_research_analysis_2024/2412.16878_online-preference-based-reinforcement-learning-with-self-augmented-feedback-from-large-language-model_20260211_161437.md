---
ver: rpa2
title: Online Preference-based Reinforcement Learning with Self-augmented Feedback
  from Large Language Model
arxiv_id: '2412.16878'
source_url: https://arxiv.org/abs/2412.16878
tags:
- trajectory
- reward
- learning
- feedback
- rl-sallm-f
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of online preference-based reinforcement
  learning (PbRL) without relying on real-time human feedback or privileged rewards.
  The authors propose RL-SaLLM-F, which leverages a large language model (LLM) to
  generate self-augmented trajectories and provide preference labels for reward learning.
---

# Online Preference-based Reinforcement Learning with Self-augmented Feedback from Large Language Model

## Quick Facts
- arXiv ID: 2412.16878
- Source URL: https://arxiv.org/abs/2412.16878
- Authors: Songjun Tu; Jingbo Sun; Qichao Zhang; Xiangyuan Lan; Dongbin Zhao
- Reference count: 40
- Primary result: Achieves comparable performance to scripted teacher with privileged rewards using LLM-generated feedback in online PbRL

## Executive Summary
This paper presents RL-SaLLM-F, a novel approach for online preference-based reinforcement learning that eliminates the need for real-time human feedback or predefined rewards. The method leverages a large language model (LLM) to generate self-augmented trajectories and provide preference labels for reward learning. By addressing query ambiguity through a double-check mechanism and using LLM-imagined trajectories that better achieve task goals, RL-SaLLM-F achieves success rates on par with SAC using predefined rewards while maintaining an average label accuracy of 72.3% on the MetaWorld benchmark.

## Method Summary
RL-SaLLM-F combines LLM-generated feedback with reinforcement learning to enable online preference-based learning without human interaction. The method converts state trajectories into text, queries an LLM for preference labels, and uses a double-check mechanism to ensure label quality. It generates self-augmented trajectories through the LLM to provide better goal-directed examples, then trains a reward model from these preference comparisons. The learned reward model is used to train the policy using SAC, with an intrinsic reward pre-training phase to bootstrap learning before LLM feedback becomes available.

## Key Results
- Achieves average label accuracy of 72.3% from LLM feedback
- Success rates comparable to SAC with predefined rewards on MetaWorld tasks
- Outperforms feedback from "scripted teacher" with privileged rewards in several cases
- GPT-4o achieves higher label accuracy (80.1%) than GPT-4o-mini (73.9%)

## Why This Works (Mechanism)
The approach works by leveraging the LLM's world knowledge to generate high-quality imagined trajectories that demonstrate optimal task completion, which serve as superior learning examples compared to raw agent trajectories. The double-check mechanism mitigates query ambiguity by requiring consistent preferences when trajectory order is swapped. The self-augmented feedback provides more informative comparisons than agent-generated trajectories alone, enabling more efficient reward learning.

## Foundational Learning
1. Preference-based RL fundamentals - Why needed: Core framework for learning from relative preferences rather than absolute rewards. Quick check: Can the agent learn from pairwise comparisons without predefined reward functions?
2. Large language model integration in RL - Why needed: LLM serves as both trajectory generator and preference labeler. Quick check: Does the LLM accurately convert state trajectories to text and provide meaningful preferences?
3. Query ambiguity in preference learning - Why needed: LLMs may produce inconsistent preferences for similar trajectories. Quick check: Does the double-check mechanism effectively filter out ambiguous preferences?
4. Reward modeling from preferences - Why needed: Converts pairwise preferences into learnable reward signals. Quick check: Can the reward model accurately predict preferences and generalize to new trajectories?
5. MetaWorld benchmark tasks - Why needed: Standard testbed for evaluating continuous control algorithms. Quick check: Are state trajectories properly formatted and converted for LLM processing?
6. SAC algorithm fundamentals - Why needed: Baseline and comparison method for evaluating PbRL performance. Quick check: Does SAC with predefined rewards achieve expected performance levels?

## Architecture Onboarding
Component map: State trajectories -> Text conversion -> LLM preference labeling -> Double-check filtering -> Reward model training -> SAC policy learning

Critical path: Agent interactions -> State trajectory collection -> LLM feedback acquisition -> Reward model update -> Policy improvement

Design tradeoffs: Uses LLM feedback instead of human or predefined rewards, trading computational cost and potential LLM bias for scalability and reduced human effort.

Failure signatures: Low label accuracy (below 70%), inconsistent double-check results, poor reward model generalization, failure to bootstrap with intrinsic rewards.

Three first experiments:
1. Test text conversion accuracy by comparing LLM-interpreted trajectories with ground truth state sequences
2. Evaluate double-check mechanism effectiveness by measuring consistency rates across trajectory permutations
3. Compare reward model performance with different segment lengths (5, 10, 15) to identify optimal discrimination granularity

## Open Questions the Paper Calls Out
## Open Question 1
- Question: What is the upper bound on query accuracy that can be achieved with larger LLMs, and how does this scale with model size?
- Basis in paper: [inferred] The paper shows that GPT-4o achieves higher label accuracy (80.1%) than GPT-4o-mini (73.9%), suggesting potential improvements with larger models, but does not explore the scaling relationship.
- Why unresolved: The paper only compares two specific models and does not investigate how accuracy changes with increasingly larger models or establish a clear scaling law.
- What evidence would resolve it: Systematic experiments testing multiple LLM sizes (e.g., GPT-3.5, GPT-4, GPT-4 Turbo) across various tasks to establish accuracy scaling trends and identify diminishing returns.

## Open Question 2
- Question: How does the quality of generated imagined trajectories vary across different task complexities and environmental configurations?
- Basis in paper: [explicit] The paper mentions that LLM-generated trajectories are evaluated on task success rates and their alignment with predefined rewards, but does not systematically analyze how trajectory quality varies with task complexity.
- Why unresolved: The experiments focus on comparing overall success rates rather than analyzing the relationship between task complexity (e.g., number of steps, environmental obstacles) and the quality of generated trajectories.
- What evidence would resolve it: Controlled experiments varying task complexity parameters while measuring trajectory quality metrics such as smoothness, efficiency, and alignment with optimal paths.

## Open Question 3
- Question: What is the impact of trajectory segment length on both label accuracy and reward model training efficiency?
- Basis in paper: [explicit] The paper uses fixed segment length of 10 for all experiments but does not explore how different segment lengths affect performance.
- Why unresolved: The choice of segment length appears arbitrary, and the paper does not investigate whether shorter or longer segments might lead to better discrimination or more efficient learning.
- What evidence would resolve it: Ablation studies testing multiple segment lengths while measuring both label accuracy and downstream policy performance to identify optimal segment lengths for different task types.

## Open Question 4
- Question: How does the double-check mechanism perform when LLM outputs are probabilistic rather than deterministic?
- Basis in paper: [inferred] The double-check mechanism assumes consistent LLM outputs when trajectory order is swapped, but the paper does not address what happens with stochastic LLM outputs.
- Why unresolved: Modern LLMs often use sampling-based decoding strategies that can produce different outputs for the same input, potentially undermining the reliability of the double-check mechanism.
- What evidence would resolve it: Experiments testing the double-check mechanism with different LLM sampling parameters (temperature, top-k, etc.) to determine robustness to output variability.

## Open Question 5
- Question: What is the theoretical convergence guarantee for the reward model when using self-augmented feedback versus traditional preference-based RL?
- Basis in paper: [explicit] The paper mentions that the learned downstream policies have a suboptimal performance bound [45] but does not provide theoretical analysis comparing convergence properties with and without self-augmented feedback.
- Why unresolved: While empirical results show improved performance, there is no theoretical framework explaining how self-augmented feedback affects the convergence rate or final policy quality.
- What evidence would resolve it: Formal proofs or theoretical analysis establishing convergence guarantees and bounds for RL-SaLLM-F compared to standard preference-based RL methods, potentially using techniques from statistical learning theory.

## Limitations
- Heavy dependence on LLM quality and specific prompting strategies for both preference labeling and trajectory generation
- No systematic analysis of how task complexity affects the quality of generated imagined trajectories
- Limited exploration of optimal segment length for preference discrimination and reward learning efficiency

## Confidence
High confidence in the core claims about achieving comparable performance to scripted teacher with privileged rewards and operating without predefined rewards or real-time human interaction.

## Next Checks
1. Verify the impact of LLM model selection and prompt engineering on label accuracy and task performance by testing multiple configurations
2. Evaluate the double-check mechanism's effectiveness by comparing performance with and without this redundancy
3. Test the method's robustness to different trajectory diversity levels in the replay buffer to understand the minimum requirements for successful reward learning