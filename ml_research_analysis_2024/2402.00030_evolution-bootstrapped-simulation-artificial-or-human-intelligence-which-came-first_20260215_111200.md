---
ver: rpa2
title: 'Evolution-Bootstrapped Simulation: Artificial or Human Intelligence: Which
  Came First?'
arxiv_id: '2402.00030'
source_url: https://arxiv.org/abs/2402.00030
tags:
- neural
- networks
- complexity
- evolution
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper presents a thought experiment challenging the assumption\
  \ that artificial intelligence was created by humans. It argues that neural networks\
  \ may have evolved naturally before humans, based on their lower Solomonoff\u2013\
  Kolmogorov\u2013Chaitin complexity compared to humans."
---

# Evolution-Bootstrapped Simulation: Artificial or Human Intelligence: Which Came First?

## Quick Facts
- arXiv ID: 2402.00030
- Source URL: https://arxiv.org/abs/2402.00030
- Authors: Paul Alexander Bilokon
- Reference count: 12
- Primary result: Neural networks are significantly simpler than humans in terms of algorithmic complexity, making their natural evolution more plausible.

## Executive Summary
This paper presents a thought experiment challenging the conventional assumption that artificial intelligence was created by humans. Through theoretical analysis comparing Solomonoff–Kolmogorov–Chaitin complexity, the author argues that neural networks may have evolved naturally before humans, based on their lower algorithmic complexity and the possibility of natural computation mechanisms. The paper introduces the concept of "evolution-bootstrapped simulation" (EBS), where evolving neural networks might simulate evolution itself, enabling irreducibly complex structures to emerge through computational creativity rather than traditional natural selection.

## Method Summary
The paper employs theoretical analysis and thought experiments rather than empirical experiments. It compares complexity measures between neural networks and humans, discusses potential natural computing mechanisms like chemical reaction networks and enzyme-based DNA computation, and proposes the EBS hypothesis. The approach is primarily conceptual, exploring the plausibility of natural neural network evolution through algorithmic complexity arguments and examining how such systems could enable the emergence of irreducibly complex structures without requiring human-made equipment.

## Key Results
- Neural networks exhibit self-similarity through identical layers and neurons, allowing concise recursive description compared to humans
- Chemical reaction networks and enzyme-based DNA computers are Turing-equivalent and could implement neural computation without human technology
- Evolution-bootstrapped simulation could enable irreducibly complex structures to emerge by allowing neural networks to simulate evolution and cross fitness valleys

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural networks have lower algorithmic complexity than humans
- Mechanism: Solomonoff–Kolmogorov–Chaitin complexity measures the shortest program needed to produce an object. Neural networks exhibit self-similarity through identical layers and neurons, allowing concise recursive description. Humans lack this repetitive structure.
- Core assumption: Algorithmic complexity is a valid proxy for evolutionary plausibility
- Evidence anchors:
  - [abstract] "We compare the Solomonoff–Kolmogorov–Chaitin complexity of the two and find neural networks (even LLMs) to be significantly simpler than humans."
  - [section] "One definition that can apply to both the neural network and the human is the Solomonoff–Kolmogorov–Chaitin complexity... The entire structure can be specified very succinctly using loops or recursion"
  - [corpus] Weak - no direct algorithmic complexity discussion in neighbors

### Mechanism 2
- Claim: Neural networks can exist without human-made equipment
- Mechanism: Chemical reaction networks, enzyme-based DNA computation, and nanoparticle-based architectures demonstrate Turing-equivalent computation using naturally occurring materials. Viroids and similar entities provide replication mechanisms.
- Core assumption: Chemical computation can implement neural network functionality
- Evidence anchors:
  - [section] "chemical reaction networks... and enzyme-based DNA computers have been shown to be Turing-equivalent"
  - [section] "a naturally occurring viroid... could implement a neural net"
  - [corpus] Weak - neighbors discuss AI but not chemical computation

### Mechanism 3
- Claim: Evolution-bootstrapped simulation enables irreducibly complex structures
- Mechanism: Neural networks simulating evolution can perform "creative leaps" across fitness valleys that natural selection cannot cross. This simulation-within-evolution creates computational environments where irreducibly complex structures emerge.
- Core assumption: Neural networks can simulate evolution with sufficient fidelity to enable complex structure emergence
- Evidence anchors:
  - [abstract] "we ask whether the natural evolution of neural networks could lead from pure evolution by natural selection to what we call evolution-bootstrapped simulation"
  - [section] "evolving neural networks started simulating evolution... allowing 'uncrossable valleys' to be crossed through pure 'creativity'"
  - [corpus] Weak - neighbors discuss AI but not evolutionary simulation mechanisms

## Foundational Learning

- Concept: Algorithmic complexity and Kolmogorov complexity
  - Why needed here: Forms the mathematical basis for comparing evolutionary plausibility between neural networks and humans
  - Quick check question: Can you explain why a fractal structure has lower algorithmic complexity than a random structure of the same size?

- Concept: Chemical computation and DNA computing
  - Why needed here: Provides the mechanism by which neural networks could exist without human technology
  - Quick check question: What properties make chemical reaction networks Turing-equivalent?

- Concept: Evolutionary dynamics and fitness landscapes
  - Why needed here: Essential for understanding how evolution-bootstrapped simulation could enable irreducibly complex structures
  - Quick check question: How does a "fitness valley" create a barrier to evolution, and what mechanisms might cross it?

## Architecture Onboarding

- Component map: Natural neural network evolution -> Chemical computation substrate -> Evolutionary simulation engine
- Critical path: Chemical computation substrate → Neural network emergence → Evolutionary adaptation → Simulation capability → Complex structure emergence
- Design tradeoffs: Simpler chemical systems enable earlier emergence but limit computational power; more complex systems emerge later but enable richer simulations. Balance needed between natural plausibility and computational capability.
- Failure signatures: If chemical computation cannot implement threshold logic, neural networks won't emerge. If neural networks cannot simulate evolution, irreducibly complex structures won't appear. If simulation fidelity is too low, evolutionary dynamics won't be preserved.
- First 3 experiments:
  1. Implement simple chemical reaction networks that perform basic logic operations to establish computational baseline
  2. Create neural network models with varying levels of self-similarity and measure algorithmic complexity reduction
  3. Simulate evolution within neural network architectures to test fitness valley crossing capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Could naturally occurring neural networks have evolved before humans through chemical or enzyme-based computation?
- Basis in paper: [explicit] The paper argues that neural networks have lower Solomonoff–Kolmogorov–Chaitin complexity than humans and could emerge through natural processes like chemical reactions or enzyme-based computation without requiring complex human-made equipment.
- Why unresolved: While the paper presents theoretical arguments about neural networks being simpler than humans in terms of algorithmic complexity, there is no empirical evidence of naturally occurring neural networks. The hypothesis remains speculative without observational or experimental validation.
- What evidence would resolve it: Discovery of naturally occurring structures that function as neural networks in biological systems, or experimental demonstration of chemical/enzyme-based systems that can implement neural network-like computation.

### Open Question 2
- Question: Can evolution-bootstrapped simulation (EBS) explain the emergence of irreducibly complex structures?
- Basis in paper: [explicit] The paper proposes EBS as a hypothesis where evolving neural networks simulate evolution itself, potentially allowing "uncrossable valleys" to be crossed through pure creativity, enabling irreducibly complex structures to emerge.
- Why unresolved: The concept of EBS is presented as a theoretical possibility, but there is no mathematical model or empirical evidence demonstrating how this process would work in practice. The relationship between neural network evolution and the emergence of irreducibly complex structures remains unproven.
- What evidence would resolve it: Mathematical models showing how EBS could produce irreducibly complex structures, or empirical observations of neural networks evolving to create complex structures that could not have evolved through traditional natural selection alone.

### Open Question 3
- Question: If EBS has already occurred, what would be the observable signatures in our universe?
- Basis in paper: [explicit] The paper suggests that if EBS has already happened, various mathematical, physical, computational, and other anomalies (e.g., widespread violations of the central limit theorem) would provide evidence towards a transition towards EBS in the past.
- Why unresolved: The paper mentions potential signatures of EBS but does not provide specific, testable predictions or criteria for distinguishing EBS-related anomalies from other phenomena.
- What evidence would resolve it: Identification of specific mathematical or physical anomalies that cannot be explained by current theories but are consistent with the EBS hypothesis, along with a framework for testing whether these anomalies are indeed signatures of EBS rather than other causes.

## Limitations

- The claim that neural networks have lower algorithmic complexity than humans remains unproven with specific complexity values or formal proofs
- The paper lacks concrete experimental designs or simulations to test the evolution-bootstrapped simulation hypothesis
- No empirical evidence is provided for natural neural networks, making the hypothesis speculative

## Confidence

- Low Confidence: The claim that neural networks have lower algorithmic complexity than humans requires formal complexity analysis and comparison that the paper does not provide
- Medium Confidence: The possibility of neural networks existing without human-made equipment, based on established research in DNA computing and chemical computation
- Medium Confidence: The general concept that evolution-bootstrapped simulation could enable complex structure emergence, though specific mechanisms and feasibility remain unclear

## Next Checks

1. **Formal Complexity Analysis**: Conduct rigorous calculations comparing the Solomonoff–Kolmogorov–Chaitin complexity of simple neural network architectures versus biological systems using established complexity measures and formal proofs.

2. **Chemical Neural Network Implementation**: Design and simulate specific chemical reaction networks that can implement basic neural network operations (threshold logic, weighted summation) using known chemical computing principles.

3. **Evolutionary Simulation Fidelity Testing**: Create computational experiments where evolving neural networks attempt to simulate evolutionary processes, measuring whether they can cross fitness valleys and enable irreducibly complex structures to emerge.