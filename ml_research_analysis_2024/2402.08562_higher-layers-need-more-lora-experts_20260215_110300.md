---
ver: rpa2
title: Higher Layers Need More LoRA Experts
arxiv_id: '2402.08562'
source_url: https://arxiv.org/abs/2402.08562
tags: []
core_contribution: This paper investigates how to allocate LoRA experts across layers
  in transformer models for parameter-efficient fine-tuning. The authors propose MoLA
  (MoE-LoRA with Layer-wise Expert Allocation), which assigns different numbers of
  experts to different layers.
---

# Higher Layers Need More LoRA Experts

## Quick Facts
- arXiv ID: 2402.08562
- Source URL: https://arxiv.org/abs/2402.08562
- Reference count: 9
- Primary result: MoLA-▽ configuration with 1.5% trainable parameters achieves 89.82% accuracy on continuous learning tasks

## Executive Summary
This paper investigates optimal allocation of LoRA experts across transformer layers for parameter-efficient fine-tuning. The authors propose MoLA (MoE-LoRA with Layer-wise Expert Allocation), which assigns varying numbers of experts to different layers based on empirical analysis. Experiments across six benchmarks demonstrate that allocating more LoRA experts to higher layers significantly outperforms uniform allocation strategies like standard LoRA and prompt tuning, achieving state-of-the-art results with only 1.5% of parameters being trained.

## Method Summary
The MoLA framework combines Mixture-of-Experts (MoE) with LoRA to enable layer-wise expert allocation. Instead of assigning the same number of experts to all layers, MoLA uses a strategic distribution where higher layers receive more experts. The authors analyze expert similarity and contribution across layers, finding that lower-layer experts exhibit higher redundancy. Based on this insight, they propose the MoLA-▽ configuration (more experts in higher layers), which achieves superior performance on continuous learning and commonsense QA tasks while maintaining parameter efficiency.

## Key Results
- MoLA-▽ configuration achieves 89.82% overall accuracy in continuous learning tasks with only 1.5% trainable parameters
- Significantly outperforms LoRA and prompt tuning baselines on commonsense QA tasks
- Demonstrates that expert redundancy is higher in lower layers, supporting the rationale for layer-wise allocation

## Why This Works (Mechanism)
The paper establishes that transformer layers have varying capacities for handling expert specialization, with higher layers requiring more diverse expert knowledge. This is evidenced by lower similarity scores among experts in higher layers, indicating greater diversity in their learned representations. The MoE routing mechanism efficiently selects appropriate experts based on layer-specific requirements, allowing the model to leverage specialized knowledge where it's most needed.

## Foundational Learning
- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning method using low-rank matrices; needed for reducing trainable parameters while maintaining performance
- **MoE (Mixture-of-Experts)**: Architecture with multiple expert networks and a gating mechanism; needed for dynamic selection of specialized knowledge
- **Layer-wise Expert Allocation**: Strategy of assigning different numbers of experts to different transformer layers; needed to match expert diversity with layer-specific requirements
- **Expert Redundancy**: Measure of similarity between experts in the same layer; needed to identify where additional experts provide diminishing returns
- **Continuous Learning**: Training on sequential tasks without catastrophic forgetting; needed to evaluate long-term adaptation capabilities
- **Commonsense QA**: Question-answering tasks requiring world knowledge; needed to test model's ability to handle complex reasoning

## Architecture Onboarding
- **Component Map**: Input -> Transformer Layers -> LoRA Experts -> MoE Routing -> Output
- **Critical Path**: Data flows through transformer layers, where LoRA experts are dynamically selected via MoE routing based on layer-specific requirements
- **Design Tradeoffs**: More experts in higher layers increases parameter count but improves performance; uniform allocation is simpler but less effective
- **Failure Signatures**: Poor performance on tasks requiring higher-layer reasoning when experts are uniformly distributed; catastrophic forgetting in continuous learning without proper expert allocation
- **First Experiments**: 1) Compare MoLA-▽ vs uniform LoRA on continuous learning tasks; 2) Measure expert similarity across layers to validate redundancy hypothesis; 3) Ablation study on number of experts per layer

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Limited to specific transformer architectures (Llama2, CodeLlama, T5) and may not generalize to other variants
- Does not establish causal mechanism for why higher layers require more experts
- Computational overhead of MoE routing is acknowledged but not thoroughly quantified across different expert counts

## Confidence
- **High confidence**: MoLA-▽ configurations outperform uniform expert allocation on tested benchmarks
- **Medium confidence**: Lower-layer experts are more redundant than higher-layer experts based on similarity metrics
- **Medium confidence**: 1.5% trainable parameters with MoLA-▽ achieves state-of-the-art performance on specific benchmarks

## Next Checks
1. Test MoLA layer-wise allocation strategies on transformer architectures beyond Llama2 and T5 (e.g., BERT, GPT variants) to assess architectural generalizability
2. Conduct ablation studies isolating the contribution of expert allocation versus routing mechanism efficiency to total performance gains
3. Measure and compare wall-clock training time and inference latency of MoLA configurations against uniform LoRA baselines across different expert counts to quantify practical overhead