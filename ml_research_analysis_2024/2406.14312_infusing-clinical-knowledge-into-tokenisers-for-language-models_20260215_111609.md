---
ver: rpa2
title: Infusing clinical knowledge into tokenisers for language models
arxiv_id: '2406.14312'
source_url: https://arxiv.org/abs/2406.14312
tags:
- clinical
- k-tokeniser
- tokeniser
- subwords
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel tokenisation mechanism, K-Tokeniser,
  which integrates semantic-based global representations of tokens from domain concepts
  (such as drugs or diseases) from either a domain ontology like Unified Medical Language
  System or the training data of the task related corpus. At the training or inference
  stage, sentence level localised context is utilised for choosing the optimal global
  token representation to realise the semantic-based tokenisation.
---

# Infusing clinical knowledge into tokenisers for language models

## Quick Facts
- arXiv ID: 2406.14312
- Source URL: https://arxiv.org/abs/2406.14312
- Authors: Abul Hasan; Jinge Wu; Quang Ngoc Nguyen; Salomé Andres; Imane Guellil; Huayu Zhang; Arlene Casey; Beatrice Alex; Bruce Guthrie; Honghan Wu
- Reference count: 40
- Primary result: K-Tokeniser improves clinical NLP performance with 13% increase in automated coding F1 score

## Executive Summary
This paper introduces K-Tokeniser, a novel tokenisation mechanism that integrates semantic-based global representations of domain concepts into tokenisers for language models. The approach leverages semantic types from ontologies like UMLS or task-specific corpora to generate domain-specific subwords, which are then optimised at both word and sentence levels. When applied to transformer-based models across four clinical NLP tasks, K-Tokeniser demonstrates consistent improvements over baseline tokenisers, particularly in automated clinical coding where it achieves a 13% increase in Micro F1 score.

## Method Summary
K-Tokeniser operates through a four-step process: (1) generating sub-corpora by semantic type (Drug and Disease categories from UMLS or MIMIC-III), (2) applying tokenisation algorithms to extract global subword representations, (3) constructing a tokeniser pool with augmented vocabulary, and (4) initialising embeddings using average representations from baseline tokeniser subwords. The method includes two optimisation stages: word-level optimisation using entropy minimisation to select optimal subword representations, and sentence-level optimisation using a fertility threshold to maintain sequence length consistency with pre-training data distributions.

## Key Results
- Achieved 13% increase in Micro F1 score for automated clinical coding task
- Required only 50% of training data to match baseline performance in concept extraction
- Achieved comparable performance with less than 20% of training data in automated coding
- Demonstrated consistent improvements across four clinical NLP tasks using three different transformer models

## Why This Works (Mechanism)

### Mechanism 1
K-Tokeniser enhances semantic representation by deriving subwords from domain concepts using ontology or corpus data. The approach extracts semantic categories from UMLS ontology or MIMIC-III corpus, then uses modified Byte Pair Encoding to generate global subword representations. These are mapped into existing BERT vocabulary and initialised using embeddings from similar subwords. This works because semantic types (drug vs. disease) determine optimal subword segmentation, and systematic derivation from ontology/corpus data captures clinical semantics.

### Mechanism 2
Sentence-level optimisation using fertility threshold ensures tokeniser outputs match pre-training data length distributions. The K-Tokeniser computes a "fertility" ratio comparing its subwords versus baseline BERT tokeniser. If the ratio exceeds a threshold, it defaults to baseline subwords to maintain sequence length consistency. This works because sequence length consistency between fine-tuning and pre-training is crucial for maintaining model performance, and fertility is a reliable proxy for this consistency.

### Mechanism 3
Embedding initialisation from existing BERT subwords transfers knowledge and avoids random initialisation degradation. When new subwords are added, their embeddings are initialised by averaging embeddings of baseline BERT subwords that tokenise the new subword. This transfers semantic knowledge from the pre-trained model to new subwords. This works because the average embedding of baseline subwords provides a reasonable semantic representation for the new subword, preserving knowledge encoded in the pre-trained model.

## Foundational Learning

**Concept**: Byte Pair Encoding (BPE) and its variants (WordPiece, Unigram)
- Why needed here: Understanding how tokenisers generate vocabularies from text corpora is fundamental to understanding how K-Tokeniser modifies this process by incorporating domain knowledge.
- Quick check question: What is the primary difference between BPE and WordPiece tokenisation algorithms?

**Concept**: Semantic types and ontologies (e.g., UMLS)
- Why needed here: The K-Tokeniser relies on semantic types from ontologies to generate domain-specific subwords. Understanding how ontologies structure medical knowledge is crucial.
- Quick check question: What are the four semantic types extracted from UMLS in this study, and how are they grouped?

**Concept**: Transformer architecture and embedding layers
- Why needed here: The K-Tokeniser modifies the embedding layer of pre-trained BERT models. Understanding how embeddings work and how they are initialised is essential.
- Quick check question: Why is random initialisation of embeddings for new subwords problematic in pre-trained models?

## Architecture Onboarding

**Component map**: UMLS ontology/MIMIC-III corpus -> Semantic type extraction -> BPE-based subword generation -> Tokeniser pool construction -> Embedding initialisation -> Downstream models (ClinicalBERT, PubMedBERT, Bioformer)

**Critical path**: Extract semantic categories → Generate subwords via BPE → Augment tokeniser vocabulary → Initialise embeddings → Apply optimisation steps → Fine-tune downstream models

**Design tradeoffs**:
- Using ontology vs. corpus: Ontologies provide structured, consistent knowledge but may lack coverage of rare terms; corpora provide broader coverage but may include noise and inconsistencies
- Global vs. local optimisation: Global optimisation ensures semantic consistency but may produce sequences inconsistent with pre-training; local optimisation maintains consistency but may sacrifice some semantic benefits
- Embedding initialisation: Averaging method transfers knowledge but may not capture complex semantic relationships; random initialisation is simple but may degrade performance

**Failure signatures**:
- Performance degradation when fertility threshold is too high (overuse of baseline tokeniser)
- Overfitting when embedding initialisation is poor (random initialisation of many new subwords)
- Inconsistent results across tasks when semantic categories are not well-defined or extracted

**First 3 experiments**:
1. **Ablation study**: Compare K-Tokeniser with and without embedding initialisation to quantify the impact of knowledge transfer
2. **Threshold sensitivity**: Vary the fertility threshold values to find the optimal balance between semantic benefits and sequence consistency
3. **Ontology vs. corpus**: Compare K-Tokeniser performance when using UMLS vs. MIMIC-III for subword generation to assess the value of structured vs. unstructured knowledge sources

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of K-Tokeniser models compare when using a domain ontology like UMLS versus training data from task-specific corpora?
- Basis in paper: [explicit] The paper compares K-Tokeniser models built from UMLS ontology and MIMIC-III corpus, noting that ontology-based tokenisation yields more consistent performances compared to corpus-based tokenisation.
- Why unresolved: While the paper mentions that ontology-based tokenisation is more consistent, it does not provide a detailed comparison of the performance differences between the two approaches across all tasks.
- What evidence would resolve it: A comprehensive analysis comparing the performance of K-Tokeniser models using UMLS versus task-specific corpora across all four clinical NLP tasks, including detailed metrics and statistical significance tests.

### Open Question 2
What is the impact of pre-training K-Tokeniser models on their performance compared to fine-tuning only?
- Basis in paper: [inferred] The paper discusses the effectiveness of increasing model capacity through augmentation of the tokeniser and embedding layer without additional pre-training, but does not explore the impact of further pre-training.
- Why unresolved: The paper acknowledges that further pre-training could potentially improve performance but does not investigate this approach.
- What evidence would resolve it: Experimental results comparing the performance of K-Tokeniser models with and without additional pre-training on the same downstream tasks, using metrics such as F1 scores and convergence rates.

### Open Question 3
How does the K-Tokeniser handle rare or unseen clinical terms that are not present in the UMLS ontology or task-specific corpora?
- Basis in paper: [inferred] The paper mentions that the K-Tokeniser expands vocabulary using semantic types from domain concepts but does not address the handling of rare or unseen terms.
- Why unresolved: The paper does not provide information on the K-Tokeniser's ability to generalize to rare or unseen clinical terms, which is crucial for real-world applications.
- What evidence would resolve it: An evaluation of the K-Tokeniser's performance on a dataset containing rare or unseen clinical terms, comparing it to baseline models and analyzing the tokeniser's ability to generate meaningful subwords for these terms.

## Limitations

- Performance improvements are demonstrated only in clinical domains with well-established ontologies like UMLS
- Lack of ablation studies to isolate the contribution of semantic tokenisation versus embedding initialisation
- Specific fertility threshold values lack rigorous justification and may not be optimal across all tasks

## Confidence

**High Confidence** (3 claims):
- K-Tokeniser can be integrated with existing transformer models without architectural changes
- The embedding initialisation method using average subword embeddings is technically feasible
- Semantic tokenisation improves performance on clinical concept extraction tasks

**Medium Confidence** (2 claims):
- The fertility threshold optimization effectively balances semantic benefits with sequence consistency
- K-Tokeniser achieves 13% improvement in automated clinical coding (Micro F1)
- The approach generalizes across four distinct clinical NLP tasks

**Low Confidence** (2 claims):
- The specific fertility threshold values are optimal across all tasks
- The approach will generalize to non-clinical domains with similar performance gains

## Next Checks

1. **Ablation Study**: Run controlled experiments comparing K-Tokeniser against baseline BERT with: (a) semantic tokenisation only, (b) embedding initialisation only, and (c) both mechanisms combined. This will quantify the individual contributions of each component to the observed improvements.

2. **Threshold Sensitivity Analysis**: Systematically vary fertility threshold values across a wider range (0.01 to 0.2 in 0.01 increments) for each task to determine whether the chosen values are truly optimal or if performance varies significantly with different thresholds.

3. **Cross-Domain Generalizability**: Apply K-Tokeniser to a non-clinical domain with available ontology (e.g., legal documents with legal concept ontologies or scientific literature with MeSH terms) to test whether the semantic tokenisation approach transfers effectively beyond healthcare applications.