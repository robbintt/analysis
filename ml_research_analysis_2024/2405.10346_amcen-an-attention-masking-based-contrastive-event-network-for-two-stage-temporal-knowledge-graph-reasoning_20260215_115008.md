---
ver: rpa2
title: 'AMCEN: An Attention Masking-based Contrastive Event Network for Two-stage
  Temporal Knowledge Graph Reasoning'
arxiv_id: '2405.10346'
source_url: https://arxiv.org/abs/2405.10346
tags:
- temporal
- attention
- events
- historical
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of imbalanced distribution between
  new and recurring events in temporal knowledge graph (TKG) reasoning, which adversely
  impacts reasoning accuracy. To overcome this challenge, the authors propose an attention
  masking-based contrastive event network (AMCEN) with local-global temporal patterns
  for two-stage prediction of future events.
---

# AMCEN: An Attention Masking-based Contrastive Event Network for Two-stage Temporal Knowledge Graph Reasoning

## Quick Facts
- arXiv ID: 2405.10346
- Source URL: https://arxiv.org/abs/2405.10346
- Reference count: 40
- Primary result: Achieves 41.8% Hits@1 on ICEWS18, surpassing existing two-stage methods by 3.8%

## Executive Summary
This paper addresses the problem of imbalanced distribution between new and recurring events in temporal knowledge graph (TKG) reasoning, which adversely impacts reasoning accuracy. To overcome this challenge, the authors propose an attention masking-based contrastive event network (AMCEN) with local-global temporal patterns for two-stage prediction of future events. The key innovation lies in the design of historical and non-historical attention mask vectors that separately explore historical and non-historical entities, thus alleviating the imbalance issue. AMCEN also incorporates a local-global message-passing module and a contrastive event classifier to comprehensively capture multi-hop structural dependencies and temporal evolution.

## Method Summary
AMCEN is a two-stage network that first classifies events as recurring or new using a local-global temporal encoder with contrastive learning, then performs entity prediction with attention masking. The method uses CompGCN for structural encoding, self-attention for local temporal patterns, and frequency-based encoding for global patterns. Historical and non-historical attention mask vectors are constructed to separate exploration of entities based on their historical occurrence. The model is trained in two stages: first optimizing the contrastive event classifier, then fine-tuning the attention masking-based decoders.

## Key Results
- Achieves 41.8% Hits@1 on ICEWS18, surpassing existing two-stage methods by 3.8%
- Improves Hits@1 by 7.1% on GDELT compared to best baseline
- Demonstrates significant performance gains on WIKI (6.9% improvement) and YAGO (7.0% improvement)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The historical and non-historical attention mask vectors address the imbalanced distribution between recurring and new events by forcing the model to separately explore entities based on their historical occurrence.
- Mechanism: The mask vectors (Mhist and Mnhist) are constructed using historical frequency counts, enabling selective attention over either historical or non-historical entity pools depending on the predicted event type.
- Core assumption: Historical frequency information is a reliable signal to distinguish recurring vs. new events, and masking prevents attention leakage across these two pools.
- Evidence anchors:
  - [abstract] "historical and non-historical attention mask vectors are designed to control the attention bias towards historical and non-historical entities, acting as the key to alleviating the imbalance."
  - [section] "To mitigate the uneven distribution of event types, we design historical and non-historical attention mask vectors to separate the exploration of historical entities from that of non-historical entities."
  - [corpus] Weak evidence; no direct mention of masking strategies in neighbors.

### Mechanism 2
- Claim: Local-global temporal encoders capture both sequential and repetitive patterns, enabling accurate event classification and refined prediction scope.
- Mechanism: Self-attention is used for local temporal patterns within a time window, while frequency-based encoding captures global repetitive event patterns. These are concatenated and passed through a binary classifier.
- Core assumption: Local patterns (adjacent snapshots) and global patterns (overall frequency) are complementary and jointly sufficient to distinguish recurring from new events.
- Evidence anchors:
  - [abstract] "A local-global message-passing module is proposed to comprehensively consider and capture multi-hop structural dependencies and local-global temporal evolution."
  - [section] "We use self-attention to selectively integrate the sequence of historical information within the time windows... We count the frequencies... to encode them as global features."
  - [corpus] No direct evidence; the neighbors focus on event detection rather than temporal graph reasoning.

### Mechanism 3
- Claim: Contrastive learning of event representations enhances the classifier by enforcing intra-class similarity and inter-class separation.
- Mechanism: A contrastive loss is applied to local-global temporal features to learn embeddings where similar (same-type) events cluster together and dissimilar ones are pushed apart.
- Core assumption: Contrastive learning improves discriminative power beyond what is achieved by raw temporal encodings.
- Evidence anchors:
  - [abstract] "A contrastive event classifier is used to classify events more accurately by incorporating local-global temporal patterns into contrastive learning."
  - [section] "We combine local-global patterns with contrastive learning to generate query contrastive representations... These representations have the characteristic of small intra-class distances and large inter-class distances."
  - [corpus] Weak evidence; neighbors do not mention contrastive learning in TKG reasoning.

## Foundational Learning

- Concept: Temporal Knowledge Graphs (TKGs)
  - Why needed here: The entire reasoning task is defined on sequences of snapshots with timestamps, so understanding how TKGs encode and evolve over time is foundational.
  - Quick check question: What is the difference between a static KG and a TKG?

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: AMCEN uses CompGCN for multi-hop structural encoding; engineers must understand how GNNs aggregate neighbor information.
  - Quick check question: How does a multi-relational GNN differ from a standard GCN?

- Concept: Self-attention and masking in sequence models
  - Why needed here: The model uses self-attention for local temporal encoding and masking for selective entity focus; knowing how attention weights are computed and how masking works is essential.
  - Quick check question: What is the effect of masking on the attention matrix in a decoder?

## Architecture Onboarding

- Component map: Input query -> CompGCN aggregation -> temporal encoding -> classifier -> mask vector -> decoder -> final prediction
- Critical path: Input query → CompGCN aggregation → temporal encoding → classifier → mask vector → decoder → final prediction
- Design tradeoffs:
  - Masking vs. full attention: Masking improves event-type balance but reduces raw information flow.
  - Two-stage vs. single-stage: Two-stage allows focused reasoning but adds complexity and latency.
  - Contrastive vs. supervised: Contrastive helps when labels are scarce but requires careful sampling.
- Failure signatures:
  - Degraded Hits@1 with good MRR: Likely masking is too aggressive or classifier is misclassifying.
  - Poor performance on datasets with high new-event ratio: Classifier may not generalize well to rare event types.
  - Overfitting on small datasets: Too many attention heads or large time windows can overfit.
- First 3 experiments:
  1. Ablation: Remove historical/non-historical mask vectors and measure the drop in Hits@1.
  2. Ablation: Remove the contrastive learning module and compare classifier accuracy.
  3. Hyperparameter sweep: Vary the time window τ and observe impact on MRR for ICEWS18.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions in the abstract or conclusion sections.

## Limitations
- The effectiveness of historical/non-historical attention masking heavily depends on the reliability of historical frequency counts, which may be noisy in sparse or highly dynamic datasets.
- The two-stage architecture introduces additional hyperparameters and potential overfitting risks, especially when the number of event types is small.
- The paper does not provide ablation results isolating the impact of contrastive learning versus attention masking, making it difficult to assess individual contributions.

## Confidence
- **High**: The paper clearly describes the problem of imbalanced event distributions in TKG reasoning and proposes a structured two-stage solution.
- **Medium**: The mechanism of attention masking and contrastive event classification is well-articulated, but empirical validation of their individual contributions is lacking.
- **Low**: No direct evidence from the corpus that attention masking or contrastive learning has been successfully applied to TKG reasoning tasks.

## Next Checks
1. Perform an ablation study removing the historical/non-historical mask vectors to quantify their impact on Hits@1 performance.
2. Test the model on a synthetic dataset with controlled event imbalance to verify that masking correctly isolates historical vs. non-historical entities.
3. Evaluate the classifier accuracy on a held-out validation set to ensure that the local-global temporal features are truly discriminative before applying them to the prediction stage.