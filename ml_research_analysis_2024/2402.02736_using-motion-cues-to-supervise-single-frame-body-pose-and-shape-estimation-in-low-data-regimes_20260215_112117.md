---
ver: rpa2
title: Using Motion Cues to Supervise Single-Frame Body Pose and Shape Estimation
  in Low Data Regimes
arxiv_id: '2402.02736'
source_url: https://arxiv.org/abs/2402.02736
tags:
- pose
- body
- data
- latexit
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using optical flow between consecutive video
  frames as a self-supervised training signal to refine a pre-trained single-image
  body pose and shape estimation network when only limited annotated data is available.
  Given a poorly trained baseline model, the method computes optical flow between
  video frames and enforces consistency between this flow and the flow inferred from
  changes in estimated body poses across frames.
---

# Using Motion Cues to Supervise Single-Frame Body Pose and Shape Estimation in Low Data Regimes

## Quick Facts
- arXiv ID: 2402.02736
- Source URL: https://arxiv.org/abs/2402.02736
- Reference count: 40
- One-line primary result: Optical flow between consecutive frames provides effective self-supervised supervision for refining pose estimation models when annotated data is scarce

## Executive Summary
This paper addresses the challenge of training body pose and shape estimation models when only limited annotated data is available. The authors propose using optical flow between consecutive video frames as a self-supervised training signal to refine a pre-trained single-image pose estimator. By enforcing consistency between the estimated optical flow and the flow inferred from changes in estimated body poses across frames, the method provides additional supervision without requiring new annotations. Experiments demonstrate that this approach outperforms existing semi-supervised methods like TexturePose, particularly when less initial supervision is used, and can bridge the gap between single-image and video-based pose estimation methods.

## Method Summary
The method takes a pre-trained single-image body pose and shape estimation network and refines it using optical flow consistency. Given consecutive frames from unannotated videos, the approach computes optical flow between them and enforces consistency between this flow and the flow that can be inferred from changes in estimated body poses across frames. The method combines this optical flow loss with the original supervised loss used to pre-train the network. Additionally, a temporal context network can be introduced to output affine transformation parameters conditioned on previous frame predictions, helping to exploit temporal context during training without requiring video at inference time.

## Key Results
- Outperforms existing semi-supervised methods like TexturePose when limited annotated data is available
- Optical flow consistency provides effective supervision even when initial pose estimates are imperfect
- Combining optical flow with other cues (e.g., 2D keypoints) yields further improvements
- Works across different backbone architectures and benefits more when less initial supervision is used

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optical flow provides additional supervision signal for pose refinement
- Mechanism: The optical flow between consecutive frames captures real motion, which is more accurate than motion inferred from the pose estimator's predictions. By enforcing consistency between the estimated optical flow and the flow inferred from pose changes, the model learns to correct its pose predictions.
- Core assumption: Optical flow estimation is accurate enough to serve as a reliable supervisory signal.
- Evidence anchors:
  - [abstract] "We compute poses in consecutive frames along with the optical flow between them. We then enforce consistency between the image optical flow and the one that can be inferred from the change in pose from one frame to the next."
  - [section] "Our interpretation is that minimizing LOF aligns the predictions of the network BL with the motion, as illustrated by Fig. 2."
  - [corpus] "Optical Flow for Pose and Shape Estimation. The central idea of this paper is to rely solely on consecutive images and the optical flow (OF) between them to refine a pre-trained network."

### Mechanism 2
- Claim: Weak labeling through motion alignment improves pose estimation
- Mechanism: The optical flow loss can be interpreted as a point-to-point loss when the inference is performed for image pairs. The subtraction in the loss function can be rewritten to show that the optical flow serves as a weak label for the pose estimate.
- Core assumption: The optical flow provides a reasonable approximation of the true motion between frames.
- Evidence anchors:
  - [section] "Thus, ˜p2 can be viewed as a weak label for the p2 estimate. In other words, motion alignment essentially serves as weak labelling process and Eq. 2 can be interpreted as a point-to-point loss when the inference is performed for image pairs."
  - [section] "In the computation of L1→2 OF , the two images play asymmetric roles. To remedy this, we also compute L2→1 OF by reversing the roles of the images and take our full loss to be LOF = 1/2(L1→2 OF + L2→1 OF )."

### Mechanism 3
- Claim: Temporal context improves motion consistency without requiring video at inference
- Mechanism: By using an auxiliary lightweight network to output affine transformation parameters conditioned on previous frame predictions, the model can exploit temporal context during training. This helps bridge the gap between single-image and video-based methods.
- Core assumption: The temporal context network can learn useful motion patterns from the previous frames.
- Evidence anchors:
  - [section] "We exploit this by introducing a feature-wise affine transformation conditioned on the predictions for the previous frames, inspired by (Perez et al., 2018)."
  - [section] "In other words, we still use the same single-frame network as before but inject temporal context via feature normalization. In effect, the MLP provides a temporal context for pose estimation in frame N."
  - [corpus] "Video-Based Pose and Shape Estimation. Soon after SMPL body estimators from single images (Kanazawa et al., 2018) appeared, they were extended to handle video inputs and mostly differ in how they encode temporal relations."

## Foundational Learning

- Concept: Optical flow estimation
  - Why needed here: The method relies on accurate optical flow between consecutive frames to provide supervision.
  - Quick check question: How does the RAFT optical flow estimator work, and what are its limitations in handling occlusions or fast motions?

- Concept: SMPL body model
  - Why needed here: The method uses SMPL parameters (pose θ, shape β, and camera projection π) as the output of the pose estimator.
  - Quick check question: What are the components of the SMPL model, and how are they used to represent human body shape and pose?

- Concept: Weak supervision
  - Why needed here: The method uses optical flow as a weak supervisory signal without requiring ground truth annotations.
  - Quick check question: How does weak supervision differ from traditional supervised learning, and what are its advantages and limitations?

## Architecture Onboarding

- Component map:
  Pre-trained pose estimator (BL) -> Optical flow estimator -> Temporal context network (optional) -> Loss functions (Lsup + LOF + LTP)

- Critical path:
  1. Pre-train baseline pose estimator on limited annotated data
  2. Compute optical flow between consecutive frames in unannotated videos
  3. Enforce consistency between optical flow and pose-inferred flow
  4. Fine-tune baseline using combined loss
  5. (Optional) Inject temporal context at training time for improved motion consistency

- Design tradeoffs:
  - Using optical flow as supervision vs. requiring additional annotations
  - Single-frame inference vs. video-based methods with temporal modules
  - ResNet backbone vs. HRNet for different data regimes
  - Unlabeled video data vs. synthetic data for training optical flow estimator

- Failure signatures:
  - Poor optical flow estimates leading to incorrect supervision
  - Over-reliance on initial weights preventing generalization
  - Correlated errors in pose estimates for similar frames
  - Large domain gaps between training and test data

- First 3 experiments:
  1. Fine-tune pre-trained baseline using optical flow loss on Human3.6M data
  2. Combine optical flow loss with texture-aware loss and evaluate on 3DPW test set
  3. Add temporal context network and evaluate motion consistency improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the optical flow estimator impact the effectiveness of the proposed refinement method, and what is the threshold of OF quality below which the method becomes ineffective?
- Basis in paper: [explicit] The paper uses an off-the-shelf pre-trained RAFT optical flow estimator and shows that OF provides useful supervision even when the body estimates are imperfect, as evidenced by Table 4 where the ratio of distances d(FB,FGT) to d(FOF,FGT) remains above 1 across datasets and time intervals.
- Why unresolved: The paper does not systematically study the relationship between OF quality and refinement performance, nor does it establish a minimum OF quality threshold for the method to be beneficial.
- What evidence would resolve it: Controlled experiments varying OF estimator quality (e.g., using different OF models or adding noise) while measuring refinement performance would establish the sensitivity of the method to OF quality and identify the threshold below which it fails to provide useful supervision.

### Open Question 2
- Question: Can the proposed method be extended to multi-person scenarios where inter-person occlusions and interactions create complex motion patterns?
- Basis in paper: [inferred] The method relies on enforcing consistency between optical flow and body motion flow, which assumes a single person's motion can be tracked across frames. The paper focuses on single-person pose and shape estimation, and the method would need to handle cases where multiple people occlude each other or have interacting motions.
- Why unresolved: The paper does not address multi-person scenarios, and the current formulation assumes a single person per image pair, making it unclear how to extend to crowded scenes with occlusions and interactions.
- What evidence would resolve it: Experiments applying the method to video sequences with multiple interacting people, along with modifications to handle occlusions (e.g., person-specific flow estimation or interaction modeling), would demonstrate the method's applicability to multi-person scenarios.

### Open Question 3
- Question: What is the optimal balance between the different supervision sources (ground truth, optical flow, texture consistency, 2D keypoints) for different amounts of available annotated data?
- Basis in paper: [explicit] The paper shows that combining optical flow with texture consistency (TexturePose) or 2D keypoints yields better results than using optical flow alone, especially when less annotated data is available. However, the optimal weighting between these sources is not systematically studied.
- Why unresolved: The paper uses fixed weights for the loss terms and demonstrates that combining sources is beneficial, but does not explore the optimal balance as a function of the amount of available annotated data.
- What evidence would resolve it: A systematic study varying the weights of different supervision sources across a range of annotated data percentages would reveal the optimal balance for each data regime, potentially leading to a data-dependent weighting scheme.

### Open Question 4
- Question: How does the proposed method compare to video-based methods that use temporal transformers when both are given the same amount of annotated and unannotated data?
- Basis in paper: [explicit] The paper shows that the method can bridge the gap between single-image and video-based methods by exploiting temporal context at inference time, achieving competitive results with video-based methods on acceleration error. However, a direct comparison with video-based methods using the same data budget is not provided.
- Why unresolved: The paper compares against video-based methods but does not control for the amount of data used by each method, making it unclear how much of the performance gap is due to the method itself versus the data used.
- What evidence would resolve it: Experiments training video-based methods (e.g., VIBE, MEVA, TCMR) using the same annotated and unannotated data as the proposed method would provide a fair comparison of the two approaches under identical data constraints.

## Limitations
- The method's effectiveness depends on the accuracy of optical flow estimation, which can degrade with occlusions or fast motions
- The approach is currently designed for single-person scenarios and would need modifications for multi-person settings with occlusions and interactions
- The optimal balance between different supervision sources is not systematically studied and may vary with the amount of available annotated data

## Confidence

- Optical flow supervision mechanism: **Medium** - The theoretical framework is sound, but empirical validation across diverse motion scenarios is limited
- Performance improvements over TexturePose: **High** - Clear quantitative improvements shown across multiple datasets and architectures
- Temporal context benefits: **Medium** - Ablation studies support the claim, but the mechanism could be more thoroughly analyzed

## Next Checks

1. Test the method's robustness to optical flow estimation errors by artificially degrading flow quality and measuring impact on pose refinement
2. Evaluate performance on datasets with challenging motions (rapid changes, occlusions) to assess real-world applicability
3. Compare against other temporal consistency approaches (e.g., video-based methods) to better understand the trade-offs of the proposed single-frame + temporal context approach