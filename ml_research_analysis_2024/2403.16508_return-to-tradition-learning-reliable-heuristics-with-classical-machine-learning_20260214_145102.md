---
ver: rpa2
title: 'Return to Tradition: Learning Reliable Heuristics with Classical Machine Learning'
arxiv_id: '2403.16508'
source_url: https://arxiv.org/abs/2403.16508
tags:
- planning
- learning
- features
- muninn
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WL-GOOSE, a learning for planning method
  that uses classical machine learning with Weisfeiler-Leman (WL) features generated
  from a novel graph representation of lifted planning tasks. Unlike deep learning
  approaches, WL-GOOSE trains up to 3 orders of magnitude faster and has up to 2 orders
  of magnitude fewer parameters.
---

# Return to Tradition: Learning Reliable Heuristics with Classical Machine Learning

## Quick Facts
- **arXiv ID**: 2403.16508
- **Source URL**: https://arxiv.org/abs/2403.16508
- **Reference count**: 19
- **Primary result**: WL-GOOSE uses WL features on Instance Learning Graphs to train classical ML heuristics 2-3 orders of magnitude faster than deep learning while matching or outperforming LAMA on 4-7 of 10 domains

## Executive Summary
This paper introduces WL-GOOSE, a learning for planning method that uses classical machine learning with Weisfeiler-Leman (WL) features generated from a novel graph representation of lifted planning tasks. Unlike deep learning approaches, WL-GOOSE trains up to 3 orders of magnitude faster and has up to 2 orders of magnitude fewer parameters. The method outperforms the hFF heuristic in coverage and ties or outperforms LAMA on 4 out of 10 domains for coverage and 7 out of 10 for plan quality. The WL feature generation is theoretically connected to previous works and shown to be more expressive than GNNs and Muninn for distinguishing planning tasks. The approach provides an efficient, reliable, and interpretable alternative to deep learning for learning domain-specific heuristics.

## Method Summary
WL-GOOSE converts lifted planning tasks into Instance Learning Graphs (ILGs), then applies the Weisfeiler-Leman graph isomorphism test iteratively to generate color histograms as fixed-length features. These WL features are used to train classical machine learning models (SVR or GPR) to predict heuristic values for greedy best-first search. The method is evaluated on 10 domains from the 2023 International Planning Competition Learning Track, comparing coverage and plan quality against hFF and LAMA baselines.

## Key Results
- Training speed: Up to 3 orders of magnitude faster than deep learning approaches
- Model size: Up to 2 orders of magnitude fewer parameters than neural network methods
- Coverage: Outperforms hFF and matches or exceeds LAMA on 4-7 of 10 domains
- Plan quality: Comparable or better than LAMA on 7 out of 10 domains

## Why This Works (Mechanism)

### Mechanism 1
WL-GOOSE uses Weisfeiler-Leman features on Instance Learning Graphs to represent lifted planning tasks, enabling classical ML to learn heuristics with far fewer parameters than deep learning. The ILG converts predicates and objects into a colored graph, then WL colors are propagated iteratively to capture structural patterns. These WL color histograms become fixed-length features for SVR or GPR. Core assumption: WL color histograms contain sufficient signal to approximate optimal heuristics without needing message-passing or gradient-based training.

### Mechanism 2
WL features subsume GNN expressivity for planning tasks, while being far cheaper to train and interpret. Theorems 4.1-4.4 show that GNN-ILG expressiveness ≤ WL feature expressiveness ≤ DL feature expressiveness, but WL is deterministic, linear in graph size, and produces explainable weights. Core assumption: The WL algorithm can distinguish all task pairs that matter for heuristic learning, unlike GNNs limited by neighborhood aggregation depth.

### Mechanism 3
Bayesian GPR provides meaningful uncertainty estimates that correlate with heuristic error and search difficulty. GPR outputs both mean and variance; experiments show Pearson correlation between variance and (1) error from optimal heuristic, (2) node expansions. Core assumption: The posterior variance from GPR on the WL feature space is a reliable proxy for prediction uncertainty in the planning context.

## Foundational Learning

- **Weisfeiler-Leman graph isomorphism test**
  - Why needed here: Provides the iterative node coloring that captures graph structure without neural nets
  - Quick check question: Can you explain why WL colors for a 6-cycle and two disjoint 3-cycles become identical after L=2 iterations?

- **Instance Learning Graph (ILG) construction**
  - Why needed here: Translates lifted planning predicates into a fixed, edge-labeled graph suitable for WL
  - Quick check question: In an ILG, what node color distinguishes a proposition that is true in the initial state only from one true in both state and goal?

- **Kernel-based regression (SVR/GPR)**
  - Why needed here: Turns WL histograms into heuristic predictions without backprop or GPU training
  - Quick check question: How does the dot-product kernel in SVR differ from the RBF kernel in terms of feature space?

## Architecture Onboarding

- **Component map**: Planning task -> ILG builder -> WL color generator -> Histogram encoder -> Classical ML model -> Heuristic value -> GBFS planner

- **Critical path**:
  1. Parse planning domain -> ILG builder
  2. Run WL algorithm -> color histogram
  3. Train ML model on (feature, optimal cost) pairs
  4. Use model in GBFS to solve new instances

- **Design tradeoffs**:
  - WL depth L vs. expressiveness vs. runtime (quadratic in L)
  - Feature sparsity: many WL colors may collapse to zero counts
  - Model choice: SVR fast, sparse; GPR slower, provides uncertainty

- **Failure signatures**:
  - All WL colors identical across tasks -> model collapses to constant
  - Variance from GPR near zero -> uncertainty estimates meaningless
  - High training time for 2-LWL -> memory blowup

- **First 3 experiments**:
  1. Generate ILG and WL colors for a tiny Blocksworld instance; verify histogram counts
  2. Train SVR on 10 training instances; test on held-out instances; compare coverage vs hFF
  3. Train GPR; check Pearson correlation between variance and actual search nodes expanded

## Open Questions the Paper Calls Out

### Open Question 1
Can the WL-GOOSE approach be extended to domains requiring traversal of maps or other spatial reasoning tasks? The paper notes that GPR performs worse in domains requiring traversal of maps, suggesting limitations in WL features' ability to express such spatial relationships.

### Open Question 2
How do the uncertainty bounds provided by GPR's Bayesian model correlate with actual search performance across different planning domains? The paper analyzes correlation between GPR's standard deviation and heuristic error/expanded nodes, finding domain-dependent results.

### Open Question 3
Can higher-order WL features (3-LWL or beyond) provide meaningful performance improvements without causing computational issues? The paper attempts to generate 3-LWL features but encounters memory issues, while 2-LWL generally performs worse than standard WL.

### Open Question 4
What modifications to the ILG representation could improve WL-GOOSE's ability to distinguish between planning tasks that currently appear equivalent? The paper provides theoretical examples of tasks that WL-GOOSE cannot distinguish and suggests its features are among the most expressive available.

## Limitations

- Theoretical expressiveness advantages may not translate to practical performance gains for typical planning tasks
- Uncertainty estimates from GPR assume Gaussian noise structure that may not hold for heuristic error distributions
- ILG representation's ability to capture all relevant planning structure depends on careful predicate/object encoding choices

## Confidence

- High confidence: Faster training times and fewer parameters compared to deep learning approaches
- Medium confidence: Coverage and plan quality improvements over LAMA in 4-7 domains
- Low confidence: Theoretical claims about WL expressivity superseding GNNs

## Next Checks

1. Test WL-GOOSE on domains with known structural complexity where deep learning approaches currently excel
2. Compare uncertainty estimates from GPR against actual heuristic error distributions across all tested domains
3. Benchmark against a broader set of classical ML baselines using the same WL features to isolate the benefit of the feature representation