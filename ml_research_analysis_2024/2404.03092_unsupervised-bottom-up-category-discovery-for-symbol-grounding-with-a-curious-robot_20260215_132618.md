---
ver: rpa2
title: Unsupervised, Bottom-up Category Discovery for Symbol Grounding with a Curious
  Robot
arxiv_id: '2404.03092'
source_url: https://arxiv.org/abs/2404.03092
tags: []
core_contribution: This paper presents a robot that learns categories from scratch
  in a bottom-up manner, mimicking early childhood language development. Instead of
  starting with pre-labeled categories, the robot explores its environment using a
  model of curiosity to autonomously carve out regions of sensorimotor space.
---

# Unsupervised, Bottom-up Category Discovery for Symbol Grounding with a Curious Robot

## Quick Facts
- arXiv ID: 2404.03092
- Source URL: https://arxiv.org/abs/2404.03092
- Reference count: 39
- Primary result: Robot learns categories from scratch using curiosity-driven exploration, achieving effective symbol grounding without pre-labeled categories

## Executive Summary
This paper presents a novel approach to symbol grounding in robotics through unsupervised, bottom-up category discovery. The method enables a robot (Cozmo) to autonomously explore its environment and learn categories based on sensorimotor interactions, without relying on pre-labeled data. By combining curiosity-driven exploration with advanced visual feature extraction models, the robot progressively refines its understanding of categories, demonstrating a promising step toward addressing the symbol grounding problem.

## Method Summary
The approach uses a curiosity-driven exploration strategy implemented with the Explauto framework, guiding the robot to sample actions that maximize learning progress across different regions of sensorimotor space. Visual information is encoded using either YOLO+CLIP or SAM+DINOv2 models into high-dimensional vectors. The robot learns categories based on actions and visual observations, with categories becoming increasingly specific over time. Quality of learned categories is evaluated using the Words-as-Classifiers (WAC) model, which predicts the correct category given sensory exemplars.

## Key Results
- Robot successfully learns categories through autonomous exploration without pre-labeled data
- Categories become increasingly specific over time as exploration continues
- Words-as-Classifiers model accurately predicts correct categories from sensory exemplars
- Both YOLO+CLIP and SAM+DINOv2 vision model combinations show consistent performance

## Why This Works (Mechanism)
The system works by leveraging intrinsic motivation (curiosity) to drive exploration in sensorimotor spaces, allowing the robot to discover meaningful patterns and boundaries without external supervision. The curiosity-driven exploration, implemented through the Explauto framework, identifies regions of high learning potential where the robot can make meaningful distinctions. These distinctions become the basis for category formation, which is then validated using the WAC model to ensure the categories are meaningful and generalizable.

## Foundational Learning

**Curiosity-driven exploration**: Autonomous learning strategy that maximizes information gain and learning progress - needed for efficient sampling of meaningful actions; quick check: verify learning progress metrics increase during exploration.

**Symbol grounding problem**: Challenge of connecting abstract symbols to real-world referents - central motivation for the research; quick check: confirm grounded categories can be mapped to word labels.

**Visual feature extraction**: Converting raw images to meaningful vector representations - essential for high-dimensional sensorimotor encoding; quick check: validate feature vectors capture relevant visual properties.

**Sensorimotor learning**: Integration of sensory inputs with motor actions - fundamental for category discovery based on interaction; quick check: ensure action-observation pairs are properly aligned.

**Explauto framework**: Implementation of intrinsically motivated exploration algorithms - provides the computational mechanism for curiosity; quick check: verify exploration targets regions of maximum learning progress.

## Architecture Onboarding

**Component map**: Vision model (YOLO/CLIP or SAM/DINOv2) -> Feature extraction -> Explauto curiosity module -> Action sampling -> Robot execution -> Observation collection -> Category refinement

**Critical path**: Visual observation → Feature encoding → Curiosity-driven action selection → Robot interaction → New observation → Category update

**Design tradeoffs**: Pre-trained vision models provide strong feature extraction but may limit discovery of novel categories; curiosity-based exploration balances exploitation and exploration but may be computationally intensive in high-dimensional spaces.

**Failure signatures**: Poor category discrimination indicates inadequate exploration coverage or feature representation issues; inconsistent category refinement suggests curiosity metrics may not align with meaningful distinctions.

**3 first experiments**:
1. Run curiosity-driven exploration in a controlled environment with known category boundaries to verify category discovery capability
2. Test feature extraction quality by clustering encoded visual observations and comparing to ground truth
3. Validate the WAC model's ability to correctly classify sensory exemplars from discovered categories

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on pre-trained vision models constrains discovery of truly novel categories
- Curiosity-driven exploration may face scalability challenges in complex environments
- Evaluation using Words-as-Classifiers requires labeled data, somewhat contradicting unsupervised approach

## Confidence

**High confidence**: Technical implementation of curiosity-driven exploration using Explauto is sound and reproducible; integration of vision models with exploration framework is well-documented.

**Medium confidence**: Claims about progressive category refinement are supported by experiments but need longer-term studies; vision model comparisons show patterns but lack statistical significance testing.

**Low confidence**: Scalability to real-world environments remains largely theoretical; claims about solving fundamental symbol grounding challenges need extensive real-world validation.

## Next Checks
1. Conduct systematic study varying sensorimotor space complexity to evaluate scalability limits of curiosity-driven exploration
2. Implement ablation studies comparing proposed method against alternative unsupervised clustering approaches on same robotic platform
3. Design long-term deployment study assessing whether progressive category refinement persists over extended operation in complex environments