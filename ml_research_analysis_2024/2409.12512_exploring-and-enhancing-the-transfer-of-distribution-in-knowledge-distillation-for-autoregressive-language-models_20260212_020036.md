---
ver: rpa2
title: Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation
  for Autoregressive Language Models
arxiv_id: '2409.12512'
source_url: https://arxiv.org/abs/2409.12512
tags:
- online
- teacher
- student
- https
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates knowledge distillation (KD) in autoregressive
  language models, identifying limitations in existing approaches that use fixed teacher
  distributions and on-policy student-generated outputs. The authors propose Online
  Knowledge Distillation (OKD), which integrates small online modules into the teacher
  network to adaptively update its distribution during training based on student feedback.
---

# Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models

## Quick Facts
- arXiv ID: 2409.12512
- Source URL: https://arxiv.org/abs/2409.12512
- Reference count: 40
- Primary result: OKD achieves superior ROUGE-L scores and reduces exposure bias compared to existing methods

## Executive Summary
This paper investigates knowledge distillation (KD) in autoregressive language models, identifying limitations in existing approaches that use fixed teacher distributions and on-policy student-generated outputs. The authors propose Online Knowledge Distillation (OKD), which integrates small online modules into the teacher network to adaptively update its distribution during training based on student feedback. This approach eliminates the need for student-generated output and significantly reduces training time. Extensive experiments across multiple model architectures and sizes (68M to 1B parameters) on various instruction-following datasets demonstrate that OKD outperforms state-of-the-art methods like GKD and MiniLLM while reducing training time by up to fourfold.

## Method Summary
OKD integrates small LoRA-based online modules into the teacher network, allowing the teacher's distribution to adapt dynamically during training based on student feedback. The student model is trained using Forward KL divergence between student and teacher distributions, while the teacher's online module parameters are updated concurrently. This eliminates the need for student-generated output during training, reducing exposure bias and improving efficiency. The method is evaluated across multiple model architectures and instruction-following datasets, comparing against baseline KD methods.

## Key Results
- OKD outperforms state-of-the-art methods like GKD and MiniLLM on ROUGE-L scores
- Training time reduced by up to fourfold compared to traditional KD methods
- OKD significantly reduces exposure bias compared to methods using student-generated outputs
- Superior performance across multiple model sizes (68M to 1B parameters) and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reverse KL divergence focuses on mode-seeking, prioritizing the teacher's peak probabilities, but ignores the overall distribution coverage.
- Mechanism: RKL maximizes the student's probability where the teacher's probability is high, but penalizes less when the teacher's probability is low, causing the student to concentrate on confident tokens.
- Core assumption: The teacher's mode captures the most important semantic signal for the student to learn.
- Evidence anchors:
  - [abstract]: "reverse Kullback-Leibler divergence (RKL) better directs models to learn specific patterns compared to forward KL (FKL)"
  - [section]: "According to Principle 2, RKL specifically targets the more challenging segments of data (those with lower probability P(x))."
  - [corpus]: Weak; corpus neighbors do not discuss RKL-specific distribution coverage.
- Break condition: When the teacher's mode is noisy or misaligned with the ground truth, the student overfits to incorrect confident tokens.

### Mechanism 2
- Claim: Student-generated output during training introduces exposure bias because the student's generated tokens deviate from the teacher's distribution, especially when model sizes differ greatly.
- Mechanism: Training on student-generated sequences changes the input distribution to the teacher, causing the teacher to see out-of-distribution data, which degrades distillation quality.
- Core assumption: The student's generation capability is weaker than the teacher's, so generated tokens are less reliable.
- Evidence anchors:
  - [abstract]: "student-generated output (SGO) to combat exposure bias"
  - [section]: "Findings 2: Using student-generated outputs when the student is poor can impair distillation results."
  - [corpus]: Weak; corpus neighbors do not directly address exposure bias from SGO.
- Break condition: When student and teacher models are close in size and capability, the bias effect diminishes.

### Mechanism 3
- Claim: Online Knowledge Distillation (OKD) adapts the teacher's distribution dynamically via small online modules, allowing the teacher to respond to the student's evolving distribution.
- Mechanism: LoRA-based online modules are trained alongside the student; teacher outputs are updated based on student feedback, reducing mismatch between teacher and student distributions.
- Core assumption: Small parameter-efficient modules can effectively capture the necessary adaptation without full teacher retraining.
- Evidence anchors:
  - [abstract]: "teacher network integrates small online modules to concurrently train with the student model"
  - [section]: "LoRA injects trainable low-rank matrices into transformer layers to approximate the weight updates."
  - [corpus]: Weak; corpus neighbors do not mention LoRA-based online modules.
- Break condition: If the online module capacity is too small, it cannot capture necessary adaptation, leading to suboptimal distillation.

## Foundational Learning

- Concept: Kullback-Leibler divergence asymmetry (forward vs. reverse)
  - Why needed here: Different behaviors of FKL vs. RKL critically impact which tokens the student learns to focus on during distillation.
  - Quick check question: If teacher probability is 0.9 and student probability is 0.1, which KL direction penalizes this more heavily?

- Concept: Exposure bias in sequence generation
  - Why needed here: Understanding how training-inference distribution mismatch degrades student performance is key to evaluating on-policy methods.
  - Quick check question: In teacher-forcing mode, which distribution mismatch metric would you expect to be lowest?

- Concept: Low-rank adaptation (LoRA) mechanics
  - Why needed here: LoRA modules are the core of OKD's online adaptation mechanism; understanding their parameterization is essential for implementation.
  - Quick check question: If LoRA down-projection matrix has rank r, how many trainable parameters does it add relative to full fine-tuning?

## Architecture Onboarding

- Component map:
  - Teacher backbone (frozen parameters) -> LoRA online module (trainable) -> Student model (trainable) -> Loss combiner (weighted sum of teacher-student KL and student ground truth KL)
- Critical path:
  1. Sample input from dataset
  2. Teacher + LoRA produces adaptive distribution
  3. Student produces distribution
  4. Compute two KL losses (teacher→student, student→ground truth)
  5. Update LoRA parameters and student parameters
- Design tradeoffs:
  - LoRA rank vs. adaptation capacity: higher rank gives better fit but more parameters
  - Temperature scaling: affects mode-seeking vs. mean-seeking behavior
  - Loss weighting: balancing teacher mimicry vs. ground truth alignment
- Failure signatures:
  - Student overfitting to teacher's mode without capturing diversity → check RKL vs FKL impact
  - Student collapse to degenerate outputs → check LoRA capacity and temperature
  - Training instability → check gradient scaling and learning rate schedule
- First 3 experiments:
  1. Compare RKL vs FKL distillation on a small dataset with identical teacher/student sizes
  2. Add LoRA online module to teacher and measure RougeL improvement over static teacher
  3. Vary LoRA rank (16, 32, 64) and measure impact on training time and final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal temperature for Reverse KL vary across different language model architectures and task types?
- Basis in paper: [explicit] The paper mentions that adjusting temperature affects Reverse KL's performance, with T=2 showing better results than T=1 or T=4 in their experiments.
- Why unresolved: The paper only tested a limited range of temperatures (T=1, 2, 4) on specific model architectures (GPT-2 and LLaMA variants). Different architectures might have different optimal temperatures, and the relationship between temperature and task complexity is not fully explored.
- What evidence would resolve it: Systematic experiments varying temperature across multiple architectures (different model sizes, transformer variants) and diverse task types (summarization, dialogue, code generation) would reveal optimal temperature ranges and their relationships to model/task characteristics.

### Open Question 2
- Question: What is the theoretical limit of knowledge transfer when the student model is significantly smaller than the teacher, and how does this affect the choice of distillation strategy?
- Basis in paper: [explicit] The paper discusses that when the student model is much smaller than the teacher (e.g., 68M vs 7B parameters), performance degrades, and existing on-policy methods like ImitKD and GKD underperform.
- Why unresolved: While the paper demonstrates performance degradation with large size gaps, it doesn't establish theoretical bounds on what knowledge can be transferred or provide a framework for predicting performance based on size ratios.
- What evidence would resolve it: A theoretical analysis framework that quantifies the information-theoretic limits of distillation between models of different sizes, combined with empirical validation across various size ratios, would establish these boundaries.

### Open Question 3
- Question: How does the online teacher adaptation mechanism affect the teacher model's performance on its original tasks, and can this degradation be minimized?
- Basis in paper: [explicit] The paper introduces OKD where the teacher model is updated during training, but doesn't analyze the impact on the teacher's original performance or explore methods to preserve it.
- Why unresolved: The paper focuses on student performance gains but doesn't investigate potential degradation in teacher performance or explore techniques like knowledge preservation layers or selective updating strategies.
- What evidence would resolve it: Experiments measuring teacher performance on its original tasks before and after OKD training, combined with ablation studies testing different teacher preservation techniques, would quantify this trade-off and identify optimal strategies.

## Limitations

- The paper does not provide systematic analysis of the relationship between LoRA rank and adaptation effectiveness across different model size gaps
- Experimental validation is limited to instruction-following datasets without testing on other domains like code generation or long-form narrative generation
- Claims about efficiency gains (up to fourfold reduction) lack detailed computational overhead analysis and concrete timing data

## Confidence

- High Confidence: The core claims about RKL's mode-seeking behavior and its impact on student learning are well-supported by the theoretical framework and consistent with established KL divergence literature.
- Medium Confidence: The exposure bias findings and the effectiveness of eliminating student-generated outputs are supported by experimental results, but the analysis could be strengthened with more diverse model size comparisons and additional failure case studies.
- Low Confidence: The specific claims about OKD's efficiency gains (up to fourfold reduction) and the universal applicability of LoRA-based online modules require more systematic ablation studies across different model architectures and training scales.

## Next Checks

1. **LoRA Rank Sensitivity Analysis**: Conduct experiments systematically varying LoRA rank (8, 16, 32, 64) across different teacher-student size gaps (2x, 5x, 10x) to establish the relationship between module capacity and distillation quality/efficiency.

2. **Domain Transfer Experiment**: Apply OKD to a non-instruction-following domain (e.g., code generation or long-form creative writing) and compare performance against traditional KD methods to test generalizability beyond the current experimental scope.

3. **Computational Overhead Benchmark**: Measure wall-clock training time and GPU memory usage for OKD versus baseline methods across different batch sizes and sequence lengths to validate the claimed efficiency improvements with concrete timing data.