---
ver: rpa2
title: 'RoMemes: A multimodal meme corpus for the Romanian language'
arxiv_id: '2410.15497'
source_url: https://arxiv.org/abs/2410.15497
tags:
- text
- memes
- meme
- romanian
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The RoMemes dataset is a multimodal corpus of 462 Romanian memes,
  annotated for text extraction, sentiment (polarity and emotion), political content,
  and fake image detection. Baseline experiments were conducted using OCR (Tesseract),
  sentiment analysis with lexicon-based and BERT features, political classification
  with LLMs, and fake image detection with CNNs and GANs.
---

# RoMemes: A multimodal meme corpus for the Romanian language

## Quick Facts
- arXiv ID: 2410.15497
- Source URL: https://arxiv.org/abs/2410.15497
- Reference count: 0
- The RoMemes dataset contains 462 Romanian memes with annotations for text, sentiment, political content, and fake image detection

## Executive Summary
The RoMemes dataset is a multimodal corpus of 462 Romanian memes, annotated for text extraction, sentiment (polarity and emotion), political content, and fake image detection. Baseline experiments were conducted using OCR (Tesseract), sentiment analysis with lexicon-based and BERT features, political classification with LLMs, and fake image detection with CNNs and GANs. OCR achieved a BLEU score of 45.40 and ChrF++ of 65.67 after preprocessing. Sentiment analysis yielded F1 scores up to 0.442 for polarity and 0.342 for emotion. Political meme classification reached F1 scores between 0.25–0.43. Fake image detection achieved F1 scores up to 0.7805 using ResNet101. The dataset is publicly available and suitable for advancing multimodal meme processing research in Romanian.

## Method Summary
The RoMemes dataset was created by collecting Romanian memes and annotating them for multiple tasks. Text extraction was performed using Tesseract OCR with preprocessing steps including upscaling, denoising, and binarization. Sentiment analysis used a combination of lexicon-based features (after translation to English for NRC lexicon compatibility) and BERT embeddings. Political meme classification employed zero-shot and few-shot learning with LLMs. Fake image detection used CNNs (ResNet50, ResNet101) and GANs (DCGAN, StyleGAN) with and without text information. All experiments were evaluated using standard metrics including BLEU, ChrF++, and F1 scores.

## Key Results
- OCR achieved BLEU score of 45.40 and ChrF++ of 65.67 after preprocessing
- Sentiment analysis F1 scores: 0.442 for polarity, 0.342 for emotion
- Political meme classification F1 scores: 0.25-0.43
- Fake image detection F1 score: 0.7805 using ResNet101

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal fusion improves fake image detection accuracy compared to single-modal approaches.
- Mechanism: Combining visual features from CNN models (ResNet50, ResNet101) with text embeddings from DistilBERT captures complementary information that helps distinguish real from fake memes.
- Core assumption: The text content of memes contains cues relevant to image authenticity that are not captured by image analysis alone.
- Evidence anchors:
  - [section] "The intuition behind this approach is that sometimes the text content may contain clues regarding the authenticity of the image."
  - [section] "The best performing model is represented by a ResNet101 image processing model, without text information. Thus it seems that our initial assumption that the text may provide useful information for classification was wrong."
- Break condition: If the text content consistently fails to provide relevant authenticity cues or introduces noise that degrades model performance.

### Mechanism 2
- Claim: OCR preprocessing pipeline improves text extraction accuracy from memes.
- Mechanism: A series of image processing steps (upscaling, noise reduction, binarization, deskewing) prepares meme images for OCR, addressing common challenges like low resolution, complex backgrounds, and text distortions.
- Core assumption: The quality of the input image significantly impacts OCR accuracy, and preprocessing can mitigate common issues in meme images.
- Evidence anchors:
  - [section] "We observe that most OCR errors come from either: a) lines with random characters in between the lines that actually contain the text of the meme"
  - [section] "By manually evaluating samples, we observe that most OCR errors come from either: a) lines with random characters in between the lines that actually contain the text of the meme (Table 3)"
- Break condition: If the preprocessing pipeline introduces artifacts that confuse the OCR model or if the computational cost outweighs the accuracy gains.

### Mechanism 3
- Claim: Lexicon-based features combined with BERT embeddings enable sentiment analysis of Romanian memes.
- Mechanism: Translating Romanian text to English allows leveraging the NRC emotion lexicon, while BERT provides context-aware representations, together creating a feature set that captures both lexical and contextual sentiment information.
- Core assumption: The NRC emotion lexicon, despite being English-based, contains sufficient emotional information that can be transferred to Romanian through translation.
- Evidence anchors:
  - [section] "The text was first translated into English using the Google Translator from the deep_translator library to allow compatibility with the NRC emotion lexicon."
  - [section] "Three binary features were created based on the number of words found in the lexicon, indicating a majority of words being positive, negative or neutral in meaning."
- Break condition: If the translation introduces significant errors or if the lexicon fails to capture the nuanced emotions expressed in memes.

## Foundational Learning

- Concept: Multimodal machine learning
  - Why needed here: The RoMemes dataset contains both images and text, requiring models that can process and fuse information from multiple modalities
  - Quick check question: What are the main challenges in developing multimodal models compared to unimodal models?

- Concept: Optical Character Recognition (OCR)
  - Why needed here: Memes often contain text overlaid on images, which must be extracted before analysis
  - Quick check question: What are the main challenges in applying OCR to meme images versus standard document images?

- Concept: Sentiment analysis and emotion detection
  - Why needed here: The dataset includes annotations for both polarity and emotion, requiring models that can classify these aspects
  - Quick check question: How do emotion detection models differ from simple sentiment analysis models?

## Architecture Onboarding

- Component map: Image processing pipeline → OCR engine → Text preprocessing → Feature extraction (BERT, lexicon) → Classification models (Random Forest, CNN, GAN) → Evaluation metrics
- Critical path: Image preprocessing → OCR → Text analysis → Classification
- Design tradeoffs: Accuracy vs. computational cost (e.g., using tessdata-best vs. tessdata models), single-modal vs. multimodal approaches, translation vs. native language models
- Failure signatures: Low OCR accuracy indicating preprocessing issues, poor classification performance suggesting feature extraction problems, high computational costs indicating optimization needs
- First 3 experiments:
  1. Test OCR accuracy on a small subset of memes with and without preprocessing
  2. Evaluate sentiment classification using only lexicon features vs. adding BERT features
  3. Compare fake image detection performance using ResNet50 vs. ResNet101 models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal algorithms be optimized to improve the detection of deep fake images in memes, particularly in the Romanian language?
- Basis in paper: [explicit] The authors note that with the development of generative artificial neural models, computers can generate vivid face images that can easily deceive human beings. They acknowledge that the dataset needs to be expanded with additional memes containing deep fake images to be successfully employed for the multimodal task of deep fake detection.
- Why unresolved: The paper indicates that current algorithms are not sufficiently accurate for deep fake detection in memes and that the dataset needs expansion, suggesting a gap in existing research and methodology.
- What evidence would resolve it: Successful experiments demonstrating improved accuracy in deep fake detection using expanded datasets and advanced multimodal algorithms, validated on a diverse set of Romanian memes.

### Open Question 2
- Question: What are the limitations of current OCR technologies in extracting text from memes, and how can preprocessing techniques be further improved to enhance OCR accuracy?
- Basis in paper: [explicit] The authors discuss the challenges of OCR in recognizing text from memes, particularly due to the background images and partially structured text. They experimented with preprocessing techniques to improve OCR accuracy, indicating that further refinement is needed.
- Why unresolved: The paper shows that while preprocessing improved OCR results, errors still occur, particularly with random characters and misspellings, suggesting room for further optimization.
- What evidence would resolve it: Development and validation of advanced preprocessing techniques that significantly reduce OCR errors, demonstrated through improved BLEU and ChrF++ scores on a benchmark dataset of memes.

### Open Question 3
- Question: How can sentiment analysis algorithms be adapted to better handle the metaphorical and context-dependent nature of meme content, particularly for languages like Romanian?
- Basis in paper: [explicit] The authors acknowledge that sentiment analysis of memes is challenging due to the metaphorical nature of meme content and the need for context understanding. They experimented with lexicon-based and BERT features but noted that further research is needed to improve performance.
- Why unresolved: The paper indicates that current sentiment analysis models, even with advanced features like BERT, struggle with the nuanced and context-dependent nature of memes, especially in Romanian.
- What evidence would resolve it: Development of sentiment analysis models that achieve higher accuracy on meme datasets, validated through improved F1 scores and cross-lingual comparisons, demonstrating better handling of metaphorical content.

## Limitations
- The dataset size of 462 memes limits generalizability of baseline results
- Translation-based sentiment analysis may introduce noise and lose cultural context
- Political classification performance (F1 0.25-0.43) indicates task difficulty or model limitations
- Text features degraded fake image detection performance, questioning multimodal fusion effectiveness

## Confidence

High confidence: OCR preprocessing pipeline effectiveness and its impact on text extraction accuracy. The preprocessing steps and their benefits are clearly demonstrated through quantitative metrics and qualitative error analysis.

Medium confidence: Sentiment analysis and emotion detection results. While the methodology is sound, the translation-based approach and modest F1 scores (up to 0.442 for polarity, 0.342 for emotion) indicate potential limitations in capturing nuanced emotions in memes.

Medium confidence: Political meme classification using LLMs. The modest performance (F1 0.25-0.43) and the acknowledgment of task difficulty suggest reasonable confidence in the reported results, though the approach may have limitations.

Medium confidence: Fake image detection using CNNs. The ResNet101 model achieved good performance (F1 up to 0.7805), but the finding that text features degraded performance raises questions about the multimodal approach's effectiveness.

## Next Checks

1. **OCR Accuracy Validation**: Manually evaluate OCR output on a stratified sample of memes to identify systematic error patterns and assess whether preprocessing parameters need adjustment for different meme types.

2. **Sentiment Analysis Ablation Study**: Compare sentiment classification performance using native Romanian sentiment lexicons versus the translation-based approach to quantify the impact of translation on emotion detection accuracy.

3. **Multimodal Fusion Reassessment**: Conduct controlled experiments testing whether text features improve fake image detection when using different image models or when focusing on specific types of authenticity cues (e.g., image manipulation vs. misleading content).