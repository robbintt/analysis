---
ver: rpa2
title: Learning Optimal Deterministic Policies with Stochastic Policy Gradients
arxiv_id: '2405.02235'
source_url: https://arxiv.org/abs/2405.02235
tags:
- policy
- deterministic
- learning
- policies
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides theoretical understanding of learning deterministic
  policies using stochastic policy gradient methods. The authors introduce a white-noise
  exploration framework unifying action-based and parameter-based exploration.
---

# Learning Optimal Deterministic Policies with Stochastic Policy Gradients

## Quick Facts
- arXiv ID: 2405.02235
- Source URL: https://arxiv.org/abs/2405.02235
- Reference count: 40
- Primary result: Theoretical framework showing deterministic policies can be learned using stochastic policy gradients with sample complexities of O(ϵ⁻⁵) for parameter-based and O(ϵ⁻⁷) for action-based exploration

## Executive Summary
This paper bridges the gap between stochastic policy gradient methods and deterministic policy optimization by introducing a unified white-noise exploration framework. The authors prove that stochastic policy gradient methods can converge to optimal deterministic policies under gradient domination assumptions, with explicit sample complexity bounds that quantify the exploration-exploitation tradeoff. The framework applies to both action-based (GPOMDP) and parameter-based (PGPE) exploration methods, showing different convergence rates depending on policy dimensionality and exploration strategy.

## Method Summary
The paper introduces white-noise exploration where additive noise is sampled independently at each step (action-based) or trajectory start (parameter-based), enabling both approaches to be modeled under a single framework. This noise perturbs either actions directly or policy parameters before execution. The authors prove that global convergence to the optimal deterministic policy can be achieved with sample complexities O(ϵ⁻⁵) for parameter-based and O(ϵ⁻⁷) for action-based exploration under gradient domination assumptions, with explicit dependencies on policy dimension, noise variance, and discount factor.

## Key Results
- White-noise exploration framework unifies action-based and parameter-based methods under a single theoretical treatment
- Convergence to optimal deterministic policy proven under gradient domination assumptions
- Sample complexities of O(ϵ⁻⁵) for parameter-based and O(ϵ⁻⁷) for action-based exploration, with explicit dependencies on policy dimension and noise variance
- Experiments on Mujoco environments validate theoretical predictions, showing PGPE outperforming GPOMDP when parameter dimension is low and horizon is long

## Why This Works (Mechanism)

### Mechanism 1
- Claim: White-noise exploration allows unified treatment of action-based (AB) and parameter-based (PB) exploration by perturbing actions or parameters independently.
- Mechanism: The paper introduces white-noise exploration where additive noise (ϵ) is sampled independently at each step (AB) or trajectory start (PB), enabling both AB and PB methods to be modeled under a single framework. This noise perturbs either the actions directly or the policy parameters before execution.
- Core assumption: The noise distribution Φd is zero-mean and has bounded variance (Definition 3.1), and the policy classes are well-behaved (Lipschitz/smooth as per Assumptions 4.1-4.5).
- Evidence anchors:
  - [abstract] "introduce a white-noise exploration framework unifying action-based and parameter-based exploration"
  - [section] "We formalize a class of stochastic (hyper)policies widely employed in the practice of AB and PB exploration, namely white noise-based (hyper)policies"
  - [corpus] No direct evidence; corpus neighbors discuss deterministic/stochastic policy gradients but not this unified white-noise framework.
- Break condition: If the noise is state-dependent or correlated across steps, the framework breaks. Also fails if policy classes violate smoothness/Lipschitz assumptions.

### Mechanism 2
- Claim: The deterministic policy performance JD can be related to the stochastic policy objectives JA/JP via Lipschitz bounds on the objective functions.
- Mechanism: Theorems 5.1 and 5.2 show that |JD(θ) - JP(θ)| ≤ LJ√(dΘ)σP and |JD(θ) - JA(θ)| ≤ L√(dA)σA. These bounds quantify how far the stochastic objective is from the deterministic one, depending on noise variance and policy dimension.
- Core assumption: The deterministic objective JD is Lipschitz continuous in its argument (Assumptions 5.1 and 5.2), and the MDP/policy classes satisfy regularity conditions (Assumptions 4.1-4.4).
- Evidence anchors:
  - [abstract] "prove convergence to the optimal deterministic policy under gradient domination assumptions"
  - [section] "Theorem 5.1 (Deterministic deployment of parameters learned with PB white-noise exploration)"
  - [corpus] No direct evidence; corpus neighbors focus on policy gradient convergence but not deterministic deployment bounds.
- Break condition: If JD is not Lipschitz (e.g., highly non-convex or discontinuous), the bounds fail. Also breaks if noise variance is infinite.

### Mechanism 3
- Claim: Global convergence to the optimal deterministic policy can be achieved with sample complexities O(ϵ⁻⁵) for PB and O(ϵ⁻⁷) for AB exploration under gradient domination assumptions.
- Mechanism: The paper uses weak gradient domination (WGD) assumptions (Assumption 6.1) to prove global convergence. By carefully tuning the exploration variance σ as a function of ϵ, the algorithms converge to a deterministic policy with error ϵ, with different sample complexities for PB and AB methods.
- Core assumption: The stochastic objectives JA/JP satisfy WGD (Assumption 6.1), and the MDP/policy classes are smooth (Assumptions 4.2, 4.4) or have bounded variance (Assumption 4.5).
- Evidence anchors:
  - [abstract] "prove convergence to the optimal deterministic policy under gradient domination assumptions" and "sample complexities of O(ϵ⁻⁵) and O(ϵ⁻⁷)"
  - [section] "Theorem 6.1" and "Theorem D.5" showing the convergence rates
  - [corpus] Corpus neighbors discuss policy gradient convergence but not these specific global convergence rates with deterministic deployment.
- Break condition: If WGD doesn't hold (e.g., many bad local optima), convergence fails. Also breaks if variance bounds are violated or if adaptive variance learning is used (see Remark C.1).

## Foundational Learning

- Concept: Weak Gradient Domination (WGD)
  - Why needed here: WGD is the key assumption that allows proving global convergence to the optimal policy, rather than just convergence to stationary points. It ensures the objective has no "bad" local optima that trap the algorithm.
  - Quick check question: If an objective satisfies J* - J(θ) ≤ α||∇J(θ)||² + β for all θ, what does β represent?

- Concept: Lipschitz Continuity and Smoothness
  - Why needed here: These properties of the MDP, policy classes, and objective functions are crucial for bounding the effects of exploration noise and for proving convergence rates. They ensure the learning dynamics are well-behaved.
  - Quick check question: If a function f is L-Lipschitz, what is the maximum possible difference |f(x) - f(y)| given ||x - y||₂ = d?

- Concept: Variance Bounds for Policy Gradient Estimators
  - Why needed here: The variance of the gradient estimators (GPOMDP, PGPE) affects the sample complexity. Bounded variance is needed to prove convergence rates and to relate the stochastic objectives to the deterministic one.
  - Quick check question: If the variance of an unbiased gradient estimator is V, how does the batch size N affect the variance of the average estimator?

## Architecture Onboarding

- Component map: MDP environment (p, r) -> Deterministic policy (µθ) -> Stochastic policy/hyperpolicy (πρ or νρ) with white noise -> Policy gradient algorithms (GPOMDP, PGPE) -> Updated parameters
- Critical path: Sample trajectories → Compute policy gradient estimate → Update policy parameters → Repeat until convergence → Deploy deterministic policy
- Design tradeoffs: AB exploration (GPOMDP) has higher sample complexity but better dependence on action dimension dA; PB exploration (PGPE) has lower sample complexity but better dependence on parameter dimension dΘ. The choice depends on whether dA or dΘ is larger. Fixed exploration variance vs. adaptive variance is another tradeoff (fixed gives guarantees, adaptive may be more practical but breaks theory).
- Failure signatures: Slow convergence or getting stuck in bad local optima suggests WGD doesn't hold. High variance in learning curves suggests the variance bounds are violated. Poor final deterministic policy performance suggests the Lipschitz bounds on JD are too loose.
- First 3 experiments:
  1. Run GPOMDP and PGPE on a simple LQR environment with known optimal deterministic policy. Measure final performance and sample complexity.
  2. Vary the exploration variance σ and plot the final deterministic policy performance. Verify the Lipschitz bounds from Theorems 5.1 and 5.2.
  3. Compare AB vs. PB exploration on environments with high dA vs. high dΘ. Verify the theoretical predictions about which method performs better.

## Open Questions the Paper Calls Out

- Question: Can we develop practical adaptive-variance schedules for policy gradient methods that achieve global convergence to deterministic policies?
  - Basis in paper: [explicit] The paper notes that the theoretical selection of policy variance is not practical and hopes that findings will guide the design of sound and efficient adaptive-variance schedules in future work.
  - Why unresolved: While the paper establishes theoretical foundations and sample complexity bounds for fixed variance cases, it does not provide concrete algorithms or empirical validation of adaptive schedules that would be practically useful.
  - What evidence would resolve it: Development and empirical validation of adaptive variance algorithms showing improved convergence to deterministic policies compared to fixed variance approaches, with theoretical guarantees matching or improving upon the paper's bounds.

- Question: Under what conditions does a sufficient amount of noise eliminate local optima in the deterministic policy objective function?
  - Basis in paper: [inferred] The paper suggests this as a natural next question in the conclusion, noting that studying whether white-noise exploration preserves weak gradient domination could lead to understanding if noise can smooth or eliminate local optima.
  - Why unresolved: The paper establishes that white-noise policies preserve weak gradient domination but does not explore whether increasing noise levels can eliminate local optima entirely.
  - What evidence would resolve it: Theoretical analysis and empirical demonstrations showing specific MDP classes where increased noise levels eliminate local optima, with quantitative characterization of the noise threshold required.

- Question: How does the Fisher-non-degeneracy parameter λE depend on the policy variance σA in practice, and can this dependence be leveraged to improve sample complexity bounds?
  - Basis in paper: [explicit] The paper notes that λE can depend on σA in highly non-trivial ways and may hide additional factors of ϵ, preventing direct comparison with inherited WGD results.
  - Why unresolved: The paper establishes theoretical bounds but acknowledges that the dependence of λE on σA is not well understood, which prevents tighter sample complexity bounds.
  - What evidence would resolve it: Empirical and theoretical characterization of λE as a function of σA for specific policy classes, demonstrating how this relationship affects sample complexity and suggesting practical variance selection strategies.

## Limitations
- Theoretical analysis relies heavily on Lipschitz and smoothness assumptions that may not hold in practice
- White-noise exploration framework assumes independent, zero-mean noise, which may not capture more sophisticated exploration strategies
- Sample complexity bounds depend on the dimension of the policy parameter space, which could be prohibitively large for complex tasks

## Confidence
- **High confidence**: The unified white-noise exploration framework and its mathematical formulation (Mechanism 1). The Lipschitz bounds relating stochastic and deterministic objectives (Mechanism 2).
- **Medium confidence**: The global convergence rates under gradient domination assumptions (Mechanism 3). The experimental validation on Mujoco environments.
- **Low confidence**: The practical applicability of the sample complexity bounds for high-dimensional policies. The performance of fixed exploration variance versus adaptive methods in practice.

## Next Checks
1. Test the convergence rates empirically on environments with known optimal deterministic policies to verify the O(ϵ⁻⁵) and O(ϵ⁻⁷) bounds.
2. Evaluate the impact of violating the Lipschitz/smoothness assumptions by using highly non-convex reward functions.
3. Compare fixed exploration variance versus adaptive variance methods on real-world robotics tasks to assess the practical value of the theoretical guarantees.