---
ver: rpa2
title: Understanding the effects of word-level linguistic annotations in under-resourced
  neural machine translation
arxiv_id: '2401.16078'
source_url: https://arxiv.org/abs/2401.16078
tags:
- tags
- language
- translation
- linguistic
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the effects of word-level linguistic annotations
  in under-resourced neural machine translation. We evaluated part-of-speech tags
  and morpho-syntactic description tags interleaved in source and/or target sentences
  for eight language pairs, different training corpus sizes, and two architectures.
---

# Understanding the effects of word-level linguistic annotations in under-resourced neural machine translation

## Quick Facts
- arXiv ID: 2401.16078
- Source URL: https://arxiv.org/abs/2401.16078
- Reference count: 8
- For highly inflected target languages, source-language annotations are helpful and morpho-syntactic descriptions outperform part-of-speech tags for some language pairs. On the contrary, when words are annotated in the target language, part-of-speech tags systematically outperform morpho-syntactic description tags in terms of automatic evaluation metrics, even though the use of morpho-syntactic description tags improves the grammaticality of the output.

## Executive Summary
This paper investigates how word-level linguistic annotations affect neural machine translation performance in under-resourced scenarios. The authors evaluate part-of-speech tags and morpho-syntactic description tags interleaved in source and/or target sentences across eight language pairs, different training corpus sizes, and two architectures. They find that source-language annotations help reduce lexical errors and improve source sentence representation, while target-language annotations improve grammaticality but can increase lexical errors. The effectiveness of annotations depends on the target language's morphological complexity and the neural architecture used.

## Method Summary
The study evaluates NMT models trained on parallel corpora with word-level linguistic annotations interleaved as additional tokens before each word. Three types of annotations are tested: dummy tags, part-of-speech tags, and morpho-syntactic description tags. The annotations are applied to either source or target languages (or both) in eight language pairs using both recurrent neural network and Transformer architectures. Models are trained on downsampled data to simulate under-resourced conditions, and performance is evaluated using BLEU scores and automatic error classification across four error types: inflection, reordering, missing/extra words, and lexical errors.

## Key Results
- Source-language POS and MSD tags systematically reduce lexical errors across architectures
- Target-language MSD tags improve grammaticality (reduce inflection errors) but increase lexical errors
- For highly inflected target languages, source-language annotations provide consistent benefits
- Transformer architectures show degraded performance with target-language tags compared to RNNs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaving SL linguistic annotations improves NMT performance by providing richer source representations that break grammatical ambiguity.
- Mechanism: Tags are interleaved as additional tokens before each word, allowing the encoder to explicitly encode grammatical features (POS, morphology) that help disambiguate syntactic roles and improve sentence encoding.
- Core assumption: The encoder can effectively integrate interleaved tags into the hidden states to enhance source sentence representation.
- Evidence anchors:
  - [abstract] "SL POS and MSD tags systematically reduce lexical errors... SL tags help to obtain a better representation of the SL sentence"
  - [section 5] "SL POS and SL MSD tags systematically reduce lexical errors (green, empty squares and triangles are below the horizontal line)"

### Mechanism 2
- Claim: TL MSD tags improve grammaticality but increase lexical errors due to sparsity and conditioning effects.
- Mechanism: MSD tags provide morphological features that guide correct word inflection, but their sparsity and the strong conditioning on them in surface form prediction lead to wrong lexical choices and reduced POS accuracy.
- Core assumption: The decoder can learn to condition on morphological features for inflection without compromising lexical selection.
- Evidence anchors:
  - [abstract] "when words are annotated in the target language, part-of-speech tags systematically outperform morpho-syntactic description tags... even though the use of morpho-syntactic description tags improves the grammaticality of the output"
  - [section 5] "TL MSD tags systematically reduce inflection errors... The prediction of MSD tags with complex morphology... also leads to an increase in lexical errors"

### Mechanism 3
- Claim: For self-attention architectures, adding interleaved tokens degrades performance due to increased sequence length and attention complexity.
- Mechanism: In Transformer models, interleaving tags increases sequence length, which may lead to attention saturation or difficulty in attending to relevant information, especially for TL tags.
- Core assumption: The self-attention mechanism's effectiveness degrades with longer sequences or when forced to attend to interleaved tag tokens.
- Evidence anchors:
  - [section 4] "TL DUM tags consistently increase the total number of translation errors when they are added to a Transformer system and the TL is highly inflected"
  - [section 4] "The results for the Englishâ€“German WMT large-scale training data... show a different picture... only recurrent systems were able to take advantage of the linguistic annotations"

## Foundational Learning

- Concept: Byte Pair Encoding (BPE)
  - Why needed here: BPE is used to segment words into subword units, and tags are interleaved before the first subword unit, affecting how the model processes the input.
  - Quick check question: How does BPE segmentation interact with interleaved tags in terms of token alignment and embedding lookup?

- Concept: Morphological analysis and tagging
  - Why needed here: The paper relies on accurate POS and MSD tags obtained from a morphological analyzer, which are critical for the interleaved annotations.
  - Quick check question: What are the differences between POS tags and MSD tags, and how does each type of information affect translation quality?

- Concept: Automatic error classification in MT
  - Why needed here: The paper uses Hjerson to classify errors into inflection, reordering, missing/extra words, and lexical errors, which informs the analysis of how annotations affect different error types.
  - Quick check question: How does Hjerson distinguish between lexical errors and other error types, and why is this distinction important for evaluating the impact of annotations?

## Architecture Onboarding

- Component map:
  Encoder (RNN or Transformer) -> Decoder (RNN or Transformer) -> Embedding layer -> BPE segmentation -> Morphological analyzer -> Error classifier (Hjerson)

- Critical path:
  1. Preprocess parallel corpus with morphological analyzer
  2. Apply BPE segmentation
  3. Interleave tags before first subword of each word
  4. Train NMT model on annotated corpus
  5. Decode test set with tag-to-word alternation
  6. Remove TL tags from output
  7. Evaluate with BLEU and error classification

- Design tradeoffs:
  - Interleaving vs. separate decoder heads: Interleaving is simpler but may suffer from sequence length issues; separate heads avoid this but require architectural changes.
  - SL vs. TL annotations: SL annotations improve source encoding; TL annotations improve target inflection but risk lexical errors.
  - MSD vs. POS tags: MSD provides more morphological information but is sparser and harder to predict accurately.

- Failure signatures:
  - BLEU drops significantly when TL MSD tags are used, especially for highly inflected languages.
  - Increase in lexical errors when MSD tags are interleaved in TL.
  - Reordering errors increase in Transformer models with TL tags.
  - Surface form prediction accuracy drops for low-frequency and OOV words with TL MSD tags.

- First 3 experiments:
  1. Train baseline NMT model (no tags) on downsampled parallel corpus for a morphologically rich TL.
  2. Train model with SL MSD tags interleaved to test source representation improvement.
  3. Train model with TL POS tags interleaved to test target inflection improvement without MSD sparsity issues.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of interleaved linguistic annotations change when using truly under-resourced language pairs (rather than downsampled data) and different morphological analyzers?
- Basis in paper: [explicit] The paper notes that their conclusions might differ for truly under-resourced language pairs and mentions that linguistic annotations can be obtained with different morphological analyzers, which could introduce distortions.
- Why unresolved: The study used downsampled data from resource-rich language pairs to simulate under-resourced scenarios, which may not fully capture the challenges of genuinely low-resource languages. Additionally, the impact of different morphological analyzer technologies and performance across languages was not evaluated.
- What evidence would resolve it: Experiments comparing interleaved linguistic annotations on truly under-resourced language pairs with various morphological analyzers, measuring translation quality and error rates.

### Open Question 2
- Question: What is the impact of using different subword segmentation strategies (e.g., BPE vs. SentencePiece) on the effectiveness of interleaved linguistic annotations in neural machine translation?
- Basis in paper: [inferred] The paper used BPE segmentation for all experiments, but different segmentation strategies might affect how well the model learns the relationship between tags and surface forms, especially for languages with complex morphology.
- Why unresolved: The study only used BPE segmentation, so the impact of alternative subword segmentation methods on the performance of interleaved linguistic annotations remains unknown.
- What evidence would resolve it: Comparative experiments using different subword segmentation strategies (e.g., BPE, SentencePiece, unigram language model) with interleaved linguistic annotations, measuring translation quality and error rates.

### Open Question 3
- Question: How do interleaved linguistic annotations affect the translation quality of low-resource language pairs when combined with transfer learning or multilingual models?
- Basis in paper: [inferred] The paper focuses on bilingual translation models, but transfer learning and multilingual models are increasingly used in low-resource settings and could potentially enhance the benefits of linguistic annotations.
- Why unresolved: The study did not explore the combination of interleaved linguistic annotations with transfer learning or multilingual models, so their potential synergistic effects remain unknown.
- What evidence would resolve it: Experiments combining interleaved linguistic annotations with transfer learning or multilingual models for low-resource language pairs, measuring translation quality and error rates.

## Limitations
- The analysis focuses on only eight language pairs, all involving European languages, limiting generalizability to diverse linguistic phenomena.
- The evaluation relies entirely on automatic metrics and error classification without human evaluation of grammaticality improvements.
- The study assumes perfect tag prediction accuracy, not accounting for real-world errors from morphological analyzers.
- The computational overhead of tag interleaving, particularly for Transformer architectures, is not analyzed.

## Confidence
- High: SL POS and MSD tags consistently reduce lexical errors across architectures; TL MSD tags improve grammaticality but increase lexical errors; for highly inflected TLs, SL annotations provide consistent benefits; Transformer architectures show degraded performance with TL tags compared to RNNs
- Medium: MSD tags outperform POS tags for SL annotations in highly inflected languages; the superiority of SL tags over TL tags is consistent but the magnitude varies by language pair; RNN architectures benefit more from linguistic annotations than Transformer architectures
- Low: The specific ranking of MSD vs POS tags for TL annotations generalizes beyond the studied language pairs; the observed performance degradation in Transformers with TL tags would persist at larger scale or with architectural modifications; the trade-off between grammaticality and lexical accuracy with TL MSD tags represents an inherent limitation rather than a training or architecture issue

## Next Checks
1. Conduct human evaluation focusing specifically on the trade-off between grammaticality improvements and lexical accuracy degradation when using TL MSD tags.
2. Evaluate system performance when using automatically predicted tags with varying accuracy levels (e.g., 95%, 90%, 85% tag accuracy) rather than perfect oracle tags.
3. Extend the analysis to language pairs involving agglutinative languages from different families (e.g., Turkish-English, Finnish-English) and comparing against fusional languages to test whether observed patterns hold across different morphological typologies.