---
ver: rpa2
title: Which questions should I answer? Salience Prediction of Inquisitive Questions
arxiv_id: '2404.10917'
source_url: https://arxiv.org/abs/2404.10917
tags:
- questions
- question
- salience
- article
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QSALIENCE, a salience predictor for inquisitive
  questions, which are open-ended questions generated while reading. The authors collected
  and annotated 1,766 question-context pairs with salience scores indicating how important
  answering each question is for understanding the text.
---

# Which questions should I answer? Salience Prediction of Inquisitive Questions

## Quick Facts
- arXiv ID: 2404.10917
- Source URL: https://arxiv.org/abs/2404.10917
- Reference count: 40
- Introduces QSALIENCE, a model that predicts the salience of inquisitive questions

## Executive Summary
This paper introduces QSALIENCE, a novel predictor for inquisitive question salience. The authors collected and annotated 1,766 question-context pairs with salience scores, finding that highly salient questions are more likely to be answered in the article. QSALIENCE, instruction-tuned from open-source LLMs, outperforms GPT-4 in predicting question salience using much smaller models like Flan-T5. The model achieves moderate agreement with human annotations, and summaries answering more salient questions are ranked higher by human evaluators, validating the practical utility of question salience prediction.

## Method Summary
The authors collected 1,766 question-context pairs from Wikipedia articles and annotated them with salience scores. They then instruction-tuned open-source LLMs to create QSALIENCE, a model that predicts the salience of inquisitive questions. The model was trained on the annotated dataset and evaluated against human judgments and GPT-4. The authors also conducted human evaluations of summaries generated by answering highly salient questions versus random questions.

## Key Results
- QSALIENCE outperforms GPT-4 in predicting question salience using much smaller models like Flan-T5
- Highly salient questions are more likely to be answered in the article, connecting potential questions to Questions Under Discussion (QUDs)
- Summaries answering more salient questions are ranked higher by human evaluators

## Why This Works (Mechanism)
QSALIENCE works by leveraging the correlation between highly salient questions and Questions Under Discussion (QUDs) in the text. The model is instruction-tuned on a dataset of annotated question-context pairs, allowing it to learn patterns in what makes questions important for understanding the text. By predicting question salience, QSALIENCE can help identify which questions are most worth answering to improve text comprehension.

## Foundational Learning
- **Inquisitive questions**: Open-ended questions generated while reading, important for understanding text. Why needed: Form the basis of the salience prediction task. Quick check: Are questions generated from actual reading sessions or synthetically created?
- **Questions Under Discussion (QUDs)**: Questions that drive discourse and are typically answered in the text. Why needed: Provides a theoretical framework for understanding question salience. Quick check: How are QUDs identified in the text?
- **Salience prediction**: The task of determining how important answering a question is for understanding the text. Why needed: Core task that QSALIENCE is designed to solve. Quick check: What annotation scheme was used for salience scores?

## Architecture Onboarding

**Component Map:**
Open-source LLM (e.g., Flan-T5) -> Instruction tuning -> QSALIENCE model

**Critical Path:**
1. Collect question-context pairs from Wikipedia articles
2. Annotate pairs with salience scores
3. Instruction-tune open-source LLM on annotated dataset
4. Evaluate QSALIENCE against human judgments and GPT-4
5. Conduct human evaluations of summaries generated using QSALIENCE

**Design Tradeoffs:**
- Uses smaller open-source models instead of larger proprietary models like GPT-4
- Focuses on Wikipedia-style content, may not generalize to other domains
- Relies on moderate-sized annotated dataset (1,766 pairs) rather than larger corpora

**Failure Signatures:**
- Mispredicts salience for questions that humans find important but model misses
- May not generalize well to texts outside the Wikipedia domain
- Could struggle with questions that don't clearly map to QUDs in the text

**3 First Experiments:**
1. Evaluate QSALIENCE on out-of-domain texts (scientific papers, news articles, social media)
2. Compare summaries generated using different question selection strategies, including random selection as a baseline
3. Analyze failure cases to understand when and why QSALIENCE mispredicts salience

## Open Questions the Paper Calls Out
None explicitly stated in the provided information.

## Limitations
- Dataset size of 1,766 annotated pairs is relatively modest for training and validation
- Moderate agreement with human annotations suggests the task remains challenging and potentially unstable
- Model may not generalize well beyond Wikipedia-style content to diverse domains or text types

## Confidence

**High confidence:**
- QSALIENCE outperforms GPT-4 on the specific benchmark tasks and dataset used

**Medium confidence:**
- The correlation between salient questions and QUDs, and the utility of salience prediction for summarization tasks
- The moderate agreement with human annotations indicates the model captures meaningful patterns, though not perfectly

## Next Checks
1. Test QSALIENCE on out-of-domain texts (e.g., scientific papers, news articles, social media) to assess generalization beyond Wikipedia-style content
2. Conduct a larger-scale human evaluation comparing summaries generated using different question selection strategies, including random selection as a baseline
3. Analyze failure cases systematically to understand when and why QSALIENCE mispredicts salience, particularly for questions that humans find important but the model misses