---
ver: rpa2
title: Out-of-Domain Generalization in Dynamical Systems Reconstruction
arxiv_id: '2402.18377'
source_url: https://arxiv.org/abs/2402.18377
tags: []
core_contribution: "This work studies out-of-domain generalization (OODG) in dynamical\
  \ systems reconstruction (DSR) from a theoretical perspective. While recent deep\
  \ learning methods have shown promise in reconstructing observed dynamical systems,\
  \ their ability to generalize to unobserved domains\u2014a crucial property for\
  \ any scientific theory\u2014remains an open challenge."
---

# Out-of-Domain Generalization in Dynamical Systems Reconstruction

## Quick Facts
- arXiv ID: 2402.18377
- Source URL: https://arxiv.org/abs/2402.18377
- Authors: Niclas Göring; Florian Hess; Manuel Brenner; Zahra Monfared; Daniel Durstewitz
- Reference count: 40
- Key outcome: Black-box deep learning methods cannot generalize across basins of attraction in multistable systems, while methods with strong priors (like SINDy) can.

## Executive Summary
This work provides the first comprehensive mathematical treatment of out-of-domain generalization (OODG) in dynamical systems reconstruction (DSR). The authors distinguish between in-distribution generalization (within one basin of attraction) and OODG across basins in multistable systems. They develop principled error measures based on ergodic theory and topology, and prove that without adequate structural priors, deep learning techniques cannot learn a generalizing DSR model. The study demonstrates empirically that state-of-the-art DSR algorithms fail to generalize across the whole phase space when trained on data from just one basin, while methods like SINDy with strong prior knowledge can succeed.

## Method Summary
The paper studies OODG in DSR by comparing universal approximators (RNNs, N-ODEs, RCs) against library-based methods (SINDy) using synthetic data from multistable systems. The key innovation is introducing statistical and topological error measures to formally assess generalization across basins of attraction. The statistical error uses sliced Wasserstein distance between occupation measures, while topological error compares Lyapunov spectra and limit sets. The authors prove that for universal approximators without priors, the parameter-function map is biased toward monostable dynamics, making OODG generally unlearnable from single-basin data.

## Key Results
- Black-box deep learning methods (RNNs, N-ODEs, RCs) trained on single-basin data fail to reconstruct other basins of attraction in multistable systems
- SINDy with appropriate function libraries can successfully generalize from single-trajectory data when the trajectory doesn't satisfy algebraic equations in the parameters
- The generalization error in DSR is proportional to the volume of basins not reconstructed, making basin coverage essential for successful OODG
- Current training methods are biased toward monostable dynamics at initialization and avoid multistable solutions even when they exist in the loss landscape

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Generalization failure across basins occurs because the training loss landscape is dominated by local minima corresponding to monostable dynamics, not multistable ones.
- **Mechanism**: When training RNNs or N-ODEs on data from a single basin, the optimizer finds parameter configurations that perfectly fit the observed trajectories but only capture one attractor. The loss function, optimized for prediction accuracy, does not penalize missing attractors as long as training error is zero. Since the parameter-function map is biased toward simpler (monostable) dynamics due to initialization schemes, the algorithm rarely discovers multistable configurations even if they exist in the loss landscape.
- **Core assumption**: The parameter-function map of RNNs and N-ODEs is biased toward monostable dynamics at initialization, and this bias is not overcome by standard optimization procedures.
- **Evidence anchors**:
  - [abstract]: "black-box DL techniques, without adequate structural priors, generally will not be able to learn a generalizing DSR model"
  - [section 4.2]: "we can show that there is an infinity of models in the hypothesis class having zero reconstruction error on the training domain but a very high error on the whole state space"
  - [corpus]: No direct evidence; inference based on stated results.
- **Break condition**: If initialization schemes or training procedures are modified to explicitly encourage multistability (e.g., by targeting higher entropy attractors at initialization or using regularization that penalizes monostability), the mechanism would break.

### Mechanism 2
- **Claim**: Sparse regression methods like SINDy with appropriate function libraries can uniquely identify the underlying vector field from a single trajectory, even in multistable systems.
- **Mechanism**: Library-based methods constrain the hypothesis space to a finite-dimensional linear function class. If the trajectory does not satisfy an algebraic equation in the basis functions, the parameter estimation problem becomes well-posed and has a unique solution. This solution generalizes across basins because the learned vector field is the correct one for the entire state space, not just the observed basin.
- **Core assumption**: The true vector field can be expressed as a sparse linear combination of the chosen basis functions, and the observed trajectory does not lie on a solution manifold of the parameter estimation problem.
- **Evidence anchors**:
  - [abstract]: "methods like SINDy with strong prior knowledge can succeed"
  - [section 4.1]: "Theorem 4.1... If there exists a trajectory Γx0 ⊂ D not solving an algebraic equation in the parameters... then the OODG problem given by (BL, D) is strictly learnable"
  - [corpus]: No direct evidence; relies on stated theorem.
- **Break condition**: If the true dynamics cannot be expressed in the chosen library (e.g., non-polynomial dynamics with polynomial library), or if the observed trajectory solves an algebraic equation in the parameters, the mechanism breaks.

### Mechanism 3
- **Claim**: The generalization error in DSR is fundamentally a problem of measuring and enforcing topological equivalence across basins, not just prediction accuracy.
- **Mechanism**: Standard losses like MSE fail to capture long-term statistical and topological properties of the system. By defining statistical error (via sliced Wasserstein distance of occupation measures) and topological error (via Lyapunov spectrum and limit set proximity), one can formally prove that failure to reconstruct one attractor implies proportional error in the basin volume. This shifts the problem from prediction to structure learning.
- **Core assumption**: Topological equivalence and agreement of occupation measures are necessary and sufficient conditions for successful generalization across basins.
- **Evidence anchors**:
  - [abstract]: "We introduce mathematical notions based on topological concepts and ergodic theory to formalize the idea of learnability"
  - [section 3.2]: "Theorem 3.3... the generalization error of ΦR is proportional to the volume of the basin of the not-reconstructed attractor"
  - [corpus]: No direct evidence; relies on stated theorem.
- **Break condition**: If the mathematical framework for measuring generalization is inadequate (e.g., occupation measures are insufficient to capture all relevant dynamics), or if practical computation of these measures is too noisy to be useful, the mechanism would break.

## Foundational Learning

- **Concept**: Ergodic theory and physical measures
  - Why needed here: To understand when and how long-term statistical properties of trajectories can be used to assess generalization across basins.
  - Quick check question: Can you explain the difference between a physical measure and an SRB measure, and why physical measures are relevant for data-driven DSR?

- **Concept**: Topological equivalence and Lyapunov exponents
  - Why needed here: To formalize what it means for two dynamical systems to have the same qualitative dynamics, which is essential for defining successful generalization.
  - Quick check question: What are the three conditions used in the paper to assess topological equivalence, and why are they weaker than requiring a full homeomorphism?

- **Concept**: Measure theory and occupation measures
  - Why needed here: To quantify how much time trajectories spend in different regions of state space, which is used to define the statistical error.
  - Quick check question: How does the sliced Wasserstein distance between occupation measures capture the difference between two dynamical systems?

## Architecture Onboarding

- **Component map**: Synthetic data generation -> Single-basin training -> Generalization evaluation -> Failure mode analysis
- **Critical path**: Generate synthetic data from one basin → Train DSR model on this data → Evaluate generalization across basins using statistical and topological errors → Analyze why generalization succeeded or failed
- **Design tradeoffs**: Expressive models (RNNs, N-ODEs) vs. inductive bias (SINDy); prediction accuracy vs. structural learning; computational cost of evaluation metrics vs. insight gained
- **Failure signatures**: Zero training error but high generalization error; perfect reconstruction of training basin but missing attractors in test basins; models that generalize when trained on full state space but fail when trained on single basin
- **First 3 experiments**:
  1. Train an RNN on data from one basin of the Duffing system and evaluate generalization to the other basin using the statistical error metric.
  2. Repeat experiment 1 with SINDy using a polynomial library and compare results.
  3. Train an RNN on data from both basins of the Duffing system and verify that generalization error drops to near zero.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can deep learning models be trained to successfully generalize across basins of attraction in multistable dynamical systems?
- Basis in paper: [explicit] The paper demonstrates that current deep learning methods like RNNs, N-ODEs, and RCs fail to generalize across basins of attraction when trained on data from only one basin, while methods like SINDy with strong prior knowledge can succeed.
- Why unresolved: The paper shows that current training routines unlearn the multistable property even upon perfect initialization, and that generalizing solutions are often saddle points that are difficult to find with standard optimization algorithms.
- What evidence would resolve it: Empirical demonstrations of deep learning models successfully generalizing across basins of attraction in multistable systems, with analysis of the training process and solution landscape to understand why generalization succeeds.

### Open Question 2
- Question: What are the precise mathematical conditions that allow for successful reconstruction of a dynamical system on the whole state space, beyond the simple cases where a single trajectory is sufficient?
- Basis in paper: [explicit] The paper states that while SINDy can generalize with a single trajectory if it does not solve an algebraic equation in the basis functions, for many complex real-world systems this assumption is likely violated, necessitating more flexible and expressive models.
- Why unresolved: The paper shows that for expressive models without explicit priors, unique identification of a generalizing solution is no longer possible, and the learnability distribution is biased towards monostability.
- What evidence would resolve it: Mathematical proofs or empirical demonstrations of conditions under which deep learning models can uniquely identify generalizing solutions for complex multistable systems, potentially involving new training algorithms or architectural priors.

### Open Question 3
- Question: How can implicit biases in deep learning models be targeted to promote multistability and enhance out-of-domain generalization?
- Basis in paper: [explicit] The paper shows that RNNs exhibit a simplicity bias towards monostable dynamics at initialization, and that generalizing solutions are often sharp minima that are avoided by SGD.
- Why unresolved: While the paper identifies these biases, it does not provide a concrete method to overcome them and promote multistability in the learned models.
- What evidence would resolve it: New initialization schemes, training algorithms, or architectural priors that explicitly encourage multistability in deep learning models, with empirical validation on multistable dynamical systems.

## Limitations

- The theoretical framework assumes perfect knowledge of basin boundaries and infinite sampling, which are unrealistic in real applications
- The computational complexity of the proposed error measures (particularly sliced Wasserstein distance and Lyapunov spectrum calculation) may limit practical utility for high-dimensional systems
- The paper's proofs rely on idealized assumptions about trajectory coverage and ergodicity that may not hold with finite, noisy real-world data

## Confidence

- High confidence: The theoretical distinction between in-distribution and out-of-distribution generalization in multistable systems is well-founded and mathematically rigorous
- Medium confidence: The empirical demonstration that black-box methods fail while SINDy succeeds is convincing but based on limited test cases
- Low confidence: The claim that current training methods are inherently biased toward monostability needs more systematic investigation across different architectures and initialization schemes

## Next Checks

1. Test the proposed framework on higher-dimensional multistable systems (5-10 dimensions) to evaluate scalability of both theoretical claims and computational methods
2. Investigate whether modified training procedures (e.g., curriculum learning starting from multiple basins) can overcome the monostability bias in universal approximators
3. Validate the topological error measures on real-world data from physical systems with known multistability (e.g., climate models, electronic circuits) to assess practical relevance