---
ver: rpa2
title: 'Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low
  Resource Languages'
arxiv_id: '2409.04512'
source_url: https://arxiv.org/abs/2409.04512
tags:
- language
- prompting
- marathi
- translation
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chain of Translation Prompting (CoTR), a
  novel strategy designed to enhance the performance of language models in low-resource
  languages. CoTR restructures prompts to first translate the input context from a
  low-resource language into a higher-resource language, such as English.
---

# Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages

## Quick Facts
- arXiv ID: 2409.04512
- Source URL: https://arxiv.org/abs/2409.04512
- Authors: Tejas Deshpande; Nidhi Kowtal; Raviraj Joshi
- Reference count: 15
- Primary result: CoTR strategy improves performance on Marathi NLP tasks by translating to high-resource languages first

## Executive Summary
This paper introduces Chain of Translation Prompting (CoTR), a novel strategy designed to enhance the performance of language models in low-resource languages. CoTR restructures prompts to first translate the input context from a low-resource language into a higher-resource language, such as English. The specified task like generation, classification, or any other NLP function is then performed on the translated text, with the option to translate the output back to the original language if needed. All these steps are specified in a single prompt. We demonstrate the effectiveness of this method through a case study on the low-resource Indic language Marathi. The CoTR strategy is applied to various tasks, including sentiment analysis, hate speech classification, subject classification and text generation, and its efficacy is showcased by comparing it with regular prompting methods.

## Method Summary
Chain of Translation Prompting (CoTR) is a novel technique that enhances language model performance for low-resource languages by incorporating translation steps directly into the prompting strategy. The method involves translating the input context from a low-resource language to a higher-resource language (typically English), performing the desired NLP task on the translated text, and optionally translating the output back to the original language. This entire process is specified within a single, comprehensive prompt. The approach was tested on Marathi, an Indic language with limited digital resources, across multiple tasks including sentiment analysis, hate speech classification, subject classification, and text generation. CoTR was compared against standard prompting methods to demonstrate its effectiveness in improving task performance.

## Key Results
- CoTR shows significant performance improvements on Marathi NLP tasks compared to regular prompting methods
- Highest accuracy improvements observed in hate speech detection tasks
- Demonstrates potential for enhancing synthetic data generation quality for underrepresented languages using LLMs

## Why This Works (Mechanism)
CoTR leverages the superior performance of large language models on high-resource languages by creating an intermediate translation step. When an LLM is prompted to perform a task directly in a low-resource language, it often lacks sufficient training data to produce accurate results. By translating the input to a high-resource language first, the model can leverage its extensive training on that language to perform the task more effectively. The translation acts as a feature extraction mechanism that transforms low-resource content into a representation the model can process with higher confidence. This approach effectively bypasses the data scarcity problem by using translation as a bridge to the model's existing capabilities.

## Foundational Learning
- **Translation as feature engineering**: Why needed - Converts low-resource content into a high-resource representation; Quick check - Verify translation accuracy before task execution
- **Prompt engineering for multi-step tasks**: Why needed - Ensures the model follows the translation-then-task sequence correctly; Quick check - Test with simple translation-only prompts first
- **Zero-shot learning in high-resource languages**: Why needed - The technique assumes the model can perform tasks in high-resource languages without fine-tuning; Quick check - Validate task performance on high-resource language examples
- **Cross-linguistic transfer learning**: Why needed - The method relies on the model's ability to transfer task knowledge from high-resource to low-resource contexts; Quick check - Compare performance on direct vs. translated prompts
- **Evaluation metrics for low-resource languages**: Why needed - Standard metrics may not capture nuances specific to low-resource language processing; Quick check - Use both automated metrics and human evaluation

## Architecture Onboarding

**Component Map**: Input (Low-resource language) -> Translation Engine -> LLM (High-resource language) -> Task Execution -> Optional Translation (Output)

**Critical Path**: The most time-consuming step is typically the initial translation, as it must complete before the LLM can process the task. This creates a sequential dependency where task performance is bottlenecked by translation quality and speed.

**Design Tradeoffs**: The primary tradeoff is between accuracy and latency. Adding translation steps improves accuracy for low-resource languages but introduces additional processing time and potential error propagation from translation inaccuracies. The optional output translation adds flexibility but increases complexity and computational cost.

**Failure Signatures**: Performance degradation occurs when translation quality is poor, particularly for nuanced tasks like hate speech detection where subtle linguistic cues may be lost. The method also fails when cultural context cannot be adequately preserved through translation, leading to task misinterpretation.

**First Experiments**: 
1. Test CoTR on a simple classification task with manually verified translations to establish baseline improvement
2. Compare performance across different translation services (Google Translate, DeepL, etc.) to assess translation quality impact
3. Evaluate the effect of translating output back to the original language versus keeping it in the high-resource language

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on translation service quality, which may introduce errors or biases when converting low-resource language content to high-resource languages
- Computational overhead and latency introduced by requiring at least one translation step per task execution
- Assumption that high-resource language models can effectively understand and process translated content for the original task may not hold for culturally specific or linguistically nuanced content

## Confidence
- **High confidence**: The observed performance improvements on Marathi tasks are well-documented and reproducible
- **Medium confidence**: The generalizability of CoTR to other low-resource languages beyond Marathi
- **Medium confidence**: The assertion that translation-based prompting will significantly improve multilingual LLM performance across all low-resource languages

## Next Checks
1. Test CoTR across a diverse set of low-resource languages with different linguistic families (e.g., Swahili, Bengali, Tamil) to assess cross-linguistic generalizability.

2. Conduct ablation studies comparing CoTR performance when using different translation quality levels (Google Translate vs. professional translation) to quantify the impact of translation accuracy on final task performance.

3. Measure the computational overhead introduced by CoTR compared to direct prompting methods, including latency and token usage, to determine practical deployment feasibility.