---
ver: rpa2
title: How to set AdamW's weight decay as you scale model and dataset size
arxiv_id: '2405.13698'
source_url: https://arxiv.org/abs/2405.13698
tags:
- epoch
- learning
- iter
- weight
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper reveals that AdamW's weight updates can be understood\
  \ as an exponential moving average (EMA) of recent gradients, with the EMA timescale\
  \ \u03C4epoch being the critical hyperparameter. The authors propose that \u03C4\
  epoch should remain roughly constant across model and dataset sizes\u2014neither\
  \ much smaller than 1 epoch (to average over all data points) nor much larger than\
  \ the total training epochs (to allow forgetting early updates)."
---

# How to set AdamW's weight decay as you scale model and dataset size

## Quick Facts
- arXiv ID: 2405.13698
- Source URL: https://arxiv.org/abs/2405.13698
- Authors: Xi Wang; Laurence Aitchison
- Reference count: 40
- This paper reveals that AdamW's weight updates can be understood as an exponential moving average (EMA) of recent gradients, with the EMA timescale τepoch being the critical hyperparameter. The authors propose that τepoch should remain roughly constant across model and dataset sizes—neither much smaller than 1 epoch (to average over all data points) nor much larger than the total training epochs (to allow forgetting early updates). This insight implies that as dataset size increases, the optimal weight decay λ should decrease (since λ = 1/(ηMτepoch)), and as model size increases (following µP's learning rate scaling), λ should increase. Experiments on ResNet-18, Vision Transformers, and NanoGPT confirm these scaling rules. The authors also show that incorrect weight decay scaling breaks µP's learning rate transferability, which is restored when weight decay is scaled appropriately. These findings provide practical guidance for transferring AdamW hyperparameters across model and dataset scales.

## Executive Summary
This paper provides a theoretical and empirical framework for setting AdamW's weight decay hyperparameter when scaling models and datasets. The key insight is that AdamW weight updates function as an exponential moving average (EMA) of recent parameter updates, with the EMA timescale (measured in epochs) being the critical hyperparameter to optimize. The authors show that this timescale should remain roughly constant across scales, which implies that weight decay should decrease with dataset size and increase with model size. This finding resolves a critical gap in the μP (model parallel) framework, which previously didn't account for weight decay effects. The paper provides practical guidance for transferring AdamW hyperparameters across scales and validates these findings across multiple architectures and datasets.

## Method Summary
The authors analyze AdamW through the lens of exponential moving averages, showing that weight updates are equivalent to EMA with timescale τiter = 1/(ηλ). They propose that the optimal EMA timescale measured in epochs (τepoch = τiter × M/η = 1/(η²λM)) should remain constant across model and dataset sizes. To validate this, they conduct experiments sweeping τepoch for fixed learning rates on ResNet-18 and Vision Transformers across CIFAR-10 and ImageNet datasets. They also test the scaling rules by varying dataset size and model width, examining how optimal weight decay changes. The experiments use AdamW with cosine learning rate decay and evaluate performance using train/test accuracy and loss.

## Key Results
- AdamW weight updates are mathematically equivalent to an exponential moving average (EMA) of recent parameter updates
- The optimal EMA timescale τepoch (measured in epochs) should remain roughly constant as model and dataset size scale
- As dataset size increases, optimal weight decay λ should decrease proportionally
- As model size increases (following μP's learning rate scaling), optimal weight decay λ should increase proportionally
- μP's learning rate transferability breaks down for AdamW unless weight decay is scaled appropriately

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** AdamW weight updates can be understood as an exponential moving average (EMA) of recent parameter updates.
- **Mechanism:** The weight update equation in AdamW, `wt = (1 - ηλ)wt-1 - η ˆmt√ˆvt + ϵ`, is mathematically equivalent to an EMA with timescale τiter = 1/(ηλ). This reformulation shows that AdamW maintains an exponentially weighted history of past updates.
- **Core assumption:** The network architecture is scale-invariant, meaning that scaling the weights by a constant doesn't change the output for any input.
- **Evidence anchors:**
  - [abstract] "weights learned by AdamW can be understood as an exponential moving average (EMA) of recent updates"
  - [section 3.1] Derivation showing AdamW updates are equivalent to EMA with timescale τiter = 1/(ηλ)
- **Break condition:** Non-scale-invariant architectures or when weight decay is applied to normalization layer parameters.

### Mechanism 2
- **Claim:** The optimal EMA timescale τepoch (measured in epochs) should remain roughly constant as model and dataset size scale.
- **Mechanism:** If τepoch is optimal at one scale, keeping it constant across scales means the weight decay λ should decrease with dataset size (since λ = 1/(ηMτepoch)) and increase with model size (when following μP's learning rate scaling).
- **Core assumption:** The optimal EMA timescale falls within a natural range: not much smaller than 1 epoch (to average over all data points) and not much larger than total training epochs (to allow forgetting early updates).
- **Evidence anchors:**
  - [abstract] "optimal timescale, measured in epochs, is roughly constant as we change model and dataset size"
  - [section 4.1] Experimental results showing optimal τepoch falls between 1 and 200 epochs for ResNet and ViT on CIFAR-10
- **Break condition:** When the natural range assumption is violated or when training dynamics fundamentally change (e.g., very short training runs).

### Mechanism 3
- **Claim:** Proper scaling of weight decay is essential for μP's learning rate transferability to work with AdamW.
- **Mechanism:** If weight decay λ is kept constant while learning rate η scales as 1/fan_in (following μP), the EMA timescale τiter = 1/(ηλ) changes with model size, breaking the stability of optimal learning rates. Scaling λ proportionally to model size restores τiter stability.
- **Core assumption:** The original μP theory considers only the first few optimization steps relative to random initialization, not the effects of weight decay on later training phases.
- **Evidence anchors:**
  - [abstract] "muP's learning rate scaling breaks down for AdamW unless weight decay is scaled appropriately"
  - [section 4.3] Experimental validation showing that μP scaling of learning rate breaks down without proper weight decay scaling, and is restored when λ scales with model size
- **Break condition:** When the relationship between learning rate and weight decay scaling deviates from the proposed 1/fan_in and proportional scaling.

## Foundational Learning

- **Concept: Exponential Moving Average (EMA)**
  - Why needed here: Understanding how AdamW maintains a history of updates through EMA is central to the paper's theoretical framework.
  - Quick check question: What is the formula for an EMA and how does it relate to the AdamW weight update equation?

- **Concept: Scale-invariant architectures**
  - Why needed here: The paper's theoretical analysis of AdamW as EMA assumes scale-invariance, and practical implementations require modifications to achieve this property.
  - Quick check question: Why do standard neural networks violate scale-invariance and what modifications are needed to make them scale-invariant?

- **Concept: Hyperparameter transfer across scales**
  - Why needed here: The paper addresses how to transfer optimal hyperparameters from smaller to larger models and datasets, which is a fundamental challenge in scaling deep learning systems.
  - Quick check question: What are the key considerations when transferring hyperparameters across different model and dataset sizes?

## Architecture Onboarding

- **Component map:** AdamW optimizer with decoupled weight decay -> Scale-invariant network modifications (output batchnorm, decoupled learning rates for normalization parameters) -> Hyperparameter search framework for τepoch and λ -> Evaluation metrics (train/test accuracy and loss)

- **Critical path:**
  1. Implement AdamW with proper weight decay parameterization
  2. Ensure network architecture is scale-invariant through modifications
  3. Search for optimal τepoch across different scales
  4. Validate that τepoch remains stable while λ scales appropriately
  5. Test learning rate transferability with properly scaled weight decay

- **Design tradeoffs:**
  - Scale-invariance vs. standard architectures: Adding normalization layers increases computational overhead but is necessary for theoretical analysis
  - Fixed τepoch vs. dataset-specific tuning: Keeping τepoch constant simplifies transfer but may sacrifice some performance
  - Weight decay scaling: Stronger scaling with model size may improve transfer but could over-regularize small models

- **Failure signatures:**
  - Learning rate transferability breaks when weight decay is not scaled appropriately
  - Performance varies significantly with learning rate under fixed τepoch (indicates lack of scale-invariance)
  - Optimal τepoch changes dramatically across scales (violates the constant timescale assumption)

- **First 3 experiments:**
  1. Train ResNet-18 on CIFAR-10 with AdamW, sweep τepoch for fixed learning rate, verify optimal τepoch falls in expected range (1-200 epochs)
  2. Scale dataset size while keeping model size fixed, verify optimal τepoch remains stable while optimal λ decreases
  3. Scale model size while keeping dataset size fixed, verify optimal τepoch remains stable while optimal λ increases proportionally to model size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed scaling of weight decay with model size (λ ∝ fan_in) generalize to other architectures beyond ResNets and Vision Transformers?
- Basis in paper: [explicit] The paper only validates the scaling rule on ResNet-18 and Vision Transformers.
- Why unresolved: Different architectures may have varying sensitivities to weight decay due to differences in layer interactions and optimization dynamics.
- What evidence would resolve it: Systematic experiments across diverse architectures (e.g., MLPs, LSTMs, Graph Neural Networks) showing consistent weight decay scaling with fan_in.

### Open Question 2
- Question: How does the EMA timescale τepoch interact with other regularization techniques like dropout or data augmentation?
- Basis in paper: [inferred] The paper focuses solely on AdamW's weight decay and does not explore interactions with other regularization methods.
- Why unresolved: Combining multiple regularization techniques could lead to complex interactions that affect the optimal EMA timescale and weight decay settings.
- What evidence would resolve it: Experiments varying τepoch and λ while systematically adding/removing dropout or data augmentation to identify optimal combinations.

### Open Question 3
- Question: Is the proposed weight decay scaling rule applicable to optimizers beyond AdamW that use decoupled weight decay?
- Basis in paper: [explicit] The paper mentions other optimizers like Lion and Sophia but does not test the scaling rule on them.
- Why unresolved: Different optimizers may have distinct update dynamics that could affect how weight decay should scale with model and dataset size.
- What evidence would resolve it: Empirical validation of the scaling rule on a range of optimizers using decoupled weight decay, comparing optimal weight decay values across model and dataset scales.

## Limitations

- The EMA interpretation relies on scale-invariance, which standard architectures violate and require normalization layer modifications to achieve
- The assumption that optimal τepoch remains constant across scales needs validation beyond the tested model and dataset families
- The relationship between weight decay and learning rate scaling may break down for very large models or different training regimes

## Confidence

*High confidence:* The mathematical derivation showing AdamW updates are equivalent to EMA, and the experimental validation that optimal τepoch remains roughly constant within tested ranges.

*Medium confidence:* The proposed scaling rules for weight decay (decreasing with dataset size, increasing with model size) are well-supported by experiments but may require refinement for extreme scales or different architectures.

*Low confidence:* The broader applicability of these findings to architectures beyond those tested (ResNet, ViT, NanoGPT) and to training scenarios with significantly different hyperparameters (batch size, learning rate schedules, etc.).

## Next Checks

1. **Cross-architecture validation**: Test the weight decay scaling rules on architectures not covered in the paper (e.g., MLP-Mixer, ConvNext, Swin Transformer) to verify the generalizability of the findings.

2. **Extreme scale testing**: Validate the scaling rules on much larger models (10B+ parameters) and datasets (billion-scale) to check if the linear relationships hold or if new scaling regimes emerge.

3. **Hyperparameter interaction study**: Systematically explore how batch size, learning rate schedule (beyond cosine decay), and gradient accumulation affect the optimal weight decay scaling, particularly whether the relationship λ ∝ η needs modification for different training configurations.