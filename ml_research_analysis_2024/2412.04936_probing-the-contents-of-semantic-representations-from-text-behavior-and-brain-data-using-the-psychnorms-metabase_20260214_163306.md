---
ver: rpa2
title: Probing the contents of semantic representations from text, behavior, and brain
  data using the psychNorms metabase
arxiv_id: '2412.04936'
source_url: https://arxiv.org/abs/2412.04936
tags:
- text
- representations
- behavior
- brain
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically compares semantic representations derived
  from text, behavior, and brain data, revealing that behavior representations capture
  psychological information rivaling that of text and contain unique variance on affective,
  agentic, and socio-moral dimensions. Using representational similarity analysis
  and representational content analysis, the authors show that despite being trained
  on orders of magnitude less data, behavior representations perform comparably to
  text on many dimensions and even outperform text on subjective categories like affect
  and agency.
---

# Probing the contents of semantic representations from text, behavior, and brain data using the psychNorms metabase

## Quick Facts
- arXiv ID: 2412.04936
- Source URL: https://arxiv.org/abs/2412.04936
- Reference count: 24
- Text, behavior, and brain representations capture overlapping but distinct semantic information

## Executive Summary
This study systematically compares semantic representations derived from text, behavior, and brain data to understand how different data sources capture psychological information. Using representational similarity analysis and representational content analysis with the psychNorms metabase, the authors find that behavior representations capture psychological information rivaling that of text and contain unique variance on affective, agentic, and socio-moral dimensions. Despite being trained on orders of magnitude less data, behavior representations perform comparably to text on many dimensions and even outperform text on subjective categories like affect and agency. An ensemble analysis demonstrates that combining text and behavior representations yields superior performance compared to text alone on 11 out of 27 norm categories.

## Method Summary
The study employs representational similarity analysis (RSA) and representational content analysis (RCA) to compare semantic representations from three sources: text (BERT and GPT-2), behavior (BeRT trained on Natural Stories corpus), and brain data (fMRI during visual word recognition). Representations are probed using the psychNorms metabase, which contains 27 norm categories spanning affective, visual, encyclopedic, functional, and subjective dimensions. The authors evaluate individual representations, compare them using Pearson correlations on representational dissimilarity matrices, and conduct an ensemble analysis combining text and behavior representations. The behavioral dataset consists of eye-tracking data from 10 participants reading the Natural Stories corpus, while the brain dataset includes fMRI data from 16 participants viewing words in isolation.

## Key Results
- Behavior representations capture psychological information comparable to text despite being trained on orders of magnitude less data
- Behavior representations outperform text on subjective categories including affect, agency, and socio-moral dimensions
- Ensemble models combining text and behavior outperform text alone on 11 out of 27 norm categories

## Why This Works (Mechanism)
Behavior representations capture unique psychological information because they reflect real-time processing constraints and cognitive states during language comprehension. Unlike text representations that learn from static text corpora, behavior representations incorporate the temporal dynamics of reading, including saccades, regressions, and fixation durations that reflect uncertainty, difficulty, and comprehension processes. These behavioral signals provide complementary information about semantic processing that text-only models cannot access, particularly for subjective dimensions like affect and agency where individual differences and processing strategies matter.

## Foundational Learning
- **Representational Similarity Analysis (RSA)**: Measures representational structure by comparing pairwise dissimilarities between stimuli; needed to quantify how different models organize semantic information in similar or different ways; quick check: verify correlation structure between RDM matrices is meaningful
- **PsychNorms Metabase**: Comprehensive database of semantic norms across 27 categories; needed to systematically probe what information different representations contain; quick check: confirm all 27 categories are properly integrated and validated
- **Behavioral representations (BeRT)**: Language model trained on behavioral data rather than text; needed to capture processing dynamics and cognitive states during comprehension; quick check: verify behavioral features are properly aligned with word representations
- **Ensemble modeling**: Combines multiple representation sources to leverage complementary information; needed to test whether hybrid approaches outperform individual sources; quick check: ensure proper weighting and combination methodology
- **Representational Content Analysis (RCA)**: Measures how well representations predict human semantic norms; needed to quantify what specific types of information are captured; quick check: validate prediction accuracy against held-out norms

## Architecture Onboarding
Component map: Text models (BERT, GPT-2) -> Behavior model (BeRT) -> Brain data (fMRI) -> PsychNorms metabase -> RSA/RCA analysis

Critical path: Text/behavior representations → Representational dissimilarity matrices → Correlation with psychNorms → Ensemble combination → Performance evaluation

Design tradeoffs: The study uses relatively small behavioral and brain datasets compared to massive text corpora, which may limit generalizability but provides unique insights into cognitive processing. The choice of RSA vs RCA provides complementary perspectives but requires careful interpretation of what each method actually measures.

Failure signatures: Poor performance on objective categories (visual, functional) suggests limitations in capturing concrete semantic information. Low correlations between text and behavior representations indicate fundamental differences in what each captures. Limited behavioral dataset may not generalize beyond narrative reading contexts.

First experiments:
1. Compare RSA correlations between text and behavior representations on individual norm categories to identify which dimensions show the strongest divergence
2. Conduct ablation studies removing individual norm categories to isolate which psychological dimensions drive behavioral representation advantages
3. Test ensemble model performance with different weighting schemes to optimize complementary information capture

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the findings raise several important directions for future research regarding the generalizability of behavior representations across different behavioral datasets and domains, the mechanisms underlying behavioral advantages on subjective categories, and the potential applications for cognitive modeling and human-LLM alignment.

## Limitations
- Behavioral dataset limited to 10 participants reading a single narrative domain, raising questions about generalizability
- Text and behavior representations derived from different model architectures and training paradigms
- Brain data analysis focuses on visual word recognition rather than naturalistic language processing
- psychNorms metabase represents specific theoretical perspectives that may not capture all relevant psychological dimensions

## Confidence
- **High confidence**: Text and behavior representations perform comparably on subjective categories (affect, agency, socio-moral dimensions)
- **Medium confidence**: Ensemble models combining text and behavior outperform text alone; behavior representations contain unique variance not captured by text
- **Low confidence**: Claims about behavior representations capturing "human-like" representations more broadly, given limited behavioral dataset and single domain

## Next Checks
1. Replicate findings using larger, more diverse behavioral datasets (e.g., multiple narrative domains, conversational speech, or multimodal behavior data) to test generalizability
2. Conduct ablation studies removing individual norm categories to identify which psychological dimensions drive the observed representational differences
3. Test whether the observed text-behavior-brain representational patterns generalize to downstream tasks like sentiment analysis, mental state inference, or cognitive modeling benchmarks