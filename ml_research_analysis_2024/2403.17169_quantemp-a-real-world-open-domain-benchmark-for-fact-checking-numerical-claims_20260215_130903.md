---
ver: rpa2
title: 'QuanTemp: A real-world open-domain benchmark for fact-checking numerical claims'
arxiv_id: '2403.17169'
source_url: https://arxiv.org/abs/2403.17169
tags: []
core_contribution: This paper introduces NUMTEMP, the first large-scale real-world
  dataset focused on numerical claim verification, containing 15,514 real-world claims
  with fine-grained metadata and evidence from diverse fact-checking domains. The
  authors evaluate various claim decomposition methods, including CLAIMDECOMP and
  PROGRAM-FC, and compare their performance against original claim verification.
---

# QuanTemp: A real-world open-domain benchmark for fact-checking numerical claims

## Quick Facts
- arXiv ID: 2403.17169
- Source URL: https://arxiv.org/abs/2403.17169
- Reference count: 25
- Introduces NUMTEMP, the first large-scale real-world dataset focused on numerical claim verification

## Executive Summary
NUMTEMP presents a comprehensive benchmark for open-domain numerical fact-checking with 15,514 real-world claims and fine-grained metadata. The dataset addresses the critical gap in existing fact-checking benchmarks by focusing specifically on numerical claims across diverse domains. The authors demonstrate that claim decomposition techniques significantly improve verification accuracy, especially for conflicting claims, while models pre-trained on numerical understanding tasks outperform general NLI models by up to 11.78% in macro-F1.

## Method Summary
The authors developed NUMTEMP by collecting 15,514 real-world numerical claims and implementing a claim decomposition approach (CLAIMDECOMP) that breaks complex claims into simpler sub-questions. They used BM25 for initial evidence retrieval from a unified corpus of 423,320 web snippets, followed by re-ranking with paraphrase-MiniLM-L6-v2. The system employs NLI models, including those pre-trained on numerical understanding tasks like FINQA-ROBERTA-LARGE and NUMT5-SMALL, to verify claims against retrieved evidence. The approach supports both fine-tuned models and few-shot/zero-shot settings with prompting-based generative models.

## Key Results
- Claim decomposition improves macro-F1 by 8.84% and weighted-F1 by 10.9%, especially for conflicting claims
- Models pre-trained on numerical understanding outperform general NLI models by up to 11.78% in macro-F1
- Smaller fine-tuned models (355M or 60M parameters) outperform larger models like GPT-3.5-TURBO in few-shot and zero-shot scenarios
- Best baseline achieves a macro-F1 of 58.32, demonstrating the challenging nature of numerical claim verification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Claim decomposition significantly improves verification, especially for conflicting claims
- Mechanism: Breaking complex numerical claims into simpler sub-questions allows retrieval of more targeted evidence that addresses individual claim aspects
- Core assumption: Complex claims contain multiple verifiable components that can be isolated and verified independently
- Evidence anchors:
  - [abstract] "Results show that claim decomposition significantly improves verification, especially for conflicting claims"
  - [section] "CLAIMDECOMP sees gains of 8.84% in macro-F1 and 10.9% in weighted-F1"
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.443" - indicates related work on claim decomposition exists
- Break condition: If decomposed questions fail to capture implicit relationships between claim components, verification may miss contradictions

### Mechanism 2
- Claim: Models pre-trained on numerical understanding outperform general NLI models by up to 11.78% in macro-F1
- Mechanism: Numerical reasoning capability from pre-training transfers to better handling of quantitative evidence during fact-checking
- Core assumption: Numerical reasoning skills learned during pre-training are generalizable to fact-checking tasks
- Evidence anchors:
  - [abstract] "models pre-trained on numerical understanding outperform general NLI models by up to 11.78% in macro-F1"
  - [section] "FINQA-ROBERTA-LARGE, a number-focused Roberta-Large model, exceeds the standard Roberta-Large model by the same margin"
  - [corpus] "A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition" - suggests numerical specialization is valuable
- Break condition: If numerical pre-training focuses on specific domains (e.g., finance) that don't overlap with fact-checking scenarios

### Mechanism 3
- Claim: Larger models improve performance when fine-tuned but not necessarily in few-shot or zero-shot settings
- Mechanism: Fine-tuning provides sufficient adaptation for smaller models, while larger models' capacity is better utilized when trained on sufficient data
- Core assumption: Few-shot and zero-shot performance depends more on model's ability to generalize from prompts than on parameter count
- Evidence anchors:
  - [abstract] "smaller models fine-tuned on numerical claims outperform larger models like GPT-3.5-TURBO under zero-shot and few-shot scenarios"
  - [section] "GPT-3.5-TURBO underperforms in few-shot and zero-shot scenarios compared to smaller fine-tuned models (355M or 60M parameters)"
  - [corpus] "FactIR: A Real-World Zero-shot Open-Domain Retrieval Benchmark for Fact-Checking" - suggests retrieval and zero-shot methods are related challenges
- Break condition: If prompts are carefully engineered to provide sufficient context, larger models might overcome few-shot limitations

## Foundational Learning

- Concept: Natural Language Inference (NLI) for fact-checking
  - Why needed here: NLI models determine whether evidence supports, refutes, or conflicts with claims, which is the core verification task
  - Quick check question: How does an NLI model handle a claim where evidence partially supports some aspects but contradicts others?

- Concept: Claim decomposition into sub-questions
  - Why needed here: Complex numerical claims often contain multiple verifiable components that are better handled separately
  - Quick check question: What criteria should be used to determine when a claim should be decomposed versus verified as a whole?

- Concept: Evidence retrieval with BM25 and re-ranking
  - Why needed here: Finding relevant evidence snippets is crucial for accurate verification, especially for numerical claims requiring specific data points
  - Quick check question: How does the choice of re-ranking model affect the quality of evidence retrieved for numerical claims?

## Architecture Onboarding

- Component map:
  - Claim collection → Evidence retrieval (BM25 + re-rank) → Claim decomposition → NLI model → Verdict prediction
  - Evidence corpus: 423,320 snippets from web search
  - Label space: True, False, Conflicting

- Critical path:
  1. Retrieve top-100 documents using BM25 for each question/claim
  2. Re-rank with paraphrase-MiniLM-L6-v2, select top 3 snippets
  3. Concatenate claim, questions, and evidence for NLI input
  4. Fine-tune classifier on veracity labels

- Design tradeoffs:
  - Using original claims vs. decomposed questions for retrieval
  - Unified evidence corpus vs. gold evidence from fact-checkers
  - Model scale vs. training data availability for different NLI approaches

- Failure signatures:
  - Poor performance on conflicting claims suggests decomposition isn't capturing all aspects
  - Underperformance of numerical models indicates domain mismatch in pre-training
  - GPT-3.5-Turbo hallucinations suggest prompt engineering issues in few-shot scenarios

- First 3 experiments:
  1. Compare unified evidence vs. gold evidence performance to establish upper bound
  2. Test claim decomposition vs. original claim retrieval for each claim category
  3. Evaluate numerical understanding models (NUMT5, FINQA-Roberta) against general NLI models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of evidence retrieval impact the performance of fact-checking numerical claims?
- Basis in paper: [explicit] The paper mentions that using better snippet selection methods could improve results and that the retrieval stage is not necessarily the most optimal setup for fact-checking.
- Why unresolved: The paper only uses BM25 first-stage retrieval and BERT-based re-ranking for evidence retrieval, without exploring other methods or their impact on performance.
- What evidence would resolve it: Conducting experiments with different evidence retrieval methods and comparing their impact on the performance of fact-checking numerical claims.

### Open Question 2
- Question: How can numerical reasoning be integrated into the NLI step to improve fact-checking of numerical claims?
- Basis in paper: [explicit] The paper suggests that using the outcome of numerical reasoning from evidence provided by answer snippets of multiple questions for predicting the veracity could be a more effective way of fact-checking numerical claims.
- Why unresolved: The paper only fine-tunes and uses models pre-trained on numerical understanding tasks but does not directly perform numerical reasoning.
- What evidence would resolve it: Developing and evaluating models that can perform numerical reasoning on evidence snippets to improve the accuracy of fact-checking numerical claims.

### Open Question 3
- Question: How can the trustworthiness and credibility of evidence sources be incorporated into the fact-checking process?
- Basis in paper: [explicit] The paper mentions that the credibility and trustworthiness of evidence sources is a critical challenge in fact-checking, but the proposed approach only uses the textual content of evidence and omits the credibility of the source.
- Why unresolved: The paper does not explore methods to incorporate the trustworthiness and credibility of evidence sources into the fact-checking process.
- What evidence would resolve it: Developing and evaluating methods that can assess the trustworthiness and credibility of evidence sources and incorporate this information into the fact-checking process to improve accuracy.

## Limitations

- The dataset relies on automatically generated sub-questions from CLAIMDECOMP, which may miss nuanced relationships between claim components
- The unified evidence corpus from web search may not capture all relevant numerical evidence, particularly for domain-specific claims
- The study doesn't compare against gold evidence from fact-checkers, limiting understanding of retrieval quality impact

## Confidence

- High confidence: claim decomposition improvements (8.84% macro-F1 gain) and numerical model advantages (up to 11.78% macro-F1)
- Medium confidence: unified evidence corpus approach, as the study doesn't compare against gold evidence from fact-checkers
- Low confidence: few-shot capabilities of larger models, given that GPT-3.5-TURBO's poor performance may reflect prompt engineering rather than fundamental limitations

## Next Checks

1. Conduct ablation studies comparing unified evidence retrieval against gold evidence provided by fact-checkers to establish performance upper bounds and identify retrieval gaps

2. Perform human evaluation of automatically generated sub-questions to assess whether CLAIMDECOMP captures all meaningful claim components and identify systematic failures

3. Test additional numerical pre-training approaches beyond FINQA-ROBERTA-LARGE and NUMT5, including models trained on diverse numerical domains like scientific measurements and statistical data