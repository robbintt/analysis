---
ver: rpa2
title: On the Expressive Power of Graph Neural Networks
arxiv_id: '2401.01626'
source_url: https://arxiv.org/abs/2401.01626
tags:
- graph
- gnns
- networks
- node
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive overview of the expressive
  power of Graph Neural Networks (GNNs) from multiple perspectives. It covers the
  Universal Approximation Theorem (UAT), showing that GNNs can approximate a wide
  range of functions on graphs, including measurable functions, Boolean functions,
  and even Turing machines.
---

# On the Expressive Power of Graph Neural Networks

## Quick Facts
- arXiv ID: 2401.01626
- Source URL: https://arxiv.org/abs/2401.01626
- Reference count: 4
- Primary result: Comprehensive overview of GNN expressive power from multiple theoretical perspectives

## Executive Summary
This paper provides a thorough theoretical examination of Graph Neural Networks' expressive power, analyzing their capabilities from multiple angles. The authors explore how GNNs can approximate various function classes, distinguish graph structures, compute graph properties, and solve combinatorial problems. The work bridges theoretical computer science with practical GNN design considerations, offering insights into both capabilities and limitations of these architectures.

## Method Summary
The paper employs a theoretical analysis approach, examining GNNs through the lens of Universal Approximation Theorems, graph isomorphism testing, spectral graph theory, and combinatorial optimization. The authors synthesize results from multiple theoretical domains to construct a comprehensive picture of GNN expressiveness, analyzing both what GNNs can theoretically achieve and the practical constraints that may limit these capabilities.

## Key Results
- GNNs can approximate measurable functions, Boolean functions, and even Turing machines
- GNNs' ability to distinguish graph structures relates directly to the Weisfeiler-Lehman test
- GNNs act as low-pass filters in the spectral domain, affecting their expressiveness
- GNNs can approximate solutions to combinatorial problems like maximum independent set

## Why This Works (Mechanism)
The theoretical framework demonstrates that GNNs' expressive power stems from their ability to aggregate and transform information across graph structures through multiple message passing iterations. This process enables them to capture increasingly complex patterns and relationships within graphs, though their ultimate capabilities are bounded by fundamental computational and representational limits.

## Foundational Learning

1. **Universal Approximation Theorem (UAT)**
   - Why needed: Establishes the theoretical foundation for GNN's ability to approximate functions on graphs
   - Quick check: Verify that the GNN architecture can theoretically represent any measurable function given sufficient capacity

2. **Weisfeiler-Lehman (WL) Test**
   - Why needed: Provides a benchmark for GNN's ability to distinguish graph structures
   - Quick check: Compare GNN's graph distinguishing power against WL test capabilities

3. **Spectral Graph Theory**
   - Why needed: Explains how GNNs process information in the frequency domain
   - Quick check: Analyze the filter properties of GNN layers in the spectral domain

## Architecture Onboarding

**Component Map:** Input Features -> Message Passing Layers -> Aggregation -> Output Layer

**Critical Path:** The sequence of message passing and aggregation operations that propagate information across the graph structure

**Design Tradeoffs:**
- Depth vs. expressiveness: Deeper networks can capture more complex patterns but may suffer from oversmoothing
- Aggregation function choice affects the network's ability to distinguish graph structures
- Spectral properties of filters impact the types of patterns that can be learned

**Failure Signatures:**
- Inability to distinguish certain graph structures (related to WL test limitations)
- Oversmoothing in deep networks
- Poor performance on graphs with specific spectral properties

**First Experiments:**
1. Test GNN's ability to distinguish simple graph structures that the WL test can differentiate
2. Evaluate the impact of aggregation function choice on expressiveness
3. Measure the effect of network depth on distinguishing power

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Theoretical results may not translate directly to practical performance due to computational constraints
- Strong assumptions about graph signal smoothness in spectral analysis
- Practical feasibility of using GNNs for approximating Turing machines remains questionable
- Lack of empirical validation for some theoretical claims

## Confidence

**High:** Graph Isomorphism Test relationship, WL test connection
**Medium:** Universal Approximation Theorem claims, Spectral analysis claims
**Low:** Turing machine approximation claims, Combinatorial problem solving claims

## Next Checks

1. Conduct empirical studies to validate the practical implications of the UAT for GNNs on real-world graph datasets
2. Implement experiments to test the relationship between GNN expressiveness and the Weisfeiler-Lehman test on diverse graph structures
3. Perform case studies to evaluate the practical feasibility of using GNNs for approximating solutions to combinatorial problems like maximum independent set and graph coloring