---
ver: rpa2
title: 'Power in Numbers: Robust reading comprehension by finetuning with four adversarial
  sentences per example'
arxiv_id: '2401.10091'
source_url: https://arxiv.org/abs/2401.10091
tags: []
core_contribution: This paper replicates and extends Jia and Liang's 2017 adversarial
  attack study on SQuAD v1.1 using the ELECTRA-Small model. The study demonstrates
  that appending a single adversarial sentence to the context paragraph causes ELECTRA-Small's
  F1 score to drop from 83.9% to 29.2%.
---

# Power in Numbers: Robust reading comprehension by finetuning with four adversarial sentences per example

## Quick Facts
- arXiv ID: 2401.10091
- Source URL: https://arxiv.org/abs/2401.10091
- Reference count: 0
- ELECTRA-Small's F1 score drops from 83.9% to 29.2% when one adversarial sentence is appended to the context

## Executive Summary
This paper replicates and extends Jia and Liang's 2017 adversarial attack study on SQuAD v1.1 using the ELECTRA-Small model. The study demonstrates that appending a single adversarial sentence to the context paragraph causes ELECTRA-Small's F1 score to drop from 83.9% to 29.2%. To improve robustness, the model is fine-tuned on training examples with 1 to 5 adversarial sentences appended. Fine-tuning on datasets with 4 or 5 adversarial sentences yields F1 scores above 70% on most evaluation datasets containing multiple adversarial sentences, indicating that sufficient adversarial examples during training enhance model resistance to such attacks. However, performance remains low on datasets with prepended adversarial sentences, suggesting the need for further research to address this specific case.

## Method Summary
The study fine-tunes the ELECTRA-Small model on SQuAD v1.1 training data augmented with 0 to 5 adversarial sentences per example. Adversarial sentences are generated using GPT-3.5-Turbo based on the question and correct answer. The model is then evaluated on test sets with 0 to 5 adversarial sentences appended or prepended to the context. Performance is measured using F1 score and exact match metrics.

## Key Results
- ELECTRA-Small's F1 score drops from 83.9% to 29.2% when one adversarial sentence is appended
- Fine-tuning on datasets with 4 or 5 adversarial sentences achieves F1 scores above 70% on most evaluation datasets
- Models trained on 4-5 adversarial sentences maintain >75% F1 on append-0 to append-5 datasets
- All models perform poorly on prepend-1 evaluation dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial sentences cause ELECTRA-Small to predict answers from the adversarial content rather than the original context.
- Mechanism: The model treats the adversarial sentence as part of the context and generates answers from that sentence when it contains semantically related content to the question.
- Core assumption: The adversarial sentences are constructed to preserve linguistic similarity with the original question, making them plausible context.
- Evidence anchors:
  - [abstract] "I found that ELECTRA-Small’s F1 score on SQuAD dropped from 83.9% to 29.3% when one adversarial sentence was appended to the end of the paragraph of context."
  - [section] "I manually reviewed 100 of the incorrect predictions and found that 67% of them were contained in the adversarial sentence appended to the end of the context paragraph."
- Break condition: If adversarial sentences are constructed to be semantically unrelated or grammatically implausible, the model may ignore them and maintain performance.

### Mechanism 2
- Claim: Training with multiple adversarial sentences per example increases robustness by exposing the model to diverse attack patterns.
- Mechanism: Fine-tuning on datasets with 4 or 5 adversarial sentences per example teaches the model to focus on the original context and ignore misleading information.
- Core assumption: The model learns to distinguish between genuine context and adversarial noise when exposed to sufficient examples of both.
- Evidence anchors:
  - [abstract] "When finetuned on four or five adversarial sentences the model attains an F1 score of more than 70% on most evaluation datasets with multiple appended and prepended adversarial sentences."
  - [section] "The models trained on 4 or 5 appended adversarial sentences (train-4 & train-5), achieved a greater than 75% F1 score on all the append-0 to append-5 evaluation datasets."
- Break condition: If the number of adversarial sentences is insufficient (e.g., only 1-2), the model overfits to specific patterns and fails to generalize.

### Mechanism 3
- Claim: ELECTRA-Small is more vulnerable to adversarial attacks than expected due to its pretraining objective.
- Mechanism: The discriminative pretraining objective (detecting replaced tokens) may not adequately prepare the model for complex reasoning tasks like reading comprehension under adversarial conditions.
- Core assumption: The pretraining task focuses on token-level discrimination rather than higher-level comprehension skills.
- Evidence anchors:
  - [abstract] "My hypothesis was that ELECTRA would be resistant to Jia and Lang’s adversarial sentence attack. Instead, I found that ELECTRA-Small’s F1 score on SQuAD dropped from 83.9% to 29.3% when one adversarial sentence was appended to the context paragraph."
  - [section] "ELECTRA models are pretrained as a discriminator which attempt to determine whether a second masked language model had replaced any of the model’s input tokens."
- Break condition: If the model is further pretrained or fine-tuned on tasks that require deeper comprehension, it may become more resistant to adversarial attacks.

## Foundational Learning

- Concept: Adversarial examples in NLP
  - Why needed here: Understanding how adversarial sentences are constructed and their impact on model performance is critical for interpreting the results.
  - Quick check question: What is the key difference between the AddOneSent and AddSent methodologies in adversarial attacks?

- Concept: Fine-tuning with augmented data
  - Why needed here: The study relies on fine-tuning ELECTRA-Small on datasets with adversarial sentences to improve robustness.
  - Quick check question: Why does fine-tuning on datasets with 4 or 5 adversarial sentences per example generalize better than fine-tuning on 1 or 2?

- Concept: Evaluation metrics in reading comprehension
  - Why needed here: The study uses F1 score and exact match to evaluate model performance, which are standard metrics in the field.
  - Quick check question: How does the F1 score differ from exact match in evaluating reading comprehension models?

## Architecture Onboarding

- Component map:
  - ELECTRA-Small model -> GPT-3.5-Turbo adversarial sentence generator -> Fine-tuning pipeline -> Evaluation pipeline

- Critical path:
  1. Load pretrained ELECTRA-Small model
  2. Generate adversarial sentences for SQuAD training and evaluation datasets
  3. Fine-tune model on augmented datasets (train-0 to train-5)
  4. Evaluate fine-tuned models on adversarial evaluation datasets (append-0 to append-5, prepend-1 to prepend-5)
  5. Analyze results and identify failure patterns

- Design tradeoffs:
  - Using ELECTRA-Small instead of ELECTRA-Base for faster training but potentially lower baseline performance
  - Generating adversarial sentences with GPT-3.5-Turbo instead of manual creation for scalability but potential quality issues
  - Evaluating on prepend datasets to test generalization but observing poor performance on prepend-1

- Failure signatures:
  - Significant drop in F1 score when adversarial sentences are appended or prepended
  - Overfitting to specific adversarial patterns when fine-tuning on datasets with 1-2 adversarial sentences
  - Poor performance on prepend-1 dataset across all fine-tuned models

- First 3 experiments:
  1. Train and evaluate ELECTRA-Small on the original SQuAD v1.1 dataset to establish baseline performance.
  2. Fine-tune ELECTRA-Small on the train-1 dataset and evaluate on append-1 to test initial robustness.
  3. Fine-tune ELECTRA-Small on the train-4 dataset and evaluate on append-0 to append-5 to test generalization.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis is based on a single pretrained model (ELECTRA-Small) and one dataset (SQuAD v1.1), limiting generalizability
- Adversarial sentence generation relies on GPT-3.5-Turbo, introducing potential variability in quality and style
- Manual review of 100 incorrect predictions provides qualitative insights but may not capture full failure modes
- Poor performance on prepend-1 dataset suggests fundamental limitations in fine-tuning approach

## Confidence
- High confidence: ELECTRA-Small is vulnerable to adversarial attacks (consistent F1 score drops)
- Medium confidence: Fine-tuning with 4-5 adversarial sentences is effective (results strong but limited to one model/dataset)
- Low confidence: Generalizability of findings to other reading comprehension datasets and model architectures

## Next Checks
1. Replicate the study using ELECTRA-Base and BERT-Base to determine if model size affects adversarial robustness
2. Test the fine-tuning approach on other reading comprehension datasets (NewsQA, Natural Questions) to assess domain transferability
3. Experiment with different adversarial sentence generation strategies (rule-based vs. LLM-based) to isolate the impact of generation method on model robustness