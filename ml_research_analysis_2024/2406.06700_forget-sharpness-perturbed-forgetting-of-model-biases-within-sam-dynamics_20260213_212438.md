---
ver: rpa2
title: 'Forget Sharpness: Perturbed Forgetting of Model Biases Within SAM Dynamics'
arxiv_id: '2406.06700'
source_url: https://arxiv.org/abs/2406.06700
tags:
- forgetting
- biases
- generalization
- perturbation
- sharpness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new perspective on Sharpness-Aware Minimization
  (SAM) by introducing the concept of "perturbed forgetting," where perturbations
  during SAM training serve to discard undesirable model biases. The authors relate
  this idea to the information bottleneck principle and show that measuring the amount
  of forgotten information correlates better with generalization than traditional
  flatness metrics.
---

# Forget Sharpness: Perturbed Forgetting of Model Biases Within SAM Dynamics

## Quick Facts
- arXiv ID: 2406.06700
- Source URL: https://arxiv.org/abs/2406.06700
- Reference count: 27
- Primary result: OBF perturbation outperforms standard SAM, GSAM, and ASAM on ImageNet, robustness benchmarks, and transfer learning tasks by targeting output-exposed model biases

## Executive Summary
This paper presents a new perspective on Sharpness-Aware Minimization (SAM) by introducing the concept of "perturbed forgetting," where perturbations during SAM training serve to discard undesirable model biases. The authors relate this idea to the information bottleneck principle and show that measuring the amount of forgotten information correlates better with generalization than traditional flatness metrics. They propose a novel output bias forgetting (OBF) perturbation that targets model biases exposed in the model's outputs, which outperforms standard SAM, GSAM, and ASAM on ImageNet, robustness benchmarks, and transfer learning tasks. Their results suggest that the benefits of SAM can be explained by alternative mechanistic principles that do not require flatness of the loss surface.

## Method Summary
The paper introduces a novel perspective on SAM by framing perturbations as a mechanism for "perturbed forgetting" - the deliberate discarding of undesirable model biases during training. The authors develop this concept through three mechanisms: standard SAM perturbations that discard biases exposed through gradients, a new OBF perturbation that targets biases exposed through model outputs, and an information-theoretic framework to quantify forgetting. They validate their approach through extensive experiments on ImageNet classification, robustness benchmarks, and transfer learning tasks, demonstrating that OBF perturbation consistently outperforms standard SAM variants while achieving better correlation with generalization through perturbed forgetting metrics.

## Key Results
- OBF perturbation achieves 0.4-0.6% higher top-1 accuracy on ImageNet compared to standard SAM
- OBF shows 0.3-0.5% improvements on robustness benchmarks (ImageNet-Real, ImageNet-V2, ImageNet-R, ImageNet-Sketch)
- Transfer learning to CIFAR-10/100 shows 0.3-0.4% improvements over standard SAM
- Perturbed forgetting (I(X; Ŷp|Y) - I(X; Ŷ|Y)) correlates more strongly with generalization than flatness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAM perturbations perform "perturbed forgetting" by discarding undesirable model biases to exhibit learning signals that generalize better
- Mechanism: During each SAM update, perturbations take a gradient ascent step to maximize loss on a small batch, which discards biases learned from that batch. This forgotten information allows computing an update gradient that generalizes better than the original gradient.
- Core assumption: Model biases are exposed through gradients and outputs, and discarding these biases during perturbation improves generalization
- Evidence anchors:
  - [abstract] "We propose that perturbations in SAM perform perturbed forgetting, where they discard undesirable model biases to exhibit learning signals that generalize better."
  - [section] "We argue that SAM can decrease Equation (10) under this constraint when the perturbation batch size is small."
- Break condition: If perturbations fail to expose model biases or if discarding biases reduces rather than improves generalization performance

### Mechanism 2
- Claim: The OBF perturbation targets biases exposed through model outputs rather than gradients
- Mechanism: Instead of maximizing loss via steepest ascent, OBF modifies the perturbation objective to avoid sharpening non-target predictions, optionally pushing predictions toward uniformity when they exceed a threshold. This targets biases exposed in model outputs that may be amplified by standard SAM perturbations.
- Core assumption: Model biases can be exposed through model outputs, and these biases may be amplified rather than discarded by standard SAM perturbations
- Evidence anchors:
  - [abstract] "While standard SAM targets model biases exposed by the steepest ascent directions, we propose a new perturbation that targets biases exposed through the model's outputs."
  - [section] "When minimizing the loss by taking a step in the negative direction of Equation (14), the non-target logits are chosen based on their current corresponding likelihoods and pushed down. While these semantics are desirable during minimization, maximizing sharpens the non-target predictions to arrive at parameters that potentially amplify the model biases if they are exposed in Ŷ."
- Break condition: If output-exposed biases are not significant or if OBF perturbation fails to improve generalization compared to standard SAM

### Mechanism 3
- Claim: Perturbed forgetting can be quantified and correlated with generalization through information-theoretic measures
- Mechanism: The amount of task-irrelevant information discarded during perturbation (measured as I(X; Ŷp|Y) - I(X; Ŷ|Y)) correlates with generalization performance. This quantification allows validation of the perturbed forgetting hypothesis and comparison with traditional flatness metrics.
- Core assumption: Mutual information between inputs and outputs (conditioned on labels) can serve as a proxy for model biases, and the reduction in this quantity through perturbation correlates with generalization
- Evidence anchors:
  - [abstract] "We relate our notion of forgetting to the information bottleneck principle, use it to explain observations like the better generalization of smaller perturbation batches, and show that perturbed forgetting can exhibit a stronger correlation with generalization than flatness."
  - [section] "We average the difference I(X; Ŷ|Y) - I(X; Ŷp|Y) across all adjusted thresholds for every model and epoch per perturbation type. We evaluate the Kendall rank correlation between this difference and the final CIFAR-10 test accuracy the model attains."
- Break condition: If information-theoretic measures fail to correlate with generalization or if alternative explanations better account for SAM's success

## Foundational Learning

- Concept: Information bottleneck principle
  - Why needed here: The paper uses this principle to justify why discarding task-irrelevant information through perturbations improves generalization
  - Quick check question: How does minimizing I(X; Z|Y) while maintaining I(Z;Y) relate to the goal of achieving good generalization?

- Concept: PAC-Bayes bounds and generalization theory
  - Why needed here: Understanding the theoretical foundation that motivates SAM and its variants, including the relationship between sharpness, flatness, and generalization
  - Quick check question: What is the difference between the original PAC-Bayes bound used to justify SAM and the m-sharpness concept that emerged in practice?

- Concept: Gradient-based optimization and first-order Taylor approximations
  - Why needed here: SAM uses a first-order approximation to make the inner maximization problem tractable, and understanding this approximation is crucial for grasping the algorithm's mechanics
  - Quick check question: Why does SAM use a first-order Taylor approximation of the loss around the current parameters when computing perturbations?

## Architecture Onboarding

- Component map: Model weights → Forward pass → Predictions → Perturbation function (steepest ascent or OBF) → Loss computation → Gradient computation → Parameter update
- Critical path: Forward pass to compute current predictions → Perturbation step to generate θp → Compute loss and gradients at θp → Use these gradients to update original parameters θ
- Design tradeoffs: Steepest ascent perturbations are simple and well-established but may amplify certain biases; OBF perturbations require additional hyperparameters (γ, λ) but may better target output-exposed biases
- Failure signatures: If OBF perturbation leads to worse generalization than steepest ascent, if the correlation between perturbed forgetting and generalization breaks down, or if the model fails to converge
- First 3 experiments:
  1. Replicate the correlation analysis between perturbed forgetting (I(X; Ŷp|Y) - I(X; Ŷ|Y)) and generalization on CIFAR-10 with different perturbation batch sizes
  2. Compare steepest ascent vs. OBF perturbation on ImageNet with ViT-S/32 using the same hyperparameters to isolate the effect of the perturbation type
  3. Test the sensitivity of OBF to its hyperparameters (γ, λ) by sweeping these values and measuring their impact on ImageNet performance

## Open Questions the Paper Calls Out
- Question: How do different perturbations compare in terms of the types of model biases they target?
- Question: Can the perturbed forgetting perspective be extended to non-image classification tasks?
- Question: What is the optimal perturbation batch size (m) for different architectures and tasks?

## Limitations
- The paper focuses primarily on image classification tasks, leaving uncertainty about whether these mechanisms generalize to other domains
- The information-theoretic quantification relies on approximations that may not capture all relevant aspects of model bias
- The causal relationship between discarding model biases and improved generalization is not definitively established

## Confidence
- Mechanism 1 (Perturbed forgetting through gradient ascent): Medium
- Mechanism 2 (OBF perturbation targeting output biases): Medium
- Mechanism 3 (Information-theoretic quantification of forgetting): Low-Medium

## Next Checks
1. Conduct ablation studies systematically varying perturbation batch size and observing both forgetting measures and generalization performance to isolate the effect of perturbed forgetting from other optimization effects.

2. Perform cross-domain validation by testing OBF perturbation on non-image datasets (e.g., text classification or tabular data) to evaluate whether the output bias forgetting mechanism is domain-agnostic.

3. Design experiments comparing models with identical final sharpness but different levels of perturbed forgetting to directly test whether forgetting correlates with generalization independent of flatness.