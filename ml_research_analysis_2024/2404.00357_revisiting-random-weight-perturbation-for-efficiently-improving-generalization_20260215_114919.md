---
ver: rpa2
title: Revisiting Random Weight Perturbation for Efficiently Improving Generalization
arxiv_id: '2404.00357'
source_url: https://arxiv.org/abs/2404.00357
tags:
- uni00000013
- uni00000011
- uni00000018
- perturbation
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper revisits random weight perturbation (RWP) as a means
  to improve generalization in deep neural networks. The authors identify a trade-off
  between generalization and convergence in RWP: larger perturbations improve generalization
  but hinder convergence.'
---

# Revisiting Random Weight Perturbation for Efficiently Improving Generalization

## Quick Facts
- arXiv ID: 2404.00357
- Source URL: https://arxiv.org/abs/2404.00357
- Reference count: 32
- Key outcome: The authors propose mixed-RWP (m-RWP) and adaptive RWP (ARWP) methods that achieve comparable or superior performance to SAM while requiring only half the computation time.

## Executive Summary
This paper revisits random weight perturbation (RWP) as a means to improve generalization in deep neural networks. The authors identify a trade-off between generalization and convergence in RWP: larger perturbations improve generalization but hinder convergence. To address this, they propose a mixed loss objective (m-RWP) that combines the expected Bayes loss with the original loss, improving convergence and allowing for larger perturbations. Additionally, they introduce an adaptive perturbation generation method (ARWP) that leverages historical gradient information for more effective perturbations. The improved methods, m-ARWP, achieve comparable or superior performance to SAM (a state-of-the-art method) while requiring only half the computation time due to parallelizable gradient steps.

## Method Summary
The authors propose two key improvements to random weight perturbation: mixed-RWP (m-RWP) and adaptive RWP (ARWP). m-RWP combines the original loss with the expected Bayes loss using a balance coefficient λ, which improves convergence while maintaining generalization benefits. ARWP leverages historical gradient information to generate more effective perturbations by scaling them based on the running sum of squared gradients. The combined method, m-ARWP, uses both improvements simultaneously. The methods are evaluated on CIFAR-10/100 and ImageNet across various architectures including VGG, ResNet, WideResNet, and ViT, demonstrating improved generalization and computational efficiency compared to RWP and SAM.

## Key Results
- m-RWP improves convergence while maintaining generalization benefits compared to RWP
- ARWP generates more effective perturbations by leveraging historical gradient information
- m-ARWP achieves comparable or superior performance to SAM while requiring only half the computation time
- The methods demonstrate effectiveness across multiple architectures (VGG, ResNet, WideResNet, ViT) and datasets (CIFAR-10/100, ImageNet)

## Why This Works (Mechanism)

### Mechanism 1
RWP requires much larger perturbations than AWP to achieve similar expected training loss due to the lack of gradient information in RWP. AWP uses precise gradient information to compute worst-case perturbations efficiently, while RWP samples random perturbations that need to be larger in magnitude to match the effectiveness of AWP's targeted perturbations. This is based on the assumption that the loss landscape is smooth enough that worst-case perturbations can be approximated by first-order Taylor expansion, and that random perturbations require larger magnitudes to achieve similar loss reduction.

### Mechanism 2
Mixing the original loss with the expected Bayes loss (m-RWP) improves convergence by reducing the variance introduced by random perturbations. The mixed loss objective combines the smoothed landscape benefits of the Bayes loss with the local information from the original loss, creating a more stable optimization path that converges faster while maintaining generalization benefits. This assumes that the trade-off between generalization and convergence in RWP is primarily due to the variance introduced by random perturbations, and this variance can be effectively reduced by incorporating the original loss.

### Mechanism 3
Using historical gradient information for adaptive perturbation generation (ARWP) creates more effective perturbations by aligning perturbation direction with loss minimization. By scaling perturbations based on historical gradient magnitudes, the method applies larger perturbations in directions where gradients have been historically small and smaller perturbations where gradients have been historically large, creating more targeted and effective exploration of the loss landscape. This assumes that historical gradient information is a good proxy for the current loss landscape structure and can be used to adaptively scale perturbations for more effective exploration.

## Foundational Learning

- Concept: Sharpness-aware minimization (SAM) and its relationship to flat minima
  - Why needed here: Understanding SAM provides the baseline comparison for why RWP methods need improvement and what performance targets they should aim for
  - Quick check question: What is the key difference between the min-max formulation in SAM and the expected loss formulation in RWP?

- Concept: Fenchel biconjugate and convex relaxation in optimization theory
  - Why needed here: The paper establishes a mathematical connection between SAM and RWP using convex optimization tools, which is important for understanding their theoretical relationship
  - Quick check question: How does the Fenchel biconjugate of the Bayes objective relate to the SAM objective?

- Concept: Non-convex optimization convergence theory and smoothness conditions
  - Why needed here: The paper provides convergence analysis for RWP and m-RWP under specific smoothness assumptions, which is crucial for understanding the trade-offs between generalization and convergence
  - Quick check question: What role do the Lipschitz continuity and smoothness constants play in the convergence bounds for RWP methods?

## Architecture Onboarding

- Component map: Base optimizer -> Perturbation generator (RWP/ARWP) -> Mixed loss computation -> Weight update
- Critical path: Forward pass → Compute perturbation → Compute mixed loss gradient → Update weights
- Design tradeoffs:
  - Larger σ improves generalization but hurts convergence
  - Parallel gradient computation (m-RWP) reduces training time but requires careful batch management
  - Adaptive perturbations (ARWP) improve effectiveness but add computational overhead for historical gradient tracking
- Failure signatures:
  - Training divergence: Likely caused by σ being too large for the chosen λ
  - Poor generalization: May indicate λ is too small, not providing enough smoothing
  - Excessive memory usage: Could result from storing historical gradient information for ARWP
- First 3 experiments:
  1. Compare training loss curves for RWP vs m-RWP with varying σ to verify the convergence improvement
  2. Test different λ values (0.3, 0.5, 0.7) to find the optimal balance between smoothing and local information
  3. Compare generalization performance of ARWP vs RWP on a small dataset to verify the effectiveness of adaptive perturbations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance coefficient λ for the mixed-RWP (m-RWP) objective across different network architectures and datasets?
- Basis in paper: The authors mention that λ=0.5 is a robust choice that achieves moderately good performance on both CIFAR-10 and CIFAR-100, but they also test other values.
- Why unresolved: The paper only tests a limited range of λ values and does not provide a systematic analysis of how the optimal λ might vary across different architectures and datasets.
- What evidence would resolve it: A comprehensive study of m-RWP performance across a wider range of λ values, architectures, and datasets would help identify the optimal balance coefficient for different scenarios.

### Open Question 2
- Question: How does the adaptive random weight perturbation (ARWP) method perform compared to other advanced weight perturbation techniques, such as those based on information geometry or gradient regularization?
- Basis in paper: The authors propose ARWP as an improvement over RWP, but they only compare it to RWP and SAM in their experiments.
- Why unresolved: The paper does not provide a direct comparison of ARWP to other advanced weight perturbation methods that have been proposed in the literature.
- What evidence would resolve it: Including a comparison of ARWP to other advanced weight perturbation techniques in the experiments would help determine its relative performance and effectiveness.

### Open Question 3
- Question: How does the proposed m-ARWP method scale to extremely large-scale problems, such as training large language models or vision transformers on massive datasets?
- Basis in paper: The authors demonstrate the effectiveness of m-ARWP on ImageNet with various architectures, including ViT, but they do not explore its performance on extremely large-scale problems.
- Why unresolved: The paper does not provide experimental results or theoretical analysis of m-ARWP's performance on extremely large-scale problems, which are becoming increasingly important in modern machine learning.
- What evidence would resolve it: Conducting experiments on extremely large-scale problems, such as training large language models or vision transformers on massive datasets, would help assess the scalability and effectiveness of m-ARWP in these settings.

## Limitations

- The paper's core claim about RWP requiring significantly larger perturbations than AWP rests on an unstated assumption about the smoothness of the loss landscape that may not hold in practice.
- The effectiveness of ARWP relies heavily on historical gradient information being a good proxy for the current optimization landscape, which is not empirically validated.
- The paper provides limited ImageNet results with only one architecture (ViT-Ti) and does not compare against state-of-the-art methods on this dataset.

## Confidence

- **High Confidence**: The mixed loss objective (m-RWP) improves convergence - The convergence analysis is mathematically rigorous and the experimental results on CIFAR-10/100 show consistent improvements across multiple architectures.
- **Medium Confidence**: m-ARWP achieves comparable performance to SAM with half the computation - While the theoretical efficiency gains are clear, the practical speedup depends heavily on implementation details and hardware capabilities.
- **Low Confidence**: The proposed methods scale effectively to ImageNet-level problems - The paper provides limited ImageNet results with only one architecture and does not compare against state-of-the-art methods on this dataset.

## Next Checks

1. **Landscape Smoothness Validation**: Conduct ablation studies measuring actual perturbation magnitudes needed for RWP vs AWP across different architectures (CNNs, Transformers) and datasets to verify the claimed orders-of-magnitude difference holds empirically.

2. **Gradient Stationarity Analysis**: Track the correlation between historical and current gradients during training to quantify how well the adaptive scaling in ARWP tracks the true loss landscape evolution.

3. **Scaling Benchmark**: Evaluate m-ARWP against modern sharpness-aware methods (Sharpness-Aware Minimization, Dynamic Sharpness-Aware Minimization) on ImageNet using multiple architectures to validate the claimed computational efficiency at scale.