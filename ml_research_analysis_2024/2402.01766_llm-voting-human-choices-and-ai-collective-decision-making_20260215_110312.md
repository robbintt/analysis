---
ver: rpa2
title: 'LLM Voting: Human Choices and AI Collective Decision Making'
arxiv_id: '2402.01766'
source_url: https://arxiv.org/abs/2402.01766
tags: []
core_contribution: This study examines the voting behaviors of Large Language Models
  (LLMs), specifically GPT-4 and LLaMA-2, and their alignment with human voting patterns.
  Using a dataset from a human voting experiment, we conducted a corresponding experiment
  with LLM agents, focusing on collective outcomes and individual preferences.
---

# LLM Voting: Human Choices and AI Collective Decision Making

## Quick Facts
- arXiv ID: 2402.01766
- Source URL: https://arxiv.org/abs/2402.01766
- Reference count: 40
- Primary result: LLMs show voting behaviors influenced by method choice, presentation order, and temperature settings, with potential biases when integrated into democratic processes

## Executive Summary
This study investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, and their alignment with human voting patterns. Using a dataset from a human voting experiment, researchers conducted a corresponding experiment with LLM agents to examine collective outcomes and individual preferences. The study found that voting methods and presentation order significantly influence LLM voting outcomes, while varying personas can reduce biases and enhance alignment with human choices. The Chain-of-Thought approach showed potential for AI explainability despite not improving prediction accuracy, and the research identified a trade-off between preference diversity and alignment accuracy in LLMs.

## Method Summary
The study employed a comparative experimental design, conducting a human voting experiment and replicating it with LLM agents using the same dataset. Researchers tested GPT-4 and LLaMA-2 models under various conditions, including different voting methods, presentation orders, and temperature settings. The Chain-of-Thought approach was evaluated for its potential to improve prediction accuracy and explainability. Persona variation was introduced to assess its impact on bias reduction and alignment with human choices. The experiment focused on collective outcomes and individual preferences to understand how LLMs process voting scenarios compared to humans.

## Key Results
- Choice of voting methods and presentation order significantly influenced LLM voting outcomes
- Varying persona reduced biases and enhanced alignment with human choices
- Identified trade-off between preference diversity and alignment accuracy in LLMs across different temperature settings
- Chain-of-Thought approach showed potential for AI explainability despite no improvement in prediction accuracy

## Why This Works (Mechanism)
The study demonstrates that LLMs process voting information sequentially, making them sensitive to presentation order and voting method selection. The models' responses are influenced by their training data patterns and the specific architectural properties of transformer-based systems. Temperature settings affect the diversity of responses, creating a fundamental trade-off between generating diverse outcomes versus maintaining alignment with human preferences. Persona variation works by providing contextual framing that helps LLMs navigate decision-making more similarly to human reasoning processes.

## Foundational Learning
1. **Voting method sensitivity** - LLMs respond differently to various voting systems (plurality, ranked-choice, etc.) due to their sequential processing nature; needed to understand how method selection affects AI voting behavior; quick check: compare LLM outputs across different voting methods with identical inputs.

2. **Temperature-diversity trade-off** - Higher temperature settings increase response diversity but may reduce alignment with human preferences; needed to quantify the balance between diverse outcomes and human-aligned decisions; quick check: measure diversity metrics (entropy, unique responses) across temperature ranges.

3. **Chain-of-Thought reasoning** - LLMs can break down complex decisions into intermediate reasoning steps, though this doesn't always improve accuracy; needed to explore explainability pathways for AI voting decisions; quick check: compare single-step vs. multi-step reasoning outputs for consistency.

4. **Persona effects** - Contextual framing through persona assignment influences LLM decision-making patterns and bias expression; needed to understand how to guide LLMs toward more human-like reasoning; quick check: test same voting scenario with multiple persona assignments.

## Architecture Onboarding

**Component Map:** Human Voting Data -> LLM Input Processing -> Temperature Sampling -> Voting Method Application -> Output Generation -> Alignment Analysis

**Critical Path:** Data Input → Temperature Setting → Voting Method Selection → Output Generation → Bias Analysis

**Design Tradeoffs:** The study reveals a fundamental tension between response diversity (controlled by temperature) and alignment accuracy with human preferences. Higher diversity settings may better represent minority views but risk reducing overall coherence with human decision patterns.

**Failure Signatures:** When temperature settings are too low, LLMs produce homogeneous outputs that may not capture diverse perspectives. When temperature is too high, outputs become less aligned with human preferences and may generate inconsistent or illogical voting patterns.

**3 First Experiments:**
1. Test temperature sensitivity by running identical voting scenarios across temperature settings (0.0, 0.5, 1.0, 1.5) and measuring diversity vs. alignment metrics
2. Compare voting outcomes when presentation order is randomized versus fixed to quantify order effects
3. Apply different personas to the same voting scenario to measure bias reduction and alignment improvement

## Open Questions the Paper Calls Out
None

## Limitations
- Limited scope to only GPT-4 and LLaMA-2 models, constraining generalizability to other LLM architectures
- Single dataset from one human voting experiment may not capture real-world voting complexity
- Focus on binary or simple categorical choices rather than complex multi-dimensional policy decisions

## Confidence

**High Confidence:** Findings regarding voting method choice and presentation order influence on LLM outcomes; persona variation effects on bias and human alignment

**Medium Confidence:** Chain-of-Thought approach potential for explainability; trade-off between preference diversity and alignment accuracy

**Low Confidence:** Broader claims about LLM integration into democratic processes and real-world implementation challenges

## Next Checks
1. Replicate study using additional LLM models (Claude, Gemini, open-source alternatives) to test robustness across architectures
2. Conduct experiments with complex voting scenarios involving multi-dimensional policy choices or ranked-preference systems
3. Test temperature settings and sampling strategies on voting diversity with larger participant pools to better quantify diversity-alignment trade-offs