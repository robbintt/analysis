---
ver: rpa2
title: 'No-Clean-Reference Image Super-Resolution: Application to Electron Microscopy'
arxiv_id: '2401.08115'
source_url: https://arxiv.org/abs/2401.08115
tags: []
core_contribution: "This paper proposes a deep learning-based super-resolution (SR)\
  \ framework called EMSR to address the challenge of acquiring clean high-resolution\
  \ (HR) electron microscopy (EM) images over large brain tissue volumes. The key\
  \ contributions are: Investigating training with no-clean references for \u2113\
  2 and \u21131 loss functions."
---

# No-Clean-Reference Image Super-Resolution: Application to Electron Microscopy

## Quick Facts
- arXiv ID: 2401.08115
- Source URL: https://arxiv.org/abs/2401.08115
- Reference count: 40
- Key outcome: Introduces EMSR network for EM super-resolution trained with no-clean references, achieving competitive results through edge-attention and self-attention mechanisms.

## Executive Summary
This paper addresses the challenge of acquiring clean high-resolution electron microscopy images over large brain tissue volumes by proposing a deep learning-based super-resolution framework called EMSR. The key innovation lies in training with no-clean references, investigating both ℓ2 and ℓ1 loss functions, and introducing a novel network architecture that leverages distinctive features in brain EM images through multi-scale edge-attention and self-attention mechanisms. Experiments on nine brain datasets demonstrate the feasibility of training with non-clean references and show that the proposed method either outperforms or is competitive with established SR techniques while mitigating noise and recovering fine details.

## Method Summary
The EMSR network architecture consists of three main components: a feature extractor (HF_E) that projects input to shallow features, an edge attention module (HEA) with multi-scale edge attention and self-attention mechanisms, and a reconstruction module (HR) that upsamples and maps features to HR output. The network is trained using acquired pairs of LR and HR 3D-EM images with two loss functions (ℓ1 and ℓ2), and employs weight sharing between original and noisier LR branches to create a noise-robust framework. Training is performed with Adam optimizer for 200,000 steps with initial learning rate of 10^-4, halved every 50,000 steps, and evaluated using SSIM, PSNR, and FRC metrics.

## Key Results
- Training with real LR-HR pairs produces high-quality SR results, demonstrating feasibility of training with non-clean references for both ℓ2 and ℓ1 loss functions
- Comparable results are achieved when employing denoised and noisy references for training
- Training with synthetically generated LR images from HR counterparts yields satisfactory SR results, sometimes outperforming training with real pairs
- The proposed EMSR network is either superior or competitive with established SR techniques in mitigating noise while recovering fine details

## Why This Works (Mechanism)

### Mechanism 1
Training with no-clean references is feasible for both ℓ2 and ℓ1 loss functions under weak noise conditions. The network can optimize using corrupted references when corruption is weak enough that clean image structure dominates. For ℓ2 loss, noise variance becomes negligible; for ℓ1 loss, the bound on the difference between clean and noisy solutions tightens when noise mean is small. Core assumption: noise is additive, i.i.d., and its mean and variance are small relative to image content (E[n] ≈ 0, σ²ₙ ≪ σ²ₓ).

### Mechanism 2
Edge-attention and self-attention mechanisms enhance SR by emphasizing edge features over less informative backgrounds. Multi-scale edge-attention extracts edge information from noisy inputs and applies it to deep features. Self-attention (via ViT) captures both local and global dependencies, allowing the network to prioritize edges and fine details while suppressing background noise. Core assumption: brain EM images have repetitive textural/geometrical patterns with less informative backgrounds, making edge-focus effective.

### Mechanism 3
Weight sharing between original and noisier LR branches creates a noise-robust framework. Sharing modules forces the network to produce consistent outputs for both clean and noisier inputs, enabling self-supervised denoising. The output from the clean input serves as a reference for the noisier input. Core assumption: underlying image content is consistent between inputs, and noise is independent and random.

## Foundational Learning

- **Image degradation models**: Understanding of blur, downsampling, and noise combination is critical since the paper investigates real-world degradations in EM, not synthetic ones. Quick check: What are main sources of degradation in EM imaging, and how do they differ from standard photography degradations?

- **Loss functions ℓ2 vs ℓ1**: Familiarity with their robustness to outliers is needed since the paper compares these for training with noisy references. Quick check: Why does ℓ1 loss tend to be more robust to outliers than ℓ2, and how does this affect training with noisy references?

- **Attention mechanisms**: Basics of multi-head, self-attention, and edge-attention are required since the network uses these via ViT. Quick check: How does multi-head self-attention differ from single-head, and why is it useful for capturing both local and global image features?

## Architecture Onboarding

- **Component map**: Input → HF_E → HEA (multi-scale edge + attention + ViT) → HR → Output
- **Critical path**: Input flows through feature extractor, edge attention module, and reconstruction module to produce output
- **Design tradeoffs**: Weight sharing reduces parameters and enforces consistency but may limit flexibility; edge-attention adds complexity but improves fine detail recovery; ViT blocks increase receptive field but are computationally heavier
- **Failure signatures**: Poor SR quality may indicate edge-attention over/under-emphasis; noisy output suggests weight sharing forces ignoring important degradation patterns; slow convergence may indicate ViT blocks are too heavy
- **First 3 experiments**: 1) Sanity check with synthetic LR→HR pair (bicubic downsample) to verify basic functionality; 2) Weight sharing ablation to quantify noise-robustness benefit; 3) Edge-attention ablation to confirm contribution on datasets with strong edge patterns

## Open Questions the Paper Calls Out
- How do different levels of noise corruption affect the feasibility of training with no-clean references for both ℓ2 and ℓ1 loss functions?
- How does the proposed EMSR network architecture compare to other state-of-the-art SR methods in terms of computational efficiency and memory usage?
- How does the proposed EMSR network handle different types of noise and artifacts in brain EM images, such as charging effects and drift distortions?

## Limitations
- Theoretical justification for no-clean reference training is somewhat limited, with break conditions not thoroughly explored
- Edge-attention effectiveness is closely tied to specific brain EM image characteristics that may not generalize to other imaging modalities
- Mathematical derivations for ℓ2 and ℓ1 loss behavior with noisy references are brief with strong assumptions about noise statistics

## Confidence
- **High confidence**: Feasibility of training with real LR-HR pairs for SR in EM, demonstrated by quantitative metrics and qualitative comparisons
- **Medium confidence**: Superiority of EMSR architecture over baselines, as edge-attention and self-attention show promise but specific contributions need isolation
- **Low confidence**: Theoretical bounds on ℓ2 and ℓ1 loss behavior with noisy references due to brief derivations and strong assumptions

## Next Checks
1. Systematically vary noise level in synthetic LR images and measure SR quality degradation to identify break point where no-clean reference training fails
2. Apply EMSR network to EM images from different brain regions or other biological tissues to test generalization beyond corpus callosum and cingulum datasets
3. Conduct controlled ablation study removing/modifying edge-attention and self-attention to quantify individual contributions to SR performance