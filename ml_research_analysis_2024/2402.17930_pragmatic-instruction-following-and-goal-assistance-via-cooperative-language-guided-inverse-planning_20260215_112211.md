---
ver: rpa2
title: Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided
  Inverse Planning
arxiv_id: '2402.17930'
source_url: https://arxiv.org/abs/2402.17930
tags: []
core_contribution: CLIPS is a Bayesian architecture for pragmatic instruction following
  and goal assistance, modeling humans as cooperative planners who communicate joint
  plans through instructions. The method performs multimodal Bayesian inference over
  human goals from actions and language, using large language models to evaluate instruction
  likelihood given hypothesized plans, then acts to minimize expected goal achievement
  cost.
---

# Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning

## Quick Facts
- arXiv ID: 2402.17930
- Source URL: https://arxiv.org/abs/2402.17930
- Reference count: 40
- CLIPS achieves 1.26-1.68× faster goal achievement with significantly higher goal accuracy than GPT-4V, literal instruction following, and unimodal inverse planning baselines

## Executive Summary
This paper introduces CLIPS (Cooperative Language-Guided Inverse Planning), a Bayesian architecture for pragmatic instruction following and goal assistance. CLIPS models humans as cooperative planners who communicate joint plans through instructions, performing multimodal Bayesian inference over human goals from actions and language. The method leverages large language models to evaluate instruction likelihood given hypothesized plans and acts to minimize expected goal achievement cost. CLIPS demonstrates significantly higher goal accuracy and helpfulness compared to strong baselines like GPT-4V and literal instruction following, while also producing human-like goal inferences and assistance options. The system effectively resolves ambiguous language and interprets joint instructions in multi-step goal assistance problems.

## Method Summary
CLIPS is a Bayesian architecture that performs pragmatic instruction following and goal assistance by modeling humans as cooperative planners. The system conducts multimodal Bayesian inference over human goals using both observed actions and natural language instructions. Large language models evaluate the likelihood of instructions given hypothesized plans, enabling the system to infer intended goals and generate appropriate assistance. The architecture then acts to minimize expected goal achievement cost, balancing efficiency with goal satisfaction. This approach allows CLIPS to resolve ambiguous language, interpret joint instructions, and produce human-like goal inferences and assistance strategies across multi-step planning tasks.

## Key Results
- CLIPS achieves significantly higher goal accuracy and helpfulness than GPT-4V, literal instruction following, and unimodal inverse planning baselines
- The method produces human-like goal inferences (Pearson's r=0.93) and assistance options (Pearson's r=0.96)
- CLIPS is 1.26-1.68× faster in achieving goals compared to baseline methods while closely matching human raters' judgments

## Why This Works (Mechanism)
CLIPS works by modeling humans as cooperative planners who communicate joint plans through natural language instructions. The Bayesian inference framework allows the system to reason about the underlying goals that motivate human actions and instructions, rather than taking instructions literally. By leveraging LLMs to evaluate instruction likelihood given hypothesized plans, CLIPS can infer the intended meaning behind ambiguous or underspecified instructions. The system's ability to minimize expected goal achievement cost ensures that assistance strategies are both efficient and aligned with human goals. This cooperative planning perspective enables CLIPS to resolve linguistic ambiguity and interpret joint instructions in a human-like manner.

## Foundational Learning
CLIPS builds on foundational work in inverse planning, Bayesian inference for human intent recognition, and cooperative communication theory. The method extends classical inverse planning approaches by incorporating natural language as an additional modality for goal inference. The use of LLMs for instruction likelihood evaluation draws from recent advances in few-shot learning and language understanding. CLIPS also connects to pragmatics in human communication, where speakers and listeners engage in collaborative reasoning about shared plans and goals. The Bayesian framework provides a principled way to combine multimodal evidence and uncertainty in goal inference and assistance planning.

## Architecture Onboarding
CLIPS employs a Bayesian architecture that integrates multimodal inputs through cooperative planning principles. The system consists of three main components: (1) multimodal Bayesian inference over human goals, (2) LLM-based instruction likelihood evaluation, and (3) goal-directed assistance planning. The inference component combines evidence from observed actions and natural language instructions to compute posterior distributions over possible goals. The LLM component evaluates how likely each instruction is given hypothesized plans, enabling pragmatic interpretation of language. The assistance planner then generates actions to minimize expected goal achievement cost, balancing efficiency with goal satisfaction. This architecture allows CLIPS to reason about human intentions and provide helpful assistance in complex, ambiguous scenarios.

## Open Questions the Paper Calls Out
- How well does CLIPS generalize to more complex, open-world environments with longer time horizons and less structured goals?
- Can the method handle diverse user populations and communication styles to ensure robustness to linguistic variation?
- What is the contribution of individual components (Bayesian inference vs. LLM likelihood evaluation) to overall performance?
- How does CLIPS perform when faced with conflicting or misleading instructions from humans?
- Can the cooperative planning framework be extended to multi-agent scenarios with competing goals?

## Limitations
- Evaluation focuses on relatively constrained environments with well-defined goals, limiting generalizability to complex, open-ended real-world scenarios
- Benchmark domains may not capture the full spectrum of pragmatic challenges encountered in practical applications
- Reliance on LLM-based likelihood evaluation introduces potential biases from training data and raises questions about robustness to novel linguistic constructions
- The cooperative planning assumption may not hold in all human-AI interaction scenarios, particularly when human goals are misaligned or adversarial
- Computational overhead of Bayesian inference and LLM evaluation may limit real-time applicability in resource-constrained settings

## Confidence
- High confidence in demonstrated performance improvements over specified baselines in tested domains
- Medium confidence in broader claims about human-like pragmatic reasoning due to controlled nature of evaluation
- Computational efficiency gains (1.26-1.68× faster) are well-supported by reported results
- Assumption: The cooperative planning model accurately captures human communication behavior in the tested domains
- Unknown: How well the method generalizes to scenarios with significantly different task structures or linguistic patterns

## Next Checks
1. Test CLIPS on more complex, open-world environments with longer time horizons and less structured goals
2. Evaluate performance with diverse user populations and communication styles to assess robustness to linguistic variation
3. Conduct ablation studies to quantify the contribution of individual components (Bayesian inference vs. LLM likelihood evaluation) to overall performance
4. Investigate the method's behavior in scenarios with conflicting or misleading instructions from humans
5. Explore extensions to multi-agent scenarios with competing goals and evaluate cooperative vs. adversarial planning strategies