---
ver: rpa2
title: Training Heterogeneous Client Models using Knowledge Distillation in Serverless
  Federated Learning
arxiv_id: '2402.07295'
source_url: https://arxiv.org/abs/2402.07295
tags:
- client
- training
- clients
- serverless
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses challenges in federated learning (FL) systems
  where clients have heterogeneous computational resources and non-IID data distributions.
  The paper proposes adapting knowledge distillation (KD) techniques to enable training
  heterogeneous client models using serverless computing (Function-as-a-Service).
---

# Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning

## Quick Facts
- arXiv ID: 2402.07295
- Source URL: https://arxiv.org/abs/2402.07295
- Authors: Mohak Chadha, Pulkit Khera, Jianfeng Gu, Osama Abboud, Michael Gerndt
- Reference count: 40
- Primary result: Optimized serverless workflows for FedMD and FedDF achieve 3.5x speedup for FedMD and 1.76x for FedDF compared to original implementations

## Executive Summary
This paper addresses the challenge of training heterogeneous client models in federated learning (FL) systems where clients have diverse computational resources and non-IID data distributions. The authors propose adapting knowledge distillation (KD) techniques to enable model heterogeneity in serverless computing environments (Function-as-a-Service). They extend an open-source serverless FL system called FedLess with optimized workflows for two KD-based FL strategies: FedMD and FedDF. The key innovations include parallelizing transfer learning using Ray for FedMD and parallelizing ensemble distillation for FedDF. Experiments across multiple datasets demonstrate that serverless FedDF is more robust to extreme non-IID distributions, faster, and lower cost than serverless FedMD.

## Method Summary
The paper extends FedLess, a serverless FL system, to support heterogeneous client models using knowledge distillation. Two strategies are implemented: FedMD (transfer learning with a public dataset) and FedDF (ensemble distillation with an unlabeled public dataset). For FedMD, Ray is used to parallelize transfer learning across all client models on the public dataset. For FedDF, multiple aggregator functions are created to perform ensemble distillation in parallel for each unique model architecture. The system also incorporates intelligent client selection based on clustering similar architectures and sorting by training duration to avoid stragglers. Experiments use 100 clients with heterogeneous model architectures (CNNs, LSTMs) on MNIST, CIFAR, Shakespeare, and Nietzsche datasets with varying non-IID distributions.

## Key Results
- FedDF is more robust to extreme non-IID data distributions than FedMD across all tested datasets
- Proposed optimizations achieve average 3.5x speedup for FedMD and 1.76x speedup for FedDF compared to original implementations
- Serverless FedDF costs less than serverless FedMD in most cases due to fewer required clients (10 vs 100) despite using more aggregator functions (6 vs 1)
- FedDF achieves better accuracy than FedMD on Shakespeare and Nietzsche datasets with high non-IID levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation enables heterogeneous client models by transferring knowledge via logits instead of raw parameters.
- Mechanism: The central server aggregates class prediction logits from all client models and redistributes these aggregated logits back to clients. Each client then trains its model to align its predictions with the aggregated logits, enabling model personalization while benefiting from collaborative learning.
- Core assumption: Model architectures can differ as long as they share the same output dimension equal to the sum of private and public dataset classes.
- Evidence anchors:
  - [abstract] "KD enables heterogeneous client models in FL since knowledge transfer is achieved by distilling model prediction probabilities or logits instead of directly exchanging model parameters"
  - [section] "KD enables heterogeneous client models in FL since knowledge transfer is achieved by distilling model prediction probabilities or logits instead of directly exchanging model parameters between the student and teacher models"
  - [corpus] Weak - corpus neighbors focus on model heterogeneity but don't provide direct evidence for logit-based distillation mechanism
- Break condition: If client models have incompatible output dimensions or if public dataset class count doesn't match private dataset class count, the distillation process fails.

### Mechanism 2
- Claim: Serverless parallelization using Ray significantly accelerates transfer learning and ensemble distillation steps.
- Mechanism: Ray creates distributed actors for each client function instance, enabling parallel training on the public dataset during initial transfer learning and parallel ensemble distillation across unique model architectures.
- Core assumption: Ray can effectively distribute training across available compute resources without introducing significant overhead or coordination bottlenecks.
- Evidence anchors:
  - [section] "we leverage Ray to simultaneously train all client models on the public dataset until convergence" and "we extend FedLess to support multiple aggregator functions... These functions are triggered in parallel for each unique model architecture"
  - [corpus] Weak - corpus doesn't directly address serverless parallelization mechanisms
- Break condition: If Ray cluster resources are insufficient or if network bandwidth becomes a bottleneck, parallelization benefits diminish.

### Mechanism 3
- Claim: Intelligent client selection based on clustering improves training efficiency by avoiding stragglers.
- Mechanism: Clients with the same model architecture are first clustered, then sorted by training duration metrics. A round-robin selection from each architecture group ensures balanced participation and avoids slow clients dominating training rounds.
- Core assumption: Training duration correlates with client performance and can be used to predict future round participation effectiveness.
- Evidence anchors:
  - [section] "we adapt the intelligent clustering-based client selection algorithm in FedLess can and integrate it with FedDF... For sorting clients, we use the metric exponential moving average that relies on the training duration of clients"
  - [corpus] Weak - corpus doesn't provide evidence for clustering-based client selection
- Break condition: If client performance varies unpredictably or if architecture-specific characteristics don't correlate with training duration, clustering becomes ineffective.

## Foundational Learning

- Concept: Federated Learning fundamentals
  - Why needed here: Understanding the synchronous training rounds, client-server architecture, and challenges of data/model heterogeneity is essential for grasping the proposed solutions
  - Quick check question: What is the key difference between traditional centralized learning and federated learning in terms of data handling?

- Concept: Knowledge Distillation principles
  - Why needed here: The paper relies on transferring knowledge through logits rather than parameters, which requires understanding how distillation works in traditional ML
  - Quick check question: In knowledge distillation, what is transferred from teacher to student model instead of raw parameters?

- Concept: Serverless computing concepts
  - Why needed here: The paper's optimizations leverage FaaS characteristics like statelessness, event-driven execution, and pay-per-use billing
  - Quick check question: What are the two key characteristics of serverless functions that make them suitable for FL clients?

## Architecture Onboarding

- Component map: FedLess controller -> MongoDB parameter servers -> Client functions -> Ray cluster -> Aggregator functions
- Critical path: Client training → Logit communication → Aggregation → Model update → Evaluation
- Design tradeoffs:
  - Model heterogeneity vs. aggregation complexity
  - Parallelization benefits vs. resource coordination overhead
  - Client selection accuracy vs. selection computation time
- Failure signatures:
  - Aggregation step consistently timing out (indicates straggler problem)
  - Model accuracy plateaus early (indicates insufficient distillation)
  - Resource utilization remains low (indicates parallelization inefficiency)
- First 3 experiments:
  1. Test basic FedMD/FedDF convergence with homogeneous models on MNIST
  2. Verify parallelization speedup by comparing sequential vs. Ray-based transfer learning
  3. Validate client selection clustering by comparing random vs. intelligent selection performance

## Open Questions the Paper Calls Out
- Question: How does the choice of public dataset affect the accuracy of FedMD compared to FedDF across different non-IID data distributions?
- Question: What is the optimal strategy for balancing the number of participating clients and aggregator functions in FedDF to minimize total cost while maintaining accuracy?
- Question: How would implementing more advanced data-free knowledge distillation algorithms affect the performance and cost of serverless federated learning?

## Limitations
- Parallelization benefits lack detailed analysis of overhead costs and scalability limits
- Client selection clustering assumes stable training duration correlations without validation under varying workloads
- Cost comparisons don't account for public dataset storage or transfer costs

## Confidence
- High confidence: Knowledge distillation enables heterogeneous models (well-established ML principle)
- Medium confidence: Serverless parallelization achieves 3.5x speedup for FedMD (empirical but limited dataset scope)
- Medium confidence: FedDF outperforms FedMD on extreme non-IID data (supported by experiments but needs broader validation)

## Next Checks
1. Test scalability limits by increasing client count beyond 100 and measuring parallelization efficiency degradation
2. Validate client selection clustering under varying network conditions and resource availability
3. Conduct cost analysis including public dataset storage and transfer costs for comprehensive comparison