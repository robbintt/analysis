---
ver: rpa2
title: Constructing Multilingual Visual-Text Datasets Revealing Visual Multilingual
  Ability of Vision Language Models
arxiv_id: '2406.15359'
source_url: https://arxiv.org/abs/2406.15359
tags:
- image
- gpt-4v
- language
- text
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper constructs multilingual visual-text datasets in English,
  Japanese, Swahili, and Urdu to evaluate vision language models across nine vision-and-language
  tasks. The authors use GPT-4V to generate answers and rationales for image-text
  pairs sourced from Wikinews and Wikipedia articles.
---

# Constructing Multilingual Visual-Text Datasets Revealing Visual Multilingual Ability of Vision Language Models

## Quick Facts
- arXiv ID: 2406.15359
- Source URL: https://arxiv.org/abs/2406.15359
- Authors: Jesse Atuhurra; Iqra Ali; Tatsuya Hiraoka; Hidetaka Kamigaito; Tomoya Iwakura; Taro Watanabe
- Reference count: 26
- Primary result: Construct multilingual visual-text datasets in English, Japanese, Swahili, and Urdu to evaluate VLMs across nine tasks

## Executive Summary
This paper constructs multilingual visual-text datasets spanning English, Japanese, Swahili, and Urdu to evaluate vision language models across nine vision-and-language tasks. The authors use GPT-4V to generate answers and rationales for image-text pairs sourced from Wikinews and Wikipedia articles, introducing a new "unrelatedness" task. Human evaluation by native speakers assesses dataset quality, revealing performance gaps across languages with GPT-4V performing best in English and Urdu. The study also fine-tunes LLaVA 1.5 on the dataset, finding significant performance differences compared to GPT-4V.

## Method Summary
The authors collected 200 image-text pairs per language from Wikinews and Wikipedia across ten categories. They designed task-specific prompts for nine VL tasks and used GPT-4V to generate answers and rationales. The datasets were then evaluated by native speakers using Likert scales. Additionally, they augmented each image-text pair with five additional images based on randomly selected entities from the text. Finally, they fine-tuned LLaVA 1.5 on the constructed dataset for comparison with GPT-4V performance.

## Key Results
- Performance gaps observed across languages, with GPT-4V performing best in English and Urdu, followed by Japanese and Swahili
- Human evaluation by native speakers validates dataset quality and VLM performance across tasks
- Fine-tuning LLaVA 1.5 on the dataset reveals significant performance differences compared to GPT-4V
- Entity-based image augmentation strategy implemented for multimodal entity extraction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V generates high-quality answers and rationales when given prompts that combine image, text, and specific questions in a single context window.
- Mechanism: In-context learning allows the model to process multimodal inputs without fine-tuning, leveraging its pre-trained cross-modal representations to align visual features with textual reasoning.
- Core assumption: GPT-4V's context window is large enough to contain the image, text, questions, and expected answer format without truncation or loss of coherence.
- Evidence anchors:
  - [abstract] "we introduced nine vision-and-language (VL) tasks... through utilizing templates containing questions and prompting GPT4-V to generate the answers and the rationales"
  - [section 3.3] "We leveraged ICL and designed prompts... An example prompt is shown in Table 2"
  - [corpus] Weak - no corpus evidence on context window capacity
- Break condition: If the prompt exceeds GPT-4V's context window or the image encoding becomes too noisy, answer quality degrades sharply.

### Mechanism 2
- Claim: Native speaker human evaluation is essential for validating multilingual dataset quality because automated metrics fail to capture linguistic nuance and cultural appropriateness.
- Mechanism: Native evaluators assess translation accuracy, answer coherence, and task-specific correctness using Likert scales anchored in linguistic and cultural knowledge.
- Core assumption: Native speakers can reliably judge answer quality even when they lack deep VL task expertise, provided they follow clear evaluation guidelines.
- Evidence anchors:
  - [section 4] "All the evaluators possess basic visual-linguistic knowledge and are instructed using well-documented evaluation guidelines"
  - [section 5] "human evaluation by recruiting native speakers (Sections 4 and 7) for each language to measure the suitability of our datasets for VL tasks"
  - [corpus] Weak - no corpus evidence on inter-annotator agreement
- Break condition: If evaluation guidelines are ambiguous or evaluators lack sufficient training, inter-rater reliability drops and dataset quality assessments become inconsistent.

### Mechanism 3
- Claim: Augmenting each image-text pair with five additional images based on randomly selected entities from the text improves downstream multimodal entity extraction and VQA performance.
- Mechanism: Multiple images per text instance force the model to disambiguate which visual features correspond to which textual entities, improving entity grounding.
- Core assumption: Randomly selecting entities ensures diverse coverage across image-text pairs and prevents overfitting to a single visual context.
- Evidence anchors:
  - [section 3.6] "we augmented the image-text pair by adding five new images, based on five entities randomly selected from the text"
  - [section 8] "we will use the augmented data for multimodal NER analysis in the future"
  - [corpus] No corpus evidence on performance impact of augmentation
- Break condition: If entity selection is biased or too sparse, augmentation adds noise rather than signal, harming model performance.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: Enables GPT-4V to generate answers without fine-tuning, preserving computational efficiency while maintaining strong zero-shot performance.
  - Quick check question: Can GPT-4V produce coherent answers for a new VL task if given only a few examples in the prompt?

- Concept: Multimodal entity grounding
  - Why needed here: Critical for tasks like entity extraction and image-text matching where the model must link textual mentions to visual objects.
  - Quick check question: Does the model correctly identify all textual entities that correspond to objects in the image?

- Concept: Cross-lingual prompt design
  - Why needed here: Ensures consistent task definitions across languages despite grammatical and cultural differences.
  - Quick check question: Do translated prompts preserve the same logical structure and expected answer format as the English version?

## Architecture Onboarding

- Component map: Wikinews/Wikipedia -> image-text pair extraction -> translation -> prompt formatting -> GPT-4V generation -> human evaluation
- Critical path: Data collection -> translation -> prompt generation -> GPT-4V inference -> human evaluation -> dataset release
- Design tradeoffs:
  - Single large prompt vs. multiple small prompts: Single prompt reduces API calls but risks context window overflow.
  - Human evaluation vs. automated metrics: Human evaluation ensures quality but is expensive and slow.
  - Entity-based augmentation vs. random sampling: Entity-based is more targeted but requires reliable entity extraction.
- Failure signatures:
  - Context truncation: GPT-4V outputs incomplete or incoherent answers.
  - Translation drift: Generated answers become misaligned with original intent after translation.
  - Low inter-annotator agreement: Human evaluation becomes unreliable.
- First 3 experiments:
  1. Test GPT-4V answer quality with single vs. multi-image prompts on a small sample.
  2. Measure inter-annotator agreement on a subset of translated prompts.
  3. Compare entity extraction accuracy with and without image augmentation on a held-out set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4V vary across different image-text categories in the multilingual datasets?
- Basis in paper: [inferred] The paper mentions that image-text pairs were collected for ten categories (animals, products, buildings, locations, events, food, drinks, hobbies, works of art, and organization) and that human evaluation was conducted on 200 samples (20 per category). However, the paper does not explicitly analyze the performance variation across these categories.
- Why unresolved: The paper focuses on the overall performance of GPT-4V across tasks and languages, but does not provide a detailed analysis of performance variations within each category.
- What evidence would resolve it: Analyzing the performance of GPT-4V on each of the ten categories separately, comparing the results across languages and tasks, and identifying any significant differences or patterns.

### Open Question 2
- Question: How does the inclusion of rationales in the datasets impact the evaluation process and the understanding of VLM reasoning?
- Basis in paper: [explicit] The paper explicitly mentions that rationales were introduced in the datasets to facilitate human evaluation and enable understanding of the VLM reasoning process.
- Why unresolved: While the paper states the importance of rationales, it does not provide a detailed analysis of how the inclusion of rationales specifically impacts the evaluation process or the insights gained into VLM reasoning.
- What evidence would resolve it: Conducting a comparative analysis of the evaluation process and the insights gained with and without the inclusion of rationales, and quantifying the impact of rationales on the accuracy and depth of evaluation.

### Open Question 3
- Question: How do the performance gaps observed across languages in the multilingual datasets relate to the quality of translations and the linguistic characteristics of each language?
- Basis in paper: [explicit] The paper mentions that the quality of translations impacted the ability to accurately comprehend text fed into GPT-4V and hypothesizes that this is one of the reasons for the observed performance gaps across languages.
- Why unresolved: While the paper acknowledges the potential impact of translation quality, it does not provide a detailed analysis of how specific linguistic characteristics of each language (e.g., grammar, vocabulary, syntax) contribute to the performance gaps.
- What evidence would resolve it: Conducting a linguistic analysis of each language, identifying the specific challenges posed by each language, and correlating these challenges with the observed performance gaps in the multilingual datasets.

## Limitations

- Reliance on GPT-4V for dataset generation without ground truth validation beyond human evaluation
- Translation process from English to other languages not independently verified, creating potential cascading errors
- Entity augmentation mechanism using random selection may introduce noise rather than signal
- Limited comparison between GPT-4V and fine-tuned LLaVA 1.5 without few-shot or full fine-tuning scenarios

## Confidence

- High Confidence: The dataset construction methodology (image-text pairing, multilingual translation, prompt design) and the basic finding that performance varies across languages.
- Medium Confidence: The human evaluation results and cross-lingual performance comparisons, given the lack of inter-annotator reliability metrics and potential translation drift.
- Low Confidence: The entity augmentation mechanism's effectiveness and the relative performance comparison between GPT-4V and LLaVA 1.5 without more rigorous fine-tuning experiments.

## Next Checks

1. **Inter-annotator Agreement Analysis**: Calculate and report Fleiss' kappa or similar metrics for human evaluation across all languages to establish reliability of native speaker assessments.

2. **Translation Quality Validation**: Conduct a blind comparison where native speakers evaluate English prompts against translated versions to measure semantic drift and ensure prompt consistency across languages.

3. **Augmentation Effectiveness Study**: Perform controlled experiments comparing entity extraction accuracy with and without image augmentation on a held-out test set, measuring precision-recall trade-offs to quantify the actual benefit of the augmentation strategy.