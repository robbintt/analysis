---
ver: rpa2
title: Recovering document annotations for sentence-level bitext
arxiv_id: '2406.03869'
source_url: https://arxiv.org/abs/2406.03869
tags:
- data
- translation
- machine
- linguistics
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the scarcity of document-level training data
  for context-aware machine translation by reconstructing document-level annotations
  from three large bitext datasets (ParaCrawl, News Commentary, and Europarl) across
  six language pairs. The authors introduce a document-level filtering technique using
  SLIDE with CometKiwi quality estimation to filter out context-inconsistent (likely
  machine-translated) documents, and train context-aware models on the resulting dataset.
---

# Recovering document annotations for sentence-level bitext

## Quick Facts
- arXiv ID: 2406.03869
- Source URL: https://arxiv.org/abs/2406.03869
- Authors: Rachel Wicks; Matt Post; Philipp Koehn
- Reference count: 20
- This work reconstructs document-level annotations from three large bitext datasets (ParaCrawl, News Commentary, and Europarl) across six language pairs, enabling context-aware machine translation models that improve both general translation quality and context-dependent phenomena.

## Executive Summary
This paper addresses the scarcity of document-level training data for context-aware machine translation by reconstructing document-level annotations from existing sentence-level bitext datasets. The authors introduce a document-level filtering technique using SLIDE with CometKiwi quality estimation to remove context-inconsistent (likely machine-translated) documents, then train context-aware models on the resulting dataset. Their models achieve consistent improvements in BLEU scores on general translation tasks and significantly better performance on context-dependent phenomena (e.g., gender and auxiliary verb translation) without degrading sentence-level translation quality. The reconstructed dataset and models are publicly released as PARA DOCS.

## Method Summary
The authors reconstruct document-level metadata from monolingual sources using exact string matching between bitext segments and original documents, then apply document-level filtering using SLIDE with CometKiwi quality estimation to rank documents by context consistency. They train context-aware models with concatenated context windows (up to 10 sentences or 256 tokens) using mixed data streams that combine context-level and sentence-level bitext. The filtering process removes documents likely to contain inconsistent translations, while the mixed training ensures models maintain strong sentence-level performance while leveraging document context for improved translation of context-dependent phenomena.

## Key Results
- Context-aware models trained on reconstructed document data achieve consistent BLEU score improvements on general translation tasks
- Significant improvements in context-dependent phenomena translation (gender, formality, auxiliary verbs) as measured by CTXPRO
- No degradation in sentence-level translation quality despite the addition of document context
- Document-level filtering effectively removes context-inconsistent translations while preserving high-quality contextual data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Document-level filtering using SLIDE-CometKiwi effectively removes context-inconsistent (likely machine-translated) documents while preserving high-quality contextual data.
- Mechanism: The SLIDE method slides a fixed-size window across documents, scores each window with CometKiwi quality estimation, and averages the scores to produce a document-level quality metric. Documents are then ranked and filtered based on these scores.
- Core assumption: Machine-translated documents on the web tend to be sentence-level translations that lack consistency across adjacent sentences, and CometKiwi can distinguish such inconsistent translations from human-translated ones.
- Evidence anchors:
  - [abstract]: "We present this filtering with analysis to show that this method prefers context-consistent translations rather than those that may have been sentence-level machine translated."
  - [section]: "Raunak et al. (2023) showed that these same quality estimators can discriminate between context-consistent and context-inconsistent translationsâ€”as one might see when translating each sentence individually."
  - [corpus]: Weak - no direct corpus-level validation provided; inference based on scoring distribution.
- Break condition: If CometKiwi quality estimation fails to differentiate between human-translated context-consistent and machine-translated inconsistent documents, the filtering will not improve data quality.

### Mechanism 2
- Claim: Reconstructing document metadata from monolingual sources restores context boundaries necessary for context-aware model training.
- Mechanism: Using exact string matching between bitext segments and original monolingual documents, the method recovers paragraph, sentence, and character offsets to reconstruct document structure.
- Core assumption: The original monolingual documents contain the exact text of the translated segments, allowing reliable reconstruction through exact matching.
- Evidence anchors:
  - [abstract]: "Most large-scale datasets have been processed through a pipeline that discards document-level metadata. In this work, we reconstruct document-level information..."
  - [section]: "Given an ordered list of segments from the unfiltered bitext, and the original web-crawled document, we can align each segment to the original document via exact string matching."
  - [corpus]: Weak - assumes high-quality string matching without reporting match accuracy rates or failure modes.
- Break condition: If string matching fails due to text normalization differences, OCR errors, or paraphrasing in the translation, reconstruction will be incomplete or incorrect.

### Mechanism 3
- Claim: Context-aware models trained on reconstructed document data improve translation of context-dependent phenomena without degrading sentence-level performance.
- Mechanism: Models trained with concatenated context windows (up to 10 sentences or 256 tokens) learn to leverage inter-sentence dependencies while maintaining sentence-level translation capability through mixed training with sentence-level data.
- Core assumption: The context-dependent phenomena (gender, formality, auxiliary verbs) require information from surrounding sentences that can be captured through window-based context concatenation.
- Evidence anchors:
  - [abstract]: "Last we train models on these longer contexts and demonstrate improvement in document-level translation without degradation of sentence-level translation."
  - [section]: "We train context-aware models with a simple concatenation strategy... The second stream pulls from a supplementary dataset composed of preprocessed sentence-level bitext."
  - [corpus]: Weak - no validation that context windows are sufficiently large or appropriately positioned for the phenomena studied.
- Break condition: If context windows are too small or misaligned with the actual dependencies needed for translation, improvements in context-dependent phenomena will not materialize.

## Foundational Learning

- Concept: Document-level alignment vs. sentence-level alignment in parallel corpora
  - Why needed here: Understanding the fundamental difference between preserving document structure versus treating parallel text as independent sentence pairs is crucial for grasping why document reconstruction matters.
  - Quick check question: What key information is lost when converting document-aligned parallel corpora to sentence-level bitext?

- Concept: Quality estimation without reference translations
  - Why needed here: The filtering method relies on CometKiwi, a reference-free quality estimator, making it essential to understand how such systems work and their limitations.
  - Quick check question: How does reference-free quality estimation differ from traditional BLEU-based evaluation, and what are its key advantages for filtering noisy web data?

- Concept: Sliding window aggregation for document-level scoring
  - Why needed here: The SLIDE method's effectiveness depends on understanding how local window scores can represent global document quality.
  - Quick check question: Why might averaging window-level quality scores provide a more robust document-level quality estimate than using a single global score?

## Architecture Onboarding

- Component map: monolingual alignment -> offset extraction -> context window creation -> SLIDE windowing -> CometKiwi scoring -> percentile-based filtering -> mixed data streaming (context + sentence-level) -> context concatenation -> model training -> BLEU/comet scoring + CTXPRO for context-dependent phenomena
- Critical path: 1. Reconstruct document metadata from monolingual sources 2. Apply document-level filtering using SLIDE-CometKiwi 3. Train context-aware models with mixed data streams 4. Evaluate on both general and context-dependent metrics
- Design tradeoffs: Window size vs. computational cost in SLIDE scoring; Context window size vs. model capacity and training efficiency; Filtering strictness vs. data quantity for training; Exact string matching vs. fuzzy matching for reconstruction reliability
- Failure signatures: Poor filtering performance: high SLIDE scores for known low-quality documents; Reconstruction failures: missing offsets, misaligned context windows; Training instability: degradation in sentence-level BLEU when adding context data; Evaluation anomalies: context-dependent metrics not improving despite increased context
- First 3 experiments: 1. Validate reconstruction accuracy by comparing reconstructed document boundaries against manually annotated samples 2. Test SLIDE-CometKiwi filtering effectiveness by manually inspecting highest and lowest quartile documents 3. Ablation study on context window size (1, 3, 5, 10 sentences) to find optimal balance between performance and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of context-aware models vary when using different document filtering cutoffs (e.g., top 10% vs. top 50%) on the PARA DOCS dataset?
- Basis in paper: [explicit] The paper discusses the use of document-level filtering with three different cutoffs (top 75%, 50%, and 25%) and their impact on model performance.
- Why unresolved: The paper only evaluates three specific filtering cutoffs and does not explore other potential cutoffs or their effects on model performance.
- What evidence would resolve it: Conduct experiments with additional filtering cutoffs (e.g., top 10%, 20%, 30%, etc.) and compare the resulting model performance on BLEU and CTXPRO scores.

### Open Question 2
- Question: Can the document-level filtering method be effectively applied to other low-resource language pairs, and how does its performance compare to sentence-level filtering in these cases?
- Basis in paper: [inferred] The paper mentions that the dataset is constrained by the availability of document-level data and that extending to lower-resource languages would be limited.
- Why unresolved: The paper focuses on high-resource language pairs and does not explore the applicability of the document-level filtering method to low-resource languages.
- What evidence would resolve it: Apply the document-level filtering method to low-resource language pairs and compare the performance with sentence-level filtering on BLEU and CTXPRO scores.

### Open Question 3
- Question: How does the performance of context-aware models change when using different context lengths (e.g., 5 sentences vs. 10 sentences) during training and inference?
- Basis in paper: [explicit] The paper uses a context length of up to 10 sentences or 256 tokens during training and inference.
- Why unresolved: The paper does not explore the impact of varying context lengths on model performance.
- What evidence would resolve it: Train and evaluate context-aware models using different context lengths (e.g., 5, 10, 15 sentences) and compare their performance on BLEU and CTXPRO scores.

## Limitations

- The effectiveness of document-level filtering depends critically on CometKiwi's ability to distinguish context-consistent from context-inconsistent translations, but limited direct validation is provided
- Exact string matching for document reconstruction assumes perfect text preservation across the translation pipeline, with no match accuracy rates or failure mode analysis
- The optimal context window size remains unclear, with the paper using up to 10 sentences without systematic ablation studies to justify these parameters

## Confidence

- **High Confidence**: The general approach of using quality estimation for document-level filtering is well-established, and the methodology for mixed training (context + sentence-level) is standard practice in the field.
- **Medium Confidence**: The reconstruction methodology is plausible given exact string matching is a common approach, but the lack of validation data reduces confidence in its reliability across diverse datasets.
- **Low Confidence**: The specific improvements in context-dependent phenomena rely heavily on CometKiwi's filtering effectiveness and the appropriateness of the context window sizes, neither of which are thoroughly validated.

## Next Checks

1. Conduct manual inspection of the highest and lowest quartile documents from the SLIDE-CometKiwi filtering to verify that the method correctly identifies context-consistent versus context-inconsistent translations, with specific attention to documents where CometKiwi scores diverge from human judgment.
2. Perform systematic ablation studies on context window size (1, 3, 5, 10 sentences) to determine the optimal balance between capturing relevant dependencies and maintaining computational efficiency, measuring both general translation quality and context-dependent phenomena performance.
3. Validate the document reconstruction accuracy by comparing a sample of reconstructed document boundaries against manually annotated gold-standard boundaries from the original monolingual sources, quantifying match accuracy and identifying common failure patterns.