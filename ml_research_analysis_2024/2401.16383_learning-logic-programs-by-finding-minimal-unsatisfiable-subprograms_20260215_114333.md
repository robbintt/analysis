---
ver: rpa2
title: Learning logic programs by finding minimal unsatisfiable subprograms
arxiv_id: '2401.16383'
source_url: https://arxiv.org/abs/2401.16383
tags:
- musp
- learning
- musps
- hypothesis
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an approach to identify minimal unsatisfiable
  subprograms (MUSPs) in logic programs to improve inductive logic programming (ILP)
  performance. The method finds MUSPs by removing literals from failed programs and
  building constraints to prune the search space.
---

# Learning logic programs by finding minimal unsatisfiable subprograms

## Quick Facts
- arXiv ID: 2401.16383
- Source URL: https://arxiv.org/abs/2401.16383
- Reference count: 29
- This paper introduces an approach to identify minimal unsatisfiable subprograms (MUSPs) in logic programs to improve inductive logic programming (ILP) performance

## Executive Summary
This paper presents a novel approach to improve inductive logic programming (ILP) performance by identifying minimal unsatisfiable subprograms (MUSPs) in failed logic programs. The method systematically removes literals from programs to find these MUSPs, then uses them to build constraints that prune the hypothesis space more effectively than traditional whole-program constraints. Experiments across multiple domains demonstrate that this approach can reduce learning times by up to 99% while maintaining high predictive accuracy.

## Method Summary
The approach identifies MUSPs by removing literals from failed programs and checking for unsatisfiability. It then builds two types of constraints: specialization constraints that prune all specialisations of MUSPs (which cannot be solutions), and redundancy constraints that prune programs redundant with respect to MUSPs (which cannot be optimal). These constraints are integrated into the ILP generate-test-constrain loop, where they eliminate many more programs than pruning whole programs would. The method uses a recursive algorithm to find all MUSPs by systematically removing literals and checking unsatisfiability conditions.

## Key Results
- MUSPs can reduce learning times by up to 99% compared to standard ILP approaches
- The approach maintains high predictive accuracy across multiple benchmark domains
- MUSP-based constraints enable more effective pruning than whole-program constraints, eliminating all specialisations and redundancies of failed programs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finding minimal unsatisfiable subprograms (MUSPs) enables more precise pruning of the hypothesis space than whole-program constraints.
- Mechanism: MUSPs are minimal subsets of literals that cannot be satisfied together. By identifying these subsets, the system can prune all specialisations and redundancies of MUSPs, eliminating many more programs than pruning whole programs.
- Core assumption: The unsatisfiability of a MUSP implies the unsatisfiability of all its specialisations and any program redundant with respect to it.
- Evidence anchors:
  - [abstract] "The key insight is that finding MUSPs enables more effective pruning than using whole-program constraints."
  - [section] "Since a MUP is totally incomplete then its specialisations cannot be solutions" (Proposition 1)
  - [corpus] Weak - the corpus neighbors discuss MUS computation but not specifically ILP applications
- Break condition: If MUSPs cannot be efficiently identified or if the overhead outweighs pruning benefits, the mechanism fails.

### Mechanism 2
- Claim: The recursive MUSP finding algorithm efficiently identifies all minimal unsatisfiable subprograms.
- Mechanism: The algorithm removes literals one by one from programs, checks satisfiability, and recursively searches for MUSPs in unsatisfiable subprograms.
- Core assumption: A program is unsatisfiable if and only if it has an unsatisfiable subprogram, and removing literals will eventually find all MUSPs.
- Evidence anchors:
  - [section] "We search for subprograms of the program h by removing a literal from it and checking that the subprogram obeys the conditions in Deﬁnition 4"
  - [corpus] Weak - the corpus neighbors discuss MUS computation algorithms but not the specific recursive approach used here
- Break condition: If the recursive search becomes too computationally expensive or misses MUSPs due to the literal-by-literal removal strategy.

### Mechanism 3
- Claim: The constraints built from MUSPs are optimally sound, meaning they never prune optimal solutions.
- Mechanism: Specialization constraints prune all specialisations of MUSPs (which cannot be solutions), and redundancy constraints prune programs redundant with respect to MUSPs (which cannot be optimal).
- Core assumption: A specialization of a totally incomplete program cannot be a solution, and a program redundant with respect to a totally incomplete program cannot be optimal.
- Evidence anchors:
  - [section] "Proposition 1 (MUSP specialisations). Let h be a program, m be a MUSP of h, and h′ be a specialisation of m. Then h′ is not a solution."
  - [section] "Proposition 2 (MUSP redundancy ). Let h and h′ be hypotheses, m be a MUSPs of h, and h′ be redundant with respect to m. Then h′ is not an optimal solution."
  - [corpus] Weak - the corpus neighbors discuss MUS computation but not soundness properties in ILP
- Break condition: If the definitions of specialization or redundancy are incorrectly implemented, optimal solutions might be pruned.

## Foundational Learning

- Concept: Logic programming fundamentals (clauses, predicates, substitutions)
  - Why needed here: The MUSP approach operates on logic programs and requires understanding of how clauses subsume each other
  - Quick check question: What is the difference between a definite clause and a constraint in logic programming?

- Concept: Inductive Logic Programming (ILP) and hypothesis space search
  - Why needed here: The MUSP approach is an ILP method that prunes the hypothesis space using constraints
  - Quick check question: How does a specialization constraint differ from a generalization constraint in ILP?

- Concept: Constraint Satisfaction Problem (CSP) formulation
  - Why needed here: The MUSP approach uses CSP constraints to represent programs and prune the search space
  - Quick check question: In the CSP formulation, what do the models represent?

## Architecture Onboarding

- Component map: MUSPer core -> MUSP finder -> Constraint builder -> Prolog interface
- Critical path: Generate program → Test on examples → If failure, find MUSPs → Build constraints → Add to CSP → Next iteration
- Design tradeoffs: The approach trades computational overhead of finding MUSPs for more effective pruning. The literal-by-literal removal strategy for finding MUSPs may not scale well to large programs.
- Failure signatures: High overhead with little pruning benefit, timeouts in MUSP finding, incorrect constraints leading to missed solutions, poor scalability with program size.
- First 3 experiments:
  1. Run MUSPer on a simple domain (e.g., trains) and compare learning time and number of programs considered vs Popper
  2. Measure the overhead of MUSP finding vs pruning benefit on a moderate complexity domain
  3. Test MUSPer on a domain with known MUSPs to verify the constraints are being correctly identified and applied

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scalability of the MUSP finding algorithm compare to existing SAT-based methods for finding minimal unsatisfiable cores, particularly on larger problems?
- Basis in paper: [explicit] The paper mentions using a "folk algorithm" that deletes literals one by one to find MUSPs, and notes that this approach "does not scale well to large formulas" according to Dershowitz et al. 2006.
- Why unresolved: The experiments only involve relatively small problems where scalability wasn't an issue, so no direct comparison is provided.
- What evidence would resolve it: Empirical comparison of MUSP finding runtime versus SAT-based core extraction on larger logic programs, measuring both absolute time and relative performance as problem size increases.

### Open Question 2
- Question: Under what specific conditions does identifying MUSPs provide the greatest reduction in learning time versus standard whole-program constraint generation?
- Basis in paper: [inferred] The results show MUSPs help most on some tasks (like sql-02) but provide minimal benefit on others (like zendo tasks), suggesting problem characteristics affect the benefit.
- Why unresolved: The paper doesn't analyze what makes certain problems more amenable to MUSP-based pruning than others.
- What evidence would resolve it: Systematic analysis of which problem features (e.g., background knowledge structure, example distribution, program complexity) correlate with greater MUSP benefits, possibly through controlled experiments varying these parameters.

### Open Question 3
- Question: Could the MUSP finding approach be extended to handle non-definite programs or programs with negation, and what theoretical challenges would arise?
- Basis in paper: [explicit] The paper explicitly focuses on definite programs with least Herbrand model semantics and doesn't address programs with negation.
- Why unresolved: The paper establishes the method for definite programs but doesn't explore generalizations to broader logic programming paradigms.
- What evidence would resolve it: Formal extension of the MUSP definitions and constraints to programs with negation, along with empirical validation on problems requiring such features.

## Limitations
- The MUSP identification algorithm's computational complexity is not thoroughly analyzed, raising concerns about scalability to larger programs
- The paper claims optimal sound constraints but doesn't provide formal proofs for all claims
- The experimental evaluation uses a limited set of domains and doesn't compare against all major ILP systems

## Confidence
- High Confidence: The core mechanism of using MUSPs to prune the search space is well-supported by the theoretical framework and experimental results
- Medium Confidence: The MUSP finding algorithm's efficiency and completeness are supported by implementation details but lack rigorous complexity analysis
- Low Confidence: Claims about scalability to larger programs and general applicability across diverse ILP domains are based on limited experimental evidence

## Next Checks
1. **Scalability Analysis**: Conduct experiments with progressively larger logic programs to measure MUSP identification overhead and pruning effectiveness at scale
2. **Constraint Soundness Proof**: Formally prove that specialization and redundancy constraints based on MUSPs are optimally sound across all ILP problem types
3. **Cross-System Comparison**: Implement the MUSP approach in multiple ILP systems (not just Popper) and compare performance across a standardized benchmark suite to validate generalizability