---
ver: rpa2
title: Assessing LLMs for Zero-shot Abstractive Summarization Through the Lens of
  Relevance Paraphrasing
arxiv_id: '2406.03993'
source_url: https://arxiv.org/abs/2406.03993
tags:
- article
- sentence
- paraphrasing
- summarization
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces relevance paraphrasing to evaluate the robustness
  of large language models (LLMs) in zero-shot abstractive summarization. The method
  identifies sentences in an article most relevant to its gold summary, paraphrases
  them to minimally perturb the input, and measures how well the LLM generates consistent
  summaries on the perturbed articles.
---

# Assessing LLMs for Zero-shot Abstractive Summarization Through the Lens of Relevance Paraphrasing

## Quick Facts
- arXiv ID: 2406.03993
- Source URL: https://arxiv.org/abs/2406.03993
- Reference count: 36
- LLMs show up to 50% ROUGE-2 performance drops after relevance paraphrasing

## Executive Summary
This paper introduces relevance paraphrasing to evaluate the robustness of large language models (LLMs) in zero-shot abstractive summarization. The method identifies sentences most relevant to gold summaries, paraphrases them minimally, and measures how well LLMs generate consistent summaries on the perturbed articles. Experiments across four datasets (CNN/DM, XSum, Reddit, News) and four LLMs (GPT-3.5 Turbo, Llama-2-13B, Mistral-7B, Dolly-v2-7B) reveal significant performance drops after relevance paraphrasing, indicating LLMs are not consistent summarizers. The findings highlight the need for improved robustness in LLM summarization.

## Method Summary
The authors propose relevance paraphrasing to assess LLM summarization robustness by identifying sentences most relevant to gold summaries using TF-IDF similarity, paraphrasing them with Llama-2-13B, and generating summaries with each LLM. They measure performance changes using ROUGE-1/2/L and BertScore before and after relevance paraphrasing. The method tests whether LLMs can maintain consistent summarization performance when minimally perturbing the most relevant input content.

## Key Results
- All LLMs experience significant performance drops after relevance paraphrasing, up to 50% on ROUGE-2
- LLMs shift to using different sentences for summary generation, losing key information
- Performance drops are consistent across all four datasets and evaluation metrics
- Zero-shot LLMs are not robust summarizers for minimally perturbed articles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Perturbing only the sentences identified as most relevant to the gold summary disrupts the LLM's internal sentence selection process.
- **Mechanism:** The LLM uses different article sentences to generate the summary after relevance paraphrasing, leading to performance drops across evaluation metrics.
- **Core assumption:** LLMs rely on specific input sentences for generating summaries, and changing these sentences alters the summary generation pathway.
- **Evidence anchors:**
  - [abstract] "LLMs also shift to using different sentences for summary generation, losing key information."
  - [section 3.2] "LLMs start utilizing different sentences to generate the summary on the paraphrased input article."
  - [corpus] No direct evidence found in corpus neighbors about sentence selection mechanisms.

### Mechanism 2
- **Claim:** Relevance paraphrasing creates minimal semantic perturbations that test the robustness of LLM summarization without changing the overall meaning of the article.
- **Mechanism:** By paraphrasing only the most relevant sentences, the input space is minimally perturbed while retaining semantic similarity, allowing for a focused robustness assessment.
- **Core assumption:** Paraphrasing maintains semantic similarity while introducing syntactic variance sufficient to test robustness.
- **Evidence anchors:**
  - [section 2.2] "We then replace those sentences in the article with their paraphrased versions. That is, for each of these article sentences xi, ∀i ∈ Ix we will now obtain a paraphrased version x′i using the paraphrasing model θ and replace each xi with paraphrased x′i to obtain a paraphrased version of the article x′."
  - [corpus] No direct evidence found in corpus neighbors about semantic perturbation testing.

### Mechanism 3
- **Claim:** The performance drop after relevance paraphrasing indicates that LLMs are not consistent summarizers and need further improvements for robustness.
- **Mechanism:** By measuring the change in evaluation metrics (ROUGE, BertScore) between original and paraphrased articles, we can assess the LLM's robustness at summarization.
- **Core assumption:** A robust LLM should maintain similar performance on semantically equivalent inputs.
- **Evidence anchors:**
  - [abstract] "Our results indicate that LLMs are not consistent summarizers for the minimally perturbed articles, necessitating further improvements."
  - [section 3.1] "summarization performance drops significantly after relevance paraphrasing for all LLMs."
  - [corpus] No direct evidence found in corpus neighbors about LLM consistency measurement.

## Foundational Learning

- **Concept: Semantic similarity metrics (e.g., BertScore, ROUGE)**
  - Why needed here: To measure the semantic similarity between original and paraphrased sentences, and to evaluate summary quality.
  - Quick check question: How does BertScore differ from ROUGE in measuring semantic similarity?

- **Concept: Zero-shot learning**
  - Why needed here: The LLMs are used without fine-tuning on the summarization datasets, testing their generalization ability.
  - Quick check question: What are the advantages and disadvantages of zero-shot learning compared to fine-tuning?

- **Concept: Prompt engineering**
  - Why needed here: The LLMs are prompted with specific instructions to generate summaries, and prompt quality affects output consistency.
  - Quick check question: How does temperature setting affect the stochasticity of LLM outputs?

## Architecture Onboarding

- **Component map:** Article + Gold Summary -> Relevance Mapping (ψ) -> Paraphrasing Model (θ) -> LLM Summarizer -> Evaluation Metrics

- **Critical path:**
  1. Load dataset and preprocess articles/summaries
  2. Apply relevance mapping to identify important sentences
  3. Generate paraphrased versions using LLM paraphraser
  4. Run LLM summarizer on both original and paraphrased articles
  5. Evaluate and compare performance metrics

- **Design tradeoffs:**
  - Using TF-IDF vs. more complex similarity metrics for relevance mapping (computational efficiency vs. accuracy)
  - Paraphrasing only relevant sentences vs. more sentences (minimal perturbation vs. comprehensive testing)
  - Using multiple evaluation metrics vs. focusing on one (comprehensive assessment vs. simplicity)

- **Failure signatures:**
  - No performance difference between original and paraphrased articles (indicates robustness but may also indicate insensitivity)
  - Significant performance drop across all metrics (indicates lack of consistency)
  - Inconsistent results across different evaluation metrics (indicates evaluation metric limitations)

- **First 3 experiments:**
  1. Run relevance paraphrasing on a small subset of data and visualize sentence distribution changes
  2. Compare performance using different similarity metrics for relevance mapping
  3. Test robustness on a dataset with single-sentence summaries vs. multi-sentence summaries

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the robustness of LLMs at summarization change when using different paraphrasing models or strategies beyond the relevance paraphrasing approach presented in the paper?
- **Basis in paper:** [explicit] The paper mentions that other paraphrasing models like Pegasus Paraphrase and chatgpt paraphraser on T5 base were investigated, but Llama-2 13B was ultimately chosen due to better output quality.
- **Why unresolved:** The paper only compares the performance of Llama-2 13B with a limited set of other paraphrasing models and does not explore a wide range of paraphrasing strategies or models to determine the most effective approach for assessing LLM summarization robustness.
- **What evidence would resolve it:** Conducting a comprehensive comparison of various paraphrasing models and strategies, including both neural and non-neural approaches, to evaluate their impact on LLM summarization robustness and identify the most effective methods.

### Open Question 2
- **Question:** How does the robustness of LLMs at summarization vary across different domains or types of documents, such as medical records, legal documents, or scientific papers, compared to the news articles and Reddit posts used in the paper?
- **Basis in paper:** [inferred] The paper mentions that future work should assess summarization robustness in the context of long-form documents and low-resource languages and domains, indicating that the current study is limited to specific types of documents.
- **Why unresolved:** The paper focuses on four specific datasets (CNN/DM, XSum, Reddit, and News) and does not explore the robustness of LLMs at summarizing other types of documents that may have different characteristics and challenges.
- **What evidence would resolve it:** Conducting experiments on a diverse set of document types and domains to compare the robustness of LLMs at summarization across different contexts and identify potential variations in performance.

### Open Question 3
- **Question:** How does the robustness of LLMs at summarization change over time as the models are updated or fine-tuned, and how can this be measured and tracked longitudinally?
- **Basis in paper:** [explicit] The paper mentions that for closed-source models like GPT-3.5Turbo, a longitudinal analysis of summarization robustness needs to be undertaken, as model performance can change over time.
- **Why unresolved:** The paper does not provide any longitudinal analysis of LLM summarization robustness, and it is unclear how the robustness of these models evolves as they are updated or fine-tuned.
- **What evidence would resolve it:** Conducting a series of experiments over an extended period, tracking the performance of LLMs at summarization on the same datasets and using the same evaluation metrics, to measure changes in robustness and identify potential trends or patterns.

## Limitations
- The study doesn't control for potential quality variations in the paraphrasing model itself, which could independently affect LLM performance
- The paper observes that LLMs use different sentences after paraphrasing but doesn't distinguish between specific failure mechanisms
- The minimal perturbation assumption relies heavily on the quality of the Llama-2-13B paraphraser, which isn't independently validated for semantic preservation across all datasets

## Confidence
- **Confidence: Medium** for the claim that LLMs are not consistent summarizers based on performance drops
- **Confidence: Low** regarding the interpretability of which specific mechanisms cause the performance drops
- **Confidence: High** in the methodological soundness of the relevance paraphrasing approach as a perturbation technique

## Next Checks
1. **Paraphrasing Quality Validation**: Compute semantic similarity (BERTScore) between original and paraphrased sentences to verify the >0.9 threshold is consistently met across all datasets
2. **Control Experiment with Random Perturbations**: Run the same evaluation pipeline but randomly select sentences for paraphrasing instead of relevance-based selection
3. **Sentence Selection Analysis**: Track which specific sentences from the original article are used in generated summaries before and after paraphrasing