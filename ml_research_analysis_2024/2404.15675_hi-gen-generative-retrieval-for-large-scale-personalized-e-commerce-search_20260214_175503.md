---
ver: rpa2
title: 'Hi-Gen: Generative Retrieval For Large-Scale Personalized E-commerce Search'
arxiv_id: '2404.15675'
source_url: https://arxiv.org/abs/2404.15675
tags:
- hi-gen
- retrieval
- information
- learning
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Hi-Gen, a generative retrieval (GR) method
  for large-scale personalized E-commerce search. It addresses two key challenges
  in applying GR to this domain: (1) existing docID generation methods ignore the
  encoding of efficiency information, which is critical in E-commerce, and (2) the
  positional information is important in decoding docIDs, while prior studies have
  not adequately discriminated the significance of positional information or well
  exploited the inherent interrelation among these positions.'
---

# Hi-Gen: Generative Retrieval For Large-Scale Personalized E-commerce Search
## Quick Facts
- arXiv ID: 2404.15675
- Source URL: https://arxiv.org/abs/2404.15675
- Reference count: 40
- 3.30% and 4.62% improvements over SOTA for Recall@1 on public and industry datasets

## Executive Summary
Hi-Gen is a generative retrieval method designed specifically for large-scale personalized E-commerce search. It addresses two key challenges in applying generative retrieval to e-commerce: the need to encode efficiency information alongside semantic relevance, and the importance of positional information in docID decoding. The method combines representation learning with metric learning to capture both semantic relevance and efficiency information, then uses a category-guided hierarchical clustering scheme to generate docIDs that encode both types of information. A position-aware loss function is designed to discriminate the importance of positions and exploit interrelation between tokens at the same position.

## Method Summary
Hi-Gen employs a representation learning model with metric learning to learn discriminative feature representations of items, capturing both semantic relevance and efficiency information critical in e-commerce. It then uses category-guided hierarchical clustering to generate docIDs that encode semantic and efficiency information in a structured way. The method introduces a position-aware loss function to discriminate the importance of positional information and mine inherent interrelation between different tokens at the same position during the decoding stage. This approach aims to improve the performance of the language model used in docID generation for personalized e-commerce search scenarios.

## Key Results
- Achieves 3.30% improvement over SOTA for Recall@1 on public dataset
- Achieves 4.62% improvement over SOTA for Recall@1 on industry dataset
- Demonstrates effectiveness of combining semantic relevance with efficiency information encoding

## Why This Works (Mechanism)
The method works by addressing the fundamental challenge that e-commerce search requires not just semantic relevance but also efficiency considerations (like price, popularity, and inventory). By using metric learning to capture these dual aspects and hierarchical clustering to structure the docID generation, Hi-Gen creates more informative and discriminative identifiers. The position-aware loss function further enhances the language model's ability to decode these structured identifiers by properly weighting positional information and capturing token relationships within positions.

## Foundational Learning
- **Metric Learning**: Used to learn discriminative feature representations that capture both semantic relevance and efficiency information; needed because standard semantic similarity is insufficient for e-commerce where efficiency factors matter; quick check: ensure learned representations preserve both types of information in embedding space
- **Hierarchical Clustering**: Guides docID generation to encode structured information; needed to create meaningful, organized identifiers that reflect category relationships and efficiency metrics; quick check: verify clustering creates meaningful groupings at each level
- **Position-Aware Loss**: Discriminates positional importance and token interrelation; needed because positional information is critical for accurate docID decoding but prior methods didn't adequately handle it; quick check: confirm loss function properly weights different positions during training

## Architecture Onboarding
Component map: Item Representation -> Metric Learning -> Hierarchical Clustering -> DocID Generation -> Position-Aware Loss -> Language Model
Critical path: The sequence from item representation through metric learning to hierarchical clustering forms the core of the feature engineering pipeline, while the position-aware loss and language model handle the decoding stage.
Design tradeoffs: Uses handcrafted hierarchical clustering rules rather than learned structures, trading adaptability for control and interpretability; employs additional position-aware loss adding training complexity but potentially improving decoding accuracy.
Failure signatures: Poor performance on cold-start items or tail queries; degradation when category structures don't align with clustering rules; computational overhead from position-aware loss during training.
First experiments: 1) Test metric learning on held-out efficiency metrics to verify dual objective capture, 2) Evaluate clustering quality by measuring intra-cluster similarity vs inter-cluster differences, 3) Ablation test of position-aware loss vs standard loss to quantify contribution.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on handcrafted hierarchical clustering rules raises scalability concerns across different e-commerce domains with varying category structures
- Evaluation focuses primarily on recall metrics without reporting precision or ranking quality measures critical for practical search systems
- Position-aware loss function may introduce computational overhead during training without clear evidence of its relative contribution

## Confidence
- **High confidence**: The general approach of combining metric learning with hierarchical clustering is technically sound and addresses real challenges in e-commerce retrieval
- **Medium confidence**: The reported performance improvements are substantial, but the specific contributions of individual components (position-aware loss vs. hierarchical clustering) are not clearly isolated
- **Medium confidence**: The scalability claims are reasonable but not empirically validated beyond the reported datasets

## Next Checks
1. Conduct ablation studies specifically isolating the impact of the position-aware loss function from the hierarchical clustering improvements to quantify their individual contributions
2. Evaluate precision@K and NDCG metrics in addition to recall to assess ranking quality, as e-commerce search typically prioritizes precision in top results
3. Test the method's performance on cold-start items and long-tail queries to validate robustness across different query and item distributions