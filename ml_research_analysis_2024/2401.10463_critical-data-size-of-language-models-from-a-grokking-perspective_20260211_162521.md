---
ver: rpa2
title: Critical Data Size of Language Models from a Grokking Perspective
arxiv_id: '2401.10463'
source_url: https://arxiv.org/abs/2401.10463
tags:
- data
- grokking
- training
- size
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the role of data size in language model
  training by formalizing a "Data Efficiency Hypothesis" that identifies critical
  dataset sizes triggering phase transitions from memorization to generalization.
  The authors develop a grokking configuration by rescaling initialization and weight
  decay to stably reproduce grokking on language tasks like IMDB and Yelp, which is
  challenging with standard training.
---

# Critical Data Size of Language Models from a Grokking Perspective

## Quick Facts
- **arXiv ID**: 2401.10463
- **Source URL**: https://arxiv.org/abs/2401.10463
- **Reference count**: 40
- **Primary result**: Language models exhibit distinct learning regimes (memorization, delayed generalization, concurrent) that depend on critical dataset size thresholds

## Executive Summary
This paper investigates the role of data size in language model training by formalizing a "Data Efficiency Hypothesis" that identifies critical dataset sizes triggering phase transitions from memorization to generalization. The authors develop a grokking configuration by rescaling initialization and weight decay to stably reproduce grokking on language tasks like IMDB and Yelp, which is challenging with standard training. Through extensive experiments, they demonstrate that language models exhibit distinct regimes: data insufficiency (only memorization), sufficiency (delayed generalization/grokking), and surplus (concurrent memorization and generalization). They verify that critical data size increases with model size, meaning larger models require more data for effective generalization. The study also provides insights into the learning mechanism through visualization of weight dynamics during training. Results show smoother phase transitions in real language datasets compared to synthetic tasks, and suggest that grokking is a fundamental phenomenon obscured by modern training practices but observable through careful experimental design.

## Method Summary
The authors formalize a "Data Efficiency Hypothesis" that posits a critical data size threshold triggering phase transitions in language model training. They develop a "grokking configuration" by rescaling initialization parameters and using weight decay to stably reproduce grokking behavior on language tasks. The methodology involves uniform data pruning to create controlled dataset sizes, training 1-layer transformer models with rescaled parameters (factor α) and weight decay, and tracking step-wise accuracy to identify phase transitions. They analyze parameter dynamics (L2 norm, weight distributions) to understand the underlying learning mechanism. Experiments are conducted on IMDB and Yelp sentiment classification tasks with varying model sizes (hidden layers 16-256) and dataset fractions.

## Key Results
- Language models exhibit three distinct learning regimes based on dataset size: data insufficiency (only memorization), sufficiency (delayed generalization/grokking), and surplus (concurrent memorization and generalization)
- Critical data size increases with model size, indicating larger models require more data for effective generalization
- Grokking phase transitions occur more smoothly in real language datasets compared to synthetic tasks
- The grokking configuration (rescaled initialization and weight decay) successfully stabilizes delayed generalization on complex language tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: There exists a critical data size that triggers a phase transition from memorization to generalization in language models.
- Mechanism: When dataset size n is below CDS(D, M), the model can only memorize training data without generalizing to test data. At CDS(D, M), the model enters a delayed generalization phase where generalization occurs after memorization completes. Beyond CDS(D, M), memorization and generalization occur concurrently.
- Core assumption: The learning process can be characterized by distinct regimes based on dataset size relative to a critical threshold.
- Evidence anchors:
  - [abstract] "We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization."
  - [section 2] "We introduce the concept of Critical Data Size (CDS) for grokking-related model training... The CDS is: CDS(D, M) := min {n | ES∼Dn [AccS(M)] ≥ ϵ}"
  - [corpus] Weak - the corpus contains related work on grokking but no direct evidence for the specific critical data size hypothesis
- Break condition: If models can achieve generalization without first completing memorization, or if generalization occurs smoothly without distinct phase transitions.

### Mechanism 2
- Claim: Larger models require more data to achieve effective generalization.
- Mechanism: As model size increases, the critical data size increases proportionally. This means larger models need proportionally larger datasets to reach the same level of generalization performance.
- Core assumption: Model capacity and data requirements are directly correlated in a predictable way.
- Evidence anchors:
  - [abstract] "Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data."
  - [section 5] "Figure 4A... a clear trend is observed: as the hidden layer size increases from 16 to 256, the area of low accuracy (≤ 0.8) gradually increases... This suggests a threshold beyond which increasing the model complexity does not yield proportional gains in performance."
  - [corpus] Moderate - corpus contains related work on neural scaling laws but not specific evidence for this particular claim about critical data size scaling
- Break condition: If smaller models can achieve the same performance with the same data, or if model size has no effect on critical data requirements.

### Mechanism 3
- Claim: The grokking configuration (rescaled initialization and weight decay) can stably reproduce grokking on language tasks.
- Mechanism: By scaling parameters initialization by factor α and using appropriate weight decay, the model's learning dynamics are slowed down sufficiently to observe the delayed generalization characteristic of grokking, even on complex language datasets.
- Core assumption: Grokking is a fundamental phenomenon that can be observed across different tasks and architectures when training dynamics are appropriately controlled.
- Evidence anchors:
  - [abstract] "We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay."
  - [section 3] "In the grokking configuration, each weight of models is rescaled by a factor α... we optimize rescaled models w by cross-entropy loss and weight decay."
  - [corpus] Moderate - corpus contains related work on inducing grokking but limited evidence for stability across language tasks
- Break condition: If the configuration fails to induce grokking on other language tasks, or if grokking disappears with slight modifications to initialization or weight decay.

## Foundational Learning

- Concept: Phase transitions in learning dynamics
  - Why needed here: The paper's central thesis relies on identifying distinct learning regimes (memorization-only, delayed generalization, concurrent memorization-generalization) that represent phase transitions.
  - Quick check question: Can you describe what distinguishes the "data insufficiency," "data sufficiency," and "data surplus" regimes in terms of model behavior?

- Concept: Weight decay and regularization effects
  - Why needed here: The mechanism for inducing grokking relies on understanding how weight decay drives generalization after memorization completes, when cross-entropy loss gradients approach zero.
  - Quick check question: Why does weight decay become the primary driver of optimization after the training loss approaches zero?

- Concept: Neural scaling laws and model capacity
  - Why needed here: The paper's finding that larger models require more data connects to scaling laws, but reveals an inverse relationship in their specific grokking configuration.
  - Quick check question: How does the relationship between model size and data requirements in this paper differ from conventional neural scaling laws?

## Architecture Onboarding

- Component map:
  - Data pipeline: uniform data pruning to create controlled dataset sizes
  - Model architecture: 1-layer encoder-only or decoder-only Transformers with configurable hidden sizes and layer counts
  - Training loop: AdamW optimizer with configurable learning rate, weight decay, and rescaled initialization
  - Evaluation: Step-wise accuracy tracking for both training and test sets to capture phase transitions

- Critical path:
  1. Prepare dataset with uniform pruning across different fractions
  2. Initialize model with rescaled parameters (α factor)
  3. Train with weight decay until convergence or specific accuracy thresholds
  4. Track accuracy step-wise to identify phase transition points
  5. Analyze parameter dynamics (L2 norm, weight distributions) to understand mechanism

- Design tradeoffs:
  - Simpler models (1-layer) vs. more complex models (multi-layer): Simpler models more reliably show grokking but may not generalize as well
  - Dataset size vs. computational cost: Larger datasets provide more data points for analysis but increase training time significantly
  - Weight decay strength vs. convergence speed: Higher weight decay speeds convergence but may obscure phase transitions

- Failure signatures:
  - No clear phase transition in accuracy curves (smooth generalization without distinct memorization phase)
  - Immediate generalization without memorization phase (indicates configuration is too strong)
  - De-grokking (memorization and generalization occurring simultaneously from the start)
  - Numerical instability in parameter rescaling (α too large)

- First 3 experiments:
  1. Reproduce modular addition grokking with standard configuration (α=10, weight decay=1e-2) to validate setup
  2. Apply uniform data pruning to IMDB dataset and train with same configuration to observe phase transitions
  3. Vary hidden size from 16 to 256 on IMDB with fixed dataset size to test critical data size scaling hypothesis

## Open Questions the Paper Calls Out
None

## Limitations
- The experimental setup relies on a specific "grokking configuration" with rescaled initialization and weight decay, which may not reflect standard training practices
- The study focuses on relatively simple 1-layer transformer architectures, limiting generalizability to deeper, more complex models used in practice
- The observed inverse relationship between model size and data efficiency in the grokking configuration contradicts conventional neural scaling laws

## Confidence
- **High Confidence**: The empirical observation that dataset size influences generalization dynamics (memorization vs. generalization phases)
- **Medium Confidence**: The specific formulation of Critical Data Size (CDS) as a precise threshold and its scaling relationship with model size
- **Low Confidence**: The claim that larger models universally require more data for generalization, given the inverse relationship observed in the grokking configuration versus conventional scaling laws

## Next Checks
1. Replicate the critical data size experiments using standard training configurations (no rescaled initialization) to verify if phase transitions persist without the artificial setup
2. Extend experiments to multi-layer transformer architectures (2-6 layers) to assess how depth affects critical data size scaling
3. Test the critical data size hypothesis on diverse language tasks beyond sentiment classification (IMDB, Yelp) to evaluate generalizability across task types