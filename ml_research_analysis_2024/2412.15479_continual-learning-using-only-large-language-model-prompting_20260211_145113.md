---
ver: rpa2
title: Continual Learning Using Only Large Language Model Prompting
arxiv_id: '2412.15479'
source_url: https://arxiv.org/abs/2412.15479
tags:
- class
- summary
- learning
- language
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CLOB, a novel continual learning paradigm that
  treats large language models as black boxes and performs learning through verbal
  prompting alone. The key innovation is CIS (Continual Learning via Incremental Summarization),
  which uses the LLM's summarization ability to generate and incrementally update
  class summaries as new data arrives, addressing the token length limitation of LLMs.
---

# Continual Learning Using Only Large Language Model Prompting

## Quick Facts
- arXiv ID: 2412.15479
- Source URL: https://arxiv.org/abs/2412.15479
- Reference count: 30
- Primary result: CIS method achieves accuracy comparable to or better than batch learning while preventing catastrophic forgetting

## Executive Summary
This paper introduces CLOB, a novel continual learning paradigm that leverages large language models (LLMs) through verbal prompting alone, treating the LLM as a black box without parameter updates. The key innovation, CIS (Continual Learning via Incremental Summarization), uses the LLM's summarization capability to generate and incrementally update class summaries as new data arrives, addressing the token length limitations of LLMs. Experiments on four text classification datasets demonstrate that CIS achieves accuracy comparable to batch learning settings while significantly outperforming state-of-the-art continual learning baselines.

## Method Summary
CLOB operates by treating LLMs as black boxes and performing learning through verbal prompting alone. The core component, CIS, generates and incrementally updates class summaries using the LLM's summarization ability. When new data arrives, CIS updates existing summaries rather than storing all previous samples, effectively managing the token length constraint. This approach prevents catastrophic forgetting by maintaining comprehensive summaries of all previously learned classes while incorporating new information. The method requires zero parameter updates to the LLM, relying entirely on prompt engineering and the model's inherent capabilities.

## Key Results
- CIS achieves accuracy comparable to or better than batch learning where all training samples are seen simultaneously
- CIS significantly outperforms state-of-the-art baselines (EWC, LAMOL, VAG) by large margins
- The method approaches the performance of joint fine-tuning/prompting upper bounds while requiring zero parameter updates

## Why This Works (Mechanism)
The approach works by leveraging the LLM's natural summarization and reasoning capabilities to maintain a compact yet comprehensive representation of learned classes. Instead of storing raw data or updating parameters, CIS incrementally refines class summaries through verbal prompting. This allows the model to retain knowledge of previous classes while adapting to new ones within the token length constraints. The verbal nature of the learning process means the LLM can apply its pre-trained understanding to integrate new information with existing knowledge, effectively preventing catastrophic forgetting without any parameter modifications.

## Foundational Learning
- **Continual Learning**: The ability to learn new tasks sequentially without forgetting previous ones. Needed because real-world scenarios often involve streaming data. Quick check: Does performance degrade on previous tasks when learning new ones?
- **Catastrophic Forgetting**: The tendency of neural networks to rapidly lose performance on previous tasks when trained on new tasks. Critical to address in sequential learning. Quick check: Compare accuracy on old tasks before and after learning new tasks.
- **LLM Prompting**: The technique of guiding LLM behavior through carefully crafted text prompts rather than parameter updates. Enables black-box usage of LLMs. Quick check: Does the prompt structure effectively communicate the learning task?
- **Incremental Summarization**: The process of progressively refining summaries as new information arrives. Allows efficient memory usage within token limits. Quick check: Are the summaries comprehensive enough to maintain task performance?
- **Zero-parameter Learning**: Learning approaches that don't modify model weights, relying instead on input/output transformations. Preserves original model capabilities. Quick check: Are any model parameters updated during training?

## Architecture Onboarding

**Component Map**
CIS (prompt engineering) -> LLM (black box) -> Class Summaries (incremental updates) -> Classification Output

**Critical Path**
1. Initial class summary generation using LLM summarization
2. Incremental update of summaries with new class data
3. Classification using updated summaries as reference

**Design Tradeoffs**
- **Token Budget vs. Accuracy**: Tighter token constraints force more aggressive summarization, potentially losing detail
- **Summary Granularity vs. Memory**: More detailed summaries better preserve information but consume more tokens
- **Prompt Complexity vs. Robustness**: More sophisticated prompts may improve performance but reduce generalizability

**Failure Signatures**
- Degradation in accuracy on previously learned classes indicates forgetting
- Inconsistent classification outputs suggest summary corruption or insufficient detail
- High variance in results across runs may indicate sensitivity to prompt formulation

**3 First Experiments**
1. Test baseline accuracy on individual datasets before any continual learning occurs
2. Evaluate catastrophic forgetting by measuring performance drop on old classes after learning new ones
3. Compare token usage efficiency between CIS and baseline methods to validate memory management claims

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Reliance on LLM summarization capabilities may not generalize to all domains or languages
- Token budget of 2048 limits applicability to tasks requiring longer contexts
- Experimental scope limited to text classification tasks with modest dataset sizes
- Potential for error accumulation over many learning sessions not explicitly evaluated

## Confidence
- High confidence: CIS outperforming EWC, LAMOL, and VAG baselines
- Medium confidence: CIS achieving accuracy comparable to batch learning
- Low confidence: Effectiveness on tasks beyond text classification domain

## Next Checks
1. Evaluate CIS on diverse tasks including question answering and generation to assess generalizability beyond classification
2. Test with larger token budgets (8k or 16k) to understand performance scaling and error accumulation
3. Conduct ablation studies to isolate contributions of prompting strategy, incremental summarization, and forgetting mitigation components