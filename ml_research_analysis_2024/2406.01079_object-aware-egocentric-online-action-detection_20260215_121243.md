---
ver: rpa2
title: Object Aware Egocentric Online Action Detection
arxiv_id: '2406.01079'
source_url: https://arxiv.org/abs/2406.01079
tags:
- object
- action
- egocentric
- module
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of online action detection (OAD)
  in egocentric videos, which have unique characteristics compared to traditional
  exocentric videos. The authors propose an Object-Aware Module that integrates object
  information into existing OAD frameworks to enhance their performance on egocentric
  video data.
---

# Object Aware Egocentric Online Action Detection

## Quick Facts
- arXiv ID: 2406.01079
- Source URL: https://arxiv.org/abs/2406.01079
- Authors: Joungbin An; Yunsu Park; Hyolim Kang; Seon Joo Kim
- Reference count: 23
- One-line primary result: Object-aware module improves online action detection in egocentric videos, achieving up to +10.8% verb accuracy and +4.0% action accuracy

## Executive Summary
This paper addresses online action detection (OAD) in egocentric videos by introducing an Object-Aware Module that integrates object information into existing OAD frameworks. The module leverages an off-the-shelf object detector and transformer-based cross-attention to effectively combine object and temporal cues. Extensive experiments on the Epic-Kitchens-100 dataset demonstrate consistent performance improvements across multiple state-of-the-art OAD models when integrated with this module.

## Method Summary
The authors propose an Object-Aware Module that extracts object information using Faster R-CNN, integrates it with temporal cues through a transformer-based architecture, and refines the combined information for action classification. The module is designed to be compatible with existing OAD frameworks like TeSTra, MAT, and MiniROAD. It uses learnable query vectors to attend to detected objects and cross-attend with temporal cues from the OAD model. The system processes video snippets at 24 fps using a TSN model pretrained on Kinetics-400, with object detection applied to the last frame of each snippet.

## Key Results
- Achieves up to +10.8% improvement in verb accuracy across different OAD models
- Shows up to +4.0% improvement in action accuracy when integrated
- Outperforms simple concatenation of object features with RGB inputs
- Demonstrates consistent gains across all metrics (Verb, Noun, Action accuracy)

## Why This Works (Mechanism)

### Mechanism 1
Integrating object information through the Object-Aware Module improves verb classification accuracy more than action classification accuracy. The module uses learnable query vectors to attend to detected objects and cross-attend with temporal cues from the OAD model, better capturing object-scene relationships that are strong predictors of verb labels. This works because object presence and interaction patterns are more discriminative for verbs than for fine-grained action labels. If objects are not relevant to the action being performed, or if object detection fails consistently, verb accuracy gains will diminish.

### Mechanism 2
The Object-Aware Module's transformer-based integration is more effective than simple concatenation of object features with RGB inputs. The module employs two transformer layers—first to fuse object info into learnable queries, then to cross-attend these queries with temporal cues—allowing structured, attention-based integration rather than naive concatenation. This works because attention-based fusion can better capture the relevance and context of objects for action prediction. If the object detector is highly accurate and object presence is always directly relevant, naive concatenation might approach module-level performance.

### Mechanism 3
Leveraging object information as contextual priors improves action detection in egocentric videos more than in exocentric videos. Egocentric videos lack explicit pose information due to camera positioning, making object interactions the primary cue for action recognition. The Object-Aware Module compensates by emphasizing object-scene context. This works because egocentric video actions are heavily object-driven, and pose cues are insufficient. If pose estimation is added or becomes reliable, the relative importance of object cues may decrease.

## Foundational Learning

- **Concept**: Object detection with Faster R-CNN
  - Why needed here: To extract object presence and confidence scores as contextual features for the Object-Aware Module
  - Quick check question: What is the output format of Faster R-CNN when applied to a frame?

- **Concept**: Transformer cross-attention
  - Why needed here: To merge object-aware queries with temporal cues from the OAD model, enabling joint object-temporal reasoning
  - Quick check question: In a cross-attention layer, which input serves as the query and which as the key/value?

- **Concept**: Online action detection (OAD)
  - Why needed here: The module is designed to be integrated into OAD models, which must predict actions without future frames
  - Quick check question: What is the main difference between online and offline action detection?

## Architecture Onboarding

- **Component map**: OAD model (e.g., MiniROAD, TeSTra, MAT) -> Object-Aware Module -> Classifier (Verb/Noun/Action)
- **Critical path**: Frame -> Faster R-CNN (object detection) -> Object vector aggregation -> Object-Aware Module (query update + cross-attention) -> Max pooling -> Classification
- **Design tradeoffs**: Using object detection adds computation but improves accuracy; using a lightweight detector keeps overhead low. The module can be swapped in/out without retraining the entire OAD model.
- **Failure signatures**: Degraded verb accuracy if object detector misses critical objects; action accuracy may plateau if object cues are redundant with existing temporal cues.
- **First 3 experiments**:
  1. Baseline OAD model without module -> measure Verb/Noun/Action accuracy
  2. Add object info via concatenation -> compare to baseline
  3. Integrate Object-Aware Module -> measure accuracy gains and confirm module impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for identifying and incorporating only "active" objects that the user is directly interacting with, rather than all objects present in the scene?
- Basis in paper: [explicit] The authors mention this as a limitation, stating "We are currently working on using only the 'active' objects that the user is actually interacting with, instead of all the objects present in the scene."
- Why unresolved: The current approach uses object detection scores for all objects in the scene, which may include irrelevant objects that do not contribute to action understanding.
- What evidence would resolve it: Experimental results comparing the current approach with a method that selectively incorporates only active objects, demonstrating improved performance metrics.

### Open Question 2
- Question: How can spatial information about active objects be effectively incorporated into the Object-Aware Module beyond just objectness scores?
- Basis in paper: [explicit] The authors state "instead of a naïve objectness score, we are experimenting to incorporate the active object's spatial information."
- Why unresolved: The current implementation uses object detection scores without considering the spatial relationship between the user's hands and the objects.
- What evidence would resolve it: Comparative experiments showing performance improvements when spatial information (e.g., hand-object distance, relative positioning) is incorporated alongside object detection scores.

### Open Question 3
- Question: How well does the Object-Aware Module generalize to other egocentric video datasets beyond Epic-Kitchens-100, such as Ego4D and Ego-Exo4D?
- Basis in paper: [explicit] The authors mention "We plan to extend this framework to more diverse datasets, including Ego4D and Ego-Exo4D" as a limitation.
- Why unresolved: The current validation is limited to a single dataset, and performance may vary across datasets with different characteristics, action distributions, and video qualities.
- What evidence would resolve it: Experimental results demonstrating consistent performance improvements across multiple egocentric video datasets with varying characteristics and annotation schemes.

## Limitations

- Performance gains are demonstrated primarily on the Epic-Kitchens-100 dataset, which may limit generalizability to other egocentric datasets
- The computational overhead of adding the Object-Aware Module is not thoroughly quantified
- The specific configurations of the learnable query vectors and hyperparameters for integration are not fully specified

## Confidence

- **High confidence**: The Object-Aware Module consistently improves performance across multiple OAD models when integrated (supported by experimental results)
- **Medium confidence**: The claim that object information is more important for verbs than actions in egocentric videos (based on ablation studies but limited theoretical backing)
- **Medium confidence**: The superiority of the transformer-based integration over simple concatenation (based on internal comparisons but lacking direct comparison to other fusion methods)

## Next Checks

1. Test the Object-Aware Module on additional egocentric datasets (e.g., ActivityNet, CharadesEgo) to verify generalizability
2. Quantify the computational overhead (inference time, memory usage) of the Object-Aware Module compared to baseline OAD models
3. Compare the Object-Aware Module's fusion approach against alternative methods (e.g., attention-based concatenation, graph-based fusion) to establish its relative effectiveness