---
ver: rpa2
title: Kolmogorov-Arnold Networks are Radial Basis Function Networks
arxiv_id: '2405.06721'
source_url: https://arxiv.org/abs/2405.06721
tags:
- functions
- basis
- radial
- fastkan
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper shows that Kolmogorov-Arnold Networks (KANs) using\
  \ 3rd-order B-splines can be approximated by Gaussian radial basis functions, leading\
  \ to FastKAN\u2014a significantly faster implementation. The core idea is to replace\
  \ B-spline calculations with Gaussian RBFs, simplifying the computation and removing\
  \ efficiency bottlenecks."
---

# Kolmogorov-Arnold Networks are Radial Basis Function Networks

## Quick Facts
- arXiv ID: 2405.06721
- Source URL: https://arxiv.org/abs/2405.06721
- Reference count: 8
- Primary result: FastKAN achieves 3.33x speedup over optimized KAN with maintained accuracy on MNIST

## Executive Summary
This paper demonstrates that Kolmogorov-Arnold Networks (KANs) using 3rd-order B-splines can be approximated by Gaussian radial basis functions, leading to FastKAN—a significantly faster implementation. The core insight is that KANs are effectively fixed-center RBF networks, and replacing B-splines with Gaussian RBFs simplifies computation while preserving expressiveness. On NVIDIA V100 GPUs, FastKAN accelerates forward computation by 3.33x compared to an optimized KAN implementation while maintaining or slightly improving accuracy on MNIST classification.

## Method Summary
The method replaces 3rd-order B-spline calculations in KANs with Gaussian radial basis functions, using layer normalization to maintain inputs within the effective RBF domain. FastKAN uses 8 fixed centers per input dimension and learned weights, reducing computational complexity while preserving the network's ability to approximate functions. The implementation is benchmarked against an optimized KAN (efficient_kan) on MNIST classification with identical architecture [28×28, 64, 10] over 20 epochs.

## Key Results
- FastKAN achieves 3.33x speedup in forward computation on NVIDIA V400 GPUs compared to optimized KAN
- MNIST classification accuracy is maintained or slightly improved with FastKAN versus traditional KAN
- The paper establishes that KANs are effectively RBF networks with fixed centers, enabling computational optimizations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing 3rd-order B-splines with Gaussian RBFs preserves functional expressiveness while reducing computational complexity
- Mechanism: Gaussian RBFs can approximate 3rd-order B-spline bases through linear transformations, maintaining mathematical expressiveness with simpler computation
- Core assumption: The linear transformation relationship between B-splines and Gaussian RBFs holds across relevant input domains
- Evidence anchors: Abstract and section 3 demonstrate visual alignment of B-spline bases with Gaussian RBFs through linear transformations

### Mechanism 2
- Claim: Layer normalization prevents input values from shifting outside the effective domain of RBFs
- Mechanism: By normalizing layer inputs, the network maintains values within the region where Gaussian RBFs provide good approximation to B-splines
- Core assumption: Layer normalization effectively constrains input distributions to stay within the operational range of Gaussian RBFs
- Evidence anchors: Abstract and section 2 explicitly state layer normalization prevents inputs from shifting away from the RBF domain

### Mechanism 3
- Claim: Fixed centers in RBF networks provide sufficient flexibility for function approximation while enabling computational optimization
- Mechanism: Using a fixed set of Gaussian centers reduces parameter space and computational overhead while maintaining approximation capability through learned weights
- Core assumption: A fixed grid of centers provides adequate coverage of the input space for target function approximation tasks
- Evidence anchors: Abstract concludes KANs are RBF networks with fixed centers; section 4.1 specifies 8 centers for Gaussian RBFs

## Foundational Learning

- Concept: B-spline basis functions and their recursive construction via de Boor-Cox algorithm
  - Why needed here: Understanding why B-splines are computationally expensive and why they need replacement
  - Quick check question: What is the time complexity of computing B-spline basis functions using the de Boor-Cox algorithm versus evaluating a Gaussian RBF?

- Concept: Radial basis function networks and their universal approximation properties
  - Why needed here: Provides theoretical foundation for why RBFs can replace B-splines while maintaining function approximation capability
  - Quick check question: Under what conditions can RBF networks approximate any continuous function on a compact set?

- Concept: Layer normalization and its effect on activation distributions
  - Why needed here: Explains how normalization prevents inputs from leaving the effective domain of Gaussian RBFs
  - Quick check question: How does layer normalization affect the mean and variance of layer inputs, and why is this important for RBF-based networks?

## Architecture Onboarding

- Component map: Input → Layer Normalization → Gaussian RBF layer (fixed centers, learned weights) → Output transformation → Next layer (repeatable structure)
- Critical path: Forward pass through RBF layer is the bottleneck being optimized; backward pass complexity is also reduced through RBF simplification
- Design tradeoffs: Fixed centers reduce flexibility but enable 3.33x speedup; Gaussian RBFs may have different approximation characteristics than B-splines in edge cases
- Failure signatures: Accuracy degradation on functions requiring highly localized approximations; instability when inputs frequently hit normalization boundaries
- First 3 experiments:
  1. Replace B-spline layer with Gaussian RBFs in a simple 1D function approximation task and compare approximation error
  2. Test layer normalization parameters (scale and shift) to find optimal settings for maintaining input distribution within RBF effective domain
  3. Benchmark forward and backward pass times for equivalent KAN and FastKAN architectures on a small classification task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical basis for using Gaussian RBFs as an approximation to 3rd-order B-splines in KANs?
- Basis in paper: [explicit] The paper claims that linear transformations can align 3rd-order B-spline bases with Gaussian RBFs with good precision, as shown in Figure 1
- Why unresolved: The paper provides a visual demonstration but lacks rigorous mathematical proof or error bounds for the approximation
- What evidence would resolve it: A formal mathematical proof showing the error bounds and conditions under which Gaussian RBFs approximate 3rd-order B-splines with sufficient accuracy for practical applications

### Open Question 2
- Question: How does the choice of the number of centers (8 in the experiments) affect the accuracy and efficiency of FastKAN compared to traditional KANs?
- Basis in paper: [inferred] The paper mentions using 8 centers for Gaussian RBFs to align with the number of parameters in the traditional KAN implementation, but does not explore the impact of varying this number
- Why unresolved: The paper does not provide experiments or analysis on how different numbers of centers influence the model's performance or computational efficiency
- What evidence would resolve it: Experimental results comparing FastKAN's accuracy and speed with varying numbers of centers, along with an analysis of the trade-offs involved

### Open Question 3
- Question: What are the implications of KANs being effectively fixed-center RBF networks for their ability to generalize and model complex functions?
- Basis in paper: [explicit] The paper concludes that KANs are RBF networks with fixed centers, suggesting a potential limitation in their flexibility compared to traditional RBF networks where centers can be learned
- Why unresolved: The paper does not explore how the fixed-center nature of KANs impacts their modeling capacity or generalization ability, especially in comparison to other neural network architectures
- What evidence would resolve it: Comparative studies of KANs' performance on various tasks against other neural network architectures, focusing on generalization and the ability to model complex functions

## Limitations

- Approximation quality between B-splines and Gaussian RBFs is shown only for specific cases without theoretical bounds on approximation error across diverse function classes
- Layer normalization effectiveness in preventing input distribution shifts lacks empirical validation under various training scenarios
- Fixed-center approach may limit the network's ability to learn highly localized features that adaptive center placement could capture

## Confidence

- **High Confidence:** Computational speedup claims (3.33x) are directly measurable and supported by timing benchmarks on specified hardware
- **Medium Confidence:** Accuracy preservation claim is supported by MNIST results but limited to one dataset and architecture; generalization to other tasks remains untested
- **Medium Confidence:** Theoretical foundation for RBF approximation of B-splines is plausible but not rigorously proven for all relevant function classes

## Next Checks

1. **Approximation Error Analysis:** Systematically measure the approximation error between 3rd-order B-splines and Gaussian RBFs across a range of 1D functions, including discontinuities and sharp transitions, to establish error bounds

2. **Layer Normalization Stability:** Monitor input distributions before and after layer normalization during training to empirically verify that inputs remain within the effective domain of Gaussian RBFs

3. **Generalization Testing:** Evaluate FastKAN on multiple datasets (e.g., CIFAR-10, Fashion-MNIST) and network depths to assess whether accuracy preservation holds beyond the MNIST experiment