---
ver: rpa2
title: 'Generative artificial intelligence in ophthalmology: multimodal retinal images
  for the diagnosis of Alzheimer''s disease with convolutional neural networks'
arxiv_id: '2406.18247'
source_url: https://arxiv.org/abs/2406.18247
tags:
- images
- synthetic
- were
- real
- amyloidpet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of predicting Alzheimer's disease
  using multimodal retinal imaging by leveraging generative AI to augment small medical
  imaging datasets. The authors developed a Denoising Diffusion Probabilistic Model
  (DDPM) to generate synthetic retinal images across four modalities (OCTA-SMAC, OCT-BONH,
  OCT-BMAC, and FAF), demonstrating that these synthetic images were diverse and did
  not memorize training data.
---

# Generative artificial intelligence in ophthalmology: multimodal retinal images for the diagnosis of Alzheimer's disease with convolutional neural networks

## Quick Facts
- arXiv ID: 2406.18247
- Source URL: https://arxiv.org/abs/2406.18247
- Reference count: 40
- The best-performing multimodal classifier with metadata achieved AUPR of 0.634 for Amyloid PET status prediction

## Executive Summary
This study explores the application of generative AI to improve Alzheimer's disease diagnosis using multimodal retinal imaging. The researchers developed a Denoising Diffusion Probabilistic Model (DDPM) to generate synthetic retinal images across four modalities (OCTA-SMAC, OCT-BONH, OCT-BMAC, and FAF), addressing the challenge of small medical imaging datasets. The synthetic images demonstrated diversity and did not memorize training data. By pretraining convolutional neural networks on these synthetic images and combining unimodal predictions with patient metadata, the team achieved improved classification performance for Amyloid PET status prediction, with the best model reaching an AUPR of 0.634.

## Method Summary
The researchers trained conditional DDPMs to generate synthetic retinal images for each of four modalities, then applied a filter CNN to remove unrealistic synthetic images. They pretrained EfficientNet-B0 unimodal classifiers on these filtered synthetic images before finetuning on real data. A multimodal classifier was developed to fuse unimodal predictions with patient metadata through late heterogeneous fusion. The approach aimed to overcome limitations of small medical imaging datasets by leveraging synthetic data augmentation while integrating complementary information from multiple imaging modalities and patient demographics.

## Key Results
- DDPMs generated diverse, realistic synthetic retinal images that did not memorize training data
- Pretraining on synthetic data improved AUPR from 0.350 to 0.579 for some modalities
- Multimodal classifier with metadata integration achieved best performance with AUPR of 0.634

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generative AI can augment small medical imaging datasets by synthesizing diverse, realistic retinal images.
- **Mechanism:** Denoising Diffusion Probabilistic Models (DDPMs) iteratively remove noise from random images to generate realistic synthetic retinal images conditioned on AmyloidPET status.
- **Core assumption:** The small dataset has enough information to guide the generative model to produce meaningful, non-memorized images.
- **Evidence anchors:**
  - [abstract] "Denoising Diffusion Probabilistic Models (DDPMs) were trained to generate synthetic images...synthetic images were diverse and did not memorize training data."
  - [section] "DDPMs generated diverse, realistic images without memorization...We observed little memorization in the generated data as the SvR distributions did not reach close to 1."
- **Break Condition:** If the original dataset is too small or lacks diversity, the generative model may fail to capture essential features, leading to low-quality or overfitted synthetic images.

### Mechanism 2
- **Claim:** Pretraining CNNs on synthetic data can improve classification performance for AmyloidPET prediction.
- **Mechanism:** CNNs pretrained on synthetic multimodal retinal images and then finetuned on real data learn better feature representations than CNNs trained only on real data.
- **Core assumption:** The synthetic images contain sufficient information about retinal features relevant to Alzheimer's disease for the CNN to learn useful features.
- **Evidence anchors:**
  - [abstract] "pretraining convolutional neural networks (CNNs) on synthetic data could improve classification performance...Pretraining unimodal CNNs with synthetic data improved AUPR at most from 0.350 to 0.579."
  - [section] "Pretraining on synthetic data showed slight AUPR improvement for OCT-BONH, OCT-BMAC and FAF on the validation set, as well as for OCTA-SMAC and OCT-BMAC on the test set."
- **Break Condition:** If the synthetic images are not sufficiently realistic or diverse, pretraining may not improve and could even degrade performance.

### Mechanism 3
- **Claim:** Combining multimodal retinal imaging with patient metadata improves AmyloidPET prediction accuracy.
- **Mechanism:** Multimodal classifiers fuse predictions from unimodal CNNs with patient metadata through a fully connected network, capturing both imaging and demographic information.
- **Core assumption:** Patient metadata (age, gender) provides additional predictive information that complements the multimodal retinal image features.
- **Evidence anchors:**
  - [abstract] "Integration of metadata in multimodal CNNs improved AUPR from 0.486 to 0.634, which was the best overall best classifier."
  - [section] "By including metadata, the performance of the multimodal classifier improved from AUPR 0.486 (AUROC 0.622)."
- **Break Condition:** If metadata does not contain relevant information for AmyloidPET prediction or introduces noise, the performance improvement may not be observed.

## Foundational Learning

- **Concept:** Denoising Diffusion Probabilistic Models (DDPMs)
  - **Why needed here:** DDPMs are used to generate synthetic retinal images to augment the small dataset.
  - **Quick check question:** How does a DDPM generate images, and what is the role of noise in this process?

- **Concept:** Convolutional Neural Networks (CNNs) for medical image classification
  - **Why needed here:** CNNs are used to classify retinal images for AmyloidPET prediction.
  - **Quick check question:** What are the key architectural features of EfficientNet-B0 that make it suitable for this task?

- **Concept:** Multimodal data fusion
  - **Why needed here:** The study combines unimodal predictions with patient metadata for improved classification.
  - **Quick check question:** What are the advantages and disadvantages of late heterogeneous fusion compared to other fusion methods?

## Architecture Onboarding

- **Component map:** DDPM -> Filter -> Unimodal classifiers (EfficientNet-B0) -> Multimodal classifier (fully connected network)
- **Critical path:** Synthetic image generation → Filter application → Pretraining and finetuning of unimodal classifiers → Fusion of unimodal predictions with metadata → Multimodal classification
- **Design tradeoffs:** Using pretrained ImageNet weights vs. training from scratch; choice of fusion method (early vs. late, homogeneous vs. heterogeneous); synthetic data budget
- **Failure signatures:** Poor performance on synthetic data may indicate issues with the generative model or dataset; degradation in performance after finetuning may suggest domain shift between synthetic and real images
- **First 3 experiments:**
  1. Train DDPM on a subset of retinal images and evaluate the diversity and realism of generated images.
  2. Train a single unimodal classifier on real data and compare its performance to a classifier pretrained on synthetic data.
  3. Implement a simple multimodal classifier fusing two unimodal predictions and evaluate its performance compared to unimodal models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of synthetic retinal images generated by DDPMs compare to the diversity of real images, and does this affect classification performance?
- Basis in paper: [explicit] The study evaluated the diversity of synthetic images by comparing the maximum Pearson's correlation coefficient (pearsonr) values among synthetic images (SvS) and among real images (RvR), finding that SvS was stronger than RvR for OCTA-SMAC and FAF.
- Why unresolved: While the study found that synthetic images were diverse and did not memorize training data, the impact of this diversity on classification performance was not fully explored.
- What evidence would resolve it: Further experiments comparing the classification performance using synthetic images with varying levels of diversity, or using a larger dataset to train the DDPMs, could provide insights into the relationship between image diversity and classification accuracy.

### Open Question 2
- Question: How does the inclusion of metadata, such as age and gender, affect the performance of unimodal classifiers in predicting AmyloidPET status?
- Basis in paper: [explicit] The study found that integrating metadata in multimodal CNNs improved AUPR from 0.486 to 0.634, which was the best overall classifier. However, the effect of metadata on unimodal classifiers was not explored.
- Why unresolved: The study only explored the effect of metadata on multimodal classifiers, leaving the potential benefits of metadata incorporation in unimodal classifiers unexplored.
- What evidence would resolve it: Experiments training unimodal classifiers with and without metadata, and comparing their performance, would provide insights into the impact of metadata on unimodal classification accuracy.

### Open Question 3
- Question: How does the use of bilateral inputs compare to unilateral inputs in predicting AmyloidPET status using multimodal retinal imaging?
- Basis in paper: [inferred] The study used unilateral inputs for the multimodal classifiers, but the potential benefits of using bilateral inputs were not explored.
- Why unresolved: The study did not investigate the impact of using bilateral inputs, which could provide additional information for predicting AmyloidPET status.
- What evidence would resolve it: Experiments comparing the performance of multimodal classifiers using unilateral and bilateral inputs, and analyzing the differences in their predictions, would provide insights into the benefits of using bilateral inputs.

## Limitations
- Small dataset size (328 eyes from 183 patients) despite synthetic data augmentation
- Modest AUPR values (0.634 for best multimodal model) suggesting room for clinical improvement
- Synthetic image generation may not fully capture complex biological variations in real retinal pathologies

## Confidence
**High Confidence:**
- Generative AI can produce diverse synthetic retinal images that do not memorize training data
- Multimodal fusion with patient metadata improves classification performance over unimodal approaches

**Medium Confidence:**
- Pretraining on synthetic data provides consistent improvement across all modalities
- The best-performing multimodal classifier with metadata achieves clinically meaningful discrimination

**Low Confidence:**
- The specific magnitude of performance improvement from synthetic pretraining is consistent across different retinal modalities
- The proposed approach would generalize to larger, more diverse clinical populations

## Next Checks
1. **External validation on independent datasets:** Test the trained models on retinal imaging data from different clinical centers or populations to assess generalizability and potential overfitting to the original dataset.

2. **Ablation study on synthetic data contribution:** Systematically vary the proportion of synthetic to real images in pretraining to quantify the optimal contribution of synthetic data and establish whether current improvements are data-efficient.

3. **Longitudinal performance assessment:** Evaluate model performance across different timepoints and disease stages to determine whether the approach captures progressive retinal changes associated with Alzheimer's disease rather than static features.