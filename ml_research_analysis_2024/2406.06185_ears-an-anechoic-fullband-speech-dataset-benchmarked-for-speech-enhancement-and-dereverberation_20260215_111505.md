---
ver: rpa2
title: 'EARS: An Anechoic Fullband Speech Dataset Benchmarked for Speech Enhancement
  and Dereverberation'
arxiv_id: '2406.06185'
source_url: https://arxiv.org/abs/2406.06185
tags:
- speech
- dataset
- processing
- test
- enhancement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the EARS dataset, a high-quality speech dataset
  comprising 107 speakers from diverse backgrounds, totaling 100 hours of clean, anechoic
  speech data. The dataset covers a wide range of speaking styles, including emotional
  speech, different reading styles, non-verbal sounds, and conversational freeform
  speech.
---

# EARS: An Anechoic Fullband Speech Dataset Benchmarked for Speech Enhancement and Dereverberation

## Quick Facts
- arXiv ID: 2406.06185
- Source URL: https://arxiv.org/abs/2406.06185
- Reference count: 0
- Primary result: Introduces EARS dataset with 107 speakers, 100 hours of anechoic speech, benchmarks speech enhancement/dereverberation methods, listening test prefers generative approach

## Executive Summary
EARS is a high-quality speech dataset comprising 107 speakers from diverse backgrounds, totaling 100 hours of clean, anechoic speech data. The dataset covers a wide range of speaking styles, including emotional speech, different reading styles, non-verbal sounds, and conversational freeform speech. The authors benchmark various methods for speech enhancement and dereverberation on the dataset and evaluate their performance through instrumental metrics. A listening test with 20 participants is conducted for the speech enhancement task, where a generative method is preferred. The paper also introduces a blind test set for automatic online evaluation of uploaded data.

## Method Summary
The EARS dataset is constructed from anechoic recordings of 107 speakers (44% male, 53% female, 3% non-binary) aged 18-75, capturing diverse ethnicities and speech styles. Speech is recorded at 48kHz, 32-bit depth in an anechoic chamber using GRAS 40HH and 48BL microphones. Two subsets are created: EARS-WHAM for speech enhancement (speech mixed with real noise at perceptually meaningful SNRs using LKFS loudness) and EARS-Reverb for dereverberation (speech convolved with real RIRs from various room types). Four baseline methods are evaluated: Conv-TasNet and Demucs (predictive), CDiffuSE and SGMSE+ (generative). Performance is measured using POLQA, PESQ, ESTOI, SI-SDR, SIGMOS, DNSMOS, and WER metrics.

## Key Results
- EARS dataset contains 100 hours of anechoic speech from 107 diverse speakers covering emotional, conversational, and non-verbal speech styles
- Generative methods (CDiffuSE, SGMSE+) outperform predictive methods in both enhancement and dereverberation tasks
- Listening test with 20 participants shows preference for generative speech enhancement methods
- Online blind test server enables automatic evaluation of uploaded dereverberation results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anechoic recording captures clean, uncolored speech signals that serve as ground truth for enhancement/dereverberation benchmarks
- Mechanism: The anechoic chamber removes reflections, allowing precise modeling of reverberation and noise without contamination from acoustic artifacts
- Core assumption: The recording setup (GRAS 40HH and 48BL mics) accurately captures the full dynamic range of human speech from whisper to yelling
- Evidence anchors:
  - [abstract] "EARS dataset, a high-quality speech dataset comprising 107 speakers from diverse backgrounds, totaling 100 hours of clean, anechoic speech data"
  - [section] "All speech is recorded in an anechoic chamber as 32-bit audio at 48 kHz"
  - [corpus] Weak evidence; corpus neighbors are mostly dereverberation papers, suggesting relevance but no direct measurement data
- Break condition: If microphone clipping occurs during high-energy speech (yelling), the lower-sensitivity recording must be used, introducing potential tonal differences despite equalization

### Mechanism 2
- Claim: Wide speaker and speech diversity improves model generalization across real-world conditions
- Mechanism: Training on 107 speakers with varied demographics, emotions, and speaking styles exposes models to diverse acoustic patterns, reducing overfitting to narrow conditions
- Core assumption: The script covers representative human speech variations including emotional and conversational speech
- Evidence anchors:
  - [abstract] "covers a large range of different speaking styles, including emotional speech, different reading styles, non-verbal sounds, and conversational freeform speech"
  - [section] "speakers range from age 18 to 75 and span various ethnicities... 44% male, 53% female, and 3% non-binary"
  - [corpus] No direct evidence in corpus; diversity claim relies on dataset description
- Break condition: If certain speech styles (e.g., whispering) are underrepresented in training data, models may fail to generalize to those conditions

### Mechanism 3
- Claim: Using perceptual metrics (POLQA, SIGMOS) provides better evaluation than purely signal-based metrics for speech quality
- Mechanism: Human perception of speech quality involves factors beyond signal fidelity, such as naturalness and absence of artifacts, which perceptual models capture better
- Core assumption: The perceptual models are trained on data representative of the speech styles in EARS
- Evidence anchors:
  - [section] "We employ intrusive metrics... and non-intrusive metrics, which assess the performance only using the processed signal" followed by listing POLQA, PESQ, SIGMOS
  - [section] "Listening Test... We conduct a MUSHRA-like listening test... where a generative method is preferred"
  - [corpus] No corpus evidence; evaluation methodology relies on internal validation
- Break condition: If perceptual models are not well-calibrated for emotional or whispered speech, they may give misleading quality scores

## Foundational Learning

- Concept: Room Impulse Response (RIR) modeling
  - Why needed here: Dereverberation requires convolving clean speech with RIRs to create realistic reverberant conditions for training and evaluation
  - Quick check question: What is the maximum RT60 reverberation time used in EARS-Reverb dataset construction?

- Concept: Signal-to-Noise Ratio (SNR) computation using loudness (LKFS)
  - Why needed here: EARS-WHAM mixes speech and noise at perceptually meaningful SNRs by computing LKFS-based loudness, avoiding bias from silent regions
  - Quick check question: What is the SNR range used when mixing speech and noise in EARS-WHAM?

- Concept: Generative vs. predictive speech enhancement
  - Why needed here: The benchmark compares both approaches (e.g., Conv-TasNet vs. SGMSE+), requiring understanding of their different operating principles and trade-offs
  - Quick check question: Which method type (generative or predictive) performed best in the listening test according to the paper?

## Architecture Onboarding

- Component map: EARS dataset -> EARS-WHAM (enhancement) / EARS-Reverb (dereverberation) -> baseline methods -> evaluation metrics -> online blind test server
- Critical path: Data collection (anechoic recording) -> dataset construction (splits, mixing) -> baseline training -> evaluation (metrics + listening test) -> blind test deployment
- Design tradeoffs: High-quality anechoic recording vs. scalability (107 speakers, 100 hours); generative models (better quality) vs. computational cost; perceptual vs. signal-based metrics
- Failure signatures: Microphone clipping during high-energy speech; insufficient representation of certain speech styles; perceptual metrics not calibrated for emotional speech; blind test data leakage
- First 3 experiments:
  1. Verify anechoic recording quality by comparing clean speech spectrum to known anechoic reference
  2. Test baseline Conv-TasNet on EARS-WHAM validation set to establish baseline SI-SDR and POLQA scores
  3. Run SGMSE+ on a small subset of EARS-Reverb to confirm dereverberation performance before full-scale training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed generative speech enhancement method (SGMSE+) perform on emotional speech compared to neutral speech in the EARS dataset?
- Basis in paper: [explicit] The paper mentions that all considered approaches trained on EARS-WHAM generalize well to emotional speech, but it does not provide specific performance metrics for emotional speech
- Why unresolved: The paper only provides overall performance metrics for the speech enhancement task without breaking down the results by emotional speech categories
- What evidence would resolve it: Detailed performance metrics (e.g., POLQA, SI-SDR, PESQ, ESTOI, WER) for each emotional speech category in the EARS dataset

### Open Question 2
- Question: How does the proposed generative speech enhancement method (SGMSE+) perform on whispered speech compared to other speaking styles in the EARS dataset?
- Basis in paper: [explicit] The paper mentions that all considered approaches trained on EARS-WHAM generalize well to whispered speech, but it does not provide specific performance metrics for whispered speech
- Why unresolved: The paper only provides overall performance metrics for the speech enhancement task without breaking down the results by speaking styles
- What evidence would resolve it: Detailed performance metrics (e.g., POLQA, SI-SDR, PESQ, ESTOI, WER) for each speaking style in the EARS dataset

### Open Question 3
- Question: How does the proposed generative speech dereverberation method (SGMSE+) perform on different room types in the EARS-Reverb dataset?
- Basis in paper: [explicit] The paper mentions that the EARS-Reverb dataset includes RIRs from various room types, but it does not provide specific performance metrics for each room type
- Why unresolved: The paper only provides overall performance metrics for the dereverberation task without breaking down the results by room types
- What evidence would resolve it: Detailed performance metrics (e.g., POLQA, SI-SDR, PESQ, ESTOI, WER) for each room type in the EARS-Reverb dataset

## Limitations

- Microphone clipping risk during high-energy speech (yelling) requires switching to lower-sensitivity microphone with equalization, potentially introducing tonal artifacts
- Perceptual metric calibration uncertainty for emotional and whispered speech styles included in EARS dataset
- Diversity claims lack external validation through corpus analysis or user studies beyond small listening test

## Confidence

- High Confidence: Dataset construction methodology (anechoic recording at 48kHz, 32-bit depth) and basic benchmarking results (SI-SDR, PESQ scores) are well-documented and reproducible
- Medium Confidence: Claims about speaker diversity and speech style coverage are supported by dataset description but lack external validation
- Low Confidence: Effectiveness of perceptual metrics (POLQA, SIGMOS) for evaluating emotional and whispered speech styles is not empirically validated in the paper

## Next Checks

1. **Microphone Response Validation**: Measure the frequency response difference between high-sensitivity and low-sensitivity microphones used in recording, then verify the equalization process fully compensates for any tonal differences

2. **Perceptual Metric Calibration Test**: Run the baseline models on EARS test sets and compare perceptual metric scores across different speech styles (emotional, whispered, conversational) to identify potential calibration issues

3. **Diversity Validation**: Conduct statistical analysis of speech feature distributions (pitch range, speaking rate, energy levels) across demographic groups in EARS to empirically verify the claimed diversity representation