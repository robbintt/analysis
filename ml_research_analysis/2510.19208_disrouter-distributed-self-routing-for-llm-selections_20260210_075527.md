---
ver: rpa2
title: 'DiSRouter: Distributed Self-Routing for LLM Selections'
arxiv_id: '2510.19208'
source_url: https://arxiv.org/abs/2510.19208
tags:
- cost
- routing
- answer
- performance
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiSRouter introduces a distributed self-routing framework that
  replaces centralized routing with a network of LLM agents, each using self-awareness
  to decide whether to answer or route queries. This approach enhances flexibility
  and scalability while enabling scenario adaptability (performance-first to cost-first)
  without retraining.
---

# DiSRouter: Distributed Self-Routing for LLM Selections

## Quick Facts
- arXiv ID: 2510.19208
- Source URL: https://arxiv.org/abs/2510.19208
- Reference count: 23
- Primary result: DiSRouter achieves up to 74.29% of oracle utility by enabling LLM agents to self-assess and route queries based on their own capabilities

## Executive Summary
DiSRouter introduces a distributed self-routing framework that replaces centralized routing with a network of LLM agents, each using self-awareness to decide whether to answer or route queries. This approach enhances flexibility and scalability while enabling scenario adaptability (performance-first to cost-first) without retraining. Self-Awareness Training, comprising supervised fine-tuning and reinforcement learning with localized rewards, improves agents' ability to accurately assess their own capabilities. Extensive experiments show DiSRouter achieves up to 74.29% of oracle utility, outperforming centralized baselines across in-domain and out-of-domain tasks. The system demonstrates strong generalization, modularity, and dynamic adjustment of routing strategies based on user-defined preferences, validating that intrinsic self-awareness is more effective than external assessment for efficient multi-agent routing.

## Method Summary
DiSRouter organizes LLM agents in a cascade ordered by increasing cost, where each agent independently decides whether to answer or forward queries based on self-assessed capability. The framework employs a two-stage Self-Awareness Training pipeline: first, supervised fine-tuning (SFT) labels queries as answerable or unanswerable based on correctness frequency; second, reinforcement learning (RL) shapes agent behavior through scenario-conditioned rewards. At inference, scenario prompts (performance-first, balance, cost-first) dynamically adjust routing behavior. The system achieves system-level coordination without inter-agent communication by localizing reward functions and leveraging intrinsic self-assessment, outperforming centralized routing methods across multiple benchmarks.

## Key Results
- DiSRouter achieves up to 74.29% of oracle utility, significantly outperforming centralized baselines across in-domain and out-of-domain tasks
- Self-assessment accuracy (0.80 accuracy, 0.81 F1) substantially outperforms external BERT-based (0.56 accuracy) and LLM-based (0.71 accuracy) classifiers
- The framework demonstrates strong generalization, modularity, and dynamic adjustment of routing strategies based on user-defined preferences (α ∈ {0.2, 0.5, 0.8})

## Why This Works (Mechanism)

### Mechanism 1: Distributed Self-Routing Architecture
A distributed routing system where each LLM agent independently decides whether to answer or route queries outperforms centralized routers that rely on external assessment. Queries traverse a cascade of LLM agents ordered by increasing cost, with each agent performing self-assessment—if confident, it executes; if uncertain, it rejects and routes to the next larger agent. This eliminates the central router bottleneck and enables modular scaling. The core assumption is that LLMs can be trained to have sufficient self-awareness to accurately assess their own capability boundaries for specific queries.

### Mechanism 2: Two-Stage Self-Awareness Training Pipeline
A pipeline combining Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) enhances LLM self-awareness and enables scenario-adaptive behavior. SFT labels queries as answerable or unanswerable based on correctness frequency, while RL shapes behavior through scenario-conditioned rewards. The model learns to answer only when expected reward exceeds rejection reward, formalized as p(x) > (1-α)^γ. This mechanism enables each agent to dynamically adjust its behavior based on the preference factor α, maintaining high reliability while optimizing for the specified scenario.

### Mechanism 3: Scenario Adaptability via Localized Rewards
Each agent dynamically adjusts its behavior from performance-first to cost-first based on scenario prompts, achieving system-level coordination without inter-agent communication. The preference factor α is conveyed via prompt instructions, and during training, models learn that higher α lowers the capability threshold for answering. At inference, scenario prompts trigger calibrated behavior shifts—conservative when α=0.2, aggressive when α=0.8. The nonlinear (1-α)^γ function with γ=0.5 maintains high reliability until extreme cost-saving is prioritized.

## Foundational Learning

- **Concept: Self-Awareness in LLMs**
  - **Why needed here:** The entire DiSRouter system hinges on each agent's ability to accurately judge whether a query falls within its capability boundary. Without this, routing decisions are unreliable.
  - **Quick check question:** Why might an external router struggle to assess an LLM's knowledge boundary for a specific query better than the LLM itself?

- **Concept: Reinforcement Learning from Scalar Rewards**
  - **Why needed here:** The RL stage shapes agent behavior through a scenario-conditioned reward function. Understanding how reward design translates to threshold behavior is critical.
  - **Quick check question:** Given reward(x) = +1 (correct), 0 (incorrect), (1-α)^γ (reject), derive why a model answers only when p(x) > (1-α)^γ.

- **Concept: Cascading System Design**
  - **Why needed here:** DiSRouter organizes agents in a cascade where queries flow from smaller to larger models. Understanding failure modes (cascading rejection, premature acceptance) is essential for debugging.
  - **Quick check question:** What happens if all agents in a cascade reject a query? How does DiSRouter prevent this?

## Architecture Onboarding

- **Component map:** Qwen2.5-0.5B -> Qwen2.5-1.5B -> Qwen2.5-3B -> Qwen2.5-7B -> Qwen2.5-14B (ordered by increasing cost)
- **Critical path:** Query arrives at entry agent m₁ with scenario prompt (α) → Agent generates response (answer or "I don't know") → If answer → return result; if rejection → forward to m_{i+1} → Repeat until execution or reach m_K (fallback, always executes) → Log final agent, compute cost/performance
- **Design tradeoffs:** Cascade depth vs. routing overhead (more agents → finer cost granularity but more rejection steps); Conservative vs. aggressive thresholds (higher δ → better accuracy, higher cost); Homogeneous vs. heterogeneous agents (single model family vs. cross-family compatibility); Local vs. global optimization (independent agent training enables modularity but may miss globally optimal paths)
- **Failure signatures:** Cascading rejection (all agents reject → forced fallback to largest model); Premature acceptance (small model accepts hard query → incorrect answer); Threshold misalignment (agent behavior doesn't match scenario); Self-awareness drift (distribution shift degrades capability assessment accuracy over time)
- **First 3 experiments:** 1) Baseline self-awareness probe: Present untrained LLM with answerable/unanswerable queries; measure discrimination. 2) SFT-only validation: Train with SFT stage, compare accuracy on answered vs. rejected queries on held-out set. 3) Full system vs. baselines: Deploy 3+ agent cascade with SFT+RL, run under α∈{0.2, 0.5, 0.8}, compare utility against GraphRouter, FrugalGPT, Random, Largest/Smallest.

## Open Questions the Paper Calls Out

### Open Question 1
How can DiSRouter be generalized to complex network topologies (e.g., tree or mesh structures) with system-level information exchange? The current work uses a cascade, but the framework theoretically supports complex topologies, and future work will "explore more complex distributed network structures and implement the system information exchange during inference." This is unresolved because the current implementation is limited to a linear cascade where agents only know their immediate successor.

### Open Question 2
Can incorporating "reasoned refusals" or more sophisticated distributed reinforcement learning rewards significantly enhance the self-awareness of smaller models? The "Future Work" section explicitly notes the current training pipeline is "relatively basic" and suggests that exploring "reasoned refusals" (explaining why a query is rejected) or sophisticated distributed RL rewards could further enhance model self-awareness, particularly for smaller agents. This is unresolved because the current method trains agents to emit a simple "I don't know" token based on a localized reward.

### Open Question 3
Does the DiSRouter framework maintain its superiority when applied to a heterogeneous pool of LLMs from different architectural families? The experimental setup exclusively uses a homogeneous pool of five models from the Qwen2.5-Instruct series. The paper does not validate if the "self-awareness" training and routing logic transfer effectively when the agents have vastly different architectures, tokenizers, or training corpora (e.g., mixing Llama, Mistral, and GPT). This is unresolved because self-awareness capabilities may not be consistent across different model families.

## Limitations
- The paper assumes self-awareness can be reliably trained through the SFT+RL pipeline, but the method's robustness to distribution shift and model size differences remains untested beyond the Qwen2.5 family
- Cross-model coordination is implicit; agents trained independently may develop misaligned thresholds, especially when mixing heterogeneous architectures
- Scenario adaptation relies on prompt conditioning, but the mechanism for generalizing to arbitrary utility functions or non-linear cost structures is unclear

## Confidence

- **High Confidence:** The core claim that distributed self-routing outperforms centralized routing (74.29% oracle utility) is well-supported by ablation and comparison experiments
- **Medium Confidence:** The two-stage training pipeline's effectiveness is plausible but lacks direct evidence isolating SFT vs. RL contributions
- **Medium Confidence:** Scenario adaptability via localized rewards is demonstrated but depends on prompt efficacy and generalization assumptions

## Next Checks

1. **Self-awareness robustness test:** Evaluate trained agents on out-of-distribution queries to measure degradation in self-assessment accuracy and system utility
2. **Cross-family compatibility test:** Train a heterogeneous agent pool (e.g., Qwen2.5 + Llama + Mistral) and measure routing efficiency and self-awareness accuracy compared to homogeneous pool
3. **SFT vs. RL ablation:** Train models with SFT only, RL only, and combined SFT+RL; compare self-assessment accuracy and utility across scenarios to isolate training stage contributions