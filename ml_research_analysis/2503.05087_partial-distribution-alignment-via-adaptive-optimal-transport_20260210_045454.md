---
ver: rpa2
title: Partial Distribution Alignment via Adaptive Optimal Transport
arxiv_id: '2503.05087'
source_url: https://arxiv.org/abs/2503.05087
tags:
- transport
- optimal
- adaptive
- mass
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Adaptive Optimal Transport (AOT), a novel
  framework that overcomes the limitations of classical optimal transport by allowing
  for adaptive-mass preserving. Unlike partial or unbalanced optimal transport, AOT
  does not require pre-defining the fixed budget of mass to transport or the extent
  of "soft" penalties on marginal constraints.
---

# Partial Distribution Alignment via Adaptive Optimal Transport

## Quick Facts
- **arXiv ID:** 2503.05087
- **Source URL:** https://arxiv.org/abs/2503.05087
- **Reference count:** 40
- **Primary result:** AOT achieves 76.68% accuracy on VisDA and 72.24% on Office-Home, outperforming state-of-the-art OT and non-OT methods.

## Executive Summary
This paper introduces Adaptive Optimal Transport (AOT), a novel framework that overcomes limitations of classical optimal transport by allowing for adaptive-mass preserving. Unlike partial or unbalanced optimal transport, AOT self-determines the optimal mass allocation in response to the intrinsic structure of the problem, making it highly effective for partial distribution alignment in the presence of noise, outliers, and distribution shifts. The authors provide theoretical insights into the mass allocation mechanism and duality theory of AOT, and apply it to unsupervised domain adaptation. Experimental results on benchmark datasets demonstrate that AOT significantly outperforms state-of-the-art methods.

## Method Summary
AOT is formulated as a relaxation of classical optimal transport with inequality marginal constraints, allowing the total transported mass to be determined adaptively. The method uses a mixed-sign cost function where negative costs trigger active transport and positive costs create inactive regions that reject outliers. The optimization employs entropy-regularized Sinkhorn with modified dual potentials constrained to be non-positive. For domain adaptation, AOT aligns source and target features using this transport plan while simultaneously learning classification.

## Key Results
- AOT achieves 76.68% accuracy on VisDA synthetic→real adaptation
- AOT achieves 72.24% accuracy on Office-Home domain adaptation
- AOT outperforms state-of-the-art OT-based and non-OT methods across all benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Mass Determination via Mixed-Sign Costs
The total transported mass emerges from cost function structure rather than pre-specification. Mixed-sign cost functions create "active" regions where c(x,z) < 0 (mass flows) and "inactive" regions where c(x,z) > 0 (no transport). The optimal transport plan automatically allocates mass only to negative-cost pairs, with total mass determined by the distribution of negative costs across the sample space.

### Mechanism 2: Outlier Rejection via Inactive Region Formation
Samples inconsistent with cross-domain structure are isolated as singletons with zero outgoing/incoming mass. Theorem 2 proves that inactive regions receive zero mass. In the toy example, x_6 and z_5 become isolated points with no transport connections—they are automatically identified as potential outliers without explicit thresholding.

### Mechanism 3: Duality-Based Efficient Optimization
The primal AOT problem can be solved via constrained Kantorovich duality. By augmenting measures with Dirac masses at infinity, the partial-transport problem transforms into a full-mass transport problem. The dual formulation adds constraints φ(x) ≤ 0, ψ(z) ≤ 0 to standard Kantorovich potentials, enabling use of modified Sinkhorn algorithms.

## Foundational Learning

- **Concept: Kantorovich Optimal Transport**
  - **Why needed here:** AOT is formulated as a relaxation of classical OT. Understanding the primal problem min_γ ∫c(x,z)dγ subject to marginal constraints is prerequisite to understanding how AOT modifies these constraints.
  - **Quick check question:** Can you explain why the constraint γ[A × Z] = μ[A] (equality) differs fundamentally from γ[A × Z] ≤ μ[A] (inequality)?

- **Concept: Convex Duality and Lagrange Multipliers**
  - **Why needed here:** The paper's duality theory builds on Kantorovich duality. Understanding how constrained optimization problems convert to unconstrained dual forms via potentials is essential.
  - **Quick check question:** What role do the additional constraints φ(x) ≤ 0, ψ(z) ≤ 0 play in the AOT dual compared to standard Kantorovich duality?

- **Concept: Borel Measures and Marginal Distributions**
  - **Why needed here:** The formalism uses Borel measures μ, ν and transport plans γ ∈ P(X × Z). Active/inactive regions are defined measure-theoretically via support sets.
  - **Quick check question:** What does γ[A × Z] ≤ μ[A] mean intuitively in terms of mass leaving subset A?

## Architecture Onboarding

- **Component map:** [Source Features X] → [Cost Matrix C] → [AOT Solver] → [Transport Plan γ*] → [Domain Adaptation Loss]
- **Critical path:** Forward pass extracts features for source (labeled) and target (unlabeled) batches → Pseudo-labels q(z) generated by current classifier for target samples → Cost matrix computed with feature distance and label agreement terms → Entropy-regularized AOT solved via modified Sinkhorn (with φ,ψ ≤ 0 constraints) → Transport plan γ* defines sample-wise alignment weights → Gradient flows through both classification loss and transport cost
- **Design tradeoffs:**
  - β (label cost weight): Higher β increases label-term influence, driving more class-aware alignment, but risks confident wrong pseudo-labels corrupting transport. Paper finds β=1.8-6.0 depending on dataset.
  - ϵ (entropy regularization): Larger ϵ → sparser transport plans, better class separation, but excessive sparsity degrades accuracy. Paper shows accuracy peaks at ϵ=0.1 for VisDA.
  - Batch size: Must be large enough to capture label diversity. Paper uses 62-72 samples; smaller batches may miss classes, triggering partial-matching behavior.
- **Failure signatures:**
  1. Mass collapse (mγ* << 1): Cost function produces mostly positive values; check α/β ratio and pseudo-label quality.
  2. Class confusion in transport plan: Off-diagonal blocks in heatmap remain active; increase β or reduce ϵ.
  3. Training instability: Loss oscillates; entropy regularization may be too weak or learning rate too high for transport component.
- **First 3 experiments:**
  1. Cost function validation: On a small batch, compute cost matrix and visualize transport plan heatmap. Verify diagonal concentration and measure total mass mγ*. If mass > 0.95 or < 0.5, adjust β before full training.
  2. Ablation on mixed-sign costs: Compare AOT (c = α||x-z||² - βp^Tq) vs. baseline using non-negative cost (c = α||x-z||² + β||p-q||²). Report accuracy gap on VisDA validation set.
  3. Partial matching robustness: Construct mini-batches with deliberately missing target classes. Measure whether AOT automatically reduces source-side mass for absent classes vs. m-POT which maintains fixed budget.

## Open Questions the Paper Calls Out
- **Application to biomedical data:** The authors plan to explore AOT applications in biomedical domains, such as understanding cell perturbation responses to treatments, which would test the method on highly sparse and noisy data.
- **Automated hyperparameter tuning:** The method currently requires manual tuning of α and β coefficients, which limits its adaptability to new datasets where grid search is computationally expensive.
- **Applicability with strictly positive costs:** The mechanism relies on negative costs to trigger active transport, raising questions about AOT's advantages in standard scenarios where ground costs cannot be negative.

## Limitations
- AOT relies on mixed-sign cost functions, which may not be available in all applications where costs are inherently strictly positive
- The method requires manual hyperparameter tuning for optimal performance across different datasets
- Theoretical advantages over Partial OT in standard positive-cost scenarios remain unproven

## Confidence
- **Mechanism claims:** Medium - supported by theorems and toy examples but limited empirical validation of outlier rejection
- **Experimental results:** High - comprehensive benchmark evaluation with multiple datasets and comparison methods
- **Reproducibility:** Medium - detailed methodology but missing specific learning rate values and exact optimization parameters

## Next Checks
1. Verify the mass allocation mechanism by computing total transport mass mγ* on small batches with known outliers
2. Test AOT's partial matching capability by constructing batches with missing target classes and measuring source mass reduction
3. Validate the duality derivation by comparing primal and dual objective values on test problems