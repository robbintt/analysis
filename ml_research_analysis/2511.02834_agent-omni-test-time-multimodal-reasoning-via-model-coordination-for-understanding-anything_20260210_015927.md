---
ver: rpa2
title: 'Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding
  Anything'
arxiv_id: '2511.02834'
source_url: https://arxiv.org/abs/2511.02834
tags:
- reasoning
- multimodal
- agent
- video
- omni
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Agent-Omni introduces a master-agent framework that coordinates
  existing foundation models to achieve test-time multimodal reasoning without retraining.
  By dynamically delegating tasks to modality-specific agents and iteratively refining
  outputs, the system integrates text, image, audio, and video inputs into coherent
  answers.
---

# Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything

## Quick Facts
- arXiv ID: 2511.02834
- Source URL: https://arxiv.org/abs/2511.02834
- Authors: Huawei Lin; Yunzhi Shi; Tong Geng; Weijie Zhao; Wei Wang; Ravender Pal Singh
- Reference count: 40
- Primary result: Master-agent coordination framework achieves state-of-the-art accuracy on diverse multimodal benchmarks without retraining

## Executive Summary
Agent-Omni introduces a master-agent framework that coordinates existing foundation models to achieve test-time multimodal reasoning without retraining. By dynamically delegating tasks to modality-specific agents and iteratively refining outputs, the system integrates text, image, audio, and video inputs into coherent answers. Experiments show Agent-Omni achieves state-of-the-art accuracy on diverse benchmarks, outperforming both unified omni models and DSPy-CoT, especially on complex reasoning tasks like MMMU-Pro and Daily-Omni.

## Method Summary
Agent-Omni employs a master agent (Claude 3.7 Sonnet) to orchestrate specialized models for multimodal reasoning. The framework decomposes user queries into modality-specific subtasks, routes them to appropriate agents (text: Deepseek R1, image/video: Claude 3.7 Sonnet, audio: Qwen2.5 Omni), and iteratively refines outputs through a decision loop. This approach achieves test-time reasoning without joint training, leveraging the strengths of specialized models while maintaining coordination through a central reasoning agent.

## Key Results
- Outperforms unified omni-models and DSPy-CoT on MMMU-Pro, Daily-Omni, and VideoMathQA benchmarks
- Iterative refinement (up to L=3 iterations) improves accuracy, particularly for complex video tasks
- Model pool hot-swapping demonstrates 10+ point accuracy differences based on agent quality
- Achieves 83.21% on MMLU-Pro with 3 iterations vs. 82.20% with 1 iteration

## Why This Works (Mechanism)

### Mechanism 1: Master-Agent Task Decomposition
The master agent parses user intent and delegates modality-specific subtasks to specialized models, separating "what to ask" from "how to answer." This leverages specialized models' strengths while maintaining coordination through the central agent.

### Mechanism 2: Iterative Self-Correction Loop
Multi-round refinement improves answer completeness by allowing the master agent to detect gaps and generate follow-up questions. Exit rate data shows 90%+ queries complete after 1 iteration for text/image, but 40-75% require 2-3 iterations for video tasks.

### Mechanism 3: Model Pool Hot-Swapping
The framework's performance tracks underlying model quality, enabling seamless integration of stronger specialists without retraining. Ablation studies show 5-15% accuracy swings based on model choice, demonstrating the importance of model selection.

## Foundational Learning

- **Agent orchestration patterns (dispatcher-aggregator model)**
  - Why needed: Master agent acts as dispatcher (routing subtasks) and aggregator (fusing outputs). Understanding this pattern is prerequisite to debugging delegation failures.
  - Quick check: Can you trace a single user query through all four stages and identify where a modal-specific agent's output would appear in the JSON?

- **Multimodal representation alignment (implicit)**
  - Why needed: While Agent-Omni avoids joint training, it assumes modalities can be "aligned" at the text level (all outputs are textual). Understanding representation gaps helps diagnose why certain cross-modal inferences fail.
  - Quick check: If an image agent returns "a red car" and audio agent returns "engine revving," how does the master agent determine these refer to the same object?

- **Test-time compute scaling**
  - Why needed: Iterative loops trade latency for accuracy. Understanding inference-time scaling helps set expectations for L (max iterations) and latency budgets.
  - Quick check: Given Table 9 shows 4-20s latency for Agent-Omni vs <2s for single models, what factors determine whether the accuracy gain justifies the latency cost?

## Architecture Onboarding

- **Component map**: Master Agent (Claude 3.7 Sonnet) -> Perception -> Reasoning -> Execution -> Decision -> JSON schemas -> Model Pool (Deepseek R1, Qwen2.5 Omni, Claude 3.7 Sonnet)

- **Critical path**: User query + multimodal inputs → Perception stage (JSON summary) → Reasoning stage generates AgentInstruction list → Execution stage invokes agents → Decision stage synthesizes final_answer → If is_final=false, loop back to Reasoning

- **Design tradeoffs**: Latency vs. accuracy (more iterations → higher accuracy but 4-7x latency increase); Model quality vs. cost (API costs scale with iteration count); Generality vs. specialization (omni-models cover all modalities but underperform by 10-20 points)

- **Failure signatures**: Infinite loop (is_final never set true); Silent modality drop (Reasoning stage fails to invoke relevant agent); Output incoherence (Decision stage concatenates without synthesis); API timeout (video processing exceeds limits)

- **First 3 experiments**:
  1. Single-modality baseline: Run text-only MMLU-Pro queries through Agent-Omni with L=1 vs. direct Deepseek R1 call; measure accuracy gap and latency overhead
  2. Iteration sweep: On VideoMathQA, vary L from 1 to 5; plot accuracy vs. latency and identify optimal operating point
  3. Agent swap ablation: Replace Qwen2.5 Omni (audio) with Qwen2 Audio 7B on VoxCeleb-Gender; quantify accuracy drop and investigate failure cases via JSON traces

## Open Questions the Paper Calls Out
1. Can parallelized execution of modality-specific agents reduce Agent-Omni's inference latency while preserving accuracy?
2. How do errors or biases from individual foundation models propagate through the coordination process?
3. How well does Agent-Omni generalize to noisy, adversarial, or safety-critical real-world scenarios?
4. Can the framework be extended to support output generation in modalities beyond text?

## Limitations
- Assumes no catastrophic error propagation across agents without quantitative analysis
- Reliance on Claude 3.7 Sonnet as master agent raises reproducibility and cost concerns
- All experiments use clean, curated benchmarks that may not reflect real-world distribution shifts

## Confidence
- **High confidence**: Master-agent coordination mechanism improves accuracy over unified models
- **Medium confidence**: Iterative refinement loop effectiveness, though lacks deeper analysis of diminishing returns
- **Low confidence**: Framework scalability and cost-effectiveness for production use

## Next Checks
1. **Error Propagation Analysis**: Instrument the pipeline to track accuracy loss at each stage for failing queries, quantifying how errors compound across agents
2. **Cost-Benefit Benchmarking**: Measure total API tokens and compute time against accuracy gains from iterations; identify break-even points where additional iterations become cost-ineffective
3. **Generalization Testing**: Replace Claude 3.7 Sonnet with an open-source multimodal model as master agent; evaluate whether the coordination pattern still delivers accuracy gains