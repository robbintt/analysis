---
ver: rpa2
title: 'Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs
  and Diffusion Models'
arxiv_id: '2505.08803'
source_url: https://arxiv.org/abs/2505.08803
tags:
- data
- collapse
- synthetic
- wang
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates model collapse in multi-modal vision-language
  generative systems, specifically text-to-image diffusion models and vision-language
  models (VLMs), under recursive training on self-generated synthetic data. Unlike
  prior studies focused on single-modality models, this work examines realistic scenarios
  where multiple models interact autonomously through synthetic data.
---

# Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models

## Quick Facts
- arXiv ID: 2505.08803
- Source URL: https://arxiv.org/abs/2505.08803
- Reference count: 40
- Key outcome: Model collapse in multi-modal vision-language systems shows unique variance behaviors and can be mitigated through model diversity and frozen relabeling strategies

## Executive Summary
This paper investigates model collapse in multi-modal vision-language generative systems, specifically text-to-image diffusion models and vision-language models (VLMs), under recursive training on self-generated synthetic data. Unlike prior studies focused on single-modality models, this work examines realistic scenarios where multiple models interact autonomously through synthetic data. The study reveals that variance does not always collapse during model collapse in VLM image captioning, increased decoding budgets generate more robust synthetic datasets, and commonly used quality measures like FID and BLEU-4 correlate well with synthetic data robustness. The research also demonstrates that model diversity is crucial for maintaining stability in recursive fine-tuning and that relabeling with frozen models significantly mitigates model collapse.

## Method Summary
The study employs a synthetic data training pipeline where vision-language models (VLMs) and diffusion models are trained recursively on their own generated outputs. The experimental setup involves generating synthetic image-text pairs using a trained diffusion model and VLMs, then using these synthetic pairs to fine-tune the models iteratively. The research examines two primary model architectures: BLIP-2 for VLM tasks and Stable Diffusion XL for image generation. The recursive training process is conducted under various conditions including different decoding budgets, model diversity configurations, and relabeling strategies with frozen models. Quality metrics such as FID and BLEU-4 are used to evaluate synthetic data robustness, while variance measurements track the collapse behavior across iterations.

## Key Results
- Variance does not always collapse during model collapse, particularly in VLM image captioning tasks
- Increased decoding budgets generate more robust synthetic datasets that resist model collapse
- FID and BLEU-4 quality measures correlate well with synthetic data robustness against model collapse
- Model diversity is crucial for maintaining stability in recursive fine-tuning scenarios
- Relabeling with frozen models significantly mitigates model collapse, while joint fine-tuning accelerates it

## Why This Works (Mechanism)
The mechanism behind model collapse in multi-modal systems differs from single-modality models due to the complex interplay between visual and textual representations. In VLM image captioning, the captioning model may maintain diversity through the inherent variability in natural language descriptions, even as visual features collapse. The increased decoding budgets provide more diverse synthetic samples during generation, creating a more robust training distribution that resists collapse. Model diversity introduces different inductive biases and representations across models, preventing the convergence to narrow modes that characterizes collapse. Frozen relabeling breaks the feedback loop by maintaining consistent labels, while joint fine-tuning without frozen models allows errors to compound through mutual reinforcement.

## Foundational Learning
- **Model Collapse**: Phenomenon where models trained on their own synthetic data progressively degenerate in quality and diversity; needed to understand the core problem being addressed, quick check: observe quality degradation across recursive training iterations
- **Variance Behavior**: Statistical measure of output diversity that may or may not collapse depending on model architecture and task; needed to distinguish multi-modal from single-modal collapse patterns, quick check: measure output distribution variance across training rounds
- **Synthetic Data Robustness**: The ability of generated datasets to maintain quality and diversity when used for training; needed to evaluate the effectiveness of different generation strategies, quick check: compare FID/BLEU scores of synthetic vs. real data
- **Model Diversity**: Using multiple models with different architectures or training objectives in ensemble approaches; needed to prevent collapse through representation diversity, quick check: test stability with varying numbers of diverse models
- **Decoding Budgets**: Computational resources allocated to generate synthetic data, affecting output diversity; needed to optimize synthetic data quality, quick check: vary decoding steps and temperature parameters
- **Frozen Relabeling**: Strategy where one model generates data while another frozen model provides labels; needed to break collapse feedback loops, quick check: compare joint vs. frozen relabeling training stability

## Architecture Onboarding

**Component Map:**
Data Generator (Diffusion Model) -> Synthetic Data Producer -> Training Pipeline -> VLM/ Diffusion Model -> Output Evaluation

**Critical Path:**
Synthetic data generation → Quality evaluation (FID/BLEU) → Recursive training → Collapse detection → Mitigation strategy application

**Design Tradeoffs:**
- High decoding budgets improve synthetic data quality but increase computational cost
- Model diversity enhances stability but requires multiple models and coordination
- Frozen relabeling prevents collapse but limits model adaptation and requires maintaining multiple model versions

**Failure Signatures:**
- Degrading FID and BLEU scores across training iterations
- Reduced variance in output distributions, particularly in image generation
- Loss of semantic diversity in generated captions or images
- Convergence to repetitive or stereotypical outputs

**First 3 Experiments:**
1. Measure variance collapse in VLM image captioning across 10 recursive training iterations
2. Compare synthetic data quality with varying decoding budgets (50 vs 500 steps)
3. Test model stability with 1, 3, and 5 diverse models in ensemble configurations

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Variance collapse behavior in VLM image captioning contradicts established single-modality model collapse theory and requires further characterization
- Results are primarily based on BLIP-2 and Stable Diffusion XL architectures, limiting generalizability to other VLM and diffusion model variants
- The correlation between quality metrics and robustness is demonstrated but causation is not established, and metrics may fail under different synthetic data generation conditions

## Confidence

**High Confidence:**
- Increased decoding budgets generate more robust synthetic datasets (well-supported by experimental evidence)
- Relabeling with frozen models mitigates model collapse (strongly validated through controlled experiments)

**Medium Confidence:**
- Model diversity is crucial for stability (supported by experiments but relies on specific diversity configurations)
- FID/BLEU-4 correlation with robustness (demonstrated relationship may not hold universally)

**Low Confidence:**
- Variance does not always collapse in VLM image captioning (contradicts established theory, may be specific to experimental setup)

## Next Checks
1. Conduct experiments across broader VLM architectures (CLIP, Flamingo) and diffusion models to verify variance behavior during model collapse
2. Test quality metric correlation using alternative metrics (CLIP similarity, DINO features, human evaluation) across different domains
3. Investigate minimum diversity requirements by systematically varying model numbers and types in ensemble configurations