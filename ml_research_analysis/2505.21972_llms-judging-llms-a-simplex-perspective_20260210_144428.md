---
ver: rpa2
title: 'LLMs Judging LLMs: A Simplex Perspective'
arxiv_id: '2505.21972'
source_url: https://arxiv.org/abs/2505.21972
tags:
- judge
- score
- candidates
- candidate
- judges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how epistemic uncertainty\u2014uncertainty\
  \ about LLM judge quality\u2014affects rankings of LLM candidates when no gold-standard\
  \ labels are available. The authors develop a novel geometric perspective using\
  \ probability simplices to represent judge and candidate score distributions, revealing\
  \ that identifiability depends on scoring levels, judge consistency, and candidate\
  \ separation."
---

# LLMs Judging LLMs: A Simplex Perspective

## Quick Facts
- arXiv ID: 2505.21972
- Source URL: https://arxiv.org/abs/2505.21972
- Reference count: 7
- Primary result: Geometric simplex framework reveals ranking identifiability depends on scoring levels, judge consistency, and candidate separation; Bayesian method achieves 20 percentage points higher coverage than existing methods

## Executive Summary
This paper addresses the critical challenge of epistemic uncertainty in LLM-as-judge evaluation systems, where uncertainty about judge quality affects candidate rankings when gold-standard labels are unavailable. The authors introduce a novel geometric perspective using probability simplices to represent judge and candidate score distributions, revealing that ranking identifiability undergoes a phase transition between 2-level and 3+-level scoring systems. Their Bayesian framework encodes epistemic uncertainty through geometric priors and conducts comprehensive sensitivity analyses across five LLM benchmarks. Experiments demonstrate that rankings are robust in some datasets (GPQA, SummEval) but sensitive in others (Omni-MATH, MMLU Pro), with the Bayesian method achieving substantially higher coverage rates than existing procedures, highlighting the importance of modeling epistemic uncertainty for reliable LLM evaluation.

## Method Summary
The approach represents LLM judges and candidates as points on an (M-1)-dimensional probability simplex, where judge confusion matrix columns form simplex vertices and candidate score distributions become convex combinations of these vertices. The framework establishes geometric conditions for ranking identifiability and implements a Bayesian inference engine using Hamiltonian Monte Carlo in Stan with Dirichlet priors on judge quality and random effects for constancy violations. The method systematically varies hyperparameters (ω for constancy, β_max for judge quality) to conduct sensitivity analysis and compares performance against bootstrap and Bradley-Terry baselines using correlation and coverage metrics across five benchmarks: GPQA, MMLU Pro, MTBench, SummEval (TLDR), and Omni-MATH.

## Key Results
- Ranking identifiability undergoes phase transition: 2-level scoring is identifiable under constancy assumptions while 3+-level scoring is not
- Bayesian framework with geometric priors achieves up to 20 percentage points higher 95% credible interval coverage than existing methods
- GPQA and SummEval rankings remain robust to relaxing constancy assumptions, while MMLU Pro and Omni-MATH show greater sensitivity to judge uncertainty
- Candidate separation in simplex space determines ranking robustness, with tightly clustered candidates leading to fundamentally uncertain rankings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representing LLM judges and candidates as points on a probability simplex enables geometric reasoning about ranking identifiability
- Mechanism: Judge confusion matrix columns form simplex vertices; candidate score distributions become convex combinations (barycentric coordinates) of these vertices. Ranking by expected score maps to comparing heights in an augmented simplex space.
- Core assumption: Judge behavior can be characterized by stable confusion patterns across candidates (constancy assumptions)
- Evidence anchors:
  - [abstract] "both LLM judges and candidates can be represented as points on an (M-1)-dimensional probability simplex, where geometric concepts (e.g., triangle areas) correspond to key ranking concepts"
  - [Section 3] Establishes that "candidate k's expected score E[S* k] corresponds precisely to the height of its vertical projection onto this augmented simplex"
  - [corpus] No direct corpus corroboration for this specific geometric framework; neighboring papers focus on bias mitigation rather than geometric representation
- Break condition: When judge vertices are adversarial (label-flippers) or when candidate distributions are too similar to distinguish geometrically

### Mechanism 2
- Claim: Ranking identifiability undergoes a phase transition between 2-level and 3+-level scoring systems
- Mechanism: In 2-level scoring, all candidates lie on a line segment between judge vertices, so relative positions uniquely determine rankings regardless of exact vertex locations. In 3+ levels, candidates occupy a 2D+ region where different judge configurations can explain the same candidate positions with opposite rankings.
- Core assumption: Judges are non-adversarial (probability of assigning lowest score decreases as true score increases)
- Evidence anchors:
  - [abstract] "we provide a formal basis for the 'folk wisdom' that LLM judges are more effective for two-level scoring (M=2) than multi-level scoring (M>2)"
  - [Section 4.1-4.2] Theorem 1 proves sufficient conditions for 2-level identifiability; Theorem 2 proves non-identifiability for 3+ levels even under strong constancy
  - [corpus] Related work (Guerdan et al. 2025, cited in paper) discusses limitations when no agreed-upon rating scale exists, but doesn't address identifiability phase transitions
- Break condition: When constancy assumptions are violated or when candidate score distributions overlap significantly in the simplex interior

### Mechanism 3
- Claim: Bayesian framework with geometric priors captures epistemic uncertainty and achieves better coverage than existing methods
- Mechanism: Encodes judge quality uncertainty through priors on simplex vertex positions (slopes between vertices) and relaxes constancy through random effects. Varying hyperparameters (ω for constancy, β_max for judge quality) enables sensitivity analysis.
- Core assumption: Prior distributions on judge behavior are reasonably specified; epistemic uncertainty is the dominant missing factor in existing methods
- Evidence anchors:
  - [abstract] "Our Bayesian method achieves substantially higher coverage rates than existing procedures, highlighting the importance of modeling epistemic uncertainty"
  - [Section 6.2, Table 1] Coverage improvements of up to 20 percentage points across benchmarks (e.g., GPQA: 0.778 vs 0.500 for bootstrap)
  - [corpus] Corpus papers on LLM-as-judge bias (e.g., Koo et al. 2024, cited) document biases but don't propose Bayesian uncertainty frameworks
- Break condition: When priors are grossly misspecified or when datasets have fundamentally non-identifiable structure (tight candidate clustering)

## Foundational Learning

- Concept: Probability simplices and barycentric coordinates
  - Why needed here: Core representational framework; understanding how multinomial distributions map to geometric positions is essential for following identifiability proofs
  - Quick check question: Given three points on a triangle representing judge vertices θ₁, θ₂, θ₃, where would a candidate with score distribution π = (0.5, 0.3, 0.2) appear?

- Concept: Aleatoric vs. epistemic uncertainty
  - Why needed here: The paper's central thesis is that existing methods ignore epistemic uncertainty about judge quality; distinguishing these is crucial for understanding the coverage improvements
  - Quick check question: If you double the number of benchmark questions, which type of uncertainty decreases and which remains unchanged?

- Concept: Confusion matrices for multi-class classification
  - Why needed here: Judge behavior is characterized through confusion matrices; each column represents the distribution of assigned scores for a given true score level
  - Quick check question: What does it mean if a judge's confusion matrix is the identity matrix? What geometric position does this correspond to on the simplex?

## Architecture Onboarding

- Component map: Data Collection -> Bayesian Model Implementation -> HMC Inference -> Sensitivity Analysis -> Coverage Evaluation
- Critical path: Understand simplex geometry → grasp identifiability conditions → implement Bayesian model with geometric priors → run sensitivity analyses → validate coverage improvements
- Design tradeoffs:
  - Prevalence perturbation vs. confusion matrix perturbation: Authors chose prevalence (O(JKM) params) over confusion matrix (O(JKM²) params) for computational tractability
  - Independence assumption: Model ignores judge score correlations for tractability; Theorem 5 proves asymptotic consistency despite this misspecification
  - Prior specification: δ parameter in Dirichlet can be tuned for specific bias detection (uniform, inflation, deflation, central tendency)
- Failure signatures:
  - Rankings highly sensitive to ω variation (e.g., MMLU Pro correlation drops to 0.77 at ω=8) → indicates dataset-specific vulnerability to constancy violations
  - Candidates clustered tightly in simplex center (e.g., Omni-MATH) → rankings fundamentally uncertain, need gold-standard labels
  - Coverage <70% despite Bayesian method → check if hyperpriors are appropriately specified or if dataset has identifiability issues
- First 3 experiments:
  1. Replicate sensitivity analysis on new benchmark: Vary ω ∈ {0, 1, 2, 4, 8} and β_max ∈ {1, 5, 10, 20}; plot correlation with baseline to identify robust vs. sensitive datasets
  2. Visualize simplex positions: For any dataset, plot candidate points and sample posterior judge configurations (as in Figure 6 bottom); tight clustering signals sensitivity
  3. Test coverage on held-out data: Split benchmark questions into train/test; verify that Bayesian credible intervals maintain target coverage (aim for >90%) while bootstrap intervals undercover

## Open Questions the Paper Calls Out

- **Open Question 1**: What measurable dataset or candidate properties predict whether rankings will be robust or sensitive to epistemic uncertainty under the simplex framework?
- **Open Question 2**: How do the theoretical identifiability limits and practical sensitivity differ across 4, 5, or more scoring levels beyond the binary and ternary cases formally analyzed?
- **Open Question 3**: Do the findings generalize across diverse LLM judge architectures, sizes, and training paradigms beyond the primarily-tested Claude 3.5 Haiku and GPT-4o mini?
- **Open Question 4**: Can incorporating correlation structures between judge scores improve uncertainty quantification accuracy without sacrificing computational tractability?

## Limitations

- Identifiability conditions rely on constancy assumptions that may not hold for real-world judge behavior with context-dependent patterns
- Prior specification sensitivity requires careful hyperparameter tuning, with performance depending on appropriate choice of ω, β_max, and δ parameters
- Self-evaluation bias handling through exclusion of self-evaluations is pragmatic but incomplete, not explicitly modeling self-referential scoring patterns

## Confidence

- **High confidence**: Geometric simplex representation and correspondence to judge confusion matrices is mathematically sound and well-supported by theorems
- **Medium confidence**: Bayesian framework's practical effectiveness across diverse benchmarks is demonstrated but relies on specific hyperparameter choices
- **Low confidence**: Claims about general applicability to arbitrary judge behavior patterns, particularly in high-stakes scenarios with complex, context-dependent biases

## Next Checks

1. Test whether the same prior hyperparameters (ω, β_max) that work well for one benchmark maintain coverage performance across all five benchmarks, or if dataset-specific tuning is required
2. Construct a synthetic benchmark with intentionally adversarial judges (label-flippers) and evaluate whether geometric identifiability conditions correctly predict ranking uncertainty
3. Apply the framework to a streaming evaluation scenario where new questions arrive continuously, testing whether Bayesian posterior maintains target coverage without frequent recalibration