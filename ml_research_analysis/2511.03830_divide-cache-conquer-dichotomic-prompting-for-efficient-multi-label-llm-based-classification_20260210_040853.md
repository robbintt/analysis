---
ver: rpa2
title: 'Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based
  Classification'
arxiv_id: '2511.03830'
source_url: https://arxiv.org/abs/2511.03830
tags:
- prompting
- dichotomic
- text
- label
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces dichotomic prompting, a method for efficient
  multi-label text classification with LLMs that reformulates the task as a sequence
  of binary (yes/no) decisions. This approach uses independent queries for each label,
  combined with prefix caching to reduce inference costs, particularly for short texts.
---

# Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification

## Quick Facts
- arXiv ID: 2511.03830
- Source URL: https://arxiv.org/abs/2511.03830
- Reference count: 40
- Key outcome: Dichotomic prompting reformulates multi-label classification as binary decisions, using prefix caching for efficiency gains, particularly for short texts in Polish affective analysis.

## Executive Summary
This paper introduces dichotomic prompting, a method that converts multi-label text classification into a sequence of binary (yes/no) decisions, significantly improving efficiency and robustness compared to traditional structured approaches. The method uses independent queries for each label combined with prefix caching to reduce inference costs. Demonstrated on Polish affective text analysis across 24 dimensions, the approach achieves comparable or better accuracy than structured JSON prompting while offering improved efficiency, especially in zero-shot scenarios. The method also enables effective LLM-to-SLM distillation, allowing smaller models to retain the benefits of the dichotomic approach.

## Method Summary
The core innovation is reformulating multi-label classification as a series of independent binary decisions through dichotomic prompting. Instead of structured JSON outputs, the approach generates separate yes/no queries for each label, enabling more efficient inference. Prefix caching is employed to further optimize performance, particularly beneficial for short text inputs. The method leverages a two-stage process: first, a large language model (DeepSeek-V3) generates annotations using the dichotomic approach, which are then aggregated and used to fine-tune smaller specialized models (HerBERT-Large, CLARIN-1B, PLLuM-8B, Gemma3-1B). This distillation process allows efficient deployment while maintaining accuracy, with particular advantages in zero-shot scenarios.

## Key Results
- Dichotomic prompting achieves comparable or better accuracy than structured JSON prompting for multi-label classification
- Prefix caching significantly improves efficiency, especially for short text inputs
- The approach demonstrates improved robustness in zero-shot scenarios compared to structured prompting
- Effective LLM-to-SLM distillation preserves performance while enabling deployment on smaller models

## Why This Works (Mechanism)
The dichotomic prompting approach works by decomposing the complex multi-label classification task into simpler binary decisions. Each label is treated independently through a yes/no query, reducing cognitive load on the model and enabling more focused reasoning. This decomposition allows the model to leverage its natural language understanding capabilities more effectively for each individual decision. The prefix caching mechanism exploits the repetitive nature of the binary queries, storing intermediate computations and reusing them across similar inputs. This is particularly effective for short texts where the prefix remains relatively stable. The approach also benefits from reduced output space complexity, as the model only needs to generate binary responses rather than structured multi-label outputs, leading to more reliable predictions.

## Foundational Learning

**Multi-label classification** - The task of assigning multiple relevant labels to a single input text. Needed to understand the baseline problem being addressed. Quick check: Verify that traditional approaches handle label dependencies and output formatting.

**Binary decomposition** - Converting a multi-class or multi-label problem into multiple binary classification problems. Needed to grasp how complex decisions are simplified. Quick check: Confirm that each label can be independently evaluated without loss of context.

**Prefix caching** - Storing and reusing intermediate computational results from similar inputs to reduce inference costs. Needed to understand the efficiency optimization mechanism. Quick check: Measure cache hit rates across different text lengths and input distributions.

## Architecture Onboarding

**Component map**: User Input -> Dichotomic Prompt Generator -> LLM/SLM Model -> Binary Responses -> Aggregation/Decision Layer -> Final Multi-label Output

**Critical path**: Input text → Dichotomic prompt generation → Binary decision queries → Model inference → Response aggregation → Final label set

**Design tradeoffs**: The approach trades structured output complexity for computational efficiency. While structured JSON prompting provides compact outputs, dichotomic prompting requires more individual queries but enables better prefix caching and simpler decision boundaries. The binary decomposition may miss label correlations that structured approaches naturally capture, but this is offset by improved reliability and reduced output space complexity.

**Failure signatures**: Performance degradation occurs when label dependencies are strong and cannot be captured through independent binary decisions. The approach may struggle with long texts where prefix caching becomes less effective. Annotation quality issues in the distillation phase can propagate to downstream models, particularly if the LLM generates inconsistent binary responses for similar inputs.

**First experiments**:
1. Compare binary decision accuracy against structured output accuracy on a small validation set
2. Measure prefix cache hit rates across varying text lengths (50-500 tokens)
3. Evaluate annotation consistency by checking agreement rates between multiple LLM passes on the same input

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Relies heavily on high-quality LLM-generated annotations, with potential biases or errors in the annotation phase not thoroughly addressed
- Evaluation primarily focused on Polish affective text analysis, limiting generalizability to other languages or domains
- Prefix caching optimization may not scale efficiently for longer documents or different task types

## Confidence

**High Confidence**: The core methodology of converting multi-label classification to binary decisions through dichotomic prompting is well-established and clearly demonstrated. The performance gains over structured JSON prompting and the effectiveness of prefix caching are supported by empirical results.

**Medium Confidence**: The scalability claims across different languages and domains are based on theoretical arguments and limited empirical evidence. The efficiency improvements, while measured, may vary significantly depending on specific implementation details and infrastructure.

**Low Confidence**: The robustness claims in zero-shot scenarios are based on a single dataset (Polish affective text) and may not hold across diverse real-world applications without further validation.

## Next Checks

1. Test the approach on a multilingual dataset spanning at least three different language families to validate cross-lingual generalization claims.

2. Conduct error analysis on the LLM-generated annotations to quantify annotation quality and its impact on downstream model performance.

3. Benchmark prefix caching efficiency across varying text lengths (100-5000 tokens) to establish performance boundaries and identify optimal use cases.