---
ver: rpa2
title: Exploiting Primacy Effect To Improve Large Language Models
arxiv_id: '2507.13949'
source_url: https://arxiv.org/abs/2507.13949
tags:
- label
- bias
- options
- primacy
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses positional bias in Large Language Models (LLMs),
  particularly the primacy effect in multiple-choice question answering (MCQA). The
  authors demonstrate that fine-tuned LLMs exhibit stronger primacy bias than pre-trained
  versions, likely due to exposure to human-like patterns during fine-tuning.
---

# Exploiting Primacy Effect To Improve Large Language Models

## Quick Facts
- arXiv ID: 2507.13949
- Source URL: https://arxiv.org/abs/2507.13949
- Reference count: 4
- Large Language Models exhibit primacy bias in multiple-choice QA, and this bias can be exploited through semantic reordering of answer options to improve accuracy.

## Executive Summary
This paper addresses positional bias in Large Language Models (LLMs) by exploiting the primacy effect in multiple-choice question answering (MCQA). The authors demonstrate that fine-tuned LLMs exhibit stronger primacy bias than pre-trained versions, likely due to exposure to human-like patterns during fine-tuning. They propose a training-free method that reorders answer options based on semantic similarity to the query using token-averaged cosine similarity, leveraging this bias to improve accuracy. Experimental results show significant performance gains across different models (Llama2, Llama3, Mistral) and datasets (CLINC, BANKING, HWU).

## Method Summary
The proposed training-free approach exploits primacy bias by reordering answer options based on semantic similarity to the query. For each query-option pair, token-wise cosine similarity is computed between embeddings, aggregated via mean over all token pairs to get Sim(O,Q), and options are sorted in descending similarity order. This reordered list is then presented to the LLM during inference with temperature=0, no sampling. The method is evaluated across three intent classification datasets (CLINC, BANKING, HWU) using different model sizes (Llama2-7B, Llama3-8B-Instruct, Mistral-7B-Instruct) and compared against a NoSort baseline where options are presented in original order.

## Key Results
- Fine-tuned LLMs show stronger primacy bias than pre-trained versions, with correct labels more frequently appearing in top positions
- Mistral-7B achieved up to 42% top-10 coverage on CLINC dataset when using semantic reordering
- The reordering method provides consistent improvements across different model architectures (Llama2, Llama3, Mistral) and datasets (CLINC, BANKING, HWU)
- Performance gains are attributed to placing semantically similar options earlier, exploiting the model's inherent primacy bias

## Why This Works (Mechanism)
The method works by exploiting the primacy effectâ€”LLMs' tendency to favor earlier-positioned answer options. By reordering options based on semantic similarity to the query, the correct answer is more likely to appear in top positions where the model's bias is strongest. This creates a self-reinforcing loop where semantic relevance and positional advantage combine to improve prediction accuracy.

## Foundational Learning
- **Positional Bias**: LLMs' tendency to favor certain positions in output sequences, particularly the primacy effect where earlier options are preferred. Needed to understand why reordering affects accuracy.
- **Cosine Similarity**: A metric measuring the cosine of the angle between two vectors, used here to quantify semantic similarity between query and option tokens. Needed to implement the similarity-based reordering.
- **Token Embeddings**: Vector representations of individual tokens extracted from the model, used as the basis for computing semantic similarity. Needed to understand how similarity is computed.
- **Top-k Coverage**: Percentage of samples where the correct label appears within the first k positions after reordering. Needed to evaluate the effectiveness of the reordering strategy.

## Architecture Onboarding

**Component Map**: Query -> Token Embeddings -> Cosine Similarity Matrix -> Average Similarity -> Option Ranking -> Reordered Options -> LLM Prediction

**Critical Path**: The critical path involves computing token-averaged cosine similarity between each query-option pair, ranking options by this similarity score, and presenting the reordered options to the LLM for prediction.

**Design Tradeoffs**: The method trades computational overhead of similarity computation during inference against improved accuracy. It avoids fine-tuning but requires embedding extraction and similarity calculations for each query-option pair.

**Failure Signatures**: Poor performance when semantic similarity does not correlate with correctness, when model bias is weak or non-existent, or when option presentation order has minimal impact on predictions.

**3 First Experiments**:
1. Implement token-averaged cosine similarity on a small sample and verify similarity scores match expected values
2. Test reordering on a single model-dataset pair to confirm accuracy improvement over NoSort baseline
3. Compare performance across different embedding extraction methods (CLS token vs. token-averaged) to validate the chosen approach

## Open Questions the Paper Calls Out
- Does semantic similarity-based reordering improve performance in complex reasoning tasks like multi-hop QA? (explicit, unresolved due to limited scope to intent classification)
- Can the "bias exploitation" framework be successfully extended to non-positional cognitive biases, such as emotional bias? (explicit, unresolved as current study only validates positional biases)
- Is the reordering method robust in low-resource languages and multimodal input settings? (explicit, unresolved due to focus on English datasets)
- Does integrating semantic reordering with prompt engineering techniques provide compounding accuracy benefits? (explicit, unresolved as study used fixed prompts to isolate reordering effect)

## Limitations
- The mechanism linking semantic similarity to position preference is not explicitly validated through ablation studies
- Improvements are demonstrated only on English intent classification datasets with relatively small sample sizes (3K-3.7K samples)
- Comparison between pre-trained and fine-tuned models doesn't control for other potential confounding factors
- No statistical significance testing across multiple random seeds or cross-validation folds

## Confidence
- **High confidence**: The observation that fine-tuned models exhibit stronger primacy bias than pre-trained versions is empirically demonstrated
- **Medium confidence**: The claim that token-averaged cosine similarity effectively captures semantic relevance for reordering is plausible but not rigorously validated
- **Low confidence**: The explanation that primacy bias emerges from exposure to human-like patterns during fine-tuning is speculative and not directly tested

## Next Checks
1. Conduct ablation studies comparing token-averaged cosine similarity against alternative embedding strategies (CLS token embeddings, sentence transformers, different model layers)
2. Perform statistical significance testing across multiple random seeds and cross-validation folds to establish confidence intervals for accuracy improvements
3. Test the reordering method on out-of-domain datasets and different task types (factoid QA, sentiment analysis) to evaluate generalizability beyond intent classification