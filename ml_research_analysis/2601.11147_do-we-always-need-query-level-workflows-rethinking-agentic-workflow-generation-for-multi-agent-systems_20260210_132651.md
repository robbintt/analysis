---
ver: rpa2
title: Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation
  for Multi-Agent Systems
arxiv_id: '2601.11147'
source_url: https://arxiv.org/abs/2601.11147
tags:
- workflow
- self
- answer
- workflows
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the costs and benefits of generating multi-agent
  workflows at query-level versus task-level. Through empirical analysis, the authors
  find that query-level workflow generation is often unnecessary, as a small set of
  top task-level workflows can cover most queries and achieve similar performance.
---

# Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems

## Quick Facts
- arXiv ID: 2601.11147
- Source URL: https://arxiv.org/abs/2601.11147
- Reference count: 40
- Key outcome: Query-level workflow generation is often unnecessary; a small set of top task-level workflows can cover most queries with similar performance while reducing token usage by up to 83%.

## Executive Summary
This paper challenges the prevailing practice of query-level workflow generation in multi-agent systems by demonstrating that task-level approaches can achieve comparable coverage and performance with significantly lower computational costs. Through empirical analysis across six benchmarks, the authors find that a small set of top task-level workflows covers most queries and achieves similar performance to query-level methods. To address the token-expensive validation in task-level approaches, they propose SCALE - a framework using calibrated self-prediction by the optimizer LLM with few-shot execution instead of full validation runs.

## Method Summary
SCALE is a low-cost task-level workflow generation framework that replaces exhaustive validation execution with calibrated self-prediction. It consists of a warm-up phase where M rounds of MCTS-style loops collect full validation execution statistics, followed by a surrogate evaluation phase. In this phase, the optimizer LLM (Qwen3-8B) performs self-prediction using a dedicated evaluation prompt, and few-shot execution on a stratified 1-3% subset of validation data. The final calibrated score combines both predictions using an adaptive weighting factor α that activates when self-prediction and few-shot results differ beyond a tolerance threshold. This approach maintains competitive performance (0.61% average degradation) while reducing token usage by 54-83%.

## Key Results
- A small set of top-5 task-level workflows achieves 93.87%-100% coverage across benchmarks
- Repeat-5 of Top-1 task-level workflow matches query-level performance, indicating stochastic execution variance rather than structural differences
- SCALE maintains competitive performance with only 0.61% average degradation compared to existing methods
- Token usage reduced by up to 83% through calibrated self-prediction and few-shot execution
- Calibrated score achieves Pearson correlation 0.52 with full execution and MAE 0.16

## Why This Works (Mechanism)

### Mechanism 1: Task-Level Workflow Coverage Sufficiency
- Claim: A small pool of task-level workflows achieves query coverage comparable to query-level methods without per-query generation overhead.
- Mechanism: Queries within a task share structural patterns; top-K task-level workflows collectively span the solution space that query-level customization would attempt to capture individually. Coverage gains often come from stochastic execution variance rather than genuinely distinct workflow structures.
- Core assumption: The task distribution has sufficient structural regularity that a discrete workflow set can approximate the performance of infinitely many query-specific workflows.
- Evidence anchors:
  - [abstract]: "a small set of top-K best task-level workflows together already covers equivalent or even more queries"
  - [section 3.1, Table 1]: Top-5 task-level workflows achieve 93.87%-100% coverage across benchmarks
  - [corpus]: Neighbor paper "Difficulty-Aware Agentic Orchestration" (FMR=0.56) confirms static/task-level workflows "over-process simple queries or underperform on complex ones"

### Mechanism 2: Calibrated Self-Prediction as Evaluation Surrogate
- Claim: An LLM optimizer can predict its own workflow's expected performance, and few-shot execution calibration corrects systematic bias without full validation runs.
- Mechanism: The optimizer performs static analysis of workflow code and outputs a probability score. This prediction is then linearly combined with a few-shot execution score from a stratified subset (1-3% of validation), where calibration weight α increases with prediction-execution discrepancy.
- Core assumption: The optimizer has sufficient self-evaluative capacity to produce ordinal rankings that correlate with true execution, even if absolute scores are biased.
- Evidence anchors:
  - [abstract]: "Self prediction of the optimizer with few shot CALibration for Evaluation instead of full validation execution"
  - [section 4.2.2, Equation 8-9]: Calibrated score Ŝ = (1-α)·S_pred + α·S_few with adaptive α
  - [section 5.3, Table 3]: Calibrated score achieves Pearson correlation 0.52 with full execution

### Mechanism 3: Warm-Up Phase for Distribution Anchoring
- Claim: A short warm-up phase with full execution provides statistical anchors that guide both few-shot sampling and self-prediction calibration.
- Mechanism: During M warm-up rounds, full validation execution establishes per-query empirical scores. These scores stratify the validation set into difficulty bins; few-shot samples are drawn proportionally to bin sizes. Warm-up experience also provides reference trajectories for the self-prediction prompt.
- Core assumption: Early workflows are representative enough that their query-level scores define stable difficulty distributions.
- Evidence anchors:
  - [section 4.2.1]: "The sampling of D_few is guided by the full-execution statistics collected during warm-up"
  - [section 4.2.2]: "With warming up workflows {W_m}, for each validation query q, we compute its empirical warm-up score s̄(q)"

## Foundational Learning

- Concept: **Workflow Search Space (W)** — the formal space of agent configurations (prompts P_i, control flow E, operator parameters θ_i).
  - Why needed here: SCALE optimizes over W without exhaustively evaluating it; understanding the structure clarifies what self-prediction analyzes.
  - Quick check question: Can you articulate why workflow evaluation cost scales with |D_val| × |W| in task-level methods?

- Concept: **Generative Reward Modeling** — using an LLM to directly generate evaluation scores rather than training a separate reward model.
  - Why needed here: SCALE's self-prediction is a form of generative reward modeling; the optimizer outputs a probability score in a dedicated evaluation context.
  - Quick check question: How does generative reward modeling differ from discriminative reward model training (e.g., DPO)?

- Concept: **Stratified Sampling with Calibration** — sampling subsets that preserve distributional properties, then using them to correct estimator bias.
  - Why needed here: Few-shot calibration samples are stratified by warm-up difficulty scores; the calibration weight α corrects self-prediction bias.
  - Quick check question: If warm-up scores are uniformly distributed, how would stratified sampling differ from uniform random sampling?

## Architecture Onboarding

- Component map: Warm-up Engine -> Self-Prediction Module -> Few-Shot Executor -> Calibration Combiner -> Selection/Expansion Policy
- Critical path: Warm-up → bin construction → few-shot sampling → self-prediction + few-shot execution → calibrated score → selection/expansion → workflow generation
- Design tradeoffs:
  - **Warm-up rounds M**: More rounds improve stratification but increase upfront cost
  - **Few-shot ratio ρ**: 1-3% of validation; higher ρ improves accuracy but reduces token savings
  - **Calibration tolerance τ**: Controls when α activates; low τ triggers calibration more aggressively
  - **α_max**: Caps calibration weight; prevents over-reliance on noisy few-shot estimates
- Failure signatures:
  - Self-prediction alone shows ~10-20% performance drops on coding tasks
  - Confidence-based scoring fails catastrophically on most benchmarks (0% on DROP, GSM8K, MATH)
  - Few-shot alone shows high variance (MAE 0.22) despite good correlation
- First 3 experiments:
  1. Ablate warm-up: Run SCALE with M=0 and measure correlation degradation vs. full SCALE
  2. Vary few-shot ratio: Test ρ ∈ {0.5%, 1%, 2%, 5%} and plot token cost vs. MAE/Pearson tradeoff curve
  3. Cross-benchmark transfer: Train warm-up statistics on one benchmark and apply stratified sampling to another

## Open Questions the Paper Calls Out

- Question: Can a hybrid framework effectively combine the generalization of task-level workflows with the fine-grained adaptability of query-level methods?
  - Basis in paper: [explicit] The authors state in the Limitations section that their method "has not yet fully leveraged the generalization capability of task-level approaches together with the fine-grained adaptability of query-level methods."
  - Why unresolved: The proposed SCALE framework focuses strictly on optimizing the task-level generation paradigm and does not implement a mixed-granularity approach.
  - What evidence would resolve it: A study evaluating an architecture that uses SCALE for base generation but switches to query-level generation for outlier queries.

- Question: To what extent does SCALE's performance depend on the specific self-evaluation capabilities of the optimizer LLM?
  - Basis in paper: [inferred] The method relies on the optimizer (Qwen3-8B) performing "self prediction," and the ablation study shows that uncalibrated self-prediction performs significantly worse.
  - Why unresolved: It is unclear if the optimizer's success is due to general reasoning or specific instruction-following training present in Qwen models.
  - What evidence would resolve it: Experiments scaling the optimizer to smaller models or different model families to see if the self-prediction accuracy holds.

- Question: How do task-level workflows generated by SCALE generalize across diverse domains?
  - Basis in paper: [explicit] The authors explicitly note they "do not assess cross-domain generalization in this work."
  - Why unresolved: The empirical validation was restricted to within-domain benchmarks, testing workflows on the same type of task they were optimized for.
  - What evidence would resolve it: Testing the transferability of a workflow optimized for one domain on a disparate domain.

## Limitations

- The method has not yet fully leveraged the generalization capability of task-level approaches together with the fine-grained adaptability of query-level methods
- Does not assess cross-domain generalization - the method was tested within-domain benchmarks only
- Performance depends on the specific self-evaluation capabilities of the optimizer LLM, which may not transfer to other model families

## Confidence

- **High**: Task-level workflow coverage sufficiency is well-supported by empirical data showing 93.87%-100% coverage with top-5 workflows
- **Medium**: Calibrated self-prediction mechanism effectiveness is demonstrated but relies on specific LLM capabilities that may not generalize
- **Medium**: Token savings claims (54-83%) are well-documented but depend on implementation details not fully specified in the paper

## Next Checks

1. Implement the warm-up stage with M rounds of MCTS-style loops and verify that stratification by difficulty scores improves few-shot sampling efficiency
2. Test the adaptive calibration mechanism by comparing performance with fixed α vs. the proposed discrepancy-based α computation
3. Validate the claim that stochastic execution variance explains much of the query-level vs. task-level performance gap by running multiple executions of the same top-1 task-level workflow