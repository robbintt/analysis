---
ver: rpa2
title: 'A New Strategy for Artificial Intelligence: Training Foundation Models Directly
  on Human Brain Data'
arxiv_id: '2601.12053'
source_url: https://arxiv.org/abs/2601.12053
tags:
- data
- human
- could
- foundation
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a new strategy for AI: training foundation
  models directly on human brain data to overcome current limitations. While foundation
  models excel across domains, they rely on surface-level statistical regularities
  from human-generated data.'
---

# A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data

## Quick Facts
- **arXiv ID:** 2601.12053
- **Source URL:** https://arxiv.org/abs/2601.12053
- **Reference count:** 26
- **Primary result:** Proposes training foundation models on neuroimaging data to overcome current AI limitations by accessing cognitive latent spaces not captured in observable actions.

## Executive Summary
This paper proposes a novel strategy for improving foundation models by training them directly on human brain data, arguing that neuroimaging can reveal cognitive processes beyond what is accessible through text and other human-generated data. The author suggests this approach could overcome key limitations in current AI systems, particularly around value alignment and reasoning. Two specific methods are proposed: reinforcement learning from human brain (RLHB) for improving value alignment using neural signals from valuation regions, and chain of thought from human brain (CoTHB) for enhancing reasoning using signals from execution regions. This represents a middle ground between scaling current architectures and exploring neuroscience-inspired solutions.

## Method Summary
The paper proposes training foundation models on neuroimaging data (EEG, fMRI, MEG) to access cognitive latent spaces beyond observable actions. Two methods are detailed: RLHB uses neural signals from valuation regions (ventral striatum, vmPFC, OFC, amygdala, insula) to improve value alignment, replacing or augmenting human ratings with continuous valuation signals. CoTHB uses neural signals from execution regions (DLPFC, ACC, FPC, PPC, pre-SMA) to enhance reasoning by guiding branching and backtracking decisions. The approach requires signal extraction from specific brain regions, cross-subject alignment, and integration into existing training pipelines through auxiliary loss functions or guidance mechanisms.

## Key Results
- Foundation models rely on surface-level statistical patterns from human-generated data, missing deeper cognitive processes
- Neuroimaging data could reveal cognitive latent space beyond observable behavior, potentially improving model alignment and reasoning
- Two proposed methods target specific limitations: RLHB for valuation/alignment, CoTHB for execution/reasoning
- Promising brain regions identified across four cognitive levels: perception, valuation, execution, and integration

## Why This Works (Mechanism)

### Mechanism 1
Neural signals from valuation regions may improve value alignment in foundation models. Regions like ventral striatum, vmPFC, and OFC encode reward, value, confidence, and self-control signals that could supplement discrete preference ratings with continuous, pre-conscious valuation data. Core assumption: Brain-generated signals capture valuation processes that human-generated ratings cannot fully express, and these signals can be reliably extracted and mapped to model training objectives.

### Mechanism 2
Neural signals from execution regions may enhance multi-step reasoning emulation. Regions like DLPFC, ACC, and FPC encode rule reliability, alternative rules, and exploratory behavior valueâ€”signals that could guide branching and backtracking in chain-of-thought reasoning. Core assumption: Cognitive control processes in prefrontal cortex can be approximated from neuroimaging and translated into computational operations for reasoning models.

### Mechanism 3
Brain-generated data may reveal cognitive latent space beyond observable behavior. Neuroimaging approximates brain activity B(t), which contains but exceeds observable actions A(t); accessing B*(t) provides information about hidden variables (values, hesitations, alternatives, effort) not captured in text or speech. Core assumption: Neuroimaging data quality is sufficient to extract meaningful signals, and these signals generalize across individuals after alignment.

## Foundational Learning

- **Transformer-based foundation model training pipeline**
  - Why needed here: The paper proposes modifying RLHF and CoT, which assume familiarity with pre-training, supervised fine-tuning, and preference optimization stages.
  - Quick check question: Can you explain how RLHF converts human ratings into a reward model that guides policy updates?

- **Neuroimaging signal characteristics (spatial/temporal resolution, invasiveness)**
  - Why needed here: Method choice (EEG vs fMRI vs ECoG) determines what neural signals are accessible and how they can be integrated into training.
  - Quick check question: Which neuroimaging technique would you choose for real-time valuation signal acquisition during an RLHF session, and what tradeoff does it entail?

- **Reverse inference in cognitive neuroscience**
  - Why needed here: Inferring cognitive processes (e.g., "value," "rule reliability") from brain region activation is non-trivial; understanding this limitation is critical for assessing mechanism viability.
  - Quick check question: Why can't we simply equate vmPFC activation with "value" without additional experimental validation?

## Architecture Onboarding

- **Component map:** Data acquisition layer -> Signal processing layer -> Neural signal decoder -> Integration layer -> Foundation model

- **Critical path:**
  1. Acquire task-aligned neuroimaging data (RLHB: model output evaluation; CoTHB: reasoning task performance)
  2. Extract region-specific signals (valuation: ventral striatum/vmPFC; execution: DLPFC/ACC/FPC)
  3. Validate signal-to-cognitive-variable mapping on held-out data
  4. Design auxiliary loss or guidance function using neural signals
  5. Integrate into fine-tuning pipeline and measure downstream task improvement

- **Design tradeoffs:**
  - EEG: Low cost, portable, high temporal resolution, but low spatial resolution and cortical-only coverage
  - fMRI: High spatial resolution, whole-brain coverage, but expensive, low temporal resolution, immobile subjects
  - ECoG: High spatiotemporal resolution, but invasive and limited to medical contexts
  - Scalability vs. depth: RLHB/CoTHB propose using limited neuroimaging for "strategically chosen, high-value steps" rather than large-scale pre-training

- **Failure signatures:**
  - High inter-subject variability preventing cross-individual generalization
  - Signal-to-noise ratio too low for reliable cognitive variable extraction
  - Neural signals correlate with confounds (attention, fatigue) rather than target processes
  - Integration method degrades base model performance on non-brain-guided tasks

- **First 3 experiments:**
  1. **Replicate perception-level alignment:** Train or fine-tune a visual model using existing EEG/fMRI datasets to verify pipeline works at a well-studied level before extending to valuation/execution.
  2. **Pilot RLHB with EEG reward signals:** Collect EEG during an RLHF session, extract error-related potentials or valuation markers, and compare reward model quality against ratings-only baseline.
  3. **Validate cross-subject signal alignment:** Test whether neural signals from valuation/execution regions can be aligned across subjects well enough to train a shared decoder, or if individual-specific models are required.

## Open Questions the Paper Calls Out

### Open Question 1
Can neural signals recorded during short-term reasoning tasks effectively generalize to guide foundation models through complex, long-horizon problems? Basis: The author notes uncertainty about whether insights from neural signals on short reasoning tasks generalize to longer problems. Why unresolved: Neuroimaging acquisition is physically constrained to shorter sessions compared to extended reasoning timeframes. What evidence would resolve it: Successful transfer learning where models trained on neural signals from short tasks improve performance on distinct, multi-step reasoning problems.

### Open Question 2
Are human cognitive processes computationally optimal enough to serve as a sufficient template for advanced AI reasoning? Basis: The paper questions whether cognitive processes are close to optimal computational trade-offs or if more efficient reasoning models exist. Why unresolved: Mimicking biological evolution might impose cognitive bottlenecks limiting AI to sub-optimal strategies. What evidence would resolve it: Comparative studies showing brain-trained models fail to exceed efficiency of non-biologically constrained models on abstract reasoning tasks.

### Open Question 3
What is the minimum volume of neuroimaging data required to achieve generalization in brain-trained foundation models? Basis: The text states the amount of data needed remains open while noting some generalization should be attainable. Why unresolved: Neuroimaging data is scarce and expensive, and it's unknown if current dataset sizes are sufficient to move beyond surface-level patterns. What evidence would resolve it: Establishing scaling laws correlating specific volumes of high-quality neuroimaging data with measurable improvements in model alignment or reasoning.

## Limitations
- The paper presents conceptual strategies without detailed algorithms for signal extraction, cross-subject alignment, or integration into training pipelines
- Key technical details such as converting raw neural signals into scalar rewards for reinforcement learning remain unspecified
- Success critically depends on signal-to-noise ratio and generalizability of neuroimaging data across subjects and experimental conditions

## Confidence
- **High confidence**: Foundation models rely on surface-level statistical patterns from human-generated data, and neuroimaging could theoretically access deeper cognitive processes
- **Medium confidence**: Proposed brain regions and their associated cognitive functions are well-established in neuroscience literature and plausibly relevant to targeted model limitations
- **Low confidence**: Feasibility of translating neural signals into practical training signals that would meaningfully improve model performance beyond current human-data approaches

## Next Checks
1. **Signal Extraction Feasibility**: Using existing open neuroimaging datasets, implement and validate decoders that can reliably extract valuation signals from vmPFC/striatum and execution signals from DLPFC/ACC across multiple subjects with correlation above 0.5 to behavioral proxies.

2. **Cross-Subject Alignment Validation**: Test whether neural signals from valuation/execution regions can be aligned across subjects well enough to train a shared decoder, or if individual-specific models are required, by measuring decoder performance when trained on pooled vs. subject-specific data.

3. **Proof-of-Concept Integration**: Implement a minimal RLHB pipeline using EEG error-related potentials during an RLHF session, comparing reward model quality against a ratings-only baseline on a simple text alignment task to establish whether neural signals provide additional signal beyond human ratings.