---
ver: rpa2
title: Evaluating Large Language Model with Knowledge Oriented Language Specific Simple
  Question Answering
arxiv_id: '2505.16591'
source_url: https://arxiv.org/abs/2505.16591
tags:
- answer
- general
- question
- language
- specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces KoLasSimpleQA, a benchmark for evaluating\
  \ multilingual factual ability of large language models. It features 9 languages\
  \ and two domains\u2014general (global facts) and language-specific (history, culture,\
  \ regional traditions)\u2014with 2,147 QA pairs."
---

# Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering

## Quick Facts
- arXiv ID: 2505.16591
- Source URL: https://arxiv.org/abs/2505.16591
- Reference count: 40
- This paper introduces KoLasSimpleQA, a benchmark for evaluating multilingual factual ability of large language models. It features 9 languages and two domains—general (global facts) and language-specific (history, culture, regional traditions)—with 2,147 QA pairs. The study evaluates mainstream LLMs and Large Reasoning Models, revealing significant performance gaps: models perform worse in language-specific domains, benefit from translating non-English queries into English only in the general domain, show poorer calibration in language-specific contexts, and exhibit less robust knowledge memorization. Notably, LRMs achieve higher calibration and better bidirectional knowledge robustness, but still struggle in language-specific tasks. The findings highlight the need for targeted multilingual optimization. KoLasSimpleQA is publicly available to support future LLM development.

## Executive Summary
This paper introduces KoLasSimpleQA, a comprehensive benchmark designed to evaluate the multilingual factual abilities of large language models (LLMs) across nine languages and two distinct domains: general knowledge and language-specific knowledge. The study systematically compares mainstream LLMs and Large Reasoning Models (LRMs), revealing significant performance disparities, particularly in language-specific contexts. The findings underscore the challenges of cross-lingual factual retrieval and highlight the need for targeted optimizations in multilingual model development.

## Method Summary
The paper presents KoLasSimpleQA, a benchmark dataset comprising 2,147 QA pairs across nine languages and two domains: general knowledge (global facts) and language-specific knowledge (history, culture, regional traditions). The dataset is constructed to evaluate multilingual factual ability, with each question paired with a correct answer and additional wrong answers for evaluation purposes. The study evaluates mainstream LLMs and Large Reasoning Models (LRMs) using this benchmark, employing metrics such as Proportion of Bidirectional Correctness ($P_{bi}$) and calibration scores to assess model performance and reliability.

## Key Results
- Models perform worse in language-specific domains compared to general domains.
- Translating non-English queries into English benefits performance in the general domain but not in the language-specific domain.
- LRMs achieve higher calibration and better bidirectional knowledge robustness than standard LLMs.
- Language-specific knowledge poses greater challenges for memorization and reasoning in LLMs.

## Why This Works (Mechanism)
The benchmark leverages a structured approach to evaluate multilingual factual ability by separating general and language-specific domains. This design allows for a granular analysis of model performance across different types of knowledge, revealing domain-specific weaknesses and strengths. The use of bidirectional correctness and calibration metrics provides insights into the robustness and reliability of model knowledge retrieval.

## Foundational Learning
- **Multilingual Factual Retrieval**: The ability to accurately retrieve factual information across multiple languages. *Why needed*: Essential for evaluating cross-lingual model performance. *Quick check*: Compare model accuracy across languages in the general domain.
- **Domain-Specific Knowledge**: Understanding the distinction between general and language-specific knowledge. *Why needed*: Highlights the challenges of cultural and regional context. *Quick check*: Analyze performance gaps between domains.
- **Bidirectional Correctness**: Measuring the consistency of knowledge retrieval in both directions (e.g., A->B and B->A). *Why needed*: Indicates the robustness of memorized knowledge. *Quick check*: Evaluate $P_{bi}$ scores across domains.
- **Calibration in Multilingual Contexts**: Assessing the reliability of model confidence in predictions across languages. *Why needed*: Critical for real-world deployment of multilingual models. *Quick check*: Compare calibration scores for translated vs. original queries.
- **Large Reasoning Models (LRMs)**: Models designed to enhance reasoning capabilities through self-correction and exploration. *Why needed*: To evaluate if advanced reasoning can overcome sparse memorization. *Quick check*: Compare LRM performance with standard LLMs in language-specific tasks.
- **Cultural Entity Representation**: How models encode and retrieve knowledge about cultural and regional entities. *Why needed*: To understand translation-induced semantic drift. *Quick check*: Analyze errors in translated language-specific queries.

## Architecture Onboarding
- **Component Map**: KoLasSimpleQA Dataset -> Benchmark Evaluation -> Model Performance Metrics -> Analysis of Gaps
- **Critical Path**: Dataset creation -> Model evaluation -> Metric computation -> Performance analysis
- **Design Tradeoffs**: Balancing dataset size with linguistic and cultural diversity; choosing between general and language-specific domains.
- **Failure Signatures**: Poor performance in language-specific domains; inconsistent bidirectional correctness; low calibration in translated queries.
- **First Experiments**:
  1. Evaluate a baseline LLM on the general domain to establish a performance baseline.
  2. Test the impact of translating non-English queries to English in both domains.
  3. Compare LRMs and standard LLMs in terms of calibration and bidirectional correctness.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Why does translating non-English queries into English fail to enhance performance in the language-specific domain while succeeding in the general domain?
- Basis in paper: [explicit] Section 4.2 states that while the "tran_en" setting improves performance in the general domain, in the language-specific domain, "models generally perform better when questions are presented in their original language."
- Why unresolved: The paper identifies this performance gap but does not isolate whether the failure is due to the loss of cultural nuance during translation or the model's English-centric bias in handling regional entities.
- What evidence would resolve it: An error analysis comparing the semantic drift of cultural entities during translation versus the model's inherent lack of knowledge for those entities in the English vector space.

### Open Question 2
- Question: To what extent does the lack of pretraining data limit the effectiveness of the "reasoning search" capabilities of Large Reasoning Models (LRMs)?
- Basis in paper: [inferred] Section 4.5 notes that LRMs have "substantial opportunities to answer correctly" in the general domain through reasoning, but for language-specific knowledge, "the lack of relevant pretraining knowledge becomes evident."
- Why unresolved: It remains unclear if the advanced reasoning processes of LRMs (self-correction and exploration) are sufficient to overcome sparse memorization, or if they hallucinate more aggressively when factual grounding is missing.
- What evidence would resolve it: Evaluating LRMs on language-specific questions where the supporting knowledge is explicitly provided in the context to see if reasoning accuracy matches general domain levels.

### Open Question 3
- Question: How can the "Reversal Curse" be mitigated specifically for language-specific knowledge where bidirectional consistency is lowest?
- Basis in paper: [explicit] Section 4.4 reports that models achieve significantly lower Proportion of Bidirectional Correctness ($P_{bi}$) in the language-specific domain (implying less robust memorization) compared to the general domain.
- Why unresolved: The paper quantifies the lack of robustness in language-specific contexts but does not explore if standard training pipelines (e.g., next-token prediction) fundamentally fail to capture reverse relations for low-frequency cultural entities.
- What evidence would resolve it: A study testing if bidirectional data augmentation (explicitly training on reverse triples) improves $P_{bi}$ for language-specific entries more significantly than for general entries.

## Limitations
- Potential sampling bias in the 2,147 QA pairs, which may not fully represent the diversity of factual knowledge across the 9 languages and two domains.
- Performance gaps observed between models could be influenced by translation quality and alignment issues when converting non-English queries to English.
- The calibration metrics used may not capture all aspects of model reliability in multilingual contexts.
- Uncertainty about whether the observed benefits of translating queries into English are consistent across different types of factual questions or language pairs.

## Confidence
- **High**: The finding that LRMs achieve higher calibration than standard LLMs is well-supported by the experimental results.
- **Medium**: The claim about significant performance gaps in language-specific domains is plausible but may depend on the specific dataset composition.
- **Low**: The assertion that translating non-English queries to English benefits performance only in the general domain requires further validation with larger and more diverse datasets.

## Next Checks
1. Replicate the benchmark evaluation with a larger and more diverse set of QA pairs across all 9 languages to assess the robustness of the observed performance gaps.
2. Conduct ablation studies to isolate the impact of translation quality on model performance in both general and language-specific domains.
3. Evaluate additional LLM and LRM architectures not included in the original study to determine if the observed trends are consistent across a broader range of models.