---
ver: rpa2
title: Optimizing Model Splitting and Device Task Assignment for Deceptive Signal
  Assisted Private Multi-hop Split Learning
arxiv_id: '2507.07323'
source_url: https://arxiv.org/abs/2507.07323
tags:
- device
- training
- devices
- information
- eavesdroppers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of secure split learning in the
  presence of eavesdroppers who attempt to intercept model and data information from
  edge devices. The authors propose a novel soft actor-critic deep reinforcement learning
  framework with intrinsic curiosity module and cross-attention (ICM-CA) that enables
  a centralized server to dynamically determine model training device sets, optimize
  model splitting, and allocate deceptive signal transmission devices while considering
  energy, time, and privacy constraints.
---

# Optimizing Model Splitting and Device Task Assignment for Deceptive Signal Assisted Private Multi-hop Split Learning

## Quick Facts
- arXiv ID: 2507.07323
- Source URL: https://arxiv.org/abs/2507.07323
- Reference count: 37
- Primary result: ICM-CA framework improves convergence rate by up to 3x and reduces information leakage to eavesdroppers by up to 13% compared to standard SAC.

## Executive Summary
This paper proposes a novel soft actor-critic deep reinforcement learning framework with intrinsic curiosity module and cross-attention (ICM-CA) for secure split learning in wireless networks with eavesdroppers. The centralized server dynamically determines model training device sets, optimizes model splitting, and allocates deceptive signal transmission devices while considering energy, time, and privacy constraints. The ICM module encourages exploration of novel states through intrinsic rewards based on prediction errors, while the cross-attention mechanism improves training efficiency by weighting historical state-action pairs based on their relevance to the current state. Simulation results demonstrate significant improvements in convergence speed and information leakage reduction compared to traditional SAC algorithms.

## Method Summary
The authors address secure split learning by formulating it as a constrained optimization problem solved through deep reinforcement learning. The method employs a soft actor-critic (SAC) framework enhanced with an intrinsic curiosity module (ICM) and cross-attention (CA) mechanism. The ICM generates exploration rewards by predicting next state features and using prediction error as an intrinsic reward signal, while the CA module processes historical state-action pairs to create a context-aware state representation. The agent learns to allocate devices for training sub-models or transmitting deceptive signals to minimize information leakage while satisfying energy and delay constraints. The framework uses action masking to prevent invalid assignments and incorporates SNR-based deception to reduce eavesdropper success rates.

## Key Results
- ICM-CA framework achieves up to 3x faster convergence compared to standard SAC algorithms
- Information leakage to eavesdroppers reduced by up to 13% compared to traditional approaches
- Successfully balances trade-offs between privacy protection, energy consumption, and training delay in multi-hop split learning scenarios

## Why This Works (Mechanism)

### Mechanism 1: Intrinsic Curiosity for Exploration Efficiency
The Intrinsic Curiosity Module (ICM) improves convergence speed by generating auxiliary rewards that incentivize exploration of novel states. It uses a forward dynamic model to predict the next state feature vector given the current state and action, with prediction error serving as an intrinsic reward. High error implies unfamiliar states, prompting further exploration. The core assumption is that state features are relevant to decision-making and prediction error correlates with "novelty" leading to optimal policies. Break condition: high stochastic noise unrelated to the task may drive the agent to explore noise rather than optimize the objective.

### Mechanism 2: Cross-Attention for Temporal Context
The Cross-Attention (CA) mechanism improves policy quality by weighting historical state-action pairs based on their relevance to the current state. The actor network computes a combined state representation using Query, Key, and Value matrices derived from the current state and history buffer. This allows the network to "pay attention" to specific past interactions that correlate with the current optimization context. The core assumption is that optimal device assignment depends on temporal dependencies, not just immediate Markovian states. Break condition: if the optimal policy is strictly Markovian, attention adds computational overhead without performance gain.

### Mechanism 3: SNR-Based Deceptive Signaling
Allocating specific devices to transmit deceptive signals reduces information leakage by lowering the effective SNR of legitimate signals at eavesdroppers. Eavesdroppers capture signals with the highest SNR, so selecting devices close to eavesdroppers to transmit interference ensures the eavesdropper's received signal is dominated by noise/deception. The core assumption is that eavesdroppers cannot distinguish legitimate model transmission from deceptive signals and select the strongest signal. Break condition: if eavesdroppers employ beamforming or interference cancellation, the SNR assumption fails.

## Foundational Learning

- **Split Learning (SL):** Partitioning a DNN into sub-models distributed across devices and server, with forward/backward propagation happening sequentially across segments. Quick check: Can you explain how loss gradient propagates from server back to first device in a chain of 4 devices?

- **Soft Actor-Critic (SAC):** Base RL algorithm using entropy regularization. Understanding the entropy term $\alpha H(\pi)$ in the actor's loss function is key to seeing why SAC was chosen over DQN for handling continuous power allocation and discrete device selection.

- **Rayleigh Fading & Path Loss:** Physical layer attributes where signal power decays with distance ($m^{-2}$) and experiences random fading ($o$). Required to understand channel capacity, delay, and deception strategy. Quick check: How does increasing distance between training device and eavesdropper affect information leakage probability in Eq. (12)?

## Architecture Onboarding

- **Component map:** Central Agent (Server) -> Actor, Critic, ICM, Cross-Attention networks -> Environment (Wireless Network with Devices $U$ and Eavesdroppers $E$) -> Action Mask layer -> Invalid action filtering

- **Critical path:** 1) State Encoding: Current network status + History Buffer $H$ processed by CA module $\to s'(n)$; 2) Policy Inference: Actor network samples action $a(n)$ (device selection, power, split point); 3) Exploration/Curiosity: ICM evaluates transition $(s, a, s')$ to generate intrinsic reward; 4) Update: Replay buffer stores experience; Critic and Actor update via gradient descent

- **Design tradeoffs:** Privacy vs. Energy (high deceptive signal power reduces leakage but consumes more energy); Convergence vs. Complexity (ICM-CA speeds up learning but introduces computational overhead)

- **Failure signatures:** ICM Distraction (high variance in loss curves indicating exploration of noise); Constraint Violation (consistent negative rewards for energy/time indicating constraints too tight); Stagnation (agent frequently selects same devices regardless of eavesdropper movement)

- **First 3 experiments:** 1) Baseline Convergence: Compare ICM-CA vs. Standard SAC on static network topology to verify 3x convergence speedup; 2) Deception Effectiveness: Run ablation with "No Deceptive Signals" to measure delta in information leakage (target: ~13% reduction); 3) Device Scaling: Increase $U$ while keeping $E$ constant to test Cross-Attention scalability

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text.

## Limitations
- Performance depends on specific network topologies and eavesdropper behaviors not fully characterized outside simulation
- Action space complexity scales poorly with network size, potentially leading to exploration stagnation in massive IoT scenarios
- Defense relies on eavesdropper's inability to distinguish legitimate from deceptive signals, which may fail against advanced signal processing techniques

## Confidence
- ICM-driven exploration efficiency: High confidence (supported by direct equations and ablation comparisons)
- Cross-Attention temporal context enhancement: High confidence (supported by direct equations)
- 3x convergence speedup: Medium confidence (depends on specific simulation environment)
- 13% leakage reduction: Medium confidence (depends on specific network topology and eavesdropper behavior)
- Real-world deployment effectiveness: Low confidence (simulation-specific results)

## Next Checks
1. **Parameter Sensitivity:** Vary curiosity scaling factor $\zeta$ and energy/time constraints ($\gamma_E$, $\gamma_T$) to identify robust operating points
2. **Adversarial Robustness:** Introduce eavesdroppers with beamforming or interference cancellation to test limits of SNR-based deception strategy
3. **State Representation Ablation:** Replace ICM feature encoder with simpler or more complex architecture to quantify contribution to exploration efficiency