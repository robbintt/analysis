---
ver: rpa2
title: AI-assisted workflow enables rapid, high-fidelity breast cancer clinical trial
  eligibility prescreening
arxiv_id: '2511.05696'
source_url: https://arxiv.org/abs/2511.05696
tags:
- clinical
- trial
- patient
- eligibility
- patients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed MSK-MATCH, an AI system for automated breast
  cancer clinical trial eligibility prescreening using large language models and retrieval-augmented
  generation. The system integrates specialized domain agents with a knowledge base
  to provide explainable predictions grounded in clinical text.
---

# AI-assisted workflow enables rapid, high-fidelity breast cancer clinical trial eligibility prescreening

## Quick Facts
- arXiv ID: 2511.05696
- Source URL: https://arxiv.org/abs/2511.05696
- Authors: Jacob T. Rosenthal; Emma Hahesy; Sulov Chalise; Menglei Zhu; Mert R. Sabuncu; Lior Z. Braunstein; Anyi Li
- Reference count: 0
- Primary result: AI-assisted workflow achieved 98.6% accuracy, 98.4% sensitivity, and 98.7% specificity for breast cancer trial eligibility prescreening.

## Executive Summary
This study developed MSK-MATCH, an AI system for automated breast cancer clinical trial eligibility prescreening using large language models and retrieval-augmented generation. The system integrates specialized domain agents with a knowledge base to provide explainable predictions grounded in clinical text. In a retrospective dataset of 731 patients across 6 trials with 88,518 documents, the AI-assisted workflow achieved 98.6% accuracy, 98.4% sensitivity, and 98.7% specificity, matching or exceeding human-only and AI-only comparisons. By triaging cases with disqualifying criteria, the system reduced screening time from 20 minutes to 43 seconds while maintaining clinical-grade performance, demonstrating practical feasibility for real-world implementation at $0.96 per patient-trial pair.

## Method Summary
The MSK-MATCH system uses a multi-agent architecture with six specialty-aligned LLM agents (pathologist, radiologist, surgical/medical/radiation oncologist, generalist) operating on specialty-partitioned document stores. Documents are chunked at 500 tokens with 50-token overlap and embedded via Text-Embedding-3-Large for retrieval-augmented generation. A trial coordinator routes each eligibility criterion to relevant agents, while a principal investigator agent aggregates and resolves discrepancies. The system incorporates a manually curated knowledge base (89 feedback elements) injected into expert prompts to address systematic errors. Patient-level eligibility is determined through deterministic rules over criterion-level labels: ineligible if any inclusion criterion is "not met" or any exclusion criterion is "met."

## Key Results
- Achieved 98.6% accuracy, 98.4% sensitivity, and 98.7% specificity on test set
- Reduced screening time from 20 minutes to 43 seconds per patient-trial pair
- Maintained clinical-grade performance while triaging 38.1% of cases for human review

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Augmented Generation for Grounding
- **Claim:** RAG appears to reduce hallucination risk and enable auditability by anchoring LLM outputs to retrieved source text.
- **Mechanism:** Documents are chunked (500 tokens, 50 overlap) → embedded via Text-Embedding-3-Large → top-k=10 similar chunks retrieved per query → injected into agent prompts for grounded generation.
- **Core assumption:** Relevant clinical facts exist in the indexed documents and are retrievable via semantic similarity.
- **Evidence anchors:**
  - [abstract] "retrieval-augmented architecture providing explanations for all AI predictions grounded in source text"
  - [section] Pages 6–7: "the prompt is first converted into an embedding vector... retrieved documents... are then incorporated directly into the prompt"
  - [corpus] RECTIFIER (Unlu et al.) showed RAG promising for eligibility adjudication in heart failure; limited oncology-specific RAG evidence.
- **Break condition:** If key eligibility facts are undocumented, in unindexed images, or use terminology divergent from query embeddings.

### Mechanism 2: Multi-Agent Specialty Routing
- **Claim:** Routing criteria to specialty-aligned agents with domain-specific document stores may improve relevance and interpretability.
- **Mechanism:** Documents tagged by note type → assigned to specialty agents (pathologist, radiologist, surgical/medical/radiation/radiation oncologist, generalist) → trial coordinator routes each criterion → principal investigator aggregates and resolves.
- **Core assumption:** Eligibility criteria decompose along specialty boundaries and relevant evidence clusters in specialty-specific notes.
- **Evidence anchors:**
  - [abstract] "integrates specialized domain agents with a knowledge base"
  - [section] Page 5: "expert agents emulating... a multidisciplinary breast cancer care team"; Figure S1 shows comparable classification but improved interpretability vs single-expert.
  - [corpus] Corpus evidence on multi-agent vs single-agent for clinical trial matching is sparse; adjacent work focuses on single-model approaches.
- **Break condition:** Criteria requiring cross-specialty synthesis may bottleneck at the principal investigator agent if specialist outputs conflict.

### Mechanism 3: Prompt-Injected Knowledge Base for Domain Adaptation
- **Claim:** Injecting a curated knowledge base into prompts substantially lifts performance without fine-tuning.
- **Mechanism:** Human reviews AI errors on training set → writes generally applicable feedback → collected into KB → KB concatenated into all expert agent prompts at inference.
- **Core assumption:** Errors are systematic and can be generalized into reusable instructions (not patient-specific fixes).
- **Evidence anchors:**
  - [section] Page 9: KB increased accuracy from 64.1% → 87.5%, sensitivity 21.6% → 62.8%, specificity 82.7% → 98.3% (test set).
  - [section] Page 7: 89 feedback elements collected from training error analysis.
  - [corpus] Generalist LLMs with prompting can match specialized models on narrow tasks (NorI et al., Jeong et al.), though domain-specific prompting strategies vary.
- **Break condition:** If KB grows too large, token costs may become prohibitive; targeted retrieval from KB may be needed (noted as limitation on page 12).

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** Core mechanism for grounding predictions in actual clinical text and reducing hallucination.
  - **Quick check question:** Given a query embedding, how does the system decide which 10 document chunks to retrieve?

- **Concept: Multi-Agent Orchestration (LangGraph)**
  - **Why needed here:** The system is implemented as a graph of communicating agents with defined roles.
  - **Quick check question:** What happens if two expert agents provide conflicting assessments for the same criterion?

- **Concept: Rules-Based Eligibility Logic**
  - **Why needed here:** Final patient-level eligibility is computed via deterministic rules over criterion-level labels.
  - **Quick check question:** If one inclusion criterion is "not met" and another is "unable to determine," what is the final eligibility?

## Architecture Onboarding

- **Component map:**
  - EHR documents → note-type classification → chunking (500 tokens) → embedding (Text-Embedding-3-Large) → specialty-partitioned vector stores → specialty agents (GPT-4o, temp=0) ← Trial Coordinator (criterion routing) ← Principal Investigator (aggregation + final criterion labels) → Rules-based eligibility aggregation → Web interface

- **Critical path:**
  1. Load {patient, trial} pair.
  2. Route each criterion to relevant expert agent(s).
  3. Each agent retrieves top-10 chunks, generates explanation + label.
  4. Principal investigator reviews, resolves discrepancies, outputs final per-criterion labels.
  5. Apply rules for patient-level eligibility.
  6. For triage: sort predicted negatives by disqualifying criteria count; flag cases with 1–2 for human review.

- **Design tradeoffs:**
  - **Multi-expert vs. single-expert:** Comparable accuracy; multi-expert chosen for interpretability (multiple domain-specific explanations).
  - **KB injection cost:** Increased from $0.38 → $0.96 per patient-trial pair; future work may use targeted KB retrieval.
  - **Triage threshold:** 1–2 disqualifying criteria captured nearly all false negatives while requiring human review of only 38.1% of cases.
  - **Vacuous/human-review criteria:** 6 criteria auto-marked "met"; 15 flagged as "unable to determine" to avoid over-automation on ambiguous or judgment-based criteria.

- **Failure signatures (from Table 4):**
  1. **Domain knowledge gaps:** e.g., misclassifying axillary spread as "metastatic disease" when protocol excludes only distant metastases.
  2. **Logical mistakes:** e.g., planned future treatment mistaken for prior treatment; explanation-determination incongruence.
  3. **Missing information mishandling:** e.g., missing ECOG status marked "not met" instead of "unable to determine."
  4. **Irrelevant criterion misclassification:** e.g., sub-study criteria incorrectly disqualifying patients from main study.

- **First 3 experiments:**
  1. **Baseline ablation:** Run single-expert system on test set; compare to multi-expert (accuracy, interpretability).
  2. **KB ablation:** Withhold KB from prompts; measure accuracy/sensitivity/specificity drop (expect ~23-point accuracy drop per page 9).
  3. **Triage threshold sweep:** Vary disqualifying-criteria cutoff (0, 1, 2, 3); plot false-negative capture rate vs. human review volume.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the MSK-MATCH system's high accuracy generalize to other cancer types and non-academic medical centers?
- Basis in paper: [explicit] The authors explicitly state a limitation: the dataset "represents a single service at a single academic center and may not capture differences in population and practice patterns in other cancer types, institutions, or regions."
- Why unresolved: The system was trained and validated exclusively on breast cancer trials at Memorial Sloan Kettering, which may have unique documentation standards and patient populations compared to community oncology settings.
- What evidence would resolve it: Validation studies of the system applied to multi-institutional datasets or other oncology subspecialties (e.g., lung or gastrointestinal cancers) without retraining the core model.

### Open Question 2
- Question: How does system performance and cost change when replacing GPT-4o with open-source or alternative commercial models?
- Basis in paper: [explicit] The authors note, "Further work should assess how changing the base LLM from GPT-4o to other models, either from commercial vendors or local LLMs... could affect performance and cost."
- Why unresolved: The current reliance on GPT-4o raises cost and data privacy concerns (despite HIPAA-compliant cloud use); it is unknown if smaller, local models can handle the complex reasoning required for eligibility tasks.
- What evidence would resolve it: Comparative benchmarking of the MSK-MATCH architecture using open-source models (e.g., Llama 3) on the same retrospective dataset to measure accuracy retention versus cost savings.

### Open Question 3
- Question: Can a targeted retrieval approach for the domain knowledge base (KB) maintain accuracy while reducing inference costs?
- Basis in paper: [explicit] The paper states that injecting the entire KB "did increase inference costs" and suggests "a targeted approach to retrieve relevant information from the KB could preserve performance gains while reducing prompt lengths."
- Why unresolved: The current method concatenates the entire KB into the prompt (increasing token usage from $0.38 to $0.96), which may not scale as the KB grows with more trials and feedback.
- What evidence would resolve it: An ablation study testing a RAG-based mechanism for the KB itself, comparing the accuracy of dynamically retrieved KB snippets versus full KB injection.

## Limitations
- System performance relies heavily on availability and quality of documented clinical information
- Manual knowledge base curation creates potential scalability challenges for broader deployment
- Assumes criterion-level labels can be deterministically aggregated to patient-level eligibility

## Confidence
- **High confidence** in accuracy metrics (98.6%) and workflow efficiency gains (20 minutes → 43 seconds) based on retrospective analysis
- **Medium confidence** in generalizability to other cancer types or clinical settings given breast cancer specialization
- **Low confidence** in long-term scalability of manual knowledge base curation approach

## Next Checks
1. **Cross-cancer validation**: Test the system on non-breast cancer trials to assess domain transferability and identify specialty agent limitations.
2. **Real-world workflow integration**: Deploy the system in a live clinical setting to measure actual time savings, user satisfaction, and error patterns compared to retrospective performance.
3. **Knowledge base scalability study**: Analyze the growth rate and maintenance burden of the knowledge base as it encounters new error types, and test automated KB generation approaches.