---
ver: rpa2
title: 'Latent Topic Synthesis: Leveraging LLMs for Electoral Ad Analysis'
arxiv_id: '2510.15125'
source_url: https://arxiv.org/abs/2510.15125
tags:
- topic
- moral
- political
- topics
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel LLM-guided framework for automatically
  inducing interpretable topic taxonomies from unlabeled corpora without requiring
  seed sets or domain expertise. The method combines unsupervised clustering with
  iterative LLM-based labeling, where the LLM refines topic labels across multiple
  passes, enforcing global consistency.
---

# Latent Topic Synthesis: Leveraging LLMs for Electoral Ad Analysis

## Quick Facts
- **arXiv ID:** 2510.15125
- **Source URL:** https://arxiv.org/abs/2510.15125
- **Reference count:** 23
- **Primary result:** Novel LLM-guided framework for unsupervised topic taxonomy induction, outperforming baselines on 2024 US election ad data with moderate annotator agreement (Cohen's Kappa = 0.66)

## Executive Summary
This paper introduces a novel framework for automatically inducing interpretable topic taxonomies from unlabeled corpora without requiring seed sets or domain expertise. The method combines unsupervised clustering with iterative LLM-based labeling, where the LLM refines topic labels across multiple passes, enforcing global consistency. Applied to a dataset of 2024 U.S. presidential election ads, the framework outperforms baseline approaches under human evaluation and uncovers key political discourse patterns, including strong correlations between moral foundations and issues.

## Method Summary
The framework processes raw ad text through embedding (all-MiniLM-L6-v2), dimensionality reduction (UMAP), and density-based clustering (HDBSCAN). For each cluster, the five items with highest membership probabilities are extracted as representatives. An LLM iteratively synthesizes a global topic list: for each cluster, it either matches to an existing topic or generates a new one. All documents are then assigned to these topics using constrained decoding. The resulting taxonomy enables scalable analysis of political messaging patterns and demographic targeting.

## Key Results
- Human evaluation shows the method achieves a mean label score of 2.8 (1-5 scale) versus BERTopic's 1.2
- Moderate annotator agreement (Cohen's Kappa = 0.66) validates the interpretability of induced topics
- Correlation analysis reveals strong links between moral foundations and issues (e.g., fairness/cheating with crime/justice)
- Spending analysis shows issue-specific funding concentration, with abortion ads polarized on sanctity/liberty frames

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative, context-aware labeling yields more consistent topic taxonomies than independent, per-cluster labeling
- **Mechanism:** The system processes clusters sequentially, maintaining a global list of established topics. For each new cluster, the LLM must first verify if an existing topic fits (binary check). Only upon rejection does it generate a new label, forcing semantic merging where possible
- **Core assumption:** The LLM possesses sufficient semantic alignment capabilities to judge equivalence between a new cluster's content and an existing label without human intervention
- **Evidence anchors:** Abstract states "LLM refines topic labels across multiple passes, enforcing global consistency"; human evaluation shows our method score 2.8 vs BERTopic 1.2
- **Break condition:** Early poorly defined labels can cause topic drift or over-generalization for subsequent clusters

### Mechanism 2
- **Claim:** Density-based representative sampling filters noise better than random sampling for LLM prompt contexts
- **Mechanism:** HDBSCAN assigns membership probabilities. By selecting only the top 5 items with highest membership probability as "cluster representatives" for the LLM prompt, the system isolates the semantic core of the cluster
- **Core assumption:** The centroid-like items in a density cluster accurately represent the semantic theme of the entire group, including outliers
- **Evidence anchors:** Section 3.1 describes extracting "five items with the highest membership probabilities... cluster representatives"
- **Break condition:** In non-convex or highly varied clusters, top 5 representatives may represent a local sub-theme, causing mislabeling

### Mechanism 3
- **Claim:** Constrained decoding enforces structural rigor, preventing the LLM from hallucinating invalid labels during assignment
- **Mechanism:** During the assignment phase, the LLM is forced to output specific tokens (e.g., "yes"/"no" or exactly one label from the list) via constrained decoding/grammars
- **Core assumption:** The implemented constraint logic perfectly maps the LLM's probability distribution to the allowed output set without suppressing the model's natural generation tendencies
- **Evidence anchors:** Section 3.2 states "We use constrained decoding... to force the LLM to only answer with 'yes' or 'no'"
- **Break condition:** If the LLM's true preference is suppressed by the constraint, log-probabilities may indicate low confidence, leading to technically valid but semantically forced assignments

## Foundational Learning

- **Concept:** HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)
  - **Why needed here:** Unlike K-Means, HDBSCAN does not require specifying the number of clusters beforehand and naturally handles noise which are prevalent in messy social media data
  - **Quick check question:** How does HDBSCAN treat points in low-density regions compared to a centroid-based algorithm like K-Means?

- **Concept:** Moral Foundations Theory (MFT)
  - **Why needed here:** The framework moves beyond simple "topic" detection to "framing" analysis. Understanding the 6 axes (e.g., Care/Harm, Liberty/Oppression) is required to interpret the downstream classification results and the correlations shown in Figure 2
  - **Quick check question:** Which moral foundation does the paper identify as strongly correlated with the "immigration" topic in the case study?

- **Concept:** Constrained Decoding / Grammar-based Generation
  - **Why needed here:** The reliability of Algorithm 1 depends entirely on the LLM outputting valid tokens for the control flow (yes/no) and final assignments. Standard LLM generation is stochastic; constrained decoding makes it deterministic and machine-readable
  - **Quick check question:** Why is forcing an LLM to output a specific JSON schema or token list crucial for pipeline automation?

## Architecture Onboarding

- **Component map:** Raw Ad Text -> all-MiniLM-L6-v2 Embeddings -> UMAP Reduction -> HDBSCAN Clustering -> Iterative Synthesis (Llama-3.2-3B) -> Constrained Assignment -> Analysis

- **Critical path:** The Topic Synthesis Loop. If the LLM generates a vague or hallucinated label in the early iterations, it poisons the global list T, causing a cascade of misalignments for all subsequent clusters

- **Design tradeoffs:**
  - Small vs. Large LLM: Uses local Llama-3.2-3B (quantized/local) for synthesis rather than GPT-4, reducing cost but potentially limiting nuance
  - Cluster Representative Count: Using 5 representatives reduces token cost but risks missing diversity of large clusters
  - Seed-free vs. Guided: Requires no seeds (flexible) but lacks domain-specific precision that seed-guided approaches might offer

- **Failure signatures:**
  - Topic Collapse: Model generates single broad topic (e.g., "Politics") and assigns all clusters to it
  - Fragmentation: Model creates new topic for every cluster because binary check is too strict
  - Outlier Flood: HDBSCAN classifies too many points as noise, leaving insufficient training data

- **First 3 experiments:**
  1. **Baseline Validation:** Run framework on 2024 Ad dataset; verify local Llama-3 generates same ~14 topics (Economy, Voting Rights, etc.)
  2. **Sensitivity Analysis:** Vary min_cluster_size in HDBSCAN (10 vs 15 vs 30); observe granularity changes in induced taxonomy
  3. **LLM Swap:** Replace synthesis LLM with larger model (GPT-4o or Llama-70B); run human evaluation on "Best Label" metric to test if iterative synthesis scales with model intelligence

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does the sequential order of cluster processing influence the stability and composition of the final induced topic taxonomy?
- **Basis in paper:** [explicit] Conclusion identifies need to "study the robustness of iterative synthesis under different prompting and ordering conditions"
- **Why unresolved:** Sequential processing may introduce path dependency where early labels anchor subsequent ones, but this sensitivity was not tested
- **What evidence would resolve it:** Running synthesis pipeline multiple times with randomized cluster orders and measuring overlap (e.g., Adjusted Rand Index) of resulting taxonomies

### Open Question 2
- **Question:** Can the framework effectively induce taxonomies in domains characterized by highly overlapping or abstract topic boundaries without architectural modifications?
- **Basis in paper:** [explicit] Limitations note method "may not generalize to all domains" and may require "additional constraints or hierarchical structure" for abstract boundaries
- **Why unresolved:** Validated on electoral ads with distinct, concrete issue boundaries; performance on nuanced or ambiguous corpora unknown
- **What evidence would resolve it:** Applying method to datasets with fluid thematic boundaries (e.g., philosophical texts or abstract poetry) and evaluating label distinctness via human annotation

### Open Question 3
- **Question:** How sensitive is the quality of the generated taxonomy to the specific density-based clustering parameters and embedding models used?
- **Basis in paper:** [inferred] States clustering performance is sensitive to parameters and recommends "investigation into the optimal parameters"
- **Why unresolved:** Uses single embedding model (all-MiniLM-L6-v2) and fixed minimum cluster size; impact of hyperparameters on LLM's ability to synthesize consistent labels not quantified
- **What evidence would resolve it:** Ablation studies varying embedding dimensionality and HDBSCAN minimum cluster sizes to observe resulting shift in topic coherence scores

## Limitations

- Framework performance critically depends on quality of initial embedding space and cluster assignments
- Human evaluation conducted by only 4 annotators, which may not represent diverse interpretive frameworks
- System's scalability to much larger corpora (beyond 8,000 ads) remains untested, particularly regarding LLM context window constraints
- Iterative synthesis lacks ablation studies comparing it against single-pass labeling or random cluster ordering

## Confidence

- **High Confidence:** Core mechanism of iterative, constrained LLM labeling for taxonomy synthesis is technically sound; human evaluation results are reliable; correlation findings between moral foundations and political issues are well-supported
- **Medium Confidence:** Claim that approach outperforms BERTopic in label quality is supported but could benefit from additional baseline comparisons; effectiveness of 5-representative sampling is plausible but not rigorously validated
- **Low Confidence:** System's robustness to different cluster qualities and optimal processing order for clusters remain underexplored; generalizability to other domains beyond political advertising is suggested but not demonstrated

## Next Checks

1. **Ablation Study:** Compare full iterative synthesis pipeline against (a) single-pass labeling where LLM processes all clusters independently, and (b) random vs. ordered cluster processing. Measure topic consistency and human preference scores.

2. **Representative Sampling Sensitivity:** Systematically vary number of cluster representatives (1, 3, 5, 10) and assess impact on label quality, topic diversity, and processing efficiency. Identify optimal tradeoff point.

3. **Domain Generalization Test:** Apply exact framework to completely different unlabeled corpus (e.g., product reviews, scientific abstracts) and evaluate whether it produces coherent, interpretable taxonomies without domain-specific tuning.