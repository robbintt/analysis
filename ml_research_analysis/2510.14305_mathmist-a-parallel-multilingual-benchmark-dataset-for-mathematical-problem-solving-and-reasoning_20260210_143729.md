---
ver: rpa2
title: 'MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem
  Solving and Reasoning'
arxiv_id: '2510.14305'
source_url: https://arxiv.org/abs/2510.14305
tags:
- language
- bangla
- reasoning
- english
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MathMist, a large-scale multilingual benchmark
  dataset designed to evaluate mathematical reasoning across diverse languages and
  reasoning paradigms. The dataset comprises over 29,000 parallel question-answer
  pairs across thirteen languages, including high-, medium-, and low-resource languages,
  enabling fine-grained assessment of cross-lingual and cross-linguistic reasoning
  abilities.
---

# MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem Solving and Reasoning

## Quick Facts
- arXiv ID: 2510.14305
- Source URL: https://arxiv.org/abs/2510.14305
- Reference count: 40
- Key outcome: MathMist introduces a multilingual benchmark revealing significant performance disparities in LLM mathematical reasoning across languages and reasoning paradigms, with pronounced degradation in low-resource settings.

## Executive Summary
MathMist is a large-scale multilingual benchmark dataset comprising over 29,000 parallel question-answer pairs across thirteen languages, designed to evaluate mathematical reasoning capabilities across diverse languages and reasoning paradigms. The dataset enables fine-grained assessment of cross-lingual and cross-linguistic reasoning abilities through controlled comparison of semantically equivalent problems. The authors evaluate a diverse suite of large language models under zero-shot, chain-of-thought, code-switched, and perturbed reasoning settings, revealing significant performance disparities across languages and models. Experimental results show pronounced degradation in low-resource language settings, highlighting persistent gaps in LLM reasoning consistency and interpretability that underscore the need for language-aware fine-tuning and balanced multilingual training strategies.

## Method Summary
The dataset construction pipeline involves OCR extraction from NCTB Bangladesh secondary school textbooks (2018-2019) using Gemini 2.0 Pro, followed by human verification and translation into 13 typologically diverse languages. The resulting 29,721 artifacts include 18,785 parallel problems (1,445 per language), 2,266 MCQs (Bangla-English), and 8,670 perturbed solutions. Evaluation uses inference-only settings with temperature T=1.0 and max generation 32K tokens, testing models including DeepSeek R1-7B, Mathstral-7B, Qwen-8B, GPT-OSS-20B, and Gemini 2.5 Flash-Lite across zero-shot, chain-of-thought, code-switched CoT, and perturbation detection tasks. LLM-as-a-Judge evaluation with Gemini 2.0 Flash-Lite achieves 91-95% human agreement for scoring mathematical equivalence.

## Key Results
- Performance degradation correlates inversely with language resource level, with low-resource languages showing 30-50 percentage point drops
- Chain-of-thought prompting improves numerical reasoning by 32.11% on average but shows inconsistent gains across languages, with some models degrading in Bangla
- Code-switching penalties range from 1-10% depending on model scale and language pair similarity, with smaller models showing larger drops
- Error detection ability significantly exceeds error identification capability, revealing a gap in models' capacity to reason about specific fallacies

## Why This Works (Mechanism)

### Mechanism 1: Pretraining Distribution Mismatch Drives Low-Resource Degradation
LLMs encode linguistic patterns and mathematical terminology from training data; low-resource languages have fewer exemplars, leading to weaker parameterization of language-specific mathematical semantics. When prompted in these languages, retrieval and reasoning pathways are less reliable.

### Mechanism 2: Chain-of-Thought Mitigates Numerical Gaps but Exposes Linguistic Fragility
CoT decomposes complex reasoning into intermediate steps, reducing working memory load and enabling verification. However, the reasoning steps are generated in the model's "dominant" language patterns, causing interference when the target language is under-represented.

### Mechanism 3: Code-Switching Penalty Reflects Cross-Lingual Grounding Failure
Models must map problem semantics from one language to reasoning processes in another, requiring robust cross-lingual representations that preserve mathematical meaning across language boundaries. Larger models have more capacity for such mappings.

## Foundational Learning

- **Parallel Benchmark Design**: Understanding how parallel corpora enable causal attribution of performance differences across languages. Quick check: Can you explain why evaluating on translated vs. native-language math problems might yield different conclusions about multilingual reasoning capability?

- **LLM-as-a-Judge Evaluation**: Understanding the flexibility required for mathematical equivalence scoring. Quick check: What are two failure modes when using an LLM to judge mathematical equivalence that human validation might miss?

- **Resource-Level Taxonomy (High/Medium/Low)**: Understanding classification factors beyond speaker count. Quick check: Would Finnish (6M speakers, Uralic family) be considered low-resource? What factors beyond speaker count affect resource classification?

## Architecture Onboarding

- **Component map**: Data Pipeline (Gemini OCR → human verification → translation → perturbation injection → MCQ generation) → Evaluation Suite (Zero-shot, CoT, CS-CoT, Perturbation detection) → Metrics (Accuracy, Pass@3, LLM-as-a-Judge) → Models (Open-source, Proprietary, Multilingual)

- **Critical path**: Dataset construction (2,890 BN-EN parallel problems → 29K+ artifacts across 13 languages) → Translation quality verification via back-translation + LLM judge → Evaluation runs with temperature T=1.0, max tokens 32K → LLM-as-Judge scoring with SME validation

- **Design tradeoffs**: Parallel vs. native enables controlled comparison but may introduce translation artifacts; broad language coverage vs. depth provides breadth but limits per-language problem diversity; automated vs. human evaluation scales but requires validation

- **Failure signatures**: Models producing >4,600 tokens without solution, then "Hmm, I'm stuck"; self-imposed reasoning limits: "I've reached the limit of my understanding"; recursive loops in trigonometry problems; mixed-language CoT when instructed to use single language

- **First 3 experiments**:
  1. Baseline cross-lingual gap: Run your model on all 13 languages with zero-shot prompting. Calculate mean accuracy and variance per resource tier. Compare against GPT-OSS-20B's 72.11% mean.
  2. CoT language sensitivity: For 3 languages (1 high, 1 medium, 1 low), test Q(language A) → CoT(language A) vs. Q(language A) → CoT(English). Measure the code-switching penalty magnitude.
  3. Error detection vs. diagnosis: Using perturbation strategies σ₁-σ₃, evaluate whether your model can detect errors (binary) better than it can identify them (specific error location). Compare the detection-diagnosis gap to Qwen-8B's ~15-25% drop.

## Open Questions the Paper Calls Out

### Open Question 1
Do Gemini models truly generalize mathematical problem solving beyond language, or do they rely on pattern-matching? The authors observe Gemini unexpectedly lost accuracy in Bangla when using Chain-of-Thought reasoning compared to Zero-Shot, while improving in English. Ablation studies on Gemini's internal representations would resolve whether this reflects linguistic pattern matching versus abstract logical reasoning.

### Open Question 2
Can Program of Thoughts (PoT) or Atom of Thoughts (AoT) prompting strategies mitigate the "infinite recursive loops" and "self-imposed limits" observed in current models? The current experimental scope is restricted to Zero-Shot and CoT settings, leaving the efficacy of structured decomposition methods on these specific pathologies untested.

### Open Question 3
Can targeted instruction tuning bridge the "drastic divergence" between a model's ability to detect an error and its ability to identify the specific fallacy? The paper establishes the existence of the gap but does not determine if this is a fundamental limitation of current alignment techniques or a solvable data scarcity issue in low-resource languages.

## Limitations

- Translation artifacts may confound cross-lingual comparisons despite human verification and back-translation efforts
- LLM-as-Judge evaluation may not capture all valid reasoning paths due to context-sensitive mathematical equivalence
- Resource taxonomy classification depends on evolving pretraining corpus statistics that may shift over time

## Confidence

- High confidence: Performance degradation in low-resource languages (supported by consistent patterns across multiple models with quantified accuracy drops)
- Medium confidence: CoT prompting benefits are task-dependent (numerical vs. proof problems show different gain patterns, but external validation is limited)
- Medium confidence: Code-switching penalties reflect grounding failures (observed patterns align with mechanism, but alternative explanations cannot be ruled out)

## Next Checks

1. Measure semantic divergence between original and translated problems using embedding similarity or human annotation of key mathematical terms across the 13 languages
2. Create edge cases where multiple valid solutions exist and evaluate whether the LLM judge treats them equivalently
3. Map each model's pretraining corpus statistics for the 13 languages against MathMist performance to quantify the relationship between pretraining exposure and reasoning accuracy