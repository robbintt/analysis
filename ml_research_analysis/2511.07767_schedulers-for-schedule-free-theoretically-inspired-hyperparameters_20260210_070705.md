---
ver: rpa2
title: 'Schedulers for Schedule-free: Theoretically inspired hyperparameters'
arxiv_id: '2511.07767'
source_url: https://arxiv.org/abs/2511.07767
tags:
- learning
- rate
- convergence
- training
- theory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the theoretical analysis of the Schedule-free
  optimization method to support general learning rate schedules beyond constant rates.
  The authors develop a new convergence theorem that allows for arbitrary schedulers
  and derive a theoretically motivated averaging parameter that depends on the learning
  rate.
---

# Schedulers for Schedule-free: Theoretically inspired hyperparameters

## Quick Facts
- arXiv ID: 2511.07767
- Source URL: https://arxiv.org/abs/2511.07767
- Reference count: 40
- One-line primary result: Extends Schedule-free optimization theory to general learning rate schedules and proposes a new adaptive Polyak step-size variant for black-box distillation with optimal convergence guarantees.

## Executive Summary
This paper develops a theoretical framework for the Schedule-free optimization method that supports arbitrary learning rate schedules beyond constant rates. The authors derive a new convergence theorem showing that the averaging parameter must be coupled with the learning rate schedule, specifically c_t = γ_t/Σγᵢ, to guarantee convergence. They specialize their theory to the warmup-stable-decay (wsd) schedule and prove an optimal O(1/√T) convergence rate. Additionally, they propose Schedulep, a Polyak step-size variant for black-box model distillation, which achieves anytime optimal O(1/√t) convergence without knowing the total training horizon.

## Method Summary
The paper extends the Schedule-free optimization framework by deriving theoretical convergence guarantees for general learning rate schedules. The key insight is that the averaging parameter c_t must be synchronized with the learning rate γ_t through the relation c_t = γ_t/Σγᵢ to ensure last-iterate convergence. The authors specialize this to the warmup-stable-decay (wsd) schedule, deriving explicit c_t formulas for each phase. They also propose Schedulep, an adaptive Polyak step-size variant that computes γ_t using local loss information via interpolation assumptions, making it suitable for black-box distillation tasks where the optimal loss can be approximated by a teacher model.

## Key Results
- Proves convergence of Schedule-free with general learning rate schedules when c_t = γ_t/Σγᵢ, achieving optimal O(1/√T) rate
- Derives theoretical averaging parameters for warmup-stable-decay schedule with explicit convergence bounds
- Proposes Schedulep with anytime optimal O(1/√t) convergence for convex functions using Polyak step-size
- Demonstrates strong empirical performance on CIFAR image classification and black-box distillation tasks despite convex assumptions

## Why This Works (Mechanism)

### Mechanism 1: Schedule-Coupled Averaging Parameters
The averaging parameter c_t must be synchronized with the learning rate schedule to guarantee convergence for non-constant schedules. The schedule-free method maintains three iterates (primal averaging x_t, gradient accumulation z_t, evaluation point y_t). To transform the non-convergent bound from prior work into a proper convergence guarantee, c_t must satisfy c_t = γ_t/Σᵢ₌₀ᵗ γᵢ. This creates telescoping cancellation in the Lyapunov analysis, ensuring the last-iterate bound converges rather than accumulating error terms.

### Mechanism 2: WSD Schedule Achieves Optimal Rate Through Balanced Phase Contributions
The warmup-stable-decay (trapezoidal) schedule, when paired with the theoretically-derived c_t, achieves the optimal O(DG/√T) convergence rate. The WSD schedule's three phases (linear warmup, constant, linear decay) have complementary effects on the convergence bound. The cumulative structure of γ_t ensures the denominator Σγᵢ grows as O(T + T_c - T_w), while the numerator terms Σγ_t² grow as O(T), yielding the √T rate.

### Mechanism 3: Polyak Step-Size via Interpolation-Based Loss Approximation
Schedulep achieves anytime optimal O(GD/√t) convergence without knowing the total horizon T, using an adaptive learning rate derived from local loss information. At each iteration, Schedulep computes γ_t = [f_ζ(y_t) - f_ζ(x*) + β⟨∇f(y_t, ζ), z_{t-1} - x_t⟩]₊ / ||∇f(y_t, ζ)||². This minimizes an upper bound on ||z_t - x*||². The interpolation assumption (access to f_ζ(x*)) makes this computable.

## Foundational Learning

- Concept: **Schedule-Free Optimization Basics**
  - Why needed here: This paper extends schedule-free methods; you must understand the three-iterate structure (y_t for gradient evaluation, z_t for gradient accumulation, x_t for primal averaging) and how they interact via β (momentum) and c_t (averaging).
  - Quick check question: What is the role of β in the interpolation between Polyak-Ruppert averaging (β=0) and primal averaging (β=1)?

- Concept: **Last-Iterate vs. Averaged-Iterate Convergence**
  - Why needed here: The paper focuses on last-iterate convergence (x_T directly), which is stronger than averaging over all iterates. The O(1/√T) rate is optimal for this setting under the stated assumptions.
  - Quick check question: Why is last-iterate convergence preferred over iterate averaging for practical deep learning training?

- Concept: **Polyak Step-Size and Interpolation**
  - Why needed here: Schedulep relies on the Polyak step-size formula, which requires knowledge of the optimal loss value. Understanding why this works in interpolation settings (where f_ζ(x*) = 0) or distillation settings (where teacher loss approximates this) is critical.
  - Quick check question: In what settings can you obtain f_ζ(x*) or a reasonable approximation, and when is this assumption violated?

## Architecture Onboarding

- Component map:
  y_t = (1-β)z_{t-1} + βx_t -> z_t = z_{t-1} - γ_t ∇f(y_t, ζ_t) -> x_{t+1} = (1-c_{t+1})x_t + c_{t+1}z_t

- Critical path:
  1. **Averaging parameter computation**: c_t = γ_t / Σᵢ₌₀ᵗ γᵢ must be computed from cumulative learning rates
  2. **Learning rate schedule**: Any schedule works theoretically, but WSD requires phase-aware c_t (Lemma 2.2)
  3. **Gradient evaluation**: Must evaluate at y_t (not x_t or z_t) for the theoretical guarantees
  4. **For Schedulep**: Must compute or approximate f_ζ(x*) before computing γ_t

- Design tradeoffs:
  - **c_t choice**: Theoretical c_t (cumulative ratio) vs. practical heuristic (γ²/Σγ²)—theory guarantees convergence; heuristic may perform slightly better empirically but lacks guarantees
  - **β value**: β=0.9 works well in practice per original schedule-free work; β=1 reduces to primal averaging/momentum
  - **Schedulep vs. fixed schedule**: Schedulep is anytime-optimal and adaptive but requires loss approximation; fixed schedules with theoretical c_t are simpler but require tuning γ

- Failure signatures:
  - **Divergence during warmup**: If base learning rate γ is too large relative to D/G, training diverges; theory predicts this (Figure 5)
  - **Loss spikes**: Theory correlates spikes with gradient norm spikes; validate by monitoring ||∇f(x)||
  - **Schedulep instability**: If f_ζ(x*) is poorly approximated, γ_t may explode; the γ_max cap is a safeguard
  - **BatchNorm interference**: Schedule-free requires batch statistics from x sequence; use GroupNorm instead

- First 3 experiments:
  1. **Validate theoretical c_t on small-scale**: Train ResNet-20 on CIFAR-10 with WSD schedule comparing theoretical c_t vs. practical heuristic vs. c_t=1/t. Monitor training loss vs. theoretical bound from Theorem 2.1.
  2. **Ablate base learning rate**: For a fixed WSD schedule, sweep γ across [0.01, 0.1, 1, 10]. Compare empirical convergence to theoretical predictions—verify that theory predicts divergence and spike locations.
  3. **Schedulep distillation test**: On TinyShakespeare with a smaller student (GPT-2 style) and fixed teacher (gpt2-medium), compare AdamW-Schedulep against AdamW-Schedulefree, IAMS-Adam, and AdamW baselines. Sweep γ_max and monitor robustness to learning rate choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the theoretically derived averaging parameter schedule (c_t) fail to improve training performance in practice compared to heuristic settings?
- Basis: The Conclusion states that the suggested averaging parameter schedule "does not improve the training performance in practice."
- Why unresolved: There is a gap between the convex theoretical assumptions used to derive c_t and the non-convex dynamics of deep learning tasks where heuristics currently perform better.
- What evidence would resolve it: A theoretical analysis bridging convex theory with non-convex dynamics, or empirical results demonstrating a scenario where the theoretical c_t consistently outperforms heuristics.

### Open Question 2
- Question: Can a quantitative metric be established to rigorously measure the alignment between convex convergence bounds and empirical non-convex training losses?
- Basis: The Conclusion notes that the comparison between the theoretical bound and empirical performance "is via visual inspection but not a quantitative analysis."
- Why unresolved: Visual inspection is subjective and cannot definitively prove that the convex theory accurately predicts non-convex behavior across diverse architectures.
- What evidence would resolve it: A statistical study correlating the theoretical upper bounds with training loss curves across multiple models and schedules.

### Open Question 3
- Question: How can the Schedulep (Polyak step-size) variant be adapted for general training regimes where access to the optimal batch loss (f_ζ(x*)) is unavailable?
- Basis: The Conclusion lists as a limitation that Schedulep "can only be applied to models that nearly interpolate the data or under the black-box model distillation setting."
- Why unresolved: The method currently relies on Assumption 3.1 (Interpolation), which requires knowing or approximating the optimal loss, restricting its use to distillation or over-parameterized models.
- What evidence would resolve it: A modification of the algorithm that estimates f_ζ(x*) dynamically or removes the dependency on the optimal loss while maintaining convergence.

## Limitations
- Theoretical guarantees rely on convexity assumptions that do not strictly hold for deep neural networks
- Schedulep variant requires access to optimal loss values, limiting use to interpolation settings or black-box distillation
- WSD schedule convergence depends on reasonable phase proportions - extreme warmup or cooldown degrades performance

## Confidence

**High**: The mechanism linking c_t to learning rate schedules and the telescoping argument in Theorem 2.1

**Medium**: The empirical performance predictions despite convex assumptions, and the Schedulep adaptation for distillation

**Low**: The practical utility of the theoretical c_t formula vs. the heuristic approximation in real-world training

## Next Checks

1. Test Schedule-free with the theoretically-derived c_t on a small-scale vision task (e.g., CIFAR-10 with ResNet-20) and compare convergence to the practical heuristic and standard 1/t averaging. Measure both training loss and theoretical bound predictions.

2. Conduct an ablation study varying the base learning rate γ across orders of magnitude for a fixed WSD schedule. Verify that theory correctly predicts divergence points and spike locations in the training curve.

3. Implement Schedulep for a distillation task where the teacher is known to be suboptimal (e.g., distilling to a larger student). Measure sensitivity to the approximation f_ζ(x*) ≈ teacher_loss and test the effectiveness of the γ_max safeguard.