---
ver: rpa2
title: 'Efficient Offline Reinforcement Learning: First Imitate, then Improve'
arxiv_id: '2406.13376'
source_url: https://arxiv.org/abs/2406.13376
tags:
- pre-training
- learning
- policy
- behavior
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes pre-training both actor and critic with supervised
  objectives before applying off-policy reinforcement learning to improve efficiency
  and stability. The core idea is to use Monte Carlo return estimates from offline
  data to initialize a consistent actor-critic pair, addressing the inefficiency of
  temporal-difference bootstrapping from random initializations.
---

# Efficient Offline Reinforcement Learning: First Imitate, then Improve

## Quick Facts
- **arXiv ID:** 2406.13376
- **Source URL:** https://arxiv.org/abs/2406.13376
- **Reference count:** 19
- **One-line primary result:** Pre-training actor and critic with supervised Monte Carlo objectives halves training time for TD3+BC and EDAC on D4RL MuJoCo benchmarks.

## Executive Summary
This paper addresses the inefficiency of temporal-difference bootstrapping from random initializations in offline reinforcement learning by proposing a two-phase approach: pre-training both actor and critic with supervised objectives before applying off-policy RL. The method uses Monte Carlo return estimates from offline data to initialize a consistent actor-critic pair, substantially improving training efficiency and stability across D4RL MuJoCo benchmarks. For data-limited Adroit environments, hybrid algorithms with actor-critic regularization prevent performance collapse. The approach enables more efficient and stable offline RL, with experimental results showing training time reductions of over 50% for TD3+BC and EDAC while improving stability.

## Method Summary
The proposed method consists of two phases: (1) Pre-training—compute return-to-go for all transitions, train actor with behavior cloning (MSE loss), train critic with supervised Monte Carlo return-to-go (MSE loss); (2) Off-policy RL—apply TD3+BC or EDAC starting from pre-trained networks. For entropy-regularized algorithms, pre-train actor with soft BC first, then compute soft returns including entropy bonuses before critic pre-training. LayerNorm is added after every linear layer except the final one in both networks. An optional λ-return blend combines MC returns with TD targets to reduce variance in stochastic environments.

## Key Results
- Pre-training both actor and critic more than halves training time for TD3+BC and EDAC on D4RL MuJoCo benchmarks
- Consistent actor-critic initialization is essential; actor-only or critic-only pre-training shows significantly degraded performance
- LayerNorm on both actor and critic networks significantly improves training efficiency and stability
- Hybrid algorithms (TD3+BC+CQL, EDAC+BC) prevent performance collapse in data-limited Adroit environments
- The method improves both efficiency and stability across diverse benchmarks including HalfCheetah, Hopper, and Walker2d

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training the critic with Monte-Carlo return estimates reduces the number of TD updates required for convergence.
- Mechanism: MC returns propagate reward information across entire trajectories in a single supervised step, bypassing the slow, iterative bootstrapping process of TD learning.
- Core assumption: The behavior policy that generated the offline data achieves returns better than random.
- Evidence anchors: Abstract states pre-training "substantially improve the training time"; Table 1 shows MC initialization achieves optimal policy immediately vs. 2 epochs for zero initialization.
- Break condition: High variance in MC estimates from stochastic policies/environments with limited trajectories can cause performance drops; mitigated by λ-blending with TD targets.

### Mechanism 2
- Claim: A consistent actor-critic pair at initialization is required for efficient policy improvement; pre-training one component alone is insufficient.
- Mechanism: If the actor is pre-trained but the critic is random, the policy optimization objective is initially misaligned with the actor's behavior, causing performance collapse.
- Core assumption: The transition from imitation to RL requires value estimates that are accurate near the behavior policy.
- Evidence anchors: Section 1 states pre-training a critic is essential; Appendix C figure 5 shows actor-only pre-training degrades to scratch performance.
- Break condition: In data-limited settings, even consistent initialization may collapse without additional regularization on both actor and critic.

### Mechanism 3
- Claim: LayerNorm in actor and critic networks reduces out-of-distribution extrapolation error, improving both efficiency and stability.
- Mechanism: Normalization constrains the magnitude of hidden activations, preventing the network from assigning erroneously high values to OOD state-action pairs.
- Core assumption: LayerNorm's stabilizing effect generalizes across offline RL algorithms and environments.
- Evidence anchors: Section 6.1 found LayerNorm "significantly improved training efficiency and stability"; Appendix B figure 3 shows best results when applied to both actor and critic.
- Break condition: LayerNorm on actor alone can worsen performance in some environments; full benefit requires LayerNorm on both networks.

## Foundational Learning

- **Temporal Difference (TD) Learning vs. Monte-Carlo Returns**
  - Why needed here: Understanding why TD bootstrapping is inefficient from random initialization is essential to grasp the core insight.
  - Quick check question: In a sparse-reward environment with trajectory length 100, how many TD updates are required to propagate a terminal reward to the initial state vs. one MC update?

- **Actor-Critic Architecture and Consistency**
  - Why needed here: The method relies on initializing both networks to agree on the behavior policy's value; misunderstanding this leads to ineffective ablations.
  - Quick check question: If you pre-train only the actor with BC and start RL with a random critic, what happens to performance immediately after pre-training ends?

- **Bias-Variance Tradeoff in Return Estimation**
  - Why needed here: The λ parameter controls MC (high variance, low bias) vs. TD (low variance, high bias) blending; improper tuning causes performance drops.
  - Quick check question: For a dataset with 25 trajectories from a stochastic policy, would you use λ close to 0 or λ > 0.1? Why?

## Architecture Onboarding

- **Component map:** Pre-training Phase (Compute returns → Train actor with BC → Train critic with MC) → RL Phase (TD3+BC/EDAC updates) → Hybrid variants (Add regularization)
- **Critical path:**
  1. Add LayerNorm to all hidden layers (except output) in both actor and critic
  2. Pre-compute MC returns for entire dataset (or soft returns for entropy-regularized)
  3. Pre-train actor + critic jointly (hard RL) or sequentially (soft RL: actor first, then critic)
  4. Monitor supervised loss convergence (typically 10-50k updates)
  5. Switch to off-policy RL updates without resetting networks
- **Design tradeoffs:**
  - λ = 0 (pure MC): Best efficiency for large datasets with good coverage; higher variance risk for small/stochastic datasets
  - λ > 0: Smoother transition to RL; trades off some efficiency for stability
  - Pre-training duration: 10-50k updates typical; over-training wastes compute without additional benefit
  - Data-limited regimes: Requires hybrid algorithms with regularization on both actor and critic to prevent collapse
- **Failure signatures:**
  - Performance drop immediately after pre-training: Likely high MC variance; increase λ or check dataset coverage
  - No improvement over BC baseline: Critic may not have converged; verify supervised loss plateaued
  - Collapse in data-limited settings: Single-sided regularization insufficient; add complementary regularization
  - Instability during RL phase: Check LayerNorm is applied to both networks; verify target network Polyak updates are active
- **First 3 experiments:**
  1. Apply pre-training to TD3+BC on D4RL HalfCheetah-medium with LayerNorm, λ=0. Compare gradient steps to reach 90% of asymptotic performance vs. baseline
  2. Run three conditions on Hopper-medium—(a) actor-only pre-training, (b) critic-only pre-training, (c) both. Plot performance vs. updates to verify consistency requirement
  3. On a smaller subset of the dataset (e.g., 100k transitions), compare λ ∈ {0, 0.1, 0.3, 0.5} to identify when MC variance causes performance drops

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does initializing with consistent actor-critic pairs reduce the subset of suboptimal solutions satisfying the Bellman equation, thereby improving asymptotic stability?
- Basis in paper: The authors hypothesize in Appendix C that the observed stability benefits arise because pre-training reduces the solution space for the Bellman error, but state they "leave investigation of this effect to further work."
- Why unresolved: The paper demonstrates empirical stability improvements but does not isolate the mechanism regarding how pre-training filters suboptimal solutions in finite data regimes.
- What evidence would resolve it: A theoretical or empirical analysis comparing the trajectory of value error versus Bellman error convergence for pre-trained vs. random initializations.

### Open Question 2
- Question: Can a dynamic pre-training objective using a threshold on Monte Carlo targets outperform the proposed static pre-training?
- Basis in paper: In Appendix F, the authors note that a hybrid approach (using a threshold for when to stop using MC targets) might alleviate the variance issues seen in Q-Transformer style objectives, but they leave this investigation to future work.
- Why unresolved: The Q-Transformer objective (max(MC, Q)) was tested and found lacking due to variance, but the specific proposed modification of using a switching threshold was not implemented or evaluated.
- What evidence would resolve it: Experiments comparing the proposed method against a max(MC, Q) baseline that deactivates MC targets based on a defined threshold or convergence metric.

### Open Question 3
- Question: Does the theoretical logarithmic efficiency improvement ($1/\ln \gamma$) scale to sparse-reward tasks with significantly longer horizons than the tested MuJoCo benchmarks?
- Basis in paper: The theoretical analysis (Eq. 16) suggests efficiency gains depend on the discount factor, implying greater benefits for long-horizon tasks, but experiments are limited to MuJoCo environments which may not fully reflect the theoretical "long-horizon" potential.
- Why unresolved: The paper validates the method on standard benchmarks, but does not specifically test environments designed to maximize the theoretical advantage of MC propagation over TD bootstrapping.
- What evidence would resolve it: Evaluation on long-horizon, sparse-reward benchmarks (e.g., AntMaze or complex robotic manipulation) to validate the predicted scaling of efficiency gains.

## Limitations
- Lacks specific architectural details (layer sizes, number of layers) and precise hyperparameter specifications for pre-training duration and convergence criteria
- LayerNorm stabilization claim is empirical but lacks theoretical grounding and is algorithm-dependent
- Performance guarantees in data-limited regimes (Adroit with 25 trajectories) rely on heuristic regularization combinations without clear principles for selecting appropriate algorithms

## Confidence

- **High Confidence**: The core mechanism of pre-training both actor and critic with supervised objectives significantly improves training efficiency on standard D4RL benchmarks
- **Medium Confidence**: The claim that LayerNorm improves stability is supported empirically but lacks theoretical justification and is algorithm-dependent
- **Low Confidence**: The performance guarantees in data-limited regimes rely on heuristic regularization combinations without clear principles for selecting appropriate algorithms

## Next Checks
1. Test λ-return blending sensitivity on a small subset (100k transitions) of HalfCheetah-medium to quantify variance reduction vs. bias introduction
2. Evaluate pre-training efficacy on a sparse-reward variant of MuJoCo to assess Monte Carlo return propagation limits
3. Benchmark the method on non-MuJoCo environments (Atari, Box2D) to verify LayerNorm generalization claims