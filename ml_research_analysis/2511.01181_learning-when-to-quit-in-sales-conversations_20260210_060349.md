---
ver: rpa2
title: Learning When to Quit in Sales Conversations
arxiv_id: '2511.01181'
source_url: https://arxiv.org/abs/2511.01181
tags:
- stopping
- salespeople
- agent
- calls
- call
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency in sales conversations, specifically
  the decision of when to quit a conversation that is unlikely to succeed. The authors
  formalize this as an optimal stopping problem and develop a generative language
  model-based sequential decision agent, a stopping agent, that learns when to quit
  by imitating an inferred optimal stopping policy.
---

# Learning When to Quit in Sales Conversations

## Quick Facts
- arXiv ID: 2511.01181
- Source URL: https://arxiv.org/abs/2511.01181
- Authors: Emaad Manzoor; Eva Ascarza; Oded Netzer
- Reference count: 24
- Primary result: LLM-based stopping agent reduces failed call time by 54% while preserving sales, increasing expected sales by 37%

## Executive Summary
This paper addresses the inefficiency in sales conversations by developing an AI agent that learns when to quit calls unlikely to succeed. The authors formalize this as an optimal stopping problem and create a generative language model-based sequential decision agent that learns from historical call data. When applied to real-world sales data from a European telecommunications firm, the stopping agent significantly reduces time wasted on failed calls while maintaining nearly all sales conversions, demonstrating how AI can correct human cognitive biases in real-time decision-making.

## Method Summary
The authors formalize sales conversation management as an optimal stopping problem where the agent must decide at discrete time points (30, 60, 90 seconds) whether to "wait" or "quit" to maximize expected cumulative reward. They use imitation learning with a two-step approach: first inferring optimal quitting times from historical data using a reward structure that balances opportunity costs against potential sales gains, then fine-tuning a generative LLM to imitate this optimal policy. The method handles high-dimensional textual states through prompt engineering and works with both open-source (Llama 3.2 3B, Gemma 3 270M) and proprietary models (GPT-4.1). A backward induction algorithm converts the stochastic policy to deterministic decisions, and the approach is evaluated on 11,627 outbound sales calls with 5.5% success rate.

## Key Results
- Reduces time spent on failed calls by up to 54% while preserving nearly all sales
- Increases expected sales by up to 37% through more efficient resource allocation
- Reveals salespeople's cognitive biases, showing they overweight a few salient expressions of consumer disinterest and mispredict call failure risk

## Why This Works (Mechanism)
The stopping agent succeeds by learning the optimal trade-off between opportunity costs and potential sales gains through imitation learning from historical optimal stopping policies. Unlike classification-based approaches that only predict outcomes, this method explicitly models the sequential decision problem with cumulative rewards, enabling it to make economically optimal decisions rather than just outcome predictions. The LLM handles the complexity of natural language states that would be intractable with traditional state-space methods.

## Foundational Learning
- Optimal stopping theory: Framework for sequential decision problems where actions must be taken at discrete times to maximize expected reward
  - Why needed: Provides mathematical foundation for deciding when to quit conversations
  - Quick check: Verify problem can be cast as maximizing expected cumulative reward
- Imitation learning: Training policy by learning from expert demonstrations
  - Why needed: Enables learning optimal stopping policy from historical data without explicit reward specification
  - Quick check: Ensure expert policy is available or can be inferred from data
- Backward induction: Dynamic programming technique for solving sequential decision problems
  - Why needed: Converts stochastic policy outputs to deterministic decisions
  - Quick check: Verify optimal substructure property holds for the problem

## Architecture Onboarding

**Component Map:** Historical Data -> Optimal Policy Inference -> LLM Fine-tuning -> Backward Induction -> Stopping Agent

**Critical Path:** The pipeline flows from raw call transcripts through optimal policy inference, LLM training, and threshold optimization to produce the final stopping agent that makes real-time decisions.

**Design Tradeoffs:** Imitation learning vs. reinforcement learning (imitation avoids training instability but requires expert demonstrations), LLM vs. traditional classifiers (LLMs handle complex linguistic states but require more compute), open-source vs. proprietary models (tradeoff between accessibility and performance).

**Failure Signatures:** RL approaches exhibit training instability with reward plateauing or collapsing; classifier-based policies underperform by ignoring opportunity cost structure; models overfit to salespeople's biased cues like "no me interesa."

**Three First Experiments:**
1. Verify optimal policy inference correctly identifies Ï„* by comparing against manually labeled optimal stopping times on a small validation set
2. Test LLM fine-tuning convergence and prompt sensitivity by evaluating on held-out prefixes with known optimal actions
3. Validate backward induction threshold tuning by checking deterministic policy performance against stochastic baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Reward parameters (opportunity cost per second and sale benefit) are not explicitly specified, making exact reproduction impossible without assumptions
- Backward induction threshold tuning algorithm is described but not detailed sufficiently for precise replication
- Evaluation is limited to one European telecommunications firm with Spanish-language data and a low conversion rate (5.5%), raising questions about external validity

## Confidence
- **High confidence** in the methodological framework: The optimal stopping formulation, imitation learning approach, and evaluation metrics are theoretically sound and well-justified
- **Medium confidence** in empirical results: The 54% time savings and 37% expected sales increase are impressive but depend on specific reward parameters and data characteristics that are not fully transparent
- **Low confidence** in generalizability: Results may not transfer to different sales domains, languages, or market conditions without careful recalibration of reward parameters

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary c and b across reasonable ranges to determine how robust the stopping agent's performance is to reward structure assumptions, and identify thresholds where performance degrades significantly
2. **Cross-Validation Across Markets**: Apply the same methodology to sales data from different industries (retail, SaaS, B2B services) and languages to assess whether the 54% time savings and 37% expected sales gains replicate or require domain-specific adjustments
3. **A/B Testing in Production**: Deploy the stopping agent in a live sales environment with controlled randomization between agent-assisted and traditional sales approaches to measure actual revenue impact, call quality, and customer satisfaction in addition to efficiency metrics