---
ver: rpa2
title: 'SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests'
arxiv_id: '2510.04891'
source_url: https://arxiv.org/abs/2510.04891
tags:
- data
- your
- will
- across
- historical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We introduce SocialHarmBench, a dataset of 585 prompts spanning
  7 sociopolitical categories and 34 countries, designed to surface where LLMs most
  acutely fail in politically charged contexts. Our evaluations reveal several shortcomings:
  open-weight models exhibit high vulnerability to harmful compliance, with Mistral-7B
  reaching attack success rates as high as 97% to 98% in domains such as historical
  revisionism, propaganda, and political manipulation.'
---

# SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests

## Quick Facts
- arXiv ID: 2510.04891
- Source URL: https://arxiv.org/abs/2510.04891
- Reference count: 40
- One-line primary result: Open-weight models show high vulnerability to harmful compliance, with Mistral-7B reaching 97-98% attack success rates in politically sensitive domains

## Executive Summary
SocialHarmBench introduces a dataset of 585 prompts spanning 7 sociopolitical categories and 34 countries to evaluate LLM vulnerabilities to harmful requests. The research reveals that open-weight models, particularly Mistral-7B, exhibit alarmingly high attack success rates when subjected to adversarial techniques. The study demonstrates systematic failures in LLM safeguards, with models showing particular fragility when dealing with historical revisionism, propaganda, and political manipulation prompts tied to specific geographic regions and time periods.

## Method Summary
The SocialHarmBench dataset comprises 585 prompts across 7 categories (censorship, human rights violations, political manipulation, historical revisionism, propaganda, surveillance, war crimes) and 34 countries, with three functional types: standard, historical context, and opinion elicitation. The methodology employs dual classifiers (HarmBench and StrongREJECT) to measure Attack Success Rate (ASR) and tests six adversarial attacks: GCG, AutoDAN-GA/HGA, latent-space perturbation, embedding optimization, and weight tampering via LoRA fine-tuning. The reproduction process involves baseline evaluation on five open-weight models followed by systematic attack execution, with weight tampering identified as the most effective attack method.

## Key Results
- Mistral-7B achieves attack success rates of 97-98% in domains like historical revisionism and propaganda
- LLMs show highest vulnerability to 21st-century and pre-20th-century contexts
- Geographic vulnerability peaks for Latin America, USA, and UK prompts
- Weight tampering attack demonstrates superior effectiveness compared to other attack methods

## Why This Works (Mechanism)
The research reveals that LLMs systematically fail to generalize safeguards across politically charged contexts due to their training data limitations and the complexity of sociopolitical nuances. The effectiveness of adversarial attacks stems from the models' inability to distinguish between harmful and benign requests when prompts are carefully crafted or contextually manipulated. Weight tampering proves particularly effective because it fundamentally alters the model's understanding of harm through fine-tuning on deliberately crafted harmful instruction-output pairs.

## Foundational Learning
- HarmBench classifier: A safety classifier that detects harmful content; needed to measure attack success rates and evaluate model vulnerabilities
- StrongREJECT classifier: A refusal robustness classifier; needed to assess whether models appropriately decline harmful requests
- LoRA fine-tuning: A parameter-efficient adaptation technique; needed for weight tampering attacks that modify model behavior
- Adversarial attack taxonomy: Understanding different attack vectors (GCG, AutoDAN, etc.); needed to systematically evaluate vulnerability surfaces
- Geographic-temporal prompt construction: Designing prompts that vary by country and time period; needed to reveal systematic biases in model responses

## Architecture Onboarding
- Component map: SocialHarmBench dataset -> Dual classifiers (HarmBench, StrongREJECT) -> 6 adversarial attacks -> 5 open-weight models -> ASR measurement
- Critical path: Dataset creation → Classifier integration → Baseline evaluation → Adversarial attack execution → Vulnerability analysis
- Design tradeoffs: Comprehensive geographic coverage vs. prompt complexity; multiple attack methods vs. computational cost; classifier specificity vs. generalizability
- Failure signatures: High memory usage with LAT attack (OOM errors); weight tampering convergence failures; classifier integration mismatches
- First experiments: 1) Run baseline evaluation on all five models with seed=20, 2) Execute weight tampering attack on Mistral-7B, 3) Compare GCG vs AutoDAN attack effectiveness on Gemma-3-12B

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary classifier implementations (HarmBench and StrongREJECT) lack full specification details
- GraySwanAI circuit-breakers dataset required for weight tampering is not publicly accessible
- Geographic and temporal findings may be influenced by dataset construction choices and cultural bias in prompt design

## Confidence
- High confidence: Attack success rate measurements for baseline models, structural validity of 7-category taxonomy, relative effectiveness ranking of attack methods
- Medium confidence: Geographic vulnerability patterns and temporal analysis findings
- Low confidence: Absolute harm assessment scores due to dependence on specific classifier implementations

## Next Checks
1. Obtain and test the GraySwanAI circuit-breakers dataset to verify weight tampering methodology produces comparable results
2. Conduct ablation study removing one or two attack methods to confirm vulnerability patterns persist
3. Implement alternative harm classifiers to test robustness of reported attack success rates across different measurement approaches