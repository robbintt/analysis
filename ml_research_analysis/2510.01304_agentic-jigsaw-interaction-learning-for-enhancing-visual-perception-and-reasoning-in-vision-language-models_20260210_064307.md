---
ver: rpa2
title: Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning
  in Vision-Language Models
arxiv_id: '2510.01304'
source_url: https://arxiv.org/abs/2510.01304
tags:
- jigsaw
- reasoning
- image
- visual
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AGILE addresses the limited visual perception and reasoning capabilities
  in large vision-language models, which struggle even on simple 2x2 jigsaw tasks.
  The proposed method formulates jigsaw solving as an interactive process where the
  model iteratively generates Python code to swap, observe, crop, and zoom into jigsaw
  pieces, receiving fine-grained visual feedback at each step.
---

# Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models

## Quick Facts
- arXiv ID: 2510.01304
- Source URL: https://arxiv.org/abs/2510.01304
- Reference count: 27
- Primary result: AGILE improves 2x2 jigsaw accuracy from 9.5% to 82.8% and generalizes to 9 vision tasks with an average improvement of 3.1%

## Executive Summary
Large vision-language models (VLMs) struggle with basic visual reasoning tasks like 2x2 jigsaw puzzles, achieving only 9.5% accuracy. AGILE addresses this limitation by formulating jigsaw solving as an interactive process where the model iteratively generates Python code to manipulate puzzle pieces and receives fine-grained visual feedback at each step. This interaction-driven learning approach is supported by scalable synthetic data generation, enabling training on large datasets without human supervision.

The method significantly improves visual perception and reasoning capabilities in VLMs, achieving 82.8% accuracy on 2x2 jigsaw puzzles while also generalizing to nine downstream vision tasks with an average performance improvement of 3.1%. By transforming a static puzzle-solving task into an iterative, code-driven interaction, AGILE enables models to develop more sophisticated visual reasoning skills through active exploration of the visual space.

## Method Summary
AGILE addresses the limited visual perception and reasoning capabilities in large vision-language models by formulating jigsaw solving as an interactive process. The model iteratively generates Python code to swap, observe, crop, and zoom into jigsaw pieces, receiving fine-grained visual feedback at each step. This interaction-driven learning is supported by scalable synthetic data generation, allowing training on large datasets without human supervision. The approach transforms the static jigsaw task into a dynamic sequence of code executions and visual observations, enabling the model to develop more sophisticated visual reasoning capabilities through active exploration.

## Key Results
- Improves 2x2 jigsaw puzzle accuracy from 9.5% to 82.8%
- Generalizes to 9 vision tasks with average improvement of 3.1%
- Demonstrates efficient solution to scarcity of high-quality multimodal reinforcement learning data

## Why This Works (Mechanism)
AGILE works by converting a static visual reasoning task into an interactive, code-driven process. By generating Python code to manipulate puzzle pieces and receiving immediate visual feedback, the model learns to iteratively refine its understanding of spatial relationships. The synthetic data generation pipeline enables training on large-scale interactions without human annotation, while the iterative nature allows the model to correct mistakes and build more robust visual representations through exploration.

## Foundational Learning
- **Vision-Language Model (VLM) Architecture**: Why needed - Foundation for multimodal reasoning; Quick check - Verify model supports both vision and language inputs
- **Python Code Generation**: Why needed - Enables precise visual manipulations; Quick check - Validate generated code executes without errors
- **Synthetic Data Generation**: Why needed - Scales training without human supervision; Quick check - Ensure generated puzzles are solvable and diverse
- **Iterative Visual Feedback**: Why needed - Enables active learning and error correction; Quick check - Confirm feedback is accurate and timely
- **Visual Reasoning**: Why needed - Core capability being enhanced; Quick check - Test model on simple spatial relationship tasks
- **Multimodal Reinforcement Learning**: Why needed - Framework for learning from interactions; Quick check - Measure reward signal consistency

## Architecture Onboarding
**Component Map**: VLM -> Code Generator -> Python Execution Environment -> Visual Feedback Loop -> Training Module
**Critical Path**: Input image → VLM reasoning → Python code generation → Code execution → Visual observation → Feedback integration → Parameter update
**Design Tradeoffs**: Interactive approach vs. computational overhead, synthetic data quality vs. real-world variability, fine-grained feedback vs. processing speed
**Failure Signatures**: Code generation errors, incorrect visual feedback interpretation, convergence issues in training, poor generalization to unseen puzzle configurations
**First 3 Experiments**: 1) Test code generation on simple image manipulations, 2) Validate visual feedback accuracy with controlled inputs, 3) Measure training stability with small synthetic datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to 2x2 puzzles and 9 downstream tasks, uncertain transfer to complex real-world scenarios
- Reliance on synthetic data raises questions about capturing real-world visual complexity and variability
- Computational overhead of iterative code generation and execution at inference time not quantified

## Confidence
- 2x2 Jigsaw Accuracy Improvement: High
- Generalization to 9 Vision Tasks: Medium
- Addressing "Limited Visual Perception and Reasoning": Medium

## Next Checks
1. Test AGILE's performance on 3x3 and 4x4 jigsaw puzzles to evaluate scalability beyond the 2x2 case
2. Conduct ablation studies removing the iterative interaction component to isolate its contribution to performance gains
3. Measure inference-time latency and computational costs compared to baseline models across all evaluated tasks