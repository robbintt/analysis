---
ver: rpa2
title: Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning
  Behavior
arxiv_id: '2506.16163'
source_url: https://arxiv.org/abs/2506.16163
tags:
- llms
- choice
- task
- humans
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models (LLMs) were evaluated on three core decision-making\
  \ tasks\u2014uncertainty, risk, and set-shifting\u2014using established psychological\
  \ paradigms. Five leading LLMs (GPT-4o, GPT-o4-mini, Claude, Gemini, DeepSeek) were\
  \ compared with 360 human participants."
---

# Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning Behavior

## Quick Facts
- **arXiv ID:** 2506.16163
- **Source URL:** https://arxiv.org/abs/2506.16163
- **Reference count:** 40
- **Primary result:** LLMs outperform humans on decision-making tasks, showing faster learning and higher consistency but divergent strategies.

## Executive Summary
Large language models (LLMs) were evaluated on three core decision-making tasks—uncertainty, risk, and set-shifting—using established psychological paradigms. Five leading LLMs (GPT-4o, GPT-o4-mini, Claude, Gemini, DeepSeek) were compared with 360 human participants. LLMs consistently outperformed humans, approaching near-optimal performance, and demonstrated faster learning and higher consistency. However, their decision strategies diverged from human patterns: LLMs were more sensitive to penalty frequency under uncertainty, showed minimal risk adjustment under probabilistic conditions, and made fewer random errors but more perseverative errors under set-shifting. Computational modeling revealed that LLMs had higher learning rates, greater outcome sensitivity, and more deterministic choices compared to humans. These findings demonstrate LLMs’ strong general decision-making abilities but also highlight fundamental cognitive differences, cautioning against using them as human substitutes in behavioral research or real-world decision-making contexts.

## Method Summary
Five LLMs and 360 human participants were tested on three psychological decision-making tasks: the Iowa Gambling Task (uncertainty, 80 rounds), Cambridge Gambling Task (risk, 64 rounds), and Wisconsin Card Sorting Task (set-shifting, 64 rounds). LLMs received identical instructions and full history prompts for each round. Computational models (PVL-DecayRL, Cumulative Model, Sequential Learning Model) were fit using hierarchical Bayesian estimation to compare parameter estimates and behavioral patterns between groups.

## Key Results
- LLMs outperformed humans on all three tasks, achieving near-optimal performance.
- LLMs demonstrated faster learning and higher consistency in decision-making.
- LLMs showed divergent strategies: higher sensitivity to penalty frequency, minimal risk adjustment, and more perseverative errors under set-shifting.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Higher learning rates enable LLMs to identify and exploit reward structures faster than humans in uncertain environments.
- Mechanism: LLMs weight cumulative past outcomes more heavily in updating expected values (learning rate parameter A), allowing quicker inference of task payoff structures from feedback.
- Core assumption: The computational model parameters (specifically learning rate) validly capture a real difference in how the systems process sequential feedback.
- Evidence anchors:
  - [abstract] LLMs demonstrated faster learning and computational modeling revealed LLMs had higher learning rates.
  - [section] Fig. 1C shows all LLMs had significantly higher learning rate (A) estimates than humans in the Iowa Gambling Task; linear regression showed steeper slopes in advantageous deck selections over time.
  - [corpus] Related work on LLMs in multi-armed bandit experiments discusses exploration-exploitation strategies but does not directly confirm the learning rate mechanism.
- Break condition: If feedback is non-stationary in complex ways not captured by the task design, high learning rates could lead to overfitting to recent noise rather than true structure.

### Mechanism 2
- Claim: Heightened sensitivity to outcome valence (particularly losses) drives divergent choice preferences between LLMs and humans.
- Mechanism: LLMs exhibit stronger reactions to penalties (higher loss aversion λ and outcome sensitivity α parameters), leading to systematic deck preferences based on penalty frequency rather than just expected value.
- Core assumption: The Prospect Valence Learning model's parameterization accurately reflects the psychological construct of loss sensitivity in both humans and LLMs.
- Evidence anchors:
  - [abstract] LLMs were more sensitive to penalty frequency under uncertainty.
  - [section] Fig. 1B shows LLMs exhibited heterogeneous but systematic preferences between advantageous decks C and D based on penalty frequency, while humans selected them equally; Fig. 1C shows higher outcome sensitivity (α) and generally higher loss aversion (λ) for LLMs.
  - [corpus] One related paper finds LLM agents display human-like biases but distinct learning patterns, which may align with this divergence but does not replicate the specific penalty-frequency sensitivity finding.
- Break condition: In contexts where losses signal useful information rather than pure punishment, this sensitivity could impair adaptive exploration.

### Mechanism 3
- Claim: Deterministic choice consistency allows LLMs to maintain near-optimal strategies but reduces adaptive flexibility under changing rules.
- Mechanism: Higher choice consistency parameters (c, γ, d across models) mean LLMs more reliably select options with highest learned expected value with less stochastic exploration, yielding higher performance in stable tasks but more perseverative errors when rules shift.
- Core assumption: The consistency parameters capture a true tendency toward deterministic exploitation rather than random noise.
- Evidence anchors:
  - [abstract] LLMs showed higher consistency and made fewer random errors but more perseverative errors under set-shifting.
  - [section] Fig. 1C shows higher choice consistency (c) for LLMs in IGT; Fig. 2C shows higher consistency (γ) for GPTo4m/DeepSeek in CGT; Fig. 3C shows higher consistency (d) for most LLMs in WCST; Fig. 3B shows LLMs made more perseverative than non-perseverative errors.
  - [corpus] Corpus papers on LLM learning dynamics note non-human patterns but do not directly address the consistency-perseveration link.
- Break condition: In environments requiring creative deviation from learned patterns or tolerance of ambiguity, high consistency could become rigidity.

## Foundational Learning

- Concept: **Reinforcement Learning Basics**
  - Why needed here: The paper uses RL-inspired computational models (Prospect Valence Learning, Sequential Learning) to explain decision behavior. Understanding expected value updates, learning rates, and exploration-exploitation tradeoffs is essential to interpret the parameter differences.
  - Quick check question: Can you explain how a learning rate of 0.9 would affect how an agent updates its belief about a deck's value after one large penalty, compared to a rate of 0.5?

- Concept: **Computational Modeling & Parameter Estimation**
  - Why needed here: The core evidence for mechanism differences comes from hierarchical Bayesian estimation of model parameters. Understanding what posterior parameter distributions represent and how to compare them is critical for assessing claims.
  - Quick check question: If a model has an R-hat value above 1.1, what does that indicate about the parameter estimation, and why is this mentioned in the paper's methods?

- Concept: **Decision Theory Under Risk vs. Uncertainty**
  - Why needed here: The paper distinguishes these concepts and uses tasks designed to isolate each. Understanding that risk involves known probabilities while uncertainty involves unknown probabilities is necessary to interpret the task designs and behavioral differences.
  - Quick check question: In the Cambridge Gambling Task, why does a 9:1 ratio of boxes represent lower risk than a 6:4 ratio, and how did humans vs. LLMs adjust their betting behavior across these conditions?

## Architecture Onboarding

- Component map:
  - Experimental Framework -> Behavioral Data -> Computational Models -> Comparison Infrastructure

- Critical path:
  1. Understand each psychological task's structure, goal, and dependent measures.
  2. Implement task administration for LLMs with controlled prompts and robustness checks (temperature, rewording, demographic framing).
  3. Fit computational models to human and LLM behavioral data using hierarchical Bayesian methods.
  4. Compare parameter posterior distributions and behavioral metrics between groups.
  5. Interpret divergences through the lens of model parameters and task-specific behavioral signatures.

- Design tradeoffs:
  - **Psychological fidelity vs. memorization**: Rewording tasks and changing payoffs reduces LLM memorization but may alter psychological validity.
  - **Model complexity vs. interpretability**: Each task uses a different cognitive model tailored to its structure, enhancing fit but complicating cross-task comparison of specific parameters.
  - **Controlled lab vs. real-world**: Simplified tasks isolate dimensions but may not predict performance in complex, ambiguous real-world decisions where multiple dimensions interact.

- Failure signatures:
  - **LLM as human substitute**: Using LLM responses to model human population responses will produce misleading signals (e.g., missing human-like risk adjustment, which is a clinical marker).
  - **Optimality assumption**: Assuming near-optimal LLM performance translates to appropriate real-world deployment ignores that LLMs may be rigidly rational in contexts where human "productive irrationality" is advantageous.
  - **Prompt brittleness**: Despite robustness checks, decision patterns may shift with untested prompt variations, context framings, or model updates.

- First 3 experiments:
  1. **Replicate one task** (e.g., IGT) with a new LLM not in the study, using the exact prompt structure from SI Appendix, to verify if the parameter signatures (high learning rate, high consistency) generalize.
  2. **Vary feedback volatility**: Modify the IGT so the payoff structure changes mid-task (increasing uncertainty), and test whether LLMs' high learning rates cause performance collapse or rapid adaptation compared to humans.
  3. **Hybrid decision context**: Create a task blending risk and uncertainty (e.g., ambiguous probabilities with known outcomes) to test if LLMs' distinct strategies per dimension combine or conflict, and whether this reveals new failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What underlying cognitive or mathematical constructs are LLMs emulating if not human decision-making processes?
- Basis in paper: [explicit] The Discussion explicitly asks: "If LLMs do not faithfully replicate human cognition, what exactly are they emulating?"
- Why unresolved: Computational modeling in this study characterized behavior (e.g., higher learning rates, deterministic choices) but did not identify the internal representations driving these non-human strategies.
- What evidence would resolve it: Mechanistic interpretability studies that map specific neural network weights or attention heads to decision-theoretic variables.

### Open Question 2
- Question: In what specific real-world contexts does human "productive irrationality" outperform LLM rationality?
- Basis in paper: [explicit] The authors argue that LLMs may struggle with "productive ‘irrationality’" (e.g., the Wright Brothers' experiments) and ask "when and for what kinds of decisions" LLMs are suitable.
- Why unresolved: This study focused on psychological tasks with clear optimization goals, excluding scenarios where deviation from utility maximization is beneficial.
- What evidence would resolve it: Comparative benchmarks in open-ended, high-risk environments where failure yields critical information (e.g., exploratory scientific research).

### Open Question 3
- Question: How does the divergence between LLM and human risk adjustment strategies affect reliance in high-stakes professional settings?
- Basis in paper: [inferred] The paper notes LLMs lack the flexible risk adjustment typical of healthy humans, a trait associated with cognitive dysfunction, yet they are increasingly used in domains like healthcare.
- Why unresolved: The study identified the behavioral gap (LLMs are rigid; humans are dynamic) but did not test the consequences of this specific mismatch in applied scenarios.
- What evidence would resolve it: Domain-specific studies (e.g., clinical triage) comparing LLM rigidness to human adaptability when probabilities shift dynamically.

## Limitations
- The study's rewording and payoff redesign efforts may not fully prevent LLM memorization of decision-making tasks.
- Computational models rely on assumptions about parameter interpretation that may not capture LLM mechanisms.
- Simplified tasks may not predict real-world performance where multiple decision dimensions interact.

## Confidence

- **High Confidence**: LLMs outperform humans on decision-making tasks and show faster learning and higher consistency.
- **Medium Confidence**: LLMs exhibit distinct decision strategies (e.g., higher sensitivity to penalty frequency, deterministic choices) due to computational modeling results.
- **Low Confidence**: The mechanisms proposed (e.g., high learning rates leading to faster exploitation) are inferred from model parameters and require further validation.

## Next Checks
1. **Generalizability**: Test a new, unseen LLM on the exact IGT prompt structure to verify if the observed parameter signatures (high learning rate, high consistency) generalize beyond the study's models.
2. **Feedback Volatility**: Introduce mid-task payoff structure changes in the IGT to assess whether LLMs' high learning rates lead to performance collapse or rapid adaptation compared to humans.
3. **Hybrid Context**: Design a task blending risk and uncertainty to examine if LLMs' distinct strategies per dimension combine or conflict, revealing new failure modes.