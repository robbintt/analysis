---
ver: rpa2
title: Contextualising Levels of Language Resourcedness that affect NLP tasks
arxiv_id: '2309.17035'
source_url: https://arxiv.org/abs/2309.17035
tags:
- language
- languages
- resources
- south
- african
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of characterising language resourcedness
  in NLP, which is typically oversimplified into binary "low" vs "high" resource categories.
  The authors propose a more nuanced 5-point scale (Very LRL, LRL, RL, HRL, Very HRL)
  based on 11 contextual dimensions beyond just counting tools and corpora.
---

# Contextualising Levels of Language Resourcedness that affect NLP tasks

## Quick Facts
- arXiv ID: 2309.17035
- Source URL: https://arxiv.org/abs/2309.17035
- Reference count: 0
- The paper proposes a 5-point scale for language resourcedness based on 11 contextual dimensions, moving beyond binary low/high resource classifications.

## Executive Summary
The paper addresses the oversimplified binary classification of languages as either low-resource or high-resource in NLP, proposing instead a nuanced 5-point scale (Very LRL, LRL, RL, HRL, Very HRL) based on 11 contextual dimensions. This framework moves beyond simple counting of tools and corpora to include factors like grammar documentation, researcher communities, funding, education systems, and government policy. The approach was operationalized by classifying South African official languages and other African and global languages, demonstrating that isiNdebele is Very LRL, isiXhosa is LRL, Afrikaans and South African English are RL, while Dutch, German, French, and Mandarin are HRL with English as the only Very HRL language.

## Method Summary
The authors developed a framework for classifying languages on a 5-point resourcedness scale using 11 contextual dimensions: grammar documentation, corpus size, basic HLTs, researcher/industry/teacher community size, funding availability, education system (LoLT, school subject, university degree), government policy, monolingual dictionary, computational grammar choice, large corpora with genres, and tool availability across devices/OSs. Languages are classified through expert-based assessment using this dimension checklist, with category assignment based on specific feature bundles mapped to each level. The classification was validated by comparing expert assessments with evidence from multiple repositories including SADiLaR, Masakhane, and Lanafrica.

## Key Results
- The framework identified isiNdebele as Very LRL, isiXhosa as LRL, and both Afrikaans and South African English as RL
- Dutch, German, French, and Mandarin were classified as HRL, with English as the only Very HRL language
- The RL category successfully captured intermediate languages that would be misclassified under binary LRL/HRL frameworks

## Why This Works (Mechanism)

### Mechanism 1: Multi-dimensional decomposition of resourcedness
The framework treats resourcedness as a composite of infrastructure, human capital, institutional support, and technical assets rather than just data volume. This surfaces dependencies hidden by corpus-size metrics alone, recognizing that features like monolingual dictionaries indicate standardization and community capacity that affect annotation pipeline feasibility. Languages at the same scale level face similar systemic constraints, not just similar data volumes.

### Mechanism 2: Dependency-aware planning
Understanding a language's resourcedness level helps project planners anticipate prerequisite-building phases. The framework maps each level to specific missing prerequisites, allowing planners to sequence interventions - for example, grammar digitisation before complex NLP pipelines - rather than assuming tool availability follows from data creation. This prevents projects from stalling due to unmet dependencies regardless of algorithmic sophistication.

### Mechanism 3: Normalising intermediate categories
Introducing RL as a middle state prevents conflating heterogeneous languages under "low-resource." The framework creates conceptual room for differentiated resource allocation and realistic expectations for tool maturity by acknowledging languages like Afrikaans or South African English that are neither LRL nor HRL, which policy and funding decisions can respond to better than binary labels.

## Foundational Learning

- **Concept: BLARK (Basic Language Resource Kit)**
  - Why needed here: Referenced as a precursor counting-based approach that the authors argue against
  - Quick check question: What minimal resources does BLARK assume, and how does the 11-dimension framework differ in scope?

- **Concept: Language vitality vs. resourcedness**
  - Why needed here: The framework explicitly separates endangerment from digital infrastructure, noting they don't correlate perfectly
  - Quick check question: Why might an endangered language still be comparatively well-resourced for NLP?

- **Concept: HLT (Human Language Technology) Audits**
  - Why needed here: Previous South African HLT audits (2011, 2018) are cited as prior categorisation attempts that the paper extends
  - Quick check question: What limitations did the authors identify in audit-based counting approaches?

## Architecture Onboarding

- **Component map:**
  Dimension inventory (11 features) -> Scoring matrix (binary/ordinal values per dimension per language) -> Classification engine (feature bundles mapped to 5 levels) -> Validation layer (expert-based assessment triangulated with repository audits)

- **Critical path:**
  1. Select target language
  2. Gather evidence per dimension from grammar publications, corpus repositories, HLT audits, university catalogs, policy documents
  3. Score each dimension and map to category using Table 2 rules
  4. Aggregate to level classification (dominant feature bundle determines category)
  5. Identify blocking dependencies for upward mobility

- **Design tradeoffs:**
  - Expert-based vs. automated: Expert classification captures tacit context but risks subjectivity; automated counting is reproducible but misses qualitative dimensions
  - Coarse vs. fine thresholds: Fuzzy boundaries enable flexibility but reduce inter-annotator agreement
  - Static vs. dynamic assessment: Periodic reassessment required; frequency depends on resource development velocity

- **Failure signatures:**
  - Classification inconsistent across evaluators → threshold operationalisation needed
  - Language classified as RL but projects still stall → hidden dependencies not captured in current dimensions
  - Policy decisions ignore RL category → communication failure, not framework failure

- **First 3 experiments:**
  1. Inter-rater reliability test: Have 3+ experts independently classify 10 languages using the matrix; measure agreement per dimension and overall
  2. Threshold calibration: Test whether proposed cut-offs (e.g., <100K tokens = Very LRL) align with observed project feasibility
  3. Retrospective prediction: Apply framework to languages that moved between levels historically (e.g., Arabic); assess whether dimension changes preceded level transitions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the empirical relationship between standard language vitality classifications (e.g., EGIDS) and the proposed multi-dimensional resourcedness scale?
- Basis in paper: Section 2 states, "How exactly language endangerment and resourcedness interrelate is yet to be investigated in more detail," noting that anecdotal evidence suggests they do not correlate perfectly
- Why unresolved: The paper focuses on defining resourcedness dimensions rather than correlating them with existing endangerment frameworks
- What evidence would resolve it: A comparative study mapping a large set of languages against both the proposed 5-point resourcedness scale and established vitality scales to identify statistical correlations or divergences

### Open Question 2
- Question: How can the "inherently fuzzy" qualitative dimensions (e.g., teacher shortages, policy effectiveness) be operationalized into quantitative metrics for robust categorization?
- Basis in paper: Section 3.2.2 acknowledges that "more research is needed to refine some of the values for the features to enable a more robust categorization" because boundaries for dimensions like education and policy are currently fuzzy
- Why unresolved: The current framework relies on expert intuition rather than defined thresholds (e.g., specific corpus token counts or percentages of teacher availability) to distinguish between categories
- What evidence would resolve it: The formulation of specific, measurable thresholds for each dimension that allow for objective, reproducible classification of languages without relying solely on expert assessment

### Open Question 3
- Question: Which specific "low-hanging" technologies or policy interventions are most effective for transitioning a language from the "Very LRL" to the "LRL" or "RL" category?
- Basis in paper: The Conclusion states that future research should investigate "what low-hanging technology and policy development can be targeted for each according to the available resources"
- Why unresolved: The paper establishes the classification matrix but does not validate which specific resource developments yield the most efficient progress up the scale for specific language types
- What evidence would resolve it: Intervention studies or longitudinal tracking of language projects that measure the impact of specific resource additions (e.g., creating a monolingual dictionary vs. a spellchecker) on a language's categorization

## Limitations
- The framework relies on expert judgment for qualitative dimensions where precise operational thresholds are not fully specified, potentially affecting inter-rater reliability
- The classification treats resourcedness as relatively stable over time, but rapid policy changes or sudden funding influxes could invalidate current categorizations
- The 25 neighbor papers found provide weak external validation, with average citations of 0 and no direct comparative frameworks identified

## Confidence

- **High confidence**: The 11-dimension framework structure and its conceptual superiority over binary LRL/HRL classification
- **Medium confidence**: The specific threshold values and mappings from dimensions to levels (Table 2), which require further empirical calibration
- **Medium confidence**: The practical utility of the RL category for distinguishing Afrikaans/South African English from truly low-resource languages

## Next Checks
1. **Inter-rater reliability test**: Have 3+ independent experts classify 10 languages using the 11-dimension rubric and measure agreement rates
2. **Threshold calibration study**: Systematically test proposed corpus size and community count thresholds against actual project success rates across multiple languages
3. **Temporal stability assessment**: Reapply the framework to languages that have undergone significant resourcedness changes (e.g., Arabic post-MSR investment) to evaluate prediction accuracy