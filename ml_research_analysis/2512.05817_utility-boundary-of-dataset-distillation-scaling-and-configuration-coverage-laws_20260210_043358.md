---
ver: rpa2
title: 'Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage
  Laws'
arxiv_id: '2512.05817'
source_url: https://arxiv.org/abs/2512.05817
tags:
- bound
- step
- configuration
- dataset
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a unified theoretical framework for dataset\
  \ distillation, showing that gradient, distribution, and trajectory matching methods\
  \ all reduce the same generalization error through a bi-level optimization mechanism.\
  \ The key contributions are two scaling laws: (1) a single-configuration law showing\
  \ error decreases as 1/\u221Ak until reaching an irreducible bound, explaining IPC\
  \ saturation; and (2) a coverage law showing that distilled sample size must scale\
  \ linearly with configuration diversity Hcov to maintain accuracy across configurations."
---

# Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws

## Quick Facts
- arXiv ID: 2512.05817
- Source URL: https://arxiv.org/abs/2512.05817
- Reference count: 40
- Primary result: Establishes unified theoretical framework for dataset distillation with scaling and coverage laws

## Executive Summary
This paper presents a unified theoretical framework for dataset distillation that demonstrates gradient matching, distribution matching, and trajectory matching methods all minimize the same generalization error through bi-level optimization. The authors derive two key scaling laws: a single-configuration law showing error decreases as 1/√k until reaching an irreducible bound (explaining IPC saturation), and a coverage law showing distilled sample size must scale linearly with configuration diversity Hcov to maintain accuracy across configurations. Extensive experiments across MNIST, CIFAR-10/100, and ImageNette validate both laws across diverse methods (DC, DSA, DM, MTT, MGD 3) with high R² values.

## Method Summary
The authors establish a unified framework by showing that gradient matching, distribution matching, and trajectory matching are mathematically equivalent in their goal of minimizing generalization error bounds through bi-level optimization. The framework models dataset distillation as minimizing the expected loss on a validation set while the network is trained on the distilled dataset. This theoretical foundation enables the derivation of scaling laws that govern the relationship between distilled dataset size, configuration diversity, and achievable accuracy. The framework provides both theoretical insights and practical guidance for method selection based on coverage complexity and sample efficiency requirements.

## Key Results
- Proved that gradient, distribution, and trajectory matching methods all reduce the same generalization error through bi-level optimization
- Derived single-configuration scaling law: error decreases as 1/√k until reaching irreducible bound (IPC saturation)
- Established coverage law: distilled sample size must scale linearly with configuration diversity Hcov
- Experiments confirm both laws across diverse methods with high R² values (0.99 for scaling law, 0.94-0.98 for coverage law)

## Why This Works (Mechanism)
The framework works by establishing that different dataset distillation methods are fundamentally optimizing the same objective: minimizing generalization error bounds through bi-level optimization. The single-configuration law emerges from the statistical properties of empirical risk minimization, where the error decreases with the square root of the number of distilled samples until reaching a theoretical lower bound determined by the intrinsic problem complexity. The coverage law arises because maintaining accuracy across diverse configurations requires sufficient samples to represent the full distribution of possible training scenarios, leading to linear scaling with configuration diversity.

## Foundational Learning
- **Bi-level optimization**: Optimization problems where one optimization problem is embedded within another, used here to jointly optimize distilled dataset and model parameters
- **Generalization error bounds**: Mathematical guarantees on how well a model trained on limited data will perform on unseen data
- **Configuration diversity (Hcov)**: Metric quantifying the variety of training configurations (e.g., architectures, hyperparameters) that must be supported
- **Intrinsic problem complexity**: Theoretical lower bound on achievable error determined by the fundamental difficulty of the task
- **Empirical risk minimization**: Principle of selecting models that minimize observed loss on training data

## Architecture Onboarding

**Component Map**
Dataset Distillation Methods (DC, DSA, DM, MTT, MGD 3) -> Unified Framework -> Scaling Laws (Single Configuration, Coverage)

**Critical Path**
1. Define configuration space and diversity metric Hcov
2. Apply bi-level optimization to derive generalization error bounds
3. Derive scaling laws from error bounds
4. Validate empirically across multiple methods and datasets

**Design Tradeoffs**
- Method selection: DC offers better sample efficiency but requires more computation; DSA is faster but needs more samples
- Coverage vs efficiency: Higher configuration diversity requires larger distilled datasets
- Single vs multi-configuration: Multi-configuration distillation provides robustness but increases computational cost

**Failure Signatures**
- Violated coverage law suggests inadequate representation of configuration space
- Deviations from 1/√k scaling indicate issues with optimization or problem complexity estimates
- Low R² values suggest model assumptions don't match empirical behavior

**3 First Experiments**
1. Test single-configuration scaling law with varying k on MNIST using DC method
2. Validate coverage law by varying configuration diversity Hcov while keeping total samples constant
3. Compare DC vs DSA performance across different coverage requirements

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical derivations assume well-behaved loss landscapes that may not hold for highly non-convex deep learning scenarios
- Experiments focus primarily on image classification tasks with standard architectures
- Limited exploration of applicability to other domains like NLP or reinforcement learning
- Real-world performance depends on implementation details and hyperparameter choices not fully explored

## Confidence

**High Confidence:**
- Mathematical equivalence between gradient, distribution, and trajectory matching methods
- Single-configuration scaling law (error ~ 1/√k) with strong theoretical and experimental support

**Medium Confidence:**
- Coverage law's linear relationship between distilled sample size and configuration diversity
- Practical implications for method selection based on coverage complexity

**Low Confidence:**
- Applicability to non-image domains and specialized architectures
- Generalization of theoretical assumptions to highly non-convex scenarios

## Next Checks
1. Test the coverage law across non-image domains (e.g., text classification, time series) to verify whether Hcov remains an appropriate metric for configuration diversity
2. Conduct ablation studies varying the number of configurations while holding total samples constant to isolate effects of diversity versus quantity
3. Evaluate the framework's predictions on architectures beyond standard CNNs (e.g., vision transformers, graph neural networks) to assess generalizability