---
ver: rpa2
title: 'HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference'
arxiv_id: '2602.00777'
source_url: https://arxiv.org/abs/2602.00777
tags:
- attention
- layers
- hylra
- layer
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "HyLRA addresses the computational bottleneck in long-context LLM\
  \ inference caused by quadratic attention complexity and memory-intensive KV caches.\
  \ The core insight is that different transformer layers exhibit distinct sensitivity\
  \ to approximation errors\u2014some require full attention while others can safely\
  \ reuse top-k indices from previous layers."
---

# HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference

## Quick Facts
- **arXiv ID:** 2602.00777
- **Source URL:** https://arxiv.org/abs/2602.00777
- **Reference count:** 10
- **Primary result:** Achieves up to 1.45× speedup with less than 1% accuracy degradation for long-context LLM inference

## Executive Summary
HyLRA addresses the computational bottleneck in long-context LLM inference by introducing a hybrid attention mechanism that selectively applies full attention to sensitive layers while reusing top-k indices from previous layers in tolerant ones. The method leverages layer-wise sensitivity profiling and dynamic programming to optimize the trade-off between accuracy and efficiency. Extensive experiments demonstrate HyLRA consistently outperforms state-of-the-art sparse attention methods across diverse benchmarks while maintaining high accuracy.

## Method Summary
HyLRA employs a two-stage profiling approach to identify layer-wise sensitivity to approximation errors, then uses dynamic programming to derive an optimal hybrid policy that selectively applies full attention or top-k index reuse. The key insight is that different transformer layers exhibit distinct tolerance to attention approximation, allowing for targeted optimization. By eliminating redundant token selection overhead in tolerant layers while preserving full attention in sensitive ones, HyLRA achieves significant speedup without compromising accuracy.

## Key Results
- Achieves up to 1.45× speedup compared to full attention baselines
- Maintains less than 1% accuracy degradation across long-context benchmarks
- Consistently outperforms state-of-the-art sparse attention methods
- Demonstrates effectiveness across diverse long-context tasks and model architectures

## Why This Works (Mechanism)
HyLRA works by exploiting the heterogeneous sensitivity of transformer layers to attention approximation errors. Through systematic profiling, it identifies which layers require full attention precision and which can tolerate approximate top-k methods. The dynamic programming approach then optimizes the selection policy across all layers, balancing accuracy preservation with computational efficiency. This selective reuse eliminates redundant computations while ensuring critical layers maintain their full representational capacity.

## Foundational Learning
- **Layer-wise sensitivity profiling**: Why needed - Different transformer layers have varying tolerance to approximation errors. Quick check - Validate sensitivity patterns are consistent across multiple runs.
- **Dynamic programming optimization**: Why needed - To find optimal trade-off between accuracy and efficiency across all layers. Quick check - Verify DP solution improves over greedy heuristics.
- **Top-k index reuse**: Why needed - Avoids redundant token selection while maintaining attention quality. Quick check - Confirm index stability across similar layers.
- **KV cache optimization**: Why needed - Memory efficiency is critical for long-context inference. Quick check - Measure memory footprint reduction.

## Architecture Onboarding
- **Component map**: Input sequence → Sensitivity Profiler → Dynamic Programming Layer Selector → Hybrid Attention Engine → Output predictions
- **Critical path**: Profiling → Policy Optimization → Attention Computation → KV Cache Management
- **Design tradeoffs**: Accuracy vs. speed (full attention vs. reuse), memory vs. computation (cache size vs. recomputation)
- **Failure signatures**: Accuracy drops when sensitivity profiling misses critical layers; speed degradation when DP selection is suboptimal
- **First experiments**:
  1. Baseline accuracy with full attention across all layers
  2. Sensitivity profiling validation on layer subsets
  3. DP optimization convergence and policy quality

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Relies on offline profiling that may not generalize across different model architectures or task distributions
- Assumes static sensitivity patterns that may not hold for dynamic workloads with varying context distributions
- Experimental validation focuses primarily on single-task performance, leaving multi-task robustness questions open

## Confidence
- **High Confidence**: The core technical contribution of selective attention reuse based on layer sensitivity is sound and well-supported by experiments
- **Medium Confidence**: The claimed 1.45× speedup and sub-1% accuracy degradation are well-documented but may be sensitive to specific hardware configurations
- **Low Confidence**: The generalization of sensitivity profiles across different models and tasks remains largely unproven

## Next Checks
1. **Cross-Architecture Transferability**: Validate whether sensitivity profiles learned on one model architecture transfer effectively to structurally different models without re-profiling
2. **Dynamic Workload Robustness**: Test HyLRA's performance on streaming or dynamic contexts where token distributions change over time
3. **End-to-End System Impact**: Measure full system-level impact including memory bandwidth effects and whether DP layer selection overhead scales favorably with sequence length