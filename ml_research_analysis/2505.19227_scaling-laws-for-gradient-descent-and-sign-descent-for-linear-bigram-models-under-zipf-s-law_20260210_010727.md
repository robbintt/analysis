---
ver: rpa2
title: Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models
  under Zipf's Law
arxiv_id: '2505.19227'
source_url: https://arxiv.org/abs/2505.19227
tags:
- descent
- scaling
- sign
- where
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the optimization dynamics of gradient descent\
  \ (GD) and sign descent (SD) on linear bigram models when word frequencies follow\
  \ Zipf's law or more generally power-law distributions. The authors derive scaling\
  \ laws for convergence rates as a function of vocabulary size d and desired relative\
  \ error \u03B5."
---

# Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf's Law

## Quick Facts
- arXiv ID: 2505.19227
- Source URL: https://arxiv.org/abs/2505.19227
- Reference count: 40
- Primary result: Derives scaling laws for GD and SD on linear bigram models, showing Zipf-distributed data (α=1) creates worst-case dimension scaling for GD (d^(1-ε)) but allows SD to achieve d^(1/2) scaling

## Executive Summary
This paper establishes scaling laws for gradient descent (GD) and sign descent (SD) on linear bigram models when token frequencies follow power-law distributions. The authors show that Zipf-distributed data (α=1) creates the worst-case optimization difficulty for GD, requiring iterations scaling nearly linearly with vocabulary size. In contrast, SD with properly tuned step-sizes achieves significantly better scaling of d^(1/2). The analysis explains empirical observations that adaptive methods like Adam outperform GD on transformer-based language models, particularly when dealing with large vocabularies following Zipf's law.

## Method Summary
The paper analyzes optimization dynamics on a linear bigram model using square loss, where the Hessian eigenvalues are the token frequencies. For GD, exact closed-form dynamics are derived showing convergence rates depend on partial sums of power-law sequences. For SD, an oscillatory model is introduced to characterize behavior under fixed step-size. The analysis uses integral approximations and rescaled time approaches to derive asymptotic scaling laws as vocabulary size grows. Synthetic experiments validate the theoretical predictions, and real data from OpenWebText confirms the Zipf-worst-case behavior. The model assumes token frequencies and conditional distributions follow the same power law with exponent α.

## Key Results
- For Zipf-distributed data (α=1), GD requires d^(1-ε) iterations to reach ε relative error, showing worst-case scaling
- For α ≤ 1, GD requires d^α log(1/ε) iterations, with worst scaling at α = 1
- For α > 1, GD scales as (1/ε)^(α/(α-1)), independent of d
- SD with properly tuned step-size scales as d^(1/2) for α = 1, significantly better than GD
- The crossover point where SD outperforms GD occurs at α = 1/2

## Why This Works (Mechanism)

### Mechanism 1: Heavy-tailed token frequencies create dimension-dependent optimization difficulty for gradient descent
When word frequencies follow Zipf's law (α=1), GD requires iterations scaling nearly linearly with vocabulary size d, while lighter tails (α>1) show no dimension dependence. The eigenvalues of the optimization problem are directly the token frequencies π_k. Under power-law decay π_k ∝ 1/k^α, the effective condition number and mass in the "tail" eigenvalues depends critically on α. For α≤1, the sum Σπ_k diverges or grows slowly with d, forcing the optimizer to make progress on increasingly many poorly-conditioned directions. The rescaled time analysis reveals that progress requires t to scale with d^α for α<1 and d^(1-ε) for α=1.

### Mechanism 2: Sign descent's coordinate-wise normalization provides dimension scaling advantage for Zipf-distributed data
For Zipf-distributed data (α=1), sign descent requires only d^(1/2) iterations versus d^(1-ε) for GD, providing ~100x speedup for practical vocabulary sizes. Sign descent uses updates proportional to sign(∇L) rather than ∇L itself. This "scale-freeness" means each coordinate's update magnitude is independent of its eigenvalue (frequency). Under the oscillation model, directions reach η/2 oscillation amplitude after O(|δ_k(0)|/η) steps. With optimal step-size tuning, the number of directions that can be "solved" in T steps scales to balance the decreasing and oscillatory regimes, yielding the d^(1/2) dependence for α>1/2.

### Mechanism 3: Classical worst-case bounds fail to capture heavy-tail optimization dynamics
Standard smooth convex rates (sublinear O(1/t) or linear (1-μ/L)^t) predict either no progress or instant convergence as d→∞, missing the actual d^(1-ε) and √d scalings. Classical bounds use only extreme eigenvalues (μ=π_d, L=π_1), yielding condition number κ=d for Zipf data. The sublinear rate gives O(d/log(d)) scaling while linear gives O(d), both predicting no progress for t<d. However, the actual dynamics show progress is made on high-frequency directions early, with the transition at k*=(1+t)^(1/α). The integral approximation captures this continuous progress that worst-case bounds miss.

## Foundational Learning

- **Power law distributions and Zipf's law**
  - Why needed here: The entire analysis depends on understanding how π_k ∝ 1/k^α behaves differently for α<1, α=1, and α>1, particularly how the partial sums H_{d,α} = Σk^{-α} scale.
  - Quick check question: For α=1 (Zipf), does Σ_{k=1}^d 1/k converge or diverge as d→∞? What about for α=1.5?

- **Gradient descent dynamics on quadratic objectives**
  - Why needed here: The loss evolution f(x_t) = (1/2)Σλ_i(1-ηλ_i)^{2t}δ_i(0)² is the foundation for all GD analysis; understanding why each eigencomponent evolves independently is crucial.
  - Quick check question: If λ₁=1 and λ₁₀₀=0.01, after 100 steps with η=1, which component has decreased more: the high-frequency or low-frequency direction?

- **Sign descent and oscillatory behavior**
  - Why needed here: Unlike GD, SD doesn't converge with fixed step-size but oscillates; the paper models this via Assumption 4.1 to derive tractable dynamics.
  - Quick check question: If δ_ij(0)=0.7 and η=0.2, after 4 steps of sign descent with δ(t+1)=δ(t)-η·sign(δ(t)), what is |δ(4)|?

## Architecture Onboarding

- **Component map**: Data model (token frequencies π_k and conditional frequencies π_k|j) -> Linear bigram model (W ∈ ℝ^{d×d} with Hessian diag(π_1,...,π_d)) -> Optimization dynamics (GD with η=1/π_1, SD with tuned η) -> Analysis pipeline (exact dynamics → integral approximation → asymptotic limit with rescaled time → time-to-ε)

- **Critical path**:
  1. Verify power-law assumption on target corpus (replicate Figure 3)
  2. Compute frequency statistics (O(n) pass through data) and conditional tables (O(n) with hashing)
  3. Choose optimizer based on α: SD preferred for α≈1, GD acceptable for α>1
  4. Scale iteration budget: t ∝ d^(1-ε) for GD with Zipf, t ∝ √d for SD
  5. For SD: tune step-size for the planned iteration budget T (Figure 5 right shows the envelope)

- **Design tradeoffs**:
  - **GD vs SD**: GD simpler (fixed step-size, guaranteed convergence) but scales poorly with d for α≤1; SD scales better but requires budget-aware step-size tuning and oscillates
  - **Step-size for SD**: Smaller η gives better final accuracy but slower; η=δ_d(0)/T ensures all directions decreasing but may be too conservative; optimal lies between δ_d(0)/T and δ_1(0)/T
  - **Model complexity**: Linear bigram is tractable but ignores cross-entropy loss, momentum, and deeper architectures; extension unclear

- **Failure signatures**:
  - **GD stalls early**: Check if α≤1; if Zipf-like, expect slow progress on tail directions—this is inherent, not a bug
  - **SD loss plateaus high**: Step-size too large for budget T; reduce η or increase T
  - **SD oscillates wildly**: Step-size too large; check that T·η < δ_1(0)
  - **Scaling predictions don't match data**: Verify power-law assumption (Figure 3); real data may deviate from Assumption 2.3

- **First 3 experiments**:
  1. **Validate power-law assumption**: On your target corpus, plot π_k vs k (log-log) and π_k|j median vs k; estimate α from the slope. Compare to Figure 3.
  2. **Replicate convergence curves**: Implement the linear bigram model with synthetic power-law frequencies; verify that GD and SD match Theorems 3.1 and 4.5 as d grows (replicate Figures 4 and 7).
  3. **Step-size sweep for SD**: For fixed d and target ε, grid-search η over [δ_d(0)/T, δ_1(0)/T] and verify the optimal follows the φ scaling in Definition 4.4. Compare against envelope in Figure 5 (right).

## Open Questions the Paper Calls Out

- **Open Question 1**: How do the derived scaling laws for gradient and sign descent extend to the online (stochastic) setting? The paper only analyzes deterministic GD and SD; stochastic variants introduce noise that changes optimization dynamics. What evidence would resolve it: Derivation of scaling laws for SGD and stochastic sign descent under the same power-law data assumptions.

- **Open Question 2**: Can momentum mechanisms be incorporated into sign descent to dampen oscillations and achieve better scaling with dimension? Sign descent enters an oscillatory regime with fixed step-size; momentum could smooth this but complicates the analysis. What evidence would resolve it: Convergence analysis of sign descent with momentum on the linear bigram model, comparing scaling laws to the momentum-free case.

- **Open Question 3**: Can the analysis be extended to bilinear models or cross-entropy loss, where closed-form training dynamics are unavailable? The current results rely on closed-form dynamics from the quadratic loss structure; cross-entropy lacks this tractability. What evidence would resolve it: Development of approximation techniques or bounds that characterize scaling behavior for cross-entropy loss under Zipf-distributed data.

- **Open Question 4**: Can frequency-aware optimization algorithms that estimate word frequencies provably improve scaling over standard GD for large vocabulary sizes? The paper shows GD scales poorly (d^(1-ε)) for Zipf data, suggesting preconditioning-based methods could help, but this is not formally established. What evidence would resolve it: Convergence analysis of frequency-adaptive methods showing improved dimension scaling (e.g., from d^(1-ε) toward sqrt(d)) on the bigram problem.

## Limitations

- The analysis critically depends on the power-law assumption for token frequencies, which may not hold precisely in real language data.
- The sign descent analysis relies on an oscillatory model that captures qualitative behavior but may not reflect exact dynamics in finite dimensions.
- The linear bigram model is a significant simplification of modern language models, and the square loss differs from the cross-entropy objectives typically used in practice.

## Confidence

- **High confidence**: The asymptotic scaling laws for gradient descent under power-law frequencies (Theorem 3.1). The integral approximation and rescaled time analysis are mathematically rigorous.
- **Medium confidence**: The sign descent convergence rates (Theorem 4.5) and the crossover point α=1/2 where SD becomes preferable to GD. While the oscillatory model is reasonable, the analysis is more heuristic.
- **Medium confidence**: The claim that Adam's empirical advantage over SGD on transformer models stems from Zipf-distributed data. This is a compelling theoretical explanation but the connection to Adam specifically is indirect.

## Next Checks

1. **Power-law validation on diverse corpora**: Test the frequency distribution assumptions on multiple language datasets beyond OpenWebText, including non-English corpora and code datasets, to verify the α=1 Zipf behavior and the conditional frequency assumptions.

2. **Finite-dimension corrections**: Extend the asymptotic analysis to include explicit finite-d correction terms, particularly for the α=1 case where the d^(1-ε) scaling has significant practical implications for vocabulary sizes of 10⁴-10⁶.

3. **Extension to cross-entropy loss**: Adapt the analysis to the cross-entropy loss typically used in language modeling, where the gradient structure differs from the squared loss in the linear bigram model, to validate whether the Zipf-worst-case behavior persists.