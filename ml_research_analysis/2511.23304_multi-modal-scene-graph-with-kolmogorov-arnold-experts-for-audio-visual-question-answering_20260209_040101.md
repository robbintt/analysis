---
ver: rpa2
title: Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question
  Answering
arxiv_id: '2511.23304'
source_url: https://arxiv.org/abs/2511.23304
tags:
- scene
- visual
- graph
- audio
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses audio-visual question answering (AVQA) by
  introducing a multi-modal scene graph combined with Kolmogorov-Arnold Network (KAN)-based
  experts. The method constructs a structured scene graph from visual and audio cues
  to explicitly model objects and their relationships, and uses KAN-based experts
  within a Mixture-of-Experts (MoE) framework to improve fine-grained temporal reasoning.
---

# Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question Answering

## Quick Facts
- arXiv ID: 2511.23304
- Source URL: https://arxiv.org/abs/2511.23304
- Reference count: 40
- Primary result: Introduces multi-modal scene graph with KAN-based experts for AVQA, achieving 78.14% and 77.33% accuracy on MUSIC-AVQA benchmarks

## Executive Summary
This paper addresses audio-visual question answering by introducing a multi-modal scene graph combined with Kolmogorov-Arnold Network (KAN)-based experts. The method constructs a structured scene graph from visual and audio cues to explicitly model objects and their relationships, and uses KAN-based experts within a Mixture-of-Experts (MoE) framework to improve fine-grained temporal reasoning. Experiments on the MUSIC-AVQA and MUSIC-AVQA v2 benchmarks show state-of-the-art performance, achieving average accuracies of 78.14% and 77.33%, respectively. The approach demonstrates superior capability in handling overlapping audio cues and complex cross-modal interactions.

## Method Summary
SHRIKE introduces a two-stage training approach for audio-visual question answering. First, it trains a Multi-Modal Scene Graph (M2SG) Decoder using pseudo-labels generated by MiniCPM-o to predict relationship triplets from video segments. Second, it freezes the decoder and trains a fusion layer and KAN-based experts to integrate visual, audio, and question features with question-conditioned Gaussian routing for temporal attention. The method leverages frozen CLIP and VGGish encoders for feature extraction, uses cross-attention for multimodal fusion, and applies Hungarian matching loss for scene graph training followed by cross-entropy loss for QA classification.

## Key Results
- Achieves state-of-the-art accuracy of 78.14% on MUSIC-AVQA and 77.33% on MUSIC-AVQA v2 benchmarks
- Demonstrates superior performance on questions requiring disambiguation of overlapping audio sources
- Shows KAN-based experts provide modest but consistent improvements over standard MLPs (0.5-0.6% accuracy gain)
- KAN experts show stronger locality for temporal reasoning compared to MLPs under same network depth

## Why This Works (Mechanism)

### Mechanism 1: Structural Disambiguation via Multi-Modal Scene Graphs
If audio cues are overlapping or ambiguous, explicitly modeling visual structure and spatial relationships allows the system to ground audio sources to specific objects. The model uses a Multi-Modal Scene Graph (M2SG) Decoder to predict relationship triplets (e.g., ⟨person, play, piano⟩, ⟨piano, left of, scene⟩) from video frames. Crucially, it defines audio-related relationships (like "louder than") alongside spatial ones. By selecting triplets relevant to the question, the model isolates specific instruments based on visual location when audio alone is insufficient.

### Mechanism 2: Fine-Grained Temporal Focus via KAN Locality
Replacing standard MLPs with Kolmogorov-Arnold Networks (KANs) in the temporal integration experts allows for sharper, more localized attention to specific moments in time. The paper utilizes KAN-based experts within a Mixture-of-Experts (MoE) framework. Unlike MLPs, KANs use learnable spline-based functions on the edges. The authors argue this provides stronger locality, enabling the model to fit specific temporal patterns (events) more precisely rather than averaging them out.

### Mechanism 3: Question-Conditioned Gaussian Routing
Aligning the temporal focus (Gaussian distributions) with the question query ensures that the model weighs video segments containing answer-critical evidence more heavily. A Gaussian Generator uses cross-attention between the question feature and fused multimodal features to predict the mean (µ) and standard deviation (σ) for Gaussian distributions. These distributions generate a soft mask over the temporal dimension, effectively filtering the output of the KAN experts to focus on "question-relevant events."

## Foundational Learning

**Scene Graphs (SG)**
- Why needed: The paper extends standard visual scene graphs to multi-modal (audio-visual) contexts. Understanding how objects and predicates are structured is required to interpret the M2SG decoder.
- Quick check: Can you distinguish a standard visual triplet (e.g., "holding") from the audio-visual triplet introduced here (e.g., "louder than")?

**Kolmogorov-Arnold Networks (KAN)**
- Why needed: This is the core architectural novelty for the experts. You must understand that KANs replace fixed activation functions on nodes with learnable functions (splines) on edges.
- Quick check: How does the "locality" of spline functions differ from the "global" nature of standard MLP weights in approximating a function?

**Mixture of Experts (MoE)**
- Why needed: The model uses an MoE framework to handle temporal integration. You need to know how a "Router" selects specific experts for different inputs.
- Quick check: In this architecture, what acts as the "Router" for the KAN experts? (Answer: The routing scores derived from the question-conditioned features)

## Architecture Onboarding

**Component map:**
Encoders (Frozen) -> M2SG Decoder -> Fusion Layer -> Gaussian KAN Experts -> Classifier
- Encoders: CLIP (Visual/Text), VGGish (Audio) - frozen feature extractors
- M2SG Decoder: Transformer Decoder predicting relationship triplets from A/V features
- Fusion Layer: Cross-attention merging Visual, Audio, and Question tokens
- Gaussian KAN Experts: Generates temporal Gaussians and processes features via KAN-experts
- Classifier: Linear layer for final answer prediction

**Critical path:**
1. Preprocessing: Extract features with CLIP/VGGish
2. Stage 1: Train M2SG Decoder using pseudo-labels from MiniCPM-o (Relationship Loss)
3. Stage 2: Freeze M2SG. Train Fusion Layer and KAN Experts using the QA Classification Loss

**Design tradeoffs:**
- Pseudo-labeling: Using MiniCPM-o for scene graph annotations is faster than human labeling but introduces noise (40% accuracy on direct QA)
- Two-Stage Training: Decoupling Scene Graph generation from QA training prevents the scene graph from being biased by specific questions, but requires a separate training schedule

**Failure signatures:**
- Missing Triplets: If the M2SG fails to detect an instrument (e.g., due to occlusion), the model cannot reason about it later
- Coarse Temporal Attention: If the Gaussian σ becomes too large, the model reverts to global averaging, losing fine-grained reasoning

**First 3 experiments:**
1. Sanity Check: Run the M2SG decoder on a sample video and verify the output triplets against the ground truth (check for "hallucinated" audio relations)
2. Ablation (KAN vs MLP): Replace the KAN experts with standard MLPs and observe the drop in "Audio-Visual QA" accuracy (Section 4.4 reports a ~0.5-0.6% drop)
3. Visualization: Reproduce Figure 4. Plot the Gaussian weights over time for a specific question to see if the "blue curve" actually spikes at the correct instrument entrance

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can the multi-modal scene graph framework generalize effectively to non-musical domains (e.g., cooking, sports) where audio-visual relationships are less structured?
- Basis in paper: [inferred] The paper evaluates exclusively on MUSIC-AVQA, which focuses on specific instruments and musical interactions.
- Why unresolved: It is unclear if the defined predicate categories (e.g., "louder than") and object classes apply to general video understanding.
- What evidence would resolve it: Performance evaluation on a general audio-visual QA benchmark (e.g., AVQA or Pano-AVQA).

**Open Question 2**
- Question: How can the model be improved to accurately capture subtle, long-tail audio relationships like relative loudness among less salient instruments?
- Basis in paper: [explicit] The authors state in Section 4.6 that the model "cannot reliably capture the relative volume of less salient instruments" leading to incorrect inferences.
- Why unresolved: The current feature extraction and scene graph generation process suppresses these subtle cues.
- What evidence would resolve it: An ablation study incorporating an audio-only contrastive loss or a specialized attention mechanism for low-amplitude events.

**Open Question 3**
- Question: To what extent does the noise in MLLM-generated pseudo-labels limit the model's performance ceiling compared to human-annotated scene graphs?
- Basis in paper: [explicit] Section 4.7 notes that MiniCPM-o achieves only ~40% direct QA accuracy, providing "coarse semantic cues" rather than oracle labels.
- Why unresolved: The model depends on these imperfect annotations for the Scene Graph Decoder pre-training.
- What evidence would resolve it: Comparing model performance when trained on MiniCPM-o labels versus a subset of human-verified ground-truth scene graphs.

## Limitations

**Scene Graph Quality Uncertainty**: The method relies on pseudo-labels generated by MiniCPM-o for scene graph annotations. While the paper reports 40% accuracy on direct QA from this model, it does not provide validation of the scene graph triplet quality specifically. If the pseudo-labels contain systematic errors (e.g., consistently mislabeling "louder than" relationships), the model may learn incorrect associations between visual objects and audio sources.

**KAN Generalization Gap**: The paper claims KANs provide "stronger locality" for temporal reasoning, but this application to AVQA is novel with limited comparative evidence. The 0.5-0.6% accuracy improvement over MLPs (Section 4.4) is modest, and the benefit may be task-specific to the MUSIC-AVQA dataset's temporal structure rather than a general architectural advantage.

**Gaussian Assumption Violation**: The question-conditioned Gaussian routing assumes answers are localized in specific temporal segments. For questions requiring global video understanding (e.g., "What is the overall mood?"), this mechanism may discard necessary context, though this limitation is not explicitly discussed or tested.

## Confidence

**High Confidence**: The overall framework architecture and two-stage training procedure are clearly specified. The reported benchmark results (78.14% and 77.33% accuracy) on MUSIC-AVQA datasets are verifiable through standard evaluation protocols.

**Medium Confidence**: The specific mechanisms by which KAN locality improves temporal reasoning and how scene graphs disambiguate overlapping audio are theoretically sound but rely on assumptions about data characteristics that may not generalize. The modest ablation improvements suggest these components help but may not be transformative.

**Low Confidence**: The quality of automatically generated scene graph annotations is uncertain without direct validation. The claim that spline-based parameterization provides "stronger locality" than MLPs lacks empirical comparison on this specific task type in the literature.

## Next Checks

1. **Scene Graph Quality Audit**: Manually validate a random sample (n=100) of generated scene graph triplets against ground truth for both visual and audio-related relationships. Calculate precision/recall specifically for audio-grounding predicates like "louder than" to assess whether visual grounding is reliable.

2. **Temporal Attention Localization Test**: For questions where ground truth identifies specific temporal regions (e.g., instrument entrances), measure whether the Gaussian weights from SHRIKE actually peak in those regions more accurately than baseline methods. Create a temporal localization accuracy metric beyond overall QA accuracy.

3. **Cross-Dataset Generalization**: Test SHRIKE on a different AVQA dataset (e.g., TVQA or How2QA with audio) to determine whether the KAN-based temporal reasoning and scene graph grounding provide consistent benefits across domains, or if performance gains are specific to MUSIC-AVQA's music performance context.