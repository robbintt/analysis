---
ver: rpa2
title: Interactive Medical Image Analysis with Concept-based Similarity Reasoning
arxiv_id: '2503.06873'
source_url: https://arxiv.org/abs/2503.06873
tags:
- concept
- similarity
- prototypes
- image
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Interactive Medical Image Analysis with Concept-based Similarity Reasoning

## Quick Facts
- **arXiv ID:** 2503.06873
- **Source URL:** https://arxiv.org/abs/2503.06873
- **Reference count:** 39
- **Primary result:** Concept-based Similarity Reasoning improves interpretability and diagnostic accuracy in medical image classification

## Executive Summary
This paper introduces a Concept-based Similarity Reasoning (CSR) framework for interpretable medical image classification that grounds high-level concepts to localized image regions through patch-level similarity reasoning. Unlike traditional concept bottleneck models that predict scalar concept scores, CSR computes 2D similarity maps between input feature patches and pre-learned concept prototypes, then uses the maximum similarity as the concept activation score. The framework employs contrastive learning with multiple prototypes per concept to capture intra-concept diversity and introduces novel spatial and concept-level human interaction mechanisms that allow clinicians to engage directly with specific image areas and reject incorrect concepts.

## Method Summary
The CSR framework implements a three-stage training pipeline: (1) Concept Model Pre-training trains a feature extractor and concept head to predict concepts using binary cross-entropy loss; (2) Prototype Learning extracts local concept vectors using CAM-weighted features, then trains a projector and multiple prototypes per concept using a contrastive loss to capture concept diversity; (3) Task Training freezes features and prototypes while training a linear task head to predict target labels from similarity scores. The model computes localized explanations by measuring cosine similarity between input feature patches and concept prototypes, producing spatially-grounded activation maps that clinicians can interact with through bounding box annotations and concept rejection.

## Key Results
- CSR provides localized concept explanations by grounding prototypes to specific image regions
- Contrastive learning with multiple prototypes per concept improves generalization of concept representations
- Spatial and concept-level human interaction improves prediction accuracy and mitigates shortcut learning
- Macro F1-score improvements demonstrated on TBX11K, VinDr-CXR, and ISIC datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Patch-level prototype similarity enables localized concept explanation without post-hoc analysis.
- **Mechanism:** CSR computes 2D similarity maps between input feature patches and pre-learned concept prototypes via cosine similarity, then takes the maximum as the concept activation score, grounding each concept to specific image regions.
- **Core assumption:** Medical concepts manifest in spatially localizable patterns that can be matched against prototypical exemplars in feature space.
- **Evidence anchors:** Abstract states CSR "provides localized explanation by grounding prototypes of each concept on image regions"; Section 2.1 defines similarity score as maximum cosine similarity between prototype and feature patches; Related work "SL-CBM" addresses similar locality challenges.
- **Break condition:** If concept prototypes overfit to spurious visual features (shortcuts), localization degrades into Clever-Hans artifacts.

### Mechanism 2
- **Claim:** Contrastive learning with multiple prototypes per concept improves generalization of concept representations.
- **Mechanism:** A projector maps local concept vectors to compact embedding space. Multi-prototype contrastive loss pulls concept vectors toward nearest same-concept prototypes while pushing away from other-concept prototypes. Multiple prototypes capture intra-concept multi-modality.
- **Core assumption:** Concepts form separable clusters in projected space; small set of prototypes can capture concept diversity better than raw local vectors.
- **Evidence anchors:** Section 2.2 states "projected feature space is more compact and has better inter-concept separation" (Figure 4); Equation 9 shows contrastive loss formulation with margin δ; Related work "Shortcuts and Identifiability in Concept-based Models" examines reliability conditions.
- **Break condition:** If M is too small, prototypes fail to capture concept diversity; if feature space lacks linear separability, contrastive loss provides limited benefit.

### Mechanism 3
- **Claim:** Spatial and concept-level human interaction improves prediction accuracy and mitigates shortcut learning.
- **Mechanism:** (1) Spatial interaction: Doctors draw positive/negative bounding boxes to create importance map that reweights similarity maps. (2) Concept interaction: Doctors reject incorrect concepts by zeroing their similarity scores. (3) Train-time: Doctors prune prototype atlas to remove shortcut examples.
- **Core assumption:** Clinicians can accurately identify relevant regions and absent concepts; model's reasoning pathway is transparent enough for meaningful intervention.
- **Evidence anchors:** Abstract mentions "novel spatial-level interaction, allowing doctors to engage directly with specific image areas"; Section 3.3, Table 3 shows human interaction provides +0.3-0.7% F1 gain; Table 2 shows Pointing Game hit rate improves from 60.9% to 79.5% after atlas refinement.
- **Break condition:** If interaction frequency exceeds clinical workflow tolerance, or if feedback is inconsistent/noisy, benefits diminish.

## Foundational Learning

- **Concept: Class Activation Maps (CAM)**
  - Why needed here: CSR uses CAM-like concept activation maps to weight feature maps when extracting local concept vectors.
  - Quick check question: How does spatial softmax normalization differ from standard softmax, and why does it matter for attention weighting?

- **Concept: Contrastive Learning / Triplet Loss**
  - Why needed here: The multi-prototype learning objective is derived from smoothed Triplet Loss in a unit sphere.
  - Quick check question: In metric learning, why does pulling positive pairs and pushing negative pairs create better embedding structure?

- **Concept: Concept Bottleneck Models (CBMs)**
  - Why needed here: CSR extends CBM's image-level concept prediction to patch-level similarity reasoning.
  - Quick check question: What is the trade-off between bottleneck size (number of concepts) and model accuracy in standard CBMs?

## Architecture Onboarding

**Component map:**
Image → Feature extractor F → Concept head C → Projector P → Multiple prototypes per concept → Cosine similarity → Task head H → Logits

**Critical path:**
Inference: Image → F → feature map f → P(f) → cosine similarity with all prototypes → spatial max → similarity vector s → H → logits

Training: Image → F → C → CAMs → weighted sum → local concept vectors → projector P → contrastive loss with prototypes

**Design tradeoffs:**
- M (prototypes per concept): Higher M captures diversity but increases atlas size and explanation complexity
- Explanation size vs accuracy: CSR keeps K concepts (small explanation) while leveraging M×K prototypes for accuracy
- Interaction threshold: Only trigger human feedback when top-1 vs top-2 probability gap < 0.3

**Failure signatures:**
- Low Pointing Game hit rate → prototypes capturing shortcuts, not semantic concepts
- High variance in intra-concept similarity → M too small for concept diversity
- Concept interaction has no effect → task head may have learned shortcut bypass

**First 3 experiments:**
1. Visualize similarity maps comparing local concept vectors vs. learned prototypes on held-out images to confirm generalization (replicate Figure 4)
2. Run Pointing Game evaluation with ground-truth bounding boxes to quantify localization accuracy vs. baselines
3. Implement concept-level interaction by zeroing specified concept scores and measuring prediction shift on ambiguous cases

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the Concept-based Similarity Reasoning (CSR) framework be extended to leverage multi-modal foundation models while maintaining its specific interpretability advantages over pure vision features?
- **Basis in paper:** Authors state in Related Works: "In this work, we focus on the research direction of vision features and explore multi-modality learning in future studies."
- **Why unresolved:** Current implementation relies solely on vision features, whereas multi-modal approaches (e.g., CLIP) are becoming standard, leaving integration of these modalities into prototype reasoning process undefined.
- **What evidence would resolve it:** Study evaluating CSR's performance and prototype fidelity when text embeddings are integrated into concept learning or similarity reasoning stages.

### Open Question 2
- **Question:** How does the CSR model's diagnostic accuracy and explanation quality degrade when the pre-defined set of concepts is incomplete or contains annotation noise?
- **Basis in paper:** Method treats "findings annotations as the concepts," assuming these labels are available and sufficient to describe pathology.
- **Why unresolved:** Paper does not test model's robustness in scenarios where "atlas" of concepts fails to cover all visual variations or contains errors, which is common in real-world clinical data.
- **What evidence would resolve it:** Experiments measuring performance drops and analyzing "concept leakage" when percentage of concept labels are removed or randomized during training.

### Open Question 3
- **Question:** Can the spatial-level interaction mechanism scale effectively to 3D volumetric medical data (e.g., CT or MRI) without imposing an infeasible cognitive load on clinicians?
- **Basis in paper:** Paper validates spatial interaction (drawing bounding boxes) exclusively on 2D datasets (Chest X-ray and Skin), which allows for simple 2D interaction.
- **Why unresolved:** Extending "spatial interactivity" to 3D volumes requires interacting with voxels or 3D regions, significantly more time-consuming and complex than drawing 2D boxes.
- **What evidence would resolve it:** User study with radiologists assessing time required and usability of performing spatial interventions on 3D slices or volumes.

## Limitations

- **Implementation complexity:** The three-stage training pipeline requires careful hyperparameter tuning and specific architectural choices not fully specified in the main text
- **Clinical workflow integration:** While human interaction theoretically improves accuracy, practical benefits depend on whether interaction frequency fits within realistic clinical time constraints
- **Concept coverage dependency:** Model performance depends heavily on the completeness and quality of pre-defined concept annotations, which may be incomplete or noisy in real clinical settings

## Confidence

- **Mechanism 1 (Patch-level similarity):** Medium - Localization approach is conceptually sound but depends heavily on whether prototypes capture true semantic concepts versus dataset-specific shortcuts
- **Mechanism 2 (Contrastive learning):** Medium - Multi-prototype approach addresses diversity, but effectiveness depends on feature space properties and appropriate margin/scale parameters
- **Mechanism 3 (Human interaction):** Low-Medium - While interaction theoretically improves accuracy, practical benefits depend on clinical workflow integration and consistency of human feedback

## Next Checks

1. **Prototype quality validation:** Visualize similarity maps comparing local concept vectors vs. learned prototypes on held-out images to verify that prototypes generalize beyond training examples and capture semantic concepts rather than shortcuts.

2. **Localization accuracy testing:** Run Pointing Game evaluation with ground-truth bounding boxes to quantify localization accuracy versus baseline approaches, specifically testing whether CSR prototypes avoid highlighting spurious features like medical devices.

3. **Interaction workflow feasibility:** Implement concept-level interaction by zeroing specified concept scores and measure prediction shift on ambiguous cases, while simultaneously assessing the practical time burden of human feedback within realistic clinical workflows.