---
ver: rpa2
title: 'LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage
  Rule-Based RL'
arxiv_id: '2503.07536'
source_url: https://arxiv.org/abs/2503.07536
tags:
- reasoning
- multimodal
- arxiv
- training
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LMM-R1, a two-stage rule-based reinforcement
  learning framework that enhances reasoning in 3B-parameter Large Multimodal Models
  (LMMs) by first strengthening foundational reasoning via text-only data, then generalizing
  to multimodal domains. The method addresses challenges of multimodal reasoning data
  scarcity and degraded reasoning from multimodal pretraining.
---

# LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL

## Quick Facts
- arXiv ID: 2503.07536
- Source URL: https://arxiv.org/abs/2503.07536
- Reference count: 40
- Primary result: 4.83% average improvement on multimodal benchmarks for 3B-parameter LMMs

## Executive Summary
LMM-R1 introduces a two-stage rule-based reinforcement learning framework that enhances reasoning in 3B-parameter Large Multimodal Models by first strengthening foundational reasoning via text-only data, then generalizing to multimodal domains. The method addresses challenges of multimodal reasoning data scarcity and degraded reasoning from multimodal pretraining. Experiments on Qwen2.5-VL-Instruct-3B show LMM-R1 achieves 4.83% average improvement over baselines on multimodal benchmarks and 4.5% on text-only benchmarks, with a 3.63% gain in complex Football Game tasks, demonstrating that text-based reasoning enhancement enables effective multimodal generalization.

## Method Summary
LMM-R1 employs a two-stage reinforcement learning approach using PPO with rule-based rewards. Stage 1 (FRE-Text) trains on text-only mathematical reasoning data (DeepScaler-40K) to establish foundational reasoning patterns. Stage 2 (MGT) adapts these circuits to visual contexts through limited multimodal RL. The reward function combines format compliance (detecting reasoning structure with ･･･ and answer format with <answer></answer>) and accuracy verification (symbolic checking of mathematical expressions). The framework specifically targets compact 3B-parameter architectures where architectural constraints limit reasoning capacity.

## Key Results
- FRE-Text improves text-only benchmarks by 4.29% while FRE-Multi degrades by 2.29%
- MGT-Geo achieves 8.25% improvement over baseline by recovering and enhancing reasoning capabilities
- LMM-R1 demonstrates 4.83% average improvement on multimodal benchmarks and 4.5% on text-only benchmarks
- Football Game tasks show 3.63% gain, validating effectiveness on complex reasoning problems

## Why This Works (Mechanism)

### Mechanism 1
Text-only rule-based RL establishes foundational reasoning patterns that transfer to multimodal domains. Complex text-only mathematical reasoning (DeepScaler-40K) forces the model to develop robust chain-of-thought capabilities. These reasoning schemata—problem decomposition, symbolic manipulation, verification—remain available when visual inputs are introduced, allowing the model to "reason about" images rather than merely "perceive and respond."

### Mechanism 2
Two-stage training creates synergistic capability gains impossible with single-stage approaches. FRE establishes high-capacity reasoning circuits without visual interference. MGT then adapts these circuits to visual contexts through limited multimodal RL. Critically, MGT on top of FRE improves both reasoning AND perception (e.g., MGT-Geo shows +11.68% on "Vision Only" tasks over FRE-Text), while direct multimodal RL (FRE-Multi) degrades reasoning.

### Mechanism 3
Response length correlates with reasoning depth and predicts downstream generalization. The paper observes that FRE-Text models develop longer responses (600→800 tokens) with explicit mathematical formulations, while FRE-Multi models compress responses (150→80 tokens). This "Think More vs. Think Less" dichotomy reflects whether RL optimizes for deep reasoning exploration or surface-level pattern matching.

## Foundational Learning

- Concept: **Rule-based RL vs. RLHF**
  - Why needed here: Understanding why rule-based rewards (format + accuracy) suffice without human preference labels
  - Quick check question: Can you explain why verifiable answers (numeric/option) enable rule-based RL while open-ended VQA tasks cannot?

- Concept: **PPO (Proximal Policy Optimization)**
  - Why needed here: The paper uses PPO with KL penalty; understanding the objective function is essential for reproduction
  - Quick check question: What role does the KL divergence term play in preventing reward hacking?

- Concept: **Catastrophic Forgetting in Multimodal Training**
  - Why needed here: The paper explicitly addresses this—multimodal RL degrades text reasoning
  - Quick check question: Why does training on simpler multimodal data (FRE-Multi) hurt performance on complex text reasoning?

## Architecture Onboarding

- Component map: DeepScaler-40K (text) -> VerMulti-65K (multimodal) -> Domain-specific (Geo15K, Sokoban) -> Qwen2.5-VL-Instruct-3B policy model with PPO training

- Critical path:
  1. Verify verifiable answer extraction pipeline works for your target tasks
  2. Implement format reward parser (detect <answer></answer> module
  3. Ensure GPU memory for 8192-token generation during rollouts

- Design tradeoffs:
  - FRE-Text vs. FRE-Multi: Text-only builds stronger reasoning but weaker perception; multimodal does the opposite
  - Response length: Longer isn't always better—MGT-PerceReason stabilizes at 200-250 tokens as optimal balance
  - Dataset size: 40K text samples outperform 65K multimodal samples for reasoning development

- Failure signatures:
  - Response length collapsing below 100 tokens → model has learned to skip reasoning
  - Accuracy reward plateauing early → task may lack verifiable answers or reward hacking occurred
  - KL divergence spiking → policy diverged from reference, reduce learning rate

- First 3 experiments:
  1. **Sanity check**: Train FRE-Text on 1K text samples; verify response length increases and format compliance reaches >95%
  2. **Ablation**: Compare FRE-Text vs. FRE-Multi on held-out MATH500 subset; expect +2-4% gap favoring text-only
  3. **Transfer test**: Take FRE-Text checkpoint, run 500 steps of MGT-Geo; verify MathVision geometry subset improves without MATH500 degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would LMM-R1's two-stage approach provide comparable benefits for larger LMMs (7B+ parameters), or is the effectiveness specific to smaller models with constrained capacity?
- Basis in paper: [explicit] The conclusion states "Future work involves extending our framework to additional LMMs" and the introduction emphasizes the approach was designed for "compact 3B-parameter architectures where architectural constraints limit reasoning capacity."
- Why unresolved: All experiments use only Qwen2.5-VL-Instruct-3B; no larger model variants are tested.
- What evidence would resolve it: Experiments applying LMM-R1 to 7B, 14B, and 72B variants of Qwen2.5-VL or other model families, comparing relative improvement percentages across model scales.

### Open Question 2
- Question: What mechanism causes direct multimodal RL training to shorten reasoning chains while text-only RL encourages longer, more elaborate reasoning processes?
- Basis in paper: [inferred] Figure 4 shows FRE-Multi decreases response length from ~150 to ~80 tokens while FRE-Text increases from ~600 to ~800 tokens. The discussion section analyzes this "counterintuitive phenomenon" and output patterns but does not identify the underlying cause.
- Why unresolved: The paper provides empirical observations (response length trends, sample output analysis) but does not investigate whether this stems from data complexity differences, reward signal properties, or training dynamics.
- What evidence would resolve it: Controlled experiments varying data complexity independently of modality, analyzing attention patterns and reward landscape characteristics during training.

### Open Question 3
- Question: What are the optimal data quantities and quality thresholds for each training stage to balance reasoning transfer efficiency with computational cost?
- Basis in paper: [inferred] The paper uses 40K text samples for FRE and varying amounts for MGT stages (15K for Geo, 65K for PerceReason) without ablation studies on data scaling or quality filtering effects.
- Why unresolved: No systematic analysis of how data quantity, difficulty distribution, or quality filtering affects final performance or training efficiency.
- What evidence would resolve it: Ablation experiments varying data sizes (e.g., 10K, 20K, 40K, 80K for FRE), quality filtering thresholds, and analyzing performance-per-training-step curves.

## Limitations
- Reward function implementation details remain underspecified, particularly the symbolic verification mechanism
- Dataset composition requires filtering large multimodal datasets to extract verifiable answers, which is computationally intensive
- Transfer learning boundaries are untested beyond the specific text-to-multimodal direction demonstrated
- Long-term stability across different random seeds and extended training periods is not reported

## Confidence

**High Confidence**: The core finding that text-only rule-based RL (FRE-Text) enhances reasoning capabilities that transfer to multimodal domains is well-supported by the 4.29% improvement on text benchmarks and 4.5% average gain across multimodal tasks.

**Medium Confidence**: The claim that two-stage training creates synergistic gains beyond single-stage approaches is supported by experimental comparisons but relies on specific architecture choices (Qwen2.5-VL-3B) and dataset selections that may not generalize universally.

**Low Confidence**: The assertion that this approach offers a "data-efficient paradigm" is questionable given the paper still uses 65K+ multimodal samples for the MGT stage.

## Next Checks

1. **Ablation on Reward Components**: Systematically vary α (format vs accuracy reward weighting) and test whether accuracy reward alone suffices for reasoning enhancement, or if format compliance is essential to the observed gains.

2. **Cross-Domain Transfer Test**: Apply FRE-Text trained on DeepScaler to completely different reasoning domains (e.g., logical puzzles, code generation) without multimodal fine-tuning to test modality-agnostic reasoning transfer.

3. **Model Capacity Scaling**: Repeat the two-stage training on larger LMMs (7B-13B) to determine whether the text-then-multimodal advantage persists at scale, or if larger models can learn reasoning and perception simultaneously without degradation.