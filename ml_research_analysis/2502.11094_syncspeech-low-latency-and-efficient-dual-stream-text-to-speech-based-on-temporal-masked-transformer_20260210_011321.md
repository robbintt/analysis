---
ver: rpa2
title: 'SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on
  Temporal Masked Transformer'
arxiv_id: '2502.11094'
source_url: https://arxiv.org/abs/2502.11094
tags:
- speech
- text
- tokens
- syncspeech
- duration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SyncSpeech, a dual-stream text-to-speech
  (TTS) model capable of receiving streaming text input while simultaneously generating
  streaming speech. The core innovation is the Temporal Masked Transformer (TMT),
  which uses a novel attention mask and a two-stage training strategy to predict both
  speech tokens and durations in a temporally-ordered manner.
---

# SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer

## Quick Facts
- arXiv ID: 2502.11094
- Source URL: https://arxiv.org/abs/2502.11094
- Authors: Zhengyan Sheng; Zhihao Du; Shiliang Zhang; Zhijie Yan; Yexin Yang; Zhenhua Ling
- Reference count: 22
- Dual-stream TTS model with 3.2-3.8× faster first packet delay and 6.4-8.5× faster real-time factor

## Executive Summary
SyncSpeech introduces a novel dual-stream text-to-speech architecture that enables low-latency streaming speech generation while maintaining high efficiency. The model uses a Temporal Masked Transformer (TMT) to predict both speech tokens and durations in a temporally-ordered manner, allowing it to begin generating speech after receiving only the second text token. Through a two-stage training strategy, SyncSpeech achieves significant latency improvements over existing dual-stream TTS models while maintaining comparable speech quality to autoregressive baselines.

## Method Summary
The proposed method uses a dual-stream architecture with a Temporal Masked Transformer core that employs a novel attention mask and two-stage training strategy. The first stage pre-trains the model using phoneme-text paired data to learn duration prediction, while the second stage fine-tunes with full text-audio pairs. The TMT architecture enables the model to predict speech tokens and their durations simultaneously, allowing efficient parallel decoding while maintaining temporal ordering. The system achieves low latency by starting speech generation early in the text stream and high efficiency through batched token generation.

## Key Results
- First packet delay reduced by 3.2-3.8× compared to recent dual-stream TTS models
- Real-time factor accelerated by 6.4-8.5× while maintaining comparable speech quality
- Demonstrated effectiveness on both English (LJSpeech) and Mandarin (ASVspoof2019 LA) datasets
- Achieves streaming capability with speech generation starting from the second text token

## Why This Works (Mechanism)
The Temporal Masked Transformer addresses the fundamental challenge of aligning variable-length text sequences with speech tokens in a streaming context. By using a carefully designed attention mask that enforces temporal ordering, the model can predict both what speech tokens to generate and how long each should last, all while processing text incrementally. The two-stage training strategy first teaches the model to predict durations from phonemes, then refines this with full text-audio pairs, creating a robust duration predictor that enables accurate synchronization between text and speech streams.

## Foundational Learning

1. **Dual-stream TTS Architecture**
   - *Why needed*: Separates text processing from speech generation to enable streaming
   - *Quick check*: Can the model start generating speech before seeing the entire text input?

2. **Temporal Masked Attention**
   - *Why needed*: Enforces sequential processing while allowing parallel token generation
   - *Quick check*: Does the attention mask prevent looking ahead in the text sequence?

3. **Two-stage Training Strategy**
   - *Why needed*: First learns duration prediction from phonemes, then refines with full supervision
   - *Quick check*: Is the duration predictor more accurate after the two-stage process?

4. **Parallel Token Decoding**
   - *Why needed*: Generates all speech tokens for a time step simultaneously for efficiency
   - *Quick check*: Can the model decode multiple speech tokens in a single forward pass?

5. **Streaming Text Processing**
   - *Why needed*: Enables real-time speech generation as text arrives
   - *Quick check*: What is the minimum number of text tokens required to start speech generation?

## Architecture Onboarding

**Component Map:** Text Encoder -> Temporal Masked Transformer -> Duration Predictor -> Speech Decoder

**Critical Path:** Text input flows through encoder, TMT applies temporal attention to predict speech tokens and durations, then decoder generates audio from tokens using predicted durations.

**Design Tradeoffs:** The model trades some potential quality gains from full-context processing for significantly reduced latency and increased efficiency. The 8k sampling rate limitation enables faster processing but may not suit all applications.

**Failure Signatures:** Poor duration prediction leads to speech-text misalignment, excessive latency indicates attention mask issues, and quality degradation suggests problems in the two-stage training process.

**Three First Experiments:**
1. Test latency by measuring first speech token generation time with varying text input lengths
2. Evaluate duration prediction accuracy on held-out data to verify two-stage training effectiveness
3. Compare speech quality with autoregressive baseline using subjective listening tests

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the provided text.

## Limitations

- Relies on complex two-stage training strategy that may limit scalability across languages
- 8k sampling rate constraint restricts applicability to high-fidelity audio applications
- Fixed maximum text length of 512 characters may not accommodate longer utterances
- Performance on noisy or accented speech datasets remains unclear

## Confidence

**High Confidence:** Latency measurements and real-time factor improvements are well-supported by experimental results on both English and Mandarin datasets.

**Medium Confidence:** Speech quality comparisons show comparable performance to autoregressive models, but evaluation relies primarily on subjective listening tests without extensive objective metrics.

**Low Confidence:** Cross-lingual generalization claims are not thoroughly validated, and model behavior with rare words or code-switching remains untested.

## Next Checks

1. Evaluate performance on noisy speech datasets (e.g., noisy LJSpeech) to verify robustness claims under realistic acoustic conditions

2. Test the model with longer utterances exceeding 512 characters to assess scalability limits and potential quality degradation

3. Conduct ablation studies on the two-stage training strategy to quantify the contribution of each training phase to final performance