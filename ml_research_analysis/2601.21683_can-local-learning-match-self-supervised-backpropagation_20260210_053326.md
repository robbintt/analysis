---
ver: rpa2
title: Can Local Learning Match Self-Supervised Backpropagation?
arxiv_id: '2601.21683'
source_url: https://arxiv.org/abs/2601.21683
tags:
- local-ssl
- learning
- layer
- clapp
- bp-ssl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges local and global self-supervised learning (SSL)
  algorithms for deep neural networks. It proves that under conditions of orthonormal
  weight matrices and unconstrained trainable lateral connections, local SSL algorithms
  (e.g., CLAPP) can implement exactly the same weight updates as global backpropagation-based
  SSL.
---

# Can Local Learning Match Self-Supervised Backpropagation?

## Quick Facts
- arXiv ID: 2601.21683
- Source URL: https://arxiv.org/abs/2601.21683
- Reference count: 40
- Local SSL algorithms (e.g., CLAPP) can match global BP-SSL gradients under orthonormality + trainable feedback, with spatial dependence and direct feedback improving approximation in non-linear convnets

## Executive Summary
This paper bridges local and global self-supervised learning algorithms for deep neural networks. It proves that under conditions of orthonormal weight matrices and unconstrained trainable lateral connections, local SSL algorithms (e.g., CLAPP) can implement exactly the same weight updates as global backpropagation-based SSL. When orthonormality is violated (e.g., shrinking layer widths), adding direct feedback from the last layer improves local SSL's approximation of global SSL gradients. The theory guides the development of CLAPP++ variants for convolutional networks, incorporating spatial dependence in feedback projections. Empirically, CLAPP++ achieves state-of-the-art performance among local SSL methods on CIFAR-10 (80.51%), STL-10 (78.66%), and Tiny ImageNet (36.63%), matching the performance of comparable global SSL baselines.

## Method Summary
The paper develops CLAPP++ variants that optimize feedback projections B_l per layer to minimize a local contrastive loss L_l. For convolutional networks, spatial dependence is introduced by allowing B_l to vary across spatial locations through local pooling. CLAPP++DFB uses direct feedback from the last layer activity z'_L as reference instead of same-layer activity. Weight updates follow a three-factor Hebbian rule: ΔW = γ_t · (B·c) · ρ'(a) · z. Training uses Adam optimizer with lr=2e-4 for 300 epochs, batch size 32, and L2 regularization on B_l. The VGG-style architecture has 6 conv layers with ReLU activations. Evaluation uses frozen representations with a linear classifier trained separately.

## Key Results
- CLAPP++ achieves 80.51% on CIFAR-10, 78.66% on STL-10, and 36.63% on Tiny ImageNet
- Spatially-dependent feedback projections improve CIFAR-10 accuracy from 73.21% to 80.51%
- CLAPP++DFB consistently improves gradient alignment and accuracy over standard CLAPP++
- Local SSL matches global SSL gradients under orthonormality + trainable feedback conditions

## Why This Works (Mechanism)

### Mechanism 1: Orthonormality + Trainable Feedback Enables Exact Gradient Matching
In deep linear networks, local-SSL can produce identical weight updates to global BP-SSL when feedforward weights W_l are orthonormal and feedback projections B_l are trainable and unconstrained. Orthonormality causes successive weight matrix products to cancel, while trainable B_l* converges to (W^L...W^{l+1})^T B_L* (W^L...W^{l+1}), matching the effective backward pass transformation. The core assumptions are linear activations, orthonormal weight matrices, unconstrained B_l optimization space, and separation of timescales (B_l evolves faster than W_l).

### Mechanism 2: Direct Top-Layer Feedback Compensates for Dimensional Reduction
When network width decreases across layers, using the final layer activity z'_L as reference (instead of same-layer z'_l) improves BP-SSL gradient approximation. In shrinking-width networks, semi-orthonormal matrices create null spaces that cause layerwise and end-to-end gradients to diverge. Direct feedback bypasses this by projecting from a consistent reference space. Theorem 3.3 proves that ||∂L_l*/∂W_l - ∂L*/∂W_l||_F ≥ ||∂L_l*,fb/∂W_l - ∂L*/∂W_l||_F for linear f.

### Mechanism 3: Spatially-Dependent Feedback Projections for Convolutional Networks
Making feedback projections B_l spatially dependent (different for different feature map locations) improves gradient alignment in convnets. BP-SSL gradients in convnets exhibit structured 2D spatial dependence due to convolution backward operators. Original CLAPP averages features across spatial locations, constraining B_l's search space. Spatial dependence allows B_l to capture position-specific feedback patterns, better approximating the true backward pass. Nearby features can share the same B_l via local pooling to reduce memory while preserving benefits.

## Foundational Learning

- Concept: **Contrastive Self-Supervised Learning**
  - Why needed here: The paper's CLAPP++ builds on contrastive local learning where positive pairs (augmentations of same image) are distinguished from negative pairs (different images) at each layer
  - Quick check question: Can you explain why SimCLR creates positive/negative pairs and how InfoNCE loss operates?

- Concept: **Hebbian Learning and Neuromodulation**
  - Why needed here: The paper frames local-SSL as biologically plausible, with weight updates interpretable as three-factor Hebbian rules (neuromodulator × dendritic prediction × Hebbian term)
  - Quick check question: How does the CLAPP++ update rule ΔW = γ_t · (B·c) · ρ'(a) · z extend classic Hebbian learning?

- Concept: **Backpropagation Gradient Flow**
  - Why needed here: Understanding how BP computes ∂L*/∂z^l via chain rule through successive layers is essential to appreciate what local-SSL approximates and why orthonormality enables equivalence
  - Quick check question: Derive the backpropagated gradient ∂L*/∂z^l for a 3-layer linear network—where does W^T appear?

## Architecture Onboarding

- Component map:
  Feedforward path (convnet with ReLU) -> Feedback projections B_l (trainable, spatially-dependent) -> Reference vectors c_l (z'_l for CLAPP++, z'_L for CLAPP++DFB) -> Loss function L_l (Type 2 contrastive with softplus)

- Critical path:
  1. Initialize W_l (orthonormal for theory, standard for practice) and B_l randomly
  2. Forward pass: compute z^l_pos, z^l_neg, c_l for all layers
  3. Per-layer: optimize B_l to minimize L_l (can use LBFGS or gradient descent)
  4. Update W_l using local gradient with optimized B_l
  5. Evaluate via frozen representation + linear classifier

- Design tradeoffs:
  - CLAPP++ vs. CLAPP++DFB: CLAPP++DFB requires only one context vector z^L (memory efficient) but introduces top-down dependency; CLAPP++ stores per-layer contexts
  - Spatial patch size: Larger patches reduce memory but weaken gradient alignment
  - B_l optimization frequency: Faster timescale improves alignment but increases compute

- Failure signatures:
  - Random fixed B_l: Gradient alignment drops to ~0.3-0.5, accuracy degrades severely
  - No spatial dependence in convnets: ~7-8% accuracy drop on CIFAR-10
  - Predictive coding with CLAPP objective: Fails to learn useful representations (Table A3, 36.75% on STL-10 vs. 78.36% with BP)

- First 3 experiments:
  1. Verify gradient equivalence in linear networks: Implement 6-layer linear MLP with orthonormal W_l, compute B_l* via LBFGS, measure cosine similarity between local-SSL and BP-SSL gradients (target: 1.0). Ablate each condition (orthonormality, trainable B_l, linearity) to observe degradation.
  2. Spatial dependence ablation on CIFAR-10: Train CLAPP++ with and without spatial dependence on same VGG architecture. Expect ~7% gap; analyze per-layer gradient alignment to confirm mechanism.
  3. Direct feedback comparison on shrinking-width network: Train 6-layer ReLU MLP with halving widths. Compare CLAPP++ vs. CLAPP++DFB gradient alignment throughout training (Figure A2 pattern). This validates Theorem 3.3 in non-linear setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise empirical benefits of CLAPP++DFB in terms of resource efficiency (memory and compute) compared to standard backpropagation?
- Basis in paper: The authors state: "We leave a deeper analysis [of the resource efficiency of CLAPP++DFB] to future work."
- Why unresolved: The paper theoretically motivates the memory advantage (needing only the single context vector $z_L$ rather than storing activations per layer) but does not quantify these gains experimentally.
- What evidence would resolve it: Benchmarks measuring peak VRAM usage and training throughput (samples/second) for CLAPP++DFB versus BP-SSL on standard hardware (GPUs) or neuromorphic chips.

### Open Question 2
- Question: Can the theoretical equivalence between local-SSL and global BP-SSL be extended to algorithms that require explicit normalization of layer activities, such as Forward-Forward?
- Basis in paper: The authors note they "have not been able to extend Theorem 3.1 to linear networks that include normalization of layer