---
ver: rpa2
title: 'Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series
  Models'
arxiv_id: '2511.11622'
source_url: https://arxiv.org/abs/2511.11622
tags:
- mean
- time
- series
- tokenization
- uniform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically examines the interplay between tokenizer
  design and pretraining in time series forecasting models. It evaluates different
  scaling (mean, min-max, normal) and quantization (uniform, normal, exponential-decay)
  strategies across vocabulary sizes of 512, 1024, and 4096 tokens, comparing random
  initialization against pretrained Qwen 3 weights.
---

# Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models

## Quick Facts
- **arXiv ID:** 2511.11622
- **Source URL:** https://arxiv.org/abs/2511.11622
- **Reference count:** 7
- **One-line primary result:** Normal scaling with uniform quantization consistently outperforms other tokenizer configurations, with pretraining amplifying benefits at smaller vocabularies.

## Executive Summary
This paper systematically examines how tokenizer design and pretraining interact in time series forecasting models. The authors evaluate different scaling and quantization strategies across vocabulary sizes of 512, 1024, and 4096 tokens, comparing random initialization against pretrained Qwen 3 weights. The study finds that tokenizer configuration primarily governs representational capacity and stability, while pretraining influences optimization efficiency. Normal scaling with uniform binning consistently delivers the best performance, with pretraining providing the greatest advantage at smaller vocabularies where discretization imposes severe information bottlenecks.

## Method Summary
The authors implement three scaling methods (mean, min-max, normal) and three quantization strategies (uniform, normal, exponential-decay) to create nine tokenizer configurations. They train Qwen 3 (600M) transformer models on the GiftEval dataset using temporal masking objectives, comparing pretrained initialization versus random initialization across vocabulary sizes of 512, 1024, and 4096 tokens. The primary metric is Mean Absolute Scaled Error (MASE), with token utilization measured via Cramér's V statistic. Theoretical bounds are derived from perfect predictor analysis to establish lower limits on achievable performance.

## Key Results
- Normal scaling combined with uniform quantization consistently outperforms other configurations across all vocabulary sizes
- Pretrained models achieve lower MASE scores, especially for challenging tokenizer configurations and small vocabularies (512 tokens)
- Token space utilization correlates positively with performance for smaller vocabularies but negatively at larger sizes (4096 tokens), suggesting fragmentation effects
- Theoretical bounds follow a power law with vocabulary size, showing diminishing returns at larger vocabularies

## Why This Works (Mechanism)

### Mechanism 1: Normal scaling with uniform quantization
Normal scaling (a = 1/σ, b = -μ/σ) transforms data to zero mean and unit variance, while uniform quantization distributes bin edges evenly across the normalized range. This ensures consistent token density across the range, avoiding the underutilization problem where most bins are empty while data clusters densely in narrow regions. This balances resolution between the distribution center and tails.

### Mechanism 2: Pretrained knowledge amplification
Pretrained weights encode optimization priors and representation structure that compensate for information loss from aggressive quantization. At small vocabularies (512 tokens), this compensation is critical because discretization error dominates. At large vocabularies (4096 tokens), sufficient representational capacity exists regardless of initialization, so pretrained knowledge primarily improves convergence speed and calibration rather than ultimate accuracy.

### Mechanism 3: Token space utilization dynamics
Higher Cramér's V indicates more balanced token usage across the vocabulary. Balanced utilization allows the model to represent temporal dynamics more precisely. However, at 4096 tokens, excessive dispersion fragments statistical efficiency, and more concentrated utilization becomes advantageous. This creates a non-monotonic relationship between token utilization and forecasting performance.

## Foundational Learning

- **Concept: Quantization (binning)**
  - Why needed: The paper discretizes continuous time series values into finite token vocabularies
  - Quick check: Given a normalized time series with values roughly following a standard normal distribution, which binning strategy would concentrate resolution near zero? (Answer: Normal CDF-based binning)

- **Concept: Normalization scaling**
  - Why needed: Three scaling methods (mean, min-max, normal) preprocess data before quantization
  - Quick check: If a time series has mean μ = 100 and standard deviation σ = 10, what are the scaling parameters a and b for normal normalization? (Answer: a = 0.1, b = -10)

- **Concept: Transfer learning from language models**
  - Why needed: The study relies on Qwen 3 (600M) pretrained weights and examines initialization strategy interaction with tokenizer design
  - Quick check: Why would a model pretrained on text tokens benefit from a well-designed time series tokenizer, given that the token vocabulary is different? (Answer: Optimization priors and representation structures transfer even when token vocabularies differ)

## Architecture Onboarding

- **Component map:** Raw time series X₁:C → Scaling module → Quantization module → Token indices → Qwen 3 transformer → Token predictions → Dequantization → Forecasted values X₍C+1:H₎
- **Critical path:** Implement scaling functions with configurable parameters (a, b) → Implement quantization with configurable bin boundaries → Optimize bin width using perfect predictor theoretical analysis → Train with temporal masking on GiftEval dataset → Evaluate using MASE metric
- **Design tradeoffs:** Small vocabularies require careful tokenizer design but enable multi-modal sharing; large vocabularies are robust to tokenizer choice but increase computational cost
- **Failure signatures:** Mean&uniform baseline at 512 tokens showing severe underperformance indicates tokenizer bottleneck dominates; low training loss with high MASE divergence indicates monitoring MASE directly; token distribution clustering in narrow bins indicates underutilization
- **First 3 experiments:**
  1. Implement normal scaling + uniform quantization with 1024 vocabulary; train both pretrained and randomly initialized models; compare MASE and convergence speed
  2. Sweep vocabulary sizes (512, 1024, 4096) with baseline mean&uniform tokenizer to replicate power law relationship
  3. Measure Cramér's V for each tokenizer configuration; correlate with MASE to validate utilization-performance relationship

## Open Questions the Paper Calls Out

- Explore adaptive tokenization strategies that dynamically co-evolve with pretrained embeddings
- Design cross-modal pretraining schemes explicitly robust to vocabulary shifts
- Investigate whether optimal tokenizer configuration is task-dependent versus universal
- Examine if saturation effects at 4096 tokens persist across domains with different data variance

## Limitations

- The correlation analysis showing Cramér's V inversion at 4096 tokens is based on small sample sizes (n=3 per vocabulary size)
- Individual hyperparameter configurations for each experimental setup were not provided
- The study focuses exclusively on univariate forecasting with GiftEval data, limiting multivariate and cross-domain generalization
- Theoretical lower bounds assume specific data distributions that may not generalize to all time series domains

## Confidence

**High Confidence:** Normal scaling with uniform quantization consistently outperforming other configurations; power law relationship between vocabulary size and representational capacity; pretraining primarily improving convergence speed at larger vocabularies.

**Medium Confidence:** Pretrained models amplifying small vocabulary tokenizer effectiveness; correlation between token utilization and performance; mechanism explaining transfer learning benefits.

**Low Confidence:** Claim that tokenizer misalignment can invert pretraining benefits; generalizability to multivariate time series and non-financial domains; specific quantitative relationships between token utilization metrics and forecasting performance.

## Next Checks

1. **Statistical Validation of Cramér's V Inversion:** Replicate correlation analysis with 10-15 diverse time series datasets spanning multiple domains to test robustness of inversion pattern at 4096 tokens.

2. **Transfer Learning Mechanism Dissection:** Conduct ablation studies comparing Qwen 3 initialization against other pretrained backbones and models pretrained specifically on discretized time series using representation similarity metrics.

3. **Multivariate Extension and Domain Transfer:** Implement best-performing tokenizer configurations on multivariate datasets like ETTh1/2 and UCI repository; test cross-domain transfer by pretraining on one domain and evaluating on another.