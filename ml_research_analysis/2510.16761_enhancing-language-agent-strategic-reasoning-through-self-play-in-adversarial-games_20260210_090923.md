---
ver: rpa2
title: Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial
  Games
arxiv_id: '2510.16761'
source_url: https://arxiv.org/abs/2510.16761
tags:
- game
- sco-pal
- actions
- games
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SCO-PAL, a step-level policy optimization method
  through play-and-learn to improve strategic reasoning in language agents for adversarial
  games. By analyzing opponent selection, the authors find that self-play is most
  effective for strategy refinement.
---

# Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games

## Quick Facts
- arXiv ID: 2510.16761
- Source URL: https://arxiv.org/abs/2510.16761
- Reference count: 34
- Primary result: ~30% win rate increase and 54.76% win rate vs GPT-4 via self-play-based step-level optimization

## Executive Summary
This paper introduces SCO-PAL, a step-level policy optimization method that significantly improves strategic reasoning in language agents for adversarial games through self-play. The authors demonstrate that self-play generates the most balanced training distributions compared to weak or strong fixed opponents, enabling more stable gradient updates. By combining Monte Carlo estimation of per-action win rates with a two-stage behavioral cloning and Kahneman-Tversky optimization pipeline, SCO-PAL achieves substantial performance gains across six games from the GTBench benchmark.

## Method Summary
SCO-PAL employs a three-stage approach: (1) self-play game interaction at temperature 0.7 to collect 1,000 episodes per game, (2) Monte Carlo estimation of per-action win rates using empirical win/loss counts across trajectories, and (3) two-stage refinement with behavioral cloning on advantageous actions (reward > 0.5) followed by Kahneman-Tversky optimization on advantageous vs disadvantageous action pairs. The method uses Qwen2-7B-Instruct as base model, trains with 8× A100 80GB GPUs, and evaluates win rates against Random, MCTS(1000), GPT-3.5, and GPT-4 over 100 episodes per game.

## Key Results
- ~30% increase in average win rate against four opponents across six GTBench games
- 54.76% win rate against GPT-4, demonstrating significant strategic improvement
- Win-rate estimation method outperforms discounted reward (48.66%) and beta distribution (36.77%) approaches
- Two-stage BC→KTO optimization outperforms joint training and direct KTO methods

## Why This Works (Mechanism)

### Mechanism 1: Self-Play Generates Balanced Training Distributions
Self-play produces the most diverse and balanced ratio of advantageous-to-disadvantageous actions compared to weak or strong fixed opponents. When skill levels are mismatched, outcomes become predictable—strong opponents suppress advantageous actions while weak opponents produce repetitive winning patterns that limit strategic diversity. Self-play maintains a ~50% win rate, naturally balancing positive and negative learning signals across the action space.

### Mechanism 2: Step-Level Monte Carlo Reward Estimation
Estimating per-action win rates via Monte Carlo provides more granular learning signals than trajectory-level rewards. The reward for a state-action pair (sᵢ, aᵢ) is computed as r(sᵢ, aᵢ) = N_win / N_all, the empirical win rate across all occurrences. This allows the optimizer to label individual moves as advantageous or disadvantageous even when the final outcome was a loss.

### Mechanism 3: Two-Stage BC → KTO Optimization
Behavioral Cloning followed by Kahneman-Tversky Optimization outperforms joint training or direct KTO. BC first adapts the model to the game environment using only advantageous actions, providing a stable policy initialization. KTO then refines preferences by treating advantageous actions as "desirable" and disadvantageous as "undesirable," using a prospect-theoretic loss that doesn't require paired comparisons.

## Foundational Learning

- **Monte Carlo Estimation**
  - Why needed: Core reward estimation technique for step-level learning
  - Quick check: Given 100 occurrences of (s, a) with 67 wins and 33 losses, what is r(s, a)? (Answer: 0.67)

- **Preference Optimization (DPO vs KTO)**
  - Why needed: KTO is the final optimization layer; understand why it works without paired data
  - Quick check: Why would DPO struggle when you have 1000 state-action pairs but few repeated states for comparison? (Answer: DPO needs paired (preferred, dispreferred) actions under the same state; KTO only needs binary desirability labels.)

- **Behavioral Cloning**
  - Why needed: First-stage training; understand it as supervised learning on filtered expert actions
  - Quick check: If you BC-train on all actions from winning trajectories, what subtle problem might emerge? (Answer: Winning trajectories can contain suboptimal moves that still led to wins due to opponent mistakes.)

## Architecture Onboarding

- Component map: Game Interaction Module -> Trajectory Buffer -> Monte Carlo Estimator -> Data Filter -> BC Trainer -> KTO Optimizer
- Critical path: Self-play data collection → Monte Carlo estimation → Threshold filtering → BC (5 epochs) → KTO (5 epochs)
- Design tradeoffs: Temperature 0.7 balances action diversity vs valid-game rate; threshold δ = 0.5 is a neutral prior; data balancing across games prevents overfitting
- Failure signatures: Win rate plateaus early (check action distribution balance); KTO increases loss without improving win rate (verify BC adaptation); MCTS opponent remains unbeatable (consider mixing in strong-opponent imitation)
- First 3 experiments: 1) Baseline sanity check: Run self-play with base model on single game for 1000 episodes; 2) Ablation on reward threshold: Compare δ ∈ {0.4, 0.5, 0.6}; 3) Opponent comparison: Train against Random, Self-Play, and MCTS(100); report action-count distribution and final win rate

## Open Questions the Paper Calls Out

- **How can SCO-PAL be adapted to function effectively in open-ended or partially observable environments, such as negotiation or web-based tasks?**
  - Basis: Explicitly stated as a limitation in the study
  - Why unresolved: Current methodology relies on well-defined MDPs with clear terminal states
  - Evidence needed: Successful application on negotiation or web navigation benchmarks

- **Does incorporating human or style-diverse LLM opponents during training improve the robustness or strategic diversity of agents trained with SCO-PAL?**
  - Basis: Limitations section notes restricted opponent pool
  - Why unresolved: Effect of human-like unpredictability on play-and-learn dynamic remains untested
  - Evidence needed: Experiments comparing current vs diverse opponent pools

- **How can the iterative self-play process be stabilized to prevent the performance decline observed in later training iterations?**
  - Basis: Appendix A.3 reports win rate decline in third iteration
  - Why unresolved: Paper identifies phenomenon but doesn't propose mitigation mechanisms
  - Evidence needed: Modified training loop maintaining win rates through 3rd-5th iterations

- **How can the learning framework bridge large skill gaps to effectively leverage strong opponents for imitation?**
  - Basis: Appendix A.1 observes performance gap widens with stronger opponents
  - Why unresolved: Specific mechanisms for aligning weak learner with strong teacher remain open
  - Evidence needed: Curriculum learning strategy enabling weak agent to imitate significantly stronger opponent

## Limitations
- Self-play advantage is empirically demonstrated but not analytically proven for games with high first-player advantage
- Monte Carlo estimation assumes sufficient repetition of state-action pairs, which may yield high-variance estimates in large state spaces
- BC threshold δ=0.5 is heuristic and may filter critical learning signals in game-specific contexts
- GPT-4 results show final model performance only, without learning curves or ablation studies during training

## Confidence
- **High Confidence**: Self-play generates more balanced action distributions than fixed opponents; step-level Monte Carlo estimation improves over trajectory-level rewards; two-stage BC→KTO outperforms joint training
- **Medium Confidence**: Self-play is "most effective" for strategy refinement—true within tested games but not proven across broader game classes
- **Low Confidence**: SCO-PAL's ~30% win-rate improvement and 54.76% vs GPT-4 generalize beyond the specific six games tested

## Next Checks
1. Run ablation studies varying δ threshold (0.4, 0.5, 0.6) and temperature (0.5, 0.7, 0.9) to quantify robustness of action distribution balance and win rate stability
2. Test self-play vs mixed training (50% self-play + 50% strong opponent) on asymmetric games like Liar's Dice to see if pure self-play still dominates
3. Perform state-space coverage analysis: measure Monte Carlo estimate variance for frequently vs rarely repeated (s,a) pairs to quantify estimation error and identify games where step-level rewards may break down