---
ver: rpa2
title: Gistify! Codebase-Level Understanding via Runtime Execution
arxiv_id: '2510.26790'
source_url: https://arxiv.org/abs/2510.26790
tags:
- execution
- test
- file
- code
- codebase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Gistify, a task requiring coding language
  models to generate a minimal, self-contained file that reproduces the runtime behavior
  of a given command on a codebase. The gistified file must be self-contained, exhibit
  execution fidelity, maintain minimalism, and preserve original code without hallucination.
---

# Gistify! Codebase-Level Understanding via Runtime Execution

## Quick Facts
- **arXiv ID**: 2510.26790
- **Source URL**: https://arxiv.org/abs/2510.26790
- **Reference count**: 38
- **Primary result**: State-of-the-art models struggle with Gistify, with Claude-4 achieving highest execution fidelity (58.7%) while GPT-5 produces most concise outputs

## Executive Summary
Gistify introduces a novel task requiring coding language models to generate a minimal, self-contained Python file that reproduces the runtime behavior of a given command on a codebase. The task demands execution fidelity, minimalism, and preservation of original code without hallucination. Experiments across multiple frameworks and models reveal that even advanced systems struggle, particularly with complex execution traces. The evaluation metrics include execution fidelity (whether the gistified file produces identical output), line execution rate (fraction of executable lines run), and line existence rate (preservation of original code blocks). Agentic models outperform static approaches, and accurate preservation of the test function strongly correlates with task success.

## Method Summary
The method involves providing an LLM agent with a target repository and specific command (e.g., pytest test case) and requiring it to generate a single file `concise.py` that reproduces the command's runtime behavior. The agent can use tools like View, Search, and Edit, with optional tracing capabilities. Evaluation involves running the original test command against the generated file to compute Execution Fidelity, Line Execution Rate, and Line Existence Rate. The process includes parsing both original and generated files into AST blocks for hierarchical matching, with normalization for multiline statements and import statements.

## Key Results
- Claude-4 achieves highest Execution Fidelity at 58.7%, while GPT-5 produces most concise outputs
- Enabling execution tools provides small performance gains, but current SOTA models fail to effectively leverage them
- Agentic models outperform static approaches, especially for tests with longer execution traces and higher coverage
- Accurate preservation of the test function strongly correlates with task success (correlation coefficient 0.76)

## Why This Works (Mechanism)

### Mechanism 1: Trace-Guided Code Selection
The model uses execution traces to identify which specific lines, functions, or classes are touched during a command's run. By following this dynamic map, the model filters out dead code and focuses context on the active execution path, reducing search space and inference complexity. Evidence shows that providing a "Tracing" tool boosts Execution Fidelity from 42.0% to 56.0%, outperforming other tools.

### Mechanism 2: Entrypoint Anchoring
The fidelity of the generated file is causally dependent on the model's ability to first locate and faithfully preserve the entrypoint (test function) code. High overlap between original and generated test serves as proxy for model successfully initializing dependency resolution chain. Ablation studies show that providing the correct test function in prompt improved Execution Fidelity from 42.0% to 60.0%.

### Mechanism 3: Dynamic Context Accumulation (Agentic)
Agentic models that iteratively select and view files outperform static models that receive all relevant files at once. They use "navigate-then-read" strategy, querying codebase to build context window containing only relevant dependencies. Static models receive all gold files simultaneously, leading to "context dilution" where relevant signals are overwhelmed by volume of text.

## Foundational Learning

- **Runtime Execution Tracing**: Understanding that `function A` calls `function B` (dynamic) is different from seeing them defined in a file (static). *Quick check*: If a codebase has 10 functions but a command only executes 2, which ones must appear in the gistified file to satisfy "Minimality"?

- **Context Dilution / Lost-in-the-Middle**: Providing "all relevant files" performs worse than finding them one by one. *Quick check*: Why might giving a model 50 files of correct code produce a worse result than giving it 5 files of correct code?

- **Code Hallucination vs. Grounding**: The "Grounded Preservation" requirement forbids the model from inventing code. *Quick check*: Is it acceptable for the model to rewrite a loop to be more efficient if the original loop works perfectly? (Answer: No, violates Grounded Preservation).

## Architecture Onboarding

- **Component map**: Execution Environment -> Agent System -> Evaluator
- **Critical path**: Receive repository + specific command -> Locate Anchor (test function) -> Trace Dependencies -> Assemble concise.py -> Verify (internal/external)
- **Design tradeoffs**: Tool Access risks "Max Steps Reached" errors; Minimality vs Fidelity requires balancing pruning with maintaining imports; Agentic is slower but handles context better than Static
- **Failure signatures**: Import Error (imports from original repo), Missing Test Function (test not copied correctly), Pytest Runtime Error (file runs but fails assertions)
- **First 3 experiments**: Baseline Fidelity on small repository using static model; Trace Ablation providing "Gold Trace" to measure delta in Execution Fidelity; Hard Subset Validation comparing Agentic vs Static on "High Coverage" tests

## Open Questions the Paper Calls Out
1. Does using gistified files as context improve performance on downstream tasks like debugging and refactoring compared to full codebase access?
2. Why do current SOTA models fail to effectively leverage execution tools and runtime tracing to improve Gistify performance?
3. Does the Gistify task and observed difficulty scaling with execution trace complexity generalize to non-Python programming languages?

## Limitations
- Proprietary nature of Claude-Sonnet-4 and GPT-5 limits reproducibility and exact benchmarking comparison
- Specific implementation details of line execution tracing and block matching normalization are not fully specified
- Dataset filtering criteria (particularly for parameterized tests) may introduce subtle biases not fully documented

## Confidence
- **High Confidence**: Execution Fidelity metric design, correlation between test function preservation and task success, agentic vs static model comparison results
- **Medium Confidence**: Mechanism 1 (trace-guided selection) strength, due to dependence on unspecified tracing implementation details
- **Low Confidence**: Generalizability of results beyond the specific repositories tested, given the limited domain coverage

## Next Checks
1. **Parameter Generalization Test**: Generate gistified files for both parameterized and non-parameterized versions of same test to verify models capture general logic rather than specific parameter values
2. **Cross-Repository Replication**: Apply same methodology to repositories outside original six (e.g., Django, pandas) to test domain generalizability
3. **Trace Implementation Verification**: Implement line execution tracing mechanism independently and compare results against reported Execution Fidelity scores to validate measurement consistency