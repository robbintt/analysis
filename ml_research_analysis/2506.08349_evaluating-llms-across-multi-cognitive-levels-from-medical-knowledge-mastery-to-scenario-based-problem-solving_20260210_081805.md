---
ver: rpa2
title: 'Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery
  to Scenario-Based Problem Solving'
arxiv_id: '2506.08349'
source_url: https://arxiv.org/abs/2506.08349
tags:
- medical
- llms
- tasks
- levels
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a multi-cognitive-level evaluation framework
  (MultiCogEval) for assessing large language models'' (LLMs) medical capabilities
  across three cognitive levels: preliminary knowledge grasp, comprehensive knowledge
  application, and scenario-based problem solving. Built on existing medical datasets
  (MedQA, MedMCQA, MIMIC-IV), the framework constructs tasks targeting each cognitive
  level and normalizes performance metrics for comparability.'
---

# Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving

## Quick Facts
- arXiv ID: 2506.08349
- Source URL: https://arxiv.org/abs/2506.08349
- Reference count: 25
- Multi-cognitive-level evaluation framework reveals significant performance declines as medical reasoning complexity increases

## Executive Summary
This study introduces MultiCogEval, a comprehensive framework for assessing large language models' medical capabilities across three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving. The framework systematically evaluates 32 state-of-the-art LLMs (2B-70B parameters) across medical datasets, revealing that while models achieve >60% accuracy on basic knowledge recall, performance drops to 19.4% on complex clinical diagnosis tasks. The research demonstrates that model size significantly impacts higher-level reasoning performance, with 70B models outperforming 7B models by 2-3x on complex tasks. Medical fine-tuning improves lower-level performance but fails to enhance high-level clinical reasoning capabilities.

## Method Summary
The MultiCogEval framework constructs tasks targeting three cognitive levels using existing medical datasets (MedQA, MedMCQA, MIMIC-IV). The evaluation systematically normalizes performance metrics across different task formats for comparability. The study evaluates 32 LLMs from six families, ranging from 2B to 70B parameters, including both general and medical-domain fine-tuned models. The framework assesses performance across knowledge recall, clinical reasoning, and full-path clinical diagnosis tasks, with additional analysis of inference-time scaling effects.

## Key Results
- Performance degrades significantly across cognitive levels: >60% accuracy on knowledge recall tasks versus 19.4% on full-path clinical diagnosis tasks
- Model size critically affects complex reasoning: 70B models outperform 7B models by 2-3x on high-complexity tasks
- Medical fine-tuning improves low/mid-level performance by up to 15% but fails to significantly enhance high-level clinical reasoning capabilities

## Why This Works (Mechanism)
The framework's effectiveness stems from its structured approach to isolating different cognitive demands in medical reasoning. By constructing tasks that progressively increase in complexity while controlling for domain knowledge, the evaluation can distinguish between models' knowledge retrieval capabilities and their reasoning abilities. The normalization of metrics across diverse task formats enables fair comparison across cognitive levels, revealing genuine performance degradation rather than artifacts from different evaluation methodologies.

## Foundational Learning

**Medical Knowledge Retrieval**: Understanding of medical concepts and terminology is essential for all cognitive levels. Quick check: Can the model accurately answer basic medical knowledge questions?

**Clinical Reasoning**: Ability to apply medical knowledge to patient scenarios requires multi-step inference. Quick check: Can the model diagnose conditions from patient symptoms with supporting reasoning?

**Information Integration**: Complex medical problem-solving requires synthesizing information from multiple sources. Quick check: Can the model request and incorporate additional information during diagnosis?

## Architecture Onboarding

**Component Map**: MultiCogEval Framework -> Task Construction -> Model Evaluation -> Performance Analysis
Critical Path: Task Construction (defines cognitive levels) → Model Evaluation (across 32 LLMs) → Performance Analysis (across cognitive levels)

**Design Tradeoffs**: Using existing medical datasets ensures coverage but may limit control over cognitive complexity factors. The framework prioritizes comprehensive evaluation over perfect task alignment with clinical reality.

**Failure Signatures**: Performance degradation patterns indicate limitations in multi-step reasoning and information acquisition capabilities rather than pure knowledge gaps. Models show competence in isolated knowledge recall but struggle with integrated clinical reasoning.

**First Experiments**:
1. Baseline evaluation of general LLMs across all three cognitive levels
2. Comparative analysis of medical fine-tuned versus general LLMs on low-complexity tasks
3. Scaling analysis of model performance improvements with parameter size on high-complexity tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Performance degradation may reflect task design artifacts rather than true cognitive complexity differences
- Evaluation relies on existing medical datasets that may not fully represent clinical reasoning requirements
- Focus on English-language medical knowledge limits generalizability to other healthcare contexts

## Confidence

**High Confidence**: Performance degradation patterns across cognitive levels, model size effects on complex tasks, and general trends in medical domain fine-tuning effectiveness

**Medium Confidence**: Specific accuracy percentages and relative performance differences between model families, as these may be influenced by dataset-specific factors

**Low Confidence**: Interpretation that medical fine-tuning fails to enhance high-level reasoning, as this conclusion may be confounded by available medical datasets and task designs

## Next Checks
1. Replicate evaluation using alternative medical datasets with different characteristics to verify cognitive-level performance patterns
2. Conduct qualitative analysis of model responses on high-complexity tasks to distinguish between true reasoning failures and dataset-specific challenges
3. Test framework with task modifications that control for potential confounding factors such as question complexity, domain specificity, and information availability