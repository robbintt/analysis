---
ver: rpa2
title: 'Be Your Own Red Teamer: Safety Alignment via Self-Play and Reflective Experience
  Replay'
arxiv_id: '2601.10589'
source_url: https://arxiv.org/abs/2601.10589
tags:
- arxiv
- safety
- attack
- defense
- jailbreak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of LLMs being vulnerable to jailbreak
  attacks by proposing a self-play system where a single model acts as both attacker
  and defender in a unified RL loop. An advanced reflective experience replay mechanism
  with UCB sampling revisits past failures, enabling the model to learn from hard
  cases while balancing exploration and exploitation.
---

# Be Your Own Red Teamer: Safety Alignment via Self-Play and Reflective Experience Replay

## Quick Facts
- arXiv ID: 2601.10589
- Source URL: https://arxiv.org/abs/2601.10589
- Reference count: 37
- Primary result: Self-play system reduces attack success rates while preserving model capabilities

## Executive Summary
This paper proposes a novel approach to LLM safety alignment using self-play, where a single model acts as both attacker and defender. The system employs a unified reinforcement learning loop combined with reflective experience replay to iteratively improve safety without external red-teaming data. By learning from past failures through UCB-based sampling, the model develops robust defenses against jailbreak attempts while maintaining task performance.

## Method Summary
The approach uses a single LLM that alternates between attacker and defender roles within a unified RL framework. The model generates adversarial prompts (attacker) and evaluates/refuses harmful content (defender). A reflective experience replay mechanism with UCB sampling prioritizes revisiting challenging cases from past interactions, enabling the model to learn from failures. This self-contained training loop eliminates the need for external safety datasets while maintaining capability preservation through balanced exploration-exploitation dynamics.

## Key Results
- Significantly lower attack success rates compared to static baseline defenses
- Maintains model capabilities without over-refusal issues
- Eliminates need for external red-teaming datasets or human safety annotators

## Why This Works (Mechanism)
The self-play mechanism works by creating a competitive dynamic where the model must anticipate and defend against attacks it generates itself. This internal adversarial training forces the model to develop robust safety boundaries. The reflective experience replay with UCB sampling ensures that particularly challenging cases are revisited, preventing the model from overfitting to easy examples while maintaining a balance between exploring new attack patterns and exploiting known weaknesses.

## Foundational Learning
- Reinforcement Learning for Safety: Required for understanding how rewards shape defensive behaviors; quick check: verify reward shaping prevents trivial solutions
- Adversarial Prompt Generation: Needed to understand how models can systematically explore attack spaces; quick check: assess diversity of generated attack patterns
- Experience Replay Mechanisms: Essential for understanding how past failures inform future defenses; quick check: evaluate buffer diversity and sampling effectiveness
- Upper Confidence Bound (UCB) Sampling: Critical for balancing exploration-exploitation in safety training; quick check: measure convergence stability with different exploration parameters
- Self-Play Dynamics: Fundamental to understanding competitive training without external agents; quick check: analyze equilibrium behavior between attacker/defender roles
- Safety-Capability Tradeoffs: Important for evaluating whether defensive improvements compromise task performance; quick check: compare benchmark performance pre/post-training

## Architecture Onboarding

Component Map:
Self-Play Agent -> Unified RL Loop -> Reflective Experience Replay Buffer -> UCB Sampler -> Attack/Defense Evaluator -> Reward Signal

Critical Path:
1. Model generates attack prompt (attacker role)
2. Same model evaluates/refuses (defender role)
3. Reward computed based on attack success/refusal correctness
4. Experience stored in replay buffer
5. UCB sampling selects challenging experiences
6. Model updated through RL optimization

Design Tradeoffs:
The unified RL approach trades diversity of external red-teaming data for scalability and self-containment. While this reduces attack vector variety compared to human red-teamers, it enables continuous, automated safety improvement without costly external annotations.

Failure Signatures:
- Over-refusal indicated by high rejection of benign prompts
- Catastrophic forgetting shown by rising attack success on previously learned defenses
- Mode collapse evidenced by repetitive, predictable attack patterns
- Reward hacking demonstrated by defensive strategies that avoid actual safety assessment

First Experiments:
1. Baseline comparison: Static safety model vs. self-play defense on standardized attack benchmark
2. Capability preservation test: Pre/post capability evaluation on standard LLM benchmarks
3. Exploration-exploitation analysis: Attack success rates under different UCB sampling parameters

## Open Questions the Paper Calls Out
None specified in provided content.

## Limitations
- Limited diversity of attack vectors compared to external red-teaming approaches
- Potential for catastrophic forgetting as replay buffer evolves
- May not capture zero-day exploits outside training distribution
- Evaluation focuses on specific model architecture without extensive cross-model validation

## Confidence

High confidence: Technical implementation of unified RL loop and basic effectiveness of self-play for safety alignment, demonstrated by reduced attack success rates.

Medium confidence: Claim that capabilities are preserved without over-refusal, given that capability preservation was measured through standard benchmarks which may not fully capture nuanced task performance.

Low confidence: Generalizability across different model architectures and sizes, as experiments primarily focus on specific LLM configuration without extensive ablation studies.

## Next Checks
1. Systematic comparison between self-play defense and ensemble-based red-teaming approaches using the same attack success rate metrics to quantify diversity gap.
2. Long-term stability test monitoring refusal rates and attack success rates over extended training periods to assess potential catastrophic forgetting.
3. Testing approach on diverse model architectures (different sizes and pre-training paradigms) to validate scalability across LLM families.