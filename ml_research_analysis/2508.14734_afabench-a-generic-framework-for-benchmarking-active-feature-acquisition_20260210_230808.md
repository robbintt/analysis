---
ver: rpa2
title: 'AFABench: A Generic Framework for Benchmarking Active Feature Acquisition'
arxiv_id: '2508.14734'
source_url: https://arxiv.org/abs/2508.14734
tags:
- features
- feature
- methods
- budget
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AFABench, the first standardized benchmark
  for Active Feature Acquisition (AFA). AFA addresses the problem of dynamically selecting
  informative features under acquisition cost constraints, balancing predictive performance
  with feature acquisition budgets.
---

# AFABench: A Generic Framework for Benchmarking Active Feature Acquisition

## Quick Facts
- arXiv ID: 2508.14734
- Source URL: https://arxiv.org/abs/2508.14734
- Reference count: 40
- Introduces first standardized benchmark for Active Feature Acquisition (AFA) methods

## Executive Summary
AFABench provides the first standardized benchmark for evaluating Active Feature Acquisition (AFA) algorithms, addressing the critical problem of dynamically selecting informative features under acquisition cost constraints. The framework supports diverse synthetic and real-world datasets, multiple acquisition policies, and allows easy extensibility for new methods. Through comprehensive evaluation of representative algorithms across greedy, non-greedy, and static feature selection paradigms, AFABench reveals important insights about algorithm performance and dataset characteristics.

## Method Summary
AFABench implements a modular framework that standardizes the evaluation of AFA methods across diverse datasets and acquisition policies. The benchmark includes synthetic datasets like AFAContext (designed to expose greedy strategy limitations) and 10 real-world datasets from OpenML. It evaluates multiple algorithm families: greedy methods (generative and discriminative), non-greedy approaches (model-free and model-based reinforcement learning), and static feature selection. The framework provides consistent evaluation metrics comparing predictive performance against feature acquisition costs, enabling fair comparison across different algorithmic approaches.

## Key Results
- Discriminative greedy methods (GDFS-DG, DIME-DG) consistently achieve top performance across most datasets
- RL-based methods excel on synthetic AFAContext dataset but struggle on real-world datasets
- Many real-world datasets lack strong non-myopic structure, making greedy methods practical due to computational efficiency
- Static feature selection often performs competitively despite not adapting to context

## Why This Works (Mechanism)
AFABench works by creating a standardized evaluation environment that isolates the core challenge of AFA: balancing predictive accuracy with feature acquisition costs. The modular design allows systematic comparison of different algorithmic approaches while controlling for dataset characteristics. By including both synthetic datasets designed to expose specific limitations and real-world datasets representing practical scenarios, the benchmark reveals fundamental trade-offs between computational complexity and performance gains.

## Foundational Learning

**Active Feature Acquisition**: The problem of selecting which features to acquire from incomplete data to maximize predictive performance under cost constraints. Needed to formalize the research problem; check by verifying understanding of the acquisition cost-performance trade-off.

**Greedy vs Non-greedy Strategies**: Greedy methods make locally optimal choices at each step, while non-greedy methods consider long-term consequences. Needed to understand algorithm families; check by explaining why greedy methods might fail on AFAContext.

**Discriminative vs Generative Models**: Discriminative models directly model the decision boundary, while generative models model the data distribution. Needed to understand algorithm design choices; check by comparing how these approaches differ in feature acquisition.

**Reinforcement Learning in AFA**: Using RL to learn optimal feature acquisition policies through interaction with the environment. Needed to understand non-myopic approaches; check by explaining why RL struggles on real-world datasets.

## Architecture Onboarding

**Component Map**: Data Generator -> Feature Acquisition Policy -> Model Trainer -> Performance Evaluator -> Cost Calculator

**Critical Path**: The evaluation pipeline processes datasets through acquisition policies, trains models on acquired features, and measures performance-cost trade-offs. This sequence enables systematic comparison across algorithms.

**Design Tradeoffs**: The framework prioritizes modularity and extensibility over computational optimization, allowing new methods to be easily integrated. This design choice favors research exploration over production deployment.

**Failure Signatures**: Poor performance on AFAContext indicates failure to capture non-myopic dependencies. Consistently low performance across real-world datasets suggests algorithmic limitations or inappropriate cost modeling.

**First Experiments**: 1) Run greedy methods on AFAContext to observe failure modes, 2) Compare discriminative vs generative greedy approaches on real-world datasets, 3) Test RL methods on datasets with known non-myopic structure.

## Open Questions the Paper Calls Out
None

## Limitations
- Focuses primarily on tabular data, potentially missing complexities of spatial, temporal, or multimodal feature acquisition
- Relatively small number of real-world datasets (10) limits generalizability of conclusions
- Single-feature acquisition per timestep excludes batch acquisition scenarios common in practice
- Synthetic datasets may oversimplify non-myopic dependencies present in real applications

## Confidence

**High confidence**: Discriminative greedy methods consistently performing well across datasets is well-supported by experimental results.

**Medium confidence**: The claim that real-world datasets lack strong non-myopic structure is reasonable but requires validation on more diverse datasets.

**Medium confidence**: RL methods excelling on AFAContext but struggling on real-world datasets is supported, though deeper analysis of performance gaps is needed.

## Next Checks

1. Evaluate AFABench methods on non-tabular data (images, time series, or multimodal datasets) to assess generalizability beyond current scope.

2. Conduct ablation studies varying acquisition cost structures to determine algorithm sensitivity to different cost models.

3. Test the benchmark framework with batch acquisition scenarios to validate whether single-feature acquisition results translate to practical settings.