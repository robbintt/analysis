---
ver: rpa2
title: 'ASR Under Noise: Exploring Robustness for Sundanese and Javanese'
arxiv_id: '2509.25878'
source_url: https://arxiv.org/abs/2509.25878
tags:
- clean
- noise
- sundanese
- javanese
- noisetrain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the robustness of Whisper-based ASR models
  for Javanese and Sundanese under noisy conditions. While clean audio performance
  is strong, WER degrades by 2-3x in low-SNR environments without noise-aware training.
---

# ASR Under Noise: Exploring Robustness for Sundanese and Javanese

## Quick Facts
- **arXiv ID**: 2509.25878
- **Source URL**: https://arxiv.org/abs/2509.25878
- **Reference count**: 40
- **Primary result**: Noise-aware training improves Whisper ASR robustness for Javanese and Sundanese by 2-3x in low-SNR conditions.

## Executive Summary
This work investigates the robustness of Whisper-based ASR models for Javanese and Sundanese under noisy conditions. While clean audio performance is strong, WER degrades by 2-3x in low-SNR environments without noise-aware training. Both SpecAugment and synthetic noise training improve robustness, with NoiseTrain consistently outperforming other methods across models and languages. Error analysis reveals language-specific challenges: Sundanese struggles with vowel confusion and name errors, while Javanese has more digraph and consonant issues. Overall, noise-aware fine-tuning is essential for real-world ASR performance in these languages.

## Method Summary
The study fine-tunes Whisper models (Tiny, Medium, Large-v3, Large-v3-Turbo) on Javanese and Sundanese speech from OpenSLR using three training strategies: clean baseline, SpecAugment (config #9), and NoiseTrain (synthetic noise injection from AudioSet at varied SNRs). Models are evaluated on clean and synthetically noisy test sets across SNR levels from -20 to 20 dB. The primary metric is WER, with CER used for detailed error analysis.

## Key Results
- NoiseTrain reduces WER degradation by 2-3x in low-SNR conditions compared to clean training
- Larger Whisper models (Large-v3) show superior noise robustness than smaller variants
- SpecAugment provides consistent but smaller improvements than NoiseTrain across all SNR levels
- Language-specific error patterns emerge: Sundanese has vowel confusion issues, Javanese struggles with digraphs and diacritics

## Why This Works (Mechanism)

### Mechanism 1: Noise-Aware Fine-Tuning via Synthetic Noise Injection
Training Whisper models on audio augmented with synthetic background noise improves robustness to low-SNR environments by learning representations invariant to specific noise types and levels used in augmentation. This exposure during fine-tuning allows the model to learn acoustic mappings more resilient to masking and distortion effects of noise during inference.

### Mechanism 2: Regularization via SpecAugment for Generalization
SpecAugment operates on input spectrograms by occluding contiguous blocks of time and frequency bands, forcing the model to rely on partial information and interpolate missing features. This regularization prevents overfitting to precise spectral details of clean training data and encourages learning more general acoustic patterns less sensitive to minor spectral distortions caused by noise.

### Mechanism 3: Transfer Learning from a Large-Scale Pre-trained Foundation
Fine-tuning pre-trained Whisper models, especially larger variants, yields significantly better performance and robustness than training from scratch due to their learned multi-lingual acoustic knowledge. Whisper's massive pre-training on diverse audio data establishes strong foundational representations that adapt well to specific acoustic and linguistic patterns of Javanese and Sundanese.

## Foundational Learning

**Concept: Signal-to-Noise Ratio (SNR)**
- Why needed here: The entire paper evaluates robustness based on performance changes across different SNR levels
- Quick check question: If the SNR of an audio clip is lowered by adding more noise, does the WER typically increase or decrease?

**Concept: Word Error Rate (WER) vs. Character Error Rate (CER)**
- Why needed here: WER is the primary evaluation metric; CER is used for fine-grained error analysis critical for understanding language-specific challenges
- Quick check question: In an agglutinative language, why might a single-character error cause a much larger increase in WER than CER?

**Concept: Data Augmentation for Regularization**
- Why needed here: The paper's core strategies (SpecAugment, NoiseTrain) are forms of data augmentation; understanding the goal of regularization is key to understanding why these methods work
- Quick check question: What is the primary risk of training a model only on pristine, clean data when deploying it to a noisy environment?

## Architecture Onboarding

**Component map**: Pre-trained Encoder-Decoder -> Noise Injection Module (optional) -> SpecAugment Module (optional) -> Fine-Tuning Loop

**Critical path**:
1. Data Preparation: Acquire OpenSLR and AudioSet; generate noisy training sets at target SNRs or configure SpecAugment
2. Model Initialization: Load pre-trained Whisper checkpoints (Tiny, Medium, Large-v3, Large-v3-Turbo)
3. Training: Run fine-tuning experiments for each configuration (Clean, SpecAugment, NoiseTrain)
4. Evaluation: Generate noisy test sets using held-out noise classes; compute WER/CER on clean and noisy test data

**Design tradeoffs**:
- SpecAugment vs. NoiseTrain: SpecAugment is computationally cheaper but provides less robustness in extreme noise; NoiseTrain is more effective but requires noise dataset and increases data pipeline complexity
- Model Size vs. Inference Speed/Cost: Larger models (Large-v3) outperform smaller ones but require significantly more compute for training and inference
- Synthetic vs. Real Noise: Synthetic noise offers controlled experimentation but may not cover all real-world acoustic variations

**Failure signatures**:
- High WER on Clean Audio: Indicates poor fine-tuning convergence or insufficient model capacity
- Catastrophic WER Degradation at Low SNR (>150%): Hallmark of models trained without noise-aware strategies
- Language-Specific Error Patterns: High diacritics mistakes indicate poor handling of Javanese orthography; high vowel confusion points to Sundanese phonological challenges

**First 3 experiments**:
1. Establish a Clean Baseline: Fine-tune Whisper Medium on clean Javanese/Sundanese training sets; evaluate on clean test set
2. Evaluate Noise Robustness Gap: Take baseline model and evaluate on synthetically noisy test set (SNR 0dB, 10dB) to quantify performance drop
3. Compare SpecAugment vs. NoiseTrain at Single SNR: Fine-tune two new models (SpecAugment and NoiseTrain at SNR 5dB); compare WER on noisy test set

## Open Questions the Paper Calls Out
- **Synthetic Noise Generalization**: Synthetic conditions cannot fully capture real-world environments such as conversational overlap or varied recording devices
- **Dialectal Coverage**: Dataset may not reflect spontaneous or dialectal variation; future work includes dialect-aware fine-tuning
- **Speech Enhancement Integration**: Speech enhancement is proposed as future work to improve real-world robustness beyond noise-aware training alone

## Limitations
- Synthetic noise may not fully capture real-world acoustic complexity and domain-specific noise conditions
- Zero-shot performance on these languages is poor, indicating fundamental limitations in pre-training data coverage
- No empirical validation against truly diverse real-world noisy conditions beyond synthetic mixtures

## Confidence

**High Confidence (9/10)**: Comparative results showing NoiseTrain outperforming both clean training and SpecAugment across all model sizes and SNR conditions; error analysis findings are well-supported and linguistically plausible

**Medium Confidence (7/10)**: Claim that larger Whisper models show superior robustness to noise; could be influenced by model capacity effects rather than noise-specific adaptation

**Low Confidence (4/10)**: Assumption that synthetic noise augmentation will generalize to all real-world deployment scenarios; paper acknowledges this limitation without empirical validation

## Next Checks

1. **Real-World Noise Validation**: Evaluate best-performing NoiseTrain models on audio recordings from actual noisy environments (cafes, streets, public transport) rather than synthetic mixtures

2. **Hyperparameter Ablation Study**: Systematically vary SpecAugment masking probabilities and SNR levels used in NoiseTrain across multiple runs to quantify sensitivity

3. **Cross-Lingual Transfer Analysis**: Apply noise-aware fine-tuning procedure to another low-resource language (e.g., Balinese or Madurese) without architectural changes to test transfer across linguistically related languages