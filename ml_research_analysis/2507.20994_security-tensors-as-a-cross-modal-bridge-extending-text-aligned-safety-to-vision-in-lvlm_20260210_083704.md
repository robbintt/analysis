---
ver: rpa2
title: 'Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to
  Vision in LVLM'
arxiv_id: '2507.20994'
source_url: https://arxiv.org/abs/2507.20994
tags:
- safety
- visual
- security
- tensors
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Security tensors are trainable input perturbations applied to either
  textual or visual modalities of large visual-language models (LVLMs) to extend text-aligned
  safety mechanisms to visual inputs. The method uses a curated training dataset containing
  malicious image-text pairs (for safety activation), benign pairs with structurally
  similar text (to prevent text-pattern overfitting), and general benign samples (to
  preserve normal functionality).
---

# Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM

## Quick Facts
- arXiv ID: 2507.20994
- Source URL: https://arxiv.org/abs/2507.20994
- Reference count: 40
- Key outcome: Security tensors extend text-aligned safety to visual inputs in LVLMs via input perturbations without modifying model parameters

## Executive Summary
Security tensors are trainable input perturbations that extend text-aligned safety mechanisms from large language models to visual inputs in large visual-language models (LVLMs). The method applies universal perturbations to either visual or textual modalities, enabling the model to reject harmful content while maintaining benign performance. By leveraging existing textual safety layers through cross-modal activation, security tensors provide parameter-free safety enhancement that generalizes to unseen harmful categories.

## Method Summary
The method trains universal input perturbations (security tensors) applied to either preprocessed visual inputs or textual embeddings of LVLMs. Training uses a curated dataset with three components: harmful image-text pairs for safety activation, benign pairs with structurally similar text to prevent overfitting, and general benign samples for utility preservation. The tensors are optimized using a dual-loss objective combining safety activation and benign performance maintenance, treating the LVLM as a fixed black-box without parameter modification.

## Key Results
- Visual and textual security tensors significantly improve Harmless Rate (HR) on harmful inputs (86.43% HR on seen-category harmful images for LLaMA-3.2-11B-Vision with δv)
- Near-identical performance on benign tasks maintained (low false rejection rate, high MM-Vet scores)
- Security tensors generalize well to unseen harmful categories across tested LVLMs (LLaMA-3.2-11B-Vision, Qwen-VL-Chat, LLaVA-1.5)
- Internal analysis shows security tensors activate language module's safety layers when processing harmful visual inputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Security tensors function as a cross-modal bridge by re-activating the language module's pre-existing textual "safety layers" when processing harmful visual inputs.
- **Mechanism:** The tensors act as input perturbations that shift hidden representations, inducing significant divergence in "safety layers" specifically for malicious image-text pairs while benign pairs remain largely unaffected, aligning harmful visual inputs with the refusal semantic space.
- **Core assumption:** The safety circuits trained via text alignment are accessible and sufficient for rejecting multimodal threats if the visual input is mapped into the correct activation subspace.
- **Evidence anchors:** Abstract confirms tensor activation of textual safety layers; section 5.2 provides overlap evidence; related work confirms LVLMs inherit text-aligned LLM vulnerabilities.
- **Break condition:** If the LLM backbone lacks strong textual safety alignment, tensors cannot extend non-existent safety circuits.

### Mechanism 2
- **Claim:** Generalization to unseen malicious categories relies on contrastive supervision to suppress text-pattern overfitting.
- **Mechanism:** The Text Contrast Benign (TCB) dataset pairs benign images with text structurally similar to malicious queries, forcing the tensor to rely on visual features rather than superficial text triggers.
- **Core assumption:** The visual encoder can distinguish harmful from benign content when textual confounders are present.
- **Evidence anchors:** Section 3.3.1 describes TCB purpose; section 4.3 shows ablation results; related work supports need for strict contrastive sets.
- **Break condition:** If textual structure between SA and TCB sets isn't sufficiently similar, tensor learns trivial text-classifier rule.

### Mechanism 3
- **Claim:** Safety is enforced via input-space optimization rather than weight modification, preserving utility through dual-loss objective.
- **Mechanism:** Tensors optimized using joint loss: Cross-Entropy on SA set for refusal, KL-Divergence on GB set to anchor outputs to original model distribution, treating LVLM as fixed.
- **Core assumption:** Single universal perturbation can generalize across diverse distribution of harmful visual concepts.
- **Evidence anchors:** Section 3.2 provides formulation; section 3.3.2 details loss function; corpus neighbors contrast with parameter-tuning approaches.
- **Break condition:** If perturbation magnitude unconstrained or benign dataset too small, tensor may cause semantic drift.

## Foundational Learning

- **Concept: Soft Prompt Tuning**
  - **Why needed here:** Textual security tensor δt is a "virtual token" prompt learned in embedding space, not vocabulary space.
  - **Quick check question:** How does optimizing a vector in embedding space differ from searching for discrete text tokens?

- **Concept: Universal Adversarial Perturbations**
  - **Why needed here:** Visual security tensor δv is applied universally to preprocessed image space regardless of input resolution or content.
  - **Quick check question:** Why is perturbation applied to preprocessed tensor v (e.g., 448×448) rather than raw image x?

- **Concept: Cross-Modal Alignment Gap**
  - **Why needed here:** Paper's premise is that visual modules trained after LLM break existing safety alignment.
  - **Quick check question:** Why would harmful image paired with benign text bypass text-only safety filter?

## Architecture Onboarding

- **Component map:** Raw Image x & Text t → Preprocessing ϕ → Tensor Injection (δv or δt) → Core LVLM (Frozen Vision Encoder + Frozen LLM)
- **Critical path:** Text Contrast Benign (TCB) dataset construction - if data is noisy or text isn't syntactically similar enough to malicious set, tensor will overfit to text triggers and fail to generalize.
- **Design tradeoffs:**
  - δv (Visual): Higher effectiveness on unseen categories in some models, but risks slightly higher False Rejection Rates due to altering visual distribution.
  - δt (Textual): Faster convergence (often <1 epoch for SA loss) and lower impact on benign performance (MM-Vet), but may have slightly lower peak HR on some benchmarks.
- **Failure signatures:**
  - High FRR on TCB-like inputs: Indicates model is ignoring image and rejecting based on text prompt structure alone (text overfitting).
  - Low HR on Unseen Categories: Indicates tensor learned specific visual artifacts rather than semantic harm.
- **First 3 experiments:**
  1. Sanity Check (TCB Ablation): Train δ with and without TCB set; plot False Rejection Rate on TCB-style test set to confirm visual reliance.
  2. Activation Analysis: Compute cosine similarity curves (N-N vs. N-M pairs) to verify δ shifts activation of layers 9-20 (safety layers).
  3. Capacity Test: Vary number of virtual tokens n for δt (e.g., 10, 100, 300) to observe trade-off between convergence speed and overfitting.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can text-pattern overfitting in security tensors be fully mitigated beyond current TCB set strategy?
  - **Basis in paper:** Appendix A.1.4 states high FRR on TCB test set compared to general benign test set "highlights need for additional strategies beyond TCB set to further mitigate text-pattern overfitting—a direction we leave for future work."
  - **Why unresolved:** While TCB set reduces over-rejection compared to baselines, security tensors still exhibit significantly higher rejection rates for benign queries sharing syntactic structures with harmful queries (e.g., 38.00 FRR vs 0.50 FRR on LLaMA-3.2), indicating tensors still rely partially on textual patterns.
  - **What evidence would resolve it:** Novel training objective or data augmentation technique that reduces FRR on textually similar benign queries to level statistically indistinguishable from general benign queries.

- **Open Question 2:** Can visual security tensors (δv) be optimized to preserve benign performance as effectively as textual security tensors (δt)?
  - **Basis in paper:** Section 4.2 notes while both tensors improve safety, visual tensor (δv) results in "slightly greater degradation in benign performance" (higher FRR and lower MM-Vet) compared to δt, attributed to δv directly perturbing image representation.
  - **Why unresolved:** Paper identifies trade-off—visual tensors are more intrusive to input distribution than embedding-space textual tensors—but doesn't propose method to close performance gap while maintaining visual modality's activation path.
  - **What evidence would resolve it:** Modified optimization constraint for δv achieving equivalent Harmless Rates to current δv while matching low False Rejection Rate of δt.

- **Open Question 3:** Are identified "Security Tensor Activation" (STA) layers consistent across diverse LVLM architectures with differing layer depths?
  - **Basis in paper:** Internal analysis in Section 5 focuses on LLaMA-3.2-11B-Vision as "representative LVLM," identifying STA layers roughly in 9-20 range; unverified if this activation mechanism localizes to same relative depths in Qwen-VL-Chat or LLaVA-1.5.
  - **Why unresolved:** Different LVLMs have different architectures and "safety layer" distributions; confirming security tensors consistently activate these specific intermediate layers across all models is necessary to generalize "cross-modal bridge" hypothesis.
  - **What evidence would resolve it:** Layer-wise cosine similarity analysis (N-N vs. N-M curves) applied to Qwen-VL-Chat and LLaVA-1.5, showing "gap" indicating safety activation peaks in analogous intermediate layers.

## Limitations

- Cross-modal activation mechanism remains unclear - while tensors activate safety layers, precise mechanism by which universal perturbation maps diverse visual threats into text-aligned safety subspace is unexplained.
- Generalization scope limited - TCB dataset construction assumes syntactic similarity sufficient, but doesn't validate whether TCB text truly shares semantic structures attackers might exploit.
- Single LVLM limitation - method demonstrated on three LVLMs but doesn't address whether security tensors transfer across different LLM backbones or must be retrained per model.

## Confidence

- **High confidence:** Core experimental results showing improved Harmless Rate on harmful inputs while maintaining benign performance, well-documented with multiple metrics across three different LVLMs.
- **Medium confidence:** Claim that security tensors function as "cross-modal bridge" supported by activation analysis but relies on correlational evidence rather than mechanistic proof of how visual features map to textual safety representations.
- **Medium confidence:** Assertion that parameter-free input perturbations preferable to weight tuning is logically sound but lacks systematic comparison of deployment overhead and long-term maintenance costs.

## Next Checks

1. **Mechanistic validation:** Conduct targeted ablation studies where individual safety layers (e.g., layers 9-20 in LLaMA-3.2) are frozen or removed to determine whether tensor effectiveness depends on specific circuit activation patterns rather than general model capacity.

2. **Adversarial robustness test:** Evaluate security tensor performance against adaptive attacks where adversaries modify prompts to bypass TCB-style syntactic constraints, measuring whether visual component genuinely provides complementary protection beyond text-only defenses.

3. **Cross-model transferability:** Train security tensors on one LVLM (e.g., LLaMA-3.2) and directly apply them to another (e.g., Qwen-VL-Chat) without retraining, quantifying performance degradation to assess whether universal perturbations truly generalize across different vision-language architectures.