---
ver: rpa2
title: Learning and Transferring Physical Models through Derivatives
arxiv_id: '2505.01391'
source_url: https://arxiv.org/abs/2505.01391
tags:
- derl
- learning
- outl
- pinn
- physical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Derivative Learning (DERL), a supervised
  approach for modeling physical systems by learning their partial derivatives. The
  key insight is that learning derivatives, along with initial and boundary conditions,
  is sufficient to fully capture a physical system's dynamics.
---

# Learning and Transferring Physical Models through Derivatives

## Quick Facts
- arXiv ID: 2505.01391
- Source URL: https://arxiv.org/abs/2505.01391
- Reference count: 40
- Primary result: DERL outperforms state-of-the-art methods on PDEs by learning partial derivatives, achieving 45% lower L2 error on Allen-Cahn while maintaining superior physical consistency.

## Executive Summary
This paper introduces Derivative Learning (DERL), a supervised approach for modeling physical systems by learning their partial derivatives. The key insight is that learning derivatives, along with initial and boundary conditions, is sufficient to fully capture a physical system's dynamics. The authors prove theoretical guarantees showing that minimizing DERL's loss leads to convergence to the true physical solution, even when using empirical derivatives calculated via finite differences. The method is evaluated on various ODEs and PDEs, including the Allen-Cahn equation, continuity equation, Navier-Stokes equations, damped pendulum, and Korteweg-de Vries equation.

## Method Summary
DERL learns physical systems by training neural networks to predict partial derivatives of the solution rather than the solution itself. The loss function combines derivative matching (comparing predicted and true derivatives), boundary conditions, and initial conditions. Theoretical guarantees show convergence to the true solution when minimizing this loss. The method is extended to enable knowledge transfer across physical models through a distillation protocol, where higher-order derivatives are distilled to incrementally build physical models, extending their working domains and parameter ranges. Experiments use MLPs with tanh activation, automatic differentiation for derivative computation, and BFGS or Adam optimizers.

## Key Results
- DERL achieves 45% lower L2 error on Allen-Cahn equation compared to state-of-the-art methods
- Maintains superior physical consistency as measured by PDE residuals across all tested equations
- Successfully extends working domains and parameter ranges through knowledge transfer with minimal forgetting
- Outperforms models trained on full domains from scratch when using incremental distillation

## Why This Works (Mechanism)
DERL works by directly learning the governing equations' structure through derivatives rather than approximating the solution function. This approach is more data-efficient because derivatives encode the underlying physics more compactly than full solutions. The method leverages automatic differentiation to compute exact derivatives during training, avoiding the approximation errors common in PINN approaches. By focusing on derivatives, DERL naturally satisfies the PDE structure and only needs to learn the specific solution instance given initial and boundary conditions.

## Foundational Learning
- **Partial derivatives:** Derivatives with respect to multiple variables; why needed: Physical laws are expressed as PDEs relating partial derivatives; quick check: Can compute ∂u/∂t and ∂²u/∂x² for sample functions
- **Sobolev spaces:** Function spaces with weak derivatives; why needed: Theoretical framework assumes functions have continuous derivatives; quick check: Understand L² norm and derivative requirements
- **Automatic differentiation:** Computing exact derivatives through computational graphs; why needed: Enables precise derivative computation during training without finite differences; quick check: Can use torch.func.jacrev and torch.func.hessian
- **Collocation methods:** Solving PDEs using randomly sampled points; why needed: DERL uses random collocation points instead of fixed grids; quick check: Understand how random sampling approximates integrals
- **Finite difference approximation:** Numerical estimation of derivatives; why needed: Used for empirical derivatives when analytical forms are unavailable; quick check: Can implement forward/backward/central differences
- **Loss function weighting:** Balancing multiple loss terms; why needed: Derivative, BC, and IC terms must be properly weighted for convergence; quick check: Can explain impact of λ_u, λ_B, λ_I ratios

## Architecture Onboarding

**Component Map**
MLP (4 layers, 50 units, tanh) -> Automatic differentiation (jacrev/hessian) -> Loss computation (derivative + BC + IC terms) -> Optimizer (BFGS/Adam)

**Critical Path**
1. Sample collocation points in domain
2. Compute analytical/empirical derivatives at points
3. Forward pass through MLP to predict solution
4. Use automatic differentiation to compute predicted derivatives
5. Calculate loss combining derivative matching, boundary, and initial conditions
6. Backpropagate and update weights

**Design Tradeoffs**
- **MLP vs. other architectures:** MLPs provide universal approximation but lack spatial awareness; simpler to implement than CNNs or transformers
- **Analytical vs. empirical derivatives:** Analytical gives exact derivatives but requires closed-form solutions; empirical enables learning from data but introduces approximation errors
- **Derivative order:** Higher-order derivatives improve accuracy but increase computational cost and training complexity

**Failure Signatures**
- **PINN-like bias:** Solution degrades with increasing time, failing to propagate information from initial conditions
- **Loss imbalance:** Poor convergence when derivative, BC, and IC loss terms differ by more than 10× in magnitude
- **Overfitting to collocation points:** High PDE residuals indicate failure to generalize beyond training points

**First Experiments**
1. Implement Allen-Cahn experiment with 1000 random collocation points, tracking L2 error and PDE residual
2. Compare DERL loss with and without boundary condition terms to measure their impact
3. Test different derivative orders (first vs. second) on the damped pendulum to observe accuracy trade-offs

## Open Questions the Paper Calls Out
**Open Question 1:** How does DERL-based continual learning perform over longer sequences of tasks (beyond 2–3 steps) and how can forgetting be further mitigated? The paper only demonstrates transfer for short sequences and notes physical models exhibit some degree of forgetting without applying existing continual learning strategies.

**Open Question 2:** Can DERL effectively handle PDEs with discontinuities, shocks, or singularities common in real-world fluid dynamics? All experiments use smooth analytical solutions with continuous derivatives; the theoretical framework assumes functions in Sobolev spaces with continuous derivatives.

**Open Question 3:** What is the optimal order of derivatives to distill for PDEs of different types, and how does this relate to the underlying PDE order? The paper empirically demonstrates benefits for KdV but provides no general guidance on matching derivative order to PDE order.

**Open Question 4:** How can physical knowledge from multiple pre-trained models (each specializing in different physical phenomena) be composed to solve coupled multi-physics problems? The current distillation protocol transfers knowledge between models solving the same PDE under different conditions, not between models solving fundamentally different physical equations.

## Limitations
- Loss weights (λ_u, λ_B, λ_I) are unspecified for most experiments beyond the KdV transfer case
- Finite difference approximation for empirical derivatives introduces discretization errors not thoroughly quantified
- Limited comparison to PINN baselines, which may be disadvantaged by known optimization pathologies

## Confidence
**High Confidence:** Theoretical foundation and empirical results showing DERL's superior generalization to unseen initial conditions and parameters
**Medium Confidence:** Knowledge transfer claims, limited by single transfer direction and absence of ablation studies
**Low Confidence:** Physical consistency claims across all PDE residuals, as selective reporting rather than comprehensive documentation

## Next Checks
1. **Hyperparameter Sensitivity:** Systematically vary loss weights (λ_u, λ_B, λ_I) for Allen-Cahn experiment to establish sensitivity profile
2. **Finite Difference Error Analysis:** Quantify discretization error from finite difference approximations by comparing against analytical derivatives on a grid
3. **Transfer Learning Scope:** Extend knowledge transfer evaluation beyond KdV to include bidirectional transfers and multi-step transfer chains