---
ver: rpa2
title: Enhancing Vision Transformer Explainability Using Artificial Astrocytes
arxiv_id: '2505.21513'
source_url: https://arxiv.org/abs/2505.21513
tags:
- astrocytes
- modulation
- grad-cam
- artificial
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a novel approach to improve the explainability
  of Vision Transformers (ViTs) by incorporating artificial astrocytes, inspired by
  neuroscience. The key idea is to add astrocytes to the first self-attention block
  of a pre-trained ViT, allowing for modulation of neuron activity without requiring
  additional training.
---

# Enhancing Vision Transformer Explainability Using Artificial Astrocytes

## Quick Facts
- **arXiv ID:** 2505.21513
- **Source URL:** https://arxiv.org/abs/2505.21513
- **Reference count:** 30
- **Primary result:** Incorporating artificial astrocytes into the first self-attention block of a pre-trained ViT significantly improves human alignment of model explanations across multiple XAI techniques and metrics.

## Executive Summary
This paper proposes a novel approach to enhance the explainability of Vision Transformers (ViTs) by incorporating artificial astrocytes inspired by neuroscience. The key innovation is adding astrocytes to the first self-attention block of a pre-trained ViT, allowing for modulation of neuron activity without requiring additional training. This modulation is implemented as an iterative process that enhances relevant visual information and suppresses irrelevant content. The proposed ViTA model is evaluated using two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared to a standard ViT on the ClickMe dataset. Results demonstrate that incorporating artificial astrocytes significantly improves the alignment of model explanations with human perception, as measured by Spearman correlation, Dice Similarity Coefficient (DSC), and Structural Similarity Index (SSIM).

## Method Summary
The authors propose to enhance ViT explainability by replacing the linear layer in the first self-attention block with an astrocytic linear layer. This layer implements activity-dependent modulation: it monitors presynaptic neuron activation levels across k iterations, applying excitatory modulation (multiplying weights by α ≥ 1) when activity exceeds threshold τ, and inhibitory modulation (multiplying weights by β < 1) when activity falls below -τ. The modulation factor accumulates multiplicatively across iterations, allowing gradual response building based on sustained activity patterns. The modulated output is then normalized and flows through residual connections to subsequent layers. The approach is evaluated on the ClickMe dataset using Grad-CAM and Grad-CAM++ to generate explanations, comparing ViTA against a standard ViT.

## Key Results
- Incorporating artificial astrocytes into ViT significantly improves alignment with human heatmaps across all metrics (Spearman, DSC, SSIM) and XAI techniques
- Best configurations show statistically significant improvements, with relative gains up to +66% in SSIM for Grad-CAM
- The approach achieves these improvements without requiring additional training of the ViT backbone
- Parameter sensitivity varies by metric and XAI technique, with optimal configurations found through grid search

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Activity-dependent astrocytic modulation enhances relevant visual features while suppressing irrelevant content, improving human alignment of explanations.
- **Mechanism:** Artificial astrocytes monitor presynaptic neuron activation levels across iterations. When a neuron's accumulated activity exceeds threshold τ, the astrocyte applies excitatory modulation (multiplies weights by α ≥ 1). When activity falls below -τ, it applies inhibitory modulation (multiplies weights by β < 1). This bidirectional gating amplifies strong signals and attenuates weak ones.
- **Core assumption:** The paper assumes that activity-based modulation in early attention layers will propagate through residual connections and bias downstream explanations toward human-relevant features.
- **Evidence anchors:**
  - [abstract] "This modulation is implemented as an iterative process that enhances relevant visual information and suppresses irrelevant content."
  - [Section 3.1.2, Equations 4-8] Formal definition of modulation factor M(t) with α/β conditions based on activity tracker Ai.
  - [corpus] Weak direct support; corpus papers address XAI broadly but do not validate astrocyte-based modulation specifically.
- **Break condition:** If the CLS token used for modulation decisions does not aggregate task-relevant information, or if k is too low to allow modulation to accumulate, the mechanism may fail to differentiate signal from noise.

### Mechanism 2
- **Claim:** Iterative processing in the astrocytic linear layer provides a computational proxy for the slower timescale of biological astrocyte-neuron interactions.
- **Mechanism:** Rather than modulating once, the input passes through the astrocytic layer k times (k ∈ {4, 6, 8} in experiments). The modulation factor M accumulates multiplicatively across iterations: Mii(t) = Mii(t-1) · mi(t). This allows the astrocyte's response to build gradually based on sustained activity patterns.
- **Core assumption:** Sustained activity patterns over iterations correlate with semantically meaningful features that align with human perception.
- **Evidence anchors:**
  - [Section 3.1] "Given the timescale differences between neurons and astrocytes, we implemented the astrocytic modulation as an iterative process."
  - [Section 4, Table 1] Best configurations show k varying by metric/technique (4-8), indicating iterative depth matters.
  - [corpus] No direct validation of the timescale analogy in AI systems.
- **Break condition:** If k is too high, modulation may over-amplify or over-suppress, destabilizing activations; if too low, the effect may be negligible.

### Mechanism 3
- **Claim:** Modulation applied only to the first self-attention block maximizes influence on subsequent layers via residual connections.
- **Mechanism:** The astrocytic layer replaces the linear layer in encoder block 0. Modulated outputs are normalized (Equation 9) to match scale, then flow through residual streams to all downstream blocks. Early-layer modulation thus shapes attention patterns extracted at the final block for Grad-CAM/Grad-CAM++.
- **Core assumption:** Explainability-relevant features are shaped early in the network and propagated rather than reconstituted entirely in later layers.
- **Evidence anchors:**
  - [Section 3.1] "Placing the artificial astrocytes in the first decoder block will maximize their influence throughout the network."
  - [Section 4] SSIM improvements are particularly large for Grad-CAM (+66% relative), suggesting structural feature alignment benefits from early modulation.
  - [corpus] Related work on attention-based explanations (e.g., "Integrating attention into explanation frameworks") supports early-layer influence but does not address astrocytes.
- **Break condition:** If the task relies heavily on late-layer feature recombination, early modulation may have limited or unintended effects on final explanations.

## Foundational Learning

- **Concept: Vision Transformer (ViT) architecture and self-attention**
  - **Why needed here:** ViTA modifies the linear layer within the first self-attention block. Understanding tokenization, multi-head attention, and residual connections is required to trace where modulation is injected and how it propagates.
  - **Quick check question:** Can you explain how the CLS token aggregates information across patches in a ViT encoder?

- **Concept: Grad-CAM and Grad-CAM++ explanation methods**
  - **Why needed here:** The paper evaluates ViTA using these gradient-based XAI techniques against human heatmaps. Understanding how they generate activation maps is needed to interpret the results.
  - **Quick check question:** How does Grad-CAM++ differ from Grad-CAM in weighting activation mappings?

- **Concept: Tripartite synapse and astrocyte function (biological inspiration)**
  - **Why needed here:** The design rationale for excitatory/inhibitory modulation and slower timescales draws directly from neuroscience. This context helps evaluate whether the artificial implementation is a meaningful abstraction.
  - **Quick check question:** What role do biological astrocytes play in modulating synaptic transmission?

## Architecture Onboarding

- **Component map:**
  - Input image -> Patch embedding -> Encoder blocks (standard until block 0's linear layer) -> Astrocytic linear layer (k iterations) -> Normalization -> Remaining encoder blocks -> Classification head

- **Critical path:**
  1. Input image → patch embedding → encoder blocks (standard until block 0's linear layer)
  2. At astrocytic linear layer: iterate k times, updating M(t) and Ai per neuron based on CLS token activation
  3. After k iterations: normalize output via Equation 9
  4. Continue through remaining encoder blocks → classification head
  5. Apply Grad-CAM/Grad-CAM++ to generate explanation heatmap

- **Design tradeoffs:**
  - **Placement:** First block maximizes propagation; later blocks may have different effects (unexplored)
  - **k (iterations):** Higher k increases modulation strength but adds compute; optimal k varies by XAI technique
  - **α/β ratios:** Strong excitatory (1.5) and strong inhibitory (0.005) work best together; asymmetric tuning may be needed for other datasets
  - **CLS token vs. all tokens:** Using only CLS simplifies implementation but may miss patch-specific modulation opportunities

- **Failure signatures:**
  - **No improvement over baseline:** Check if normalization is applied correctly; unnormalized outputs may not propagate meaningfully
  - **Degraded classification performance:** Verify astrocytic layer is used only for explainability pass, not classification (as noted in Section 3.1)
  - **Inconsistent results across XAI methods:** Parameter sensitivity differs; re-run grid search per technique

- **First 3 experiments:**
  1. **Reproduce Table 2 results on ClickMe subset:** Implement ViTA with best Grad-CAM configuration (k=6, τ=3, ϕ=-0.5, α=1.5, β=0.05) and verify SSIM improvement
  2. **Ablation on k and τ:** Test k ∈ {2, 4, 6, 8, 10} and τ ∈ {1, 2, 3, 4} to characterize sensitivity beyond reported grid
  3. **Alternative XAI method test:** Apply Attention Rollout or LIME to ViTA vs. ViT to assess whether improvements generalize beyond Grad-CAM family

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the astrocytic modulation mechanism effectively generalize to other Vision Transformer architectures, such as self-supervised or hierarchical models?
- **Basis in paper:** [explicit] The authors explicitly state in the "Future Work" section that they plan to incorporate astrocytes into other architectures, specifically mentioning DINOv2.
- **Why unresolved:** The current study validates the approach only on a standard Vision Transformer (ViT) pre-trained on ImageNet.
- **What evidence would resolve it:** Successful application and evaluation of the artificial astrocyte mechanism on distinct transformer architectures like Swin Transformers or DINOv2.

### Open Question 2
- **Question:** To what extent does the iterative astrocytic modulation impact the model's original classification accuracy?
- **Basis in paper:** [inferred] The paper reports improvements in alignment metrics (Spearman, DSC, SSIM) but does not report Top-1 or Top-5 classification accuracy for the ViTA model compared to the standard ViT.
- **Why unresolved:** While the modulation enhances explainability by affecting the residual stream, it is unclear if this modification degrades the model's primary task performance.
- **What evidence would resolve it:** A comparative analysis of classification accuracy on the ClickMe or ImageNet validation sets between the standard ViT and the proposed ViTA configurations.

### Open Question 3
- **Question:** Is the improvement in human alignment consistent across non-gradient-based explainability techniques?
- **Basis in paper:** [explicit] The authors list "exploring additional XAI techniques" as a direction for future work.
- **Why unresolved:** The results are limited to gradient-based methods (Grad-CAM and Grad-CAM++); it remains unknown if astrocytic modulation aids attention-mapping or perturbation-based methods.
- **What evidence would resolve it:** Evaluation of ViTA using attention-rollback, LIME, or SHAP to determine if the benefits are method-agnostic.

## Limitations

- **Architecture specificity:** The paper uses a specific ViT variant from timm but does not specify which exact model (e.g., ViT-Base, ViT-Large, patch size). This creates uncertainty in reproducing the exact baseline.
- **Parameter sensitivity and generalizability:** Best parameters (k, τ, ϕ, α, β) are highly dependent on the XAI technique and metric. While the authors report best configurations, the sensitivity across different datasets or vision tasks remains unclear.
- **Biological plausibility:** The computational astrocyte model is inspired by neuroscience but simplified. The correspondence between iterative linear modulation and biological astrocyte-neuron dynamics is conceptual, not empirically validated.

## Confidence

- **High confidence:** The core claim that astrocytic modulation can be implemented without training and that it improves alignment with human heatmaps is well-supported by the experimental results.
- **Medium confidence:** The mechanism by which activity-based modulation enhances human-aligned explanations is plausible but not fully validated across tasks or datasets.
- **Low confidence:** The claim that the computational implementation meaningfully mirrors biological astrocyte function is speculative; the timescale analogy is illustrative but not experimentally grounded.

## Next Checks

1. **Reproduce Table 2 results on ClickMe subset:** Implement ViTA with best Grad-CAM configuration (k=6, τ=3, ϕ=-0.5, α=1.5, β=0.05) and verify SSIM improvement
2. **Ablation on k and τ:** Test k ∈ {2, 4, 6, 8, 10} and τ ∈ {1, 2, 3, 4} to characterize sensitivity beyond reported grid
3. **Alternative XAI method test:** Apply Attention Rollout or LIME to ViTA vs. ViT to assess whether improvements generalize beyond Grad-CAM family