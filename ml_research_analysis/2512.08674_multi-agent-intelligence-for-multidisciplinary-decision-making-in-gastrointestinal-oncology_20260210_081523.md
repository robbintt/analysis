---
ver: rpa2
title: Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal
  Oncology
arxiv_id: '2512.08674'
source_url: https://arxiv.org/abs/2512.08674
tags:
- reasoning
- agent
- clinical
- multi-agent
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a multi-agent architecture for automated\
  \ decision-making in gastrointestinal oncology, addressing the limitations of monolithic\
  \ multimodal models in handling complex, heterogeneous clinical data. The proposed\
  \ system decomposes the diagnostic reasoning process into five specialized agents\u2014\
  Endoscopy, Text, Radiology, Laboratory, and a central MDT-Core\u2014each responsible\
  \ for processing distinct data modalities."
---

# Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology

## Quick Facts
- arXiv ID: 2512.08674
- Source URL: https://arxiv.org/abs/2512.08674
- Reference count: 14
- Multi-agent system achieves 4.60/5.00 composite expert score on GI oncology MDT decision support, outperforming monolithic baseline (3.76/5.00)

## Executive Summary
This paper introduces a multi-agent architecture for automated decision-making in gastrointestinal oncology, addressing the limitations of monolithic multimodal models in handling complex, heterogeneous clinical data. The proposed system decomposes the diagnostic reasoning process into five specialized agents—Endoscopy, Text, Radiology, Laboratory, and a central MDT-Core—each responsible for processing distinct data modalities. The framework explicitly handles cross-modal conflicts and grounds reasoning in structured intermediate evidence, reducing hallucination and improving accuracy. Evaluated on a multi-institutional dataset of 2,174 cases, the system achieved a composite expert score of 4.60/5.00, outperforming the monolithic baseline (3.76/5.00) and demonstrating significant gains in reasoning logic and medical accuracy. The approach validates multi-agent collaboration as a scalable, interpretable, and clinically robust paradigm for AI-driven oncology decision support.

## Method Summary
The system processes 2,174 multi-institutional patient cases containing EMR text, endoscopy image sequences (5-10 frames), CT/MRI reports, and laboratory results. Five specialized agents—Text, Endoscopy (Intern S1 Mini), Radiology, Laboratory, and MDT-Core (Qwen-3 32B)—are trained via supervised fine-tuning on modality-specific tasks. The MDT-Core agent synthesizes outputs with explicit cross-modal conflict detection, flagging staging discrepancies exceeding one TNM level. The system was trained on 1,500 cases with 10% human audit of reverse-decomposition SFT data.

## Key Results
- Achieved composite expert score of 4.60/5.00 across 7 dimensions versus 3.76/5.00 for monolithic baseline
- Explicit conflict detection reduced hallucinated recommendations and improved safety
- VQA-based endoscopy agent generated high-resolution morphological descriptions for TNM staging
- System demonstrated significant gains in reasoning logic and medical accuracy over single-agent approaches

## Why This Works (Mechanism)

### Mechanism 1
Task decomposition into specialized agents reduces context dilution and preserves modality-specific nuances. Each specialized agent independently maps raw inputs to structured intermediate reasoning states, which are then aggregated rather than processing all modalities in a single context window.

### Mechanism 2
Explicit cross-modal conflict detection reduces hallucinated recommendations and improves safety. The system extracts staging claims from Endoscopy and Radiology agents; if discrepancies exceed one TNM level, it flags "Staging Discrepancy Detected" and recommends pathological confirmation rather than forcing consensus.

### Mechanism 3
VQA-based visual-language training for endoscopy produces clinically actionable descriptions that bridge raw pixels to MDT-level reasoning. The Endoscopy Agent is trained on 50,000+ image-caption pairs using structured multi-question VQA rather than free-form captioning, eliciting specific morphological details.

## Foundational Learning

- **Multi-Agent Systems with Role Specialization**: Why needed here—The architecture decomposes a complex multimodal reasoning task into distinct agent roles; understanding agent coordination, communication protocols, and synchronization is essential. Quick check: Can you explain why parallel agent execution with synchronized aggregation outperforms sequential single-agent processing for heterogeneous inputs?

- **Visual Question Answering (VQA) for Medical Imaging**: Why needed here—The Endoscopy Agent uses VQA rather than classification or free-form captioning; understanding how structured queries elicit diagnostically relevant features is critical. Quick check: What is the advantage of asking "Describe the lesion surface texture" versus "Caption this image" for clinical decision-making?

- **TNM Staging and Cross-Modal Clinical Reasoning**: Why needed here—Conflict detection relies on understanding oncology staging systems and when imaging modalities may legitimately disagree. Quick check: If endoscopy suggests T1 and radiology suggests T3, what clinical follow-up is typically required?

## Architecture Onboarding

- **Component map**: Raw multimodal input → parallel specialist processing → intermediate state generation → conflict detection → MDT-Core synthesis → structured MDT report
- **Critical path**: Raw multimodal input → parallel specialist processing → intermediate state generation → conflict detection → MDT-Core synthesis → structured MDT report
- **Design tradeoffs**: Complexity vs. interpretability (more agents increase complexity but enable traceability); Parallel vs. sequential (parallel reduces latency but requires synchronization); Rule-based vs. learned conflict detection (current uses explicit thresholds for auditability)
- **Failure signatures**: Staging discrepancies repeatedly flagged suggests calibration issue; Generic endoscopy descriptions suggest VQA training insufficiency; Missing laboratory evidence suggests agent output formatting issues
- **First 3 experiments**: Ablation validation (replicate conflict detection ablation on held-out subset); VQA prompt iteration (test alternative question sets for endoscopy agent); Conflict threshold calibration (systematically vary staging discrepancy threshold)

## Open Questions the Paper Calls Out

- **Cross-population validation**: Can the framework maintain performance across diverse genetic backgrounds and clinical reporting standards outside the Shanghai cohort? This remains unresolved as the dataset is geographically localized, requiring external validation from international cohorts.

- **Vision-based radiology integration**: Does integrating direct vision-encoders for raw CT/MRI volumes improve diagnostic accuracy over the current text-based Radiology Agent? The authors identify reliance on textual reports rather than raw DICOM volumes as a key limitation.

- **Dynamic agent debate**: Would a dynamic debate mechanism, where the Core Agent queries peripheral agents for clarification, improve outcomes in ambiguous cases? The current static feed-forward architecture lacks iterative refinement loops for complex cases.

## Limitations

- **Geographic localization**: The model was trained and validated exclusively on data from Shanghai hospitals, limiting generalizability to other populations and clinical practices.

- **Static reasoning architecture**: The system operates in a single-pass feed-forward manner without iterative refinement or multi-turn agent interactions for complex cases.

- **Limited radiology vision integration**: The current architecture relies on radiologist-generated reports rather than performing autonomous analysis of raw imaging volumes.

## Confidence

- **Task Decomposition Reduces Context Dilution**: High confidence
- **Conflict Detection Improves Safety**: Medium confidence
- **VQA Yields Clinically Actionable Descriptions**: Low confidence

## Next Checks

1. **Conflict Threshold Sensitivity Analysis**: Systematically vary the staging discrepancy threshold (e.g., 0.5, 1.0, 1.5 TNM levels) on the test set and measure composite score, safety flag rate, and clinician acceptance.

2. **VQA Question Set Validation**: Conduct a randomized trial comparing the current structured VQA approach to a free-form captioning baseline on a held-out endoscopy subset, measuring inter-rater agreement on morphological detail relevance.

3. **Cross-Institutional Generalizability Test**: Apply the trained model to a distinct, multi-institutional dataset from a different geographic region or hospital system and re-evaluate the 7-dimension expert rubric to quantify overfitting.