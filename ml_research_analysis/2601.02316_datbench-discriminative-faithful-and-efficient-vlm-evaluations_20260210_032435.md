---
ver: rpa2
title: 'DatBench: Discriminative, Faithful, and Efficient VLM Evaluations'
arxiv_id: '2601.02316'
source_url: https://arxiv.org/abs/2601.02316
tags:
- evaluation
- discriminative
- evaluations
- datbench
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a framework for evaluating vision-language
  models (VLMs) that addresses key limitations in existing benchmarks. The authors
  identify three desiderata for effective VLM evaluation: faithfulness to real-world
  usage, discriminability between models of varying quality, and computational efficiency.'
---

# DatBench: Discriminative, Faithful, and Efficient VLM Evaluations

## Quick Facts
- arXiv ID: 2601.02316
- Source URL: https://arxiv.org/abs/2601.02316
- Reference count: 35
- Primary result: Achieves 13× average speedup (up to 50×) while maintaining discriminative power for VLM evaluation

## Executive Summary
This paper introduces DatBench, a framework for evaluating vision-language models (VLMs) that addresses key limitations in existing benchmarks. The authors identify three critical desiderata for effective VLM evaluation: faithfulness to real-world usage, discriminability between models of varying quality, and computational efficiency. Through systematic data transformation and filtering rather than discarding existing benchmarks, DatBench achieves significant improvements in evaluation efficiency while maintaining rigorous standards for model comparison.

The core methodology involves four key interventions: converting multiple-choice questions to generative tasks, filtering out "blindly solvable" questions that don't require visual input, removing mislabeled or ambiguous examples, and selecting high-discriminative samples to maximize signal per compute unit. The resulting benchmarks, DatBench and DatBench-Full, demonstrate 13× average speedup (up to 50×) while maintaining discriminative power. The analysis reveals important insights including that inference-time scaling can degrade perceptual performance ("overthinking penalty"), and that language priors systematically mask true multimodal capability across popular benchmarks.

## Method Summary
The methodology centers on transforming existing VLM benchmarks through four systematic interventions rather than creating entirely new datasets. First, multiple-choice questions are converted to generative tasks, which reduces accuracy by up to 35% but provides a more faithful assessment of model capabilities. Second, samples that can be solved without visual input ("blindly solvable" questions) are filtered out, removing up to 70% of samples from some datasets. Third, mislabeled or ambiguous examples are identified and removed through consistency checks across multiple models. Finally, high-discriminative samples are selected to maximize the signal-to-noise ratio per compute unit spent on evaluation. This approach creates two benchmark variants: DatBench for rapid iteration (high efficiency) and DatBench-Full for comprehensive reporting (maintaining broader coverage while still improved over baselines).

## Key Results
- Achieves 13× average speedup (up to 50×) in VLM evaluation while maintaining discriminative power
- Reveals "overthinking penalty" where inference-time scaling can degrade perceptual performance
- Identifies systematic masking of true multimodal capability by language priors across popular benchmarks
- Filters out up to 70% of samples as "blindly solvable," significantly improving evaluation faithfulness

## Why This Works (Mechanism)
DatBench works by addressing the fundamental tension between evaluation comprehensiveness and computational feasibility. Traditional VLM benchmarks often include samples that don't require visual reasoning, are mislabeled, or provide insufficient discriminative signal relative to their computational cost. By systematically identifying and removing these problematic samples while transforming question formats to better reflect real-world usage, DatBench concentrates evaluation resources on the most informative samples. The selection of high-discriminative samples ensures that each evaluation provides maximum information about model differences, while the conversion to generative tasks eliminates the ability of models to exploit dataset-specific patterns or language priors. This approach creates a more faithful representation of model capabilities while dramatically reducing the computational burden of evaluation.

## Foundational Learning

**VLM Architecture Fundamentals** - Understanding how vision-language models process multimodal inputs is crucial for designing effective evaluations. These models typically use separate encoders for visual and textual inputs with cross-attention mechanisms. Quick check: Can you explain how cross-attention enables VLM reasoning?

**Benchmark Evaluation Principles** - Knowledge of evaluation methodology, including concepts like discriminative power, faithfulness, and computational efficiency, is essential. Quick check: What distinguishes a discriminative benchmark from a non-discriminative one?

**Dataset Quality Assessment** - Understanding how to identify mislabeled, ambiguous, or "blindly solvable" samples is critical for benchmark curation. Quick check: How would you systematically identify samples that don't require visual input?

## Architecture Onboarding

**Component Map**: Raw Benchmark Data -> Filtering Pipeline -> Multiple-Choice to Generative Conversion -> Discriminative Sample Selection -> DatBench Output

**Critical Path**: The critical evaluation path involves sample filtering (70% reduction potential), format conversion (35% accuracy reduction), and discriminative selection. Each step must maintain evaluation integrity while improving efficiency.

**Design Tradeoffs**: The primary tradeoff is between evaluation comprehensiveness and efficiency. Aggressive filtering improves efficiency but may reduce coverage. Format conversion improves faithfulness but may introduce new challenges in evaluation consistency.

**Failure Signatures**: Common failures include over-filtering that eliminates representative samples, format conversion that introduces ambiguity, and discriminative selection that inadvertently favors certain model architectures. These manifest as reduced benchmark coverage, inconsistent evaluation results, or biased model rankings.

**First Experiments