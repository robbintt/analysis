---
ver: rpa2
title: Bridging the Modality Gap by Similarity Standardization with Pseudo-Positive
  Samples
arxiv_id: '2511.22141'
source_url: https://arxiv.org/abs/2511.22141
tags:
- text
- modality
- query
- retrieval
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a similarity standardization approach to mitigate
  the modality gap in multi-modal retrieval without manually labeled data or image
  captioning. The method computes modality-specific mean and variance from pseudo-positive
  pairs, which are constructed by retrieving the most similar text and image candidates
  for each query.
---

# Bridging the Modality Gap by Similarity Standardization with Pseudo-Positive Samples

## Quick Facts
- arXiv ID: 2511.22141
- Source URL: https://arxiv.org/abs/2511.22141
- Authors: Shuhei Yamashita; Daiki Shirafuji; Tatsuhiko Saito
- Reference count: 40
- One-line primary result: Achieves 64% average Recall@20 gains on MMQA and 28% on WebQA by standardizing similarity scores using modality-specific statistics from pseudo-positive pairs.

## Executive Summary
This paper addresses the modality gap in multi-modal retrieval, where contrastive learning causes vision-language models to assign higher similarity scores to same-modality items regardless of relevance. The proposed method computes modality-specific mean and variance from pseudo-positive pairs (constructed by retrieving the most similar text and image candidates for each query) and uses these statistics to standardize cosine similarity scores. This standardization makes scores comparable across text and image modalities without requiring manually labeled data or image captioning. Experiments on MMQA and WebQA benchmarks with seven vision-language models show significant performance improvements, particularly for cross-modality retrieval tasks.

## Method Summary
The method standardizes cosine similarity scores using modality-specific mean and variance computed from pseudo-positive pairs. For each query, the top-1 text and image candidates are retrieved as pseudo-positives. From these pairs, modality-specific statistics (μ_text, σ_text, μ_image, σ_image) are calculated. During retrieval, all similarity scores are standardized using z-score normalization: sim(q,d) = (cos(f(q), f(d)) - μ_m) / σ_m, where m is the modality of document d. This calibration enables fair comparison between text and image candidates, effectively bridging the modality gap while preserving visual information.

## Key Results
- Achieves average Recall@20 gains of 64% on MMQA and 28% on WebQA when query and target belong to different modalities
- Outperforms E5-V, which uses image captioning to address the modality gap
- Superior performance on image retrieval tasks compared to caption-based approaches, preserving visual information
- Standardization using pseudo-positive pairs sometimes outperforms using ground-truth labeled pairs

## Why This Works (Mechanism)

### Mechanism 1: Modality-Specific Score Calibration
The modality gap causes VLMs to systematically assign higher similarity scores to same-modality candidates. By computing separate statistics (μ_text, σ_text, μ_image, σ_image) and applying z-score normalization, the scale difference is neutralized, making scores comparable across modalities. The core assumption is that pseudo-positive pair distributions approximate true positive distributions.

### Mechanism 2: Skewness Exploitation in Image Distributions
CLIP-based models produce positively skewed similarity distributions for images (skewness ~0.21-0.47) compared to negative or near-zero skewness for text. Standardization boosts outlier scores, allowing relevant images to out-rank text candidates even when the mean image score is lower. The positively skewed tail contains the relevant images.

### Mechanism 3: Pseudo-Positive Pair Approximation
Unlabeled queries paired with their highest-similarity candidates can substitute for labeled query-positive pairs when estimating modality-specific statistics. For each query q, retrieving argmax_(d∈D_m) cos(f(q), f(d)) as pseudo-positive d̂⁺_m provides statistics that approximate true positive statistics well enough that performance equals or exceeds supervised standardization.

## Foundational Learning

- **Concept: Modality Gap in Contrastive Learning**
  - Why needed: Explains why VLMs like CLIP cluster same-modality embeddings together, causing retrieval failures
  - Quick check: Can you explain why contrastive learning with paired image-text data creates separate embedding clusters rather than a unified space?

- **Concept: Z-Score Standardization**
  - Why needed: The core technique transforms raw cosine similarities into comparable z-scores using modality-specific parameters
  - Quick check: Given μ_text=0.75, σ_text=0.1, μ_image=0.30, σ_image=0.05, and raw scores of 0.78 (text) and 0.38 (image), which candidate ranks higher after standardization?

- **Concept: Retrieval Evaluation Metrics (Recall@k, MRR, NDCG)**
  - Why needed: The paper reports gains primarily via Recall@20; understanding these metrics is essential for interpreting results
  - Quick check: If a system retrieves 5 relevant items in top-20 and 10 relevant items exist total, what is Recall@20?

## Architecture Onboarding

- **Component map:** VLM Encoder -> Pseudo-Pair Constructor -> Statistics Calculator -> Standardization Layer -> Unified Ranker

- **Critical path:** Pseudo-pair quality → accurate statistics → effective standardization → fair cross-modal ranking. Errors propagate: poor pseudo-pairs → wrong μ/σ → miscalibrated scores.

- **Design tradeoffs:**
  - Static vs. dynamic statistics: Pre-computed is fast but assumes distribution stability; dynamic updates add latency but handle drift
  - Top-1 vs. top-k pseudo-positives: Top-1 is simplest but may be noisy; top-k aggregation could smooth estimates (unexplored)
  - Standardization vs. model fine-tuning: Standardization is inference-only and requires no training data; fine-tuning may close gap more thoroughly but needs labeled pairs

- **Failure signatures:**
  - Near-zero ImageQ Recall with cosine similarity (modality gap dominant)
  - Large TextQ performance drop after standardization (over-correction)
  - Statistics becoming stale: μ/σ computed on old data no longer match new query distributions

- **First 3 experiments:**
  1. Baseline sanity check: Run cosine-only retrieval on your VLM + dataset; verify ImageQ Recall@20 is near-zero (confirms modality gap)
  2. Pseudo-pair construction: Sample 1000 queries, retrieve top-1 text and image candidates, compute μ/σ per modality
  3. Ablation on skewness: Plot similarity distributions for text vs. image candidates; confirm image distributions show positive skewness

## Open Questions the Paper Calls Out

- **Open Question 1:** How can modality-specific statistics be dynamically updated to remain accurate in retrieval systems where the database content changes frequently?
  - Basis: The Limitations section states that pre-computed statistics may become obsolete as new content is added, and explicitly calls for future work to "focus on developing mechanisms to dynamically update these statistics."

- **Open Question 2:** Why does standardization using pseudo-positive pairs sometimes yield higher retrieval performance than using ground-truth labeled pairs?
  - Basis: In Table 3, the "Ours" (pseudo-pairs) condition frequently outperforms the "Std" (labeled pairs) condition, but the authors do not provide a theoretical explanation for why unlabeled data outperforms labeled data in this context.

- **Open Question 3:** Does similarity standardization effectively generalize to multi-modal retrieval tasks involving more than two modalities (e.g., audio, video)?
  - Basis: The Task Formulation and experiments are restricted to text and image modalities. The scalability of this variance-based normalization to a higher number of distinct embedding spaces is unexplored.

## Limitations
- The method relies on pre-computed statistics that may become obsolete as new content is added to the retrieval database
- The core assumption that pseudo-positive pairs approximate true positive distributions lacks extensive external validation
- The skewness exploitation mechanism is specific to this paper and needs broader empirical support beyond the tested datasets

## Confidence
- **High confidence:** The standardization mechanism (z-score normalization) and its implementation are technically sound and produce measurable gains on tested datasets
- **Medium confidence:** The claim that pseudo-positive pairs approximate true positives is supported by internal comparisons but lacks external validation
- **Medium confidence:** The skewness exploitation mechanism is specific to this paper and needs broader empirical support beyond the tested datasets

## Next Checks
1. Apply the method to other multi-modal retrieval datasets (e.g., Flickr30k, COCO) with different VLM architectures to test distribution assumptions
2. Implement a rolling update mechanism for μ_m, σ_m to handle distribution drift in evolving retrieval corpora
3. Experiment with aggregating top-k pseudo-positive candidates to reduce noise and improve statistical estimates