---
ver: rpa2
title: Distributional Reinforcement Learning on Path-dependent Options
arxiv_id: '2507.12657'
source_url: https://arxiv.org/abs/2507.12657
tags:
- learning
- distribution
- quantile
- distributional
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies Distributional Reinforcement Learning to estimate
  the full payoff distribution of path-dependent options, focusing on Asian options.
  Unlike traditional pricing methods that estimate only expected payoffs, the proposed
  framework learns the entire conditional distribution of payoffs using quantile regression
  and radial basis function (RBF) approximations.
---

# Distributional Reinforcement Learning on Path-dependent Options

## Quick Facts
- arXiv ID: 2507.12657
- Source URL: https://arxiv.org/abs/2507.12657
- Reference count: 18
- Primary result: Distributional RL estimates full payoff distribution of Asian options using quantile regression and RBF features

## Executive Summary
This paper introduces a distributional reinforcement learning framework for pricing path-dependent options by learning their full payoff distribution rather than just expected values. The method uses quantile regression with RBF function approximation to recursively estimate the conditional distribution of option payoffs, treating option pricing as a distributional dynamic programming problem. The approach successfully captures distributional moments including mean and skewness, though tail behavior (kurtosis) remains challenging. Numerical experiments demonstrate accurate pricing for Asian options with absolute errors typically below 1 for in-the-money options.

## Method Summary
The framework models option pricing as a distributional Bellman update where the state includes current price, running average, and time index. It uses quantile regression with 50 quantiles and RBF feature expansion (40 centers) to approximate the payoff distribution at each state. The method performs semi-gradient updates using the pinball loss, with backward propagation from terminal payoffs through time. Key implementation details include payoff clipping for stability, gradient clipping to prevent explosions, and initializing quantile estimates near the mean payoff from Monte Carlo simulation.

## Key Results
- Learned payoff distributions capture mean and skewness accurately for in-the-money Asian options
- Absolute pricing errors remain below 4% for ATM options compared to 100,000-path Monte Carlo benchmarks
- Wasserstein distances between learned and true distributions are low (typically below 10)
- Method successfully learns distributional moments but systematically underestimates kurtosis (tail behavior)

## Why This Works (Mechanism)

### Mechanism 1: Distributional Bellman Contraction Enables Stable Recursive Distribution Learning
- Claim: The distributional Bellman operator T, defined as (TZ)(st) := L[Rt + γY_{t+1} | st], is a γ-contraction in the Wasserstein metric W_p, guaranteeing convergence to a unique fixed-point distribution.
- Mechanism: Because the same sampled reward r(s, S') appears in both arms of the coupling, the shift invariance of W_p cancels the reward term, and the discount factor γ < 1 yields contraction. Errors in learned distributions at one time step do not amplify through recursion—both mean and tail shape evolve stably.
- Core assumption: The state space S is measurable with transition kernel P(·|s), and return distributions have finite p-th moments (sups∈S E[|r(s,s')|^p] < ∞).
- Evidence anchors:
  - [abstract]: "We demonstrate the efficacy of this method on Asian options, using quantile-based value function approximators."
  - [section]: Proposition 2 (adapted from Bellemare et al., 2017) proves T is a γ-contraction on (Z, W_p), with full proof via optimal coupling argument.
  - [corpus]: Distributional Soft Actor-Critic with Diffusion Policy (arXiv:2507.01381) corroborates distributional RL's value in capturing multi-modal value distributions, though for control tasks. Error Propagation in Dynamic Programming (arXiv:2509.20239) analyzes convergence in related DP settings but does not address distributional contraction.
- Break condition: If rewards or value distributions have unbounded support without moment constraints, or if the transition kernel lacks regularity, the contraction guarantee may not hold.

### Mechanism 2: Quantile Regression Provides Unbiased Stochastic Gradients for Non-Parametric Distribution Approximation
- Claim: Minimizing the quantile (pinball) loss ρ_τ(u) = u(τ - I{u<0}) via SGD yields unbiased gradient estimators that converge to true quantiles of the Bellman target distribution, even under RBF function approximation.
- Mechanism: The subgradient g_i(s,s') = -(τ_i - I{r+γθ_i(s')-θ_i(s)<0}) · φ(s) is unbiased (Proposition 3). Under i.i.d. sampling, continuous CDF at quantiles, and step sizes satisfying Σα_k = ∞, Σα_k² < ∞, quantile estimates θ_i(s) converge almost surely to F_Z^{-1}(τ_i) (Proposition 4).
- Core assumption: The CDF of Z(s) is continuous at its τ_i-quantile (ensuring unique minimizer), and sufficient i.i.d. samples are available.
- Evidence anchors:
  - [abstract]: "using quantile-based value function approximators" is the core technical choice.
  - [section]: Proposition 3 proves unbiased stochastic gradient; Proposition 4 proves almost-sure convergence; Proposition 5 gives explicit semi-gradient for RBF approximation.
  - [corpus]: Optimizing Return Distributions with Distributional DP (arXiv:2501.13028) introduces distributional DP methods for optimizing return distribution functionals, aligning with quantile-based objectives but without RBF-specific analysis.
- Break condition: If the CDF has discontinuities at target quantiles, or if sampling is non-i.i.d. with unaccounted dependence, convergence guarantees weaken. Sparse positive payoffs in deep OTM regimes provide weak learning signals (Section 3 acknowledges this).

### Mechanism 3: Finite-Dimensional Markovian State Embedding Sufficiently Summarizes Path Information
- Claim: For path-dependent payoffs like Asian options, a finite-dimensional state s_t = (S_t, A_t, t)—spot price, running average, time—suffices for the conditional distribution L[f(S_{0:T}) | F_t] to depend on the past only through s_t.
- Mechanism: By Proposition 1 (Doob-Dynkin style), if there exists an F_t-measurable summary s_t = ψ_t(S_{0:t}) such that E[h(f(S_{0:T})) | F_t] = g_{t,h}(s_t), then the conditional law depends only on s_t. For Asian payoffs f = max(A_T - K, 0), the running average A_t captures all relevant path history.
- Core assumption: The payoff functional admits a finite-dimensional sufficient statistic; for Asian options, A_t alone suffices. More complex path dependencies may require additional statistics (e.g., running maximum for lookback options).
- Evidence anchors:
  - [abstract]: "conditional distribution of payoffs given a Markovian state representation that includes current price, running average, and time."
  - [section]: Proposition 1 formalizes measurability; Equation (6) defines s_t = (S_t, A_t, t); Section 2.2 justifies this for Asian options.
  - [corpus]: Weak direct corpus evidence on path-dependent state embeddings for RL; Valuation of Exotic Options (arXiv:2509.13374) addresses exotic option pricing via diffusion models but does not formalize Markovian state augmentation.
- Break condition: If the payoff depends on path features not captured by s_t (e.g., local volatility history, path-dependent barriers not reducible to running statistics), the Markovian reduction fails and the conditional distribution will have residual dependence on omitted path information.

## Foundational Learning

- Concept: **Distributional Bellman Operator**
  - Why needed here: This operator defines how return distributions propagate backward through time. Understanding that it is a contraction in Wasserstein metric is essential to see why recursive quantile learning converges rather than diverging.
  - Quick check question: Given a discount factor γ = 0.99 and initial Wasserstein distance W_1(Z_1, Z_2) = 10 between two candidate distributions, what is the maximum distance after one Bellman update?

- Concept: **Quantile Regression (Pinball Loss)**
  - Why needed here: The paper approximates distributions via quantiles θ_i(s), trained by minimizing asymmetric loss ρ_τ(u). Without understanding why this loss targets specific quantiles, the gradient updates appear arbitrary.
  - Quick check question: For τ = 0.95, if the residual u = 2 (prediction too low), what is the value of ρ_{0.95}(2)? What if u = -2 (prediction too high)?

- Concept: **Radial Basis Function (RBF) Feature Expansion**
  - Why needed here: The paper uses θ_i(s) = w_i^T φ(s) with Gaussian RBF features φ_j(s) = exp(-||s - c_j||² / 2σ²). This provides smooth, localized function approximation suited to continuous financial state spaces, unlike deep networks which are heavier and less interpretable.
  - Quick check question: If you have 40 RBF centers uniformly sampled in [0,1]³ and a query state s = (0.5, 0.5, 0.5), which centers will have highest activation? What happens to all activations as σ → ∞?

## Architecture Onboarding

- Component map:
  State s_t = (S_t, A_t, t) → Normalize to [0,1]³ → RBF Feature Map φ(s) ∈ R^{41}
                                                                   ↓
  For each quantile i ∈ {1,...,50}: θ_i(s) = w_i^T φ(s) → Dirac mixture: Ĵ(s) = (1/N) Σ δ_{θ_i(s)}
                                                                   ↓
  Sample transition (s, r, s') → Compute TD residual Δ_i = r + γθ_i(s') - θ_i(s)
                                                                   ↓
  Semi-gradient update: w_i ← w_i + η(τ_i - I{Δ_i < 0})φ(s)

- Critical path:
  1. **State normalization**: Divide each component by appropriate scale (paper uses 200 for price/average) to keep RBF inputs in [0,1].
  2. **Quantile initialization**: Initialize all w_i so θ_i(s_0) ≈ E[f(S_{0:T})] from a small MC pre-simulation. This prevents early gradient explosion.
  3. **Backward Bellman updates**: Starting from terminal state with δ_{f(S_{0:T})}, propagate quantile targets backward through time.
  4. **Gradient clipping**: Clip gradient L2-norm to prevent destabilization from outlier payoffs.

- Design tradeoffs:
  - **RBF vs. Neural Networks**: RBF is lighter, more interpretable, and stable for low-dimensional state spaces (d=3). Deep networks may scale better to higher dimensions but require more data and tuning. Paper explicitly chooses RBF for "interpretable and computationally efficient" approximation.
  - **Number of quantiles (N=50)**: More quantiles capture finer distribution detail but increase computational cost. Paper found 50 sufficient for 252-step Asian options.
  - **Clipping payoffs**: Clipping at a maximum (e.g., 200) stabilizes learning but risks underestimating tail outcomes. Must clip both training and evaluation sets consistently, or risk severe distribution mismatch (Figure 2).

- Failure signatures:
  - **Negative predicted prices** (Figure 3): Occurs when gradients are not clipped and payoffs have extreme range. Gradient explosion in quantile regression pushes weights to absurd values.
  - **Underestimation of kurtosis** (Figure 1b): Wasserstein distance may converge while higher moments remain inaccurate; the metric does not fully capture tail behavior.
  - **Collapse in OTM regimes**: When S_0 << K, most payoffs are zero with sparse positive values; quantile gradients have weak signal. Quantile estimates may collapse to near-zero or fail to spread.
  - **Distribution mismatch from asymmetric clipping** (Figure 2a): Clipping only DistRL outputs but not training targets causes horrifying underestimation.

- First 3 experiments:
  1. **Replicate ATM Asian option pricing**: Set S_0 = 105, K = 100, r = 0.03, σ = 0.2, T = 1 year, N_quantiles = 50, n_RBF = 40. Train for 100 epochs with 100 paths/epoch. Compare learned mean and quantiles against a 100,000-path MC benchmark. Verify absolute error < 4%.
  2. **Ablate quantile initialization**: Train two models—one with mean-payoff initialization as described, one with random weight initialization. Compare convergence speed and incidence of gradient explosion. Hypothesis: random init leads to higher early instability.
  3. **Test OTM degradation**: Systematically vary S_0 - K from -5 to -30 (deepening OTM). Monitor absolute error and quantile spread. Identify the regime where learning signals become too sparse for convergence. Test proposed mitigation (initializing quantiles to small positive values) and report impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be stabilized for deep out-of-the-money options where learning signals are sparse?
- Basis in paper: [explicit] The authors note that deep out-of-the-money configurations result in weak signals and state they leave strategies like drift-shifted importance sampling "for a further study."
- Why unresolved: The current method relies on clipping payoffs to manage outliers, which introduces bias, and the unmodified algorithm struggles with sparse positive values.
- What evidence would resolve it: Demonstrated convergence and accurate quantile estimation in rare-event regimes without relying on aggressive payoff clipping.

### Open Question 2
- Question: Does the approach generalize to American options through policy optimization?
- Basis in paper: [explicit] The conclusion states that any path-dependent option, including American options, could be priced via this framework if extended with policy optimization, which they leave as a "further study."
- Why unresolved: The current formulation explicitly omits the agent/action dynamic required for optimal stopping problems, treating the state evolution as purely exogenous.
- What evidence would resolve it: A modification of the DistRL framework that successfully learns an optimal stopping policy and prices American options against standard benchmarks.

### Open Question 3
- Question: Is the method robust under stochastic volatility and interest rate models?
- Basis in paper: [explicit] The conclusion remarks on the "necessity of testing our framework on stochastic volatility and interest rate scenarios along with real life option payoffs."
- Why unresolved: Numerical experiments were restricted to Geometric Brownian Motion with constant volatility and interest rates.
- What evidence would resolve it: Accurate distributional approximations when the underlying asset follows complex dynamics, such as Heston or Hull-White models.

## Limitations

- The RBF approximation may struggle with high-dimensional state spaces or path dependencies requiring more complex sufficient statistics beyond running averages
- The method's performance on deep out-of-the-money options remains poorly characterized due to sparse positive payoff signals
- Systematic underestimation of kurtosis (tail behavior) occurs despite low Wasserstein distances between learned and true distributions

## Confidence

- **High Confidence**: The distributional Bellman contraction proof (Proposition 2) and its convergence guarantees for quantile regression (Propositions 3-4) are mathematically rigorous and well-established in the distributional RL literature.
- **Medium Confidence**: The RBF-based quantile approximation performs well for the tested Asian option scenarios, but generalization to other path-dependent payoffs or higher-dimensional state spaces remains to be validated.
- **Low Confidence**: The paper's claim about learning accurate kurtosis (tail behavior) is not well-supported by the evidence, as Figure 1b shows systematic underestimation despite low Wasserstein distances.

## Next Checks

1. **Generalization Test**: Apply the framework to at least two additional path-dependent option types (e.g., lookback and barrier options) and evaluate whether the finite-dimensional Markovian state representation remains sufficient for accurate distribution learning.

2. **Tail Behavior Assessment**: Design experiments specifically targeting tail estimation accuracy, comparing learned kurtosis against high-precision MC benchmarks across different moneyness regimes to quantify systematic biases.

3. **Scalability Analysis**: Evaluate the RBF approximation's performance as state dimensionality increases (e.g., adding local volatility or interest rate paths) and compare against alternative function approximators like neural networks in terms of accuracy and computational efficiency.