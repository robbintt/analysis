---
ver: rpa2
title: 'A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with
  SFT and Efficiency with Reinforcement Learning'
arxiv_id: '2507.08267'
source_url: https://arxiv.org/abs/2507.08267
tags:
- accuracy
- performance
- arxiv
- reasoning
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-stage training recipe for mathematical
  reasoning in large language models. The core idea is to first apply extensive supervised
  fine-tuning (SFT) for up to 10 epochs to maximize accuracy, followed by reinforcement
  learning with Group Relative Policy Optimization (GRPO) to improve token efficiency
  without sacrificing accuracy.
---

# A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2507.08267
- **Source URL:** https://arxiv.org/abs/2507.08267
- **Authors:** Hiroshi Yoshihara; Taiki Yamaguchi; Yuichi Inoue
- **Reference count:** 5
- **Primary Result:** 2-stage SFT + GRPO recipe achieves top-4 public / top-8 private AIMO ranking out of 2,200+ teams

## Executive Summary
This paper presents a two-stage training approach for mathematical reasoning in large language models that first applies extensive supervised fine-tuning (SFT) for up to 10 epochs to maximize accuracy, followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to improve token efficiency without sacrificing accuracy. The method achieves top-tier performance on challenging benchmarks including AIME 2024, AIME 2025, and MATH-500, ranking 4th on the public set and 8th on the private set of the AI Mathematical Olympiad (AIMO) out of 2,200+ teams. The approach demonstrates that prolonged SFT is crucial for accuracy breakthroughs, while GRPO primarily optimizes solution length, resulting in shorter and more efficient outputs.

## Method Summary
The core methodology involves a sequential two-stage training process where mathematical LLMs first undergo extensive supervised fine-tuning (SFT) for up to 10 epochs to maximize accuracy on mathematical reasoning tasks. This is followed by reinforcement learning using Group Relative Policy Optimization (GRPO), which optimizes for token efficiency without degrading accuracy. The approach leverages the complementary strengths of both methods: SFT provides the accuracy foundation through extensive exposure to correct solutions, while GRPO refines the model's ability to generate concise, efficient solutions. The training pipeline is evaluated on challenging mathematical benchmarks including AIME 2024, AIME 2025, and MATH-500, demonstrating significant improvements over baseline approaches.

## Key Results
- Achieved top-4 ranking on public AIMO set and top-8 on private AIMO set out of 2,200+ competing teams
- Demonstrated that prolonged SFT (up to 10 epochs) is crucial for achieving accuracy breakthroughs in mathematical reasoning
- Showed that GRPO optimization primarily improves solution length efficiency without sacrificing accuracy gains from SFT
- Validated approach on multiple challenging benchmarks: AIME 2024, AIME 2025, and MATH-500

## Why This Works (Mechanism)
The two-stage approach works by first establishing a strong accuracy foundation through extensive supervised learning, then refining the model's efficiency characteristics through reinforcement learning. The prolonged SFT phase allows the model to internalize complex mathematical reasoning patterns and solution strategies through repeated exposure to correct solutions across multiple epochs. This extensive training ensures the model develops robust understanding of mathematical concepts and problem-solving approaches. The subsequent GRPO phase then optimizes the model's token generation behavior, encouraging more concise and efficient solutions while maintaining the accuracy gains achieved during SFT. This sequential approach leverages the strengths of both supervised learning (accuracy through pattern recognition) and reinforcement learning (efficiency through reward optimization).

## Foundational Learning
- **Supervised Fine-Tuning (SFT):** Why needed - Establishes baseline accuracy through exposure to correct solutions; Quick check - Monitor validation accuracy across epochs to identify optimal training duration
- **Reinforcement Learning with GRPO:** Why needed - Optimizes token efficiency and solution conciseness; Quick check - Measure solution length reduction while maintaining accuracy
- **Mathematical Reasoning Patterns:** Why needed - Core competency for solving complex math problems; Quick check - Evaluate performance on diverse problem types (algebra, geometry, number theory)
- **Group Relative Policy Optimization:** Why needed - Enables efficient RL training by comparing policies within groups; Quick check - Compare token efficiency before and after GRPO fine-tuning
- **Benchmark Evaluation:** Why needed - Validates performance on standardized mathematical challenges; Quick check - Track performance across AIME, MATH-500, and AIMO datasets
- **Two-Stage Training Pipeline:** Why needed - Combines accuracy maximization with efficiency optimization; Quick check - Measure accuracy retention and efficiency gains after each stage

## Architecture Onboarding

**Component Map:** Data → SFT Training (10 epochs) → GRPO Fine-tuning → Evaluation (AIME/MATH-500/AIMO)

**Critical Path:** The critical path follows sequential dependency: data preparation must complete before SFT, SFT must complete before GRPO fine-tuning, and evaluation only occurs after both training stages finish.

**Design Tradeoffs:** The primary tradeoff involves training time versus performance gains - extended SFT (10 epochs) provides accuracy benefits but increases computational cost, while GRPO adds another training phase focused on efficiency rather than accuracy improvements.

**Failure Signatures:** Common failure modes include overfitting during prolonged SFT (indicated by validation accuracy plateau or decline), GRPO instability causing accuracy degradation, and benchmark-specific weaknesses suggesting gaps in mathematical coverage.

**First Experiments:** 1) Run SFT ablation study varying epochs from 1-10 to identify optimal duration for accuracy gains, 2) Implement controlled GRPO fine-tuning with different reward structures to optimize token efficiency, 3) Conduct cross-dataset validation to ensure generalization across mathematical problem types.

## Open Questions the Paper Calls Out
None

## Limitations
- Approach validated exclusively on mathematical reasoning tasks, limiting generalizability to other domains
- No systematic analysis of optimal SFT duration or identification of diminishing returns thresholds
- GRPO stage focuses primarily on token efficiency rather than accuracy improvements, limiting applicability when accuracy is the primary objective
- Evaluation relies on specific benchmarks that may not represent full diversity of mathematical problem types

## Confidence
- **High Confidence:** Empirical results demonstrating improved performance on tested benchmarks are well-supported by data
- **Medium Confidence:** Claim that prolonged SFT is "crucial" for accuracy breakthroughs is supported but lacks systematic exploration of mechanisms
- **Medium Confidence:** Assertion that GRPO primarily optimizes solution length rather than accuracy is supported but could benefit from more detailed ablation studies

## Next Checks
1. Conduct ablation studies varying SFT duration systematically to identify optimal epochs and determine when diminishing returns begin, testing whether the claimed "crucial" nature of prolonged SFT holds across different model scales and mathematical subdomains.

2. Test the two-stage recipe on non-mathematical reasoning tasks (e.g., code generation, logical reasoning, or scientific problem-solving) to assess domain generalizability beyond the mathematical focus.

3. Implement controlled experiments comparing the two-stage approach against alternative RLHF methods (like PPO or DPO) to isolate whether the specific benefits come from the SFT-GRPO combination or could be achieved through other training recipes.