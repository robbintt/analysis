---
ver: rpa2
title: 'SAGE: An Agentic Explainer Framework for Interpreting SAE Features in Language
  Models'
arxiv_id: '2511.20820'
source_url: https://arxiv.org/abs/2511.20820
tags:
- feature
- features
- sage
- explanation
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of interpreting features learned
  by sparse autoencoders (SAEs) in language models. While SAEs can decompose complex
  neural activations into interpretable features, understanding what these features
  represent remains difficult.
---

# SAGE: An Agentic Explainer Framework for Interpreting SAE Features in Language Models

## Quick Facts
- arXiv ID: 2511.20820
- Source URL: https://arxiv.org/abs/2511.20820
- Authors: Jiaojiao Han; Wujiang Xu; Mingyu Jin; Mengnan Du
- Reference count: 24
- Primary result: 29-458% improvements in generative accuracy over Neuronpedia baseline

## Executive Summary
SAGE introduces an agentic framework that transforms feature interpretation from passive observation into active scientific experimentation. The system uses a multi-turn loop where an Explainer LLM generates hypotheses, a Designer LLM creates targeted test sentences, an Analyzer LLM observes empirical activation results, and a Reviewer LLM decides whether to accept, reject, refine, or refute each hypothesis. Experiments show SAGE achieves 29-458% improvements in generative accuracy and consistent gains in predictive accuracy compared to Neuronpedia across multiple language models and layers.

## Method Summary
SAGE implements an iterative hypothesis testing framework for interpreting SAE features. The process begins by extracting top-k activating text segments from a corpus, then generates multiple parallel explanations for each feature. Through a multi-turn loop, the Designer LLM creates targeted test sentences to validate hypotheses, the Analyzer LLM measures activation responses, and the Reviewer LLM determines state transitions (Accept/Reject/Refine/Refute) based on empirical evidence. The framework maintains multiple explanations to handle polysemantic features and synthesizes accepted hypotheses into final interpretations.

## Key Results
- 29-458% improvements in generative accuracy compared to Neuronpedia baseline
- Consistent predictive accuracy gains across all tested models and layers
- Most pronounced improvements at deeper layers (layer 23: 0.67 vs Neuronpedia's 0.12)
- Effective handling of polysemantic features through multiple parallel explanations

## Why This Works (Mechanism)

### Mechanism 1
Iterative hypothesis testing with empirical activation feedback improves explanation quality over single-pass generation. The multi-turn loop generates test sentences → measures SAE activations → evaluates hypothesis validity → refines or rejects based on evidence, converting interpretation from pattern-matching into hypothesis-driven experimentation.

### Mechanism 2
Maintaining multiple parallel explanations captures polysemantic features more effectively than single-explanation approaches. Rather than committing to one interpretation, SAGE maintains {H1, ..., Hn} hypotheses and validates each independently, allowing distinct facets to be discovered and preserved through synthesis.

### Mechanism 3
Deeper layers benefit more from SAGE's iterative approach than single-pass methods. Layer 23 shows 458% improvement (Neuronpedia: 0.12 → SAGE: 0.67), suggesting that abstract, high-level features require more systematic testing to characterize.

## Foundational Learning

- **Sparse Autoencoders (SAEs)**: Why needed - SAGE interprets SAE features. Quick check - Can you explain why SAEs use L1 penalty and how dsae ≫ dmodel enables decomposition?
- **Feature Activation Patterns**: Why needed - SAGE's empirical feedback loop relies on interpreting activation magnitudes as evidence. Quick check - If a test sentence produces activation ai = 0.3 on feature fj, what does this tell you about your hypothesis?
- **Polysemantic vs Monosemantic Features**: Why needed - SAGE's design explicitly handles features that activate on multiple distinct patterns. Quick check - Why might maintaining multiple parallel explanations be necessary for some features but not others?

## Architecture Onboarding

- **Component map**: Explainer LLM -> Designer LLM -> Analyzer LLM -> Reviewer LLM
- **Critical path**: 1. Extract top-k exemplars from corpus 2. Explainer generates n=4 hypotheses 3. Loop: Designer generates test text → measure activation → Analyzer evaluates → Reviewer decides state 4. Terminal state reached → Reviewer synthesizes final explanation
- **Design tradeoffs**: k=10 explanations vs computational cost (ablation shows k>10 yields diminishing returns), more iterations improve precision but increase token consumption (~26,500 tokens/turn), multiple hypotheses improve polysemantic coverage but require parallel tracking
- **Failure signatures**: Hypotheses that never reach terminal state suggest unclear feature semantics, high predictive but low generative accuracy suggests overfitting, contradictory evidence may indicate truly polysemantic behavior OR unreliable feature
- **First 3 experiments**: 1. Replicate layer comparison across layers 3, 7, 11, 23 for single model 2. Test polysemantic handling with known multi-language code constructs 3. Ablate Designer by replacing targeted generation with random sentences

## Open Questions the Paper Calls Out
None

## Limitations
- Practical relevance of generative accuracy improvements uncertain without downstream task validation
- Ability to distinguish true polysemantic behavior from noisy features remains unclear
- Framework's generalizability across different model architectures and training regimes unverified

## Confidence

**High confidence**: The iterative hypothesis testing mechanism and multi-turn loop architecture are well-defined and empirically validated through systematic improvements across multiple layers and models.

**Medium confidence**: Superiority over Neuronpedia baselines is demonstrated, but practical significance of generative accuracy improvements without downstream task validation introduces uncertainty.

**Low confidence**: Framework's ability to reliably distinguish polysemantic features from noisy features, and generalizability of explanations across different model architectures, remain unverified.

## Next Checks

1. **Downstream Task Validation**: Apply SAGE-interpreted features to concrete applications like model steering, circuit discovery, or safety interventions. Measure whether improved interpretability correlates with better intervention success rates or more precise control over model behavior.

2. **Feature Reliability Assessment**: Systematically evaluate whether SAGE explanations persist across model fine-tuning, temperature variations, and different prompt distributions. Identify which features produce stable versus unstable interpretations and characterize failure modes.

3. **Cross-Model Generalization**: Test SAGE on models with different architectures (RNNs, transformers of varying sizes) and training objectives. Determine whether the framework's iterative methodology generalizes or requires architecture-specific adaptations for optimal performance.