---
ver: rpa2
title: 'The Price of Format: Diversity Collapse in LLMs'
arxiv_id: '2505.18949'
source_url: https://arxiv.org/abs/2505.18949
tags:
- diversity
- template
- simple
- full
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Instruction-tuned large language models use structured prompts\
  \ to improve format consistency, but this comes at the cost of reduced output diversity\u2014\
  a phenomenon we term \"diversity collapse.\" Across multiple models and tasks, structured\
  \ prompts yield significantly lower semantic and topical diversity than simple,\
  \ unstructured prompts, even under high-temperature sampling. This collapse is driven\
  \ by structural tokens and formatting cues that act as behavioral anchors, narrowing\
  \ the model's output space and reducing entropy early in generation."
---

# The Price of Format: Diversity Collapse in LLMs

## Quick Facts
- **arXiv ID:** 2505.18949
- **Source URL:** https://arxiv.org/abs/2505.18949
- **Reference count:** 23
- **Key outcome:** Structured prompts cause diversity collapse in LLMs, reducing semantic and topical variety even under high-temperature sampling, driven by structural tokens acting as behavioral anchors.

## Executive Summary
Instruction-tuned LLMs use structured prompts to improve format consistency, but this comes at the cost of reduced output diversity—a phenomenon termed "diversity collapse." Across multiple models and tasks, structured prompts yield significantly lower semantic and topical diversity than simple, unstructured prompts, even under high-temperature sampling. This collapse is driven by structural tokens and formatting cues that act as behavioral anchors, narrowing the model's output space and reducing entropy early in generation. The best diversity is achieved with fully structure-free prompting, while explicit creativity prompts or mixed templates provide limited improvement.

## Method Summary
The study evaluates five instruction-tuned models (Llama-3-8B-Instruct, Tulu-3-8B-SFT, Qwen2.5-7B-Instruct, Mistral-7B-Instruct-v0.1, Phi-3.5-mini-instruct) across nine tasks using four prompt modes: Full Template, Fake Template, Minimal Dialog, and Simple Steer. Semantic diversity is measured via pairwise cosine distances of sentence embeddings (all-MiniLM-L6-v2), while label diversity uses normalized entropy over GPT-4o-extracted entities. Fine-tuning uses LLaMA-3.2-3B with 3 epochs, and downstream benchmarks include MMLU, GSM8K, HumanEval, WebQS, IFEval, and WSC273. Inference uses temperature T=1.0 and top-p=0.9.

## Key Results
- Structured prompts reduce semantic diversity by up to 65% compared to simple prompts
- Diversity collapse persists even under high-temperature sampling (T=1.0, top-p=0.9)
- Format consistency matters for structure-sensitive tasks (GSM8K, IFEval) but not knowledge-heavy tasks (MMLU, WebQuestions)
- Simple Steer prompts achieve maximum diversity but reduce response quality on some tasks

## Why This Works (Mechanism)

### Mechanism 1: Structural Tokens as Behavioral Anchors
Instruction-tuned models internalize template patterns as strong priors during training. At inference, structural tokens trigger these priors, reducing entropy at initial decoding steps and causing premature commitment to narrow generation trajectories. This anchoring effect persists regardless of decoding randomness.

### Mechanism 2: Format Consistency Requirements Differ by Task Sensitivity
Structure-sensitive tasks (math, instruction-following) rely on learned formatting patterns to trigger appropriate reasoning modes. Knowledge-heavy tasks depend more on internal representations formed during pretraining, which are less sensitive to scaffolding. This explains why format mismatch affects performance asymmetrically across task types.

### Mechanism 3: High-Temperature Sampling Cannot Compensate for Template Constraints
Templates shift the entire output distribution toward narrower modes before sampling occurs. Temperature only affects how samples are drawn from this already-constrained distribution, explaining why high-temperature settings provide limited diversity recovery when structured templates are used.

## Foundational Learning

- **Concept: Entropy in Autoregressive Generation**
  - **Why needed here:** The paper measures token-level entropy to explain how templates constrain output space. Understanding entropy as a measure of distribution uncertainty is essential.
  - **Quick check question:** At a given decoding step, if entropy = 0.5 vs entropy = 2.5, which indicates the model is more uncertain about the next token?

- **Concept: Instruction Tuning vs. Pretraining**
  - **Why needed here:** The paper distinguishes effects from instruction tuning (format learning) versus pretraining (knowledge acquisition). Task performance varies based on which training phase the task depends on.
  - **Quick check question:** Would you expect a base model (pretrained only) to follow multi-step formatting instructions without instruction tuning?

- **Concept: Semantic Diversity Metrics**
  - **Why needed here:** The paper uses embedding-based distance and topic entropy rather than n-gram metrics. Understanding why semantic metrics better capture meaningful variation is important for interpreting results.
  - **Quick check question:** Two responses with different words but identical meaning—would semantic embedding distance capture this similarity?

## Architecture Onboarding

- **Component map:** Prompt formatting layer -> Instruction-tuned model -> Decoding layer (temperature/top-p sampling) -> Evaluation suite
- **Critical path:** Select prompting strategy based on task type (structure-sensitive vs. knowledge-heavy) → If diversity is critical, prefer Simple Steer or Natural Instruction → If instruction-following precision is critical, match inference format to training format → Do not rely on temperature alone to recover diversity when using templates
- **Design tradeoffs:** Diversity vs. response quality (Simple Steer improves diversity but reduces AlpacaEval win rates); Format consistency vs. flexibility (mixed templates underperform because structural homogeneity, not repetition, drives collapse); Explicit creativity prompts vs. structural changes (prompting for creativity within templates shows limited improvement)
- **Failure signatures:** Generated outputs cluster around same topics despite varied prompts; Structural metrics (token count, sentence count) show low variance across generations; Temperature scaling produces marginal diversity gains
- **First 3 experiments:** 1) Establish baseline collapse: Generate 256 samples with Full Template vs. Simple Steer on story completion; compute semantic diversity scores; 2) Isolate anchoring component: Compare Minimal Dialog vs. Simple Steer to quantify lightweight structure costs; 3) Validate task-type hypothesis: Evaluate same model on GSM8K and MMLU with format-matched vs. mismatched prompts

## Open Questions the Paper Calls Out

- **Open Question 1:** Does diversity collapse manifest differently in chain-of-thought (CoT) or retrieval-augmented generation (RAG) settings? The study only compared chat-style and simple-steer templates, leaving effects of CoT or RAG prompting strategies for future work.

- **Open Question 2:** How does reduced utterance-level diversity impact discourse-level coherence and the downstream utility of generated text? The study relied on automated semantic and lexical metrics at the utterance level, leaving discourse-level variation and downstream impact unexplored.

- **Open Question 3:** Can instruction tuning be disentangled from knowledge degradation to preserve factual recall while maintaining alignment? The paper identifies an "alignment tax" on knowledge but does not test methods to mitigate this specific trade-off within the instruction-tuning framework.

## Limitations

- The causal mechanism attributing diversity collapse to structural tokens as "behavioral anchors" remains somewhat speculative, with limited direct ablation studies isolating specific token effects
- Task-type classification (structure-sensitive vs. knowledge-heavy) is based on limited benchmark tasks without a systematic taxonomy for generalization
- Temperature interaction experiments are limited to fixed T=1.0 and top-p=0.9, not exploring extreme temperature ranges

## Confidence

- **High confidence:** The empirical observation of diversity collapse across multiple models and tasks is well-supported by experimental results
- **Medium confidence:** The explanation that structural tokens act as behavioral anchors constraining output space is plausible but mechanistically inferential
- **Low confidence:** The task-type classification system and its implications for prompt design are the most speculative elements

## Next Checks

1. **Token-level ablation study:** Systematically remove or replace specific token types (special tokens, role markers, formatting cues) to isolate which components drive the diversity collapse.

2. **Dynamic template modification experiment:** Implement prompts that allow the model to modify its own formatting during generation (e.g., starting with structured format then transitioning to unstructured).

3. **Cross-task format sensitivity mapping:** Design a systematic evaluation across a broader range of tasks to establish a quantitative measure of format sensitivity and create a generalizable task taxonomy.