---
ver: rpa2
title: 'Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks
  to Applications'
arxiv_id: '2510.17764'
source_url: https://arxiv.org/abs/2510.17764
tags:
- arxiv
- language
- clinical
- medical
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey reframes medical LLM evaluation around levels of autonomy\
  \ (L0\u2013L3) to align evidence with clinical roles. By mapping benchmarks and\
  \ metrics to what systems are permitted to do, it clarifies that L0\u2013L1 must\
  \ focus on factual grounding, bias, and structural correctness, while L2\u2013L3\
  \ require calibrated uncertainty, selective answering, subgroup robustness, tool-use\
  \ correctness, and verified human oversight."
---

# Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications

## Quick Facts
- **arXiv ID:** 2510.17764
- **Source URL:** https://arxiv.org/abs/2510.17764
- **Reference count:** 40
- **Primary result:** Proposes a levels-of-autonomy framework (L0–L3) to align medical LLM evaluation with clinical risk, shifting focus from average accuracy to risk-coverage trade-offs.

## Executive Summary
This survey reframes medical LLM evaluation around levels of autonomy (L0–L3) to align evidence with clinical roles. By mapping benchmarks and metrics to what systems are permitted to do, it clarifies that L0–L1 must focus on factual grounding, bias, and structural correctness, while L2–L3 require calibrated uncertainty, selective answering, subgroup robustness, tool-use correctness, and verified human oversight. It advocates reporting risk-coverage rather than average accuracy, ensuring systems act only when a target risk can be met and otherwise defer. The result is a practical blueprint for credible, risk-aware evaluation that moves beyond scores toward safe, reliable clinical use.

## Method Summary
The paper is a survey that defines a levels-of-autonomy taxonomy (L0–L3) for medical LLM evaluation. It maps existing benchmarks (e.g., MedQA, HealthBench) to these levels and recommends level-specific metrics: factual grounding, attribution, and readability for L0/L1; ECE, Brier score, and risk-coverage curves for L2; tool-use correctness and auditability for L3. It also outlines human evaluation protocols and selective prediction strategies to enable safe deferral. The work does not introduce new models or training procedures but provides a blueprint for aligning evaluation with clinical risk.

## Key Results
- Reframes evaluation through a levels-of-autonomy lens (L0–L3), making evaluation targets explicit per clinical role.
- Identifies cumulative, not isolated, challenges: each higher autonomy level inherits unresolved issues from lower levels.
- Advocates for risk-coverage reporting over average accuracy to ensure systems act only when a target risk can be met and otherwise defer.

## Why This Works (Mechanism)

### Mechanism 1: Autonomy-Aligned Evidence Mapping
- **Claim:** Evaluating medical LLMs based on permitted clinical actions (autonomy levels) rather than generic capability scores creates a verifiable safety boundary.
- **Mechanism:** The framework segments system functionality into four tiers (L0–L3). It posits that as autonomy increases (e.g., from answering general questions to drafting orders), the evidence requirements escalate from factual correctness to calibrated uncertainty and auditable provenance. This prevents high-stakes actions from relying on low-stakes evidence.
- **Core assumption:** The risks associated with clinical AI are discrete and level-dependent, rather than uniform across all tasks.
- **Evidence anchors:**
  - [abstract] "This survey reframes evaluation through a levels-of-autonomy lens (L0–L3)... making the evaluation targets explicit."
  - [section 4] "This organization makes evaluation targets explicit: factual grounding at L0/L1; calibrated reasoning and coverage at L2; tool-use safety and auditability at L3."
  - [corpus] Related work (CARES, MedQA-CS) supports the need for specialized evaluation beyond general benchmarks, though specific L0-L3 taxonomies are unique to this survey.
- **Break condition:** If risks are found to be primarily driven by context rather than autonomy level (e.g., an L0 "informational" query causes harm due to misinterpretation), the strict level-segmented evaluation strategy may under-protect low-autonomy tiers.

### Mechanism 2: Risk-Coverage Trade-offs over Average Accuracy
- **Claim:** Replacing average accuracy with risk-coverage curves incentivizes safe abstention, aligning model behavior with clinical caution.
- **Mechanism:** Instead of forcing a model to answer all inputs, this mechanism evaluates the "selective prediction" capability. By plotting the error rate (risk) against the fraction of questions answered (coverage), systems are tuned to act only when a pre-defined confidence threshold is met.
- **Core assumption:** Clinical workflows can tolerate unautomated cases (deferral) in exchange for higher reliability in automated cases.
- **Evidence anchors:**
  - [abstract] "...reporting risk-coverage rather than average accuracy, ensuring systems act only when a target risk can be met and otherwise defer."
  - [section 5] "Reliable high accuracy on a subset with explicit deferral... is preferable to broad moderate accuracy."
  - [corpus] Corpus neighbors (e.g., Trustworthy Medical QA) highlight reliability and safety, consistent with this shift, but do not explicitly validate the superiority of risk-coverage curves in live deployment.
- **Break condition:** If deferral rates become too high (low coverage), the system ceases to be useful; if the confidence estimator is miscalibrated, the risk-coverage plot misrepresents actual safety.

### Mechanism 3: Cumulative Challenge Inheritance
- **Claim:** Reliability at higher autonomy levels (L2/L3) implies successful mitigation of lower-level (L0/L1) failure modes.
- **Mechanism:** The survey models challenges (hallucination, bias, attribution) as cumulative. An L3 agent (e.g., one drafting orders) relies on L1 extraction and L2 reasoning; thus, evaluation at L3 must verify that L0/L1 failures (like missing a contraindication in the source text) do not propagate into action.
- **Core assumption:** System architecture is modular, and errors in early stages (retrieval/extraction) deterministically degrade downstream safety.
- **Evidence anchors:**
  - [section 1] "Challenges in each level... are cumulative, not isolated: each higher autonomy level inherits unresolved issues from lower levels."
  - [section 4.3] "L2 depends on upstream EHR transformation and retrieval... but adds reasoning..."
  - [corpus] MedEinst paper notes models rely on shortcuts (low-level flaws) causing misdiagnosis (high-level failure), supporting the inheritance hypothesis.
- **Break condition:** If higher-level reasoning modules can independently correct for lower-level input errors (e.g., an agent realizing a retrieved document is irrelevant), the strict cumulative risk assumption over-estimates failure rates.

## Foundational Learning

- **Concept: Calibration (Expected Calibration Error - ECE)**
  - **Why needed here:** Essential for L2 Decision Support. The paper argues that accuracy is insufficient; the model must "know when it doesn't know." ECE measures the gap between model confidence and actual correctness.
  - **Quick check question:** If a model assigns 80% confidence to 100 predictions, did it get exactly 80 of them right? (If yes, it is well-calibrated).

- **Concept: Risk-Coverage Curves**
  - **Why needed here:** This is the proposed metric for selective prediction. It visualizes the trade-off between the fraction of inputs handled (coverage) and the error rate (risk).
  - **Quick check question:** To achieve a target error rate of <5%, what percentage of cases must the model defer to a human (coverage drop)?

- **Concept: Attribution / Grounding**
  - **Why needed here:** Critical for L1 (Transformation) and RAG. It ensures that generated summaries or answers are strictly derived from retrieved evidence, preventing hallucinations.
  - **Quick check question:** Can every claim in the model's output be explicitly linked to a specific segment in the source document?

## Architecture Onboarding

- **Component map:**
  - Input: Clinical Query or EHR Data.
  - L0/L1 Core: Knowledge Base (RAG) + Extraction Engine (NER/Summarization). *Focus: Faithfulness, Completeness.*
  - L2 Reasoning: Diagnostic/Predictive Layer. *Focus: Calibration, Fairness, Subgroup robustness.*
  - L3 Action Layer: Tool/API Interface (e.g., Draft Order Generator). *Focus: Auditability, Oversight, Tool correctness.*
  - Oversight: Human-in-the-loop review gate (mandatory for L3).

- **Critical path:** The evaluation pipeline depends on mapping a proposed application to an Autonomy Level (L0–L3) *before* selecting metrics. Do not apply L2 calibration metrics to an L0 informational tool; do not deploy L3 agents without L1 extraction verification.

- **Design tradeoffs:**
  - **Utility vs. Safety:** Increasing autonomy (L0 -> L3) increases workflow utility but introduces cumulative risks (tool failure, actionability) requiring expensive oversight.
  - **Coverage vs. Risk:** High coverage (answering more questions) typically increases risk (error rate) unless the model is exceptionally well-calibrated.

- **Failure signatures:**
  - **L0/L1 Failure:** "Hallucinated constraints" (inventing details in a summary) or "Attribution error" (citing a source that doesn't support the claim).
  - **L2 Failure:** "Overconfidence" (recommending a harmful drug with high probability) or "Subgroup shift" (working for majority populations but failing on edge cases).
  - **L3 Failure:** "Tool invocation error" (calling the wrong API endpoint) or "Automation bias" (clinician accepting a draft order without checking).

- **First 3 experiments:**
  1. **Level Classification:** Take your current medical LLM task and classify it as L0, L1, L2, or L3 based on the "Scope" definitions (e.g., does it give advice? does it invoke tools?).
  2. **Risk-Coverage Profiling (L2 focus):** Run the model on a validation set, plot risk vs. coverage by thresholding on confidence, and identify the "deferral rate" required to meet a clinically safe error target (e.g., <1% error).
  3. **Attribution Audit (L1 focus):** Manually verify 20 random citations/extractions from the model to verify they are faithful to the source text and do not contain "intrinsic" hallucinations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the field standardize risk–coverage reporting to prioritize safe deferral over broad average accuracy in clinical evaluations?
- **Basis in paper:** [explicit] Section 5 states the need to reframe "good" performance around risk-controlled selectivity, noting that systems should act only when a target risk is met and otherwise defer.
- **Why unresolved:** Current benchmarks prioritize aggregate accuracy scores. Risk-coverage curves are difficult to compare across tasks without principled area measures, and standard target risk levels for clinical tasks are undefined.
- **What evidence would resolve it:** Consensus on task-specific risk thresholds and the adoption of evaluation plots showing accuracy versus deferral rates (coverage) in major medical LLM benchmarks.

### Open Question 2
- **Question:** How can evaluation methodologies reliably distinguish between plausible yet unfaithful rationales and genuine causal reasoning in L2 decision support systems?
- **Basis in paper:** [explicit] Section 4.3 identifies reasoning consistency and faithfulness as a key challenge, noting that generated "explanations" can be plausible yet unfaithful to the features that truly drove the prediction.
- **Why unresolved:** Chain-of-thought prompting improves transparency but does not guarantee faithful causal grounding, leaving a gap between textual justification and actual model behavior.
- **What evidence would resolve it:** Evaluation frameworks that correlate generated rationales with counterfactual sensitivity tests or internal attention mechanisms to verify that stated reasons align with decision drivers.

### Open Question 3
- **Question:** How can evaluation frameworks effectively quantify human–AI interaction risks, such as automation bias and alert fatigue, in L3 supervised agents?
- **Basis in paper:** [explicit] Section 4.4 lists human–AI interaction dynamics as a challenge, citing studies that show overreliance on automated suggestions and alert fatigue where users ignore or over-accept outputs.
- **Why unresolved:** Current evaluation focuses on agent correctness (tool-use, answer quality) rather than the human's ability to sustain critical review during busy clinical workflows.
- **What evidence would resolve it:** User studies measuring error detection rates and correction behaviors in "supervised" vs. fully manual workflows over extended periods.

### Open Question 4
- **Question:** What simple, auditable protocols are required for handoff, disagreement resolution, and safety gating in closed-loop, hospital-scale multi-agent systems?
- **Basis in paper:** [explicit] Section 5 calls for "simple, auditable protocols" for handoff and disagreement resolution as deployments shift from single-model helpers to hospital-scale systems with role-specialized agents.
- **Why unresolved:** Current evaluation is model-centric; moving to system-level governance involves complex inter-agent interactions and escalation policies not covered by standard benchmarks.
- **What evidence would resolve it:** Operational outcome reporting (deferral rates, near-misses) from shadow deployments of multi-agent systems utilizing these protocols.

## Limitations

- The autonomy-level taxonomy, while intuitive, lacks empirical validation against real-world harm data to confirm that risks are truly discrete and level-dependent rather than context-dependent.
- The proposed shift from average accuracy to risk-coverage curves assumes clinical workflows can tolerate high deferral rates, but the survey does not quantify the utility threshold below which systems become clinically irrelevant.
- Human evaluation components (BARS-style rubrics, rater training protocols) are referenced but not provided in full detail, creating a gap between the theoretical framework and practical implementation.

## Confidence

- **High Confidence:** The core mechanism of aligning evaluation targets with autonomy levels (L0–L3) is internally consistent and addresses a real problem in medical LLM assessment. The mapping of factual grounding to L0/L1 and calibration/uncertainty to L2/L3 follows logically from the clinical risk hierarchy.
- **Medium Confidence:** The risk-coverage trade-off recommendation is theoretically sound and supported by selective prediction literature, but lacks direct validation in medical contexts showing that deferral-based systems achieve better clinical outcomes than traditional accuracy-focused approaches.
- **Low Confidence:** The cumulative challenge inheritance assumption (that L3 failures necessarily imply L0/L1 failures) is plausible but not empirically proven. Higher-level reasoning modules may be capable of detecting and correcting lower-level errors, breaking the strict inheritance chain.

## Next Checks

1. **Empirical Risk Mapping Study:** Conduct a retrospective analysis of actual medical AI incidents to determine whether harms correlate more strongly with autonomy levels or with specific contextual factors (patient acuity, information complexity). This would validate or invalidate the discrete risk-level hypothesis.

2. **Clinical Utility Threshold Testing:** Deploy a calibrated selective-prediction medical LLM in a simulated clinical environment and measure both clinical outcomes and workflow efficiency across different deferral thresholds. Determine the minimum coverage level that maintains clinical utility while achieving target safety.

3. **Error Propagation Experiment:** Systematically inject controlled errors at L0/L1 stages (hallucinations in retrieval, extraction errors) and measure whether these errors propagate through L2 reasoning to L3 actions, or whether higher-level modules can independently detect and correct them.