---
ver: rpa2
title: 'FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts
  for General Continual Learning'
arxiv_id: '2602.01976'
source_url: https://arxiv.org/abs/2602.01976
tags:
- uni00000013
- uni00000011
- uni00000015
- uni00000018
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FlyPrompt addresses the challenge of general continual learning
  (GCL), where models must learn from single-pass, non-stationary data streams without
  clear task boundaries. Inspired by the fruit fly's hierarchical memory system, it
  decomposes GCL into expert routing and expert competence improvement.
---

# FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning

## Quick Facts
- arXiv ID: 2602.01976
- Source URL: https://arxiv.org/abs/2602.01976
- Reference count: 40
- Primary result: Achieves up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200 respectively

## Executive Summary
FlyPrompt addresses general continual learning by drawing inspiration from the fruit fly's hierarchical memory system. It decomposes the challenge into expert routing and expert competence improvement, introducing a randomly expanded analytic router (REAR) for instance-level expert activation and a temporal ensemble of output heads (TE²) to dynamically adapt decision boundaries over time. The method achieves significant performance improvements across multiple datasets and pretrained model paradigms without requiring gradient-based routing updates.

## Method Summary
FlyPrompt implements a continual learning system with three core components: a Random Expanded Analytic Router (REAR) that performs forward-only expert selection via closed-form ridge regression on high-dimensional random projections, a Temporal Ensemble (TE²) that maintains multiple exponential moving average heads per expert to capture multi-timescale decision boundaries, and an average-prompt warm-start strategy for initializing new experts. The system operates on a frozen ViT-B/16 backbone with prompts in the first five layers, routing each input to an appropriate expert and aggregating predictions from the online head plus EMA heads using SoftMax + Max probability.

## Key Results
- Achieves 11.23%, 12.43%, and 7.62% improvements over SOTA baselines on CIFAR-100, ImageNet-R, and CUB-200 respectively
- Outperforms existing methods across all pretrained model paradigms (Sup-21K, Sup-21K/1K, MoCo, iBOT, DINO)
- Demonstrates consistent gains in anytime accuracy (A_auc), final accuracy (A_last), and forgetting metrics (F_last)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** REAR enables forward-only expert selection without gradient-based routing updates
- **Mechanism:** Maps backbone features to high-dimensional sparse space via fixed random projection and ReLU, maintaining running statistics and solving closed-form ridge regression for router weights
- **Core assumption:** Sufficiently large expansion dimension M maintains linear separability of expanded features
- **Evidence anchors:** [Section 3.1, Eq. 1-5] defines the projection and analytic solution; [Theorem 1] formalizes excess risk decomposition; [Corpus] shows analytic approach breaks dependency on backpropagation
- **Break condition:** If expansion dimension M is too low or regularization λ is ill-tuned, router fails to distinguish expert assignments

### Mechanism 2
- **Claim:** TE² heads capture multi-timescale decision boundaries to handle representation drift and class imbalance
- **Mechanism:** Each expert maintains one online head and n EMA heads with varying decay rates; at inference, computes logits from all heads, applies SoftMax independently, and takes element-wise maximum
- **Core assumption:** Data stream exhibits non-stationary dynamics where different effective window sizes are optimal at different times
- **Evidence anchors:** [Section 3.2, Eq. 7-10] defines EMA update and SoftMax + Max ensemble; [Table 5] shows SoftMax+Max Prob outperforms mean aggregation
- **Break condition:** If stream is purely static, EMA variance adds noise; if stream shifts violently without recurring patterns, EMA heads lag too far behind online head

### Mechanism 3
- **Claim:** Average-prompt warm-start improves expert competence under single-pass, blurry boundaries
- **Mechanism:** New expert prompt initialized as average of all previous prompts, providing better prior than random initialization
- **Core assumption:** Tasks share semantic manifold, making centroid of previous prompt parameters closer to good initialization than random point
- **Evidence anchors:** [Section 3.2] describes informed initialization and empirical acceleration; [Fig 2c] shows baseline experts lack competence
- **Break condition:** If new tasks are semantically orthogonal to all previous tasks, averaged prompt may induce negative transfer

## Foundational Learning

- **Concept:** Ridge Regression / Analytic Classifiers
  - **Why needed here:** REAR relies entirely on solving regularized least-squares problem in closed form rather than gradient descent
  - **Quick check question:** How does adding λI to Gram matrix G affect eigenvalues and stability of inverse?

- **Concept:** Random Projections & Sparsity
  - **Why needed here:** Fly inspiration uses fixed random matrix to expand dimensionality followed by nonlinearity to induce sparsity, separating overlapping inputs without learning
  - **Quick check question:** Why does high-dimensional sparse representation help in separating similar inputs for routing?

- **Concept:** EMA (Exponential Moving Average) dynamics
  - **Why needed here:** TE² uses EMA heads to smooth updates; decay rate α controls trade-off between stability and plasticity
  - **Quick check question:** If α=0.99, how many steps of past data does model effectively "remember"?

## Architecture Onboarding

- **Component map:** Input -> Backbone -> REAR (Assign Expert E_t) -> Expert (Retrieve Prompt p_t) -> Heads (Compute Logits) -> Ensemble (SoftMax + Max) -> Output
- **Critical path:** Input flows through frozen backbone to REAR for expert assignment, then to selected expert's prompt and head bank for logit computation, finally through ensemble aggregation
- **Design tradeoffs:** Higher M improves routing accuracy but increases memory for Gram matrix; analytic routing is stable but rigid compared to learned methods; "Max Prob" is more robust to outliers than "Mean" but might overfit to confident noise
- **Failure signatures:** Router collapse (uniform distribution predictions) indicates λ too high or M too low; forgetting (accuracy drops on old tasks) suggests EMA decay rates need adjustment; logit mismatch (predicting unseen classes) indicates batch seen-class mask not applied correctly
- **First 3 experiments:** 1) Validate REAR vs. Similarity by replicating Table 17 on specific data stream; 2) Hyperparameter sweep of M following Figure 5 analysis to find minimum saturating performance; 3) Ablate TE² by comparing "Online Head Only" vs. "+ 0.9, 0.99 EMA" following Table 6

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can temporal ensemble mechanism be modified to adapt EMA decay rates dynamically to varying rates of data drift?
- **Basis in paper:** [explicit] "For instance, the temporal ensemble relies on a fixed composition of EMA decay rates, and adapting these dynamically to data drift could enhance robustness"
- **Why unresolved:** Current implementation requires manual selection of fixed rates (e.g., 0.9, 0.99) which may not be optimal for all types of non-stationary streams
- **What evidence would resolve it:** Empirical results showing mechanism capable of adjusting decay rates in real-time outperforms fixed rates on streams with heterogeneous drift speeds

### Open Question 2
- **Question:** How robust is FlyPrompt under extreme long-tailed distributions compared to current benchmarks?
- **Basis in paper:** [explicit] "Additionally, performance under extreme long-tailed distributions warrants further study"
- **Why unresolved:** Paper evaluates on standard benchmarks (CIFAR-100, ImageNet-R, CUB-200) which don't fully represent challenges of severe class imbalance in real-world data
- **What evidence would resolve it:** Study evaluating FlyPrompt on specific long-tailed GCL datasets or metrics designed to measure performance on tail classes

### Open Question 3
- **Question:** How can trade-off between expansion dimension M and memory usage be optimized for resource-constrained environments?
- **Basis in paper:** [inferred] Authors note memory cost grows linearly with M, yet Theorem 1 suggests performance improves with higher M; paper sets M=10,000 as practical trade-off
- **Why unresolved:** Theoretical guarantee suggests arbitrarily high M improves routing, but practical implementation creates hard constraint on memory, leaving optimal balance for edge devices unexplored
- **What evidence would resolve it:** Analysis of performance degradation versus memory savings when using lower dimensions or compression techniques for random expansion matrix

### Open Question 4
- **Question:** Does brain-inspired routing mechanism scale effectively to significantly larger PTMs such as ViT-Large or ViT-Huge?
- **Basis in paper:** [inferred] Experiments restricted to ViT-Base/16 backbone across all training paradigms
- **Why unresolved:** While method claims general applicability to PTMs, routing and expert capacity improvements haven't been validated on models with substantially higher representational capacity
- **What evidence would resolve it:** Comparative benchmarks on larger backbone architectures to determine if relative gains over baselines remain consistent

## Limitations
- Performance gains depend critically on hyperparameter choices (λ, M, EMA decay rates) that may not generalize across domains
- M=10,000 expansion dimension creates O(M²) memory requirements for Gram matrix, potentially prohibitive for larger models
- Theoretical analysis provides guarantees for REAR component but lacks formal guarantees for full system regarding forgetting and catastrophic interference

## Confidence
- **High Confidence:** Analytic routing approach provides forward-only expert selection without gradient-based updates, directly implementable from closed-form solution and verified through comparative experiments
- **Medium Confidence:** Temporal ensemble improves performance through multi-timescale adaptation, supported by ablation studies showing benefit of EMA heads
- **Low Confidence:** Average-prompt warm-start strategy meaningfully accelerates convergence, relying primarily on qualitative statements about "informed initialization"

## Next Checks
1. **Router Sensitivity Analysis:** Systematically vary M and λ across datasets to identify breaking points where routing accuracy degrades, verifying Johnson-Lindenstrauss intuition about linear separability
2. **EMA Head Robustness:** Test TE² performance under extreme non-stationarity versus controlled gradual drift to quantify when EMA heads help versus hinder, validating SoftMax + Max aggregation choice
3. **Cross-Domain Transferability:** Apply FlyPrompt to domain-shifted dataset with same hyperparameters to assess whether reported gains extend beyond CIFAR-100/ImageNet-R/CUB-200 distribution