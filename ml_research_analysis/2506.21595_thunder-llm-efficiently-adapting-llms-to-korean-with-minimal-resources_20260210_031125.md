---
ver: rpa2
title: 'Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources'
arxiv_id: '2506.21595'
source_url: https://arxiv.org/abs/2506.21595
tags:
- korean
- training
- data
- table
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a cost-effective approach to adapting English-based
  LLMs to Korean. The method involves collecting and preprocessing Korean data, extending
  the tokenizer with Korean tokens, and performing continual pre-training followed
  by post-training using supervised fine-tuning and direct preference optimization.
---

# Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources

## Quick Facts
- arXiv ID: 2506.21595
- Source URL: https://arxiv.org/abs/2506.21595
- Reference count: 40
- Cost-effective approach to adapt English-based LLMs to Korean with minimal resources

## Executive Summary
This paper presents a cost-effective approach to adapting English-based LLMs to Korean using significantly less data and computational resources than training from scratch. The method involves collecting and preprocessing Korean data, extending the tokenizer with Korean tokens, and performing continual pre-training followed by post-training using supervised fine-tuning and direct preference optimization. The resulting models, Thunder-LLM and Thunder-LLM-Ins, achieve the best performance on Korean benchmarks among similar-scale models while maintaining comparable English performance. This demonstrates an effective and efficient bilingual adaptation strategy for low-resource languages.

## Method Summary
The approach consists of several key steps: first, collecting and preprocessing Korean data to create a comprehensive corpus; second, extending the tokenizer with Korean-specific tokens to improve token coverage and efficiency; third, performing continual pre-training on the extended tokenizer using the Korean corpus; and finally, conducting post-training using supervised fine-tuning and direct preference optimization to fine-tune the model for Korean-specific tasks. This multi-stage process enables effective adaptation while minimizing resource requirements compared to full-scale training from scratch.

## Key Results
- Thunder-LLM achieves the best performance on Korean benchmarks among similar-scale models
- Models maintain comparable English performance while excelling in Korean tasks
- The approach uses significantly less data and computational resources compared to training from scratch

## Why This Works (Mechanism)
The success of this approach stems from its strategic combination of token-level optimization and task-specific fine-tuning. By extending the tokenizer with Korean tokens, the model gains better representation of Korean text, reducing the sequence length and improving computational efficiency. The continual pre-training phase allows the model to learn Korean language patterns from a large corpus, while the post-training phase using supervised fine-tuning and direct preference optimization adapts the model to Korean-specific tasks and preferences. This multi-stage approach efficiently leverages existing English LLM capabilities while effectively incorporating Korean language knowledge.

## Foundational Learning

1. **Tokenization and Subword Models**
   - Why needed: Proper tokenization is crucial for language representation and computational efficiency
   - Quick check: Verify Korean token coverage and sequence length reduction after tokenizer extension

2. **Continual Pre-training**
   - Why needed: Allows models to adapt to new languages while retaining knowledge from previous training
   - Quick check: Monitor language modeling loss on Korean corpus during pre-training

3. **Supervised Fine-tuning**
   - Why needed: Enables task-specific adaptation beyond general language understanding
   - Quick check: Track performance on Korean benchmark tasks during fine-tuning

4. **Direct Preference Optimization (DPO)**
   - Why needed: Refines model outputs based on human preferences and task-specific requirements
   - Quick check: Evaluate preference alignment on Korean task-specific datasets

## Architecture Onboarding

**Component Map:**
Tokenizer Extension -> Continual Pre-training -> Supervised Fine-tuning -> Direct Preference Optimization

**Critical Path:**
The critical path for adapting LLMs to Korean involves first extending the tokenizer to properly handle Korean text, followed by continual pre-training to learn Korean language patterns, and finally post-training through supervised fine-tuning and DPO to adapt to specific Korean tasks and preferences.

**Design Tradeoffs:**
- Token Extension vs. Computational Efficiency: More tokens improve Korean representation but increase model size
- Pre-training Data Quality vs. Quantity: Balancing comprehensive coverage with focused, high-quality Korean data
- Fine-tuning vs. Overfitting: Ensuring adaptation to Korean without losing English capabilities

**Failure Signatures:**
- Poor Korean token coverage leading to high sequence lengths and reduced efficiency
- Catastrophic forgetting of English capabilities during Korean adaptation
- Overfitting to specific Korean tasks during post-training

**3 First Experiments:**
1. Evaluate token coverage and sequence length before and after tokenizer extension
2. Monitor language modeling loss on Korean corpus during continual pre-training
3. Compare Korean and English benchmark performance after post-training

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on Korean benchmarks with limited testing on English performance
- Comparison with English-only models like LLaMA-7B is not comprehensive
- No detailed ablation study to isolate the impact of each component of the approach

## Confidence
- High: The approach is cost-effective and achieves competitive performance on Korean benchmarks
- Medium: The method maintains comparable English performance
- Low: The scalability of the approach to other languages and the long-term effects on model behavior

## Next Checks
1. Conduct a comprehensive evaluation on a wider range of English and multilingual benchmarks to verify the claim of maintained English performance
2. Perform an ablation study to isolate the impact of each component of the approach (tokenizer extension, continual pre-training, post-training) on model performance
3. Test the scalability of the approach to other low-resource languages and evaluate the long-term effects of the adaptation on model behavior