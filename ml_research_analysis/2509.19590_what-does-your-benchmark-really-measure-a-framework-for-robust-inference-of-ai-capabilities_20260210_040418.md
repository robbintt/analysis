---
ver: rpa2
title: What Does Your Benchmark Really Measure? A Framework for Robust Inference of
  AI Capabilities
arxiv_id: '2509.19590'
source_url: https://arxiv.org/abs/2509.19590
tags:
- benchmark
- question
- perturbations
- which
- ability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a principled framework for evaluating AI capabilities
  using benchmark data, treating evaluations as inferences about latent abilities
  rather than simple measurements. It identifies that conventional benchmarks violate
  statistical assumptions due to model sensitivity to input perturbations, leading
  to biased performance estimates.
---

# What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities

## Quick Facts
- arXiv ID: 2509.19590
- Source URL: https://arxiv.org/abs/2509.19590
- Reference count: 40
- The paper proposes a framework for evaluating AI capabilities using benchmark data, treating evaluations as inferences about latent abilities rather than simple measurements.

## Executive Summary
This paper argues that conventional AI benchmarks systematically bias capability estimates due to model sensitivity to input perturbations. The authors propose a principled framework that treats benchmark evaluations as inferences about latent abilities rather than simple measurements. They demonstrate that even large language models remain sensitive to input perturbations, leading to biased performance estimates. The framework introduces two inference strategies: clustered bootstrapping to estimate accuracy and an adaptive test based on item response theory to estimate latent ability, both accounting for perturbation sensitivity. Experiments show benchmarks can bias estimates by up to 15 percentage points, while the adaptive method significantly reduces sample complexity while providing more reliable ability estimates.

## Method Summary
The framework generates m natural perturbations per benchmark question to create pseudo-independent samples. For accuracy estimation, it uses clustered bootstrapping (CBA) that resamples questions but not perturbations within questions. For ability estimation, it extends IRT models to include perturbation sensitivity terms and uses an adaptive testing algorithm (LAAT) that selects questions maximizing Fisher information about latent ability. The method requires either no prior assumptions (CBA) or pre-calibrated item parameters (LAAT). Experiments were conducted on LMEntry, Big-Bench Hard, and GPQA using Llama-3.2, Qwen-2.5, Gemma, and GPT-4.1 models with ~150 distinct formatting perturbations per task.

## Key Results
- Benchmarks systematically bias capability estimates by up to 15 percentage points
- Adaptive testing reduces sample complexity by 4-27x compared to full benchmark evaluation
- Even large language models remain sensitive to input perturbations (MAD metrics show consistent sensitivity)
- The proposed framework provides uncertainty quantification and accounts for sensitivity in ability estimates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Benchmark estimates are systematically biased because phrasings are dependently sampled from the perturbation space.
- Mechanism: Under the CTT model, observed performance includes a perturbation-sensitivity term. Benchmark curation independently samples questions but dependently samples phrasings within each question, violating independence assumptions. Proposition 1 proves true performance is unidentifiable under dependent phrasing.
- Core assumption: Models have a stable "true performance" that exists in expectation over natural perturbations.
- Evidence anchors: Section 4.1 shows most benchmarks violate independence by dependently sampling phrasings.
- Break condition: If perturbations are generated via a deterministic process unrepresentative of natural variation, bias will persist.

### Mechanism 2
- Claim: Random perturbations create pseudo-independent samples that enable unbiased estimation of true accuracy via clustered bootstrapping.
- Mechanism: Generating m perturbations per question creates dataset enabling estimation of true performance. Proposition 5 proves consistency: if perturbations are independently sampled, estimates converge to true values. Algorithm 1 applies hierarchical bootstrap to construct valid confidence intervals.
- Core assumption: Perturbations are high-quality, semantically equivalent, and represent true distribution of natural variation.
- Evidence anchors: Section 4.2 describes generating m perturbations per question to create pseudo-independent samples.
- Break condition: If perturbation quality is poor, estimated performance will reflect question quality confounds rather than model capability.

### Mechanism 3
- Claim: Adaptive testing based on Item Response Theory reduces sample complexity while accounting for perturbation sensitivity.
- Mechanism: Extends IRT model to include perturbation terms. Proposition 7 shows averaging over perturbations eliminates sensitivity from marginal distribution. Algorithm 2 selects questions maximizing Fisher information and updates ability estimates via Newton-Raphson.
- Core assumption: Item parameters are properly calibrated to true data-generating process via representative model populations.
- Evidence anchors: Section 5.2 describes selecting questions via Fisher information; Figure 4 shows sample reduction.
- Break condition: If item parameters are misspecified or IRT model does not match true data-generating process, ability estimates will be biased.

## Foundational Learning

- Concept: **Classical Test Theory (CTT)**
  - Why needed here: Provides baseline model underlying conventional accuracy reporting; understanding why assumptions fail for LLMs is essential for grasping the critique.
  - Quick check question: In CTT, what condition must the error term ε satisfy for θ to represent "true score"?

- Concept: **Item Response Theory (IRT) and Fisher Information**
  - Why needed here: Underpins adaptive testing mechanism; Fisher information quantifies how much a question reduces uncertainty about latent ability.
  - Quick check question: In the two-parameter IRT model, what do aᵢ (discrimination) and bᵢ (difficulty) parameterize?

- Concept: **Hierarchical Bootstrap / Clustered Sampling**
  - Why needed here: Standard bootstrap fails when data has hierarchical structure; resampling clusters maintains validity.
  - Quick check question: Why does Algorithm 1 resample questions but not perturbations within questions?

## Architecture Onboarding

- Component map:
  Theory layer (CTT/IRT models) -> Data layer (benchmark + perturbations) -> Inference layer (CBA/LAAT) -> Diagnostics (MAD, bias metrics)

- Critical path:
  1. Define capability theory (CTT or IRT model with perturbation term)
  2. Generate m perturbations per question
  3. For CTT: Run CBA (Algorithm 1) with hierarchical bootstrap
  4. For IRT: Calibrate item parameters -> Run LAAT (Algorithm 2)
  5. Report θ̂, CI/SE, and MAD sensitivity metric

- Design tradeoffs:
  - CBA vs. LAAT: CBA requires no prior assumptions but needs more samples; LAAT reduces samples 4-27× but requires calibrated item parameters
  - Perturbation quality vs. coverage: More perturbations improve convergence but increase cost
  - CTT accuracy vs. IRT ability: CTT measures accuracy on specific benchmark; IRT measures latent ability relative to N(0,1)

- Failure signatures:
  - Non-convergent LAAT: Item parameters poorly calibrated -> Fisher information near zero
  - Excessive MAD: Model highly sensitive -> need larger m or report wide CIs
  - Systematic bias in ā: Benchmark phrasings correlated with model biases -> original benchmark over/underestimates
  - Misspecified IRT model: If s(x) correlates with item parameters, marginalization fails

- First 3 experiments:
  1. Baseline sensitivity audit: Run 5+ models with 20 perturbations/question, compute MAD and systematic bias
  2. CBA validation: Compare CBA confidence intervals against naive bootstrap on perturbed data
  3. LAAT sample efficiency test: Calibrate item parameters, run LAAT with budget stopping, compare to CBA

## Open Questions the Paper Calls Out

- **Question 1**: How can evaluation frameworks incorporate other major confounders—such as hyperparameters (e.g., temperature, top-p) and environmental context (e.g., system prompts)—beyond input perturbations?
  - Basis: Section 3 explicitly states the model could include terms for hyperparameters and context; Section 6.2 identifies this as key future work.
  - Why unresolved: The paper provides proof of concept only for perturbation sensitivity, leaving other sources of variance unaddressed.
  - What evidence would resolve it: A study extending CTT/IRT models to include hyperparameter and context terms with empirical validation.

- **Question 2**: Do large language models exhibit a uni-dimensional measure of general intelligence or ability, and if so, how can it be robustly tested and measured?
  - Basis: Section 6.2 references Ilić & Gignac (2024) and poses this question about testing existence and measuring it.
  - Why unresolved: The paper measures ability only on specific benchmark tasks without addressing whether a single latent trait underlies performance across diverse tasks.
  - What evidence would resolve it: A multi-benchmark study applying IRT-based adaptive testing to determine if a single latent ability parameter consistently explains performance.

- **Question 3**: How can the effect of question quality be disentangled from model sensitivity to perturbations when interpreting benchmark results?
  - Basis: Appendix B acknowledges it's hard to disentangle question quality effects from model sensitivity effects, suggesting generalizability theory.
  - Why unresolved: The current method treats observed variance as sensitivity but conflates genuine model instability with poorly phrased questions.
  - What evidence would resolve it: An experiment using controlled question sets with independently rated quality measures, applying generalizability theory.

## Limitations
- Perturbation quality and representativeness are major concerns; the framework assumes perturbations capture "natural" variation but validation is minimal
- IRT model validity depends on independence assumptions between perturbation sensitivity and item parameters
- Framework is demonstrated on classification tasks and extension to generation tasks requires additional assumptions about grading consistency

## Confidence

- **Systematic Benchmark Bias**: High confidence - directly demonstrated across multiple benchmarks and models with clear quantitative evidence
- **Sample Complexity Reduction**: High confidence - Figure 4 provides clear comparative evidence across multiple model pairs  
- **Perturbation Sensitivity Persists**: High confidence - MAD values across different model sizes show consistent sensitivity patterns

## Next Checks
1. **Perturbation Coverage Validation**: Generate perturbations using multiple methods and measure correlation between different perturbation sets to validate natural variation capture
2. **IRT Model Diagnostics**: Fit full model to perturbed data and examine residual correlations between perturbation sensitivity and item parameters to test independence assumptions
3. **Cross-Benchmark Transfer**: Calibrate item parameters on one benchmark and validate whether LAAT provides accurate ability estimates on a different benchmark to test general capability measurement