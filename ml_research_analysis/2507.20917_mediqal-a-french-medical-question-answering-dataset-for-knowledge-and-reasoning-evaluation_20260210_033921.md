---
ver: rpa2
title: 'MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning
  Evaluation'
arxiv_id: '2507.20917'
source_url: https://arxiv.org/abs/2507.20917
tags:
- medical
- question
- reasoning
- answer
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MediQAl, a large-scale French medical question-answering
  dataset with 32,603 questions across 41 medical subjects. It includes three tasks:
  Multiple-Choice Questions with Unique/Multiple Answers and Open-Ended Short Answers.'
---

# MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation

## Quick Facts
- arXiv ID: 2507.20917
- Source URL: https://arxiv.org/abs/2507.20917
- Authors: Adrien Bazoge
- Reference count: 9
- One-line primary result: Large-scale French medical QA dataset with 32,603 questions, showing significant performance gaps between understanding and reasoning tasks across 14 evaluated models.

## Executive Summary
This paper introduces MediQAl, a comprehensive French medical question-answering dataset with 32,603 questions across 41 medical subjects, designed to evaluate both knowledge recall and reasoning capabilities. The dataset includes three tasks: Multiple-Choice Questions with Unique Answers (MCQU), Multiple-Choice Questions with Multiple Answers (MCQM), and Open-Ended Short Answer Questions (OEQ). Each question is labeled for either Understanding (factual recall) or Reasoning (multi-step clinical logic) to assess different cognitive capabilities. The dataset was constructed from French medical licensing exams and validated through evaluation of 14 large language models, revealing a substantial performance gap between understanding and reasoning tasks.

## Method Summary
The MediQAl dataset was constructed by curating questions from French medical licensing exams (ECN) and automatically annotating them for medical subject and question type using GPT-4o. The dataset was then used to evaluate 14 large language models across three task types: MCQU, MCQM, and OEQ. Evaluation included both vanilla and reasoning-based models, with performance measured using task-specific metrics including accuracy, Exact Match Ratio, Hamming score, ROUGE, BLEU, BERTScore, and an LLM-as-Judge approach for open-ended responses. The study also examined the effects of supervised fine-tuning on a general-purpose medical LLM using the MediQAl training sets.

## Key Results
- BioMistral-7B-SFT shows substantial performance gains of 15.64% accuracy over its base model on MCQU tasks after fine-tuning on MediQAl training sets.
- Significant performance gap exists between Understanding and Reasoning tasks, with reasoning models narrowing but not eliminating the gap.
- Lexical overlap metrics (ROUGE, BLEU) tend to compress differences in model performance, while LLM-as-Judge provides more nuanced evaluation for open-ended responses.

## Why This Works (Mechanism)

### Mechanism 1: Supervised Fine-Tuning on Domain-Specific Medical Data Improves Task Performance
Training a general-purpose medical LLM on the MediQAl training sets yields substantial performance gains across multiple-choice tasks by learning French medical terminology, question formats, and clinical reasoning patterns specific to French medical licensing context. The core assumption is that curated training data is of high quality and representative of the target task, allowing the model to internalize patterns without catastrophic forgetting. Evidence shows BioMistral-7B-SFT achieves 15.64% accuracy gain over base model on MCQU subset.

### Mechanism 2: Inference-Time Reasoning Augmentation Narrows the Understanding-Reasoning Performance Gap
Models explicitly designed or trained for extended reasoning chains outperform their vanilla counterparts more significantly on Reasoning-labeled questions than Understanding-labeled questions. These models allocate additional compute/tokens at inference time to decompose complex clinical scenarios, explore potential diagnoses, and synthesize information before generating final answers. Evidence shows average performance gain of 10.67 for reasoning questions across three model families compared to 4.02 for understanding questions.

### Mechanism 3: Automatic Classification via LLM-as-a-Judge Enables Scalable Evaluation of Open-Ended Responses
Using a powerful LLM to score open-ended answers against reference provides more nuanced evaluation than simple lexical overlap metrics for medical QA. The judge LLM compares model's free-text answer with expert-provided reference, considering clinical correctness and patient safety. Evidence shows DeepSeek-R1 outperforms o3 on ROUGE and BLEU while the opposite is observed with LLM-as-Judge metric, demonstrating its ability to capture differences that lexical metrics miss.

## Foundational Learning

- **Concept: Evaluation Metrics for Structured vs. Generative Tasks.**
  - Why needed here: The paper uses different metrics for different tasks (Accuracy/EMR for multiple-choice and ROUGE/BLEU/BERTScore/LLM-as-Judge for open-ended text). Understanding why these are chosen and their limitations is critical for interpreting results.
  - Quick check question: Why is Exact Match Ratio (EMR) a harsher metric for multi-answer MCQs than the Hamming score, and why might lexical metrics like BLEU be insufficient for evaluating the clinical accuracy of a free-text medical answer?

- **Concept: Zero-Shot Evaluation vs. Supervised Fine-Tuning (SFT).**
  - Why needed here: The paper evaluates models both in a zero-shot setting and after SFT on the dataset. This distinction is fundamental to understanding a model's generalization capability versus its ability to adapt to a specific benchmark.
  - Quick check question: What does it signify when a model performs well on a benchmark in a zero-shot setting compared to a model that requires fine-tuning to achieve similar results?

- **Concept: The Understanding vs. Reasoning Distinction in Medical QA.**
  - Why needed here: The dataset's core contribution is labeling questions as one of these two types. This distinction is used to isolate a model's capacity for factual recall from its ability to perform multi-step clinical logic.
  - Quick check question: Give an example of a question that would be labeled "Understanding" (factual recall) versus one labeled "Reasoning" (clinical logic).

## Architecture Onboarding

- **Component map:** MediQAl dataset (MCQU, MCQM, OEQ subsets) -> Evaluation framework (metrics: Accuracy, EMR, Hamming, ROUGE, BLEU, BERTScore, LLM-as-Judge) -> Models (Vanilla vs. Reasoning LLMs)
- **Critical path:**
  1. Data Curation & Annotation: Sourcing from ECN exams -> Filtering -> Automatic annotation of medical subject and question type (Understanding/Reasoning) using GPT-4o
  2. Model Evaluation: Running zero-shot inference with various LLMs on the test sets
  3. Metric Calculation: Computing task-specific scores and analyzing performance gap between Understanding and Reasoning questions
- **Design tradeoffs:**
  - Automatic Annotation: Fast and scalable but introduces 8.4% error rate; alternative is slow, expensive manual annotation
  - Open-Source Reasoning Model Limits: Token length constraints (8192) were applied for practical inference, causing some models to generate incomplete answers and artificially lowering scores
  - LLM-as-Judge: More nuanced evaluation for OEQ but introduces another model's potential biases and requires careful prompt engineering
- **Failure signatures:**
  - Distillation Penalty: Large performance drops (~28 EMR points) observed in distilled reasoning models, indicating aggressive compression can harm complex reasoning abilities
  - Inference-Time Truncation: Open-source reasoning models failing to output answer after 8192 tokens
  - Format Violation: Reasoning models outputting letter of self-created MCQ option instead of free-text for OEQ, making answer unparseable
- **First 3 experiments:**
  1. Reproduce Baseline & SFT Gains: Download MediQAl dataset and BioMistral-7B model; evaluate zero-shot, perform SFT on training set, re-evaluate to confirm ~15% accuracy gain
  2. Analyze the Reasoning Gap: Run evaluation with GPT-4o (vanilla) and DeepSeek-R1 (reasoning) on manually-verified subset of "Understanding" and "Reasoning" questions; inspect reasoning traces to confirm how processes differ
  3. Test LLM-as-Judge Correlation: Evaluate models on OEQ subset and compare lexical metrics (ROUGE/BLEU) against LLM-as-Judge score; review cases where metrics diverge significantly to understand judge's criteria

## Open Questions the Paper Calls Out
None

## Limitations
- Automatic annotation process with 8.4% error rate may introduce systematic biases in question labeling
- Performance gap between Understanding and Reasoning tasks may be influenced by specific labeling criteria rather than fundamental cognitive distinctions
- Limited number of open-source reasoning models tested reduces generalizability of reasoning capability conclusions

## Confidence
- High confidence in dataset construction methodology and observed performance gap between Understanding and Reasoning tasks
- Medium confidence in evaluation results due to potential biases in LLM-as-a-Judge approach and limited open-source reasoning models tested
- Low confidence in generalizability to other languages or medical systems, as dataset is specifically tailored to French medical licensing exams

## Next Checks
1. **Manual Annotation Verification**: Select random sample of 100 questions across different subjects and have two independent medical experts manually annotate for Understanding vs Reasoning classification; compare against GPT-4o annotations to quantify actual error rates and identify systematic biases.

2. **Judge LLM Ablation Study**: Evaluate OEQ subset using multiple judge LLMs (different sizes, different base models) and compare scores; calculate inter-annotator agreement between judge LLMs and between judge LLMs and human experts on small validation set to establish reliability.

3. **Cross-Lingual Generalization Test**: Translate subset of MediQAl questions to English and evaluate with English medical LLMs (Med-PaLM 2, GPT-4V); compare performance drops to assess whether observed gaps are language-specific or reflect fundamental reasoning capabilities.