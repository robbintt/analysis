---
ver: rpa2
title: 'A New Strategy for Artificial Intelligence: Training Foundation Models Directly
  on Human Brain Data'
arxiv_id: '2601.12053'
source_url: https://arxiv.org/abs/2601.12053
tags:
- data
- human
- could
- foundation
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a new strategy for AI: training foundation
  models directly on human brain data to overcome current limitations. While foundation
  models excel across domains, they rely on surface-level statistical regularities
  from human-generated data.'
---

# A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data

## Quick Facts
- arXiv ID: 2601.12053
- Source URL: https://arxiv.org/abs/2601.12053
- Reference count: 26
- Primary result: Proposes training foundation models on neuroimaging data to access cognitive processes not observable in human actions

## Executive Summary
This paper proposes training foundation models directly on human brain data to overcome limitations of current AI systems that rely solely on surface-level statistical patterns in human-generated data. The author argues that neuroimaging data provides access to cognitive latent spaces including values, motivations, and reasoning processes that precede observable actions. Two methods are proposed: reinforcement learning from human brain (RLHB) to improve value alignment using neural signals from valuation regions, and chain of thought from human brain (CoTHB) to enhance reasoning using signals from execution regions. The approach aims to create a realistic middle ground between scaling current architectures and exploring neuroscience-inspired solutions, potentially advancing toward artificial general intelligence.

## Method Summary
The method involves integrating neuroimaging data into foundation model training through two approaches. RLHB extends reinforcement learning from human feedback by extracting neural signals (reward, confidence, salience) from regions like ventral striatum and vmPFC to serve as richer training targets than discrete ratings. CoTHB extends chain-of-thought reasoning by extracting execution signals (rule reliability, exploration value) from regions like DLPFC and ACC to guide reasoning path selection. The approach requires collecting neuroimaging data during model evaluation or reasoning tasks, training signal decoders to extract cognitive variables, and integrating these as auxiliary rewards or regularization terms in existing training pipelines.

## Key Results
- Neuroimaging data could reveal cognitive processes (values, motivations, uncertainties) not accessible through observable actions
- RLHB could improve value alignment by using neural valuation signals as richer training targets than discrete ratings
- CoTHB could enhance reasoning by training on neural execution signals that encode rule reliability and exploration value

## Why This Works (Mechanism)

### Mechanism 1
Neuroimaging data provides access to cognitive processes that precede observable actions. Brain activity B(t) generates observable actions A(t), but also includes latent cognitive processes. Neuroimaging B*(t) approximates B(t) and can capture elements not present in A(t)—the "cognitive latent space" including hesitation, effort, and alternative considerations that never manifest as text or behavior.

### Mechanism 2
RLHB improves value alignment by using neural valuation signals as richer training targets than discrete ratings. Replace/augment scalar RLHF ratings with continuous neural signals from valuation regions (ventral striatum, vmPFC, OFC, amygdala, insula). These signals encode reward prediction, confidence, effort cost, self-control modulation, and novelty—capturing the generative process behind preference expression rather than just its output.

### Mechanism 3
CoTHB improves reasoning by training on neural execution signals that encode rule reliability and exploration value. Extract signals from DLPFC, ACC, FPC, and pre-SMA during human reasoning tasks to guide when models should continue current reasoning path, explore alternatives, or decompose into sub-problems. Neural markers like "reliability of current rule" (vmPFC), "reliabilities of alternative rules" (FPC), and "value of exploratory behavior" (ACC) could replace or augment CoT heuristics.

## Foundational Learning

- **Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: RLHB is positioned as a direct extension of RLHF; you cannot understand what RLHB adds without understanding the baseline it modifies
  - Quick check question: Can you explain how a reward model is trained from human preferences and then used to optimize a language model via PPO?

- **Neuroimaging Modalities and Tradeoffs**
  - Why needed here: The paper assumes you can match cognitive processes to appropriate recording techniques (e.g., fMRI for subcortical valuation, EEG for portable/low-cost collection)
  - Quick check question: Given a need to record ventral striatum activity during a 30-minute task, which modality would you choose and what are its limitations?

- **Representational Similarity and Brain-Model Alignment**
  - Why needed here: The paper builds on prior work showing that deep networks trained on vision/audio tasks exhibit representational similarity to sensory cortices
  - Quick check question: What does it mean for a neural network layer to have high "representational similarity" to fMRI responses in V4, and how would you measure it?

## Architecture Onboarding

- Component map:
  Current: Pre-training → SFT → RLHF (ratings) → Inference with CoT
  Proposed: Pre-training → SFT → RLHB (neural signals + ratings) → Inference with CoTHB (neural reasoning patterns)

- Critical path:
  1. Identify target cognitive process (e.g., confidence encoding in vmPFC for RLHB)
  2. Validate neural signal can be reliably extracted from your chosen modality (EEG vs fMRI tradeoffs)
  3. Collect neuroimaging data during relevant task (preference evaluation or reasoning)
  4. Train decoder/signal extractor to produce training targets
  5. Integrate into existing RLHF or CoT pipeline as auxiliary loss or guidance signal

- Design tradeoffs:
  - EEG vs fMRI: EEG is portable, cheap, high-temporal-resolution but cannot access subcortical structures (ventral striatum, amygdala); fMRI accesses whole brain but requires immobility and is expensive
  - Pattern decoding vs cognitive inference: Decoding (perception/integration) works on larger regions with more data; inference (valuation/execution) targets specific signals but requires stronger theoretical grounding
  - Scalability vs strategic use: Scale acquisition for perception-level signals; prioritize high-value training steps (RLHF/CoT) for scarce valuation/execution signals

- Failure signatures:
  - High cross-subject variance in neural signals → need better alignment/normalization methods
  - Neuroimaging cost exceeds RLHF annotation cost → reconsider scope or modality
  - Extracted signals don't correlate with behavioral preferences → valuation hypothesis may not hold
  - Short-task training doesn't generalize to long reasoning → compositionality assumption violated

- First 3 experiments:
  1. Feasibility pilot: Add EEG to existing RLHF pipeline with 10-20 annotators; extract error-related potentials (ErrP) and test whether they improve reward model accuracy on held-out preference comparisons
  2. Signal validation: Collect fMRI during simple reasoning tasks (e.g., rule-switching); verify that DLPFC/ACC signals correlate with task performance and can be decoded above chance
  3. Transfer test: Train a small language model with CoTHB-style guidance on short reasoning tasks; evaluate whether step-selection patterns transfer to longer multi-step problems in the same domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do neural signals extracted from short-term reasoning tasks generalize to long-horizon problem solving?
- Basis in paper: The paper notes that while human reasoning can span hours, neuroimaging is time-limited, raising the issue of whether "insights gained from neural signals on relatively short reasoning tasks may, or may not, generalize well to the longer problems."
- Why unresolved: There is a fundamental mismatch between the limited temporal window of neuroimaging acquisition and the extended duration of complex human reasoning.
- What evidence would resolve it: Demonstrating that models trained on neural data from short-duration tasks successfully improve performance on benchmarks requiring long-term planning or reasoning.

### Open Question 2
- Question: Are human cognitive processes an optimal computational trade-off for reasoning, or do significantly more efficient reasoning models exist?
- Basis in paper: The author asks if "the cognitive processes underlying human reasoning are close to an optimal computational trade-off, or if significantly more efficient reasoning models can be discovered."
- Why unresolved: It is currently unclear if the biological constraints shaping human reasoning represent a ceiling or merely one of many possible efficient solutions for AI.
- What evidence would resolve it: Comparative analysis showing whether architectures constrained by biological plausibility underperform, match, or outperform non-biological architectures on efficiency metrics.

### Open Question 3
- Question: What specific volume of neuroimaging data is required to successfully implement Chain of Thought from Human Brain (CoTHB)?
- Basis in paper: The text states that for CoTHB, "The amount of data that should be acquired to provide enough examples of human reasoning remains an open question."
- Why unresolved: Unlike text data, neuroimaging data is scarce and expensive, making the necessary scale for effective model training unknown.
- What evidence would resolve it: Scaling laws identifying the dataset size threshold at which CoTHB yields diminishing returns or significant performance improvements over standard Chain of Thought prompting.

## Limitations
- Feasibility of extracting reliable cognitive signals from neuroimaging data at scale remains unverified
- Limited detail on signal extraction algorithms and cross-subject alignment methods
- Scalability of neuroimaging acquisition, particularly for subcortical structures requiring fMRI

## Confidence

- **High confidence**: The theoretical framework positioning neuroimaging as a source of cognitive latent space inaccessible through behavior alone. The identification of brain regions associated with specific cognitive functions is well-established in neuroscience literature.
- **Medium confidence**: The specific methods for integrating neural signals into RLHF and CoT pipelines. While the general approach is sound, the technical implementation details (loss functions, hyperparameters, signal processing) are underspecified.
- **Low confidence**: The claim that extracted neural signals will generalize across reasoning domains and problem scales. The compositionality assumption for transferring short-task neural patterns to long reasoning chains is particularly uncertain without empirical validation.

## Next Checks
1. **Feasibility pilot**: Add EEG to existing RLHF pipeline with 10-20 annotators; extract error-related potentials (ErrP) and test whether they improve reward model accuracy on held-out preference comparisons
2. **Signal validation**: Collect fMRI during simple reasoning tasks (e.g., rule-switching); verify that DLPFC/ACC signals correlate with task performance and can be decoded above chance
3. **Cross-subject generalization test**: Train neural signal decoders on subjects 1-5 and evaluate performance on subjects 6-10; measure signal consistency across individuals to assess individual variability constraints