---
ver: rpa2
title: 'AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models'
arxiv_id: '2506.05960'
source_url: https://arxiv.org/abs/2506.05960
tags:
- quantization
- diffusion
- each
- loss
- quantized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AQUATIC-Diff, a codebook-based additive vector
  quantization method for compressing diffusion models. Unlike prior diffusion model
  quantization approaches based on uniform scalar quantization, AQUATIC-Diff adapts
  vector quantization techniques from the language modeling domain to the heterogeneous
  U-Net architecture of diffusion models.
---

# AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models

## Quick Facts
- arXiv ID: 2506.05960
- Source URL: https://arxiv.org/abs/2506.05960
- Reference count: 40
- Primary result: SOTA compression of diffusion models at 2-bit quantization (W2A8) with 9% FLOPs reduction

## Executive Summary
This paper introduces AQUATIC-Diff, a codebook-based additive vector quantization method for compressing diffusion models. Unlike prior diffusion model quantization approaches based on uniform scalar quantization, AQUATIC-Diff adapts vector quantization techniques from the language modeling domain to the heterogeneous U-Net architecture of diffusion models. The method employs a two-stage process: per-layer calibration followed by knowledge distillation, with innovations including convolutional kernel-aware quantization, layer heterogeneity-aware quantization, and selective momentum invalidation during fine-tuning. Evaluated on the standard LDM-4 ImageNet benchmark at 20 inference steps, AQUATIC-Diff achieves state-of-the-art results, notably achieving sFID 1.92 points lower than the full-precision model at W4A8 and the best-reported results for FID, sFID, and ISC at W2A8.

## Method Summary
AQUATIC-Diff is a two-stage codebook-based additive vector quantization method for compressing diffusion models. Stage 1 performs per-layer calibration using AQLM with kernel-aware group sizing (g=9 for 3×3 convolutions, g=8 otherwise) and early stopping at 0.01 relative error tolerance. Stage 2 employs PV-Tuning distillation with random uncorrelated sampling and normalized loss across timesteps, training for 32K iterations with continuous LR decaying from 4e-5 to 1e-6. The method preserves first and last convolutional layers in full precision and stores time embeddings as LUT rather than quantizing them. An efficient inference kernel provides theoretical FLOPs reduction of approximately 9% at W2A8.

## Key Results
- Achieves state-of-the-art FID/sFID/ISC at W2A8 on LDM-4 ImageNet
- sFID 1.92 points lower than full-precision model at W4A8
- Approximately 9% FLOPs reduction at W2A8 through efficient inference kernel
- Maintains competitive generative quality while achieving extreme compression (2-bit weights)

## Why This Works (Mechanism)

### Mechanism 1: Additive Vector Quantization with Codebook Decomposition
Replacing uniform scalar quantization with additive vector quantization improves weight reconstruction at extremely low bit-widths (≤2 bits). Groups of g weights are quantized jointly as vectors and reconstructed as the sum of M codebook entries, allowing exponentially more representable values while storing only n-bit indices per codebook.

### Mechanism 2: Convolutional Kernel-Aware Group Sizing
Setting group size g=9 for 3×3 convolutions (matching the spatial filter dimension) reduces quantization error by keeping all weights from a single filter in one quantization group, preserving spatial correlation structure.

### Mechanism 3: Selective Momentum Invalidation During Trajectory-Aware Distillation
Zeroing optimizer momentum at epoch boundaries prevents training instability from abrupt input distribution shifts when using trajectory-aware sampling, where consecutive timesteps in denoising order create smooth training within epochs but abrupt jumps between epochs.

### Mechanism 4: Normalized Loss Across Timesteps
Normalizing teacher-student loss by the average loss at each timestep outperforms weighted sampling strategies by equalizing gradient influence across timesteps despite varying raw loss magnitudes.

## Foundational Learning

- Concept: Vector Quantization vs Scalar Quantization
  - Why needed: The paper's core innovation is moving from scalar (per-weight) to vector (per-group) quantization. Without understanding this distinction, the codebook mechanism and group sizing rationale are opaque.
  - Quick check: Given a weight matrix, would grouping every 8 weights and assigning each group one of 256 codebook vectors yield more or fewer representable values than assigning each weight independently to one of 4 values?

- Concept: Knowledge Distillation for Quantization
  - Why needed: Stage 2 of AQUATIC-Diff uses distillation to recover accuracy lost during Stage 1 calibration. Understanding teacher-student training and feature matching losses is essential for debugging distillation failures.
  - Quick check: Why would minimizing MSE between teacher and student outputs be insufficient, necessitating additional feature-level losses?

- Concept: Diffusion Model U-Net Architecture
  - Why needed: The paper exploits architectural heterogeneity (convolutions, attention layers, time embeddings) for kernel-aware quantization. Understanding which layers have which properties is prerequisite to reproducing or extending the method.
  - Quick check: In a U-Net, why would 3×3 convolutional layers in early downsampling blocks have different channel counts than those in later blocks, and how does this affect quantization strategy?

## Architecture Onboarding

- Component map: Stage 1 Calibrator -> Stage 2 Distillation Framework -> Efficient Inference Kernel -> GreedyQuant Module (ablated)
- Critical path: 1) Generate calibration dataset (5120 inputs uniformly sampled across timesteps) -> 2) Run Stage 1 per-layer quantization (can parallelize across layers/GPUs) -> 3) Generate denoising trajectories from teacher model (1280 images, 100 steps) -> 4) Run Stage 2 PV-Tuning distillation (32k iterations, ~36 hours on RTX 3090) -> 5) Evaluate with DDIM sampler (20 steps, CFG 7.5)
- Design tradeoffs: Group size selection (g=9 improves 3×3 conv accuracy but complicates implementation vs uniform g=8); trajectory-aware vs random sampling (trajectory-aware enables momentum invalidation fix but introduces complexity; paper ultimately prefers random with normalized loss); GreedyQuant ablated away (mixed-precision layer selection helped before distillation but hurt after; uniform codebook count is simpler and better post-distillation); quantization time vs accuracy (36 hours vs 3 for EfficientDM for potentially better final metrics)
- Failure signatures: Diverging distillation loss (check if momentum invalidation is missing or if learning rates are too high); poor reconstruction at W2A8 (verify group sizes match layer types, check codebook count M is sufficient); FID/sFID much worse than baseline (ensure first/last convolutions are not quantized, verify time embedding outputs are stored as LUT rather than quantized); FLOPs not reducing with inference kernel (confirm Cout > 9(2^(k-1))M / (2·9-M) for the layer configuration)
- First 3 experiments: 1) Reproduce Stage 1 quantization on a single layer: Quantize one 3×3 convolution with g=9 vs g=8, measure MSE on calibration data. Confirms kernel-aware sizing benefit. 2) Ablate distillation sampling strategy: Train with random uncorrelated sampling + normalized loss vs weighted sampling. Compare IS/FID after 10k iterations. Validates core Stage 2 design choice. 3) Measure VRAM reduction at W2A8: Load full-precision and quantized models, compare peak VRAM during inference on identical batch size. Quantifies actual memory benefit claimed.

## Open Questions the Paper Calls Out

- Can faster gradient-based optimization algorithms be developed for additive vector quantization to significantly reduce the quantization-time? (Section 5.1: method takes 36 hours vs 3 for EfficientDM)
- Can the theoretical FLOPs reduction be translated into measurable wall-clock inference speedups on commodity hardware? (Section 4.4: efficient kernel not implemented for actual acceleration)
- How does the performance of AQUATIC-Diff scale to larger, text-conditional architectures like Stable Diffusion 3? (Introduction highlights hindrance of 8-billion parameter models, but evaluation restricted to LDM-4)

## Limitations

- The method requires substantial quantization-time GPU hours (36 hours on RTX 3090) compared to faster approaches like EfficientDM (3 hours)
- Theoretical FLOPs reduction has not been translated into actual wall-clock inference speedups due to lack of efficient kernel implementation
- Evaluation is limited to class-conditional LDM-4 model; performance on larger text-to-image models like Stable Diffusion remains untested

## Confidence

**High Confidence**: The core additive quantization mechanism with kernel-aware group sizing is well-validated through ablation studies and achieves SOTA results at extreme bit-widths. The normalized loss across timesteps showing clear superiority over weighted sampling is reproducible.

**Medium Confidence**: FLOPs reduction claims depend on specific hardware implementation details not fully disclosed. The inference kernel optimization assumes particular computational characteristics that may vary across architectures.

**Low Confidence**: Direct comparison to EfficientDM is complicated by different task settings (imagenet_32×32 vs imagenet_256×256) and the ablated GreedyQuant module that showed promise in Stage 1 but degraded Stage 2 performance.

## Next Checks

1. Reproduce the normalized loss mechanism: Implement both random sampling with normalized loss and weighted sampling on a small diffusion model, measuring FID after 10k training steps to verify the reported 3x improvement.

2. Verify kernel-aware sizing benefit: Quantize a single 3×3 convolution layer with g=9 versus g=8 using identical codebooks and measure MSE on calibration data to confirm the reported accuracy improvement.

3. Measure actual memory reduction: Load the full-precision LDM-4 model and AQUATIC-Diff W2A8 quantized model, measure peak VRAM usage during identical inference workloads to validate the claimed memory savings.