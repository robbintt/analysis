---
ver: rpa2
title: 'ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided
  Attention Refinement'
arxiv_id: '2509.13282'
source_url: https://arxiv.org/abs/2509.13282
tags:
- attention
- chart
- question
- loss
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces ChartGaze, an eye-tracking dataset for chart
  question answering that captures human gaze patterns during reasoning tasks. It
  addresses the challenge of attention misalignment in large vision-language models
  (LVLMs), where model attention often diverges from human fixations, leading to reduced
  accuracy and interpretability.
---

# ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement

## Quick Facts
- arXiv ID: 2509.13282
- Source URL: https://arxiv.org/abs/2509.13282
- Reference count: 23
- Introduces ChartGaze dataset and gaze-guided attention refinement method, improving LVLM chart QA accuracy by up to 2.56 percentage points

## Executive Summary
This work introduces ChartGaze, an eye-tracking dataset capturing human gaze patterns during chart question-answering tasks, and presents a gaze-guided attention refinement method for large vision-language models (LVLMs). The approach addresses the challenge of attention misalignment in LVLMs, where model attention often diverges from human fixations, leading to reduced accuracy and interpretability on chart QA tasks. By aligning image-text attention with human gaze using a weighted mean squared error loss combined with language modeling loss, the method improves both reasoning quality and interpretability. Fine-tuning multiple models on ChartGaze demonstrates gains of up to 2.56 percentage points in accuracy and enhanced attention alignment metrics.

## Method Summary
ChartGaze captures human gaze patterns during chart question-answering using eye-tracking data from 32 participants. The gaze-guided attention refinement method extracts cross-modal attention from the first M layers of LVLMs, averages across heads and text tokens, and reshapes to image dimensions. A weighted mean squared error (W-MSE) loss, with weights inversely proportional to fixation density, aligns model attention with human gaze maps. The total loss combines language modeling loss and W-MSE loss. Fine-tuning with LoRA on the ChartGaze dataset improves accuracy and attention alignment metrics across multiple LVLM architectures.

## Key Results
- ChartGaze dataset contains 4,638 attention maps from 1,620 charts across four visualization types
- Accuracy improvements of up to 2.56 percentage points on chart QA tasks
- Enhanced attention alignment metrics: Pearson correlation increased from -0.017 to 0.417, similarity improved
- Ablation studies confirm human-attended regions are critical for accurate reasoning

## Why This Works (Mechanism)

### Mechanism 1: Gaze-Guided Attention Alignment via W-MSE Loss
- Claim: Aligning model attention maps with human gaze distributions improves both reasoning accuracy and interpretability on chart QA tasks
- Mechanism: W-MSE loss prioritizes high-fixation regions (weight $w_i = 1/(\alpha - G_i)$ where $\alpha=1.1$), encouraging models to attend to semantically meaningful chart regions
- Core assumption: Human gaze patterns during chart reasoning reflect task-relevant regions
- Evidence: Accuracy improvements of up to 2.56% with CC increasing from -0.017 to 0.417; ablation shows masking human-attended regions causes larger accuracy drops (8.00%) for gaze-trained models vs. language-only models (5.22%)

### Mechanism 2: Early-Layer Attention Extraction
- Claim: Cross-modal attention is most malleable and informative in early transformer layers
- Mechanism: Attention maps extracted from first M layers (M=10 for TinyLLaVA, M=12 for InternVL2, M=6 for ChartGemma) and averaged across heads and tokens
- Core assumption: Early layers contain spatially-grounded cross-modal attention before information converges in deeper layers
- Evidence: Choice based on Zhang et al. (2025b); TinyLLaVA with first 10 layers achieves 63.77% accuracy vs. 57.27% using all layers

### Mechanism 3: Task-Dependent Gaze as Implicit Supervision
- Claim: Human gaze during question-answering encodes reasoning-relevant attention that, when used as supervision, forces models to rely on semantically important regions
- Mechanism: Eye-tracking captures fixations while participants answer chart questions; gaze maps serve as ground-truth attention targets
- Core assumption: Eye-tracking captures cognitively-grounded, task-specific attention
- Evidence: Performance drops significantly for attention-tuned models when masking human-attended regions (8.00% decrease) vs. language-only models (5.22% decrease)

## Foundational Learning

- **Concept: Cross-modal attention in LVLMs**
  - Why needed: Understanding how vision-language models compute attention between image patches and text tokens is prerequisite to extracting and modifying these maps
  - Quick check: Given a batch of images tokenized into 256 patches and a text prompt with 50 tokens, what is the shape of the attention matrix for a single head before softmax?

- **Concept: Eye-tracking data processing**
  - Why needed: Raw gaze data requires fixation extraction, temporal aggregation, and spatial smoothing to create usable attention maps
  - Quick check: Why does the paper apply a Gaussian filter ($\sigma=40$ pixels) to raw fixation maps, and what happens if $\sigma$ is set too high (e.g., 80)?

- **Concept: Weighted loss functions for imbalanced targets**
  - Why needed: Gaze maps are sparse with high fixation in small regions; W-MSE up-weights high-fixation pixels to prevent the loss from being dominated by the many zero/low-attention pixels
  - Quick check: In equation (2), why is the weight $w_i = 1/(\alpha - G_i)$ with $\alpha=1.1$ instead of simply $w_i = G_i$?

## Architecture Onboarding

- **Component map**: Chart image + yes/no question → Vision encoder (InternVL2/TinyLLaVA/ChartGemma) → Vision-language transformer with cross-attention layers → Text decoder

- **Critical path**: 
  1. Identify which layers handle image-text cross-attention (architecture-specific)
  2. Extract and aggregate attention tensors during forward pass
  3. Align attention spatial resolution with gaze map resolution
  4. Tune $\sigma$ for Gaussian smoothing and $\alpha$ for loss weighting

- **Design tradeoffs**:
  - Dataset size vs. gaze quality: High-precision eye-tracking is expensive; mouse-tracking is cheaper but lower fidelity
  - Loss function choice: W-MSE outperforms KL divergence, Focal Loss, and Dice+BCE on both accuracy and alignment metrics
  - Instruction-tuned models: ChartGemma showed minimal gains (0.18%)—prior chart exposure may fix attention patterns

- **Failure signatures**:
  - High alignment metrics but low accuracy: May indicate $\sigma$ too high, causing diffuse attention
  - Accuracy drop after fine-tuning: Training only on yes/no questions can degrade language capabilities

- **First 3 experiments**:
  1. Baseline verification: Fine-tune target LVLM on ChartGaze with language loss only; measure accuracy and CC on validation set
  2. Layer selection ablation: Extract attention from first 5, 10, 15 layers separately; identify which yields best accuracy-alignment tradeoff
  3. Loss function comparison: Train identical models with W-MSE vs. KL divergence vs. Focal Loss; plot accuracy vs. CC

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can gaze-guided attention refinement be adapted to significantly improve performance in heavily instruction-tuned models like ChartGemma?
- Basis: Authors note ChartGemma showed only marginal gains (0.18%) and suggest future work should "explore strategies to better integrate attention refinement into instruction-tuned models"
- Why unresolved: ChartGemma's extensive prior exposure to over 120,000 charts may have fixed its attention patterns, making it less responsive to gaze supervision
- What evidence would resolve it: Demonstrating statistically significant accuracy improvements on instruction-tuned models using a modified training strategy

### Open Question 2
- Question: Does gaze-guided attention refinement generalize to free-form, open-ended chart question answering tasks?
- Basis: Conclusion states future work should "extend this work to... free-form question formats," and Limitations note the focus on Yes/No questions may limit generalizability
- Why unresolved: The current dataset relies solely on binary queries; attention mechanisms for generating complex, open-ended answers may differ significantly
- What evidence would resolve it: Evaluation on open-ended benchmarks showing gaze alignment improves generative answer quality

### Open Question 3
- Question: Is the proposed attention supervision method effective for complex or densely packed visualization types outside the current dataset?
- Basis: Conclusion lists "more diverse chart types" as a necessary extension, and Limitations acknowledge focus on "relatively simple chart types"
- Why unresolved: Complex visualizations may present different attention challenges not included in ChartGaze dataset
- What evidence would resolve it: Testing refined models on diverse chart benchmarks to verify improved attention alignment correlates with accuracy on complex visual structures

## Limitations

- Limited dataset size (1,620 charts, 4,638 attention maps) raises questions about generalization to more complex or varied chart types
- Architecture-specific assumptions about early-layer cross-modal attention may not generalize across all LVLM architectures
- Task-specific gaze patterns may not capture reasoning about regions humans don't naturally fixate on, limiting effectiveness for certain question types

## Confidence

**High confidence**: The core mechanism of using gaze maps as supervision is well-supported by ablation studies showing masking human-attended regions causes larger accuracy drops for gaze-trained models (8.00%) vs. language-only models (5.22%). W-MSE loss function consistently outperforms alternatives.

**Medium confidence**: Specific hyperparameter choices (α=1.1, σ=40 pixels, layer counts) are justified by ablation studies but optimality for other datasets/architectures remains uncertain. Improvement magnitude (up to 2.56%) is consistent but may vary.

**Low confidence**: Generalizability of gaze patterns across different cultural contexts, chart design conventions, or user expertise levels is not addressed. Potential negative effects on other downstream tasks beyond chart QA are unexplored.

## Next Checks

1. **Architecture transfer test**: Apply gaze-guided attention refinement to a third, architecturally distinct LVLM to validate generalizability of early-layer attention extraction assumptions.

2. **Cross-domain generalization**: Evaluate refined models on chart datasets from different sources or domains not included in ChartGaze training data to assess generalization beyond specific chart types.

3. **Long-range reasoning assessment**: Test whether gaze-guided attention refinement improves or degrades performance on chart QA tasks requiring distributed reasoning across multiple chart regions.