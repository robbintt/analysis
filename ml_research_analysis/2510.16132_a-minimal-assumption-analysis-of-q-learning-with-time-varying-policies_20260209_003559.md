---
ver: rpa2
title: A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies
arxiv_id: '2510.16132'
source_url: https://arxiv.org/abs/2510.16132
tags:
- q-learning
- learning
- have
- policy
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides the first finite-time analysis of Q-learning\
  \ with time-varying learning policies (on-policy sampling) under minimal assumptions,\
  \ specifically assuming only the existence of a policy inducing an irreducible Markov\
  \ chain over the state space. The authors establish a last-iterate convergence rate\
  \ for E[\u2225\U0001D444 \U0001D458 \u2212\U0001D444\u2217\u22252\u221E] with sample\
  \ complexity O(1/\u03B52) for achieving E[\u2225\U0001D444 \U0001D458 \u2212\U0001D444\
  \u2217\u2225\u221E] \u2264 \u03B5, matching off-policy Q-learning but with worse\
  \ dependence on exploration-related parameters."
---

# A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies

## Quick Facts
- **arXiv ID:** 2510.16132
- **Source URL:** https://arxiv.org/abs/2510.16132
- **Reference count:** 40
- **Key outcome:** Provides the first finite-time analysis of Q-learning with time-varying learning policies under minimal assumptions, achieving sample complexity $O(1/\epsilon^2)$ for on-policy sampling.

## Executive Summary
This paper provides the first finite-time analysis of Q-learning with time-varying learning policies under minimal assumptions, specifically assuming only the existence of a policy inducing an irreducible Markov chain over the state space. The authors establish a last-iterate convergence rate for $E[\|Q_k - Q^*\|_\infty^2]$ with sample complexity $O(1/\epsilon^2)$ for achieving $E[\|Q_k - Q^*\|_\infty] \leq \epsilon$, matching off-policy Q-learning but with worse dependence on exploration-related parameters. They also derive an explicit rate for $E[\|Q^{\pi_k} - Q^*\|_\infty^2]$ where $\pi_k$ is the learning policy at iteration $k$. The key technical challenge of handling rapidly time-inhomogeneous Markovian noise under minimal assumptions is addressed using a Poisson equation approach that decomposes the noise into martingale-difference and residual terms, with sensitivity analysis performed for the Poisson equation solution.

## Method Summary
The paper analyzes standard Q-learning with time-varying $\epsilon$-softmax policies (Algorithm 1), where the learning policy $\pi_k$ changes at every iteration. The analysis assumes only that there exists a baseline policy $\pi_b$ inducing an irreducible Markov chain. The method employs a Poisson equation to decompose the time-inhomogeneous Markovian noise into a martingale difference sequence and a bounded residual term. A Lyapunov function approach is used to handle the contraction of the Bellman operator, while a "lazy" Markov chain transformation is applied to bound the sensitivity of the Poisson equation solution to policy changes.

## Key Results
- Establishes $O(1/\epsilon^2)$ sample complexity for on-policy Q-learning, matching off-policy Q-learning's dependence on $\epsilon$ but with worse dependence on problem-specific constants
- Derives explicit convergence rates for both $E[\|Q_k - Q^*\|_\infty^2]$ and $E[\|Q^{\pi_k} - Q^*\|_\infty^2]$
- Shows that on-policy Q-learning exhibits weaker exploration than off-policy Q-learning but enjoys an exploitation advantage as its policy converges to an optimal one
- Introduces a Poisson equation approach to handle time-varying policies, decomposing noise into martingale and residual terms

## Why This Works (Mechanism)

### Mechanism 1: Lyapunov Function for Contraction Analysis
The algorithm achieves finite-time convergence by decoupling the contraction of the Bellman operator from the noise of the sampling process using a Lyapunov function $M(Q)$ approximating the squared $\ell_\infty$ norm. By analyzing the one-step drift $E[M(Q_{k+1}) - M(Q_k)]$, the proof exploits the contraction property of the Bellman optimality operator to drive the "bias" term (initialization error) to zero geometrically, provided the step-size is small enough. The core assumption is that the Bellman optimality operator $H(\cdot)$ is a $\gamma$-contraction mapping, and the learning rate $\alpha$ satisfies $\alpha < 1/c_1$.

### Mechanism 2: Poisson Equation for Time-Inhomogeneous Noise
The approach handles rapidly time-inhomogeneous Markovian noise by decomposing it into a martingale difference sequence and a bounded residual term. Because the learning policy $\pi_k$ changes at every step, the Markov chain of samples $\{Y_k\}$ is time-inhomogeneous, preventing standard mixing time arguments. The authors employ a Poisson equation to transform the noise $F(Q_k, Y_k) - \bar{F}(Q_k, \pi_k)$ into a telescoping sum (martingale) that cancels out in expectation, plus a residual term that must be bounded separately. The core assumption is that the solution $h(Q, \pi, y)$ to the Poisson equation exists and is sufficiently regular.

### Mechanism 3: Lazy Chain Sensitivity Analysis
Convergence is guaranteed under minimal assumptions by analyzing a "lazy" Markov chain to bound the sensitivity of the Poisson equation solution. To control the residual terms from the Poisson decomposition despite time-varying policies, the analysis maps the original transition matrix $P$ to a lazy chain $\bar{P} = (P+I)/2$. This guarantees aperiodicity and geometric mixing, allowing the authors to derive an "almost-Lipschitz" continuity property for the Poisson solution, ensuring that small changes in the policy do not explode the error bounds. The core assumption is Assumption 3.1: Existence of a policy $\pi_b$ inducing an irreducible Markov chain.

## Foundational Learning

- **Concept: Stochastic Approximation (SA)**
  - **Why needed here:** Q-learning is framed as an SA algorithm for solving the Bellman equation. Understanding how noisy samples iteratively solve a fixed-point equation is the baseline for the analysis.
  - **Quick check question:** How does the step size $\alpha_k$ balance the bias (convergence speed) and variance (noise) in the update $Q_{k+1} = Q_k + \alpha_k (\text{sample} - Q_k)$?

- **Concept: Markov Chain Irreducibility & Mixing**
  - **Why needed here:** The minimal assumption rests on the existence of an irreducible chain. You must understand that irreducibility ensures every state is eventually reachable, which is required for the "lazy chain" analysis to guarantee a stationary distribution.
  - **Quick check question:** Why does the existence of *one* irreducible policy $\pi_b$ allow us to bound the mixing time of *any* policy $\pi$ encountered during learning?

- **Concept: Martingale Difference Sequences**
  - **Why needed here:** The Poisson equation decomposition produces a martingale difference term. Recognizing that the conditional expectation of this term is zero allows the proof to eliminate a large chunk of the noise complexity.
  - **Quick check question:** If $E[X_{k+1} | \mathcal{F}_k] = 0$, what is the expectation of the sum $\sum \alpha_k X_k$?

## Architecture Onboarding

- **Component map:** MDP -> Q-learning with time-varying policies -> Poisson equation decomposition -> Lazy chain analysis -> Lyapunov drift analysis -> Convergence bounds
- **Critical path:** The time-varying nature of $\pi_k$ breaks standard off-policy assumptions. The critical analytical jump is the use of the Poisson equation to decompose the error into a martingale (controllable) and a residual (requires lazy-chain sensitivity analysis). Without this decomposition, the correlation between time-varying noise and the Q-estimate cannot be bounded.
- **Design tradeoffs:**
  - On-Policy vs. Off-Policy: On-policy sampling allows the policy to converge to the optimal one (exploitation advantage) but suffers from weaker exploration (worse dependence on constants $\epsilon, \tau$ in Theorem 3.3).
  - Step Size $\alpha$: Constant step size leads to a convergence radius proportional to $\alpha$ (bias-variance trade-off). To reach accuracy $\xi$, $\alpha$ must be small, increasing iteration time.
- **Failure signatures:**
  - Insufficient Exploration: If $\epsilon$ and $\tau$ are too small, $\pi_k$ becomes greedy too quickly. If the underlying MDP structure doesn't support the greedy policy as irreducible, the "lazy chain" assumption degrades, and the bounds in Lemma 4.9 fail.
  - Non-Irreducible MDP: If no policy $\pi_b$ exists to make the chain irreducible (e.g., disconnected state regions), the assumption is violated, and convergence is not guaranteed.
- **First 3 experiments:**
  1. Validate Sample Complexity: Run the algorithm on the MDP described in Section 6 (Figure 1). Plot $\|Q_k - Q^*\|_\infty$ against $k$ for varying accuracy $\epsilon$. Verify the $O(1/\epsilon^2)$ scaling.
  2. Test the Irreducibility Assumption: Construct an MDP where the only optimal policy is deterministic and periodic (not irreducible/lazy-mixing). Compare convergence against a slightly perturbed $\epsilon$-greedy version where irreducibility holds.
  3. Exploration vs. Exploitation: Plot $\|Q^{\pi_k} - Q^*\|_\infty$ (policy performance) against time for varying $\epsilon$ (exploration rate). Verify Theorem 3.5 by showing that low $\epsilon$ improves asymptotic performance but slows initial convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can both $\|Q_k - Q^*\|_\infty$ and $\|Q^{\pi_k} - Q^*\|_\infty$ achieve convergence rates matching the statistical lower bound under time-varying policies?
- **Basis in paper:** [explicit] The Conclusion explicitly asks whether these two performance metrics can achieve convergence rates matching the statistical lower bound.
- **Why unresolved:** Existing statistical lower bounds are established for the generative model setting, and it is unclear what the lower bound is for practical RL with rapidly time-varying policies.
- **What evidence would resolve it:** A derivation of the lower bound for this specific setting or an algorithm proven to achieve it.

### Open Question 2
- **Question:** Can the Poisson equation-based framework be extended to provide finite-time analysis for single-timescale actor-critic methods?
- **Basis in paper:** [explicit] The Abstract and Conclusion state that the developed tools may facilitate the analysis of general RL algorithms, specifically citing single-timescale actor-critic methods and learning-in-games.
- **Why unresolved:** The paper validates the framework for Q-learning, but the extension to the coupled value-policy updates in actor-critic algorithms remains unproven.
- **What evidence would resolve it:** A convergence proof for single-timescale actor-critic that utilizes the Poisson equation decomposition for time-inhomogeneous noise.

### Open Question 3
- **Question:** Is the dependence on exploration-related parameters (e.g., $\delta_b$, $\lambda$) in the sample complexity bound tight, or can it be improved?
- **Basis in paper:** [inferred] The paper notes in Section 3.1 that the dependence on problem-specific constants is significantly worse than off-policy Q-learning due to exploration limitations.
- **Why unresolved:** The paper provides the first upper bound under minimal assumptions; the optimality regarding these specific parameters has not been established.
- **What evidence would resolve it:** A minimax lower bound that quantifies the necessary dependence on these exploration parameters.

## Limitations

- The analysis assumes a finite state-action space and bounded rewards, which may not hold in practical applications.
- The use of the Poisson equation approach is innovative but relies heavily on the existence of a well-behaved solution, which may not be guaranteed in all MDPs.
- The paper's confidence in the theoretical bounds is high for the established results, but the practical applicability and robustness of these bounds under more realistic assumptions remain to be tested.

## Confidence

- **High Confidence:** The theoretical framework and convergence rates for the specific MDP and assumptions outlined in the paper.
- **Medium Confidence:** The applicability of the results to broader classes of MDPs, especially those with infinite state spaces or unbounded rewards.
- **Low Confidence:** The sensitivity of the bounds to the specific choice of the Lyapunov function and the Poisson equation approach in practical scenarios.

## Next Checks

1. **Empirical Validation:** Implement the Q-learning algorithm with time-varying policies on a variety of MDPs, including those with infinite state spaces or unbounded rewards, to test the robustness of the theoretical bounds.
2. **Sensitivity Analysis:** Conduct a thorough sensitivity analysis of the bounds to variations in the MDP parameters, such as the discount factor γ and the step size α, to understand the practical implications of the theoretical results.
3. **Comparison with Existing Methods:** Compare the performance of the proposed Q-learning algorithm with time-varying policies against existing methods, such as off-policy Q-learning, on standard benchmark MDPs to assess the practical advantages and disadvantages of the approach.