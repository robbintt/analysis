---
ver: rpa2
title: 'Imitate Optimal Policy: Prevail and Induce Action Collapse in Policy Gradient'
arxiv_id: '2509.02737'
source_url: https://arxiv.org/abs/2509.02737
tags:
- action
- policy
- collapse
- optimal
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the representational structures of deep
  neural networks used in policy gradient methods for reinforcement learning. While
  analyzing optimal policy networks, the authors observed a geometric phenomenon called
  "Action Collapse," where state-action activations and action selection layer weights
  converge to a simplex equiangular tight frame (ETF).
---

# Imitate Optimal Policy: Prevail and Induce Action Collapse in Policy Gradient

## Quick Facts
- arXiv ID: 2509.02737
- Source URL: https://arxiv.org/abs/2509.02737
- Authors: Zhongzhu Zhou; Yibo Yang; Ziyan Chen; Fengxiang Bie; Haojun Xia; Xiaoxia Wu; Robert Wu; Ben Athiwaratkun; Bernard Ghanem; Shuaiwen Leon Song
- Reference count: 31
- One-line primary result: Action Collapse Policy Gradient (ACPG) induces optimal policy structure and improves RL training efficiency across 10+ environments

## Executive Summary
This paper investigates the representational geometry of policy networks in reinforcement learning, identifying a phenomenon called "Action Collapse" where state-action activations and action layer weights converge to a simplex equiangular tight frame (ETF). The authors propose ACPG, a method that fixes the action selection layer as a synthetic ETF and optimizes only the state-action activations. The approach is theoretically supported by proving that Action Collapse naturally emerges under ACPG training. Experiments across 10+ OpenAI Gym environments demonstrate that ACPG can be integrated with any discrete policy gradient algorithm and consistently leads to faster, more robust convergence with significant reward improvements—up to 161% in challenging Atari domains—while maintaining lower variance across runs.

## Method Summary
The Action Collapse Policy Gradient (ACPG) method replaces the learnable action selection layer in policy networks with a fixed Simplex Equiangular Tight Frame (ETF). The ETF is initialized using a specific geometric construction and frozen during training, while only the backbone network (feature extractor) is optimized. This forces the network to organize its feature space to align with the ideal ETF geometry, which the authors prove leads to Action Collapse—a state where state-action activations converge to the vertices of the simplex. The method is compatible with standard discrete policy gradient algorithms like REINFORCE, PPO, TRPO, and A3C.

## Key Results
- ACPG consistently improves convergence speed and reward stability across 10+ OpenAI Gym environments
- Experimental results show up to 161% reward improvement in challenging Atari domains compared to baselines
- The method maintains lower variance across multiple random seeds
- Performance gains are most pronounced in environments with sparse rewards or high stochasticity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Imposing a fixed Simplex Equiangular Tight Frame (ETF) geometry on the action selection layer enforces maximal separation between action prototypes, mimicking the structure of an optimal converged policy.
- Mechanism: The authors initialize the final layer weights $W$ as a Simplex ETF. By definition (Eq. 3), this geometry ensures all action vectors have equal norm and maximally separated angles ($\cos \theta = -1/(K-1)$). By locking this structure, the method prevents the "biased" weights often seen in standard training (Fig 2b), where under-explored actions drift in the embedding space. Instead, the network is forced to organize the feature space to align with this ideal geometry.
- Core assumption: The optimal policy for the task can be represented by equidistant, maximally separated action vectors, meaning no semantic "closeness" exists between specific actions that the ETF geometry would violate.
- Evidence anchors:
  - [abstract] "...maximally separates the pair-wise angles of all actions in the state-action space..."
  - [Section 4.1] "...affixes a synthetic ETF as our action selection layer. ACPG induces the policy DNN to produce such an ideal configuration..."
  - [corpus] Weak direct support; neighbors discuss Neural Collapse in classification/continual learning but do not validate the ETF-in-RL mechanism specifically.
- Break condition: In tasks where actions have inherent hierarchical or semantic overlap (e.g., action "walk" should be closer to "run" than "jump"), the equiangular constraint might act as a bottleneck, degrading performance.

### Mechanism 2
- Claim: Freezing the action selection layer simplifies the optimization landscape, reducing variance and preventing convergence to suboptimal local minima caused by data imbalance.
- Mechanism: Standard Policy Gradient methods optimize both the backbone (features) and the action layer (classifier). In realistic environments with sparse exploration, the action layer can overfit to frequently sampled states. ACPG fixes the layer $W$, turning the problem into a pure feature alignment task. The optimization relies on the backbone's capacity to map states to the fixed target vectors, which the authors prove (Theorem 1) leads to a global optimum under specific constraints.
- Core assumption: The backbone network (feature extractor) has sufficient capacity to map the state distribution into the rigid structure required by the ETF without needing to adjust the ETF itself.
- Evidence anchors:
  - [Section 5.1] "By contrast, conventional learnable layers frequently become trapped in local minima... ACPG's fixed ETF action selection layer precisely aligns activations with the Action Collapse targets."
  - [Section 4.1] "Although fixing the action selection layer as an ETF simplifies the learning problem, it brings theoretical merits..."
- Break condition: If the backbone is too shallow or small to model the necessary complexity, forcing alignment with a fixed layer could result in underfitting or high training error.

### Mechanism 3
- Claim: The method relies on induced "Action Collapse," where state features are trained to align directly with their corresponding action vectors in the fixed ETF.
- Mechanism: The paper models the optimization using a "Layer-Peeled Model" (LPM). Under this model, the gradient update for a state feature $h$ consists of a "pull" term towards the optimal action vector $w_{k^*}$ and "push" terms against other actions. Because $W$ is a fixed ETF (where $\sum w_k = 0$), these push-pull dynamics theoretically guarantee that features collapse to the ETF vertices, satisfying the "Action Collapse" condition (NC1-NC4 adapted for RL).
- Core assumption: The theoretical proof relies on the "Layer-Peeled Model" simplification, which assumes separation between feature dynamics and classifier dynamics.
- Evidence anchors:
  - [Section 4.2] "Our analytical proof shows that learning activations with a fixed ETF as action selection layer naturally leads to the Action Collapse."
  - [Section 5.2] "But under ACPG, the mean action selection layer activations trend towards equiangularity and maximal-angle equiangularity..."
- Break condition: Insufficient exploration. The paper notes (Tab 2) that if exploration $\epsilon$ is too low, ACPG fails because it cannot observe enough state diversity to construct the collapse.

## Foundational Learning

- Concept: **Simplex Equiangular Tight Frame (ETF)**
  - Why needed here: This is the core architectural constraint. You must understand that an ETF is a specific geometric arrangement of vectors where they are equidistant and maximally separated (vertices of a simplex). This is the target structure the paper argues represents an optimal policy.
  - Quick check question: If I have 4 actions, does the ETF geometry allow one action to be "closer" to another in the feature space? (Answer: No, all pairwise angles are equal).

- Concept: **Neural Collapse (NC)**
  - Why needed here: "Action Collapse" is an adaptation of Neural Collapse (observed in classification) to RL. Understanding NC helps contextualize why the authors believe features converging to class means (action prototypes) is a desirable "optimal" state.
  - Quick check question: In standard classification, what happens to the features of samples belonging to the same class during the "Terminal Phase of Training"?

- Concept: **Policy Gradient (PG) Optimization Objective**
  - Why needed here: The paper modifies the standard PG setup (maximizing expected return). You need to grasp that PG is essentially a weighted classification problem (states → actions) to understand why fixing the classifier (action layer) is a valid intervention.
  - Quick check question: How does ACPG modify the standard PG parameter update? (Answer: It removes the action selection layer weights from the set of trainable parameters).

## Architecture Onboarding

- Component map:
  - State/Input → Backbone (Encoder) → Fixed ETF Layer → Logits → Policy Loss
  - Optimizer updates Backbone only

- Critical path:
  1. Initialize the action selection layer using the ETF construction formula (Eq. 3). Ensure $\sum w_k = 0$ and norms are equal.
  2. **LOCK** the layer. Do not pass gradients through $W$.
  3. Run standard Policy Gradient (e.g., PPO, REINFORCE), calculating loss based on the logits output by the fixed ETF layer.

- Design tradeoffs:
  - **Stability vs. Flexibility:** You gain convergence speed and lower variance (stability) but lose the flexibility to learn custom action embeddings that might better fit specific state distributions (flexibility).
  - **Exploration Reliance:** The method is heavily dependent on the exploration strategy. As noted in Table 2, performance gains vanish if exploration is too low ($\epsilon \approx 0$).

- Failure signatures:
  - **Stagnant Reward:** If the reward plateaus early, the backbone may lack the capacity to stretch the features to the fixed ETF dimensions (check activation norms).
  - **Divergence:** If exploration is insufficient (e.g., purely greedy exploitation), the fixed ETF creates a "self-reinforcing" bias where the agent cannot discover states required to align the ETF correctly.

- First 3 experiments:
  1. **Sanity Check (CartPole):** Integrate ACPG into a simple PPO loop on CartPole. Verify that standard deviation of rewards decreases compared to vanilla PPO (replicating Tab 1 trends).
  2. **Visualization of Collapse:** Run on "Ideal Cliff Walking" (or similar tabular/simple env). Plot the cosine similarity of mean activations. Confirm they approach the maximal-angle equiangularity ($-1/(K-1)$).
  3. **Exploration Ablation:** Replicate Table 2. Run PPO+ACPG with varying $\epsilon$-greedy values to observe the sensitivity of the method to exploration noise.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What geometric structure characterizes policy networks at a saddle or sub-optimal point?
- Basis in paper: [explicit] The authors explicitly list this limitation in Appendix E, stating, "what does geometric structure characterize at a saddle (sub-optimal) point?"
- Why unresolved: The current analysis and proofs focus exclusively on the properties of global optima and the terminal phase of training (TPT).
- What evidence would resolve it: A theoretical or empirical analysis of activation geometries when the policy converges to local optima or saddle points rather than the global optimum.

### Open Question 2
- Question: Does Action Collapse emerge in complex environments with sparse rewards or high stochasticity?
- Basis in paper: [explicit] Appendix E states, "if the environment becomes more complex, with more random transition and sparse rewards, this phenomenon may be difficult to observe."
- Why unresolved: The experiments were limited to OpenAI Gym and Atari domains; the theory relies on idealized conditions or specific constraints that may break down in more chaotic settings.
- What evidence would resolve it: Testing ACPG in environments specifically designed with high stochasticity and sparse reward structures to observe if the ETF geometry is maintained.

### Open Question 3
- Question: How does the construction of the sampling RL dataset influence Action Collapse?
- Basis in paper: [explicit] Appendix E lists "the impact of construction of sampling RL dataset" as an area warranting further investigation.
- Why unresolved: While the paper links performance drops in TRPO to sampling bias (Table 1), the specific relationship between dataset construction methods and the stability of the collapse remains unquantified.
- What evidence would resolve it: Ablation studies varying the balance and distribution of state-action pairs in the replay buffer to measure the impact on the formation of the simplex ETF.

## Limitations
- The method relies heavily on sufficient exploration; performance degrades significantly when exploration is too low
- The theoretical framework assumes idealized conditions that may not hold in highly stochastic or sparse-reward environments
- The paper does not specify explicit activation norm constraints (E_H) or scaling factors (E_W) used in experiments

## Confidence
- **High Confidence:** The geometric intuition (ETF structure) and empirical results showing improved convergence and reward (up to 161% in Atari) are well-supported
- **Medium Confidence:** The theoretical proof of Action Collapse emergence assumes idealized Layer-Peeled Model conditions that may not fully capture practical RL dynamics
- **Medium Confidence:** The exploration sensitivity (Table 2) is well-demonstrated, but the exact exploration thresholds for different environments are not fully characterized

## Next Checks
1. Verify ETF initialization scaling (E_W) and activation constraints (E_H) match theoretical assumptions
2. Test ACPG in environments with semantically related actions to evaluate potential bottlenecks
3. Compare convergence speed and reward variance against additional baselines beyond PPO, including more recent algorithms like SAC