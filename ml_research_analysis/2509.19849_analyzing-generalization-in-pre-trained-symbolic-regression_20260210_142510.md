---
ver: rpa2
title: Analyzing Generalization in Pre-Trained Symbolic Regression
arxiv_id: '2509.19849'
source_url: https://arxiv.org/abs/2509.19849
tags:
- data
- recovery
- performance
- pre-training
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically evaluates the generalization capabilities
  of pre-trained transformer-based symbolic regression methods. It reveals a critical
  performance gap: while these models perform well within their pre-training domains,
  their recovery rates drop significantly (e.g., from 31% to 4%) and accuracy degrades
  when applied to out-of-pre-training domain (OOPD) data.'
---

# Analyzing Generalization in Pre-Trained Symbolic Regression

## Quick Facts
- arXiv ID: 2509.19849
- Source URL: https://arxiv.org/abs/2509.19849
- Reference count: 40
- Primary result: Pre-trained transformer-based symbolic regression models show catastrophic performance drops on out-of-pre-training-domain data, with recovery rates falling from 31% to 4%, revealing fundamental generalization limitations.

## Executive Summary
This study systematically evaluates the generalization capabilities of pre-trained transformer-based symbolic regression methods. While these models perform well within their pre-training domains, they show catastrophic performance drops when applied to out-of-pre-training-domain (OOPD) data. The methods are also highly sensitive to common data perturbations like input shifts, scaling, and noise. Data standardization improves OOPD robustness but does not fully solve the issue, suggesting that current pre-trained approaches learn patterns specific to their training distributions rather than generalizable compositional structures.

## Method Summary
The paper evaluates four pre-trained symbolic regression models (NeSymReS, E2E, TF4SR, SymFormer) across three data regimes: in-pre-training-domain (IPD), within-pre-training-domain (WIPD), and out-of-pre-training-domain (OOPD). An ablation model with 86M parameters was also trained on expression DAGs using diverse sampling. Performance was measured using recovery rate (symbolic equivalence via SymPy), accuracy (R² on ID and OOD test sets), Tree Edit Distance, Jaccard Index, and complexity metrics. The study systematically tested robustness to input perturbations including shifts, scaling, noise, and distribution changes.

## Key Results
- Pre-trained models achieve 31% recovery on IPD data but drop to 4% on OOPD data
- High R² scores often mask complete failure to recover correct symbolic expressions (E2E pattern)
- Models show catastrophic sensitivity to input perturbations - small shifts or scaling cause recovery rate collapse
- Data standardization improves OOPD robustness but cannot fully recover performance
- Recovery rates are highly dependent on alignment between inference-time data and pre-training domain characteristics

## Why This Works (Mechanism)

### Mechanism 1: Statistical Pattern Matching
Transformer-based models perform symbolic regression by memorizing mappings from statistical properties of numerical point clouds to symbolic structures, rather than learning compositional generalization. During pre-training, the model encodes associations between input data distributions and their generating expressions. At inference, it matches new point clouds against these memorized patterns.

### Mechanism 2: Domain-Specific Sampling Sensitivity
Performance is critically dependent on alignment between inference-time data sampling and pre-training data characteristics (domain, distribution, noise levels). Pre-training creates strong priors over specific numerical ranges and sampling distributions. Misalignment causes performance collapse due to out-of-distribution inputs.

### Mechanism 3: Standardization as Partial Mitigation
Data standardization improves OOPD robustness but does not enable true compositional generalization. Standardization normalizes input ranges, allowing the model to map OOPD data into a familiar numerical domain. However, it cannot recover functional forms that are structurally sensitive to affine transformations (e.g., trigonometric functions).

## Foundational Learning

- **Distribution Shift and Out-of-Distribution Generalization**: The entire paper is structured around testing how models perform when data is IPD, WIPD, or OOPD. Quick check: Can you explain why a model trained on [0,1] might fail on [-10,-5] even if the underlying function is the same?

- **Symbolic Recovery vs. Numerical Accuracy**: High R² scores can mask complete failure to recover the correct symbolic expression (e.g., E2E). Quick check: If a model achieves R²=0.99 but predicts a completely different functional form, has it "solved" symbolic regression?

- **Pre-training Data Generation (Expression Trees/DAGs, Sampling Strategies)**: The paper attributes performance differences to choices in how pre-training data is generated (random trees vs. enumerated DAGs, uniform vs. diverse sampling). Quick check: Why might training on semantically equivalent but syntactically different expressions harm generalization?

## Architecture Onboarding

- **Component map**: Point Encoder -> Expression Decoder -> Constant Fitting (optional)

- **Critical path**:
  1. Pre-training: Generate synthetic expressions via tree/DAG enumeration; sample points using chosen distribution
  2. Training: Encoder-decoder learns to map point sets to tokenized expression strings
  3. Inference: For new (x, y) data, encoder produces embeddings; decoder generates candidate expressions (possibly with beam search)

- **Design tradeoffs**:
  - Random expression trees vs. enumerated DAGs: Trees introduce syntactic bias; DAGs cover more semantic equivalence classes
  - Standardization vs. raw inputs: Standardization improves OOPD robustness but may reduce IPD peak performance
  - Diverse vs. uniform sampling: Diverse sampling improves generalization to varied test distributions but is more complex

- **Failure signatures**:
  - Sudden drop in recovery rate when training data moves OOPD (e.g., 31% → 4%)
  - High R² but zero recovery (curve-fitting behavior)
  - Catastrophic sensitivity to small noise or scaling perturbations

- **First 3 experiments**:
  1. Baseline Recovery Test: Run a pre-trained model (e.g., NeSymReS) on SRBench formulas with IPD, WIPD, and OOPD training data; plot recovery vs. domain
  2. Perturbation Sensitivity: Systematically scale/shift/noise a simple function (y=x²); record recovery rate vs. perturbation magnitude
  3. Standardization Ablation: Train two models (with/without standardization) and compare OOPD recovery and accuracy profiles

## Open Questions the Paper Calls Out

### Open Question 1
What architectural or training innovations are required to enable pre-trained models to learn generalizable compositional rules rather than memorizing statistical patterns? The authors conclude that "data scaling is not the answer" and "innovations in modelling are needed" to move beyond pattern matching.

### Open Question 2
How can pre-trained transformers be integrated with search-based methods so that the learned prior remains robust and provides reliable guidance in out-of-pre-training-domain (OOPD) scenarios? Current hybrid approaches show improved in-domain performance but degrade sharply in OOPD scenarios because the transformer prior becomes unreliable.

### Open Question 3
Does the observed generalization gap and brittleness to distribution shifts persist when models are applied to scientific domains with mathematical structures distinct from the physics-heavy SRBench? The study's reliance on SRBench, which is "primarily composed of small-to-medium complexity formulas drawn from physics," leaves generalization to fields like chemistry or economics untested.

## Limitations

- The study relies on undisclosed pre-trained models with unknown pre-training configurations, limiting reproducibility
- The analysis focuses on specific perturbation types and domains, leaving open questions about performance under other distribution shifts
- The proposed standardization mitigation shows partial success but lacks a theoretical explanation for why certain functional forms remain sensitive

## Confidence

- **High Confidence**: Claims about catastrophic OOPD performance drops (31% → 4%) and sensitivity to perturbations are well-supported by empirical results
- **Medium Confidence**: The mechanism of statistical pattern matching is plausible but not definitively proven; alternative explanations are possible
- **Low Confidence**: The effectiveness of standardization as a mitigation strategy is demonstrated but not fully understood theoretically

## Next Checks

1. Reproduce perturbation sensitivity: Systematically vary input shift, scaling, and noise levels on simple functions to confirm the reported sensitivity curves
2. Test standardization limits: Apply standardization to models trained on trigonometric functions to quantify the remaining performance gap
3. Compare with hybrid approaches: Evaluate whether combining pre-trained models with search-based methods improves OOPD robustness beyond standardization alone