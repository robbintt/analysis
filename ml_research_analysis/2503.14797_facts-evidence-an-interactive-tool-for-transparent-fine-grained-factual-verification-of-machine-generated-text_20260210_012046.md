---
ver: rpa2
title: 'FACTS&EVIDENCE: An Interactive Tool for Transparent Fine-Grained Factual Verification
  of Machine-Generated Text'
arxiv_id: '2503.14797'
source_url: https://arxiv.org/abs/2503.14797
tags:
- evidence
- text
- claim
- claims
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FACTS &EVIDENCE, an interactive tool for fine-grained
  factual verification of machine-generated text. The tool addresses the limitations
  of existing binary fact verification approaches by providing transparent, claim-level
  verification with multiple diverse evidence sources.
---

# FACTS&EVIDENCE: An Interactive Tool for Transparent Fine-Grained Factual Verification of Machine-Generated Text

## Quick Facts
- arXiv ID: 2503.14797
- Source URL: https://arxiv.org/abs/2503.14797
- Reference count: 14
- Primary result: Outperforms strong baselines by approximately 44 F1 points in detecting factual errors on FavaBench dataset

## Executive Summary
This paper presents FACTS&EVIDENCE, an interactive tool for fine-grained factual verification of machine-generated text. The tool addresses the limitations of existing binary fact verification approaches by providing transparent, claim-level verification with multiple diverse evidence sources. The system breaks down complex text into atomic claims, retrieves relevant evidence from the web, and uses LLMs to judge factuality while providing rationales for each decision. Users can interactively include/exclude specific evidence sources based on their preferences. Evaluated on the FavaBench dataset, FACTS&EVIDENCE outperforms strong baselines by approximately 44 F1 points in detecting factual errors, demonstrating its effectiveness for open-ended generation tasks.

## Method Summary
The FACTS&EVIDENCE system implements a pipeline for fine-grained factual verification that consists of five main steps: (1) Atomic claim generation using few-shot LLM prompting to decontextualize sentences into standalone, searchable claims; (2) Web search and scraping via Serper API to retrieve top-N evidence sources per claim; (3) Dense passage retrieval using jina-embeddings-v3 to find relevant content within retrieved documents; (4) Zero-shot LLM source categorization into predefined types (news, blogs, scientific articles, etc.); (5) Chain-of-thought LLM factuality judgment that produces both verdicts and rationales. The system aggregates judgments across multiple evidence sources to compute credibility scores for each claim, then visualizes these at sentence and passage levels. The tool allows users to interactively filter evidence sources by category and inspect intermediate steps of the verification process.

## Key Results
- FACTS&EVIDENCE achieves approximately 44 F1 points improvement over strong baselines on FavaBench dataset
- Atomic claim generation ablation shows F1 drops from 0.68 to 0.40, demonstrating its critical importance
- Multi-evidence aggregation (3 sources) improves F1 from 0.23 (single source) to 0.68, a 3× improvement
- Evidence retrieval failures account for 53.3% of errors, while incorrect atomic claims account for 26.7%

## Why This Works (Mechanism)

### Mechanism 1: Atomic Claim Decomposition
- **Claim:** Breaking complex text into self-contained atomic claims improves verification accuracy by enabling targeted evidence retrieval.
- **Mechanism:** An LLM decontextualizes sentences into standalone claims (resolving pronouns, implicit references) that can be independently searched and verified against web sources.
- **Core assumption:** Complex factual errors are often localized to specific sub-claims within sentences, not distributed across entire passages.
- **Evidence anchors:**
  - [abstract]: "presenting its users a breakdown of complex input texts to visualize the credibility of individual claims"
  - [section 4/Table 1]: Ablation shows F1 drops from 0.68 to 0.40 (ChatGPT Text) when atomic claim generation is removed
  - [corpus]: JointCQ (arXiv:2510.19310) similarly improves hallucination detection via joint claim-query generation, suggesting decomposition is a convergent mechanism
- **Break condition:** When claims are inherently relational (e.g., "A is larger than B") and decontextualization strips comparative context needed for verification.

### Mechanism 2: Multi-Evidence Aggregation with Equal Weighting
- **Claim:** Aggregating verification judgments across 3 diverse evidence sources improves prediction reliability over single-source verification.
- **Mechanism:** Each claim triggers web search retrieving top-N sources; the credibility score is computed as `# supporting evidence / # total evidence` with equal weighting per source.
- **Core assumption:** Evidence sources are roughly independent; errors in one source's retrieval don't systematically correlate with errors in others.
- **Evidence anchors:**
  - [abstract]: "retrieving diverse evidence from the web"
  - [section 4/Table 1]: No:Ev 1 yields F1=0.23; No:Ev 3 yields F1=0.68 (ChatGPT Text)—a 3× improvement
  - [corpus]: No direct corpus comparison on multi-evidence aggregation specifically; this appears to be a novel contribution of this work
- **Break condition:** When retrieval failures are correlated (e.g., for niche topics where few sources exist) or when low-quality sources dominate top search results.

### Mechanism 3: Interactive Source Filtering
- **Claim:** Allowing users to include/exclude evidence sources by category (news, blogs, scientific articles) enables domain-appropriate verification.
- **Mechanism:** Each evidence source is classified into predefined categories via zero-shot LLM; users toggle categories and credibility scores recompute in real-time.
- **Core assumption:** Users have domain knowledge to determine which source types are trustworthy for their specific verification context.
- **Evidence anchors:**
  - [abstract]: "allowing users to inspect and customize the verification process"
  - [section 2.2]: "users may choose to exclude blogs and social media posts as evidence and only focus on scientific articles or government websites"
  - [corpus]: FineDialFact (arXiv:2508.05782) addresses fine-grained dialogue fact verification but does not implement user-controlled source filtering
- **Break condition:** When users lack domain expertise to judge source appropriateness, or when source categorization is noisy (LLM classification errors).

## Foundational Learning

- **Concept: Decontextualization in claim extraction**
  - **Why needed here:** Atomic claims often contain pronouns or implicit references ("this treatment") that fail as search queries. Decontextualization rewrites claims to be self-contained.
  - **Quick check question:** Given "She won the award in 2019," what information is missing to make this a verifiable atomic claim?

- **Concept: Dense vs. sparse retrieval**
  - **Why needed here:** The tool offers both embedding-based (dense) and keyword-based (sparse) retrieval; understanding when each excels helps configure the system.
  - **Quick check question:** Which retrieval mode would better handle a claim with rare technical terminology vs. a claim with common words but specific meaning?

- **Concept: Chain-of-thought prompting for verification**
  - **Why needed here:** The factuality judgment step uses CoT to produce both a verdict and rationale; understanding CoT helps interpret and debug model decisions.
  - **Quick check question:** Why might a CoT rationale disagree with its final verdict, and how would you detect this in the UI?

## Architecture Onboarding

- **Component map:** Input Text → Atomic Claim Generator (LLM) → Claims → Query Generation → Serper API + Web Scraper → Raw Evidence → jina-embeddings-v3 Retrieval → Ranked Passages → Source Categorizer (LLM) → Tagged Evidence → Factuality Judge (LLM + CoT) → (Verdict, Rationale) → Aggregation → Sentence/Passage Credibility Scores → UI Visualization

- **Critical path:** Evidence retrieval is the bottleneck (30s-1min for 5-7 sentences). The ablation shows 53.3% of errors originate here—from search engine gaps or scraper failures due to security blocks.

- **Design tradeoffs:**
  - **More evidence = better accuracy but slower:** Table 1 shows No:Ev 3 outperforms No:Ev 1, but each additional evidence requires additional LLM calls for judgment.
  - **Larger context window = minimal gain:** Ctxt W 15 vs 30 shows no significant improvement; keep windows small for efficiency.
  - **Transparency vs. complexity:** Exposing all intermediate steps aids user agency but increases cognitive load.

- **Failure signatures:**
  - **Retrieval failures (53.3%):** Search returns irrelevant documents; scraper blocked by paywalls or anti-bot measures.
  - **Incorrect atomic claims (26.7%):** Decontextualization incomplete; claims remain ambiguous.
  - **Judgment errors (20.0%):** LLM misinterprets evidence-claim relationship, especially with subjective claims.

- **First 3 experiments:**
  1. **Retrieve-only baseline:** Pass claims directly to retrieval without decontextualization; measure F1 drop to quantify decontextualization value.
  2. **Evidence count sweep:** Test No:Ev ∈ {1, 2, 3, 5, 7} on a held-out set; plot F1 vs. latency to find the optimal operating point for your latency budget.
  3. **Source category ablation:** For medical-domain text, exclude blogs/social media and measure precision/recall change vs. including all sources.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can local, fine-tuned small language models (SLMs) be implemented for atomic claim generation and factuality judgment to reduce latency while maintaining the F1 performance of larger models?
- **Basis in paper:** [explicit] The Limitations section states, "We plan to focus on improving the efficiency of the system in the future by introducing local small LMs that are finetuned for individual steps."
- **Why unresolved:** The current pipeline relies on sequential external API calls (GPT-3.5/GPT-4o), causing processing times of 30s–1min for short passages, which hinders real-time user experience.
- **What evidence would resolve it:** A comparative benchmark on FavaBench showing the latency and F1 scores of fine-tuned SLMs (e.g., Llama-3-8B) against the current GPT-3.5 baseline.

### Open Question 2
- **Question:** How can quantitative confidence measures be integrated into the interactive interface to help users distinguish between model hallucinations and reliable predictions?
- **Basis in paper:** [explicit] The Limitations section notes, "In future, we would like to include measures of confidence in model generations and predictions and present that information to the user."
- **Why unresolved:** While the tool provides transparency through intermediate steps, it currently lacks a metric to signal the certainty of the LLM's judgment, which is necessary because the internal models can still produce errors.
- **What evidence would resolve it:** A user study measuring decision-making accuracy and trust levels when confidence scores (e.g., entropy or probability scores) are displayed alongside rationale.

### Open Question 3
- **Question:** What retrieval augmentation strategies can mitigate the 53.3% error rate caused by evidence retrieval failures without linearly increasing operational costs?
- **Basis in paper:** [inferred] The paper identifies "Incorrect Evidence Retrieval" as the predominant failure mode (Table 3) and suggests increasing the number of evidence items as a mitigation, though this comes at a "higher cost."
- **Why unresolved:** The system depends on external search and scraping tools which fail due to security blocks or parsing errors; simply increasing the volume of retrieval is an inefficient solution.
- **What evidence would resolve it:** Experiments evaluating robust retrieval mechanisms—such as query expansion or fallback scraping techniques—that demonstrate a reduction in retrieval errors on a dataset of difficult claims.

## Limitations
- Evidence retrieval is the primary failure point, with 53.3% of errors stemming from search gaps or scraper blocks, suggesting struggles with niche or rapidly evolving topics.
- The tool requires significant computational resources (30s-1min per 5-7 sentence input) and API costs, limiting scalability for production use.
- Source categorization relies on zero-shot LLM classification, which may produce noisy results for ambiguous or emerging content types.

## Confidence
- **High (90-95%):** The mechanism of atomic claim decomposition improving verification accuracy is well-supported by ablation results showing 44 F1-point drops when removed.
- **Medium (70-85%):** The interactive evidence filtering feature's practical utility depends on user domain expertise, which varies significantly across applications.
- **Low (50-65%):** The tool's performance on non-English content or specialized technical domains remains untested.

## Next Checks
1. **Domain Transfer Test**: Evaluate FACTS&EVIDENCE on a dataset of LLM-generated code or technical documentation. Compare F1 scores against the FavaBench results to quantify domain-specific performance degradation.

2. **Source Diversity Stress Test**: Create a test suite where evidence sources for the same claim span the quality spectrum (high-quality scientific papers to low-quality blogs). Measure how the credibility score changes when users toggle source categories and whether the aggregation mechanism maintains robustness.

3. **Latency-Performance Trade-off Analysis**: Systematically vary the number of evidence items (1, 2, 3, 5, 7) and context window sizes on a fixed dataset. Plot the F1-latency curve to identify the optimal configuration for different use cases (real-time vs. batch processing).