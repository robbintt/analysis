---
ver: rpa2
title: Grounding Computer Use Agents on Human Demonstrations
arxiv_id: '2511.07332'
source_url: https://arxiv.org/abs/2511.07332
tags:
- element
- arxiv
- wang
- elements
- desktop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces GROUNDCUA, a large-scale expert-annotated
  dataset for desktop GUI grounding, containing over 3.56 million UI element annotations
  across 56K screenshots from 87 applications. The dataset addresses the lack of high-quality
  resources for desktop environments, capturing dense, high-resolution interfaces
  with small elements and diverse real-world tasks.
---

# Grounding Computer Use Agents on Human Demonstrations

## Quick Facts
- arXiv ID: 2511.07332
- Source URL: https://arxiv.org/abs/2511.07332
- Reference count: 40
- The paper introduces GROUNDCUA, a large-scale expert-annotated dataset for desktop GUI grounding, containing over 3.56 million UI element annotations across 56K screenshots from 87 applications.

## Executive Summary
This paper introduces GROUNDCUA, a large-scale expert-annotated dataset for desktop GUI grounding, containing over 3.56 million UI element annotations across 56K screenshots from 87 applications. The dataset addresses the lack of high-quality resources for desktop environments, capturing dense, high-resolution interfaces with small elements and diverse real-world tasks. Using GROUNDCUA, the authors train GROUNDNEXT, a family of vision-language models (3B and 7B) that map natural language instructions to on-screen UI elements. GROUNDNEXT achieves state-of-the-art performance across five benchmarks, including ScreenSpot-Pro, OSWorld-G, and UI-Vision, using only 700K SFT datapoints—far fewer than prior models trained on millions of samples. RL post-training further improves results, and agentic evaluations on OSWorld-Verified show that the 3B model outperforms many larger and proprietary systems. The work demonstrates that high-quality, expert-driven datasets enable efficient and effective grounding, advancing the development of reliable computer-use agents.

## Method Summary
The paper introduces GROUNDCUA, a large-scale expert-annotated dataset for desktop GUI grounding, containing over 3.56 million UI element annotations across 56K screenshots from 87 applications. The dataset addresses the lack of high-quality resources for desktop environments, capturing dense, high-resolution interfaces with small elements and diverse real-world tasks. Using GROUNDCUA, the authors train GROUNDNEXT, a family of vision-language models (3B and 7B) that map natural language instructions to on-screen UI elements. GROUNDNEXT achieves state-of-the-art performance across five benchmarks, including ScreenSpot-Pro, OSWorld-G, and UI-Vision, using only 700K SFT datapoints—far fewer than prior models trained on millions of samples. RL post-training further improves results, and agentic evaluations on OSWorld-Verified show that the 3B model outperforms many larger and proprietary systems. The work demonstrates that high-quality, expert-driven datasets enable efficient and effective grounding, advancing the development of reliable computer-use agents.

## Key Results
- GROUNDNEXT achieves state-of-the-art performance across five benchmarks, including ScreenSpot-Pro, OSWorld-G, and UI-Vision, using only 700K SFT datapoints.
- RL post-training further improves results, and agentic evaluations on OSWorld-Verified show that the 3B model outperforms many larger and proprietary systems.
- The work demonstrates that high-quality, expert-driven datasets enable efficient and effective grounding, advancing the development of reliable computer-use agents.

## Why This Works (Mechanism)
The paper introduces GROUNDCUA, a large-scale expert-annotated dataset for desktop GUI grounding, containing over 3.56 million UI element annotations across 56K screenshots from 87 applications. The dataset addresses the lack of high-quality resources for desktop environments, capturing dense, high-resolution interfaces with small elements and diverse real-world tasks. Using GROUNDCUA, the authors train GROUNDNEXT, a family of vision-language models (3B and 7B) that map natural language instructions to on-screen UI elements. GROUNDNEXT achieves state-of-the-art performance across five benchmarks, including ScreenSpot-Pro, OSWorld-G, and UI-Vision, using only 700K SFT datapoints—far fewer than prior models trained on millions of samples. RL post-training further improves results, and agentic evaluations on OSWorld-Verified show that the 3B model outperforms many larger and proprietary systems. The work demonstrates that high-quality, expert-driven datasets enable efficient and effective grounding, advancing the development of reliable computer-use agents.

## Foundational Learning
- **Desktop GUI grounding**: The task of mapping natural language instructions to specific UI elements on a desktop screen. This is needed because existing datasets and models focus on mobile interfaces, leaving a gap for desktop environments. Quick check: Can the model accurately identify UI elements in complex, high-resolution desktop screenshots?
- **Vision-language models**: Models that combine visual and textual information to understand and generate responses. This is needed because grounding requires understanding both the visual layout of the UI and the semantic meaning of the instruction. Quick check: Does the model correctly interpret the relationship between the instruction and the UI elements?
- **Expert annotations**: High-quality, human-labeled data used to train and evaluate models. This is needed because accurate annotations are crucial for training models to perform precise grounding tasks. Quick check: Are the annotations consistent and accurate across different annotators and applications?

## Architecture Onboarding
- **Component map**: Instruction -> Vision Encoder -> Text Encoder -> Grounding Head -> UI Element Coordinates
- **Critical path**: The critical path involves processing the instruction through the text encoder, the screenshot through the vision encoder, and combining these representations in the grounding head to predict UI element coordinates.
- **Design tradeoffs**: The choice between a 3B and 7B model involves balancing model size and performance. The 3B model is more efficient and still outperforms larger models, suggesting that dataset quality may be more important than model size.
- **Failure signatures**: The model may struggle with very small UI elements, dense interfaces, or instructions that require understanding context beyond the immediate UI layout.
- **First experiments**:
  1. Test the model on a held-out set of screenshots from applications not seen during training to evaluate generalization.
  2. Perform an ablation study by training the model on a subset of GROUNDCUA to determine the minimum dataset size needed for good performance.
  3. Evaluate the model's performance on a set of synthetic instructions designed to test specific aspects of grounding, such as spatial reasoning or understanding of UI hierarchies.

## Open Questions the Paper Calls Out
None

## Limitations
- The major uncertainty in this work centers on whether the performance gains from GROUNDNEXT stem primarily from the dataset quality versus the model architecture and training pipeline. While the authors claim GROUNDCUA's expert annotations and high resolution are key differentiators, direct ablation studies comparing training on GROUNDCUA versus other datasets with identical model configurations are not provided.
- Confidence in the main claims is High for the dataset construction and annotation process, as the scale (3.56M+ annotations), diversity (87 applications), and expert involvement are well-documented. Confidence in the benchmark results is Medium because while multiple datasets are evaluated, some are synthetic or lack real-world deployment validation.
- The claim that GROUNDNEXT "outperforms larger and proprietary systems" on OSWorld-Verified is supported but could benefit from more diverse task types and longer-term user studies.

## Confidence
- Dataset construction and annotation process: High
- Benchmark results: Medium
- Outperformance claims: Medium

## Next Checks
1. Ablation study: Train GROUNDNEXT on GROUNDCUA vs. UI-Vision or synthetic datasets using identical hyperparameters to quantify dataset impact.
2. Long-horizon task evaluation: Test grounding accuracy on extended, multi-step workflows requiring sustained context retention and error recovery.
3. Cross-device and platform robustness: Evaluate performance on different screen sizes, resolutions, and operating systems not represented in the training set.