---
ver: rpa2
title: 'Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable
  AI Coding Agents'
arxiv_id: '2506.20062'
source_url: https://arxiv.org/abs/2506.20062
tags:
- code
- computing
- agent
- reasoning
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COPILOTLENS, a framework designed to make
  AI code generation more transparent by reconstructing an AI coding agent's reasoning
  process. Current AI code assistants like GitHub Copilot and Cursor lack explainability,
  hindering developers' ability to critically evaluate outputs and form accurate mental
  models.
---

# Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents

## Quick Facts
- arXiv ID: 2506.20062
- Source URL: https://arxiv.org/abs/2506.20062
- Reference count: 40
- Key outcome: Introduces COPILOTLENS, a two-level explanation framework that reconstructs AI coding agents' reasoning to enable critical evaluation and calibrated trust through immediate summaries (Level 1) and deep contextual analysis (Level 2)

## Executive Summary
This paper addresses the opacity of AI code generation tools like GitHub Copilot and Cursor, which hinder developers' ability to critically evaluate outputs and form accurate mental models. COPILOTLENS is introduced as a framework that provides transparent explanations by reconstructing an AI coding agent's reasoning process through a two-level approach. The system operates as a model-agnostic explanation layer built on top of Kilo Code, offering immediate post-hoc summaries of code modifications (Level 1) and on-demand deep contextual analysis linking changes to specific codebase influences and alternatives (Level 2). By illuminating the AI's decision-making process, the framework aims to support critical evaluation and foster calibrated trust among both novice and professional developers.

## Method Summary
The method involves creating an interactive explanation layer that reconstructs AI coding agents' reasoning processes post-hoc. The system intercepts outputs from coding agents (specifically Kilo Code), then generates two levels of explanations: Level 1 provides immediate, post-hoc summaries of file modifications with highlighting and sequential navigation, while Level 2 offers on-demand deep analysis including Codebase Influences, Coding Conventions, Implementation Reasoning, and Alternative Implementations. The approach is model-agnostic, analyzing completed agent outputs rather than requiring white-box access to the underlying model. The framework requires setting up Kilo Code, implementing interception hooks, building UI components for both explanation levels, and creating prompts to generate context-aware analysis linking code changes to specific project elements.

## Key Results
- Introduces COPILOTLENS, a two-level explanation framework for AI coding agents
- Addresses opacity in tools like GitHub Copilot and Cursor through post-hoc reasoning reconstruction
- Provides immediate summaries (Level 1) and deep contextual analysis (Level 2) linking changes to codebase influences
- Operates as a model-agnostic explanation layer built on Kilo Code

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-level explanation framework may reduce cognitive load while preserving depth for users who need it.
- Mechanism: Level 1 provides immediate, concise post-hoc summaries ("what changed"), while Level 2 offers on-demand deep analysis ("why it was done this way"). This separation allows users to self-select into deeper engagement only when needed.
- Core assumption: Users can accurately assess when they need deeper explanation versus when a summary suffices.
- Evidence anchors: [abstract] "Level 1 offers immediate, post-hoc summaries of code modifications, while Level 2 provides on-demand, deep contextual analysis" [section 4.2] "The central design principle is a two-level dynamic explanation framework designed to manage the inherent trade-off between informational quality and cognitive load"
- Break condition: If Level 1 summaries are incomplete or misleading, users may develop false confidence and never trigger Level 2, worsening miscalibrated trust.

### Mechanism 2
- Claim: Surfacing codebase influences and coding conventions may improve critical evaluation by making the AI's reasoning tangible.
- Mechanism: Level 2 explicitly links generated code to specific files, documentation, naming conventions, and architectural patterns in the project. This provides verifiable evidence the AI used appropriate context, enabling users to validate alignment with project standards.
- Core assumption: The explanation layer can accurately identify which codebase elements influenced the agent's decisions.
- Evidence anchors: [section 4.4] "The system first identifies Codebase Influences by surfacing the existing functional components that likely guided the agent's implementation" [section 3.2] "Research shows that integrating architectural, API, and file-level context is critical for improving a user's ability to evaluate generated suggestions"
- Break condition: If influence attribution is inaccurate (e.g., hallucinated links to irrelevant files), trust may decrease or users may be misled into accepting flawed code.

### Mechanism 3
- Claim: Post-hoc reconstruction of reasoning enables model-agnostic explainability without requiring white-box access.
- Mechanism: Instead of instrumenting the model during generation, COPILOTLENS analyzes completed agent outputs to reconstruct a plausible reasoning chain. This allows use with any coding agent (proprietary or open-weight) without latency penalties from real-time interpretability methods.
- Core assumption: Post-hoc reconstruction produces explanations that faithfully reflect the agent's actual decision process, not just plausible post-rationalizations.
- Evidence anchors: [section 4.1] "By instead reconstructing the agent's 'thought process' from its final outputs, COPILOTLENS remains model-agnostic" [section 4.1] "Current techniques often require privileged 'white-box' model access... risk introducing significant latency or producing low-level, token-centric explanations"
- Break condition: If reconstruction is fundamentally divergent from actual agent reasoning, explanations become rationalizations rather than faithful accounts, potentially reinforcing flawed mental models.

## Foundational Learning

- Concept: **Mental models in human-AI collaboration**
  - Why needed here: The paper frames its entire intervention around fixing misaligned mental models caused by opaque AI suggestions.
  - Quick check question: Can you explain the difference between what an AI coding agent actually does versus what you assume it does when it makes a suggestion?

- Concept: **Cognitive load tradeoffs in explainable AI**
  - Why needed here: The two-level design explicitly trades off information depth against cognitive burden.
  - Quick check question: What happens to user decision quality when explanations are too detailed versus too sparse?

- Concept: **Calibrated trust vs. overreliance**
  - Why needed here: The paper targets "fragile, uncalibrated trust" as a key failure mode of current tools.
  - Quick check question: How would you detect whether you're trusting an AI assistant appropriately versus over-relying on it?

## Architecture Onboarding

- Component map: Agent output interceptor -> Level 1 summary generator -> Level 2 deep analyzer -> Evidence linker -> UI layer
- Critical path: 1. Agent completes task -> 2. Output intercepted -> 3. Level 1 summary generated and displayed -> 4. User optionally triggers Level 2 -> 5. Deep analyzer queries codebase -> 6. Structured explanation rendered with evidence links
- Design tradeoffs: Post-hoc vs. real-time (sacrifices reasoning fidelity for model-agnosticism and latency), summary depth vs. cognitive load (Level 1 may be too thin for complex changes, Level 2 may overwhelm in large codebases), reconstruction fidelity (explanations are inferred, not grounded in actual model internals)
- Failure signatures: Users never trigger Level 2, suggesting Level 1 is "good enough" or friction is too high; users report explanations don't match their mental model of why code was generated; evidence links point to irrelevant or incorrect files (hallucinated influences); cognitive overload complaints when Level 2 is used on large, multi-file changes
- First 3 experiments: 1. A/B test with/without Level 2 access: Measure whether deep explanations change acceptance rates, error detection, and self-reported trust calibration. 2. Influence attribution accuracy audit: Manually verify whether surfaced "codebase influences" actually plausibly affected generated code across 50+ tasks. 3. Cognitive load measurement: Use dual-task or self-report methods to assess whether Level 1/2 separation successfully manages load compared to a single-depth explanation baseline.

## Open Questions the Paper Calls Out
- Does the two-level explanation framework impose a cognitive load that degrades developer productivity in efficiency-driven workflows?
- To what extent do the post-hoc explanations accurately reflect the actual causal influences and reasoning of the underlying coding agent?
- Does exposing AI reasoning and alternatives actually calibrate trust by reducing over-reliance, or does the verbosity of explanations lead to an "illusion of understanding"?

## Limitations
- Post-hoc reconstruction may not accurately reflect actual agent reasoning processes
- Two-level design may not effectively balance cognitive load across diverse user types
- Influence attribution may hallucinate connections to irrelevant codebase elements

## Confidence
- Mechanism 1 (two-level cognitive load): Medium
- Mechanism 2 (codebase influence attribution): Medium
- Mechanism 3 (post-hoc reconstruction fidelity): Low

## Next Checks
1. Conduct A/B testing comparing developer performance with/without Level 2 explanations on complex multi-file tasks
2. Perform blind review of 50+ explanations to assess whether identified codebase influences plausibly affected generated code
3. Measure explanation generation latency and user-reported cognitive load across tasks of varying complexity