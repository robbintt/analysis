---
ver: rpa2
title: 'ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution'
arxiv_id: '2502.00989'
source_url: https://arxiv.org/abs/2502.00989
tags:
- chart
- table
- chartcitor
- answer
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChartCitor is a multi-agent LLM framework for fine-grained chart
  visual attribution that addresses hallucination in chart QA by grounding LLM-generated
  answers with precise bounding box citations. The system orchestrates six specialized
  agents to extract structured data tables from charts, reformulate answers into logical
  steps, generate contextual entity descriptions, retrieve supporting evidence through
  pre-filtering and re-ranking, and localize cited cells in chart images.
---

# ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution

## Quick Facts
- arXiv ID: 2502.00989
- Source URL: https://arxiv.org/abs/2502.00989
- Reference count: 24
- Multi-agent LLM framework achieving 27.4% IoU for chart visual attribution

## Executive Summary
ChartCitor is a multi-agent framework that addresses hallucination in LLM-based chart question answering by grounding answers with precise visual citations. The system uses six specialized agents to extract structured data tables from charts, reformulate questions, generate contextual entity descriptions, retrieve supporting evidence, and localize cited cells in chart images. ChartCitor achieves 27.4% IoU for chart attribution, significantly outperforming baselines like Kosmos-2 (3.89%) and LISA (4.34%) by 9-15%. User studies show 41% of citations rated completely accurate versus 28% for direct GPT-4o prompting, demonstrating enhanced trust and verification utility.

## Method Summary
ChartCitor employs a multi-agent architecture to perform fine-grained visual attribution for chart QA tasks. The framework uses an Extractor Agent to convert chart images into structured data tables, a Reformulator Agent to generate intermediate reasoning steps, a Describer Agent to create contextual entity descriptions, a Retriever Agent to fetch supporting evidence through pre-filtering and re-ranking, and a Visualizer Agent to localize cited cells with bounding boxes. The system grounds LLM-generated answers by providing verifiable evidence through visual citations, addressing the hallucination problem common in chart QA systems.

## Key Results
- Achieves 27.4% IoU for chart attribution, outperforming Kosmos-2 (3.89%), LISA (4.34%), GPT-4V (12.5%), and Claude-3.5 (13.8%) by 9-15%
- User studies show 41% of citations rated completely accurate versus 28% for direct GPT-4o prompting
- Increases overall QA accuracy from 90.9% to 97.0% (6.1% improvement)
- Reduces verification time from hours to minutes for professionals

## Why This Works (Mechanism)
The framework's success stems from its systematic approach to grounding LLM answers with verifiable visual evidence. By breaking down the chart QA task into specialized agent roles, ChartCitor ensures each component handles a specific aspect of the attribution process. The Reformulator Agent generates intermediate reasoning steps that create logical evidence chains, while the Visualizer Agent provides precise bounding box localization for cited entities. This multi-stage approach prevents the LLM from making unsupported claims by requiring evidence retrieval and visual citation at each step.

## Foundational Learning
- **Bounding box annotation**: Essential for training visual localization models; quick check: verify annotation accuracy using IoU metrics
- **LLM hallucination mitigation**: Understanding how multi-agent systems can ground LLM outputs; quick check: compare hallucination rates between single-agent and multi-agent approaches
- **Chart data extraction**: Converting visual chart elements to structured tables; quick check: validate extracted data against ground truth tables
- **Evidence chain construction**: Building logical reasoning paths for answers; quick check: trace citation paths to ensure completeness
- **Multi-agent orchestration**: Coordinating specialized agents for complex tasks; quick check: measure latency impact of each agent in the pipeline

## Architecture Onboarding

**Component map**: Extractor -> Reformulator -> Describer -> Retriever -> Visualizer -> LLM

**Critical path**: The essential workflow involves chart image input flowing through Extractor Agent to create structured data, Reformulator Agent to generate reasoning steps, Retriever Agent to find supporting evidence, and Visualizer Agent to create bounding box citations that ground the final LLM answer.

**Design tradeoffs**: The system trades computational complexity for accuracy by using multiple specialized agents instead of a single end-to-end model. This increases latency but provides verifiable visual citations and reduces hallucination.

**Failure signatures**: 
- Poor bounding box localization indicates Extractor or Visualizer Agent failure
- Incomplete evidence chains suggest Reformulator or Retriever Agent issues
- Hallucinated answers point to LLM grounding problems
- Slow response times indicate orchestration bottlenecks

**First experiments**:
1. Test Extractor Agent performance on simple bar charts with known ground truth tables
2. Evaluate Reformulator Agent's ability to generate logical reasoning steps for straightforward questions
3. Validate Visualizer Agent's bounding box accuracy on single data point citations

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Requires extensive preprocessing including bounding box annotation for training
- Evaluation limited to three chart types (bar, line, pie), raising questions about performance on complex or novel formats
- Reliance on multiple specialized agents introduces complexity that could affect production reliability
- 6.1% accuracy improvement needs verification across diverse chart datasets

## Confidence
- **Medium**: 27.4% IoU metric confidence due to limited evaluation scale (36 questions)
- **Low**: Time savings claims lack broader user studies across professional contexts
- **Medium**: User study results confidence due to small sample size and potential selection bias

## Next Checks
1. Conduct a larger-scale user study with 100+ participants across different professional domains to validate claimed time savings and accuracy improvements in real-world settings.

2. Evaluate the framework's performance on a more diverse chart corpus including scatter plots, histograms, heatmaps, and complex multi-chart dashboards to assess generalizability beyond the three tested chart types.

3. Perform ablation studies to quantify the individual contributions of each specialized agent and assess system performance when some agents are disabled or replaced with simpler alternatives.