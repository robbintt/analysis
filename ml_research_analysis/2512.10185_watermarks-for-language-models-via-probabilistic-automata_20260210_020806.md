---
ver: rpa2
title: Watermarks for Language Models via Probabilistic Automata
arxiv_id: '2512.10185'
source_url: https://arxiv.org/abs/2512.10185
tags:
- watermarking
- wepa
- text
- distribution
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new class of watermarking schemes for language
  models constructed through probabilistic automata. The proposed method, WEPA (Watermarking
  schemeE through Probabilistic Automata), addresses limitations in existing watermarking
  approaches, particularly their reduced generation diversity and high detection overhead.
---

# Watermarks for Language Models via Probabilistic Automata

## Quick Facts
- arXiv ID: 2512.10185
- Source URL: https://arxiv.org/abs/2512.10185
- Authors: Yangkun Wang; Jingbo Shang
- Reference count: 40
- This paper introduces WEPA, a new watermarking scheme using probabilistic automata that improves generation diversity from Θ(λ) to Ω(λᵈ n) and reduces detection time complexity from Θ(λn k²) to Θ(λn).

## Executive Summary
This paper introduces WEPA (Watermarking schemeE through Probabilistic Automata), a novel watermarking framework for language models that addresses key limitations in existing approaches. By organizing watermark keys as probabilistic automata with regular graph topologies, WEPA achieves exponential generation diversity while maintaining computational efficiency. The scheme demonstrates superior performance in robustness to edit-based attacks and detection speed compared to prior watermarking methods, validated through extensive experiments on LLaMA-3B and Mistral-7B.

## Method Summary
The WEPA framework constructs watermark keys as probabilistic automata (PA) with λ virtual states arranged in a d-regular graph, where each state transitions to d successors. During generation, subordinate PAs produce noise vectors that feed an exponential-minimum decoder to select output tokens. Detection computes generalized Levenshtein distance between the generated sequence and the PA's support language via dynamic programming, achieving O(λn) complexity. The approach enables robust detection under insertions, deletions, and substitutions while improving generation diversity from Θ(λ) to Ω(λᵈ n).

## Key Results
- Generation diversity improves from Θ(λ) to Ω(λᵈ n) while maintaining detection efficiency
- Detection time complexity reduced from Θ(λnk²) to Θ(λn) via generalized Levenshtein distance
- Experimental validation shows superior robustness to edit-based attacks compared to EXP baseline
- Theoretical framework establishes learnability conditions for watermarking output distributions under PAC framework

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Automata Watermark Structure
Organizing watermark keys as probabilistic automata with d-regular graph topologies enables both efficient detection and increased generation diversity. The automaton produces noise vectors from subordinate PAs that feed an exponential-minimum decoder, allowing exponential growth in possible output sequences while maintaining a compact state representation.

### Mechanism 2: Detection via Generalized Levenshtein Distance on Automaton Paths
Computing Levenshtein distance against the PA's support language enables robust detection under edits with O(λn) complexity. The asymmetric insertion (γᵢ=2) and deletion (γd=0) costs handle length mismatches while maintaining computational efficiency through dynamic programming.

### Mechanism 3: Undetectability via PAC-Unlearnable PNFAs
Watermarking schemes represented by probabilistic non-deterministic finite automata (PNFA) can achieve undetectability under sparse LPN hardness assumptions. The distinction between PAC-learnable PDFAs and PAC-unlearnable PNFAs determines whether adversaries can detect the watermark structure even with polynomial query access.

## Foundational Learning

- **Concept: Probabilistic Automata (PA) / PDFA vs PNFA**
  - Why needed here: The entire framework depends on understanding that PDFAs have deterministic transitions per symbol (PAC-learnable), while PNFAs allow multiple transitions (harder to learn). This distinction determines whether a watermark is detectable.
  - Quick check question: Can you explain why a PDFA with μ-distinguishable states is PAC-learnable, but a general PNFA may not be?

- **Concept: Edit Distance (Levenshtein) for Sequence Alignment**
  - Why needed here: Detection robustness relies on computing minimum-cost alignments between generated text and automaton paths under insertions, deletions, and substitutions.
  - Quick check question: Why does the paper use asymmetric costs (γᵢ ≠ γd) in the generalized Levenshtein definition?

- **Concept: PAC Learning and Cryptographic Hardness (LPN)**
  - Why needed here: The theoretical undetectability claim rests on the connection between PAC-unlearnability and cryptographic pseudorandomness via sparse LPN.
  - Quick check question: How does the hardness of learning parity functions with noise translate to watermark undetectability?

## Architecture Onboarding

- **Component map:**
  Key Generation (Gen) → PA Construction (λ states, d-regular) → Subordinate PA → noise ξᵢ → Decoder Γ → token yᵢ → Detection: Sequence y → DP over PA support NFA → d_L(y, M) → p-value

- **Critical path:** PA state topology (d, λ) → noise diversity → detection complexity. The d parameter controls the expressiveness/efficiency tradeoff.

- **Design tradeoffs:**
  - d=1: Maximum detection power, lowest diversity (matches prior work)
  - d=2: Higher diversity, slightly weaker detection (Figure 2)
  - bitwidth b: Higher b improves detection but reduces diversity; stabilize at b≥6 (Figure 9)
  - key length λ: Larger λ improves security but increases p-values (Figure 8)

- **Failure signatures:**
  - Low-entropy prompts → deterministic outputs → watermark cannot embed
  - Paraphrasing attacks → outside threat model (not defended)
  - γᵢ too low → spurious alignments → false positives

- **First 3 experiments:**
  1. Baseline comparison: Run WEPA (d=1, d=2) vs EXP on C4 news subset with 4-20 token generations; verify p-value distributions match Figure 2.
  2. Robustness test: Apply substitution/deletion/insertion corruption at 0-80% rates to 50-token generations; confirm median p-values remain below 0.5 (Figure 3).
  3. Efficiency validation: Time detection on 256-token sequences; target <10s for WEPA vs >1000s for EXP baseline (Table 1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can tight analytical bounds on the p-value distribution under the null hypothesis be derived for PA-based watermarking schemes?
- Basis in paper: Appendix A states: "the lack of tight analytical bounds on the p-value remains a challenge, as the edit distance metric is inherently difficult to analyze."
- Why unresolved: The edit distance metric's combinatorial complexity makes formal analysis difficult; current bounds rely on the Vysochanskij–Petunin inequality which may be loose.
- What evidence would resolve it: A closed-form or polynomial-time computable bound on p-values with provable tightness guarantees.

### Open Question 2
- Question: Can an asymmetric watermarking scheme be developed where detection does not require the private key?
- Basis in paper: Appendix A states: "the detection algorithm requires knowledge of the private key, which limits detection to only the key holder. Developing an asymmetric decoding and detection system could improve security."
- Why unresolved: Current PA-based schemes require key-dependent automaton traversal; asymmetric variants would require fundamentally new cryptographic constructions.
- What evidence would resolve it: A scheme enabling public verification without compromising key security or undetectability properties.

### Open Question 3
- Question: Can PA-based watermarking schemes be extended to resist semantic paraphrasing attacks?
- Basis in paper: Section 6.2 states: "our watermark is specifically designed to resist edit-based perturbations, and does not aim to defend against semantic or paraphrasing attacks."
- Why unresolved: Paraphrasing attacks fundamentally alter token sequences beyond edit-distance metrics, potentially breaking automaton alignment.
- What evidence would resolve it: Detection robustness evaluations against paraphrasing models (e.g., DIPPER, GPT-based rewriters) with maintained low false positive rates.

## Limitations
- Detection requires private key knowledge, limiting verification to key holders
- No defense against semantic paraphrasing attacks that alter token sequences fundamentally
- Theoretical undetectability construction relies on sparse LPN hardness assumption

## Confidence
- Claims about generation diversity improvement: High
- Claims about detection efficiency improvement: High
- Claims about experimental robustness: Medium
- Theoretical undetectability guarantees: Low (cryptographic assumptions unverified)

## Next Checks
1. Verify that PA state traversal produces correct noise vectors by comparing with Figure 11's subordinate PA structure
2. Confirm that detection algorithm correctly computes generalized Levenshtein distance with asymmetric costs by testing on synthetic sequences
3. Validate that p-value distributions match experimental results in Figure 2 for both d=1 and d=2 configurations