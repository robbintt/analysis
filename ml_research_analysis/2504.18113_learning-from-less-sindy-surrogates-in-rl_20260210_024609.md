---
ver: rpa2
title: 'Learning from Less: SINDy Surrogates in RL'
arxiv_id: '2504.18113'
source_url: https://arxiv.org/abs/2504.18113
tags:
- surrogate
- environment
- sindy
- environments
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SINDy-based surrogate environments for reinforcement
  learning, addressing the challenge of data-intensive training by leveraging the
  Sparse Identification of Nonlinear Dynamics algorithm to create interpretable, computationally
  efficient models. The method uses pre-trained RL agents to collect minimal state
  transition data (75 for Mountain Car, 1000 for Lunar Lander), then applies SINDy
  to derive sparse governing equations that replace physics engines in OpenAI Gym
  environments.
---

# Learning from Less: SINDy Surrogates in RL

## Quick Facts
- arXiv ID: 2504.18113
- Source URL: https://arxiv.org/abs/2504.18113
- Reference count: 1
- Achieves >0.997 state correlation with MSE as low as 3.11e-06 using only 75-1000 transitions

## Executive Summary
This paper introduces SINDy-based surrogate environments for reinforcement learning, addressing the challenge of data-intensive training by leveraging the Sparse Identification of Nonlinear Dynamics algorithm to create interpretable, computationally efficient models. The method uses pre-trained RL agents to collect minimal state transition data (75 for Mountain Car, 1000 for Lunar Lander), then applies SINDy to derive sparse governing equations that replace physics engines in OpenAI Gym environments. Experimental results show state-wise correlations exceeding 0.997 with mean squared errors as low as 3.11e-06 for Mountain Car velocity and 1.42e-06 for Lunar Lander position, while reducing computational costs by 20-35%. RL agents trained in these surrogate environments require fewer total steps (65,075 vs. 100,000 for Mountain Car and 801,000 vs. 1,000,000 for Lunar Lander) while achieving comparable performance to those trained in original environments, demonstrating the approach's effectiveness for sample-efficient, interpretable surrogate environment development.

## Method Summary
The method involves collecting state transitions from pre-trained RL agents with ε-greedy exploration (ε=0.2), then using SINDy's STLSQ algorithm to identify sparse governing equations from these transitions. The learned equations replace the physics engine in OpenAI Gym environments, creating computationally efficient surrogates. The approach was validated on Mountain Car (2D state) and Lunar Lander (6D state), with library functions hand-selected based on environment characteristics—trigonometric terms for Mountain Car's oscillatory dynamics and polynomial terms for Lunar Lander's smoother dynamics. Surrogate environments were then used to train RL agents, achieving comparable performance to those trained in original environments while requiring fewer total training steps.

## Key Results
- Achieved state-wise correlations exceeding 0.997 for both Mountain Car and Lunar Lander
- Mean squared errors as low as 3.11e-06 (Mountain Car velocity) and 1.42e-06 (Lunar Lander position)
- Reduced computational costs by 20-35% compared to training in original environments
- Required only 65,075 steps for Mountain Car and 801,000 steps for Lunar Lander versus 100,000 and 1,000,000 respectively in original environments

## Why This Works (Mechanism)

### Mechanism 1: Sparse Regression Recovers Compact Dynamics from Limited Transitions
- Claim: SINDy's sparse regression can identify minimal governing equations from remarkably few state transitions when underlying dynamics have low intrinsic complexity.
- Mechanism: The Sequential Thresholded Least Squares (STLSQ) algorithm iteratively fits coefficients on a library of candidate functions (polynomials, trigonometric terms), then thresholds small coefficients to zero. This yields parsimonious models that exclude spurious terms while retaining essential dynamics.
- Core assumption: The true environment dynamics are sparse in the chosen basis library (Assumption: environments with highly irregular or discontinuous dynamics may not satisfy this).
- Evidence anchors:
  - [abstract] "With only 75 interactions for Mountain Car and 1000 for Lunar Lander, we achieve state-wise correlations exceeding 0.997"
  - [section 2.2] "Prediction errors guided the progressive addition of nonlinear terms (trigonometric for Mountain Car, polynomial for Lunar Lander)"
  - [corpus] Limited direct corpus support for SINDy-specific mechanisms; neighboring papers focus on surrogate ODEs and sample-efficient learning broadly rather than sparse identification specifically.
- Break condition: If state space dimensionality grows large or dynamics require basis functions not in the library (e.g., discontinuous contacts, hybrid modes), sparsity assumptions may fail and MSE will plateau despite added data.

### Mechanism 2: High-Fidelity Surrogate Enables Policy Transfer via Dynamics Preservation
- Claim: When surrogate dynamics achieve very high correlation (>0.99) and low MSE, policies trained in surrogates transfer with comparable performance to original environments.
- Mechanism: The surrogate replaces the physics engine with the learned SINDy transition function $s_{t+1} = f_{SINDy}(s_t, a_t)$ while preserving reward structure and episode termination. Agents learn identical strategies because the decision-relevant structure (momentum building regions, control boundaries) is preserved within numerical tolerance.
- Core assumption: Reward signals and termination conditions in the surrogate sufficiently match the original environment; small dynamics errors do not compound into divergent trajectories during training.
- Evidence anchors:
  - [section 3.1] "For Mountain Car, both policies learned identical strategies: momentum building in valley regions (blue), oscillatory behavior in middle regions, and stabilization near the goal (red)"
  - [section 3.1] "Both policies demonstrate identical velocity control and attitude regulation patterns, confirming the surrogate environment's ability to enable effective policy learning"
  - [corpus] "On Approaches to Building Surrogate ODE Models for Diffusion Bridges" discusses surrogate fidelity for generative modeling but does not directly validate policy transfer in RL contexts.
- Break condition: If surrogate dynamics drift accumulates over long horizons, or if key state regions are under-sampled during data collection, learned policies may exploit surrogate artifacts not present in the true environment.

### Mechanism 3: Pre-Trained Agent Data Collection Maximizes Information Density
- Claim: Collecting transitions from pre-trained policies with ε-greedy exploration yields diverse, information-rich samples far more efficiently than random exploration.
- Mechanism: Pre-trained SAC (Mountain Car) and PPO (Lunar Lander) agents visit high-value regions of state space. The ε=0.2 random action component ensures coverage of exploratory regions without wasting samples on low-probability states.
- Core assumption: Pre-trained policies are available or can be obtained; the state distribution induced by near-optimal policies plus noise covers the regions needed for downstream policy learning.
- Evidence anchors:
  - [section 2.1] "This strategy provided diverse state-action pairs covering both optimal paths and exploratory regions of the state space"
  - [section 3.2] "SINDy required only 75 and 1,000 environment interactions for Mountain Car and Lunar Lander respectively, compared to tens of thousands typically needed for model-free approaches"
  - [corpus] "Sample-Efficient Behavior Cloning Using General Domain Knowledge" addresses sample efficiency through domain knowledge rather than data collection strategy specifically.
- Break condition: If the pre-trained policy is suboptimal or the target task differs significantly from the source task, collected data may miss critical state regions, yielding surrogate models that fail on out-of-distribution states.

## Foundational Learning

- Concept: **Sparse Identification of Nonlinear Dynamics (SINDy)**
  - Why needed here: Core algorithm enabling interpretable, data-efficient dynamics modeling. Understanding how library selection and thresholding affect model quality is essential for implementation.
  - Quick check question: Given state variables [position, velocity] and control [force], which library functions would you include to capture $v_{t+1} = v_t + \alpha \cdot \text{force} - \beta \cdot \sin(\text{position})$?

- Concept: **Model-Based Reinforcement Learning**
  - Why needed here: The paper positions SINDy surrogates within model-based RL. Understanding the tradeoff between model bias and sample efficiency clarifies why high-fidelity surrogates matter.
  - Quick check question: If a surrogate model has 1% error per transition, what is the approximate accumulated error after 100 steps under naive compounding assumptions?

- Concept: **OpenAI Gym Environment Interface**
  - Why needed here: The surrogate environment must implement the standard Gym API (reset, step, render) to be compatible with existing RL algorithms.
  - Quick check question: What does the `step(action)` method return, and which component does the SINDy model replace?

## Architecture Onboarding

- Component map:
  1. Data Collector: Pre-trained agent + ε-greedy wrapper → CSV of (s_t, a_t, s_{t+1}) transitions
  2. SINDy Model Builder: Library configuration → STLSQ fitting → sparse coefficient matrix
  3. Surrogate Environment: Gym wrapper with SINDy transition function replacing physics engine
  4. RL Training Loop: Standard algorithm (PPO, SAC, etc.) trains on surrogate environment
  5. Policy Validator: Evaluate trained policy on original environment to verify transfer

- Critical path:
  1. Obtain or train a reasonable policy for the target environment (can be suboptimal)
  2. Collect N transitions (paper suggests 75-1000 depending on complexity) with ε-greedy
  3. Select library functions based on domain knowledge (trigonometric for oscillatory, polynomial for smooth)
  4. Run STLSQ with threshold grid search; validate on held-out transitions
  5. Wrap SINDy model in Gym environment; train RL agent; validate on original

- Design tradeoffs:
  - **Library complexity vs. sparsity**: Larger libraries capture more dynamics but risk overfitting and lose interpretability
  - **Threshold selection**: Higher thresholds yield sparser models but may drop important terms; paper used grid search
  - **Data collection strategy**: More exploration (higher ε) improves coverage but reduces sample quality in optimal regions
  - **Assumption**: Paper does not report sensitivity analysis on ε or threshold ranges; optimal values may be environment-specific

- Failure signatures:
  - MSE plateaus despite adding data → library missing required basis functions
  - Policy transfer fails (large performance gap) → surrogate dynamics drift or missing state regions
  - Learned equations include implausibly large coefficients → threshold too low, overfitting to noise
  - Surrogate trajectories diverge rapidly → integration instability; may need smaller timesteps or regularization

- First 3 experiments:
  1. Replicate Mountain Car with 75 transitions, trigonometric library; verify MSE < 1e-5 and policy matches Figure 1 qualitatively
  2. Ablate library functions: train with polynomials only vs. polynomials + trigonometric; quantify MSE difference
  3. Test generalization: train SINDy on data from one initial condition, evaluate prediction accuracy on trajectories starting from different initial conditions

## Open Questions the Paper Calls Out
- **Scalability to higher-dimensional state spaces and more complex dynamics**: The paper acknowledges that scalability to higher-dimensional state spaces and more complex dynamics remains to be fully explored, noting that only tested on Mountain Car (2D state) and Lunar Lander (6D state).
- **Generalization to different initial conditions**: The paper explicitly states that the generalization capability to significantly different initial conditions remains unexplored, as data was collected from single episodes using pre-trained policies.
- **Real-world applicability**: The paper identifies testing on physical systems as necessary to validate real-world applicability, noting that current work only evaluated simulated environments with deterministic physics.

## Limitations
- The approach assumes access to pre-trained policies for data collection, creating a circular dependency for novel environments
- Library function selection appears hand-tuned for each environment, suggesting domain expertise is required for new problems
- Analysis focuses on relatively simple control tasks without extensive validation on environments with discontinuous dynamics or high-dimensional state spaces

## Confidence
- **High confidence**: Data efficiency claims (75-1000 samples sufficient) supported by quantitative metrics and direct comparison to model-free baselines
- **Medium confidence**: Policy transfer effectiveness—while visual and qualitative results support claims, transfer performance metrics are limited to single environments
- **Low confidence**: Generalizability to complex, high-dimensional, or stochastic environments—limited empirical validation beyond two relatively simple control tasks

## Next Checks
1. Test SINDy surrogate performance on a stochastic environment (e.g., CartPole with random force perturbations) to evaluate robustness to noise
2. Evaluate policy transfer from surrogate to original environment after 10,000 training steps rather than 65,075-801,000 to assess early training behavior
3. Conduct ablation study on library complexity: measure performance degradation when using only polynomial vs. adding trigonometric terms systematically