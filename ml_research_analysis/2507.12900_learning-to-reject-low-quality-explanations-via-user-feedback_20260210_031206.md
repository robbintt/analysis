---
ver: rpa2
title: Learning to Reject Low-Quality Explanations via User Feedback
arxiv_id: '2507.12900'
source_url: https://arxiv.org/abs/2507.12900
tags:
- explanations
- explanation
- uler
- quality
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Learning to Reject Low-Quality Explanations
  (LtX) problem, where classifiers can defer predictions when their explanations are
  unsatisfactory to users. The authors propose ULER (User-centric Low-quality Explanation
  Rejector), which learns to reject low-quality explanations by leveraging expert
  annotations of explanation quality and per-feature relevance judgments.
---

# Learning to Reject Low-Quality Explanations via User Feedback
## Quick Facts
- arXiv ID: 2507.12900
- Source URL: https://arxiv.org/abs/2507.12900
- Authors: Luca Stradiotti; Dario Pesenti; Stefano Teso; Jesse Davis
- Reference count: 40
- Key outcome: ULER outperforms standard learning-to-reject strategies and explanation-aware methods with AUROC 0.82-0.93 across eight benchmark datasets

## Executive Summary
This paper introduces the Learning to Reject Low-Quality Explanations (LtX) problem, where classifiers can defer predictions when their explanations are unsatisfactory to users. The authors propose ULER (User-centric Low-quality Explanation Rejector), which learns to reject low-quality explanations by leveraging expert annotations of explanation quality and per-feature relevance judgments. ULER employs a quality-aware augmentation strategy to address limited annotation data and trains a simple rejector to mirror human judgments of explanation quality.

## Method Summary
ULER addresses the challenge of learning to reject low-quality explanations when limited user feedback is available. The method combines feature importance explanations from an underlying classifier with expert annotations about explanation quality. A quality-aware augmentation strategy is employed to address the scarcity of labeled data by generating additional training examples. The rejector model is trained to predict whether an explanation will be judged as high or low quality by users, using both the explanation features and the underlying classifier's predictions as input.

## Key Results
- ULER achieves AUROC scores of 0.82-0.93 across eight benchmark datasets, outperforming standard learning-to-reject strategies
- The method demonstrates superior performance compared to state-of-the-art explanation quality metrics in rejecting low-quality explanations
- On a novel human-annotated soccer xG dataset (1050 examples with 5 annotations each), ULER predicts human judgments of explanation quality with AUROC of 0.63

## Why This Works (Mechanism)
Assumption: ULER's success stems from its ability to effectively combine feature importance information with human quality judgments, creating a rejector that captures the nuanced aspects of what makes explanations useful to users. The quality-aware augmentation likely helps the model generalize to different types of explanation quality issues by exposing it to a broader range of scenarios during training.

## Foundational Learning
- Learning to Reject (LtR): Why needed? Enables classifiers to defer predictions when confidence is low. Quick check: Verify the rejector correctly identifies uncertain predictions.
- Explanation Quality Metrics: Why needed? Provides quantitative measures of explanation faithfulness and plausibility. Quick check: Ensure metrics align with human judgments.
- Feature Importance Explanations: Why needed? Offers interpretable insights into model decisions. Quick check: Confirm features are correctly extracted and weighted.
- Quality-aware Data Augmentation: Why needed? Addresses limited availability of labeled explanation quality data. Quick check: Validate augmented examples maintain realistic quality patterns.

## Architecture Onboarding
**Component Map:**
Classifier -> Explanation Generator -> Feature Extractor -> Rejector

**Critical Path:**
The critical path flows from classifier predictions through explanation generation to the rejector's final decision on whether to accept or reject the explanation.

**Design Tradeoffs:**
- Simple rejector architecture vs. complex models: favors interpretability and efficiency
- Quality-aware augmentation vs. limited real data: addresses data scarcity while maintaining quality patterns
- Feature-based rejector vs. black-box approaches: enables interpretability of rejection decisions

**Failure Signatures:**
- High rejection rates on high-quality explanations indicate overly conservative thresholds
- Low rejection rates on poor explanations suggest inadequate quality detection
- Performance degradation on out-of-distribution examples reveals limited generalization

**3 First Experiments:**
1. Compare ULER's rejection performance against random baseline on benchmark datasets
2. Evaluate feature importance ranking stability across different rejector thresholds
3. Test augmentation strategy's impact on rejector performance with varying amounts of real data

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly identify open questions or future research directions.

## Limitations
- Evaluation relies heavily on automated quality metrics and a single human-annotated dataset
- The soccer xG dataset represents only one domain and may not generalize to other application areas
- Quality-aware augmentation assumes augmented examples can effectively represent low-quality explanation space

## Confidence
- High confidence: The core methodology and implementation are sound, with clear experimental design and reproducible results
- Medium confidence: The generalization of results to other domains and explanation types requires further validation
- Medium confidence: The effectiveness of quality-aware augmentation across diverse scenarios needs more extensive testing

## Next Checks
1. Evaluate ULER across multiple diverse domains (e.g., healthcare, finance, legal) to assess cross-domain generalization
2. Conduct user studies with different user expertise levels to validate that ULER's rejection decisions align with varied user expectations
3. Test ULER's performance with alternative explanation types (e.g., counterfactual, example-based) beyond feature importance explanations