---
ver: rpa2
title: 'InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and
  Reflection'
arxiv_id: '2501.04575'
source_url: https://arxiv.org/abs/2501.04575
tags:
- arxiv
- reasoning
- agents
- wang
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InfiGUIAgent, a multimodal GUI agent trained
  with a two-stage supervised fine-tuning pipeline to address limitations in multi-step
  reasoning and reliance on textual annotations in existing GUI agents. The first
  stage improves fundamental abilities such as GUI understanding and grounding using
  diverse datasets, while the second stage integrates hierarchical and expectation-reflection
  reasoning skills through synthesized data to enable native reasoning capabilities.
---

# InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection

## Quick Facts
- arXiv ID: 2501.04575
- Source URL: https://arxiv.org/abs/2501.04575
- Reference count: 20
- Primary result: 76.3% accuracy on ScreenSpot, 0.09 success rate on AndroidWorld

## Executive Summary
InfiGUIAgent is a multimodal GUI agent that addresses limitations in existing agents' multi-step reasoning and reliance on textual annotations. The model is trained using a two-stage supervised fine-tuning pipeline: Stage 1 enhances fundamental skills like GUI understanding and grounding using diverse datasets, while Stage 2 integrates hierarchical and expectation-reflection reasoning skills through synthesized data. The agent operates directly on raw screenshots without additional GUI metadata, achieving competitive performance on GUI benchmarks while maintaining a compact 2B parameter size.

## Method Summary
The method employs a two-stage supervised fine-tuning approach on Qwen2-VL-2B. Stage 1 uses ~1M samples from 15+ datasets covering GUI understanding, grounding, QA, and general knowledge, with coordinates normalized to [0,1000] scale using reference-augmented annotation format. Stage 2 synthesizes 45K samples from existing trajectories using Qwen2-VL-72B to generate structured reasoning traces (reflection → strategic → tactical → action → expectation). The agent operates on raw screenshots only, avoiding dependency on accessibility trees or Set-of-Marks annotations.

## Key Results
- Achieves 76.3% accuracy on ScreenSpot grounding benchmark
- Achieves 0.09 success rate on AndroidWorld task completion
- Outperforms several strong baselines including GPT-4V, Qwen2-VL-7B, and LLaVA-Next
- Maintains competitive performance despite using raw screenshots only (no metadata)

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Capability Layering
Sequential training on fundamental skills before reasoning skills yields better GUI agent performance than joint training. Stage 1 establishes visual grounding and GUI understanding through diverse datasets, while Stage 2 builds hierarchical and reflection reasoning on top of frozen foundational representations. The assumption is that fundamental skills must reach competence threshold before reasoning skills can be effectively learned.

### Mechanism 2: Native Reasoning via Synthesized Trajectory Data
Embedding hierarchical and expectation-reflection reasoning directly into training data enables "native" reasoning without inference-time prompting. The paper synthesizes reasoning traces using stronger MLLMs (Qwen2-VL-72B) on existing trajectories, structuring each step with reflection, strategic layer, tactical layer, action execution, and expectation generation.

### Mechanism 3: Reference-Augmented Annotation for Spatial Grounding
Embedding spatial coordinates directly into natural language responses improves visual grounding accuracy compared to separate coordinate prediction. Uses structured XML-like notation `<ref type="box" coords={...} note="...">text</ref>` that bidirectionally links GUI elements with textual descriptions, with coordinates normalized to [0, 1000] scale.

## Foundational Learning

- **Concept: GUI Grounding**
  - Why needed: Core capability to map natural language instructions to specific screen coordinates. Without grounding, agent cannot execute any action.
  - Quick check: Given a screenshot and instruction "tap the settings icon," can your model output correct coordinates within the icon bounding box?

- **Concept: Supervised Fine-Tuning (SFT) for Multimodal Models**
  - Why needed: Entire training pipeline is SFT-based. Requires understanding of how to format image-text-instruction triplets, handle coordinate normalization, and manage multi-dataset mixing.
  - Quick check: Can you construct an SFT sample with screenshot, instruction, and reference-augmented response in the paper's format?

- **Concept: Action Space Design**
  - Why needed: Paper defines modular action space (tap, swipe, scroll, input, etc.) with function-calling interface. Agent must output structured JSON actions the environment can execute.
  - Quick check: What's the correct action format for "scroll down on the current screen" according to Table 3?

## Architecture Onboarding

- **Component map:** Input: [Goal text] + [Current screenshot] + [History: n prior (screenshot, reasoning, action) tuples] → Vision Encoder → Modality Interface → LLM Backbone → Output: [Reflection] → [Strategic reasoning] → [Tactical reasoning] → [Action JSON] → [Expectation]

- **Critical path:** Screenshot preprocessing (ensure resolution compatibility with Qwen2-VL) → History window construction (n=variable) → Reference-augmented response parsing during training → Action JSON execution via environment interface

- **Design tradeoffs:** Small model (2B params) vs. larger models enables easier deployment but limits reasoning capacity; raw screenshots only vs. accessibility tree/Set-of-Marks more realistic deployment but harder grounding; synthesized reasoning data vs. human annotations more scalable but quality bounded by teacher model

- **Failure signatures:** Repetitive actions without progress → expectation-reflection loop not triggering correction; wrong element clicked → grounding failure from Stage 1 insufficient training; nonsensical reasoning traces → teacher model hallucination in synthesized data; success rate drops sharply on "Hard" tasks → hierarchical reasoning insufficient for complex multi-step planning

- **First 3 experiments:** Baseline grounding test: Evaluate pretrained Qwen2-VL-2B on ScreenSpot without any fine-tuning; Ablation: Stage 1 only: Train with Stage 1 data only, evaluate on both ScreenSpot and AndroidWorld; Annotation format comparison: Train two Stage 1 models—one with reference-augmented annotations, one with standard coordinate prediction

## Open Questions the Paper Calls Out

### Open Question 1
Does training agents to predict expectations using the subsequent state ($s_{t+1}$) actually impair their ability to recover from errors during deployment compared to predicting expectations based solely on the current state and action? The authors hypothesize a trade-off between accurate state modeling and robustness to error but do not provide experimental evidence comparing agents trained with $s_{t+1}$ access versus their chosen method.

### Open Question 2
Is the reasoning capability of InfiGUIAgent fundamentally limited by the performance ceiling of the teacher model (Qwen2-VL-72B) used to synthesize the Stage 2 reasoning data? The paper relies entirely on "more capable MLLMs" to generate the reasoning processes, but does not analyze the quality of the synthesized reasoning traces or test if a stronger teacher model yields a more capable agent.

### Open Question 3
Can the two-stage SFT approach overcome the "catastrophic forgetting" of general multimodal capabilities often observed when specializing small models (2B parameters) on specific GUI tasks? The paper notes Stage 1 includes "General Knowledge" datasets to maintain capabilities, but the experimental results focus exclusively on GUI benchmarks, leaving the retention of general vision-language skills unverified.

## Limitations
- Unknown training hyperparameters (learning rate, epochs, batch size) not specified
- Limited evaluation on "Hard" AndroidWorld tasks (0.00 success rate)
- Quality of synthesized reasoning data depends entirely on teacher model performance

## Confidence
- **High confidence:** The fundamental two-stage training architecture and the use of reference-augmented annotations for spatial grounding
- **Medium confidence:** The claim of competitive performance on ScreenSpot (76.3%) and AndroidWorld (0.09 success rate)
- **Low confidence:** The assertion that synthesized reasoning data enables "native" reasoning capabilities

## Next Checks
1. **Grounding baseline verification:** Test Qwen2-VL-2B on ScreenSpot before any fine-tuning to establish the starting point for Stage 1 training effectiveness
2. **Ablation study:** Train and evaluate a model using only Stage 1 data to isolate the contribution of hierarchical reasoning from Stage 2
3. **Annotation format validation:** Compare grounding accuracy between reference-augmented annotations and standard coordinate prediction formats to verify the claimed benefit of the proposed annotation scheme