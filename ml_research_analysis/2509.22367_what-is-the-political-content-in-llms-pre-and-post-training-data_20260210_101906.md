---
ver: rpa2
title: What Is The Political Content in LLMs' Pre- and Post-Training Data?
arxiv_id: '2509.22367'
source_url: https://arxiv.org/abs/2509.22367
tags:
- documents
- political
- policy
- right
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the origins of political bias in large
  language models (LLMs) by analyzing the political content of their pre- and post-training
  data. Using the OLMO2 model family and its fully open-source training datasets,
  the authors sample and classify documents for political orientation (left, right,
  or neutral) using a robust zero-shot classifier.
---

# What Is The Political Content in LLMs' Pre- and Post-Training Data?

## Quick Facts
- arXiv ID: 2509.22367
- Source URL: https://arxiv.org/abs/2509.22367
- Reference count: 40
- Primary result: Left-leaning documents predominate by 3-12x in OLMO2 training data, correlating strongly (r=0.90) with model political biases

## Executive Summary
This study investigates the origins of political bias in large language models (LLMs) by analyzing the political content of their pre- and post-training data. Using the OLMO2 model family and its fully open-source training datasets, the authors sample and classify documents for political orientation (left, right, or neutral) using a robust zero-shot classifier. They find that left-leaning documents consistently outnumber right-leaning ones by a factor of 3 to 12 across all datasets, with pre-training corpora containing about four times more politically engaged content than post-training data. Source domain analysis reveals that right-leaning content comes largely from blogs, while left-leaning content is dominated by established news outlets. Topic modeling shows systematic framing differences on shared issues such as climate change and animal rights. Finally, a strong correlation (Pearson r = 0.90) is found between the stances expressed in the training data and the political biases exhibited by the models when probed on eight policy issues. The results suggest that political biases are largely encoded during the pre-training stage and highlight the need for transparent data curation and mitigation strategies in LLM development.

## Method Summary
The study samples documents from OLMO2's pre-training (DOLMA, DOLMINO) and post-training (SFT-MIX, DPO-MIX) datasets, then classifies them for political orientation using an LLM-based zero-shot classifier. Document stances on eight policy issues are also scored using a similar approach. The authors then probe OLMO2 models at various training stages (base, SFT, DPO, instruct) using the ProbVAA dataset to measure political bias. Correlation analysis between training data stances and model behavior reveals that biases are primarily encoded during pre-training, with post-training mainly affecting consistency rather than reversing existing biases.

## Key Results
- Left-leaning documents predominate by 3-12x across all analyzed training datasets
- Pre-training corpora contain approximately 4x more politically engaged content than post-training data
- Strong correlation (r = 0.90) exists between training data stances and model political biases
- Source domain analysis shows right-leaning content comes mainly from blogs while left-leaning content is dominated by established news outlets
- Topic modeling reveals systematic framing differences on shared issues like climate change and animal rights

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A model's political bias on policy issues is strongly correlated with the dominant political stance in its training data, a relationship primarily encoded during the pre-training phase.
- **Mechanism:** The model is exposed to an imbalanced corpus during pre-training (e.g., left-leaning documents outnumber right-leaning ones by 3-12x in OLMO2). It learns statistical patterns and associations from this data. When later probed, the model's outputs reflect this imbalance, favoring the policy positions that were more prevalent and consistently framed in its training data. Post-training may sharpen the consistency of these outputs but does not fundamentally reverse the encoded preference.
- **Core assumption:** The automated classifier used to label political orientation in the training data is sufficiently accurate, and the model learns generalized political worldviews, not just surface-level text patterns, from the corpus.
- **Evidence anchors:**
  - [abstract] "...the predominant stance in the training data strongly correlates with models' political biases when evaluated on policy issues."
  - [abstract] "Our analysis shows that left-leaning documents predominate across datasets..."
  - [section 6.2] "Overall, the stances of the training documents are very strongly correlated with those of the models, with an average Pearson r of 0.90 across model–dataset pairs... allowing us to hypothesize that political biases are encoded already in the pre-training stage."
  - [corpus] The paper "Political Ideology Shifts in Large Language Models" supports the idea that LLMs can be influenced toward specific ideologies, consistent with the model learning from its training distribution.
- **Break condition:** If a model trained on a carefully balanced political corpus still exhibited a strong, consistent bias, this correlation-based mechanism would be insufficient, implying other factors like model architecture or the language modeling objective itself are at play.

### Mechanism 2
- **Claim:** Pre-training is the critical stage for establishing a model's political worldview, with post-training primarily serving to align the model's behavior for dialogue without erasing the core bias.
- **Mechanism:** Pre-training on a massive, web-scraped corpus exposes the model to the bulk of its "knowledge" about the world, including the dominant framing and frequency of political concepts. Post-training datasets (SFT, DPO) are much smaller and focused on task performance (helpfulness, instruction-following). While they can marginally steer the model, they are insufficient to overwrite the deep-seated statistical patterns and representations established by the massive pre-training dataset.
- **Core assumption:** The scale of the pre-training dataset gives it an overwhelming influence on the model's core representations, making it resistant to significant alteration by the smaller-scale post-training process.
- **Evidence anchors:**
  - [abstract] "...suggesting that political biases are primarily encoded during pre-training."
  - [section 7] "This interpretation could also help explain why recent work... report only limited success in aligning models to different political leanings through post-training interventions... when strong imbalances are already encoded in pre-training data... alignment can only partially steer models' political biases, rather than fully reverse them."
  - [section 6.1] "The base model instead shows slightly weaker stances... attributed to the lower consistency in the answers generated by the base model..." (This implies the bias is present but less consistently expressed before post-training sharpens it).
  - [corpus] The paper "Better Aligned with Survey Respondents or Training Data? Unveiling Political Leanings of LLMs on U.S. Supreme Court Cases" investigates a similar question about the origin of political leanings, supporting the focus on training data.
- **Break condition:** If future research demonstrates that a relatively small, targeted post-training intervention can reliably and durably flip a model's political bias on policy issues, the claim of primary pre-training encoding would be weakened.

### Mechanism 3
- **Claim:** Political bias is not merely a question of label (left/right) but is also encoded through the differing epistemic styles, source authority, and framing associated with each side of the spectrum in the training data.
- **Mechanism:** The analysis reveals systematic differences in content sources (e.g., established news outlets for left-leaning content vs. blogs for right-leaning content) and argumentation styles (e.g., science-led urgency vs. appeals to stability and sovereignty). The model learns to associate these different rhetorical patterns and sources of legitimacy with the underlying political concepts, which shapes not just what it believes but how it argues and what it cites as evidence.
- **Core assumption:** The qualitative differences in text (source, style, framing) from different political perspectives are learned by the model as part of its representation of those perspectives.
- **Evidence anchors:**
  - [section 5.1] "Among the top 25 source domains, blogs account for a larger share of right-leaning documents... In contrast, the left-leaning documents have a higher proportion of highly-ranked news outlets..."
  - [section 5.2] "Overall, the results show that while left- and right-leaning documents often address the same topics, they frame them through different arguments and emphases."
  - [corpus] The paper "RooseBERT: A New Deal For Political Language Modelling" focuses on the unique nature of political language, supporting the idea that the model learns specific linguistic properties of political discourse.
- **Break condition:** If a model trained on such data showed no preference for the argumentation styles or source types associated with its dominant political leaning, this more nuanced mechanism would be unsupported.

## Foundational Learning

- **Concept: Training Data Curation and Filtering in LLMs**
  - **Why needed here:** The paper's central thesis is that bias is a data problem. Understanding how pre-training (DOLMA) and post-training (SFT-MIX) corpora are built, filtered, and mixed is a prerequisite for diagnosing where and how biases enter the system.
  - **Quick check question:** Describe the primary differences between a pre-training corpus like DOLMA and a post-training corpus like SFT-MIX in terms of scale, content source, and the model's learning objective.

- **Concept: Correlation vs. Causation in Empirical Research**
  - **Why needed here:** The paper's main finding is a strong *correlation* (r=0.90), from which it infers a likely causal link. It is critical to understand the difference to avoid overstating the conclusions and to recognize where further experimental validation (e.g., ablation studies) is needed.
  - **Quick check question:** The paper finds a strong correlation between data bias and model bias. What kind of experiment would be required to prove a *causal* link?

- **Concept: The Difference Between Base Models and Instruct/Chat Models**
  - **Why needed here:** The analysis includes both base and post-trained model variants. A base model, trained only for next-token prediction, will express its "knowledge" differently than an instruct model fine-tuned for helpful dialogue. This distinction is key to interpreting the results, especially regarding why base models may show "weaker" or noisier stances.
  - **Quick check question:** Why might a base model's responses on a political probe be less consistent than those of an instruct model, even if both share the same underlying bias?

## Architecture Onboarding

- **Component map:** Pre-training Corpora (DOLMA, DOLMINO) -> Post-training Corpora (SFT-MIX, DPO-MIX) -> Automated Classifiers (LLAMA-3.1-70B-Instruct) -> Model Probing Methodology (ProbVAA dataset)

- **Critical path:**
    1.  **Sampling:** Extract a representative sample from the petabyte-scale training corpora.
    2.  **Classification:** Use automated classifiers to label the political lean and policy stance of the sampled documents.
    3.  **Probing:** Query the final trained model with a standardized set of political statements to determine its aggregated stance.
    4.  **Correlation Analysis:** Compare the dominant stance from the data analysis (Step 2) with the model's probed stance (Step 3) to calculate the correlation.

- **Design tradeoffs:**
    - **Sampling vs. Full Corpus Analysis:** The authors could not analyze the entire corpus. They chose random sampling, trading perfect certainty for feasibility.
    - **Automated vs. Manual Annotation:** Manually annotating political content is subjective and expensive at scale. The authors relied on a validated LLM-based classifier, trading some annotation accuracy for the ability to process large datasets.
    - **Analytical Simplicity vs. Nuance:** The framework uses a simplified left-right axis. This makes the analysis tractable but sacrifices real-world political nuance.

- **Failure signatures:**
    - **Classifier Bias:** The political leaning classifier itself may be biased, which would systematically skew all data analysis results.
    - **Prompt Brittleness:** The model's probed stance can be highly sensitive to the exact wording of the prompt, leading to noisy measurements.
    - **Base Model Inconsistency:** Base models are not trained for dialogue and often provide incoherent or non-answers to direct questions, making their probed stance difficult to measure reliably.

- **First 3 experiments:**
    1.  **Replicate with a Different Model:** Repeat the correlation analysis on another open-source model (e.g., a LLaMA variant) with known training data. The goal is to see if the r=0.90 correlation generalizes beyond the OLMO2 architecture and data mix.
    2.  **Source Upsampling Ablation:** Train a small model on a modified version of a pre-training corpus where the underrepresented political side's documents (e.g., right-leaning blogs) are upsampled to create a balanced dataset. This tests the causal hypothesis that data imbalance directly causes model bias.
    3.  **Post-Training Debiasing Limit:** Take a biased pre-trained model and apply a more aggressive, politically targeted post-training intervention (e.g., using a curated dataset of balanced political arguments). Measure how much the model's stance can be shifted, testing the paper's hypothesis that pre-training biases are difficult to reverse.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does pre-filtering heavily politicized text from pre-training corpora effectively reduce political bias in LLM outputs without degrading model capabilities?
- **Basis in paper:** [explicit] Conclusion states: "Future research could explore strategies to mitigate political bias in LLMs... pre-filter pre-training data and remove heavily politicized text, which plausibly reduces political leanings of LLMs, and soften the impact on political views reflected in models."
- **Why unresolved:** The study establishes correlation between training data bias and model behavior but does not test causal intervention through data removal.
- **What evidence would resolve it:** Train models on filtered corpora with reduced political content and measure changes in political bias alongside task performance.

### Open Question 2
- **Question:** Are political biases reinforced during post-training, or do models merely become more consistent in expressing pre-training-encoded biases?
- **Basis in paper:** [explicit] Section 7 states: "We cannot disentangle stance reinforcement from improved consistency with the current evaluation setup."
- **Why unresolved:** The correlation between training data stances and model outputs (r=0.90) does not distinguish whether post-training modifies existing biases or improves response coherence.
- **What evidence would resolve it:** Compare bias expression with controlled intervention, e.g., post-training on politically neutral or counter-balanced datasets while measuring consistency independently.

### Open Question 3
- **Question:** Why does Open Foreign Policy show divergent correlation patterns between training data stances and model behavior?
- **Basis in paper:** [explicit] Section 6.2 notes: "This is the only policy issue that does not correlate well with the results of the models, given that the base model expresses support in this policy."
- **Why unresolved:** The mechanism causing this anomaly among eight policy domains is not explained.
- **What evidence would resolve it:** Fine-grained analysis of topic framing in Open Foreign Policy documents, and controlled probing with additional policy categories.

## Limitations
- The correlation between training data bias and model bias (r=0.90) is robust but not necessarily causal—the study relies on observational data rather than experimental intervention
- The automated political classifiers, while validated on established benchmarks, may introduce systematic bias that propagates through the analysis
- The simplified left-right framing excludes more nuanced political spectrums and regional variations

## Confidence
- **High Confidence:** The documented imbalance in training data (3-12x left-leaning predominance) and its measurement through multiple validation sets
- **Medium Confidence:** The correlation (r=0.90) between training data stance and model behavior, given potential classifier bias and the observational nature of the evidence
- **Low Confidence:** The assertion that post-training cannot reverse pre-training biases, as this requires direct experimental manipulation not performed in the study

## Next Checks
1. **Cross-Architecture Replication:** Replicate the correlation analysis on a different open-source model family (e.g., LLaMA variants) with publicly available training data to test whether the r=0.90 relationship generalizes beyond OLMo2's specific architecture and data mix
2. **Controlled Corpus Ablation:** Train small models from scratch on modified versions of the pre-training corpus where the underrepresented political side's documents are upsampled to create balanced datasets, then probe for bias changes to establish causality
3. **Post-Training Intervention Experiment:** Take a biased pre-trained model and apply an aggressive, politically targeted post-training intervention using a curated dataset of balanced political arguments, then measure whether the model's stance can be durably shifted to test the pre-training primacy hypothesis