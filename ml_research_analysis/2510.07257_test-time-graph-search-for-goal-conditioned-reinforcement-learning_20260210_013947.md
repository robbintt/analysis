---
ver: rpa2
title: Test-Time Graph Search for Goal-Conditioned Reinforcement Learning
arxiv_id: '2510.07257'
source_url: https://arxiv.org/abs/2510.07257
tags:
- ttgs
- distance
- hiql
- planning
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Test-Time Graph Search (TTGS) addresses the long-horizon planning
  challenge in offline goal-conditioned reinforcement learning by leveraging the geometric
  structure encoded in learned value functions. The method constructs a graph over
  dataset states using value-derived distances and performs Dijkstra search to generate
  subgoal sequences for a frozen policy at test time.
---

# Test-Time Graph Search for Goal-Conditioned Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.07257
- Source URL: https://arxiv.org/abs/2510.07257
- Reference count: 17
- Long-horizon goal-conditioned RL planning via test-time graph search over learned value functions

## Executive Summary
Test-Time Graph Search (TTGS) addresses long-horizon planning challenges in offline goal-conditioned reinforcement learning by constructing a graph over dataset states using value-derived distances and performing Dijkstra search to generate subgoal sequences. The method operates at test time without requiring retraining, additional supervision, or online interaction, making it a lightweight planning wrapper for frozen policies. On OGBench long-horizon locomotion tasks, TTGS consistently improves success rates across multiple base learners (HIQL, GCIQL, QRL), with notable gains on giant maze navigation and stitching tasks where one-shot execution fails.

## Method Summary
TTGS constructs a graph where nodes represent sampled states from the dataset and edges connect nearby states weighted by distance estimates derived from the value function. At test time, given a start state and goal, TTGS performs Dijkstra search to find the shortest path through this graph, generating a sequence of subgoals. The frozen base policy then executes each subgoal sequentially. The method can use either value-based distances or domain-specific distances, and employs a lazy search variant that stops when a nearby state is found to reduce computational overhead.

## Key Results
- Consistent success rate improvements across HIQL, GCIQL, and QRL base learners on long-horizon locomotion tasks
- Significant gains on giant maze navigation and stitching tasks where one-shot execution fails
- TTGS achieves near-100% success on simple tasks while base learners plateau around 60-70%
- State-space distances perform comparably to value-based distances in some environments

## Why This Works (Mechanism)
TTGS leverages the geometric structure encoded in learned value functions to decompose long-horizon tasks into manageable subgoals. By constructing a graph over dataset states using value-derived distances, it captures the reachability landscape learned by the base policy. The Dijkstra search then finds optimal subgoal sequences that respect the policy's learned capabilities. This approach is particularly effective when the value function provides meaningful distance estimates between states, allowing the planner to navigate around obstacles or through complex state spaces that would challenge direct one-step execution.

## Foundational Learning
- **Value function geometry**: Understanding how value functions encode state reachability and distance is crucial for constructing meaningful graph edges. Quick check: Verify that value differences correlate with actual transition difficulty in the dataset.
- **Graph search algorithms**: Dijkstra's algorithm forms the backbone of subgoal sequence generation. Quick check: Confirm the implementation correctly handles edge weights and finds shortest paths.
- **Subgoal decomposition**: Breaking long-horizon tasks into intermediate goals requires understanding the policy's local capabilities. Quick check: Test that the policy can reliably reach randomly selected nearby states.
- **Offline RL dataset coverage**: The quality of graph construction depends on having representative states throughout the state space. Quick check: Analyze dataset state distribution for coverage gaps in relevant regions.
- **Lazy search optimization**: Stopping early when finding nearby states trades completeness for efficiency. Quick check: Measure the impact on success rates when using different distance thresholds for early termination.
- **Distance estimator calibration**: Value-based distances must be properly scaled to reflect true reachability. Quick check: Compare value-based distances against ground truth step counts in validation environments.

## Architecture Onboarding

**Component Map**: Dataset States -> Value Function -> Distance Estimator -> Graph Constructor -> Dijkstra Search -> Subgoal Sequence -> Frozen Policy

**Critical Path**: At test time, TTGS samples states from the dataset, computes pairwise distances using the value function, constructs a k-nearest neighbor graph, performs Dijkstra search from start to goal, and sequentially executes the resulting subgoals using the frozen policy.

**Design Tradeoffs**: TTGS trades computational overhead at test time for improved success rates without retraining. The method must balance graph density (more edges improve planning but increase computation) and vertex sampling strategy (uniform sampling is simple but may miss critical waypoints). The choice between value-based and domain-specific distances involves accuracy versus generality tradeoffs.

**Failure Signatures**: TTGS will fail when the value function provides poor distance estimates, when the dataset lacks coverage of critical regions, when the frozen policy cannot reliably reach subgoals, or when the graph becomes too sparse to find feasible paths. Success rate degradation typically correlates with increasing task horizon length.

**3 First Experiments**:
1. Run TTGS with a random walk policy as the base learner to verify that poor value functions lead to ineffective planning
2. Test TTGS on a simple gridworld with known optimal paths to validate the graph construction and search components
3. Compare success rates when using uniform versus clustered vertex sampling on a simple maze environment

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can combining multiple noisy distance estimates produce more robust edge weights for graph-based planning than any single distance estimator alone?
- Basis in paper: [explicit] The authors state: "Future work could explore combining multiple noisy distance estimates to obtain more robust edge weights, as well as developing more sophisticated strategies for selecting representative states as graph vertices."
- Why unresolved: The current method relies on a single distance estimator (value-derived or domain-specific), which can be noisy or optimistic. No ensemble or uncertainty-weighting approach was tested.
- What evidence would resolve it: Experiments comparing single-estimator TTGS against variants that aggregate distances from multiple value functions or distance predictors, measuring success rate and robustness across environments with varying noise levels.

### Open Question 2
- Question: What vertex selection strategies beyond uniform random sampling can improve planning quality without significantly increasing computational cost?
- Basis in paper: [explicit] The authors tested trajectory efficiency filtering and clustering but found "it did not produce a statistically significant gain over random sampling." They explicitly call for "more sophisticated strategies for selecting representative states as graph vertices."
- Why unresolved: Uniform sampling may miss critical waypoints in sparse regions of the state space. The paper's preliminary alternatives did not yield gains, leaving optimal vertex selection open.
- What evidence would resolve it: Systematic comparison of vertex selection methods (e.g., coverage-based, uncertainty-guided, or trajectory-aware sampling) on tasks with varying dataset coverage, analyzing both planning success and graph construction overhead.

### Open Question 3
- Question: How does TTGS performance degrade when the frozen base policy cannot reliably reach nearby subgoals within the predicted step budget?
- Basis in paper: [inferred] The authors note: "The framework assumes that the policy can reliably reach nearby subgoals, and its performance is influenced by both the accuracy of the distance estimator and the coverage of states in the offline dataset."
- Why unresolved: The paper does not characterize failure modes when subgoal reachability is low. Understanding this boundary could inform when TTGS is applicable versus when policy improvement is needed.
- What evidence would resolve it: Controlled experiments varying subgoal reliability (e.g., by degrading base policy quality or increasing step-budget mismatch), correlating reachability metrics with overall task success.

## Limitations
- Performance gains primarily demonstrated on locomotion tasks in OGBench, limiting generalizability to other domains
- Heavy dependence on quality of learned value function - poor value estimates lead to suboptimal planning
- Test-time computational overhead from graph construction and search may not be acceptable for real-time applications

## Confidence
- **High Confidence**: The core claim that TTGS can improve success rates on long-horizon tasks without retraining is well-supported by empirical results across multiple base learners
- **Medium Confidence**: The assertion that TTGS is a lightweight planning wrapper is supported, though the computational overhead at test time requires more careful analysis
- **Medium Confidence**: The flexibility of TTGS with domain-specific distances is demonstrated, but the practical benefits beyond value-based planning need further validation

## Next Checks
1. Evaluate TTGS on non-locomotion domains (e.g., robotic manipulation tasks) to assess cross-domain applicability and identify task-specific limitations

2. Conduct ablation studies comparing TTGS with alternative planning approaches (e.g., model-based planning, hierarchical RL) on the same tasks to better understand its relative advantages and disadvantages

3. Measure and analyze the computational overhead of graph construction and search at test time across different graph sizes and state space complexities to quantify the real-time performance trade-offs