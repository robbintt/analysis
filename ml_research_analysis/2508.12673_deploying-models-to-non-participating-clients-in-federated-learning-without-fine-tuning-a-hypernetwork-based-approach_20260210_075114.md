---
ver: rpa2
title: 'Deploying Models to Non-participating Clients in Federated Learning without
  Fine-tuning: A Hypernetwork-based Approach'
arxiv_id: '2508.12673'
source_url: https://arxiv.org/abs/2508.12673
tags:
- learning
- clients
- distribution
- data
- hyperfedzero
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying federated learning
  models to non-participating clients with in-domain distribution shifts and limited
  resources. The proposed HyperFedZero method uses a hypernetwork conditioned on distribution-aware
  embeddings to dynamically generate specialized classifiers for unseen clients without
  fine-tuning.
---

# Deploying Models to Non-participating Clients in Federated Learning without Fine-tuning: A Hypernetwork-based Approach

## Quick Facts
- arXiv ID: 2508.12673
- Source URL: https://arxiv.org/abs/2508.12673
- Authors: Yuhao Zhou, Jindi Lv, Yuxin Tian, Dan Si, Qing Ye, Jiancheng Lv
- Reference count: 40
- Primary result: HyperFedZero achieves 14.68% zACC on Tiny-ImageNet with ResNet, significantly exceeding baseline methods.

## Executive Summary
This paper introduces HyperFedZero, a method for deploying federated learning models to non-participating clients without fine-tuning. The approach uses a hypernetwork conditioned on distribution-aware embeddings to dynamically generate specialized classifiers for unseen clients. By incorporating NoisyEmbed and Balancing Penalty mechanisms, HyperFedZero extracts robust distribution embeddings and prevents feature collapse, enabling effective zero-shot personalization. Experiments across 7 datasets and 5 models demonstrate substantial improvements over competing methods while maintaining minimal computational overhead.

## Method Summary
HyperFedZero addresses the challenge of serving non-participating clients in federated learning by using a hypernetwork that generates classifier parameters conditioned on distribution embeddings. The distribution extractor maps input data to embeddings, which are enhanced with NoisyEmbed (learnable Gaussian noise) and regularized with Balancing Penalty to prevent feature collapse. The hypernetwork then generates classifier weights chunk-by-chunk, enabling zero-shot personalization without fine-tuning. The method is integrated into a FedAvg-style aggregation framework where only the hypernetwork and distribution extractor parameters are shared and updated.

## Key Results
- HyperFedZero achieves 14.68% zACC on Tiny-ImageNet with ResNet, substantially exceeding baseline methods.
- The approach maintains minimal computational overhead, with hypernetwork parameters approximately equal to classifier parameters (+2.30%).
- On MNIST with N=50 clients, HyperFedZero achieves 97.32% zACC compared to 94.64% for FedAvg, demonstrating robust zero-shot personalization performance.

## Why This Works (Mechanism)

### Mechanism 1: Distribution Embedding with Collapse Prevention
Adding noise and a balancing penalty to the distribution extractor prevents all embeddings from collapsing into a narrow region of the embedding space. This collapse would otherwise occur because all client distributions are visible during training, reducing the incentive to customize for unseen distributions. NoisyEmbed injects learnable Gaussian noise via `softmax(f(x) + z · softplus(noisy(x)))`, increasing randomness. The Balancing Penalty (`α var(P e)/mean(P e) + β E(-e log e)`) simultaneously encourages even distribution across the embedding space and clustering along specific dimensions.

### Mechanism 2: Hypernetwork-Conditioned Parameter Generation
Conditioning the hypernetwork on distribution embeddings to generate classifier parameters provides better zero-shot personalization than conditioning inputs directly on embeddings. Opt. 2 uses `Pr(y|x; h(e; θh))` where the hypernetwork `h` generates classifier weights `θc` from embedding `e`. Chunked generation maintains shared global knowledge while enabling model flexibility. Explicitly generating different models for different `e` is more flexible than a single model trying to use `e` as auxiliary input, which may ignore `e`.

### Mechanism 3: Zero-Shot Generalization via Embedding Space Structure
Non-participating clients with in-domain distribution shifts can be served by mapping their data to the same embedding space used during training, where similar embeddings imply similar distributions. The distribution extractor learns geometric relationships such that data from unseen clients with shifted distributions falls into regions where the hypernetwork can generate appropriate classifier parameters. This relies on the smoothness and continuity of neural networks under label supervision to enable generalization to unseen distributions within the same domain.

## Foundational Learning

- **Concept: Federated Learning (FedAvg) and Data Heterogeneity**
  - Why needed: HyperFedZero builds on FedAvg; understanding non-IID data, local objectives `Fi`, aggregation weights `wi`, and the cold-start problem for non-participating clients is essential.
  - Quick check: Can you explain why a global model trained with FedAvg on participating clients might perform poorly on a non-participating client with a different class frequency distribution?

- **Concept: Hypernetworks and Chunked Parameter Generation**
  - Why needed: The core innovation is a hypernetwork that generates classifier weights; understanding how hypernetworks condition on side information (here, embeddings) and why chunking reduces size is critical.
  - Quick check: How does a chunked hypernetwork differ from generating all parameters at once, and what is the trade-off?

- **Concept: Feature Collapse and Regularization**
  - Why needed: Without NoisyEmbed and Balancing Penalty, embeddings collapse; understanding collapse (similar to mode collapse in GANs) and regularization (variance + entropy) helps diagnose training failures.
  - Quick check: What might happen if you set `α = 0` and `β = 0` in Equation 4 during training?

## Architecture Onboarding

- **Component map**: Input `x` -> Distribution Extractor `f` -> Embedding `e` -> Hypernetwork `h` -> Classifier Parameters `θc` -> Output `y`

- **Critical path**: During training, client `i` computes `e_i` from `x_i` via `f`. Hypernetwork `h` generates `θc_i` from `e_i`. Classifier with `θc_i` predicts `y_i`; loss computed with Balancing Penalty. Gradients update `θf` and `θh`; these are aggregated server-side. During deployment, non-participating client computes `e` from its data, `h` generates `θc`, and inference proceeds without fine-tuning.

- **Design tradeoffs**:
  - Embedding dimension `P`: Larger `P` (e.g., 32, 64) reduces generalization; default is 16.
  - Hypernetwork architecture: Smaller `h` limits capacity; larger `h` increases params and may hurt convergence. Target: `|θf| + |θh| ≈ |θc|`.
  - Chunk size: Too small (144) or too large (2304) reduces zACC; 576 is optimal for Tiny-ImageNet ResNet.
  - Conditioning option: Opt. 2 (parameter conditioning) outperforms Opt. 1 (input conditioning).

- **Failure signatures**:
  - Low zACC with good gACC/pACC: Hypernetwork may not be learning diverse embeddings; check NoisyEmbed and Balancing Penalty (`α`, `β`).
  - Feature collapse (all `e` similar): Visible in embedding visualizations; `α` or `β` may be 0 or too low.
  - Hypernetwork size issues: If params >> `|θc|` (e.g., +102.04%), expect convergence issues; if << `|θc|` (e.g., -69.13%), expect low zACC.
  - High data heterogeneity (`αd` low): pACC drops sharply; may need to adjust `α`, `β` or accept trade-off.

- **First 3 experiments**:
  1. Baseline reproduction: Run HyperFedZero on MNIST (N=10, M=5, `αd=1.0`) with default hyperparameters; verify zACC ≈ 95.49% and compare to FedAvg (93.06%).
  2. Ablation on NoisyEmbed and Balancing Penalty: Disable NoisyEmbed and set `α=β=0`; expect feature collapse and significant zACC drop.
  3. Embedding dimension sensitivity: Vary `P` in {2, 8, 16, 32} on Tiny-ImageNet ResNet; expect optimal zACC at `P=16` and degradation at extremes.

## Open Questions the Paper Calls Out

### Open Question 1
Can diffusion-based parameter generation techniques overcome the scalability limitations of chunked-hypernetworks for generating billions of parameters in HyperFedZero? The current hypernetwork architecture is constrained by the need to generate parameters chunk-by-chunk, which becomes impractical for very large models (e.g., modern transformers with billions of parameters). The trade-off between hypernetwork capacity and computational efficiency remains unexplored for scale.

### Open Question 2
How can the observed trade-off between personalized accuracy (pACC) and zero-shot personalization accuracy (zACC) be theoretically characterized and practically optimized? The paper empirically demonstrates this trade-off but does not provide a theoretical framework explaining why optimizing for zero-shot adaptation appears to come at the cost of personalized performance for participating clients, nor mechanisms to balance both objectives.

### Open Question 3
How robust is HyperFedZero's distribution embedding extraction when non-participating clients exhibit out-of-domain distribution shifts rather than in-domain shifts? The paper explicitly focuses on "in-domain distribution shifts" and differentiates this from "out-of-domain" scenarios. The distribution extractor is trained only on data from participating clients within the training domain. For out-of-domain data, the extracted embeddings may fall outside the learned embedding space, potentially causing the hypernetwork to generate inappropriate parameters.

## Limitations
- The approach relies on generalization of learned embedding space to unseen distributions within the same domain, but this assumption is not fully validated across heterogeneous domains.
- Chunked hypernetwork design introduces multiple hyperparameters (chunk size, architecture depth/width) with unclear sensitivity beyond reported ranges.
- Balancing Penalty and NoisyEmbed mechanisms prevent feature collapse but may introduce training instability not quantified in ablation studies.

## Confidence

- **High confidence**: Zero-shot personalization mechanism (hypernetwork conditioning on embeddings), computational efficiency claims (minimal overhead), and dataset-level performance comparisons.
- **Medium confidence**: Feature collapse prevention through NoisyEmbed and Balancing Penalty, optimal hyperparameter choices (P=16, chunk size=576, [300,300] hypernetwork), and trade-off characterization between zACC and pACC.
- **Low confidence**: Generalization to significantly different domains beyond training distribution, long-term stability of learned embeddings, and robustness to extreme data heterogeneity (αd approaching 0).

## Next Checks

1. **Cross-domain transfer**: Test HyperFedZero on a dataset from a different domain (e.g., medical imaging if trained on natural images) to evaluate embedding space generalization limits.

2. **Dynamic heterogeneity**: Gradually vary αd from 0.1 to 10 during training and evaluate the trade-off curve between zACC and pACC to understand sensitivity.

3. **Feature collapse stress test**: Systematically disable NoisyEmbed and/or Balancing Penalty components and quantify embedding variance collapse, then compare zACC degradation to determine their individual contributions.