---
ver: rpa2
title: 'DocVAL: Validated Chain-of-Thought Distillation for Grounded Document VQA'
arxiv_id: '2511.22521'
source_url: https://arxiv.org/abs/2511.22521
tags:
- reasoning
- spatial
- anls
- student
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DocV AL addresses the challenge of transferring spatial reasoning
  capabilities from large vision-language models to compact, deployable models for
  document visual question answering. The framework uses validated chain-of-thought
  distillation, where a large teacher model generates spatial reasoning traces that
  are filtered through multi-aspect validation, then transferred to a smaller student
  model through two-stage training: initial supervised learning on validated traces,
  followed by iterative refinement using detailed validation feedback.'
---

# DocVAL: Validated Chain-of-Thought Distillation for Grounded Document VQA

## Quick Facts
- **arXiv ID**: 2511.22521
- **Source URL**: https://arxiv.org/abs/2511.22521
- **Authors**: Ahmad Mohammadshirazi; Pinaki Prasad Guha Neogi; Dheeraj Kulshrestha; Rajiv Ramnath
- **Reference count**: 40
- **Primary result**: Achieves 91.4% ANLS and 82.4% mAP on DocVQA using a 12B parameter student model

## Executive Summary
DocVAL addresses the challenge of transferring spatial reasoning capabilities from large vision-language models to compact, deployable models for document visual question answering. The framework uses validated chain-of-thought distillation, where a large teacher model generates spatial reasoning traces that are filtered through multi-aspect validation, then transferred to a smaller student model through two-stage training: initial supervised learning on validated traces, followed by iterative refinement using detailed validation feedback. The student learns to perform pure visual-to-bounding-box mapping without requiring text detection at inference. DocVAL achieves 91.4% ANLS and 82.4% mAP on DocVQA using a 12B parameter student model, demonstrating state-of-the-art performance while operating as a pure VLM requiring no external text detection or OCR at inference.

## Method Summary
DocVAL is a three-phase pipeline for document VQA that transfers spatial reasoning from large teacher models to compact student models. Phase A generates CoT traces using a teacher VLM (Gemini 2.5 Pro) with text detection scaffolding, then filters them through VAL (5-module rule-based validator) to retain 95K high-quality examples. Phase B consists of two stages: B1 supervised fine-tuning of Gemma3-12B on filtered data, and B2 iterative refinement using VAL Verifier's detailed feedback. Phase C deploys the student as a pure VLM without detection/OCR dependencies. The framework achieves 82.4 mAP on DocVQA through validated reasoning transfer rather than implicit feature matching.

## Key Results
- Achieves 91.4% ANLS and 82.4% mAP on DocVQA with 12B parameter student
- Validated feedback contributes 6.3 mAP gain over binary filtering
- Iterative refinement contributes 9.7 mAP improvement over supervised learning alone
- Full validation prevents 18.7 mAP degradation compared to unfiltered training data

## Why This Works (Mechanism)

### Mechanism 1: Multi-Aspect Quality Filtering
VAL Filter computes composite quality score Q = 0.4·Qans + 0.4·Qbbox + 0.2·Qreason across five modules to remove hallucinated coordinates and inconsistent spatial relations. This prevents error propagation during student training. Evidence shows 18.7 mAP degradation when training on unfiltered data (102K) vs. filtered (95K).

### Mechanism 2: Asymmetric Detection Scaffolding
Teacher receives (I, R, q, agt, bgt) with detected regions R for higher-quality CoT generation, while student receives only (I, q) and learns pure visual spatial mapping. This enables quality supervision while training without detection dependencies. Removing detection drops mAP from 82.4 to 74.1 (-8.3).

### Mechanism 3: Iterative Refinement with Pixel-Level Feedback
VAL Verifier generates structured error reports with pixel-level corrections δ = [∆x₁, ∆y₁, ∆x₂, ∆y₂]ᵀ and priority-ordered fixes. Student fine-tunes on feedback-augmented examples over 12-16 iterations. This contributes 9.7 mAP improvement, with converged iteration reaching 82.4 mAP vs. 72.7 mAP for Stage B1 alone.

## Foundational Learning

- **Chain-of-Thought Reasoning in Vision-Language Models**
  - Why needed here: The framework relies on explicit spatial reasoning traces rather than direct coordinate prediction
  - Quick check question: Can you explain why "The answer is in the upper-right table cell, below the 'Total' label" provides stronger training signal than outputting coordinates [510, 800, 570, 830] directly?

- **Knowledge Distillation Paradigms**
  - Why needed here: DocVAL transfers capabilities through reasoning traces, not logits or features
  - Quick check question: What is the fundamental difference between distilling output distributions versus distilling explicit reasoning processes?

- **Bounding Box Regression and IoU Metrics**
  - Why needed here: Spatial grounding quality is measured via mAP across IoU thresholds [0.5:0.95:0.05]
  - Quick check question: Why does predicting a bbox with IoU=0.5 against ground truth indicate significantly worse localization than IoU=0.9?

## Architecture Onboarding

- **Component map**:
  Phase A: Text Detector (DB-ResNet) → Teacher VLM (Gemini 2.5 Pro) → VAL Filter → D_filtered (95K traces)
  VAL: OCR Grounding Engine → Answer Validator → BBox Validator → Reasoning Validator → Feedback Generator
  Stage B1: Student VLM (Gemma3-12B) → Supervised learning on D3 (76K)
  Stage B2: Student → VAL Verifier → Student fine-tuning on D4 (9.5K)

- **Critical path**: Phase A data quality → Stage B1 establishes baseline spatial reasoning → Stage B2 iterative refinement drives 9.7 mAP improvement

- **Design tradeoffs**:
  - Rule-based VAL vs. VLM-based validation: Deterministic feedback and zero cost vs. potential lack of nuance
  - Full fine-tuning vs. LoRA: Paper reports 8-12 mAP degradation with parameter-efficient methods
  - Detection method choice: DB-ResNet achieves best results (82.4 mAP) vs. CRAFT (80.6), PSENet (79.3), EasyOCR (78.7)

- **Failure signatures**:
  - Multi-line addresses: Student correctly extracts text (ANLS=1.0) but bbox covers only first line (IoU=0.31)
  - Handwritten text: Lower bbox precision (IoU=0.68 vs. >0.9 for print) due to detection gaps
  - Semantic confusion: Predicting "Subtotal" bbox when question asks for "Total"

- **First 3 experiments**:
  1. **VAL Filter ablation**: Train student on unfiltered 102K teacher outputs vs. filtered 95K. Expect ~18.7 mAP degradation on DocVQA.
  2. **Detection method comparison**: Run Phase A with DB-ResNet vs. CRAFT vs. no detection. Measure impact on VAL filtering throughput and downstream student mAP.
  3. **Iteration convergence analysis**: Log mAP at each Stage B2 iteration (5, 10, 14, 20). Verify convergence criterion (ΔmAP < 0.2 over 3 iterations).

## Open Questions the Paper Calls Out

### Open Question 1
Can DocVAL be extended to predict multiple bounding boxes for answers spanning disconnected document regions (e.g., multi-line addresses)?
- Basis in paper: Appendix D documents failure case with three-line address where student correctly extracted text (ANLS=1.0) but bbox covered only first line (IoU=0.31)
- Why unresolved: Current student output format produces single bbox per answer; architecture doesn't support variable-length bbox sequences
- What evidence would resolve it: Modified framework supporting multi-bbox outputs evaluated on dataset annotated with discontinuous answer regions

### Open Question 2
How does validation quality degrade when text detection fails on handwritten or degraded document regions, and can this be mitigated?
- Basis in paper: Appendix D reports handwritten text where student read correctly (ANLS=1.0) but bbox imprecise (IoU=0.68 vs. typical >0.9 for print)
- Why unresolved: DB-ResNet optimized for printed text; missed regions cause VAL to skip semantic validation
- What evidence would resolve it: Ablation using handwriting-aware detectors or synthetic handwritten augmentation

### Open Question 3
Does the validated CoT distillation paradigm transfer effectively to non-document spatial reasoning domains such as natural image VQA or robotic manipulation?
- Basis in paper: Section 5.3.4 states paradigm generalizes beyond documents to any domain requiring spatial reasoning
- Why unresolved: Paper only evaluates document benchmarks; visual features and spatial priors differ across domains
- What evidence would resolve it: Application to RefCOCO, GQA, or robotic pick-and-place task with domain-specific validators

### Open Question 4
What are the trade-offs between DocVAL's rule-based validator and a learned VLM-based validator with respect to feedback precision, scalability, and error propagation?
- Basis in paper: Section 3.3 explicitly defends rule-based design but leaves unexplored whether learned validators could capture nuanced reasoning errors
- Why unresolved: Paper doesn't compare against VLM-based validator baseline
- What evidence would resolve it: Head-to-head comparison using VLM validator (e.g., GPT-4o) on same refinement set

## Limitations
- The framework's effectiveness depends heavily on teacher's CoT reasoning quality, which is not fully specified
- Rule-based VAL heuristics may not generalize well to diverse document layouts or handwriting
- Iterative refinement requires careful tuning of convergence criteria and feedback quality, sensitive to document-specific characteristics

## Confidence
- **High confidence**: Two-stage training with validated data filtering demonstrably improves spatial reasoning (82.4 mAP vs. 63.7 mAP without validation)
- **Medium confidence**: Architectural design to withhold detection inputs is justified empirically, but mechanism for transferring spatial reasoning without region anchors remains opaque
- **Low confidence**: VAL Filter's multi-aspect scoring system and reasoning validator heuristics are described but not fully detailed

## Next Checks
1. **Cross-dataset generalization test**: Evaluate DocVAL on held-out document collection with different layouts (forms, receipts, scientific papers) to verify validated CoT traces generalize beyond training distribution
2. **Failure mode analysis**: Systematically examine student predictions that fail at low IoU thresholds (0.5) to determine whether errors stem from semantic confusion, coordinate precision issues, or architectural limitations
3. **VAL Filter sensitivity analysis**: Vary quality threshold Q from 0.7 to 0.95 and measure tradeoff between training data quantity and spatial reasoning accuracy to identify optimal filtering parameters