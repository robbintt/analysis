---
ver: rpa2
title: 'GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning'
arxiv_id: '2511.15256'
source_url: https://arxiv.org/abs/2511.15256
tags:
- grpo-rm
- learning
- segmentation
- representation
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes GRPO-RM, a reinforcement learning method that
  adapts Group Relative Policy Optimization (GRPO) for fine-tuning visual representation
  models like DINOv2. The method addresses the challenge of generalizing GRPO from
  language models to representation learning by replacing token sequence sampling
  with a predefined output set (dataset categories) and designing reward functions
  (accuracy and uniformity) tailored for visual features.
---

# GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.15256
- Source URL: https://arxiv.org/abs/2511.15256
- Reference count: 40
- Proposes GRPO-RM, achieving 4.26% average accuracy improvement on out-of-distribution classification tasks

## Executive Summary
GRPO-RM adapts Group Relative Policy Optimization from language models to fine-tune visual representation models like DINOv2. The method replaces token sequence sampling with dataset categories and designs accuracy and uniformity rewards for visual features. It achieves significant performance gains across multiple classification and segmentation datasets while converging faster than traditional fine-tuning.

## Method Summary
GRPO-RM extends GRPO to visual representation learning by modifying the sampling strategy and reward functions. Instead of sampling from token sequences, it samples from predefined output categories (dataset classes). The method uses accuracy rewards (r_acc_i = c if correct, 0 otherwise) and uniformity rewards (r_uni_i = -p_i) to optimize visual features. Advantages are computed via group normalization, and the optimization follows the standard GRPO objective with β=0 and ε=0.2. The method is evaluated on DINOv2 ViT-S/14 for image classification (CIFAR-10/100, STL-10, Tiny-ImageNet, ImageNet-1k) and semantic segmentation (Pascal VOC 2012, ADE20k, COCO-stuff).

## Key Results
- Achieves 4.26% average accuracy improvement on out-of-distribution classification tasks compared to standard fine-tuning
- Consistently improves semantic segmentation performance with 1.2% mIoU improvement on Pascal VOC
- Demonstrates faster convergence than traditional fine-tuning while maintaining computational efficiency

## Why This Works (Mechanism)
GRPO-RM works by adapting the reinforcement learning framework from NLP to vision through careful modification of the sampling and reward mechanisms. The key insight is replacing token-based sampling with category-based sampling, which better suits the structured output space of classification tasks. The uniformity reward encourages diverse predictions across categories, preventing the model from collapsing to a few dominant outputs. The accuracy reward provides direct supervision for correct predictions. Group normalization of advantages stabilizes training by reducing variance across samples.

## Foundational Learning

**Transformer Architecture**
- Why needed: Understanding how ViTs process input through self-attention layers is crucial for interpreting feature representations
- Quick check: Verify you understand how class tokens are extracted and processed in DINOv2

**Reinforcement Learning Optimization**
- Why needed: GRPO-RM fundamentally relies on policy gradient methods for optimization
- Quick check: Confirm you understand the difference between policy gradient and supervised learning

**Reward Function Design**
- Why needed: The method's success hinges on appropriate reward engineering for visual tasks
- Quick check: Verify you can explain why r_uni_i = -p_i is more effective than log-based uniformity rewards

## Architecture Onboarding

**Component Map**
- Pretrained DINOv2 backbone -> Projection head -> GRPO-RM training loop -> Evaluation head

**Critical Path**
1. Load pretrained DINOv2 model
2. Build appropriate projection head (classification or segmentation)
3. Implement GRPO-RM loop with accuracy and uniformity rewards
4. Train for 100 epochs
5. Evaluate with frozen backbone and new random head

**Design Tradeoffs**
- Uses group normalization instead of pairwise comparisons to reduce computational complexity
- Removes KL divergence term (β=0) to simplify optimization when reference model is unavailable
- Employs simple reward functions rather than complex metric-based rewards for efficiency

**Failure Signatures**
- Slow convergence: May indicate incorrect ε value or missing group normalization
- Degraded performance: Could result from improper reward function implementation
- Memory issues: Likely caused by incorrect batch size or model configuration

**First Experiments**
1. Verify baseline fine-tuning performance on a single dataset before implementing GRPO-RM
2. Test GRPO-RM with only accuracy reward (no uniformity) to isolate its contribution
3. Compare convergence curves of GRPO-RM vs standard fine-tuning on the same dataset

## Open Questions the Paper Calls Out

**Scaling to Larger Backbones**
The authors only tested on ViT-S/14 due to GPU limitations. It's unclear if the rapid convergence and performance gains transfer to larger models like ViT-L or ViT-G where gradient variance and memory constraints differ.

**Convolutional Architecture Adaptation**
While the paper mentions ResNet and U-Net, it restricts the method to ViTs. Adapting the output group logic to CNN feature maps is non-trivial and untested.

**Reference Model Impact**
The paper removes the KL divergence term (β=0) since no reference model is available, but doesn't analyze whether this leads to feature drift or instability common in RL fine-tuning.

## Limitations

- Limited to DINOv2 ViT-S/14 architecture without validation on larger models or convolutional backbones
- Learning rate schedules are vaguely specified as "gradually decreasing" without exact decay strategy
- Computational efficiency claims lack proper baseline comparisons for validation
- Limited ablation studies on alternative reward structures and their impact on convergence

## Confidence

**High confidence** in the core technical contribution of adapting GRPO to vision through category-based sampling
**Medium confidence** in empirical results due to limited ablation studies and unclear hyperparameter specifications
**Medium confidence** in convergence claims without comparison to established baselines
**Low confidence** in computational efficiency claims without proper baseline comparisons

## Next Checks

1. Implement and compare multiple learning rate schedules (cosine decay, linear decay, step decay) to verify the impact of the unspecified "gradually decreasing" schedule
2. Conduct ablation studies testing alternative uniformity reward formulations to validate the claimed superiority of r_uni_i = -p_i
3. Test GRPO-RM on a different backbone architecture (e.g., ConvNeXt or Swin Transformer) to assess generalizability beyond DINOv2 ViT-S/14