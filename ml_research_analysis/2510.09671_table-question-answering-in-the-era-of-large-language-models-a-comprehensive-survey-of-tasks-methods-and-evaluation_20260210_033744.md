---
ver: rpa2
title: 'Table Question Answering in the Era of Large Language Models: A Comprehensive
  Survey of Tasks, Methods, and Evaluation'
arxiv_id: '2510.09671'
source_url: https://arxiv.org/abs/2510.09671
tags:
- table
- zhang
- linguistics
- computational
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically organizes table question answering (TQA)
  research in the LLM era, categorizing diverse task setups, modeling approaches,
  and evaluation methods. It highlights the shift from retrieval-only to reasoning-intensive
  setups, the integration of multimodal data (text, images, charts), and the growing
  role of RLVR and interpretable reasoning.
---

# Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation

## Quick Facts
- **arXiv ID**: 2510.09671
- **Source URL**: https://arxiv.org/abs/2510.09671
- **Reference count**: 40
- **Primary result**: Systematically organizes TQA research, highlighting the shift to reasoning-intensive tasks, multimodal integration, and the role of RLVR and interpretable reasoning, while identifying key challenges and research gaps.

## Executive Summary
This survey provides a comprehensive overview of Table Question Answering (TQA) research in the era of Large Language Models (LLMs) and Multi-Modal LLMs (MLLMs). It categorizes diverse task setups, modeling approaches, and evaluation methods, emphasizing the transition from retrieval-focused to reasoning-intensive tasks. The work highlights the integration of multimodal data (text, images, charts) and the growing importance of RLVR and interpretable reasoning. While LLMs show promise, challenges remain in handling noisy, large, and hierarchical tables, and in achieving faithful, robust explanations. The survey identifies key research gaps and offers a consolidated roadmap for future TQA development.

## Method Summary
The survey conducted a systematic literature review, collecting 215 papers from arXiv, ACL, EMNLP, NAACL, NeurIPS, and ICLR using keywords related to table reasoning and question answering. Papers were filtered to focus on TQA/TFV (Table Fact Verification) and modeling work post-2022. The analysis involved applying taxonomies for task setups (Figure 2) and modeling challenges (Figure 3) to organize the collected papers. Performance claims were verified through exact match (EM) accuracy comparisons on the WTQ dataset for GPT-4, Gemini, and fine-tuned models (Figure 5).

## Key Results
- RLVR training improves LLMs' reasoning capabilities and generalizability over tabular data compared to supervised fine-tuning alone.
- Decomposing complex table reasoning questions into multi-step agentic workflows with tool use (e.g., Python/SQL code execution) reduces hallucination and improves accuracy.
- Retrieval-augmented generation (RAG) with fine-tuned retrievers is effective for handling large tables or multi-table scenarios by identifying and extracting relevant sub-tables.

## Why This Works (Mechanism)

### Mechanism 1: RLVR for Enhanced Reasoning
Reinforcement learning with verifiable rewards (RLVR) provides scalar reward signals based on answer correctness, code executability, or format compliance. This allows models to explore reasoning pathways that yield higher rewards, optimizing not just for final accuracy but for the reasoning process itself. Process-based rewards further refine this by assigning credit to intermediate steps. The core assumption is that the reward function accurately reflects desired behavior and the base model has sufficient reasoning primitives to be shaped by the reward signal.

### Mechanism 2: Multi-Step Agentic Workflows with Tool Use
A multi-step agent (often using ReAct-style prompting) first plans a solution, generates executable code (Python/SQL) to query or manipulate the table, executes the code, observes the result or error, and iterates. This externalizes precise computation and data retrieval, leveraging the LLM's semantic understanding for planning and its code generation capabilities for reliable execution. The core assumption is that the LLM can generate syntactically and logically correct code for the given question and table schema.

### Mechanism 3: Retrieval-Augmented Generation for Large/Multi-Table Scenarios
A retrieval model (fine-tuned on table-question pairs) first identifies the most relevant rows, columns, or entire tables from a large corpus. Only this pruned, relevant context is then passed to the LLM for reasoning. This mitigates the context length limitation and reduces noise. The core assumption is that relevance for answering the question can be determined from the question and table embeddings or heuristics.

## Foundational Learning

- **Concept: Large Language Model (LLM) & Multi-Modal LLM (MLLM) Basics**
  - **Why needed here**: The entire survey is framed around "(M)LLM-based methods." Understanding context windows, prompting strategies (CoT, ReAct), and the difference between text-only and multi-modal inputs is essential.
  - **Quick check question**: Can you explain the core difference between a standard LLM and an MLLM in processing a table image?

- **Concept: Reinforcement Learning (RL) Principles, especially RLVR**
  - **Why needed here**: The paper highlights RLVR as a key emerging method for TQA. Grasping the loop of policy, action, reward, and update is crucial for understanding this mechanism.
  - **Quick check question**: In RLVR for TQA, what would be a concrete example of a "verifiable reward"?

- **Concept: Program Synthesis & Tool-Augmented Agents**
  - **Why needed here**: A dominant solution pattern involves LLMs generating code (Python/SQL) as tools. Understanding how an agent decides to call a tool, uses its output, and handles errors is foundational.
  - **Quick check question**: Describe the workflow of a ReAct-style agent for a TQA question that requires a numerical calculation on a table.

## Architecture Onboarding

- **Component map**: The TQA system landscape can be mapped into: 1) **Input Processor** (handles text, image, hybrid formats), 2) **Retriever/Selector** (for large/multi-table setups), 3) **Reasoner Core** (the LLM/MLLM itself, potentially fine-tuned), 4) **Tool Interface** (optional code executor for Python/SQL), 5) **Reward Model** (for RLVR training). The choice of which components to use is dictated by the **Task Setup**.

- **Critical path**: Start by diagnosing the **Task Setup**. Is it retrieval-based? Multi-modal? Requires complex reasoning? This determines the architecture. For a standard closed-domain, single text-table, complex query: the path is `Input Processor -> Reasoner Core (with code tool interface)`. For open-domain multi-table: the path includes `Retriever/Selector` before the reasoner. For training a new model, `RLVR` loops with a `Reward Model` become part of the path.

- **Design tradeoffs**:
  1. **Tuning-Free vs. Tuning-Based**: Tuning-free (agentic) is flexible, no training needed, but has high inference cost and latency. Tuning-based is inference-efficient but risks overfitting and domain shift. A hybrid is promising.
  2. **Textual vs. Visual Table Input**: Textual (e.g., Markdown) is easier for LLMs but loses layout. Visual (image) preserves layout but challenges MLLMs with OCR and spatial reasoning. The choice depends on table complexity and available models.
  3. **End-to-End vs. Decomposed Models**: End-to-end models are simpler but harder to debug. Decomposed pipelines (retriever -> reasoner) are interpretable and modular but introduce error propagation between stages.

- **Failure signatures**:
  1. **Hallucination in Free-Form Answers**: LLM generates plausible-sounding but factually unsupported explanations.
  2. **Brittleness to Table Perturbations**: Performance drops significantly with row/column shuffles or minor format changes.
  3. **Retrieval Failure in Open-Domain Settings**: The retriever returns irrelevant tables, causing the reasoner to fail.
  4. **Code Generation/Execution Errors**: In tool-augmented setups, the LLM generates buggy code, or the code executes but on wrong assumptions.

- **First 3 experiments**:
  1. **Baseline Establishment**: Implement a simple, tuning-free agentic baseline (e.g., using GPT-4 or an open LLM with a ReAct prompt and Python tool) on a standard benchmark like WTQ or TabFact. Measure accuracy (EM), cost, and latency.
  2. **Retriever Integration for Large Tables**: Take a dataset with large tables (e.g., from TableBench or RealHiTBench). Implement a simple dense retriever to fetch top-k rows. Compare the performance of the baseline agent with vs. without the retriever on the same questions.
  3. **RLVR Feasibility Test**: Using a smaller open-source LLM (e.g., Llama-3-8B), set up a simple RLVR training loop on a synthetic dataset. The reward function can be `1` if the generated code executes and the answer matches the ground truth, `0` otherwise. Compare the post-RLVR model's performance on a held-out set against the same model after SFT.

## Open Questions the Paper Calls Out

- **Open Question 1**: Is there a universally optimal textual representation format (e.g., JSON, Markdown, HTML) for processing tables in Large Language Models? The survey states that there seems to be no optimal format across datasets and models when using fine-grained textual representations. A comprehensive comparative study across major LLMs that isolates performance variance attributable solely to the input serialization format would resolve this.

- **Open Question 2**: Can unified multi-modal models with dedicated encoders outperform ensemble strategies for table reasoning? The survey suggests that using separate encoders to integrate text and images potentially leads to more robust and generalizable methods. A model architecture that fuses visual and textual table encodings, demonstrating superior performance on complex tables compared to modality-selection ensembles, would provide evidence.

- **Open Question 3**: How can models be trained to produce faithful explanations that accurately reflect their internal table reasoning process? The survey argues that current work focuses on post-hoc justifications rather than genuine explanations that transparently reveal the reasoning process. An evaluation framework that successfully validates a causal link between the model's generated reasoning steps and its final answer would resolve this.

- **Open Question 4**: How can TQA systems natively handle low-resource languages without relying on inconsistent translation pipelines? The survey notes that multilingual fine-tuning does not consistently improve performance and translation-based approaches often fall short. A modeling approach that achieves high accuracy on low-resource TQA benchmarks strictly using native inputs, significantly outperforming translation-based baselines, would provide evidence.

## Limitations

- The effectiveness of RLVR for TQA remains largely theoretical, with limited published empirical comparisons against strong supervised baselines.
- For multi-modal tables, the survey acknowledges but does not resolve the fundamental tension between preserving layout information and ensuring robust OCR-to-text conversion.
- The taxonomy of task setups, while comprehensive, may not fully capture hybrid or emerging scenarios (e.g., real-time table QA in financial dashboards).

## Confidence

- **High Confidence**: The classification of task setups (textual, visual, hybrid) and their associated challenges is well-supported by the cited literature and the survey's systematic methodology.
- **Medium Confidence**: The effectiveness of RLVR and tool-augmented agents is inferred from general LLM literature and a few TQA-specific studies; broader empirical validation across diverse table types is needed.
- **Low Confidence**: Claims about open-domain and multilingual TQA being "emerging frontiers" are based on the survey's identification of a research gap rather than concrete evidence of imminent breakthroughs.

## Next Checks

1. **Reproduce the WTQ benchmark results** (Fig 5) using the exact prompts and evaluation criteria specified in the survey to verify the claimed performance gap between GPT-4, Gemini, and fine-tuned models.
2. **Implement a minimal RLVR loop** on a synthetic TQA dataset to empirically test whether verifiable rewards improve reasoning over tables compared to supervised fine-tuning alone.
3. **Benchmark OCR pipeline robustness** by evaluating the same visual table through multiple OCR engines and measuring the impact on downstream TQA accuracy to quantify the layout-to-text information loss.