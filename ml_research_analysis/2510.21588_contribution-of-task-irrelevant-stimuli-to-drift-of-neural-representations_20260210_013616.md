---
ver: rpa2
title: Contribution of task-irrelevant stimuli to drift of neural representations
arxiv_id: '2510.21588'
source_url: https://arxiv.org/abs/2510.21588
tags:
- drift
- learning
- network
- where
- subspace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the contribution of task-irrelevant stimuli
  to representational drift in neural networks through theoretical analysis and simulations.
  The study demonstrates that even when neural networks learn to suppress task-irrelevant
  information, these stimuli can still act as a source of learning noise that causes
  gradual changes in the representation of task-relevant stimuli over time.
---

# Contribution of task-irrelevant stimuli to drift of neural representations

## Quick Facts
- arXiv ID: 2510.21588
- Source URL: https://arxiv.org/abs/2510.21588
- Reference count: 40
- Key result: Task-irrelevant stimuli cause gradual changes in neural representations even when networks suppress this information, with drift rates scaling as variance and dimension of irrelevant stimuli

## Executive Summary
This paper demonstrates that task-irrelevant stimuli act as a persistent source of learning noise that causes gradual drift in neural representations, even after networks have converged and learned to suppress irrelevant information. Through theoretical analysis and simulations across multiple architectures (Oja's rule, Similarity Matching, autoencoders, and supervised networks), the study shows that drift rates increase with both the variance and dimension of task-irrelevant stimuli. The theoretical predictions show excellent agreement with simulations on both synthetic Gaussian data and real MNIST data, revealing that learning-induced drift has distinct geometric properties compared to drift from synaptic noise.

## Method Summary
The study analyzes representational drift in neural networks trained online after convergence, focusing on how task-irrelevant stimuli contribute to gradual changes in task-relevant representations. The method uses synthetic Gaussian data with structured covariance (first m eigenvalues = 1, remaining n-m eigenvalues = λ⊥ < 1) and MNIST data (optionally PCA-projected). Multiple architectures are examined: Oja's rule networks, Similarity Matching networks, linear autoencoders, and two-layer supervised networks. Drift is measured through autocorrelation/cosine similarity decay of task-relevant representations over time, with theoretical diffusion coefficients derived analytically. Key parameters include learning rates (η=0.025 for Oja, η=0.001 for Figure 3, η=0.01 for MNIST) and weight decay (γ=0.2 for supervised networks).

## Key Results
- Task-irrelevant stimuli cause drift in neural representations even when networks learn to suppress this information
- Drift rates scale as D ∝ λ⊥²(n-m) and D ∝ η³ across different architectures
- Theoretical predictions match simulation results on both synthetic and MNIST data
- Learning-induced drift has different geometric properties than drift from synaptic noise
- Non-monotonic relationship between drift rate and bottleneck dimension in autoencoders

## Why This Works (Mechanism)
The mechanism works because task-irrelevant stimuli, while suppressed in the final representation, still interact with the learning dynamics during online updates. Even after convergence, each training example perturbs the weight matrix slightly, and because task-irrelevant dimensions have non-zero variance, these perturbations accumulate over time. The learning rule's attempt to minimize the effect of irrelevant stimuli on the output creates a feedback loop where small deviations compound, causing gradual drift in how task-relevant stimuli are represented. This drift is governed by a diffusion process where the diffusion coefficient depends on the variance and dimension of irrelevant stimuli, as well as the