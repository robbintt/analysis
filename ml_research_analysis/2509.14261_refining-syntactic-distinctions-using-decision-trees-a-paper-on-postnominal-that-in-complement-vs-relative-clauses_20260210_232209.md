---
ver: rpa2
title: 'Refining Syntactic Distinctions Using Decision Trees: A Paper on Postnominal
  ''That'' in Complement vs. Relative Clauses'
arxiv_id: '2509.14261'
source_url: https://arxiv.org/abs/2509.14261
tags:
- syntactic
- treebank
- relative
- clauses
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addressed the challenge of distinguishing the syntactic
  roles of postnominal "that" in English as either a relative pronoun or a complementizer.
  The core method involved retraining the TreeTagger model using reannotated corpus
  data from the EWT Treebank to improve parsing accuracy.
---

# Refining Syntactic Distinctions Using Decision Trees: A Paper on Postnominal 'That' in Complement vs. Relative Clauses

## Quick Facts
- arXiv ID: 2509.14261
- Source URL: https://arxiv.org/abs/2509.14261
- Authors: Hamady Gackou
- Reference count: 9
- One-line result: Decision tree model accuracy degrades catastrophically with increased training data due to annotation inconsistency when distinguishing postnominal "that" as relative pronoun vs. complementizer.

## Executive Summary
This study addresses the challenge of distinguishing postnominal "that" in English as either a relative pronoun or a complementizer using decision tree models. The research retrained the TreeTagger model using reannotated corpus data from the EWT Treebank to improve parsing accuracy. Results showed significant performance variations across different training approaches, with accuracy for tagging "that" as a subordinator (WPR) dropping from 12% to 0.02% with increased training data, highlighting critical issues with annotation quality and model confusion.

## Method Summary
The method involved annotating the Brown Corpus using UDPipe's EWT model, then applying a heuristic algorithm to reannotate "that" instances as either WPR (relative pronoun) or CST (complementizer) based on UD dependency relations (acl:relcl vs. ccomp). TreeTagger models were trained using varying amounts of this reannotated data (10, 30, 100, 200, 300, 500 files) and evaluated on 200-sentence test sets per category. The approach compared Penn Treebank and BNC Treebank models, measuring precision, recall, and F1-score across different syntactic categories.

## Key Results
- WPR tag accuracy catastrophically degraded from 12% to 0.02% as training data increased
- CST tag performance declined by 40% at scale, mirroring difficulties in maintaining noun phrase boundary detection
- Penn model achieved moderate precision (0.6) for determiners but struggled with other categories, while BNC model excelled in conjunctions (precision=1.0) but lacked broader coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heuristic reannotation using dependency relations can distinguish relative pronoun vs. complementizer uses of "that"
- Mechanism: The algorithm maps Universal Dependencies relations (acl:relcl → WPR for relative clauses, ccomp → CST for complement clauses) to CLAWS C8 tags, creating function-specific training labels from syntactic structure
- Core assumption: UD dependency annotations accurately capture the functional distinction between clause types
- Evidence anchors:
  - [abstract] "we employed an algorithm to reannotate a corpus that had originally been parsed using the Universal Dependency framework"
  - [section 2.6] "analyzing the dependency relations and POS tags for each occurrence of 'that'"
  - [corpus] Related work on that-mentioning (arXiv:2509.05254) explores similar syntactic distinctions but uses UID hypothesis, not decision trees—limited direct validation
- Break condition: If source UD annotations are inconsistent (the paper itself notes this limitation), reannotation propagates errors

### Mechanism 2
- Claim: Decision-tree-based taggers can learn context-dependent syntactic role assignment for ambiguous words
- Mechanism: TreeTagger's decision tree classifies tokens using surrounding context and syntactic features; retraining adapts probability distributions to specialized distinctions
- Core assumption: Local context windows contain sufficient disambiguating information
- Evidence anchors:
  - [section 2.12] "TreeTagger uses a decision tree algorithm that classifies words based on their context and syntactic features"
  - [section 1.6] Tighidet and Ballier (2022) "demonstrated that reannotation and model training with clearer function-based distinctions could enhance syntactic parsing accuracy"
  - [corpus] CART-ELC paper (arXiv:2505.05402) shows exhaustive-search decision trees can improve classification—but for oblique splits, not NLP specifically
- Break condition: If syntactic boundaries are misidentified, contextual features become unreliable

### Mechanism 3
- Claim: Increased training data can degrade rather than improve tagging accuracy when annotation quality is inconsistent
- Mechanism: Model confusion emerges when larger corpora introduce more ambiguous cases without consistent labels, diluting learned patterns
- Core assumption: Annotation errors are systematic rather than random, creating conflicting training signals
- Evidence anchors:
  - [section 4.1] "WPR tag shows catastrophic performance degradation, falling from 12% to 0.02% accuracy"
  - [section 4.2] "CST tag's initial superiority... however, its 40% decline at scale mirrors difficulties in maintaining noun phrase boundary detection"
  - [corpus] No direct corpus validation of negative scaling effects; this finding is internal to the study
- Break condition: If annotations were consistent at scale, performance should plateau or improve, not degrade

## Foundational Learning

- Concept: Universal Dependencies (UD) framework
  - Why needed here: The reannotation pipeline depends on understanding UD relations (acl:relcl, ccomp) to classify "that" instances
  - Quick check question: In UD, which relation marks a clause modifying a noun vs. a clause serving as a verbal complement?

- Concept: Part-of-speech tagsets (CLAWS, Penn Treebank)
  - Why needed here: The study maps between tagsets (CLAWS C8, Penn) and evaluates model performance per category
  - Quick check question: What syntactic categories does CLAWS C8 use to distinguish "that" functions that Penn Treebank conflates?

- Concept: Decision tree classification
  - Why needed here: TreeTagger's core algorithm; understanding its feature-based splitting is essential for interpreting results
  - Quick check question: How does a decision tree handle ambiguous contexts where the same local features could indicate different syntactic roles?

## Architecture Onboarding

- Component map: UDPipe API → CoNLL-U annotations → Heuristic reannotation module → CLAWS C8 tagged corpus → TreeTagger training → .par model files → Evaluation on 200-sentence test sets

- Critical path: Annotation consistency in reannotation step (Section 2.6) is the bottleneck—errors here cascade to all downstream training

- Design tradeoffs:
  - Penn model: broader category coverage (DT, IN, RB, WDT, WP) but lower precision; BNC model: perfect on conjunctions (n_CJT=1.0) but zero coverage elsewhere
  - Larger lexicons capture more patterns but introduce more ambiguity—optimal training size may be smaller than maximum available

- Failure signatures:
  - Accuracy declining with training data size indicates annotation inconsistency, not model capacity issues
  - Categories with zero recall (RB, WP in Penn; n_DTO, n_DTQ in BNC) suggest tagset-training data misalignment
  - WPR at 0.02% suggests the model learned to avoid the tag entirely due to conflicting examples

- First 3 experiments:
  1. Validate reannotation accuracy: manually sample 50 "that" instances from reannotated corpus and compare heuristic labels against gold-standard linguistic analysis
  2. Test training size ceiling: train models at 10, 30, 100, 200, 300, 500 files and plot accuracy curves to identify where degradation begins
  3. Hybrid baseline: compare TreeTagger performance against a BERT-based tagger (as suggested in Discussion) on the same CST/WPR test sets to quantify the contextual representation gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does increasing the training dataset size cause a catastrophic drop in accuracy for WPR (subordinating *that*) and CST tags?
- Basis in paper: [inferred] The paper reports in Section 4.1 and 4.2 that WPR accuracy fell from 12% to 0.02% and CST tagging declined by 40% as training data scaled, contrary to standard machine learning expectations.
- Why unresolved: The authors suggest "annotation inconsistencies" or "feature engineering" gaps but do not isolate the specific factor causing the negative correlation with data volume.
- What evidence would resolve it: An ablation study comparing model performance on manually verified "clean" subsets versus raw noisy data at increasing scales.

### Open Question 2
- Question: Can transformer-based architectures successfully resolve the syntactic boundary detection failures observed in TreeTagger?
- Basis in paper: [explicit] The Conclusion (6) explicitly recommends "incorporating deep learning approaches and transformer-based models, such as BERT," to better handle the ambiguities where the decision-tree model failed.
- Why unresolved: The study was strictly limited to evaluating the decision-tree-based TreeTagger model.
- What evidence would resolve it: Benchmarking a BERT-based tagger on the specific WPR/CST test sets created for this study to measure improvement in contextual disambiguation.

### Open Question 3
- Question: How do specific inconsistencies in the EWT Treebank's dependency relations (e.g., `acl:relcl` vs. `ccomp`) propagate errors during the reannotation process?
- Basis in paper: [explicit] Section 3.1 states the analysis revealed "limitations in distinguishing relative clauses and nominal complement clauses... leading to syntactic ambiguities," noting that *that* is often annotated identically regardless of function.
- Why unresolved: The paper identifies the source of the noise but does not quantify how often these specific UD relation errors cause the reannotation algorithm to fail.
- What evidence would resolve it: A manual error analysis of the reannotation output mapping specific UD relation errors to resulting CST/WPR misclassifications.

## Limitations

- The core limitation is the assumption that UD dependency annotations reliably distinguish relative clauses from complement clauses for the reannotation heuristic, which the paper acknowledges as problematic.
- The 200-sentence test files per category are described as "generated via prompt" without specification of generation method or access to actual sentences, making independent validation impossible.
- The study lacks direct comparison to state-of-the-art contextual models (BERT, RoBERTa) beyond passing mention in the Discussion section, leaving the actual performance gap unquantified.

## Confidence

- **High confidence**: The observation that accuracy degrades with increased training data due to annotation inconsistency is well-supported by the data showing WPR dropping from 12% to 0.02% and CST declining by 40% at scale.
- **Medium confidence**: The effectiveness of the heuristic reannotation approach is plausible given the UD framework's design, but unverified due to lack of manual validation samples.
- **Low confidence**: The comparative performance analysis between Penn and BNC models is difficult to interpret due to their fundamentally different tagset coverage and the paper's focus on different categories for each.

## Next Checks

1. **Validate reannotation accuracy**: Manually sample and annotate 50 "that" instances from the reannotated Brown Corpus, comparing heuristic labels against gold-standard linguistic analysis to quantify annotation error rates.

2. **Quantify contextual model gap**: Train and evaluate a BERT-based tagger on the same CST/WPR test sets to establish baseline performance and measure the improvement potential from contextual representations.

3. **Test annotation consistency at scale**: Train models at 10, 30, 100, 200, 300, and 500 files while tracking accuracy curves to precisely identify where and how annotation inconsistency begins degrading performance.