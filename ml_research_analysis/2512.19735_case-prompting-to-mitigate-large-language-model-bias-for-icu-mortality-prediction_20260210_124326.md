---
ver: rpa2
title: Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction
arxiv_id: '2512.19735'
source_url: https://arxiv.org/abs/2512.19735
tags:
- bias
- fairness
- clinical
- performance
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses demographic bias in large language model (LLM)-based
  ICU mortality prediction, where models may underperform for specific patient subgroups.
  The proposed CAse Prompting (CAP) framework mitigates this bias by integrating conventional
  debiasing prompts with case-based reasoning, guiding models to learn from historically
  mispredicted cases.
---

# Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction

## Quick Facts
- arXiv ID: 2512.19735
- Source URL: https://arxiv.org/abs/2512.19735
- Authors: Gangxiong Zhang; Yongchao Long; Yong Zhang; Yuxi Zhou; Shenda Hong
- Reference count: 36
- One-line result: CAP improves ICU mortality prediction AUROC from 0.806 to 0.873 and reduces sex/race prediction disparities by over 90%

## Executive Summary
This study addresses demographic bias in large language model (LLM)-based ICU mortality prediction, where models may underperform for specific patient subgroups. The proposed CAse Prompting (CAP) framework mitigates this bias by integrating conventional debiasing prompts with case-based reasoning, guiding models to learn from historically mispredicted cases. Evaluated on MIMIC-IV data, CAP achieved substantial improvements: AUROC increased from 0.806 to 0.873 (+0.067) and AUPRC from 0.497 to 0.694 (+0.197), while reducing sex- and race-related prediction disparities by over 90%. Feature reliance analysis showed high consistency across demographic groups (similarity >0.98), demonstrating that CAP effectively co-optimizes fairness and performance without retraining.

## Method Summary
CAP uses MIMIC-IV data (45,173 patients) to predict ICU mortality while mitigating demographic bias. The framework constructs a case repository from training set misclassifications (FP/FN) identified through GPT-4 counterfactual bias analysis. For each test case, similar misprediction cases are retrieved via weighted cosine similarity (threshold ≥0.8, ≥2 demographic matches) and injected into structured four-module prompts. The frozen Qwen3-32B model then reasons through the combined prompt-case context. Evaluation measures performance (AUROC, AUPRC, F1, Brier) alongside fairness (Equal Opportunity Difference) and feature consistency (Jaccard/Cosine/JS Divergence similarity metrics). XGBoost serves as baseline.

## Key Results
- AUROC improved from 0.806 to 0.873 (+0.067)
- AUPRC improved from 0.497 to 0.694 (+0.197)
- Sex- and race-related prediction disparities reduced by over 90%
- Feature reliance similarity across demographic groups exceeded 0.98

## Why This Works (Mechanism)

### Mechanism 1: Case-Based Analogical Learning from Mispredictions
Providing similar historical misprediction cases with correct outcomes enables the model to correct biased reasoning patterns without parameter updates. The retrieval system identifies cases where the base model produced false positives/negatives and counterfactual analysis confirmed demographic bias. These labeled cases are injected into the prompt, allowing the LLM to perform in-context analogical reasoning—applying corrections from specific exemplars rather than abstract fairness instructions alone.

### Mechanism 2: Feature Reliance Consistency Normalization
By learning from corrected cases, the model shifts attention from demographic-correlated spurious features to clinically universal features, improving both fairness and accuracy. Case examples demonstrate which clinical features should drive predictions. When the model internalizes these patterns, it applies consistent feature attention across demographic subgroups rather than relying on group-specific shortcuts.

### Mechanism 3: Structured Prompt Composition with Constraint-Exemplar Integration
Combining explicit fairness constraints with concrete case exemplars outperforms either approach in isolation. The four-module prompt (Role Definition, Task Description, Decision Constraints, Case Information) provides both top-down rule enforcement and bottom-up pattern learning. Fairness instructions set boundaries; cases demonstrate correct behavior within those boundaries.

## Foundational Learning

- **Concept**: Equal Opportunity Difference (EOD)
  - Why needed: EOD quantifies fairness by comparing false negative rates across groups. The paper claims >90% reduction in sex/race disparities using this metric—understanding it is essential to evaluate that claim.
  - Quick check: If EOD for Male vs. Female is 0.003, what does this imply about the relative rate at which the model misses mortality cases for each sex?

- **Concept**: Feature Dependence Similarity Metrics (Jaccard/Cosine/JS Divergence)
  - Why needed: The paper's innovation includes measuring "latent bias" through whether different groups receive predictions based on different feature patterns. These metrics operationalize that measurement.
  - Quick check: A model achieves equal AUROC across groups but Jaccard similarity of top features is 0.65. Is this model "fair"? Why or why not?

- **Concept**: In-Context Learning via Retrieval Augmentation
  - Why needed: CAP's mechanism relies on the LLM learning from retrieved cases during inference. Understanding this capacity explains why no retraining is required.
  - Quick check: How does presenting a misprediction case with its correct outcome differ epistemically from instructing the model to "avoid demographic bias"?

## Architecture Onboarding

- **Component map**: MIMIC-IV data → Case repository construction → Retrieval engine → Prompt composer → Frozen Qwen3-32B → Evaluation suite
- **Critical path**: Case repository quality → Retrieval relevance → Case-patient match → Prompt composition → LLM reasoning quality
- **Design tradeoffs**:
  1. Repository size vs. precision: Paper uses "small but representative subset"; larger repositories risk noise
  2. Similarity threshold (0.8) vs. coverage: Higher threshold ensures relevance but may fail on rare profiles
  3. Demographic matching strictness: Requiring 2+ attribute matches ensures demographic relevance but excludes clinically similar cases with different demographics
  4. Training-free constraint: Preserves base model capabilities but limits domain adaptation
- **Failure signatures**:
  1. Low retrieval match rate: If <60% of test cases find ≥0.8 similarity matches, mitigation will be inconsistent
  2. Feature similarity remains <0.90: Indicates case prompting isn't shifting reasoning patterns
  3. EOD improves but AUROC drops >5%: Suggests fairness achieved via degraded sensitivity, not corrected reasoning
  4. Age EOD resistant: Age showed highest base bias (EOD=0.180); persistent high values indicate mechanism limitations
- **First 3 experiments**:
  1. Baseline validation: Reproduce base Qwen3-32B performance (AUROC ≈0.806, inter-group AUROC spread ≈0.07) to confirm evaluation pipeline
  2. Retrieval coverage audit: Measure what fraction of test cases retrieve valid matches; profile cases that fail to match
  3. Ablation: CAP vs. components: Run CAP against Fairness-Aware Prompt and System 2 Prompt in isolation to quantify contribution of case-based component to the 90%+ EOD reduction

## Open Questions the Paper Calls Out
- **Question**: Does the CAse Prompting (CAP) framework generalize effectively to LLMs with different architectures or smaller parameter scales?
  - **Basis**: The authors acknowledge that "the generalizability of this approach to LLMs of different scales or architectures remains to be systematically verified," noting the study relied solely on Qwen3-32B.
  - **Why unresolved**: Analogical reasoning capabilities vary significantly across model sizes and families; it is unclear if smaller models can utilize the case context effectively without hallucination.
  - **What evidence would resolve it**: Evaluating CAP across diverse model families (e.g., Llama, GPT) and sizes (7B, 70B) using the identical MIMIC-IV protocol.

- **Question**: Can the case retrieval mechanism be optimized for real-time application in dynamic clinical settings?
  - **Basis**: The authors state that "developing more intelligent and efficient mechanisms for case retrieval and adaptation to enable real-time application in dynamic clinical settings will be critical."
  - **Why unresolved**: The current framework relies on weighted cosine similarity and threshold-based penalties, which may introduce latency prohibitive to acute care workflows.
  - **What evidence would resolve it**: Latency benchmarks of the retrieval pipeline and an analysis of performance retention when using approximate nearest-neighbor search algorithms.

- **Question**: How robust is the CAP framework to variations in the quality and composition of the historical misprediction case library?
  - **Basis**: The method relies on a library constructed from training set errors (FN/FP) and counterfactual analysis by GPT-4. The paper does not analyze performance sensitivity if the case library is sparse for specific demographic subgroups or contains labeling noise.
  - **Why unresolved**: If the case library lacks diverse examples for minority groups, the retrieval mechanism might fail to provide relevant analogies, potentially failing to mitigate bias for those specific subgroups.
  - **What evidence would resolve it**: Ablation studies varying the density and noise levels of the case library, specifically stratified by demographic subgroups.

## Limitations
- Case repository representativeness is unclear, raising questions about whether retrieved cases adequately represent minority subgroups where bias is most problematic
- Prompt template specificity is incomplete, making exact reproduction difficult without full prompt texts
- GPT-4 bias labeling reliability is unvalidated, relying on LLM's ability to identify demographic bias in predictions

## Confidence
- **High confidence**: AUROC/AUPRC improvements (0.806→0.873, 0.497→0.694) are well-documented with standard metrics and statistical significance
- **Medium confidence**: EOD reductions >90% are methodologically sound but depend on the counterfactual analysis quality
- **Medium confidence**: Feature similarity improvements (>0.98) are measured consistently but may not fully capture clinical validity of feature distributions

## Next Checks
1. **Case repository audit**: Verify the size, demographic composition, and bias-labeling accuracy of the curated case database used for retrieval
2. **Retrieval coverage analysis**: Quantify what percentage of test cases successfully retrieve matches and identify systematic gaps in coverage
3. **Cross-cohort validation**: Test CAP performance on an independent ICU dataset (e.g., eICU) to assess generalizability beyond MIMIC-IV