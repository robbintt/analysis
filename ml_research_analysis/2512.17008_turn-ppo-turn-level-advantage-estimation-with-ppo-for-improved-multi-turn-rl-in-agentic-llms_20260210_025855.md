---
ver: rpa2
title: 'Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn
  RL in Agentic LLMs'
arxiv_id: '2512.17008'
source_url: https://arxiv.org/abs/2512.17008
tags:
- click
- color
- product
- grpo
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies key limitations in applying Group Relative
  Policy Optimization (GRPO) to multi-turn LLM agent training, particularly its instability
  and suboptimal advantage estimation in long-horizon reasoning tasks. To address
  these issues, the authors introduce turn-PPO, a variant of PPO that reformulates
  the MDP at the turn level rather than the token level, enabling more accurate and
  stable advantage estimation.
---

# Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs

## Quick Facts
- **arXiv ID**: 2512.17008
- **Source URL**: https://arxiv.org/abs/2512.17008
- **Reference count**: 16
- **Primary result**: Turn-PPO outperforms GRPO and token-level PPO on multi-turn reasoning tasks through turn-level advantage estimation.

## Executive Summary
This paper addresses critical limitations in applying Group Relative Policy Optimization (GRPO) to multi-turn LLM agent training, particularly its instability and suboptimal advantage estimation in long-horizon reasoning tasks. The authors introduce turn-PPO, a variant of PPO that reformulates the Markov Decision Process at the turn level rather than the token level, enabling more accurate and stable advantage estimation. Through experiments on WebShop and Sokoban tasks, turn-PPO demonstrates superior performance in both reward achievement and training stability compared to existing approaches, especially in settings involving long reasoning chains.

## Method Summary
Turn-PPO reformulates the MDP at the turn level to address the limitations of token-level advantage estimation in multi-turn reasoning tasks. Unlike token-level PPO, which treats each token as an independent action, turn-PPO considers entire turns as atomic decision units, reducing variance in advantage estimates. The method introduces a turn-level advantage estimator that accumulates rewards across turns while maintaining the temporal structure of reasoning processes. The policy update follows the standard PPO clipped objective but operates on turn-level trajectories, enabling more stable gradient estimates. This approach specifically targets the sparsity and chain-like dependencies inherent in multi-turn reasoning, where rewards are typically sparse and distributed across multiple turns.

## Key Results
- Turn-PPO achieves higher final rewards than both GRPO and token-level PPO on WebShop and Sokoban tasks
- Training stability is significantly improved with turn-PPO, showing smoother learning curves and faster convergence
- The method demonstrates particular effectiveness in long reasoning chains where token-level approaches suffer from high variance

## Why This Works (Mechanism)
The core mechanism behind turn-PPO's success lies in its reformulation of the MDP at the turn level. In multi-turn reasoning tasks, rewards are typically sparse and distributed across multiple turns, creating a challenge for standard RL algorithms that estimate advantages at the token level. By treating entire turns as atomic decision units, turn-PPO reduces the variance in advantage estimates and better captures the temporal dependencies in reasoning chains. The turn-level advantage estimator accumulates rewards across turns while maintaining the sequential structure of the reasoning process, providing more accurate credit assignment. This approach effectively addresses the instability and suboptimal performance of GRPO in long-horizon tasks by providing more stable gradient estimates during policy updates.

## Foundational Learning

**Multi-turn reasoning tasks**
*Why needed*: Understanding the sequential nature of reasoning processes and why standard RL approaches struggle with sparse rewards distributed across multiple turns
*Quick check*: Can identify scenarios where rewards are delayed or sparse across multiple steps

**Advantage estimation**
*Why needed*: Critical for understanding how value estimates guide policy updates and why variance reduction matters
*Quick check*: Can explain the difference between Monte Carlo returns and advantage functions

**Markov Decision Process (MDP) formulation**
*Why needed*: Essential for grasping how the problem is structured and why turn-level vs token-level reformulations matter
*Quick check*: Can describe state transitions and reward structures in both formulations

**Policy gradient methods**
*Why needed*: Understanding how policy updates work and why stable gradient estimates are crucial for training
*Quick check*: Can explain the relationship between advantage estimates and policy gradient direction

**GRPO limitations**
*Why needed*: Provides context for why turn-PPO was developed and what specific problems it addresses
*Quick check*: Can articulate why group-relative baselines may be problematic in multi-turn settings

## Architecture Onboarding

**Component map**
PPO policy network -> Turn-level trajectory processor -> Advantage estimator -> Clipped policy update -> Value function approximator

**Critical path**
Input sequence -> Turn segmentation -> Reward accumulation -> Advantage calculation -> Policy gradient computation -> Parameter update

**Design tradeoffs**
- Turn-level vs token-level action space: Reduces variance but may lose fine-grained control
- Computational efficiency: Turn-level processing reduces trajectory length but requires additional segmentation logic
- Credit assignment: More accurate across turns but potentially less precise within turns

**Failure signatures**
- High variance in advantage estimates indicates poor turn segmentation or reward accumulation
- Unstable training curves suggest issues with the clipped objective implementation
- Suboptimal performance on short-horizon tasks may indicate over-generalization to turn-level processing

**3 first experiments**
1. Compare training stability (learning curves) between turn-PPO and token-level PPO on a simple multi-turn task
2. Analyze variance in advantage estimates across different segmentations (token vs turn level)
3. Test sensitivity to turn boundary detection on tasks with varying reasoning chain lengths

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to only two tasks (WebShop and Sokoban), constraining generalizability
- Computational overhead compared to GRPO not thoroughly quantified
- Does not explore highly stochastic environments or scenarios with extremely sparse rewards
- Missing analysis of interactions with other RLHF techniques like KL regularization

## Confidence
- **High confidence**: The core claim that token-level PPO suffers from high variance in multi-turn settings due to reward sparsity and chain-like dependencies
- **Medium confidence**: The assertion that turn-PPO consistently outperforms GRPO in training stability and final reward
- **Low confidence**: The claim that turn-PPO is universally superior to all existing multi-turn RL methods

## Next Checks
1. Evaluate turn-PPO on a broader set of multi-turn reasoning tasks with varying reward structures (sparse, delayed, stochastic)
2. Conduct ablation studies to isolate the impact of turn-level advantage estimation versus other algorithm components
3. Measure and compare computational overhead and wall-clock training time between turn-PPO and GRPO for scalability assessment