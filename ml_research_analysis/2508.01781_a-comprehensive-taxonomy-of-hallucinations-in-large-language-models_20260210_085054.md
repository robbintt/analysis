---
ver: rpa2
title: A comprehensive taxonomy of hallucinations in Large Language Models
arxiv_id: '2508.01781'
source_url: https://arxiv.org/abs/2508.01781
tags:
- hallucination
- hallucinations
- arxiv
- llms
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report provides a comprehensive taxonomy of LLM hallucinations,
  categorizing them into intrinsic (contradicting input context) and extrinsic (inconsistent
  with training data or reality) types, as well as factuality (absolute correctness)
  and faithfulness (adherence to input) errors. It details specific manifestations
  like factual errors, contextual and logical inconsistencies, temporal disorientation,
  ethical violations, and task-specific hallucinations across domains such as code
  generation and multimodal applications.
---

# A comprehensive taxonomy of hallucinations in Large Language Models

## Quick Facts
- arXiv ID: 2508.01781
- Source URL: https://arxiv.org/abs/2508.01781
- Reference count: 40
- Primary result: Comprehensive taxonomy of LLM hallucinations with theoretical framework proving inevitability and systematic mitigation approaches

## Executive Summary
This report provides a comprehensive taxonomy of LLM hallucinations, categorizing them into intrinsic (contradicting input context) and extrinsic (inconsistent with training data or reality) types, as well as factuality (absolute correctness) and faithfulness (adherence to input) errors. It details specific manifestations like factual errors, contextual and logical inconsistencies, temporal disorientation, ethical violations, and task-specific hallucinations across domains such as code generation and multimodal applications. The report analyzes underlying causes categorized into data-related issues, model-related factors, and prompt-related influences, and introduces a theoretical framework proving hallucination's inevitability in computable LLMs. It surveys evaluation benchmarks and metrics for detection, outlines mitigation strategies including architectural (e.g., retrieval-augmented generation, guardrails) and systemic approaches, and introduces web-based resources for monitoring LLM releases and performance. The report underscores that given their theoretical inevitability, future efforts must focus on robust detection, mitigation, and continuous human oversight for responsible and reliable deployment in critical applications.

## Method Summary
This is a comprehensive survey and taxonomy paper that synthesizes existing research on LLM hallucinations. The methodology involves systematic literature review across 40+ references covering hallucination benchmarks, evaluation metrics, detection approaches, and mitigation strategies. The report does not propose novel methods but rather provides a structured framework for understanding hallucination types, causes, and solutions. The theoretical framework for hallucination inevitability is based on computability theory and diagonalization arguments. Evaluation of approaches relies on existing benchmarks (TruthfulQA, HalluLens, FActScore) and metrics (FactCC, SummaC, NLI-based methods).

## Key Results
- Hallucination is theoretically inevitable for computable LLMs due to computability limitations
- Auto-regressive token prediction inherently prioritizes statistical plausibility over factual correctness
- Retrieval-Augmented Generation reduces but does not eliminate hallucinations
- A comprehensive taxonomy distinguishes intrinsic vs extrinsic and factuality vs faithfulness errors
- Web-based monitoring resources provide ongoing performance tracking across multiple leaderboards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hallucination is theoretically inevitable for any computable LLM operating on input-output pairs
- Mechanism: Diagonalization proofs from computability theory show that for any computably enumerable set of LLMs, there exists a computable ground truth function that causes all models to hallucinate on infinitely many inputs
- Core assumption: LLMs are computable functions trained on finite input-output samples; the "formal world" contains ground truth functions that may lie outside the training distribution
- Evidence anchors:
  - [abstract]: "a theoretical framework that posits its inherent inevitability in computable LLMs, irrespective of architecture or training"
  - [section 2.2]: Theorems 1-3 and Corollary 1 provide formal proofs; "all computable LLMs inherently lack the capacity to prevent themselves from hallucinating"
  - [corpus]: "Hallucination is Inevitable for LLMs with the Open World Assumption" (arXiv:2510.05116) corroborates the theoretical inevitability result
- Break condition: Assumption: the formal framework models real-world LLM behavior accurately; if LLMs can access external verification or human oversight at inference time, the inevitability may not translate to deployed system failures

### Mechanism 2
- Claim: Auto-regressive token prediction prioritizes statistical plausibility over factual correctness
- Mechanism: LLMs generate text by sequentially predicting the most probable next token given preceding context; factual accuracy is only indirectly encoded via training data patterns, not as an explicit objective
- Core assumption: Training data contains noise, biases, and gaps; the model cannot distinguish between statistically likely and factually true continuations
- Evidence anchors:
  - [abstract]: "generating plausible but factually incorrect or fabricated content"
  - [section 5.2.1]: "Factual accuracy is not the direct, explicit goal of this process; rather, accuracy is inferred from a high probability of adequate token prediction"
  - [corpus]: "Large Language Models Hallucination: A Comprehensive Survey" (arXiv:2510.06265) confirms auto-regressive generation as a root cause
- Break condition: If decoding strategies or training objectives explicitly optimize for factual consistency (e.g., reinforcement learning from fact-checked data), this mechanism weakens

### Mechanism 3
- Claim: Retrieval-Augmented Generation (RAG) reduces but does not eliminate hallucinations by constraining generation to retrieved evidence
- Mechanism: External retrieval provides grounding documents that the LLM uses as additional context; the model generates responses conditioned on both retrieved content and its parametric knowledge, reducing reliance on potentially incorrect internal representations
- Core assumption: Retrieved documents are relevant, accurate, and correctly interpreted by the model; the LLM can faithfully synthesize from retrieved context
- Evidence anchors:
  - [abstract]: "mitigation strategies including architectural (e.g., retrieval-augmented generation)"
  - [section 8.1.2]: "RAG reduces the likelihood of hallucination by constraining generation to content retrieved from external knowledge bases"
  - [corpus]: "Attribution Techniques for Mitigating Hallucinated Information in RAG Systems" (arXiv:2601.19927) discusses RAG limitations; weak corpus evidence on complete elimination—RAG is described as reduction, not cure
- Break condition: If retrieval fails (irrelevant or missing documents), the model reverts to parametric generation; if the model misinterprets retrieved content, hallucinations persist

## Foundational Learning

- Concept: **Computability and diagonalization arguments**
  - Why needed here: To understand the theoretical inevitability proof; diagonalization shows that no computable model can capture all computable ground truth functions
  - Quick check question: Can you explain why the set of all computable functions is larger than any computably enumerable set of LLMs?

- Concept: **Auto-regressive language modeling**
  - Why needed here: Core architectural cause of hallucination; explains why models generate plausible but false content
  - Quick check question: What objective does an auto-regressive LLM optimize during generation, and how does this differ from factual accuracy?

- Concept: **Evaluation metrics for faithfulness vs. factuality**
  - Why needed here: Different hallucination types require different detection approaches; faithfulness metrics (e.g., FactCC, SummaC) check input consistency, while factuality metrics (e.g., FActScore, KILT) verify against external knowledge
  - Quick check question: For a summarization task, which metric type would detect a fabricated claim not present in the source document?

## Architecture Onboarding

- Component map:
  Taxonomy layer: Intrinsic vs. extrinsic; factuality vs. faithfulness
  Detection layer: Benchmarks (TruthfulQA, HalluLens, FActScore) + metrics (ROUGE/BLEU limitations, NLI-based)
  Mitigation layer: Architectural (RAG, tool augmentation, fine-tuning) + systemic (guardrails, human-in-the-loop)
  Monitoring layer: Leaderboards (Vectara, LM Arena, Artificial Analysis) for ongoing performance tracking

- Critical path:
  1. Identify hallucination type via taxonomy (intrinsic/extrinsic, factuality/faithfulness)
  2. Select appropriate detection benchmark/metric for the task domain
  3. Apply layered mitigation: retrieval grounding + guardrails + human oversight
  4. Monitor via leaderboards; iterate based on failure patterns

- Design tradeoffs:
  - RAG adds latency and infrastructure complexity but improves factual grounding
  - Stricter guardrails reduce hallucination risk but may over-constrain useful generation
  - Human-in-the-loop provides highest reliability but does not scale

- Failure signatures:
  - Intrinsic hallucinations: Output contradicts provided context → suggests attention/reasoning failures
  - Extrinsic hallucinations: Output fabricates entities/events → suggests knowledge boundary issues
  - Temporal disorientation: Outdated facts → indicates stale training data
  - Logical inconsistencies: Self-contradictory statements → points to reasoning limitations

- First 3 experiments:
  1. Run a baseline model on TruthfulQA and HalluLens; categorize errors by taxonomy type to identify dominant failure modes
  2. Implement RAG with a curated knowledge base for a high-stakes domain (e.g., medical QA); measure hallucination reduction via FActScore
  3. Add rule-based guardrails (e.g., logic validators for arithmetic, factual filters against a knowledge graph) and compare failure rates on a held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the research community develop a standardized, taxonomy-aware evaluation framework that enables comparable assessments of hallucination rates across diverse models and tasks?
- Basis in paper: [explicit] The paper notes in Section 7.3.1 that the "absence of a universally accepted definition" hinders cumulative discourse, and Section 7.4 explicitly calls for "unified, taxonomy-aware... frameworks" to resolve current fragmentation
- Why unresolved: Current benchmarks and metrics are highly task-dependent (Section 7.3.2) and rely on inconsistent definitions, making fair, cross-model comparisons scientifically difficult
- What evidence would resolve it: The adoption of a benchmark that consistently categorizes errors (e.g., intrinsic vs. extrinsic) across multiple domains (code, medical, general) with high inter-rater reliability

### Open Question 2
- Question: What methodologies are required to detect "subtle hallucinations"—such as low-level factual shifts or inferential errors—that are missed by current surface-level metrics?
- Basis in paper: [explicit] Section 7.3.3 states that current metrics are "insensitive to subtle hallucinations," including slight numerical alterations or context-dependent misalignments
- Why unresolved: Detecting these errors requires deep semantic understanding and complex logical reasoning capabilities that automated metrics currently lack
- What evidence would resolve it: The development of automated evaluation tools that successfully identify inferential errors with a level of accuracy comparable to human expert annotation

### Open Question 3
- Question: How can hybrid mitigation systems be architected to dynamically balance factual grounding against fluency and creativity based on specific application contexts?
- Basis in paper: [explicit] Section 8.3 suggests future systems must be "context-aware," enforcing strict grounding for high-stakes domains like medicine while allowing relaxed constraints for creative tasks
- Why unresolved: Existing mitigation strategies often apply "one-size-fits-all" constraints, lacking the architectural nuance to adapt to the varying needs of different domains in real-time
- What evidence would resolve it: A system architecture that successfully modulates hallucination rates in real-time (e.g., high factuality in medical mode, higher novelty in creative mode) without manual reconfiguration

### Open Question 4
- Question: To what extent can user interface designs (e.g., uncertainty displays, source-grounding indicators) effectively counteract cognitive biases like automation bias and the persistence of over-trust?
- Basis in paper: [inferred] Section 6.3.4 notes that "warnings are often insufficient" to prevent over-reliance, while Section 6.4 proposes design strategies, implying their relative efficacy remains unproven
- Why unresolved: It is unclear if "factuality-aware" interfaces genuinely prevent users from accepting hallucinated content or if they merely provide a false sense of security
- What evidence would resolve it: User studies demonstrating a statistically significant reduction in the acceptance of hallucinated content when using specific interface prototypes compared to standard chat interfaces

## Limitations
- Theoretical framework relies on abstract computability assumptions that may not fully capture practical LLM behavior
- Taxonomy classification can be ambiguous at category boundaries (e.g., intrinsic vs faithfulness errors)
- Most mitigation strategies are presented conceptually without empirical validation results
- Survey coverage may not capture the most recent advances in hallucination detection and prevention

## Confidence
- **High**: The taxonomy structure (intrinsic/extrinsic, factuality/faithfulness) and basic mechanism of auto-regressive generation are well-established
- **Medium**: The theoretical inevitability proof and most architectural mitigation approaches have supporting literature but limited empirical validation in real-world deployments
- **Low**: Specific implementation details for hybrid mitigation systems and precise evaluation protocols for the taxonomy classification

## Next Checks
1. **Empirical validation of inevitability**: Test whether state-of-the-art LLMs with RAG and guardrails still hallucinate on carefully constructed diagonalization-style prompts that probe knowledge boundaries
2. **Taxonomy application validation**: Apply the classification framework to a blind test set of LLM outputs and measure inter-annotator agreement to establish reliability of the taxonomy categories
3. **Mitigation effectiveness comparison**: Implement a standardized pipeline (RAG + fine-tuning + guardrails) and measure hallucination reduction across multiple benchmarks (TruthfulQA, HalluLens, FActScore) versus baseline models