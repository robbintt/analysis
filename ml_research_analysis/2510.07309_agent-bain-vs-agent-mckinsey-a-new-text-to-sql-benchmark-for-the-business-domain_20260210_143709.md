---
ver: rpa2
title: 'Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business
  Domain'
arxiv_id: '2510.07309'
source_url: https://arxiv.org/abs/2510.07309
tags:
- type
- data
- business
- evaluation
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The CORGI benchmark addresses the gap in text-to-SQL evaluation
  by introducing realistic business databases and complex question types (descriptive,
  explanatory, predictive, and recommendational) that go beyond simple past data retrieval.
  It uses simulated databases from industries like DoorDash and Airbnb, along with
  a multi-agent evaluation framework that incorporates business literature-inspired
  criteria.
---

# Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain

## Quick Facts
- **arXiv ID:** 2510.07309
- **Source URL:** https://arxiv.org/abs/2510.07309
- **Reference count:** 40
- **Primary result:** CORGI benchmark shows 33.12% lower success execution rate on LLMs compared to existing benchmarks, with performance degrading as question complexity increases from descriptive to recommendational queries.

## Executive Summary
CORGI addresses the gap in text-to-SQL evaluation by introducing realistic business databases and complex question types that go beyond simple past data retrieval. The benchmark uses simulated databases from industries like DoorDash and Airbnb, along with a multi-agent evaluation framework that incorporates business literature-inspired criteria. Experiments demonstrate that leading LLMs struggle significantly with higher-order business intelligence queries, with execution success rates dropping from 79.47% for simple descriptive queries to 47.33% for complex recommendational questions.

## Method Summary
CORGI is a text-to-SQL benchmark featuring 10 SQLite databases with realistic business schemas and 1,200 questions across four complexity types. An LLM generates SQL queries (up to 5 for complex types) which are executed against the database, with results synthesized into final answers. A multi-agent evaluation framework routes responses to specialized scoring agents (Structure, Data Sense, Insightfulness, Operational Implementability, Purpose Alignment, Compliance) using a discriminator. The benchmark uses simulation rules including operational constraints, latent feature distributions, and seasonal trends to create realistic business data.

## Key Results
- LLMs exhibit 33.12% lower success execution rate (SER) on CORGI compared to existing benchmarks like BIRD
- Performance degrades monotonically from Type 1 (79.47% SER) to Type 4 (47.33% SER) questions
- Type 2-4 questions require 2.31 queries per question on average versus 1.0 for Type 1, with significantly more JOINs and subqueries
- Human-LLM evaluator agreement shows moderate correlation (0.31-0.71 Kendall's τ) that decreases with question complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing question complexity causes monotonic degradation in LLM SQL execution success rates.
- Mechanism: Higher-order query types require multi-step reasoning chains where each step compounds error probability; Type 2-4 questions average 2.31 queries per question versus 1.0 for Type 1, with more JOINs (7.48 vs. 0.93) and subqueries.
- Core assumption: Failure propagates linearly through reasoning pipelines; a single SQL execution failure invalidates downstream synthesis.
- Evidence anchors:
  - [abstract] "LLMs exhibit an average 33.12% lower success execution rate (SER) on CORGI compared to existing benchmarks such as BIRD, with performance degrading as question complexity increases."
  - [section 6.2, Table 4] "We observe an overall decline in average SER as task difficulty increases, decreasing from 79.47% for type 1 to 59.16% for type 2, 58.14% for type 3, and 47.33% for type 4."
  - [corpus] Related benchmarks (LogicCat, PRACTIQ) similarly report reasoning-chain challenges, though CORGI uniquely isolates business complexity dimensions.

### Mechanism 2
- Claim: Specialized scoring agents evaluating distinct dimensions improve alignment between automatic and human judgments for qualitative business responses.
- Mechanism: The discriminator routes responses to dimension-specific agents rather than monolithic scoring; this atomization mirrors human committee-based assessment where diverse perspectives improve reliability.
- Core assumption: Human evaluators implicitly decompose business judgment into separable rubric dimensions; LLMs better approximate this when explicitly prompted per dimension.
- Evidence anchors:
  - [abstract] "CORGI also introduces and encourages the text-to-SQL community to consider new automatic methods for evaluating open-ended, qualitative responses in data access tasks."
  - [section 4.3] "By allowing agents to specialize in distinct evaluation dimensions, this architecture mirrors human committee-based assessment, where diverse perspectives improve reliability and validity."
  - [section 4.4, Table 3] "Overall, the agreement is moderate and decreases with question complexity, from 0.52-0.71 (type 2) to 0.43-0.64 (type 3) and 0.31-0.57 (type 4), reflecting the increased difficulty of evaluating qualitative, judgment-intensive responses."

### Mechanism 3
- Claim: Business-logic-driven database simulation creates evaluation distributions that better approximate real-world performance gaps.
- Mechanism: Three rule types—deterministic operational constraints, latent feature distributions, and seasonal fluctuations—generate synthetic data that exposes LLM weaknesses on domain-aware reasoning absent in generic benchmarks.
- Core assumption: Business realism requires not just schema complexity but underlying generative processes that produce plausible temporal and demographic patterns.
- Evidence anchors:
  - [section 3.1] "We generate data using three types of rules: (i) business operational constraints... (ii) latent feature distributions... (iii) seasonal trends and fluctuations."
  - [section 3.1, Figure 2] Illustrates Persona Nutrition simulation rules including customer type distribution (35% trial, 44% regular, 17% loyal, 4% VIP) and seasonal trends.

## Foundational Learning

- Concept: **Text-to-SQL translation vs. business intelligence reasoning**
  - Why needed here: CORGI reframes text-to-SQL from pure translation (Type 1: "How many Labubu were sold?") to multi-stage BI workflows (Type 4: "How can we expand the Labubu market in Europe?"); understanding this distinction is prerequisite to interpreting benchmark results.
  - Quick check question: Given a query "Why did Q3 revenue decline?", can you distinguish which data needs retrieval vs. what reasoning must follow retrieval?

- Concept: **LLM-as-evaluator alignment and limitations**
  - Why needed here: CORGI's multi-agent evaluation replaces gold-label matching with qualitative scoring; you must understand when LLM evaluators align with humans (moderate τ=0.52-0.71 for Type 2) versus when they diverge (τ=0.31-0.57 for Type 4).
  - Quick check question: Why might an LLM evaluator systematically differ from human experts on recommendational questions versus descriptive ones?

- Concept: **Success Execution Rate (SER) vs. Execution Accuracy (EA)**
  - Why needed here: CORGI reports SER across all types (query must execute without error) and EA only for Type 1 (results must match gold labels); Type 2-4 evaluation combines SER with qualitative scores since multiple valid responses exist.
  - Quick check question: If a Type 3 query executes successfully but uses a suboptimal forecasting model, would it fail SER or the evaluation framework's Model Selection Rationality metric?

## Architecture Onboarding

- **Component map**: Database layer -> Question generation layer -> Answer synthesis layer -> Evaluation layer (Discriminator -> 6 Scoring Agents)
- **Critical path**:
  1. Start with Type 1 descriptive queries on a single database (e.g., AppStore) to establish SQL generation baseline
  2. Progress to Type 2 explanatory questions requiring multi-query reasoning; examine why SER drops 20+ percentage points
  3. Implement the multi-agent evaluator on held-out responses; compare your scores against Table 5 baselines
  4. Only then attempt Type 3-4 questions where forecasting and strategic reasoning compound complexity

- **Design tradeoffs**:
  - Single-turn vs. multi-turn: CORGI uses single-turn queries; real business users iterate through clarifications
  - SQLite only: Limits dialect-specific function evaluation (e.g., no PostGIS geospatial queries)
  - Independent simulation rules: Simplifies data generation but omits interaction effects between operational constraints and seasonal trends
  - LLM evaluator dependency: While faster than human annotation, introduces model-specific biases

- **Failure signatures**:
  - Low SER with high answer scores: Suggests answer synthesis fakes reasoning without proper data retrieval
  - High SER with low evaluation scores: SQL executes but response lacks business insight
  - Type 4 Compliance failures: Recommendations ignore risk/ethics
  - Evaluator-generator mismatch: If using different LLMs for generation vs. evaluation, verify no systematic bias

- **First 3 experiments**:
  1. **Baseline SER calibration**: Run GPT-4o or equivalent on all 4 question types for AppStore database; plot SER degradation curve; compare against Table 4 overall averages (79.47% → 47.33%)
  2. **Single-agent vs. multi-agent evaluation**: For 20 Type 3 responses, compare scores from: (a) single GPT-4o prompt with all 6 rubrics, vs. (b) discriminator + 6 specialized agents; measure score variance and Kendall's τ against human annotations if available
  3. **Ablation on query budget**: Restrict Type 2-4 questions to 1, 2, 3, 5 maximum queries; measure how SER and evaluation scores change to understand whether failure stems from insufficient retrieval or flawed synthesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM performance change when extending the benchmark from single-turn queries to multi-turn conversational refinement?
- Basis in paper: [explicit] The limitations section states, "CORGI focuses on single-turn queries, leaving out multi-turn conversational refinement that is common in real-world systems."
- Why unresolved: The current benchmark design restricts evaluation to isolated inputs, ignoring the dialogue history and context retention required for follow-up questions.
- What evidence would resolve it: A study introducing multi-turn question pairs and measuring the Success Execution Rate (SER) degradation relative to single-turn baselines.

### Open Question 2
- Question: How robust are current models against linguistic variations and informal inputs?
- Basis in paper: [explicit] The paper notes that "each query is paired with only one natural language formulation, limiting the evaluation of model robustness to paraphrasing or informal inputs."
- Why unresolved: Current evaluation tests only one specific phrasing per query, so performance variance caused by synonyms, ambiguity, or user-specific jargon remains unknown.
- What evidence would resolve it: Experiments augmenting the dataset with diverse paraphrases and informal variations to measure variance in Execution Accuracy and qualitative evaluation scores.

### Open Question 3
- Question: Can the consistency between automatic LLM evaluation and human judgment be improved for complex, open-ended recommendation tasks?
- Basis in paper: [inferred] While the paper validates the LLM evaluator, results show human-LLM agreement (Kendall's τ) drops significantly for Type 4 (recommendational) questions (0.31-0.57) compared to Type 2.
- Why unresolved: The multi-agent framework struggles to consistently assess qualitative "strategic" value, leading to potential misalignment with human consultant standards.
- What evidence would resolve it: A comparison of new evaluation protocols (e.g., fine-tuned critic models) against human annotations on the Type 4 subset to achieve higher correlation coefficients.

## Limitations

- The paper's performance metrics rely on unreleased or hypothetical model versions (GPT-5, Llama 4), making exact replication challenging
- The multi-agent evaluation framework introduces evaluator-specific biases - while the paper finds no clear self-preference bias, alignment with human judgments decreases significantly for complex question types
- The simulation rules assume independent application of operational constraints, latent features, and seasonal trends, potentially underestimating real-world interaction effects between these dimensions

## Confidence

- **High Confidence**: CORGI introduces a novel benchmark with realistic business databases and demonstrates that LLM performance degrades with increasing question complexity (Type 1 SER: 79.47% → Type 4 SER: 47.33%). The methodology for generating simulated business data is clearly specified.
- **Medium Confidence**: The multi-agent evaluation framework improves alignment with human judgments compared to monolithic scoring, though correlation remains moderate (0.31-0.71). The framework's design is well-justified but dependent on LLM evaluator quality.
- **Low Confidence**: Direct performance comparisons with other benchmarks (BIRD, PRACTIQ) are difficult to verify without access to the exact model versions used. The 33.12% performance gap may vary with different evaluation models.

## Next Checks

1. **Baseline Replication**: Implement Algorithm 1 on AppStore database with accessible models (e.g., GPT-4o, Gemini-1.5-Pro) to establish SER baseline curve and verify the reported degradation pattern
2. **Evaluator Consistency Test**: Compare single-agent vs. multi-agent evaluation on 20 held-out responses to measure score variance and Kendall's τ correlation with human annotations
3. **Query Budget Ablation**: Systematically vary maximum query limits (1-5) for Types 2-4 to isolate whether failure stems from insufficient data retrieval or flawed synthesis logic