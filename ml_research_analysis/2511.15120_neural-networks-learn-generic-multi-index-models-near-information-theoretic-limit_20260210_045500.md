---
ver: rpa2
title: Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic
  Limit
arxiv_id: '2511.15120'
source_url: https://arxiv.org/abs/2511.15120
tags:
- lemma
- logd
- proof
- have
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how neural networks can efficiently learn low-dimensional
  hidden features from high-dimensional data, focusing on multi-index models where
  outputs depend only on a small number of linear combinations of inputs. The core
  insight is that standard two-layer neural networks trained with layer-wise gradient
  descent can perform a power iteration process on the data's second-order statistics.
---

# Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit
## Quick Facts
- arXiv ID: 2511.15120
- Source URL: https://arxiv.org/abs/2511.15120
- Reference count: 40
- Two-layer neural networks can learn low-dimensional hidden features from high-dimensional data with optimal sample complexity

## Executive Summary
This paper demonstrates that standard two-layer neural networks can efficiently learn low-dimensional hidden features from high-dimensional data using a carefully timed training schedule. The key insight is that gradient descent implicitly performs power iteration on data statistics, recovering the hidden subspace when the first layer is trained for an optimal number of steps (Θ(√log d)). Under generic assumptions, this approach achieves information-theoretic optimal sample complexity of eO(d) and time complexity of eO(d²), significantly improving upon prior work.

## Method Summary
The method leverages layer-wise gradient descent on two-layer neural networks, where the first layer is trained for a specific duration before the second layer is introduced. This training schedule exploits the implicit bias of gradient descent to perform power iteration on the second-order statistics of the data, effectively recovering the low-dimensional hidden subspace. The activation function and target function are assumed to be generic, and the network width must be sufficiently large. The analysis shows that after the first training stage, the learned features span the hidden subspace while eliminating noise, enabling efficient learning in the second stage.

## Key Results
- Two-layer networks achieve optimal sample complexity eO(d) for learning multi-index models
- Training the first layer for Θ(√log d) steps balances signal recovery and noise suppression
- The method matches information-theoretic limits with time complexity eO(d²)
- The learned features after first stage span the hidden subspace while eliminating noise

## Why This Works (Mechanism)
The mechanism relies on gradient descent implicitly performing power iteration on the data's second-order statistics. During the first training stage, the network's weights evolve to capture the dominant directions in the data covariance structure. By stopping the first layer training at the optimal time (Θ(√log d) steps), the network balances between recovering the true hidden subspace and suppressing noise. This creates a feature representation that spans the target subspace while filtering out irrelevant dimensions, enabling efficient learning in the subsequent stage.

## Foundational Learning
- **Power iteration**: Iterative method for finding dominant eigenvectors; needed for understanding how gradient descent recovers hidden subspaces
- **Information-theoretic limits**: Fundamental bounds on learning complexity; needed to benchmark the achieved sample complexity
- **Generic assumptions**: Conditions ensuring properties hold with high probability; needed for probabilistic analysis of network behavior
- **Layer-wise training**: Sequential training of network layers; needed to exploit temporal dynamics of gradient descent
- **Multi-index models**: Models depending on linear combinations of inputs; needed as the target learning problem
- **Implicit bias of gradient descent**: Tendency of GD to find specific solutions; needed to explain why the network learns the right features

## Architecture Onboarding
**Component Map**: Input data -> First layer (trained Θ(√log d) steps) -> Second layer (full training) -> Output prediction
**Critical Path**: Data → First layer feature extraction → Second layer target learning
**Design Tradeoffs**: Network width vs. training efficiency; training duration vs. noise suppression
**Failure Signatures**: Under-training leads to poor feature recovery; over-training causes noise amplification
**First Experiments**:
1. Verify power iteration behavior by tracking first layer weight evolution
2. Test optimal training duration across different activation functions
3. Measure feature recovery quality as function of training steps

## Open Questions the Paper Calls Out
None

## Limitations
- Results rely on specific assumptions about activation and target functions being generic
- Requires precise timing control for optimal training schedule
- Analysis limited to two-layer networks with specific architectural constraints
- Theoretical bounds assume exact gradient descent rather than practical variants

## Confidence
**High Confidence**: Sample complexity bound Õ(d) and power iteration interpretation are rigorously proven
**Medium Confidence**: Optimality claims hold under specific assumptions but may not extend to all practical settings
**Low Confidence**: Practical applicability to deep networks and robustness to various noise forms remain uncertain

## Next Checks
1. Empirically validate the optimal training schedule (Θ(√log d) steps) across different activation functions and data distributions
2. Investigate how results extend to deeper networks and practical SGD variants, particularly examining power iteration validity
3. Test feature recovery robustness to various noise forms and non-standard initialization schemes beyond random Gaussian initialization