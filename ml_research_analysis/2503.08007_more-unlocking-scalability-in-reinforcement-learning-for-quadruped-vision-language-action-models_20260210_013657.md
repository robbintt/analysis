---
ver: rpa2
title: 'MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action
  Models'
arxiv_id: '2503.08007'
source_url: https://arxiv.org/abs/2503.08007
tags:
- data
- more
- tasks
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoRE introduces a scalable reinforcement learning framework for
  quadruped VLA models by integrating a mixture-of-experts architecture with RL-based
  training objectives. The method fine-tunes a pre-trained MLLM backbone with LoRA
  experts to enable task-specific specialization while maintaining computational efficiency.
---

# MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models

## Quick Facts
- **arXiv ID:** 2503.08007
- **Source URL:** https://arxiv.org/abs/2503.08007
- **Reference count:** 40
- **Primary result:** MoRE achieves 60% average success rate across 6 tasks, outperforming baselines by up to 20%

## Executive Summary
MoRE introduces a scalable reinforcement learning framework for quadruped Vision-Language-Action (VLA) models by integrating a mixture-of-experts architecture with RL-based training objectives. The method fine-tunes a pre-trained MLLM backbone with LoRA experts to enable task-specific specialization while maintaining computational efficiency. By training as a Q-function on mixed-quality data (expert and sub-optimal trajectories), MoRE leverages offline RL to extract robust policies from automatically collected data. Extensive experiments demonstrate superior performance across six skills with a 60% average success rate, outperforming baselines by up to 20%. The model shows strong generalization in out-of-distribution scenarios and real-world deployment on a Unitree Go2 robot.

## Method Summary
MoRE fine-tunes a pre-trained Fuyu 8B backbone with MixLoRA experts to create a multi-task VLA model for quadruped robots. The architecture uses sparse-activated mixture-of-experts with LoRA adapters for task-specific specialization, routing tokens via Top-K selection. Training occurs as a Q-function using offline RL on mixed-quality data from QUARD (expert) and QUARD-Auto (sub-optimal) datasets. The model outputs discretized 12-dimensional robot commands through autoregressive action tokenization. Key innovations include task-specific expert routing, RL-based Q-function training, and effective utilization of mixed-quality data for enhanced data efficiency and performance.

## Key Results
- 60% average success rate across 6 tasks, outperforming baselines by up to 20%
- Strong generalization in out-of-distribution scenarios
- Successful real-world deployment on Unitree Go2 robot
- Effective utilization of mixed-quality data for enhanced data efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sparse-activated mixture-of-experts with LoRA adapters enables multi-task specialization without catastrophic interference.
- **Mechanism:** Each expert (LoRA module) learns task-specific adaptations while sharing the frozen MLLM backbone. A learnable router routes tokens to relevant experts via Top-K selection, allowing different skills (locomotion, navigation, manipulation) to develop specialized pathways.
- **Core assumption:** Task-relevant tokens can be distinguished by the router; expert capacity is sufficient for skill diversity.
- **Evidence anchors:** [abstract] "integrates multiple low-rank adaptation modules as distinct experts... forming a sparse-activated mixture-of-experts model"; [Section III-B] "each expert in the MoE layer contains an FFN with parameters shared among experts, and LoRA adapters are applied to each linear layer"
- **Break condition:** If tasks require shared low-level representations that conflict across experts, routing may fail to isolate skills; router collapse (all tokens to one expert) would degrade specialization.

### Mechanism 2
- **Claim:** Offline RL training as a Q-function extracts better policies from mixed-quality data than imitation learning.
- **Mechanism:** The model outputs Q-values (via sigmoid-constrained logits) rather than action probabilities. The Bellman backup (Eq. 7) propagates sparse success rewards backward through trajectories, enabling learning from both successful and failed demonstrations. Conservative regularization prevents overestimation on out-of-distribution actions.
- **Core assumption:** Task structure has "limited critical points"—most states allow recovery, so sub-optimal trajectories contain learnable signal.
- **Evidence anchors:** [abstract] "employ a reinforcement learning-based training objective to train our model as a Q-function"; [Section III-C] "offline RL algorithms can rapidly converge to good-enough actions in non-critical states, a capability that IL methods lack"
- **Break condition:** If reward signal is too sparse or critical states are frequent, Q-learning may fail to propagate meaningful gradients; excessive conservatism may underutilize expert data.

### Mechanism 3
- **Claim:** Task structural properties (sparse rewards, recoverable states) make offline RL preferable to IL for quadruped VLA.
- **Mechanism:** The identified properties—horizon-independent returns, limited critical points, long horizons, distribution shift—mean that most trajectory deviations are non-fatal. RL can identify "good-enough" actions via reward information, while IL只能mimic the data distribution without distinguishing quality.
- **Core assumption:** Assumption: quadruped tasks genuinely exhibit this structure; not all robotic tasks do.
- **Evidence anchors:** [Section III-C] Explicit structural analysis with 4 properties identified; [Table II] Ablation shows w/o RL drops from 60% to 51% average success
- **Break condition:** Tasks with dense critical states (e.g., precise manipulation requiring consistent accuracy) may not benefit; IL with filtered expert data could outperform.

## Foundational Learning

- **Concept: Offline Reinforcement Learning / Conservative Q-Learning**
  - **Why needed here:** MoRE trains on fixed datasets with mixed-quality trajectories; understanding Bellman backups, bootstrapping, and conservative regularization is essential to debug training instability.
  - **Quick check question:** Can you explain why Q-learning on offline data can overestimate values for unseen actions, and how conservative regularization addresses this?

- **Concept: Mixture-of-Experts Routing and Load Balancing**
  - **Why needed here:** The MoE architecture's router determines which experts activate; load balancing loss (Eq. 9) prevents expert collapse.
  - **Quick check question:** If 90% of tokens route to a single expert, what symptom would you observe, and which loss term addresses it?

- **Concept: Autoregressive Action Tokenization**
  - **Why needed here:** Actions are discretized into tokens and predicted auto-regressively; the Q-function decomposes across dimensions (Eq. 7).
  - **Quick check question:** How does the Q-value computation differ between the first action token and the final one in the sequence?

## Architecture Onboarding

- **Component map:** RGB Image + Text Instruction → Tokenization (image tokens, text tokens) → Fuyu-8B Backbone (32 decoder layers, frozen) → Self-Attention + LoRA adapter (per layer, shared) → MoE Layer (replaces FFN) → Shared FFN weights + N × LoRA experts (routed via Top-K) → Action Tokens (12-dim discretized commands) → De-tokenization → Robot Commands

- **Critical path:** Router → Expert selection → LoRA computation → Q-value output. If routing fails or experts don't specialize, multi-task performance degrades.

- **Design tradeoffs:**
  - More experts → better specialization but higher memory and routing complexity
  - Higher K (activated experts) → more computation but richer representations
  - α (conservative term) → higher values reduce overestimation but may underutilize expert data

- **Failure signatures:**
  - Router collapse: One expert dominates (>80% routing); check load balancing loss
  - Q-value explosion: Training unstable, Q-values >1; check α regularization
  - Poor sim-to-real transfer: Success in simulation but not real world; may need more real-world fine-tuning data

- **First 3 experiments:**
  1. **Validate expert specialization:** Train with 2-3 experts, visualize routing distributions per task. Expect clear task-expert correlation; if not, reduce K or increase expert count.
  2. **Ablate RL vs IL on same data:** Train identical architecture with cross-entropy loss (IL) vs Q-learning loss. Expect 5-10% gap on mixed-quality data per Table II.
  3. **Test critical point hypothesis:** Manually label "critical states" in validation trajectories; analyze whether Q-values peak at these states. If not, reward shaping or architecture may need adjustment.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the MoRE framework effectively handle tasks with horizon-dependent returns (e.g., speed constraints or energy efficiency) given its current reliance on sparse, terminal rewards?
- **Basis in paper:** [inferred] Section III-C explicitly identifies "Horizon-independent returns" as a structural property of the MDP leveraged for the Q-function training.
- **Why unresolved:** The current formulation appears optimized for binary success/failure outcomes; it is unclear if the Bellman operator adaptation can simultaneously optimize for continuous costs like time or energy without destabilizing the expert routing.
- **Evidence:** Evaluation results on navigation tasks where the success metric is contingent on completing the task within a specific time limit, demonstrating optimization beyond simple task completion.

### Open Question 2
- **Question:** What is the zero-shot transfer performance of MoRE from simulation to the real world without the fine-tuning step?
- **Basis in paper:** [inferred] Section IV-D states the model was "further fine-tuned on a small real-world dataset" before deployment, despite claims of strong generalization from simulation data.
- **Why unresolved:** The necessity of a real-world fine-tuning stage obscures the true magnitude of the sim-to-real gap and the standalone robustness of the simulation-trained policy.
- **Evidence:** Real-world ablation results comparing the success rate of the simulation-only checkpoint against the fine-tuned model on the Unitree Go2 robot.

### Open Question 3
- **Question:** How can the training objective be modified to prevent performance degradation in tasks where rewards are overly sparse or critical points are ill-defined?
- **Basis in paper:** [explicit] Section IV-C notes that "overly sparse reward function introduces learning challenges, which leads to performance degradation in 'Go avoid' task."
- **Why unresolved:** The current method relies on identifying "critical points" to learn robust policies, a mechanism that appears to fail when the reward signal does not clearly delineate these points.
- **Evidence:** An ablation study introducing auxiliary rewards or intrinsic motivation to the "Go avoid" task, demonstrating recovery of performance without manual reward shaping.

## Limitations
- Performance degradation on tasks with overly sparse rewards or ill-defined critical points
- Unknown zero-shot sim-to-real transfer performance due to required fine-tuning
- Unclear handling of horizon-dependent return optimization (speed, energy efficiency)

## Confidence
- **Method validity:** High - Clear architectural description and ablation studies support key claims
- **Performance claims:** Medium - Strong results but limited comparison to state-of-the-art VLA methods
- **Generalization claims:** Low - Real-world evaluation relies on fine-tuning; zero-shot transfer not demonstrated

## Next Checks
1. **Verify expert specialization:** Train with 2-3 experts, visualize routing distributions per task. Expect clear task-expert correlation; if not, reduce K or increase expert count.
2. **Ablate RL vs IL:** Train identical architecture with cross-entropy loss (IL) vs Q-learning loss. Expect 5-10% gap on mixed-quality data per Table II.
3. **Test critical point hypothesis:** Manually label "critical states" in validation trajectories