---
ver: rpa2
title: 'Rethinking Large Language Model Distillation: A Constrained Markov Decision
  Process Perspective'
arxiv_id: '2509.22921'
source_url: https://arxiv.org/abs/2509.22921
tags:
- length
- distillation
- reward
- reasoning
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of knowledge distillation in
  large language models (LLMs) by formulating it as a constrained reinforcement learning
  problem. The core method idea is to maximize task-specific rewards while constraining
  the divergence from the teacher model below a predefined threshold, using a modified
  reward function that maintains theoretical guarantees without requiring state augmentation
  or teacher model access during deployment.
---

# Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective

## Quick Facts
- arXiv ID: 2509.22921
- Source URL: https://arxiv.org/abs/2509.22921
- Authors: Matthieu Zimmer; Xiaotong Ji; Tu Nguyen; Haitham Bou Ammar
- Reference count: 40
- Primary result: Formulates LLM distillation as constrained MDP with superior constraint satisfaction on mathematical reasoning tasks

## Executive Summary
This paper presents a novel approach to knowledge distillation for large language models by formulating the problem as a constrained Markov decision process (CMDP). The authors propose a method that maximizes task-specific rewards while constraining the divergence from the teacher model below a predefined threshold. This constrained formulation addresses the common issue in traditional distillation where student models drift too far from the teacher, potentially losing important knowledge. The approach introduces a modified reward function that maintains theoretical guarantees without requiring state augmentation or teacher model access during deployment.

## Method Summary
The core innovation lies in treating LLM distillation as a constrained optimization problem where the student model must maximize task performance while staying within a bounded distance from the teacher model's policy. The authors formulate this as a constrained MDP and propose a modified reward function that incorporates the constraint directly. Unlike traditional Lagrangian relaxation methods that use penalty coefficients, this approach maintains theoretical convergence guarantees while avoiding the need for teacher model access during inference. The method employs a scaling factor to balance the trade-off between task reward maximization and constraint satisfaction, enabling practical implementation without sacrificing performance.

## Key Results
- Achieves superior constraint satisfaction rates compared to soft Lagrangian relaxation baselines
- Demonstrates better reasoning quality on mathematical tasks while maintaining competitive task performance
- Shows the method can maintain theoretical guarantees without requiring state augmentation or teacher model access during deployment

## Why This Works (Mechanism)
The method works by explicitly incorporating the divergence constraint into the optimization objective through a carefully designed reward modification. This ensures that the student model remains close to the teacher's policy space while still optimizing for task performance. The key insight is that by treating distillation as a CMDP, the algorithm can provably satisfy the constraint while optimizing the primary objective, avoiding the instability issues common in Lagrangian relaxation approaches where penalty coefficients require careful tuning.

## Foundational Learning

**Constrained Markov Decision Processes (CMDP)**: Framework for sequential decision-making with constraints on expected cumulative costs. Needed for modeling the trade-off between task performance and teacher-student divergence. Quick check: Verify that the Bellman optimality equations can be extended to include constraint satisfaction.

**Knowledge Distillation**: Technique for transferring knowledge from a large teacher model to a smaller student model. Needed as the core problem being addressed. Quick check: Ensure the KL divergence or other divergence measures are properly computed between teacher and student distributions.

**Lagrangian Relaxation**: Method for converting constrained optimization problems into unconstrained ones by adding penalty terms. Needed as a baseline comparison. Quick check: Verify that the penalty coefficients in Lagrangian methods converge properly.

## Architecture Onboarding

**Component Map**: Teacher Model -> Divergence Measure -> Constraint Module -> Modified Reward Function -> Student Model

**Critical Path**: The optimization loop where the student model generates responses, the divergence from the teacher is measured, the constraint is evaluated, and the modified reward is computed for policy updates.

**Design Tradeoffs**: The method trades off computational overhead from constraint checking against the benefit of maintaining teacher-student proximity. The scaling factor introduces a hyperparameter that requires tuning but enables flexible constraint satisfaction.

**Failure Signatures**: If the scaling factor is too small, constraints may be violated; if too large, task performance may degrade. Divergence measures that are too strict may prevent the student from learning task-specific improvements.

**First Experiments**:
1. Verify constraint satisfaction on a simple synthetic task before scaling to complex mathematical reasoning
2. Test sensitivity to the scaling factor by running sweeps across different values
3. Compare constraint satisfaction rates on GSM8K with varying constraint thresholds

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses exclusively on mathematical reasoning tasks, limiting generalizability to other domains
- The method introduces additional hyperparameters (constraint threshold, scaling factor) that require careful tuning
- Theoretical framework assumes access to gradient information and smooth reward functions, which may not hold for all LLM applications

## Confidence

**High confidence**: The mathematical formulation of the constrained MDP and the derivation of the modified reward function are sound and well-justified

**Medium confidence**: The empirical results showing improved constraint satisfaction and reasoning quality are compelling but limited to a narrow task set

**Medium confidence**: The claim of maintaining competitive task performance while satisfying constraints is supported by experiments, though direct comparison to more baselines would strengthen this

## Next Checks

1. Test the method on diverse NLP tasks beyond mathematical reasoning (e.g., summarization, code generation) to assess domain generalizability

2. Conduct ablation studies to quantify the impact of different hyperparameters and the necessity of each component in the modified reward function

3. Compare performance with alternative constrained RL approaches that do not rely on Lagrangian relaxation, such as barrier methods or primal-dual optimization techniques