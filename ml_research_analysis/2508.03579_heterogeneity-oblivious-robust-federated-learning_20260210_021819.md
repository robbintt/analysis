---
ver: rpa2
title: Heterogeneity-Oblivious Robust Federated Learning
arxiv_id: '2508.03579'
source_url: https://arxiv.org/abs/2508.03579
tags:
- clients
- poisoning
- aggregation
- global
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Horus addresses the vulnerability of federated learning to poisoning
  attacks in hyper-heterogeneous environments where clients differ in data distributions,
  communication capabilities, and model architectures. To mitigate these challenges,
  Horus leverages low-rank adaptations (LoRAs) inserted into stable layers (feature-first
  and classifier) and aggregates only these LoRA updates to reduce the attack surface.
---

# Heterogeneity-Oblivious Robust Federated Learning

## Quick Facts
- arXiv ID: 2508.03579
- Source URL: https://arxiv.org/abs/2508.03579
- Authors: Weiyao Zhang; Jinyang Li; Qi Song; Miao Wang; Chungang Lin; Haitong Luo; Xuying Meng; Yujun Zhang
- Reference count: 38
- Primary result: LoRA-based aggregation with heterogeneity-oblivious detection outperforms state-of-the-art robust FL baselines under hyper-heterogeneity and poisoning attacks

## Executive Summary
Horus addresses the vulnerability of federated learning to poisoning attacks in hyper-heterogeneous environments where clients differ in data distributions, communication capabilities, and model architectures. To mitigate these challenges, Horus leverages low-rank adaptations (LoRAs) inserted into stable layers (feature-first and classifier) and aggregates only these LoRA updates to reduce the attack surface. A key empirical finding is that LoRA-A (input projection) is more stable than LoRA-B (output projection) under heterogeneity and poisoning. Based on this, Horus introduces a heterogeneity-oblivious poisoning score using LoRA-A for detection and projection-aware aggregation for robust aggregation. Experimental results demonstrate that Horus consistently outperforms state-of-the-art baselines in both robustness and accuracy across diverse datasets, attacks, and heterogeneous configurations.

## Method Summary
Horus employs LoRA modules (rank 8) inserted at feature-first and classifier layers only, freezing the backbone and exchanging only the low-rank update parameters (A_c, B_c). For poisoning detection, Horus uses HOPS - a heterogeneity-oblivious poisoning score computed purely on LoRA-A singular values via spectral entropy and top-k energy ratio. Detection uses a 95th percentile adaptive threshold. For aggregation, Horus aligns heterogeneous LoRA dimensions via masked zero-padding and reweights updates by their consistency with global principal directions (projection-guided weighting). The method trains over 200 rounds with 1 local epoch per round on non-IID data (Dirichlet α ∈ {0.10, 0.30, 0.50}) with 20% malicious clients starting attacks from round 20.

## Key Results
- Horus achieves 77.72% global accuracy on CIFAR-10 with LIE attack under hyper-heterogeneity (α=0.5), outperforming robust FL baselines by 4.05-5.29%
- On CIFAR-100 with Min-Sum attack, Horus achieves 43.15% global accuracy versus 33.96% for FedPA (best baseline)
- Horus maintains strong performance even when detection is disabled (w/o detect variant), suggesting aggregation mechanism provides inherent robustness

## Why This Works (Mechanism)

### Mechanism 1: LoRA-Based Attack Surface Reduction
Aggregating only low-rank LoRA parameters (instead of full model weights) reduces the attack surface and mitigates the curse of dimensionality in hyper-heterogeneous FL. LoRA constrains updates to a low-rank subspace (rank r ≪ min(d_in, d_out)), limiting the directions in which adversarial perturbations can hide. By freezing the backbone and only exchanging (A_c, B_c) pairs, the effective dimensionality of the aggregation space shrinks dramatically. The core assumption is that poisoning attacks rely on distributing perturbations across high-dimensional parameter spaces to evade coordinate-wise detection thresholds.

### Mechanism 2: Heterogeneity-Oblivious Poisoning Detection via LoRA-A Spectral Features
LoRA-A (input projection) provides a stable, discriminative signal for detecting poisoned clients even under hyper-heterogeneity, enabling a heterogeneity-agnostic poisoning score. HOPS fuses spectral entropy (measuring energy dispersion) and top-k energy ratio (measuring concentration). Poisoned updates tend to deviate in either direction—dispersive attacks increase entropy, directional attacks increase concentration. LoRA-A exhibits lower temporal variance across rounds and clients compared to LoRA-B, reducing false positives. The core assumption is that benign LoRA-A updates exhibit consistent spectral patterns across heterogeneous clients; poisoning perturbations disrupt these patterns measurably.

### Mechanism 3: Projection-Guided Aggregation with Dimensional Alignment
Aligning heterogeneous LoRA dimensions via masked zero-padding and reweighting by consistency with global principal directions preserves collaborative signals while suppressing poisoning drift. Zero-pad to global max dimensions with binary masks to avoid diluting valid updates. Compute SVD of LoRA matrices; project each client's first right singular vector onto the global direction from the previous round. Weight updates by projection magnitude (α_c = |⟨v_c^(1), v_g^(1)⟩|), amplifying directionally consistent contributions. The core assumption is that benign updates align with the global optimization trajectory; poisoned updates deviate in dominant directions.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: LoRA is the core parameter-efficient mechanism that enables dimension reduction, architectural flexibility, and the A/B stability analysis underlying Horus.
  - Quick check question: Given a weight matrix W ∈ R^(d_out × d_in), can you explain why ΔW = BA with rank r constrains updates to an r-dimensional subspace?

- **Concept: Singular Value Decomposition (SVD) and Spectral Analysis**
  - Why needed here: HOPS relies on computing singular values of LoRA-A to derive entropy and energy ratio metrics for poisoning detection.
  - Quick check question: If σ_1, ..., σ_r are singular values, what does a high top-k energy ratio (R_k ≈ 1) indicate about the structure of the update?

- **Concept: Byzantine-Robust Aggregation in FL**
  - Why needed here: Horus builds on prior robust FL work (Krum, Bulyan, DnC, FLDetector) and must be understood in context of their failure modes under hyper-heterogeneity.
  - Quick check question: Why do distance-based detection methods (e.g., Krum) fail when clients have fundamentally different model architectures (CNNs vs. RNNs)?

## Architecture Onboarding

- **Component map:**
  - Client-side: Local model backbone (heterogeneous CNN/RNN) → LoRA modules inserted at feature-first and classifier layers → Local training → Decouple A_c, B_c → Transmit to server
  - Server-side: Receive all (A_c, B_c) → HOPS computation (singular values → entropy + energy ratio → score) → Threshold-based filtering → Dimensional alignment (zero-pad + mask) → Projection-guided weighting (SVD → consistency weights) → Masked weighted aggregation → Broadcast aggregated (Ā, B̄)

- **Critical path:**
  1. Correct LoRA insertion at feature-first and classifier layers only (not middle layers—see ablation).
  2. Use only LoRA-A for HOPS computation; do not combine with LoRA-B.
  3. Maintain global direction vectors (v_A,g^(1), v_B,g^(1)) across rounds for projection weighting.
  4. Ensure masking correctly excludes padded entries from aggregation denominator.

- **Design tradeoffs:**
  - Rank r: r=8 balances communication/robustness; r=16 gives marginal accuracy gain (+0.35%) but 2× payload. Higher ranks (≥32) degrade robustness by expanding attack subspace.
  - HOPS λ parameter: λ=0.7 better for directional attacks (emphasizes energy ratio); λ=0.3 better for dispersive attacks (emphasizes entropy). Paper uses fixed λ; adaptive tuning is future work.
  - Layer selection: Middle layers excluded due to instability under heterogeneity (Fig. 1)—aggregating them increases noise and attack surface.

- **Failure signatures:**
  - High false positive rate in detection → likely using LoRA-B instead of LoRA-A, or combining both.
  - Global accuracy drops despite filtering → check if projection weighting is disabled (w/o weight variant).
  - Dimension mismatch errors → verify zero-padding and masking logic handles variable d_in/d_out across clients.
  - Convergence oscillation → global direction vectors may be corrupted; consider re-initialization or robust estimation.

- **First 3 experiments:**
  1. **Reproduce LoRA-A vs. LoRA-B stability:** Train with 10 heterogeneous clients (5 CNN, 5 RNN) on CIFAR-10 with Dirichlet α=0.5; apply LIE attack from round 20; plot top-5 energy ratio for A and B from classifier layer. Confirm A shows ~90%+ stability with poisoned clients lower than benign; B shows high variance.
  2. **Ablation on layer selection:** Compare (a) full-parameter all-layers, (b) full-parameter selected-layers, (c) LoRA random-layers, (d) LoRA selected-layers (Horus). Expect Horus > selected-layers full > random LoRA > all-layers full in global accuracy under attack.
  3. **Rank sensitivity under attack:** Test r ∈ {4, 8, 16, 32, 64} with LIE attack on CIFAR-10. Expect rise-saturation-drop pattern with optimum at r=8–16; r≥32 should show degradation due to expanded attack subspace.

## Open Questions the Paper Calls Out
- Can the HOPS coefficient λ be adapted online per round based on attack patterns, rather than manually tuned? The paper manually tunes λ (0.3 for dispersive attacks, 0.7 for directional attacks), but the optimal λ depends on attack type which may not be known a priori in real deployments.
- What is the theoretical basis for LoRA-A's superior stability over LoRA-B under heterogeneity and poisoning? The paper states "we uncover a key empirical observation that the input projection (LoRA-A) is markedly more stable than the output projection (LoRA-B)" but provides only empirical validation without theoretical explanation.
- How does Horus scale to federations with hundreds or thousands of clients and more than two architecture families? Experiments use only 10 clients spanning two architectures (CNNs and RNNs). The paper claims Horus handles "fundamentally different structures" but does not validate beyond this limited setting.

## Limitations
- The claim that LoRA-A is consistently more stable than LoRA-B under heterogeneity and poisoning is based on empirical observations but lacks theoretical grounding or ablation studies showing why A is superior to B specifically.
- The projection-guided weighting mechanism assumes benign updates align with global directions, but this may not hold under extreme heterogeneity or when global direction is corrupted over multiple rounds by persistent attacks.
- Several experimental details (exact architectures, learning rates, attack implementations) are unspecified, limiting reproducibility and confidence in claimed performance gains.

## Confidence
- **High confidence:** The core claim that LoRA-based aggregation reduces attack surface by constraining updates to a low-rank subspace. This follows directly from LoRA's mathematical formulation.
- **Medium confidence:** The effectiveness of HOPS detection using LoRA-A spectral features. While Figure 2 shows empirical stability, the method's robustness to adaptive attacks matching benign heterogeneity signatures is unproven.
- **Medium confidence:** Overall robustness claims versus baselines. Results show consistent improvements, but ablation studies suggest the detection component contributes more than aggregation (w/o weight variant shows only 2.94% drop vs. 1.02% w/o detect).

## Next Checks
1. **Adaptive attack test:** Implement an adaptive poisoning attack specifically designed to match the spectral signatures of benign LoRA-A updates under heterogeneity. Measure HOPS false negative rate and compare against non-adaptive baselines.
2. **Cross-architecture stability:** Test Horus with fundamentally different architectures beyond VGG-like CNNs and vision-LSTMs (e.g., ResNets vs. Transformers). Verify if LoRA-A stability advantage persists across architecture families.
3. **Global direction corruption:** Simulate persistent poisoning attacks that gradually corrupt the global direction vectors used in projection weighting. Measure the degradation in Horus's detection and aggregation performance over multiple rounds.