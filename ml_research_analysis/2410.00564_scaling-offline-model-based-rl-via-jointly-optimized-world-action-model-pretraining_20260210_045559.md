---
ver: rpa2
title: Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining
arxiv_id: '2410.00564'
source_url: https://arxiv.org/abs/2410.00564
tags:
- games
- offline
- learning
- table
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JOWA, a jointly-optimized world-action model
  for scaling offline model-based reinforcement learning across multiple Atari games.
  JOWA uses a shared transformer backbone to jointly optimize both world modeling
  and Q-value criticism, enabling large-scale training with improved stability through
  auxiliary regularization.
---

# Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining

## Quick Facts
- arXiv ID: 2410.00564
- Source URL: https://arxiv.org/abs/2410.00564
- Reference count: 37
- Primary result: 150M parameter JOWA achieves 78.9% human-level performance on Atari using 10% of dataset

## Executive Summary
This paper introduces JOWA, a jointly-optimized world-action model for scaling offline model-based reinforcement learning across multiple Atari games. JOWA uses a shared transformer backbone to jointly optimize both world modeling and Q-value criticism, enabling large-scale training with improved stability through auxiliary regularization. A provably efficient and parallelizable planning algorithm compensates for Q-value estimation errors at inference time. Pretrained on 15 Atari games with 6 billion tokens, the largest JOWA variant (150M parameters) achieves 78.9% human-level performance using only 10% of the dataset, outperforming state-of-the-art baselines by 71.4% on average. Furthermore, JOWA enables sample-efficient transfer to unseen games, achieving 64.7% DQN-normalized scores with just 5k fine-tuning transitions per game. Performance scales favorably with model capacity, and ablation studies validate the importance of joint optimization and planning.

## Method Summary
JOWA introduces a transformer-based architecture that jointly optimizes world modeling and Q-value estimation through a shared backbone network. The model employs auxiliary regularization to improve training stability and incorporates a provably efficient planning algorithm that can be parallelized during inference to compensate for Q-value estimation errors. The approach is trained on a large corpus of 15 Atari games with 6 billion tokens, demonstrating strong performance and transfer capabilities.

## Key Results
- 150M parameter JOWA achieves 78.9% human-level performance using only 10% of the dataset
- Outperforms state-of-the-art baselines by 71.4% on average
- Achieves 64.7% DQN-normalized scores on unseen games with just 5k fine-tuning transitions per game
- Performance scales favorably with model capacity

## Why This Works (Mechanism)
JOWA works by leveraging joint optimization of world modeling and Q-value criticism through a shared transformer backbone, which enables more efficient learning and better generalization. The auxiliary regularization improves training stability by preventing overfitting to specific game dynamics. The parallelizable planning algorithm compensates for Q-value estimation errors during inference, providing more robust decision-making. The large-scale pretraining on 15 Atari games with 6 billion tokens creates rich representations that transfer effectively to unseen games, requiring only minimal fine-tuning data.

## Foundational Learning
- **Transformer architectures**: Multi-head self-attention mechanisms that enable long-range dependencies; needed for capturing complex game dynamics across time steps
- **Joint optimization**: Simultaneous training of multiple objectives through shared parameters; required for efficient learning and parameter sharing
- **Auxiliary regularization**: Additional loss terms beyond primary objectives; used to stabilize training and prevent overfitting
- **Offline RL**: Learning from fixed datasets without environment interaction; critical for scaling to large pretraining corpora
- **Planning algorithms**: Sequential decision-making processes; essential for compensating Q-value estimation errors at inference
- **Transfer learning**: Applying knowledge from source tasks to target tasks; enables sample-efficient adaptation to unseen games

## Architecture Onboarding

**Component Map**: Input Frames -> Shared Transformer Backbone -> World Model Head + Q-value Head -> Planning Module

**Critical Path**: Input frames → Shared transformer → Both heads → Planning → Action selection

**Design Tradeoffs**: Joint optimization vs separate models (parameter efficiency vs specialization), planning overhead vs estimation error compensation, model capacity vs computational cost

**Failure Signatures**: Q-value head degradation under distribution shift, world model collapse during pretraining, planning inefficiency with high-dimensional action spaces

**Three First Experiments**:
1. Compare joint vs separate optimization performance on single game
2. Measure planning contribution by disabling planning component
3. Evaluate transfer performance with varying fine-tuning dataset sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalization to domains beyond Atari games and discrete action spaces
- High computational requirements for training large models on extensive datasets
- Evaluation focuses on relative improvements rather than absolute performance across game difficulty levels
- Ablation studies don't fully explore interaction effects between components

## Confidence

**High Confidence Claims**:
- JOWA architecture and training methodology are technically sound
- Performance improvements over baselines on Atari games are reproducible
- The planning algorithm provides measurable benefits for Q-value error compensation
- Model capacity scaling correlates positively with performance

**Medium Confidence Claims**:
- Sample efficiency gains for transfer learning (5k transitions per game)
- The 71.4% average improvement over baselines across all metrics
- The specific contribution of each component (joint optimization vs auxiliary regularization)

**Low Confidence Claims**:
- Long-term generalization to unseen game types
- Performance sustainability under distribution shift
- Computational efficiency relative to alternative approaches

## Next Checks

1. Cross-domain validation: Test JOWA's performance on continuous control benchmarks (e.g., DM Control Suite) to assess domain transferability beyond Atari games.

2. Ablation under data scarcity: Systematically evaluate performance degradation when reducing pretraining data volume below 6 billion tokens to determine minimum effective data requirements.

3. Planning algorithm stress test: Quantify the exact contribution of the planning algorithm by comparing performance with and without planning under controlled Q-value estimation error rates.