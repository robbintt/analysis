---
ver: rpa2
title: 'Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation'
arxiv_id: '2512.19210'
source_url: https://arxiv.org/abs/2512.19210
tags:
- player
- strategy
- rock
- reasoning
- scissors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an interactive framework for evaluating whether
  large language models exhibit genuine "understanding" in a simple strategic environment,
  focusing on Rock-Paper-Scissors (RPS) as a running example. The system positions
  the LLM as an Observer tasked with identifying which strategies are being played
  and articulating the reasoning behind this judgment.
---

# Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation

## Quick Facts
- **arXiv ID:** 2512.19210
- **Source URL:** https://arxiv.org/abs/2512.19210
- **Authors:** Jerry Wang; Ting Yiu Liu
- **Reference count:** 40
- **Primary result:** Introduces an interactive framework evaluating LLM "understanding" via Rock-Paper-Scissors observation, quantifying strategy identification with Union Loss metric combining Cross-Entropy, Brier score, and EV discrepancy.

## Executive Summary
This paper introduces an interactive framework for evaluating whether large language models exhibit genuine "understanding" in a simple strategic environment, focusing on Rock-Paper-Scissors (RPS) as a running example. The system positions the LLM as an Observer tasked with identifying which strategies are being played and articulating the reasoning behind this judgment. The framework provides a benchmark consisting of both static strategies and lightweight dynamic strategies, and quantifies alignment between the Observer's predictions and ground-truth distributions using three complementary signals: Cross-Entropy, Brier score, and Expected Value (EV) discrepancy. These metrics are integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment. The demo emphasizes interactivity, transparency, and reproducibility, allowing users to adjust LLM distributions in real time, visualize losses as they evolve, and directly inspect reasoning snippets to identify where and why failures occur. The framework captures not only predictive accuracy but also whether the model can stably identify the latent strategies in play, providing insights into the strengths and limitations of current LLM reasoning.

## Method Summary
The framework evaluates LLMs as Observers in Rock-Paper-Scissors by having them identify which strategies two players are using based on game history. The system uses a Candidate Pool of 19 strategies (static A-C, dynamic D-P, reactive X-Z), an RPS Engine to simulate matches and generate trajectory history, and a Prompt Module that constructs a Chain-of-Thought prompt containing candidate information, role specification, game history, and a request for prediction/reasoning. The Observer LLM processes this prompt and outputs predictions (guess_s1, guess_s2, confidence, reasoning). A Steady-State Solver computes the ground-truth outcome distribution for the matchup, and an Evaluation Dashboard provides real-time visualization of metrics including Union Loss (normalized average of Cross-Entropy, Brier score, and EV discrepancy) and Strategy Identification Rate (SIR). The framework uses zero-shot CoT prompting with temperature 0.2, TOP-P=0.7, and runs 200 rounds per match with a 10-round warm-up and 50-round history window.

## Key Results
- Models exhibit distinct loss profiles: GPT-4o-mini shows high, flat loss indicating minimal belief updating; Claude shows high variance/oscillating loss indicating sensitivity without stable hypotheses; o3 achieves the lowest loss.
- Strategy Identification Rate (SIR) reveals that models can have moderate predictive accuracy while failing to stably identify correct strategy labels, particularly in challenging matchups like D vs. Y.
- The Union Loss metric successfully captures both behavioral prediction accuracy and utility inference, with lower values indicating better alignment between model predictions and ground-truth distributions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Positioning the LLM as an Observer rather than a Player provides a clearer signal of "game understanding" by decoupling prediction from action.
- Mechanism: By removing the agent from the action loop, evaluation focuses on distributional alignment (how well the model predicts outcomes) rather than win rate, which conflates pattern recognition, planning, and luck.
- Core assumption: A model that truly understands the game state and strategies will produce predictions that closely match the ground-truth outcome distribution.
- Evidence anchors:
  - [abstract] "Our system positions the LLM as an Observer whose task is to identify which strategies are being played... to probe whether the model can exhibit mind-like reasoning."
  - [section 1] "When the opponent or the initial conditions change, metrics based on win rate become even less reliable."
  - [corpus] Neighbor papers like *Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents* also emphasize reasoning under incomplete information, supporting the use of observation-based evaluation.
- Break condition: If Observer performance does not correlate with performance in active play or if models can achieve low loss via simple heuristics (e.g., predicting uniform distributions).

### Mechanism 2
- Claim: Chain-of-Thought (CoT) prompting allows the LLM to externalize its reasoning process, making evaluation transparent and interpretable.
- Mechanism: The CoT prompt encapsulates the game context, strategy catalog, and trajectory history, requiring the model to output explicit reasoning steps alongside its predictions.
- Core assumption: The generated reasoning traces faithfully reflect the model's internal inference process and are not merely post-hoc rationalizations.
- Evidence anchors:
  - [abstract] "articulate the reasoning behind this judgment"
  - [section 1] "This unified context leverages recent advances in prompting techniques... chain-of-thought prompting... enables LLMs to solve complex tasks with greater accuracy."
  - [corpus] Weak direct corpus evidence on the specific CoT mechanism used here. Neighbor papers discuss reasoning but not this specific CoT implementation.
- Break condition: If reasoning traces are inconsistent with predictions or if models can game the metric by generating plausible but unfaithful explanations.

### Mechanism 3
- Claim: The Union Loss metric, by combining Cross-Entropy, Brier score, and Expected Value discrepancy, provides a comprehensive evaluation of both behavioral prediction and utility inference.
- Mechanism: Cross-Entropy and Brier score assess calibration and distributional accuracy, while EV discrepancy specifically tests if the model correctly infers the direction and magnitude of the payoff advantage.
- Core assumption: A model exhibiting mind-like reasoning should perform well on all three components, indicating both accurate belief updating and correct utility assessment.
- Evidence anchors:
  - [abstract] "These metrics are further integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment."
  - [section 4] "A low Union Loss thus indicates that the model aligns with both action prediction (cognition) and utility inference—the dual capacities fundamental to robust theory-of-mind–style game understanding."
  - [corpus] No direct corpus evidence on the specific Union Loss formulation. Related work on game benchmarks uses win rate or task-specific metrics.
- Break condition: If optimizing for Union Loss does not correlate with improved strategy identification or if models can achieve low loss on one component at the expense of another.

## Foundational Learning

- **Concept: Proper Scoring Rules (Brier Score, Cross-Entropy)**
  - Why needed here: The framework's core metrics (Brier, CE) are proper scoring rules used to evaluate the quality of probabilistic predictions. Understanding them is essential to interpret the loss values.
  - Quick check question: Why is a proper scoring rule necessary for evaluating probabilistic forecasts?

- **Concept: Fixed-Point Iteration / Steady-State Analysis**
  - Why needed here: The system uses a damped fixed-point iteration (Eq. 1) to approximate the steady-state outcome distribution for a matchup, especially between adaptive strategies.
  - Quick check question: What is the condition for the iteration to stop, and what does the resulting steady state represent?

- **Concept: Theory of Mind (ToM) as Operationalized by the Framework**
  - Why needed here: The paper frames its entire evaluation around "mind-like reasoning." Understanding how ToM is operationally defined here—via strategy identification, belief updating, and utility inference—is critical.
  - Quick check question: What are the three evaluation "lenses" used to operationalize ToM-like competencies?

## Architecture Onboarding

- **Component map:** Candidate Pool -> RPS Engine -> Prompt Module -> Observer (LLM) -> Steady-State Solver -> Evaluation Dashboard

- **Critical path:** User selects strategies -> RPS Engine runs match -> Prompt Module builds context -> Observer LLM predicts -> Steady-State Solver computes ground truth -> Evaluation Dashboard computes and displays Union Loss + SIR.

- **Design tradeoffs:**
  - **RPS vs. Complex Games:** The authors chose RPS for its simplicity and controllability, trading off the ability to test complex, long-horizon planning for experimental transparency.
  - **Zero-Shot CoT vs. Advanced Methods:** Standard zero-shot CoT is used for interpretability over more powerful but opaque methods like "Coconut" (latent-space reasoning).
  - **SIR as an explicit-commitment metric:** This is a binary check of strategy naming, which is a strong signal but may miss partial understanding.

- **Failure signatures:**
  - **High, flat loss (e.g., GPT-4o-mini):** Indicates the model is not updating its beliefs from trajectory evidence, possibly emitting a fixed prior.
  - **High variance / oscillating loss (e.g., Claude):** Indicates sensitivity to local history without forming a stable hypothesis.
  - **Near-zero SIR despite moderate loss:** The model's predictions are somewhat accurate on average, but it cannot stably identify the correct strategy label.

- **First 3 experiments:**
  1. **Reproduce the core result:** Run the provided demo or code with the default settings (e.g., H vs. C matchup) for all three models to observe the characteristic loss profiles.
  2. **Ablate the history:** Reduce the `history_limit` (e.g., to 10, 20 rounds) to test the sensitivity of each model's inference to available evidence.
  3. **Test against the D vs. Y (Human-inspired) matchup:** This is the most challenging regime. Analyze why all models struggle (low SIR, higher loss) to identify the reactive strategy Y.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Observer framework's predictive accuracy scale when applied to games with larger action spaces or imperfect information?
- Basis in paper: [explicit] The conclusion states that current evaluation is limited to simple environments and suggests future work extend to "more complex, multi-agent, or strategic games."
- Why unresolved: The current study is restricted to Rock-Paper-Scissors, which has a tiny, discrete action space and perfect information.
- What evidence would resolve it: Testing the Union Loss and Strategy Identification Rate (SIR) of LLMs in complex environments like Hanabi or Poker.

### Open Question 2
- Question: To what extent does the models' performance rely on memorized dataset artifacts rather than genuine on-the-fly computation of strategy rules?
- Basis in paper: [explicit] Section 1 highlights the unresolved debate on whether LLMs exhibit Theory of Mind reasoning or "simply exploit dataset artifacts and surface cues."
- Why unresolved: Standard evaluation struggles to disentangle pre-trained heuristics from actual contextual reasoning capabilities.
- What evidence would resolve it: Evaluating models on novel, out-of-distribution strategy rules (e.g., arbitrary "lose-shift" variants) not found in common training data.

### Open Question 3
- Question: Can architectural changes or refined prompting strategies mitigate the "description-attribution inconsistency" observed in smaller models like GPT-4o-mini?
- Basis in paper: [inferred] Section 6.1 notes that GPT-4o-mini exhibits "description–attribution inconsistency," correctly describing a bias but assigning the wrong strategy label.
- Why unresolved: The paper identifies this failure mode but does not test methods to enforce logical consistency between the reasoning trace and the final prediction.
- What evidence would resolve it: Implementing a verification step that forces the model to validate its final strategy choice against its own computed frequency statistics.

## Limitations
- The RPS framework, while transparent, may not scale to richer strategic environments requiring long-term planning and deeper coordination.
- The specific CoT prompt implementation contains formatting ambiguities (particularly the "........" placeholder in the strategy catalog).
- The evaluation conflates explicit strategy identification (SIR) with implicit distributional alignment, making it difficult to disentangle genuine "mind-like" reasoning from pattern-matching.

## Confidence
- **High confidence:** The experimental results showing distinct loss profiles across models (flat vs. oscillating) are well-documented and reproducible. The demonstration that simple baselines (random, frequency) achieve zero loss is also clearly established.
- **Medium confidence:** The claim that Observer-based evaluation is superior to win-rate-based metrics for assessing strategic understanding is plausible but not conclusively proven. The connection between Union Loss performance and genuine "mind-like" reasoning remains correlative rather than demonstrated causally.
- **Low confidence:** The assertion that this framework is entirely novel or that it provides the most transparent evaluation of LLM theory-of-mind capabilities. The specific CoT prompt format and its relationship to the observed results is also uncertain due to incomplete specification.

## Next Checks
1. **Replicate with different damping parameters:** Systematically vary the α parameter in the steady-state solver to verify that results are robust to this hyperparameter choice.
2. **Cross-validate with active play:** Test whether models that achieve low Union Loss as Observers also demonstrate superior performance when playing actively against these same strategies, controlling for luck and short-term variance.
3. **Test against alternative benchmarks:** Evaluate the same models on established theory-of-mind benchmarks like Eval4ToM or Mind-Game to determine if the RPS Observer framework provides unique insights or merely recapitulates known limitations.