---
ver: rpa2
title: Adaptive Neighborhood-Constrained Q Learning for Offline Reinforcement Learning
arxiv_id: '2511.02567'
source_url: https://arxiv.org/abs/2511.02567
tags:
- learning
- policy
- constraint
- offline
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extrapolation errors in offline
  reinforcement learning (RL) caused by out-of-distribution (OOD) actions. To mitigate
  this, it proposes a novel neighborhood constraint that restricts action selection
  to the union of neighborhoods around dataset actions.
---

# Adaptive Neighborhood-Constrained Q Learning for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.02567
- Source URL: https://arxiv.org/abs/2511.02567
- Authors: Yixiu Mao; Yun Qu; Qi Wang; Xiangyang Ji
- Reference count: 40
- Primary result: State-of-the-art performance on D4RL benchmarks with strong robustness to noisy and limited data

## Executive Summary
This paper addresses extrapolation errors in offline reinforcement learning caused by out-of-distribution actions. The authors propose a novel neighborhood constraint that restricts action selection to the union of neighborhoods around dataset actions, approximating support constraints without requiring behavior policy modeling. The algorithm, called Adaptive Neighborhood-constrained Q learning (ANQ), uses adaptive neighborhood radii based on action quality to enable pointwise conservatism. ANQ achieves state-of-the-art performance on standard offline RL benchmarks and demonstrates superior robustness in noisy or limited data scenarios.

## Method Summary
ANQ uses a bilevel optimization framework where an auxiliary policy learns to perturb dataset actions within adaptive neighborhoods to maximize Q-values. The neighborhood radius is defined as ε exp(-αA(s,a)), where A(s,a) is the advantage function. In the inner loop, the auxiliary policy finds local optima within each action's neighborhood. In the outer loop, expectile regression approximates the maximum Q-value across all neighborhoods. The final policy is extracted via weighted regression toward the optimized actions. The method approximates support constraints without explicit behavior modeling and adapts conservatism based on action quality.

## Key Results
- Achieves state-of-the-art normalized scores on Gym locomotion and AntMaze tasks
- Demonstrates superior robustness to noisy and limited data compared to density, support, and sample constraint methods
- Shows strong runtime efficiency while maintaining high performance
- Validated on standard D4RL benchmarks including HalfCheetah, Hopper, Walker2d, and AntMaze environments

## Why This Works (Mechanism)

### Mechanism 1: Support Approximation via Local Neighborhoods
The union of ε-neighborhoods around dataset actions approximates the true support of the behavior policy without requiring an explicit behavioral model. Instead of modeling the full distribution (support constraint) or sticking strictly to dataset points (sample constraint), the algorithm restricts the Bellman target to actions within a small distance ε of seen actions. Theorem 1 proves that under the "standardness" assumption, the Hausdorff distance between this union of neighborhoods and the true support is bounded by ε. The core assumption is that the data distribution satisfies "standardness," meaning the measure does not exhibit "holes" at small scales, and the sample size is sufficient relative to the covering number of the support.

### Mechanism 2: Pointwise Conservatism via Adaptive Radii
Adapting the neighborhood radius based on action quality (advantage) allows the algorithm to be conservative for good actions (avoiding error) and exploratory for bad actions (seeking improvement). The radius is defined as ε exp(-αA(s,a)). High-advantage dataset actions receive small radii (clamping down on extrapolation error to preserve known good value), while low-advantage actions receive large radii (allowing the Q-function to search for better actions in the vicinity). The core assumption is that the estimated advantage function A(s,a) correlates well with the true quality of the action, and in-distribution advantage estimation is reliable.

### Mechanism 3: Bilevel Optimization for Constrained Q-Learning
A bilevel optimization framework allows the Q-learning target to effectively maximize over the constrained neighborhood set without explicit enumeration. In the inner optimization, an auxiliary policy μ_ω learns to perturb a dataset action to maximize the Q-value within the neighborhood (finding the local peak). In the outer optimization, expectile regression is used on the Q-values of these optimized actions to approximate the maximum Q-value available across all neighborhoods for a given state (approximating the global max over the constraint set). The core assumption is that the expectile regression with τ ≈ 1 successfully approximates the max operator over the discrete set of optimized actions.

## Foundational Learning

- **Concept: Extrapolation Error in Offline RL**
  - Why needed here: The entire paper is structured around mitigating the overestimation of Q-values for Out-of-Distribution (OOD) actions caused by the distribution shift between the learned policy and the static dataset.
  - Quick check question: Why does standard Q-learning fail when trained on a fixed dataset without exploration?

- **Concept: Expectile Regression**
  - Why needed here: Used in the "outer" optimization to train the Value function V_ψ. You must understand that expectile loss (asymmetric L2 loss) with a high quantile τ pushes the estimate toward the upper tail of the distribution, approximating a maximum.
  - Quick check question: How does the parameter τ in expectile regression affect the "pessimism" or "maximization" of the learned value function?

- **Concept: Geometric Measure Theory (Covering Number)**
  - Why needed here: The theoretical justification for the neighborhood constraint relies on covering numbers (how many balls of size ε are needed to cover the support). This provides the mathematical link between sample size and constraint tightness.
  - Quick check question: According to Theorem 1, does increasing the dataset size n allow for a smaller neighborhood radius ε while still guaranteeing support approximation?

## Architecture Onboarding

- **Component map:**
  - Q Network (Q_θ) -> V Network (V_ψ) -> Auxiliary Policy (μ_ω) -> Final Policy (π_φ)

- **Critical path:**
  1. Inner Max: Sample batch (s, a). Optimize μ_ω to find δ that maximizes Q(s, a+δ) subject to the norm constraint ||δ|| ≤ radius.
  2. Outer Max: Update V_ψ using expectile loss against Q(s, a+μ_ω(s,a)).
  3. Q-Update: Update Q_θ using standard TD loss with V_ψ(s') as the target.
  4. Policy Extraction: Update π_φ to regress toward the improved actions a + μ_ω(s,a), weighted by exponentiated advantage.

- **Design tradeoffs:**
  - Lagrange multiplier λ: Controls the overall softness of the neighborhood constraint. Small λ → larger neighborhoods (more risk, less conservative). Large λ → strict adherence to dataset actions (degenerates to sample constraint).
  - Inverse temperature α: Controls adaptiveness. α=0 is a uniform neighborhood. α > 0 enables adaptive radii.
  - Expectile τ: Must be high enough to approximate max, but too high causes instability.

- **Failure signatures:**
  - Q-value Explosion: The neighborhood constraint is too loose (λ too small), allowing OOD actions with erroneously high Q-values to dominate.
  - Performance Collapse to BC: The constraint is too tight (λ too large or α too high for poor data), preventing the policy from finding better actions than the dataset.
  - Slow Convergence: The inner optimization for μ_ω is unstable or learning rate is too low, failing to find the local optimum before the Q-network updates.

- **First 3 experiments:**
  1. Hyperparameter Sensitivity (λ): Sweep λ on a medium-quality dataset (e.g., halfcheetah-medium) to verify the "rise-then-fall" curve shown in Figure 3 of the paper to find the sweet spot between over-conservatism and divergence.
  2. Ablation on Adaptive Radii (α): Compare ANQ (α=1.0) vs. Uniform Neighborhood (α=0) on a noisy dataset (mixed random/expert) to validate the robustness claim in Figure 1.
  3. Constraint Category Comparison: Benchmark ANQ against IQL (Sample Constraint) and SPOT (Support Constraint) specifically on a sparse data task (e.g., antmaze-umaze) to verify that the neighborhood constraint bridges the gap between sample and support methods.

## Open Questions the Paper Calls Out
The paper explicitly suggests that incorporating additional information, such as uncertainty quantification, could potentially lead to more effective neighborhood construction compared to the current advantage-based method for determining neighborhood radii.

## Limitations
- Theoretical analysis assumes "standardness" condition on data distribution which may not hold in practice for high-dimensional or sparse datasets
- Adaptive neighborhood radius depends critically on accuracy of advantage estimation, which can be unstable during early training phases
- Bilevel optimization introduces additional hyperparameters (expectile τ, Lagrange multiplier λ) that require careful tuning for stability

## Confidence
- **High confidence**: The neighborhood constraint framework is theoretically sound under stated assumptions, and the connection between adaptive radii and pointwise conservatism is well-motivated
- **Medium confidence**: The empirical performance claims on D4RL benchmarks, as the results show strong relative performance but with limited hyperparameter sensitivity analysis
- **Low confidence**: The robustness claims to noisy or limited data, as this was demonstrated only on a single dataset variant (mixed random/expert) in Figure 1

## Next Checks
1. Test ANQ on multiple dataset quality levels (clean expert, medium, mixed random) for the same task to verify consistent performance improvements across data conditions
2. Evaluate ANQ's performance when the "standardness" assumption is violated by using datasets with sparse or clustered action distributions
3. Conduct a comprehensive sweep of λ, α, and τ across multiple tasks to map the stability region and identify failure modes