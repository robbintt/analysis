---
ver: rpa2
title: 'VACoT: Rethinking Visual Data Augmentation with VLMs'
arxiv_id: '2512.02361'
source_url: https://arxiv.org/abs/2512.02361
tags:
- image
- arxiv
- visual
- text
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes VACoT, a framework that dynamically invokes
  image augmentations during inference for visual language models to improve robustness
  on challenging and out-of-distribution inputs, especially in OCR-related adversarial
  scenarios. Unlike prior approaches limited to local cropping, VACoT integrates a
  structured collection of general visual augmentations, broadening query image views
  while reducing training complexity and computational overhead through efficient
  agentic reinforcement learning.
---

# VACoT: Rethinking Visual Data Augmentation with VLMs

## Quick Facts
- arXiv ID: 2512.02361
- Source URL: https://arxiv.org/abs/2512.02361
- Reference count: 40
- Primary result: Post-hoc visual augmentation during inference improves robustness on adversarial/OCR inputs more effectively than pre-training on augmented data.

## Executive Summary
VACoT introduces a novel post-hoc visual augmentation framework for visual language models that dynamically invokes image transformations during inference rather than relying on pre-training data augmentation. The approach employs a three-stage training pipeline (knowledge SFT, format SFT, agentic RL) to teach VLMs when and which augmentations to apply, using a conditional reward scheme to prevent verbose reasoning while encouraging effective transformations. Extensive experiments across 13 perception benchmarks demonstrate superior robustness, particularly on challenging adversarial OCR scenarios where VACoT shows significant generalization benefits compared to traditional augmentation approaches.

## Method Summary
VACoT implements a three-stage pipeline: Stage 1 uses difficulty-filtered QA data for knowledge fine-tuning, Stage 2 teaches API syntax through format SFT with synthetic API insertions, and Stage 3 employs Group Relative Policy Optimization (GRPO) to learn adaptive augmentation policies. The framework wraps seven deterministic augmentations (crop, resize, rotate, flip, denoise, edge) as API calls that can be invoked during inference through a stop-token-based interleaved generation loop. When a stop token is emitted, the model executes the parsed operation, re-encodes the transformed image, and continues reasoning with new visual tokens, creating a closed-loop process that exposes the model to multiple views of the same query image.

## Key Results
- VACoT achieves state-of-the-art accuracy on 13 perception benchmarks compared to baseline VLMs
- Post-hoc visual augmentation shows superior performance on AdvOCR adversarial dataset versus pre-training on augmented data
- The three-stage training pipeline converges efficiently, with format SFT critical for learning valid API syntax
- Conditional reward scheme effectively limits reasoning length while maintaining high accuracy on challenging inputs

## Why This Works (Mechanism)

### Mechanism 1
Post-hoc visual augmentations applied during inference improve robustness on adversarial/OCRed inputs more effectively than pre-training on augmented data. The model dynamically invokes augmentations (crop, resize, rotate, flip, denoise, edge) via lightweight API calls embedded in the generation loop. When a stop token is emitted, the model executes the parsed operation, re-encodes the transformed image, and continues reasoning with new visual tokens. This iterative closed-loop process exposes the model to multiple "views" of the same query image, improving perception of degraded or obfuscated content. Core assumption: The VLM's frozen visual encoder can extract meaningful information from transformed images without additional pre-training.

### Mechanism 2
Three-stage training pipeline enables the model to learn when and which augmentations to apply without requiring hand-labeled augmentation trajectories. Stage 1 (Knowledge SFT) uses difficulty-filtered QA data. Stage 2 (Format SFT) teaches API syntax by prompting a teacher model to insert random API calls. Stage 3 (Agentic RL with GRPO) optimizes a multi-component reward to learn adaptive augmentation policies. This progression separates format learning from policy learning. Core assumption: The model can transfer API syntax knowledge from Stage 2 to meaningful augmentation selection in Stage 3 without extensive human annotation.

### Mechanism 3
Conditional reward scheme prevents unnecessary augmentation sequences while encouraging effective transformations. Five reward components (R_vqa, R_fmt, R_cst, R_api, R_suc) balance answer correctness, format compliance, reasoning consistency, valid API calls, and efficient augmentation. R_suc penalizes sequences with more than 2 API calls even when correct, creating a soft constraint against verbose exploration. Core assumption: Perception tasks benefit from concise augmentation sequences; longer reasoning does not necessarily improve accuracy.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: The RL stage uses GRPO to train the augmentation policy. Understanding on-policy RL, advantage estimation, and KL regularization is essential for debugging reward hacking or policy collapse. Quick check: Can you explain why GRPO uses intra-trajectory normalization (σ(·)) instead of per-sample rewards?

- **Stop-token based interleaved generation**: VACoT's core architecture uses special tokens to interrupt autoregressive generation, execute code, and resume with new visual tokens. This differs from standard VLM inference. Quick check: What happens if the model generates an API call with invalid syntax—how does the system recover?

- **Difficulty-based data filtering (pass@k)**: Stage 1 uses a judge model to score data difficulty based on pass@4 inference results. Understanding this filtering strategy is critical for reproducing the knowledge SFT stage. Quick check: Why does the pipeline retain 100% of difficulty level 1-3 samples but only 10% of level 0 samples?

## Architecture Onboarding

- **Component map**: User query + image → Vision Encoder & Projector → Visual tokens → Language Base Model → Text tokens (with API calls) → API Code Executor → Transformed image → Re-encoded visual tokens → Chat History Manager → Final answer

- **Critical path**: 1) User query + image → visual tokens v⁰ encoded 2) Model generates tokens until stop token (e.g., </code>) 3) Parser extracts API call → Executor applies transformation → New image Îᵏ 4) Re-encode Îᵏ → visual tokens vᵏ → append to history as <output>hᵏ</output> 5) Resume generation; repeat until <answer> or max turns 6) Final answer extracted from <answer> tags

- **Design tradeoffs**: Deterministic APIs vs. learnable transformations (wrapping augmentations as API calls ensures controllability but limits flexibility compared to generative approaches); Frozen ViT vs. full fine-tuning (freezing the vision encoder in Stages 2-3 reduces compute but may limit adaptation to novel augmentations); Maximum API calls (K=5) caps exploration but may truncate optimal sequences on complex adversarial inputs

- **Failure signatures**: Format collapse (model generates syntactically invalid API calls; check Stage 2 data quality); Verbose loops (model repeats augmentations without convergence; verify reward weights); Direct answer bypass (model skips augmentations on challenging inputs; may indicate insufficient RL training); Token limit exceeded (multi-round augmentations exceed 10,240 context length; reduce completion length limit or increase K penalty)

- **First 3 experiments**: 1) Ablate Stage 2: Train agentic RL directly after Stage 1 (skip format SFT). Measure API call success rate and compare to full pipeline. 2) Reward sensitivity: Vary R_suc weight (0.0, 0.25, 0.5, 1.0) and plot completion length vs. AdvOCR accuracy. 3) Transformation analysis: Log frequency of each augmentation type on AdvOCR vs. OCRBench. Verify that adversarial samples trigger more `denoise` and `resize`.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can VACoT's post-hoc visual augmentation strategy generalize effectively to non-perception tasks such as visual reasoning, spatial understanding, or image generation? The framework is designed and evaluated exclusively on perception-oriented benchmarks; its applicability to higher-level cognitive tasks remains untested.

- **Open Question 2**: Are brightness, contrast, saturation, sharpening, and thresholding augmentations fundamentally ineffective for VLMs, or does their utility depend on alternative training schemes or task domains? The authors discard these augmentations based on manual evaluation without systematic ablation or exploration of whether different integration methods could make them useful.

- **Open Question 3**: Does the VACoT framework transfer effectively to VLM architectures beyond Qwen2.5VL (e.g., InternVL, LLaVA, or proprietary models)? All experiments use Qwen2.5VL-3B as the base model; no architectural ablation or cross-architecture validation is provided.

- **Open Question 4**: What is the precise inference latency overhead of VACoT's multi-round augmentation, and how does it scale with image resolution or API call depth? The paper claims acceptable overhead but lacks systematic profiling; practical deployment requires understanding cost-performance tradeoffs.

## Limitations

- Limited ablation of the three-stage pipeline makes it difficult to isolate the contribution of each component
- Reward design and hyperparameter sensitivity lack comprehensive analysis of alternative configurations
- Dataset and evaluation transparency issues with unspecified data sources, filtering thresholds, and AdvOCR benchmark details

## Confidence

**High confidence**: The core mechanism of post-hoc visual augmentation via API calls is clearly specified and technically sound.

**Medium confidence**: The three-stage training pipeline is logically structured, but the necessity of Stage 2 (Format SFT) is inferred from limited ablation.

**Low confidence**: The dataset composition, exact filtering criteria, and AdvOCR benchmark details are insufficiently specified for independent replication.

## Next Checks

1. **Ablate Stage 2 (Format SFT)**: Train agentic RL directly after Stage 1 (skipping format SFT) and measure API call success rate and final performance on AdvOCR. Compare to full pipeline to quantify the marginal benefit of format pre-training.

2. **Reward sensitivity analysis**: Vary the R_suc weight (0.0, 0.25, 0.5, 1.0) and maximum API calls K (2, 3, 5, 10) while plotting completion length vs. AdvOCR accuracy. Identify whether stricter or looser constraints improve accuracy-efficiency trade-offs.

3. **Transformation frequency audit**: Log the frequency and success rate of each augmentation type (crop, resize, rotate, flip, denoise, edge) on AdvOCR vs. OCRBench. Verify that adversarial samples trigger more `denoise` and `resize`, and analyze failure cases where the model uses inappropriate augmentations or exceeds K without convergence.