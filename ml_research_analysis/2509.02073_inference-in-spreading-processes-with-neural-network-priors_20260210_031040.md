---
ver: rpa2
title: Inference in Spreading Processes with Neural-Network Priors
arxiv_id: '2509.02073'
source_url: https://arxiv.org/abs/2509.02073
tags:
- where
- algorithm
- spreading
- overlap
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of inferring the initial state of
  spreading processes on graphs, such as epidemics, when the initial sources are determined
  by an unknown function of node features. Unlike standard models that assume random
  initial sources, the authors propose a "Neural Sources Spreading" model where the
  initial state is generated by a simple neural network (specifically, a single-layer
  perceptron) acting on known node-wise covariate variables.
---

# Inference in Spreading Processes with Neural-Network Priors

## Quick Facts
- arXiv ID: 2509.02073
- Source URL: https://arxiv.org/abs/2509.02073
- Authors: Davide Ghio; Fabrizio Boncoraglio; Lenka Zdeborová
- Reference count: 0
- Key outcome: This paper studies the problem of inferring the initial state of spreading processes on graphs, such as epidemics, when the initial sources are determined by an unknown function of node features. Unlike standard models that assume random initial sources, the authors propose a "Neural Sources Spreading" model where the initial state is generated by a simple neural network (specifically, a single-layer perceptron) acting on known node-wise covariate variables.

## Executive Summary
This paper introduces a novel framework for inferring initial sources in spreading processes (like epidemics) where the source nodes are determined by an unknown function of node features rather than being randomly selected. The authors propose a "Neural Sources Spreading" (NSS) model where a single-layer perceptron generates the initial states from node features, and develop a hybrid belief propagation and approximate message passing (BP-AMP) algorithm to infer both the sources and the underlying neural network weights. The key insight is that incorporating neural-network prior information significantly enhances inference performance compared to traditional spreading models, and the analysis reveals interesting phase transition phenomena when using binary weights that create statistical-to-computational gaps.

## Method Summary
The authors develop a hybrid BP-AMP algorithm that combines inference from spreading dynamics with information from node features. The method uses belief propagation for the sparse spreading interactions on the graph and approximate message passing for the dense neural network prior. The algorithm iteratively exchanges information between these two components through shared beliefs on the initial states, with a modified output denoiser that incorporates BP marginals as effective likelihoods. The authors derive denoising functions for both the spreading process and the perceptron prior, and show how to combine them in a unified iterative procedure. The algorithm is validated on Random Regular Graphs with both Gaussian and Rademacher weights for the neural network.

## Key Results
- The BP-AMP algorithm consistently outperforms baselines that use only spreading information or only feature information when using Gaussian weights in the perceptron.
- For Rademacher (binary) weights, the model exhibits first-order phase transitions creating a statistical-to-computational gap where perfect recovery becomes theoretically possible but the BP-AMP algorithm fails to achieve it.
- Free entropy analysis reveals that at certain parameter regimes, the algorithm gets trapped in metastable states despite the existence of Bayes-optimal solutions.
- The Nishimori condition is satisfied, validating the asymptotic optimality of the BP-AMP algorithm among polynomial-time algorithms.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining BP for sparse graph structure with AMP for dense neural network connections enables efficient inference that neither method achieves alone.
- Mechanism: The posterior factorizes into a dense GLM part coupling (u, x0) and a sparse spreading part coupling (x0, {ti}). AMP handles the dense perceptron prior via moment-matched Gaussian approximations (valid by CLT for large N, M), while BP handles locally tree-like spreading dynamics. The two exchange information through shared beliefs on x0.
- Core assumption: Locally tree-like graph structure with loops scaling as log N, and replica symmetry holds.
- Evidence anchors:
  - [abstract] "hybrid belief propagation and approximate message passing (BP-AMP) algorithm that combines inference from the spreading dynamics with information from the node features"
  - [section IV] "Using the cavity method, we merge these components to derive a hybrid BP-AMP algorithm that handles both the sparse spreading interactions and the dense Neural-Network prior"
  - [corpus] Limited direct corpus evidence for this specific hybrid approach in spreading contexts.
- Break condition: Replica symmetry breaking at very low source densities [29, 30] invalidates BP assumptions.

### Mechanism 2
- Claim: Incorporating BP marginals ν_i(x0_i) as effective likelihoods in AMP's output denoiser creates beneficial feedback that improves source estimation beyond either information source alone.
- Mechanism: The standard AMP output denoiser g_o is modified to include prior knowledge ν_i(x0_i) from BP. This creates iterative exchange: BP provides data-dependent likelihoods to AMP, and AMP provides prior beliefs η_i(x0_i) back to BP. Final marginals combine as χ_i = η_i · ν_i / Z.
- Core assumption: Cavity fields concentrate to Gaussians characterizable by first two moments.
- Evidence anchors:
  - [section IV] Equation (22) defines g_o(ω_i, ν_i, V_i) integrating BP marginals into the perceptron denoiser
  - [abstract] "The authors derive denoising functions for both the spreading process and the perceptron prior, and show how to combine them in a unified iterative procedure"
  - [corpus] Weak - corpus papers discuss neural priors but not this specific denoiser combination.
- Break condition: Nishimori condition violation indicates approximation breakdown; finite-size effects near phase transitions.

### Mechanism 3
- Claim: Free entropy analysis reveals first-order phase transitions creating statistical-to-computational gaps specifically for Rademacher (binary) weights.
- Mechanism: Comparing free entropy at random vs informed fixed points identifies when the algorithm traps in metastable states. At ρ_IT < ρ < ρ_c, the perfect-recovery fixed point has higher free entropy (Bayes-optimal), but random initialization converges to metastable partial-recovery fixed point.
- Core assumption: Assumption: BP-AMP is asymptotically optimal among polynomial-time algorithm classes.
- Evidence anchors:
  - [abstract] "the model can exhibit first-order phase transitions when using a Rademacher distribution for the neural-network weights. These transitions create a statistical-to-computational gap"
  - [section VI.A] "If the 'perfect recovery' fixed point is dominant, the algorithm gets trapped in the metastable 'partial recovery' state. This defines a computationally hard phase"
  - [corpus] No direct corpus evidence for this phase transition mechanism in spreading-neural hybrid models.
- Break condition: Gaussian weights avoid this phenomenology entirely—transitions are continuous.

## Foundational Learning

- Concept: **Belief Propagation on Factor Graphs**
  - Why needed here: Core algorithm for sparse graph inference—must understand message passing, cavity method, and conditions for BP exactness on trees.
  - Quick check question: Can you explain why BP is exact on trees but approximate on graphs with loops?

- Concept: **Approximate Message Passing (AMP)**
  - Why needed here: Handles dense perceptron prior—requires understanding Onsager reaction terms, state evolution, and scalar denoisers.
  - Quick check question: Why does AMP require Onsager correction terms (e.g., V·g_o in ω update) that standard message passing doesn't?

- Concept: **Phase Transitions in Inference**
  - Why needed here: Understanding statistical-to-computational gaps requires recognizing first-order transitions, spinodal points, and metastable states.
  - Quick check question: In the phase diagram, what's the operational difference between ρ_IT and ρ_c?

## Architecture Onboarding

- Component map:
  - BP module: Handles spreading dynamics via messages m_{i→j}(t_i, t_j) on sparse graph G with factors Ψ_i
  - AMP module: Manages perceptron prior via weight moments (a_a, v_a), effective fields ω_i, variance V
  - Integration layer: Modified output denoiser g_o(ω,ν,V) receives ν_i from BP, returns η_i to BP
  - Free entropy monitor: Tracks ϕ_RS to detect phase transitions and compare fixed points

- Critical path:
  1. Initialize: AMP variables (a, v) randomly or informed; BP messages uniformly
  2. BP-step: Update m_{i→j} using η_i(x0_i) from AMP; compute ν_i marginals
  3. AMP-step: Update V, ω, A, B, a, v using ν_i in modified denoiser; compute η_i
  4. Combine: χ_i(x0_i) = η_i · ν_i / Z gives final posterior marginals
  5. Monitor: Check convergence via message stability and Nishimori conditions

- Design tradeoffs:
  - **Gaussian vs Rademacher weights**: Gaussian avoids phase transitions (smooth curves) but may not capture discrete structure; Rademacher reveals hardness but risks algorithmic trapping
  - **Random vs informed initialization**: Informed finds perfect-recovery fixed point but requires ground truth unavailable in practice
  - **Damping**: Higher damping improves convergence stability but slows mixing

- Failure signatures:
  - BP divergence: Messages fail to converge at very low source densities (δ < 0.023) where RSB may occur
  - AMP trapping: Algorithm stuck at partial recovery despite ρ > ρ_IT—characteristic of hard phase
  - Nishimori violation: E[O_t=0] ≠ E[MO_t=0] indicates approximation breakdown; check finite-size effects

- First 3 experiments:
  1. **Baseline reproduction (Gaussian)**: Replicate Fig. 2—SI model with sensor observations on RRG (d=3, N=20000), varying ρ ∈ [0,1] and α ∈ {1,2,5,10,20,40}. Verify BP-AMP outperforms BP-only and AMP-only baselines with smooth performance curves.
  2. **Phase transition detection (Rademacher)**: Replicate Fig. 5 for κ=-1, α=6—compare free entropy from random vs informed initialization across ρ. Identify ρ_IT (where informed entropy exceeds random) and ρ_c (where random initialization achieves perfect recovery).
  3. **Nishimori validation**: At fixed κ=-1, α=4, run experiments at N ∈ {5000, 20000, 80000}. Plot E[O_t=0 - MO_t=0] vs N to confirm convergence to zero, validating Bayes-optimality claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does replica symmetry breaking occur in the Neural Sources Spreading model at very low source densities, and how does it affect the BP-AMP algorithm's convergence?
- Basis in paper: [explicit] The authors state "leaving the low-density regime with a neural-network prior for future study" and note that recent work shows RSB in spreading models at very low source densities where BP stops converging.
- Why unresolved: The paper deliberately restricts analysis to higher source densities where RS assumption holds, avoiding this potentially complex phenomenology.
- What evidence would resolve it: Analysis of BP-AMP convergence behavior and free entropy landscapes at source densities approaching zero, potentially requiring full RSB solutions.

### Open Question 2
- Question: Can rigorous mathematical proofs establish the asymptotic optimality of the BP-AMP algorithm among polynomial-time algorithms for the NSS model?
- Basis in paper: [explicit] "developing rigorous mathematical proofs for the more general hardness conjectures presented here remains an important open question."
- Why unresolved: While the authors conjecture BP-AMP is asymptotically optimal and identify statistical-to-computational gaps, formal proof is beyond the paper's scope.
- What evidence would resolve it: Mathematical proof that BP-AMP achieves optimal performance among all polynomial-time algorithms, or identification of algorithm classes that outperform it.

### Open Question 3
- Question: How does extending the neural-network prior to multi-layer architectures affect inference performance and phase transition phenomenology?
- Basis in paper: [explicit] "Future work could also explore more complex neural-network priors, such as multi-layer architectures... to study the influence of network depth."
- Why unresolved: The paper only analyzes single-layer perceptron priors; multi-layer networks require combining different technical frameworks and may introduce new phenomenology.
- What evidence would resolve it: Derivation of multi-layer AMP denoising functions and analysis of whether deep priors amplify or mitigate statistical-to-computational gaps.

### Open Question 4
- Question: Can the BP-AMP algorithm be extended to learn unknown spreading model parameters jointly with the inference task?
- Basis in paper: [explicit] "We assumed all model parameters were known; a natural extension is to study the more realistic case where these parameters must be learned."
- Why unresolved: The current framework assumes known infectivity parameters λ, graph structure, and recovery times; parameter learning introduces additional complexity.
- What evidence would resolve it: Development and validation of expectation-maximization or gradient-based extensions that recover both model parameters and initial states.

## Limitations

- The statistical-to-computational gap analysis relies on the assumption that BP-AMP is Bayes-optimal among polynomial-time algorithms, which remains unproven.
- The phase transition analysis depends on replica symmetry assumptions that may break down at very low source densities (δ < 0.023) [29, 30].
- Free energy comparisons between random and informed initialization require careful finite-size scaling to distinguish true phase transitions from algorithmic artifacts.

## Confidence

- High confidence: Gaussian weight results showing BP-AMP consistently outperforms baselines (smooth performance curves in Fig. 2)
- Medium confidence: Rademacher phase transition detection (ρ_IT, ρ_c identification requires precise free energy calculation and initialization)
- Medium confidence: Nishimori condition validation as Bayes-optimality check (finite-size effects near transitions may cause violations)

## Next Checks

1. **Phase transition reproducibility**: Replicate Fig. 5 for multiple parameter settings (κ=-1, varying α) to confirm statistical-to-computational gap persistence across different regimes.
2. **Algorithm initialization sensitivity**: Test BP-AMP performance with varying random seeds and damping factors to characterize stability of metastable states.
3. **Alternative graph structures**: Validate phase transition phenomena on Erdos-Renyi graphs beyond Random Regular Graphs to test generality of results.