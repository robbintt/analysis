---
ver: rpa2
title: 'Explore-Construct-Filter: An Automated Framework for Rich and Reliable API
  Knowledge Graph Construction'
arxiv_id: '2502.13412'
source_url: https://arxiv.org/abs/2502.13412
tags:
- entity
- class
- type
- types
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Explore-Construct-Filter framework to
  automatically construct a knowledge-rich and reliable API Knowledge Graph (KG).
  The method uses LLMs to explore entity and relation types from seed texts, construct
  a rich but unreliable KG based on a fully connected schema, and filter suspicious
  triples using association rules.
---

# Explore-Construct-Filter: An Automated Framework for Rich and Reliable API Knowledge Graph Construction

## Quick Facts
- arXiv ID: 2502.13412
- Source URL: https://arxiv.org/abs/2502.13412
- Reference count: 40
- Primary result: Improves API Knowledge Graph richness by 133.6% and reliability by 26.6% over existing methods

## Executive Summary
This paper introduces the Explore-Construct-Filter framework to automatically construct knowledge-rich and reliable API Knowledge Graphs. The method uses LLMs to explore entity and relation types from seed texts, construct a rich but unreliable KG based on a fully connected schema, and filter suspicious triples using association rules. Experiments show that the method improves KG richness by 133.6% and reliability by 26.6% over existing methods, achieving a 25.2% F1 score improvement. The framework is validated across multiple LLMs, demonstrating strong generalizability. The key innovation is automating schema design and instance extraction while maintaining high accuracy and comprehensiveness.

## Method Summary
The Explore-Construct-Filter framework automates API Knowledge Graph construction through three stages: Exploration uses LLMs to identify potential entity types and relation types from seed documents; Construction builds a fully connected schema and generates candidate triples using carefully crafted prompts; Filtering applies association rule mining with predefined thresholds to remove unreliable triples. The method eliminates manual schema design and leverages LLMs for both schema generation and instance extraction, with association rules providing a statistical approach to quality control.

## Key Results
- Achieves 25.2% F1 score improvement over existing LLM-driven KG construction methods
- Increases KG richness by 133.6% while maintaining reliability improvements of 26.6%
- Demonstrates strong generalizability across multiple LLMs including GPT-3.5, GPT-4, and Claude

## Why This Works (Mechanism)
The framework works by leveraging LLMs' ability to understand complex relationships in API documentation while using statistical filtering to maintain quality. The fully connected schema approach ensures comprehensive coverage of potential relationships, while association rule mining with specific thresholds (Support ≥ 0.005, Confidence ≥ 0.02, Lift > 1.0) effectively identifies and removes unreliable triples. The combination of rich schema generation and intelligent filtering balances comprehensiveness with accuracy.

## Foundational Learning
- **Association Rule Mining**: Statistical technique for discovering interesting relations between variables in large databases; needed to filter unreliable triples from the constructed KG; quick check: verify support, confidence, and lift calculations on sample data
- **Fully Connected Schema**: Schema design where all entity types are connected to all relation types; needed to ensure comprehensive relationship coverage; quick check: count entity-relation pairs in generated schema
- **Knowledge Graph Triples**: Subject-predicate-object statements representing facts; fundamental unit of KG construction; quick check: validate triple format consistency
- **LLM Prompt Engineering**: Careful design of prompts to guide LLM responses for specific tasks; critical for accurate entity and relation extraction; quick check: test prompts on sample API documentation
- **Type Classification**: Categorization of entities and relations for KG organization; essential for schema generation; quick check: verify type consistency across extracted entities

## Architecture Onboarding

Component Map:
Exploration Module -> Construction Module -> Filtering Module -> Final KG Output

Critical Path:
Seed Texts → LLM Exploration → Schema Generation → Instance Extraction → Association Rule Filtering → Knowledge Graph

Design Tradeoffs:
- Richness vs. Reliability: Fully connected schema maximizes coverage but introduces noise, addressed by filtering
- Automation vs. Control: LLM-driven approach reduces manual effort but requires careful prompt design
- Static vs. Dynamic Thresholds: Fixed thresholds provide consistency but may not adapt to all datasets

Failure Signatures:
- Low F1 score: Poor prompt design or insufficient filtering thresholds
- Sparse KG: Overly aggressive filtering or LLM extraction failures
- Schema mismatch: LLM misunderstanding of API documentation structure

First Three Experiments:
1. Test LLM entity extraction accuracy on sample API documentation
2. Validate association rule thresholds on small synthetic dataset
3. Measure schema coverage completeness against manually designed schemas

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the Explore-Construct-Filter framework be effectively adapted to construct knowledge graphs for non-technical domains (e.g., medical or legal) that possess different semantic characteristics and data structures?
- Basis in paper: [explicit] The authors state in Section VII (Discussion) that while the method is theoretically universal, "true universality presents some challenges" due to differences in data structures across fields, and they plan to focus on adjusting the method for specific fields in the future.
- Why unresolved: The current implementation is specialized for API documentation and Stack Overflow posts, utilizing prompts specifically designed to identify API entities (classes, methods) and relations (dependency, similarity).
- What evidence would resolve it: Experimental results applying the framework with domain-adapted prompts to a non-software engineering dataset (e.g., biomedical literature) and a comparison of resulting KG richness and reliability against domain-specific baselines.

### Open Question 2
- Question: Can an adaptive or dynamic thresholding mechanism outperform the fixed heuristic thresholds (Support, Confidence, Lift) currently used in the KG Filtering module?
- Basis in paper: [inferred] Section VII (Threats to Validity) notes that the chosen thresholds are based on only five representative cases and may not be optimal, potentially filtering out valuable type triples or retaining noise.
- Why unresolved: The current method relies on static values (0.005, 0.02, 1.0) determined by limited experimentation, which may not generalize well to datasets with different distributions of instance triples.
- What evidence would resolve it: A comparative study evaluating a dynamic thresholding algorithm (e.g., one that adjusts based on data distribution) against the fixed Case 3 thresholds, specifically measuring the retention of valid but low-frequency type triples.

### Open Question 3
- Question: Does integrating the constructed API Knowledge Graph with retrieval-augmented generation (RAG) systems improve performance in downstream tasks such as code generation or complex query answering?
- Basis in paper: [explicit] Section IX (Conclusion and Future Work) explicitly lists the goal to "integrate this method with KG retrieval tools like GraphRAG to create a comprehensive knowledge extraction, analysis, and utilization toolkit."
- Why unresolved: The current paper evaluates the KG's construction quality (F1 score, richness, reliability) but does not assess the utility of the resulting KG when used as a knowledge base for LLM-based retrieval or generation tasks.
- What evidence would resolve it: An end-to-end evaluation where an LLM performs specific tasks (e.g., API recommendation or code snippet generation) using a RAG system powered by this framework's KG, compared to a RAG system using a baseline KG.

## Limitations
- Evaluation methodology lacks comparison against established KG construction baselines beyond LLM-driven methods
- Filtering mechanism relies on association rules that may not generalize well to APIs with different documentation styles
- F1 score improvements need independent verification as they were calculated using potentially subjective ground truth annotations

## Confidence
- High Confidence: Framework's three-stage architecture is well-defined and technical implementation details are clearly described
- Medium Confidence: Generalizability across multiple LLMs is demonstrated but only with a limited set of models
- Low Confidence: Absolute performance metrics and robustness of fully connected schema approach in handling edge cases remain uncertain

## Next Checks
1. Independent replication study using different API documentation sources to verify reported F1 score improvements
2. Comparative evaluation against traditional information extraction methods to establish true incremental value
3. Analysis of filtering mechanism's performance on edge cases including sparse documentation and inconsistent terminology