---
ver: rpa2
title: Fully-Decentralized MADDPG with Networked Agents
arxiv_id: '2503.06747'
source_url: https://arxiv.org/abs/2503.06747
tags:
- agents
- maddpg
- agent
- consensus
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces fully decentralized multi-agent reinforcement
  learning algorithms based on MADDPG, addressing scalability and communication limitations
  in centralized training approaches. The core method involves replacing centralized
  critics with local ones trained on individual replay buffers, using surrogate policies
  to approximate other agents' behaviors from local observations.
---

# Fully-Decentralized MADDPG with Networked Agents

## Quick Facts
- arXiv ID: 2503.06747
- Source URL: https://arxiv.org/abs/2503.06747
- Reference count: 40
- This paper introduces fully decentralized multi-agent reinforcement learning algorithms based on MADDPG, addressing scalability and communication limitations in centralized training approaches.

## Executive Summary
This paper introduces fully decentralized multi-agent reinforcement learning algorithms based on MADDPG, addressing scalability and communication limitations in centralized training approaches. The core method involves replacing centralized critics with local ones trained on individual replay buffers, using surrogate policies to approximate other agents' behaviors from local observations. Two communication strategies are proposed: hard consensus updates that average critic parameters across neighboring agents, and soft consensus updates that add a penalty term to encourage parameter similarity. Experiments in the multi-particle environment show that decentralized MADDPG achieves comparable performance to the original algorithm for 2-5 agents, with faster convergence and improved stability for larger agent populations (10 agents). The algorithms also generalize to adversarial settings by treating opponents' actions as fixed during policy optimization.

## Method Summary
The method replaces MADDPG's centralized critic with local critics trained on individual replay buffers. Each agent learns a surrogate policy that outputs joint actions from local observations, enabling decentralized target computation. Two communication strategies are proposed: hard consensus (averaging critic parameters) and soft consensus (adding a penalty term for parameter similarity). The approach maintains MADDPG's actor-critic structure but decentralizes training while preserving multi-agent credit assignment through joint action conditioning in local buffers.

## Key Results
- Decentralized MADDPG achieves comparable performance to original MADDPG for 2-5 agents in multi-particle environment
- Soft consensus updates show more stability than hard consensus, especially for larger agent populations
- For 10 agents, decentralized approach demonstrates faster convergence and improved stability compared to centralized training
- The algorithms generalize to adversarial settings by treating opponents' actions as fixed during policy optimization

## Why This Works (Mechanism)

### Mechanism 1: Surrogate Policy Approximation
- Claim: Agents can approximate joint actions for critic targets without accessing other agents' observations or policies.
- Mechanism: Each agent i learns a joint policy µθi(oi) that outputs all N agent actions from local observations only. This "imagined" joint action substitutes for true joint actions in computing critic targets ŷi. The surrogate acts as an intrinsic representation of teammates—an "imaginary friend" enabling decentralized training.
- Core assumption: Surrogate policies trained on local observations can produce sufficiently accurate joint action estimates for stable TD-learning; true joint actions remain available for critic updates (observed post-hoc).
- Evidence anchors:
  - [abstract] "We introduce surrogate policies in order to decentralize the training while allowing for local communication during training."
  - [Section 3.2] Describes the local replay buffer structure and target computation using surrogates; notes critics still use true joint actions from buffer.
  - [corpus] Related work on distributed value decomposition (DVDN) similarly factors joint Q-functions without centralized access, supporting feasibility of decentralized approximations—though no direct citation links.
- Break condition: If surrogate predictions diverge from true joint actions (e.g., non-stationary policies, high environmental stochasticity), TD-target errors compound and critic learning destabilizes.

### Mechanism 2: Consensus Regularization via Communication Network
- Claim: Soft penalty on critic parameter differences across neighbors stabilizes decentralized learning better than hard averaging.
- Mechanism: Instead of overriding local critics with neighbor averages (hard consensus), add a penalty term ζ·∑C(i,j)||µi - µj||² / ||µj||² to the critic loss. This lets agents integrate communication requirements gradually, avoiding abrupt overwrites that the authors found destabilizing.
- Core assumption: The communication graph C^t is sufficiently connected over time; consensus objective aligns with task objective (cooperative reward).
- Evidence anchors:
  - [abstract] "The networked variants with soft consensus updates show more stability than hard consensus updates."
  - [Section 3.3.2] Formal definition of soft consensus loss with hyperparameter ζ; Section 5 notes hard consensus instability when hyperparameters are not carefully chosen.
  - [corpus] Consensus-based decentralized MARL (arXiv:2508.07001) uses similar averaging for wireless random access optimization, suggesting broader applicability of consensus mechanisms—though stability trade-offs are context-dependent.
- Break condition: High ζ forces premature consensus before individual critics converge; sparse/disconnected graphs prevent effective parameter sharing; adversarial settings break cooperation assumption.

### Mechanism 3: Local Replay Buffers with Joint Action Observation
- Claim: Decentralized critics can learn from local observation-action-reward transitions if joint actions are observable post-hoc.
- Mechanism: Each agent stores (oi, a, ri, o'i) where a is the true joint action. Critics learn Q(oi, a) locally without centralized parameter servers. This reduces communication overhead compared to MADDPG's centralized critic while preserving multi-agent credit assignment through joint action conditioning.
- Core assumption: Joint actions are observable after execution (reasonable in many physical systems); rewards are locally computable or received.
- Evidence anchors:
  - [Section 3.2] "The critic still learns with transitions (oi, a, ri, o'i) from the local replay buffer, where the true joint actions are used, meaning that we assume that each agent's critic has access to the true joint actions."
  - [Section 4.3.2] Shows comparable scores to MADDPG for N=4,5 agents with faster convergence, suggesting decentralized critics suffice for moderate-scale cooperation.
  - [corpus] DVDN and similar distributed methods also assume post-hoc joint action observability; no corpus papers challenge this assumption directly.
- Break condition: If joint actions are not observable or are corrupted; if reward signals require global information not locally available.

## Foundational Learning

- Concept: **Actor-Critic with Deterministic Policies (DDPG foundation)**
  - Why needed here: MADDPG extends DDPG to multi-agent settings; understanding the single-agent base case clarifies why centralization helps (global Q-function) and what decentralization sacrifices.
  - Quick check question: Can you explain why a deterministic policy gradient requires a differentiable Q-function with respect to actions?

- Concept: **Partially Observable Stochastic Games (POSG)**
  - Why needed here: The paper formalizes the problem as a POSG with networked agents; distinguishing between full state S and local observations Oi is critical for understanding what information each algorithm assumes.
  - Quick check question: How does a POSG differ from a Dec-POMDP, and what does the paper assume about reward functions Ri?

- Concept: **Consensus Algorithms in Distributed Systems**
  - Why needed here: The hard/soft consensus mechanisms borrow from distributed averaging literature; understanding convergence conditions (graph connectivity, weight matrices) helps diagnose stability issues.
  - Quick check question: For a time-varying communication graph Gt, what properties must the consensus matrix Ct satisfy for convergence?

## Architecture Onboarding

- Component map:
  - Actor network µθi: MLP (5 hidden layers, 256 units) outputting joint action dimensions; uses only local observations oi
  - Critic network Qi: MLP with same architecture; takes (oi, joint action a) as input
  - Target networks: Soft-updated with τ parameter
  - Local replay buffer Di: Stores (oi, a, ri, o'i) transitions
  - Communication module: Applies Ct for consensus (hard or soft); operates only during training

- Critical path:
  1. Environment step → agents select actions via actors (using only ith component of surrogate output)
  2. Store transition in local buffers (include observed joint action a)
  3. Sample minibatch from local buffer
  4. Compute target yi using surrogate-predicted next actions
  5. Update critic via TD loss (+ consensus penalty if soft)
  6. Update actor via deterministic policy gradient through critic
  7. If hard consensus: average critic parameters with neighbors

- Design tradeoffs:
  - Hard vs. soft consensus: Hard is simpler but unstable; soft adds ζ hyperparameter but more robust
  - Communication connectivity η: Higher η accelerates consensus but may override local learning; sparse graphs reduce compute with negligible performance impact (Section B.3)
  - Surrogate output dimension: Outputs full joint action; scales with N but enables decentralized TD targets

- Failure signatures:
  - Hard consensus instability at high η (>0.05) or large N (Figure 3)
  - No learning in adversarial/mixed settings (Section 4.4.2) — not a bug, inherits MADDPG limitation
  - Diminishing returns as N increases (Section 4.3.2) — suggests fundamental scalability limits of MPE/MADDPG base

- First 3 experiments:
  1. **Reproduce cooperative N=3 "simple spread"**: Compare MADDPG vs. fully-decentralized vs. soft-consensus; verify faster decentralized convergence, match reported ~25 score ceiling
  2. **Ablate consensus penalty ζ**: Test ζ ∈ {0.001, 0.01, 0.1} at N=5; confirm soft consensus stability vs. hard consensus collapse at η=0.05
  3. **Sparsity test at N=10**: Compare fully-connected Ct vs. circle topology (Section B.3); verify similar performance with lower communication cost

## Open Questions the Paper Calls Out
- Can the proposed decentralized training mechanisms (surrogate policies and networked consensus) be effectively integrated with MAPPO to improve performance in Multi-Petparticle Environments (MPE)?
- Can the requirement for agents to access true joint actions in the replay buffer be removed without destabilizing the surrogate policy learning mechanism?
- Is the failure to learn in adversarial settings a result of the underlying MADDPG instability or the specific gradient descent implementation used for the surrogate adversarial policies?

## Limitations
- Several core assumptions and unreported hyperparameters limit confidence in the results, including unspecified learning rate, batch size, and other critical training parameters.
- The scalability analysis is constrained to MPE environments with up to 10 agents, leaving open questions about performance in larger-scale or more complex domains.
- While the algorithms generalize to adversarial settings, they inherit MADDPG's known limitation of instability in mixed cooperative-competitive scenarios, which the paper acknowledges but does not resolve.

## Confidence

- **High Confidence:** The core mechanism of using surrogate policies for decentralized TD-target computation is well-specified and directly supported by the paper's description and pseudocode. The consensus regularization framework (hard and soft variants) is also clearly defined and empirically validated for moderate agent counts.
- **Medium Confidence:** The claim that decentralized training matches centralized MADDPG performance for 2-5 agents is supported by experiments, but lacks ablation studies on hyperparameter sensitivity (e.g., ζ, η) and does not address whether this holds for more complex tasks or larger agent populations.
- **Low Confidence:** The assertion that decentralized training is "more stable" than centralized MADDPG for large N is based on informal observations and anecdotal evidence (e.g., hyperparameter tuning difficulties) rather than systematic comparison or theoretical guarantees.

## Next Checks
1. **Hyperparameter Sensitivity:** Systematically vary ζ (consensus penalty) and η (communication weight) across multiple agent counts (N = 3, 5, 10) to identify stability boundaries and optimal settings for both hard and soft consensus.
2. **Graph Topology Robustness:** Compare performance under different communication graph structures (fully connected, sparse, time-varying) to assess the impact of network connectivity on convergence and stability.
3. **Scalability Beyond MPE:** Test the algorithms on more complex multi-agent tasks (e.g., StarCraft II micromanagement, robotic swarm control) with larger agent populations to evaluate real-world applicability and identify new failure modes.