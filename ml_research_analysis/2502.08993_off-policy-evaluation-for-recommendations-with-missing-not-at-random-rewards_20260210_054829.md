---
ver: rpa2
title: Off-Policy Evaluation for Recommendations with Missing-Not-At-Random Rewards
arxiv_id: '2502.08993'
source_url: https://arxiv.org/abs/2502.08993
tags:
- bias
- mips
- data
- position
- estimator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the bias in off-policy evaluation (OPE) estimators
  when rewards are missing not at random (MNAR) due to display position in recommender
  systems. We analyze how position bias affects OPE estimators and propose a novel
  marginalized inverse propensity score (MIPS) estimator that combines both logging
  policy and reward observation propensities.
---

# Off-Policy Evaluation for Recommendations with Missing-Not-At-Random Rewards

## Quick Facts
- arXiv ID: 2502.08993
- Source URL: https://arxiv.org/abs/2502.08993
- Reference count: 9
- Primary result: Proposed MIPS(w/ROIPS) estimator reduces MSE by ~88.4% compared to standard MIPS under strong position bias

## Executive Summary
This study addresses the bias in off-policy evaluation (OPE) estimators when rewards are missing not at random (MNAR) due to display position in recommender systems. The authors analyze how position bias affects OPE estimators and propose a novel marginalized inverse propensity score (MIPS) estimator that combines both logging policy and reward observation propensities. The proposed approach leverages two types of propensity scores to mitigate biases from both display positions and logging policies, demonstrating significant improvements in mean squared error through synthetic experiments.

## Method Summary
The proposed method builds on marginalized importance sampling (MIS) to correct for logging policy bias, then extends it with reward observation inverse propensity scores (ROIPS) to address MNAR reward issues. The approach assumes rewards depend only on action embeddings (no direct effect), allowing marginalization over embedding space to reduce variance. The dual propensity correction divides weighted rewards by both logging policy and observation propensities, with a heuristic context-free approach for reward observation estimation that balances bias and variance.

## Key Results
- Standard MIPS estimator introduces bias under MNAR reward conditions, even when no-direct-effect assumption holds
- MIPS(w/ROIPS) with heuristic reward observation propensities achieves approximately 88.4% lower mean squared error compared to standard MIPS as position bias increases
- The proposed estimator remains unbiased under theoretical assumptions while effectively balancing bias-variance trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard MIPS estimator introduces bias when rewards are missing-not-at-random due to position effects, even when the no-direct-effect assumption holds.
- Mechanism: MIPS implicitly assumes all rewards are observable. When θ(ok|x) decreases for lower positions, the expected estimator value becomes E_p(x)π(eak|x)[qk(x,eak)], creating a bias term proportional to (1 - θ(ok|x)). The bias compounds as position bias strengthens.
- Core assumption: Rewards are generated conditional on action embeddings (no direct effect of specific action on reward beyond its embedding).
- Evidence anchors:
  - [abstract] "standard OPE estimators like MIPS introduce bias under MNAR reward conditions"
  - [Section 3.1] Derives bias formula: Bias = Ep(x)π(eak|x)[qk(x,eak)(1 - θ(ok|x))]
  - [corpus] Related work on OPE under nonignorable missing data confirms theoretical importance of modeling missingness mechanisms
- Break condition: When position bias is weak (α≈0) or all rewards are observed (θ≈1), MIPS bias approaches zero.

### Mechanism 2
- Claim: Combining marginalized importance weights with reward observation inverse propensity scores produces unbiased estimates under dual bias sources.
- Mechanism: MIPS(w/ROIPS) divides each weighted reward by θ(ok|x), which mathematically cancels the missingness-induced bias: E[w(x,eak)·rk/θ(ok|x)] = E[w(x,eak)·rk·I{ok=1}/θ(ok|x)] = V(k)(π). The double propensity correction ensures unbiasedness when both terms are correctly specified.
- Core assumption: No direct effect (Assumption 1) and accurate reward observation probabilities θ(ok|x).
- Evidence anchors:
  - [Section 3.2] Proposition proves unbiasedness: ED[V̂(k)_MIPS(w/ROIPS)] = V(k)(π)
  - [abstract] "combines propensity scores from both logging policies and reward observations"
  - [corpus] Context-action embedding OPE work shows marginalized weights reduce variance while maintaining unbiasedness under embedding assumptions
- Break condition: When no-direct-effect assumption is violated or θ(ok|x) is misspecified, bias re-emerges.

### Mechanism 3
- Claim: Heuristic context-free reward observation probabilities achieve lower MSE than true probabilities by trading small bias for significant variance reduction.
- Mechanism: True θ(ok|x) requires context-dependent estimation, introducing variance from the double propensity scores. Heuristic θ̂(ok|·) = (1/n)Σoi,k averages observations per position, producing simpler estimates with lower variance. The variance reduction outweighs the bias introduced.
- Core assumption: Position-based observation patterns are relatively context-stable, making position-averaged heuristics reasonable approximations.
- Evidence anchors:
  - [Section 4.3] "MIPS(w/heuristic ROIPS) improved MSE by approximately 88.4% compared to MIPS when α=3.0"
  - [Section 4.3] "MIPS(w/true ROIPS) suffers from high variance owing to its double propensity scores"
  - [corpus] Uncertainty quantification work highlights bias-variance tradeoffs in OPE decision-making
- Break condition: When context significantly affects observation probability beyond position, heuristic approximation degrades.

## Foundational Learning

- Concept: **Inverse Propensity Score (IPS) Weighting**
  - Why needed here: Core technique for correcting selection bias by reweighting observed outcomes by the inverse of their selection probability.
  - Quick check question: If you observe reward r=5 with selection probability θ=0.1, what is the IPS-weighted contribution to the estimator? (Answer: 5/0.1 = 50)

- Concept: **Missing-Not-At-Random (MNAR)**
  - Why needed here: Distinguishes whether missingness depends on unobserved values. Position-based observation means lower positions are less observed regardless of reward value, but this creates bias in standard estimators.
  - Quick check question: If high-reward items are systematically placed in higher positions and also more likely to be observed, is this MNAR? (Answer: Yes - observation depends on position, which correlates with reward)

- Concept: **Marginalized Importance Weights**
  - Why needed here: Reduces variance by computing importance weights over action embedding space (e.g., categories) rather than individual actions, under the assumption that rewards depend on embeddings rather than specific actions.
  - Quick check question: If action space is 500 items with 5 embedding categories, what is the dimensionality reduction for importance weight computation? (Answer: 500 → 5, 100x reduction)

## Architecture Onboarding

- Component map:
  - Data Logging Layer -> Embedding Mapper -> Logging Policy π0 -> Target Policy π -> Marginalized Weight Calculator -> Reward Observation Estimator -> OPE Estimator

- Critical path:
  1. Extract action embeddings for all logged actions
  2. Compute marginalized importance weights w(x,ea) using π and π0
  3. Estimate reward observation probabilities (heuristic: per-position average; model-based: context-dependent)
  4. Apply dual correction: sum over (w·r/θ) for all observations
  5. Aggregate across positions for final policy value estimate

- Design tradeoffs:
  - **True vs. heuristic θ**: True probabilities give unbiasedness but high variance; heuristics reduce variance with small bias
  - **Embedding granularity**: Coarser embeddings reduce variance but risk violating no-direct-effect assumption
  - **Position modeling**: Context-free simpler but may miss heterogeneity; context-dependent more accurate but requires more data

- Failure signatures:
  - **Exploding variance**: Double IPS weights produce extreme values → check θ estimates aren't too small
  - **Persistent bias under correction**: No-direct-effect assumption violated → rewards depend on specific actions beyond embeddings
  - **Negative MSE improvement**: Heuristic θ performs worse than standard MIPS → position bias may be weak or context-dependent

- First 3 experiments:
  1. **Validate MIPS bias under MNAR**: Generate synthetic data with varying position bias α, confirm standard MIPS bias increases with α while MIPS(w/ROIPS) remains stable
  2. **Compare true vs. heuristic θ**: Fix α=3.0, compare MSE of MIPS(w/true ROIPS) vs. MIPS(w/heuristic ROIPS), verify ~88% MSE reduction holds
  3. **Stress test no-direct-effect assumption**: Introduce action-specific reward noise that breaks embedding-only dependence, observe bias emergence in all estimators

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the violation of the "no direct effect" assumption impact the bias magnitude of the MIPS(w/ROIPS) estimator?
- Basis in paper: [explicit] The authors explicitly state in the conclusion that "when the assumption of no direct effect does not hold, a considerably strong assumption, our estimator may suffer from significant bias."
- Why unresolved: The theoretical derivation and synthetic experiments rely on the assumption that rewards depend only on the action embedding ($e_a$) and not the specific action ($a$), which may not hold in complex, real-world recommendation scenarios.
- What evidence would resolve it: A theoretical analysis of the bias bounds under assumption violation or experiments using synthetic data where rewards are generated based on specific actions rather than just embeddings.

### Open Question 2
- Question: Can the proposed method maintain its superior performance over baselines when evaluated on real-world datasets?
- Basis in paper: [inferred] The paper relies entirely on synthetic experiments to demonstrate performance, noting that "Synthetic experiments are effective for assessing the theoretical properties," but does not validate against real-world logged data.
- Why unresolved: Real-world data involves more complex noise distributions, context dependencies, and unobservable confounders that synthetic data may not capture.
- What evidence would resolve it: Empirical results from off-policy evaluation on public real-world datasets (e.g., Open Bandit Dataset) or correlation with online A/B test results.

### Open Question 3
- Question: Can more sophisticated, context-aware estimators for the reward observation probability $\theta(o|x)$ outperform the context-free heuristic?
- Basis in paper: [inferred] The results show that MIPS(w/true ROIPS) suffers from high variance, while the simple "context-free" heuristic achieved the lowest MSE by balancing bias and variance.
- Why unresolved: It is unclear if the poor performance of the "true" probability estimator is intrinsic to the double-propensity-score structure or simply a result of high estimation variance that could be mitigated by better modeling.
- What evidence would resolve it: Experiments comparing the heuristic against advanced machine learning models trained to estimate $\theta(o|x)$ using logged features.

## Limitations
- Heavy reliance on no-direct-effect assumption which may not hold in real recommender systems where specific items can have inherent properties affecting rewards
- Proposed heuristic reward observation propensities assume position-based patterns are context-independent, which could fail when contextual factors significantly influence observation rates
- Empirical validation limited to synthetic data without real-world recommendation system evaluation

## Confidence
- Core theoretical claims about MIPS bias under MNAR rewards: High confidence
- Experimental results showing 88.4% MSE improvement with heuristic ROIPS: Medium confidence
- Claim that heuristic propensities outperform true propensities due to variance reduction: Medium confidence

## Next Checks
1. **Real-world validation**: Test the proposed estimators on actual logged recommender system data with known position bias patterns to verify synthetic experiment findings hold in practice
2. **Assumption relaxation analysis**: Systematically evaluate estimator performance when the no-direct-effect assumption is violated by introducing item-specific reward components beyond embeddings
3. **Heuristic sensitivity study**: Conduct ablation studies varying the granularity of position-based heuristics (e.g., per-position vs. per-segment vs. context-dependent) to understand when and why the heuristic approach succeeds or fails