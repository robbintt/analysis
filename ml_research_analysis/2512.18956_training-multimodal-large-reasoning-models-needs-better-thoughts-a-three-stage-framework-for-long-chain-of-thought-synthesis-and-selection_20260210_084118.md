---
ver: rpa2
title: 'Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage
  Framework for Long Chain-of-Thought Synthesis and Selection'
arxiv_id: '2512.18956'
source_url: https://arxiv.org/abs/2512.18956
tags:
- reasoning
- think
- multimodal
- answer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving multimodal large
  reasoning models (MLRMs) by proposing a three-stage synthesis-selection framework
  called SynSelect for generating high-quality long Chain-of-Thought (CoT) data. The
  method leverages multiple heterogeneous MLRMs to produce diverse reasoning paths,
  applies instance-level and batch-level selection strategies to filter optimal CoTs,
  and refines the dataset for effective supervised fine-tuning.
---

# Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection

## Quick Facts
- arXiv ID: 2512.18956
- Source URL: https://arxiv.org/abs/2512.18956
- Reference count: 40
- Primary result: SynSelect framework generates high-quality long CoT data, improving MLRM performance by up to 2% on complex reasoning tasks

## Executive Summary
This paper addresses the challenge of improving multimodal large reasoning models (MLRMs) by proposing a three-stage synthesis-selection framework called SynSelect for generating high-quality long Chain-of-Thought (CoT) data. The method leverages multiple heterogeneous MLRMs to produce diverse reasoning paths, applies instance-level and batch-level selection strategies to filter optimal CoTs, and refines the dataset for effective supervised fine-tuning. Experiments on multiple multimodal benchmarks show that models trained on SynSelect-generated data achieve significant performance gains—outperforming baseline models by up to 2% on complex reasoning tasks and further improving after reinforcement learning post-training. The framework also demonstrates strong scalability and the ability to self-improve through iterative bootstrap refinement.

## Method Summary
The SynSelect framework operates in three stages: (I) Synthesis using three heterogeneous MLRMs (R1-OneVision, MM-Eureka, Vision-R1) each generating 6 CoT samples per instance, producing 18 candidates per query; (II) Instance-level selection using LLMJudge for answer correctness, LLMPlayer for reasoning validity, and rationale ratio for length appropriateness; (III) Batch selection ranking instances by a composite score (accuracy gain, confidence gain, correctness reward) and selecting top 20% for training. The resulting dataset undergoes SFT followed by GRPO RL post-training on CLEVR. The approach avoids vision-to-text conversion errors by using native multimodal reasoning models and employs hierarchical selection to maintain agent diversity while filtering low-quality CoTs.

## Key Results
- SynSelect-generated data improves MLRM performance by up to 2% on complex reasoning benchmarks
- Full dataset D_cot outperforms refined subset D'_cot on heterogeneous multi-domain benchmark (R1-Onevision-Bench)
- Performance gains persist after GRPO RL post-training on CLEVR dataset
- Batch selection with 20% ratio achieves comparable performance to 60% random selection
- SynSelect demonstrates scalability through iterative self-improvement over three iterations

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Ensemble Diversification
Generating CoTs with multiple architecturally distinct MLRMs produces broader reasoning coverage than single-model synthesis. Three heterogeneous SynAgents (R1-OneVision, MM-Eureka, Vision-R1) each perform 6 sampling rounds per query, yielding 18 diverse candidate paths. Architectural differences in attention patterns, training data, and modality fusion create complementary reasoning trajectories. Diversity in candidate CoTs correlates with higher probability of discovering high-quality reasoning paths.

### Mechanism 2: Proxy-Based CoT Validation via Weak Model Transfer
A CoT's quality can be indirectly measured by whether it improves a weaker model's performance on the same query. A small non-reasoning model (LLMPlayer: Qwen2.5-VL-3B) attempts to solve each query when given a candidate CoT. If LLMPlayer fails with the CoT, the CoT is flagged as potentially unreliable. Success suggests the CoT provides meaningful guidance. CoTs that transfer reasoning utility to weaker models contain more coherent logic chains.

### Mechanism 3: Training-Value Weighted Instance Selection
Prioritizing instances where CoT assistance yields large accuracy/confidence gains produces more instructive training data. Batch selection scores each instance using S(n) = λαΔα + λβΔβ + λγΔγ, where Δα = accuracy gain with CoT vs. without, Δβ = confidence gain, Δγ = correctness reward. Top 20% instances by score are retained. Queries that "need" CoT reasoning more strongly provide better supervision signal for learning reasoning patterns.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: The entire framework operates on generating, filtering, and selecting long CoT trajectories. Understanding CoT structure (reasoning steps, self-reflection, verification) is prerequisite to evaluating quality metrics.
  - Quick check question: Can you explain why a CoT with correct final answer but flawed intermediate logic would harm SFT training?

- Concept: Supervised Fine-Tuning (SFT) → Reinforcement Learning (RL) Pipeline
  - Why needed here: SynSelect generates data for SFT, which precedes GRPO-based RL post-training. The paper shows SFT quality determines RL convergence and final performance.
  - Quick check question: Why does the paper recommend SFT before RL rather than RL-only training for multimodal reasoning?

- Concept: Multimodal Alignment and Cross-Modal Grounding
  - Why needed here: The paper explicitly addresses modality conversion errors (vision-to-text) as a failure mode in prior work. SynSelect uses native MLRMs to avoid intermediate conversion losses.
  - Quick check question: How does native multimodal processing differ from caption-then-reason approaches in terms of error propagation?

## Architecture Onboarding

- Component map: R1-OneVision/MM-Eureka/Vision-R1 (M=3 SynAgents) -> 6 samples per agent -> 18 CoTs per instance -> LLMJudge/LLMPlayer/Ratio filter -> Hierarchical selection -> Batch scoring (S(n)) -> Top 20% selection -> D'_cot dataset -> SFT training -> GRPO RL

- Critical path: CoT diversity (synthesis) → CoT validity (instance selection) → training value (batch selection) → SFT data quality → RL convergence stability

- Design tradeoffs:
  - Selection ratio η (20% vs 100%): Smaller η increases efficiency but risks discarding useful edge-case training samples. Paper shows η=20% selected data ≈ η=60% random data performance.
  - LLMPlayer model size: Smaller model (3B) is cheaper but may reject valid CoTs due to its own limitations; larger model increases validation cost.
  - Number of SynAgents (M) and samples (K): More agents/samples increases diversity but linearly increases synthesis cost.

- Failure signatures:
  - **Modality conversion artifacts**: If SynAgents rely on vision-to-text pipelines, CoTs may reference hallucinated visual details.
  - **Verbose CoTs**: Overly long CoTs with redundant self-verification pass length filter if rationale ratio isn't enforced.
  - **Selection collapse**: If one SynAgent dominates instance selection, ensemble diversity is lost (Figure 1 shows D'_cot has more balanced agent contributions than Dcot).

- First 3 experiments:
  1. **Ablate instance-level selection**: Train on Dcot without batch selection vs. D'_cot with η=0.2. Measure performance gap to isolate batch selection contribution.
  2. **Vary SynAgent composition**: Remove one SynAgent (e.g., use only R1-OneVision + MM-Eureka) and measure CoT diversity metrics (rationale ratio, validity score) and downstream performance.
  3. **Sensitivity analysis on scoring weights**: Run grid search over (λα, λβ, λγ) combinations around the default (2,1,1) to validate whether current weights are optimal or task-dependent.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the full dataset D_cot outperform the refined subset D'_cot on R1-Onevision-Bench (a heterogeneous multi-domain benchmark) while the opposite holds for mathematical reasoning benchmarks?
- Basis in paper: The authors observe this discrepancy and offer only a "plausible explanation" that larger-scale SFT data provides wider exploration space for heterogeneous problem types, but do not validate this hypothesis.
- Why unresolved: The trade-off between data quality (refined subset) and data diversity (full dataset) for different reasoning domains is not systematically investigated.
- What evidence would resolve it: A controlled study varying selection ratios across different domain mixtures, coupled with analysis of which reasoning patterns are preserved or lost during batch selection.

### Open Question 2
- Question: To what extent does the LLMPlayer-based reasoning validity metric correlate with human judgments of CoT quality, and can it detect subtle logical errors that still lead to correct answers?
- Basis in paper: The method uses a weak model (Qwen2.5-VL-3B) to validate CoTs, assuming that helpful CoTs improve weak model performance. The paper acknowledges this is "coarse-grained filtering" but does not validate against human evaluation.
- Why unresolved: The approach may miss CoTs with correct answers but flawed intermediate reasoning, or conversely reject valid CoTs that require capabilities beyond the weak model.
- What evidence would resolve it: Human evaluation of sampled CoTs with varying LLMPlayer scores, comparing against metrics of logical consistency and visual grounding accuracy.

### Open Question 3
- Question: What are the theoretical convergence guarantees and failure modes of the iterative bootstrap refinement process?
- Basis in paper: The paper demonstrates empirical improvement over three iterations but does not analyze whether performance eventually plateaus, whether error amplification can occur, or what conditions enable stable self-improvement.
- Why unresolved: Self-bootstrapping systems can theoretically accumulate biases or errors; the paper shows positive results but does not establish bounds or failure conditions.
- What evidence would resolve it: Extended iterative experiments (e.g., 5-10 iterations) with analysis of data quality metrics per iteration and failure case studies.

### Open Question 4
- Question: How sensitive is SynSelect to the composition and capability gap among the heterogeneous synthesis agents?
- Basis in paper: The method assumes diverse MLRMs provide complementary strengths, but if all agents share similar failure modes or one agent dominates in quality, the synthesis-selection benefits may diminish.
- Why unresolved: The paper uses three specific agents (R1-Onevision, MM-Eureka, Vision-R1) without ablation studies on agent selection or analysis of how agent heterogeneity affects final data quality.
- What evidence would resolve it: Ablation experiments with different agent combinations (including homogeneous vs. heterogeneous groups) and correlation analysis between agent capability diversity and downstream performance gains.

## Limitations

- **Weak proxy validation**: The core claim that weak-model transfer (LLMPlayer) reliably proxies CoT quality lacks empirical validation in the literature. This assumption is untested—we cannot confirm whether failures in LLMPlayer correlate with actual reasoning flaws or are artifacts of model bias.
- **Hyperparameter opacity**: Critical hyperparameters (CoT sampling temperature, LLMJudge consistency thresholds, GRPO reward shaping) are underspecified. This creates uncertainty about whether reported gains stem from the framework architecture or parameter tuning.
- **Architectural dependence**: Performance gains heavily rely on three specific SynAgents (R1-OneVision, MM-Eureka, Vision-R1). Without ablation studies on agent composition, we cannot determine whether gains come from the synthesis-selection framework or from leveraging particularly strong base models.

## Confidence

- **High confidence**: The three-stage framework architecture is clearly specified and internally consistent. The mechanism connecting CoT diversity to improved reasoning coverage is logically sound.
- **Medium confidence**: Empirical results show consistent performance gains (up to 2%) across multiple benchmarks. The claim that SFT quality affects RL stability is supported by experimental evidence.
- **Low confidence**: The weak-model proxy validation mechanism lacks corpus support. Claims about optimal selection ratios (η=20%) and scoring weights ((λα=2, λβ=1, λγ=1)) are not validated through sensitivity analysis.

## Next Checks

1. **Validate weak-model proxy**: Replace LLMPlayer with a larger, more capable reasoning model and measure correlation between validation decisions and actual CoT quality. Document whether small-model failures accurately predict reasoning flaws.

2. **Test architectural generality**: Replicate the framework using different SynAgent combinations (e.g., remove Vision-R1, add a different MLRM). Measure whether performance gains persist when changing the base model ensemble.

3. **Conduct hyperparameter sensitivity analysis**: Systematically vary selection ratio η (10%, 20%, 40%, 60%), scoring weights (λα, λβ, λγ), and LLMPlayer model size. Determine whether reported optimal values are robust or task-specific.