---
ver: rpa2
title: 'Think Global, Act Local: Bayesian Causal Discovery with Language Models in
  Sequential Data'
arxiv_id: '2506.16234'
source_url: https://arxiv.org/abs/2506.16234
tags:
- causal
- data
- discovery
- structure
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BLANCE, a Bayesian framework for causal discovery
  in sequential batch data that integrates observational data with language model
  (LM) knowledge while accounting for dual uncertainties from data-induced and LM-induced
  biases. The key innovation is shifting from DAGs to Partial Ancestral Graphs (PAGs)
  to represent structural uncertainty, combined with a sequential optimization strategy
  that selects maximally informative LM queries under a fixed budget.
---

# Think Global, Act Local: Bayesian Causal Discovery with Language Models in Sequential Data

## Quick Facts
- arXiv ID: 2506.16234
- Source URL: https://arxiv.org/abs/2506.16234
- Reference count: 40
- Primary result: BLANCE outperforms existing methods in structural accuracy (Modified SHD), interventional soundness (SID), and F1-score across multiple datasets while incorporating LM knowledge and handling latent confounders.

## Executive Summary
This paper introduces BLANCE, a Bayesian framework for causal discovery in sequential batch data that integrates observational data with language model (LM) knowledge while accounting for dual uncertainties from data-induced and LM-induced biases. The key innovation is shifting from DAGs to Partial Ancestral Graphs (PAGs) to represent structural uncertainty, combined with a sequential optimization strategy that selects maximally informative LM queries under a fixed budget. BLANCE employs a Bayesian parameter estimation algorithm that incorporates noisy LM priors for latent confounders. Across multiple datasets including EARTHQUAKE, ASIA, and two real-world user-level datasets, BLANCE outperforms existing methods in structural accuracy (Modified SHD), interventional soundness (SID), and F1-score, demonstrating robustness to LM noise and improved parameter recovery.

## Method Summary
BLANCE processes sequential batch data through a four-stage pipeline: (1) Initial PAG extraction using FCI per batch, (2) Sequential LM edge querying via UCB-inspired selection scoring, (3) Histogram aggregation of LM responses with dynamic threshold-based background knowledge updates, and (4) EM-based parameter estimation incorporating LM-provided confounder priors. The framework uses PAGs instead of DAGs to explicitly represent uncertainty, allowing LM responses to encode ambiguity through expanded edge vocabulary. The sequential optimization treats edge queries as a multi-armed bandit problem, balancing exploration and exploitation under a fixed LM budget. Parameter estimation handles latent confounders through iterative EM with LM-regularized priors.

## Key Results
- BLANCE consistently achieves lower Modified SHD and higher F1-scores compared to FCI-Vanilla and FCI-Cumulative across EARTHQUAKE, ASIA, and real-world datasets
- Parameter estimation with LM priors shows convergence even with misspecified priors, outperforming MLE baseline
- Sequential edge selection strategy demonstrates 30-40% lower structural error compared to random selection on user-level datasets

## Why This Works (Mechanism)

### Mechanism 1: Representation Shift to PAGs
The shift from DAGs to PAGs allows explicit encoding of uncertainty through expanded edge vocabulary {→, ←, ↔, ◦→, ←◦, ◦−◦, ·}, permitting LMs to output "I'm uncertain" or "latent confounder possible" instead of forcing premature commitments. This reduces structural errors by accumulating LM responses into histograms that form empirical posteriors over edge types.

### Mechanism 2: Sequential Optimization for Budgeted LM Queries
Treating edge queries as a multi-armed bandit problem maximizes information gain under fixed LM budgets. The scoring function Sei = w1Eei + w2(1/TDei) + w3√(log Ti/Tei) selects edges balancing epistemic uncertainty, proximity to threshold, and exploration bonuses. This ensures informative queries while respecting budget constraints.

### Mechanism 3: EM-Based Parameter Estimation with LM Priors
Iterative Expectation-Maximization incorporating LM-suggested confounder priors and correlations enables parameter recovery even when latent confounders exist. The E-step uses LM priors for latent variables, while the M-step maximizes expected log-likelihood with regularization anchoring latent edge weights toward LM-suggested correlations.

## Foundational Learning

- **Concept: Partial Ancestral Graphs (PAGs) and FCI algorithm**
  - Why needed here: The entire framework hinges on PAGs as the representational substrate; FCI generates initial PAGs per batch
  - Quick check question: Given edges A ↔ B and B ◦→ C in a PAG, what can you conclude about ancestral relationships among A, B, C?

- **Concept: Bayesian posterior updating with histogram representations**
  - Why needed here: LM responses are aggregated into histograms HEi that approximate posteriors; dynamic thresholds τei convert these to background knowledge
  - Quick check question: If HE5(X,Y) = [15, 5, 0] for edge types [→, ↔, ·], what is the empirical probability of X→Y? How does entropy Eei change if one more → response is added?

- **Concept: Multi-armed bandits and UCB-style exploration**
  - Why needed here: The edge selection policy is derived from UCB principles; understanding exploration-exploitation tradeoffs is essential for tuning w1, w2, w3
  - Quick check question: In the scoring function, what happens to edge selection if w3 is set to zero?

## Architecture Onboarding

- **Component map**: Data batch Di → FCI algorithm → initial PAG GDi → sequential optimization (Eq. 10) → edge selection → LM queries → histogram updates HEi, threshold check τei → background knowledge Bi → Final GXi + LM confounder priors → EM parameter estimation → ϕ = {θ, σ²}

- **Critical path**: The dynamic threshold τei (Eq. 6) is the linchpin—incorrectly tuned α causes either (a) too few edges promoted to background knowledge (under-constrained) or (b) noisy edges prematurely fixed (over-confident). The threshold balances entropy-based uncertainty against sampling uncertainty.

- **Design tradeoffs**:
  - Higher LM budget mE → more accurate histograms but higher cost per batch
  - Higher α in τei → more conservative (requires more evidence before promoting edges)
  - Weight w1 vs w2 in scoring: w1 prioritizes uncertain edges (exploration), w2 prioritizes near-threshold edges (exploitation)

- **Failure signatures**:
  - Histograms never converging (flat distributions): LM not providing discriminative responses; check prompt design
  - F1-score degrading across batches: Background knowledge Bi accumulating errors; reduce α or increase minimum threshold
  - Parameter estimation diverging: LM-provided correlation ρ contradicts observational data; reduce regularization λ

- **First 3 experiments**:
  1. Baseline validation: Run FCI-Vanilla and FCI-Cumulative on EARTHQUAKE dataset (1500 samples, 6 batches) to establish Modified SHD bounds; BLANCE should fall between them with mE=20
  2. Ablation on selection score: Compare BLANCE scoring (Eq. 10) vs random edge selection on USER LEVEL DATA-I; expect ~30-40% lower Mod. SHD per Figure 3 (left/middle)
  3. Prior misspecification robustness: On RED WINE QUALITY dataset, run parameter estimation with priors N(12.5, 2.5), N(0, 1), and N(50, 1.5); verify convergence trajectory matches Figure 5 pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can theoretical regret bounds be established for the sequential optimization scheme when LM reward distributions are non-stationary and edge arms are dependent?
- Basis in paper: [explicit] Appendix C.3 states: "A full theoretical analysis of regret in this setting would require modeling non-stationary, dependent reward structures. Promising directions include contextual and Bayesian bandits... We leave these extensions for future work."
- Why unresolved: The paper shows classical MAB regret bounds cannot be applied because LM behavior is batch-context dependent, edge posteriors are interdependent due to graph constraints, and LM outputs have structured non-i.i.d. noise
- What evidence would resolve it: Derivation of sublinear regret bounds under relaxed assumptions, or empirical regret analysis demonstrating convergence properties across diverse LM behaviors and graph structures

### Open Question 2
- Question: How can LM responses be adaptively calibrated to reduce hallucination and inconsistency while preserving informative priors?
- Basis in paper: [explicit] Section 7 states: "Future work may also explore adaptive calibration of LM responses or memory-based accumulation of knowledge across batches."
- Why unresolved: The paper treats LM responses as noisy samples from an implicit distribution but does not actively correct for systematic biases or domain-specific miscalibration that may persist across batches
- What evidence would resolve it: Development of calibration mechanisms (e.g., conformal prediction, learned reliability weights) with demonstrated improvement in precision without sacrificing recall across varied domains

### Open Question 3
- Question: How does BLANCE's performance degrade as the number of latent confounders per variable or confounding complexity increases beyond the single-atomic-confounder assumption?
- Basis in paper: [inferred] Assumptions 6 and 7 restrict each variable to at most one confounder and require atomic confounding (exactly one latent variable per bidirected edge). Real-world causal structures commonly violate these constraints
- Why unresolved: The parameter estimation algorithm (EM-based) assumes tractable posteriors over latent variables under these simplifying assumptions; multiple or non-atomic confounders would complicate the E-step significantly
- What evidence would resolve it: Experiments on synthetic datasets with controlled confounding complexity, measuring Modified SHD, SID, and parameter recovery error as confounder count increases

## Limitations
- Assumption 6 (at most one confounder per variable pair) restricts applicability to more complex causal structures
- LM calibration uncertainty—framework assumes LMs can meaningfully distinguish between certainty and uncertainty, which may not hold across all domains
- Computational overhead—FCI runs per batch plus sequential LM queries increase complexity compared to single-pass methods

## Confidence
- **High**: Structural accuracy improvements (Modified SHD, F1-score) with consistent outperformance across multiple datasets
- **Medium**: Parameter estimation robustness to LM noise based on controlled experiments with misspecified priors
- **Medium**: Sequential optimization efficiency given bandit-based selection rationale, though optimal weight configuration remains dataset-dependent

## Next Checks
1. Test BLANCE on datasets with multiple latent confounders per variable pair to evaluate performance degradation when Assumption 6 is violated
2. Conduct ablation studies varying LM selection weights (w1, w2, w3) across diverse domains to identify optimal configurations
3. Evaluate robustness when LM responses are systematically overconfident or underconfident through controlled noise injection