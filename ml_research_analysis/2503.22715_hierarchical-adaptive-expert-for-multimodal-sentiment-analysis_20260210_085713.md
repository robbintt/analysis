---
ver: rpa2
title: Hierarchical Adaptive Expert for Multimodal Sentiment Analysis
arxiv_id: '2503.22715'
source_url: https://arxiv.org/abs/2503.22715
tags:
- multimodal
- sentiment
- haemsa
- learning
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Hierarchical Adaptive Expert for Multimodal
  Sentiment Analysis (HAEMSA) framework that addresses limitations in existing methods
  for combining modality-shared and modality-specific information in multimodal learning.
  HAEMSA employs a hierarchical structure of adaptive experts to capture global and
  local modality representations, leveraging evolutionary algorithms to dynamically
  optimize network architectures and modality combinations.
---

# Hierarchical Adaptive Expert for Multimodal Sentiment Analysis

## Quick Facts
- arXiv ID: 2503.22715
- Source URL: https://arxiv.org/abs/2503.22715
- Authors: Jiahao Qin; Feng Liu; Lu Zong
- Reference count: 40
- Key outcome: HAEMSA achieves up to 6.3% increase in 7-class accuracy on CMU-MOSI, 2.6% on CMU-MOSEI, and 2.84% increase in weighted-F1 score for emotion recognition on IEMOCAP.

## Executive Summary
This paper introduces HAEMSA, a Hierarchical Adaptive Expert framework for multimodal sentiment analysis that addresses limitations in existing methods for combining modality-shared and modality-specific information. The framework employs a hierarchical structure of adaptive experts to capture global and local modality representations, using evolutionary algorithms to dynamically optimize network architectures and modality combinations. By integrating cross-modal knowledge transfer and multi-task learning, HAEMSA demonstrates significant improvements over state-of-the-art methods on three benchmark datasets.

## Method Summary
HAEMSA combines hierarchical expert networks with evolutionary optimization for multimodal sentiment analysis. The framework uses modality-specific experts (text, audio, visual) to learn fine-grained representations, while a unified expert learns shared representations from concatenated modalities. These are fused hierarchically, with evolutionary algorithms optimizing architecture and modality combinations. Cross-modal knowledge transfer via KL-divergence aligns modality-specific experts with the unified expert, and multi-task learning improves overall performance. The model is evaluated on CMU-MOSI, CMU-MOSEI, and IEMOCAP datasets using Acc-2, Acc-7, MAE, and F1 metrics.

## Key Results
- HAEMSA achieves up to 6.3% increase in 7-class accuracy on CMU-MOSI
- HAEMSA achieves 2.6% improvement on CMU-MOSEI
- HAEMSA achieves 2.84% increase in weighted-F1 score for emotion recognition on IEMOCAP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical expert decomposition enables more effective separation of modality-shared and modality-specific representations than flat fusion architectures.
- Mechanism: Modality-specific experts (text, audio, visual) learn fine-grained representations h_t, h_a, h_v via dedicated functions f_t, f_a, f_v, while a unified expert learns shared representation h_s from concatenated modalities. These are fused hierarchically: h_l^(i) = g_l^(i)([h_l^(i-1); h_s^(i-1)]), allowing local and global representations to interact at multiple granularities.
- Core assumption: Sentiment signals distribute unevenly across modalities with both shared semantic content and modality-unique cues that require separate expert specialization.
- Evidence anchors:
  - [abstract] "HAEMSA employs a hierarchical structure of adaptive experts to capture both global and local modality representations, enabling more nuanced sentiment analysis."
  - [section 3.1] Equations 1-5 define the hierarchical fusion of modality-specific and modality-shared representations.
  - [corpus] Related work (Senti-iFusion, DashFusion) confirms hierarchical fusion improves MSA robustness.
- Break condition: If modalities are highly redundant (minimal modality-specific signal), expert specialization degrades to redundancy.

### Mechanism 2
- Claim: Evolutionary optimization can automate discovery of effective multimodal architectures that would otherwise require manual design.
- Mechanism: A population P of candidate architectures is evolved via fitness F(p) = 1/|D_val| Σ L(p(x), y). Tournament selection chooses parents; crossover combines them (p_o = α·p_i + (1-α)·p_j); mutation adds Gaussian noise (p' = p + N(0,σ)). The fittest individual after convergence becomes the optimized architecture.
- Core assumption: The fitness landscape is sufficiently smooth that evolutionary search can find improved architectures without getting trapped in poor local optima.
- Evidence anchors:
  - [abstract] "Our approach leverages evolutionary algorithms to dynamically optimize network architectures and modality combinations."
  - [section 3.2] Equations 8-10 define fitness evaluation, crossover, and mutation operators.
  - [corpus] Multi-Stage Evolutionary Model Merging paper shows evolutionary methods effectively optimize sentiment-specialized LLMs, supporting general utility.
- Break condition: Evolutionary search becomes computationally prohibitive or converges prematurely with small population sizes.

### Mechanism 3
- Claim: KL-divergence-based knowledge transfer from unified to modality-specific experts improves unimodal representations by injecting cross-modal context.
- Mechanism: The unified expert's output distribution p_s(y|x) guides modality-specific experts via KL loss: L_KL^t = Σ p_s(y_i|x_i) log[p_s(y_i|x_i)/p_t(y_i|x_i)]. This is combined with task losses in the joint objective L = Σλ_k L_task_k + γ(L_KL^t + L_KL^a + L_KL^v).
- Core assumption: Modality-specific experts benefit from soft supervision signals derived from the unified expert's learned multimodal representations.
- Evidence anchors:
  - [abstract] "The framework integrates cross-modal knowledge transfer and multi-task learning to improve sentiment analysis performance."
  - [section 3.3.1] Equations 11-13 define KL divergence losses for each modality's knowledge transfer.
  - [corpus] EGMF paper uses expert-guided multimodal fusion, supporting expert-based guidance mechanisms.
- Break condition: If the unified expert is poorly calibrated, negative transfer degrades modality-specific expert performance.

## Foundational Learning

- Concept: Mixture of Experts (MoE)
  - Why needed here: HAEMSA's hierarchical expert network is a specialized MoE variant. Understanding how expert routing and specialization work is prerequisite to grasping why hierarchical decomposition helps.
  - Quick check question: Can you explain how a gating network routes inputs to different experts in standard MoE, and what determines expert specialization?

- Concept: Evolutionary Algorithms (Selection, Crossover, Mutation)
  - Why needed here: The architecture optimization uses genetic operators to evolve network configurations. Without understanding tournament selection, crossover blending, and mutation perturbations, the optimization loop will be opaque.
  - Quick check question: What is the difference between tournament selection and roulette wheel selection, and why might tournament selection be preferred for architecture search?

- Concept: Knowledge Distillation and KL Divergence
  - Why needed here: Cross-modal knowledge transfer uses KL divergence to align modality-specific expert outputs with the unified expert—effectively distilling multimodal knowledge into unimodal branches.
  - Quick check question: Why is KL divergence asymmetric (KL(P||Q) ≠ KL(Q||P)), and which direction should be used when a teacher guides a student?

## Architecture Onboarding

- Component map:
  - Modality-specific experts (A) -> Unified expert (B) -> Fusion layers (C) -> Task-driven attention module (D) -> Adaptive tower network (E)

- Critical path:
  1. Input features x_t, x_a, x_v → modality-specific encoders → h_t, h_a, h_v
  2. Concatenated inputs → unified expert → h_s
  3. Hierarchical fusion at levels i=1,2,...n → fused representations
  4. Task-driven attention → selected features
  5. Tower outputs → predictions + losses

- Design tradeoffs:
  - Evolutionary population size vs. computational cost: Larger populations explore more architectures but increase training time (Figure 1 suggests population size matters)
  - Hierarchy depth vs. overfitting: Deeper hierarchies capture more granular interactions but risk overfitting on small datasets
  - KL transfer weight (γ) vs. task-specific learning: High γ forces modality experts toward unified expert behavior; low γ leaves them under-constrained

- Failure signatures:
  - Evolution stagnates: Population converges to similar low-fitness architectures → increase mutation rate σ or population diversity
  - Modality collapse: One modality dominates all expert learning → check modality-specific expert gradients, reduce λ weights on dominant modality
  - Negative transfer: Removing KL loss improves performance → unified expert is poorly calibrated; reduce γ or add unified expert regularization

- First 3 experiments:
  1. Baseline verification: Train HAEMSA without evolutionary optimization (random architecture) on CMU-MOSI validation to establish floor performance.
  2. Ablation on hierarchy: Remove one fusion level and measure Acc-7/MAE degradation to quantify hierarchical contribution.
  3. KL transfer weight sweep: Vary γ ∈ {0.0, 0.1, 0.5, 1.0, 2.0} on CMU-MOSEI validation to find optimal knowledge transfer balance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can HAEMSA be adapted to effectively handle missing or incomplete modalities in real-world scenarios?
- **Basis in paper:** [explicit] The authors state the current implementation assumes complete modality availability and identifies missing modalities as a common occurrence in real-world applications that presents a challenge for the model's robustness.
- **Why unresolved:** The current hierarchical structure and fusion towers depend on the simultaneous presence of text, audio, and visual inputs to optimize modality-specific and shared representations.
- **What evidence would resolve it:** Successful implementation of imputation mechanisms or robustness masks, validated by performance metrics on benchmark datasets with synthetically removed modalities.

### Open Question 2
- **Question:** Can the evolutionary optimization process be modified to reduce computational overhead for resource-constrained or real-time applications?
- **Basis in paper:** [explicit] The paper notes that while effective, the evolutionary optimization process increases computational cost compared to simpler fusion methods, potentially restricting applicability in real-time scenarios.
- **Why unresolved:** Population-based search algorithms require evaluating multiple candidate architectures over generations, which is inherently resource-intensive.
- **What evidence would resolve it:** Demonstration of a "lightweight" search strategy or early-stopping mechanism within the evolutionary process that maintains state-of-the-art accuracy while significantly reducing training time and inference latency.

### Open Question 3
- **Question:** How does HAEMSA perform in cross-lingual or cross-cultural contexts where emotional expression norms differ?
- **Basis in paper:** [explicit] The conclusion explicitly lists the model's performance in cross-lingual and cross-cultural settings as unexplored, noting the diverse nature of emotional expression across cultures.
- **Why unresolved:** The model is trained and evaluated exclusively on English-language datasets (CMU-MOSI, CMU-MOSEI, IEMOCAP), potentially learning culture-specific cues.
- **What evidence would resolve it:** Evaluation of the framework on multilingual sentiment analysis datasets (e.g., Chinese or Spanish equivalents) to assess generalization capabilities without retraining.

## Limitations
- Missing critical implementation details including exact hyperparameter settings, architecture configurations, and complete multi-task setup
- Evolutionary optimization requires significant computational resources that may limit reproducibility
- Evaluation only considers datasets with pre-extracted features, not addressing end-to-end feature learning from raw multimodal data

## Confidence
- **High confidence** in the hierarchical expert architecture design and its theoretical benefits for modality separation
- **Medium confidence** in the evolutionary optimization methodology, though empirical validation of convergence and optimality is limited
- **Low confidence** in the exact quantitative improvements due to missing hyperparameter details and potential overfitting on small datasets

## Next Checks
1. **Architecture ablation study**: Systematically remove hierarchical levels or modality experts to quantify their individual contributions beyond what the paper reports.

2. **Evolutionary optimization validation**: Compare HAEMSA performance with randomly initialized architectures of equivalent complexity to determine if evolutionary search genuinely discovers superior configurations.

3. **Cross-dataset generalization**: Test the same trained HAEMSA model on multiple datasets (MOSI, MOSEI, IEMOCAP) to evaluate whether the evolutionary optimization leads to generalizable architectures or dataset-specific overfitting.