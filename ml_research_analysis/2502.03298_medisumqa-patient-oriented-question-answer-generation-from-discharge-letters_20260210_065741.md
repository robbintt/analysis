---
ver: rpa2
title: 'MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge Letters'
arxiv_id: '2502.03298'
source_url: https://arxiv.org/abs/2502.03298
tags:
- discharge
- medical
- llms
- medisumqa
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MeDiSumQA addresses the challenge of patient understanding of complex
  medical documents by providing a standardized benchmark for evaluating large language
  models (LLMs) in patient-oriented question-answering. The dataset was created from
  MIMIC-IV discharge summaries through an automated pipeline combining LLM-based question-answer
  generation with manual quality checks by physicians.
---

# MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge Letters

## Quick Facts
- arXiv ID: 2502.03298
- Source URL: https://arxiv.org/abs/2502.03298
- Reference count: 21
- MeDiSumQA provides standardized benchmark for evaluating LLMs in patient-oriented medical Q&A generation

## Executive Summary
MeDiSumQA addresses the critical challenge of patient comprehension of complex medical discharge documents by creating a standardized benchmark for evaluating large language models (LLMs) in generating patient-oriented question-answer pairs. The dataset was constructed from MIMIC-IV discharge summaries through an automated pipeline combining LLM-based question-answer generation with physician-led manual quality verification. The resulting resource contains 416 high-quality question-answer pairs across six medical categories, enabling systematic evaluation of LLM performance in healthcare communication.

The study reveals surprising findings about model performance, with general-purpose LLMs like Meta-Llama-3.1-8B-Instruct frequently outperforming specialized biomedical-adapted models. Automatic evaluation metrics (ROUGE, BERT Score, UMLS F1) demonstrated strong correlation with human judgment, validating their use for assessing generated medical Q&A quality. By releasing MeDiSumQA on PhysioNet, the authors provide a valuable resource for advancing LLM development in patient education and healthcare communication.

## Method Summary
The dataset creation pipeline involved extracting 1,100 sentences from MIMIC-IV discharge summaries, generating question-answer pairs using GPT-4, and having two physicians manually verify quality. Generated pairs underwent classification into six medical categories using a fine-tuned BERT model. Physicians rated pairs on factuality, brevity, patient-friendliness, relevance, and safety, with only pairs achieving at least four out of five positive ratings being included in the final dataset. The evaluation phase tested seven LLMs including general-purpose models (Meta-Llama-3.1-8B-Instruct, GPT-4, GPT-3.5) and biomedical-adapted models (BioMistral-7B, BioMedLM-2.7B, BioMedLM-6.7B, BioMedGPT-7B), measuring performance using ROUGE, BERT Score, and UMLS F1 metrics.

## Key Results
- Meta-Llama-3.1-8B-Instruct achieved highest automatic scores with ROUGE-L of 31.43 and BERT Score of 10.24
- General-purpose LLMs outperformed biomedical-adapted models despite domain-specific training
- Manual physician assessment confirmed that higher automatic scores correlated with better quality ratings across all five evaluation dimensions
- Treatment & Hospital Course category was most prevalent at 22.4% of all question-answer pairs

## Why This Works (Mechanism)
The pipeline's effectiveness stems from combining automated LLM generation with human expertise verification. The iterative prompting strategy ensures comprehensive coverage of medical topics while physician oversight maintains quality standards. The classification system enables targeted evaluation across medical domains, and the multi-metric assessment captures both semantic similarity and medical terminology accuracy.

## Foundational Learning

**MIMIC-IV Discharge Summaries**: Why needed - Provides standardized medical documentation for dataset creation. Quick check - Verify dataset contains typical hospital discharge information across medical specialties.

**LLM-based Question Generation**: Why needed - Enables scalable creation of patient-oriented Q&A pairs. Quick check - Confirm generated questions are answerable from source text and use lay terminology.

**Medical Category Classification**: Why needed - Organizes Q&A pairs for targeted evaluation. Quick check - Validate classification accuracy across all six medical categories.

**Automatic Evaluation Metrics**: Why needed - Provides scalable quality assessment aligned with human judgment. Quick check - Compare metric scores with physician ratings to establish correlation.

**Physician Quality Verification**: Why needed - Ensures medical accuracy and patient-friendliness. Quick check - Confirm at least 80% of generated pairs meet quality thresholds.

## Architecture Onboarding

**Component Map**: Discharge summaries -> Sentence extraction -> GPT-4 generation -> Physician verification -> Category classification -> Dataset compilation

**Critical Path**: Sentence extraction -> GPT-4 generation -> Physician verification -> Category classification

**Design Tradeoffs**: Automated generation vs. manual verification balances scalability with quality; general-purpose vs. biomedical models tradeoffs in domain specificity vs. general language understanding.

**Failure Signatures**: Poor physician ratings indicate factual errors or inappropriate terminology; low automatic metric scores suggest semantic mismatch or terminology issues; imbalanced category distribution may indicate generation bias.

**First Experiments**:
1. Test single-sentence generation to isolate quality issues
2. Compare GPT-4 vs. GPT-3.5 performance on identical inputs
3. Evaluate category-specific performance across all medical domains

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size of 416 pairs from single medical center limits generalizability
- No reported inter-rater reliability (kappa coefficient) for physician verification
- English-language focus constrains cross-linguistic validation
- Automatic metrics may not fully capture patient-oriented communication quality

## Confidence

**High**: Dataset creation methodology and quality assurance processes
**Medium**: Comparative model evaluation results and metric correlations
**Low**: Generalizability of findings to diverse clinical contexts and patient populations

## Next Checks

1. Expand validation to multi-center discharge summaries from diverse healthcare systems to assess generalizability across different documentation practices and patient populations

2. Conduct blinded clinical trials with actual patients or caregivers to evaluate comprehension and usability of generated Q&A pairs in real-world settings

3. Perform ablation studies testing the contribution of each pipeline component (LLM generation, manual verification, category classification) to isolate which elements drive observed performance differences