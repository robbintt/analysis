---
ver: rpa2
title: 'BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation
  Architectures'
arxiv_id: '2506.05871'
source_url: https://arxiv.org/abs/2506.05871
tags:
- decode
- prefill
- requests
- time
- instance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BestServe is a framework for optimizing large language model (LLM)
  serving strategies by estimating goodput under various operating scenarios. It addresses
  the challenge of finding efficient resource allocation and parallelism strategies
  for serving LLMs to millions of users, which traditionally requires labor-intensive
  trial-and-error experiments.
---

# BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation Architectures

## Quick Facts
- arXiv ID: 2506.05871
- Source URL: https://arxiv.org/abs/2506.05871
- Reference count: 39
- BestServe achieves <20% error vs. manual benchmarking for LLM serving strategy optimization

## Executive Summary
BestServe is a framework for optimizing large language model (LLM) serving strategies by estimating goodput under various operating scenarios. It addresses the challenge of finding efficient resource allocation and parallelism strategies for serving LLMs to millions of users, which traditionally requires labor-intensive trial-and-error experiments. The framework uses a three-layer hierarchical structure combining latency estimation, temporal dynamics simulation, and systematic strategy optimization to determine optimal serving configurations without costly benchmarking.

## Method Summary
BestServe employs a three-layer hierarchical structure: an Estimator that predicts operator-level latencies using an adapted roofline model and CPU-GPU dispatch dynamics, a Simulator that models the temporal dynamics of request processing, and an Optimizer that systematically explores serving strategies to identify the configuration that maximizes goodput while satisfying service level objectives (SLOs). The framework supports both collocated and disaggregated architectures and uses bisection search to find the maximum arrival rate meeting latency constraints.

## Key Results
- Determines optimal serving strategy in minutes on a single standard CPU, eliminating costly benchmarking
- Achieves predictions within 20% error margin compared to ground truth from manual benchmarking
- Demonstrates 11.2%, 12.1%, 8.6%, and 30.1% average absolute relative error across four tested operating scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BestServe's adapted roofline model provides latency estimates by correcting for dispatch-bound bottlenecks in the decode phase and by calibrating compute/memory efficiency.
- Mechanism: It decomposes each Transformer block into operators, computes FLOPs (W) and traffic (Q), derives arithmetic intensity (I=W/Q), then applies P = min{ecSc, I·emSm} (adapted roofline). It sums per-operator times and adds CPU dispatch time; if dispatch exceeds compute, it inflates the compute timeline (dispatch-bound), otherwise compute accumulates (compute-bound). For TP, it includes communication via T+ ≈ bsh/(t·e+·S+) and adjusts W/Q for sharding.
- Core assumption: Operator-level dispatch time and efficiency params (ec, em, e+) are relatively stable within a hardware/software stack and can be profiled once and reused.
- Evidence anchors:
  - [abstract]: "adapted roofline model and CPU-GPU dispatch dynamics"
  - [section 3.3.1/3.3.3]: Eq. (3), Algo. 1, and Fig. 5 patterns
  - [corpus]: DistServe uses a queueing view and optimized placement; complementary but not a direct validation of this roofline+dispatch mechanism.
- Break condition: If dispatch-time constants or efficiency parameters drift across library/compiler versions or GPUs, predictions degrade; recalibration is required.

### Mechanism 2
- Claim: The simulator models temporal dynamics of prefill/decode as a queueing system, including continuous batching via pseudo batch size, enabling fast goodput estimation without real inference runs.
- Mechanism: Prefill batches requests arriving by Tcurrent and assigns to idle instances; decode uses a pseudo batch b† = max(⌊(b+1)/τ⌋,1) (τ≈2.5) to approximate continuous batching. Collocation mimics vLLM's prefill-prioritization and prefill/decode non-mixing with status flags and resume queues. Goodput is found via bisection on arrival rate λ until P90 TTFT/TPOT satisfy SLOs (with relaxation τ=0.1).
- Core assumption: Poisson arrivals and fixed sequence/generation lengths approximate real workloads; pseudo batch size captures average batching behavior.
- Evidence anchors:
  - [abstract]: "inference simulator built on an adapted roofline model and CPU-GPU dispatch dynamics"
  - [section 3.4.1-3.4.4]: Algo. 2-7, pseudo batch size formula (9), SLO checks in Algo. 8-9
  - [corpus]: JITServe and SCORPIO emphasize SLO-aware scheduling; LAPS and SCORPIO explore length-aware prefill disaggregation; none validate this specific pseudo-batch heuristic.
- Break condition: For long output lengths or highly variable batch dynamics (e.g., OP4), the per-request pseudo-batch approximation accumulates error; token-level simulation would reduce error at higher cost.

### Mechanism 3
- Claim: Enumerative optimization with bisection identifies the highest arrival rate (goodput) satisfying SLOs, enabling ranking of architectures (collocated vs. disaggregated) and TP/PD ratios.
- Mechanism: Optimizer enumerates admissible strategies (instance counts, TP sizes) and uses the Simulator to evaluate feasibility at each λ via bisection; SLO feasibility checks include a 10% relaxation to handle simulation stochasticity from Poisson arrivals. BestServe returns normalized goodput (goodput/GPUs) to compare resource efficiency.
- Core assumption: The optimal strategy lies within the enumerated set and can be detected by simulation-derived P90 metrics; the relaxation factor adequately compensates for variance.
- Evidence anchors:
  - [abstract]: "ranking serving strategies by estimating goodput" and "determines the optimal strategy in minutes"
  - [section 3.5]: Algo. 8-9, relaxation factor τ=0.1, normalized goodput metric
  - [corpus]: DistServe and Mooncake seek PD disaggregation optimization via heuristic placement; no direct corpus validation of this bisection+relaxation approach.
- Break condition: If the strategy space is large or includes additional dimensions (e.g., pipeline parallelism, MoE), exhaustive enumeration may be infeasible; stochastic variability may cause mis-ranking without sufficient repetitions or refined τ.

## Foundational Learning

- Concept: Roofline model and arithmetic intensity
  - Why needed here: It determines whether operators are compute- or memory-bound, and with efficiency scalars predicts per-operator latency.
  - Quick check question: If an operator has I < ecSc/(emSm), which roofline region is it in and what limits performance?

- Concept: CPU-GPU dispatch latency and kernel launch overheads
  - Why needed here: Decode-phase latency is often dispatch-bound; understanding CPU-side launch overheads clarifies why memory-bandwidth upgrades may not help.
  - Quick check question: In a dispatch-bound regime, will increasing GPU memory bandwidth reduce token latency?

- Concept: Continuous batching and its simulation approximations
  - Why needed here: Continuous batching is standard in LLM serving; BestServe approximates it with pseudo batch size to keep simulation tractable.
  - Quick check question: Why does per-token simulation better capture continuous batching than per-request simulation, and what is the tradeoff?

## Architecture Onboarding

- Component map: Estimator (per-operator latency via adapted roofline + dispatch) → Simulator (prefill+decode, collocation/disaggregation, pseudo batch, SLO feasibility) → Optimizer (enumeration of strategies + bisection on λ for goodput).
- Critical path: Profiling to get ec, em, e+ and dispatch-time constants; validating per-phase latency estimates; calibrating τ for pseudo batch size; then running the optimizer on a grid of strategies.
- Design tradeoffs: Pseudo batch size balances accuracy vs. simulation speed; relaxation factor balances robustness vs. optimistic goodput; enumeration is simple but may not scale to huge strategy spaces.
- Failure signatures: Consistent over-prediction of goodput may indicate uncalibrated ec/em or ignoring memory limits; high variance in P90 TTFT/TPOT across runs suggests need for more repetitions or larger request samples; large errors on long-output scenarios (like OP4) point to pseudo-batch limitations.
- First 3 experiments:
  1. Profile a small model (e.g., LLaMA-3.2-1B) on target hardware to estimate dispatch times and ec/em for prefill and decode; validate single-operator latency predictions vs. microbenchmarks.
  2. Run BestServe's disaggregation simulator on 1p1d and 2p2d scenarios with known sequence/generation lengths; compare predicted P90 TTFT/TPOT vs. vLLM-Ascend real runs to calibrate τ and relaxation factor.
  3. Use the optimizer to rank a small set of strategies (e.g., 2m, 1p1d, 2p2d) under OP1-OP3; compare normalized goodput rankings and absolute errors against manual benchmarking to identify systematic biases and adjust efficiency params.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a token-level simulation approach for the decode phase be implemented that retains high accuracy for long-generation scenarios without compromising computational efficiency?
- Basis in paper: [explicit] The authors state the "pseudo batch size" heuristic introduces inaccuracies in long-generation scenarios (like OP4) and suggest developing token-level simulations as a refinement.
- Why unresolved: The current request-level heuristic fails to capture dynamic continuous batching nuances for long sequences, while token-level simulation is computationally expensive.
- What evidence would resolve it: A simulation method that reduces the absolute relative error in long-generation scenarios (e.g., OP4) to below 15% while maintaining fast execution on standard CPUs.

### Open Question 2
- Question: How can the relaxation factor $\tau$ used in feasibility checks be determined based on statistical principles rather than empirical heuristics?
- Basis in paper: [explicit] The authors explicitly invite experts to provide guidance on making "more educated choices for $\tau$ based on statistical principles" to address the variability of simulated percentiles.
- Why unresolved: The current choice of $\tau=0.1$ is empirically derived to handle stochastic oscillations but lacks a rigorous theoretical foundation.
- What evidence would resolve it: A statistical model linking the relaxation factor to the confidence intervals of the underlying Poisson arrival process and simulation variance.

### Open Question 3
- Question: Can the BestServe framework be extended to accurately model and optimize serving strategies for advanced architectures like Mixture of Experts (MoE) and Pipeline Parallelism (PP)?
- Basis in paper: [explicit] The Conclusion lists generalizing BestServe to support MoE and PP as a key direction for future research to expand applicability.
- Why unresolved: The current Estimator and Simulator are designed for dense models with Tensor Parallelism; MoE and PP introduce distinct routing, communication, and pipeline bubble dynamics not currently modeled.
- What evidence would resolve it: Successful integration of MoE routing logic and pipeline stage scheduling into the Estimator and Simulator components, validated against real-world MoE inference benchmarks.

## Limitations
- Dispatch-time profiling dependency: Framework accuracy depends on correctly profiling CPU-GPU dispatch times and efficiency parameters that vary by hardware/software stack
- Pseudo batch size approximation: τ=2.5 heuristic shows increasing error for long generation lengths (30.1% error on OP4 scenario)
- Enumeration scalability: Computational cost grows exponentially with strategy space size, limiting scalability to richer parallelism dimensions

## Confidence

**High Confidence**: The core three-layer architecture (Estimator → Simulator → Optimizer) is well-defined and the adapted roofline model with dispatch-time modeling is technically sound. The 11.2-30.1% error range across scenarios is reasonable for a predictive framework that avoids benchmarking.

**Medium Confidence**: The specific parameter values (ec=0.65, em=0.6 for prefill; ec=0.65, em=0.3 for decode) and the τ=2.5 pseudo batch heuristic are likely tuned to the tested hardware/software stack and may not generalize without recalibration. The enumeration approach is simple but may not scale to richer strategy spaces.

**Low Confidence**: The framework's behavior under highly variable workloads (bursty arrivals, mixed sequence/generation lengths) is not extensively validated. The paper focuses on Poisson arrivals with fixed lengths, which may not capture real-world traffic patterns.

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary ec, em, e+, and τ parameters within reasonable ranges to quantify their impact on goodput predictions. Identify which parameters are most critical and develop guidelines for when recalibration is needed.

2. **Token-Level Validation for Long Outputs**: Implement token-level simulation for a subset of strategies and compare against the pseudo-batch approach for long-generation scenarios (like OP4). Quantify the accuracy tradeoff and establish when token-level simulation is necessary.

3. **Stress Test with Variable Workloads**: Evaluate BestServe's accuracy under bursty arrival patterns, mixed sequence/generation lengths, and non-Poisson distributions. Compare predicted goodput vs. actual performance under these more realistic conditions to identify failure modes.