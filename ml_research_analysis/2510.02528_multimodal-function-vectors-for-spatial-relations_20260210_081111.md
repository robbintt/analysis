---
ver: rpa2
title: Multimodal Function Vectors for Spatial Relations
arxiv_id: '2510.02528'
source_url: https://arxiv.org/abs/2510.02528
tags:
- function
- vectors
- relation
- relations
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates whether function vectors\u2014compact representations\
  \ of task knowledge\u2014can be extracted and manipulated in multimodal models to\
  \ control relational reasoning. By applying causal mediation analysis, the authors\
  \ identify a small set of attention heads in the OpenFlamingo-4B vision-language\
  \ model whose activations encode spatial relations."
---

# Multimodal Function Vectors for Spatial Relations

## Quick Facts
- **arXiv ID:** 2510.02528
- **Source URL:** https://arxiv.org/abs/2510.02528
- **Reference count:** 15
- **Key outcome:** Sparse attention heads in OpenFlamingo-4B encode spatial relations that can be extracted as function vectors and linearly combined to solve analogy problems involving untrained relations.

## Executive Summary
This paper demonstrates that spatial relation knowledge in vision-language models can be extracted and manipulated through function vectors—compact representations derived from specific attention head activations. Using causal mediation analysis, the authors identify a small subset of attention heads whose activations causally influence relational predictions. These relation-specific function vectors can be injected at intermediate layers to improve zero-shot relational reasoning and linearly combined to solve analogy problems involving untrained spatial relations, revealing that LMMs encode relational knowledge in localized, optimizable internal structures.

## Method Summary
The method extracts relation-specific function vectors from OpenFlamingo-4B by identifying attention heads with high Average Indirect Effect (AIE) scores on spatial relations. Function vectors are computed as the sum of mean activations from top-ranked heads, then injected at intermediate layers during inference. The vectors can be fine-tuned on modest data while keeping the model frozen. For analogy problems, composite function vectors are created by linearly combining trained relation vectors weighted by their conditional probabilities, enabling reasoning about untrained spatial relations.

## Key Results
- A small subset of attention heads (6-12) encodes spatial relation knowledge that can be extracted as function vectors
- Zero-shot relational prediction accuracy improves significantly (>2×) when function vectors are injected at intermediate layers
- Composite function vectors enable solving analogy problems involving untrained spatial relations (16.8% accuracy vs 9.6% for 10-shot baseline)

## Why This Works (Mechanism)

### Mechanism 1: Sparse Causal Subnetwork in Attention Heads
The model encodes spatial relations in a small cluster of attention heads in mid-layers. AIE analysis identifies these heads by measuring how replacing their activations with relation-averaged values affects prediction accuracy. The optimal head count (6-12) suggests sparse encoding rather than diffuse distribution.

### Mechanism 2: Layer-Sensitive Vector Injection
Function vectors act as computation triggers when injected at intermediate layers (layer ~19 for synthetic data, layer ~8 for GQA real images). Early layers lack sufficient semantic abstraction while late layers are too downstream for effective intervention.

### Mechanism 3: Linear Composability for Novel Relations
Complex spatial relations can be represented as weighted linear combinations of primitive relation vectors. For analogies, the model computes weights proportional to conditional probabilities and constructs composite vectors that transfer relational structure from source to target pairs.

## Foundational Learning

- **Causal Mediation Analysis in Neural Networks**: Understanding intervention vs correlation is essential for identifying which heads causally influence relational predictions. *Quick check:* If replacing head activations increases correct prediction probability, does this prove causation or mere correlation?

- **In-Context Learning (ICL) in Transformers**: Function vectors are extracted from ICL prompts, assuming task knowledge is encoded in activations rather than weights. *Quick check:* How does a transformer learn a new task from 4 demonstration examples without weight updates?

- **Cross-Attention in Vision-Language Models**: OpenFlamingo fuses visual and textual streams via cross-attention, with function vectors extracted from language-module layers after fusion. *Quick check:* Why would relation-specific information be found in language-module layers rather than vision-only layers?

## Architecture Onboarding

- **Component map:** Vision encoder (CLIP ViT-L/14) → Vision features → Cross-attention layers → Language model (RedPajama-INCITE 3B) → Function vector extraction site (language-module attention heads after cross-attention)

- **Critical path:** 1) Construct 4-shot ICL prompts with consistent spatial relation 2) Run forward pass, collect attention head activations 3) Compute AIE per head to identify top performers 4) Extract function vector as sum of mean activations from top heads 5) Inject at target layer during inference 6) Optionally fine-tune vector on zero-shot set

- **Design tradeoffs:** Mid-layer injection balances abstraction vs control; 6-12 heads optimize signal-to-noise ratio; 2-4 shot context sufficient for extraction without overfitting

- **Failure signatures:** Uniform AIE scores across heads, monotonic accuracy increase with head count, late-layer injection outperforming mid-layer, composite vectors failing on held-out relations

- **First 3 experiments:** 1) AIE mapping to verify concentrated high scores in mid-layers 2) Layer sweep to confirm non-monotonic peak at intermediate layers 3) Zero-shot intervention comparing baseline vs function-vector-injected accuracy

## Open Questions the Paper Calls Out

- **Generalization to larger LMMs:** Whether the framework scales to more advanced architectures with different training regimes and fusion mechanisms remains untested beyond OpenFlamingo-4B
- **Extension to non-spatial relations:** The approach's flexibility for representing physical causality, agentic actions, or social interactions is unexplored
- **Real-world analogy problems:** Composite vectors were tested only on synthetic images; their effectiveness on complex real-world imagery with noise and occlusion is unknown

## Limitations

- The causal mediation analysis assumes AIE uniquely identifies functionally relevant heads without comparing to alternative importance metrics
- Layer sensitivity shows inconsistent optimal positions between synthetic (layer 19) and real (layer 8) datasets without clear explanation
- Linear composability experiments cover only 4 trained relations creating 4 novel relations, a limited scope that may not generalize to complex reasoning

## Confidence

**High Confidence:** Identification of sparse causal subnetworks and improved zero-shot accuracy through function vector injection
**Medium Confidence:** Fine-tuning procedure for further improvement, though alternative representation learning methods could achieve similar results
**Low Confidence:** Linear composability for analogy problems, given the narrow experimental scope and lack of comparison to nonlinear methods

## Next Checks

1. **Ablation of AIE ranking method:** Replicate head selection using integrated gradients or ablation studies to verify AIE uniquely identifies functional subnetworks
2. **Layer sensitivity across dataset scales:** Systematically vary dataset size and complexity to determine if optimal injection layers shift consistently with data characteristics
3. **Linear composability stress test:** Expand analogy experiments to 8-10 trained relations with 8-10 untrained relations and compare against nonlinear composition methods