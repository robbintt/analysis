---
ver: rpa2
title: Studying the Role of Synthetic Data for Machine Learning-based Wireless Networks
  Traffic Forecasting
arxiv_id: '2601.07646'
source_url: https://arxiv.org/abs/2601.07646
tags:
- data
- synthetic
- real
- network
- train
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Studying the Role of Synthetic Data for Machine Learning-based Wireless Networks Traffic Forecasting

## Quick Facts
- arXiv ID: 2601.07646
- Source URL: https://arxiv.org/abs/2601.07646
- Reference count: 40
- Key outcome: Synthetic-data-trained ML models improve prediction accuracy by up to 50% compared to real-data-trained baselines when generalization to unseen access points is required

## Executive Summary
This paper proposes a method for generating synthetic wireless network traffic data using a Gauss-Markov process seeded with statistical summaries from limited real measurements. The approach extracts hourly means and variances for each day-of-week combination from real AP data, then generates synthetic traces with AR(1) noise injection. Experimental results on the FLAG dataset show that models trained on synthetic data generalize better to unseen access points compared to those trained on real data, with synthetic LSTM models achieving up to 50% lower MAE for cross-AP prediction tasks.

## Method Summary
The method extracts hourly statistical descriptors (means and variances) from real AP measurements, creating 168 (24×7) seed values per AP. These compressed representations preserve weekly cyclical patterns while discarding instance-specific noise. Synthetic traffic is generated using a Gauss-Markov approach that combines a deterministic baseline (hourly statistics), independent Gaussian noise, and AR(1) noise (α=0.9) to capture temporal dependencies. The synthetic data is then used to train CNN and LSTM models for traffic forecasting, with evaluation showing improved generalization to unseen access points compared to real-data-trained models.

## Key Results
- Synthetic-data-trained LSTM models achieve up to 50% lower MAE than real-data-trained models when generalizing to unseen access points
- For cross-AP evaluation, synthetic LSTM models maintain <7% false positive rate while real-data-trained models exceed 19%
- Synthetic data generation requires minimum 7 days of seed data, with diminishing returns beyond 15 days
- CNN models require K≥20 synthetic APs for stable performance, while LSTM models require K≥50

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Limited real network measurements can serve as statistical anchors to generate larger synthetic datasets that preserve essential temporal patterns for ML training.
- **Mechanism**: Extract hourly means (µ) and variances (Var) for each combination of day-of-week and hour-of-day from real AP data, creating 168 (24×7) statistical descriptors. These compressed representations preserve weekly cyclical patterns while discarding instance-specific noise, then serve as the deterministic backbone for synthetic generation.
- **Core assumption**: Aggregated first and second-order statistics sufficiently capture the temporal structures needed for downstream traffic prediction, even when higher-order moments diverge.
- **Evidence anchors**:
  - [section III.A]: "we derive new time series M(k) = {(µ(k)w,h, Var(k)w,h) | w ∈ {Mon, Tue, …, Sun }, h ∈ { 0, 1, . . . ,23}} with statistical features extracted from the real dataset L(k)"
  - [section IV-C, Table I]: Weekly pattern correlations average ρp,µ = 0.84 across 20 APs, demonstrating robust preservation of cyclical traffic behaviors despite distributional differences
  - [corpus]: Weak direct evidence—corpus papers focus on prediction model architectures rather than statistical seed extraction methods
- **Break condition**: When APs exhibit highly asymmetric or heavy-tailed distributions (e.g., AP 7-111 with ∆γ2 = 471.68 kurtosis difference), the seed statistics inadequately characterize traffic behavior, leading to poor synthetic fidelity.

### Mechanism 2
- **Claim**: First-order autoregressive (Gauss-Markov) noise injection preserves short-term temporal dependencies while introducing controlled variability that prevents overfitting.
- **Mechanism**: Synthetic traffic is computed as Ĺ(t) = max(0, g(M(t)) + Ngauss(t) + NAR(t)), where the AR(1) component NAR(t) = α·NAR(t-1) + W(t) with α=0.9 creates strong temporal memory. This captures traffic burstiness and load persistence inherent to multi-user wireless environments.
- **Core assumption**: Traffic temporal dependencies can be adequately modeled as a Markovian process where current variations depend primarily on the immediately preceding state.
- **Evidence anchors**:
  - [section III.B, Eq. 4-6]: "Gauss-Markov noise-based approach—a well-established approach for modeling stochastic processes with memory—that captures both temporal dependencies and realistic traffic variations"
  - [Figure 6]: Autocorrelation analysis shows synthetic data preserves lag-1 structure (∆AC1 = 0.21±0.09) and lag-6 patterns (∆AC6 = 0.31±0.15) reasonably close to real data
  - [corpus]: No direct corpus evidence—neighboring papers address prediction models, not AR-based synthetic generation
- **Break condition**: When temporal dependencies extend beyond first-order structure or exhibit non-Markovian long-range correlations, the AR(1) model underrepresents medium-term patterns (as evidenced by higher ∆AC6 variability).

### Mechanism 3
- **Claim**: Deliberate statistical divergence in higher-order moments (skewness, kurtosis) expands the training distribution's support, improving generalization to unseen access points.
- **Mechanism**: The combination of independent Gaussian noise and correlated AR noise creates controlled variability that generates traffic realizations beyond exact reproductions of observed data. This broader statistical support helps ML models learn more robust decision boundaries applicable to new deployments.
- **Core assumption**: Models trained on diverse (though less statistically faithful) data will generalize better than models overfitting to limited real observations.
- **Evidence anchors**:
  - [abstract]: "when generalization is required, synthetic-data-trained models improve prediction accuracy by up to 50% compared to real-data-trained baselines"
  - [section V-D, Figure 9]: For Φtest ≠ Φtrain with K=50 APs, synthetic LSTM models significantly outperform real-data baselines on unseen access points
  - [Table I]: Mean kurtosis difference ∆γ2 = 87.40 suggests controlled divergence that may benefit generalization; section IV-D explicitly links this to preventing overfitting
  - [corpus]: Limited corpus support—related work mentions GANs for synthetic data but not the fidelity-diversity tradeoff for generalization
- **Break condition**: For very challenging prediction tasks (low load threshold γ=0.01, long horizon s=6 steps), excessive variability can catastrophically increase error rates (up to 92.36% false negatives for CNN on unseen APs per Table IV).

## Foundational Learning

- **Concept: Time Series Forecasting with Sliding Windows**
  - Why needed here: The paper formulates traffic prediction as supervised learning where historical windows predict future steps; understanding this framing is essential for interpreting lookback (l) and step (s) parameters in experiments.
  - Quick check question: Given 30 days of 10-minute resolution traffic data, approximately how many training samples are generated with a lookback window of 6 steps and prediction horizon of 1 step?

- **Concept: Autoregressive Processes and Temporal Memory**
  - Why needed here: The Gauss-Markov generator's behavior depends critically on the AR coefficient α; understanding how α=0.9 creates strong temporal persistence vs. α→0 producing independent noise is fundamental to tuning the system.
  - Quick check question: If you observe that synthetic traffic lacks realistic burstiness, should you increase or decrease α? What tradeoff does this introduce?

- **Concept: Bias-Variance Tradeoff in Synthetic Data Design**
  - Why needed here: The paper argues that perfect statistical fidelity can cause overfitting; understanding why synthetic data should be "different enough" from real data requires grasping how training distribution diversity affects generalization.
  - Quick check question: A colleague argues synthetic data must match real data's kurtosis exactly. Based on Section IV-D, what counterargument would you present?

## Architecture Onboarding

- **Component map**:
  - Feature Extraction Module (Section III.A) -> Gauss-Markov Generator (Section III.B) -> ML Training Pipeline (Section III.C) -> Evaluation Framework (Section V)

- **Critical path**:
  1. Collect seed data: Minimum 7 days recommended; 15 days provides diminishing returns (Fig. 7)
  2. Extract statistics: Compute µ and Var for each (day-of-week, hour) combination per AP
  3. Generate synthetic traces: Use Eq. 4-6; ensure |DS| ≥ 60 days for best results
  4. Train ML model: Use K≥20 synthetic APs for CNN, K≥50 for LSTM (Fig. 8)
  5. Deploy: Same-AP evaluation for personalized models; different-AP for pre-trained general models

- **Design tradeoffs**:
  - **Seed data vs. synthetic quality**: More seed days improve MAE but gains saturate; 15 days approaches 30-day real baseline within 10-15% (abstract)
  - **Dataset volume (K) vs. architecture**: CNNs stabilize at K=20 APs; LSTMs require K=50 for equivalent performance (Section V-C)
  - **Fidelity vs. generalization**: High weekly correlations (ρ>0.8) preserved, but kurtosis divergence may aid generalization (Section IV-D)
  - **Model selection**: CNNs better for low-load detection (fewer false positives); LSTMs superior for unseen-AP generalization (Tables III-IV)

- **Failure signatures**:
  - **High false negatives (>90%)**: CNN on challenging tasks (γ=0.01, s=6) with unseen APs—indicates architecture mismatch for generalization requirements
  - **Synthetic underperforming real on same-AP**: Insufficient K (need K≥20 CNN, K≥50 LSTM) or |DS| too small
  - **Poor weekly correlation (ρp,µ<0.7)**: Specific APs (e.g., 7-323 in Table I) have irregular patterns unsuitable for this generator; consider alternative approaches or AP-specific tuning

- **First 3 experiments**:
  1. **Seed data sensitivity**: For a single AP, generate synthetic data from {1, 7, 15} days of real measurements; train CNN on 30 synthetic days; evaluate MAE on held-out real data. Expected outcome: MAE decreases with seed size, approaching but not matching 30-day real baseline.
  2. **Generalization stress test**: Train LSTM on synthetic data from K=50 APs (60 days each); evaluate on completely unseen APs; compare against real-data-trained model. Expected outcome: Synthetic model achieves 20-50% lower MAE on unseen APs (per abstract claims).
  3. **Binary detection calibration**: At γ=0.05 threshold, 6-step horizon, measure false positive/negative rates for CNN vs. LSTM on both same-AP and different-AP scenarios. Expected outcome: LSTM maintains <7% false positives on unseen APs while CNN exceeds 19% (Table III).

## Open Questions the Paper Calls Out
- **Question**: Does the proposed statistical aggregation method provide formal privacy guarantees against reconstruction or membership inference attacks?
  - **Basis in paper**: [explicit] The Conclusion lists "the study of the privacy-preserving capabilities of the synthetic data approach" as future work.
  - **Why unresolved**: While the paper argues that aggregating data into statistical descriptors enhances privacy, it does not empirically validate the method's resilience against adversarial attacks or compare it to formal differential privacy standards.
  - **What evidence would resolve it**: A security analysis using membership inference attacks to demonstrate that individual real data points cannot be extracted from the synthetic generator.

- **Question**: How do different neural network architectures interact with the controlled noise and diversity introduced by the Gauss-Markov synthetic generation process?
  - **Basis in paper**: [explicit] The authors explicitly include "the evaluation of synthetic data into different ML model architectures" as part of their future work agenda.
  - **Why unresolved**: The study observed divergent behaviors between CNNs and LSTMs (LSTMs generalized better), but the response of other architectures (e.g., Transformers, GNNs) to the specific noise profile of this generator remains unknown.
  - **What evidence would resolve it**: Benchmarking the generator using a broader set of state-of-the-art architectures to identify which structural features best exploit the synthetic data's properties.

- **Question**: What is the optimal trade-off between synthetic dataset volume and diversity to prevent performance degradation in specific models?
  - **Basis in paper**: [explicit] Section V-E states, "how much synthetic data must be used requires further analysis," noting that increasing data volume did not always improve performance.
  - **Why unresolved**: Experiments showed that larger synthetic datasets occasionally led to worse performance (e.g., in LSTMs), suggesting a complex, non-monotonic relationship between data scaling and model accuracy that the current study did not fully map.
  - **What evidence would resolve it**: A systematic ablation study analyzing model performance as a function of both synthetic dataset size (K) and noise variance to identify optimal scaling laws.

## Limitations
- The synthetic generator assumes Markovian temporal dependencies (AR(1)) which may not capture complex multi-user wireless dynamics with long-range correlations
- Performance degrades significantly on challenging tasks (γ=0.01, s=6) with false negatives exceeding 90% for CNN on unseen APs
- The FLAG dataset's specific characteristics (AP distribution, traffic patterns) may limit generalizability to other network environments
- No validation of synthetic data quality under different sampling rates or for traffic prediction tasks beyond load forecasting

## Confidence
- **High Confidence**: The core synthetic data generation mechanism (hourly statistical seeding + AR noise) is well-specified and demonstrably preserves weekly traffic patterns (ρp,µ = 0.84 average). The general trend of synthetic data improving generalization on unseen APs is supported by multiple experiments.
- **Medium Confidence**: The specific performance improvements (50% accuracy gain, MAE values) are context-dependent on FLAG dataset characteristics and the particular CNN/LSTM implementations. The claimed superiority of synthetic data for generalization relies on statistical divergence that may not transfer to radically different traffic distributions.
- **Low Confidence**: The exact architectural details of the CNN and LSTM models are missing, requiring assumptions from [21]. The binary detection thresholds (γ = 0.01, 0.05, 0.1) and their practical significance in real deployments are not thoroughly validated beyond the FLAG dataset.

## Next Checks
1. **Statistical fidelity audit**: For each AP in Table I, generate synthetic data and compute all reported statistics (means, variances, skew, kurtosis); verify that ρp,µ ≥ 0.7 and |Δγ2| aligns with reported values before ML training
2. **Cross-dataset generalization**: Train synthetic-data models on FLAG dataset, then evaluate on a different Wi-Fi traffic dataset (e.g., CRAWDAD or another university/campus network); measure MAE degradation compared to FLAG→FLAG transfer
3. **Architecture sensitivity test**: Implement 3 variants of CNN/LSTM (varying depth, width, dropout) per the assumed [21] baseline; quantify how sensitive the synthetic data benefits are to model architecture choices