---
ver: rpa2
title: 'Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins
  of Cognitive Biases in LLMs'
arxiv_id: '2507.07186'
source_url: https://arxiv.org/abs/2507.07186
tags:
- bias
- biases
- instruction
- 'true'
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) exhibit cognitive biases, but it was
  unclear whether these biases originate from pretraining, instruction finetuning,
  or training randomness. To disentangle these factors, we conducted a two-step causal
  experiment.
---

# Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs

## Quick Facts
- arXiv ID: 2507.07186
- Source URL: https://arxiv.org/abs/2507.07186
- Authors: Itay Itzhak; Yonatan Belinkov; Gabriel Stanovsky
- Reference count: 30
- Primary result: Pretraining is the dominant source of cognitive biases in LLMs, with instruction finetuning making only minor adjustments to underlying bias patterns

## Executive Summary
Large language models exhibit cognitive biases, but the source of these biases—whether from pretraining, instruction finetuning, or training randomness—was unclear. Through a two-step causal experiment, researchers found that training randomness introduces only minor variability in bias scores, while pretraining is the primary source of cognitive biases. Models with the same pretrained backbone exhibit more similar bias patterns than those sharing only finetuning data, even when cross-tuned with different instruction datasets.

## Method Summary
The study employed a two-step experimental design. First, models were fine-tuned with different random seeds to assess training randomness effects on over 30 cognitive biases. Second, cross-tuning was introduced—swapping instruction datasets between models—to isolate the effects of pretraining and instruction data. This created a 2×2 factorial design where each pretrained model underwent finetuning with both instruction datasets (Flan and Tulu-2), using 3 random seeds each. Bias evaluation used treatment/control prompt pairs, producing normalized bias scores that were aggregated into bias vectors for clustering analysis.

## Key Results
- Training randomness introduces only minor variability in bias scores, with aggregation across seeds recovering stable bias trends
- Models with the same pretrained backbone exhibit more similar bias patterns than those sharing only finetuning data
- Clustering analysis confirmed that models group more strongly by pretraining identity than by instruction dataset, even in community-finetuned models

## Why This Works (Mechanism)

### Mechanism 1: Pretraining Dominance in Bias Formation
Pretraining establishes latent bias patterns that persist through finetuning. When models with different pretraining but identical finetuning data are compared, they retain distinct bias patterns, indicating pretraining carries the dominant signal. Evidence shows models with the same pretrained backbone exhibit more similar bias patterns than those sharing only finetuning data.

### Mechanism 2: Cross-Tuning as Causal Isolation
The 2×2 factorial design where each pretrained model is finetuned on both instruction datasets isolates causal contributions. If bias patterns cluster by pretraining identity rather than instruction dataset, pretraining is the dominant causal factor. The study found pretraining-based clusters are significantly tighter and more well-separated than instruction-based ones.

### Mechanism 3: Training Randomness as Minor Perturbation
Random seed variation affects batch ordering and LoRA parameter initialization, causing fluctuations in observed bias scores. However, these perturb outputs without reshaping latent representations, so averaging across seeds recovers the pretraining-determined bias pattern. More than two-thirds of cases preserve the original bias direction or show only statistically insignificant variations.

## Foundational Learning

- **Cognitive Biases as Systematic Decision Deviations**: The paper evaluates 32 cognitive biases (framing effect, anchoring, belief bias, etc.) as primary dependent variables. Why needed: Understanding what these biases represent is essential for interpreting results. Quick check: Why does preferring a treatment described as "90% survival rate" over "10% mortality rate" indicate framing bias, given both statements are logically equivalent?

- **Factorial Causal Design**: Cross-tuning uses a 2×2 factorial structure to isolate causal contributions. Why needed: Without understanding this design, the logic of conclusions is opaque. Quick check: In a 2×2 design testing Pretraining (A vs. B) × Instruction Data (X vs. Y), what pattern of results would indicate pretraining is the dominant factor?

- **Clustering Quality Metrics**: The paper quantifies whether bias vectors cluster by pretraining or instruction data using Silhouette score, Calinski-Harabasz index, and Davies-Bouldin index. Why needed: These metrics determine whether pretraining or instruction data drives bias pattern similarity. Quick check: If pretraining-based clusters have higher Silhouette scores and Calinski-Harabasz indices than instruction-based clusters, what does this imply about bias sources?

## Architecture Onboarding

- **Component map**: Pretrained models (OLMo-7B, T5-11B) → Cross-tuning matrix (2×2×3 = 12 variants) → Bias evaluation pipeline → Bias vectors (32-dimensional) → Clustering analysis

- **Critical path**: 1) Select pretrained models with established bias pattern differences 2) Finetune each on both instruction datasets using 3 random seeds 3) Evaluate all 32 cognitive biases per model 4) Construct bias vectors and compute clustering metrics

- **Design tradeoffs**: LoRA vs. full finetuning (paper uses high-rank LoRA due to compute constraints, validates by achieving ≥85% of original finetuned model's MMLU improvement); Flan downsampling (reduced from 15M to 350K examples to match Tulu-2 size); seed count limit (3 seeds provides statistical signal)

- **Failure signatures**: LoRA models achieving <85% of original MMLU improvement; bias vectors showing no consistent clustering structure; cross-tuned models clustering perfectly by instruction data

- **First 3 experiments**: 1) Reproduce seed variance analysis: Finetune OLMo-7B on Tulu-2 with 3 random seeds; compute standard deviation across 32 bias scores 2) Single cross-tuning replication: Finetune T5-11B on Tulu-2; construct bias vector and compute cosine similarity to T5-Flan's bias vector 3) Clustering sanity check: Run K-Means (K=2) on all bias vectors; verify cluster assignments align with pretraining identity

## Open Questions the Paper Calls Out
None

## Limitations
- The study uses only two pretrained models (OLMo-7B and T5-11B) for cross-tuning experiments, limiting generalizability to other model families or scales
- Community-finetuned models were only included in clustering analysis, not in the controlled cross-tuning experiments
- The two instruction datasets (Flan and Tulu-2) may not capture the full diversity of finetuning approaches used in practice

## Confidence
- **High Confidence**: Pretraining is the dominant source of cognitive bias patterns
- **Medium Confidence**: Training randomness has minimal effect on bias patterns
- **Medium Confidence**: Cross-tuning effectively isolates pretraining and finetuning contributions

## Next Checks
1. Replicate cross-tuning with additional pretrained models (e.g., Llama, Mistral) to test whether pretraining dominance generalizes across model families
2. Extend seed analysis to 5-10 seeds per condition to better characterize variance bounds and test the stability of bias direction across training runs
3. Test cross-tuning with instruction datasets that differ more dramatically in structure (e.g., pure dialogue vs. task-oriented instruction) to assess sensitivity to finetuning content