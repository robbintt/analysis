---
ver: rpa2
title: 'One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate
  Cold-Starting Short CoT LLMs in RL'
arxiv_id: '2506.02338'
source_url: https://arxiv.org/abs/2506.02338
tags:
- reasoning
- long
- dataset
- thought
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the cold-start problem in training large reasoning
  models by developing a 100K-instance dataset of long chain-of-thought (CoT) rationales
  generated from short CoT LLMs. The authors introduce a step-by-step pipeline that
  first collects 1K demonstrations of o1's reasoning flow, then uses these as guidance
  to expand short CoT outputs into long, structured rationales with controllable thought
  budgets.
---

# One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL

## Quick Facts
- **arXiv ID:** 2506.02338
- **Source URL:** https://arxiv.org/abs/2506.02338
- **Reference count:** 37
- **Primary result:** 100K-instance dataset of long CoT rationales generated from short CoT LLMs improves reasoning performance and RL training efficiency

## Executive Summary
This paper addresses the cold-start problem in training large reasoning models by creating a dataset of long chain-of-thought (CoT) rationales generated from short CoT LLMs. The authors develop a step-by-step pipeline that first collects demonstrations of o1's reasoning flow, then uses these as guidance to expand short CoT outputs into long, structured rationales with controllable thought budgets. The dataset shows reasoning quality comparable to R1 with only slightly lower correctness. When used for training and RL fine-tuning, the dataset enables 2-3x larger performance gains compared to baselines while helping mitigate overthinking issues in long-sequence reasoning models.

## Method Summary
The authors introduce a three-stage pipeline to generate long CoT rationales from short-CoT models. First, they collect 1K demonstrations of o1's reasoning flow on selected problems. Second, they use these demonstrations as step-by-step guidance to expand short CoT outputs into long, structured rationales. Third, they implement a controllable thought budget mechanism to regulate the depth of reasoning steps. The dataset contains 100K instances covering GSM8K, MATH, LiveCodeBench, and AIME problems. Quality analyses show the generated long CoT rationales have reasoning flow and strategies comparable to R1, with only slight reductions in correctness. The dataset is then used to train models and initialize RL fine-tuning, demonstrating improved performance across reasoning benchmarks.

## Key Results
- Models trained on the generated long CoT dataset show improved performance on GSM8K, MATH, and LiveCodeBench benchmarks
- RL fine-tuning initialized with models trained on this dataset achieves 2-3x larger performance gains compared to baseline initialization
- The controllable thought budget mechanism effectively mitigates overthinking issues in long-sequence reasoning models

## Why This Works (Mechanism)
The dataset addresses the cold-start problem by providing high-quality long CoT rationales that bridge the gap between short-CoT model capabilities and the requirements for effective RL training. By using o1 demonstrations as guidance, the generated rationales capture sophisticated reasoning strategies and flows that would be difficult for short-CoT models to produce independently. The controllable thought budget allows fine-tuning of reasoning depth, preventing the overthinking that often plagues long-sequence reasoning models. This combination of high-quality guidance and controlled depth enables more effective learning during RL fine-tuning, resulting in the observed performance improvements.

## Foundational Learning
- **Chain-of-Thought Reasoning**: Step-by-step problem-solving approach where models generate intermediate reasoning steps - needed for understanding how models decompose complex problems; quick check: verify models can generate coherent intermediate steps
- **Cold-Start Problem in RL**: Difficulty in initializing RL training due to lack of quality demonstrations - needed to understand why standard pretraining fails for reasoning tasks; quick check: assess baseline RL performance without proper initialization
- **Thought Budget Control**: Mechanism to regulate reasoning depth and complexity - needed to prevent overthinking while maintaining solution quality; quick check: measure performance across different budget settings
- **Demonstration Distillation**: Process of extracting reasoning patterns from expert models - needed to capture sophisticated reasoning strategies; quick check: compare distilled patterns against original expert behavior
- **Reinforcement Learning Fine-tuning**: Training paradigm that optimizes models through reward signals - needed to understand how the dataset improves RL efficiency; quick check: measure reward signal quality during training

## Architecture Onboarding

**Component Map:**
Data Collection -> Long CoT Generation -> Thought Budget Control -> Model Training -> RL Fine-tuning

**Critical Path:**
Demonstration Collection (1K samples) → Long CoT Generation Pipeline → Dataset Creation (100K instances) → Model Training → RL Fine-tuning

**Design Tradeoffs:**
- Generated vs human-annotated data: Model-generated data enables large-scale creation but may miss nuanced reasoning strategies
- Thought budget parameter: Balances depth of reasoning against efficiency and overthinking risk
- Evaluation scope: Focused on math and coding tasks, potentially limiting generalizability to other reasoning domains

**Failure Signatures:**
- Inconsistent reasoning flow between generated long CoT and original short CoT
- Overthinking when thought budget is set too high
- Performance degradation on tasks outside training distribution

**First Experiments:**
1. Human evaluation of random samples from generated long CoT rationales to assess reasoning quality
2. Ablation study on thought budget parameter across different reasoning task complexities
3. Transfer learning evaluation on reasoning tasks outside math and coding domains

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset generation relies entirely on short-CoT models rather than human annotation, potentially missing high-level reasoning strategies
- Quality gap exists between generated long CoT and human-annotated R1 data, though small
- Evaluation focuses on math and coding benchmarks, leaving uncertainty about performance on other reasoning domains
- Thought budget control mechanism requires further validation across diverse task types

## Confidence
**Dataset Quality Claims (Medium):** The methodology is well-documented, but reliance on model-generated data introduces uncertainty about reasoning quality upper bounds.

**Performance Improvement Claims (High):** Experimental results on GSM8K, MATH, and LiveCodeBench are clearly presented with consistent improvements across multiple metrics.

**RL Fine-tuning Benefits (Medium):** The 2-3x improvement is impressive but limited to specific datasets, raising questions about generalizability to other reasoning tasks or RL objectives.

## Next Checks
1. Conduct human evaluation of a random sample of generated long CoT rationales to assess reasoning quality, correctness, and novelty compared to human-annotated data.

2. Test the dataset's effectiveness on reasoning tasks outside math and coding, such as commonsense reasoning or scientific question answering, to evaluate domain transfer.

3. Perform ablation studies on the thought budget parameter to determine optimal values across different reasoning task complexities and quantify the overthinking mitigation effect.