---
ver: rpa2
title: Deep Polynomial Chaos Expansion
arxiv_id: '2507.21273'
source_url: https://arxiv.org/abs/2507.21273
tags:
- deeppce
- input
- nodes
- product
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deep Polynomial Chaos Expansion (DeepPCE),
  a scalable extension of classical polynomial chaos expansion that addresses the
  curse of dimensionality in high-dimensional uncertainty quantification. DeepPCE
  combines PCE with deep probabilistic circuits, embedding orthonormal polynomial
  bases within a structured sum-product network architecture that can represent exponentially
  many orthogonal polynomial terms compactly.
---

# Deep Polynomial Chaos Expansion

## Quick Facts
- arXiv ID: 2507.21273
- Source URL: https://arxiv.org/abs/2507.21273
- Authors: Johannes Exenberger; Sascha Ranftl; Robert Peharz
- Reference count: 40
- Primary result: DeepPCE scales polynomial chaos expansion to high dimensions while maintaining exact uncertainty quantification

## Executive Summary
This paper introduces Deep Polynomial Chaos Expansion (DeepPCE), a scalable extension of classical polynomial chaos expansion that addresses the curse of dimensionality in high-dimensional uncertainty quantification. DeepPCE combines PCE with deep probabilistic circuits, embedding orthonormal polynomial bases within a structured sum-product network architecture that can represent exponentially many orthogonal polynomial terms compactly. The method maintains PCE's ability to compute expectations, variances, and higher-order statistical quantities analytically while scaling to high-dimensional problems where traditional PCE fails.

## Method Summary
DeepPCE is a scalable extension of polynomial chaos expansion (PCE) that addresses the curse of dimensionality by embedding orthonormal polynomial bases within a deep probabilistic circuit architecture. The key innovation is using tensorized PCE input nodes as the leaves of a smooth, decomposable circuit, enabling exact computation of statistical moments and Sobol sensitivity indices through simple forward passes. This approach combines the analytical uncertainty quantification capabilities of PCE with the representational power and scalability of sum-product networks, allowing the method to handle high-dimensional problems while maintaining the ability to compute expectations, variances, and Sobol indices analytically.

## Key Results
- DeepPCE achieves predictive performance comparable to multi-layer perceptrons on two high-dimensional PDE benchmarks (Darcy flow and steady-state diffusion)
- On a 100-dimensional synthetic function with analytic Sobol indices, DeepPCE performs on-par with shallow PCE variants but with orders of magnitude faster sensitivity analysis computation times
- The method provides a principled, fast mechanism for feature importance analysis and serves as a scalable, tractable alternative for surrogate modeling in science and engineering

## Why This Works (Mechanism)
DeepPCE works by leveraging the structure of sum-product networks to represent the exponential growth of polynomial terms in a compact, tractable form. The tensorized PCE input nodes serve as leaves in a smooth, decomposable circuit, where sum nodes represent mixture distributions and product nodes represent factorized distributions. This architecture allows for exact computation of statistical moments and Sobol sensitivity indices through forward passes, as the network maintains the probabilistic semantics required for analytical uncertainty quantification. The smooth decomposability ensures that the circuit can be evaluated efficiently while preserving the orthonormality of the polynomial basis functions.

## Foundational Learning

**Polynomial Chaos Expansion (PCE)**: A spectral method for uncertainty quantification that represents random variables as series expansions of orthogonal polynomials. Why needed: Provides the mathematical foundation for representing stochastic processes in uncertainty quantification. Quick check: Verify that the orthonormal polynomial basis functions are properly defined for the input distribution.

**Sum-Product Networks (SPNs)**: Probabilistic circuits that represent complex distributions through a hierarchy of sum and product operations. Why needed: Enables compact representation of exponentially many polynomial terms while maintaining tractable inference. Quick check: Confirm that the SPN structure is smooth and decomposable to ensure valid probabilistic semantics.

**Sobol Sensitivity Indices**: Variance-based measures of sensitivity that quantify the contribution of input variables to output uncertainty. Why needed: Essential for feature importance analysis in uncertainty quantification. Quick check: Validate that the computed Sobol indices match analytical values for benchmark problems.

**Orthonormal Polynomial Bases**: Sets of polynomials that are orthogonal with respect to a specific probability distribution. Why needed: Ensures that the PCE expansion coefficients can be computed efficiently and that statistical moments have closed-form expressions. Quick check: Verify that the polynomial basis functions satisfy the orthonormality condition for the given input distribution.

## Architecture Onboarding

**Component Map**: Input distributions → Tensorized PCE nodes → Sum nodes (mixture) → Product nodes (factorization) → Output

**Critical Path**: The forward pass through the network computes the PCE expansion, where each layer combines polynomial terms through sum and product operations. The critical path involves evaluating the tensorized input nodes, propagating through the sum and product layers, and computing the final output distribution.

**Design Tradeoffs**: DeepPCE trades off the interpretability and analytical guarantees of shallow PCE for scalability to high dimensions. While shallow PCE provides exact uncertainty quantification with clear polynomial terms, it suffers from exponential growth in the number of terms. DeepPCE addresses this by using a compact SPN representation, but this may reduce interpretability and introduce potential issues with training stability.

**Failure Signatures**: Poor performance may indicate issues with the tensorization of input nodes, problems with the smooth decomposability of the circuit, or insufficient training data to learn the complex relationships in high-dimensional spaces. Vanishing or exploding gradients during training could also indicate architectural issues.

**First Experiments**:
1. Verify orthonormality of polynomial basis functions for different input distributions
2. Test sensitivity analysis computation times on increasing dimensionalities
3. Compare Sobol index accuracy against analytical solutions for synthetic benchmark functions

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation is limited to two PDE benchmarks and one synthetic function, which may not represent the full diversity of uncertainty quantification problems
- The paper lacks comparisons with other modern UQ methods beyond shallow PCE and MLPs
- Potential issues with training stability and hyperparameter sensitivity are not addressed
- The claim about maintaining PCE's analytical computation of statistical moments needs more rigorous verification across different distribution types and higher-dimensional settings

## Confidence

| Claim | Confidence |
|-------|------------|
| DeepPCE scales PCE to high dimensions while maintaining exact uncertainty quantification | High |
| DeepPCE achieves comparable performance to MLPs on PDE benchmarks | Medium |
| DeepPCE computes Sobol indices orders of magnitude faster than shallow PCE | High |
| The method provides a principled mechanism for feature importance analysis | High |

## Next Checks

1. Test DeepPCE on additional benchmark problems with varying dimensionalities and distribution types to verify robustness across different uncertainty quantification scenarios
2. Conduct ablation studies to quantify the impact of the tensorized input structure and smooth decomposability on computational efficiency and accuracy
3. Compare DeepPCE's performance against modern UQ methods like deep Gaussian processes or variational autoencoders for uncertainty quantification in high-dimensional settings