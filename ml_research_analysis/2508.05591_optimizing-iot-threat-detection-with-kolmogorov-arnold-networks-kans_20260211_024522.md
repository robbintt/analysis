---
ver: rpa2
title: Optimizing IoT Threat Detection with Kolmogorov-Arnold Networks (KANs)
arxiv_id: '2508.05591'
source_url: https://arxiv.org/abs/2508.05591
tags:
- kans
- training
- detection
- security
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated machine learning models for IoT intrusion
  detection, comparing traditional methods with Kolmogorov-Arnold Networks (KANs).
  KANs, which use learnable spline-based activation functions, outperformed standard
  MLPs and matched the accuracy of Random Forest and XGBoost while offering superior
  interpretability.
---

# Optimizing IoT Threat Detection with Kolmogorov-Arnold Networks (KANs)

## Quick Facts
- **arXiv ID**: 2508.05591
- **Source URL**: https://arxiv.org/abs/2508.05591
- **Reference count**: 31
- **Primary result**: KANs match RF/XGBoost accuracy while offering superior interpretability for IoT intrusion detection

## Executive Summary
This study evaluates machine learning models for IoT intrusion detection, comparing traditional methods with Kolmogorov-Arnold Networks (KANs). KANs, which use learnable spline-based activation functions, outperformed standard MLPs and matched the accuracy of Random Forest and XGBoost while offering superior interpretability. The KAN model achieved near-perfect precision (1.00) and recall (0.99) for malicious traffic and strong overall F1-scores (0.99), though training time was substantially higher. Feature selection improved computational efficiency but reduced detection reliability. Random Forest and XGBoost remain top choices for real-time deployment due to speed and accuracy, while KANs excel in transparency-critical applications. The research highlights KANs' potential for explainable IoT security, especially in scenarios requiring auditable decision-making.

## Method Summary
The study uses the CIC IoT 2023 dataset with 1,048,575 samples and 47 features to evaluate KANs for binary classification of IoT network traffic (Benign vs. Malicious). Data is split 70/15/15 for train/validation/test. KANs employ learnable B-spline activation functions on network edges, with architecture [input_dim, 16, 8, 2]. Training uses Adam optimizer, CrossEntropyLoss, 20 epochs, batch size 128, and StandardScaler preprocessing. Feature selection via Random Forest importance reduces features from 46 to 10. The study compares KAN against MLPs, Random Forest, XGBoost, and other baselines using precision, recall, and F1-score metrics.

## Key Results
- KANs achieved precision of 1.00 and recall of 0.99 for malicious traffic with full features
- Feature selection reduced training time from 7 hours to 14 minutes but dropped malicious recall to 0.48
- Random Forest and XGBoost outperformed KAN on training speed while maintaining similar accuracy
- KANs generated interpretable symbolic formulas explaining classification decisions
- MLP baseline achieved F1=0.88 vs KAN's 0.82 on malicious traffic with full features

## Why This Works (Mechanism)

### Mechanism 1: Learnable Spline-Based Activation Functions on Network Edges
KANs replace fixed node activations with learnable spline functions on network edges, enabling dynamic shaping of activation responses per feature. Each edge applies a univariate spline function s(t) = ΣcᵢBᵢ(t) where coefficients cᵢ are learned during training. This allows the network to adapt to complex, non-linear IoT traffic patterns rather than applying uniform transformations like ReLU across all inputs.

### Mechanism 2: Symbolic Formula Extraction for Intrinsic Interpretability
KANs generate human-readable symbolic formulas that explain classification decisions without requiring external explainability methods. After training, learned spline functions can be approximated as symbolic mathematical expressions combining linear, trigonometric, and polynomial terms. This enables security analysts to understand attack signatures and tune detection rules directly from the model's learned representations.

### Mechanism 3: Feature Selection Efficiency-Detection Trade-off
Reducing features via Random Forest importance ranking dramatically lowers training time but degrades malicious traffic recall. The study shows training on top 10 features reduces time by 97% but drops malicious recall from 1.00 to 0.48, indicating critical attack indicators exist in excluded features. This reveals a tension between computational efficiency and detection reliability.

## Foundational Learning

- **Concept: Spline Functions and Basis Expansion**
  - Why needed here: KANs use B-spline basis functions to represent learnable activations. Understanding piecewise polynomial interpolation is essential to diagnose why KANs capture non-linearity better than fixed activations.
  - Quick check question: Can you explain why a cubic spline with k control points can approximate a smooth function more flexibly than a single polynomial of the same degree?

- **Concept: Bias-Variance Trade-off in Intrusion Detection**
  - Why needed here: The paper shows KANs achieve high precision but variable recall depending on feature count. High precision is critical for operational IDS to avoid alert fatigue, but recall gaps mean missed attacks.
  - Quick check question: In a security context, would you prioritize precision or recall for a system detecting life-critical infrastructure attacks? How does this differ from spam filtering?

- **Concept: Kolmogorov-Arnold Representation Theorem**
  - Why needed here: The theoretical justification for KANs rests on this theorem—that any continuous multivariate function can decompose into sums of univariate functions. This explains why edge-based (univariate) splines can theoretically replace node-based (multivariate) transformations.
  - Quick check question: The theorem guarantees existence of such a decomposition, but does it guarantee learnability from finite data? What constraints might arise in practice?

## Architecture Onboarding

- **Component map**: Input Layer (46/47 features) -> Hidden Layers [16, 8] neurons -> Output Layer (2 neurons) -> Edge Functions (B-splines with learnable coefficients) -> Node Operation (summation of incoming edge outputs)

- **Critical path**:
  1. Data preprocessing: StandardScaler normalization (zero mean, unit variance)
  2. Feature selection (optional): Random Forest importance → top N features
  3. Tensor conversion for PyTorch
  4. KAN training: Adam optimizer, CrossEntropyLoss, 20 epochs, batch size 128
  5. Evaluation: precision/recall/F1 per class, training time, prediction time
  6. Symbolic extraction: auto-generated formula for interpretability

- **Design tradeoffs**:
  - Full features (46) → high accuracy (F1=0.82 malicious), long training (7h), strong interpretability
  - Top 10 features → fast training (14 min), poor malicious recall (0.48), reduced formula complexity
  - Random Forest/XGBoost → best speed/accuracy balance (F1=0.94 malicious, train in seconds), no symbolic interpretability

- **Failure signatures**:
  - Malicious traffic recall drops sharply with feature reduction → critical attack indicators lost
  - Training time scales poorly with feature count → not viable for real-time model updates on edge devices
  - Benign traffic precision at 0.70 (full features) → 30% false positives on benign traffic; may cause alert fatigue

- **First 3 experiments**:
  1. Baseline comparison: Replicate the paper's MLP vs. KAN vs. Random Forest comparison on CIC IoT 2023 with full features. Verify reported F1-scores and training times on your hardware.
  2. Feature ablation study: Instead of Random Forest importance, use mutual information or SHAP-based ranking to select features for KAN. Test if recall degradation is mitigated with KAN-aware selection.
  3. Interpretability stress test: Generate symbolic formulas for correctly classified and misclassified samples. Determine if formula complexity correlates with error types, and whether formulas reveal actionable attack patterns.

## Open Questions the Paper Calls Out
- Can hybrid architectures combining KAN symbolic layers with efficient detection backends successfully balance interpretability with real-time performance requirements?
- To what extent can hardware acceleration (GPUs) and optimization techniques reduce KAN training time for large-scale IoT deployment?
- Can federated learning be effectively integrated with KANs to enable collaborative training across distributed IoT networks while preserving privacy?

## Limitations
- The study lacks ablation experiments comparing KAN against other intrinsically interpretable models like decision trees or linear models with feature importance.
- Feature selection methodology relies on Random Forest importance without KAN-specific optimization, potentially missing spline-relevant features.
- Training time (7 hours for KAN vs. seconds for RF/XGBoost) is not benchmarked against model complexity analysis, making it difficult to determine if runtime is architecture-dependent.

## Confidence

**High Confidence**: KAN achieves comparable accuracy to RF/XGBoost on full feature sets; KAN generates symbolic formulas from learned spline activations; feature reduction improves speed but harms malicious traffic detection.

**Medium Confidence**: KAN's spline-based activations provide superior interpretability compared to tree ensembles; the recall drop with feature selection is unavoidable with current methodology.

**Low Confidence**: KAN is the optimal choice for real-time IoT IDS deployment; symbolic formulas are immediately actionable for security analysts without additional domain expertise.

## Next Checks
1. **Cross-Model Interpretability Comparison**: Implement and compare KAN against decision tree ensembles and linear models with LIME/SHAP on the same dataset. Quantify whether KAN's symbolic formulas are more interpretable or actionable than post-hoc explanations from black-box models.
2. **KAN-Specific Feature Selection**: Develop a feature selection method that optimizes for KAN's spline activations (e.g., using spline sensitivity or mutual information with spline-based reconstructions) rather than tree importance. Test if recall degradation can be mitigated while maintaining computational gains.
3. **Formula Actionability Audit**: Generate symbolic formulas for correctly classified and misclassified samples. Conduct a user study with security analysts to assess whether formulas reveal attack patterns and whether analysts can derive new detection rules from them.