---
ver: rpa2
title: Online Learning-guided Learning Rate Adaptation via Gradient Alignment
arxiv_id: '2506.08419'
source_url: https://arxiv.org/abs/2506.08419
tags:
- learning
- rate
- page
- adam
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GALA, a framework that adapts the learning
  rate in SGD and Adam by tracking the alignment between consecutive gradients and
  using local curvature estimates. The method formulates learning rate selection as
  an online learning problem and solves it using algorithms like Follow-the-Regularized-Leader.
---

# Online Learning-guided Learning Rate Adaptation via Gradient Alignment

## Quick Facts
- arXiv ID: 2506.08419
- Source URL: https://arxiv.org/abs/2506.08419
- Reference count: 40
- This paper introduces GALA, a framework that adapts the learning rate in SGD and Adam by tracking the alignment between consecutive gradients and using local curvature estimates.

## Executive Summary
GALA is a learning rate adaptation framework that treats learning rate selection as an online learning problem. It tracks gradient alignment between consecutive steps and estimates local curvature to automatically adjust learning rates, increasing them in stable regions and decreasing them in volatile ones. The method uses Follow-the-Regularized-Leader (FTRL) to solve this online learning problem and can be applied to both SGD and Adam optimizers.

## Method Summary
GALA adapts the learning rate by tracking the inner product between gradients evaluated at the current point and an interpolated point, using this as a signal for alignment. When gradients are aligned, learning rates increase; when misaligned, they decrease. Local curvature is estimated via differences between gradients at nearby points, providing stability. The framework reformulates learning rate selection as an online convex optimization problem and solves it using FTRL with a quadratic surrogate loss. The practical implementation uses a heuristic that approximates the theoretical sampling requirements for computational efficiency.

## Key Results
- GALA improves robustness across a wide range of initial learning rates while maintaining competitive performance
- Achieves O(σ^(1/2)/T^(1/4) + 1/√T) convergence rate, matching best known rates for constant learning rate methods
- On CIFAR-10/100 and Flower102, GALA improves final accuracy and reduces sensitivity to initial learning rate choice
- Outperforms standard decaying schedules and parameter-free methods in terms of robustness to initial learning rate

## Why This Works (Mechanism)

### Mechanism 1: Gradient Alignment-Based Step Size Adjustment
GALA increases learning rates when consecutive gradients are aligned and decreases them when misaligned. The surrogate loss function uses the inner product between gradients evaluated at two points as the alignment signal. Positive alignment (gradients pointing in similar directions) encourages larger steps; negative alignment triggers reduction. This works because alignment directly governs function value decrease.

### Mechanism 2: Local Curvature Regularization via Lipschitz Estimation
Local curvature estimates prevent learning rate explosions by penalizing aggressive steps in high-curvature regions. The method computes Lₜ as a local Lipschitz estimate and uses it in a quadratic regularization term that acts as curvature-aware regularization. This automatically reduces step sizes when gradients change rapidly between nearby points.

### Mechanism 3: Online Learning Formulation with FTRL
By reformulating learning rate selection as a one-dimensional online learning problem, GALA achieves principled adaptivity with provable regret bounds. FTRL is applied to cumulative surrogate losses, yielding a closed-form update that emerges from solving argmin over quadratic losses plus regularization. Optimistic FTRL uses next-step hints for improved regret.

## Foundational Learning

- **Concept: Online Convex Optimization and Regret**
  - Why needed here: GALA's core innovation is treating learning rate selection as an online problem; you must understand regret to grasp why sublinear regret enables convergence without knowing optimal η upfront.
  - Quick check question: If an online learner achieves O(√T) regret against comparator η*, what does this imply about its average performance over T rounds?

- **Concept: Nonconvex Stochastic Optimization Convergence Rates**
  - Why needed here: The paper proves O(σ^(1/2)/T^(1/4) + 1/√T) convergence; understanding why this matches best known rates for constant-LR SGD contextualizes the contribution.
  - Quick check question: Why does normalized SGD (update: xₜ₊₁ = xₜ − ηₜ mₜ/∥mₜ∥) simplify convergence analysis compared to unnormalized SGD?

- **Concept: Gradient Lipschitz Continuity and Curvature**
  - Why needed here: Lₜ estimation relies on the gradient being Lipschitz continuous; understanding this assumption clarifies when local estimates are valid.
  - Quick check question: For a smooth nonconvex function, how does local curvature (characterized by Hessian eigenvalues) relate to the Lipschitz constant L of the gradient?

## Architecture Onboarding

- **Component map:**
  Main Training Loop -> Base Optimizer (SGD or Adam) -> Standard parameter update: xₜ₊₁ = xₜ − ηₜ · directionₜ
  Main Training Loop -> GALA Learning Rate Module -> Gradient Sampler (requires 2 forward-backward passes per step)
  GALA Learning Rate Module -> Alignment Computer: inner_prod = ⟨g′ₜ(wₜ), gₜ(xₜ)⟩
  GALA Learning Rate Module -> Curvature Estimator: Lₜ = ∥g′ₜ(wₜ) − g′ₜ(xₜ)∥ / ∥wₜ − xₜ∥
  GALA Learning Rate Module -> FTRL Updater: ηₜ₊₁ = clip(cumulative_alignment / (δ + cumulative_curvature_penalty))

- **Critical path:**
  1. Initialize η₀ (paper uses 10⁻⁵ to 10⁰ range in experiments)
  2. For each iteration t:
     - Sample ξₜ, compute gₜ(xₜ), update xₜ₊₁ with current ηₜ
     - Sample ξ′ₜ (independent), sample sₜ ~ Uniform[0,1], compute wₜ
     - Evaluate g′ₜ(wₜ) and g′ₜ(xₜ) at same batch ξ′ₜ for curvature estimate
     - Update cumulative alignment and curvature sums
     - Compute ηₜ₊₁ via FTRL formula
  3. Monitor: learning rate trajectory, gradient alignment statistics, final loss/accuracy

- **Design tradeoffs:**
  - Extra gradient computation vs. adaptivity: Heuristic implementation sets wₜ = xₜ₊₁ and reuses next batch's gradient, reducing overhead but sacrificing theoretical guarantees
  - Maximum LR clipping (η_max): Prevents unbounded growth but introduces hyperparameter; paper uses η_max ≈ √α · η̄ but empirically tests wide ranges
  - Stability parameter δ: Controls regularization strength; small δ enables faster adaptation but risks instability; paper uses δ = 10⁻⁸ for Adam
  - Normalized vs. standard SGD: Normalization simplifies theory but may amplify noise; momentum parameter α ∈ (0,1] controls smoothing

- **Failure signatures:**
  - Learning rate collapse to zero: Cumulative alignment negative or curvature term dominating; check gradient variance and alignment statistics
  - Divergence with large initial η₀: η_max too high or Lₜ estimates unreliable; examine curvature estimates Lₜ for outliers
  - No adaptation (ηₜ ≈ constant): Alignment signal weak; verify gradient sampling uses independent batches ξₜ ≠ ξ′ₜ
  - Oscillating ηₜ: Hint function mismatch in optimistic FTRL; try standard FTRL or increase δ

- **First 3 experiments:**
  1. Reproduce robustness sweep on CIFAR-10 with ResNet-18: Run SGD-GALA and Adam-GALA with initial learning rates [1, 0.1, 0.01, 0.001, 0.0001, 10⁻⁵, 10⁻⁸]; plot final training loss vs. η₀ to verify baseline robustness
  2. Ablate gradient sampling strategy: Compare full Algorithm 1 with random interpolation wₜ = xₜ + sₜ(xₜ₊₁−xₜ), heuristic wₜ = xₜ₊₁ with next-batch gradient, and same-batch gradients; measure performance gap on CIFAR-100
  3. Stress-test curvature estimation under label noise: Inject synthetic label noise (10-30%) into CIFAR-10 and track Lₜ variance and learning rate trajectories; identify noise thresholds where GALA benefits degrade

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the GALA framework be theoretically and empirically extended to a broader class of optimizers beyond SGD and Adam?
- Basis in paper: [explicit] The conclusion states: "One potential limitation of our work is that the convergence analysis is established for one instantiation of GALA and our experiments focus on its integration with SGD and Adam. An interesting future venue is to extend our framework to a broader class of optimizers."
- Why unresolved: The current theoretical analysis and empirical validation are restricted to SGD and Adam, leaving the adaptability of the gradient alignment signal for other adaptive methods or second-order optimizers unexplored.
- What evidence would resolve it: Convergence guarantees for GALA applied to methods like RMSProp or AdaGrad, alongside empirical benchmarks showing performance improvements on those baselines.

### Open Question 2
- Question: Does the heuristic implementation of GALA retain the theoretical convergence guarantees derived for normalized SGD?
- Basis in paper: [inferred] Theorem 4.1 establishes convergence for a normalized SGD variant, but Section 5.2 describes a heuristic implementation for standard SGD that omits clipping and uses a specific sampling strategy.
- Why unresolved: There is a gap between the algorithm analyzed theoretically and the practical implementation, creating uncertainty about whether the theoretical bounds hold for the practical version.
- What evidence would resolve it: A formal proof extending Theorem 4.1 to the non-normalized heuristic update, or empirical studies showing the heuristic violates the theoretical bounds under specific conditions.

### Open Question 3
- Question: How does GALA perform on large-scale language models or architectures with distinct loss landscapes compared to the ResNet image classification tasks tested?
- Basis in paper: [inferred] Section 5.1 limits the experimental setup to ResNet-18 on image classification datasets.
- Why unresolved: The paper does not verify if the gradient alignment signal and local curvature estimates generalize to the noisy, high-dimensional landscapes of Large Language Models or transformer architectures.
- What evidence would resolve it: Empirical results from training LLMs using GALA, demonstrating robustness to initial learning rates comparable to the image classification results.

## Limitations
- The theoretical analysis relies on idealized assumptions that may not hold in practice
- The heuristic implementation sacrifices theoretical guarantees for computational efficiency
- Local curvature estimation can be noisy with high-variance gradients, potentially destabilizing learning rate adaptation
- FTRL regret analysis relies on specific quadratic loss structure that may not hold with approximation errors

## Confidence
- **High confidence**: The gradient alignment mechanism works as described for reducing learning rates in volatile regions
- **Medium confidence**: The online learning formulation provides theoretical justification, but the practical heuristic implementation may not achieve the claimed regret bounds
- **Low confidence**: The curvature estimation mechanism provides reliable stability across all neural network architectures and optimization landscapes

## Next Checks
1. Ablate the gradient sampling strategy: Compare full Algorithm 1 with random interpolation against the heuristic on CIFAR-100; measure the performance gap to quantify cost of practical shortcuts

2. Stress-test curvature estimation under label noise: Inject synthetic label noise (10-30%) into CIFAR-10 and track Lₜ variance and learning rate trajectories; identify noise thresholds where GALA benefits degrade

3. Validate gradient alignment signal quality: Compute and visualize ⟨∇f(xₜ₊₁; ξₜ₊₁), ∇f(xₜ; ξₜ)⟩ distributions across different learning rate regimes on CIFAR-10; verify that positive alignment correlates with successful optimization steps and negative alignment with instability