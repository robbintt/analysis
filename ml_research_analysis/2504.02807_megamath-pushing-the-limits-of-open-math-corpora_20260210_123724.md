---
ver: rpa2
title: 'MegaMath: Pushing the Limits of Open Math Corpora'
arxiv_id: '2504.02807'
source_url: https://arxiv.org/abs/2504.02807
tags:
- data
- code
- math
- text
- megamath
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MegaMath, a large-scale open-source math
  pre-training dataset comprising 371B tokens from web, code, and synthetic sources.
  The authors developed optimized data curation pipelines including improved math-focused
  HTML extraction, fastText-based filtering, and two-stage processing to enhance quality
  while maintaining scale.
---

# MegaMath: Pushing the Limits of Open Math Corpora

## Quick Facts
- arXiv ID: 2504.02807
- Source URL: https://arxiv.org/abs/2504.02807
- Reference count: 40
- Introduces MegaMath, a 371B-token open-source math pre-training dataset achieving 15-20% CoT performance improvement

## Executive Summary
This paper introduces MegaMath, the largest open-source math pre-training dataset comprising 371B tokens from web, code, and synthetic sources. The authors developed optimized data curation pipelines including improved math-focused HTML extraction, fastText-based filtering, and two-stage processing to enhance quality while maintaining scale. Extensive ablation studies validated each design choice, showing that MegaMath-Web-Pro subset outperforms existing math corpora by 4%+ in downstream benchmarks. Training on Llama-3.2 models demonstrates 15-20% CoT performance improvement, with MegaMath-Llama-3.2-3B achieving 56.2% on GSM8K and 25.1% on MATH, establishing MegaMath as the largest and highest-quality open math dataset to date.

## Method Summary
The MegaMath pipeline processes 99 Common Crawl snapshots through a two-stage extraction system: fast Resiliparse for initial filtering followed by clean trafilatura extraction. Math optimization converts MathML/KaTeX to LaTeX before extraction to preserve equations. A two-stage fastText classifier with balanced CC sampling and CoT seeds filters math-relevant documents. MinHash LSH (r=11, b=10, t=0.75) deduplicates content. Code data comes from Stack-V2 filtered through SLM recall, while synthetic data is generated via LLMs with AST/runtime verification. The final dataset contains 279B web tokens, 28.1B code tokens, and 64.5B synthetic tokens.

## Key Results
- MegaMath-Web-Pro subset outperforms FineMath-4+ by 4%+ on Core benchmarks
- MegaMath-Llama-3.2-3B achieves 56.2% on GSM8K and 25.1% on MATH
- 15-20% CoT performance improvement over baseline models
- text & code block synthetic data achieves 22.5 CoT Avg vs 18.8 raw code

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Math-optimized HTML extraction preserves equation fidelity and improves downstream performance
- Mechanism: Converting MathML/KaTeX to LaTeX before extraction prevents equation stripping; two-stage extraction (fast Resiliparse → clean trafilatura) balances efficiency and quality
- Core assumption: Mathematical content in HTML is systematically lost by generic extractors
- Evidence anchors:
  - [abstract]: "math-oriented HTML optimizations, fasttext-based filtering and deduplication"
  - [section 2.1.2]: "these extractors often fail to preserve math symbols and equations, even omitting them entirely"
  - [Table 2]: trafilatura + HTML optimization achieves 23.8 Core Avg vs 22.0 without optimization
- Break condition: If your HTML sources use image-based equations or non-standard math markup, this pipeline may not preserve content

### Mechanism 2
- Claim: Two-stage filtering with distribution-aware fastText training improves math document recall
- Mechanism: First-stage fastText filters candidates; second-stage fastText is retrained on LLM-annotated samples from all CC dumps to reduce distribution shift
- Core assumption: Math-relatedness is learnable from n-gram features and doesn't require semantic understanding
- Evidence anchors:
  - [abstract]: "fastText-based filtering"
  - [section 2.1.3]: "we thus re-trained fastText with LLM-annotated math-related documents from all dumps as the positive seed data"
  - [Figure 6]: V2 (Balance + CoT data) outperforms V1 consistently across years
- Break condition: If target math documents use vocabulary outside training distribution (e.g., specialized notation), classifier may have low recall

### Mechanism 3
- Claim: Synthetic interleaved text-code blocks substantially improve both CoT and PAL performance
- Mechanism: Generating executable code blocks with verification (syntax + runtime) creates training data that links mathematical reasoning to executable solutions
- Core assumption: Code execution feedback provides a supervisory signal for mathematical reasoning
- Evidence anchors:
  - [abstract]: "synthesized QA-style text, math-related code, and interleaved text-code blocks"
  - [Table 7]: text & code block achieves 22.5 CoT Avg vs 18.8 raw code; 28.1 PAL Avg vs 19.5 raw code
  - [corpus]: Nemotron-CC-Math (arXiv:2508.15096) reports similar benefits from structured math-code data
- Break condition: If generated code contains subtle logical errors that pass runtime verification, model may learn incorrect reasoning patterns

## Foundational Learning

- Concept: **HTML DOM manipulation and text extraction**
  - Why needed here: Core to MegaMath's pipeline; requires understanding how extractors handle structured markup
  - Quick check question: Can you explain why Resiliparse might preserve different content than trafilatura on the same HTML?

- Concept: **fastText text classification**
  - Why needed here: Enables scalable filtering of math documents; requires understanding n-gram features and training strategies
  - Quick check question: How would you detect if your fastText classifier is overfitting to training distribution?

- Concept: **MinHash LSH for deduplication**
  - Why needed here: Reduces redundant data; requires understanding Jaccard similarity and parameter tuning (b, r thresholds)
  - Quick check question: Given parameters (r=11, b=10, t=0.75), what happens to document retention if you increase threshold to 0.80?

## Architecture Onboarding

- Component map:
  Web pipeline: Common Crawl → URL filter → HTML optimization → 2-stage extraction → fastText filter → MinHash dedup → MegaMath-Web
  Code pipeline: Stack-V2 → language filter → SLM recall (fine-tuned Qwen-2.5-0.5B) → MegaMath-Code
  Synthetic pipeline: Web/code source → LLM generation (Qwen-2.5-72B, Llama-3.3-70B) → AST/runtime verification → MegaMath-Synth

- Critical path: HTML optimization → fastText training data quality → deduplication parameters. Errors in early stages compound (lost equations can't be recovered; poor filtering wastes compute on irrelevant data)

- Design tradeoffs:
  - Speed vs quality: Two-stage extraction trades compute for cleaner output
  - Scale vs precision: Stricter filtering (S_edu≥4, S_math≥4) improves PAL but reduces data volume
  - Code ratio: >20% code hurts CoT performance (Table 5: 33.3% mix drops to 16.4 CoT Avg)

- Failure signatures:
  - Low CoT performance + high PAL: Too much code, not enough text diversity
  - Poor performance on recent years: fastText not retrained on distribution shift
  - Equation corruption in output: HTML optimization not applied or MathML not converted properly

- First 3 experiments:
  1. Validate HTML extraction: Process 1000 math-heavy pages with/without HTML optimization, manually inspect equation preservation rate
  2. Test fastText recall: Sample 500 documents from held-out CC dump, annotate math relevance, measure precision/recall at different score thresholds
  3. Code mixing ablation: Train proxy model (TinyLlama-1B) with 5B tokens at 3 code ratios (12.5%, 20%, 33%), evaluate on Core benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-grained deduplication techniques (e.g., sentence-level or exact substring) be adapted to preserve the integrity of mathematical expressions while reducing redundancy?
- Basis in paper: [explicit] Appendix B.1 notes that while the authors experimented with exact substring and sentence-level deduplication, these methods "disrupted text consistency" and "identified many math expressions," ultimately degrading downstream performance. The authors state, "We thus leave this for future exploration."
- Why unresolved: Standard aggressive deduplication methods fragment the contiguous structure of mathematical notation (like LaTeX), harming model learning.
- What evidence would resolve it: A new deduplication algorithm that reduces dataset size (token count) while matching or exceeding the downstream performance of the current document-level MinHash approach.

### Open Question 2
- Question: What is the optimal data mixture ratio for integrating code data to maximize both Chain-of-Thought (CoT) reasoning and Program-Aided Language (PAL) reasoning simultaneously?
- Basis in paper: [inferred] Section 3.3 and Table 5 demonstrate a trade-off: increasing code data (up to 50%) improves PAL performance but causes a sharp drop in CoT performance (from 19.0 to 16.4).
- Why unresolved: The authors identified that $\le 20\%$ code maximizes CoT, but this may not fully unlock the potential for code-augmented reasoning, leaving the "Pareto optimal" frontier undefined.
- What evidence would resolve it: A multi-objective ablation study identifying a mixture ratio or dynamic curriculum that yields peak scores on both CoT (e.g., GSM8K) and PAL benchmarks concurrently.

### Open Question 3
- Question: Does the exclusion of non-English documents limit the cross-lingual transfer capabilities or the diversity of mathematical problem-solving strategies in models trained on MegaMath?
- Basis in paper: [inferred] Section 2.1.1 explicitly states that the pipeline applied a fastText language identifier to "retain only English documents," discarding all other linguistic data from Common Crawl.
- Why unresolved: While the dataset maximizes English math quality, it systematically ignores the value of multilingual math data for generalization.
- What evidence would resolve it: A comparative evaluation of models trained on MegaMath versus a multilingual-augmented version on cross-lingual benchmarks (e.g., MGSM).

## Limitations
- The synthetic data generation pipeline's quality control (AST/runtime verification) is described but not validated against potential subtle logical errors
- FastText classifier performance across the full distribution of CC dumps may vary significantly despite retraining with CoT seeds
- HTML optimization mechanism requires empirical validation across diverse web sources with different math markup approaches

## Confidence
- **High Confidence:** MegaMath dataset scale (371B tokens) and composition (web/code/synthetic mix) are verifiable from reported statistics
- **Medium Confidence:** The claim that MegaMath-Web-Pro outperforms FineMath-4+ by 4%+ is supported by ablation studies but depends on exact implementation details
- **Low Confidence:** The mechanism by which code execution feedback improves mathematical reasoning remains underspecified

## Next Checks
1. **HTML extraction validation:** Process 1000+ math-heavy HTML pages from diverse sources with and without the described optimizations, measuring equation preservation rates and comparing against ground truth LaTeX content
2. **fastText classifier robustness:** Test the two-stage fastText classifier on held-out CC snapshots from 2023-2024 to measure distribution shift performance, and evaluate precision/recall at different score thresholds across all dumps
3. **Synthetic data quality audit:** Generate 1000 synthetic math-code pairs using the described LLM pipeline, then manually inspect for logical consistency and identify any subtle reasoning errors that pass both AST and runtime verification