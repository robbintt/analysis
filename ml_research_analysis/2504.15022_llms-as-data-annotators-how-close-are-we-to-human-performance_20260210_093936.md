---
ver: rpa2
title: 'LLMs as Data Annotators: How Close Are We to Human Performance'
arxiv_id: '2504.15022'
source_url: https://arxiv.org/abs/2504.15022
tags:
- label
- entity
- examples
- llms
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models for data annotation
  tasks, particularly Named Entity Recognition, by comparing retrieval-augmented generation
  (RAG) approaches with in-context learning and zero-shot baselines. The proposed
  RAG method automatically retrieves relevant context examples using embedding models,
  improving annotation quality over manually selected examples.
---

# LLMs as Data Annotators: How Close Are We to Human Performance

## Quick Facts
- arXiv ID: 2504.15022
- Source URL: https://arxiv.org/abs/2504.15022
- Reference count: 40
- Primary result: RAG-based LLM annotation achieves within 3% of human performance on structured NER datasets

## Executive Summary
This study evaluates large language models for data annotation tasks, particularly Named Entity Recognition, by comparing retrieval-augmented generation (RAG) approaches with in-context learning and zero-shot baselines. The proposed RAG method automatically retrieves relevant context examples using embedding models, improving annotation quality over manually selected examples. Experiments on four datasets show that RAG consistently outperforms other methods, with the gpt-4o-mini model achieving performance within 3% of human-level annotation on structured datasets. However, performance significantly declines on more complex datasets like SKILLSPAN, where the best F1 score reaches only 34.06%.

## Method Summary
The study proposes a retrieval-augmented generation approach for data annotation that automatically retrieves relevant context examples from existing annotated datasets using embedding models. The method compares RAG against in-context learning with manually selected examples and zero-shot baselines across multiple LLM models including gpt-4o-mini and other variants. Experiments are conducted on four datasets with systematic evaluation of performance metrics, dataset complexity effects, and embedding model selection impacts.

## Key Results
- RAG approach consistently outperforms in-context learning and zero-shot baselines across all tested datasets
- gpt-4o-mini achieves performance within 3% of human-level annotation on structured datasets
- Performance drops significantly on complex datasets (SKILLSPAN F1: 34.06%), highlighting fundamental limitations
- Larger models do not always yield statistically significant improvements over smaller models when paired with effective retrieval strategies

## Why This Works (Mechanism)
The RAG approach works by automatically retrieving semantically relevant examples from existing annotated datasets, providing rich contextual information that improves the LLM's ability to generalize to new annotation tasks. This automatic retrieval eliminates the need for manual example selection and provides more diverse and contextually appropriate training signals compared to static in-context examples.

## Foundational Learning
- Named Entity Recognition (NER): Core task for evaluating annotation quality; why needed to establish baseline performance metrics
- Retrieval-Augmented Generation: Combines retrieval with generation; quick check: embedding models must effectively match query-document similarity
- Embedding Models: Convert text to vector representations; why needed for semantic similarity matching in RAG
- In-context Learning: Few-shot prompting approach; quick check: effectiveness depends on quality and relevance of provided examples
- Zero-shot Learning: Direct annotation without examples; quick check: baseline for measuring improvement from retrieval

## Architecture Onboarding
Component Map: Query -> Embedding Model -> Retriever -> LLM -> Annotation Output
Critical Path: Text input → Embedding similarity search → Retrieved examples → Prompt construction → LLM generation → Entity annotation
Design Tradeoffs: Automatic retrieval vs manual example selection balances scalability with potential retrieval noise
Failure Signatures: Poor retrieval quality manifests as incorrect entity boundaries or missed entities; complex datasets exceed model capacity
First Experiments:
1. Compare retrieval quality across different embedding models on target dataset
2. Evaluate prompt template variations with retrieved examples
3. Test annotation consistency across multiple runs with same inputs

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow focus on NER tasks may not generalize to other annotation types or complex linguistic phenomena
- Performance primarily evaluated on structured datasets, limiting external validity for unstructured or domain-specific data
- Embedding model selection significantly impacts results but analysis lacks exploration of domain-specific embedding architectures

## Confidence
- Within 3% of human performance on structured datasets: Medium confidence
- Larger models don't always improve performance: High confidence
- RAG outperforms other methods: High confidence

## Next Checks
1. Evaluate RAG approach across diverse annotation tasks beyond NER, including sentiment analysis, relation extraction, and coreference resolution
2. Test embedding model selection with domain-specific embeddings trained on target domain data
3. Conduct human evaluation studies comparing LLM-annotated data with human-annotated data across multiple quality dimensions beyond F1 scores