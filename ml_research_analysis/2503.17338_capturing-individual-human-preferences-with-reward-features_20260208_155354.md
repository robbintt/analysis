---
ver: rpa2
title: Capturing Individual Human Preferences with Reward Features
arxiv_id: '2503.17338'
source_url: https://arxiv.org/abs/2503.17338
tags:
- reward
- preferences
- training
- user
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of personalizing reward models
  in reinforcement learning from human feedback, particularly in scenarios with diverse
  user preferences like large language model training. The authors propose a method
  to specialize a reward model to individual users by learning a set of general reward
  features from group data, which can then be linearly combined to represent individual
  preferences.
---

# Capturing Individual Human Preferences with Reward Features

## Quick Facts
- arXiv ID: 2503.17338
- Source URL: https://arxiv.org/abs/2503.17338
- Authors: André Barreto; Vincent Dumoulin; Yiran Mao; Nicolas Perez-Nieves; Bobak Shahriari; Yann Dauphin; Doina Precup; Hugo Larochelle
- Reference count: 38
- One-line primary result: RFM significantly outperforms non-adaptive baselines and matches or surpasses adaptive counterparts in personalizing reward models, especially with high training data disagreement.

## Executive Summary
This paper addresses the challenge of personalizing reward models in reinforcement learning from human feedback, particularly in scenarios with diverse user preferences like large language model training. The authors propose a method to specialize a reward model to individual users by learning a set of general reward features from group data, which can then be linearly combined to represent individual preferences. The key innovation is the use of reward-feature models (RFMs) that distinguish between shared parameters (for general features) and user-specific parameters (for preference coefficients). Experiments on the UltraFeedback dataset demonstrate that RFMs significantly outperform non-adaptive baselines and match or surpass adaptive counterparts, especially when there is high disagreement among training data.

## Method Summary
The method learns shared reward features ϕ_θ(x,y) → R^d from group preference data, then represents each user's reward as r_h(x,y) = ⟨ϕ_θ(x,y), w_h⟩. Training uses a Bradley-Terry model extended with user-specific weights, optimizing θ and W jointly via gradient ascent. Adaptation to new users reduces to convex logistic regression with frozen features. The approach uses Gemma 1.1 2B base with modified final layer for feature extraction, training on UltraFeedback dataset with synthetic preference distributions.

## Key Results
- RFMs achieve up to 90% accuracy in predicting individual preferences in heterogeneous scenarios
- Non-adaptive baselines drop to chance accuracy when training raters have opposing preferences, while RFM maintains ~65%
- RFMs consistently outperform baselines in best-of-n experiments for steering LLM outputs based on personalized preferences
- Simpler architecture provides more stable training compared to fully adaptive methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing rewards into shared features plus user-specific weights enables fast adaptation to new users.
- Mechanism: The model learns ϕ_θ(x,y) → R^d (reward features) from group data, then represents each user's reward as r_h(x,y) = ⟨ϕ_θ(x,y), w_h⟩. New users require only learning d coefficients via logistic regression rather than fine-tuning the full network.
- Core assumption: Individual preferences lie in a low-dimensional subspace spanned by shared features.
- Evidence anchors:
  - [abstract] "individual preferences can be captured as a linear combination of a set of general reward features"
  - [Section 4] Equation 5 defines rh(x,y) = ⟨ϕ(x,y), wh⟩; Section 4.2 shows adaptation reduces to max_w Σ log σ(⟨δ_θ, w⟩)—convex logistic regression.
  - [corpus] Weak direct validation; related work (Poddar et al. VPL) encodes user history latently but doesn't guarantee linear structure.
- Break condition: If user preferences require fundamentally different feature bases (non-stationary or task-specific), linear combination of fixed features will underfit.

### Mechanism 2
- Claim: Training with rater-identified data separates shared structure from individual variation.
- Mechanism: Dataset D+ = {(x_i, y_i, y'_i, z_i, h_i)} enables the model to attribute preference variance to specific raters. The shared parameters θ learn features useful across raters, while W learns per-rater weights. This prevents contradictory signals from different raters from canceling out.
- Core assumption: Rater disagreement reflects genuine preference diversity, not noise.
- Evidence anchors:
  - [Section 3] Formal distinction between intra-user and inter-user generalization; D+ with rater IDs required.
  - [Section 5.1.2] Scenario 2 with opposing preference distributions: non-adaptive baseline drops to chance accuracy while RFM maintains ~65%.
  - [corpus] No direct corpus validation; assumption aligns with pluralistic alignment literature but unverified here.
- Break condition: If raters are not sufficiently diverse or if labels are systematically biased, learned features won't generalize to held-out users.

### Mechanism 3
- Claim: Bradley-Terry model extended with user-specific weights provides differentiable training signal for both features and coefficients.
- Mechanism: p(y ≻ y' | x, h) = σ(⟨ϕ(x,y) - ϕ(x,y'), w_h⟩). The inner product structure means gradients flow through both ϕ_θ and W during training. At adaptation time, freezing θ makes the problem convex.
- Core assumption: Preferences follow the Bradley-Terry assumption (reward difference maps to preference probability via sigmoid).
- Evidence anchors:
  - [Section 4.1] Equation 7 defines the training MLE objective over θ and W.
  - [Section 5.1] Experiments use UltraFeedback with synthetic preferences generated via ground-truth features, validating the model can recover structure.
  - [corpus] Bradley-Terry is standard but contested; Azar et al. (2024) discuss limitations.
- Break condition: If human preferences violate transitivity or depend on unmodeled context, the BT assumption fails.

## Foundational Learning

- Concept: **Bradley-Terry preference model**
  - Why needed here: Core probabilistic framework linking rewards to binary preferences; RFM extends it with user-specific weights.
  - Quick check question: Given rewards r(a)=2, r(b)=1, what is p(a ≻ b) under BT?

- Concept: **Inner product as similarity/scoring**
  - Why needed here: The reward rh(x,y) = ⟨ϕ, wh⟩ interprets user weights as a "preference direction" in feature space.
  - Quick check question: If wh = [1, -1] and ϕ = [0.5, 0.5], what is the reward?

- Concept: **Convex optimization / logistic regression sample complexity**
  - Why needed here: Adaptation is just logistic regression; understanding its sample complexity informs how few examples suffice.
  - Quick check question: Why does freezing features make adaptation convex?

## Architecture Onboarding

- Component map:
  - **Backbone encoder**: Gemma 1.1 2B (or similar) processes (x, y) → hidden representation
  - **Feature head**: Linear layer mapping hidden → R^d (reward features ϕ_θ)
  - **User weight matrix**: W ∈ R^{|Ĥ|×d} storing per-rater coefficients during training
  - **Adaptation module**: Logistic regression solver for new w given frozen ϕ_θ

- Critical path:
  1. Preprocess preference data with rater IDs → D+
  2. Train: joint optimization of θ and W via gradient ascent on Equation 7
  3. Adapt: collect m examples from new user, freeze θ, solve Equation 8 for w
  4. Deploy: use r_θ,w(x,y) to score/rank responses (e.g., best-of-n)

- Design tradeoffs:
  - Feature dimension d: Larger d captures more nuance but requires more adaptation data. Paper tests d ∈ {8, 32, 128}.
  - Regularization on W: Prevents collapse to independent per-user models.
  - Assumption: Training data must have rater diversity; homogeneity reduces adaptation benefits.

- Failure signatures:
  - Non-adaptive baseline matches RFM → insufficient disagreement in training data (Scenario 1 vs 2)
  - Adaptation accuracy doesn't improve with m → features may not capture relevant preference dimensions
  - RFM matches but doesn't exceed in-context LLMs → features may be underfitting

- First 3 experiments:
  1. Reproduce Scenario 2 on UltraFeedback: verify RFM outperforms baseline when raters have opposing preferences
  2. Ablation on d: plot adaptation accuracy vs. feature dimension for held-out users with varying preference complexity
  3. Real rater test: use multiple public reward models as synthetic raters (Section 5.2 protocol), measure leave-one-out generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can optimization schemes like block coordinate descent or separate learning rates for features and weights improve RFM training stability or speed over vanilla gradient ascent?
- Basis in paper: [explicit] The authors state they "leave open many opportunities" regarding technical aspects, specifically suggesting "different learning rates for the features $\phi_\theta$ and the matrix of coefficients $W$" or algorithms like "block coordinate descent."
- Why unresolved: The paper deliberately used standard gradient ascent to keep the experimental setup simple and did not exploit the structure of the optimization problem.
- What evidence would resolve it: Comparative training curves showing faster convergence or higher final accuracy using these specialized optimization methods.

### Open Question 2
- Question: Can active learning strategies significantly reduce the number of examples required for adaptation compared to uniform sampling?
- Basis in paper: [explicit] The authors note that adaptation "can probably be made more efficient if the process is formulated as an active learning problem."
- Why unresolved: The experiments used uniformly sampled data for adaptation rather than querying the most informative examples.
- What evidence would resolve it: Experiments demonstrating that an active selection method (e.g., using uncertainty sampling derived from the logistic regression adaptation) achieves target accuracy with fewer user queries.

### Open Question 3
- Question: How can RFM features be utilized during the LLM training phase to induce personalized policies, rather than solely for post-hoc re-ranking?
- Basis in paper: [explicit] The authors suggest that "A more promising approach would be to take RFM's features into account when training the LLM," proposing the training of multiple LLMs induced by different feature combinations.
- Why unresolved: The paper focused on "best-of-n" selection for steering outputs and left the integration of features into the policy training loop for future work.
- What evidence would resolve it: A method that uses the learned features to modulate policy gradients or merge policy weights, resulting in a model that generates personalized responses directly.

### Open Question 4
- Question: Does the linear decomposition of user preferences limit the model's ability to capture complex, non-linear preference interactions?
- Basis in paper: [inferred] The core assumption of the method is that "individual preferences can be captured as a linear combination" of features. The paper does not test if this linear structure fails to represent users with non-linear or conditional criteria.
- Why unresolved: While effective in the experiments, the authors do not investigate if the linear assumption restricts representational capacity for highly complex or heterogeneous user groups.
- What evidence would resolve it: A comparison between RFM and a non-linear personalized reward model on a dataset specifically constructed to contain non-linear preference interactions.

## Limitations

- The core assumption that preferences lie in a low-dimensional subspace remains theoretically justified but empirically unproven beyond synthetic data
- The method requires rater-identified data, limiting applicability to datasets that track who provided each label
- Real-world deployment with thousands of users raises questions about W matrix scalability and feature dimension choice

## Confidence

**High Confidence**: The mathematical framework (Equations 7-8), the adaptation mechanism (freezing θ for convex logistic regression), and the core experimental finding that RFM outperforms non-adaptive baselines when training data contains preference disagreement.

**Medium Confidence**: The claim that RFM "consistently outperforms" adaptive baselines in best-of-n experiments, as this depends on the specific adaptive method comparison and implementation details.

**Low Confidence**: The practical scalability of RFM to real-world LLM alignment scenarios with thousands of diverse users, and the claim that the simpler architecture provides "more stable training" without quantitative stability comparisons.

## Next Checks

1. **Real Human Preference Test**: Apply RFM to a subset of UltraFeedback where actual rater IDs are available (not synthetic). Measure inter-user generalization to held-out human raters, comparing adaptation accuracy gains against non-adaptive baselines.

2. **Extreme Preference Distribution**: Design synthetic scenarios where held-out users have preferences orthogonal or anti-correlated to training rater preferences. Measure the breaking point where RFM fails to adapt versus traditional fine-tuning.

3. **Sample Efficiency Benchmark**: Systematically vary adaptation dataset size m (down to 5 examples) for held-out users with different preference complexities. Compare RFM's adaptation curve against prompt-based in-context learning using the same Gemma model.