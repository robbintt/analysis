---
ver: rpa2
title: 'Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs'
arxiv_id: '2504.07866'
source_url: https://arxiv.org/abs/2504.07866
tags:
- training
- pangu
- ultra
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pangu Ultra is a 135-billion-parameter dense Transformer model
  trained on Ascend NPUs, designed to explore the potential of dense architectures
  at scale. The study addresses training instability in deep models by introducing
  depth-scaled sandwich normalization, which replaces pre-layer norm with a depth-scaled
  initialization scheme to stabilize gradients and eliminate loss spikes.
---

# Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs

## Quick Facts
- **arXiv ID:** 2504.07866
- **Source URL:** https://arxiv.org/abs/2504.07866
- **Reference count:** 40
- **Primary result:** 135B-parameter dense Transformer achieving competitive performance with leading sparse models through depth-scaled sandwich normalization and 52%+ MFU on 8,192 Ascend NPUs

## Executive Summary
Pangu Ultra is a 135-billion-parameter dense Transformer model trained on Ascend NPUs, designed to explore the potential of dense architectures at scale. The study addresses training instability in deep models by introducing depth-scaled sandwich normalization, which replaces pre-layer norm with a depth-scaled initialization scheme to stabilize gradients and eliminate loss spikes. The model is pre-trained on 13.2 trillion diverse tokens across three phases—general knowledge, reasoning, and annealing—and further enhanced with post-training alignment using supervised fine-tuning and reinforcement learning. To handle large-scale training, the system leverages 8,192 Ascend NPUs with optimized parallelism strategies and kernel fusions, achieving over 52% Model FLOPs Utilization. Evaluations show Pangu Ultra outperforms leading dense models like Llama 405B and Mistral Large 2, and achieves competitive results with sparse models like DeepSeek-R1, demonstrating the efficacy of dense architectures at scale.

## Method Summary
Pangu Ultra employs a 94-layer dense Transformer with 12,288 hidden size and 28,672 FFN dimension, using Grouped-Query Attention (96Q/8KV) and SwiGLU activations. The key innovation is depth-scaled sandwich normalization (DSSN), which applies layer normalization both before and after each sub-layer with learnable gamma parameters initialized at $\gamma = c/\sqrt{L}$ to control gradient variance. TinyInit initialization scales weights by $\sqrt{1/(2dL)}$ for both depth and width. The model trains on 13.2T tokens across three phases: 12T general knowledge, 0.8T reasoning (65% math+code), and 0.4T annealing. Distributed training uses 8-way tensor, 8-way pipeline, and 6-way virtual pipeline parallelism across 8,192 Ascend NPUs, achieving >52% MFU through kernel fusion and optimized scheduling.

## Key Results
- Eliminates loss spikes during deep model training through depth-scaled sandwich normalization
- Achieves 52%+ Model FLOPs Utilization on 8,192 Ascend NPUs with virtual pipeline parallelism
- Outperforms Llama 405B and Mistral Large 2 on MMLU, MATH, GSM8K, and other benchmarks
- Competitive with sparse models like DeepSeek-R1 while maintaining dense architecture simplicity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Depth-scaled sandwich normalization eliminates loss spikes during training of deep models.
- Mechanism: Applies layer normalization both before and after each sub-layer (sandwich norm), with learnable gamma parameters initialized at $\gamma = c/\sqrt{L}$ where L is layer count. This controls gradient fluctuation magnitude that would otherwise accumulate through residual connections in deep networks.
- Core assumption: Loss spikes arise from uncontrolled gradient variance during backpropagation through many layers.
- Evidence anchors:
  - [abstract] "depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models"
  - [section 5.4.1] Figure 7 shows smooth loss curves with DSSN vs. spiking curves with Pre-LN; Table 5 shows improved benchmark scores
  - [corpus] No external validation found; mechanism is self-reported in this technical report
- Break condition: If using fewer than ~50 layers, the depth-scaling factor may become negligible and standard Pre-LN may suffice.

### Mechanism 2
- Claim: TinyInit accelerates convergence and improves final task performance.
- Mechanism: Initializes weights with standard deviation $\sqrt{1/(2dL)}$ rather than the common $\sqrt{2/(5d)}$, scaling by both hidden dimension d and depth L. This maintains more consistent parameter scales across the full network depth.
- Core assumption: Consistent activation/gradient magnitudes across all layers facilitates optimization landscape navigation.
- Evidence anchors:
  - [section 2.2] "scaling initialization by both model depth and width, using $\sqrt{1/(2dL)}$, leads to faster loss convergence and improved performance"
  - [section 5.4.2] Table 6 shows TinyInit outperforms baseline across EN basic, ZH basic, LAMBADA, WPLC, C-Eval, MMLU, and BIG-bench
  - [corpus] No external validation found
- Break condition: If model is shallow (L < 20), TinyInit may not show significant gains over standard initialization.

### Mechanism 3
- Claim: Virtual pipeline parallelism with 6-way VPP stages dramatically reduces pipeline bubbles at scale.
- Mechanism: Interleaved scheduling assigns multiple virtual pipeline stages per device, allowing finer-grained work distribution and reducing idle time from $(p-1)/(p-1+n)$ to 6.8%.
- Core assumption: The overhead of VPP scheduling coordination is smaller than the bubble-time savings.
- Evidence anchors:
  - [section 4.2] "reduce the PP bubble ratio from 30.45% to 6.8%... achieve approximately 43% MFU on an 8,192 NPU cluster as a baseline"
  - [section 4.3] Combined optimizations push MFU to >52%
  - [corpus] VPP is a known technique from Megatron-LM; not novel to this work
- Break condition: At smaller scales (fewer devices), standard 1F1B scheduling may have comparable efficiency with simpler implementation.

## Foundational Learning

- Concept: **Pre-Layer Normalization vs Post-Layer Normalization**
  - Why needed here: The paper proposes replacing Pre-LN with sandwich norm; understanding why Pre-LN became standard helps evaluate this change.
  - Quick check question: Can you explain why Pre-LN improves gradient flow compared to Post-LN in deep Transformers?

- Concept: **Pipeline Bubbles in Distributed Training**
  - Why needed here: The paper's main systems contribution addresses pipeline bubble reduction at 8K+ device scale.
  - Quick check question: In 1F1B scheduling, what determines the bubble ratio formula $(p-1)/(p-1+n)$?

- Concept: **Loss Spikes and Gradient Instability**
  - Why needed here: The core motivation for DSSN is eliminating loss spikes that "knock the model out of the ideal parameter landscape."
  - Quick check question: What are two common causes of loss spikes during LLM pre-training?

## Architecture Onboarding

- Component map:
  Input → Embedding (std=0.5 init) → 94× [Depth-Scaled Sandwich-Norm → Attention (GQA: 96Q/8KV) → DSSN → FFN (SwiGLU) → DSSN] → Output projection

- Critical path:
  1. Implement DSSN: Pre-norm → sublayer → Post-norm with $\gamma = c/\sqrt{L}$ initialization
  2. Apply TinyInit: Weight std = $\sqrt{1/(2 \times 12288 \times 94)} \approx 0.0026$
  3. Configure 8-way TP × 8-way PP × 6-way VPP × 128-way DP for 8192 NPUs
  4. Three-phase training: 12T general → 0.8T reasoning → 0.4T annealing

- Design tradeoffs:
  - Dense 135B vs MoE: Dense is easier to optimize (no load balancing heuristics) but requires more compute per token
  - 94 layers vs wider/shallower: Depth may improve reasoning but introduces training instability (hence DSSN)
  - TinyInit: Slower early training possible, but better final convergence

- Failure signatures:
  - Loss spikes appearing after 500B+ tokens → DSSN gamma initialization may be incorrect
  - Gradient norm explosion in early layers → Check embedding std (should be 0.5, not 1.0)
  - MFU below 40% at scale → PP/VPP scheduling misconfiguration or communication bottleneck
  - Attention OOM at 128K context → NFA operator not enabled or mask caching not implemented

- First 3 experiments:
  1. **DSSN ablation at proxy scale**: Train 1.6B/94-layer model with plain sandwich-norm vs depth-scaled sandwich-norm for 50B tokens; verify loss spike elimination
  2. **TinyInit convergence test**: Compare standard init vs TinyInit on 13B model at 100B tokens; measure loss curve slope and final benchmark scores
  3. **VPP bubble measurement**: Profile pipeline utilization at 512 NPUs with 1F1B vs 6-way VPP; validate bubble ratio reduction matches theoretical prediction

## Open Questions the Paper Calls Out

- **Question:** What are the specific architectural details of the latency-tolerant reinforcement learning framework optimized for Ascend NPUs?
- **Basis in paper:** [explicit] The paper mentions the framework but states it "will be detailed in a future report."
- **Why unresolved:** The current text confirms the existence of a hybrid reward system and RL usage but omits the implementation specifics required for reproducibility on non-standard hardware.
- **What evidence would resolve it:** Release of the promised technical report or open-sourcing of the RL training code.

- **Question:** What is the functional role of the "super activations" observed in shallow layers, and do they represent a necessary feature or a numerical stability risk?
- **Basis in paper:** [inferred] The authors observe activations with magnitudes over $10^3$ but only note they decrease with depth without explaining their origin or necessity.
- **Why unresolved:** Empirical observation exists, but no causal theory is provided for why these large magnitude values form or if they should be regularized.
- **What evidence would resolve it:** Ablation studies constraining these activation values to determine their impact on model convergence and reasoning capabilities.

- **Question:** Does the Depth-Scaled Sandwich-Norm (DSSN) generalize to sparse Mixture-of-Experts (MoE) architectures without conflicting with router stability?
- **Basis in paper:** [inferred] The paper contrasts its dense approach with MoE models, noting that MoE dynamic components "usually need to turn to additional heuristics for stable training."
- **Why unresolved:** DSSN is proven effective for deep dense models, but it is untested whether the depth-scaled gamma initialization interferes with the load-balancing mechanisms in MoE layers.
- **What evidence would resolve it:** Applying DSSN to a standard MoE baseline (e.g., DeepSeek-V2 style) and measuring loss spike frequency.

## Limitations
- Training stability mechanism validation relies on self-reported evidence without external validation
- Data composition opacity prevents full reproducibility of training methodology
- Systems optimizations depend on proprietary Ascend hardware and operators

## Confidence
- **Dense architecture performance claims**: Medium confidence
- **Training stability claims**: High confidence
- **Systems efficiency claims**: Medium confidence

## Next Checks
1. **DSSN effectiveness at different depths**: Train 1.6B/50-layer and 1.6B/94-layer models with both Pre-LN and DSSN for 50B tokens, measuring loss spike frequency and final task performance
2. **TinyInit convergence comparison**: Implement standard initialization versus TinyInit on a 13B model at 100B tokens, measuring both loss curve slopes and final MMLU/BIG-bench scores
3. **VPP bubble ratio measurement**: At 512-device scale, profile pipeline utilization with 1F1B scheduling versus 6-way VPP implementation, measuring actual bubble ratios and MFU