---
ver: rpa2
title: 'NonverbalTTS: A Public English Corpus of Text-Aligned Nonverbal Vocalizations
  with Emotion Annotations for Text-to-Speech'
arxiv_id: '2507.13155'
source_url: https://arxiv.org/abs/2507.13155
tags:
- speech
- emotion
- dataset
- nvtts
- laughter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scarcity of open-source datasets with
  diverse nonverbal vocalizations (NVs) for expressive speech synthesis. The authors
  introduce NVTTS, a 17-hour English corpus derived from VoxCeleb and Expresso, containing
  10 types of NVs (e.g., laughter, coughs) and 8 emotion categories.
---

# NonverbalTTS: A Public English Corpus of Text-Aligned Nonverbal Vocalizations with Emotion Annotations for Text-to-Speech

## Quick Facts
- arXiv ID: 2507.13155
- Source URL: https://arxiv.org/abs/2507.13155
- Reference count: 0
- Introduces NVTTS, a 17-hour English corpus with 10 NV types and 8 emotion categories, achieving parity with CosyVoice2 in human evaluation

## Executive Summary
This paper addresses the scarcity of open-source datasets for expressive speech synthesis with nonverbal vocalizations (NVs). The authors introduce NVTTS, a 17-hour English corpus derived from VoxCeleb and Expresso, containing 10 types of NVs (laughter, coughs, sighs, etc.) and 8 emotion categories. The dataset was created using automated detection followed by human validation, with a comprehensive annotation pipeline integrating ASR, NV tagging, emotion classification, and annotation fusion. When fine-tuned on open-source TTS models, NVTTS achieves performance parity with closed-source systems like CosyVoice2, as measured by human evaluation and automatic metrics including speaker similarity and NV fidelity.

## Method Summary
The authors developed a comprehensive annotation pipeline that combines automated detection (BEATs) with human validation via Argilla, using a majority-vote fusion algorithm to merge transcriptions from multiple annotators. The corpus was derived from VoxCeleb and Expresso, containing 10 NV types and 8 emotion categories. For TTS fine-tuning, only the language model component of CosyVoice-300M was trained in a supervised setup, using NV tokens (e.g., `[laugh]`, `[breath]`) as explicit conditioning signals. The dataset is strategically partitioned to ensure no speaker overlap between train/test subsets, enabling zero-shot NV synthesis for unseen speakers.

## Key Results
- NVTTS achieves human evaluation parity with CosyVoice2 in NV synthesis quality
- Automatic metrics show competitive speaker similarity (SIM-o) and NV fidelity (Jaccard distance)
- Ablation studies demonstrate the essential role of explicit NV modeling in synthesis quality
- Zero-shot evaluation shows successful NV generation for unseen speakers without speaker overlap

## Why This Works (Mechanism)

### Mechanism 1: Weak-to-Strong Annotation Pipeline
The pipeline uses BEATs for low-threshold NV detection (0.1), emotion2vec+ for emotion classification, human refinement via Argilla, and a majority-vote fusion algorithm that aligns multiple annotator transcriptions and retains tokens appearing in ≥2 of 3 annotations. This approach assumes noisy automated labels can be corrected through structured human oversight without introducing systematic bias.

### Mechanism 2: Explicit NV Token Modeling in Language Model Fine-tuning
NVs are represented as inline tokens (e.g., `[laugh]`, `[breath]`) within transcriptions. The LM learns to predict these tokens alongside text, conditioning the speech generator on explicit NV markers. The core assumption is that the pre-trained speech tokenizer and decoder from CosyVoice-300M can synthesize NVs without additional fine-tuning if the LM provides appropriate conditioning.

### Mechanism 3: Speaker-Disjoint Evaluation for Zero-Shot Generalization
The dataset partitions ensure no speaker overlap between train/test (VoxCeleb speakers allocated to test only). The model must generalize NV generation to novel voices, based on the assumption that NV production patterns are speaker-independent enough that learning from 1,314 training speakers transfers to 147 test speakers.

## Foundational Learning

- **Forced Alignment (Montreal Forced Aligner)**
  - Why needed here: Places NV tags at precise timestamps within transcriptions; without alignment, NV tags cannot be correctly positioned relative to words.
  - Quick check question: Given audio with a cough at 2.3s and transcription "Hello world," where would MFA place `[cough]` if "Hello" spans 0.0-1.5s and "world" spans 1.5-3.0s?

- **Sequence Alignment (Dynamic Programming)**
  - Why needed here: The fusion algorithm uses Pyalign to align multiple annotator transcriptions; this is the same core technique as DNA sequence alignment.
  - Quick check question: If two transcriptions are "It's a cat" and "It's the cat," what gap characters would alignment insert to maximize matches?

- **Zero-Shot TTS Paradigm**
  - Why needed here: Evaluation assumes the model generates speech for speakers not seen during training; this requires understanding speaker embeddings as conditioning signals.
  - Quick check question: What is the difference between zero-shot synthesis (unseen speaker) and few-shot adaptation (fine-tune on 3 samples)?

## Architecture Onboarding

- **Component map:** Raw audio → Canary ASR (transcription) + BEATs (NV detection) + emotion2vec+ (emotion) + MFA (alignment) → Argilla platform → 3 annotators → Pyalign fusion → final NV-enriched transcription → CosyVoice-300M LM component (speech tokenizer frozen)

- **Critical path:**
  1. NV detection threshold (0.1) determines candidate pool
  2. Human validation filters false positives
  3. Fusion algorithm determines final annotation quality
  4. LM fine-tuning on NV-enriched text enables synthesis

- **Design tradeoffs:**
  - Low detection threshold (0.1) maximizes recall but increases human annotation burden
  - 3-annotator setup enables majority voting but 8.5% of samples have no emotion label due to disagreement
  - Training only LM (not tokenizer/decoder) limits NV types to those already supported by base model

- **Failure signatures:**
  - NV generation drops when emotion tags removed (ablation shows J-scores decline)
  - Laughter underperforms vs. CosyVoice2 due to tokenization granularity (single vs. multi-token)
  - Rare NVs (sneeze: 13, snore: 13, grunt: 7) may have insufficient training signal

- **First 3 experiments:**
  1. **Reproduce annotation pipeline on 100 samples:** Run BEATs → human validation → fusion; measure annotator agreement rate and compare to paper's fusion output.
  2. **Fine-tune CosyVoice-300M LM on NVTTS-train:** Train for 25 epochs with Adam (lr=1e-5), evaluate SIM-o and J-scores on NVTTS-test; verify results are within 5% of reported values.
  3. **Ablation on single NV type:** Train model on only `[laugh]` annotations, measure if laugh-specific J-score improves vs. full training; tests whether NV interference exists.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do speech encoders implicitly capture nonverbal cues without explicit text-aligned supervision?
- Basis in paper: The conclusion states that future work could leverage NVTTS to "investigate whether speech encoders implicitly capture nonverbal cues."
- Why unresolved: The paper focuses on constructing explicit text-aligned annotations and training models with these direct labels, rather than probing the internal representations of pre-trained encoders for unsupervised NV features.
- What evidence would resolve it: Probing experiments using NVTTS labels to evaluate how well self-supervised speech encoder representations (e.g., from Whisper or WavLM) correlate with NV timestamps without fine-tuning.

### Open Question 2
- Question: Can retrieval-based systems effectively scale nonverbal data collection using NVTTS as a seed?
- Basis in paper: The authors propose that NVTTS could serve as "a seed for semi-supervised discovery of nonverbal data at scale" and help "develop retrieval systems that identify NV-rich segments."
- Why unresolved: The current work relies on automated detection followed by costly human validation; the utility of this dataset for training automated retrieval agents to filter larger, unannotated corpora (like Emilia) remains untested.
- What evidence would resolve it: Training a lightweight classifier on NVTTS embeddings and evaluating its precision/recall in extracting high-quality NV segments from a larger, diverse audio corpus.

### Open Question 3
- Question: Why does the removal of explicit emotion labels marginally improve nonverbal vocalization synthesis quality?
- Basis in paper: Section 5.3.2 notes that "removing emotion annotations marginally improves NV generation quality" in automatic metrics, a counter-intuitive result the authors observe but do not fully explain.
- Why unresolved: The paper identifies this trade-off but does not investigate whether this is due to data sparsity in specific emotion classes, conflicting gradient signals, or error propagation from the emotion classifier.
- What evidence would resolve it: A detailed analysis of attention mechanisms or a parameter-isolation study to determine if emotion and NV tokens compete for model capacity during training.

## Limitations

- Evaluation relies heavily on CosyVoice-300M baseline without independent retraining or identical conditions
- Annotation pipeline may propagate systematic biases from automated NV detection
- Zero-shot evaluation assumes speaker-independent NV patterns that may not hold for highly individual vocalizations

## Confidence

**High Confidence:** The annotation pipeline methodology is technically sound and follows established NLP annotation practices. The dataset creation process and basic statistics are verifiable.

**Medium Confidence:** Human evaluation results showing parity with CosyVoice2 are credible but limited by small sample size (12 participants) and potential anchoring effects. Automatic metrics provide objective measures but may not fully capture perceptual quality.

**Low Confidence:** The claim that training only the LM component enables zero-shot NV synthesis without architectural changes is not independently validated. The paper does not demonstrate handling of NV tags not present in base tokenizer vocabulary.

## Next Checks

1. **Annotation Pipeline Validation:** Run the complete annotation pipeline (BEATs detection at threshold 0.1 → human validation → Pyalign-based fusion) on 100 randomly selected samples from NVTTS-train. Measure annotator agreement rates and compare the fusion output to the published annotations to verify the pipeline's reliability.

2. **Per-NV Performance Analysis:** Train the model on individual NV types (e.g., only `[laugh]` annotations) and measure NV-specific Jaccard scores. Compare to full-model performance to determine if NV types interfere with each other during training and whether rare NVs (<20 samples) can be reliably synthesized.

3. **Base Model Tokenization Investigation:** Test the CosyVoice-300M tokenizer's ability to process NV tags not present in its vocabulary (cough, sigh). Generate speech conditioned on these tags and analyze whether the model produces recognizable NV sounds or ignores the conditioning entirely, validating the claim that LM fine-tuning alone suffices.