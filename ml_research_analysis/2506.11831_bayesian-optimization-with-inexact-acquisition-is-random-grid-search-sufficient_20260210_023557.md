---
ver: rpa2
title: 'Bayesian Optimization with Inexact Acquisition: Is Random Grid Search Sufficient?'
arxiv_id: '2506.11831'
source_url: https://arxiv.org/abs/2506.11831
tags:
- acquisition
- function
- optimization
- grid
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of inexact acquisition function
  maximization in Bayesian optimization (BO). The authors formalize a measure of worst-case
  accumulated inaccuracy in acquisition solutions and establish regret bounds for
  both GP-UCB and GP-TS under such inexactness.
---

# Bayesian Optimization with Inexact Acquisition: Is Random Grid Search Sufficient?

## Quick Facts
- arXiv ID: 2506.11831
- Source URL: https://arxiv.org/abs/2506.11831
- Reference count: 40
- Key result: Random grid search with linear growth achieves sublinear cumulative regret in Bayesian optimization, matching theoretical guarantees and offering computational savings over gradient-based solvers.

## Executive Summary
This paper addresses the challenge of inexact acquisition function maximization in Bayesian optimization (BO). The authors formalize a measure of worst-case accumulated inaccuracy in acquisition solutions and establish regret bounds for both GP-UCB and GP-TS under such inexactness. They show that sublinear cumulative regret is achievable even when acquisition maximization is solved approximately, provided the accumulated inaccuracy grows slowly. A key contribution is the theoretical justification of random grid search as a valid and computationally efficient acquisition function solver, relaxing prior theoretical requirements from exponential to linear grid growth. Experiments on synthetic functions and a real-world AutoML task confirm that random grid search achieves competitive regret while offering significant computational savings compared to gradient-based solvers. The work bridges a practical-implementational gap in BO by showing convergence guarantees without exact acquisition maximization.

## Method Summary
The paper formalizes inexact acquisition maximization in BO through an accumulated inaccuracy metric and derives regret bounds for GP-UCB and GP-TS. The core method uses random grid search with linear grid growth (size 100t at iteration t) as an acquisition solver, compared against gradient-based methods (L-BFGS-B, Nelder-Mead, CG). Experiments use GP surrogate with Matérn kernel, βt = √(log(t+2)), and Sobol sequence initialization. The analysis covers synthetic benchmarks (Branin, Rastrigin, Hartmann functions) and a real-world AutoML task with gradient boosting on breast cancer dataset.

## Key Results
- Random grid search with linear growth achieves sublinear cumulative regret matching theoretical guarantees
- Computational time per acquisition optimization is significantly lower than gradient-based solvers
- Theoretical framework relaxes prior exponential grid size requirements to linear growth (O(t))
- Competitive performance on both synthetic benchmarks and real-world AutoML task

## Why This Works (Mechanism)
Random grid search works because the theoretical framework shows that exact acquisition maximization is not required for convergence in BO. The key insight is that accumulated inaccuracy grows slowly enough with linear grid growth to maintain sublinear regret bounds. The random sampling provides sufficient exploration while avoiding the computational burden of exact optimization, and the theoretical guarantees ensure that approximate solutions still converge to the global optimum.

## Foundational Learning
- **Bayesian Optimization**: Sequential optimization using GP surrogate to model objective function; needed to understand the context of acquisition maximization
- **Accumulated Inaccuracy Metric**: Formal measure of approximation error in acquisition solutions; needed to quantify inexactness impact on regret
- **Regret Bounds**: Theoretical guarantees on cumulative loss compared to optimal solution; needed to establish convergence conditions
- **Random Grid Search**: Sampling-based acquisition solver with linear growth; needed as practical alternative to exact optimization
- **GP-UCB/GP-TS**: Two common acquisition strategies with different exploration-exploitation trade-offs; needed as test cases for theoretical analysis
- **Matérn Kernel**: Covariance function for GP that balances smoothness and flexibility; needed for surrogate modeling

## Architecture Onboarding
- **Component Map**: BO loop -> GP update -> Acquisition computation -> Query selection -> Next iteration
- **Critical Path**: GP training (every iteration) → Acquisition function evaluation on grid → Max selection → Query point recommendation
- **Design Tradeoffs**: Exact maximization vs computational efficiency; grid size growth rate vs regret bounds; random vs structured sampling
- **Failure Signatures**: Diverging regret indicates insufficient grid resolution; numerical instability suggests kernel matrix issues; slow convergence may require larger initial exploration
- **First Experiments**:
  1. Implement GP with Matérn kernel and verify posterior mean/variance computation
  2. Test random grid search on simple 1D function (e.g., sine with noise)
  3. Compare random grid vs gradient-based acquisition optimization on Branin function

## Open Questions the Paper Calls Out
**Open Question 1**: Can adaptive inexact optimization strategies be developed that dynamically allocate computational effort to acquisition maximization based on the accumulated inaccuracy Mt? The conclusion states the framework "opens avenues for studying adaptive inexact optimization strategies, where the computational effort... is dynamically adjusted based on accumulated inaccuracy." This remains unresolved as the paper only analyzes fixed strategies.

**Open Question 2**: Do the inexact regret bounds derived for GP-UCB and GP-TS hold for look-ahead acquisition functions like Knowledge Gradient (KG) or Entropy Search (ES)? The authors restrict theoretical analysis to GP-UCB and GP-TS, noting that look-ahead functions involve expectations over future steps, complicating the analysis in the inexact context.

**Open Question 3**: Does random grid search retain its competitive advantage over gradient-based solvers in dimensions significantly higher than d=11? Page 9 notes that "the gap in runtime between uniform random grid search and existing acquisition function solvers narrows in higher dimensions," suggesting the linear grid size O(t) may eventually become computationally burdensome relative to gradient methods in high-d spaces.

## Limitations
- Theoretical analysis assumes certain regularity conditions on objective function and kernel that may not hold exactly in practice
- Experimental validation limited to specific benchmark functions and one real-world dataset
- Missing implementation details for GP kernel hyperparameters and gradient-based solver configurations
- Linear grid growth may become inefficient in very high-dimensional spaces

## Confidence
- **High confidence**: Theoretical regret bounds under inexact acquisition maximization are sound and well-established
- **Medium confidence**: Random grid search with linear growth achieves sublinear cumulative regret in practice, based on experimental evidence
- **Low confidence**: The specific computational time savings of random grid search over gradient-based solvers will depend on implementation details and hardware

## Next Checks
1. Verify that cumulative regret for random grid search with linear growth (|Xt| = 100t) matches reported trend of sublinear growth across all benchmark functions
2. Compare wall-clock time per acquisition optimization for random grid search versus L-BFGS-B with 10 multi-start restarts to quantify computational savings
3. Test sensitivity of results to number of initial Sobol points (n_init) and grid size scaling factor to establish robustness