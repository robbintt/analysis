---
ver: rpa2
title: 'Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?'
arxiv_id: '2506.10912'
source_url: https://arxiv.org/abs/2506.10912
tags:
- toxicity
- molecular
- repair
- mllms
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToxiMol, the first benchmark for molecular
  toxicity repair using general-purpose multimodal large language models (MLLMs).
  It constructs a standardized dataset of 660 toxic molecules across 11 primary tasks
  and 660 representative toxic molecules spanning diverse mechanisms and granularities.
---

# Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?

## Quick Facts
- **arXiv ID**: 2506.10912
- **Source URL**: https://arxiv.org/abs/2506.10912
- **Reference count**: 40
- **Primary result**: Introduced ToxiMol benchmark showing MLLMs face significant challenges but begin demonstrating promising capabilities in molecular toxicity repair

## Executive Summary
This paper introduces ToxiMol, the first benchmark for molecular toxicity repair using multimodal large language models (MLLMs). It constructs a standardized dataset of 660 toxic molecules across 11 primary tasks and 660 representative toxic molecules spanning diverse mechanisms and granularities. A prompt annotation pipeline with mechanism-aware and task-adaptive capabilities is designed, informed by expert toxicological knowledge. An automated evaluation framework, ToxiEval, integrates toxicity endpoint prediction, synthetic accessibility, drug-likeness, and structural similarity into a high-throughput assessment chain. The study evaluates 43 mainstream MLLMs and conducts multiple ablation studies to analyze key issues. Results show that while current MLLMs face significant challenges, they begin to demonstrate promising capabilities in toxicity understanding, semantic constraint adherence, and structure-aware editing.

## Method Summary
The ToxiMol benchmark is constructed from TDC toxicity datasets using structure-aware representative sampling (ECFP4 fingerprints + Butina clustering, threshold 0.4) to select 60 molecules per task. A three-stage prompt annotation pipeline creates molecule-specific prompts combining base templates, task-level annotations (describing ~30 toxicity mechanisms), and subtask-specific instructions. The ToxiEval evaluation chain applies five conjunctive criteria: Safety Score via TxGemma, QED≥0.5, SAS≤6, RO5≤1 violation, and SS≥0.4. The study evaluates 43 mainstream MLLMs with single-round QA (temperature=0.7, 3 candidates per sample), measuring success as at least one candidate passing all criteria.

## Key Results
- Current MLLMs achieve success rates of only 15-43% on molecular toxicity repair tasks
- Reasoning-enhanced MLLMs show limited gains over standard models, suggesting reasoning capability hasn't established clear advantage
- Multimodal perception (2D images) provides only modest performance gains (e.g., 30.5% vs. 28.9%)
- Type-T failures (toxicity bottleneck) dominate in LD50, DILI, and hERG tasks; Type-O failures (drug-likeness bottleneck) dominate in Tox21 and ToxCast

## Why This Works (Mechanism)

### Mechanism 1: Mechanism-Aware Prompt Annotation
Task-specific prompt engineering improves MLLM alignment to diverse toxicity mechanisms through a three-stage pipeline that loads base templates, injects task-level annotations describing ~30 toxicity mechanisms, and adds subtask-specific instructions before assembling multimodal prompts. Explicit mechanism-to-constraint mapping enables better semantic comprehension than generic templates.

### Mechanism 2: Multi-Criteria Conjunctive Evaluation
A strict conjunctive decision protocol filters candidates across toxicity, drug-likeness, and synthetic feasibility by applying five criteria (Safety Score via TxGemma, QED≥0.5, SAS≤6, RO5≤1 violation, SS≥0.4) with hard thresholds. Toxicity oracle provides sufficiently reproducible toxicity predictions to serve as stable evaluation coordinate.

### Mechanism 3: Structure-Aware Representative Sampling
Cluster-based sampling ensures structural diversity across chemical space by computing ECFP4 fingerprints, measuring Tanimoto similarity, and performing Butina clustering (distance threshold 0.4); centroids and boundary molecules are selected to cover structural space. Chemical space coverage improves benchmark robustness and prevents task dominance by large-sample endpoints.

## Foundational Learning

- **SMILES representation and 2D molecular images**
  - Why needed here: Inputs require both string-based (SMILES) and visual (2D structure) modalities; MLLMs must parse and correlate both
  - Quick check question: Can you convert a simple benzene structure to its SMILES string and back?

- **Toxicity mechanisms and endpoints**
  - Why needed here: 11 primary tasks span ~30 mechanisms (hERG blockade, genotoxicity, hepatotoxicity, etc.); understanding mechanism-to-structure relationships is essential for repair
  - Quick check question: What structural features typically contribute to hERG channel blockade?

- **Drug-likeness criteria (QED, RO5, SAS)**
  - Why needed here: ToxiEval enforces these constraints; candidates failing any are rejected regardless of toxicity reduction
  - Quick check question: What does a QED score of 0.3 indicate about drug-likeness?

## Architecture Onboarding

- **Component map**: Data construction (TDC datasets → structure-aware sampling → RDKit 2D rendering) → Prompt pipeline (base template + task annotations + subtask fragments → molecule-specific prompts) → MLLM inference (Prompt + SMILES + 2D image → 3 candidate SMILES) → Evaluation chain (ToxiEval: Validity check → TxGemma prediction → QED/SAS/RO5/SS computation → conjunctive decision)

- **Critical path**: Data sampling → prompt assembly → MLLM generation → ToxiEval evaluation. The evaluation chain is the bottleneck; each candidate requires multiple property computations.

- **Design tradeoffs**: Strict conjunctive evaluation (all 5 criteria) yields low success rates (15-43%) but ensures high-quality repairs; relaxed subsets (e.g., Ssafe+SAS+RO5) increase throughput. Using a single oracle (TxGemma-27B) improves reproducibility but may introduce systematic bias.

- **Failure signatures**:
  - Type-T (Toxicity Bottleneck): Candidates pass drug-likeness but fail safety (Ssafe≠1); dominant in LD50, DILI, hERG tasks
  - Type-O (Drug-Likeness Bottleneck): Candidates reduce toxicity but violate QED/SAS/RO5/SS; dominant in Tox21, ToxCast
  - Near-zero validity on DILI task indicates generation challenges, not just evaluation

- **First 3 experiments**:
  1. Run baseline evaluation on a subset (e.g., 60 molecules from hERG_Central) with Claude 3.7 Sonnet to reproduce reported success rates (~78%)
  2. Ablate the prompt pipeline: compare mechanism-aware prompts vs. generic templates on the same molecules to quantify annotation contribution
  3. Vibrate the ToxiEval thresholds (e.g., QED from 0.5 to 0.4, SS from 0.4 to 0.3) on fixed candidates to map sensitivity to constraint strictness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Why do reasoning-enhanced MLLMs fail to outperform standard counterparts in molecular toxicity repair tasks?
- **Basis in paper**: Section 4.2 states that "reasoning-enhanced variants exhibit only limited gains relative to their standard counterparts... suggesting that reasoning capability has not yet established a clear advantage in the current task setting."
- **Why unresolved**: The paper identifies the lack of performance gain but does not analyze whether the reasoning chains lack toxicological validity or fail to map to structural edits.
- **What evidence would resolve it**: An analysis of the intermediate "thinking" tokens to verify if models correctly identify toxicophores before attempting structural modifications.

### Open Question 2
- **Question**: To what extent does multimodal perception (2D images) actually aid toxicity repair compared to text-only inputs?
- **Basis in paper**: Appendix C.4 shows multimodal perception yields only a "modest but consistent gain" (e.g., 30.5% vs. 28.9%), noting that "simply providing molecular images does not guarantee substantial performance gains."
- **Why unresolved**: It remains unclear if models effectively utilize visual structural cues or rely primarily on SMILES token patterns.
- **What evidence would resolve it**: Attention map visualization to confirm if the model focuses on relevant visual substructures (toxicophores) in the image during the generation process.

### Open Question 3
- **Question**: How reliably do ToxiEval results using the TxGemma-Predict oracle transfer to real-world wet-lab validation?
- **Basis in paper**: Section 6 and Appendix A state, "We do not equate the outputs of this surrogate predictor with definitive real-world toxicological correctness" and note the risk of "unknown and unquantified systematic biases."
- **Why unresolved**: The benchmark uses a computational model as the ground truth ("model-as-judge"), which may diverge from experimental toxicity.
- **What evidence would resolve it**: Experimental synthesis and testing of MLLM-generated "repaired" molecules to validate predicted toxicity reductions.

## Limitations
- Reliance on TxGemma as a single toxicity oracle introduces potential systematic bias in evaluation
- Strict conjunctive evaluation protocol likely underestimates true MLLM capabilities by requiring all five criteria to pass
- Prompt engineering approach has not been fully validated against alternative prompting strategies
- Low validity rates for DILI task generation (10-20%) even for strong models suggest fundamental limitations

## Confidence

- **High**: The benchmark construction methodology (structure-aware sampling, standardized prompt pipeline, and automated evaluation framework) is well-specified and reproducible
- **Medium**: The characterization of MLLM performance bottlenecks (Type-T vs Type-O failures) is supported by ablation studies, though the underlying causes require further investigation
- **Medium**: The claim that MLLMs show "promising capabilities" in toxicity understanding and structure-aware editing is supported by observed performance patterns, but absolute success rates remain low

## Next Checks
1. Conduct multi-oracle evaluation by running the same candidates through multiple toxicity prediction models (not just TxGemma) to assess evaluation consistency and potential oracle bias
2. Perform controlled prompt ablation studies comparing mechanism-aware prompts against both generic templates and fine-tuned molecular models on identical molecule sets
3. Test relaxed evaluation protocols (e.g., disjunctive criteria where passing 4/5 conditions suffices) to establish performance bounds and identify which constraints are most restrictive