---
ver: rpa2
title: 'Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning Large
  Language Models with Less Overthinking'
arxiv_id: '2503.19602'
source_url: https://arxiv.org/abs/2503.19602
tags:
- uni00000013
- uni00000055
- uni00000048
- uni00000011
- uni00000046
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether Chain-of-Thought (CoT) prompting still
  benefits Reasoning Large Language Models (RLLMs) that already possess innate reasoning
  capabilities. The study analyzes the impact of Zero-shot CoT and Few-shot CoT across
  RLLMs ranging from 1.5B to 32B parameters on mathematical reasoning tasks.
---

# Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning Large Language Models with Less Overthinking

## Quick Facts
- arXiv ID: 2503.19602
- Source URL: https://arxiv.org/abs/2503.19602
- Authors: Yuyao Ge; Shenghua Liu; Yiwei Wang; Lingrui Mei; Lizhe Chen; Baolong Bi; Xueqi Cheng
- Reference count: 25
- Key outcome: Chain-of-Thought prompting improves RLLM performance in most cases, with distinct patterns based on model size and task complexity

## Executive Summary
This paper investigates whether Chain-of-Thought (CoT) prompting still benefits Reasoning Large Language Models (RLLMs) that already possess innate reasoning capabilities. Through systematic experiments across six mathematical reasoning benchmarks, the study reveals that CoT prompting significantly enhances RLLM performance in most cases, with distinct patterns based on model size and task complexity. Large-capacity models show minimal improvement on simple tasks but substantial gains on complex problems, while smaller models exhibit the opposite behavior. The research also identifies that CoT prompting effectively controls the distribution of thinking tokens and reasoning steps, reducing excessive reflections by up to 90% in some cases.

## Method Summary
The study evaluates Zero-shot CoT and Few-shot CoT prompting on RLLMs for mathematical reasoning tasks across six benchmarks: GSM8K, ASDiv, SAT_MATH, MATH, AIME24, and AMC23. Three prompting styles are compared: Direct prompting, Zero-shot CoT ("Let's think step by step"), and Few-shot CoT with 1-5 exemplars. Models tested include DeepSeek-R1-Distill series (1.5B-32B parameters), OpenO1-8B, and MARCO-7B. The experiments use greedy decoding with temperature=0, max tokens of 2048 for simple datasets and 32768 for complex datasets, running on 4× NVIDIA A800 hardware. Key metrics include accuracy, thinking token distribution, reasoning steps, and reflection frequency.

## Key Results
- CoT prompting significantly improves RLLM performance in most cases, with distinct patterns based on model size and task complexity
- Large-capacity models show minimal improvement on simple tasks but substantial gains on complex problems, while smaller models exhibit the opposite behavior
- One-shot CoT consistently yields superior performance compared to Few-shot CoT approaches for RLLMs

## Why This Works (Mechanism)
The study identifies that RLLMs overfit to reflection-related words, leading to excessive internal reasoning. CoT prompting mitigates this by providing external guidance that controls the distribution of thinking tokens and reasoning steps. The attention logits analysis reveals that RLLMs develop hyperattention to reflection tokens like "wait" and "check," which is effectively reduced through CoT guidance.

## Foundational Learning
- **Chain-of-Thought prompting**: Why needed - Provides step-by-step reasoning guidance; Quick check - Append "Let's think step by step" to prompts
- **Reasoning Large Language Models (RLLMs)**: Why needed - Models specifically trained for logical reasoning tasks; Quick check - Use DeepSeek-R1-Distill or similar models
- **Attention logits analysis**: Why needed - Reveals model focus on specific token patterns; Quick check - Extract attention weights from model layers
- **Reflection token identification**: Why needed - Quantifies excessive reasoning behavior; Quick check - Use keyword list from Appendix A.3
- **Few-shot vs Zero-shot comparison**: Why needed - Determines optimal prompting strategy; Quick check - Test with 1-5 exemplars systematically
- **Model scaling effects**: Why needed - Understands how performance varies with capacity; Quick check - Compare across 1.5B to 32B parameter models

## Architecture Onboarding

### Component Map
RLLM (1.5B-32B) -> Prompt Generator (Direct/Zero-shot/Few-shot) -> Token Counter -> Reflection Analyzer -> Accuracy Evaluator

### Critical Path
Prompt generation → Model inference → Token counting → Reflection analysis → Performance evaluation

### Design Tradeoffs
- Token limits: 2048 vs 32768 based on dataset complexity
- Exemplar count: One-shot vs few-shot balancing simplicity and guidance
- Decoding strategy: Greedy decoding (temperature=0) for reproducibility

### Failure Signatures
- Excessive token generation on complex problems
- Incorrect prompt format for different model families
- Inconsistent reflection counting due to keyword variations

### First Experiments
1. Test DeepSeek-R1-Distill-Qwen-1.5B on GSM8K with Direct, Zero-shot CoT, and one-shot CoT prompts
2. Implement reasoning step counting using LLAMA3.1-8B-INSTRUCT template
3. Verify reflection frequency reduction across prompting methods

## Open Questions the Paper Calls Out

### Open Question 1
Do the "overthinking" and attention overfitting phenomena generalize beyond mathematical reasoning to other complex domains such as code generation or logical deduction?
- Basis in paper: [Explicit] The authors explicitly state that their comprehensive analysis is limited to "mathematical reasoning tasks" (Abstract) and "six mainstream English mathematical benchmarks" (Section 3.2).
- Why unresolved: While the paper identifies a structural cause (attention overfitting to reflection words), it is unclear if this mechanism behaves identically in non-mathematical contexts where reflection patterns (e.g., syntax checking vs. logic checking) differ.
- What evidence would resolve it: Replicating the attention logits analysis on datasets like HumanEval (code) or LogiQA (logic) to verify if RLLMs exhibit the same high attention weights on reflection tokens like "Wait" or "Check."

### Open Question 2
What is the mechanistic explanation for why one-shot CoT consistently outperforms few-shot CoT for RLLMs, contrary to the behavior of standard LLMs?
- Basis in paper: [Inferred] The paper observes that "one-shot CoT consistently yields superior performance" (Abstract) and speculates that models "struggle with interference from multiple examples" (Section 4.2), but does not provide a definitive causal mechanism.
- Why unresolved: The authors verify the existence of the one-shot optimum but do not isolate specific failure modes (e.g., attention sink dilution or context confusion) that occur when $n > 1$ in RLLMs.
- What evidence would resolve it: A causal mediation analysis or attention head ablation study comparing interference patterns between one-shot and few-shot inputs in RLLMs versus base LLMs.

### Open Question 3
Can the identified overfitting to reflection-related tokens be mitigated through specific regularization techniques or data curation during the training phase?
- Basis in paper: [Explicit] The authors conclude that overthinking stems from "RLLMs overfitting to reflection-related linguistic tokens" (Section 5), but their proposed solution relies entirely on external inference-time guidance (CoT prompting).
- Why unresolved: The paper demonstrates that external guidance alleviates the symptom at inference time but does not explore if the model's internal weights can be regularized to prevent the "hyperattention" to reflection cues from developing in the first place.
- What evidence would resolve it: Training an RLLM with a modified loss function that penalizes excessive attention to specific reflection keywords and evaluating if the model maintains accuracy with fewer thinking tokens.

## Limitations
- The study focuses exclusively on mathematical reasoning tasks, limiting generalizability to other domains
- Critical implementation details for few-shot exemplars and reasoning step counting remain unspecified
- The analysis of attention patterns provides qualitative insights but lacks quantitative validation

## Confidence
- High confidence in core experimental setup and findings due to detailed methodological description
- Medium-High confidence in reproducibility given publicly available model checkpoints
- Medium confidence in generalizability due to focus on single family of RLLMs

## Next Checks
1. Implement the complete experimental pipeline with specified prompt formats and evaluate DeepSeek-R1-Distill-Qwen-1.5B on GSM8K using Direct, Zero-shot CoT, and one-shot CoT prompts, comparing accuracy and token counts against reported values
2. Reconstruct the reasoning step counting methodology using the provided prompt template and validate against ground truth annotations from at least two datasets
3. Reproduce the reflection frequency analysis by implementing the keyword-based counting system and verifying the 90% reduction claim for complex problem datasets