---
ver: rpa2
title: 'Improving RAG Retrieval via Propositional Content Extraction: a Speech Act
  Theory Approach'
arxiv_id: '2503.10654'
source_url: https://arxiv.org/abs/2503.10654
tags:
- content
- speech
- propositional
- queries
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the mismatch between user queries and document
  embeddings in Retrieval-Augmented Generation (RAG) systems, caused by the presence
  of pragmatic markers like interrogatives and polite requests in queries. Drawing
  on speech act theory, the study proposes extracting the propositional content from
  queries by removing illocutionary force indicators before embedding.
---

# Improving RAG Retrieval via Propositional Content Extraction: a Speech Act Theory Approach

## Quick Facts
- **arXiv ID**: 2503.10654
- **Source URL**: https://arxiv.org/abs/2503.10654
- **Reference count**: 40
- **Primary result**: Propositional content extraction from queries significantly improves semantic similarity with document embeddings in RAG systems

## Executive Summary
This paper addresses a fundamental mismatch in Retrieval-Augmented Generation systems where user queries containing pragmatic markers (interrogatives, polite requests) produce embeddings misaligned with the assertive statements typical in knowledge bases. Drawing on speech act theory, the study proposes extracting the propositional content from queries by removing illocutionary force indicators before embedding. Experiments on a Brazilian telecommunications news corpus demonstrate that this transformation significantly improves semantic similarity between query and document embeddings, particularly for queries with strong illocutionary markers like expressives and commissives.

## Method Summary
The method transforms user queries into simplified statements by removing speech act markers using GPT-4 with a structured prompt. Queries are classified into seven speech act types (assertive, interrogative, expressive, directive, commissive, declaration, verdictive), then converted to propositional form by stripping illocutionary force indicators while preserving core semantic content. The transformed queries are embedded using text-embedding-3-large (truncated to 256 dimensions via Matryoshka Representation Learning) and compared against pre-indexed document embeddings via cosine similarity. The approach was evaluated on 63 queries across seven speech act types against a corpus of 254,568 document vectors.

## Key Results
- Propositional content extraction significantly improved semantic similarity between query and document embeddings
- Maximum similarity improvements were largest for expressives (0.7504→0.8196) and commissives (0.6786→0.7530)
- Assertive queries showed minimal improvement (0.8137→0.8100) as they already contain propositional content
- Query length reduction correlated with improved retrieval, but over-stripping key phrases could decrease similarity

## Why This Works (Mechanism)

### Mechanism 1
Queries stripped of illocutionary markers achieve higher semantic similarity with document embeddings because knowledge bases predominantly contain assertive statements. The transformation removes pragmatic "noise" (question syntax, politeness terms, performative verbs) that displaces query embeddings away from the assertive-statement distribution of indexed documents. By converting "What did Artur Coimbra say?" to "Artur Coimbra said," the resulting vector lands closer to paragraph-level embeddings containing that factual content. Core assumption: Embedding models encode syntactic and pragmatic markers as meaningful dimensions that shift vectors even when core semantics are identical.

### Mechanism 2
Speech act categories with stronger illocutionary markers (expressives, commissives, directives) show larger similarity gains than assertives. Assertives already encode propositional content directly; other speech acts layer illocutionary force on top of propositions. Removing these layers yields greater proportional signal-to-noise improvement. Expressives (emotional markers like "I'm thrilled") showed maximum similarity jump from 0.7504 to 0.8196. Core assumption: Illocutionary force is encoded as separable semantic content by embedding models, not treated as negligible noise.

### Mechanism 3
Query length reduction correlates with improved retrieval but requires preserving key semantic phrases. Shorter, denser queries concentrate embedding dimensions on core content. However, removing essential phrases can decrease similarity (author notes dropping from 0.7444 to 0.7334 when key phrases omitted). Core assumption: Illocutionary markers are semantic distractors rather than disambiguating context.

## Foundational Learning

- **Speech Act Theory (Austin/Searle)**: The method directly operationalizes F(p) notation—separating illocutionary force F from propositional content p. Without this, the transformation rules seem arbitrary. Quick check: Can you identify which part of "Could you please tell me the capital of France?" is F versus p?

- **Dense Retrieval and Semantic Embedding Spaces**: The paper measures success via cosine similarity between query and document vectors. Understanding why mismatch occurs (different syntactic distributions) is prerequisite. Quick check: Why would "What is X?" and "X is Y" produce different embeddings despite shared semantics?

- **Query Reformulation Strategies**: This approach sits alongside HyDE, synthetic question generation, and other reformulation techniques. Understanding the landscape helps position this method. Quick check: How does propositional extraction differ from generating a hypothetical answer document?

## Architecture Onboarding

- **Component map**: Query → Propositional Extractor → Embedding Generator → Vector Search → Top-k Results
- **Critical path**: Query → Propositional Extractor → Embedding Generator → Vector Search → Top-k Results
- **Design tradeoffs**: 
  - LLM choice: GPT-4 accurate but expensive/slow; smaller models may struggle with indirect speech acts
  - Preservation vs. simplification: Aggressive stripping risks losing nuance; conservative approach leaves noise
  - Corpus awareness: If corpus contains mixed speech acts, bidirectional normalization may be needed
- **Failure signatures**:
  - Assertive queries showing degraded similarity (already minimal proposition transformation possible)
  - Over-shortened queries losing critical disambiguation terms
  - Indirect speech acts misclassified, receiving wrong transformation rules
  - Domain mismatch: prompt optimized for general text may fail on highly technical jargon
- **First 3 experiments**:
  1. Baseline comparison: Run 50 queries through standard embedding and propositional extraction; measure similarity delta per speech act type to validate paper's patterns in your domain
  2. Ablation on extraction prompt: Test simplified prompts (rule-based regex for question marks) vs. full LLM extraction to quantify accuracy-cost tradeoff
  3. Corpus distribution audit: Sample 200 documents; classify speech act distribution. If >80% assertive, propositional extraction is appropriate; if mixed, consider hybrid approach

## Open Questions the Paper Calls Out

- Do improvements in semantic similarity scores from propositional extraction translate to significant gains in standard Information Retrieval metrics (e.g., precision, recall) on established public benchmarks?
- How can the propositional extraction method be optimized to balance query brevity with the preservation of essential key phrases?
- Is the efficacy of propositional content extraction consistent across multilingual contexts and diverse domains outside of telecommunications?

## Limitations
- Study relies on a proprietary Brazilian telecommunications corpus and fixed set of 63 queries, limiting generalizability
- LLM-based transformation introduces variability and depends on GPT-4's accuracy in identifying speech acts
- Paper doesn't address potential degradation when documents contain mixed speech acts or when queries require disambiguation through length
- 256-dimensional truncation via Matryoshka Representation Learning could affect similarity scores differently across domains

## Confidence

- **High Confidence**: The core mechanism that removing illocutionary markers improves alignment between query and document embeddings, supported by direct similarity comparisons in the experiments
- **Medium Confidence**: The per-speech-act performance patterns (assertives showing minimal improvement vs. expressives showing large gains), as this depends on accurate speech act classification which wasn't independently validated
- **Low Confidence**: Generalizability claims beyond the specific corpus and query set, since no cross-domain validation was performed

## Next Checks

1. **Cross-domain replication**: Test the method on a different domain (e.g., medical literature or legal documents) with 50+ queries to verify the speech act-based improvement pattern holds
2. **Speech act classifier validation**: Implement an independent speech act classifier to verify the GPT-4 classifications, then re-run similarity comparisons to isolate transformation accuracy from classification accuracy
3. **Ablation on extraction granularity**: Compare three approaches—full LLM extraction, rule-based question removal, and no transformation—to quantify the value added by sophisticated speech act identification versus simple syntactic heuristics