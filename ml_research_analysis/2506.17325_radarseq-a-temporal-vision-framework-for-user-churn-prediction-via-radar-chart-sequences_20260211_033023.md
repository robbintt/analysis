---
ver: rpa2
title: 'RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar
  Chart Sequences'
arxiv_id: '2506.17325'
source_url: https://arxiv.org/abs/2506.17325
tags:
- churn
- temporal
- radar
- prediction
- customer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel temporal vision framework for predicting
  user churn in non-subscription gig platforms. The method models daily user behavior
  as a sequence of radar chart images, encoding multivariate features into visual
  representations.
---

# RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences

## Quick Facts
- arXiv ID: 2506.17325
- Source URL: https://arxiv.org/abs/2506.17325
- Reference count: 40
- This paper introduces a novel temporal vision framework for predicting user churn in non-subscription gig platforms, achieving gains of +17.7 in F1 score, +29.4 in precision, +16.1 in AUC, and +0.24 in Matthews Correlation Coefficient.

## Executive Summary
This paper introduces a novel temporal vision framework for predicting user churn in non-subscription gig platforms. The method models daily user behavior as a sequence of radar chart images, encoding multivariate features into visual representations. A hybrid CNN+LSTM architecture is proposed to capture both spatial patterns within each radar chart and temporal dependencies across the sequence. Extensive experiments on a real-world dataset demonstrate significant improvements over classical models and ViT-based radar chart baselines.

## Method Summary
The framework transforms daily behavioral feature vectors into radar chart images using polar coordinate transformation. A truncated MobileNetV2 (ImageNet pretrained) extracts spatial embeddings from each radar chart, which are then processed by a 2-layer bidirectional LSTM to capture temporal patterns. The concatenated final hidden states from both LSTM directions form a 256-dim representation for binary classification. The model is trained with BCE loss using Adam optimizer (lr=1e-4), batch size 256 with gradient accumulation, and mixed precision training. Early stopping is applied based on validation AUC with patience of 5 epochs.

## Key Results
- RadarSeq achieves F1 score of 0.847, precision of 0.884, and recall of 0.813
- ROC-AUC of 0.981 and Matthews Correlation Coefficient of 0.71
- Outperforms classical models by +17.7 F1, +29.4 precision, +16.1 AUC, and +0.24 MCC
- CNN+LSTM architecture shows 6.9-point AUC improvement over CNN+MLP baseline
- Radar chart encoding provides significant advantage over raw tabular features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoding daily behavioral features as radar chart images preserves multivariate spatial relationships and enables transfer learning from pretrained vision models.
- Mechanism: Each day's d-dimensional feature vector is rendered as a radar chart via polar coordinate transformation R: R^d → R^{H×W}. The polygonal shape encodes feature magnitudes as vertex positions, creating consistent geometric patterns that a CNN pretrained on ImageNet can process as visual features.
- Core assumption: The spatial arrangement of features in radar charts contains learnable patterns that correlate with churn behavior, and ImageNet-pretrained features transfer meaningfully to this synthetic image domain.
- Evidence anchors:
  - [abstract] "models user behavioral patterns as a sequence of radar chart images, each encoding day-level behavioral features"
  - [section 3.1] "Radar charts preserve spatial relationships between multi-dimensional data points, maintaining contextual dependencies and enabling visual feature learning via CNNs"
  - [corpus] Limited direct validation. Related work (Wang et al.) uses Gramian Angular Fields for time-series imaging, but radar chart encoding specifically lacks external corpus support.
- Break condition: If feature ordering on the radar chart is arbitrary and reshuffling doesn't affect model performance; or if ImageNet pretrained weights provide no benefit over random initialization.

### Mechanism 2
- Claim: The hybrid CNN+BiLSTM architecture captures both intra-day spatial feature patterns (via CNN) and inter-day behavioral trajectories (via BiLSTM), which static approaches miss.
- Mechanism: MobileNetV2 (truncated to penultimate layer) extracts a 1280-dim embedding z(t) from each radar image. A 2-layer bidirectional LSTM processes the sequence {z(1), ..., z(T)}, and the concatenated final hidden states from both directions form a 256-dim representation for classification.
- Core assumption: Churn manifests as detectable trajectories in the CNN embedding space over time, not merely as static feature configurations at a single time point.
- Evidence anchors:
  - [abstract] "integrating a pretrained CNN encoder with a bidirectional LSTM, our architecture captures both spatial and temporal patterns underlying churn behavior"
  - [section 4.3] "CNN+MLP vs. CNN+LSTM: Removing the LSTM results in a 6.9-point drop in AUC, confirming the critical role of temporal modeling"
  - [section 4.3] "Raw features vs. Radar images: Feeding raw tabular features into the LSTM (bypassing radar encoding) reduces AUC to 0.901"
  - [corpus] Related temporal modeling papers (BehaveGPT, user trajectory prediction) support sequence modeling for behavior, but do not validate this specific CNN+LSTM hybrid.
- Break condition: If ablation shows BiLSTM provides no gain over temporal mean-pooling; or if attention-based alternatives (Transformers) significantly outperform BiLSTM.

### Mechanism 3
- Claim: Using a 50-day sliding window aligned with a business-defined 45-day inactivity threshold provides more reliable supervision than clustering-based pseudo-labels.
- Mechanism: Churn labels are assigned based on observed 45-day consecutive inactivity (validated by operational insights). Sequences of 50 daily radar charts precede this threshold, with coarse (stride=5) and fine (stride=1) windowing to capture diverse temporal contexts.
- Core assumption: The 45-day inactivity threshold accurately identifies true churn in this domain, and the 50-day window captures the behavioral degradation period before disengagement.
- Evidence anchors:
  - [section 1] "a courier inactive for 45 consecutive days is typically considered lost to competitors or other employment—thus providing a robust, real-world validated definition of churn"
  - [section 3.5] "fixed-length windowing strategy with length T = 50 days, aligned to precede the 45-day churn threshold"
  - [section 3.7] "This real-world definition offers a more reliable supervision signal for training compared to unsupervised bootstrapping methods such as k-means on RFM features"
  - [corpus] Indirect support from customer LTV and churn prediction papers (HT-GNN, comprehensive churn analysis), but no direct validation of the 45-day threshold across platforms.
- Break condition: If threshold sensitivity analysis shows significant performance variation with different inactivity windows; or if the approach fails to generalize to platforms with different engagement patterns.

## Foundational Learning

- Concept: Radar Chart Encoding
  - Why needed here: You must understand how multivariate features map to polygonal shapes in polar coordinates to debug rendering issues and interpret model attention.
  - Quick check question: Given a 5-feature vector [0.8, 0.2, 0.5, 0.9, 0.3] normalized to [0,1], can you sketch the approximate radar chart shape and identify which features dominate the visual pattern?

- Concept: Bidirectional LSTM Sequence Classification
  - Why needed here: The model uses the concatenated final hidden states from both directions; understanding this extraction is critical for debugging and modifying the architecture.
  - Quick check question: For a 50-step sequence input to a BiLSTM with hidden size 128, what is the dimensionality of the representation passed to the classifier, and why does bidirectionality matter for churn prediction?

- Concept: Transfer Learning with Pretrained CNNs on Synthetic Images
  - Why needed here: MobileNetV2 pretrained on ImageNet is applied to radar charts; you need to understand what transfers and what doesn't.
  - Quick check question: Would you expect early convolutional layers (edge detectors) or later layers (object semantics) to be more useful for radar chart classification, and should you freeze or fine-tune them?

## Architecture Onboarding

- Component map:
  1. **Data Layer**: Raw transaction logs → feature engineering (Table 2) → daily feature vectors x(t) ∈ R^15
  2. **Rendering Layer**: Polar plot function transforms x(t) → 32×32 grayscale radar chart I(t)
  3. **CNN Encoder**: MobileNetV2 (truncated, ImageNet pretrained) → 1280-dim embedding per image
  4. **Temporal Encoder**: 2-layer BiLSTM (hidden=128) → 256-dim sequence representation
  5. **Classification Head**: Linear layer + sigmoid → churn probability

- Critical path:
  Feature engineering → radar rendering (offline, Pillow-SIMD) → batch sequence loading → CNN forward pass (per image, can parallelize) → LSTM sequence processing → binary classification. The 32×32 resolution and mixed-precision training are key efficiency choices.

- Design tradeoffs:
  - **Image resolution**: 32×32 vs. 224×224. Paper chose 32×32 for 70-80% I/O reduction. Tradeoff: may lose fine-grained visual details, but ablation suggests it's sufficient.
  - **CNN fine-tuning**: Initially frozen to reduce overfitting; end-to-end fine-tuning explored later. Tradeoff: frozen weights limit adaptation to radar chart domain.
  - **Window stride**: Coarse (stride=5) for diversity vs. fine (stride=1) for resolution near churn events. Both used to maximize coverage.

- Failure signatures:
  - **Low feature variance**: If radar charts all look similar (check visual inspection of rendered images across users/days), CNN embeddings will be uninformative.
  - **No embedding separation**: UMAP of LSTM outputs shows high overlap between churners and retained users (Figure 4 shows this is partially the case—success relies on temporal dynamics, not static separability).
  - **LSTM not learning**: If validation loss plateaus early and AUC doesn't improve beyond CNN-MLP baseline, temporal component may be ineffective.
  - **Class imbalance issues**: High false negative rate indicates model is conservative; may need threshold adjustment or reweighting.

- First 3 experiments:
  1. **Temporal ablation**: Compare CNN+LSTM vs. CNN+MLP (temporal mean-pooling) on the same data. Expect ~7-point AUC drop if temporal modeling matters (paper reports this).
  2. **Encoding ablation**: Feed raw tabular features directly to LSTM (bypass radar charts). Expect AUC drop to ~0.90 if visual encoding provides value.
  3. **Threshold sensitivity**: Vary the inactivity threshold (30, 45, 60 days) and measure label quality impact. This validates whether the 45-day definition is robust or arbitrary.

## Open Questions the Paper Calls Out

- **Multi-modal integration**: The paper explicitly calls for future work to "explore hybrid multi-modal representations (e.g., combining geolocation and radar sequences)" to potentially improve predictive performance.

- **International generalizability**: The authors note that "future work includes expanding evaluation to international datasets to substantiate generalization performance" given that the current model is trained on a specific regional platform.

- **Sparse activity logs**: The paper acknowledges that the method "depends on consistent daily data collection, which may limit applicability in platforms with sparse or irregular activity logs" and calls for investigation of performance under such conditions.

## Limitations

- **Underspecified radar rendering**: The exact radar chart rendering function (axis ordering, angular layout, normalization) is not defined, making exact reproduction difficult.
- **Implicit class imbalance handling**: Class imbalance is addressed through data sampling rather than explicit weighting, which may not generalize to more extreme imbalances.
- **Domain-specific feature engineering**: The model relies on platform-specific features engineered from transaction logs, limiting direct transfer to other churn prediction contexts.

## Confidence

- **High confidence**: Hybrid CNN+LSTM architecture outperforms static baselines (supported by direct ablation studies showing 6.9-point AUC improvement) and operational definition of churn (45-day inactivity) providing robust supervision.
- **Medium confidence**: Radar chart encoding mechanism demonstrates superior performance over raw features and ViT-based baselines, but lacks extensive external corpus validation and may depend on sensitive hyperparameter choices.
- **Medium confidence**: Substantial claimed gains over classical models (+17.7 F1, +29.4 precision, +16.1 AUC) appear significant but lack rigorous statistical significance testing across multiple random seeds or cross-validation folds.

## Next Checks

1. **Radar rendering ablation**: Train identical architectures using raw tabular features, Gramian Angular Fields, and the proposed radar charts to isolate the contribution of the visual encoding beyond pretraining benefits.

2. **Threshold sensitivity analysis**: Vary the 45-day inactivity threshold (30, 45, 60 days) and measure impact on model performance to validate whether the operational definition is robust or arbitrarily tuned.

3. **Cross-platform generalization**: Test the framework on a different gig platform or subscription-based service with different engagement patterns to assess domain specificity of the approach.