---
ver: rpa2
title: Multi-instance Learning as Downstream Task of Self-Supervised Learning-based
  Pre-trained Model
arxiv_id: '2505.21564'
source_url: https://arxiv.org/abs/2505.21564
tags:
- learning
- images
- dataset
- hematoma
- self-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deep multi-instance learning
  with large numbers of instances per bag, specifically in brain hematoma CT image
  classification. When 256 patches per slice are used, performance degrades due to
  spurious correlations.
---

# Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model

## Quick Facts
- **arXiv ID**: 2505.21564
- **Source URL**: https://arxiv.org/abs/2505.21564
- **Reference count**: 5
- **Primary result**: Self-supervised pre-training improves deep MIL accuracy by 5-13% and F1-score by 40-55% on brain hematoma CT images

## Executive Summary
This paper addresses the challenge of deep multi-instance learning (MIL) with large numbers of instances per bag in brain hematoma CT image classification. The authors demonstrate that when using 256 patches per slice, performance degrades due to spurious correlations between patches. To overcome this, they propose pre-training the instance feature extractor via self-supervised learning on patch images using contrastive learning and reconstruction tasks. The pre-trained encoder is then used as the feature extractor in downstream MIL, showing significant improvements over baseline Deep MIL without self-supervised pre-training.

## Method Summary
The proposed method involves two main stages: self-supervised pre-training and downstream MIL. In the pre-training stage, the instance feature extractor is trained on patch images using a combination of contrastive learning (to learn instance-level representations) and reconstruction tasks (to capture fine-grained details). The pre-trained encoder is then used in the downstream MIL task for brain hematoma detection and hypodensity classification. The MIL framework uses attention-based pooling to aggregate instance features into bag-level predictions. The approach is evaluated on brain CT images, showing that self-supervised pre-training effectively provides valid representations for large-instance MIL tasks.

## Key Results
- Self-supervised pre-training improves MIL accuracy by 5% to 13% over baseline Deep MIL without pre-training
- F1-score improvements of 40% to 55% achieved in hematoma detection and hypodensity classification tasks
- Outperforms ImageNet-supervised baselines in transfer learning settings
- Attention visualizations confirm better focus on relevant regions with pre-trained models

## Why This Works (Mechanism)
The method works by addressing the spurious correlation problem that occurs when using large numbers of patches per bag in MIL. By pre-training the feature extractor on patch images using self-supervised learning, the model learns meaningful representations that are less sensitive to random correlations between patches. The combination of contrastive learning (which encourages similar patches to have similar representations) and reconstruction tasks (which preserve detailed information) creates robust features that generalize better to the downstream MIL task. This pre-training effectively provides a better initialization for the instance feature extractor, leading to improved bag-level predictions through more reliable instance-level representations.

## Foundational Learning

**Multi-instance Learning (MIL)**: Learning framework where training instances are arranged in bags, with bag labels available but instance labels unknown. *Why needed*: MIL is essential for medical imaging where individual lesion annotations are expensive but bag-level diagnoses are available. *Quick check*: Verify that the problem involves weak supervision at the bag level rather than instance level.

**Self-supervised Learning**: Learning paradigm that creates supervisory signals from the data itself without manual labels. *Why needed*: Enables pre-training on large unlabeled medical image datasets to learn meaningful representations. *Quick check*: Confirm the pre-training task uses only input data without external labels.

**Contrastive Learning**: Self-supervised method that learns representations by comparing similar and dissimilar pairs of data. *Why needed*: Helps the model learn instance-level features that are robust to spurious correlations. *Quick check*: Verify that positive pairs are meaningfully similar while negative pairs are truly different.

**Attention-based Pooling**: Mechanism that weights instance contributions differently when aggregating to bag-level predictions. *Why needed*: Allows the model to focus on relevant instances while suppressing irrelevant ones in MIL. *Quick check*: Ensure attention weights correlate with instance relevance to the bag label.

## Architecture Onboarding

**Component Map**: Raw CT patches → Self-supervised Pre-training (Contrastive + Reconstruction) → Pre-trained Encoder → MIL Attention Pooling → Bag-level Classification

**Critical Path**: The critical path flows from raw patches through the self-supervised pre-training stage to the downstream MIL task. The quality of the pre-trained encoder directly determines the performance of the MIL classifier, making the pre-training stage the most critical component.

**Design Tradeoffs**: The main tradeoff is between computational cost of self-supervised pre-training and performance gains in MIL. Using 256 patches per slice maximizes local detail but increases spurious correlations, requiring more sophisticated pre-training. The choice between contrastive learning and reconstruction tasks involves balancing global structure preservation with fine-grained detail capture.

**Failure Signatures**: Performance degradation occurs when pre-training is insufficient or when patch sizes are mismatched between pre-training and MIL stages. Poor self-supervised learning leads to representations that don't transfer well, while inconsistent patch sizing causes domain shift between stages.

**3 First Experiments**:
1. Ablation study removing either contrastive or reconstruction components from pre-training to assess their individual contributions
2. Comparison of different patch sizes (e.g., 128 vs 256 patches per slice) to find optimal granularity
3. Evaluation of attention visualization differences between pre-trained and non-pre-trained models on the same test cases

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations

- Computational resource requirements for pre-training on large patch datasets may limit practical applicability
- Evaluation focuses exclusively on brain CT hematoma detection and hypodensity classification tasks, raising generalizability questions
- 256 patches per slice may not represent optimal sizing for all imaging modalities or disease types
- Comparison with ImageNet-supervised baselines lacks alternative medical-specific pre-training approaches

## Confidence

- **High confidence**: Self-supervised pre-training improves MIL performance on large-instance bags with consistent accuracy (5-13%) and F1-score (40-55%) gains
- **Medium confidence**: Outperformance of ImageNet-supervised pre-training in transfer learning settings is supported but limited to single medical domain
- **Medium confidence**: Spurious correlations are identified as primary cause of degradation with 256 patches, but not definitively proven through ablation studies

## Next Checks

1. Cross-domain validation: Test the self-supervised pre-trained MIL approach on at least two additional medical imaging tasks (e.g., chest X-ray pathology detection, brain tumor segmentation)

2. Computational efficiency analysis: Conduct detailed study of training time, GPU memory requirements, and inference latency comparisons between approaches

3. Alternative pre-training strategy comparison: Implement and evaluate at least two alternative pre-training strategies (e.g., medical image-specific pre-training, domain-adaptive ImageNet initialization)