---
ver: rpa2
title: 'GGBond: Growing Graph-Based AI-Agent Society for Socially-Aware Recommender
  Simulation'
arxiv_id: '2505.21154'
source_url: https://arxiv.org/abs/2505.21154
tags:
- social
- agent
- agents
- module
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces GGBond, a high-fidelity simulation platform\
  \ for recommender systems that addresses the limitations of static offline data\
  \ by integrating psychologically grounded AI agents with dynamic social networks.\
  \ The platform comprises simulated user agents equipped with a five-layer cognitive\
  \ architecture, including memory, emotion, preference, and a novel ICR\xB2 motivational\
  \ engine, enabling realistic decision-making processes."
---

# GGBond: Growing Graph-Based AI-Agent Society for Socially-Aware Recommender Simulation

## Quick Facts
- arXiv ID: 2505.21154
- Source URL: https://arxiv.org/abs/2505.21154
- Reference count: 40
- Introduces a high-fidelity simulation platform integrating psychologically grounded AI agents with dynamic social networks for recommender system evaluation

## Executive Summary
This paper presents GGBond, a simulation platform that addresses the limitations of static offline data in recommender system evaluation by integrating psychologically grounded AI agents with dynamic social networks. The platform combines a five-layer cognitive architecture for agents with a multi-layer heterogeneous social graph to model evolving trust and social ties. Agents interact with recommendation algorithms through autonomous decision-making processes, forming a feedback loop that captures long-term behavioral dynamics. The system achieves strong behavioral realism, with GGBond agents showing significantly higher distributional similarity to human rating patterns compared to static agents, while recommendation performance improves with interaction depth.

## Method Summary
GGBond builds a simulation platform comprising psychologically grounded AI agents with a five-layer cognitive architecture (memory, emotion, preference, ICR² motivational engine, and action execution) and a multi-layer heterogeneous social graph modeling trust dynamics. The platform integrates Stanford Facebook social graph data with MovieLens ratings, using RoBERTa to extract Big-Five personality traits from textual reviews. Agents autonomously decide whether to consume, rate, and share content based on ICR² motivational scoring that combines social proximity, curiosity, expected reciprocity, and risk aversion. The system runs discrete-time simulations over 30 rounds with pluggable recommender algorithms (Matrix Factorization, MultVAE, LightGCN), updating agent profiles and social ties after each interaction to form a closed feedback loop.

## Key Results
- GGBond agents achieve significantly higher behavioral realism than static agents: KL divergence 0.0108 vs. 0.1832, EMD 0.0900 vs. 0.3027
- Recommendation performance improves with interaction depth: LightGCN Recall@20 increases from 0.1721 to 0.1813, NDCG@20 from 0.3824 to 0.3975 after 30 rounds
- Agent satisfaction and acceptance rates increase substantially over simulation rounds, validating the platform's effectiveness in modeling realistic recommendation scenarios

## Why This Works (Mechanism)

### Mechanism 1
Psychologically grounded ICR² motivational scoring produces more human-aligned decision distributions than static rule-based agents. Each agent computes motivation score C = αI + βN + γR − δK integrating Intimacy, Novelty, Reciprocity potential, and Risk perception. If C ≥ θ (emotion-modulated threshold), the agent consumes content. This integrates Multi-Attribute Utility Theory with Mood-as-Information Theory: positive valence lowers θ, increasing acceptance likelihood.

### Mechanism 2
Multi-layer social graph fusion (interest + personality + structural homophily) enables more accurate trust propagation and preference drift than single-layer graphs. Three semantically distinct layers are aggregated via weighted combination w_uv = α·w_int + β·w_pers + γ·w_struct. This unified layer feeds both trust estimation and multi-round preference simulation. Accepted social recommendations trigger personality drift.

### Mechanism 3
Closed-loop feedback between agent decisions and recommender models generates behavioral trajectories that improve recommendation metrics over interaction depth. Each round: recommender generates candidates, agents evaluate via ICR² and decide actions, satisfaction updates internal states, updated profiles feed next recommender cycle. After T rounds, metrics improve (LightGCN Recall@20: 0.1721→0.1813).

## Foundational Learning

- **Concept: Multi-Attribute Utility Theory (MAUT)**
  - Why needed here: The ICR² engine uses MAUT to combine four decision factors into a single motivation score
  - Quick check: If α=0.4, β=0.35, γ=0.2, δ=0.25, and I=0.7, N=0.5, R=0.3, K=0.4, what is C? (Answer: C = 0.415)

- **Concept: Valence–Arousal Affect Space**
  - Why needed here: Agent emotion is modeled as 2D affect (V, A). Valence modulates decision threshold θ
  - Quick check: After a highly satisfying movie (M ≈ +1), does valence V increase or decrease, and how does this affect θ? (Answer: V increases; θ decreases, making agent more likely to accept future recommendations)

- **Concept: Exponential Forgetting Curve**
  - Why needed here: Episodic memory events decay via w_t = exp(−λ_mem·Δt). This weights recent interactions more heavily in reciprocity and trust calculations
  - Quick check: If λ_mem = 0.1 and Δt = 10 rounds, what is the memory weight? (Answer: w_t ≈ 0.368)

## Architecture Onboarding

- **Component map:**
  Sim-User Agents (5 modules) -> GGBond Graph (4 layers) -> Recommender Engine (MF/MultVAE/LightGCN) -> Simulation Scheduler

- **Critical path:**
  1. Initialize social graph from Stanford Facebook + Big-Five personality inference
  2. For each round: Recommender → Candidate items → Agent ICR² evaluation → Actions (watch/rate/share) → Update internal states + graph edges → Log metrics
  3. After T rounds: Compute Recall@20, NDCG@20, satisfaction, acceptance rate

- **Design tradeoffs:**
  - LLM dependency: Module 0 relies on DeepSeek-R1 for natural language generation; decoupled from decision logic for controllability but adds latency and cost
  - Linear vs. non-linear motivation: ICR² uses linear combination for transparency; may miss interaction effects between factors
  - Fixed vs. adaptive edge weights: Unified graph uses fixed α,β,γ; could be learned but risks overfitting to simulation artifacts

- **Failure signatures:**
  - Rating collapse: Agent distributions converge to uniform (KL divergence rises)
  - Trust explosion: Intimacy scores saturate at 1.0 without decay
  - Personality drift instability: ∆personality > 0.5 after 30 rounds
  - Recommendation plateau: Metrics stall before round 20

- **First 3 experiments:**
  1. Ablate ICR² factors: Run simulations with α=0 (no intimacy) or δ=0 (no risk) to measure each factor's contribution to behavioral realism
  2. Vary interaction depth: Compare 0, 10, 20, 30 rounds on held-out items to verify that metric improvements generalize
  3. Stress-test trust dynamics: Inject adversarial agents that consistently recommend low-quality items; measure if trust decay prevents cascading acceptance failures

## Open Questions the Paper Calls Out
None

## Limitations
- External validation of the ICR² motivational model remains limited; behavioral realism claims rely heavily on internal distributional metrics without independent human study replication
- Personality drift dynamics are simulated but not empirically grounded in longitudinal human data
- The unified graph weighting scheme (α,β,γ) is fixed rather than learned, potentially limiting adaptability

## Confidence

- **High confidence:** Multi-round simulation framework architecture and implementation feasibility
- **Medium confidence:** Behavioral realism metrics (KL/EMD similarity to human ratings)
- **Low confidence:** Long-term personality drift predictions and social influence propagation accuracy

## Next Checks
1. Conduct ablation studies removing individual ICR² factors (α=0, β=0, γ=0, δ=0) to quantify each component's contribution to behavioral realism
2. Validate personality drift magnitude against real-world longitudinal studies of social influence on preferences
3. Test model generalization by evaluating recommendation performance on held-out item sets after each simulation round to ensure improvements reflect genuine learning rather than overfitting to training rounds