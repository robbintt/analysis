---
ver: rpa2
title: 'Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference
  Feedback'
arxiv_id: '2512.01979'
source_url: https://arxiv.org/abs/2512.01979
tags:
- grounding
- wang
- visual
- chen
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately grounding natural
  language instructions to precise UI elements in complex interfaces. The authors
  propose Chain-of-Ground (CoG), a training-free multi-step grounding framework that
  leverages multimodal large language models (MLLMs) for iterative visual reasoning
  and refinement.
---

# Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback

## Quick Facts
- arXiv ID: 2512.01979
- Source URL: https://arxiv.org/abs/2512.01979
- Reference count: 40
- Key outcome: CoG achieves 68.4% accuracy on ScreenSpot-Pro (+4.8% over baseline), demonstrating iterative refinement improves GUI grounding.

## Executive Summary
This paper introduces Chain-of-Ground (CoG), a training-free multi-step grounding framework that leverages multimodal large language models (MLLMs) for iterative visual reasoning and refinement. Instead of direct prediction, CoG enables the model to progressively reflect and adjust its hypotheses through reference feedback, achieving more accurate and interpretable localization. On the ScreenSpot-Pro benchmark, CoG achieves 68.4% accuracy, surpassing the previous state-of-the-art by 4.8%. The method also shows strong performance on a new real-world industrial control panel dataset (TPanel-UI), improving over the baseline Qwen3-VL-235B by 6.9%. These results demonstrate the effectiveness of structured, iterative refinement in unlocking MLLMs' grounding potential without additional training.

## Method Summary
Chain-of-Ground is a training-free multi-step framework that transforms GUI grounding from a single-shot prediction into an iterative refinement process. The method works by first having an MLLM produce an initial coordinate prediction (anchor), then encoding that prediction as visual feedback (e.g., drawing a marker on the screenshot) or textual feedback (e.g., describing the coordinate). The annotated image or updated prompt is then fed back to the same or a different MLLM for refinement. This reflect–refine loop is repeated 1-2 times, allowing the model to progressively converge toward the correct grounding. The framework is evaluated using ScreenSpot-Pro and a new industrial control panel dataset (TPanel-UI), demonstrating consistent accuracy gains over single-step baselines.

## Key Results
- CoG achieves 68.4% accuracy on ScreenSpot-Pro (+4.8% over GTA1-32B baseline).
- Dual-step CoG achieves 90.0% accuracy on TPanel-UI (+6.9% over Qwen3-VL-235B baseline).
- Visual feedback (65.8%) outperforms text feedback (64.3%) in accuracy.
- Heterogeneous model combinations (UI-TARS-1.5-7B → Qwen3-VL-235B → Qwen3-VL-32B) yield best results.

## Why This Works (Mechanism)

### Mechanism 1: Iterative Self-Correction
Converting grounding from a single-shot prediction into an iterative process enables self-correction and progressive convergence. The model makes an initial coordinate prediction, receives explicit feedback about that prediction, and re-evaluates the instruction with this added context in a "reflect–refine loop." Over 2-3 steps, the model can adjust based on its own intermediate outputs. Core assumption: MLLMs possess latent reasoning capacity underutilized during single-shot inference; exposing intermediate predictions as inputs unlocks better performance.

### Mechanism 2: Visual Markers Provide Effective Spatial Feedback
Visual markers on predicted coordinates provide more effective spatial feedback than textual coordinates alone. By drawing a marker (e.g., a colored circle) directly on the screenshot at the predicted location, the model receives a spatially grounded reference it can compare against other UI elements. This enables relational reasoning (e.g., "the marked icon is similar but the correct one is to its right"). Core assumption: The model can interpret visual overlays as spatial references and update predictions based on their relationship to context.

### Mechanism 3: Heterogeneous Model Chaining
Chaining different models across steps leverages complementary strengths and mitigates individual "blind spots." A smaller, specialized model (e.g., UI-TARS-1.5-7B) produces the initial anchor, while larger, more general models (e.g., Qwen3-VL-235B/32B) refine it. Different models have different error profiles, so errors in one step may be caught in the next. Core assumption: Different MLLMs have systematically different failure modes for GUI grounding; chaining diversifies errors.

## Foundational Learning

- **Concept: Visual Grounding**
  - Why needed here: CoG's entire purpose is to improve visual grounding—mapping natural language instructions to precise screen coordinates in cluttered interfaces.
  - Quick check question: Given a screenshot with multiple similar buttons and the instruction "Click Submit," what visual or contextual cues might a model use to disambiguate?

- **Concept: Multimodal Large Language Models (MLLMs)**
  - Why needed here: CoG is a training-free framework built on top of existing MLLMs, using their vision-language reasoning without modification.
  - Quick check question: How does an MLLM differ from a standard object detector when processing a GUI screenshot?

- **Concept: Iterative Reasoning / Chain-of-Thought**
  - Why needed here: CoG's core insight is applying structured, iterative reasoning to grounding rather than treating it as one-shot prediction.
  - Quick check question: Why might decomposing a task into multiple reasoning steps improve outcomes over a single forward pass?

## Architecture Onboarding

- **Component map**: Instruction + Screenshot → Anchor MLLM → [x₁, y₁] → Feedback Generator → Annotated Screenshot → Refinement MLLM → [x₂, y₂] → ... → Final Prediction

- **Critical path**: `Instruction + Screenshot → Anchor MLLM → [x₁, y₁] → Feedback Generator → Annotated Screenshot → Refinement MLLM → [x₂, y₂] → ... → Final Prediction`

- **Design tradeoffs**:
  - Iteration count vs. cost: Triple-step yields +4.8% over baseline but triples inference time/cost; dual-step gives +3.1% at lower cost.
  - Feedback modality: Visual markers (65.8%) outperform text-only (64.3%) but require image manipulation.
  - Marker size: Large markers (r=100px) slightly outperform small (r=10px) but risk occlusion.
  - Model selection: Heterogeneous combinations (e.g., UI-TARS→Qwen3-VL) may outperform homogeneous but add orchestration complexity.

- **Failure signatures**:
  - Stagnant predictions: Coordinates barely change across steps → model ignoring feedback.
  - Oscillation: Predictions alternate between locations → feedback causing confusion.
  - Drift: Final prediction further from target than initial anchor → feedback actively misleading.

- **First 3 experiments**:
  1. Establish a single-step baseline with your chosen MLLM (e.g., Qwen3-VL-32B) on a held-out grounding set.
  2. Implement dual-step CoG with visual feedback (large red circle); measure accuracy gain; ablate to text-only feedback.
  3. Test a heterogeneous model combination (smaller anchor, larger refiner) and compare against homogeneous dual-step.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the Chain-of-Ground (CoG) framework generalize effectively to visual grounding domains beyond GUIs, such as natural images or document understanding?
  - Basis in paper: [explicit] The authors state in the Limitations section that "our experiments are conducted on a specific benchmark, and the generalization of our model combinations and feedback strategies to other domains remains an open question."
  - Why unresolved: The evaluation was restricted to ScreenSpot-Pro (digital interfaces) and TPanel-UI (industrial panels), leaving the efficacy of iterative reference feedback on unstructured visual data unverified.
  - What evidence would resolve it: Applying the CoG framework to standard general-purpose visual grounding benchmarks (e.g., RefCOCO, Flickr30k) and comparing performance against single-step baselines.

- **Open Question 2**: Can the high computational cost and inference latency of the multi-step CoG process be reduced to enable real-time application?
  - Basis in paper: [explicit] The paper notes that "compared to single-pass approaches, our iterative method incurs higher inference time and computational cost."
  - Why unresolved: The framework requires sequential MLLM calls (Anchor → Refinement → Refinement), which linearly increases latency, potentially hindering deployment in interactive agents requiring immediate feedback.
  - What evidence would resolve it: Research into optimization techniques, such as parallel step execution or early-exit mechanisms, that maintain the 4-5% accuracy gain while significantly reducing wall-clock inference time.

- **Open Question 3**: What is the optimal strategy for selecting heterogeneous model combinations (e.g., Anchor vs. Refinement models) to maximize complementarity?
  - Basis in paper: [inferred] While the ablation study shows that combining diverse models (UI-TARS-1.5-7B → Qwen3-VL-235B → Qwen3-VL-32B) yields the best results, the authors only "speculate" that this is due to compensating "blindspots" without defining a selection principle.
  - Why unresolved: The optimal triple-step combination was likely found empirically; it remains unclear if smaller models are universally better anchors or if specific architectural differences drive the improvement.
  - What evidence would resolve it: A systematic analysis correlating the error distributions of various MLLs with the performance of their combinations to establish a predictive metric for model pairing.

## Limitations

- **Dataset Specificity**: The framework is evaluated only on GUI-focused benchmarks (ScreenSpot-Pro, TPanel-UI), leaving generalization to other visual domains uncertain.
- **Computational Cost**: Multi-step inference significantly increases latency and inference cost compared to single-pass approaches.
- **Model Dependency**: Performance depends heavily on the underlying MLLM capabilities and prompt design, which are not standardized in the paper.

## Confidence

**High Confidence**: The core mechanism of iterative refinement through reference feedback is well-supported by experimental results showing consistent accuracy gains over baselines across multiple model combinations and feedback modalities.

**Medium Confidence**: The claim that heterogeneous model combinations outperform homogeneous ones is supported but could benefit from more extensive ablation studies across diverse model families.

**Medium Confidence**: The assertion that visual markers provide superior feedback to text coordinates is supported by direct comparison but lacks analysis of why visual feedback is more effective.

## Next Checks

1. **Prompt Template Validation**: Implement a systematic prompt engineering study to identify effective anchor and refinement prompts. Compare performance across different prompt styles (e.g., direct instruction vs. step-by-step reasoning) to isolate the impact of prompt design on grounding accuracy.

2. **Coordinate System Clarification**: Conduct controlled experiments to determine the correct coordinate normalization scheme. Test absolute pixels, normalized coordinates, and percentage-based outputs against ground truth to identify the model's expected format.

3. **Evaluation Protocol Specification**: Reach out to the authors or conduct experiments to determine the exact success criteria for ScreenSpot-Pro. Test different IoU thresholds and point-in-bbox criteria to establish the evaluation protocol used in the paper.