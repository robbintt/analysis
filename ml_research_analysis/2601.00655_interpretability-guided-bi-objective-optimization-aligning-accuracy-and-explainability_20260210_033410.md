---
ver: rpa2
title: 'Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability'
arxiv_id: '2601.00655'
source_url: https://arxiv.org/abs/2601.00655
tags:
- igbo
- path
- training
- gradients
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IGBO introduces a principled framework for training interpretable
  models by integrating structured domain knowledge through bi-objective optimization.
  The method encodes feature importance hierarchies as a Directed Acyclic Graph (DAG)
  and employs Temporal Integrated Gradients (TIG) to measure feature importance, addressing
  Out-of-Distribution (OOD) issues via an Optimal Path Oracle that generates data-manifold-aware
  integration paths.
---

# Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability

## Quick Facts
- arXiv ID: 2601.00655
- Source URL: https://arxiv.org/abs/2601.00655
- Authors: Kasra Fouladi; Hamta Rahmani
- Reference count: 26
- Primary result: IGBO framework enforces structured feature importance hierarchies with minimal accuracy loss

## Executive Summary
IGBO introduces a principled framework for training interpretable models by integrating structured domain knowledge through bi-objective optimization. The method encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and employs Temporal Integrated Gradients (TIG) to measure feature importance, addressing Out-of-Distribution (OOD) issues via an Optimal Path Oracle that generates data-manifold-aware integration paths. Theoretical analysis establishes convergence properties and robustness to gradient noise, while empirical results demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines on time-series data.

## Method Summary
IGBO optimizes a main task loss and an interpretability loss simultaneously, where the interpretability loss enforces expected feature importance differences specified by a DAG with interval constraints. The method uses Temporal Integrated Gradients for sequential data, generating integration paths via an Optimal Path Oracle trained to stay within the data distribution. A geometric projection mapping enables simultaneous descent on both objectives even when gradients conflict, combining convex combinations when aligned and orthogonal projections when opposing. DAG construction can be expert-specified or derived from data using Central Limit Theorem approximations of feature importance statistics.

## Key Results
- Achieves >80% DAG satisfaction rate while maintaining <5% accuracy degradation versus standard training
- Outperforms standard regularization baselines on time-series data in enforcing interpretability constraints
- Geometric projection mapping guarantees simultaneous descent for conflicting gradients, proven through theoretical convergence analysis

## Why This Works (Mechanism)

### Mechanism 1: Geometric Projection Mapping for Simultaneous Descent
The projection function P enables guaranteed descent for both task loss and interpretability loss, even when their gradients conflict. When gradients are aligned (g₁·g₂ ≥ 0), P uses convex combination λg₁ + (1-λ)g₂. When conflicting (g₁·g₂ < 0), P projects each gradient onto the orthogonal complement of the other, combining λg₂^⊥₁ + (1-λ)g₁^⊥₂ to ensure v·g₁ > 0 and v·g₂ > 0 simultaneously. Core assumption: Gradients are reasonably estimated and the loss landscape permits Pareto-stationary solutions.

### Mechanism 2: Optimal Path Oracle for Distribution-Aware Integration
A learned path generator produces integration paths that remain within the data manifold, yielding more reliable TIG attributions than straight-line interpolation. The oracle G is trained with dual objectives: minimize path length (L_path) and maximize in-distribution probability (L_valid = -Σlog D(p_i)), where D is a frozen pre-trained GAN discriminator. During inference, K oracle anchor points are combined with M-K interpolated points. Core assumption: A pre-trained discriminator provides a reliable proxy for Pr(p ∈ data distribution).

### Mechanism 3: Interval-Constrained DAG Enforcement via Differentiable Hinge Loss
The interpretability loss H(θ) enforces expected importance differences within user-specified intervals [ε_u,v, δ_u,v], providing nuanced control beyond binary constraints. For each DAG edge (u→v), compute batch-mean difference d_u,v(θ;B) = mean[H_v - H_u]. The hinge loss activates only when d_u,v falls outside [ε, δ], penalizing both under- and over-satisfaction. Core assumption: The DAG structure correctly encodes domain knowledge and edge intervals are achievable given model capacity and data.

## Foundational Learning

- **Concept: Integrated Gradients (IG)**
  - Why needed here: TIG extends IG to sequential data; understanding the baseline integral formulation is prerequisite to grasping the OOD problem and oracle solution.
  - Quick check question: Given input X and baseline X', write the IG formula and explain why straight-line paths may traverse OOD regions.

- **Concept: Multi-Objective Optimization / Pareto Stationarity**
  - Why needed here: IGBO formalizes training as bi-objective; convergence guarantees target Pareto-stationary points, not single-objective minima.
  - Quick check question: Define Pareto-stationarity. Why does a descent direction for both objectives require v·g₁ > 0 AND v·g₂ > 0?

- **Concept: Central Limit Theorem for Bounded Random Variables**
  - Why needed here: DAG construction relies on CLT approximations for batch-mean feature importance scores; variance estimation determines edge orientation confidence.
  - Quick check question: If H_k ∈ [0,1] with true mean μ_k ≈ 0.95 and batch size n=50, what correction might be needed before applying the Gaussian approximation?

## Architecture Onboarding

- **Component map:**
  - F_θ (Main Model) -> G (Optimal Path Oracle) -> D (Discriminator)
  - DAG G=(V,E) constrains feature importance relationships
  - H_k(X,θ) aggregates TIG over time for each feature

- **Critical path:**
  1. Pre-training phase: Train GAN → freeze D → train Oracle G (minimize L_path + L_valid)
  2. Per-iteration training loop: Sample batch → query Oracle for path → compute TIG → aggregate to H_k → compute d_u,v → compute H(θ;B) → compute gradients → apply projection P → update θ
  3. DAG construction (optional): Run CLT-based edge orientation on validation data

- **Design tradeoffs:**
  - K vs M: Larger K improves path fidelity but increases oracle inference cost
  - λ (trade-off parameter): Higher λ prioritizes task loss; lower λ prioritizes interpretability
  - Batch size B: Larger B reduces gradient noise but increases memory for TIG computation
  - DAG edge count |E|: More constraints improve interpretability control but increase optimization difficulty

- **Failure signatures:**
  - Gradient collapse: ||g₁|| < ε or ||g₂|| < ε early in training
  - Oracle path drift: L_valid plateaus high → discriminator may be miscalibrated
  - DAG satisfaction oscillates: Edge violations persist across epochs
  - Accuracy degradation > 5%: Constraint intervals too tight

- **First 3 experiments:**
  1. Sanity check—Oracle transferability: Train oracle on subset, apply to held-out samples; measure L_valid and path length vs straight-line baseline.
  2. Ablation—Projection vs convex combination: Compare IGBO with P(g₁,g₂,λ) against naive λg₁+(1-λ)g₂ on synthetic data with known conflicting gradients.
  3. End-to-end validation—DAG enforcement: Train F_θ on real time-series with expert-specified 5-node DAG; report DAG satisfaction rate, accuracy vs baseline, and TIG attribution stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the geometric projection mapping be generalized to optimize three or more objectives simultaneously (e.g., adding fairness or robustness to accuracy and interpretability)?
- Basis in paper: Section VI states, "Extending this principle to handle three or more concurrent objectives... is a challenging but promising direction."
- Why unresolved: The current theoretical guarantees and projection function are strictly defined for the bi-objective case.
- What evidence would resolve it: A theoretical extension of the gradient combination rule for k-vector scenarios and empirical demonstration on a multi-objective benchmark.

### Open Question 2
- Question: Can the interpretability DAG extracted via IGBO effectively guide the training of smaller, task-specialized models through structured knowledge transfer?
- Basis in paper: Section VI notes, "The structured interpretability DAG... can guide the training of a smaller, task-specialized model... using interpretability as a mechanism for structured knowledge transfer."
- Why unresolved: The paper validates IGBO on single models but does not experiment with using the learned DAG constraints to train a separate, compressed student model.
- What evidence would resolve it: Experiments showing a student model trained under a teacher's extracted DAG constraints achieves higher efficiency with minimal accuracy loss.

### Open Question 3
- Question: How can the DAG construction methodology be adapted for black-box models where gradients are inaccessible, specifically utilizing human expert input?
- Basis in paper: Section VI discusses exploring "pairwise comparison data from domain experts to construct the DAG, bypassing gradient computation entirely."
- Why unresolved: The current framework relies on differentiable Temporal Integrated Gradients (TIG).
- What evidence would resolve it: A modified IGBO framework accepting non-gradient importance priors and empirical results on non-differentiable models or datasets with strong expert consensus.

## Limitations
- Empirical validation relies on synthetic data and specific time-series benchmarks without extensive ablation studies
- Effectiveness of optimal path oracle versus straight-line TIG is asserted but not directly benchmarked against other OOD-aware integration methods
- Interval-based DAG enforcement advantages over simpler regularization approaches are not thoroughly quantified

## Confidence

- **High Confidence:** Theoretical convergence guarantees for the projection mapping, DAG acyclicity conditions, and batch gradient variance scaling
- **Medium Confidence:** Empirical DAG satisfaction rates and accuracy preservation claims, as they are demonstrated on specific datasets but lack extensive ablation
- **Low Confidence:** The practical impact of the optimal path oracle on attribution quality and the necessity of interval constraints over simpler alternatives

## Next Checks

1. **Oracle Ablation:** Compare IGBO's DAG satisfaction and attribution stability using oracle-generated paths vs. straight-line TIG on a held-out test set to quantify the oracle's contribution.

2. **Constraint Granularity:** Train IGBO with binary DAG constraints (ε = δ = 0) and interval constraints on the same data; measure DAG satisfaction, accuracy, and convergence speed to assess the benefit of interval flexibility.

3. **Gradient Conflict Stress Test:** Construct synthetic data where task and interpretability gradients are known to conflict; verify the projection mapping's ability to find simultaneous descent directions and compare convergence to convex combination baselines.