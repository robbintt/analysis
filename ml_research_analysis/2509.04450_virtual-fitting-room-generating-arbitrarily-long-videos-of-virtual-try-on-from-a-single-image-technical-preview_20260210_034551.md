---
ver: rpa2
title: 'Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On
  from a Single Image -- Technical Preview'
arxiv_id: '2509.04450'
source_url: https://arxiv.org/abs/2509.04450
tags:
- video
- try-on
- virtual
- conference
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents Virtual Fitting Room (VFR), a method for generating
  arbitrarily long high-resolution virtual try-on videos from a single user image.
  The core innovation is an auto-regressive segment-by-segment generation approach
  that addresses two key challenges: local smoothness between adjacent segments and
  global temporal consistency across the entire video.'
---

# Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview

## Quick Facts
- **arXiv ID:** 2509.04450
- **Source URL:** https://arxiv.org/abs/2509.04450
- **Authors:** Jun-Kun Chen; Aayush Bansal; Minh Phuoc Vo; Yu-Xiong Wang
- **Reference count:** 40
- **Primary result:** Generates arbitrarily long high-resolution virtual try-on videos (720×1152 at 24FPS) from a single user image while maintaining both local smoothness and global temporal consistency.

## Executive Summary
Virtual Fitting Room (VFR) presents a novel method for generating minute-scale virtual try-on videos from a single user image, garment image, and motion reference. The core innovation is an auto-regressive segment-by-segment generation approach that maintains both local smoothness between adjacent segments and global temporal consistency across the entire video. This is achieved through two key components: an anchor video that captures the user's whole-body appearance in a 360° view, and prefix video conditioning that ensures smooth transitions between segments. The method demonstrates effectiveness across four evaluation tasks including 360° garment consistency, hand-body interaction faithfulness, and free viewpoint rendering capabilities.

## Method Summary
VFR is built on the Dress&Dance diffusion model foundation and employs an auto-regressive segment-by-segment generation strategy. The method generates minute-scale virtual try-on videos by first creating a short 360° "anchor" video in an "A" pose to capture the user's whole-body appearance. During the main generation loop, each segment is conditioned on both this anchor video and the prefix (overlapping frames) from the previous segment to ensure global consistency and local smoothness respectively. The system outputs 720×1152 resolution videos at 8 FPS, which can be upscaled to 24 FPS, with the entire generation process taking approximately 1-2 hours for a 30-second video.

## Key Results
- Generates minute-scale videos (720×1152 at 24FPS) with both local smoothness and global temporal consistency
- Achieves subject consistency scores of 92.84-94.06, background consistency scores of 95.38-96.53, and motion smoothness scores of 98.35-99.37
- Demonstrates capability for 360° garment consistency (5s), 360° human+garment consistency (30s), hand-body interaction faithfulness (90s), and arbitrary poses (30-60s)
- Implicitly learns 3D consistency, enabling free viewpoint rendering without explicit 3D supervision

## Why This Works (Mechanism)

### Mechanism 1: Anchor Video as a Global Appearance Constraint
The framework decouples appearance definition from motion generation. By generating a short, comprehensive 360° view (the anchor) of the user in a standard pose (e.g., "A" pose), the model establishes a fixed "ground truth" for the subject's appearance. During auto-regressive generation, conditioning on this anchor forces every segment to replicate the specific garment details and identity features defined in the anchor, preventing appearance "drifting."

### Mechanism 2: Prefix Conditioning for Local Smoothness
To stitch segments together without visible seams, the model takes the final frames (prefix) of the previously generated segment as an input condition. This provides strong temporal context, effectively extending the video from the last known state rather than generating blindly, bridging the gap between independent segment generations.

### Mechanism 3: Implicit 3D Consistency Emergence
The generation of the 360° anchor video requires the model to maintain appearance across viewpoints. This constraint results in outputs that are geometrically consistent enough to be reconstructed into 3D meshes via NeRF. The consistency pressure effectively forces the 2D generator to behave like a 3D renderer.

## Foundational Learning

- **Concept: Diffusion Model Conditioning (CondNets/ControlNets)**
  - **Why needed here:** The VFR architecture relies heavily on adding external conditions (Garment, Pose/Motion, Anchor Video, Prefix Video) to a base video diffusion model.
  - **Quick check question:** How does adding a "Prefix CondNet" differ from simply concatenating the previous frame as input?

- **Concept: Auto-regressive Sequence Modeling**
  - **Why needed here:** The core strategy for long video generation is segment-by-segment auto-regression.
  - **Quick check question:** What are the primary failure modes (e.g., error accumulation) in auto-regressive generation that the "Anchor" mechanism attempts to solve?

- **Concept: Virtual Try-On (VTON) Geometry**
  - **Why needed here:** Understanding how garments warp and fit bodies is essential to diagnosing why the "Anchor" video uses a specific "A" pose to capture whole-body appearance.
  - **Quick check question:** Why might a simple 2D warping approach fail for the 360° consistency required by VFR?

## Architecture Onboarding

- **Component map:** User Image + Garment Image + Motion Reference -> Anchor Video Generation (5s 360° A-pose) -> Auto-regressive Loop (Prefix + Anchor + Motion) -> Segment Generation -> Immediate Refiner -> Concatenated Video

- **Critical path:**
  1. **Inputs:** Single User Image + Garment Image + Motion Reference
  2. **Anchor Gen:** Generate 5s 360° "A" pose video (The "Outline")
  3. **Auto-regressive Loop:**
     - Take **Prefix** (tail of previous segment) + **Anchor Video** + Motion Reference
     - Generate next segment
     - **Refine** segment (Immediate Refiner)
  4. **Output:** Concatenated long video (720×1152, 8 FPS -> Upscaled to 24 FPS)

- **Design tradeoffs:**
  - **Compute vs. Consistency:** Generating the Anchor video adds upfront cost but is cheaper than training on long-video data
  - **FPS Strategy:** Generation at 8 FPS and upscaling reduces computational load compared to native 24 FPS generation

- **Failure signatures:**
  - **"Morphing" artifacts:** Sudden changes in background or body shape indicate Prefix conditioning failure
  - **"Drifting" identity:** Garment patterns or facial features changing over time indicate Anchor conditioning failure
  - **Slow generation:** 1–2 hours for a 30s video indicates expected computational overhead

- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run the "No Anchor" (NA) variant to observe temporal drift vs. full model
  2. **Consistency Stress Test:** Generate a 90s video with "Hand-Body Interaction" to test Anchor maintenance under self-occlusion
  3. **3D Emergence Test:** Reconstruct the generated 360° Anchor video into a mesh to verify "Free Viewpoint" claim

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the VFR framework be optimized to achieve near real-time generation speeds for long video virtual try-on?
- **Basis in paper:** The authors explicitly identify computational efficiency as a limitation, noting it "takes 1∼2 hours to generate a 30s video" and stating, "we leave this speed-up as an interesting future work."
- **Why unresolved:** The current auto-regressive, segment-by-segment diffusion process is computationally expensive, making the method currently impractical for interactive, real-time e-commerce applications despite high quality results.
- **What evidence would resolve it:** A demonstration of the VFR architecture generating video segments at significantly faster rate (reducing 30s video generation time from hours to minutes or seconds) without compromising resolution or temporal consistency.

### Open Question 2
- **Question:** How can the implicit 3D consistency observed in VFR be explicitly extended to generate fully controllable 4D (dynamic 3D) content?
- **Basis in paper:** The authors state they "believe that our work will pave the way for the transition from long videos to arbitrary 4D content, where a user can both change camera perspective and motion."
- **Why unresolved:** While the paper demonstrates ability to reconstruct a static 3D mesh from the generated anchor video, it does not demonstrate the ability to render arbitrary viewpoints for dynamic motion sequences (4D) directly within the generation pipeline.
- **What evidence would resolve it:** A system that utilizes the VFR backbone to synthesize novel views of the human subject in motion simultaneously with temporal generation, rather than reconstructing 3D post-hoc from a fixed trajectory.

### Open Question 3
- **Question:** Is the specific "A" pose 360-degree anchor video the optimal mechanism for enforcing global consistency, or could a more compact latent representation achieve similar or better results with lower computational overhead?
- **Basis in paper:** The paper proposes a specific "anchor video" (a short 360° video of the subject in an "A" pose) to guide generation, but does not explore if the "whole-body appearance" information could be encoded more efficiently.
- **Why unresolved:** The paper validates the anchor video against "No Anchor" baselines but does not explore alternative conditioning methods that could reduce processing time and potential error propagation from the anchor generation step.
- **What evidence would resolve it:** An ablation study comparing the current video-based anchor condition against alternative conditioning methods (such as canonical image grids or explicit 3D priors) in terms of both consistency metrics and generation speed.

## Limitations
- **Computational cost:** Generation takes 1-2 hours for a 30-second video, limiting practical deployment
- **Architectural underspecification:** Key components like CondNet architectures and training procedures lack sufficient detail for complete reproduction
- **Dependency on base model:** Success is partially contingent on Dress&Dance foundation model's capabilities

## Confidence

- **High Confidence:** The auto-regressive segment-by-segment approach for long video generation, use of VBench metrics for consistency evaluation, and observed 3D consistency emergence through 360° anchor generation
- **Medium Confidence:** Effectiveness of anchor video conditioning for global consistency and prefix conditioning for local smoothness based on ablation studies
- **Low Confidence:** Exact CondNet architectures, training procedures, and mechanism by which implicit 3D consistency emerges from 2D generation due to limited technical detail

## Next Checks

1. **Ablation Replication:** Reproduce the "No Anchor" (NA) variant to verify temporal drift observed in Figure 4 and quantify impact on subject consistency scores

2. **Long Video Consistency:** Generate a 90s video with complex hand-body interactions to test whether the anchor maintains garment details under self-occlusion and extreme poses

3. **3D Reconstruction Verification:** Attempt to reconstruct the generated 360° anchor video into a 3D mesh using NeRF or similar methods to validate the "free viewpoint" capability claim