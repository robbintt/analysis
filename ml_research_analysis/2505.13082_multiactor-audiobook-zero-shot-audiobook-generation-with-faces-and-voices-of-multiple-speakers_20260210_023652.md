---
ver: rpa2
title: 'MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices
  of Multiple Speakers'
arxiv_id: '2505.13082'
source_url: https://arxiv.org/abs/2505.13082
tags:
- each
- speaker
- generation
- story
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MultiActor-Audiobook is a zero-shot system for generating emotionally
  expressive audiobooks without requiring manual annotation or costly model training.
  It introduces two key processes: Multimodal Speaker Persona Generation (MSP) and
  LLM-based Script Instruction Generation (LSI).'
---

# MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices of Multiple Speakers

## Quick Facts
- **arXiv ID**: 2505.13082
- **Source URL**: https://arxiv.org/abs/2505.13082
- **Reference count**: 0
- **Primary result**: Zero-shot audiobook generation system that produces emotionally expressive speech for multiple speakers without manual annotation or training, achieving competitive performance against commercial baselines.

## Executive Summary
MultiActor-Audiobook introduces a zero-shot system for generating audiobooks with multiple speakers, each with distinct voices and emotional expressions. The system uses two key processes: Multimodal Speaker Persona Generation (MSP) to create consistent speaker identities through AI-generated faces and voices, and LLM-based Script Instruction Generation (LSI) to produce emotionally appropriate prosody instructions for each sentence. Evaluated on 12 stories from the ReedsyPrompts dataset, the system demonstrates competitive performance against commercial solutions like ElevenLabs, with strong speaker consistency and emotional expressiveness achieved through careful integration of LLM-driven persona extraction, face generation, and voice synthesis.

## Method Summary
The system processes stories through two main stages. First, MSP extracts speaker descriptions from text using GPT-4o, generates photorealistic faces with Stable Diffusion, and synthesizes matching voice samples using FleSpeech's masked generation. Second, LSI uses GPT-4o to identify which speaker is speaking each sentence and generates detailed emotional and prosodic instructions based on full story context. These components feed into FleSpeech for sentence-by-sentence synthesis, using the generated face, voice sample, and instructions to produce emotionally expressive audiobook output. The entire pipeline operates without training, relying on zero-shot inference across all components.

## Key Results
- Human MOS scores show competitive performance: 2.9/3.9 (ours vs 4.2/3.7 for ElevenLabs) for character-voice consistency
- MLLM evaluations demonstrate 0.225-point improvement over baselines
- Speaker embedding similarity reaches 51.334, indicating strong voice consistency
- Pitch turning points count reaches 146,885.1, reflecting high emotional expressiveness

## Why This Works (Mechanism)
The system achieves zero-shot multi-speaker audiobook generation by leveraging LLM-driven persona extraction and instruction generation combined with multimodal synthesis. The MSP process creates consistent speaker identities by linking visual (faces), auditory (voices), and textual (descriptions) representations, while LSI ensures emotionally appropriate prosody by generating context-aware instructions for each sentence. The tight integration of these components with FleSpeech enables consistent voice quality and emotional expression across diverse speaker personas without requiring any training data or manual configuration.

## Foundational Learning
- **Multimodal persona consistency**: Why needed - Ensures each character maintains consistent voice and appearance throughout the story. Quick check - Verify speaker embedding similarity remains stable across multiple sentences from the same character.
- **Context-aware instruction generation**: Why needed - Enables emotionally appropriate prosody by considering full story context. Quick check - Compare instruction quality when using local vs. full story context as input.
- **Zero-shot face-to-voice mapping**: Why needed - Eliminates need for speaker-specific training data. Quick check - Test voice quality using real vs. AI-generated faces as input to FleSpeech.

## Architecture Onboarding

**Component map**: ReedsyPrompts text → GPT-4o (speaker extraction) → Stable Diffusion (face generation) → GPT-4o (per-sentence speaker ID) → GPT-4o (emotional instructions) → FleSpeech (TTS synthesis) → Audiobook output

**Critical path**: GPT-4o LSI instruction generation → FleSpeech synthesis. The quality of emotional instructions directly determines the expressiveness of the final output, making this the most critical component.

**Design tradeoffs**: The system trades off computational cost (heavy GPT-4o usage) for zero-shot flexibility and ease of deployment. While manual annotation would be cheaper at scale, the zero-shot approach enables rapid processing of new stories without any setup.

**Failure signatures**: Inconsistent emotion across sentences indicates problems with LSI context processing. Poor voice quality or artifacts suggest FleSpeech struggles with AI-generated faces. Missing or confused speakers point to MSP extraction failures.

**First experiments**:
1. Test MSP pipeline on a single story to verify speaker extraction, face generation, and voice synthesis work end-to-end
2. Validate LSI instruction quality by generating and listening to audiobook output for one paragraph
3. Benchmark speaker embedding similarity across multiple sentences from the same character to verify consistency

## Open Questions the Paper Calls Out
- How does MultiActor-Audiobook performance scale with stories containing significantly more than the average 4.3 speakers tested, and at what point does speaker confusion or voice similarity degradation occur?
- To what extent does the domain mismatch between FleSpeech's training data (TED lecturers) and AI-generated photorealistic faces introduce systematic voice synthesis errors or reduced emotional expressiveness?
- What is the sensitivity of audiobook quality to LLM prompt design in the MSP and LSI processes, and can smaller open-source models achieve comparable instruction generation quality?

## Limitations
- FleSpeech struggles with AI-generated faces due to training domain mismatch, limiting voice quality and expressiveness
- Performance scaling with larger speaker counts remains untested beyond the average 4.3 speakers in the evaluation dataset
- Heavy reliance on GPT-4o for critical pipeline components creates computational cost and potential prompt sensitivity issues

## Confidence
- Human MOS scores (2.9/3.9 vs 4.2/3.7): Medium - lacks rater demographics and inter-rater agreement metrics
- MLLM evaluations and quantitative metrics (speaker embedding similarity, pitch turning points): High - objective and reproducible
- Overall approach novelty and effectiveness: Medium - combines established methods in novel pipeline but with known component limitations

## Next Checks
1. Reproduce the speaker persona and voice generation pipeline with a known subset of ReedsyPrompts stories and verify output quality and consistency
2. Compare generated audiobooks against the stated baselines using both human evaluation (MOS) and objective metrics (speaker embedding similarity, pitch turning points) to confirm reported performance
3. Conduct ablation studies to isolate the contribution of each component (e.g., removing LSI or using real vs. AI-generated faces) and assess their impact on character consistency and expressiveness