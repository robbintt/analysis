---
ver: rpa2
title: 'From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics'
arxiv_id: '2601.23048'
source_url: https://arxiv.org/abs/2601.23048
tags:
- formulation
- problem
- mathematical
- reasoning
- scenario
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Contextual mathematical reasoning\u2014the ability to formulate\
  \ and solve math problems embedded in narrative scenarios\u2014remains a fundamental\
  \ challenge for large language models. This paper introduces ContextMATH, a benchmark\
  \ that transforms standard math problems into Scenario Grounding (SG) and Complexity\
  \ Scaling (CS) variants to test this capability."
---

# From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics

## Quick Facts
- arXiv ID: 2601.23048
- Source URL: https://arxiv.org/abs/2601.23048
- Reference count: 40
- Key outcome: Contextual mathematical reasoning remains a fundamental challenge for large language models

## Executive Summary
This paper investigates the gap between abstract and contextual mathematical reasoning capabilities in large language models (LLMs). The authors introduce ContextMATH, a benchmark that transforms standard math problems into Scenario Grounding (SG) and Complexity Scaling (CS) variants to test contextual reasoning abilities. Evaluations across 61 models reveal significant performance drops when problems are embedded in narrative scenarios, with open-source models declining by 13% on SG and 34% on CS, while proprietary models drop by 13% and 20%, respectively. Error analysis indicates that incorrect problem formulation is the dominant failure mode, particularly as problem difficulty increases.

The study demonstrates that while training with scenario data improves performance, substantial gaps remain between abstract and contextual reasoning capabilities. Correct problem formulation emerges as a prerequisite for successful contextual mathematical reasoning, with its sufficiency improving as model scale increases. However, both formulation and reasoning remain complementary bottlenecks that limit overall performance. The findings highlight contextual mathematical reasoning as a central unsolved challenge for LLMs, despite progress in understanding and reasoning capabilities.

## Method Summary
The paper introduces ContextMATH, a benchmark designed to evaluate contextual mathematical reasoning by transforming standard math problems into two variants: Scenario Grounding (SG) and Complexity Scaling (CS). SG problems are embedded in narrative scenarios requiring the model to identify relevant mathematical elements, while CS problems maintain complexity while preserving the contextual framework. The benchmark tests 61 models, including both open-source and proprietary systems, across various mathematical domains. The authors conduct comprehensive error analysis to identify failure modes, distinguishing between formulation errors (incorrectly setting up the problem) and reasoning errors (incorrect solution paths). Performance is measured by comparing abstract problem accuracy against contextual variants, revealing the additional challenge posed by contextual embedding.

## Key Results
- Open-source models show 13% accuracy decline on Scenario Grounding and 34% on Complexity Scaling tasks
- Proprietary models experience 13% and 20% accuracy drops respectively on the same tasks
- Correct problem formulation is identified as a prerequisite for successful contextual mathematical reasoning
- Training with scenario data improves performance but does not eliminate the performance gap between abstract and contextual reasoning

## Why This Works (Mechanism)
The research works by systematically isolating the challenge of contextual mathematical reasoning from pure mathematical ability. By transforming standard math problems into scenario-embedded variants, the benchmark forces models to perform dual tasks: understanding narrative context and extracting relevant mathematical components. This approach reveals that the primary bottleneck is not mathematical reasoning per se, but rather the ability to correctly formulate problems from narrative descriptions. The mechanism demonstrates that contextual reasoning requires an additional layer of cognitive processing beyond abstract mathematical operations, specifically the translation of real-world scenarios into formal mathematical representations.

## Foundational Learning
- Contextual reasoning: Understanding how to extract mathematical problems from narrative scenarios; needed to bridge the gap between real-world problems and formal mathematics; quick check: can the model identify relevant quantities and relationships in a story problem?
- Problem formulation: The ability to translate narrative descriptions into mathematical expressions; critical as incorrect formulation prevents correct solutions regardless of reasoning ability; quick check: does the model set up equations that match the problem constraints?
- Mathematical abstraction: Converting concrete scenarios into abstract mathematical representations; essential for solving problems that don't present mathematical structure explicitly; quick check: can the model identify variables and relationships without explicit mathematical notation?
- Error analysis methodology: Systematic categorization of failure modes to identify bottlenecks; necessary for targeted model improvement; quick check: are errors predominantly formulation or reasoning failures?
- Benchmark design: Creating controlled variants of problems to isolate specific capabilities; important for understanding distinct components of mathematical reasoning; quick check: do performance drops isolate to contextual components only?

## Architecture Onboarding

### Component Map
ContextMATH Benchmark -> Problem Transformation (Abstract to SG/CS) -> Model Evaluation -> Error Analysis -> Performance Comparison

### Critical Path
1. Problem transformation from abstract to contextual variants
2. Model evaluation across 61 systems
3. Error analysis distinguishing formulation vs reasoning failures
4. Performance comparison between abstract and contextual tasks

### Design Tradeoffs
- Open vs proprietary models: Open models show larger performance gaps but are more accessible for research
- SG vs CS variants: SG tests scenario understanding while CS tests complexity handling in context
- Error categorization: Distinguishing formulation from reasoning errors reveals complementary bottlenecks
- Training data augmentation: Scenario training improves performance but doesn't eliminate gaps

### Failure Signatures
- Dominant formulation errors indicate models struggle with problem comprehension
- Performance decline with difficulty suggests scaling limitations in contextual understanding
- Persistent gaps between abstract and contextual performance reveal fundamental limitations

### First Experiments
1. Evaluate a baseline model on abstract problems, then on transformed SG and CS variants to measure contextual reasoning gap
2. Conduct error analysis on model outputs to categorize failures as formulation or reasoning errors
3. Train a model with scenario-augmented data and re-evaluate on ContextMATH to measure improvement

## Open Questions the Paper Calls Out
The paper does not explicitly enumerate open questions, but several emerge from the findings:
- What architectural modifications could better support contextual mathematical reasoning?
- How can models be trained to more effectively bridge the gap between narrative scenarios and formal mathematical representations?
- Are there specific mathematical domains where contextual reasoning poses greater challenges than others?
- How does the scale of models relate to improvements in contextual understanding versus pure reasoning ability?

## Limitations
- The benchmark focuses on mathematical reasoning, limiting generalizability to other domains requiring contextual understanding
- Performance improvements from scenario training are incremental, suggesting fundamental architectural limitations
- The study does not address potential biases in scenario design that might advantage certain problem types
- The error analysis methodology, while systematic, may not capture all failure modes in complex contextual reasoning tasks

## Confidence
- Key finding (contextual reasoning gap): High
- Error analysis methodology: Medium
- Scenario training effectiveness: Medium
- Benchmark design validity: High

## Next Checks
1. Test whether fine-tuning on scenario data closes the performance gap between abstract and contextual reasoning
2. Evaluate whether chain-of-thought prompting improves formulation accuracy on contextual problems
3. Investigate if multi-modal models show different performance patterns on contextMATH compared to text-only models