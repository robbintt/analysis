---
ver: rpa2
title: 'Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models
  For Cross-Task Generalization'
arxiv_id: '2502.12672'
source_url: https://arxiv.org/abs/2502.12672
tags:
- fine-tuning
- speech-ft
- speech
- pre-trained
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of maintaining cross-task generalization
  ability in speech representation models during fine-tuning. Fine-tuning often degrades
  cross-task generalization due to representational drift, where the model loses information
  learned during pre-training.
---

# Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization

## Quick Facts
- arXiv ID: 2502.12672
- Source URL: https://arxiv.org/abs/2502.12672
- Authors: Tzu-Quan Lin; Wei-Ping Huang; Hao Tang; Hung-yi Lee
- Reference count: 40
- Primary result: Speech-FT reduces phone error rate from 5.17% to 3.94% and increases speaker identification accuracy from 81.86% to 84.11% on HuBERT fine-tuning

## Executive Summary
Speech-FT addresses the problem of representational drift during fine-tuning of speech models, where fine-tuning degrades cross-task generalization by losing information learned during pre-training. The proposed two-stage framework combines stable fine-tuning (freezing downsampling module and warming up task prediction model) with weight-space interpolation to merge pre-trained and fine-tuned models. This approach consistently improves performance across diverse supervised, unsupervised, and multitask fine-tuning scenarios while maintaining higher feature similarity with the pre-trained model.

## Method Summary
Speech-FT uses a two-stage fine-tuning framework. First, stable fine-tuning freezes the downsampling module (CNN feature extractor) and gradually trains only the task prediction model D for the first 10% of steps, then trains the full model (except downsampling) for remaining steps. Second, weight-space interpolation merges pre-trained weights θ₀ and fine-tuned weights θ′ using θ̂ = (1−α)·θ₀ + α·θ′ with α=0.25. This preserves feature similarity while retaining task-specific improvements. The method requires single RTX 3090, learning rates 10⁻⁴–2×10⁻⁴, and batch sizes 16–32.

## Key Results
- Speech-FT reduces phone error rate from 5.17% to 3.94% and word error rate from 6.38% to 5.75% on ASR tasks
- Speaker identification accuracy increases from 81.86% to 84.11% after Speech-FT fine-tuning
- Maintains ~0.95 cosine similarity with pre-trained features across layers, compared to ~0.85 for weight-space regularization
- Achieves SUPERB score of 881.07 on SID fine-tuning, compared to 836.08 without stable fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Freezing the downsampling module and warming up the task prediction head reduces representational drift during fine-tuning. The CNN feature extractor captures low-level frequency patterns that are task-agnostic, so freezing it preserves these features. Training only the task prediction model D during the first β% of steps allows it to stabilize before backpropagating gradients that could corrupt pre-trained representations. Core assumption: Low-level acoustic features learned during pre-training remain useful across all downstream speech tasks.

### Mechanism 2
Linear interpolation in weight-space restores feature similarity with the pre-trained model while retaining task-specific improvements. Given pre-trained weights θ₀ and fine-tuned weights θ′, the merged model is θ̂ = (1−α)·θ₀ + α·θ′. This preserves the task vector τ = θ′ − θ₀ at scaled magnitude α·τ while anchoring the model to its original representations. Core assumption: The interpolation path between pre-trained and fine-tuned models lies in a functionally benign region where linear combinations preserve meaningful representations.

### Mechanism 3
Layerwise Linear Feature Connectivity (LLFC) explains why interpolation preserves feature geometry. Per-layer features along the interpolation path approximate a linear mixture: Fℓ(θ̂(α)) ≈ (1−α)·Fℓ(θ₀) + α·Fℓ(θ′). Regression analysis shows R² > 0.98, confirming that representation geometry is preserved during interpolation. Core assumption: Models fine-tuned from the same initialization satisfy LLFC.

## Foundational Learning

- Concept: Representational drift
  - Why needed here: Explains the core problem Speech-FT addresses—fine-tuning changes representations excessively, causing loss of cross-task generalization.
  - Quick check question: If you fine-tune a speech model on speaker identification only, what happens to its performance on phoneme recognition?

- Concept: Cross-task generalization
  - Why needed here: Speech-FT aims to improve a representation model's performance across multiple downstream tasks, not just the fine-tuning task.
  - Quick check question: Why does the paper discard the task prediction model D after fine-tuning and re-train downstream models from scratch for evaluation?

- Concept: Model merging via task arithmetic
  - Why needed here: Speech-FT's interpolation step can be interpreted as adding a scaled task vector (τ = θ′ − θ₀) to the pre-trained model.
  - Quick check question: If α = 0.25, what fraction of the task vector is retained in the final model?

## Architecture Onboarding

- Component map: Pre-trained encoder (Transformer + CNN downsampling) -> Downsampling module (CNN, frozen) -> Task prediction model D (task-specific head) -> Interpolation operation: θ̂ = (1−α)·θ₀ + α·θ′

- Critical path:
  1. Load pre-trained model θ₀
  2. Freeze downsampling module
  3. Attach task prediction model D for fine-tuning task
  4. Train D only for β% of steps (default: 10%)
  5. Train full model (except downsampling) for remaining steps → θ′
  6. Discard D, interpolate: θ̂ = (1−α)·θ₀ + α·θ′
  7. Evaluate θ̂ on SUPERB with new downstream models

- Design tradeoffs:
  - α value: Lower α preserves more generalization but may sacrifice task-specific gains; α = 0.25 worked well across experiments
  - β value: Longer warm-up stabilizes fine-tuning but delays representation learning; 10% was sufficient in experiments
  - Freezing downsampling: Protects low-level features but prevents adaptation to domains with different acoustic properties

- Failure signatures:
  - Cross-task performance drops significantly on unrelated tasks → α too high (too much drift)
  - Task-specific performance barely improves → α too low (insufficient task knowledge)
  - Fine-tuning diverges → β too low (unstable task prediction model corrupting representations)

- First 3 experiments:
  1. Replicate HuBERT fine-tuning on ASR (TED-LIUM) with Speech-FT; verify PER drops from 5.17% toward 3.94% and SID accuracy improves from 81.86% toward 84.11%
  2. Ablate stable fine-tuning: fine-tune without freezing downsampling or D warm-up, compare SUPERB scores to confirm degradation
  3. Sweep α ∈ {0.1, 0.25, 0.5, 0.75} on a single fine-tuning task; plot feature cosine similarity vs. α to verify tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
Can Speech-FT be effectively applied to generative spoken language modeling or large language model architectures? The study exclusively evaluates discriminative speech representation models (HuBERT, wav2vec 2.0) on the SUPERB benchmark and does not test generative capabilities or text-based architectures.

### Open Question 2
Does Speech-FT mitigate catastrophic forgetting in sequential task adaptation (continual learning) scenarios? The current evaluation focuses on single-stage fine-tuning or multi-task merging, leaving the model's ability to handle a stream of sequential tasks untested.

### Open Question 3
Is there a theoretically grounded method to automatically determine the optimal interpolation scaling factor (α)? The authors fix α=0.25 based on empirical observation that it "generalizes well," suggesting a lack of adaptive mechanisms for this hyperparameter.

## Limitations
- The approach may not generalize to models with significantly different architectures or those trained from different initializations
- The method requires storing both pre-trained and fine-tuned models during interpolation, doubling memory usage
- The optimal α value (0.25) appears somewhat arbitrary and may not be universally optimal

## Confidence
- High confidence: The stable fine-tuning mechanism (freezing downsampling module) effectively reduces representational drift, supported by consistent ablation results across experiments
- Medium confidence: Weight-space interpolation reliably restores cross-task generalization, though the optimal α value (0.25) appears somewhat arbitrary
- Medium confidence: LLFC explains why interpolation preserves feature geometry, but the theoretical justification remains incomplete

## Next Checks
1. Test Speech-FT on models with significantly different architectures (e.g., different encoder depths or attention mechanisms) to verify LLFC holds beyond HuBERT variants
2. Conduct experiments with α values outside the 0.1-0.5 range to determine if performance degrades monotonically or if other optimal points exist
3. Evaluate Speech-FT on non-speech domains (e.g., vision or NLP) where fine-tuning typically causes representational drift to assess generalizability of the approach