---
ver: rpa2
title: 'SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code
  Evaluation'
arxiv_id: '2505.24324'
source_url: https://arxiv.org/abs/2505.24324
tags:
- code
- language
- benchmark
- swift
- programming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SwiftEval, the first language-specific benchmark
  for evaluating LLM-generated code in Swift. The authors identify significant issues
  with existing multilingual benchmarks (HumanEval-XL, MultiPL-E) when applied to
  Swift, including critical translation errors and lack of support for Swift-specific
  features.
---

# SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation

## Quick Facts
- arXiv ID: 2505.24324
- Source URL: https://arxiv.org/abs/2505.24324
- Reference count: 32
- Key outcome: Introduces SwiftEval, the first language-specific benchmark for Swift LLM code generation, showing significantly higher validity through stronger model size-score correlation (0.50 vs 0.30) compared to HumanEval.

## Executive Summary
This paper introduces SwiftEval, the first language-specific benchmark for evaluating LLM-generated code in Swift. The authors identify critical issues with existing multilingual benchmarks when applied to Swift, including translation errors and lack of support for Swift-specific features. Unlike approaches that translate Python benchmarks, SwiftEval consists of 28 hand-crafted problems designed to leverage Swift's unique features like static typing, protocols, and optionals. The benchmark evaluates 44 popular code LLMs and demonstrates that even small, language-tailored benchmarks can provide more insightful results than large, general-purpose ones. Results show significant performance drops for models on Swift-specific problems, particularly for smaller models.

## Method Summary
SwiftEval consists of 28 hand-crafted problems with natural language queries, code contexts, entry points, and 3-5 unit tests each. The evaluation uses the pass@k metric with k=1, generating 20 completions per problem using temperature=0.2 and top_p=0.95. Code is compiled with Apple Swift compiler v5.10 on macOS 14.6.1, and unit tests are executed if compilation succeeds. The benchmark evaluates 44 code LLMs using model-specific prompt templates from tokenizer configurations, with results aggregated to compute pass@1 scores and bootstrap confidence intervals.

## Key Results
- SwiftEval shows a stronger correlation between model size and performance (0.50) compared to HumanEval (0.30), suggesting better measurement validity
- 86% of evaluations failed due to compilation errors, while only 14% failed due to unit test failures
- Smaller models showed the most significant performance drops on Swift-specific problems
- Existing translated benchmarks like HumanEval-XL and MultiPL-E contain critical translation errors that make problems unsolvable in Swift

## Why This Works (Mechanism)

### Mechanism 1: Native Language Feature Constraints
Replacing automatic translation pipelines with hand-crafted, language-native problems exposes model failures in type systems and semantics that translated benchmarks miss. By enforcing Swift-specific constraints—such as fixed-size tuples, optionals, and strict static typing—the benchmark forces models to generate syntactically and semantically valid code rather than "translated pseudocode" that fails at compile time. This mechanism assumes models optimized for Python-centric benchmarks rely on translation patterns that break under strict compilation rules of languages like Swift.

### Mechanism 2: Compilation as a High-Pass Filter
Using a compiled language (Swift) as the evaluation target significantly shifts the failure mode from runtime logic errors to compile-time syntax and type errors. Unlike interpreted Python, Swift requires successful compilation before execution, filtering out code that "looks" correct but violates strict syntax or type rules. This mechanism effectively penalizes hallucinated APIs or syntax errors that might execute in looser environments, assuming that functional correctness in Python is insufficient to verify structural and semantic validity in compiled languages.

### Mechanism 3: Validity via Size-Score Correlation
A higher correlation coefficient between model size and benchmark score indicates a benchmark is measuring capability (scaling laws) rather than memorization or noise. The authors argue that HumanEval's weak correlation (0.30) suggests contamination (overfitting), allowing small models to score artificially high. SwiftEval's stronger correlation (0.50) implies it captures a skill distribution that scales with model parameters, providing a more faithful signal of generalization. This assumes model capability scales with parameter count, so a valid benchmark should reflect this positive trend.

## Foundational Learning

- Concept: **pass@k Metric**
  - Why needed here: This is the standard evaluation metric used in the paper to estimate functional correctness. Understanding it is required to interpret the results tables.
  - Quick check question: Why did the authors choose `pass@1` with 20 completions (temperature 0.2) instead of a higher `k` value or deterministic greedy decoding?

- Concept: **Benchmark Contamination (Data Leakage)**
  - Why needed here: The paper's motivation rests on the hypothesis that existing high scores on HumanEval are partly due to models memorizing the test set during pre-training.
  - Quick check question: How does the weak correlation (0.30) between model size and HumanEval score serve as evidence for potential memorization?

- Concept: **Static vs. Dynamic Typing Semantics**
  - Why needed here: To understand why translating Python (dynamic) problems to Swift (static) creates invalid tests (e.g., generic `AnyHashable` vs. specific types, tuple resizing).
  - Quick check question: Why is a problem requiring the expansion of an array into a tuple "completely unsolvable" in Swift compared to Python?

## Architecture Onboarding

- Component map: Dataset (28 hand-crafted problems) -> Generator (44 Code LLMs with prompt templates) -> Execution Environment (macOS 14.6.1 + Swift 5.10 compiler) -> Evaluator (compile + run unit tests + calculate pass@1)

- Critical path: 1. Select model and apply native prompt template 2. Generate 20 completions per problem 3. Attempt compilation against Swift 5.10 4. If compiled, execute unit tests 5. Aggregate successes to compute pass@1 and bootstrap confidence intervals

- Design tradeoffs: Quantity vs. Quality (28 hand-crafted problems vs. 164+ translated ones for language idiom coverage); Sandboxing (direct compiler invocation vs. containerized execution for security assessment)

- Failure signatures: Compilation Error (86% - syntax errors, incorrect type signatures, Pythonic idioms); Test Failure (14% - code compiles but fails logical assertions); Template Mismatch (non-default prompt templates cause suboptimal tokenization)

- First 3 experiments: 1. Baseline Establishment (run top 3 models on SwiftEval to establish SOTA ceiling and verify correlation); 2. Translation vs. Native Ablation (reverse-translate problems to Python-style logic and compare mid-sized model performance); 3. Error Classification (categorize compilation errors from smaller model to determine type mismatch vs. syntax issues)

## Open Questions the Paper Calls Out

### Open Question 1
Would expanding SwiftEval beyond 28 problems significantly change model rankings or score variance? Basis: "SwiftEval currently contains 28 problems... we assume that results may vary slightly when more problems are added." Unresolved because hand-crafting language-specific problems is labor-intensive. Resolution requires replicating evaluation with 50-100+ hand-crafted Swift problems and comparing rank stability.

### Open Question 2
What specific security vulnerabilities appear in LLM-generated Swift code, and how can they be systematically detected? Basis: "We do not assess the security aspect of the generated code" and plans for future security analysis. Unresolved because current evaluation only measures functional correctness via unit tests. Resolution requires applying static analysis tools to generated samples and categorizing vulnerability types.

### Open Question 3
How do Code LLMs perform across fine-grained Swift subcategories (design patterns, system frameworks, user interfaces)? Basis: "We also plan to create subcategories... to understand model strengths and weaknesses." Unresolved because current benchmark aggregates scores without subcategory labels. Resolution requires tagging each problem with subcategory labels and reporting per-category pass@1 scores.

### Open Question 4
What training data gaps cause poor performance on Swift-specific features like optionals, protocols, and generics? Basis: "Such analysis will help to understand what current training datasets are missing." Unresolved because paper identifies performance gap but doesn't analyze training corpus composition. Resolution requires correlating per-problem scores with training data statistics for Swift-specific syntax patterns.

## Limitations
- Small benchmark size (28 problems vs. 164+ in HumanEval) may limit task diversity
- No security assessment of generated code, focusing only on functional correctness
- Hand-crafted problems require significant effort to create, limiting scalability

## Confidence

| Claim | Label | Evidence |
|-------|-------|----------|
| SwiftEval provides more valid measurements than HumanEval | High | Stronger correlation (0.50 vs 0.30) between model size and performance |
| Compilation errors dominate failure modes | High | Authors report 86% of failures were compilation errors |
| Translated benchmarks contain critical Swift-specific errors | High | MultiPL-E problems involving tuples are "completely unsolvable in Swift" due to fixed-size tuple semantics |

## Next Checks

1. Reproduce baseline evaluation using the 3 top-performing models (GPT-4o, DeepSeek V2, Qwen2.5 32B) to verify correlation coefficient and SOTA ceiling
2. Download benchmark problems from Zenodo and verify compilation success rates across different model sizes
3. Run error classification on compilation failures from a small model to determine dominant failure types (type mismatch vs. syntax issues)