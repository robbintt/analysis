---
ver: rpa2
title: 'An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning:
  A Systematic Benchmark on RAVEN-FAIR'
arxiv_id: '2511.11916'
source_url: https://arxiv.org/abs/2511.11916
tags:
- reasoning
- answer
- architecture
- performance
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku,
  Gemini-1.5-Flash, Llama-3.3-70b) on abstract visual reasoning using four reasoning
  architectures on the RAVEN-FAIR dataset. GPT-4.1-Mini consistently achieved the
  highest accuracy (46.91% in single-shot configuration) across all architectures.
---

# An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR

## Quick Facts
- arXiv ID: 2511.11916
- Source URL: https://arxiv.org/abs/2511.11916
- Authors: Sinan Urgun; Seçkin Arı
- Reference count: 33
- GPT-4.1-Mini achieved highest accuracy (46.91% single-shot) across all architectures on RAVEN-FAIR

## Executive Summary
This systematic benchmark evaluates four LLM models on abstract visual reasoning using four distinct reasoning architectures. GPT-4.1-Mini consistently outperformed other models across all architectures, achieving 46.91% accuracy in single-shot configuration and 53.92% under embedding-controlled repetition. The study reveals critical insights about architectural trade-offs, including a systematic disconnect between explanation quality and execution quality, and identifies coverage variation as a major methodological confounder. Multi-agent architecture provided the largest performance gains for LLaMA-3.3-70b (+8.76% accuracy), while embedding-based repetition improved GPT-4.1-Mini by 7.01%. Key findings document semantic-numeric trade-offs across architectures and highlight the importance of accounting for coverage when comparing architectural performance.

## Method Summary
The study evaluates four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) on 50 RAVEN-FAIR problems using four reasoning architectures: single-shot, embedding-controlled repetition, self-reflection, and multi-agent. Each architecture processes JSON representations of 3×3 matrix completion problems, with outputs rendered to 160×160 PNGs via a Tool Function. Evaluation combines SSIM/LPIPS visual similarity scores with Chain-of-Thought scoring (0-10) and error classification into semantic hallucination vs. numeric misperception. Coverage rates varied significantly across architectures (75.2%-99.8%), identified as a key confounding factor in performance comparisons.

## Key Results
- GPT-4.1-Mini achieved highest accuracy (46.91% single-shot, 53.92% embedding-controlled) across all architectures
- Multi-agent architecture improved LLaMA-3.3-70b accuracy by 8.76% but increased numeric misperception from 66.08% to 95.86%
- Self-reflection architecture caused Claude-3.5-Haiku coverage to drop from 99.7% to 75.2% (24.5% loss)
- Systematic disconnect found between explanation quality and execution quality across all models

## Why This Works (Mechanism)

### Mechanism 1: Embedding-controlled repetition stabilizes output consistency
The architecture generates two answers, compares their embeddings via cosine similarity, and regenerates if similarity < 0.8. This filters stochastic drift at the decision layer, improving accuracy only when the base model has strong implementation capability. GPT-4.1-Mini accuracy increased +7.01 points; LLaMA showed only +0.45% gain due to "consistency-in-error" phenomenon.

### Mechanism 2: Self-reflection architectures can degrade performance by reinforcing confirmation bias
After generating an answer, the model scores its confidence (0-10). If below threshold (8), it regenerates. This assumes meta-cognitive access to reasoning quality. Claude-3.5-Haiku coverage dropped from 99.7% to 75.2% (24.5% loss); GPT-4.1-Mini semantic hallucination increased from 55.43% to 81.20%.

### Mechanism 3: Multi-agent decomposition improves accuracy for some models but introduces semantic-numeric trade-off
Specialized agents analyze distinct features (shape, color, position, angle); a master agent integrates outputs into final JSON. This disentangles reasoning but increases coordination overhead. LLaMA-3.3-70B accuracy rose to 41.33% (+8.76%); numeric misperception climbed from 66.08% to 95.86%.

## Foundational Learning

- **Concept: Raven's Progressive Matrices (RPM)**
  - Why needed here: RAVEN-FAIR is derived from RPM; understanding 3×3 matrix completion with abstract rules is essential
  - Quick check question: Can you explain why RPM requires discovering rules across rows and columns, not just pattern matching?

- **Concept: Semantic Hallucination vs. Numeric Misperception**
  - Why needed here: These are the two primary error types analyzed; distinguishing them is critical for diagnosis
  - Quick check question: If a model correctly identifies a "size decreasing" rule but misreads size 0.6 as 0.3, which error type is this?

- **Concept: Coverage as Confounding Factor**
  - Why needed here: Coverage varied 75.2%-99.8%; comparing accuracy across architectures without normalizing for coverage yields misleading conclusions
  - Quick check question: Why does Claude's 24.5% coverage drop under self-reflection invalidate direct accuracy comparison with other architectures?

## Architecture Onboarding

- **Component map:** JSON input → LLM reasoning (architecture-specific) → Tool Function → PNG output → SSIM/LPIPS + CoT evaluation
- **Critical path:** 1. JSON extraction from .npz/.xml → 2. LLM reasoning (architecture-specific) → 3. Tool Function call → 4. Visual + textual evaluation
- **Design tradeoffs:**
  - Single-shot: Fastest, lowest cost, but no error recovery
  - Embedding-controlled: +7% accuracy for strong models; risk of "consistency-in-error"
  - Self-reflection: Meta-cognitive potential; high risk of coverage loss (up to 24.5%)
  - Multi-agent: Best accuracy gains for some models (+8.76%); introduces semantic-numeric trade-off and coordination overhead
- **Failure signatures:**
  - Low coverage (<90%): Indicates tool-function calling failures or over-critical self-assessment
  - High CoT score + low accuracy: Post-hoc rationalization (CoT-Accuracy Paradox)
  - Embedding similarity >0.8 but wrong answer: Consistency-in-error
- **First 3 experiments:**
  1. Baseline single-shot on GPT-4.1-Mini: Expect ~47% accuracy, high coverage (>99%); validates pipeline integrity
  2. Embedding-controlled on target model with threshold sweep (0.7, 0.8, 0.9): Identify if consistency filtering helps or amplifies errors
  3. Multi-agent on same model: Measure accuracy gain vs. numeric misperception increase to quantify semantic-numeric trade-off

## Open Questions the Paper Calls Out
None

## Limitations
- Coverage variation across architectures (75.2%-99.8%) invalidates direct accuracy comparisons without normalization
- Semantic-numeric trade-off in multi-agent architecture suggests benefits may be task-dependent rather than universally applicable
- Embedding-controlled repetition improvements rely on assumption that inconsistency stems from noise rather than fundamental misunderstanding

## Confidence
**High Confidence**: GPT-4.1-Mini consistently outperforming other models across all architectures (46.91% baseline accuracy, 53.92% under embedding-controlled repetition)
**Medium Confidence**: Multi-agent architecture benefits for LLaMA-3.3-70B (+8.76% accuracy) with noted semantic-numeric trade-off
**Low Confidence**: Embedding-controlled repetition improvements for GPT-4.1-Mini (+7.01%) based on consistency assumption

## Next Checks
1. **Coverage normalization**: Re-run self-reflection architecture on Claude-3.5-Haiku with coverage >95% by adjusting confidence thresholds, then compare accuracy to normalized baseline
2. **Error type decomposition validation**: For each architecture, independently verify classification of semantic hallucination vs. numeric misperception by human annotators on stratified sample of 50 errors
3. **Cross-dataset generalization**: Apply embedding-controlled repetition to different abstract reasoning benchmark (I-RAVEN or PGM) with 100+ problems to test architecture transfer