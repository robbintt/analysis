---
ver: rpa2
title: On the Evaluation of Large Language Models in Multilingual Vulnerability Repair
arxiv_id: '2508.03470'
source_url: https://arxiv.org/abs/2508.03470
tags:
- repair
- vulnerability
- code
- llms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of automated vulnerability
  repair approaches and large language models (LLMs) in multilingual contexts. While
  existing deep learning-based approaches excel at repairing C/C++ vulnerabilities,
  they are limited to single programming languages and often require detailed vulnerability
  information.
---

# On the Evaluation of Large Language Models in Multilingual Vulnerability Repair

## Quick Facts
- arXiv ID: 2508.03470
- Source URL: https://arxiv.org/abs/2508.03470
- Authors: Dong wang; Junji Yu; Honglin Shu; Michael Fu; Chakkrit Tantithamthavorn; Yasutaka Kamei; Junjie Chen
- Reference count: 40
- Primary result: GPT-4o with instruction-tuning and few-shot prompting achieves competitive performance with state-of-the-art approaches in multilingual vulnerability repair

## Executive Summary
This study investigates automated vulnerability repair approaches and large language models (LLMs) across seven programming languages, addressing the limitations of existing deep learning methods that are restricted to single languages and require detailed vulnerability information. The research evaluates state-of-the-art automated repair techniques, pre-trained language models, and five advanced LLMs using a comprehensive multi-language vulnerability dataset. GPT-4o demonstrates competitive performance with the leading approach (VulMaster) while showing superior generalization to unseen vulnerabilities and better handling of dangerous vulnerabilities. Go consistently achieves the highest effectiveness across all model types, while C/C++ performs the worst, highlighting both the promise and challenges of multilingual vulnerability repair.

## Method Summary
The study evaluates automated vulnerability repair techniques by testing them on a comprehensive multi-language vulnerability dataset spanning seven programming languages. The evaluation compares traditional deep learning approaches with five advanced LLMs, including GPT-4o, using various prompting strategies such as instruction-tuning and few-shot prompting. Performance metrics include effectiveness in repairing vulnerabilities, generalization to unseen vulnerabilities, and handling of dangerous vulnerabilities. The experimental setup involves systematic comparison across different programming languages to assess both language-specific and cross-language performance characteristics.

## Key Results
- GPT-4o with instruction-tuning and few-shot prompting achieves competitive performance with VulMaster
- LLMs demonstrate superior generalization to unseen vulnerabilities compared to traditional approaches
- Go consistently achieves the highest effectiveness across all model types, while C/C++ performs the worst

## Why This Works (Mechanism)
The effectiveness of GPT-4o in multilingual vulnerability repair likely stems from its large-scale pre-training across diverse programming languages and code patterns, combined with instruction-tuning that adapts the model to repair-specific tasks. The few-shot prompting strategy enables the model to leverage patterns from similar vulnerability types, facilitating cross-language transfer learning where knowledge from one language can inform repairs in another.

## Foundational Learning
The study builds on established automated program repair techniques while extending them to a multilingual context. The foundational learning includes understanding how different programming languages express vulnerabilities, how repair patterns generalize across languages, and how LLMs can leverage their broad training to identify and fix security issues across diverse codebases.

## Architecture Onboarding
For practical implementation, teams would need to integrate LLM-based vulnerability repair into existing development workflows. This involves setting up API connections to LLMs, creating standardized prompt templates for different vulnerability types, implementing verification mechanisms to validate repairs, and establishing monitoring systems to track repair success rates across different programming languages.

## Open Questions the Paper Calls Out
- How can the performance gap between languages (particularly C/C++ versus Go) be addressed through targeted model improvements or dataset augmentation?
- What specific characteristics of LLMs enable better generalization to unseen vulnerabilities compared to traditional deep learning approaches?
- How can the handling of dangerous vulnerabilities be further improved, especially for languages with complex memory management like C/C++?

## Limitations
- Experimental setup relies on curated dataset that may not represent real-world vulnerability patterns
- Direct comparison between traditional deep learning and LLMs is challenging due to fundamentally different methodologies
- Evaluation focuses on specific vulnerability types and may miss broader security issues developers encounter

## Confidence
**High Confidence:** GPT-4o with instruction-tuning achieves competitive performance with VulMaster; Go's consistent superior performance across model types
**Medium Confidence:** LLMs' superior generalization claims require careful interpretation; language performance differences should be viewed cautiously
**Low Confidence:** Claims about LLMs' better handling of dangerous vulnerabilities need further validation

## Next Checks
1. Cross-dataset validation: Test models on independently collected vulnerability datasets to verify performance robustness
2. Real-world deployment assessment: Conduct longitudinal study of LLM-based vulnerability repair in actual development environments
3. Language-specific vulnerability pattern analysis: Analyze why certain languages outperform others to understand if results reflect language characteristics or dataset biases