---
ver: rpa2
title: 'SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and
  Balanced Transformer Scaling'
arxiv_id: '2510.04286'
source_url: https://arxiv.org/abs/2510.04286
tags:
- slicemoe
- routing
- slices
- expert
- slice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SliceMoE introduces a sub-token routing mechanism for Mixture-of-Experts
  (MoE) transformers by partitioning each token's embedding into contiguous slices
  and routing each slice independently to top-k experts. This fine-grained routing
  addresses expert load imbalance and underutilization in standard token-level MoE
  while enabling interpretable expert specialization over syntactic and semantic subspaces.
---

# SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling

## Quick Facts
- arXiv ID: 2510.04286
- Source URL: https://arxiv.org/abs/2510.04286
- Authors: Harshil Vejendla
- Reference count: 15
- Primary result: Achieves 25.4 perplexity on WikiText-103 vs 29.1 for token-level MoE

## Executive Summary
SliceMoE introduces a novel sub-token routing mechanism for Mixture-of-Experts (MoE) transformers by partitioning each token's embedding into contiguous slices and routing each slice independently to top-k experts. This fine-grained routing addresses expert load imbalance and underutilization in standard token-level MoE while enabling interpretable expert specialization over syntactic and semantic subspaces. The method demonstrates significant improvements in language modeling perplexity, machine translation BLEU scores, and text classification accuracy while maintaining near-optimal load balance across experts.

## Method Summary
SliceMoE implements a sub-token routing mechanism where each token's embedding is divided into multiple contiguous slices, with each slice routed independently to top-k experts. The method includes slice-level capacity loss to prevent expert overload, cross-slice dropout for regularization, and fused batched GEMM kernels for efficient implementation. The routing mechanism allows for more balanced expert utilization compared to token-level MoE, with the number of slices per token being a key hyperparameter that affects both performance and computational overhead.

## Key Results
- WikiText-103: 25.4 perplexity (vs 29.1 for token-MoE, 31.0 for dense)
- WMT En-De: 29.8 BLEU score (vs 28.2 for token-MoE)
- Text classification: 2-4 percentage point accuracy improvement over token-MoE
- Expert load entropy: 0.97 (vs 0.88 for token-MoE)

## Why This Works (Mechanism)
The fine-grained routing in SliceMoE enables more granular allocation of expertise across different semantic and syntactic subspaces within tokens. By routing individual embedding slices rather than entire tokens, the model can better distribute workload across experts, preventing the load imbalance that occurs in token-level MoE where certain tokens consistently route to the same experts. This mechanism allows for more specialized expert utilization and improved overall model capacity utilization.

## Foundational Learning
SliceMoE builds upon the foundational concept of Mixture-of-Experts architectures, extending token-level routing to sub-token embedding slices. The method leverages the principle that different parts of a token's embedding may benefit from different expert processing, similar to how hierarchical mixture models operate in other domains. The approach also incorporates capacity-based regularization techniques that have proven effective in balancing expert utilization in large-scale MoE systems.

## Architecture Onboarding
SliceMoE modifies the standard MoE transformer architecture by introducing a slice-level routing mechanism. The token embedding dimension is partitioned into equal-sized contiguous slices, each routed independently through the router network to select top-k experts. The selected experts process their respective slices, and outputs are concatenated to form the final token representation. This architecture requires careful consideration of slice size and count to balance routing granularity with computational overhead.

## Open Questions the Paper Calls Out
The paper identifies several open questions including the optimal number of slices per token for different task domains, the long-term stability of router decisions during extended training, and the potential for applying slice-level routing to other transformer variants beyond the standard architecture. Additionally, the paper questions whether the interpretability gains from slice-level routing can be quantified and whether the method generalizes effectively to non-text modalities.

## Limitations
- Computational overhead concerns with additional routing complexity
- Router stability questions due to increased routing decisions
- Limited task-specific generalization beyond text domains
- Incomplete interpretability analysis of expert specialization

## Confidence
- High confidence: Core technical contribution and perplexity/BLEU improvements
- Medium confidence: Load balancing improvements and classification results
- Low confidence: Semantic/syntactic interpretability claims and efficiency analysis

## Next Checks
1. Ablation on slice count: Evaluate performance across 4, 8, and 16 slices per token
2. Router stability analysis: Monitor router entropy and convergence patterns during training
3. Cross-modal generalization: Test on vision-language or pure vision tasks