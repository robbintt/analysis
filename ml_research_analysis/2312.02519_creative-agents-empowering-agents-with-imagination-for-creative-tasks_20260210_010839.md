---
ver: rpa2
title: 'Creative Agents: Empowering Agents with Imagination for Creative Tasks'
arxiv_id: '2312.02519'
source_url: https://arxiv.org/abs/2312.02519
tags:
- building
- minecraft
- agents
- tasks
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces creative agents, a novel framework that empowers
  embodied AI agents with imagination to solve open-ended creative tasks. The core
  idea is to enhance a controller with an imaginator that generates detailed imaginations
  of task outcomes from language instructions.
---

# Creative Agents: Empowering Agents with Imagination for Creative Tasks

## Quick Facts
- arXiv ID: 2312.02519
- Source URL: https://arxiv.org/abs/2312.02519
- Authors: Penglin Cai; Chi Zhang; Yuhui Fu; Haoqi Yuan; Zongqing Lu
- Reference count: 40
- Primary result: Introduces creative agents that enhance embodied AI with imagination to solve open-ended creative tasks, achieving superior performance in Minecraft building generation

## Executive Summary
This paper introduces creative agents, a novel framework that empowers embodied AI agents with imagination to solve open-ended creative tasks. The core idea is to enhance a controller with an imaginator that generates detailed imaginations of task outcomes from language instructions. Two imaginator variants are proposed: one using a large language model (LLM) for textual imagination via Chain-of-Thought prompting, and another using a diffusion model for visual imagination. The controller can be either a behavior-cloning policy or a vision-language model (VLM) like GPT-4V that generates executable code. Experiments on building creation in Minecraft show that creative agents outperform baseline methods across multiple evaluation metrics including correctness, complexity, quality, and functionality. Notably, the Diffusion+GPT-4V variant achieves the best overall performance, demonstrating strong creativity and robustness in generating diverse, visually appealing buildings from abstract instructions.

## Method Summary
The creative agents framework decomposes the agent into two modules: an imaginator and a controller. The imaginator takes a free-form language instruction and generates a detailed imagination (text via LLM with Chain-of-Thought or image via finetuned diffusion model). This imagination serves as a concrete goal representation for the controller, which can be either a VLM generating executable code or a behavior-cloning policy mapping voxel data to actions. The method is evaluated on 20 diverse creative building tasks in Minecraft, using GPT-4V for automated evaluation based on correctness, complexity, quality, and functionality metrics.

## Key Results
- Creative agents significantly outperform baseline methods across all evaluation metrics
- Diffusion+GPT-4V achieves the best overall performance with superior creativity and robustness
- Automated VLM-based evaluation shows high correlation with human judgments
- Text-only imagination (CoT+GPT-4) performs well but is outperformed by visual imagination
- The framework successfully generates diverse, visually appealing buildings from abstract instructions

## Why This Works (Mechanism)

### Mechanism 1: Imagination-Guided Decomposition for Open-Ended Tasks
- Claim: Decomposing an agent into an imaginator and a controller enables handling abstract, creative tasks by grounding ambiguous language instructions into concrete goal representations
- Mechanism: The imaginator generates detailed imagination from language instructions, which the controller uses as a concrete "blueprint" to reduce uncertainty
- Core assumption: Open-ended creative tasks are too ambiguous for direct instruction-to-action mapping
- Evidence: Experimental results show clear performance improvements over vanilla approaches without imagination

### Mechanism 2: Multimodal Grounding via Diffusion and VLM
- Claim: Using a finetuned diffusion model for visual imagination combined with a VLM controller provides stronger grounding than text-only methods
- Mechanism: Diffusion imaginator produces visual targets that VLMs like GPT-4V perceive alongside text instructions to generate more precise executable code
- Core assumption: Visual imagination provides richer spatial information than text alone
- Evidence: Diffusion+GPT-4V achieves the best performance overall in experiments

### Mechanism 3: Automated Evaluation via VLM-as-Judge
- Claim: A VLM can be used as a reliable automated evaluator for open-ended creative tasks, correlating well with human judgments
- Mechanism: GPT-4V rates creations based on predefined aspects using specific prompts, with scores showing statistically significant positive correlation with human ratings
- Core assumption: Modern VLMs possess sufficient aesthetic and analytical capabilities to critique visual creations consistently with human judgment
- Evidence: Human evaluation correlation studies validate the automated evaluation pipeline

## Foundational Learning

- **Markov Decision Process (MDP) without Reward**
  - Why needed here: The paper models creative tasks not as reward-maximization problems but as instruction-following within a state-action transition framework
  - Quick check question: How does the lack of a fixed reward function change the agent's learning goal compared to standard reinforcement learning?

- **Diffusion Models**
  - Why needed here: A core component is a finetuned Stable Diffusion model used as the "visual imaginator"
  - Quick check question: What is the role of the text-image dataset in finetuning the diffusion model for this application?

- **Behavior Cloning (BC)**
  - Why needed here: One controller variant uses a BC policy to map voxel blueprints to executable actions
  - Quick check question: What kind of data is required to train this behavior cloning controller, and what are its limitations compared to the VLM-based controller?

## Architecture Onboarding

- **Component map:**
  - Imaginator (I): Language Instruction → Imagination (text or image)
  - Controller (π): State, Imagination, Instruction → Action (code or behavior)
  - Evaluator: Screenshot → Ratings (correctness, complexity, quality, functionality)
  - Environment: Minecraft (MineDojo or Mineflayer APIs)

- **Critical path:**
  1. Task Definition: Define free-form language instructions for creative building
  2. Imagination Generation: Pass instruction to imaginator (LLM or diffusion model)
  3. Action Planning & Execution: Controller generates code or actions based on imagination
  4. Evaluation: Capture final state and use VLM evaluator to score against criteria

- **Design tradeoffs:**
  - Text vs. Visual Imagination: Text is easier but less spatially rich; visual is more expressive but requires finetuning data
  - VLM vs. BC Controller: VLM is more flexible but may generate simple code; BC is specialized but requires large domain-specific datasets
  - Code Generation vs. Direct Action: Code allows high-level planning but is API-limited; direct action allows fine control but is harder to train

- **Failure signatures:**
  - Simple/Solid Structures: VLM controller tends to generate simple square buildings or solid blocks
  - Noise Artifacts: Diffusion imaginator may generate images with background noise
  - Pathfinding Issues: Agent may destroy its own blocks due to poor pathfinding

- **First 3 experiments:**
  1. Ablation on Imagination Modality: Compare vanilla GPT-4, CoT+GPT-4, and Diffusion+GPT-4V on creative tasks
  2. Controller Comparison: Compare Diffusion+GPT-4V against Diffusion+BC using same visual imaginations
  3. Evaluation Metric Validation: Perform human evaluation and calculate correlation with VLM scores

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the performance of the behavior-cloning (BC) controller be improved, particularly regarding the Pix2Vox module's ability to accurately map RGB colors to specific Minecraft blocks and maintain structural plausibility?
- **Basis in paper:** The authors state, "First, there is much room for improving the BC controller, especially for the performance of Pix2Vox module."
- **Why unresolved:** The current Pix2Vox implementation struggles to capture accurate block types from RGB values and often misses plausible structures
- **What evidence would resolve it:** A comparative study showing a new BC architecture or Pix2Vox modification yielding higher accuracy in voxel reconstruction

### Open Question 2
- **Question:** How can the creativity and complexity of generated buildings be enhanced beyond simple geometric shapes given the current limitations of LLM-based code generation?
- **Basis in paper:** The authors ask, "How to enhance the creativity of agents can be a challenging problem," and note the "limitation lies in the simplicity of the building."
- **Why unresolved:** Current LLM controllers tend to use simple for-loops within the constraints of the Mineflayer API, resulting in square or simple structures
- **What evidence would resolve it:** Demonstration of agents consistently generating non-rectilinear, complex structures and achieving higher "Complexity" scores

### Open Question 3
- **Question:** How can visual imagination be augmented to convey internal structural details to the controller, preventing the generation of solid, non-functional buildings?
- **Basis in paper:** The paper notes that GPT-4V often generates "solid house instead of a hollow one" because diffusion images lack "sectional view to show the internal structures."
- **Why unresolved:** The visual imagination provides only an external "facade" view, lacking depth information for inferring hollowness or room layouts
- **What evidence would resolve it:** Introduction of a multi-view or sectional imagination module resulting in generated buildings with verified hollow interiors

## Limitations

- The framework's performance is primarily demonstrated in Minecraft, limiting evidence for generalization to other creative domains
- The VLM-based evaluation, while showing good correlation with human judgment, may still contain evaluator biases that could affect metric reliability
- The method's robustness in dynamic survival mode environments with real-time pathfinding challenges is not fully explored

## Confidence

- **High Confidence:** The mechanism of imagination-guided decomposition for reducing task ambiguity is well-supported by experimental results
- **Medium Confidence:** The multimodal grounding advantage of Diffusion+GPT-4V is demonstrated, but edge cases where visual imagination might mislead the controller are not fully explored
- **Medium Confidence:** The VLM-as-judge evaluation framework shows good correlation with human judgment, but prompts and criteria could potentially be optimized

## Next Checks

1. **Generalization Test:** Evaluate the creative agents framework on non-Minecraft creative tasks (e.g., story generation or art creation) to verify imagination decomposition mechanism's domain generality

2. **Ablation on Imagination Quality:** Systematically vary the quality and relevance of generated imaginations to quantify the relationship between imagination fidelity and controller performance

3. **Long-term Stability Analysis:** Test the agents over extended Minecraft survival sessions to evaluate pathfinding robustness and building durability under dynamic environmental conditions