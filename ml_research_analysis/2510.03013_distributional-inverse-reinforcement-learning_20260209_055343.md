---
ver: rpa2
title: Distributional Inverse Reinforcement Learning
arxiv_id: '2510.03013'
source_url: https://arxiv.org/abs/2510.03013
tags:
- reward
- learning
- distribution
- policy
- return
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a distributional inverse reinforcement learning
  (DistIRL) framework that jointly models uncertainty over reward functions and full
  return distributions, moving beyond traditional approaches that only recover deterministic
  rewards or match expected returns. By minimizing first-order stochastic dominance
  (FSD) violations, the method learns complete reward distributions and integrates
  distortion risk measures into policy learning, enabling risk-aware imitation.
---

# Distributional Inverse Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.03013
- Source URL: https://arxiv.org/abs/2510.03013
- Reference count: 38
- Primary result: First IRL method to learn full reward distributions using FSD minimization, achieving up to 0.92 correlation with true returns

## Executive Summary
This paper introduces a distributional inverse reinforcement learning (DistIRL) framework that learns complete reward distributions rather than point estimates. By minimizing first-order stochastic dominance (FSD) violations between expert and agent return distributions, DistIRL jointly models uncertainty over reward functions and full return distributions. The method integrates distortion risk measures into policy learning, enabling risk-aware imitation from expert demonstrations. Experiments demonstrate state-of-the-art imitation performance and interpretable reward recovery across synthetic benchmarks, neurobehavioral data, and MuJoCo tasks.

## Method Summary
DistIRL employs a three-network architecture: a reward distribution network outputting skew-normal parameters (μ,σ²,α), a policy network, and a quantile regression critic with 200 quantiles. The training alternates between updating the critic via quantile Huber loss, the policy via distortion risk measure maximization, and the reward distribution via FSD loss with KL regularization to a prior. The FSD objective ensures the agent's return distribution first-order stochastically dominates expert returns, while the DRM component enables risk-aware policy optimization. Hyperparameters include learning rate 3e-4, batch size 512, and 5000 training iterations.

## Key Results
- Achieves up to 0.92 correlation between inferred and true returns in control tasks
- First IRL method to learn full reward distributions rather than deterministic point estimates
- Demonstrates state-of-the-art imitation performance across synthetic, neurobehavioral, and MuJoCo benchmarks
- Captures higher-order moments of reward distributions, improving robustness and interpretability

## Why This Works (Mechanism)
DistIRL's core innovation is replacing traditional IRL's expected return matching with full distribution matching via FSD. This allows learning expressive reward representations that capture uncertainty and risk preferences. By minimizing FSD violations, the method ensures the agent's return distribution dominates the expert's, while DRM-based policy learning enables risk-aware behavior. The skew-normal parameterization provides flexibility for asymmetric reward distributions, and the two-timescale optimization balances reward recovery with policy improvement.

## Foundational Learning
- **Skew-normal distributions**: Needed for modeling asymmetric reward uncertainties; quick check: verify μ,σ,α parameterization correctly generates diverse shapes
- **First-order stochastic dominance**: Ensures agent's returns stochastically dominate expert's; quick check: plot CDF comparisons between expert and agent returns
- **Distortion risk measures**: Enables risk-aware policy learning; quick check: test CVaR with different α values affects policy behavior
- **Quantile regression for distributional RL**: Stabilizes return distribution learning; quick check: monitor quantile Huber loss convergence during training
- **Two-timescale stochastic approximation**: Coordinates reward and policy updates; quick check: verify learning rates maintain appropriate timescale separation

## Architecture Onboarding
- **Component map**: State-Action -> Reward Dist Net -> Return Dist -> FSD Loss; State -> Policy Net -> Action; State -> Quantile Critic Net -> Return Dist
- **Critical path**: Reward distribution → return distribution → FSD loss → reward update; simultaneously: policy → return distribution → DRM loss → policy update
- **Design tradeoffs**: Skew-normal parameterization offers flexibility but increases complexity vs. Gaussian; 200 quantiles provide resolution but increase computational cost
- **Failure signatures**: Quantile instability (diverging critic loss), KL collapse (reward distribution collapsing to prior), poor FSD gradients (insufficient return variance)
- **First experiments**: 1) Gridworld reward recovery with κ=1.0; 2) FSD loss gradient analysis on synthetic distributions; 3) Quantile critic stability across κ values

## Open Questions the Paper Calls Out
1. Under what conditions does the two-timescale stochastic approximation scheme used in DistIRL converge to a stationary point of the joint reward-policy optimization objective?
2. Can the relationship between the FSD objective and the DRM surrogate objective be characterized more precisely, such that a minimal set of distortion functions suffices to guarantee FSD optimality?
3. How should the prior distribution over rewards be selected or adapted for different domains, and what is the impact of prior misspecification on reward distribution recovery?
4. Can DistIRL be extended to online or interactive IRL settings where the agent can query the environment or expert during learning?

## Limitations
- Theoretical convergence analysis remains incomplete due to non-convex coupling between components
- Requires careful hyperparameter tuning, particularly quantile Huber threshold κ and prior selection
- Assumes stationary expert demonstrations, limiting applicability to non-stationary or interactive settings
- Computational overhead from quantile regression and full distribution modeling

## Confidence
- FSD-based reward learning framework: High
- Risk-aware policy optimization via DRM: Medium
- Practical performance claims (0.92 correlation): Medium (depends on exact implementation details)
- Convergence guarantees: Low (explicitly noted as beyond scope)

## Next Checks
1. Reproduce gridworld reward distributions with varying κ values to assess sensitivity
2. Verify quantile critic stability across κ=0.5, 1.0, and 2.0 thresholds
3. Implement and test FSD loss gradient quality on synthetic return distributions with known dominance relationships