---
ver: rpa2
title: Bayesian Coreset Optimization for Personalized Federated Learning
arxiv_id: '2511.01800'
source_url: https://arxiv.org/abs/2511.01800
tags:
- learning
- coreset
- client
- data
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CORESET-PFEDBAYES, a personalized federated
  learning framework that optimizes coreset selection at each client to reduce communication
  and computation costs while maintaining performance. The method integrates Bayesian
  coresets into the federated learning objective by assigning weights to data points
  and minimizing KL divergence between coreset and full-data distributions, along
  with likelihood matching.
---

# Bayesian Coreset Optimization for Personalized Federated Learning

## Quick Facts
- **arXiv ID:** 2511.01800
- **Source URL:** https://arxiv.org/abs/2511.01800
- **Reference count:** 40
- **Primary result:** Achieves minimax optimal convergence rates with reduced communication by optimizing coreset selection for personalized federated learning

## Executive Summary
This paper introduces CORESET-PFEDBAYES, a personalized federated learning framework that optimizes data subset selection (coresets) at each client to reduce communication and computation costs while maintaining or improving model performance. The method assigns weights to data points through a sparse regression formulation that minimizes KL divergence between coreset and full-data distributions, along with likelihood matching. Theoretical analysis proves the approach achieves minimax optimal convergence rates up to logarithmic bounds. Experiments on benchmark datasets demonstrate significant accuracy gains over random sampling and baselines, while medical dataset results validate effectiveness in privacy-preserving settings.

## Method Summary
The framework integrates Bayesian coresets into federated learning by optimizing coreset weights at each client through a sparse regression problem. Clients maintain local data distributions and perform Bayesian updates using the global distribution as a prior. The coreset optimizer module, implemented via Accelerated Iterative Hard Thresholding (A-IHT), selects representative data subsets by minimizing the divergence between variational distributions learned on the coreset and full dataset. The system balances KL divergence regularization with likelihood matching in its objective function, enabling efficient communication while preserving model accuracy across heterogeneous client data.

## Key Results
- Achieves 5-10% accuracy improvements over random sampling on MNIST, FashionMNIST, and CIFAR-10
- Reduces communication rounds by 30-40% while maintaining or improving performance
- Demonstrates superior performance to PFEDBAYES and submodular selection baselines
- Validated on medical datasets showing effectiveness in privacy-preserving settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing coreset weights to minimize divergence between variational distributions acts as a regularizer preventing model drift from optimal full-data solutions.
- **Mechanism:** The modified client objective explicitly minimizes KL divergence between local distribution $q_i(\theta)$ and global distribution $z(\theta)$ weighted by coreset parameters, forcing the subset to yield a posterior statistically close to the full dataset's posterior.
- **Core assumption:** The optimal variational distribution learned on full data serves as a stable reference target for the coreset-based distribution to approximate.
- **Evidence anchors:** Abstract mentions KL divergence minimization; Section 4.3 argues optimal local distribution should be close to weighted coreset objective.
- **Break condition:** If full-data distribution is not representative due to extreme noise, aligning coreset distribution may propagate errors.

### Mechanism 2
- **Claim:** Formulating coreset selection as sparse regression over log-likelihood vectors enables efficient full-data posterior approximation with bounded error.
- **Mechanism:** Maps subset selection to Hilbert space by minimizing $L_2$ distance between full log-likelihood vector and weighted coreset log-likelihood vector, making "representativeness" measurable as geometric distance.
- **Core assumption:** Monte Carlo approximations of log-likelihood gradients provide sufficient embedding of data characteristics into Hilbert space.
- **Evidence anchors:** Section 3.2 details reduction to sparse regression formulation $G_i(w_i)$; abstract mentions minimax optimal convergence rates.
- **Break condition:** If intrinsic dimension $\Lambda$ is significantly higher than coreset sample size $n_k$, approximation bounds may become too loose.

### Mechanism 3
- **Claim:** Accelerated Iterative Hard Thresholding resolves non-convexity of sparse subset selection, preventing local minima typical of standard gradient descent.
- **Mechanism:** Uses adaptive momentum and projection steps to navigate non-convex landscape with $||w_i||_0 \leq n_k$ constraint, ensuring convergence to sparse solutions that gradient matching alone might miss.
- **Core assumption:** Loss landscape allows momentum-based acceleration to effectively jump out of poor local optima.
- **Evidence anchors:** Appendix 10.7 describes A-IHT with adaptive momentum acceleration and Nesterov Accelerated Gradient.
- **Break condition:** If momentum parameter is poorly tuned or sparsity constraint is too aggressive, hard thresholding may discard informative data points causing oscillation.

## Foundational Learning

- **Concept: Variational Inference (VI) & the Reparameterization Trick**
  - **Why needed here:** The entire architecture relies on optimizing distributions $q(\theta)$ (Gaussian parameters $\mu, \sigma$) rather than point estimates; client update step (Eq. 9) and KL divergence loss require understanding VI posterior approximation.
  - **Quick check question:** Can you explain why we optimize $\mu$ and $\rho$ (where $\sigma = \log(1+e^\rho)$) instead of $\mu$ and $\sigma$ directly in Eq. 9?

- **Concept: The Bi-Level Optimization in Federated Learning**
  - **Why needed here:** The system has inner loop (client local updates/CoresetOpt) and outer loop (server aggregation), with a third nested loop for coreset optimization within the client.
  - **Quick check question:** In Algorithm 1, does the `CoresetOptUpdate` happen before or after the local model parameters $v$ are updated by the server?

- **Concept: Sparse Regression & Compressed Sensing**
  - **Why needed here:** Understanding that selecting $k$ points from $n$ is an $L_0$-norm problem clarifies why authors use A-IHT (from compressed sensing) rather than standard SGD.
  - **Quick check question:** Why does the constraint $||w_i||_0 \leq n_k$ make Eq. 4 non-convex?

## Architecture Onboarding

- **Component map:** Global Server -> Client Node (Standard) -> Coreset Optimizer (Novel)
- **Critical path:** The `CoresetOptUpdate` block (Algorithm 1) is critical; if this module fails to converge or produces degenerate weights (all zeros), the local client model trains on effectively no data, causing global divergence. The "Monte Carlo sample size $K$" and "minibatch size $b$" strictly control the fidelity of this path.
- **Design tradeoffs:**
  - Coreset Size ($n_k$) vs. Convergence Speed: Reducing $n_k$ reduces computation cost but tightens minimax bound, potentially requiring more communication rounds.
  - Likelihood Matching vs. KL Alignment: Objective balances matching data likelihood (empirical fit) with posterior distribution (regularization); over-weighting likelihood may cause overfitting, over-weighting KL may result in generic subsets.
- **Failure signatures:**
  - Oscillating Weights: Large A-IHT step sizes may cause weights to flip-flop between subsets, causing loss to plateau.
  - Singular Posterior: Too small or unrepresentative coreset may cause variance $\sigma^2$ to collapse to zero, making model overconfident on wrong predictions.
  - Ablation Collapse: Removing KL term from coreset objective may cause training to saturate early, indicating subset is not representative of global structure.
- **First 3 experiments:**
  1. **Toy Validation (Housing Prices):** Implement Riemann linear regression experiment (Section 7.1) to verify A-IHT implementation correctly recovers true posterior shape before deploying to complex neural nets.
  2. **Ablation on Objective Terms:** Run sweep comparing Eq. 7 (KL only) vs. Eq. 8 (KL + Likelihood) vs. Likelihood-only on MNIST to reproduce "early saturation" result in Appendix 10.1.
  3. **Communication Efficiency Test:** Reproduce Table 3 by varying sampling fraction (15%, 30%, 50%) to observe trade-off between communication rounds and accuracy, ensuring "minimax optimal" trend holds empirically.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical convergence bounds depend on intrinsic dimension $\Lambda$ remaining small relative to coreset size $n_k$, which may not hold for high-dimensional or highly heterogeneous client data
- Computational efficiency gains rely on Monte Carlo approximations whose accuracy depends on problem-specific variance characteristics
- Performance in extremely non-IID settings with data drift between rounds is not thoroughly explored

## Confidence

- **High Confidence:** Empirical validation showing CORESET-PFEDBAYES outperforms random sampling and achieves superior accuracy with reduced communication (Tables 2, 3); core algorithmic contributions (A-IHT for sparse regression, bi-level optimization framework) are well-documented and reproducible
- **Medium Confidence:** Theoretical convergence bounds (Theorem 2) and their implications for communication efficiency; while derivation appears sound, practical tightness across diverse FL scenarios requires further empirical verification
- **Low Confidence:** Claim that this represents a "model-centric alternative to data-agnostic subset selection"; paper demonstrates improved performance but doesn't conclusively establish whether gains stem from Bayesian coreset formulation specifically versus general improvements in subset selection methodology

## Next Checks

1. **Intrinsic Dimension Sensitivity:** Systematically vary estimated intrinsic dimension $\Lambda$ across different client datasets (from low-dimensional tabular to high-dimensional image data) to empirically validate theoretical convergence bounds and identify break points where efficiency gains diminish.

2. **Non-IID Stress Test:** Implement controlled experiment with extreme data heterogeneity (clients having disjoint label sets or highly imbalanced class distributions) and measure both accuracy degradation and KL divergence between coreset-based and full-data posteriors to quantify method's robustness to severe non-IID conditions.

3. **Computational Overhead Analysis:** Profile actual runtime overhead of A-IHT coreset optimization step versus standard FedAvg training, measuring both wall-clock time and energy consumption across different client compute capabilities to verify claimed efficiency benefits in practical deployment scenarios.