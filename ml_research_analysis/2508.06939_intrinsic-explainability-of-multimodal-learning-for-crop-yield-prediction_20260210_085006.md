---
ver: rpa2
title: Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction
arxiv_id: '2508.06939'
source_url: https://arxiv.org/abs/2508.06939
tags:
- yield
- data
- satellite
- weather
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of explaining multimodal learning
  models for crop yield prediction, which are often complex and lack interpretability
  despite their high performance. The authors leverage the intrinsic explainability
  of Transformer-based models to analyze intermediate representations, temporal attributions,
  and modality importance in yield prediction tasks.
---

# Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction

## Quick Facts
- arXiv ID: 2508.06939
- Source URL: https://arxiv.org/abs/2508.06939
- Authors: Hiba Najjar; Deepak Pathak; Marlon Nuske; Andreas Dengel
- Reference count: 40
- Key outcome: Transformer-based models outperformed CNN/LSTM baselines, achieving R² scores 0.10 and 0.04 higher at subfield and field levels respectively, while demonstrating intrinsic explainability through attention rollout and modality attribution methods.

## Executive Summary
This study addresses the challenge of explaining complex multimodal learning models for crop yield prediction. The authors leverage the intrinsic explainability of Transformer-based models to analyze intermediate representations, temporal attributions, and modality importance. They compare model-specific methods (Attention Rollout and Generic Attention) with model-agnostic methods (Shapley Value Sampling) and propose a new intrinsic modality attribution method (Weighted Modality Activation). The research demonstrates how intrinsic explainability can provide agronomically relevant insights into model predictions while maintaining high performance.

## Method Summary
The approach uses intermediate multimodal fusion where static modalities (soil, DEM) pass through MLPs while temporal modalities (satellite, weather) use Transformer encoders with positional encoding based on calendar days. Each modality is processed by a dedicated encoder, producing d-dimensional representations that are concatenated and passed to a linear regression head. The model is trained using AdamW optimizer with linear warmup and cosine decay, evaluated on R², RMSE, and MAE metrics. Explainability is achieved through Attention Rollout for temporal attributions and Weighted Modality Activation for modality importance, compared against Shapley Value Sampling.

## Key Results
- Transformer-based model outperformed convolutional and recurrent networks, achieving R² scores 0.10 and 0.04 higher at subfield and field levels respectively
- Attention Rollout provided more robust temporal attributions than other methods, with lower sensitivity and comparable infidelity scores
- Modality importance analysis revealed conflicting results: SVS attributed 89.5% importance to satellite data, while WMA attributed only 29.4%, suggesting further investigation is needed

## Why This Works (Mechanism)

### Mechanism 1: Intermediate Multimodal Fusion with Modality-Specific Encoders
Processing heterogeneous modalities through specialized encoders before concatenation-based fusion improves predictive performance over single-architecture or early-fusion approaches. Static modalities pass through MLPs while temporal modalities use Transformer encoders with positional encoding based on calendar days. Each encoder produces a d-dimensional representation that is concatenated and passed to a linear regression head. This assumes modality-specific processing captures domain-relevant patterns before fusion.

### Mechanism 2: Attention Rollout for Intrinsic Temporal Attribution
Iteratively multiplying attention weight matrices across Transformer layers provides more robust temporal attributions than gradient-based or model-agnostic methods. Raw attention matrices from each layer are treated as proportion factors and multiplied together to trace information flow from input to final embedding. This assumes attention weights meaningfully represent information importance and combining layers preserves rather than distorts this signal.

### Mechanism 3: Weighted Modality Activation for Architecture-Native Importance Estimation
The final regression layer's weights can directly decompose predictions into modality-specific contributions, providing an intrinsic alternative to perturbation-based methods. Since fusion uses concatenation followed by a linear layer, the prediction can be rewritten as the weighted sum of modality activations. This assumes the regression layer weights reflect learned modality importance and concatenation-based fusion makes this decomposition valid.

## Foundational Learning

- **Self-Attention Mechanism and Positional Encoding**: The paper relies on Transformer encoders whose attention weights are extracted for explainability. Understanding how attention distributes across time steps and how positional encoding injects temporal order is essential to interpret attribution results. Quick check: Given a 4-head attention layer with sequence length T, what is the shape of the attention matrix for one head, and how would you aggregate across heads for interpretation?

- **Feature Attribution Evaluation Metrics (Sensitivity vs. Infidelity)**: The paper compares AR, GA, and SVS using sensitivity and infidelity scores. Understanding what each metric measures is required to evaluate why AR is deemed more "robust." Quick check: If perturbing input X slightly causes large changes in attributions but the model output remains stable, would sensitivity be high or low? What does this indicate about explanation reliability?

- **Linear Probing of Intermediate Representations**: The paper uses linear probes to assess whether learned representations become more linearly separable across layers, informing which modalities contribute most predictive information. Quick check: If a linear probe on layer 3 achieves RMSE = 1.5 t/ha while layer 1 achieves RMSE = 2.8 t/ha, what does this suggest about feature evolution through the network?

## Architecture Onboarding

- **Component map**: Input Pipeline -> Modality Encoders -> Fusion Layer -> Regression Head -> Attribution Extraction
- **Critical path**: Input alignment → Temporal encoding (Transformers only) → Attention computation → Regression token aggregation → Concatenation → Linear prediction. For explainability: Extract attention matrices → Apply AR/GA → Compare with SVS/WMA.
- **Design tradeoffs**: Single-head vs. multi-head: Paper chose single-head for interpretability (avoids averaging across heads), but this may reduce model capacity. Intermediate vs. early fusion: Chose intermediate to handle heterogeneous modalities, but may miss low-level cross-modal patterns. Intrinsic (AR/WMA) vs. post-hoc (SVS): Intrinsic is faster and architecture-tied, but may not capture non-linear feature interactions that SVS captures through perturbation.
- **Failure signatures**: High sensitivity in attributions (GA shows this—pixels in same field have divergent attributions despite similar inputs). Low entropy in attention (if attention concentrates entirely on one time step, the model may be overfitting to temporal artifacts rather than agronomic signals). Modality attribution disagreement (SVS and WMA giving contradictory results indicates the attribution method choice significantly impacts conclusions).
- **First 3 experiments**: Validate AR robustness on held-out crops/regions. Ablate positional encoding. Cross-validate WMA vs. SVS with synthetic data.

## Open Questions the Paper Calls Out

### Open Question 1
Which modality attribution method (intrinsic WMA or model-agnostic SVS) provides the most reliable estimates of feature importance? The study found a massive discrepancy where SVS attributed 89.5% importance to satellite data, while WMA attributed only 29.4%, and no ground truth exists to verify which is correct.

### Open Question 2
Can the alignment observed between temporal attributions and crop phenology be generalized across diverse fields and crops? The agronomic validation was limited to a subset of soybean fields with approximated growth stage data, leaving the consistency of these patterns across the wider dataset uncertain.

### Open Question 3
Can model performance be optimized by enforcing rules or constraints derived from intrinsic explainability findings? The current study focused on interpreting existing models rather than using those interpretations to regularize or guide the training process.

## Limitations
- Dataset access limitations: The combine harvester yield maps are proprietary, requiring proxy data for reproduction and introducing uncertainty about whether findings generalize beyond the specific farms and regions studied.
- Method novelty validation: The Weighted Modality Activation method is presented as novel but lacks direct comparison to established linear attribution methods, making its advantages uncertain without synthetic validation.
- Conflicting attribution results: SVS and WMA provide contradictory modality importance estimates (89.5% vs 29.4% for satellite), indicating the method choice significantly impacts conclusions.

## Confidence

- **High confidence**: Transformer architecture outperforming CNN/LSTM baselines (supported by clear performance metrics across multiple crops and countries).
- **Medium confidence**: Attention Rollout providing more robust temporal attributions (supported by sensitivity/infidelity metrics, but requires validation on held-out crops).
- **Low confidence**: Modality importance conclusions due to SVS and WMA providing contradictory results, indicating the method choice significantly impacts conclusions.

## Next Checks

1. **Generalization testing**: Apply the selected Transformer architecture with AR attribution to soybean and wheat datasets from different countries, comparing sensitivity/infidelity scores against the corn baseline to test attribution robustness.
2. **Positional encoding ablation**: Train the satellite encoder without calendar-based positional encoding and compare predictive performance and temporal attention distribution to isolate the contribution of temporal ordering.
3. **Synthetic data validation**: Create controlled datasets where ground-truth modality importance is known (e.g., yield depends only on satellite), then compare WMA and SVS recovery of known importance to determine which method is more faithful to true mechanism.