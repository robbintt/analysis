---
ver: rpa2
title: 'OVD: On-policy Verbal Distillation'
arxiv_id: '2601.21968'
source_url: https://arxiv.org/abs/2601.21968
tags:
- training
- verbal
- distillation
- reasoning
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces On-policy Verbal Distillation (OVD), a memory-efficient\
  \ knowledge distillation framework that replaces token-level probability matching\
  \ with trajectory matching using discrete verbal scores (0\u20139) from teacher\
  \ models. The approach addresses severe memory bottlenecks in reinforcement learning\
  \ by dramatically reducing memory consumption while enabling on-policy distillation\
  \ from teacher models with verbal feedback, and avoiding token-level alignment to\
  \ allow the student model to freely explore the output space."
---

# OVD: On-policy Verbal Distillation

## Quick Facts
- arXiv ID: 2601.21968
- Source URL: https://arxiv.org/abs/2601.21968
- Reference count: 40
- Primary result: Memory-efficient distillation using verbal scores achieves up to +12.9% EM on Web Q&A and +25.7% on math tasks

## Executive Summary
This paper introduces On-policy Verbal Distillation (OVD), a memory-efficient knowledge distillation framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0-9) from teacher models. The approach addresses severe memory bottlenecks in reinforcement learning by dramatically reducing memory consumption while enabling on-policy distillation from teacher models with verbal feedback. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods while exhibiting superior training efficiency.

## Method Summary
OVD introduces a novel distillation approach that eliminates the memory-intensive token-level probability matching used in traditional methods. Instead, it employs trajectory matching where teacher models provide discrete verbal scores (0-9) evaluating the quality of complete reasoning trajectories. This verbal feedback is used in a rejection sampling framework that provides unbiased gradient estimates while avoiding the need to store intermediate token probabilities. The framework enables on-policy distillation, allowing real-time feedback from teachers without requiring offline dataset generation, and permits students to freely explore the output space without strict token-level alignment constraints.

## Key Results
- Up to +12.9% absolute improvement in average EM on Web Q&A tasks compared to baseline methods
- Up to +25.7% gain on math benchmarks when trained with only one random sample
- Demonstrates superior training efficiency with dramatically reduced memory consumption compared to traditional token-level distillation approaches

## Why This Works (Mechanism)
OVD works by fundamentally changing the distillation objective from token-level probability matching to trajectory-level evaluation. By using discrete verbal scores from teacher models, the framework avoids the memory bottleneck of storing token probabilities for every intermediate step. The rejection sampling mechanism ensures unbiased gradient estimates while the on-policy nature allows immediate feedback from teachers. This approach is particularly effective for reasoning tasks where the quality of the complete trajectory matters more than individual token predictions, and where token-level alignment can constrain the student's ability to develop its own reasoning strategies.

## Foundational Learning

**Verbal Feedback Systems**
- Why needed: Provides interpretable, high-level evaluation of complex trajectories
- Quick check: Test if discrete scores correlate with task-specific quality metrics

**Reinforcement Learning Distillation**
- Why needed: Enables learning from non-differentiable teacher feedback
- Quick check: Verify gradient estimates remain unbiased under various sampling rates

**Trajectory Matching**
- Why needed: Focuses on end-to-end quality rather than intermediate steps
- Quick check: Compare performance against token-level baselines on reasoning tasks

**Rejection Sampling**
- Why needed: Provides unbiased gradient estimates from discrete feedback
- Quick check: Measure variance in gradient estimates across different acceptance rates

**On-policy Learning**
- Why needed: Enables real-time feedback without offline dataset generation
- Quick check: Test memory usage reduction compared to offline approaches

## Architecture Onboarding

**Component Map:**
Student Model -> Trajectory Generator -> Teacher Evaluator -> Verbal Score -> Rejection Sampler -> Gradient Estimator -> Updated Student

**Critical Path:**
The core execution flow involves generating trajectories from the student model, evaluating them with the teacher model to obtain verbal scores, applying rejection sampling based on these scores, and using the accepted samples to compute unbiased gradient estimates for updating the student.

**Design Tradeoffs:**
- Discrete verbal scores vs. continuous probability distributions (memory efficiency vs. granularity)
- On-policy vs. off-policy training (real-time feedback vs. computational overhead)
- Trajectory-level vs. token-level evaluation (memory efficiency vs. detailed supervision)

**Failure Signatures:**
- Poor performance when teacher verbal scores are miscalibrated or inconsistent
- High variance in training when acceptance rates in rejection sampling are too low
- Convergence issues if the mixture training distribution is poorly balanced

**First Experiments:**
1. Ablation study comparing OVD with token-level distillation on memory usage and performance
2. Sensitivity analysis of verbal score granularity (0-9 vs. alternative ranges)
3. Teacher quality evaluation to assess impact of different teacher model architectures

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Discrete verbal scores may not capture nuanced quality differences in complex reasoning tasks
- Framework relies on teacher model quality and consistent scoring behavior
- Generalizability across diverse reasoning domains remains to be fully validated

## Confidence

**High Confidence:**
- Memory efficiency claims are well-supported through direct comparison metrics
- Convergence guarantees under mixture training distributions are theoretically sound

**Medium Confidence:**
- Performance improvements are substantial but could benefit from additional ablation studies
- Evaluation methodology is solid but cross-domain validation is limited

**Low Confidence:**
- Generalizability of discrete verbal scores across diverse reasoning tasks is uncertain
- Sensitivity to teacher model bias and scoring consistency needs further investigation

## Next Checks
1. Conduct cross-domain experiments testing OVD's performance on tasks requiring different types of reasoning (e.g., legal reasoning, medical diagnosis)
2. Perform sensitivity analysis on the discrete score range (0-9) by testing alternative granularities and continuous score alternatives
3. Implement teacher model ablation studies comparing OVD performance when using different teacher architectures and training them with varying quality criteria