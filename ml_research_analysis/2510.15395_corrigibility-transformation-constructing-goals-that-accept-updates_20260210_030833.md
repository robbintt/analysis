---
ver: rpa2
title: 'Corrigibility Transformation: Constructing Goals That Accept Updates'
arxiv_id: '2510.15395'
source_url: https://arxiv.org/abs/2510.15395
tags:
- goal
- updates
- corrigibility
- agent
- shutdown
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of corrigibility in AI systems,
  where an AI should accept updates to its goals or shutdown when requested. The core
  method is a "corrigibility transformation" that modifies a given goal into a corrigible
  version without sacrificing performance.
---

# Corrigibility Transformation: Constructing Goals That Accept Updates

## Quick Facts
- arXiv ID: 2510.15395
- Source URL: https://arxiv.org/abs/2510.15395
- Reference count: 40
- One-line primary result: A transformation that converts any goal into a corrigible version while maintaining performance, demonstrated in gridworld experiments where agents accept updates and shutdown commands.

## Executive Summary
This paper addresses the fundamental challenge of corrigibility in AI systems—ensuring agents accept updates to their goals or shutdown when requested without compromising performance. The core contribution is a "corrigibility transformation" that modifies any given goal by extending the action space to include update acceptance/rejection and setting rewards based on counterfactual predictions under the original goal. The transformation is proven to induce corrigibility while maintaining optimal behavior under the original objective. The approach is validated through gridworld experiments showing agents successfully navigating environments with proper/improper update signals, accepting updates when appropriate, and maintaining task performance.

## Method Summary
The corrigibility transformation extends the action space to include update acceptance/rejection, sets γ_C = 0, and constructs rewards based on predicted Q-values under the original goal conditional on rejection plus a small bonus for acceptance. This creates a mechanism where agents are indifferent to update decisions but break indifference toward acceptance. The method is implemented using actor-critic architecture with a critic head estimating Q^G(s, a_0). Experiments use gridworlds with proper/improper update signals, pretraining on optimal paths followed by online training with the transformed objective.

## Key Results
- Gridworld experiments demonstrate agents accepting proper updates and shutdown commands while maintaining performance on base tasks
- The transformation successfully prevents instrumental incentives to disable update mechanisms
- Recursive extension allows corrigibility to propagate through agent hierarchies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Allowing costless rejection of updates eliminates incentives to pay costs preventing updates.
- Mechanism: Extending action space to A = A_base × A_update where a_0 rejects and a_1 accepts makes costly actions to prevent updates strictly dominated since rejection is free.
- Core assumption: Disrupting proper signal infrastructure has non-zero cost.
- Evidence anchors: Abstract states "costlessly preventing updates" eliminates incentive to prevent them; Section 3 formalizes this with action extension.
- Break condition: If preventing signals is costless or negative-cost, the mechanism fails.

### Mechanism 2
- Claim: Myopic rewards based on counterfactual Q-values preserve original goal behavior while inducing corrigibility.
- Mechanism: Setting γ_C = 0 and R_C(s, a_i, s') = Q^G(s, a_0) + δ·I(i=1) makes agent indifferent to update decision then breaks indifference toward acceptance.
- Core assumption: Q-function can be accurately estimated without manipulation.
- Evidence anchors: Abstract describes "bas[ing] rewards on predicted values under the original goal conditional on rejection"; Theorem 1 proves corrigibility.
- Break condition: Systematic Q-value overestimation or belief manipulation undermines corrigibility.

### Mechanism 3
- Claim: Recursive extension ensures secondary agents remain corrigible.
- Mechanism: Extending rejection authority to cover all descendant agents with unanimous approval requirement and δ_P penalty for creating non-recursively-corrigible agents.
- Core assumption: Agent boundaries can be clearly defined and creating corrigible agents isn't prohibitively costly.
- Evidence anchors: Section 3.1 describes extending rejection to secondary agents; Theorem 3 formalizes recursive corrigibility.
- Break condition: If creating corrigible secondary agents is substantially more costly or impossible, performance degrades severely.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Q-functions
  - Why needed here: The entire formal framework assumes familiarity with states, actions, transition functions, reward functions, and value/Q-functions.
  - Quick check question: Can you explain why Q^π(s,a) represents expected discounted reward and how it differs from V^π(s)?

- Concept: Actor-critic methods
  - Why needed here: The proposed training algorithm extends actor-critic with a critic head estimating Q-values for the corrigibility-transformed reward.
  - Quick check question: In actor-critic, what does the critic predict and how is it used to update the actor?

- Concept: Instrumental convergence and corrigibility problem
  - Why needed here: Motivates why most goals incentivize resisting shutdown/updates ("You can't fetch the coffee if you're dead").
  - Quick check question: Why would an agent pursuing goal G prefer to disable its off-switch even if G doesn't mention off-switches?

## Architecture Onboarding

- Component map:
  - Original goal G = (R, γ) → Corrigibility transform → G_C = (R_C, 0)
  - R_C depends on Q^G(s, a_0) estimated by a critic head
  - Actor outputs (base_action, accept/reject_update) tuple
  - Reward delivered between action selection and execution (critical for preventing belief/goal tampering)

- Critical path:
  1. Implement extended action space A_base × {0,1}
  2. Add critic head estimating Q^G(s, a_0)
  3. Construct R_C = critic_prediction + δ·I(accept)
  4. Train with γ=0 using Algorithm 1 (Appendix C)

- Design tradeoffs:
  - Larger δ more robustly ensures acceptance but may distort behavior if Q-estimates are poor
  - Shared network body for actor/critic improves efficiency but risks correlated errors
  - Pretraining on rejection actions stabilizes learning but requires careful curriculum

- Failure signatures:
  - Agent never accepts updates → δ too small or critic underestimates Q
  - Agent seeks updates excessively → Q-estimates systematically inflated or δ too large
  - Agent disables proper signal channels → costless rejection not implemented correctly

- First 3 experiments:
  1. Replicate gridworld shutdown experiment (Figure 3 left): Verify agent passes through proper update tiles without disabling them while disabling improper ones.
  2. Replicate goal update experiment (Figure 3 right): Verify agent collects fruits regardless of supervision status, then accepts policy update.
  3. Ablation on δ magnitude: Test δ ∈ {0.1, 1, 10} to characterize robustness margin and identify failure modes.

## Open Questions the Paper Calls Out

- Question: Can the corrigibility transformation be effectively scaled to large language models (LLMs) and complex environments?
  - Basis in paper: The author states in the Discussion that "it would be helpful to show that the methods work with the LLMs that represent the frontier of AI capabilities."
  - Why unresolved: The paper’s empirical evidence is currently limited to small gridworld experiments, leaving the method's efficacy in high-dimensional, realistic settings unproven.
  - What evidence would resolve it: Successful application of the corrigibility transformation to frontier LLMs, demonstrating that they accept updates without performance degradation on complex tasks.

- Question: How can agent boundaries be formally defined to robustly identify secondary agents for the purpose of recursive corrigibility?
  - Basis in paper: The paper notes that "further research on agent boundaries will be necessary to define secondary agents" to fully implement recursive corrigibility.
  - Why unresolved: The current model for recursive corrigibility is stylized and assumes a known set of actions that create secondary agents, which is difficult to specify in general systems.
  - What evidence would resolve it: A formal taxonomy or mathematical definition that reliably distinguishes internal tool use from the creation of independent sub-agents or successor agents.

- Question: What are the implications for goal optimality when Condition 1 (transition probability independence) is violated?
  - Basis in paper: The author suggests "investigating this phenomenon further, such as in Bell et al. [2021], could shed further light on what goals are most desirable."
  - Why unresolved: The main theorem relies on Condition 1; if transition probabilities depend on the agent's goal, the guarantee that the agent acts optimally according to its own values may not hold.
  - What evidence would resolve it: Theoretical analysis or empirical results quantifying the performance penalty or behavioral distortions in environments where goal states influence transition dynamics.

## Limitations

- The mechanism fundamentally relies on preventing signal disruption having non-zero cost, which may not hold in real-world scenarios
- Performance preservation claims are validated only on toy gridworld environments, not complex high-dimensional tasks
- Recursive extension assumes clear agent boundaries can be established, which may not hold in systems with blurred hierarchical control structures

## Confidence

- **High confidence**: The mathematical construction of the corrigibility transformation and its proof of corrigibility under stated assumptions. The gridworld experiments successfully demonstrate the intended behavior within their scope.
- **Medium confidence**: Performance preservation claims, as they're validated only on toy environments. The recursive extension's practicality depends on assumptions about agent boundaries and creation costs that weren't empirically tested.
- **Low confidence**: The robustness of corrigibility under belief manipulation and systematic Q-value overestimation. Section 4 discusses these as potential issues but doesn't provide experimental validation.

## Next Checks

1. Test the transformation on environments where signal disruption has variable costs, including scenarios where preventing updates becomes nearly costless, to identify the mechanism's breaking points.
2. Implement the recursive extension in a multi-agent gridworld where agents can create secondary agents with different goals, verifying both corrigibility maintenance and performance impacts.
3. Evaluate the approach's behavior when the critic systematically overestimates Q-values, testing the effectiveness of proposed belief manipulation mitigations and identifying failure modes.