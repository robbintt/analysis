---
ver: rpa2
title: 'Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task
  Experts'
arxiv_id: '2506.10357'
source_url: https://arxiv.org/abs/2506.10357
tags:
- craft
- task
- minecraft
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Optimus-3, a generalist multimodal agent for
  Minecraft that integrates perception, planning, action, grounding, and reflection
  capabilities. The key challenges addressed are insufficient domain-specific data,
  task interference in heterogeneous learning, and visual diversity in open-world
  environments.
---

# Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts

## Quick Facts
- **arXiv ID:** 2506.10357
- **Source URL:** https://arxiv.org/abs/2506.10357
- **Reference count:** 40
- **Primary result:** Achieves SOTA performance across 5 Minecraft agent tasks (Planning 20%↑, Captioning 66%↑, EQA 76%↑, Grounding 3.4x↑, Reflection 18%↑)

## Executive Summary
Optimus-3 addresses the challenge of building generalist multimodal agents for Minecraft by integrating perception, planning, action, grounding, and reflection capabilities. The key innovations include a knowledge-enhanced automated data generation pipeline that leverages Minecraft Wiki and expert models, a task-level routing Mixture-of-Experts architecture to prevent task interference, and a Multimodal Reasoning-Augmented Reinforcement Learning method that improves visual reasoning through IoU-Density Reward. The system achieves state-of-the-art performance across all evaluated tasks, surpassing previous approaches by significant margins while maintaining task-specific expertise without interference.

## Method Summary
Optimus-3 employs a three-phase training pipeline starting from Qwen2.5-VL-7B, which is converted to a task-level routing MoE architecture with one shared expert and five task-specific experts. Phase 1 involves supervised fine-tuning of the shared expert using 230k samples. Phase 2 freezes the vision backbone and tunes task experts with Chain-of-Thought reasoning using 58k samples. Phase 3 applies GRPO reinforcement learning with IoU-Density Reward using 5k samples. The knowledge-enhanced data generation pipeline combines Minecraft Wiki, knowledge graphs, STEVE-1, and expert models with environment feedback. The system uses VPT action head integration and evaluates across 67 long-horizon tasks with multiple benchmarks including Success Rate, Accuracy, LLM-as-Judge scores, and IoU metrics.

## Key Results
- **Planning:** 20% improvement in success rate over previous SOTA
- **Captioning:** 66% accuracy improvement on 134 samples
- **Embodied QA:** 76% accuracy improvement on 400 samples
- **Grounding:** 3.4× improvement in IoU@0.5 metric on 500 samples
- **Reflection:** 18% accuracy improvement on 64 samples

## Why This Works (Mechanism)
The task-level routing MoE architecture prevents interference between heterogeneous tasks by assigning each task to specialized experts while maintaining a shared foundation. The knowledge-enhanced data generation pipeline addresses the domain-specific data scarcity problem by automatically creating diverse, high-quality training samples using Minecraft Wiki, knowledge graphs, and expert models with environment feedback. The IoU-Density Reward in reinforcement learning directly optimizes visual reasoning performance by combining spatial accuracy with prediction confidence, leading to improved grounding and perception capabilities.

## Foundational Learning
- **Task-level routing MoE:** Specialized experts for different tasks prevent interference during multi-task learning; quick check: monitor accuracy drops when training new tasks
- **Knowledge-enhanced data generation:** Automated pipeline using Minecraft Wiki and expert models creates domain-specific training data; quick check: validate hallucination rates in generated samples
- **IoU-Density Reward:** Combines Intersection-over-Union with confidence density for better visual reasoning optimization; quick check: test reward stability across different object sizes
- **Chain-of-Thought reasoning:** Structured reasoning improves planning and reflection task performance; quick check: compare CoT vs direct prediction accuracy
- **VPT action head integration:** Enables precise action execution in Minecraft environment; quick check: measure action execution success rate
- **GRPO reinforcement learning:** Policy optimization using reward shaping for task-specific performance; quick check: monitor reward convergence during training

## Architecture Onboarding
**Component Map:** Data Generation -> MoE Architecture -> 3-Phase Training -> VPT Action Head -> Evaluation
**Critical Path:** Knowledge pipeline → Task routing → SFT → CoT tuning → GRPO → Action execution
**Design Tradeoffs:** Task-level vs token-level routing (interference vs flexibility), knowledge augmentation vs training efficiency, IoU-Density vs simpler rewards
**Failure Signatures:** Task interference in dense models, hallucinations in vision tasks, catastrophic forgetting in token-level routing
**First Experiments:** 1) Test MoE conversion with 2 experts (planning + action), 2) Reconstruct knowledge pipeline with 1k samples, 3) Validate IoU-Density Reward on simple detection task

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Missing implementation details for MoE architecture conversion from base model
- Incomplete specifications for knowledge-enhanced data generation pipeline orchestration
- Unspecified hyperparameters for IoU-Density Reward and GRPO training
- Unclear integration method for VPT action head with MoE LLM output
- Missing exact prompt templates for reflection and partial-resource planning

## Confidence
- **High confidence** in task-level routing MoE preventing interference (supported by Figure 7 and established expert routing literature)
- **Medium confidence** in knowledge-enhanced data generation effectiveness (conceptually sound but implementation details missing)
- **Low confidence** in exact performance improvements without implementation details (dependent on unspecified hyperparameters and data generation specifics)

## Next Checks
1. Implement minimal MoE conversion prototype using Qwen2.5-VL-7B with 2 task experts to verify routing effectiveness
2. Reconstruct knowledge-enhanced pipeline using available STEVE-1 and DeepSeek-VL2 checkpoints to generate 1k validation samples
3. Validate IoU-Density Reward formulation with placeholder hyperparameters on simple Minecraft object detection task