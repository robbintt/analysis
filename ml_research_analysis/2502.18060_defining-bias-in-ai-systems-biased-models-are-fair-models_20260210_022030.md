---
ver: rpa2
title: 'Defining bias in AI-systems: Biased models are fair models'
arxiv_id: '2502.18060'
source_url: https://arxiv.org/abs/2502.18060
tags:
- bias
- fairness
- what
- mitigation
- term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper clarifies the ambiguous use of the term "bias" in AI
  discussions by distinguishing between technical bias (an architectural concept in
  neural networks) and social bias (unequal treatment based on outcome-irrelevant
  characteristics). It argues that "unbiased" does not equate to "fair," as true fairness
  requires accommodating individual differences rather than enforcing uniform treatment.
---

# Defining bias in AI-systems: Biased models are fair models

## Quick Facts
- arXiv ID: 2502.18060
- Source URL: https://arxiv.org/abs/2502.18060
- Reference count: 7
- One-line primary result: This paper argues that technical bias (architectural) is necessary for model function while fairness requires distinguishing between relevant and irrelevant discrimination, challenging the assumption that "unbiased" equals "fair"

## Executive Summary
This paper addresses the pervasive confusion around "bias" terminology in AI fairness discourse by distinguishing between technical bias (the constant term in neural network neurons) and social bias (unequal treatment based on outcome-irrelevant characteristics). The authors argue that "unbiased" does not equate to "fair," as true fairness requires accommodating individual differences rather than enforcing uniform treatment. They propose that addressing "Capital D Discrimination" (unfair discrimination) is more effective than focusing solely on eliminating technical bias, emphasizing the need to differentiate between harmful discrimination and equitable differentiation.

## Method Summary
This is a conceptual position paper rather than an empirical study. The authors provide a historical review tracing "bias" terminology from early neural networks (MCP neurons, perceptrons, ADALINE) to establish the technical definition, then build a framework distinguishing technical bias from social bias. The paper uses illustrative examples (facial recognition, children's curriculum model, cucumber vs. apple classification) as thought experiments rather than presenting data or conducting experiments. The primary contribution is a conceptual clarification of terminology and a proposed framework for thinking about fairness that prioritizes relevance-based discrimination over technical bias elimination.

## Key Results
- Technical bias (architectural constant in neurons) is distinct from social bias and necessary for model function
- Fairness requires accommodating differences rather than enforcing uniform treatment across groups
- "Capital D Discrimination" (unfair discrimination on outcome-irrelevant dimensions) is the target for mitigation, not technical bias elimination
- Relevance criteria—whether classification uses outcome-relevant versus outcome-irrelevant features—determines whether differentiation is equitable or discriminatory

## Why This Works (Mechanism)

### Mechanism 1: Technical-Conceptual Separation
- Claim: Distinguishing architectural bias from social bias enables more effective fairness interventions by preventing category errors in mitigation strategies.
- Mechanism: By recognizing that technical bias is value-neutral and necessary for model function, practitioners can redirect mitigation efforts toward social harm rather than attempting to eliminate a fundamental architectural component.
- Core assumption: Precise conceptual definitions lead to better-targeted interventions in practice.
- Evidence anchors: [abstract] "we highlight the importance of distinguishing between bias and discrimination"; [section 2] "This architectural definition is distinct from the statistical concept in the bias-variance tradeoff"; [corpus] Weak direct validation—related papers focus on bias mitigation methods but don't test this conceptual framework empirically
- Break condition: When technical and social bias cannot be cleanly separated (e.g., architectural choices that encode social assumptions into feature representations).

### Mechanism 2: Relevance-Based Differentiation
- Claim: Fairness is determined by whether classification uses outcome-relevant versus outcome-irrelevant criteria, not by the presence or absence of differentiation.
- Mechanism: "Discriminating between" (value-neutral classification) operates on relevant dimensions; "Capital D Discrimination" operates on irrelevant dimensions. Relevance determines ethical valence.
- Core assumption: Task-specific relevance criteria can be objectively established and agreed upon.
- Evidence anchors: [section 5] "Relevance is what separates useful classification from arbitrary or harmful discrimination"; [section 5] "the difference between 'discrimination between groups' and 'Capital D Discrimination' is whether the distinction in question is made along relevant or irrelevant dimensions"; [corpus] Limited validation—neighbor papers address fairness metrics but don't directly test the relevance-criterion framework
- Break condition: When criteria relevance is contested, context-dependent, or when "relevant" criteria correlate with protected characteristics (proxy discrimination).

### Mechanism 3: Equity-Over-Equality Principle
- Claim: Uniform treatment across groups does not produce fairness; true fairness requires differentiated accommodation of differing circumstances.
- Mechanism: Equal treatment ignores structural disadvantages—an unbiased model applying identical standards to heterogeneous populations perpetuates inequity. Fairness requires recognizing outcome-relevant differences rather than enforcing uniformity.
- Core assumption: Differentiated treatment can be implemented without introducing new forms of unfairness or gaming.
- Evidence anchors: [section 4] The hypothetical child development model example demonstrates how equal treatment fails children with vastly different circumstances; [section 5] "simply treating everyone the same... does not ensure the best outcome for disadvantaged groups. On the contrary, only accommodation of differences... improves access"; [corpus] Indirect support—fairness papers acknowledge equity/equality distinction but don't validate this specific mechanism
- Break condition: When accommodation logic becomes pretext for exclusion, or when differentiation mechanisms introduce more bias than they remove.

## Foundational Learning

- Concept: **Neural network bias term (architectural)**
  - Why needed here: The paper traces "bias" to ADALINE (1960), where it denotes a constant input that shifts decision boundaries independently of input values. Understanding this prevents conflating a necessary architectural component with social harm.
  - Quick check question: In a perceptron with weighted inputs Σwᵢxᵢ, what functional role does adding a bias term b play in the activation function?

- Concept: **Linear separability and hyperplane classification**
  - Why needed here: The paper uses linear separation (apples vs. cucumbers) to illustrate how classification requires relevant dimensions. Irrelevant dimensions produce meaningless or harmful separations.
  - Quick check question: If you're classifying job candidates by "qualifications," what makes a feature outcome-relevant versus outcome-irrelevant?

- Concept: **Equality vs. equity distinction**
  - Why needed here: The central argument hinges on why identical treatment produces inequitable outcomes. Fairness (equity) requires differential accommodation; equality does not guarantee fairness.
  - Quick check question: A facial recognition system achieves 99% accuracy overall but only 70% accuracy for darker-skinned individuals. If you equalize treatment (same algorithm, same thresholds), have you achieved fairness?

## Architecture Onboarding

- Component map:
  Input features → [Protected attributes?] → [Outcome-relevant features?]
  ↓
  Bias terms (architectural) → Shift activation thresholds → Enable learning
  ↓
  Decision boundary → Separates classifications → Where relevance criteria operate
  ↓
  Output → Differentiated outcomes → [Relevant differentiation?] OR [Irrelevant discrimination?]

- Critical path:
  1. Define task-specific relevance criteria—What features legitimately differentiate outcomes for this task?
  2. Audit data representation across outcome-relevant dimensions (not just demographic parity)
  3. Evaluate output patterns—Is differentiation based on relevant or irrelevant criteria?
  4. Implement accommodations where uniform treatment would produce inequity

- Design tradeoffs:
  - Relevance precision vs. edge-case handling: Tight relevance definitions reduce discrimination risk but may miss legitimate contextual factors
  - Uniform vs. differentiated treatment: Uniform policies are computationally simpler but risk inequity; differentiated approaches require more sophisticated logic and monitoring
  - Mitigation scope: Eliminating technical "bias" is straightforward; addressing Capital D Discrimination requires domain expertise and stakeholder input

- Failure signatures:
  - Claiming a model is "fair" because it's "unbiased" (uniform treatment)
  - Using outcome-irrelevant features (gender, race) for decisions where they don't affect qualifications
  - Assuming demographic parity equals fairness without examining relevance
  - Treating equal error rates across groups as sufficient when base conditions differ substantially

- First 3 experiments:
  1. Relevance audit: List all features your model uses; for each, document whether it's outcome-relevant or outcome-irrelevant for the specific task. Flag any features where relevance is ambiguous.
  2. Uniform treatment stress test: Identify a subgroup with distinct circumstances (e.g., users with disabilities, non-native speakers). Compare outcomes under current uniform treatment vs. a differentiated accommodation. Measure outcome gaps.
  3. Bias-elimination counterfactual: Implement a standard "bias mitigation" approach (e.g., removing sensitive attributes). Test whether fairness outcomes actually improve or whether you've merely achieved uniform treatment without addressing underlying inequity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the distinction between "discriminating between" (neutral differentiation) and "Capital D Discrimination" be operationalized into measurable fairness constraints for model training?
- Basis in paper: [inferred] The paper provides a taxonomic clarification but lacks a technical implementation for how algorithms should distinguish "good" from "bad" bias during optimization.
- Why unresolved: Translating the nuanced, context-dependent definition of "relevance" into a binary or continuous constraint for a loss function is a non-trivial engineering challenge not addressed by the conceptual framework.
- What evidence would resolve it: A formal mathematical definition or algorithmic framework that successfully penalizes "irrelevant" differentiation (e.g., race in hiring) while preserving "relevant" differentiation (e.g., needs in education).

### Open Question 2
- Question: What methodologies can be developed to automatically identify whether a data dimension is "outcome-relevant" (equity) or "outcome-irrelevant" (discrimination) in complex, high-dimensional datasets?
- Basis in paper: [explicit] The authors explicitly state that "Relevance is what separates useful classification from arbitrary or harmful discrimination," but note that failing to apply meaningful dimensions leads to subjective or harmful outcomes.
- Why unresolved: Determining relevance is currently a subjective, human-driven process; an open research gap exists in creating computational methods to detect relevance dynamically across different contexts.
- What evidence would resolve it: An automated auditing tool that can flag "Capital D Discrimination" by detecting correlations with outcome-irrelevant characteristics without requiring manual labeling of every feature.

### Open Question 3
- Question: Does prioritizing the mitigation of "Capital D Discrimination" over the elimination of technical bias result in higher subjective fairness ratings from affected stakeholders?
- Basis in paper: [inferred] The paper concludes that "true fairness requires distinguishing between harmful discrimination and equitable differentiation," challenging the standard approach of simply removing bias to ensure uniform treatment.
- Why unresolved: While theoretically sound, there is no empirical evidence presented to prove that users prefer "biased" models that accommodate differences over "unbiased" models that treat everyone identically.
- What evidence would resolve it: User studies or A/B tests demonstrating that populations rate systems designed around the authors' "fair differentiation" framework as fairer than systems optimized for traditional statistical "unbiasedness."

## Limitations
- No empirical validation: The paper provides theoretical arguments but no experiments testing whether the framework improves fairness outcomes in practice
- Subjective relevance determination: The paper lacks operational definitions for determining "outcome-relevant" vs. "outcome-irrelevant" criteria, making implementation challenging
- Limited stakeholder perspective: No evidence that affected populations prefer "biased" equitable models over "unbiased" uniform models

## Confidence

- Technical history of neural network bias terms: High confidence—well-established historical development from MCP neurons through ADALINE
- Conceptual distinction between technical and social bias: Medium confidence—theoretically sound but lacks empirical validation
- Relevance-based discrimination framework: Medium confidence—relies on subjective determination of "outcome-relevant" criteria without operational definitions
- Equity-over-equality principle: Medium confidence—supported by theoretical reasoning but limited real-world testing of differentiated treatment implementations

## Next Checks

1. **Relevance audit experiment**: Implement the paper's framework by auditing an existing classification system for feature relevance, documenting which inputs are outcome-relevant versus outcome-irrelevant, then measure impact on fairness metrics.

2. **Differentiated treatment test**: Design and evaluate a system that provides equitable accommodations (rather than uniform treatment) for a disadvantaged subgroup, measuring whether outcomes improve relative to equal treatment baselines.

3. **Conceptual framework validation**: Conduct expert interviews with fairness practitioners to assess whether distinguishing technical from social bias improves their ability to design and implement effective fairness interventions.