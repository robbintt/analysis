---
ver: rpa2
title: Privacy-Preserving Conformal Prediction Under Local Differential Privacy
arxiv_id: '2505.15721'
source_url: https://arxiv.org/abs/2505.15721
tags:
- privacy
- conformal
- prediction
- aggregator
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of applying conformal prediction
  when labels in the calibration set are privacy-sensitive and must be protected through
  local differential privacy (LDP). The authors propose two complementary approaches:
  LDP-CP-L, which perturbs labels locally and shifts computation to the aggregator,
  and LDP-CP-S, which allows users to compute and perturb conformity scores locally
  while preserving both feature and label privacy.'
---

# Privacy-Preserving Conformal Prediction Under Local Differential Privacy

## Quick Facts
- **arXiv ID**: 2505.15721
- **Source URL**: https://arxiv.org/abs/2505.15721
- **Reference count**: 6
- **Primary result**: Two LDP conformal prediction methods (LDP-CP-L and LDP-CP-S) that preserve coverage guarantees while protecting label and/or feature privacy on medical imaging datasets.

## Executive Summary
This paper addresses the challenge of applying conformal prediction when calibration labels are privacy-sensitive and must be protected through local differential privacy (LDP). The authors propose two complementary approaches: LDP-CP-L, which perturbs labels locally and shifts computation to the aggregator, and LDP-CP-S, which allows users to compute and perturb conformity scores locally while preserving both feature and label privacy. Both methods compute conformal thresholds directly from noisy data without accessing true labels, providing finite-sample coverage guarantees. The experimental results on medical imaging datasets demonstrate that LDP-CP-L achieves performance comparable to non-private CP for moderate privacy levels (ε≥4), while LDP-CP-S shows robustness to the number of classes and performs well with larger calibration sets. The methods successfully maintain coverage guarantees while protecting user privacy, with the shuffle model further improving practicality by reducing effective privacy loss.

## Method Summary
The paper proposes two LDP conformal prediction methods. LDP-CP-L uses k-ary randomized response to perturb labels locally, then applies a noise-aware calibration procedure at the aggregator to estimate the clean-label score distribution and find the threshold. LDP-CP-S enables users to compute conformity scores locally and estimate the (1-α)-quantile using LDP binary search with disjoint user subsets per iteration. Both methods provide finite-sample coverage guarantees by accounting for the LDP noise structure. The shuffle model can be added to amplify privacy, reducing effective privacy loss from ε to approximately ε/√n. The methods are evaluated on MedMNIST datasets (TissueMNIST, OrganSMNIST, OrganAMNIST, OrganCMNIST, OCTMNIST) with coverage guarantee 1-α (α=0.1) as the primary metric.

## Key Results
- LDP-CP-L achieves coverage ≥ 1-α-Δ for moderate privacy levels (ε≥4) and shows robustness to class count k
- LDP-CP-S maintains coverage guarantees with larger calibration sets (n≥10^5) and is robust to the number of classes
- The shuffle model reduces effective privacy loss from ε to ε/√n, making high privacy levels (ε≥3) more practical
- Both methods successfully protect label privacy while maintaining prediction set coverage close to non-private CP

## Why This Works (Mechanism)

### Mechanism 1
Perturbing labels via k-ary randomized response and adjusting the calibration threshold accounts for the known noise structure, preserving coverage guarantees on true labels. Users apply k-RR to labels, creating a uniform noise channel with β = k/(k-1+e^ε). The aggregator estimates the clean-label score distribution using ǨF^c(q) = (ǨF^n(q) - β·ǨF^r(q))/(1-β), then binary-searches for the threshold satisfying this corrected coverage. Core assumption: The k-ary randomized response creates a uniform noise model that can be analytically inverted; the aggregator knows the noise mechanism parameters.

### Mechanism 2
Users can privately estimate the (1-α)-quantile of conformity scores using LDP binary search, without exposing features or labels. Each user computes S(x_i, y_i) locally. Over T binary search iterations, disjoint user subsets report (via ε-LDP randomized response) whether their score is below threshold q^(j). Aggregator debiases: Z^(j) = ((e^ε+1)/(e^ε-1))·(1/n')ΣRR(b_i) - 1/(e^ε-1). Core assumption: Users have model access and compute resources; each user participates in at most one iteration to preserve privacy budget.

### Mechanism 3
Introducing an anonymizing shuffler between users and aggregator amplifies privacy, reducing effective privacy loss from ε to approximately ε/√n. Users send perturbed data to a secure shuffler that permutes messages uniformly at random before forwarding to aggregator, breaking user-message associations. Core assumption: An honest-but-curious shuffler that correctly permutes without leaking associations.

## Foundational Learning

- **Conformal Prediction (CP)**: Underlying framework being privatized. You must understand how CP uses a calibration set to find threshold q so prediction sets C_q(x) = {y | S(x,y) ≤ q} have guaranteed coverage 1-α. Quick check: Given calibration scores [0.1, 0.3, 0.5, 0.7, 0.9], what threshold yields 80% coverage?

- **Local Differential Privacy (LDP)**: Privacy model where users perturb data locally before sharing with an untrusted aggregator—no trusted curator. Quick check: For k-RR with ε=2 and k=10 classes, what's the probability a user reports their true label?

- **Quantile Estimation under Privacy**: Core technical challenge is estimating the (1-α)-quantile from noisy responses. Requires understanding how randomized response affects aggregate statistics and debiasing. Quick check: If 60% report "score below threshold" via ε-LDP binary RR with ε=1, what's your estimate of the true proportion?

## Architecture Onboarding

- Component map: User-side (LDP-CP-L) k-RR label perturbation; (LDP-CP-S) local score computation + binary RR -> Aggregator-side (LDP-CP-L) noise-aware threshold via binary search on ǨF^c(q); (LDP-CP-S) LDP quantile binary search with debiasing -> Optional shuffle layer for privacy amplification

- Critical path: 1) Assess privacy constraints (label-only vs full data) → choose LDP-CP-L or LDP-CP-S 2) Estimate n, k → compute Δ correction term (Theorems 3–4) 3) Deploy user-side perturbation; run aggregator calibration 4) Apply threshold q at inference to construct prediction sets

- Design tradeoffs: LDP-CP-L exposes features, needs less user compute; LDP-CP-S hides both, needs n ≥ 10^5 for small Δ_S; Lower ε → stronger privacy but larger Δ → weaker coverage; Large k hurts LDP-CP-L; large n required for LDP-CP-S

- Failure signatures: Coverage < 1-α: Δ correction insufficient → verify noise model, increase n; Oversized prediction sets: Low ε or large k → consider shuffle model or LDP-CP-S; LDP-CP-S unstable: Small n → switch to LDP-CP-L or collect more data

- First 3 experiments: 1) Standard CP on clean data to establish non-private coverage/set-size baselines; 2) LDP-CP-L on TissueMNIST (k=8) with ε ∈ {2, 4, 8}; verify coverage ≥ 1-α-Δ; 3) Compare LDP-CP-S vs LDP-CP-L at ε=4 with n ≥ 5000; measure coverage, set size, per-user compute overhead

## Open Questions the Paper Calls Out

- **Advanced LDP mechanisms**: Can more advanced LDP mechanisms (e.g., RAPPOR, Bassily et al. 2017) be integrated into LDP-CP-L while maintaining valid coverage guarantees? A key challenge lies in determining whether the aggregator can effectively process data from users who submit a potentially large subset of labels.

- **Joint feature-label LDP**: How can conformal prediction be performed under LDP when neither user-side score computation nor feature exposure is permissible? Computing scores on the user's side is prohibited while simultaneously ensuring the privacy of both the signal/features x and the label y.

- **Extreme multi-class settings**: How does LDP-CP-L performance degrade in extreme multi-class settings (e.g., k > 1000), and can class hierarchies or embeddings mitigate this? The theoretical bound grows with k, but structured label spaces may admit more efficient private estimation.

## Limitations

- Limited external validation—no direct corpus papers address LDP in conformal prediction context, relying heavily on internal theoretical proofs and experimental results
- Technical assumptions untested: aggregation of disjoint user subsets per iteration in LDP-CP-S, correctness of the shuffle model amplification, and practical feasibility of requiring n ≥ 10^5 for small Δ_S
- Empirical evaluation restricted to medical imaging datasets; generalization to other domains (tabular, text) remains unverified

## Confidence

- **High**: Theoretical coverage guarantees under LDP (Theorems 3-4); effectiveness of k-RR for label perturbation and analytical inversion; practical utility of shuffle model for privacy amplification
- **Medium**: LDP-CP-S robustness to class count and performance with large calibration sets; generalizability of empirical results beyond tested MedMNIST datasets
- **Low**: Practical feasibility of disjoint user subsets for LDP-CP-S; effectiveness of shuffle model implementation details; broader domain applicability

## Next Checks

1. **Proof Validation**: Re-derive Theorems 3-4, verifying that Δ_L and Δ_S corrections properly account for LDP noise. Test with extreme cases (k=100, ε=0.1, n=100) to ensure guarantees hold.

2. **Scalability Test**: Implement LDP-CP-S on a larger dataset (e.g., ImageNet subset with >100 classes) to verify robustness to high class counts and validate n ≥ 10^5 requirement.

3. **Shuffle Model Implementation**: Reproduce shuffle model results by implementing a secure shuffler; measure actual ε_eff versus theoretical ε/√n and validate privacy amplification empirically.