---
ver: rpa2
title: 'Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning
  Benchmark for LLMs'
arxiv_id: '2504.11239'
source_url: https://arxiv.org/abs/2504.11239
tags:
- problems
- problem
- reasoning
- error
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Nondeterministic Polynomial-time Problem Challenge (NPPC) introduces
  an ever-scaling reasoning benchmark designed to evaluate large language models (LLMs)
  on 25 NP-complete problems across 10 difficulty levels. The benchmark addresses
  the rapid obsolescence and exploitation vulnerabilities of static benchmarks by
  providing infinite problem instances with systematic complexity scaling.
---

# Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs

## Quick Facts
- **arXiv ID**: 2504.11239
- **Source URL**: https://arxiv.org/abs/2504.11239
- **Reference count**: 40
- **Primary result**: NPPC achieves <10% LLM accuracy at high difficulty levels, confirming its uncrushable nature through infinite problem instances

## Executive Summary
NPPC introduces an ever-scaling reasoning benchmark for LLMs using 25 NP-complete problems across 10 difficulty levels. The benchmark addresses the rapid obsolescence of static benchmarks by providing infinite problem instances with systematic complexity scaling. Extensive experiments demonstrate that advanced LLMs like DeepSeek-R1, Claude-3.7-Sonnet, and o3-mini achieve superior performance across 7 of 12 core problems, while all models fail below 10% accuracy at high difficulty levels. The benchmark reveals distinct failure patterns including cascading assumptions, manual computation errors, and reasoning repetition.

## Method Summary
NPPC comprises three modules: npgym for problem generation and verification, npsolver for model evaluation, and npeval for comprehensive performance analysis. Problem instances are generated with problem-specific parameters that control search space size, and solutions must be produced in JSON format for automated verification. The benchmark uses a two-stage difficulty calibration: human experts configure parameters based on complexity theory, then LLM performance calibrates the 10-level scale (Level 1: >90% success, Level 10: <10% success). Evaluation uses IQM with stratified bootstrap confidence intervals across 30 instances × 3 seeds per difficulty level.

## Key Results
- NPPC effectively reduces advanced LLMs' performance to below 10% accuracy at high difficulty levels
- DeepSeek-R1, Claude-3.7-Sonnet, and o3-mini emerge as the most capable models
- Token analysis shows advanced models scaling token usage with problem complexity, first increasing then decreasing at higher difficulties
- Distinct failure patterns include cascading assumptions, manual computation errors, and reasoning repetition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NP-complete problems provide a theoretically-grounded foundation for ever-scaling benchmarks because they are "difficult to solve, easy to verify"
- Mechanism: NPC problems have no known polynomial-time algorithms (making them computationally intractable for LLMs), yet candidate solutions can be verified in polynomial time. This asymmetry enables automatic verification at any scale while maintaining difficulty.
- Core assumption: LLMs cannot trivialize NPC problems through pattern matching or memorization when forced to produce verifiable solutions rather than binary outputs
- Evidence anchors: [abstract] "no polynomial-time algorithms have been discovered for solving NPC problems"; [section 3] "NPC problems are intrinsically 'difficult to solve, easy to verify'"

### Mechanism 2
- Claim: Difficulty scaling is achieved through controlled parameter manipulation rather than subjective difficulty ratings
- Mechanism: Each NPC problem has well-defined parameters (e.g., num_variables, num_clauses for 3SAT; num_nodes, cover_size for Vertex Cover) that directly control search space size. The paper uses a two-stage approach: human experts configure parameters based on complexity theory, then LLM performance calibrates the 10-level difficulty scale.
- Core assumption: Parameter-based difficulty scales translate meaningfully to LLM reasoning difficulty, and the calibration is not circular
- Evidence anchors: [section 4.1] "difficulty levels are determined with a two-stage approach"; "calibration confirms that higher difficulty levels correspond to lower LLM success rates"

### Mechanism 3
- Claim: Token scaling behavior reveals reasoning breakdown—advanced models increase token usage with difficulty until reaching a failure threshold, then decrease tokens at extreme difficulty
- Mechanism: Reasoning-specialized models scale up tokens for harder problems as a test-time compute strategy. However, beyond a certain difficulty threshold, models "give up" and produce shorter, lower-quality responses. This inverted-U pattern signals where genuine reasoning collapses.
- Core assumption: Token count is a meaningful proxy for reasoning effort, and the decrease at high difficulty reflects reasoning failure
- Evidence anchors: [abstract] "numbers of tokens... first to increase and then decrease when problem instances become more difficult"; [section 5.2] "DeepSeek-R1 demonstrates highest consumption (10,000-20,000 tokens) for successful solutions"

## Foundational Learning

- **P vs NP complexity classes**: Why needed here: The entire benchmark rests on the assumption that NPC problems cannot be solved efficiently but can be verified efficiently. Without understanding why 3SAT is "harder" than sorting, the benchmark's design rationale is opaque. Quick check: Given a proposed satisfying assignment for a 3SAT formula with 100 variables, can you verify it in polynomial time? Can you find such an assignment in polynomial time?

- **Benchmark contamination and memorization**: Why needed here: The paper's motivation (benchmarks being "crushed" and "hacked") depends on understanding how static datasets become unreliable through training data leakage. NPPC's infinite instance generation directly addresses this. Quick check: If an LLM was trained on a dataset containing 10,000 3SAT instances with solutions, would high accuracy on those same instances indicate genuine reasoning capability?

- **Test-time compute scaling**: Why needed here: The paper's analysis of token usage patterns and the performance difference between reasoning models (o1, DeepSeek-R1) and non-reasoning models (GPT-4o) depends on understanding how models allocate compute during inference. Quick check: Why might allowing more output tokens during inference improve an LLM's problem-solving accuracy, and what are the limits of this approach?

## Architecture Onboarding

- **Component map**: npgym (Problem Suite) -> npsolver (LLM Interface) -> npeval (Evaluation Suite)
- **Critical path**: 1. Define problem + difficulty level → npgym.generate_instance() 2. Construct prompt → npsolver builds from template 3. LLM generates response → npsolver extracts JSON solution 4. npgym.verify_solution() checks correctness 5. npeval aggregates across seeds and computes IQM with CIs
- **Design tradeoffs**:
  - JSON output enables automated verification but causes errors for some models
  - Difficulty calibration using LLM performance risks circularity
  - 90 instances per difficulty level across 25 problems × 10 levels = 22,500+ API calls
- **Failure signatures**:
  - JSON Error: Model doesn't complete reasoning or omits format prefix
  - Verification Error: Output format doesn't match expected structure
  - Problem-specific errors: Invalid vertex indices, unsatisfied clauses, tour length exceeded
  - Reasoning failures: cascading assumptions, manual computation errors, reliance on examples
- **First 3 experiments**:
  1. Baseline calibration: Run GPT-4o-mini on all 12 core problems at levels 1-5 with 3 seeds
  2. Single-problem deep dive: Focus on 3SAT across all 10 levels with DeepSeek-R1 and Claude-3.7-Sonnet
  3. Format sensitivity test: Run QwQ-32B with standard parsing vs. relaxed parsing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can NPPC be effectively extended to multimodal domains without exceeding the context windows of current multimodal models?
- Basis in paper: Section 6 lists multimodal extension as a future direction but highlights obstacles: images/videos may exceed input limits, and symbolic problems often lack natural visual components.
- Why unresolved: Multimodal LLMs have fixed context limits, making it difficult to scale visual complexity alongside problem difficulty.
- What evidence would resolve it: A multimodal NPPC implementation that successfully maintains scalable difficulty without hitting context constraints.

### Open Question 2
- Question: Can AI agents equipped with tool use (e.g., code execution, solvers) crush NPPC, or do fundamental computational barriers persist?
- Basis in paper: Appendix A.5 explicitly asks "Can Tool Use Crush NPPC?" and Section 6 identifies the exclusion of tool use evaluation as a limitation.
- Why unresolved: While tools accelerate subroutines, they do not alter the exponential worst-case complexity of NPC problems, and models struggle with correct problem decomposition.
- What evidence would resolve it: Empirical evaluation of tool-augmented agents on high-difficulty NPPC instances to determine if performance saturates.

### Open Question 3
- Question: Is there a unified theoretical principle for determining difficulty levels across structurally heterogeneous NP-complete problems that aligns with LLM capabilities?
- Basis in paper: Section 4.1 explicitly poses the question: "Is There a Unified Principle for Difficulty Levels of NPC Problems?"
- Why unresolved: NPC problems vary in representation complexity and search space topology, leading to highly variable LLM performance that prevents a universal metric.
- What evidence would resolve it: A theoretical framework that consistently predicts LLM difficulty across diverse problem types (e.g., graph vs. logic).

## Limitations

- **Calibration circularity risk**: Using LLM performance to calibrate difficulty levels could create circular reasoning, conflating computational complexity with observed LLM capabilities
- **Limited human baseline**: Lack of human performance baselines makes it unclear whether observed LLM limitations reflect genuine reasoning deficits or inherent problem difficulty
- **Single-instance representation**: Using single instances per evaluation rather than averaging across multiple instances may miss problem variations and lead to overfitting

## Confidence

- **High Confidence** in the benchmark's foundational design: NPC problems provide theoretically sound basis for scaling difficulty
- **Medium Confidence** in the scaling methodology: Parameter-based scaling is theoretically grounded but LLM performance calibration introduces uncertainty
- **Low Confidence** in the claim of being "uncrushable": Depends on LLMs not developing polynomial-time algorithms or reliable code execution capabilities

## Next Checks

1. **Human baseline establishment**: Conduct controlled experiments measuring human performance on representative NPC problems across the 10 difficulty levels to validate difficulty calibration and interpret LLM limitations

2. **Instance diversity testing**: Modify the benchmark to evaluate multiple instances per parameter configuration (e.g., 10 instances per level) and analyze performance variance to test single-instance evaluation sufficiency

3. **Alternative format evaluation**: Implement and test alternative solution formats (plain text, code, structured prose) alongside JSON to determine whether format requirements artificially constrain model performance