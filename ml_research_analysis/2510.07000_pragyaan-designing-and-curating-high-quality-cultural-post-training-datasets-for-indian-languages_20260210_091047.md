---
ver: rpa2
title: 'Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets
  for Indian Languages'
arxiv_id: '2510.07000'
source_url: https://arxiv.org/abs/2510.07000
tags:
- arxiv
- data
- indic
- datasets
- indian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces a human-in-the-loop pipeline that combines
  translations and synthetic expansion to create high-quality, culturally grounded
  post-training datasets for Indian languages. The approach addresses the lack of
  multilingual, culturally inclusive instruction-tuning and preference-tuning data
  by curating two datasets: Pragyaan-IT (22.5K) and Pragyaan-Align (100K) across 10
  Indian languages.'
---

# Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages

## Quick Facts
- arXiv ID: 2510.07000
- Source URL: https://arxiv.org/abs/2510.07000
- Reference count: 18
- This work introduces a human-in-the-loop pipeline that combines translations and synthetic expansion to create high-quality, culturally grounded post-training datasets for Indian languages

## Executive Summary
Pragyaan presents a novel approach to creating high-quality post-training datasets for Indian languages by combining human translations with synthetic expansion. The framework addresses the critical gap in multilingual, culturally inclusive instruction-tuning and preference-tuning data through a carefully designed pipeline. The resulting datasets, Pragyaan-IT (22.5K examples) and Pragyaan-Align (100K examples), cover 10 Indian languages across 13 broad and 56 sub-categories, ensuring task diversity and cultural nuance.

The paper demonstrates that models fine-tuned with these datasets show measurable improvements in cultural alignment and task performance. Pilot experiments using the Updesh benchmark show win rates of 60-61% for models trained on Pragyaan-Align, validating the effectiveness of the approach. The human-in-the-loop methodology ensures high instruction fidelity, safety alignment, and authentic cultural representation across the diverse linguistic landscape of India.

## Method Summary
The Pragyaan methodology employs a human-in-the-loop pipeline that combines professional translations with synthetic data expansion to create culturally grounded datasets. The process begins with human translators creating high-quality examples across 13 broad and 56 sub-categories, ensuring cultural authenticity and task diversity. These translations undergo quality checks and are then expanded using controlled synthetic generation techniques to scale the dataset while maintaining fidelity. The pipeline specifically addresses multi-turn dialogue, instruction following, safety alignment, and cultural nuance. Two datasets are produced: Pragyaan-IT (22.5K examples for instruction tuning) and Pragyaan-Align (100K examples for preference alignment), both covering 10 Indian languages.

## Key Results
- Pragyaan-IT contains 22.5K high-quality examples across 10 Indian languages covering 13 broad and 56 sub-categories
- Pragyaan-Align provides 100K examples specifically for preference alignment tasks
- Pilot experiments show models fine-tuned with Pragyaan-Align outperform baselines with 60-61% win rates on the Updesh benchmark

## Why This Works (Mechanism)
The effectiveness of Pragyaan stems from its human-in-the-loop approach that combines professional cultural knowledge with scalable synthetic expansion. By starting with high-quality human translations created by native speakers familiar with cultural contexts, the datasets capture authentic linguistic patterns and cultural nuances that purely synthetic approaches often miss. The synthetic expansion phase then scales these high-quality examples while maintaining the cultural integrity established by human translators. This hybrid approach ensures both quality and quantity, addressing the typical trade-off in dataset curation where either quality or scale must be sacrificed.

## Foundational Learning
- **Multilingual Instruction Tuning**: Training language models to follow instructions across multiple languages simultaneously; needed because Indian language models require cross-lingual instruction capabilities; quick check: model can follow instructions in all 10 covered languages
- **Cultural Grounding in NLP**: Incorporating cultural context and nuances into language model training; needed because Indian languages carry unique cultural expressions that generic models miss; quick check: examples reflect authentic cultural scenarios
- **Human-in-the-Loop Dataset Curation**: Combining human expertise with automated processes for dataset creation; needed because cultural nuance requires human judgment that AI cannot fully replicate; quick check: human reviewers validate synthetic expansions
- **Synthetic Data Expansion**: Using controlled generation to scale high-quality datasets; needed because manual translation alone cannot achieve the scale required for effective model training; quick check: expanded data maintains quality metrics of original human translations
- **Preference Alignment**: Training models to align with human preferences across multiple dimensions; needed because Indian users have diverse expectations shaped by cultural contexts; quick check: preference examples cover cultural diversity within each language

## Architecture Onboarding

**Component Map**: Human Translation -> Quality Validation -> Synthetic Expansion -> Cultural Validation -> Dataset Packaging

**Critical Path**: The pipeline's critical path is Human Translation → Quality Validation → Cultural Validation → Dataset Packaging. This sequence ensures that every example meets quality and cultural standards before being included in the final dataset.

**Design Tradeoffs**: The primary tradeoff is between quality and scale. The human-in-the-loop approach prioritizes quality through human translation and validation but requires synthetic expansion to achieve training-scale datasets. Another tradeoff involves balancing cultural specificity with general applicability—examples must be culturally authentic while remaining useful across diverse contexts within each language.

**Failure Signatures**: 
- Low instruction fidelity indicates translation quality issues or inadequate quality validation
- Cultural misrepresentation suggests insufficient cultural expertise among translators or inadequate validation
- Synthetic drift occurs when expanded examples deviate from the quality and style of human-translated examples
- Language imbalance suggests scaling issues in the synthetic expansion phase

**3 First Experiments**:
1. Test synthetic expansion on a small subset of human translations to measure quality retention
2. Evaluate cultural alignment through native speaker review of a sample dataset
3. Pilot model fine-tuning on Pragyaan-IT with cross-lingual evaluation across the 10 languages

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research:
- How to systematically evaluate cultural authenticity across diverse Indian contexts
- Whether the human-in-the-loop approach can be effectively automated for larger-scale deployment
- The long-term impact of culturally grounded training on model behavior in real-world applications
- How to extend this methodology to other low-resource language families beyond Indian languages
- The optimal balance between human and synthetic data contributions for different task types

## Limitations
- Evaluation results are based on a single benchmark (Updesh), limiting generalizability across different Indian languages and downstream tasks
- The paper does not provide detailed human evaluation metrics or inter-annotator agreement scores for quality assessment
- The synthetic expansion process and its impact on dataset quality are not thoroughly validated
- The claim of "outperforming baselines" is supported by pilot experiments with unspecified sample sizes and conditions
- The dataset's scalability and applicability to other low-resource languages beyond the 10 Indian languages covered are uncertain

## Confidence

**Confidence Labels:**
- Dataset Curation Process: Medium
- Cultural Grounding Claims: Medium
- Model Performance Improvements: Medium

## Next Checks

1. Conduct human evaluations across multiple Indian languages to assess dataset quality and cultural alignment, including inter-annotator agreement scores
2. Expand evaluation to multiple benchmarks and downstream tasks beyond Updesh to validate generalizability of performance improvements
3. Perform ablation studies to quantify the impact of synthetic expansion on dataset quality and model performance