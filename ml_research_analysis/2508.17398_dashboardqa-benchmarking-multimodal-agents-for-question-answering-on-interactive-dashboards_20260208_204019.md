---
ver: rpa2
title: 'DashboardQA: Benchmarking Multimodal Agents for Question Answering on Interactive
  Dashboards'
arxiv_id: '2508.17398'
source_url: https://arxiv.org/abs/2508.17398
tags:
- dashboard
- question
- answer
- views
- dashboards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DASHBOARD QA introduces the first benchmark for evaluating multimodal
  agents on interactive dashboards, requiring complex GUI navigation and visual reasoning.
  The dataset includes 112 dashboards and 405 questions across five types: factoid,
  multiple-choice, hypothetical, multi-dashboard, and conversational.'
---

# DashboardQA: Benchmarking Multimodal Agents for Question Answering on Interactive Dashboards

## Quick Facts
- arXiv ID: 2508.17398
- Source URL: https://arxiv.org/abs/2508.17398
- Reference count: 25
- Primary result: Introduces the first benchmark for evaluating multimodal agents on interactive dashboards, revealing significant performance gaps (best: 38.69% accuracy)

## Executive Summary
DashboardQA introduces the first benchmark for evaluating multimodal agents on interactive dashboards, requiring complex GUI navigation and visual reasoning. The dataset includes 112 dashboards and 405 questions across five types: factoid, multiple-choice, hypothetical, multi-dashboard, and conversational. Experiments with 12 agents show significant performance gaps, with the best (Gemini-Pro-2.5 with accessibility trees) achieving only 38.69% accuracy. Key challenges include grounding UI elements, planning interaction sequences, and performing multi-view visual reasoning. The benchmark reveals that even advanced agents struggle with tasks humans perform routinely, highlighting the need for improved grounding and planning in GUI-based reasoning.

## Method Summary
The benchmark uses 112 Tableau Public dashboards with 405 QA pairs across five question types. Agents operate in an Ubuntu VM with Chrome, capturing screenshots and accessibility trees as observations. The action space includes GUI operations (click, type, scroll) and meta-actions (DONE, FAIL). Agents are evaluated under two settings: Screenshot only and Screenshot + Accessibility Tree. The environment uses the OSWorld framework with pyautogui for action execution. Twelve agents are tested, including closed-source (GPT-4o, Gemini Pro 2.5), hybrid (JEDI with GPT-4o), and open-source (UI-TARS) models.

## Key Results
- Best performing agent (Gemini-Pro-2.5 with accessibility trees) achieves 38.69% accuracy
- Accuracy drops to 11.86% for screenshot-only setting, highlighting importance of structural data
- Multi-Dashboard questions remain hardest category at only 15.22% accuracy
- Plan tracking errors cause agents to loop and repeat actions until reaching maximum steps

## Why This Works (Mechanism)

### Mechanism 1: Structural Augmentation for Grounding
Providing the Accessibility Tree (A11y) alongside raw pixels significantly improves agent performance by reducing the visual grounding search space. Raw pixel input requires the model to perform OCR and layout estimation implicitly. Injecting the A11y tree provides explicit coordinates and element types (e.g., "dropdown," "button"), allowing the model to bypass low-level visual detection and focus computational resources on reasoning and planning.

### Mechanism 2: Hybrid Decoupling of Planning and Acting
Decomposing the agent into a specialized "Planner" (Large VLM) and a specialized "Actor/Grounder" (Smaller SLM) outperforms monolithic models in GUI navigation. High-capacity models (e.g., GPT-4o, Gemini) excel at high-level reasoning but struggle with precise pixel coordinate output. Specialized grounding models (e.g., JEDI) are trained specifically to map instructions to coordinates. The hybrid approach prevents the "planner" from hallucinating UI actions it cannot precisely locate.

### Mechanism 3: Multi-Step State Tracking
Success in DashboardQA correlates with the ability to maintain a "state belief" over multiple interaction steps (e.g., filtering, tab switching). Unlike static QA, interactive QA requires the agent to update its internal context after every action. The mechanism relies on the model's context window retaining the visual history or a summary of previous states to prevent redundant actions.

## Foundational Learning

**Concept: Accessibility Trees (A11y)**
- Why needed here: This is the single most effective signal for improving agent scores in this benchmark. Understanding the structure (roles, names, values) of UI widgets is crucial for robust navigation.
- Quick check question: How does the agent behavior change if the A11y tree returns an empty list for a Canvas-based chart?

**Concept: Grounding vs. Planning**
- Why needed here: The paper explicitly distinguishes errors in *locating* elements (Grounding) from errors in *deciding what to do next* (Planning/Reasoning).
- Quick check question: If an agent clicks the correct button but for the wrong logical reason, is this a grounding failure or a planning failure?

**Concept: Coordinated Views**
- Why needed here: Dashboards link multiple charts. An interaction in one (e.g., selecting a region in a map) alters data in others (e.g., updating a bar chart).
- Quick check question: Why is static screenshot analysis insufficient for answering questions about "filtered" states?

## Architecture Onboarding

**Component map:**
Environment (Ubuntu VM + Chrome) -> Observation (Screenshot + A11y Tree) -> Agent (Planner + Optional Grounder) -> Action Space (pyautogui commands + Meta actions)

**Critical path:**
1. Parse Question: Identify the goal (e.g., "Compare X and Y")
2. Observe: Capture Screenshot + A11y
3. Plan: Identify the necessary UI elements and the sequence of interactions (e.g., "Select Dropdown A, then read Chart B")
4. Act: Execute mouse/keyboard command
5. Verify: Check if the new state contains the answer; repeat or terminate

**Design tradeoffs:**
- Pixels vs. A11y: Pixels are universal but brittle; A11y is robust but reliant on web-standard implementation
- Monolithic vs. Hybrid: Hybrid models offer better grounding accuracy but increase system complexity and inference cost

**Failure signatures:**
- Looping: Agent repeats the same dropdown selection or tab switch (Plan Tracking Error)
- Hallucination: Agent claims to see data that isn't visible in the current state (Incorrect Information Retention)
- Grounding Drift: Agent clicks slightly outside the target button, failing to register the action

**First 3 experiments:**
1. Ablate Input Modalities: Run a baseline model (e.g., GPT-4o) with *only* Screenshots vs. *only* A11y trees vs. *both* to isolate the value of structural data
2. Step-Limit Analysis: Evaluate the "Plan Tracking Error" rate as the required number of interaction steps increases (e.g., bins of 1-5, 6-10, 11+ steps)
3. Hybrid vs. Monolithic: Compare the error types of a unified model (OpenAI CUA) against a pipeline model (Jedi + GPT-4o) to determine if failures are dominated by grounding or reasoning

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How can multimodal agents be improved to maintain accurate state tracking and prevent action looping during long-horizon dashboard navigation?
- Basis in paper: The authors explicitly identify "Plan Tracking Errors" in the qualitative analysis, noting that models often fail to recall completed steps and "loop back and repeats the previous actions... until the maximum number of steps was reached."
- Why unresolved: Current VLMs lack robust mechanisms to store and update the history of UI interactions, leading to redundant actions in tasks requiring more than a few steps.
- What evidence would resolve it: A significant reduction in the frequency of "max steps reached" failures and repetitive action cycles in the trajectory logs of agents tested on the benchmark.

**Open Question 2**
- Question: What architectural innovations are necessary to facilitate robust cross-view and cross-dashboard reasoning in interactive environments?
- Basis in paper: The paper highlights that "Multi-Dashboard remains the hardest category" with a top accuracy of only 15.22%, identifying "multi-view integration" and "cross-dashboard navigation" as key limitations.
- Why unresolved: Agents struggle to synthesize information when it requires switching contexts between multiple visualizations or dashboards, often failing to link related data points across views.
- What evidence would resolve it: A substantial increase in accuracy on the Multi-Dashboard and Hypothetical question categories, which explicitly require synthesizing information from disparate visual sources.

**Open Question 3**
- Question: To what extent do current grounding and reasoning capabilities transfer to desktop-based analytics platforms (e.g., Power BI) where accessibility tree inputs are unavailable?
- Basis in paper: The "Limitations" section notes the dataset is "tableau-centric" and relies on web URLs, explicitly stating that accessibility tree access "is not going to be available in desktop apps."
- Why unresolved: The benchmark currently relies on web-based accessibility trees (A11y) to aid grounding, a luxury that may not exist in native desktop applications commonly used in real-world workflows.
- What evidence would resolve it: Performance evaluation on a benchmark extension comprising native desktop dashboards, using strictly pixel-based observation spaces.

## Limitations
- The benchmark relies on web-based accessibility trees that may not be available in desktop applications like Power BI
- Maximum step limit of 25 may truncate longer reasoning chains
- Evaluation environment may not fully capture real-world dashboard latency and interaction nuances
- Closed-source baselines limit ability to reproduce exact behavior and debug failures

## Confidence
- **High Confidence**: The core claim that multimodal agents struggle significantly with interactive dashboard QA (best accuracy ~38.69%) is well-supported by the experimental results across 12 agents.
- **Medium Confidence**: The assertion that accessibility trees improve grounding accuracy is supported, but the magnitude of improvement may vary depending on dashboard implementation and A11y completeness.
- **Low Confidence**: The relative contribution of planning vs. grounding errors to overall failure rates is not precisely quantified, as error categorization appears heuristic.

## Next Checks
1. **A11y Coverage Validation**: Systematically test agent performance on dashboards known to have sparse or missing accessibility tree data (e.g., Canvas-based charts) to quantify the brittleness of the A11y-dependent approach.
2. **Step Limit Sensitivity**: Vary the maximum interaction steps (e.g., 10, 15, 25, 50) to determine if performance plateaus before the current limit, indicating a fundamental reasoning bottleneck rather than a context window constraint.
3. **Error Type Attribution**: Implement a more granular logging system to automatically classify failures as planning, grounding, or state-tracking errors based on action logs and observation changes.