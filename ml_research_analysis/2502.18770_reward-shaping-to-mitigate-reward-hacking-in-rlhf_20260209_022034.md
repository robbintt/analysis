---
ver: rpa2
title: Reward Shaping to Mitigate Reward Hacking in RLHF
arxiv_id: '2502.18770'
source_url: https://arxiv.org/abs/2502.18770
tags:
- reward
- training
- hacking
- shaping
- rlhf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles reward hacking in RLHF, where language models
  exploit reward function flaws instead of learning true alignment. The authors propose
  two key principles: RL rewards should be bounded and benefit from rapid initial
  growth with gradual convergence.'
---

# Reward Shaping to Mitigate Reward Hacking in RLHF

## Quick Facts
- arXiv ID: 2502.18770
- Source URL: https://arxiv.org/abs/2502.18770
- Reference count: 40
- One-line primary result: PAR achieves at least 5 percentage points higher win rates on AlpacaEval 2.0 than baseline methods while maintaining robustness through two epochs of training

## Executive Summary
This paper tackles reward hacking in RLHF, where language models exploit reward function flaws instead of learning true alignment. The authors propose two key principles: RL rewards should be bounded and benefit from rapid initial growth with gradual convergence. Based on these, they introduce Preference As Reward (PAR), which applies a sigmoid function to centered rewards, effectively reducing variance and stabilizing training. PAR is shown to outperform other reward shaping methods, achieving at least 5 percentage points higher win rates on AlpacaEval 2.0 and maintaining robustness even after two epochs of training. Additionally, PAR is highly data-efficient, requiring only a single reference reward for optimal performance.

## Method Summary
The paper proposes Preference As Reward (PAR), a reward shaping method that computes r_RL = (1/M) Σ σ(r - r_ref^m), where r is the proxy reward from the reward model and r_ref^m are M reference rewards from reference model responses. PAR applies a sigmoid to centered rewards (r - r_ref^m), effectively bounding rewards between 0 and 1 while maximizing gradient at initialization for rapid early learning. The method is integrated into PPO training with standard hyperparameters: policy lr=3e-7, critic lr=5e-6, γ=1.0, λ=0.95, ε=0.2, KL penalty=0.005, buffer=4, with linear warmup for the first 0.1 epoch.

## Key Results
- PAR achieves at least 5 percentage points higher win rates on AlpacaEval 2.0 compared to baseline methods
- PAR maintains training stability through two epochs while other methods show reward hacking
- PAR requires only M=1 reference sample for near-optimal performance, demonstrating high data efficiency

## Why This Works (Mechanism)

### Mechanism 1: Bounded Rewards Reduce Critic Variance
- Claim: Bounding RL rewards via sigmoid stabilizes critic training by limiting return variance
- Mechanism: The sigmoid constrains shaped rewards to (0,1). This bounds the variance of discounted returns to ≤ 1/(1-γ)², reducing noise in the critic's MSE regression target
- Core assumption: Unbounded proxy rewards create high-variance returns that destabilize the critic
- Evidence anchors: PAR shows markedly more stable critic loss than vanilla training in Figure 2; Theorem 3.1 provides variance bound proof

### Mechanism 2: Centered-Sigmoid Gradient Structure Accelerates Early Learning
- Claim: Applying sigmoid to centered rewards maximizes gradient at initialization, enabling rapid early learning
- Mechanism: Sigmoid σ(x) has maximum slope at x=0. Since policy is initialized from reference, centered rewards start near zero, providing strong learning signal early
- Core assumption: Policy and reference models are sufficiently similar at initialization
- Evidence anchors: Figure 5 shows centered sigmoid achieves ~0.675 winrate vs ~0.575 for uncentered variants

### Mechanism 3: Preference Interpretation Grounds Optimization in Reward Model Calibration
- Claim: PAR's formulation equals P(y ≻ y_ref|x) under Bradley-Terry assumptions, aligning optimization with reward model's actual preference judgments
- Mechanism: Reward models trained on preference data encode P(y ≻ y') = σ(r(y) - r(y')). PAR uses this probability directly as the RL reward
- Core assumption: Preference probabilities are better calibrated than raw reward magnitudes
- Evidence anchors: Figure 8 shows PAR maintains better calibration between preference score and actual winrate than baselines

## Foundational Learning

### Concept: Generalized Advantage Estimation (GAE)
- Why needed here: PAR's bounded rewards reduce variance of returns G_t, which propagates to advantage estimates A_t = Σ(γλ)^(l-t) δ_l
- Quick check question: If per-token rewards are bounded to |r_l| < 1 and γ = 0.99, what's the maximum variance of G_t? (Answer: ≤ 1/(1-0.99)² = 10,000)

### Concept: Bradley-Terry Preference Model
- Why needed here: PAR's core insight relies on P(y₁ ≻ y₂) = σ(r(y₁) - r(y₂))
- Quick check question: If r(y₁) = 3.0 and r(y₂) = 1.0, what preference probability does Bradley-Terry predict? (Answer: σ(2.0) ≈ 0.88)

### Concept: Reward Hacking (Goodhart's Law in RL)
- Why needed here: The central problem - agents exploit proxy reward flaws rather than learning intended behavior
- Quick check question: In Figure 6, why does vanilla PPO's proxy reward keep rising while winrate crashes? (Answer: The policy exploits reward model weaknesses - generating outputs that score highly on the proxy but degrade actual quality)

## Architecture Onboarding

### Component map:
- Reference Model (π_ref) -> Reward Model (r_φ) -> Policy Model (π_θ) -> Critic Model (V_α)
- PPO trainer applies PAR shaping: r_RL = (1/M) Σ σ(r - r^m_ref)

### Critical path:
1. Sample prompt x from dataset
2. Generate policy response y ~ π_θ(·|x)
3. Generate M reference responses y^m_ref ~ π_ref(·|x)
4. Compute proxy reward r = r_φ(x, y) and reference rewards r^m_ref = r_φ(x, y^m_ref)
5. Apply PAR shaping: r_RL = (1/M) Σ σ(r - r^m_ref)
6. Construct per-token rewards (final token gets r_RL; others get KL penalty only)
7. Compute GAE and returns; run PPO update

### Design tradeoffs:
- Reference sample count (M): Paper shows M=1 performs nearly identically to M=10 (Figure 7a). Use M=1 for efficiency; increase only if reference responses show high variance
- Sigmoid vs alternatives: Tanh bounds symmetrically; paper finds sigmoid(centered) wins empirically (Figure 5). Prefer sigmoid unless domain suggests asymmetric saturation
- Reference model selection: Must match policy initialization for centered rewards to start near zero. Using a different reference model weakens early gradient benefits

### Failure signatures:
- Winrate drops while proxy reward climbs: Classic reward hacking (see vanilla curve in Figure 6)
- Critic loss spikes or oscillates: Unbounded rewards creating high return variance (compare Figure 2 vanilla vs PAR)
- Early winrate plateau at low levels: Gradient too weak at initialization; suggests centered rewards not starting near zero
- Proxy reward hits ~6.0 then winrate crashes: Paper identifies this threshold (Figures 4, 6); methods that don't bound rewards fail here

### First 3 experiments:
1. Baseline sanity check: Train vanilla PPO vs PAR on UltraFeedback-Binarized with Gemma-2B. Plot proxy reward and winrate over 1 epoch. Expect vanilla to show hacking pattern; PAR should maintain stable or rising winrate
2. Reference sample ablation: Compare PAR with M=1, M=3, M=10. Per Figure 7a, all should converge similarly. If M=1 underperforms, check reference response quality
3. Shaping function sweep: Test sigmoid(centered), tanh(centered), and uncentered sigmoid on the same data. Expect centered sigmoid to win (Figure 5 pattern). If uncentered competes, investigate whether your reward model produces near-zero-initialized outputs naturally

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the specific dynamics of reward adjustment, such as the initial rate of increase and the pace of convergence, influence the onset of reward hacking?
- Basis in paper: Section 7 (Limitations) states that "the dynamics of reward adjustment—such as the initial rate of increase and the pace of convergence—are not fully elucidated"
- Why unresolved: While the paper proposes a sigmoid shape to implement these dynamics, it does not analyze the sensitivity of training stability to the specific hyperparameters (e.g., steepness) of the growth and convergence phases
- What evidence would resolve it: An ablation study varying the slope parameter k in the sigmoid function and correlating it with the duration of the stable training window

### Open Question 2
- Question: Can the PAR method be modified to improve the model's peak performance, rather than just extending the training window?
- Basis in paper: Section 7 notes that PAR "does not improve peak performance, as measured by the winrate of the best checkpoint"
- Why unresolved: The current design prioritizes variance reduction and stability (extending the tolerance window) over maximizing the absolute upper bound of the policy's capability
- What evidence would resolve it: A variant of PAR that achieves a higher maximum win-rate on AlpacaEval 2.0 compared to the peak of the best-performing baseline methods

### Open Question 3
- Question: Is the critic model the primary driver of reward hacking in RLHF, given the observation that critic-free methods like GRPO do not exhibit it?
- Basis in paper: Section 5 states that GRPO does not exhibit hacking and hypothesizes this is "likely due to the absence of a critic model"
- Why unresolved: The paper establishes that unbounded rewards destabilize the critic (Theorem 3.1), but it is not confirmed if removing the critic entirely is a superior or generalizable solution compared to shaping rewards
- What evidence would resolve it: A controlled comparison of PPO and GRPO using identical unbounded rewards to isolate the critic's contribution to reward hacking

## Limitations

- PAR does not improve peak performance - it extends the training window but doesn't raise the absolute upper bound of winrate
- The specific dynamics of reward adjustment (initial growth rate, convergence pace) are not fully elucidated and their sensitivity to hyperparameters remains untested
- Performance with more complex reward model architectures (beyond linear heads) has not been evaluated

## Confidence

- **High Confidence**: The variance reduction mechanism and its mathematical grounding in Theorem 3.1 are well-supported by theory and empirical evidence in Figure 2
- **Medium Confidence**: The centered-sigmoid gradient acceleration is supported by Figure 5 but relies on the assumption of reference model alignment with policy initialization
- **Medium Confidence**: The preference interpretation grounding is theoretically sound but empirical advantage depends on reward model's actual preference generalization capabilities

## Next Checks

1. **Architecture Ablation**: Test PAR with different reward model architectures (MLP heads of varying sizes, transformer-based reward heads) on UltraFeedback-Binarized. Compare winrate stability and reward hacking resistance against the linear head baseline

2. **Dataset Scale Analysis**: Train PAR on subsets of UltraFeedback-Binarized ranging from 10% to 100% of the data. Measure winrate and data efficiency relative to baseline methods. This validates the claim of PAR's data efficiency across scales

3. **Long Horizon Stability**: Extend PPO training to 5 epochs on HH-RLHF with PAR. Monitor winrate, proxy reward, and critic loss stability over time. Compare against baseline methods to assess whether PAR's hacking resistance persists