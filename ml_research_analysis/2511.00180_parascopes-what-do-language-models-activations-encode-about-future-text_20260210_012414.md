---
ver: rpa2
title: 'ParaScopes: What do Language Models Activations Encode About Future Text?'
arxiv_id: '2511.00180'
source_url: https://arxiv.org/abs/2511.00180
tags:
- arxiv
- outline
- text
- parascope
- residual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ParaScopes, a framework for decoding paragraph-
  and document-scale planning information from language model activations. The authors
  propose Residual Stream Decoders as a method to extract future content from current
  model activations, testing the hypothesis that language models encode forward-looking
  information.
---

# ParaScopes: What do Language Models Activations Encode About Future Text?

## Quick Facts
- arXiv ID: 2511.00180
- Source URL: https://arxiv.org/abs/2511.00180
- Authors: Nicky Pochinkov; Yulia Volkova; Anna Vasileva; Sai V R Chereddy
- Reference count: 40
- Primary result: Both Continuation and TAE ParaScopes decode 5+ tokens of future context from LLM activations, with planning signals concentrated in middle layers.

## Executive Summary
This paper introduces ParaScopes, a framework for decoding paragraph- and document-scale planning information from language model activations. The authors propose Residual Stream Decoders to extract future content from current model activations, testing whether language models encode forward-looking information. Using Llama 3.2 3B, they demonstrate that both Continuation ParaScope (zero-shot generation) and TAE ParaScope (linear mapping to structured embeddings) successfully decode information equivalent to 5+ tokens of future context, with layer-wise analysis revealing that planning-relevant signals concentrate in middle layers (60-80% depth).

## Method Summary
ParaScopes employs two complementary approaches to decode future paragraph content from LLM activations at "\n\n" boundary tokens. The Continuation ParaScope extracts the residual at the paragraph boundary, injects it into a blank context, and generates 128 tokens to produce a continuation. The TAE ParaScope extracts residual diffs from the final 12 layers, normalizes them, and trains a linear probe to map activations to SONAR embeddings. Both methods are evaluated against ground-truth next paragraphs using cosine similarity, BLEURT-20, and LLM-as-a-Judge rubric scoring. The framework tests the Decodability Hypothesis that planning information is accessible from current activations.

## Key Results
- Both TAE ParaScope and Continuation ParaScope outperform random baseline (0.20) and approach cheat-5 baseline (0.50) on paragraph-level decoding tasks.
- Layer-wise analysis reveals planning-relevant signals concentrate in middle layers (60-80% depth) of Llama 3.2 3B.
- TAE ParaScope better preserves general subject matter while Continuation ParaScope better captures specific details.
- Outline-level planning experiments show limited success, suggesting constrained long-horizon planning in small models.

## Why This Works (Mechanism)
The framework works by exploiting the hypothesis that language models encode forward-looking planning information in their residual streams. At paragraph boundaries (marked by "\n\n" tokens), the model must have sufficient context to plan the next paragraph's content. By extracting and decoding these residual activations, ParaScopes can reconstruct future content without additional generation. The middle layers' concentration of planning signals suggests that intermediate computational stages integrate current context with prospective content organization, creating a bridge between immediate generation and future planning.

## Foundational Learning

**Activation Space and Residual Streams**: Language models process information through residual connections where each layer adds modifications to the token's representation. Why needed: Understanding how activations encode information is fundamental to decoding future content. Quick check: Verify that residual streams capture meaningful semantic differences between token positions.

**Embedding Spaces and Similarity Metrics**: SONAR embeddings provide structured representations of text, while cosine similarity measures vector alignment. Why needed: These enable quantitative comparison between predicted and actual future content. Quick check: Confirm that ground-truth next paragraphs achieve high cosine similarity with themselves via auto-decode.

**Layer-wise Activation Analysis**: Different model layers specialize in different computational tasks. Why needed: Identifying which layers contain planning information is crucial for efficient decoding. Quick check: Validate that planning signals concentrate in middle layers rather than input/output layers.

**Zero-shot Generation vs. Supervised Learning**: Two paradigms for extracting information from models. Why needed: Continuation ParaScope uses zero-shot generation while TAE uses supervised linear mapping, representing different trade-offs. Quick check: Compare output quality and computational efficiency between both approaches.

**Structured Output Spaces**: SONAR provides sentence-level embeddings while TAE ParaScope targets paragraph-level planning. Why needed: Choosing appropriate output spaces affects what planning information can be decoded. Quick check: Verify that linear probes can effectively map activations to target embedding spaces.

## Architecture Onboarding

**Component Map**: FineWeb-Edu chunks -> Gemma 2 27B prompt conversion -> Llama-3.2-3B generation -> "\n\n" boundary detection -> Residual extraction -> TAE linear mapping OR Continuation generation -> Evaluation via cosine/BLEURT/LLM-judge

**Critical Path**: The most computationally intensive step is generating the 1M synthetic prompts with Llama 3.2 3B-Instruct at temperature 0.3, followed by training the TAE linear probe on 100K samples with batch size 1024.

**Design Tradeoffs**: The paper balances between zero-shot interpretability (Continuation) and supervised accuracy (TAE). Continuation ParaScope is more interpretable but may conflate planning with generation artifacts, while TAE ParaScope is more accurate but requires training data and assumes linear separability.

**Failure Signatures**: TAE outputs incoherent gibberish when SONAR embedding space is misaligned or wrong checkpoint used. Continuation ParaScope generates generic text when activation injection occurs at wrong token position or residual is corrupted.

**First Experiments**: 1) Verify residual extraction works by comparing activations at "\n\n" versus preceding tokens. 2) Test SONAR auto-decode baseline to confirm embedding space alignment. 3) Implement simple linear probe on small sample to verify TAE framework functionality.

## Open Questions the Paper Calls Out

**Causal vs. Correlational Planning Signals**: Whether decoded planning signals are causally involved in model generation or merely correlated byproducts remains unclear. The Decodability Hypothesis is correlational, and probes could infer "plans that are not actually present, due to correlations."

**Model Size and Architecture Scaling**: Planning capacity may scale differently across model sizes, but only Llama 3.2 3B was tested. Larger models (70B+) might encode richer plans at longer horizons.

**Generalization to Structured Domains**: The approach doesn't generalize cleanly to math, code, and chemistry where explicit reasoning is required. These domains may have different planning structures or units.

**Temporal Distribution of Planning Information**: Planning information may be distributed across many tokens rather than concentrated at boundaries. The sharp shifts at "\n\n" tokens might reflect task design rather than fundamental properties.

## Limitations

- The synthetic corpus generation pipeline (FineWeb-Edu → prompts → controlled generation) differs markedly from natural text, potentially limiting generalizability.
- Evaluation relies on proxy metrics (cosine similarity, BLEURT-20, LLM-as-a-Judge) that may not fully capture semantic equivalence or practical utility.
- Outline-level planning experiments show limited success, suggesting the framework works better for paragraph-scale than long-horizon planning.

## Confidence

**High Confidence**: The finding that middle layers (60-80% depth) concentrate planning-relevant signals is well-supported by layer-wise analysis across both decoding approaches.

**Medium Confidence**: Quantitative results showing both methods outperforming random baselines and approaching cheat-5 performance are methodologically sound but depend heavily on the controlled corpus and evaluation pipeline.

**Low Confidence**: Outline-level planning results showing limited success for topic sequences should be interpreted cautiously due to less developed methodology and potential dataset limitations.

## Next Checks

1. **Prompt Template Replication**: Reconstruct and test the exact FineWeb-Edu to prompt conversion pipeline using publicly available FineWeb-Edu samples and Gemma 2 27B, verifying synthetic corpus generation matches specifications.

2. **SONAR Embedding Verification**: Implement TAE ParaScope with exact SONAR checkpoint from Duquenne et al. 2023, validating 0.94 auto-decode baseline and probe generalization to held-out data.

3. **Natural Text Generalization**: Evaluate both ParaScope methods on held-out naturally occurring paragraphs (not from controlled generation pipeline) to assess whether planning signals transfer to real writing using same evaluation metrics.