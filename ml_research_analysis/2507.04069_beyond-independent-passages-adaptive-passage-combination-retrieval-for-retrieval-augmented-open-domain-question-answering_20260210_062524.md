---
ver: rpa2
title: 'Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval
  Augmented Open-Domain Question Answering'
arxiv_id: '2507.04069'
source_url: https://arxiv.org/abs/2507.04069
tags:
- retrieval
- passage
- arxiv
- passages
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adaptive Passage Combination Retrieval (AdaPCR),
  a retrieval framework for open-domain question answering that explicitly models
  dependencies between passages to improve retrieval quality. Traditional RAG methods
  retrieve passages independently, which often leads to redundancy and insufficient
  diversity, particularly problematic for multi-hop questions and noisy corpora.
---

# Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval Augmented Open-Domain Question Answering

## Quick Facts
- arXiv ID: 2507.04069
- Source URL: https://arxiv.org/abs/2507.04069
- Reference count: 17
- Primary result: AdaPCR achieves 1.10 EM points on NQ, 8.11 EM points on TriviaQA, and 4.30 EM points on HotpotQA by modeling passage dependencies

## Executive Summary
This paper introduces Adaptive Passage Combination Retrieval (AdaPCR), a retrieval framework that addresses the limitations of traditional RAG methods by explicitly modeling dependencies between passages. Instead of retrieving passages independently, AdaPCR jointly selects passage combinations, using context-aware query reformulation and a reranking step trained with a predictive objective aligned with downstream answer likelihood. The framework adaptively selects the number of retrieved passages without requiring additional stopping modules.

Experiments demonstrate AdaPCR's effectiveness across multiple QA benchmarks, with particularly strong gains on multi-hop reasoning tasks. The method outperforms baselines like IC-RALM, showing improvements of 1.10 EM points on NQ, 8.11 EM points on TriviaQA, and 4.30 EM points on HotpotQA. The paper also explores different loss functions, finding that RAG loss consistently outperforms alternatives like KL divergence and cross-entropy loss.

## Method Summary
AdaPCR implements a two-stage retrieval process where passages are scored and selected jointly rather than independently. First, a bi-encoder retrieves top-k candidates from the corpus. Each candidate is then concatenated with the original query to form a new query, triggering a second retrieval round. All candidates—singles from the first stage and pairs from the second—are scored with the same bi-encoder, and the highest-scoring combination is selected. The reranker is trained with RAG loss, which computes a weighted average of retriever confidence and language model answer score, providing a stable learning signal that handles varying input ranges.

## Key Results
- AdaPCR outperforms IC-RALM by 1.10 EM points on NQ, 8.11 EM points on TriviaQA, and 4.30 EM points on HotpotQA
- RAG loss consistently outperforms KL divergence and cross-entropy loss across all datasets
- Particularly strong gains on multi-hop reasoning tasks like HotpotQA
- The framework adaptively selects passage combinations without requiring additional stopping modules

## Why This Works (Mechanism)

### Mechanism 1
Scoring passage combinations jointly improves retrieval for multi-hop reasoning over independent passage selection. First-stage retrieval produces k candidates, each concatenated with the query to form a new query, triggering a second retrieval round. All candidates—singles from D₁ and pairs from D₂—are scored with the same bi-encoder, and the highest-scoring combination is selected. Relevant evidence is often distributed across passages, and combinations capture inter-passage dependencies that single-passage scores miss. Break condition: When queries are simple, single-hop, or the answer is fully contained in one passage, combination overhead provides marginal or no benefit while increasing latency.

### Mechanism 2
Context-aware query reformulation (passage ⊕ query) improves second-stage retrieval relevance for follow-up evidence. By concatenating the first retrieved passage with the original query, the reformulated query encodes both the information need and the context already obtained, enabling the second retrieval to find complementary rather than redundant evidence. The retriever's encoder can meaningfully integrate passage and query semantics, and the first-stage passage provides a useful signal rather than noise. Break condition: If the first-stage passage is irrelevant or misleading, concatenation may amplify noise and degrade second-stage retrieval precision.

### Mechanism 3
Training the reranker with RAG loss (aligned with downstream answer likelihood) produces more stable and effective retrievers than KL divergence or cross-entropy. RAG loss computes a weighted average of retriever confidence P_ret(d|x) and LM answer likelihood P_LM(y|[d;x]), avoiding softmax normalization on LM outputs. This naturally handles varying answer score ranges and prioritizes examples with higher answer likelihood during optimization. Downstream answer likelihood is a reliable proxy for retrieval quality, and the gradient signal from marginalizing over passage combinations is informative. Break condition: When models can answer correctly from parametric knowledge without retrieval, or when LM likelihood is miscalibrated, the alignment signal becomes noisy.

## Foundational Learning

- **Bi-encoder dense retrieval**
  - Why needed here: AdaPCR uses a bi-encoder (query encoder E_q and passage encoder E_d) with cosine similarity. Understanding this architecture clarifies how scoring is shared across stages and why it's efficient for large corpora.
  - Quick check question: Can you explain why bi-encoders are preferred over cross-encoders for first-stage retrieval at scale, and what the tradeoffs are?

- **RAG-Sequence loss and marginalization over documents**
  - Why needed here: The paper's training objective marginalizes over retrieved passage combinations to compute P(y|x). Understanding this probabilistic formulation is essential for implementing and debugging the reranker training loop.
  - Quick check question: How does RAG-Sequence loss differ from treating retrieval as a hard selection, and what does marginalization add to the gradient signal?

- **Contrastive learning with in-batch negatives**
  - Why needed here: The retriever is trained with in-batch negative sampling, which affects how positive and negative passage combinations are formed and how the embedding space is shaped.
  - Quick check question: What are in-batch negatives, and how do they differ from hard negatives in retrieval training?

## Architecture Onboarding

- **Component map**: Query → First-stage retrieval (k=5) → For each: concatenate passage⊕query → Second-stage retrieval (k per reformulated query) → Score 30 combinations → Select best → LLM generation

- **Critical path**: Query → First-stage retrieval (k=5) → For each: concatenate passage⊕query → Second-stage retrieval (k per reformulated query) → Score 30 combinations → Select best → LLM generation

- **Design tradeoffs**: k=5 balances computational cost (k² combinations) with retrieval breadth; larger k increases latency and context window pressure. Limiting to max 2 passages avoids context overflow but may miss queries requiring 3+ evidences. Discarding training examples where answer is not in top-100 improves stability but may bias toward "easy" retrieval scenarios. RAG loss vs. CE loss: theoretically equivalent at convergence (Appendix B), but RAG loss is more numerically stable in practice.

- **Failure signatures**: Redundant combinations: if first-stage passages are near-duplicates, second-stage will still generate redundant pairs, wasting context. Noise amplification: irrelevant first-stage passage concatenated into query degrades second-stage retrieval. Training collapse with KL loss: temperature sensitivity causes softmax to flatten or spike; RAG loss mitigates this. Context overflow: long passages plus query may exceed input length, requiring truncation that loses information.

- **First 3 experiments**:
  1. Ablation on combination size: Run AdaPCR allowing only singles vs. pairs vs. triples on HotpotQA to measure gains from deeper combinations and identify where returns diminish.
  2. Loss function replication: Reproduce Table 3 (KL vs. CE vs. RAG loss) on your own QA dataset to validate stability claims and observe training dynamics.
  3. First-stage quality sensitivity: Swap the first-stage retriever (BM25 vs. DPR vs. fine-tuned DPR) and measure final EM/F1 to quantify error propagation from initial retrieval quality.

## Open Questions the Paper Calls Out

### Open Question 1
Does the RAG loss empirically converge to the same optimal retriever behavior as Cross-Entropy (CE) loss given sufficient training compute? The authors terminated training early due to resource constraints, leaving the theoretical proof unverified by experimental data. Training retrievers with RAG loss until full convergence and comparing the resulting score distributions against CE-trained models would resolve this.

### Open Question 2
Does extending the adaptive retrieval process beyond two rounds improve performance on tasks requiring reasoning chains longer than two hops? The current framework architecture is fixed at a two-stage pipeline (D1 and D2), limiting its applicability to complex n-hop reasoning. Implementing a recursive version of AdaPCR and evaluating on datasets specifically constructed for 3+ hop reasoning would resolve this.

### Open Question 3
How does AdaPCR's joint passage selection compare to methods that aggregate probabilities from multiple independent inference passes? The paper focuses on constructing a single optimal context rather than marginalizing over multiple contexts, leaving the relative efficacy of these approaches unknown. A comparative analysis of AdaPCR against marginalization-based baselines (e.g., REPLUG) on the same multi-hop benchmarks would resolve this.

## Limitations
- The framework assumes answers can be found within max 2 passages, which may not hold for complex reasoning tasks
- Query reformulation could amplify noise if first-stage retrieval is poor
- Training data filtering (requiring answers in top-100) may bias results toward easier retrieval scenarios
- The approach increases computational cost through combination scoring and second-stage retrieval

## Confidence
- **High confidence**: Claims about RAG loss stability and effectiveness (validated by consistent Table 3 results across multiple datasets)
- **Medium confidence**: Claims about multi-hop reasoning improvements (strong on HotpotQA but more modest on NQ/TriviaQA)
- **Medium confidence**: Claims about context-aware query reformulation benefits (supported by design but limited direct empirical comparison)

## Next Checks
1. **Ablation study**: Compare AdaPCR's performance when allowing only single passages versus pairs versus triples on HotpotQA to quantify the marginal benefit of deeper combinations and identify where returns diminish.

2. **Loss function replication**: Independently reproduce the KL vs. CE vs. RAG loss comparison on a separate QA dataset to verify training stability claims and observe whether temperature sensitivity affects convergence patterns.

3. **First-stage quality sensitivity**: Replace the first-stage retriever with alternatives (BM25 vs. DPR vs. fine-tuned DPR) and measure impact on final EM/F1 to quantify how much downstream performance depends on initial retrieval quality versus the combination framework itself.