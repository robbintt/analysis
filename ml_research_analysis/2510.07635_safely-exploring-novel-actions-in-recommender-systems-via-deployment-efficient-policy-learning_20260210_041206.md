---
ver: rpa2
title: Safely Exploring Novel Actions in Recommender Systems via Deployment-Efficient
  Policy Learning
arxiv_id: '2510.07635'
source_url: https://arxiv.org/abs/2510.07635
tags:
- policy
- actions
- safety
- novel
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of safely exploring novel actions
  in recommender systems where new items are frequently added. Existing off-policy
  learning methods can be unsafe when novel actions are present, as they may significantly
  underperform the logging policy.
---

# Safely Exploring Novel Actions in Recommender Systems via Deployment-Efficient Policy Learning

## Quick Facts
- **arXiv ID:** 2510.07635
- **Source URL:** https://arxiv.org/abs/2510.07635
- **Reference count:** 40
- **Primary result:** Proposes DEPSUE framework that safely explores novel actions in recommender systems with minimal deployments while guaranteeing performance

## Executive Summary
This paper addresses the challenge of safely exploring novel actions (new items) in recommender systems where existing off-policy learning methods can be unsafe due to unknown rewards for novel items. The authors propose a two-stage approach: first, a model-free Safe Off-Policy Policy Gradient (Safe OPG) method that uses High Confidence Off-Policy Evaluation (HCOPE) to guarantee safety during training, and second, a Deployment-Efficient Policy Learning framework for Safe User Exploration (DEPSUE) that gradually relaxes safety constraints across multiple deployments. Experiments on semi-synthetic and real-world data show that DEPSUE successfully explores novel actions while satisfying safety constraints, outperforming baseline methods that either violate safety or fail to explore sufficiently.

## Method Summary
The method consists of Safe OPG for initial policy learning and DEPSUE for deployment-efficient exploration. Safe OPG uses a two-fold data split (D₀^(S1) for training, D₀^(S2) for validation) to compute HCOPE lower bounds on policy value, providing statistical safety guarantees. The policy is trained via primal-dual optimization where regularization strength λ increases when safety constraints are violated. DEPSUE then deploys the policy iteratively, tracking safety margins from on-policy data to progressively relax constraints in subsequent deployments, enabling more aggressive exploration of novel actions while maintaining cumulative safety guarantees.

## Key Results
- DEPSUE successfully explores novel actions while satisfying safety constraints, achieving higher novelty than conservative baselines
- The method requires only 2-5 deployments to achieve effective exploration, making it practical for real-world systems
- On synthetic data, DEPSUE achieves novelty levels comparable to unsafe methods while maintaining zero safety violations
- Real-world experiments on Wiki10-31K dataset demonstrate the framework's effectiveness with 10K labels (2K novel)

## Why This Works (Mechanism)

### Mechanism 1: High-Confidence Off-Policy Evaluation for Safety Certification
Safe OPG provides statistical guarantees that a learned policy will not underperform a safety threshold with high probability, even when novel actions with unknown rewards exist. The method estimates a high-confidence lower bound on policy value using HCOPE (Eq. 4), which applies Bennett's inequality to importance-weighted rewards. The lower bound estimator V̂⁻(π; D₀) is computed by optimizing a clipping threshold τ on a held-out fold, then applying the bound to an independent validation fold. The safety constraint (Pr(V(π) > C) ≥ 1 - δ) is checked during training, and if violated, regularization strength λ increases to push the policy toward safer behavior.

### Mechanism 2: Safety Margin Accumulation Across Deployments
DEPSUE enables progressively more aggressive exploration of novel actions by tracking cumulative "safety margin"—the excess performance above the safety threshold from prior deployments. After each deployment k, DEPSUE computes the empirical on-policy value V̂_on(π_k'; D_k). The constraint for the next policy π_k becomes: the *cumulative* performance across all deployments must exceed k×C. This transforms the constraint from requiring each policy to individually exceed C to allowing the *average* to exceed C. If prior policies exceeded C by a margin, the k-th policy can safely be more exploratory (potentially even V(π_k) < C) while maintaining the overall guarantee.

### Mechanism 3: Adaptive Imitation Regularization for Novel Actions
The regularization term R(π; D₀) = E_n[r_i log π(a_i|x_i)] encourages the policy to imitate the logging policy *only* for actions with observed positive rewards, providing a model-free approach to handle novel action uncertainty. Rather than building a reward model (which can over/under-estimate novel action values), Safe OPG uses a weighted imitation objective. When the regularization strength λ is high, the policy is pushed toward actions the logging policy chose that yielded positive rewards. Novel actions (never logged) receive no positive gradient signal from R, making the policy conservative about them. As λ decreases (via the adaptive update in Eq. 8 when safety margin permits), exploration signals from entropy regularization can dominate.

## Foundational Learning

- **Concept: Off-Policy Evaluation (OPE) with Importance Sampling**
  - **Why needed here:** Understanding how HCOPE extends standard IPS-based OPE to provide confidence bounds is essential. IPS estimates V(π) = E_n[(π(a|x)/π₀(a|x)) · r], but has high variance and provides no guarantees.
  - **Quick check question:** Given logged data from policy π₀, can you compute IPS estimate for a new policy π? Do you understand why π needs "support" from π₀ (non-zero π₀(a|x) wherever π(a|x) > 0)?

- **Concept: Lagrangian Duality for Constrained Optimization**
  - **Why needed here:** Safe OPG reformulates the constrained problem (maximize V(π) subject to safety) as a min-max problem via λ. Understanding how λ acts as a penalty that increases when constraints are violated is critical.
  - **Quick check question:** In the Lagrangian L(π, λ) = V̂(π) + λ·R(π), what happens to π when λ → ∞? What happens to λ when the safety constraint is satisfied?

- **Concept: Distributional Shift in Offline RL/OPL**
  - **Why needed here:** Novel actions have no logged data, causing reward models to extrapolate (often poorly). This is why model-based safety estimates fail and why model-free approaches like HCOPE are preferred here.
  - **Quick check question:** Why does training a reward model q̂(x, a) on logged data tend to overestimate novel action values? (Hint: what actions does the model see most, and how does it generalize to unseen actions?)

## Architecture Onboarding

- **Component map:** Logged Data D₀ → [Split] → D₀^(S1) (training) + D₀^(S2) (validation) → Policy Network π_ψ ← [Gradient Update via Eq. 7 on D₀^(S1)] → HCOPE Validator ← [Computes V̂⁻(π_ψ; D₀^(S2))] → λ Update (Eq. 8) → Deploy π_ψ → Collect D_k → [DEPSUE: Compute safety margin] → Relax λ for next deployment

- **Critical path:** The two-fold data split is essential. Training and validation must remain independent or HCOPE's statistical guarantees break. In DEPSUE, each deployment must accumulate enough on-policy data (m_k samples) to reliably estimate V̂_on for safety margin calculation.

- **Design tradeoffs:**
  - **K (number of deployments):** Higher K allows more gradual relaxation of safety constraints and better exploration of novel actions, but increases operational cost. Paper shows K=2-5 is often sufficient.
  - **Safety threshold C:** Setting C = V(π₀) ensures no degradation from baseline; can be made more aggressive if business tolerates some risk.
  - **Confidence level δ:** Lower δ (e.g., 0.01) gives stronger safety guarantees but more conservative policies.

- **Failure signatures:**
  - **Zero novel action selection:** λ is too high or safety margin is insufficient; reduce C or increase K.
  - **Safety violations:** HCOPE estimate is unreliable (check fold independence) or on-policy value estimates in DEPSUE are noisy (increase m_k).
  - **No improvement over π₀:** Logging policy is already near-optimal; exploration provides little benefit, and safety constraints correctly limit deviation.

- **First 3 experiments:**
  1. **Sanity check HCOPE on logged data:** Apply HCOPE to estimate V(π₀) using held-out data from π₀ itself. Verify that V̂⁻(π₀) ≈ V(π₀) with appropriate confidence. If not, check data splits and clipping threshold τ optimization.
  2. **Baseline comparison (K=1):** Run Safe OPG with entropy regularization on synthetic data (as in Section 4). Verify: (a) safety violations approach zero, (b) novelty is low but non-zero. Compare to OPG (naive) to confirm safety vs. exploration tradeoff.
  3. **DEPSUE progressive deployment (K=3):** On semi-synthetic data, run 3 deployments with safety margin tracking. Log: (a) V(π_k) and cumulative average, (b) N(π_k) (novelty), (c) λ_k over time. Verify that novelty increases while cumulative safety is maintained.

## Open Questions the Paper Calls Out
None

## Limitations
- Safety guarantees depend critically on i.i.d. assumption for logged data and independence between training and validation folds for HCOPE
- The framework assumes the logging policy's past choices reliably indicate action safety, which may not hold if the logging policy is highly suboptimal
- Safety margin estimates may be unreliable with small sample sizes per deployment, potentially leading to over- or under-exploration

## Confidence

| Claim | Confidence |
|-------|------------|
| HCOPE provides reliable safety guarantees when assumptions hold | High |
| DEPSUE framework is theoretically sound | Medium |
| Adaptive imitation regularization effectively handles novel actions | Medium |

## Next Checks

1. **HCOPE Reliability Test:** Apply HCOPE to estimate V(π₀) on held-out data from π₀ itself. Verify that the lower bound V̂⁻(π₀) ≈ V(π₀) with appropriate confidence (e.g., 95%). If the bound is too loose, investigate data splitting and clipping threshold optimization.

2. **Sample Size Sensitivity:** For DEPSUE, systematically vary the number of on-policy samples m_k per deployment (e.g., 100, 1000, 10000) and measure how safety margin estimates and subsequent exploration behavior change. Identify the minimum m_k required for reliable safety guarantees.

3. **Logging Policy Quality Impact:** Run experiments with varying logging policy quality (β parameter in synthetic data). Quantify how Safe OPG's exploration/novelty changes as the logging policy shifts from near-optimal to highly suboptimal, and assess whether the safety guarantees hold across this spectrum.