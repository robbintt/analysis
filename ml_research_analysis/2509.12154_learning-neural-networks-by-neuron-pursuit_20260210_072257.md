---
ver: rpa2
title: Learning Neural Networks by Neuron Pursuit
arxiv_id: '2509.12154'
source_url: https://arxiv.org/abs/2509.12154
tags:
- weights
- point
- gradient
- training
- saddle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the gradient flow dynamics of homogeneous neural
  networks near saddle points with a sparsity structure. It shows that when initialized
  near such saddle points, gradient flow remains close for a long time, during which
  weights with small magnitudes stay small and converge in direction, maintaining
  proportionality between incoming and outgoing weights of hidden neurons.
---

# Learning Neural Networks by Neuron Pursuit

## Quick Facts
- arXiv ID: 2509.12154
- Source URL: https://arxiv.org/abs/2509.12154
- Reference count: 40
- Key outcome: This paper analyzes gradient flow dynamics near saddle points and introduces Neuron Pursuit (NP), a greedy algorithm that incrementally adds neurons by maximizing a constrained neural correlation function, achieving better sample efficiency than standard gradient descent on sparse non-linear functions and algorithmic tasks.

## Executive Summary
This paper analyzes how gradient flow dynamics behave near saddle points in homogeneous neural networks with sparsity structures. The key insight is that when initialized near such saddle points, small weights align with optimal directions for reducing residual error before growing in magnitude, maintaining proportionality between incoming and outgoing weights. Building on this observation, the authors introduce Neuron Pursuit (NP), a greedy algorithm that incrementally adds neurons by maximizing a constrained neural correlation function. NP selects new neurons, initializes them with small weights aligned along dominant KKT points, and then minimizes training loss using gradient descent.

## Method Summary
The Neuron Pursuit algorithm works by iteratively selecting and adding new neurons to a neural network. At each iteration, it computes the residual error and uses projected gradient ascent to maximize the constrained Neural Correlation Function (NCF) on the unit sphere, finding the dominant KKT point that represents the optimal incoming/outgoing weights for a new neuron. The algorithm then adds this neuron with small weights (scaled by δ) aligned to the KKT point and performs a short burst of gradient descent to fit the residual. This process repeats until the residual error falls below a threshold or maximum iterations are reached. The method is specifically designed for homogeneous networks and leverages the theoretical insight that small weights near saddle points converge in direction before growing in magnitude.

## Key Results
- NP successfully learns sparse non-linear functions over various input distributions (hypersphere, hypercube, Gaussian) with better sample efficiency than standard gradient descent
- The algorithm achieves strong performance on algorithmic tasks like modular addition (p=59) and pointer value retrieval, often requiring fewer samples
- Theoretical analysis proves that gradient flow near saddle points exhibits "directional convergence" where small weights align with optimal directions for reducing residual error
- The sparsity structure is maintained through weight proportionality constraints between incoming and outgoing weights of hidden neurons

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Near specific saddle points with a sparsity structure, gradient flow exhibits "directional convergence" where small weights align with the optimal direction for reducing residual error before growing in magnitude.
- **Mechanism:** The paper proves (Theorem 7) that when initialized near a saddle point where a subset of weights is zero, the trajectory stays near the saddle for a long time ($O(1/\delta^{L-2})$). During this "plateau," the small weights ($w_z$) evolve to maximize the Neural Correlation Function (NCF) with the residual error ($y$), effectively identifying the most useful features to add next without immediately disrupting the existing structure.
- **Core assumption:** The network is $L$-homogeneous, and the saddle point satisfies Lojasiewicz's inequality (Assumption 2).
- **Evidence anchors:**
  - [abstract] "gradient flow remains close for a long time... weights with small magnitudes stay small and converge in direction"
  - [section 3] Theorem 7 details the directional convergence to a non-negative KKT point of the constrained NCF.
  - [corpus] The corpus on "Saddle-to-Saddle Dynamics" supports the broader hypothesis that training traverses these structures.
- **Break condition:** If the initial direction of small weights does not lie in the "stable set" of a dominant KKT point, the weights may converge to zero (become inactive) rather than aligning with a useful feature.

### Mechanism 2
- **Claim:** Incremental "Neuron Pursuit" (NP) acts as a greedy approximation of the gradient flow dynamics by explicitly optimizing the NCF to select new neurons.
- **Mechanism:** The NP algorithm bypasses the slow "plateau" phase of gradient descent by explicitly solving the constrained NCF maximization (using projected gradient ascent) to find the optimal incoming/outgoing weights for a new neuron. It initializes this neuron with small weights ($\delta$) aligned along this KKT point, mimicking the "directional convergence" phase, and then performs a short burst of gradient descent to fit the residual.
- **Core assumption:** Maximizing the constrained NCF yields a "positive" KKT point (Lemma 12), ensuring the new neuron strictly reduces the training loss.
- **Evidence anchors:**
  - [abstract] "At each iteration, NP maximizes an appropriate constrained neural correlation function... adds it with small weights aligned along the dominant KKT point"
  - [section 5] Algorithm 1 description.
- **Break condition:** If the NCF maximization step is trapped in a local optimum or yields a non-positive KKT point, the descent property is not guaranteed, and the algorithm may fail to reduce loss.

### Mechanism 3
- **Claim:** Enforcing a "proportionality" condition between incoming and outgoing weights preserves sparsity and stabilizes the learning process.
- **Mechanism:** Lemma 3 and 9 show that at the KKT points of the NCF, the norms of incoming and outgoing weights for a hidden neuron are proportional. The NP algorithm initializes neurons to respect this proportionality. This structural constraint prevents unbalanced growth (where one side grows while the other stays small) and ensures that "inactive" neurons (zero weights) remain inactive, maintaining the sparsity structure as the network grows.
- **Core assumption:** The activation function allows for such a decomposition and balance (e.g., homogeneous activations).
- **Evidence anchors:**
  - [section 3.3] Lemma 9 explicitly links KKT points to weight proportionality ($p\|W_{l+1}\|^2 = \|W_l\|^2$).
  - [section 5.2.2] Appendix D.6 discusses the necessity of re-balancing weights in the PVR task to prevent failure.
- **Break condition:** If weights become severely unbalanced (e.g., due to large step sizes in the gradient descent phase), the effective contribution of a neuron can vanish or explode, breaking the sparsity assumptions and potentially misleading the next iteration of neuron selection.

## Foundational Learning

- **Concept: Homogeneous Neural Networks**
  - **Why needed here:** The theoretical guarantees (Theorem 7) rely entirely on the property that $H(x; cw) = c^L H(x; w)$. This allows the analysis to decouple the "scale" ($\delta$) of initialization from the "direction" of convergence.
  - **Quick check question:** Can you explain why a function being $L$-homogeneous implies that $\nabla H(cw) = c^{L-1} \nabla H(w)$?

- **Concept: Neural Correlation Function (NCF)**
  - **Why needed here:** This is the objective function the algorithm maximizes to find new neurons. It measures the correlation between the network output (for a specific neuron configuration) and the residual error.
  - **Quick check question:** Why is maximizing $y^\top H(X; w)$ a sensible strategy for selecting a neuron to reduce the residual error $\|y - H(X; w)\|^2$?

- **Concept: Lojasiewicz Inequality**
  - **Why needed here:** The paper assumes this inequality (Assumption 2) holds near saddle points to bound the path length of gradient flow. It ensures that the weights $w_n$ don't drift too far from the saddle point while the small weights $w_z$ are converging directionally.
  - **Quick check question:** How does the Lojasiewicz inequality provide a guarantee on the convergence time or path length for gradient-based methods?

## Architecture Onboarding

- **Component map:** Input -> Residual Calculator -> NCF Optimizer -> Network Expander -> Loss Minimizer -> Iterator
- **Critical path:** The **NCF Optimizer** is the most critical component. If it fails to find the global maximum (or a strong local maximum) of the correlation function, the subsequent gradient descent will waste capacity fitting noise, and sample efficiency will drop.
- **Design tradeoffs:**
  - **NCF Optimization Depth:** More steps in the inner NCF ascent yield better neurons but increase compute cost per iteration.
  - **Initialization Scale ($\delta$):** Must be small enough to trigger the directional convergence dynamics but large enough to avoid numerical underflow.
  - **Gradient Descent Steps:** Too few steps might leave the network at a poor local minimum; too many might overfit the current residual before adding the next neuron.
- **Failure signatures:**
  - **Loss Plateau/Increase:** If adding a neuron increases loss, the NCF optimization likely found a non-positive KKT point (violation of Lemma 12 conditions).
  - **Unbalanced Weights:** If incoming and outgoing weights diverge in magnitude, the network is violating the theoretical sparsity conditions (check if re-balancing is needed, as per Appendix D.6).
  - **Slow Convergence:** If NCF values are near zero, the model capacity may be insufficient for the residual structure, or the activation function is mismatched (e.g., ReLU issues mentioned in Remark 2).
- **First 3 experiments:**
  1. **Sanity Check (Descent Property):** Implement the "Descent Property" test (Lemma 12). Verify that for a synthetic dataset, adding a neuron strictly decreases training loss before the main optimization loop.
  2. **Sparsity Visualization:** Train on a sparse function (e.g., $f_1(x)$ in Section 5.2.1) and visualize the weight matrices. Confirm that "inactive" neurons (those added but not needed) retain small norm weights ($\approx \delta$).
  3. **Ablation on Initialization:** Run the algorithm with "large" random initialization for new neurons (skipping the NCF step) vs. the proposed "small, aligned" initialization. Plot the difference in sample efficiency to quantify the benefit of the specific alignment mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the strict condition on Łojasiewicz's inequality exponent $\alpha \in (0, \frac{L}{2(L-1)})$ in Assumption 2 be validated for general feed-forward networks or relaxed?
- **Basis in paper:** [explicit] The paper states (Sec 3.2, Conclusion) that validating Assumption 2 more generally or relaxing it is an important direction, noting it is only shown for a simple instance in Appendix D.2.
- **Why unresolved:** The condition is stricter than standard PL-inequalities for $L > 2$, and the current proof relies on specific examples rather than general architecture properties.
- **What evidence would resolve it:** A theoretical proof establishing the inequality for generic feed-forward networks, or a modified theorem achieving directional convergence under relaxed smoothness/inequality assumptions.

### Open Question 2
- **Question:** Can the sparsity structure and saddle-to-saddle dynamics observed after escaping the first saddle point (Section 4) be formally established?
- **Basis in paper:** [explicit] The authors explicitly state in Section 4 and the Conclusion: "It is also important to formally establish the empirical observations in Section 4 about the dynamics of gradient descent after escaping the saddle points."
- **Why unresolved:** The paper provides rigorous analysis only for the dynamics *near* the saddle point (Theorem 7) and *after escaping the origin* (prior work), but the transition between subsequent saddles remains empirical observation.
- **What evidence would resolve it:** A theorem proving that weights escaping a saddle point $S_1$ preserve the sparsity structure until reaching the next saddle point $S_2$.

### Open Question 3
- **Question:** How can the performance gap between the Neuron Pursuit (NP) algorithm and standard Gradient Descent (GD) be theoretically closed?
- **Basis in paper:** [explicit] The Conclusion lists "Another direction is to close the gap between NP algorithm and gradient descent."
- **Why unresolved:** NP uses random initial weights to find KKT points, whereas GD dynamics depend on specific initial directions not characterized by Lemma 5, leading to different solutions (e.g., minimum $\ell_1$ vs. OMP-like solutions).
- **What evidence would resolve it:** Characterization of the initial direction of small weights at saddle points in GD, or a modification of the NP algorithm that aligns its KKT point selection with the implicit bias of GD.

### Open Question 4
- **Question:** How does the gradient flow dynamics near saddle points change when the analysis is extended from square loss to logistic loss?
- **Basis in paper:** [explicit] The Conclusion states: "Another is to extend our theoretical analysis from square loss to other loss functions such as logistic loss."
- **Why unresolved:** For logistic loss, saddle points may be located at infinity rather than finite points, rendering the local neighborhood analysis used in Theorem 7 inapplicable.
- **What evidence would resolve it:** A theoretical extension of Theorem 7 that accounts for saddle points at infinity or a transformation of the coordinate system to handle the unbounded nature of logistic loss dynamics.

## Limitations
- The theoretical analysis assumes homogeneous networks and square loss, limiting applicability to ReLU networks and other activation functions
- The sparsity structure assumption may not hold for complex real-world datasets requiring dense representations
- The algorithm requires careful hyperparameter tuning, particularly for initialization scale $\delta$ and NCF optimization steps
- Theoretical guarantees depend on Lojasiewicz's inequality, which is assumed but not empirically verified for the tasks studied

## Confidence
- **High confidence**: The descent property of NP (adding neurons strictly reduces loss) is mathematically proven (Lemma 12) and empirically validated across all experiments. The directional convergence phenomenon near saddle points (Theorem 7) is theoretically sound for homogeneous networks.
- **Medium confidence**: The sample efficiency improvements over standard gradient descent are demonstrated on synthetic and algorithmic tasks, but may not generalize to complex real-world datasets. The sparsity maintenance mechanism (weight proportionality) is theoretically justified but requires empirical verification.
- **Low confidence**: The theoretical guarantees for ReLU networks are limited (Remark 2) due to the non-smoothness and non-homogeneity at zero. The algorithm's performance on tasks requiring dense representations is not explored.

## Next Checks
1. **Lojasiewicz Constant Verification**: Measure the actual Lojasiewicz constant for the Modular Addition task to quantify how well the theoretical assumptions match practice. Plot the path length vs. theoretical bounds.
2. **Weight Proportionality Monitoring**: During training on PVR, log the ratio of incoming to outgoing weights for each neuron. Verify that this ratio stays bounded around the theoretical value from Lemma 9 throughout training.
3. **ReLU Robustness Test**: Run the PVR experiment with ReLU activation using the weight balancing algorithm from Appendix D.6. Compare sample efficiency against the baseline to quantify the cost of using non-homogeneous activations.