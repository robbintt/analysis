---
ver: rpa2
title: 'CLEAR-KGQA: Clarification-Enhanced Ambiguity Resolution for Knowledge Graph
  Question Answering'
arxiv_id: '2504.09665'
source_url: https://arxiv.org/abs/2504.09665
tags:
- ambiguity
- entity
- clarification
- dataset
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses ambiguity resolution in knowledge graph question
  answering (KGQA), where user queries often contain entity and intent ambiguities
  that existing methods struggle to handle. The proposed CLEAR-KGQA framework employs
  a Bayesian inference mechanism to quantify query ambiguity and guide large language
  models (LLMs) in requesting clarification from users within a multi-turn dialogue
  framework.
---

# CLEAR-KGQA: Clarification-Enhanced Ambiguity Resolution for Knowledge Graph Question Answering

## Quick Facts
- arXiv ID: 2504.09665
- Source URL: https://arxiv.org/abs/2504.09665
- Reference count: 31
- Key outcome: 83.00 F1 on WebQSP, 52.46 F1 on CWQ using clarification-enhanced KGQA

## Executive Summary
CLEAR-KGQA addresses ambiguity resolution in Knowledge Graph Question Answering by employing a Bayesian inference mechanism to quantify query ambiguity and guide large language models in requesting clarification from users within a multi-turn dialogue framework. The framework introduces an AskForClarification tool and a two-agent interaction framework with an LLM-based user simulator for iterative refinement. Experiments show significant improvements over state-of-the-art baselines, achieving 83.00 F1 on WebQSP and 52.46 F1 on CWQ. The authors also contribute a refined dataset of disambiguated queries derived from interaction histories to facilitate future research.

## Method Summary
CLEAR-KGQA employs a two-agent interaction framework where a QA agent generates actions (tool calls) and a user simulator generates clarification responses. The core innovation is a Bayesian inference mechanism that quantifies entity and intent ambiguity using perplexity-based scoring. When ambiguity scores exceed predefined thresholds (0.6 for entities, 0.8 for intents), the system injects clarification hints into observations to guide the LLM to invoke the AskForClarification tool. The framework uses four tools: SearchNodes, SearchGraphPattern, ExecuteSPARQL, and AskForClarification. Experiments are conducted on WebQSP and CWQ datasets using GPT-4o for the QA agent and Llama-3.1-8B-Instruct for fine-tuning in low-resource settings.

## Key Results
- Achieves 83.00 F1 score on WebQSP, outperforming Interactive-KBQA (71.07) and Ensemble-KBQA (77.02)
- Achieves 52.46 F1 score on CWQ, significantly higher than Interactive-KBQA (34.27) and Ensemble-KBQA (42.67)
- Ablation studies show the clarification plugin contributes 8-10 F1 points across different LLMs and datasets
- Unambiguous dataset derived from interaction histories shows 10-20 point F1 improvements over ambiguous questions

## Why This Works (Mechanism)

### Mechanism 1: Bayesian Inference for Ambiguity Quantification
- **Claim**: Bayesian inference quantifies ambiguity in entity linking and intent interpretation, enabling systematic detection of when clarification is needed.
- **Mechanism**: The plugin computes posterior probabilities P(e|Q) using entity popularity as prior and LLM perplexity on entity descriptions as likelihood. Entropy of the normalized posterior distribution measures uncertainty; scores above threshold (0.6 for entities, 0.8 for intents) trigger clarification hints injected into observations.
- **Core assumption**: Perplexity-based relevance scoring accurately captures semantic alignment between questions and entity/predicate descriptions.
- **Evidence anchors**: [abstract]: "employs a Bayesian inference mechanism to quantify query ambiguity and guide LLMs"; [section III.E]: Equations 5-10 formalize the scoring; Figure 3 shows score distributions differ between datasets.
- **Break condition**: If entity descriptions are sparse or uninformative, perplexity signals degrade; threshold tuning becomes dataset-specific.

### Mechanism 2: Clarification Hints in Observations
- **Claim**: Explicit clarification hints in observations guide LLMs to invoke AskForClarification at appropriate turns, improving action selection.
- **Mechanism**: When ambiguity scores exceed thresholds, the plugin appends "(Hint: The entity may be ambiguous.)" to tool observations. This structured prompt augmentation reduces reliance on implicit LLM judgment about when to clarify.
- **Core assumption**: LLMs follow injected hints reliably and do not over-use clarification when unnecessary.
- **Evidence anchors**: [section III.B]: "we design a plugin that quantifies entity and intent ambiguity to guide LLM's tool usage"; [section IV.I]: Ablation shows w/o plugin drops from 83.00 to 73.15 F1 (GPT-4o), 73.77 to 66.33 (FT open-LLM).
- **Break condition**: If hint format conflicts with LLM prompt style, instruction-following may degrade; requires prompt engineering per model.

### Mechanism 3: Two-Agent Interaction with User Simulator
- **Claim**: Two-agent interaction (QA agent + user simulator) enables iterative logical form refinement without real users during development.
- **Mechanism**: The dummy user agent conditions on golden SPARQL S' and clarification request a_t to generate realistic user response c_t (Eq. 4). This creates a feedback loop for the QA agent to refine queries.
- **Core assumption**: Simulator responses accurately reflect real user clarification behavior; golden SPARQL provides sufficient grounding.
- **Evidence anchors**: [abstract]: "two-agent interaction framework where an LLM-based user simulator enables iterative refinement"; [section III.D]: Eq. 4 formalizes simulator response generation.
- **Break condition**: If golden SPARQL is incorrect or incomplete (Table VI notes "Golden Incorrect" errors), simulator provides misleading feedback.

## Foundational Learning

- **Concept: Bayesian posterior entropy as uncertainty quantification**
  - Why needed here: The ambiguity scoring mechanism normalizes entropy by maximum entropy to produce comparable 0-1 scores across queries with different candidate counts.
  - Quick check question: Given entities {A, B, C} with posteriors {0.5, 0.3, 0.2}, compute the ambiguity score.

- **Concept: Multi-turn tool-use agents with observation-based feedback**
  - Why needed here: CLEAR-KGQA extends tool-based KGQA by adding clarification as a tool; understanding action-observation loops is prerequisite.
  - Quick check question: In Eq. 2-3, what is the difference between observation o_t and clarification response c_t in the history H?

- **Concept: Perplexity as proxy for conditional probability**
  - Why needed here: The mechanism uses PPL(Q, Desc(e_i))^-1 as P(Q|e_i); lower perplexity implies higher relevance.
  - Quick check question: If PPL(Q, Desc(e1)) = 10 and PPL(Q, Desc(e2)) = 20, which entity has higher conditional probability?

## Architecture Onboarding

- **Component map**: QA Agent (LLM) -> Ambiguity Scoring Plugin -> Tool Suite (SearchNodes, SearchGraphPattern, ExecuteSPARQL, AskForClarification) -> User Simulator (LLM)
- **Critical path**: Question → SearchNodes → Entity Ambiguity Score → (if >0.6) Hint → AskForClarification → User Response → SearchGraphPattern → Intent Ambiguity Score → (if >0.8) Hint → AskForClarification → ExecuteSPARQL
- **Design tradeoffs**: Lower thresholds = more clarifications, higher accuracy, but more user interaction turns (Figure 2). Entity threshold (0.6) lower than intent threshold (0.8) reflects observation that entity disambiguation is more critical for WebQSP.
- **Failure signatures**: Predicate Recognition errors (25 for GPT-4o): LLM fails to map NL to KG predicates. Constraint Missing (29): Temporal/comparative constraints not encoded in SPARQL. Golden Incorrect (10): Ground truth annotations questionable—validate before trusting simulator.
- **First 3 experiments**:
  1. Reproduce baseline Interactive-KBQA on WebQSP subset (100 samples); verify F1 ~71
  2. Add plugin with entity scoring only; measure F1 delta (expected ~8 points per ablation)
  3. Generate unambiguous questions from interaction history; run baseline on Un-Amb dataset to validate 10-20 point gain

## Open Questions the Paper Calls Out
- **Question**: How does CLEAR-KGQA perform with real human users compared to the LLM-based user simulator used in experiments?
- **Basis in paper**: [inferred] The paper employs a "Dummy User Agent" (ct = LLM({Promptu, S′, at})) to simulate user responses to clarification requests, but this simulator has access to the golden SPARQL query S′, which real users would not have.
- **Why unresolved**: No human evaluation was conducted; all experiments use simulated user feedback, which may be cleaner and more consistent than actual human responses.
- **What evidence would resolve it**: A user study comparing system performance with real human participants versus the LLM simulator, measuring F1 scores, clarification quality, and user satisfaction.

## Limitations
- The Bayesian scoring mechanism depends heavily on entity popularity data and perplexity calculations that are not fully validated in the paper.
- The effectiveness of injected clarification hints may be model-dependent, as prompt engineering for different LLMs shows varying results.
- The user simulator's ability to generate realistic clarification responses is assumed but not empirically validated against actual user interactions.

## Confidence
- **High**: The two-agent interaction framework is well-defined and reproducible; the F1 improvements over baselines are substantial and consistent across experiments.
- **Medium**: The Bayesian inference mechanism for ambiguity quantification is mathematically sound, but its practical effectiveness depends on data quality (entity descriptions, popularity metrics).
- **Low**: The claim that the AskForClarification tool significantly improves KGQA accuracy relies on simulated interactions rather than real user studies, making the ecological validity uncertain.

## Next Checks
1. Validate entity popularity data: Verify the source and quality of uei values used in Bayesian scoring; test sensitivity to different popularity distributions.
2. Test with real users: Replace the user simulator with actual human annotators for a subset of queries to measure the real-world effectiveness of clarification hints.
3. Cross-model prompt validation: Test the clarification hint injection mechanism with different LLMs (e.g., Claude, Gemini) to assess model dependency and generalization.