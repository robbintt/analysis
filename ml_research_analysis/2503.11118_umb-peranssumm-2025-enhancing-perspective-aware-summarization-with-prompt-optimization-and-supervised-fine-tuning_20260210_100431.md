---
ver: rpa2
title: 'UMB@PerAnsSumm 2025: Enhancing Perspective-Aware Summarization with Prompt
  Optimization and Supervised Fine-Tuning'
arxiv_id: '2503.11118'
source_url: https://arxiv.org/abs/2503.11118
tags:
- prompt
- perspective
- dspy
- optimization
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach for perspective-aware summarization
  in community question-answering (CQA) threads. The authors use ensemble learning
  combining BERT, RoBERTa, and DeBERTa models for perspective span identification,
  achieving 82.9% F1-score on the test set.
---

# UMB@PerAnsSumm 2025: Enhancing Perspective-Aware Summarization with Prompt Optimization and Supervised Fine-Tuning

## Quick Facts
- arXiv ID: 2503.11118
- Source URL: https://arxiv.org/abs/2503.11118
- Reference count: 9
- Primary result: Ensemble span identification (82.9% F1) + CoT prompting + DSPy optimization + SFT improves medical CQA summarization relevance (+21.4% ROUGE-L) and factuality metrics

## Executive Summary
This paper presents an approach for perspective-aware summarization in community question-answering (CQA) threads. The authors use ensemble learning combining BERT, RoBERTa, and DeBERTa models for perspective span identification, achieving 82.9% F1-score on the test set. For summarization, they design Chain-of-Thought (CoT) prompting strategies that incorporate keyphrases and guide information, then apply DSPy framework for prompt optimization and supervised fine-tuning (SFT) on Llama-3. Their experimental results show that CoT prompting with keyphrases and guidance improves summary alignment with references, while combining DSPy prompt optimization with SFT yields significant improvements in both relevance (ROUGE-L: +21.4%, Meteor: +7.0%, BLEU: +8.8%) and factuality metrics. The approach demonstrates the effectiveness of structured prompting combined with automated optimization and domain adaptation for generating perspective-aware summaries in medical CQA contexts.

## Method Summary
The approach combines ensemble span identification with perspective-aware summarization. For span identification, BERT, RoBERTa, and DeBERTa models are fine-tuned and their predictions averaged. For summarization, a four-step Chain-of-Thought prompting strategy extracts keyphrases, integrates them with guide information (tone, anchor text, perspective definitions), and generates summaries using Llama-3-8B-Instruct. DSPy's 0-shot MIPRO optimization iteratively refines prompts against a composite metric (equal weights of ROUGE-L, BLEU, Meteor, BERTScore), followed by LoRA-based supervised fine-tuning for two epochs.

## Key Results
- Ensemble span identification achieves 82.9% F1-score on test set, improving over individual models
- CoT prompting with keyphrases and guidance increases ROUGE-1 from 0.229 to 0.318
- DSPy optimization + SFT yields +25.6% improvement on validation set and +9.1% on test set
- Combined approach improves relevance metrics (ROUGE-L +21.4%, Meteor +7.0%, BLEU +8.8%) and factuality metrics

## Why This Works (Mechanism)

### Mechanism 1: Ensemble Prediction Averaging Reduces Single-Model Blind Spots
Averaging predictions from architecturally diverse transformers (BERT, RoBERTa, DeBERTa) yields more robust span boundaries than any single model. Each model encodes different inductive biases (BERT's bidirectional attention, RoBERTa's robust pretraining, DeBERTa's disentangled attention). Averaging probability distributions smooths over model-specific errors while reinforcing confident predictions shared across models.

### Mechanism 2: Chain-of-Thought with Anchored Guidance Constrains Generation Space
Structured CoT prompts with keyphrases, anchor text, and perspective-specific guidance reduce semantic drift and improve alignment with reference summaries. Four-step decomposition (keyphrase extraction → integration → guide information → generation) forces intermediate reasoning states. Anchor texts and tone instructions provide lexical and stylistic constraints that narrow the output distribution toward perspective-consistent language.

### Mechanism 3: DSPy Iterative Prompt Refinement Optimizes Multi-Metric Trade-offs
Automated prompt optimization via DSPy's 0-shot MIPRO improves relevance metrics by iteratively refining instructions against a composite metric, even without ground-truth prompts. MIPRO generates multiple prompt variants per iteration, evaluates them against a balanced composite metric, and uses Bayesian optimization to select candidates.

## Foundational Learning

- **Ensemble Probability Averaging**
  - Why needed here: Span identification requires precise token-level boundaries; single models have idiosyncratic weaknesses.
  - Quick check question: Can you explain why averaging probabilities (not hard predictions) preserves uncertainty information and enables smoother decision boundaries?

- **Chain-of-Thought Prompting**
  - Why needed here: Summarization with perspective constraints requires multi-step reasoning; direct prompting skips intermediate verification.
  - Quick check question: How does forcing a model to "extract keyphrases first" before summarization change the attention distribution over the input?

- **LoRA (Low-Rank Adaptation) for SFT**
  - Why needed here: Full fine-tuning of 8B+ parameter models is expensive; LoRA enables efficient domain adaptation with minimal trainable parameters.
  - Quick check question: What is the relationship between LoRA rank size and the model's ability to learn domain-specific vocabulary vs. general patterns?

## Architecture Onboarding

- **Component map:**
  Training Data → BERT + RoBERTa + DeBERTa → Ensemble Averaging → Predicted Span Texts
  ↓
  Question + Context + Spans → CoT Prompt Construction → Llama-3-8B-Instruct
  (keyphrases + guide info) ↓
  DSPy MIPRO Optimizer → Optimized Prompt → SFT with LoRA (2 epochs) → Final Summary Output

- **Critical path:** Span identification accuracy directly gates summarization quality (incorrect spans → missing or wrong perspective content). DSPy optimization + SFT must be applied sequentially: prompt optimization first identifies effective instruction patterns, then SFT locks domain knowledge into weights.

- **Design tradeoffs:**
  - Ensemble vs. single model: Ensemble improves robustness (+2.6% F1 over BERT baseline) but triples inference cost and latency.
  - Equal-weight composite metric vs. learned weights: Simpler but potentially suboptimal; no validation that 0.25 per metric is optimal.
  - Llama-3 vs. API models (GPT-4/Claude-3): Authors explicitly note Llama-3 underperforms these baselines, but offers data privacy and fine-tuning control.

- **Failure signatures:**
  - Span identification: Ensemble returns inconsistent boundaries across models (high variance in P_i(y|x)) → indicates correlated errors or insufficient training data.
  - DSPy optimization: Validation metrics improve but test metrics degrade → overfitting to prompt variations specific to validation set.
  - SFT: Factuality metrics (AlignScore, SummaC) do not improve or decline → model learns to mimic reference style without grounding in source content.

- **First 3 experiments:**
  1. Ablate ensemble components: Remove one model at a time (BERT, RoBERTa, DeBERTa) to quantify each's contribution to strict vs. proportional match F1.
  2. Metric sensitivity analysis: Test alternative weightings for DSPy's composite metric and compare validation vs. test generalization.
  3. Zero-shot cross-domain test: Apply trained pipeline to a non-medical CQA dataset (if available) with same perspective types. Measure degradation to assess overfitting to medical vocabulary and anchor texts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would benchmarking against proprietary models (GPT-4, Claude-3) reveal systematic performance gaps in perspective-aware summarization, and if so, which model capabilities account for the differences?
- Basis in paper: [explicit] The authors state: "We use Llama-3 as our LLM without benchmarking against API-based models such as GPT-4 or Claude-3. Compared with other teams' submissions, it indicates that Llama-3 underperforms relative to GPT-4 and Claude-3."
- Why unresolved: No comparative analysis was conducted; the claim about underperformance is based on external observations rather than controlled experiments.
- What evidence would resolve it: Controlled experiments comparing Llama-3, GPT-4, and Claude-3 on the same PerAnsSumm test set with identical prompting strategies.

### Open Question 2
- Question: Does the equal-weight composite metric (0.25 each for ROUGE-L, BLEU, Meteor, BERTScore) in DSPy optimization produce suboptimal prompts compared to learned or dynamically weighted metrics?
- Basis in paper: [explicit] The authors acknowledge: "This equal-weight approach may oversimplify the relationships between different evaluation metrics and potentially reduce accuracy."
- Why unresolved: The weight selection was based on an assumption of equal contribution, not empirical validation.
- What evidence would resolve it: Ablation studies comparing equal-weight optimization against gradient-based weight learning or human preference alignment.

### Open Question 3
- Question: To what extent do the CoT prompting strategies and DSPy-optimized prompts transfer to perspective-aware summarization in non-medical CQA domains?
- Basis in paper: [explicit] The authors note: "The generalizability of our prompt optimization strategy also remains an open question" and "our prompt design is tailored to the medical CQA."
- Why unresolved: All experiments were conducted on medical forum data; no cross-domain evaluation was performed.
- What evidence would resolve it: Evaluation on CQA datasets from other domains (e.g., legal, technical support) using the same prompting pipeline without modification.

## Limitations
- The ensemble prediction averaging shows only partial complementarity (ensemble F1 sits between individual model performances).
- Equal-weight composite metric in DSPy optimization lacks empirical validation and may oversimplify metric relationships.
- Claims about cross-domain generalization and performance gaps with proprietary models are speculative due to limited testing.

## Confidence
- **High Confidence**: Ensemble averaging improves span identification robustness; CoT prompting with keyphrases improves relevance metrics; DSPy optimization shows measurable validation improvements.
- **Medium Confidence**: DSPy + SFT significantly improves factuality metrics; effectiveness of fixed anchor texts for perspective alignment is demonstrated but not tested across domains.
- **Low Confidence**: Claims about cross-domain generalization and the optimality of equal-weight metric combinations are speculative.

## Next Checks
1. **Ensemble Ablation Study**: Systematically remove each model (BERT, RoBERTa, DeBERTa) from the ensemble and measure changes in strict vs. proportional match F1.
2. **Metric Weight Sensitivity Analysis**: Test alternative weightings for DSPy's composite metric and compare validation vs. test generalization.
3. **Zero-Shot Cross-Domain Transfer**: Apply the trained pipeline to a non-medical CQA dataset with the same perspective types. Measure performance degradation to assess overfitting.