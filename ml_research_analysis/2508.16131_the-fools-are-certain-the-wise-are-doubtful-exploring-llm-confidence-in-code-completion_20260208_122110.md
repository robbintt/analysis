---
ver: rpa2
title: 'The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in
  Code Completion'
arxiv_id: '2508.16131'
source_url: https://arxiv.org/abs/2508.16131
tags:
- code
- perplexity
- language
- https
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores how Large Language Models (LLMs) generate code
  across different programming languages by measuring model confidence through perplexity.
  We find that strongly-typed languages exhibit lower perplexity than dynamically-typed
  ones, and that perplexity depends more on the LLM model than on the code dataset
  used for evaluation.
---

# The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion

## Quick Facts
- **arXiv ID:** 2508.16131
- **Source URL:** https://arxiv.org/abs/2508.16131
- **Reference count:** 24
- **Primary result:** Perplexity depends more on LLM model than code dataset; strongly-typed languages show lower perplexity than dynamically-typed ones.

## Executive Summary
This study investigates Large Language Models' confidence in code generation across 14 programming languages by measuring perplexity. The research reveals that strongly-typed languages exhibit lower perplexity than dynamically-typed ones, and that perplexity rankings are primarily determined by the model family rather than the evaluation dataset. Comments consistently increase perplexity across languages, though they don't affect language rankings. Perl shows consistently high perplexity while Java shows low perplexity. These findings provide practical guidance for developers selecting programming languages and models for code generation tasks, suggesting that language choice and model selection significantly impact LLM performance.

## Method Summary
The study measures code perplexity using LLaMA 3.2 1B across 14 programming languages with 72 GPL-licensed source files per language (1,008 total files). Files were preprocessed to remove boilerplate headers and filtered to ensure sufficient token count. Perplexity was calculated using a sliding window approach with context size 64 and stride 1 through the llama.cpp implementation. The analysis compared perplexity across languages, examined the impact of comments, and tested consistency across different model families to determine whether perplexity depends on the model or dataset.

## Key Results
- Perplexity depends more on LLM model family than on the code dataset used for evaluation
- Strongly-typed languages exhibit lower perplexity than dynamically-typed languages
- Comments consistently increase perplexity, though they don't affect language ranking

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Strongly-typed programming languages yield lower perplexity (higher model confidence) than dynamically-typed languages.
- **Mechanism:** Static type systems constrain the vocabulary of valid subsequent tokens (e.g., defining specific object methods or variable types). This reduces the entropy of the probability distribution for the next token, lowering the calculated perplexity.
- **Core assumption:** The model has learned to leverage type signatures as predictive signals during pre-training.
- **Evidence anchors:**
  - [abstract] "strongly-typed languages exhibit lower perplexity than dynamically-typed ones"
  - [section 4.2] "Strongly-typed languages have lower perplexity than scripting languages."
  - [corpus] Corpus signals regarding "Fairness or Fluency" suggest metric sensitivity, but do not directly contradict typing constraints; direct evidence is primarily internal to the paper.
- **Break condition:** If the code relies heavily on type inference or reflection that obscures types from the static context window.

### Mechanism 2
- **Claim:** The inclusion of natural language comments in source code increases perplexity.
- **Mechanism:** Natural language (comments) possesses higher entropy and variability compared to the structured syntax and limited vocabulary of programming languages. When the model encounters comments, the probability distribution flattens, increasing the negative log-likelihood.
- **Core assumption:** Comments adhere to natural language statistics rather than code syntax statistics.
- **Evidence anchors:**
  - [abstract] "comments often increase perplexity"
  - [section 4.2] "language ranking based on perplexity is barely affected by [comments]... comments appear to increase perplexity in most languages."
  - [corpus] "How do Humans and LLMs Process Confusing Code?" relates to code comprehension but does not provide direct perplexity statistics for comments.
- **Break condition:** If comments are strictly boilerplate, auto-generated, or follow a repetitive formulaic pattern indistinguishable from code.

### Mechanism 3
- **Claim:** Perplexity rankings are dependent on the model family but independent of the evaluation dataset.
- **Mechanism:** The internal probability distribution is a function of the model's architecture and training data. Architecturally similar models (e.g., LLaMA family) learn similar statistical representations of code, resulting in consistent language rankings regardless of the specific files used for evaluation.
- **Core assumption:** The evaluation datasets are sufficiently large and diverse to average out file-specific anomalies.
- **Evidence anchors:**
  - [abstract] "perplexity depends more on the LLM model than on the code dataset"
  - [section 4.4] "Perplexity is independent of the code dataset used for its evaluation."
  - [section 4.3] Fig. 8 shows high correlation within model families (e.g., LLaMA 3 variants).
- **Break condition:** If the evaluation dataset contains out-of-distribution code (e.g., a new language unseen during training) or is too small to be statistically significant.

## Foundational Learning

- **Concept: Perplexity (PPL)**
  - **Why needed here:** This is the primary metric used to proxy "confidence" or "certainty" throughout the paper.
  - **Quick check question:** If a model assigns a probability of 1.0 to every token in a sequence, what is the resulting perplexity? (Answer: 1.0).

- **Concept: Sliding Window Attention**
  - **Why needed here:** The paper utilizes a sliding window strategy (stride 1) to calculate perplexity to approximate true sequence probability better than disjoint chunks.
  - **Quick check question:** Why is a stride of 1 preferred over a stride equal to context size for perplexity calculation? (Answer: It ensures every token is scored using the maximum available preceding context).

- **Concept: Static vs. Dynamic Typing**
  - **Why needed here:** Explains the variance in model performance (low vs. high perplexity) across languages like Java versus Perl.
  - **Quick check question:** Does a variable declaration in a dynamically typed language provide more or less constraint on the next token compared to a statically typed language? (Answer: Less constraint).

## Architecture Onboarding

- **Component map:** GitHub repositories (GPL licensed, 657 projects) -> Filtered for quality (stars/forks) -> Deduplicated -> `chardet` (encoding) -> `pygments` (comment stripping/tokenizing) -> `llama.cpp` (C++ implementation) -> GPU (cuBLAS) -> LLaMA 3.2 1B (and others) -> Strided Perplexity Calculation (Context 64, Stride 1)

- **Critical path:**
  1. Clone projects and filter files to ensure `tokens >= 3 * ctx_size`
  2. Clean files (remove boilerplate headers)
  3. Execute `llama.cpp` perplexity command with `ctx_size=64` and `ppl_stride=1`
  4. Aggregate median perplexity per language

- **Design tradeoffs:**
  - **Context Size (64):** A small context (64) was chosen to accommodate languages with smaller file sizes (Ruby), sacrificing longer-range dependency modeling for cross-language comparability
  - **Stride 1 vs. Stride N:** Stride 1 provides the most accurate approximation of sequence probability but is computationally more expensive than non-strided or larger-stride methods
  - **Model Size (1B):** Using a smaller model (LLaMA 3.2 1b) allows execution on conventional GPUs but may reflect different capabilities than SOTA large models

- **Failure signatures:**
  - **Insufficient Token Count:** Files with `< context_size * 3` tokens must be excluded to prevent calculation errors
  - **Memory Leakage:** The paper notes Hugging Face implementations leaked memory, necessitating the switch to `llama.cpp`
  - **Training Leakage:** Using permissive licenses (MIT/Apache) for evaluation might overlap with pre-training data; strictly using GPL licenses mitigates this

- **First 3 experiments:**
  1. **Baseline Verification:** Run the perplexity tool on the provided dataset for a high-confidence language (Java) and a low-confidence one (Perl) to verify the ranking matches Figure 2
  2. **Comment Ablation:** Select a sample of Python files, compute perplexity, strip comments, and re-compute to quantify the "comment penalty" suggested in Finding 3
  3. **Model Comparison:** Swap the model weights in `llama.cpp` (e.g., switch from LLaMA 3.2 to Mistral 7b) and check if the genealogical correlation holds for the C# language

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does code perplexity quantitatively correlate with downstream functional correctness metrics (e.g., Pass@k)?
- **Basis in paper:** [explicit] The authors explicitly list as a supplementary question: "how code perplexity correlates with LLMs' downstream performance metrics."
- **Why unresolved:** The study focused exclusively on intrinsic metrics (perplexity) as a proxy for correctness but did not execute the generated code to verify functional validity.
- **What evidence would resolve it:** A joint empirical study that measures both perplexity and functional correctness (e.g., unit test pass rates) on the same set of generated code samples.

### Open Question 2
- **Question:** To what extent do vocabulary size and file length influence code perplexity?
- **Basis in paper:** [explicit] The conclusion states: "Future research could investigate further the impact of vocabulary and file size on perplexity."
- **Why unresolved:** The current study found no correlation, but the authors attribute this null result to a restricted sample size limiting the statistical power of the analysis.
- **What evidence would resolve it:** A replication of the analysis using a significantly larger dataset of source code files to detect subtle relationships between code size/vocabulary and model confidence.

### Open Question 3
- **Question:** Does calibrating perplexity scores improve their reliability as indicators of code correctness?
- **Basis in paper:** [inferred] The limitations section notes the authors "did not calibrate the perplexity metric before using it as confidence indicator," despite "confidence metrics can benefit from calibration."
- **Why unresolved:** It remains unclear if raw perplexity values accurately map to probability estimates of correctness without applying calibration techniques like Platt scaling.
- **What evidence would resolve it:** Experiments comparing raw perplexity against calibrated confidence scores to see which better predicts the actual error rate or hallucination risk in generated code.

## Limitations
- The study uses a small context window (64 tokens) that may not capture long-range dependencies in code
- The exclusive focus on GPL-licensed repositories may introduce selection bias
- Perplexity provides only a proxy for confidence without establishing ground truth correctness
- The study doesn't account for different comment patterns that might affect perplexity measurements

## Confidence
- **High Confidence:** Perplexity rankings are consistent within model families (Mechanism 3)
- **Medium Confidence:** Strongly-typed languages show lower perplexity than dynamically-typed ones (Mechanism 1)
- **Low Confidence:** Comments universally increase perplexity (Mechanism 2)

## Next Checks
1. **Context Size Sensitivity Analysis:** Re-run the perplexity calculations with multiple context sizes (32, 128, 256) to determine whether the typing system advantage persists at scales that capture more program structure
2. **Comment Type Categorization:** Perform an ablation study separating different comment categories (documentation, licensing, inline notes, TODOs) to quantify which comment types contribute most to perplexity increases
3. **Cross-Architecture Consistency:** Test the typing hypothesis using architecturally distinct models (transformer vs. state-space models) to determine whether the typing advantage is a universal property or specific to the LLaMA family's training patterns