---
ver: rpa2
title: 'The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free
  ADMM'
arxiv_id: '2510.01650'
source_url: https://arxiv.org/abs/2510.01650
tags:
- sparsity
- elsa
- perplexity
- preprint
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of pruning large language models
  (LLMs) to extreme sparsity levels (up to 90%) without significant loss in performance,
  a problem that has seen diminishing returns in recent years. Existing methods relying
  on layer-wise reconstruction error minimization fail beyond moderate sparsity (50-60%)
  due to compounding errors and reliance on surrogate objectives.
---

# The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM
## Quick Facts
- **arXiv ID:** 2510.01650
- **Source URL:** https://arxiv.org/abs/2510.01650
- **Reference count:** 40
- **Primary result:** Achieves 90% sparsity in LLMs with 7.8x lower perplexity than existing methods while maintaining near-dense performance at 80% sparsity

## Executive Summary
This work addresses the challenge of pruning large language models (LLMs) to extreme sparsity levels (up to 90%) without significant loss in performance, a problem that has seen diminishing returns in recent years. Existing methods relying on layer-wise reconstruction error minimization fail beyond moderate sparsity (50-60%) due to compounding errors and reliance on surrogate objectives. The proposed method, ELSA (Extreme LLM Sparsity via Surrogate-free ADMM), directly solves a sparsity-constrained optimization problem using ADMM, avoiding surrogates and layer-wise restrictions. ELSA incorporates objective-aware projection using Hessian information and scales to large models via low-precision auxiliary states (ELSA-L). Extensive experiments across models from 125M to 27B parameters show that ELSA achieves up to 7.8x lower perplexity than the best existing method at 90% sparsity (e.g., 27.84 vs. >100 for others on LLaMA-2-13B), maintains near-dense performance at 80% sparsity, and sets new Pareto frontiers in the perplexity vs. parameter count space. Zero-shot accuracy also remains competitive, with 6% higher average accuracy than alternatives at 90% sparsity. These results demonstrate that the "sparsity wall" is not an inherent limitation but an artifact of problem formulation, opening new directions for extreme LLM pruning.

## Method Summary
ELSA tackles LLM pruning by reformulating it as a sparsity-constrained optimization problem solved via Alternating Direction Method of Multipliers (ADMM). Unlike existing approaches that minimize reconstruction error as a proxy for maintaining model quality, ELSA directly optimizes the actual task objective while enforcing sparsity constraints. The method introduces objective-aware projection using Hessian information to guide the pruning process toward parameters that matter most for the target task. To scale to large models, ELSA-L employs low-precision auxiliary states that reduce memory overhead while preserving optimization quality. The approach removes layer-wise restrictions common in prior work, allowing the pruning process to consider the model holistically and avoid the compounding errors that plague reconstruction-based methods at extreme sparsity levels.

## Key Results
- ELSA achieves 90% sparsity with 7.8x lower perplexity than the best existing method (27.84 vs. >100 for others on LLaMA-2-13B)
- Maintains near-dense performance at 80% sparsity across all tested models
- Sets new Pareto frontiers in perplexity vs. parameter count space, demonstrating superior compression efficiency

## Why This Works (Mechanism)
ELSA succeeds by directly addressing the fundamental limitation of reconstruction-based pruning methods: they optimize for a surrogate objective (reconstruction error) rather than the actual task performance. By formulating pruning as a sparsity-constrained optimization problem solved via ADMM, ELSA ensures that the retained parameters are those most critical for the target task. The objective-aware projection using Hessian information provides gradient information about which parameters contribute most to task performance, guiding the pruning process more effectively than reconstruction error. The removal of layer-wise restrictions allows the method to make globally optimal decisions rather than being constrained by local considerations that can compound into significant performance degradation at extreme sparsity levels.

## Foundational Learning
- **ADMM (Alternating Direction Method of Multipliers)**: A powerful optimization framework that splits complex problems into simpler subproblems, needed to handle the non-differentiable sparsity constraints while maintaining convergence properties. Quick check: Verify convergence criteria and parameter updates in the ADMM iterations.
- **Hessian-based projection**: Uses second-order information to identify parameters with the greatest impact on task performance, providing more accurate guidance than first-order methods. Quick check: Confirm that Hessian computations are tractable and correctly implemented for the target models.
- **Surrogate-free optimization**: Directly optimizes the actual task objective rather than reconstruction error, avoiding the misalignment between proxy objectives and real performance. Quick check: Compare optimization trajectories between surrogate-free and reconstruction-based approaches.
- **Low-precision auxiliary states**: Employs reduced-precision representations for auxiliary variables in ADMM to scale to large models while preserving optimization quality. Quick check: Validate that precision reduction doesn't introduce significant approximation errors.
- **Sparsity-constrained optimization**: Formulates pruning as a constrained optimization problem rather than a heuristic search, providing theoretical guarantees and better optimization behavior. Quick check: Verify constraint satisfaction throughout the optimization process.
- **Pareto efficiency in model compression**: Demonstrates superior trade-offs between model size and performance, establishing new benchmarks for efficient LLM deployment. Quick check: Plot and analyze the Pareto frontier across different sparsity levels.

## Architecture Onboarding
- **Component map**: Input model -> ADMM optimization loop -> Hessian projection -> Auxiliary state update -> Output pruned model
- **Critical path**: The ADMM iterations where the sparsity constraints are enforced and the objective-aware projection is applied represent the core of the method, as these steps directly determine which parameters are retained
- **Design tradeoffs**: ELSA trades computational complexity during optimization for superior final model quality, requiring multiple ADMM iterations but producing models that outperform alternatives at extreme sparsity levels
- **Failure signatures**: If Hessian computations become unstable or if ADMM convergence is slow, the method may revert to performance levels similar to reconstruction-based approaches or fail to achieve extreme sparsity without degradation
- **First experiments**: 1) Verify ADMM convergence on a small model at moderate sparsity levels, 2) Compare perplexity between ELSA and reconstruction-based methods at 70% sparsity, 3) Test Hessian projection stability across different model architectures

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions, focusing instead on demonstrating the effectiveness of the proposed approach and establishing new benchmarks for extreme LLM pruning.

## Limitations
- Computational overhead of ADMM optimization not fully characterized in terms of wall-clock time or resource requirements
- Hessian-based projection may become prohibitively expensive for larger models or more complex architectures
- Method requires careful hyperparameter tuning for ADMM parameters and projection settings

## Confidence
- **Pruning effectiveness at extreme sparsity**: High - Extensive experiments across multiple model sizes demonstrate consistent superiority
- **Computational scalability**: Medium - While ELSA-L addresses memory concerns, full computational profiling is limited
- **Generalizability to diverse architectures**: Medium - Results primarily focus on decoder-only transformer models
- **Long-term stability of pruned models**: Low - Limited analysis of model behavior over extended training or fine-tuning periods

## Next Checks
1. Full computational profiling of ELSA including training time, memory usage, and scalability to models larger than 27B parameters
2. Ablation studies isolating the contributions of each ELSA component (ADMM formulation, Hessian projection, low-precision states) to verify their individual impact
3. Extensive testing across diverse LLM architectures including decoder-only, encoder-decoder, and multimodal models to assess method robustness