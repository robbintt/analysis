---
ver: rpa2
title: 'RAC: Retrieval-Augmented Clarification for Faithful Conversational Search'
arxiv_id: '2601.11722'
source_url: https://arxiv.org/abs/2601.11722
tags:
- questions
- clarifying
- generation
- clarification
- passages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating clarifying questions
  in conversational search that are grounded in the underlying document corpus. Prior
  approaches often produced fluent but ungrounded questions that could mislead users.
---

# RAC: Retrieval-Augmented Clarification for Faithful Conversational Search

## Quick Facts
- **arXiv ID:** 2601.11722
- **Source URL:** https://arxiv.org/abs/2601.11722
- **Reference count:** 34
- **Primary result:** RAC significantly improves faithful clarifying questions in conversational search, outperforming baselines in both automatic metrics and faithfulness assessments.

## Executive Summary
This paper addresses the problem of generating clarifying questions in conversational search that are grounded in the underlying document corpus. Prior approaches often produced fluent but ungrounded questions that could mislead users. The authors introduce RAC (Retrieval-Augmented Clarification), a framework that first retrieves relevant passages for an ambiguous query and then generates clarifying questions conditioned on both the query and these passages. To improve faithfulness, they fine-tune a large language model using supervised learning and apply contrastive preference optimization to favor questions supported by retrieved passages over hallucinated ones. Evaluated on four benchmarks, RAC significantly outperforms baselines in both automatic metrics and faithfulness assessments.

## Method Summary
RAC uses a two-stage pipeline with LLaMA-3.1-8B-base. First, a retriever (BM25) identifies top-k passages for each query. An SFT model is fine-tuned on (query, passages, gold question) tuples. A second ungrounded model is trained on (query, gold question) only. Negative questions are generated via mixture decoding between the grounded and ungrounded models. Finally, a DPO model is trained with contrastive pairs using a joint loss combining preference and supervised objectives. The approach is evaluated on four benchmarks with reference-based metrics, faithfulness scores, and LLM-as-Judge evaluation.

## Key Results
- RAC significantly outperforms baselines on ROUGE-L, BLEU, METEOR, and BERTScore metrics across four benchmarks
- RAC improves faithfulness scores (AlignScore, PARENT Recall) by 2-3 points over strong baselines
- LLM-as-Judge evaluations confirm RAC consistently produces more corpus-grounded clarifications than alternatives
- BM25 retrieval outperforms dense retrieval in this domain, likely due to domain mismatch with MS MARCO training

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Conditioned Grounding
Conditioning question generation on retrieved passages constrains output space to corpus-supported content. Retrieved passages expose corpus-attested entities, relations, and facets. The generation model attends to this context, reducing reliance on parametric memory that may contain hallucinated or outdated information. Core assumption: Retrieved passages contain sufficient signal to disambiguate the query; retriever quality is adequate.

### Mechanism 2: Contrastive Preference Optimization for Faithfulness
Training with faithful/unfaithful question pairs shifts model preference toward corpus-grounded outputs. DPO objective explicitly penalizes ungrounded generations by learning to rank faithful questions higher. The contrastive signal is stronger than SFT alone because it provides explicit negative examples. Core assumption: Negative examples are plausible but clearly unfaithful; pairwise comparison captures faithfulness distinction.

### Mechanism 3: Tailored Negative Generation via Grounded/Ungrounded Model Mixture
Mixing a passage-conditioned model with a query-only model produces fluent but unfaithful negatives better suited for preference learning. p_θ0 (passage-conditioned) produces mostly faithful outputs; p_uncond (query-only) produces fluent but ungrounded outputs. Interpolating with α controls faithfulness degradation, yielding challenging negatives that resemble valid questions but lack corpus support. Core assumption: p_uncond learns question form from training data but lacks grounding signal; mixture preserves fluency while introducing hallucinations.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: RAC extends RAG from answer generation to clarification generation; understanding how retrieval conditions generation is prerequisite.
  - Quick check question: Given a query and 5 retrieved passages, what happens to generation if all passages are irrelevant?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: Core alignment mechanism; requires understanding pairwise preference learning without explicit reward model.
  - Quick check question: In DPO, what does the β parameter control, and what happens if β is set too high?

- **Concept: NLI-based Faithfulness Evaluation**
  - Why needed here: Paper adapts AlignScore (NLI-based) for evaluating question groundedness; requires understanding entailment as faithfulness proxy.
  - Quick check question: Why must clarifying questions be converted to declarative statements before NLI evaluation?

## Architecture Onboarding

- **Component map:** Retriever (BM25) -> SFT Model (p_θ0) -> Ungrounded Model (p_uncond) -> Negative Generator -> DPO Trainer -> Evaluation
- **Critical path:** 1) Index corpus -> retrieve top-k passages per query (k=4-5 optimal) 2) Train p_θ0 via SFT on passage-conditioned data 3) Train p_uncond via SFT on query-only data 4) Generate negative questions via mixture decoding (α=0.7) 5) Train final model via DPO with joint loss (γ=0.5)
- **Design tradeoffs:** BM25 vs. dense retrieval: BM25 outperformed TCT-ColBERT in this setting—likely due to domain mismatch. Test both; don't assume dense is better. Number of passages: 4-5 passages optimal; more adds noise without benefit. Start with k=5. Negative generation source: p_uncond > p_LM for negatives. Task-specific ungrounded model produces more challenging contrasts than generic base model.
- **Failure signatures:** Low faithfulness scores (AlignScore <50) despite high reference metrics → model ignoring passages, likely undertrained DPO or poor negative quality. Random-passage performance matches Q-Cond baseline → retriever not providing useful signal. DPO degrades SFT performance → α too high (negatives too noisy) or β too high (over-regularization).
- **First 3 experiments:** 1) Retrieval ablation: Compare BM25, dense retriever, and random passages on validation set; confirm BM25 advantage and establish passage-quality dependency. 2) Passage count sweep: Test k ∈ {1,2,3,4,5,10} on ROUGE-L and AlignScore; verify saturation at ~4 passages. 3) Negative generation comparison: Train DPO with p_uncond negatives vs. p_LM negatives; measure faithfulness gap to confirm Table 3 finding.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does RAC extend to multi-turn clarification scenarios, and what mechanisms are needed to maintain coherence across multiple clarifying exchanges? Basis: "As future work, we plan to extend this task to multi-turn clarification."
- **Open Question 2:** What is the impact of RAC-generated clarifying questions on downstream retrieval performance compared to baseline clarification methods? Basis: "As future work, we plan to... evaluate its impact on downstream retrieval performance."
- **Open Question 3:** How can retrieval strategies be adapted to reduce domain mismatch when applying RAC to corpora with different structures than MS MARCO? Basis: BM25's advantage is likely due to domain mismatch with dense retriever trained on MS MARCO.
- **Open Question 4:** How do RAC-generated questions compare to human-authored clarifications in actual user studies measuring satisfaction and task completion? Basis: Paper uses LLM-as-Judge but conducts no human evaluation.

## Limitations

- The approach depends heavily on retriever quality; poor retrieval leads to ungrounded questions regardless of generation model quality
- The framework is limited to single-turn clarification scenarios and doesn't address multi-turn conversational coherence
- No human evaluation was conducted to validate LLM-as-Judge results or measure user satisfaction with generated clarifications

## Confidence

- **High confidence:** Retrieval-conditioned generation improves grounding vs. query-only baselines; DPO yields measurable faithfulness gains over SFT alone; BM25 outperforms dense retrieval in this domain
- **Medium confidence:** The specific α=0.7 mixture ratio for negative generation is optimal; 4-5 passages are the sweet spot for signal-to-noise ratio; the joint loss (γ=0.5) is the best balance between faithfulness and fluency
- **Low confidence:** The paper does not report statistical significance tests on performance differences; the LLM-as-Judge evaluation protocol is not fully specified; the long-term stability of contrastive preference optimization on clarification tasks is untested

## Next Checks

1. **Retrieval quality dependency test:** Remove the retrieval step and feed random passages to the model. If performance matches Q-Cond, the retriever is the bottleneck—not the generation model.
2. **Negative quality ablation:** Compare DPO-trained models using p_uncond negatives vs. p_LM negatives vs. synthetic unfaithful questions (e.g., question inversion). Measure faithfulness gap to confirm that task-specific noise is critical.
3. **Cross-domain generalization:** Evaluate RAC on a domain-shifted corpus (e.g., medical or legal) where the retriever is BM25 over PubMed or statutes. If faithfulness drops sharply, the approach is tightly coupled to the original corpus domain.