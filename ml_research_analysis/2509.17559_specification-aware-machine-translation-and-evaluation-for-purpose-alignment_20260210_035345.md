---
ver: rpa2
title: Specification-Aware Machine Translation and Evaluation for Purpose Alignment
arxiv_id: '2509.17559'
source_url: https://arxiv.org/abs/2509.17559
tags:
- translation
- evaluation
- specifications
- error
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces specification-aware machine translation\
  \ (MT), arguing that professional translation quality depends on fulfilling communicative\
  \ goals, not just linguistic equivalence. It presents a framework that integrates\
  \ translation specifications\u2014such as purpose, audience, style, and terminology\u2014\
  into MT workflows and evaluation, drawing on functionalist translation theory and\
  \ standards like ISO 5060 and MQM."
---

# Specification-Aware Machine Translation and Evaluation for Purpose Alignment

## Quick Facts
- arXiv ID: 2509.17559
- Source URL: https://arxiv.org/abs/2509.17559
- Authors: Yoko Kayano; Saku Sugawara
- Reference count: 40
- Primary result: Specification-guided LLM translations outperform official human translations and Google Translate in human evaluations

## Executive Summary
This paper introduces specification-aware machine translation (MT), arguing that professional translation quality depends on fulfilling communicative goals, not just linguistic equivalence. It presents a framework that integrates translation specifications—such as purpose, audience, style, and terminology—into MT workflows and evaluation, drawing on functionalist translation theory and standards like ISO 5060 and MQM. In a case study translating investor relations texts from 33 Japanese companies, the authors compare five translation methods: official human translations, Google Translate, and three prompt-based large language model (LLM) outputs (basic, specification-guided, and post-edited). Human evaluations (expert error analysis and user preference rankings) show that specification-aware LLM translations consistently outperform official human translations and Google Translate. Automatic metrics like COMETKiwi, however, favor Google Translate, highlighting a gap between functional adequacy and surface-level fluency. The findings demonstrate that integrating specifications into MT workflows—with human oversight—can improve translation quality in professional settings, aligning outputs with real-world communicative needs.

## Method Summary
The study examines Japanese-to-English translation of corporate philosophy sections from integrated reports of 33 publicly listed Japanese companies, prioritized by market cap. Five translation methods were compared: official human translations, Google Translate, ChatGPT-4o with basic prompt, ChatGPT-4o with specification-guided prompt, and ChatGPT-4o post-editing Google Translate output with specifications. Specifications included purpose (appeal to investors), target audience (international investors), and style (clear, persuasive). Evaluation employed three methods: MQM-based error analysis with weighted scores (Accuracy 0.7, Linguistic Conventions 0.8, Style 1.5), subjective user preference rankings by 18 native English speakers with finance/accounting backgrounds, and COMETKiwi automatic scores. Error evaluation used MQM categories, and ranking evaluation hid translation methods from participants.

## Key Results
- Specification-guided ChatGPT translations consistently outperformed official human translations in both expert error analysis and user preference rankings
- COMETKiwi scores favored Google Translate (0.830) over specification-guided LLM outputs, revealing misalignment with human judgment
- ChatGPT + Spec and ChatGPT PE + Spec received the lowest weighted error scores in MQM analysis
- Specification-aware evaluation better captured functional adequacy than automatic metrics optimized for literal fidelity

## Why This Works (Mechanism)

### Mechanism 1: Specification-Guided Constraint of Translation Decision Space
- Claim: Explicit specifications reduce translator uncertainty by constraining valid output choices, improving alignment with communicative goals
- Mechanism: Specifications act as a structured schema that limits the hypothesis space for translation decisions, reducing reliance on implicit assumptions
- Core assumption: Translation quality is context-dependent; without explicit guidance, translators default to generic strategies that may not serve communicative goals
- Evidence anchors: Abstract notes specifications often treated implicitly; Section 3.3 discusses difficulty starting translation without specifications; related work suggests theory-informed prompts affect LLM outputs

### Mechanism 2: Prompt-Based LLMs as Functionalist Translation Agents
- Claim: Prompt-based LLMs can operationalize functionalist translation principles when provided with structured specifications
- Mechanism: LLMs interpret specifications as high-level task instructions, generating outputs that prioritize functional adequacy over literal equivalence
- Core assumption: LLMs have sufficient world knowledge and linguistic flexibility to adapt style, register, and content based on abstract specifications
- Evidence anchors: Abstract shows specification-guided ChatGPT outperforms official human translations; Section 5.2.1 shows ChatGPT + Spec and PE + Spec receive lowest error scores; Section 5.2.2 shows ChatGPT PE + Spec most often ranked 1st

### Mechanism 3: Specification-Aware Evaluation Realigns Quality Metrics with Communicative Goals
- Claim: Evaluation grounded in specifications better captures functional adequacy than automatic metrics optimized for literal fidelity
- Mechanism: Specification-aware evaluation assesses whether translations meet contextual and communicative requirements, not just source-text alignment
- Core assumption: Translation quality is multidimensional; accuracy and fluency are necessary but not sufficient for professional use
- Evidence anchors: Abstract notes COMETKiwi favors Google Translate while human evaluators prefer specification-guided ChatGPT; Section 5.3 shows COMETKiwi scores contradict human evaluation rankings

## Foundational Learning

**Translation Specifications (ISO 17100, JTF, MQM)**
- Why needed here: Specifications formalize communicative goals, audience, style, and constraints; they are the core input for specification-aware MT and evaluation
- Quick check: Can you list at least three typical specification parameters (e.g., purpose, audience, tone) and explain how each influences translation decisions?

**Skopos Theory / Functionalist Translation**
- Why needed here: Provides theoretical rationale for specification-aware approaches—translation is purpose-driven, not equivalence-driven
- Quick check: How does Skopos theory redefine translation quality compared to equivalence-based approaches?

**MQM Framework and Weighted Error Analysis**
- Why needed here: MQM provides error typology and weighting; specification-aware evaluation adapts weights based on project priorities
- Quick check: Given a specification emphasizing investor persuasion over literal accuracy, how would you adjust MQM category weights (Accuracy vs. Style)?

## Architecture Onboarding

**Component map:**
Specification definition -> Prompt design engine -> Translation generation -> Human review/post-editing -> Evaluation system (MQM + subjective) -> Metric calibration layer

**Critical path:** Define specifications → design prompts → generate translation → human review → evaluate (error + subjective) → calibrate automatic metrics

**Design tradeoffs:**
- Prompt specificity vs. flexibility: Overly detailed prompts may over-constrain; underspecified prompts reduce alignment
- Human oversight vs. automation: Full automation risks hallucination/over-generation; human review adds cost/time
- Automatic metric selection: COMETKiwi favors literal fidelity; specification-aware evaluation may require custom or composite metrics

**Failure signatures:**
- Kanji/domain terminology misinterpretation
- Over-literal translation in non-specification-guided MT
- Stylistic inconsistency or grammatical errors in official human translations
- Low inter-annotator agreement in error evaluation

**First 3 experiments:**
1. Implement minimal specification-aware pipeline for single domain (e.g., corporate IR texts): define 3–5 specification parameters, design basic prompts, generate translations, conduct MQM-weighted error analysis
2. Compare specification-guided LLM outputs vs. non-guided LLM vs. Google Translate on small held-out set; measure both COMETKiwi scores and human preference rankings
3. Test prompt robustness: vary specification completeness (full vs. partial vs. missing) and measure impact on translation quality and human evaluation alignment

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Do specification-guided translation findings generalize across different LLM architectures (e.g., Claude, Gemini, open-source models), or are they specific to ChatGPT-4o?
- Basis in paper: [explicit] "Evaluating other LLMs remains an important area for future research to assess whether the findings generalize across models."
- Why unresolved: Study used only ChatGPT-4o; model behavior may differ in prompt sensitivity, stylistic control, and hallucination tendencies
- What evidence would resolve it: Replicate experiment with multiple LLMs using identical specifications and prompts, comparing performance across models

**Open Question 2**
- Question: Can automatic metrics be developed to capture functional adequacy and specification alignment rather than prioritizing source-text fidelity?
- Basis in paper: [inferred] COMETKiwi favors Google Translate while human evaluators prefer specification-guided ChatGPT
- Why unresolved: Current reference-free metrics are not designed to assess whether translations meet communicative goals or stylistic specifications
- What evidence would resolve it: Develop and validate metrics that incorporate specification parameters and test correlation with human judgment

**Open Question 3**
- Question: How robust is the specification-aware framework across diverse domains and language pairs beyond Japanese-to-English investor relations texts?
- Basis in paper: [explicit] "Broader validation will require larger datasets covering more diverse domains (e.g., legal and medical) and content types (e.g., marketing and technical manuals), as well as other language pairs."
- Why unresolved: Japanese-to-English IR materials represent one specific domain with particular linguistic challenges and functional requirements
- What evidence would resolve it: Conduct experiments across multiple domains and language pairs with varying linguistic distances

**Open Question 4**
- Question: Can LLMs serve as reliable evaluators for specification-aware translation, providing a scalable alternative to scarce human experts?
- Basis in paper: [explicit] "Future work could explore the LLM as a Judge... where an LLM evaluates outputs based on the same detailed specifications provided to human experts"
- Why unresolved: Recruiting qualified human evaluators proved difficult (2 of 24 completed), and human evaluation is cognitively demanding and inconsistent
- What evidence would resolve it: Compare LLM-based specification-aware evaluation against human expert judgments

## Limitations
- Prompt specificity and reproducibility: Exact specification-guided prompts are not fully disclosed, limiting reproducibility
- Automatic metric limitations: COMETKiwi scores diverge from human evaluations, but paper does not propose specification-aware automatic metrics
- Generalizability: Corpus limited to Japanese corporate IR texts; findings may not transfer to other domains or language pairs
- Human evaluation reliability: High annotator dropout and low inter-annotator agreement suggest potential reliability issues

## Confidence
- **High confidence**: Specification-aware LLMs outperform non-guided MT and official human translations in human evaluations
- **Medium confidence**: Specification-aware evaluation better aligns with functional adequacy than automatic metrics, but limited by small sample sizes
- **Low confidence**: Generalizability of results across domains, languages, and specification types due to narrow corpus scope

## Next Checks
1. **Prompt robustness testing**: Systematically vary specification completeness and prompt structure to measure impact on translation quality and human evaluation alignment
2. **Automatic metric development**: Design and test specification-aware automatic metrics (e.g., weighted COMET variants) to better align with human evaluation of functional adequacy
3. **Cross-domain replication**: Replicate specification-aware MT pipeline on a different domain (e.g., technical manuals, legal documents) to assess generalizability and domain-specific prompt adaptation needs