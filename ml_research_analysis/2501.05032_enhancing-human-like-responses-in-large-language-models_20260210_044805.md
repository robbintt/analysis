---
ver: rpa2
title: Enhancing Human-Like Responses in Large Language Models
arxiv_id: '2501.05032'
source_url: https://arxiv.org/abs/2501.05032
tags:
- responses
- human-like
- language
- like
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper focuses on making large language models (LLMs) more
  human-like by fine-tuning them to generate natural, conversational responses instead
  of formal, impersonal ones. To achieve this, the authors create synthetic datasets
  using custom prompts to elicit both human-like and formal responses, then train
  models using Direct Preference Optimization (DPO) to prioritize the more engaging
  responses.
---

# Enhancing Human-Like Responses in Large Language Models

## Quick Facts
- arXiv ID: 2501.05032
- Source URL: https://arxiv.org/abs/2501.05032
- Authors: Ethem Yağız Çalık; Talha Rüzgar Akkuş
- Reference count: 40
- Primary result: Fine-tuned models achieved 89.6-79.6% human-likeness selection rates while maintaining general benchmark performance

## Executive Summary
This paper focuses on making large language models (LLMs) more human-like by fine-tuning them to generate natural, conversational responses instead of formal, impersonal ones. The authors create synthetic datasets using custom prompts to elicit both human-like and formal responses, then train models using Direct Preference Optimization (DPO) to prioritize the more engaging responses. They fine-tune three models (Llama 3-8B, Qwen-2.5-7B, and Mistral-Nemo) using Low-Rank Adaptation (LoRA) and evaluate their human-likeness through an anonymous voting system.

The fine-tuned models outperformed official models in perceived human-likeness (89.6%, 89.5%, and 79.6% selection rates respectively) while maintaining comparable performance on general benchmarks, with only minor reductions in IFEval scores. The results demonstrate that open-source models can be effectively fine-tuned to produce more conversational and relatable responses without sacrificing overall accuracy.

## Method Summary
The method uses synthetic preference data generated by prompting Llama 3 405B/70B to create question-answer pairs, with dual system prompts producing "chosen" (human-like) and "rejected" (formal) responses. These pairs are used for Direct Preference Optimization (DPO) training with LoRA (rank 8) to constrain weight updates and preserve pretrained capabilities. The models are evaluated through anonymous human voting and standard benchmarks including IFEval, BBH, MATH, GPQA, MuSR, and MMLU-PRO.

## Key Results
- Human-likeness selection rates: Llama 3-8B-Instruct (89.6%), Qwen-2.5-7B-Instruct (89.5%), Mistral-Nemo-Instruct (79.6%)
- IFEval performance reduction: -9.4 to -12.0 points across models
- Open LLM Leaderboard scores: Average change of -0.02 to +1.07 points excluding IFEval
- Training completed in 2-4 hours on 2×A100 80GB GPUs

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Prompt Pairing for Preference Signal Generation
Creating synthetic chosen/rejected response pairs via differentiated system prompts produces a learnable preference signal for conversational style. Llama 3 405B generates questions; Llama 3 70B generates two responses per question—one with a "conversational buddy" system prompt (chosen), one with a "formal professional" prompt (rejected). DPO then optimizes the model to increase likelihood of chosen responses while decreasing rejected ones.

### Mechanism 2: DPO Reward Shaping Without Explicit Reward Model
Direct Preference Optimization bypasses training a separate reward model by reformulating RLHF as a classification loss on preference pairs. DPO uses the reference policy and preference data to directly optimize the policy, with β=0.1 controlling the preference optimization strength.

### Mechanism 3: LoRA Rank Constraint for Capability Preservation
Low-rank adaptation with r=8 limits weight perturbations, preserving pretrained knowledge while enabling style adaptation. This constrains the solution space to directions that don't disrupt core competencies like reasoning and knowledge retrieval.

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: Core training method replacing RLHF; requires understanding how preference pairs translate to policy updates without explicit reward models.
  - Quick check question: Can you explain why DPO doesn't require training a separate reward model, and what β controls?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Determines how much the model can change; r=8 is a deliberate constraint. Understanding rank-approximation is essential for tuning the adaptation-vs-preservation tradeoff.
  - Quick check question: If LoRA r=8 and model hidden dimension is 4096, what percentage of parameters are trainable for a single weight matrix?

- Concept: **Instruction-Following Evaluation (IFEval)**
  - Why needed here: Primary benchmark showing degradation (9+ point drops); understanding what IFEval measures explains why style fine-tuning hurts instruction adherence.
  - Quick check question: Why might a model fine-tuned for casual conversation struggle with strict instruction-following tasks?

## Architecture Onboarding

- Component map: [Question Generation: Llama 3 405B] → [Answer Generation: Llama 3 70B + Dual System Prompts] → {chosen, rejected} pairs → [DPO Training: Base Model + LoRA (r=8)] → [Evaluation: Gradio Voting + Open LLM Leaderboard]

- Critical path:
  1. System prompt design (Appendix A.1/A.2) → determines quality of preference signal
  2. Temperature/top-p settings (1.0/1.0) → controls response diversity
  3. LoRA hyperparameters (r=8, α=4, dropout=0.05) → controls adaptation magnitude
  4. DPO β=0.1 → controls preference optimization strength

- Design tradeoffs:
  - **r=8 vs higher**: Lower r preserves capabilities but limits style shift; authors explicitly chose low r to minimize IFEval degradation
  - **Synthetic vs human data**: Synthetic enables scale/control but lacks real conversation variability (acknowledged limitation)
  - **DPO vs RLHF**: DPO is simpler/faster but less flexible for complex reward shaping

- Failure signatures:
  - IFEval drops >10 points: LoRA rank too high or training steps excessive
  - Reward margin doesn't increase (Figure 3): Chosen/rejected pairs insufficiently differentiated
  - Model still outputs "I'm an AI language model": DPO didn't converge; check prompt adherence during data generation
  - Benchmark scores collapse: Catastrophic forgetting; reduce r or epochs

- First 3 experiments:
  1. **Reproduce with subset**: Take 1000 samples from the published dataset, train for 0.5 epochs with same hyperparameters; verify reward margin increases and spot-check for "As an AI" disclaimers.
  2. **Ablate r values**: Train with r=4, 8, 16, 32; plot IFEval score vs human-likeness vote rate to find optimal tradeoff point.
  3. **Test generalization**: Evaluate on out-of-distribution conversational topics not in training data (check Appendix B topic clusters for coverage gaps); assess whether style transfers to unseen domains.

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data may not capture the full variability of real human conversations
- Significant IFEval degradation (-9 to -12 points) indicates tradeoff between conversational style and instruction-following capability
- LoRA rank selection (r=8) appears somewhat arbitrary without systematic analysis of the adaptation-vs-preservation tradeoff

## Confidence
- **High confidence**: Models achieve measurably higher human-likeness selection rates (89.6%, 89.5%, 79.6%) compared to baselines in anonymous voting evaluation
- **Medium confidence**: Claim that models "maintain comparable performance" on general benchmarks is technically accurate but potentially misleading due to substantial IFEval drops
- **Low confidence**: Assertion that these models are "more human-like" in absolute terms rather than simply more casual or less formal is not well-supported

## Next Checks
1. **Ablation study on LoRA rank**: Systematically evaluate r=4, 8, 16, 32 to identify optimal tradeoff point between human-likeness improvement and capability preservation
2. **Cross-topic generalization test**: Evaluate fine-tuned models on conversational topics outside the 256-topic training distribution to assess style transfer to unseen domains
3. **Instruction-following capability assessment**: Conduct detailed analysis of which specific IFEval tasks show largest degradation and whether targeted fine-tuning could preserve instruction-following while maintaining conversational improvements