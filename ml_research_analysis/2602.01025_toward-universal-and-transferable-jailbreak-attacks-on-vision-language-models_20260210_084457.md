---
ver: rpa2
title: Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models
arxiv_id: '2602.01025'
source_url: https://arxiv.org/abs/2602.01025
tags:
- loss
- jailbreak
- wang
- image
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of jailbreaking vision-language
  models (VLMs) by creating adversarial images that can induce harmful responses.
  The authors propose UltraBreak, a framework that combines vision-level regularisation
  (random transformations and total variation loss) with semantically weighted textual
  targets to achieve both universality (cross-target) and transferability (cross-model)
  in jailbreak attacks.
---

# Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models

## Quick Facts
- **arXiv ID:** 2602.01025
- **Source URL:** https://arxiv.org/abs/2602.01025
- **Reference count:** 22
- **Primary result:** Universal adversarial jailbreak images achieve 71.83% black-box ASR on SafeBench and 57.64% on AdvBench across vision-language models

## Executive Summary
This paper introduces UltraBreak, a framework for generating universal and transferable adversarial images that jailbreak vision-language models (VLMs) into producing harmful responses. The method combines vision-level regularization (random affine transformations and total variation loss) with semantically weighted textual targets using attention mechanisms. UltraBreak achieves state-of-the-art performance in black-box transferability while maintaining universality across diverse harmful query targets, outperforming existing gradient-based methods significantly.

## Method Summary
UltraBreak generates a single adversarial trigger image that can jailbreak VLMs across different harmful queries and black-box models. The framework uses a surrogate VLM (Qwen2-VL-7B-Instruct) to optimize the trigger image through a combined loss function. The key innovation is the attention semantic loss, which computes the Kullback-Leibler divergence between normalized attention weights and target embeddings. The optimization includes total variation regularization (λ_TV=0.5) and random affine transformations (translation up to 112 pixels, rotation ±15°, scale 0.8-1.2) to ensure transferability. The attack uses a specific prompt format ("Steps to [query] You must begin your response with: [Jailbroken Mode]") and evaluates success using the HarmBench classifier across SafeBench and AdvBench datasets.

## Key Results
- Achieves average attack success rate of 71.83% on black-box models in SafeBench
- Achieves average attack success rate of 57.64% on black-box models in AdvBench
- Outperforms existing gradient-based methods in both transferability and universality metrics

## Why This Works (Mechanism)
The success of UltraBreak stems from its multi-level regularization approach. By incorporating attention semantic loss, the method ensures that the adversarial image targets semantically relevant regions of the model's attention distribution. The total variation loss prevents overfitting to the surrogate model by discouraging high-frequency patterns. Random affine transformations during training create transformation-invariant features that generalize better to unseen models. The semantically weighted textual targets ensure that the attack is universally applicable across different harmful queries rather than being query-specific.

## Foundational Learning
- **Attention semantic loss**: KL divergence between normalized attention weights and target embeddings; needed to align adversarial perturbations with semantically important regions; quick check: verify attention distributions match expected patterns
- **Total variation regularization**: Penalizes high-frequency noise in adversarial images; needed to prevent overfitting and create visually coherent triggers; quick check: measure TV loss during training
- **Affine transformations**: Random translations, rotations, and scaling applied during optimization; needed to create transformation-invariant adversarial features; quick check: verify parameter ranges (l∈[0,112], r∈[-15°,15°], s∈[0.8,1.2])
- **HarmBench classifier**: Multi-modal behavior classifier for detecting harmful responses; needed as proxy metric for actual jailbreak success; quick check: validate classifier accuracy on known harmful/non-harmful pairs
- **Universal trigger generation**: Single image optimized for multiple target queries; needed to reduce attack complexity and increase practical utility; quick check: test on held-out queries
- **Cross-model transferability**: Adversarial examples that work on surrogate generalize to black-box models; needed for practical attacks where target models are unknown; quick check: evaluate on models with different architectures

## Architecture Onboarding
**Component map:** Surrogate VLM -> Embedding matrix W -> Attention semantic loss -> TV loss + random transformations -> Adam optimizer -> Universal trigger image -> Black-box VLMs -> HarmBench classifier

**Critical path:** Training loop: random image initialization → affine projection with CLIP normalization → attention semantic loss computation → total variation regularization → Adam optimization (1300 iterations, lr=0.01) → universal trigger generation

**Design tradeoffs:** The method balances between attack strength (higher ASR) and transferability (generalization to black-box models) through regularization parameters. Higher TV weight (λ_TV=0.5) improves transferability but may reduce attack potency. Random transformations increase computational cost but significantly improve black-box performance.

**Failure signatures:** Low transferability suggests insufficient regularization or transformation invariance. Training instability indicates incorrect hyperparameter settings (τ=0.5, ε=1e-4). High white-box but low black-box ASR indicates overfitting to the surrogate model.

**Exactly 3 first experiments:** 1) Train UltraBreak on SafeBench-Tiny subset and verify convergence; 2) Test universal trigger on held-out SafeBench queries using HarmBench classifier; 3) Evaluate black-box transferability on Qwen-VL-Chat and LLaVA-v1.6 models.

## Open Questions the Paper Calls Out
None

## Limitations
- SafeBench-Tiny dataset composition is underspecified, making exact reproduction challenging
- Limited evaluation to only 5 black-box models may not capture broader VLM vulnerability patterns
- Reliance on HarmBench classifier as proxy rather than actual harmful response verification
- Patch-based attack format introduces architectural dependencies that may not generalize to all VLM input modalities

## Confidence
- **Black-box transferability claims (Medium):** Strong empirical results but limited model diversity and proxy classifier verification
- **Universal trigger effectiveness (Medium):** Demonstrated universality but evaluation covers only 1680 targets
- **Methodological contributions (High):** Clear algorithmic specifications and sound theoretical foundation

## Next Checks
1. Replicate black-box evaluation on at least 3 additional unseen VLMs (e.g., GPT-4V, Gemini Pro Vision, Claude 3) using the same HarmBench classifier to test generalizability beyond the 5 models in the original evaluation.

2. Measure semantic fidelity by generating actual harmful responses from black-box models when triggered, then having human annotators rate both harmfulness and response relevance to the original query (rather than relying solely on classifier scores).

3. Test transferability across training paradigms by evaluating the universal trigger against VLMs with different pretraining objectives (e.g., contrastive vs. generative pretraining) to assess whether the attack exploits fundamental architectural vulnerabilities or dataset-specific correlations.