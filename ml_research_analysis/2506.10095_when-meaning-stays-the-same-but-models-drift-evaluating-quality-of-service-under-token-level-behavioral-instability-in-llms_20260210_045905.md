---
ver: rpa2
title: 'When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service
  under Token-Level Behavioral Instability in LLMs'
arxiv_id: '2506.10095'
source_url: https://arxiv.org/abs/2506.10095
tags:
- pbss
- prompt
- drift
- behavioral
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Prompt-Based Semantic Shift (PBSS), a diagnostic
  framework that measures how large language models respond to semantically equivalent
  prompts that differ only in surface-level token form. The authors construct controlled
  prompt sets across ten tasks, generate model outputs under two decoding temperatures,
  and use sentence embeddings to compute pairwise behavioral drift.
---

# When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs

## Quick Facts
- arXiv ID: 2506.10095
- Source URL: https://arxiv.org/abs/2506.10095
- Authors: Xiao Li; Joel Kreuzwieser; Alan Peters
- Reference count: 40
- This paper introduces Prompt-Based Semantic Shift (PBSS), a diagnostic framework that measures how large language models respond to semantically equivalent prompts that differ only in surface-level token form.

## Executive Summary
This paper introduces Prompt-Based Semantic Shift (PBSS), a diagnostic framework that measures how large language models respond to semantically equivalent prompts that differ only in surface-level token form. The authors construct controlled prompt sets across ten tasks, generate model outputs under two decoding temperatures, and use sentence embeddings to compute pairwise behavioral drift. Their analysis reveals consistent, model-specific drift patterns that persist across embedding architectures, showing that tokenization and decoding dynamics contribute to post-training quality-of-service instability. Higher-capacity instruction-tuned models exhibit lower drift than smaller or pre-alignment models, with Kruskal-Wallis tests confirming statistically significant differences between model tiers. PBSS offers a lightweight, encoder-agnostic diagnostic for detecting prompt-induced behavioral variance, with implications for reliability in high-stakes domains such as medicine and legal applications.

## Method Summary
The PBSS framework measures behavioral drift by generating semantically equivalent prompts, collecting model outputs at different temperatures, embedding outputs with sentence transformers, and computing pairwise cosine distances. The method uses 10 task domains with 5 prompt sets each, 15 paraphrases per set (4,500 prompts per model), and three S-BERT variants for embedding. Outputs are generated at temperatures 0.2 and 1.3, then analyzed using CDF curves, z-score heatmaps, and Kruskal-Wallis statistical tests to identify significant differences between model tiers.

## Key Results
- PBSS reveals consistent behavioral drift patterns across semantically equivalent prompts that persist across different embedding architectures
- Higher-capacity instruction-tuned models exhibit significantly lower drift than smaller or pre-alignment models, with Kruskal-Wallis tests confirming statistical significance
- The drift patterns show a categorical shift between instruction-tuned and pre-alignment models rather than gradual improvement with scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantically equivalent prompts with different token-level realizations produce divergent model outputs due to tokenization pathway differences.
- Mechanism: Different paraphrases of the same intent map to distinct token sequences. These sequences activate different regions of the model's learned representation space, leading to variations in tone, structure, and rhetorical framing even when semantic content should be preserved.
- Core assumption: Tokenization dynamics create non-equivalent internal activations even for meaning-preserving inputs.
- Evidence anchors:
  - [abstract] "suggesting statistical regularities linked to tokenization and decoding"
  - [Section 3.1] "prompt variance often arises from different token sequences—variations that standard accuracy metrics ignore"
  - [corpus] "Training Language Models with homotokens" confirms distinct token sequences decoding to same surface form induce different internal computations
- Break condition: If models developed true semantic invariance at scale, drift would approach zero across all paraphrases regardless of tokenization differences.

### Mechanism 2
- Claim: Instruction tuning creates behavioral phase boundaries where aligned models exhibit fundamentally different consistency profiles than pre-alignment models.
- Mechanism: Instruction tuning shapes model responses through supervised fine-tuning on curated prompts, creating more robust mapping from intent to output that resists surface-level token variation. This is not gradual improvement but a categorical shift in behavioral stability.
- Core assumption: Alignment training modifies how models weight semantic intent versus surface token patterns.
- Evidence anchors:
  - [abstract] "Higher-capacity instruction-tuned models exhibit lower drift than smaller or pre-alignment models"
  - [Section 5.5] "This is not a gradual trend but a behavioral phase shift—newer models adapt phrasing without semantic drift, older ones collapse into repetition or divergence"
  - [corpus] Weak direct evidence; corpus papers focus on drift quantification rather than alignment effects specifically
- Break condition: If alignment merely improves output quality without changing consistency structure, drift patterns would scale uniformly with model size rather than showing categorical differences.

### Mechanism 3
- Claim: Sentence embedding models can serve as behavioral diagnostics for detecting prompt-induced output variance without requiring ground-truth labels.
- Mechanism: By projecting outputs into semantic embedding space and computing pairwise distances, PBSS captures rhetorical and structural differences beyond surface similarity. The consistency of rankings across different encoder architectures (MiniLM-L6, MiniLM-L12, MPNet) suggests the metric captures model-intrinsic behavioral properties rather than encoder artifacts.
- Core assumption: Semantic embedding distances correlate meaningfully with behavioral consistency under prompt variation.
- Evidence anchors:
  - [abstract] "using sentence embeddings to compute pairwise behavioral drift"
  - [Section 5.4] "Across encoders, we observe consistent model rankings and curve shapes"
  - [Section 5.9] "Were PBSS merely reflecting prompt design idiosyncrasies, we would expect encoder disagreement and temperature instability, neither of which are observed"
  - [corpus] "Mapping from Meaning" paper addresses prompt sensitivity but does not validate embedding-based diagnostics specifically
- Break condition: If embedding models failed to capture rhetorically relevant dimensions or introduced systematic biases, PBSS rankings would diverge across encoders.

## Foundational Learning

- Concept: Cosine similarity/distance in embedding space
  - Why needed here: PBSS relies on computing pairwise distances between embedded outputs. Understanding that cosine distance measures angular separation (1 - cosine similarity) in high-dimensional space is essential for interpreting drift scores.
  - Quick check question: If two outputs have cosine similarity of 0.7, what is their cosine distance?

- Concept: Cumulative Distribution Functions (CDFs) for behavioral analysis
  - Why needed here: The paper uses CDFs of PBSS scores to compare model stability. A steeper CDF rising near zero indicates high consistency; flatter curves indicate broader drift.
  - Quick check question: Model A's CDF reaches 80% at PBSS=0.4 while Model B's reaches 80% at PBSS=0.6. Which model is more stable?

- Concept: Kruskal-Wallis non-parametric testing
  - Why needed here: The paper uses this test to confirm statistically significant differences between model tiers without assuming normal distributions of drift scores.
  - Quick check question: Why might researchers choose Kruskal-Wallis over ANOVA for comparing PBSS distributions across model tiers?

## Architecture Onboarding

- Component map: Prompt generation layer -> Model inference layer -> Embedding layer -> Drift computation layer -> Visualization layer
- Critical path: Prompt validation → Model inference → Embedding generation → Pairwise distance computation → Statistical aggregation
  - If semantic validation fails (prompts not equivalent), downstream drift may reflect meaning differences rather than tokenization effects
  - If embedding model is poorly matched to task domain, drift scores may capture noise
- Design tradeoffs:
  - Encoder choice: Deeper encoders (MPNet vs MiniLM-L6) provide more granular semantic distinctions but slower inference
  - Temperature settings: Low temperature (0.2) isolates tokenization effects; high temperature (1.3) reveals combined tokenization + sampling variance
  - Prompt set size: More variants improve statistical power but increase compute cost (paper uses 15 variants × 5 prompt sets × 10 tasks = 750 prompts per model)
- Failure signatures:
  - Encoder disagreement on rankings suggests embedding artifacts dominate signal
  - Flat CDFs across all models indicates prompt set lacks sufficient semantic variation
  - High drift even at temperature 0 with identical prompts suggests implementation error
  - Clustering failure in t-SNE (no task separation) suggests prompts lack semantic coherence
- First 3 experiments:
  1. Replicate single-task PBSS analysis with your target model using provided prompt sets; compare CDF shape to paper's baseline models (GPT-2 as high-drift reference, GPT-3.5 as low-drift reference)
  2. Test encoder sensitivity by computing PBSS with all three S-BERT variants; verify model rankings remain stable across encoders (semantic resonance check)
  3. Probe a specific high-stakes domain (e.g., medical explanation) with your production prompts; identify if any prompt variants cause outsized drift (z-score > 2.0) indicating instability zones

## Open Questions the Paper Calls Out
None

## Limitations
- Incomplete specification of prompt sets and validation protocols creates uncertainty about whether reproduced results reflect claimed mechanisms
- Reliance on sentence embedding distances as proxy for behavioral consistency lacks validation against human judgment of output quality
- Temperature-based experimental design assumes T=0.2 isolates tokenization effects, but other generation parameters remain unspecified

## Confidence
**High Confidence**: The existence of consistent behavioral drift patterns across semantically equivalent prompts.
**Medium Confidence**: The categorical distinction between instruction-tuned and pre-alignment models.
**Low Confidence**: The generalizability of PBSS scores as a universal diagnostic tool.

## Next Checks
1. **Encoder Sensitivity Validation**: Replicate the full PBSS analysis using three different sentence embedding models and verify that model rankings remain stable across all encoders. Calculate inter-encoder correlation coefficients for PBSS distributions.
2. **Human Judgment Correlation**: For a subset of high-drift prompt pairs, collect human ratings on output quality and task completion. Compute correlation between PBSS scores and human judgment variance.
3. **Cross-Domain Generalization**: Apply PBSS to a new domain (e.g., legal document analysis or medical diagnosis explanation) using domain-specific prompt sets. Compare drift patterns to the paper's findings.