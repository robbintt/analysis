---
ver: rpa2
title: 'MALBO: Optimizing LLM-Based Multi-Agent Teams via Multi-Objective Bayesian
  Optimization'
arxiv_id: '2511.11788'
source_url: https://arxiv.org/abs/2511.11788
tags:
- optimization
- cost
- performance
- agent
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis introduces MALBO, a Bayesian optimization framework
  for optimizing the assignment of LLMs to specialized roles in multi-agent teams.
  The key problem is a vast combinatorial search space with expensive black-box evaluations
  and conflicting objectives of performance and cost.
---

# MALBO: Optimizing LLM-Based Multi-Agent Teams via Multi-Objective Bayesian Optimization

## Quick Facts
- arXiv ID: 2511.11788
- Source URL: https://arxiv.org/abs/2511.11788
- Reference count: 0
- Multi-objective Bayesian optimization framework for LLM team composition

## Executive Summary
MALBO addresses the challenge of efficiently assigning LLMs to specialized roles in multi-agent teams while balancing performance and cost objectives. The framework uses multi-objective Bayesian optimization with Gaussian Processes to navigate a vast combinatorial search space of LLM configurations. By representing LLMs as continuous feature vectors and leveraging the q-Expected Hypervolume Improvement acquisition function, MALBO efficiently discovers Pareto-optimal configurations that minimize cost while maintaining maximum performance. The approach demonstrates significant improvements over random search and homogeneous baselines, with average cost reductions exceeding 45% and up to 65.8% in optimal cases.

## Method Summary
MALBO frames LLM team composition as a multi-objective optimization problem where the goal is to find optimal assignments of LLMs to specialized agent roles while balancing competing objectives of system performance and operational cost. The framework represents LLMs as continuous feature vectors, enabling the use of Gaussian Process models to approximate the expensive black-box evaluation of team performance. The optimization process employs the q-Expected Hypervolume Improvement acquisition function to guide exploration of the configuration space, efficiently converging to a Pareto front of optimal solutions. This approach handles the combinatorial complexity of agent-to-role assignments while providing interpretable insights into which model features and roles most influence system outcomes.

## Key Results
- Reduced average cost of high-performing configurations by over 45% compared to random search
- Achieved up to 65.8% cost reductions versus homogeneous baseline approaches while maintaining maximum performance
- Discovered Pareto front of optimal configurations providing performance-cost trade-off insights

## Why This Works (Mechanism)
The effectiveness of MALBO stems from its ability to efficiently navigate the complex, high-dimensional search space of LLM team configurations using Bayesian optimization. By representing LLMs as continuous feature vectors, the framework transforms the discrete assignment problem into a continuous optimization space that Gaussian Processes can model effectively. The q-Expected Hypervolume Improvement acquisition function enables batch evaluations that accelerate convergence to the Pareto front while maintaining diversity in the solution set. This probabilistic modeling approach is particularly well-suited for expensive black-box evaluations where each configuration assessment requires actual deployment and measurement of team performance.

## Foundational Learning
**Gaussian Process Regression**: A probabilistic model for approximating expensive objective functions by placing a distribution over possible functions, providing uncertainty estimates that guide exploration. Needed for modeling the relationship between LLM configurations and team performance/cost metrics; quick check: verify GP kernel choice matches feature characteristics.

**Expected Hypervolume Improvement (EHVI)**: A multi-objective acquisition function that selects configurations expected to improve the dominated hypervolume of the Pareto front, balancing exploration and exploitation. Required to guide search toward the Pareto front efficiently; quick check: confirm EHVI calculations correctly handle multiple objectives.

**Multi-objective Optimization**: The process of simultaneously optimizing multiple conflicting objectives to find Pareto-optimal solutions where no objective can be improved without degrading another. Essential for balancing performance-cost trade-offs in LLM team composition; quick check: verify Pareto front solutions are non-dominated.

## Architecture Onboarding

**Component Map**: LLM Feature Extraction -> Gaussian Process Modeling -> Multi-objective Bayesian Optimization -> Pareto Front Discovery -> Configuration Selection

**Critical Path**: The optimization pipeline follows a sequential flow where LLM feature representations feed into GP models, which inform the Bayesian optimization process to discover Pareto-optimal configurations. The most computationally intensive step is the black-box evaluation of candidate configurations during optimization iterations.

**Design Tradeoffs**: The framework trades computational overhead during optimization (multiple GP evaluations and acquisition function calculations) for reduced overall evaluation costs by avoiding exhaustive search. The continuous feature representation simplifies optimization but may lose some discrete characteristics of LLM capabilities.

**Failure Signatures**: Optimization may converge prematurely if the GP model overfits to early observations, or fail to find diverse solutions if the acquisition function becomes too exploitative. Poor feature representations can lead to suboptimal Pareto fronts that don't capture true performance-cost relationships.

**First Experiments**: 1) Validate GP model accuracy on a small subset of configurations before full optimization; 2) Compare single-objective vs multi-objective optimization results to confirm benefit of joint optimization; 3) Test sensitivity to initial design points and acquisition function parameters.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Computational overhead of Bayesian optimization process not fully characterized
- Limited evaluation of scalability to larger agent teams and more complex role hierarchies
- Focus on cost-performance trade-offs without considering other factors like latency or reliability

## Confidence
- High: Multi-objective Bayesian optimization approach is technically sound and well-established
- Medium: Performance improvements depend heavily on specific benchmarks and evaluation setup
- Low: Claims about insights into feature importance lack robust causal validation

## Next Checks
1. Test MALBO's performance across diverse multi-agent domains to assess generalizability beyond evaluated benchmarks
2. Conduct ablation studies to quantify contributions of continuous feature representation versus Bayesian optimization methodology
3. Evaluate computational efficiency and wall-clock time overhead of MALBO optimization process compared to simpler baselines across varying team sizes