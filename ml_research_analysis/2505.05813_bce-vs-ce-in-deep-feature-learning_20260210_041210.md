---
ver: rpa2
title: BCE vs. CE in Deep Feature Learning
arxiv_id: '2505.05813'
source_url: https://arxiv.org/abs/2505.05813
tags:
- adamw
- scores
- feature
- training
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper theoretically and empirically compares Cross-Entropy
  (CE) and Binary Cross-Entropy (BCE) losses in deep feature learning. The authors
  prove that BCE can also maximize intra-class compactness and inter-class distinctiveness
  when reaching its minimum, leading to neural collapse (NC), a phenomenon previously
  established only for CE.
---

# BCE vs. CE in Deep Feature Learning

## Quick Facts
- arXiv ID: 2505.05813
- Source URL: https://arxiv.org/abs/2505.05813
- Authors: Qiufu Li; Huibin Xiao; Linlin Shen
- Reference count: 40
- Primary result: BCE induces Neural Collapse by enforcing absolute decision score constraints, leading to better feature compactness and higher classification accuracy than CE

## Executive Summary
This paper theoretically and empirically compares Cross-Entropy (CE) and Binary Cross-Entropy (BCE) losses in deep feature learning. The authors prove that BCE can maximize intra-class compactness and inter-class distinctiveness when reaching its minimum, leading to neural collapse (NC) - a phenomenon previously established only for CE. BCE measures absolute decision scores across all samples, while CE measures relative scores per sample, enabling BCE to explicitly enhance feature properties. The classifier biases in BCE play a substantial role in constraining decision scores during training, unlike in CE where they have minimal effect. Experiments show BCE-trained models consistently achieve higher classification accuracy and uniform accuracy, indicating better feature compactness and distinctiveness across multiple architectures, datasets, and optimizers.

## Method Summary
The paper compares CE and BCE losses for multi-class image classification and deep feature learning analysis. BCE is formulated as K independent binary classification tasks using sigmoid activation, while CE uses softmax over all classes. The experimental setup includes ResNet18, ResNet50, and DenseNet121 architectures trained on MNIST, CIFAR-10/100, and ImageNet-1k (fine-tuning only). Two optimization strategies are used: SGD with LR=0.01, step scheduler, batch size 128 for 100 epochs, and AdamW with LR=0.001, cosine scheduler for 100 epochs. Weight decay is applied globally for classification experiments, while NC experiments use decoupled weight decay on classifier weights, features, and biases. The paper evaluates classification accuracy, uniform accuracy, and NC metrics (variability collapse, simplex ETF convergence, self-duality) to assess feature quality.

## Key Results
- BCE induces Neural Collapse properties (NC1 and NC2) at the loss minimum, a first theoretical proof for BCE
- BCE-trained models achieve consistently higher classification accuracy and uniform accuracy across all tested architectures and datasets
- Classifier biases in BCE converge to values that separate positive and negative decision scores, while CE biases remain near initialization values
- BCE converges to NC states faster in early training epochs compared to CE

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** BCE induces feature compactness by optimizing absolute decision scores rather than relative rankings.
- **Mechanism:** BCE enforces positive scores approach a high uniform level and negative scores approach a low uniform level (absolute), creating "closed" geometric regions for features. CE only requires positive scores be larger than negative scores (relative), creating "unbounded" regions.
- **Core assumption:** Feature dimension d is sufficient to support class separation (d ≥ K-1).
- **Evidence anchors:** Abstract states BCE adjusts positive/negative decision scores to uniformly high/low levels; Section 3.3 shows BCE requires features to fall within closed hyperspheres.

### Mechanism 2
- **Claim:** Classifier biases in BCE act as critical constraints for feature separation, unlike in CE where they are negligible.
- **Mechanism:** In BCE, bias converges to a unique value separating means of positive and negative decision scores. In CE, bias gradients vanish or rely solely on initialization, failing to constrain feature geometry.
- **Core assumption:** Optimization finds unique bias solution defined in Theorem 3.2 rather than getting stuck in local minima.
- **Evidence anchors:** Abstract notes BCE biases present substantial constraint on decision scores; Section 4.1 shows CE biases tracking initialization values while BCE biases consistently separate score distributions.

### Mechanism 3
- **Claim:** BCE leads to Neural Collapse, guaranteeing maximum inter-class distinctiveness at loss minimum.
- **Mechanism:** Minimizing BCE forces features to collapse to class means (NC1) and class means to form simplex equiangular tight frame (NC2), ensuring features are maximally distant from each other relative to their norm.
- **Core assumption:** Unconstrained features model holds sufficiently well in practical deep networks.
- **Evidence anchors:** Abstract states BCE can maximize intra-class compactness and inter-class distinctiveness; Section 3.2 establishes conditions under which BCE global minimizers satisfy NC properties.

## Foundational Learning

- **Concept: Neural Collapse (NC)**
  - **Why needed here:** Central theoretical framework for evaluating "good" feature learning in the paper.
  - **Quick check question:** Can you define the difference between NC1 (variability collapse) and NC2 (convergence to simplex ETF)?

- **Concept: Decision Scores (Logits)**
  - **Why needed here:** Core distinction between CE and BCE lies in how they treat logits - relative probabilities (Softmax) vs. independent binary signals (Sigmoid).
  - **Quick check question:** How does the gradient for a positive decision score differ in BCE (absolute) versus CE (relative) when the score is already high?

- **Concept: Regularization (Weight Decay)**
  - **Why needed here:** Theoretical proofs rely heavily on weight decay parameters (λ_W, λ_H, λ_b) to ensure loss minimum is finite and features don't explode.
  - **Quick check question:** Why does removing weight decay on the bias (λ_b) prevent CE from effectively learning a decision boundary offset, while BCE adapts?

## Architecture Onboarding

- **Component map:** Input -> Backbone (ResNet/DenseNet) -> Feature h (Penultimate layer) -> Linear Classifier (W, b) -> BCE Loss (K independent binary tasks)
- **Critical path:** Switch from Softmax (CE) to Sigmoid (BCE) on final layer, treating multi-class problem as K independent binary classifications (one-vs-all) rather than single multivariate distribution.
- **Design tradeoffs:** BCE converges to NC states faster in early epochs (first 20); may require careful batch size tuning as batch size affects numerical values of decision scores.
- **Failure signatures:** Zero Bias in CE (biases remain at initialization values); Low Uniform Accuracy (high standard accuracy but low uniform accuracy indicating non-compact features).
- **First 3 experiments:**
  1. **Metric Validation:** Train ResNet18 on CIFAR10 with both losses and plot NC1/NC2/NC3 metrics over epochs to verify faster BCE convergence.
  2. **Bias Ablation:** Visualize distribution of final classifier biases and decision scores (Fig 3 style) to confirm BCE biases separate scores while CE biases stay static.
  3. **Uniform Accuracy Test:** Compare standard accuracy vs. Uniform Accuracy (A_Uni) on test set to quantify improvement in feature distinctiveness.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does BCE effectively increase the threshold of class imbalance required to induce minority collapse compared to CE? The paper provides initial empirical evidence on CIFAR100-LT but presents this as speculation rather than formally proven characteristic.

- **Open Question 2:** Why does uniform classification accuracy (A_Uni) fail to generalize to test datasets in scenarios where standard classification accuracy generalizes well? The paper observes this phenomenon in certain SGD-trained models but lacks theoretical explanation for why learned decision score margins don't transfer to test set.

- **Open Question 3:** Do theoretical guarantees of Neural Collapse under BCE hold in deep networks with constrained feature dimensions (K > d)? Current theoretical proof assumes d ≥ K-1, leaving the many-class scenario theoretically unverified for BCE.

## Limitations

- Theoretical framework relies on unconstrained features model and linear classifiers, which may not fully capture deep networks with non-linear activations and batch normalization.
- BCE formulation requires careful implementation as multi-label problem rather than standard multi-class setup; incorrect implementation could invalidate results.
- Proofs assume weight decay parameters are sufficiently large to ensure finite global minima, but practical values may vary across architectures and datasets.

## Confidence

- **High Confidence:** Empirical observation that BCE consistently achieves higher classification accuracy and uniform accuracy across multiple architectures and datasets with clearly specified experimental setup.
- **Medium Confidence:** Theoretical proof that BCE induces Neural Collapse properties (NC1 and NC2) under unconstrained features model - mathematical derivations appear sound but applicability to practical deep networks requires further validation.
- **Medium Confidence:** Mechanism explanation that BCE biases provide substantial constraints on decision scores while CE biases have minimal effect - experimental evidence shows pattern but underlying optimization dynamics may be more complex.

## Next Checks

1. **Bias Behavior Verification:** Train ResNet18 on CIFAR-10 with both losses and plot classifier bias distributions over training epochs to confirm BCE biases separate positive/negative score distributions while CE biases remain near initialization values.

2. **NC Metric Convergence:** Implement NC1, NC2, and NC3 metric calculations from supplementary material and verify BCE converges to these states faster than CE during training, particularly in early epochs (first 20).

3. **Feature Geometry Analysis:** Using trained models, visualize decision score distributions for BCE (clear separation between positive and negative scores) versus CE (relative ranking without absolute separation), and compute/compare feature compactness (E_com) and distinctiveness (E_dis) metrics between the two losses.