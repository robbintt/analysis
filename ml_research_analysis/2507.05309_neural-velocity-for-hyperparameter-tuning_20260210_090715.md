---
ver: rpa2
title: Neural Velocity for hyperparameter tuning
arxiv_id: '2507.05309'
source_url: https://arxiv.org/abs/2507.05309
tags:
- uni00000013
- learning
- training
- velocity
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NeVe addresses hyperparameter tuning challenges in deep learning
  by introducing neural velocity as a novel convergence metric. This approach enables
  dynamic learning rate adjustment and stopping criteria without requiring a held-out
  validation set, solving the problem of limited training data in small-data scenarios.
---

# Neural Velocity for hyperparameter tuning

## Quick Facts
- **arXiv ID:** 2507.05309
- **Source URL:** https://arxiv.org/abs/2507.05309
- **Reference count:** 40
- **Primary result:** NeVe achieves 69.18% accuracy on CIFAR-100 with ResNet-32 without using validation data

## Executive Summary
NeVe introduces neural velocity as a novel convergence metric for hyperparameter tuning in deep learning. By measuring how rapidly neurons change their input-output behavior during training, NeVe enables dynamic learning rate adjustment and stopping criteria without requiring a held-out validation set. This addresses the challenge of limited training data in small-data scenarios by maximizing data utilization while maintaining or improving model performance.

The method demonstrates state-of-the-art performance across CIFAR-10, CIFAR-100, and ImageNet-100 datasets, achieving competitive accuracy compared to validation-loss-based approaches. By eliminating the need for validation sets, NeVe provides a validation-free alternative for adaptive hyperparameter tuning that is robust across different optimizers and scales to complex architectures including Swin Transformer V2.

## Method Summary
NeVe computes neural velocity by measuring the rate of change in neuron transfer functions between epochs using normalized outputs and correlation-based similarity. The method uses random noise samples as auxiliary data to estimate velocity without requiring validation images. When velocity plateaus for a specified patience period, learning rate is reduced by a factor α. Training stops when velocity drops below a threshold ε. The approach works across different optimizers (SGD and Adam) and architectures, with experiments showing competitive performance compared to baseline training procedures and validation-loss-based methods.

## Key Results
- Achieves 69.18% accuracy on CIFAR-100 with ResNet-32 without validation data
- Eliminates need for validation sets while maintaining competitive performance
- Robust across optimizers (SGD and Adam) and scales to complex architectures
- Outperforms baseline training procedures and validation-loss-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Neural velocity serves as a proxy for model convergence by measuring the rate of change in neuron transfer functions between epochs.
- **Mechanism:** For each neuron, normalized outputs are compared across consecutive epochs using a correlation-based change rate (ρ̂). The velocity v_t = |(1 - ρ̂^t) - μ_vel · v^(t-1)| captures how much a neuron's input-output behavior has shifted. High velocity indicates ongoing learning; values approaching zero indicate the neuron has reached a stable function.
- **Core assumption:** The rate of change in neuron outputs correlates with the model's distance from convergence, regardless of whether validation loss would show the same signal.
- **Evidence anchors:**
  - [abstract] "The neural velocity measures the rate of change of each neuron's transfer function and is an indicator of model convergence"
  - [section III-A] Eq. (1-2) define the velocity computation; "when the neural velocity has settled close to zero, it has reached convergence despite it is potentially still changing parameters"
  - [corpus] Weak direct corpus support for velocity-convergence link; related work on "Neuronal Fluctuations" (arXiv:2511.10435) discusses parameter fluctuations during training but does not validate this specific metric.
- **Break condition:** If velocity drops to near-zero while training loss is still decreasing significantly, the correlation assumption fails—the model is still learning but velocity no longer captures it.

### Mechanism 2
- **Claim:** Velocity plateaus predict performance plateaus, enabling adaptive learning rate decay without validation monitoring.
- **Mechanism:** When model velocity (average across all neurons) stabilizes to a value k for δ epochs of patience, the model is in a "meta-stable" state. At this point, learning rate is multiplied by factor α (< 1), allowing finer-grained optimization. This mimics validation-loss-based schedulers but uses internal dynamics instead.
- **Core assumption:** Velocity plateaus caused by high learning rates (stochastic noise-induced) correspond to loss plateaus that would benefit from rate reduction.
- **Evidence anchors:**
  - [section III-A] "a velocity plateau allows detecting and coping with performance plateaux early in the learning process by lowering the learning rate"
  - [section IV-C, Fig. 7] Shows correlation between velocity plateau onset (~epoch 20) and validation loss stabilization
  - [corpus] AdaLRS (arXiv:2506.13274) proposes loss-guided adaptive LR search; similar adaptive principle but uses loss directly rather than velocity.
- **Break condition:** If velocity plateaus prematurely due to architectural factors (e.g., ReLU dead units) rather than optimization dynamics, learning rate may decay too early, underfitting.

### Mechanism 3
- **Claim:** Random noise inputs suffice as auxiliary data for velocity estimation, eliminating the need for held-out validation samples.
- **Mechanism:** Velocity is computed by forwarding samples through the network and measuring output changes. The paper shows that Gaussian noise samples (|D_aux| = 100) produce velocity signals comparable to actual validation images because velocity measures internal function change, not task-specific accuracy.
- **Core assumption:** Neuron transfer function changes are distributed such that random inputs probe the relevant regions of the function space without requiring task-relevant data.
- **Evidence anchors:**
  - [section IV-A, Fig. 4b] "computing model velocity on random noise yields results comparable to using a validation set, even when considering a small size for D_RND"
  - [section IV-D] "we believe this is due to the distribution of the tested noise, which allows a more 'task agnostic' estimation of the velocity"
  - [corpus] No direct corpus validation of noise-based auxiliary data for hyperparameter tuning; this appears novel.
- **Break condition:** If neuron activations are highly sparse or input-domain-specific (e.g., specialized pathways activated only by certain image features), random noise may not probe the relevant function regions, yielding uninformative velocity estimates.

## Foundational Learning

- **Concept: Correlation-based similarity between vectors**
  - **Why needed here:** The change rate ρ̂^t_i (Eq. 1) is computed as a correlation between normalized neuron outputs at epochs t and t-1. Understanding how correlation measures similarity is essential.
  - **Quick check question:** Given two normalized output vectors [0.8, 0.2] and [0.7, 0.3], would their correlation be positive, zero, or negative?

- **Concept: Early stopping and generalization**
  - **Why needed here:** NeVe's stopping criterion (v < ε) is a validation-free alternative to traditional early stopping based on validation loss increase.
  - **Quick check question:** Why does continuing to train after the optimal point typically reduce test accuracy even when training loss keeps decreasing?

- **Concept: Learning rate schedules and plateau detection**
  - **Why needed here:** NeVe replaces hand-crafted LR schedules (step decay, cosine annealing) with velocity-based adaptive reduction.
  - **Quick check question:** If learning rate is too high near convergence, what typically happens to the loss curve?

## Architecture Onboarding

- **Component map:**
  - Training loop -> VelocityTracker (hooks) -> VelocityBuffer (stores outputs) -> PlateauDetector (monitors velocity) -> LRScheduler (adjusts LR) -> StoppingCriterion (checks ε)

- **Critical path:**
  1. Before training: Initialize model, forward D_aux (random noise), store normalized outputs
  2. Each epoch end: Forward D_aux, compute per-neuron ρ̂ via correlation with stored outputs, compute v_i, average to model velocity
  3. Check plateau → potentially reduce LR
  4. Check stopping threshold → potentially halt training
  5. Update buffer with current epoch outputs

- **Design tradeoffs:**
  - **|D_aux| size:** Paper uses 100 noise samples. Smaller = faster but noisier velocity estimate; larger = more stable but higher compute.
  - **ε (stopping threshold):** Smaller ε = train longer, risk overfitting; larger ε = stop earlier, risk underfitting. Paper recommends 10⁻³ based on softmax output analysis.
  - **δ (patience):** Low δ = frequent LR changes, risk of converging to poor local minima; high δ = slower adaptation.
  - **α (LR decay factor):** Low α = aggressive decay; high α = conservative adjustment.

- **Failure signatures:**
  - Velocity never drops below ε → training runs indefinitely (ε too small or learning dynamics incompatible)
  - Early stopping at epoch < 10 with poor accuracy → ε too large
  - LR decays too frequently causing underfitting → δ too small
  - Velocity fluctuates wildly → check normalization; ReLU networks need μ_vel = 0.5 smoothing

- **First 3 experiments:**
  1. **Reproduce CIFAR-10 baseline:** ResNet-32, |D_aux| = 100 Gaussian noise images, ε = 10⁻³, δ = 10, α = 0.1. Verify stopping epoch and final accuracy match reported ~93%.
  2. **Ablate ε sensitivity:** Run with ε ∈ {10⁻², 10⁻³, 10⁻⁴} on CIFAR-100/ResNet-32. Plot epochs trained vs. test accuracy to confirm 10⁻³ is near-optimal.
  3. **Test optimizer robustness:** Compare SGD vs. Adam on same task, confirming NeVe adapts stopping appropriately despite different convergence speeds (paper shows Adam stops ~8 epochs earlier with slightly lower final accuracy).

## Open Questions the Paper Calls Out

- **Can NeVe be extended to optimize other critical hyperparameters, such as momentum or weight decay, in addition to the learning rate?**
  - **Basis in paper:** [explicit] The authors state, "to the present date, it is not known whether NeVe can be exploited to optimize other hyperparameters, such as momentum or weight decay."
  - **Why unresolved:** The current study restricts the application of neural velocity to dynamic learning rate adjustment and stopping criteria only.
  - **What evidence would resolve it:** Successful integration of velocity-based metrics into the update rules for momentum or weight decay that results in improved convergence or accuracy.

- **Can the manual learning rate rescaling factor α be eliminated by directly linking velocity convergence to validation loss plateaus?**
  - **Basis in paper:** [explicit] The paper notes, "This could allow us to tune the learning rate without needing the parameter α; however, more investigations are required."
  - **Why unresolved:** The relationship between velocity convergence and validation loss is currently a hypothesis supported by empirical trend correlation but not a fully automated mechanism.
  - **What evidence would resolve it:** A modified NeVe algorithm that achieves state-of-the-art performance without requiring the manual specification of α.

- **Does NeVe maintain its efficiency and robustness when applied to non-computer vision tasks, such as training on tabular data?**
  - **Basis in paper:** [explicit] The authors list among future works the intent to "conduct a comprehensive set of experiments... as well as domains beyond computer vision, such as tabular data."
  - **Why unresolved:** All reported experimental results are confined to image classification datasets (CIFAR and ImageNet) using CNN and Transformer architectures.
  - **What evidence would resolve it:** Benchmark results on tabular or NLP datasets showing NeVe performing comparably to or better than validation-loss-based methods.

## Limitations

- The paper claims validation-free tuning but still requires 100 auxiliary samples and 4 hyperparameters (ε, δ, α, μ_vel), representing a shift rather than elimination of tuning complexity
- No comparison to existing validation-free methods like Auto-Curricula (arXiv:2301.05734) or plateau-based stopping in the same experimental framework
- The mechanism connecting velocity to convergence is plausible but lacks theoretical grounding or ablation showing which layers/neurons drive the signal
- ImageNet-100 results show minimal improvement over baseline despite being a harder dataset, raising questions about scalability

## Confidence

- **High confidence:** NeVe can stop training without validation loss and achieve competitive accuracy on CIFAR-10/100
- **Medium confidence:** Random noise suffices as auxiliary data for velocity estimation
- **Low confidence:** The velocity metric reliably predicts meta-stable states and optimal stopping points across architectures beyond those tested

## Next Checks

1. **Ablation on auxiliary data quality:** Compare velocity-based stopping using random noise, validation images, and training images on CIFAR-100 to verify noise sufficiency claim
2. **Scaling test on diverse architectures:** Apply NeVe to ViT and MLP-Mixer architectures beyond ResNet and Swin Transformer to assess generalizability
3. **Theoretical validation:** Analyze whether velocity distributions differ meaningfully between converged and underfitted states across multiple runs to test if the metric captures the claimed convergence signal