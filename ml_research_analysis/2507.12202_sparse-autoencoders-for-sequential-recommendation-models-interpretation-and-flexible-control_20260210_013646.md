---
ver: rpa2
title: 'Sparse Autoencoders for Sequential Recommendation Models: Interpretation and
  Flexible Control'
arxiv_id: '2507.12202'
source_url: https://arxiv.org/abs/2507.12202
tags:
- features
- feature
- genre
- recommendations
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies sparse autoencoders (SAE) to sequential recommendation
  models to enhance interpretability and enable flexible control. The authors train
  a transformer-based recommendation model and use SAE to extract interpretable features
  from its hidden states.
---

# Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control

## Quick Facts
- arXiv ID: 2507.12202
- Source URL: https://arxiv.org/abs/2507.12202
- Reference count: 40
- Key outcome: This paper applies sparse autoencoders (SAE) to sequential recommendation models to enhance interpretability and enable flexible control.

## Executive Summary
This paper introduces sparse autoencoders (SAE) to sequential recommendation models, enabling interpretable feature extraction and flexible model control. The authors train a transformer-based recommendation model and use SAE to decompose its hidden states into sparse, monosemantic features. By correlating these features with item attributes like genres, they demonstrate significantly improved interpretability compared to raw transformer neurons. The learned features can be used to steer recommendations toward or away from specific attributes, achieving performance comparable to supervised methods while maintaining unsupervised learning benefits.

## Method Summary
The method involves training a transformer-based sequential recommender (GPTRec) on user-item interaction sequences, then extracting activations from a target layer to train SAEs. The SAEs decompose these activations into sparse feature representations using L1 regularization. Interpretability is evaluated by correlating SAE features with item attributes using Pearson correlation, ROC AUC, and sensitivity metrics. For control, the method modifies SAE feature activations during inference and decodes them back to the original space to replace the original layer output, thereby steering recommendations.

## Key Results
- SAE features show significantly higher correlations with item genres compared to original transformer neurons, demonstrating improved interpretability
- Feature steering interventions effectively control recommendation behavior, increasing or decreasing attribute-specific recommendations as desired
- The method achieves comparable control performance to supervised linear probing while maintaining unsupervised learning benefits
- SAE features demonstrate more monosemantic behavior than original neurons, with individual features strongly correlating with specific attributes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse Autoencoders (SAE) decompose polysemantic transformer hidden states into monosemantic, interpretable features.
- Mechanism: An autoencoder is trained on hidden state activations from a transformer layer. An L1 penalty on the latent representation forces the model to reconstruct the original activation using only a small subset of its "features" (decoder neurons). Each feature is a direction in activation space.
- Core assumption: The Linear Representation Hypothesis holds, where interpretable concepts are represented linearly in the activation space, and superposition can be disentangled with sparsity.
- Evidence anchors: [abstract] "These autoencoders learn to reconstruct hidden states... from sparse linear combinations of directions in their activation space." [section 3] Describes the SAE formulation with L1 penalty: `L(x) = ||x - x̂||² + λ||h(x)||₁`.

### Mechanism 2
- Claim: Learned SAE features causally influence downstream model predictions, enabling control via feature steering.
- Mechanism: After identifying an SAE feature that correlates with a target attribute (e.g., Sci-Fi genre), its activation can be artificially set during inference. The modified SAE latent representation is decoded, and this reconstructed activation replaces the original transformer layer's output, altering the model's final recommendations.
- Core assumption: The SAE has successfully learned a feature that the base model uses for prediction, and the reconstruction is sufficiently faithful to not corrupt other model computations.
- Evidence anchors: [abstract] "...features learned by SAE can be used to effectively and flexibly control the model's behavior, providing end-users with a straightforward method to adjust their recommendations..."

### Mechanism 3
- Claim: Interpretability of SAE features can be quantitatively evaluated using external item attributes.
- Mechanism: In recommendation systems, items possess ground-truth attributes (e.g., movie genres). The correlation between an SAE feature's activation and these binary attributes is measured using metrics like Pearson correlation and ROC AUC.
- Core assumption: The item attributes provided in the dataset are relevant to the model's internal decision-making process and should be learnable from user interaction sequences.
- Evidence anchors: [abstract] "The authors introduce metrics to evaluate SAE interpretability by correlating learned features with item attributes like genres." [section 4.2] "We compute the following metrics for each pair of SAE features and item attributes: Pearson correlation coefficient... ROC AUC... Sensitivity..."

## Foundational Learning

### Concept: Polysemanticity & Superposition
- Why needed here: This is the core problem SAEs are designed to solve. Single neurons in a transformer layer encode multiple concepts, making them uninterpretable.
- Quick check question: Why can't you simply interpret a transformer's hidden state neurons directly?

### Concept: Linear Representation Hypothesis (LRH)
- Why needed here: The SAE approach is predicated on the assumption that concepts are represented as directions in activation space.
- Quick check question: What is the core assumption about how neural networks store concepts that allows SAEs to work?

### Concept: Steering / Activation Interventions
- Why needed here: The practical utility demonstrated in the paper is based on modifying model behavior at inference time.
- Quick check question: After modifying an SAE feature's activation, what must you do before passing the data to the next transformer layer?

## Architecture Onboarding

### Component map:
Base Recommender Model -> SAE Encoder (h(x) = σ(Wx + b)) -> SAE Decoder (x̂ = Wᵀh(x) + b') -> Intervention Module (modify h(x) → decode → replace layer output)

### Critical path:
1. Train the base recommender model on user-item sequences
2. Extract activations from a chosen layer to create a dataset for the SAE
3. Train the SAE on these activations, optimizing for reconstruction and sparsity
4. Analyze features by correlating their activations with item attributes to find interpretable directions
5. For control: During inference, run forward pass to target layer, encode into SAE space, modify a feature, decode, and continue forward pass

### Design tradeoffs:
- **Reconstruction vs. Sparsity (L1 Penalty)**: Higher L1 penalty increases sparsity and potentially interpretability but degrades reconstruction quality, which can harm task performance
- **Dictionary Size**: Larger dictionary offers more capacity but is harder to train and analyze; stability suggested above 1024 features
- **Layer Choice**: Earlier layers may capture more generic features, while later layers capture task-specific ones

### Failure signatures:
- **Collapse**: L1 penalty too high, L0 goes to near zero, reconstruction fails completely, model outputs become garbage
- **Triviality**: L1 penalty too low, SAE learns identity-like mapping, features remain polysemantic
- **Dead Features**: Large portions of SAE dictionary never activate, indicating issues with initialization or learning rate

### First 3 experiments:
1. **Reconstruction Ablation**: Vary L1 penalty and measure impact on reconstruction error (RMSE