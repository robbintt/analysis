---
ver: rpa2
title: Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream
  Fine-tuning
arxiv_id: '2507.16302'
source_url: https://arxiv.org/abs/2507.16302
tags:
- fine-tuning
- diffusion
- resalign
- harmful
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the fragility of safety-driven unlearning
  in diffusion models under downstream fine-tuning, where existing methods fail to
  retain effectiveness even on benign data. To mitigate this, the authors propose
  ResAlign, a framework that explicitly minimizes the recovery of harmful behaviors
  during simulated fine-tuning via a Moreau envelope-based approximation and a meta-learning
  strategy over diverse adaptation scenarios.
---

# Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning

## Quick Facts
- arXiv ID: 2507.16302
- Source URL: https://arxiv.org/abs/2507.16302
- Reference count: 40
- This paper proposes ResAlign to enhance safety-driven unlearning resilience against downstream fine-tuning in diffusion models.

## Executive Summary
This paper addresses the fragility of safety-driven unlearning in diffusion models when subjected to downstream fine-tuning, where existing methods fail to retain effectiveness even on benign data. The authors propose ResAlign, a framework that explicitly minimizes the recovery of harmful behaviors during simulated fine-tuning via a Moreau envelope-based approximation and a meta-learning strategy over diverse adaptation scenarios. Experiments show ResAlign consistently outperforms baselines in retaining safety after fine-tuning while preserving benign generation quality across datasets, fine-tuning methods, and model architectures.

## Method Summary
ResAlign enhances unlearning resilience by treating downstream fine-tuning as an implicit optimization problem solved via Moreau envelope approximation, enabling efficient hypergradient estimation through implicit differentiation. A meta-learning outer loop simulates diverse fine-tuning scenarios (different learning rates, optimizers, step counts) to improve generalization across unseen configurations. The method combines standard unlearning objectives with a resilience term that penalizes harmful behavior recovery during simulated fine-tuning, optimizing for safety retention under parameter perturbations.

## Key Results
- ResAlign achieves significantly lower Inappropriate Rates (IP) after fine-tuning compared to baselines like ESD and AdvUnlearn across multiple datasets
- The method maintains benign generation quality with minimal FID and CLIP score degradation
- ResAlign demonstrates resilience against both benign and malicious fine-tuning scenarios, including harmful data contamination
- Theoretical insights suggest ResAlign encourages flatter loss landscapes, reducing sensitivity to downstream updates

## Why This Works (Mechanism)

### Mechanism 1
ResAlign reduces post-fine-tuning harmfulness recovery by approximating the effect of downstream fine-tuning as a Moreau envelope optimization, enabling efficient gradient estimation through implicit differentiation. Instead of backpropagating through the entire fine-tuning trajectory (computationally prohibitive), ResAlign treats the fine-tuned model parameters as the minimizer of a proximal-regularized objective and uses the implicit function theorem to compute hypergradients via a simple linear system solve (Richardson iteration). The core assumption is that downstream fine-tuning on diffusion models typically involves small parameter shifts, making the Moreau envelope a valid approximation of the true fine-tuning dynamics.

### Mechanism 2
ResAlign generalizes resilience across diverse, unseen fine-tuning configurations by sampling from a distribution of simulated fine-tuning scenarios during meta-training. A meta-learning outer loop aggregates hypergradients from multiple inner-loop adaptations, each with randomly sampled fine-tuning hyperparameters (learning rate, optimizer, steps, loss) and data batches. This prevents overfitting to a single proxy configuration. The core assumption is that the distribution of fine-tuning configurations used during meta-learning is sufficiently representative of the distribution encountered at deployment.

### Mechanism 3
ResAlign may encourage convergence to flatter regions of the harmful loss landscape, reducing sensitivity to parameter perturbations from fine-tuning. The additional regularization term in the objective implicitly penalizes the trace of the Hessian of the harmful loss, which is associated with loss landscape flatness. The core assumption is that flatter minima in the harmful loss landscape translate to more stable safety under parameter perturbations; the second-order Taylor expansion used in the proposition is valid for typical fine-tuning perturbations.

## Foundational Learning

- **Concept**: Moreau Envelope and Proximal Operators
  - **Why needed here**: The core efficiency gain in ResAlign comes from reformulating fine-tuning as a proximal-regularized optimization, which avoids unrolling through time.
  - **Quick check question**: Can you explain why the Moreau envelope allows one to treat the output of a multi-step optimization as a single implicit function of the initial parameters?

- **Concept**: Bilevel Optimization and Hypergradients
  - **Why needed here**: The problem is structured as a bilevel optimization: the outer loop (unlearning) depends on the solution of an inner loop (fine-tuning). Understanding hypergradient estimation via implicit differentiation is essential.
  - **Quick check question**: Why is computing the hypergradient $\nabla_\theta L(\theta^*_{FT})$ challenging, and how does the implicit function theorem provide a solution?

- **Concept**: Meta-Learning for Distributional Robustness
  - **Why needed here**: The method uses meta-learning not for few-shot adaptation, but to average over a distribution of tasks (fine-tuning configurations) to achieve robustness.
  - **Quick check question**: How does sampling multiple fine-tuning configurations during meta-training differ from training on a single, fixed proxy configuration?

## Architecture Onboarding

- **Component map**: Base model $\theta$ -> Hypergradient Computation Module (Algorithm 1) -> Meta-Training Loop (Algorithm 2) -> Final unlearned model $\theta_I$
- **Critical path**: Initialize from pretrained model $\theta_0$ -> For each outer iteration: compute standard unlearning gradient, then for $J$ inner iterations: sample $D_{FT}$ and $C$, adapt model to get $\theta^*_{FT}$, compute hypergradient via implicit differentiation, aggregate -> Update $\theta$ using aggregated gradients -> Output final unlearned $\theta_I$
- **Design tradeoffs**:
  - Accuracy vs. Efficiency: Increasing the number of Richardson iterations $K$ improves hypergradient accuracy but costs more compute
  - Generalization vs. Overfitting: A larger and more diverse meta-learning pool improves generalization but increases training time and complexity
  - Resilience vs. Utility: Stronger emphasis on the resilience term (higher $\beta$) may harm benign generation quality
- **Failure signatures**:
  1. High IP/US after fine-tuning: Indicates insufficient resilience; may be due to a poor meta-learning pool or too-small $\beta$
  2. Degraded benign generation (high FID, low CLIP/Aesthetics): Utility loss too high; may require adjusting $\alpha$ or the preservation set $D_{preserve}$
  3. Training instability (loss spikes): Can occur if the meta-learning pool includes extreme configurations (e.g., very high learning rate with many steps)
- **First 3 experiments**:
  1. Reproduce main result: Run ResAlign on SD v1.4 with default hyperparameters, evaluate IP/US and benign metrics before and after fine-tuning on DreamBench++ and DiffusionDB
  2. Ablate components: Remove meta-learning (sample single fixed $C$), remove Moreau envelope (use first-order approximation), and remove each to isolate their contribution, measuring IP after fine-tuning
  3. Test out-of-distribution fine-tuning: Fine-tune the unlearned model using a method not in the meta-training pool (e.g., SVDiff) on a novel dataset (e.g., ArtBench) and measure IP to assess generalization claims

## Open Questions the Paper Calls Out

### Open Question 1
How can unlearning loss functions be redesigned to achieve the high resilience of ResAlign without inducing visual artifacts (e.g., mosaics, distortions) or over-rejection of semantically safe prompts? While the paper demonstrates that ResAlign is compatible with ESD/CA losses, these combinations yield higher Inappropriate Rates (IP) compared to the GA version. The trade-off between maximizing resilience (via GA) and maintaining visual coherence/utility remains unsolved.

### Open Question 2
What are the theoretical limits of safety resilience against malicious fine-tuning before the model's benign generalization or adaptation capabilities are fundamentally compromised? The paper empirically shows ResAlign resists harmful recovery better than baselines, but the theoretical upper bound for this resistance—and its cost to benign plasticity—is not defined.

### Open Question 3
How robust is the Moreau envelope-based gradient approximation under fine-tuning regimes that involve large parameter shifts, such as high learning rates or extensive step counts? The paper validates ResAlign on standard fine-tuning schedules (small learning rates), but the efficiency and accuracy of the implicit gradient estimation are not verified for aggressive fine-tuning scenarios that violate the proximity assumption.

## Limitations

- The core assumption that downstream fine-tuning involves only small parameter shifts may break down with aggressive fine-tuning or novel adaptation methods not well-represented in the meta-learning distribution
- The theoretical justification for flat minima is qualitative and based on second-order approximations that may not hold for all fine-tuning scenarios
- The method's generalization to other diffusion architectures (e.g., SDXL) or completely different downstream tasks remains untested

## Confidence

- **High Confidence**: Claims about ResAlign outperforming baselines on the tested SD v1.4 configuration with the specified datasets and metrics (IP/US after fine-tuning, FID/CLIP preservation)
- **Medium Confidence**: Claims about meta-learning providing distributional robustness across fine-tuning configurations, as the generalization bounds and sensitivity to meta-learning pool composition are not rigorously quantified
- **Low Confidence**: Theoretical claims about flatter loss landscapes translating to improved resilience under arbitrary fine-tuning perturbations, as this is primarily supported by Proposition 1 and qualitative discussion rather than comprehensive empirical validation

## Next Checks

1. Test Generalization to Novel Fine-tuning Methods: Fine-tune the ResAlign-learned model using a method not in the original meta-training pool (e.g., SVDiff) on a novel dataset (e.g., ArtBench) and measure IP to validate robustness claims

2. Analyze Sensitivity to Meta-Learning Pool: Systematically vary the diversity and size of the fine-tuning configuration distribution π(C) during meta-training and measure the resulting resilience (IP after fine-tuning) to quantify the trade-off between generalization and overfitting

3. Validate Theoretical Claims Empirically: For the final ResAlign model, empirically measure the trace of the Hessian of the harmful loss and compare it to baseline models to test the proposed connection between flatness and resilience