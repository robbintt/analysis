---
ver: rpa2
title: 'GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted
  Scene Confusion'
arxiv_id: '2504.20829'
source_url: https://arxiv.org/abs/2504.20829
tags:
- attack
- viewpoints
- backdoor
- viewpoint
- ssim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GaussTrap, the first backdoor attack targeting
  3D Gaussian Splatting (3DGS) models. It proposes a three-stage poisoning pipeline
  (attack, stabilization, normal training) to embed malicious views at specific viewpoints
  while preserving high-quality rendering elsewhere.
---

# GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion

## Quick Facts
- arXiv ID: 2504.20829
- Source URL: https://arxiv.org/abs/2504.20829
- Authors: Jiaxin Hong; Sixu Chen; Shuoyang Sun; Hongyao Yu; Hao Fang; Yuqi Tan; Bin Chen; Shuhan Qi; Jiawei Li
- Reference count: 40
- One-line primary result: First backdoor attack targeting 3D Gaussian Splatting models with high stealth and targeted scene confusion

## Executive Summary
This paper introduces GaussTrap, a novel backdoor attack targeting 3D Gaussian Splatting (3DGS) models that embed malicious views at specific viewpoints while preserving normal rendering quality elsewhere. The attack operates through a three-stage poisoning pipeline (attack, stabilization, normal training) that injects poisoned data during training to manipulate scene rendering from predetermined viewpoints. The method achieves high-quality rendering on attack viewpoints (PSNR > 36 dB, SSIM > 0.98) while maintaining visual quality on normal views, making it particularly stealthy compared to existing approaches.

## Method Summary
GaussTrap employs a three-stage poisoning pipeline: first, it injects malicious views during the attack stage to embed the backdoor; second, it uses Viewpoint Ensemble Stabilization (VES) to smooth transitions between attack and normal views and stabilize the malicious content; finally, it continues normal training to ensure the poisoned model maintains high-quality rendering on non-attack viewpoints. The VES mechanism is critical for maintaining stealth by preventing visual artifacts at viewpoint boundaries. The attack assumes white-box access to the training pipeline and loss functions, allowing precise manipulation of the 3D Gaussian representation during optimization.

## Key Results
- Achieves PSNR > 36 dB and SSIM > 0.98 on attack viewpoints while maintaining high quality on normal views
- Outperforms baseline IPA-NeRF+3DGS in both attack effectiveness and visual stealth
- Successfully demonstrates attack effectiveness on both synthetic (Blender) and real-world (MipNeRF-360) datasets

## Why This Works (Mechanism)
The attack exploits the 3D Gaussian Splatting training process by strategically injecting poisoned views that the model must learn to render correctly. During the stabilization phase, VES ensures smooth interpolation between malicious and benign viewpoints, preventing detection through visual artifacts. The three-stage approach allows the model to first learn the malicious content, then stabilize it within the existing Gaussian representation, and finally blend it seamlessly with normal training data. This method takes advantage of the fact that 3DGS models are trained to minimize rendering loss across all viewpoints, allowing carefully crafted poisoned views to persist while maintaining overall model performance.

## Foundational Learning
- **3D Gaussian Splatting**: A rasterization-based neural rendering technique that represents scenes using millions of anisotropic Gaussians - needed for understanding the target attack surface and optimization objectives
- **Backdoor attacks in machine learning**: Techniques that embed hidden behaviors during training that activate under specific conditions - needed to understand the attack paradigm and threat model
- **Viewpoint ensemble stabilization**: Methods for ensuring smooth transitions between different camera viewpoints in 3D rendering - needed to understand how VES prevents detection through visual artifacts
- **Poisoning attacks vs. evasion attacks**: Training-time vs. inference-time adversarial techniques - needed to distinguish this attack from more common evasion-based approaches
- **White-box attack assumptions**: Scenarios where attackers have full knowledge of the target system - needed to understand the attack's feasibility constraints
- **3DGS optimization objectives**: The loss functions and rendering metrics used to train Gaussian splatting models - needed to understand how the attack manipulates the training process

## Architecture Onboarding

Component map: Poisoned data injection -> Three-stage training pipeline -> VES stabilization -> 3DGS model optimization

Critical path: Attack stage (inject malicious views) → Stabilization stage (apply VES to smooth transitions) → Normal training stage (blend with benign data) → Final poisoned model

Design tradeoffs: The three-stage approach trades increased training complexity for better stealth and effectiveness compared to single-stage poisoning. VES adds computational overhead but is essential for preventing detection through visual discontinuities.

Failure signatures: If VES is insufficient, visual artifacts appear at viewpoint boundaries; if poisoning is too aggressive, overall rendering quality degrades on normal views; if attack stage is too weak, the backdoor fails to activate.

First experiments to run:
1. Test attack effectiveness on a simple synthetic scene with known ground truth to verify backdoor activation
2. Evaluate visual quality metrics (PSNR/SSIM) across attack vs. normal viewpoints to quantify stealth
3. Perform ablation study removing VES to demonstrate its necessity for maintaining stealth

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications emerge: the generalizability of the attack to larger-scale scenes and diverse 3DGS applications, the potential for detection through statistical analysis of viewpoint transitions, and the effectiveness of potential defenses against such poisoning attacks.

## Limitations
- Relies on white-box access assumptions including knowledge of training pipeline and loss functions
- Security implications demonstrated only through controlled experiments, not real-world deployment scenarios
- Does not address potential countermeasures or feasibility of detecting such attacks in practice

## Confidence
- **High**: The proposed three-stage poisoning pipeline and VES mechanism are technically sound and well-documented
- **Medium**: The experimental results demonstrate effectiveness on the tested datasets, but real-world applicability is uncertain
- **Low**: The long-term security implications and robustness against defenses are not addressed

## Next Checks
1. Test GaussTrap on larger, more complex 3DGS scenes to evaluate scalability and generalization
2. Assess the attack's impact on downstream tasks (e.g., semantic segmentation or object detection) to quantify functional degradation
3. Investigate the feasibility of detecting GaussTrap through anomaly detection or adversarial defense mechanisms