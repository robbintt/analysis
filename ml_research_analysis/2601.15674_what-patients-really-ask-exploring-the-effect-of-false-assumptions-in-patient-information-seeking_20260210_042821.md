---
ver: rpa2
title: 'What Patients Really Ask: Exploring the Effect of False Assumptions in Patient
  Information Seeking'
arxiv_id: '2601.15674'
source_url: https://arxiv.org/abs/2601.15674
tags:
- questions
- question
- incorrect
- type
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Patients increasingly use LLMs to seek health information, but\
  \ benchmarks often focus on exam-style questions rather than real-world queries.\
  \ This work introduces a dataset of patient-generated medical questions collected\
  \ via Google\u2019s People Also Ask feature for 200 top U.S."
---

# What Patients Really Ask: Exploring the Effect of False Assumptions in Patient Information Seeking

## Quick Facts
- arXiv ID: 2601.15674
- Source URL: https://arxiv.org/abs/2601.15674
- Authors: Raymond Xiong; Furong Jia; Lionel Wong; Monica Agrawal
- Reference count: 22
- Primary result: 23.6% of real patient medical queries contain incorrect assumptions or dangerous intent, and LLMs struggle to detect these errors

## Executive Summary
This study introduces a novel dataset of 4,012 real patient medical questions collected from Google's People Also Ask feature for 200 top U.S. medications. The dataset reveals that 23.6% of queries contain incorrect assumptions or dangerous intent, with these corrupted questions propagating through query sequences based on prior question correctness. Top-performing models like GPT-5 achieve 91% accuracy on identifying flawed questions, while most other models fall below 50%, highlighting the significant gap between medical exam benchmarks and real-world patient queries. The findings emphasize the need for LLMs to better identify and address embedded misconceptions in patient information seeking beyond benchmark performance.

## Method Summary
The study collected 4,012 unique medical questions from Google's PAA feature using a randomized DFS traversal (max depth 10, 2 trials per medication query) for 200 top U.S. prescribed medications. Questions were classified into three categories using GPT-5 as an LLM-as-a-judge with few-shot prompting: Type A (incorrect assumptions), Type B (dangerous intent), and Type C (benign). A high-confidence subset (536 questions) was identified using dual-judge agreement between GPT-5 and GPT-5-mini. Ten LLMs were tested on this subset without system prompts, and responses were evaluated for correctness using GPT-5. Logistic regression and proportion Z-tests analyzed propagation patterns in query sequences.

## Key Results
- 23.6% of real patient medical queries contain incorrect assumptions or dangerous intent
- Top models like GPT-5 achieve 91% accuracy, while most others fall below 50% on identifying flawed questions
- Corrupted questions propagate through query histories, with prior incorrectness significantly increasing likelihood of subsequent errors (p < 0.001)
- 75% agreement between GPT-5 classifier and human labels for question classification, 85% for response evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Corrupted questions propagate through query sequences via dependency on prior incorrectness
- Mechanism: Questions in Google's PAA feature form tree-like trajectories; probability of current question being ill-formed increases when preceding questions contain similar flaws
- Core assumption: Markovian dependency — current question correctness depends on immediately preceding question and cumulative history
- Evidence anchors:
  - [abstract] "emergence of these corrupted questions is not uniformly random and depends heavily on the degree of incorrectness in the history of questions"
  - [section 3.2] OR 95% CI = [1.68, 3.16] for combined types; Table 4 shows 32.82% incorrect rate given incorrect previous Q vs 21.18% given correct previous Q (p < 2.269×10⁻¹²)

### Mechanism 2
- Claim: LLMs trained on exam-style benchmarks underperform on pragmatic error detection in naturalistic queries
- Mechanism: Medical benchmarks use unambiguous, well-formed questions with single correct answers, while real patient queries embed implicit misconceptions requiring both medical knowledge and pragmatic reasoning
- Core assumption: Benchmark performance transfers poorly to distribution shift of real-world patient language patterns
- Evidence anchors:
  - [abstract] "benchmarking efforts in LLMs for question answering often focus on medical exam questions, which differ significantly in style and content from the questions patients actually raise in real life"
  - [section 1] "exam-style questions are carefully designed to be unambiguous and admit a single answer, while real-world health queries are systematically different"

### Mechanism 3
- Claim: LLM-as-a-judge classification can achieve reasonable alignment with human labels for detecting question flaws
- Mechanism: Using GPT-5 with few-shot prompting and optimized prompts, automated classification of questions into Type A/B/C achieves 75% agreement with human labels for question classification and 85% for response evaluation
- Core assumption: Judge model's reasoning capabilities generalize sufficiently to the annotation task after prompt optimization on a labeled training set
- Evidence anchors:
  - [section 2.1] "we used gpt-5 as an automatic judge, and we used few-shot prompting to improve reliability"
  - [appendix D] "GPT classifier achieved 75% agreement with human labels [for questions]... 85% agreement with human labels [for responses]"

## Foundational Learning

- Concept: **Presupposition vs assertion**
  - Why needed here: Type A questions ("Why is metformin banned?") embed false premises as background assumptions rather than explicit claims
  - Quick check question: In "How do I flush statins from my body?", what is the presupposition that makes this potentially harmful?

- Concept: **Markov property in sequence modeling**
  - Why needed here: The paper's propagation analysis assumes current question depends on immediately preceding question
  - Quick check question: If query sequences exhibited longer-range dependencies, would the Markovian analysis under- or over-estimate propagation effects?

- Concept: **LLM-as-a-judge validation**
  - Why needed here: The entire classification pipeline relies on automated judging
  - Quick check question: If judge agreement were 60% instead of 75-85%, would the dataset be usable? What minimum threshold would you require?

## Architecture Onboarding

- Component map:
  - Medication list -> PAA scraper with randomized DFS traversal (depth ≤10, 2 trials) -> raw questions
  - Raw questions -> GPT-5 classification with few-shot prompts -> Type A/B/C labels
  - High-confidence subset (dual-judge agreement) -> LLM responses -> correctness evaluation
  - Full labeled dataset -> propagation analysis

- Critical path:
  1. Medication list → PAA queries → trajectory sampling → raw questions
  2. Raw questions → GPT-5 classification → Type A/B/C labels
  3. High-confidence subset → LLM responses → correctness evaluation
  4. Full labeled dataset → propagation analysis

- Design tradeoffs:
  - Depth cap of 10 improves sampling efficiency but may miss longer propagation chains
  - High-confidence filtering reduces false positives but discards ~17% of corrupted questions
  - No system prompts during LLM testing simulates naive users but underestimates potential with prompt engineering

- Failure signatures:
  - Models accepting false premises directly (e.g., listing methods to "flush statins")
  - Non-committal responses with generic disclaimers that don't actually correct the assumption
  - Open-source models collapsing to <30% accuracy on Type B (dangerous intent) questions

- First 3 experiments:
  1. Reproduce the propagation analysis: sample PAA trajectories for a subset of medications, classify questions, fit logistic regression to validate OR and CI ranges
  2. Test a single model (e.g., LLaMA3.1 70B) on the high-confidence subset with and without explicit system prompts instructing premise-checking; measure accuracy delta
  3. Extend depth limit to 15-20 on 5 medications to test whether propagation effects strengthen or weaken with longer trajectories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do similar patterns of incorrect assumption emergence arise in data drawn from individual patient search behavior compared to aggregated sources?
- Basis in paper: [explicit] Limitations section states, "Future work could investigate whether similar patterns of incorrect assumption emergence arise in data drawn from individual patient search behavior."
- Why unresolved: The current study relies entirely on Google's "People Also Ask" feature, which reflects aggregated search behavior rather than direct, longitudinal patient-authored narratives.

### Open Question 2
- Question: Can probabilistic labeling schemes or formal pragmatic frameworks reduce subjectivity in classifying underspecified medical questions?
- Basis in paper: [explicit] Limitations section notes that identifying incorrect assumptions involves subjective judgment and suggests future work could "incorporate probabilistic labeling schemes or develop more formal, pragmatic frameworks."
- Why unresolved: The current binary classification (Type A/B/C) struggles with the inherent ambiguity of natural language, potentially affecting reproducibility.

### Open Question 3
- Question: To what extent do the findings regarding incorrect assumption propagation generalize to non-English languages and other cultural contexts?
- Basis in paper: [explicit] Limitations section mentions the analysis "focused primarily on English-language questions and on medications commonly prescribed in the United States" and calls for extension to other regions.
- Why unresolved: Medical misinformation and the way users formulate "dangerous" or "ill-formed" questions may vary significantly based on cultural context and language structure.

### Open Question 4
- Question: What specific model architectures or training modifications are required to make LLMs inherently resistant to false presuppositions without prompt engineering?
- Basis in paper: [explicit] Conclusion urges "future research to... develop models that address these issues inherently rather than through external prompt engineering."
- Why unresolved: While the paper shows current models fail in vanilla settings, it does not explore how to train models to inherently distinguish these flawed queries without relying on input manipulation.

## Limitations
- Analysis focused primarily on English-language questions and U.S. medications, limiting generalizability to other regions
- Relies on aggregated search behavior rather than individual patient search patterns, which may differ
- Identifying incorrect assumptions involves subjective judgment, potentially affecting reproducibility

## Confidence

- **High**: Benchmark performance gap between exam-style questions and real-world patient queries is well-established with clear distribution shift explanation
- **Medium**: Propagation analysis relies on Markovian dependency assumption which may oversimplify real query dynamics
- **Medium**: LLM-as-a-judge approach shows reasonable agreement but validation process details are sparse and potential circularity concerns exist

## Next Checks

1. **Reproduce propagation analysis**: Collect PAA trajectories for a subset of medications using the same randomized DFS approach, classify questions using the provided prompt template, and fit logistic regression to validate the reported odds ratios (OR = 1.68-3.16) and confidence intervals.

2. **Validate LLM-as-a-judge reliability**: Create a manually labeled validation set of 100 questions (50 corrupted, 50 benign) and test the judge model's classification accuracy. Test edge cases with ambiguous presuppositions to assess robustness.

3. **Test prompt engineering impact**: Evaluate LLaMA3.1 70B on the high-confidence subset with and without explicit system prompts instructing premise-checking. Measure accuracy improvement to determine if naive testing underestimates model capabilities.