---
ver: rpa2
title: 'Trust, But Verify: A Self-Verification Approach to Reinforcement Learning
  with Verifiable Rewards'
arxiv_id: '2505.13445'
source_url: https://arxiv.org/abs/2505.13445
tags:
- verification
- rise
- reasoning
- self-verification
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of superficial self-reflection in
  reinforcement learning with verifiable rewards (RLVR), where models struggle to
  robustly verify their own outputs. The authors introduce RISE, a novel online RL
  framework that simultaneously trains LLMs to improve both problem-solving and self-verification
  abilities within a single integrated process.
---

# Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards

## Quick Facts
- arXiv ID: 2505.13445
- Source URL: https://arxiv.org/abs/2505.13445
- Reference count: 40
- Primary result: RISE framework achieves up to 2.8× increase in verification accuracy and +0.2% to +1.9% improvement over majority voting on mathematical reasoning benchmarks

## Executive Summary
This paper addresses the challenge of superficial self-reflection in reinforcement learning with verifiable rewards (RLVR), where language models struggle to robustly verify their own outputs. The authors propose RISE, an innovative online reinforcement learning framework that trains models to simultaneously improve both problem-solving and self-verification abilities through a single integrated process. By leveraging verifiable rewards from outcome verifiers to provide real-time feedback for both solution generation and self-verification tasks, RISE creates a dual-learning loop where the model generates solutions and critiques its own outputs in each iteration. Experiments on mathematical reasoning benchmarks demonstrate consistent improvements in problem-solving accuracy while fostering strong self-verification skills, with RISE models outperforming standard majority voting approaches.

## Method Summary
RISE introduces a novel online RL framework that simultaneously trains LLMs to enhance both problem-solving and self-verification capabilities within a unified process. The key innovation lies in using verifiable rewards from outcome verifiers to provide on-the-fly feedback for both solution generation and self-verification tasks. In each iteration, the model generates solutions and then critiques its own on-policy generated solutions, with both trajectories contributing to the policy update. This dual-training approach addresses the fundamental challenge in RLVR where models struggle to robustly verify their own outputs, creating a more effective self-reflection mechanism that improves both the quality of solutions and the ability to verify them.

## Key Results
- RISE achieves up to 2.8× increase in verification accuracy compared to Zero-RL baselines
- RISE models outperform standard majority voting by +0.2% to +1.9% under k=4 inference budget
- Consistent improvements in problem-solving accuracy across mathematical reasoning benchmarks while fostering strong self-verification skills

## Why This Works (Mechanism)
The RISE approach works by creating a feedback loop where verifiable rewards serve dual purposes: guiding solution generation and training self-verification capabilities simultaneously. By having the model critique its own outputs using the same reward signals used for solution optimization, RISE develops more robust internal verification mechanisms. This integrated approach prevents the superficial self-reflection problem common in RLVR, where models may generate plausible-sounding solutions without genuine verification capability. The on-policy generation and critique process ensures that the model learns to identify and correct its own errors, leading to both better solutions and more reliable self-assessment.

## Foundational Learning
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Why needed - Provides the framework for training with outcome-based feedback; Quick check - Verify that rewards are truly objective and computable
- **Self-verification in LLMs**: Why needed - Critical for reducing hallucinations and improving reliability; Quick check - Test if model can accurately identify its own correct/incorrect outputs
- **Online reinforcement learning**: Why needed - Enables real-time feedback and iterative improvement; Quick check - Monitor learning stability across iterations
- **Dual-task training**: Why needed - Simultaneously improves generation and verification capabilities; Quick check - Compare performance when tasks are trained separately vs. together
- **Verifiable reward signals**: Why needed - Provides objective ground truth for training; Quick check - Ensure reward computation is reliable and consistent

## Architecture Onboarding

**Component Map**: Problem Generator -> Solution Generator -> Self-Verifier -> Outcome Verifier -> Policy Updater -> Solution Generator

**Critical Path**: Problem generation → solution generation → self-verification → outcome verification → policy update → improved solution generation

**Design Tradeoffs**: RISE trades increased computational complexity for improved self-verification capabilities. The dual-training process requires more computation per iteration but develops stronger internal verification mechanisms. The approach may have higher training costs compared to standard RLVR, but the improved reliability could justify the overhead in safety-critical applications.

**Failure Signatures**: 
- Inconsistent verification performance across different problem types
- Degradation in solution quality when verification capability improves
- Overfitting to specific verification patterns rather than genuine understanding
- Computational bottlenecks in the dual-training loop

**3 First Experiments**:
1. Compare RISE against standard RLVR on mathematical reasoning benchmarks with varying problem complexity
2. Test RISE's generalization to non-mathematical domains requiring verification (e.g., code generation)
3. Conduct ablation studies removing either the self-verification or solution generation components

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation focused primarily on mathematical reasoning benchmarks, limiting generalization assessment to other domains
- Modest improvements over majority voting (+0.2% to +1.9%) may not justify complexity in all applications
- Paper lacks analysis of computational overhead introduced by dual-training process
- No comparison against other self-verification approaches or human-level performance benchmarks

## Confidence
- **High confidence**: The core mechanism of using verifiable rewards for simultaneous training of solution generation and self-verification is technically sound and well-described
- **Medium confidence**: The experimental results showing improved accuracy on mathematical benchmarks are convincing but limited in scope
- **Low confidence**: Claims about broader applicability beyond mathematical reasoning and the practical significance of the modest improvements over majority voting

## Next Checks
1. Test RISE on non-mathematical domains where verification is more subjective (e.g., code generation, creative writing) to assess generalization
2. Conduct ablation studies comparing computational efficiency against baseline approaches to quantify the overhead of dual-training
3. Evaluate performance with varying inference budgets (k>4) to determine if the modest improvements scale or diminish with increased resources