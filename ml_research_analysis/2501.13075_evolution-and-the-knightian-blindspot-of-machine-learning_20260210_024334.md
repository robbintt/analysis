---
ver: rpa2
title: Evolution and The Knightian Blindspot of Machine Learning
arxiv_id: '2501.13075'
source_url: https://arxiv.org/abs/2501.13075
tags:
- learning
- evolution
- arxiv
- such
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical blind spot in machine learning
  (ML): its insufficient robustness to Knightian uncertainty (KU), or unknown unknowns
  in an open world. While ML has achieved remarkable progress, it struggles with situations
  qualitatively different from its training distribution, unlike biological evolution
  which produces robust organisms through open-ended search and continual diversification.'
---

# Evolution and The Knightian Blindspot of Machine Learning

## Quick Facts
- arXiv ID: 2501.13075
- Source URL: https://arxiv.org/abs/2501.13075
- Authors: Joel Lehman; Elliot Meyerson; Tarek El-Gaaly; Kenneth O. Stanley; Tarin Ziyaee
- Reference count: 40
- Key outcome: ML struggles with qualitatively novel situations (Knightian uncertainty) due to closed-world formalisms, while evolution achieves robustness through open-ended diversification and persistent filtering

## Executive Summary
This paper identifies a critical blind spot in machine learning: its insufficient robustness to Knightian uncertainty (KU), or unknown unknowns in an open world. While ML has achieved remarkable progress, it struggles with situations qualitatively different from its training distribution, unlike biological evolution which produces robust organisms through open-ended search and continual diversification. The paper argues that ML's reliance on closed-world formalisms like Markov Decision Processes, combined with assumptions of static environments and fixed time horizons, limits its ability to handle KU. Through contrasting RL with evolution, the paper highlights mechanisms like continual generation of novel behaviors and empirical filtering of unsuccessful strategies. It concludes that significant gains in AI robustness may require either revising ML's formalisms or incorporating principles from artificial life and open-endedness research, emphasizing the need for direct engagement with the challenge of KU rather than assuming it will be solved through scale and generalization alone.

## Method Summary
The paper employs a conceptual analysis comparing machine learning with biological evolution to identify why ML is vulnerable to Knightian uncertainty. It contrasts the "anticipate-and-train" paradigm of standard ML with evolution's "diversify-and-filter" approach, examining how formalisms like Markov Decision Processes embed closed-world assumptions that systematically exclude unknown unknowns. The analysis spans multiple levels: formalisms (MDP assumptions), algorithms (RL vs evolutionary approaches), architectures (fixed vs open-ended), and time horizons (discounted vs unbounded). The paper suggests minimal computational experiments for reproducing the core insight: creating environments with qualitative novelty and comparing standard RL agents against population-based evolutionary approaches. No specific algorithms are prescribed, but the framework emphasizes maintaining diversity and filtering through persistent exposure to novelty.

## Key Results
- ML's reliance on closed-world formalisms systematically excludes scenarios qualitatively different from training distribution
- Evolution achieves robustness to KU through continual diversification, persistent filtering, and open-ended search
- Standard RL formalisms embed assumptions (fixed environment, discount factors, episodic boundaries) that blind agents to long-term consequences
- Significant gains in AI robustness may require either revising ML's formalisms or incorporating principles from artificial life and open-endedness research

## Why This Works (Mechanism)

### Mechanism 1: Diversify-and-Filter Temporal Strategy
- Claim: Evolution achieves robustness to Knightian uncertainty by maintaining diverse "bets" on how to persist, which are continuously filtered by reality as novel situations arise across time.
- Mechanism: Rather than optimizing a single policy against anticipated scenarios, the system maintains population-level diversity where each organism encodes an implicit hypothesis about future persistence. Unsuccessful bets are culled through selection pressure while new variants continually refresh the bet pool.
- Core assumption: The temporal process through which novel problems emerge is integral to robustness; diversification must be ongoing and unbounded.
- Evidence anchors:
  - [abstract] "evolution achieves such robustness without explicit theory, formalisms, or mathematical gradients... through continual diversification, persistent filtering of unsuccessful strategies, and open-ended search"
  - [section 3] "evolution's robustness in the face of KU is due to four interlocking factors: (1) The selection criterion of persisting across (potentially vast) swaths of time filters bets for robustness. (2) The drive to accumulate an abundant diversity of solutions results in many diverse bets"
  - [corpus] Weak direct evidence; corpus focuses on synthetic data robustness and multimodal learning rather than diversify-and-filter paradigms.
- Break condition: If the search space lacks sufficient diversity pressure, or if filtering operates on too short a time horizon, the mechanism fails to accumulate robustness.

### Mechanism 2: Open-ended Architecture Search
- Claim: Evolution's ability to revise any aspect of an agent's learning mechanisms (architecture, learning rules, development) enables discovery of robust solutions that fixed formalisms cannot reach.
- Mechanism: By exploring an open-ended search space with no commitment to particular neural architectures, learning algorithms, or representational schemes, evolution can discover specialized mechanisms for handling novel situations (e.g., fear responses, episodic memory) that generalize beyond training distribution.
- Core assumption: Robust solutions to KU may require qualitatively different architectures than standard neural networks provide.
- Evidence anchors:
  - [section 3.1.1] "evolution benefits from lacking a firm commitment to any particular ingredient or formalism (unlike a single NN that follows a fixed path of computation from input to output)"
  - [section 4.4.1] "evolution is not committed to a single mechanism for generalizing to unseen states... and often composes many separate learning processes together"
  - [corpus] Indirect support from "Towards Robust Multimodal Learning in the Open World" which addresses open-world robustness but not architectural open-endedness.
- Break condition: If the search space is constrained (e.g., fixed to gradient-based optimization, specific NN architectures), the mechanism cannot discover solutions outside that constraint class.

### Mechanism 3: Persistent Filtering Through Unbounded Time Horizons
- Claim: Robustness to KU emerges from filtering across vast, unbounded time horizons where long-term persistence is the sole criterion.
- Mechanism: Actions with delayed catastrophic consequences are eventually culled because lineages that take such actions fail to persist. The discount factor γ in RL formalisms explicitly blinds agents to consequences beyond a fixed horizon.
- Core assumption: Many KU-relevant failures manifest only across long timescales that exceed typical training/evaluation horizons.
- Evidence anchors:
  - [section 4.2.2] "an RL algorithm is indifferent to catastrophic events beyond its time horizon... In contrast, the time horizon across which evolution manages risk has no intrinsic limit"
  - [section 3.1.2] "A lineage's evolutionary success extends far beyond the lifetime of one organism, requiring continuing persistence over long swaths of time"
  - [corpus] "A Study of the Efficacy of Generative Flow Networks for Robotics" addresses OOD situations in robotics but focuses on immediate adaptation rather than long-horizon filtering.
- Break condition: If feedback is truncated (episodic boundaries, discount factors near zero), long-term consequences become invisible and robustness cannot emerge.

## Foundational Learning

- Concept: **Knightian Uncertainty vs. Risk**
  - Why needed here: The paper's central distinction; conflating "known unknowns" (quantifiable risk) with "unknown unknowns" (KU) leads to misplaced confidence in formalized robustness methods.
  - Quick check question: Can you enumerate all possible future scenarios your system might encounter? If no, you're in KU territory.

- Concept: **Markov Decision Process (MDP) Assumptions**
  - Why needed here: The foundational RL formalism embeds closed-world assumptions (static environment, fixed transition dynamics, episodic structure) that systematically exclude KU.
  - Quick check question: Does your formalism assume the deployment environment matches training? Does it have a discount factor that truncates consequence-awareness?

- Concept: **Open-endedness in Search**
  - Why needed here: The proposed solution class; understanding how open-ended evolutionary systems differ from objective-optimizing ML systems is prerequisite to implementing diversify-and-filter approaches.
  - Quick check question: Does your search process have a terminal condition or fixed objective, or does it continue generating novelty indefinitely?

## Architecture Onboarding

- Component map:
  - Training distribution -> RL algorithm -> Discount factor γ -> Episode boundary -> Deployment environment -> KU buffer (missing)
  - Training distribution: The set of environments/tasks used for optimization (assumed closed-world)
  - RL algorithm: Policy optimization method (constrained to fixed architecture and update rules)
  - Discount factor γ: Determines effective time horizon
  - Episode boundary: Artificial segmentation of experience
  - Deployment environment: Open world with qualitatively novel situations
  - KU buffer: (Missing in current architectures) Component that maintains diverse policies, detects novelty, and enables graceful degradation

- Critical path:
  1. Identify where closed-world assumptions enter the system (MDP formulation, training distribution, episode structure)
  2. Map time horizons: γ → effective horizon; episode boundaries → assumed independence
  3. Trace failure propagation: When OOD input arrives → NN generalization (unreliable) → correlated failures across deployed instances

- Design tradeoffs:
  - Formalization tractability vs. KU-relevance: More restrictive formalisms (POMDP, BMDP) improve guarantees but exclude qualitative novelty
  - Single-policy optimization vs. diversity maintenance: Standard RL optimizes one policy; diversify-and-filter requires population-level computation
  - Short-term performance vs. long-term robustness: Aggressive optimization can increase near-term returns while reducing KU robustness

- Failure signatures:
  - Brittle OOD generalization: Small distribution shifts cause catastrophic failures (e.g., self-driving cars confused by trailer-mounted traffic lights)
  - Correlated deployment failures: Multiple policy instances fail simultaneously on rare inputs
  - Overconfident uncertainty estimates: Bayesian methods report low uncertainty on OOD inputs that violate prior assumptions

- First 3 experiments:
  1. **Temporal horizon ablation**: Train identical policies with varying discount factors (γ ∈ {0.9, 0.99, 0.999, 0.9999}) and measure robustness to delayed-consequence scenarios. Expect longer horizons to improve KU robustness but slow convergence.
  2. **Diversity injection**: Implement population-based training with explicit diversity pressure (e.g., SMERL-style multi-policy optimization) and evaluate on qualitatively novel test environments. Compare failure rate distribution to single-policy baseline.
  3. **Episode boundary stress test**: Create environments where episode boundaries mask cumulative damage (e.g., motor wear, resource depletion). Compare policies trained with/without episodic assumptions on metrics that span multiple "episodes."

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can architectures and learning rules evolved in open-ended Artificial Life (ALife) simulations demonstrate superior robustness to Knightian Uncertainty compared to standard deep reinforcement learning algorithms?
- Basis in paper: [explicit] The authors ask, "if evolved ALife architectures and learning rules, potentially quite alien from ML architectures, could handle KU better than a general-purpose deep RL algorithm?" (Section 5.2).
- Why unresolved: While ALife creates diverse agents, these have not been rigorously benchmarked against standard ML models specifically for robustness to qualitatively novel, unanticipated situations (KU).
- What evidence would resolve it: An experiment where agents evolved in an ALife environment (like Avida or EvoSphere) are transferred to a qualitatively different, complex test environment and outperform standard RL agents on survival metrics.

### Open Question 2
- Question: Is it possible to mathematically formalize Knightian Uncertainty (KU) within reinforcement learning objectives in a way that productively improves robustness, or does handling KU require moving away from formal problem definitions?
- Basis in paper: [explicit] The authors pose a "gauntlet for the RL community: Whether or not KU can be mathematically formalized in a productive way... is a challenging and important question" (Section 5.4).
- Why unresolved: There is a tension between the need for mathematical optimization targets in ML and the nature of KU, which concerns "unknown unknowns" that defy quantification.
- What evidence would resolve it: The derivation of a novel formal objective or metric that captures KU robustness (e.g., via the Lindy effect) that, when optimized, demonstrably reduces failure rates in black-swan scenarios compared to standard risk-sensitive RL.

### Open Question 3
- Question: How can Large Language Models (LLMs) be best leveraged to generate qualitative variations in training environments to simulate Knightian Uncertainty?
- Basis in paper: [explicit] The authors suggest using LLMs to brainstorm and implement rare but realistic situations, asking, "how to best leverage such additional scenarios to encourage robustness to further unanticipated variation?" (Section 5.4).
- Why unresolved: While LLMs can generate code for environment variations, it is unclear how to automate the selection of variations that specifically target the "blind spot" of KU rather than just increasing standard diversity.
- What evidence would resolve it: A training pipeline where an LLM automatically proposes and encodes environment variations, resulting in an agent that handles out-of-distribution scenarios better than agents trained on human-anticipated variations.

### Open Question 4
- Question: How can the biological "diversify-and-filter" paradigm be effectively synthesized with modern machine learning's "anticipate-and-train" paradigm?
- Basis in paper: [inferred] The paper contrasts evolution's "diversify-and-filter" approach with ML's "anticipate-and-train" approach (Figure 2) and notes that "nothing precludes machine learning from more deeply integrating diversify-and-filter approaches" (Section 2.2).
- Why unresolved: Current ML focuses on converging on a single optimal policy for anticipated scenarios. Shifting to a paradigm that maintains a diverse, filtered portfolio of strategies for an open-ended future presents significant algorithmic and computational challenges.
- What evidence would resolve it: The development of an ML algorithm that maintains a diverse population of policies, filters them through exposure to open-ended novelty, and outperforms single-policy baselines in an environment that shifts qualitatively over time.

## Limitations
- The mechanisms proposed lack concrete empirical validation within the ML context
- The precise operational definition of "qualitatively novel" situations that constitute KU remains vague
- The paper doesn't specify concrete architectural changes or algorithmic implementations for diversify-and-filter approaches

## Confidence
- High: The distinction between risk and Knightian uncertainty, and how current RL formalisms embed closed-world assumptions
- Medium: The proposed mechanisms from evolution that could address KU (diversify-and-filter, open-ended search, persistent filtering)
- Low: The claim that no combination of current ML techniques can achieve sufficient KU robustness

## Next Checks
1. **Formalism Analysis:** Systematically catalog which KU-relevant scenarios are excluded by different RL formalisms (MDP, POMDP, BMDP) through mathematical proof
2. **Empirical Benchmarking:** Create standardized benchmarks that explicitly test KU scenarios versus known-unknown scenarios to measure performance gaps
3. **Hybrid Architecture Study:** Implement and evaluate hybrid approaches that combine standard RL with population-level diversity maintenance to quantify robustness gains