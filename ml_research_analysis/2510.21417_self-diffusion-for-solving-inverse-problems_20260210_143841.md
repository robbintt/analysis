---
ver: rpa2
title: Self-diffusion for Solving Inverse Problems
arxiv_id: '2510.21417'
source_url: https://arxiv.org/abs/2510.21417
tags:
- noise
- self-diffusion
- xtrue
- 'true'
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-diffusion is a novel framework for solving inverse problems
  without pretrained generative models. It uses a self-contained iterative process
  that alternates between noising and denoising steps to progressively refine its
  estimate of the solution.
---

# Self-diffusion for Solving Inverse Problems

## Quick Facts
- arXiv ID: 2510.21417
- Source URL: https://arxiv.org/abs/2510.21417
- Reference count: 40
- For 4× undersampled MRI, achieved PSNR of 39.21 dB and SSIM of 0.9640, outperforming traditional methods like Aseq (32.18/0.8661) and DIP (38.35/0.9574)

## Executive Summary
Self-diffusion is a novel framework for solving inverse problems without pretrained generative models. It uses a self-contained iterative process that alternates between noising and denoising steps to progressively refine its estimate of the solution. At each step, noise is added to the current estimate, and a self-denoiser, an untrained convolutional network, is continuously trained via a data fidelity loss to predict the solution from the noisy estimate. This approach exploits the spectral bias of neural networks and modulates it through a scheduled noise process. The method achieves competitive or superior performance compared to other methods across various inverse problems including MRI reconstruction, low-level vision tasks, and radar angle estimation.

## Method Summary
Self-diffusion is a novel framework for solving inverse problems (Ax=y) without pretrained models. It uses an untrained convolutional network (U-Net) with random Gaussian initialization as a self-denoiser. The method iteratively alternates between noising the current estimate and training the denoiser to minimize data fidelity loss ||AD(x_noisy) - y||². A scheduled noise process with decreasing σ(t) exploits neural networks' spectral bias (learning low frequencies first) to enable coarse-to-fine reconstruction. The approach remains adaptive to arbitrary forward operators and noisy observations, making it highly flexible and broadly applicable.

## Key Results
- Achieved PSNR of 39.21 dB and SSIM of 0.9640 on 4× undersampled MRI, outperforming traditional methods
- Demonstrated competitive performance across diverse inverse problems: MRI reconstruction, inpainting, deblurring, super-resolution, denoising, and radar angle estimation
- Showed that self-diffusion can achieve similar or superior performance compared to diffusion models with pretrained score functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Noise-modulated spectral bias enables coarse-to-fine reconstruction without external priors
- Mechanism: Neural networks exhibit spectral bias (low-frequency components learned first). The noise schedule σ(t) creates an implicit regularizer σ(t)²||AJ_D||²_F that penalizes high-frequency components proportionally to |k|² in Fourier space. High noise early enforces smooth outputs; decreasing noise allows finer details to emerge
- Core assumption: The Neural Tangent Kernel framework accurately predicts training dynamics for overparameterized CNNs on this task
- Evidence anchors:
  - [abstract] "exploits the spectral bias of neural networks and modulates it through a scheduled noise process"
  - [Section 2, Noise-Modulated Spectral Bias] Expected loss expansion shows σ(t)²||AJ_D||²_F regularization term; Fourier analysis shows penalty scales as |k|²
  - [corpus] Weak direct evidence; neighboring papers focus on pretrained models, not spectral bias exploitation
- Break condition: If the forward operator A has non-trivial null-space structure conflicting with low-frequency priors, coarse-to-fine learning may fail to converge

### Mechanism 2
- Claim: Self-referential iterative denoising converges to true solution without pretrained score functions
- Mechanism: Initialize x_T = ε₀ ~ N(0,I). At each step t: add noise σ(t)ε_t to current estimate, train denoiser D_θ via data fidelity loss ||AD_θ(x_noisy) - y||² for K iterations, produce refined estimate. Process repeats with decreasing σ(t) until σ→0
- Core assumption: The denoiser error ζ_t = E[||δ_t||²] → 0 as t → 0, which requires measurements y to be sufficiently informative
- Evidence anchors:
  - [abstract] "self-contained iterative process that alternates between noising and denoising steps to progressively refine its estimate"
  - [Section A.1] Error propagation analysis shows E[||d_t||²] = ζ_{t+1} and convergence requires ζ_t → 0
  - [corpus] Generative Modeling from Black-box Corruptions shares self-consistent theme but uses different approach
- Break condition: If K (inner iterations) is insufficient for denoiser to satisfy measurement constraint AD ≈ y at each step, error accumulates

### Mechanism 3
- Claim: Single randomly-initialized network serves as both prior and reconstructor across all noise levels
- Mechanism: The inductive bias of overparameterized CNNs (tendency toward smooth, natural-image-like outputs) replaces learned priors. Continuous training across noise levels allows network to capture multi-scale structure without external data. Gaussian initialization (μ=0, σ=0.02) provides stable starting point
- Core assumption: CNN architectural biases align with image statistics for the target domain
- Evidence anchors:
  - [abstract] "self-denoiser, an untrained convolutional network randomly initialized from scratch"
  - [Section 4.1] "2D U-Net with Gaussian initialization {μ=0, σ=0.02}"; Table 1 shows SDI (39.21/0.9640) outperforms DIP (38.35/0.9574)
  - [corpus] Whitened Score Diffusion addresses anisotropic diffusion but requires pretrained models
- Break condition: If target signal statistics differ significantly from natural image priors (e.g., radar, medical), may require domain-specific architecture choices

## Foundational Learning

- Concept: **Spectral Bias in Neural Networks**
  - Why needed here: Core theoretical justification for why noise-modulated training enables progressive reconstruction
  - Quick check question: Can you explain why overparameterized networks learn low-frequency components faster than high-frequency ones?

- Concept: **Inverse Problems and Ill-Posedness**
  - Why needed here: Motivates the need for regularization and explains why Ax = y has infinite solutions when m < n
  - Quick check question: Given undersampled MRI k-space, why doesn't simple least-squares reconstruction work?

- Concept: **Diffusion Model Fundamentals**
  - Why needed here: Understanding forward/reverse processes helps contrast self-diffusion with pretrained approaches
  - Quick check question: How does self-diffusion differ from score-based generative models in terms of training data requirements?

## Architecture Onboarding

- Component map:
  - Self-Denoiser (U-Net) -> Noise Scheduler (β_t schedule) -> Data Fidelity Loss (||AD_θ(x_noisy) - y||²) -> Forward Operator A

- Critical path:
  1. Initialize network θ and x_T = ε₀
  2. For t = T-1 to 0: sample ε_t, compute x_t = x_true,t + σ(t)ε_t
  3. Train D_θ for K iterations on loss L_t,k
  4. Update estimate: x_true,t-1 = D_θ(x_t)
  5. Return x_true,0 when t=0

- Design tradeoffs:
  - **T vs K**: More noise steps (T) improve quality but increase runtime; more inner iterations (K) improve denoiser convergence per step (Table 5 shows T=40, K=200 achieves 36.17 PSNR vs T=10, K=25 at 24.65)
  - **β schedule**: Large β_start (e.g., 1e-2 for denoising) vs small (1e-3 for MRI) affects initial regularization strength
  - **Architecture depth**: Deeper networks capture more complex structure but risk overfitting; paper uses standard U-Net without architectural search

- Failure signatures:
  - **Noise resampling disabled** ("w/o ε_t SDI"): Performance drops (Table 1: 4× MRI drops from 39.21 to 39.09 PSNR), indicating resampling prevents overfitting to specific noise realization
  - **Insufficient K**: Plateau in PSNR (Table 5 shows diminishing returns after K~200)
  - **Mismatched scale factor**: If network output norm doesn't match A^H y norm, unstable gradients occur (Section 4.1 notes normalization requirement)

- First 3 experiments:
  1. **Reproduce 1D compressed sensing**: Signal length 128, 35 measurements, T=40, K=200, verify frequency progression from low to high (Figure 3 shows error 0.5326 vs DIP 2.4329)
  2. **Ablate noise resampling**: Compare SDI vs "w/o ε_t SDI" on MRI 4× to quantify contribution of noise resampling (expect ~0.1-0.5 PSNR drop)
  3. **Hyperparameter sweep on T×K**: Test T∈{10,20,40}, K∈{50,100,200} on single MRI image to find compute-quality frontier (Table 5 provides reference points)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can learning task-specific or image-adaptive noise schedules improve the convergence rate and reconstruction fidelity of self-diffusion compared to the fixed schedules currently employed?
- Basis in paper: [explicit] Section 5 states that "learning task-specific or image-adaptive noise schedules may yield better convergence and fidelity."
- Why unresolved: The current implementation uses a fixed noise schedule defined by $\beta_{start}$ and $\beta_{end}$, which may be suboptimal for different forward operators or data complexities.
- What evidence would resolve it: A study comparing the standard fixed schedule against meta-learned or gradient-based adaptive schedules across diverse inverse problem tasks.

### Open Question 2
- Question: How can self-diffusion be effectively hybridized with pretrained generative models to handle complex semantic tasks while maintaining the framework's flexibility?
- Basis in paper: [explicit] Section 5 suggests that "combining self-diffusion with pretrained features or priors may boost performance on more complex or semantic tasks."
- Why unresolved: The paper focuses exclusively on untrained networks to solve inverse problems in a zero-shot manner, leaving the integration of external data priors unexplored.
- What evidence would resolve it: A method that injects pretrained latent features into the self-denoiser, validated by performance gains on high-level vision inverse problems.

### Open Question 3
- Question: Can Neural Architecture Search (NAS) identify specific denoiser architectures that offer superior efficiency and reconstruction quality over the standard U-Net?
- Basis in paper: [explicit] Section 5 proposes "automatically discovering architectures better suited for self-diffusion could improve both efficiency and reconstruction quality."
- Why unresolved: All experiments in the paper utilize standard U-Net architectures; the specific structural properties that best exploit the spectral bias in this context remain unknown.
- What evidence would resolve it: Implementing a NAS strategy within the self-diffusion loop and comparing the discovered architectures against the baseline U-Net in terms of FLOPs and PSNR.

### Open Question 4
- Question: Does the theoretical convergence and empirical efficacy of self-diffusion extend to non-linear inverse problems?
- Basis in paper: [inferred] Section 2 formulates the problem specifically as $Ax=y$ (linear), and all experiments in Section 4 involve linear forward operators (MRI, deblurring, SR).
- Why unresolved: The derivation of the Jacobian regularization term relies on the linear mapping of the forward operator $A$, and the method's behavior under non-linear operators is untested.
- What evidence would resolve it: Theoretical analysis of the stability of the noise-modulated spectral bias under non-linear operators, followed by empirical validation on tasks like phase retrieval.

## Limitations
- Computational cost is significantly higher than traditional methods due to the iterative training process
- Performance can be sensitive to initialization and scale normalization procedures
- Requires careful hyperparameter tuning (T, K, β schedules) for each application

## Confidence
- **High Confidence**: The self-contained iterative denoising mechanism works as described and produces verifiable results on benchmark inverse problems
- **Medium Confidence**: The spectral bias exploitation theory accurately explains coarse-to-fine reconstruction, though quantitative predictions are limited  
- **Medium Confidence**: The architectural choices (U-Net with Gaussian initialization) are effective, but optimal configurations may be task-dependent
- **Low Confidence**: Generalization to highly complex forward operators or domains with non-natural image statistics

## Next Checks
1. Test self-diffusion on inverse problems with known challenging null-space structures (e.g., highly undersampled Fourier measurements) to verify spectral bias exploitation holds
2. Conduct ablation studies varying architectural components (attention, normalization, depth) to identify which design choices are critical vs flexible
3. Compare computational efficiency against alternative iterative methods while controlling for final reconstruction quality across a range of problem difficulties