---
ver: rpa2
title: Automatic Proficiency Assessment in L2 English Learners
arxiv_id: '2505.02615'
source_url: https://arxiv.org/abs/2505.02615
tags:
- proficiency
- assessment
- speech
- language
- wav2vec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a deep learning framework for automated assessment
  of L2 English proficiency using both speech and text data. It evaluates multiple
  architectures including 2D CNNs, ResNet, wav2vec 2.0 for speech and BERT for text,
  across structured speech, written essays, and spontaneous dialogues.
---

# Automatic Proficiency Assessment in L2 English Learners

## Quick Facts
- arXiv ID: 2505.02615
- Source URL: https://arxiv.org/abs/2505.02615
- Reference count: 40
- Primary result: wav2vec 2.0 achieves 75% accuracy on ANGLISH speech classification, outperforming traditional CNN and BERT-based models

## Executive Summary
This paper presents a deep learning framework for automated assessment of L2 English proficiency using both speech and text data. It evaluates multiple architectures including 2D CNNs, ResNet, wav2vec 2.0 for speech and BERT for text, across structured speech, written essays, and spontaneous dialogues. Experiments are conducted under speaker-independent conditions on the ANGLISH, EFCamDat, and a private dataset. wav2vec 2.0 achieves the highest performance, with 75% accuracy on ANGLISH speech classification and up to 62% on spontaneous dialogue, outperforming traditional CNN and BERT-based models. Text-based models improve with fine-tuning and sequence optimization, but speech-based models generally perform better. Results demonstrate the effectiveness of deep learning for automated L2 assessment, particularly in capturing fluency and interactual competence, with multimodal approaches recommended for future work.

## Method Summary
The paper proposes a deep learning framework for L2 English proficiency assessment using both speech and text modalities. For speech, it employs wav2vec 2.0, 2D CNNs, Frequency-Axis CNNs, and ResNet architectures trained on structured speech tasks and spontaneous dialogues. For text, it uses BERT with frozen and fine-tuned embeddings, combined with SVM, MLP, or BiLSTM+Attention classifiers. The experiments use ANGLISH corpus (67 speakers, structured tasks), EFCamDat (essays mapped to CEFR levels), and a private dialogue dataset (L3-L5 levels). All experiments use speaker-independent splits with 10-fold cross-validation. Audio is preprocessed to 16kHz, segmented into 8-second chunks with 40-dim FBanks and z-score normalization. The wav2vec 2.0 model is fine-tuned with frozen CNN feature extractor, using separate optimizers for encoder (1e-5) and classifier (1e-4). A multi-task setup jointly predicts proficiency level and speaker gender with weighted loss combination.

## Key Results
- wav2vec 2.0 achieves 75% accuracy on ANGLISH structured speech classification, significantly outperforming traditional CNN (20.8%), Frequency-CNN (27.4%), and ResNet (35.9%) architectures
- For spontaneous dialogues, wav2vec 2.0 reaches 62% accuracy on the private dataset, demonstrating effectiveness in real-world assessment scenarios
- Fine-tuned BERT with MLP classifier achieves 92% accuracy on EFCamDat text classification at optimal 50-token sequence length
- Multi-task learning with gender classification improves wav2vec 2.0 accuracy from 75% to 80% on ANGLISH speech

## Why This Works (Mechanism)

### Mechanism 1
Pretrained wav2vec 2.0 representations outperform handcrafted acoustic features for L2 proficiency classification in speaker-independent settings. Self-supervised pretraining on large-scale raw audio produces contextualized representations that encode prosodic, phonetic, and fluency-related patterns without explicit feature engineering. Fine-tuning adapts these representations to proficiency-specific boundaries. Core assumption: Acoustic patterns relevant to L2 proficiency (hesitations, pronunciation deviations, fluency) are partially captured during self-supervised pretraining on native speech. Evidence anchors: [abstract] "wav2vec 2.0 achieves the highest performance, with 75% accuracy on ANGLISH speech classification... outperforming traditional CNN and BERT-based models." [section IV.D, Table II] wav2vec 2.0 achieves 75% vs. 2D CNN at 20.8%, Frequency-Axis CNN at 27.4%, ResNet at 35.9% on ANGLISH test set. [corpus] Neighbor paper "Proficiency assessment of L2 spoken English using wav2vec 2.0" (Bannò & Matassoni, 2023) corroborates wav2vec 2.0 effectiveness for L2 assessment, though in speaker-dependent settings. Break condition: Performance degrades significantly on highly spontaneous dialogues with overlapping speech, disfluencies, and noise (62% accuracy on private dataset vs. 75% on structured speech), suggesting pretrained representations may not fully capture interactional competence.

### Mechanism 2
Multi-task learning jointly predicting proficiency level and speaker gender improves proficiency classification accuracy. Shared representations learned through auxiliary gender classification may regularize the model and encode speaker-invariant features, reducing overfitting to speaker-specific characteristics. Core assumption: Proficiency-relevant features are partially orthogonal to gender-related acoustic features; auxiliary task provides beneficial regularization signal. Evidence anchors: [section IV.D, Table II] Multi-task wav2vec 2.0 achieves 80% vs. single-task 75%; ResNet improves from 35.9% to 43.8%. [section III.C] Loss formulation: L_final = 3L_level + L_gender, with weighted emphasis on proficiency task. [corpus] Limited direct evidence in corpus neighbors for multi-task L2 assessment; most related work focuses on single-task or multimodal fusion. Break condition: Multi-task gains are modest (5% absolute) and architecture-dependent; if gender correlates spuriously with proficiency in training data, auxiliary task could introduce bias rather than regularization.

### Mechanism 3
Optimal input segmentation differs by modality—full dialogue audio benefits speech models; student-only text benefits text models. Speech models leverage interactional dynamics (turn-taking, interviewer prompts, response timing) as proficiency signals. Text models suffer when interviewer content introduces noise or dilutes discriminative learner-specific linguistic patterns. Core assumption: Interactional competence is audible (pauses before responses, fluency under conversational pressure) but not reliably captured in ASR transcripts. Evidence anchors: [section IV.D, Table VI] wav2vec 2.0 full-audio (FA) 60-sec: 62% vs. student-only (SA) 60-sec: 61%; BERT FA 7-sent: 52% vs. BERT SA 7-sent: 57%. [section V] "Include full context in audio but focus on concise, student-only content in text." [corpus] Neighbor paper "Automatic assessment of conversational speaking tests" (McKnight et al., 2023) uses multimodal wav2vec 2.0 + Longformer for dialogue, supporting interactional dynamics as relevant. Break condition: Assumption relies on ASR quality (Whisper); overlapping speech and noise cause transcription errors that compound text model degradation.

## Foundational Learning

- **Self-supervised speech representations (wav2vec 2.0)**: Why needed here: Core architecture achieving best results; understanding pretraining-finetuning paradigm is essential for replicating and extending this work. Quick check question: Can you explain why freezing the CNN feature extractor while fine-tuning the transformer encoder might improve generalization?

- **Speaker diarization**: Why needed here: Dialogue preprocessing requires separating learner from interviewer speech; understanding PyAnnote + SpeechBrain pipeline is necessary for handling multi-speaker audio. Quick check question: What heuristic does the paper use to identify the learner after diarization?

- **CEFR proficiency levels**: Why needed here: Ground truth labels are CEFR-aligned (A1-C1); understanding this framework is necessary for interpreting classification granularity and class imbalance issues. Quick check question: How many proficiency classes are used in ANGLISH vs. EFCamDat experiments?

## Architecture Onboarding

- **Component map**: Raw audio → 16kHz downsampling → FBanks (40 bins) → [2D CNN / Frequency-CNN / ResNet / wav2vec 2.0] → Classification head; ASR (Whisper) → Transcript → BERT (frozen/fine-tuned) → [SVM / MLP / BiLSTM+Attention] → Classification head; Shared encoder → Dual DNN heads