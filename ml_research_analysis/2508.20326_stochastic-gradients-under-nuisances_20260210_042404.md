---
ver: rpa2
title: Stochastic Gradients under Nuisances
arxiv_id: '2508.20326'
source_url: https://arxiv.org/abs/2508.20326
tags:
- gprop
- nuisance
- gradient
- loss
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the first non-asymptotic convergence guarantees
  for stochastic gradient methods under unknown nuisance parameters, a common setting
  in semiparametric inference and causal learning. The authors show that when the
  loss function satisfies Neyman orthogonality, standard SGD converges linearly to
  a ball around the true parameter with radius depending on the nuisance estimation
  error to the fourth power, whereas without orthogonality the radius scales with
  the squared error.
---

# Stochastic Gradients under Nuisances

## Quick Facts
- **arXiv ID:** 2508.20326
- **Source URL:** https://arxiv.org/abs/2508.20326
- **Reference count:** 40
- **Primary result:** First non-asymptotic convergence guarantees for SGD under unknown nuisance parameters, with error scaling from $O(r^2)$ to $O(r^4)$ under Neyman orthogonality.

## Executive Summary
This paper establishes non-asymptotic convergence guarantees for stochastic gradient descent (SGD) when nuisance parameters are unknown and must be estimated. The key insight is that when the loss satisfies Neyman orthogonality, the bias from nuisance estimation error scales as the fourth power of the error, rather than the second power. The authors prove linear convergence to a ball around the true parameter with radius depending on nuisance estimation quality. They also propose orthogonalized SGD (OSGD), which interpolates between regimes based on the quality of an orthogonalizing operator, providing practical algorithms for general settings.

## Method Summary
The authors analyze SGD convergence when gradients depend on estimated nuisance parameters $\hat{g}$ instead of true $g_0$. Under strong convexity and smoothness, they show standard SGD converges linearly to a neighborhood whose radius depends quadratically on nuisance error. When the loss satisfies Neyman orthogonality (locally insensitive to nuisance at optimum), this radius scales quartically instead. OSGD uses an approximately orthogonalized gradient oracle to achieve similar quartic scaling even for non-orthogonal losses. The analysis covers both fixed and iteratively updated nuisance estimates, with convergence degrading gracefully as nuisance estimate quality decreases.

## Key Results
- Standard SGD converges linearly to a ball around $\theta^*$ with radius scaling as $\|\hat{g} - g_0\|_G^2$ without orthogonality
- Under Neyman orthogonality, radius scales as $\|\hat{g} - g_0\|_G^4$, significantly improving robustness
- OSGD achieves quartic scaling for general losses by subtracting the nuisance gradient component
- Results hold for both fixed and iteratively updated nuisance estimates with graceful degradation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standard SGD converges linearly to a neighborhood of $\theta^*$, with radius scaling as $\|\hat{g} - g_0\|_G^2$.
- **Mechanism:** The gradient oracle is biased because it uses $\hat{g}$ instead of $g_0$. Under strong convexity, this bias manifests as a constant error term scaling with squared nuisance error.
- **Core assumption:** Population risk is strongly convex and smooth.
- **Evidence anchors:** Abstract (radius scales with squared error), Theorem 1 Part 1 (includes $2\alpha_1^2 / \mu^2 \|\hat{g} - g_0\|_G^2$ term).
- **Break condition:** Large nuisance estimation error causes convergence ball to become too large to be useful.

### Mechanism 2
- **Claim:** Neyman orthogonality reduces bias scaling from $O(r^2)$ to $O(r^4)$.
- **Mechanism:** Orthogonality means the score function is locally insensitive to nuisance at optimum ($D_g S_\theta(\theta^*, g_0) = 0$), canceling first-order impact in Taylor expansion.
- **Core assumption:** Risk satisfies higher-order smoothness and Neyman orthogonality.
- **Evidence anchors:** Definition 2 (Neyman orthogonality), Theorem 1 Part 2 (bias term improves to $\beta_1^2 / 2\mu^2 \|\hat{g} - g_0\|_G^4$).
- **Break condition:** Non-orthogonal losses revert to sensitive $O(r^2)$ regime.

### Mechanism 3
- **Claim:** OSGD achieves quartic scaling for non-orthogonal losses via orthogonalization.
- **Mechanism:** OSGD estimates orthogonalizing operator $\Gamma_0$ and subtracts correlated nuisance gradient component, interpolating between regimes based on $\hat{\Gamma}$ quality.
- **Core assumption:** Orthogonalizing operator $\Gamma_0$ exists and can be estimated.
- **Evidence anchors:** Equation (11) (defines NO gradient oracle), Theorem 3 (bounds error with quartic terms and correction).
- **Break condition:** Poor $\hat{\Gamma}$ estimate causes correction term to dominate, degrading convergence.

## Foundational Learning

- **Score Functions & Influence Functions**
  - **Why needed here:** The paper analyzes the "score" (gradient of loss) to determine sensitivity to nuisance parameters.
  - **Quick check question:** Can you define the score function for a simple logistic regression loss?

- **Strong Convexity & Smoothness in Optimization**
  - **Why needed here:** These properties are prerequisites for the linear convergence guarantees of SGD.
  - **Quick check question:** Does a loss function with multiple local minima satisfy strong convexity?

- **Semiparametric Efficiency**
  - **Why needed here:** The paper frames results within orthogonal statistical learning, aiming for parametric rates in semiparametric models.
  - **Quick check question:** Why is $\sqrt{n}$-consistency difficult to achieve if the nuisance parameter is estimated at $n^{-1/4}$ rate?

## Architecture Onboarding

- **Component map:** Nuisance Estimator -> Gradient Oracle -> (Optional) Orthogonalizer -> Update Target $\theta$ via SGD
- **Critical path:** Initialize parameters → Estimate Nuisance (Batch/Stream) → (Optional) Estimate Orthogonalizer → Update $\theta$ via SGD
- **Design tradeoffs:**
  - Analytical vs. Learned Orthogonalization: Analytical is exact but problem-specific; OSGD (learned $\Gamma$) is general but introduces estimation noise
  - Data Splitting: Theoretical guarantees require splitting data for nuisance vs. target estimation to avoid overfitting bias
- **Failure signatures:**
  - Convergence Saturation: Training loss plateaus; check $\|\hat{g} - g_0\|$
  - Divergence in OSGD: Exploding gradients; check if $\hat{\Gamma}$ is invertible
- **First 3 experiments:**
  1. **Partially Linear Model Baseline:** Implement SGD on PLM using naive vs. orthogonal loss to observe $r^2$ vs $r^4$ scaling
  2. **OSGD Ablation:** Implement OSGD with varying $\hat{\Gamma}$ noise to verify interpolation effect per Theorem 3
  3. **Interleaved Nuisance:** Set up two-stream experiment with iteratively updated $\hat{g$; plot $\theta$ trajectory against batch version

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can convergence analysis be extended to adaptive methods like Adam under estimated nuisance parameters?
- **Basis in paper:** [explicit] Appendix H.2 states it "still remains unclear that how to analysis Adam under an estimated nuisance $\hat{g}$ and what should be the nuisance effect"
- **Why unresolved:** Analysis focuses on iterate convergence in strongly convex settings, while Adam is analyzed via gradient norm stationarity in non-convex landscapes
- **What evidence would resolve it:** Convergence rate for Adam under nuisance estimation with gradient norm or stationarity gap bounds

### Open Question 2
- **Question:** How can the orthogonalizing operator $\Gamma_0$ be efficiently computed or approximated in general settings?
- **Basis in paper:** [explicit] Discussion section identifies "how to compute or approximate such an operator" as a "central question"
- **Why unresolved:** Theoretical guarantees assume $\hat{\Gamma}$ is provided, but deriving this operator via automated differentiation or other methods is open
- **What evidence would resolve it:** Algorithm for estimating $\Gamma_0$ with finite-sample error guarantees for OSGD bounds

### Open Question 3
- **Question:** Do convergence guarantees for OSGD hold for non-convex objective functions?
- **Basis in paper:** [inferred] Theoretical results rely on strong convexity assumption
- **Why unresolved:** Linear convergence relies on strong convexity for contraction; neural network training often violates this
- **What evidence would resolve it:** Convergence analysis for OSGD in non-convex settings targeting stationarity rather than global optimality

## Limitations

- **Empirical validation scope:** Experiments focus on simple PLM and one real dataset, not capturing complexity of high-dimensional or non-parametric nuisance estimation scenarios
- **Generalizability to complex architectures:** Framework assumes strong convexity and smoothness, which may not hold for deep neural networks or non-convex models
- **Nuisance estimation error characterization:** Bounds depend on specific norm of estimation error, which can be challenging to estimate accurately in practice

## Confidence

- **High:** Linear convergence of standard SGD to neighborhood under strong convexity (well-established optimization result, extension follows standard Taylor arguments)
- **Medium:** Improvement from $O(r^2)$ to $O(r^4)$ under Neyman orthogonality (theoretically sound but practical significance depends on nuisance error magnitude)
- **Medium:** OSGD effectiveness in interpolating regimes (theoretically supported but practical impact of correction term not fully characterized in high dimensions)

## Next Checks

1. **Robustness to Nuisance Degradation:** Intentionally degrade nuisance estimator quality (reduce training data/model capacity) and measure if convergence bounds and $r^2$ vs $r^4$ scaling hold as predicted

2. **Orthogonalizer Estimation Error Analysis:** Systematically vary quality of $\hat{\Gamma}$ (different data amounts/regularization) and plot convergence trajectory and final error against Frobenius norm of orthogonalizer error to verify interpolation effect

3. **Extension to Non-Convex Losses:** Design experiment using non-convex loss (shallow neural network with non-convex activation) in semiparametric setting; compare standard SGD vs OSGD convergence behavior despite lack of formal guarantees