---
ver: rpa2
title: 'STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output
  Reliability'
arxiv_id: '2512.23712'
source_url: https://arxiv.org/abs/2512.23712
tags:
- consistency
- semantic
- structured
- sted
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STED (Semantic Tree Edit Distance) and a
  consistency scoring framework for evaluating the reliability of structured outputs
  generated by large language models (LLMs). The method addresses the problem of distinguishing
  benign variations (e.g., key reordering, synonym substitution) from critical differences
  (e.g., schema violations) in JSON outputs.
---

# STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability

## Quick Facts
- arXiv ID: 2512.23712
- Source URL: https://arxiv.org/abs/2512.23712
- Authors: Guanghui Wang; Jinze Yu; Xing Zhang; Dayuan Jiang; Yin Song; Tomal Deb; Xuefeng Liu; Peiyang He
- Reference count: 40
- Primary result: STED outperforms baselines like BERTScore, TED, and DeepDiff in distinguishing benign JSON variations from critical schema violations, achieving 0.86-0.90 similarity for semantic equivalents and 0.0 for structural breaks.

## Executive Summary
This paper introduces STED (Semantic Tree Edit Distance) and a consistency scoring framework for evaluating the reliability of structured outputs generated by large language models (LLMs). The method addresses the problem of distinguishing benign variations (e.g., key reordering, synonym substitution) from critical differences (e.g., schema violations) in JSON outputs. STED extends classical tree edit distance with semantic awareness, using embedding-based similarity for field names and values, and Hungarian algorithm-based matching for optimal subtree alignment. It also includes normalization to produce interpretable similarity scores in [0,1]. Experiments on synthetic datasets with controlled variations show STED outperforms baselines like BERTScore, TED, and DeepDiff, achieving 0.86-0.90 similarity for semantic equivalents and 0.0 for structural breaks. Benchmarking six LLMs reveals significant consistency differences: Claude-3.7-Sonnet maintains near-perfect structural reliability even at high temperatures, while others like Claude-3-Haiku and Nova-Pro degrade substantially. The framework enables model selection, prompt refinement, and diagnostic analysis for production systems.

## Method Summary
The STED framework converts JSON outputs to tree representations where each node contains type, label, value, and path information. It computes pairwise similarity via semantic-enhanced tree edit distance using embedding-based key/value similarity (Amazon Titan Text Embeddings v2) combined with structural constraints. The Hungarian algorithm finds optimal child matching at each tree level, ensuring globally optimal alignment rather than greedy local decisions. The consistency score aggregates n-choose-2 pairwise STED measurements across repeated generations, normalizing observed standard deviation against maximum possible deviation and applying a steepness factor α=20 to amplify small deviations common in model outputs. The framework supports structural-only, semantic-only, or hybrid consistency reporting.

## Key Results
- STED achieves 0.86-0.90 similarity scores for semantically equivalent JSON outputs while correctly assigning 0.0 for structural breaks like flattening or nesting changes
- Claude-3.7-Sonnet maintains near-perfect structural reliability (consistency >0.95) even at high temperatures (0.5-0.9), while Claude-3-Haiku and Nova-Pro show significant degradation
- The consistency scoring framework successfully distinguishes between LLM models with different reliability profiles, enabling informed model selection for production deployment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** STED distinguishes benign structural variations from critical schema violations by combining embedding-based semantic matching with structural constraints.
- **Mechanism:** JSON outputs are converted to tree representations where each node contains type, label, value, and path information. The update cost function (Equation 2) combines structural similarity (γstruct) and content similarity (γcontent) with configurable weights. Field names are normalized and compared via embeddings, allowing "user_name" and "userName" to be recognized as equivalent while still penalizing actual structural breaks like flattening or nesting changes.
- **Core assumption:** Modern embedding models achieve sufficient semantic understanding for short field names and values that the choice of embedding model has minimal impact on performance.
- **Evidence anchors:**
  - [abstract] "STED achieves superior performance (0.86-0.90 similarity for semantic equivalents, 0.0 for structural breaks) compared to existing metrics"
  - [section 4.3.1] "For structural modifications (flattening and nesting changes), STED correctly assigns zero similarity, recognizing these as breaking changes for downstream systems"
  - [corpus] Related papers focus on structured output correctness but do not validate STED's specific semantic-structural combination; corpus evidence is weak for direct comparison
- **Break condition:** If embedding models fail to capture domain-specific equivalences (e.g., "email" vs "correo" in non-English contexts), semantic matching may incorrectly penalize valid variants.

### Mechanism 2
- **Claim:** Hungarian algorithm-based matching enables order-invariant comparison of arrays and object properties without sacrificing global optimality.
- **Mechanism:** At each tree level, child matching is formulated as an assignment problem (Equation 3). A cost matrix M is constructed where Mij represents the cost of matching child i from T1 with child j from T2. The Hungarian algorithm finds minimum-cost assignment in O(n³) time, ensuring globally optimal alignment rather than greedy sequential matching that could miss better pairings.
- **Core assumption:** The cubic complexity of Hungarian algorithm remains tractable for typical structured output branching factors in production JSON.
- **Evidence anchors:**
  - [abstract] "Hungarian algorithm-based matching for optimal subtree alignment"
  - [section 3.5] "The Hungarian algorithm finds the minimum-cost assignment in O(n³) time, ensuring globally optimal child alignment rather than greedy local decisions"
  - [corpus] OrderProbe paper [81919] explores LLM order sensitivity but from input reconstruction perspective; not direct validation of STED's matching approach
- **Break condition:** For JSON structures with very large arrays (>100 elements), O(n³) complexity may become prohibitive; approximation methods would be needed.

### Mechanism 3
- **Claim:** Consistency scoring via normalized deviation aggregation produces interpretable reliability metrics that correlate with production deployment requirements.
- **Mechanism:** Pairwise STED similarities are aggregated using Equation 1 (average over all pairs). The consistency score (Section 3.7) normalizes observed standard deviation σ against maximum possible deviation σmax for n values in [0,1], then applies steepness factor α=20 via the formula ConsistencyScore = (1/(1+2σ̂))^α. This amplifies small deviations common in model outputs while saturating for large deviations.
- **Core assumption:** The steepness factor α=20 appropriately balances discrimination in low-deviation ranges against saturation for rare large deviations.
- **Evidence anchors:**
  - [abstract] "consistency scoring framework aggregating multiple STED measurements across repeated generations to quantify reliability"
  - [section 3.7] "α=20 is a steepness factor that amplifies the typically small deviations observed in model outputs, providing better discrimination"
  - [corpus] SCALAR paper [86873] addresses structural consistency in materials science but uses different formalization; limited direct validation of this specific aggregation formula
- **Break condition:** If production systems require finer-grained discrimination in high-consistency regimes (scores >0.95), the current steepness factor may cause premature saturation.

## Foundational Learning

- **Concept: Tree Edit Distance (TED)**
  - Why needed here: STED extends classical TED; understanding the base algorithm (insert/delete/update operations with costs) is prerequisite to grasping semantic enhancements.
  - Quick check question: Given two trees where T2 is T1 with one leaf node removed, what is the TED cost?

- **Concept: Hungarian Algorithm for Assignment Problems**
  - Why needed here: Core to STED's order-invariant matching; the algorithm finds minimum-cost bipartite matching in polynomial time.
  - Quick check question: Why does greedy sequential matching fail to guarantee globally optimal child alignment in tree comparison?

- **Concept: Embedding-Based Semantic Similarity**
  - Why needed here: STED uses cosine similarity on embeddings to recognize semantically equivalent field names; understanding embedding spaces and similarity metrics is essential.
  - Quick check question: If two field names have embedding cosine similarity of 0.95, should they be treated as equivalent? What threshold would you choose?

## Architecture Onboarding

- **Component map:** JSON Parser -> Tree Transformation -> Embedding Module -> STED Core -> Consistency Aggregator -> Evaluation Modes

- **Critical path:** JSON input → Tree transformation → Per-level Hungarian matching with semantic costs → Pairwise similarity → Multi-sample aggregation → Consistency score [0,1]

- **Design tradeoffs:**
  - **Embedding model choice:** Paper claims minimal impact for short field names, but domain-specific terminology may require specialized embeddings
  - **Weight configuration (ws, wc in Equation 2):** Default equal weighting; structural vs. semantic emphasis depends on use case
  - **Steepness factor α:** Set to 20 for discrimination; higher values increase sensitivity to small deviations
  - **Computational vs. accuracy:** Hungarian algorithm O(n³) ensures optimality but limits scalability for large arrays

- **Failure signatures:**
  - **High similarity for structural breaks:** May indicate embedding model failing to distinguish semantically different field names in domain-specific contexts
  - **Near-zero consistency at low temperatures:** Suggests prompt engineering issues or schema ambiguity rather than model limitation
  - **Erratic consistency patterns across temperatures (e.g., Nova-Pro's local maximum at T=0.5):** May indicate non-monotonic sampling behavior requiring investigation

- **First 3 experiments:**
  1. **Baseline validation:** Run STED on provided synthetic dataset (2,400 samples) and verify reproduction of reported scores (0.86-0.90 for semantic equivalents, 0.0 for structural breaks)
  2. **Embedding sensitivity:** Replace Amazon Titan with all-MiniLM-L6-v2 or Sentence-BERT on subset of 75 base samples; compare similarity scores to quantify embedding model impact
  3. **Single-model temperature sweep:** Generate 10 outputs per temperature (0.1-0.9) for 20 test prompts with a single LLM; plot consistency degradation curve to validate framework before multi-model benchmarking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the STED framework generalize effectively to other structured data formats like XML and HTML while maintaining its discrimination capabilities?
- Basis in paper: [explicit] The conclusion explicitly lists extending to "other formats" as a limitation and direction for future work.
- Why unresolved: The current implementation and experimental validation focus exclusively on JSON structures, leaving other hierarchical formats unexplored.
- What evidence would resolve it: Benchmark results on XML and HTML datasets showing STED's ability to distinguish semantic variations from structural breaks similar to its JSON performance.

### Open Question 2
- Question: How can the semantic similarity metrics be adapted to capture domain-specific equivalences in specialized fields like medicine or finance?
- Basis in paper: [explicit] The authors state in the conclusion that "The semantic similarity metrics may not capture domain-specific equivalences" as a specific limitation.
- Why unresolved: The framework currently relies on general-purpose embeddings which may lack the nuance to identify valid equivalences in highly technical or jargon-heavy schemas.
- What evidence would resolve it: Evaluation of STED on domain-specific datasets where ground-truth equivalences are defined by subject matter experts, comparing general vs. fine-tuned embeddings.

### Open Question 3
- Question: Can the computational complexity of the STED algorithm be reduced for highly-branched structures without compromising the accuracy of the consistency score?
- Basis in paper: [inferred] The conclusion calls for efforts to "optimize evaluation efficiency," while the methodology identifies the O(N×B³) complexity of the Hungarian algorithm on large arrays as a potential bottleneck.
- Why unresolved: The current reliance on the Hungarian algorithm for optimal matching creates a computational cost that may limit adoption in high-volume production systems.
- What evidence would resolve it: Performance benchmarks comparing the standard algorithm against approximation heuristics (e.g., greedy matching) on synthetic trees with high branching factors.

## Limitations
- **Parameter specification gaps:** The paper does not specify exact weights (ws, wc) for structural vs. content similarity in Equation 2, nor provides complete field name normalization rules
- **Generalizability constraints:** STED's effectiveness on real-world production JSON schemas with domain-specific terminology remains untested
- **Scalability limitations:** The Hungarian algorithm's O(n³) complexity may become prohibitive for JSON structures with very large arrays (>100 elements)

## Confidence
**High Confidence Claims:**
- STED's semantic-structural combination effectively distinguishes benign variations from critical schema violations on controlled synthetic datasets
- The Hungarian algorithm-based matching provides order-invariant comparison with global optimality guarantees
- Claude-3.7-Sonnet demonstrates superior structural reliability across temperature ranges compared to other evaluated models

**Medium Confidence Claims:**
- The specific α=20 steepness factor optimally amplifies small deviations in consistency scoring
- Embedding-based semantic matching has minimal performance impact regardless of embedding model choice
- The consistency scoring framework reliably predicts production deployment requirements

**Low Confidence Claims:**
- STED's performance on real-world production JSON schemas without controlled variations
- Scalability characteristics for JSON structures with very large arrays or deep nesting
- Transferability of consistency patterns across different prompt formulations and schema types

## Next Checks
1. **Baseline Reproduction:** Run STED on the provided synthetic dataset (2,400 samples) and verify reproduction of reported scores (0.86-0.90 for semantic equivalents, 0.0 for structural breaks) across all baseline comparisons.

2. **Embedding Sensitivity Analysis:** Replace Amazon Titan with all-MiniLM-L6-v2 or Sentence-BERT on a subset of 75 base samples; compare similarity scores to quantify the actual impact of embedding model choice on STED performance.

3. **Single-Model Temperature Sweep:** Generate 10 outputs per temperature (0.1-0.9) for 20 test prompts with a single LLM; plot consistency degradation curves to validate the framework's ability to detect temperature-dependent reliability changes before scaling to multi-model benchmarking.