---
ver: rpa2
title: 'IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages'
arxiv_id: '2512.00333'
source_url: https://arxiv.org/abs/2512.00333
tags:
- languages
- language
- question
- indic
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IndicParam is a human-curated benchmark of over 13,000 multiple-choice
  questions covering 11 low- and extremely low-resource Indic languages. It evaluates
  LLM performance on language understanding and general knowledge tasks using graduate-level
  exam questions.
---

# IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages

## Quick Facts
- **arXiv ID:** 2512.00333
- **Source URL:** https://arxiv.org/abs/2512.00333
- **Reference count:** 40
- **Primary result:** Human-curated benchmark of 13K+ multiple-choice questions across 11 low-resource Indic languages

## Executive Summary
IndicParam introduces a comprehensive benchmark for evaluating large language models on low- and extremely low-resource Indic languages. The benchmark comprises over 13,000 multiple-choice questions spanning 11 languages, focusing on language understanding and general knowledge through graduate-level exam questions. The dataset distinguishes between knowledge-oriented and linguistic questions, supporting diverse formats including text and vision-language questions. The benchmark reveals significant challenges in cross-lingual transfer for underrepresented languages, with even state-of-the-art models like Gemini-2.5 achieving only 58% average accuracy.

## Method Summary
The benchmark employs human curation to develop multiple-choice questions across 11 Indic languages, with questions categorized as either knowledge-oriented or linguistic. The dataset covers a range of question formats including text and vision-language questions, designed to evaluate both language understanding and general knowledge capabilities. The curation process specifically targets graduate-level difficulty, creating a challenging evaluation framework for language models operating in low-resource contexts.

## Key Results
- Over 13,000 multiple-choice questions covering 11 low- and extremely low-resource Indic languages
- Gemini-2.5 achieves only 58% average accuracy, demonstrating benchmark difficulty
- Clear distinction between knowledge-oriented and linguistic questions
- Supports diverse question formats including text and vision-language questions

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of low-resource Indic languages through human-curated questions that capture both linguistic complexity and domain knowledge requirements. By focusing on graduate-level difficulty and distinguishing between different question types, the benchmark provides a nuanced evaluation framework that exposes limitations in current LLM capabilities for underrepresented languages.

## Foundational Learning
**Language Resource Scarcity** - Understanding why traditional benchmarks fail for low-resource languages; needed to contextualize the benchmark's importance. Quick check: Verify Wikipedia article counts for target languages.
**Cross-Lingual Transfer** - Knowledge transfer between high-resource and low-resource languages; needed to understand the benchmark's evaluation goals. Quick check: Compare model performance across language pairs.
**Multiple-Choice Question Design** - Principles for creating effective multiple-choice questions; needed to ensure benchmark quality. Quick check: Validate question difficulty through pilot testing.

## Architecture Onboarding

**Component Map:** Human Curation -> Question Categorization -> Multiple Languages -> Diverse Formats -> Evaluation Framework

**Critical Path:** Question creation → Quality verification → Language-specific adaptation → Format diversification → Benchmark deployment

**Design Tradeoffs:** Manual curation ensures quality but limits scalability; multiple-choice format enables consistent evaluation but may not capture all reasoning capabilities; language-specific questions ensure relevance but complicate cross-lingual comparisons.

**Failure Signatures:** Inconsistent difficulty levels across languages; bias toward certain question types; limited generalizability beyond Indic language family; potential cultural context dependencies.

**First Experiments:**
1. Baseline evaluation of current LLMs on benchmark questions
2. Comparative analysis of knowledge-oriented versus linguistic question performance
3. Cross-lingual transfer analysis between high-resource and low-resource language pairs

## Open Questions the Paper Calls Out
None

## Limitations
- Human curation process may introduce inconsistencies across languages and question types
- Multiple-choice format may not fully capture LLM reasoning capabilities
- Focus on Indic languages limits generalizability to other language families

## Confidence

**High Confidence:**
- Benchmark construction methodology is clearly described and reproducible
- Empirical finding of 58% accuracy ceiling is verifiable and measurable

**Medium Confidence:**
- Language resource characterization based on Wikipedia counts provides reasonable proxy
- Cross-lingual transfer implications are plausible but require further validation

**Low Confidence:**
- Claim of addressing cross-lingual transfer challenges lacks direct experimental testing within the study

## Next Checks
1. Conduct inter-annotator reliability analysis to quantify consistency in human curation quality across languages
2. Perform systematic evaluation across broader range of LLM architectures to determine if 58% accuracy is model-specific or fundamental
3. Design controlled experiments comparing performance on knowledge-oriented versus linguistic questions within each language