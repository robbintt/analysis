---
ver: rpa2
title: On the Effectiveness of Large Language Models in Automating Categorization
  of Scientific Texts
arxiv_id: '2502.15745'
source_url: https://arxiv.org/abs/2502.15745
tags:
- research
- scientific
- classification
- llms
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of Large Language Models (LLMs) for
  automatic classification of scientific texts into hierarchical classification systems.
  Using the FORC dataset as ground truth, the authors evaluate multiple LLMs (Llama
  3.1, Gemma, Mistral Nemo, and Phi) with different prompting strategies and temperature
  settings.
---

# On the Effectiveness of Large Language Models in Automating Categorization of Scientific Texts

## Quick Facts
- **arXiv ID:** 2502.15745
- **Source URL:** https://arxiv.org/abs/2502.15745
- **Reference count:** 11
- **Primary result:** Llama 3.1 achieved accuracy of 0.82 for scientific text classification, outperforming traditional BERT models by 0.08

## Executive Summary
This paper evaluates the effectiveness of Large Language Models (LLMs) for automatic classification of scientific texts into hierarchical classification systems. Using the FORC dataset with 59,344 samples, the authors test multiple LLMs including Llama 3.1, Gemma, Mistral Nemo, and Phi with different prompting strategies and temperature settings. The study finds that few-shot prompting significantly improves classification accuracy compared to zero-shot approaches, with Llama 3.1 (70B parameters) achieving the best performance at 0.82 accuracy. The research demonstrates that LLMs are superior to traditional models for research area identification in scientific literature.

## Method Summary
The study uses the FORC dataset containing scientific papers with title and abstract pairs mapped to ORKG taxonomy domains. Four LLMs (Llama 3.1, Gemma, Mistral Nemo, Phi) were tested using both zero-shot and few-shot prompting strategies via the LangChain framework with Ollama serving. Models were evaluated across temperature settings from 0.2 to 1.0, with the best configuration using few-shot prompting at temperature 0.8. The primary metric was accuracy at the top-level domain classification task, comparing LLM performance against traditional BERT and BiLSTM baselines.

## Key Results
- Llama 3.1 (70B) achieved accuracy of 0.82, outperforming BERT models by 0.08
- Few-shot prompting significantly improved accuracy from 0.62 (zero-shot) to 0.82
- Temperature 0.8 yielded optimal performance across models
- Larger models (70B parameters) consistently outperformed smaller models (3.8B-27B)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Few-shot prompting provides necessary semantic grounding for hierarchical taxonomy mapping
- **Mechanism:** By injecting example pairs of scientific texts and their corresponding research areas into the prompt, the model aligns its pre-trained internal knowledge graph with the specific structural rules of the target taxonomy (ORKG), reducing ambiguity in cross-domain texts
- **Core assumption:** The LLM has sufficient pre-existing domain knowledge to map new texts to the provided examples; performance relies on the model's ability to generalize from the examples without explicit gradient updates
- **Evidence anchors:**
  - [abstract] "Few-shot prompting significantly improves classification accuracy"
  - [section 6] "Llama... achieved an Accuracy of 0.82 for the few-shot prompt" vs 0.62 for zero-shot
  - [corpus] Neighbor paper "Comparative Analysis of OpenAI GPT-4o..." supports the efficacy of prompt engineering for scientific categorization tasks
- **Break condition:** If the taxonomy labels are semantically distinct but the provided examples are poor representations of the class diversity, few-shot performance may degrade or hallucinate categories

### Mechanism 2
- **Claim:** Moderate temperature settings (0.8) optimize classification by balancing determinism with semantic flexibility
- **Mechanism:** The paper suggests that a non-zero temperature allows the model sufficient "creativity" to bridge the gap between the specific vocabulary of a scientific abstract and the broader category label of the taxonomy, whereas low temperatures may force the model into repetitive or rigid patterns that miss nuanced associations
- **Core assumption:** The optimal temperature is data-dependent and may not generalize to taxonomies with highly granular or overlapping sub-domains
- **Evidence anchors:**
  - [section 6] "Increasing the temperature seems to help... best performance at 0.8"
  - [section 6, Table 6] Shows a distinct performance peak at 0.8 for Llama and Gemma before dropping at 1.0
  - [corpus] Corpus evidence for specific temperature tuning in this exact task is weak; most neighbors focus on model selection or general prompt strategies
- **Break condition:** If the classification task requires selecting between very similar fine-grained labels (e.g., sub-domains), higher temperatures might increase error rates by introducing excessive randomness

### Mechanism 3
- **Claim:** Scale-dependent parameter count acts as a proxy for scientific domain coverage
- **Mechanism:** Larger models (e.g., 70B parameters) likely encode a more extensive "world knowledge" of scientific jargon and interdisciplinary relationships during pre-training, allowing them to outperform smaller models (e.g., 3.8B parameters) and traditional fine-tuned encoders (BERT) on zero- or few-shot tasks
- **Core assumption:** The superior performance is primarily due to model capacity and pre-training data quality rather than the specific architecture optimizations of the smaller models
- **Evidence anchors:**
  - [abstract] "Llama 3.1... accuracy of up to 0.82, which is up to 0.08 better than traditional BERT models"
  - [section 6] "The model largest in terms of parameters... delivered the best results"
  - [corpus] General consensus in neighboring papers (e.g., "Strategic Innovation Management") supports LLMs' ability to integrate transdisciplinary insights
- **Break condition:** Performance gains may diminish if the model size exceeds the complexity required for the specific taxonomy level (e.g., classifying only 5 top-level domains)

## Foundational Learning

- **Concept:** **Zero-shot vs. Few-shot Prompting**
  - **Why needed here:** The paper relies on these distinctions to demonstrate that LLMs can perform classification without task-specific training data (zero-shot) but improve significantly with context (few-shot)
  - **Quick check question:** Can you explain why providing a single example in a prompt changes the model's output distribution for a scientific abstract?

- **Concept:** **Temperature Parameter**
  - **Why needed here:** The study identifies a non-intuitive optimal temperature (0.8) for a classification task, challenging the standard practice of using near-zero temperatures for deterministic outputs
  - **Quick check question:** If you set the temperature to 0.0, how might that negatively impact the model's ability to map a complex abstract to a broad category?

- **Concept:** **Hierarchical Taxonomy (ORKG)**
  - **Why needed here:** The target variable is not a simple label but a node in a structured hierarchy (Domain -> Subdomain -> Subject). Understanding this structure is essential for interpreting the 0.82 accuracy metric, which applies only to the *top-level* domains
  - **Quick check question:** Why might accuracy drop if the model attempts to classify the 3rd-level "subject" instead of the 1st-level "domain"?

## Architecture Onboarding

- **Component map:** FORC Dataset (Title + Abstract) -> Preprocessing: Clean text (remove URLs/authors) -> Prompt Engine: Inject ORKG taxonomy + Examples (LangChain) -> Inference Engine: Local LLM via Ollama (Llama 3.1 70B) -> Output: Predicted Research Area

- **Critical path:**
  1. **Data Cleaning:** Ensure abstracts are present; the paper notes errors due to missing metadata
  2. **Prompt Construction:** Formatting the few-shot examples exactly as the best-performing configuration (Prompt 2)
  3. **Inference:** Running the 70B model with temperature=0.8

- **Design tradeoffs:**
  - **Accuracy vs. Resource Cost:** The best model (Llama 70B) required 4x NVIDIA RTX A6000 GPUs and took 22 hours. Smaller models (Phi) were faster (3 hours) but significantly less accurate
  - **Reproducibility:** High temperature (0.8) introduces variance; results are not strictly deterministic across runs

- **Failure signatures:**
  - **Short/Missing Abstracts:** The manual error analysis identified these as primary causes for misclassification
  - **Interdisciplinary Blur:** The model struggles when a text bridges two top-level domains, a limitation of the flat top-level evaluation

- **First 3 experiments:**
  1. **Baseline Verification:** Reproduce the 0.82 accuracy using Llama 3.1 (70B) with few-shot prompting and temp 0.8 on a 10% sample of the FORC dataset to validate the setup
  2. **Error Boundary Test:** Run the classification specifically on papers identified as "short abstracts" or "missing titles" to quantify the performance drop compared to the full-text average
  3. **Generalization Test:** Apply the identical prompt/temperature configuration to a different taxonomy (e.g., ACM Classification) to see if the 0.8 temperature heuristic transfers or if it is overfitted to the ORKG structure

## Open Questions the Paper Calls Out

- **Open Question 1:** How does LLM performance change when classifying scientific texts into fine-grained subdomains and subjects rather than top-level domains?
  - **Basis in paper:** [explicit] The authors state that the current study "merely tackled the highest level of the FORC classification" and that future work should target "subdomains and subjects"
  - **Why unresolved:** The current evaluation methodology was restricted to predicting the five top-level ORKG domains to maintain a high-level perspective
  - **What evidence would resolve it:** Experimental results showing accuracy and F1-scores for the second and third hierarchical levels of the ORKG taxonomy

- **Open Question 2:** How effective are LLMs in categorizing texts using alternative hierarchical schemes like ACM or DDC compared to the ORKG taxonomy?
  - **Basis in paper:** [explicit] The authors explicitly list "integrating alternative classification schemes, such as the ACM Computing Classification System and the Dewey Decimal Classification (DDC)" as a future research direction
  - **Why unresolved:** The experiments were limited solely to the ORKG taxonomy, leaving the transferability of the prompting approach to other structures untested
  - **What evidence would resolve it:** A comparative evaluation of the same LLMs on datasets labeled with ACM or DDC codes

- **Open Question 3:** Does domain-specific fine-tuning of LLMs yield higher accuracy than the few-shot prompting strategies evaluated in this study?
  - **Basis in paper:** [inferred] The paper focuses on "off-the-shelf" models using prompt engineering, noting that traditional models require "specific training data," but it does not test if training the LLMs improves results
  - **Why unresolved:** The study compares prompted LLMs against trained BERT models but does not isolate the potential gains from training (fine-tuning) the LLMs themselves
  - **What evidence would resolve it:** A comparison of accuracy scores between few-shot prompted LLMs and the same LLMs after fine-tuning on the FORC training set

## Limitations

- Performance metrics only validated for top-level domain classification, not subdomains or subjects
- High computational requirements (4x RTX A6000 GPUs, 22 hours) limit practical deployment
- Dataset representativeness limited to Springer Nature publications without systematic coverage of all scientific domains

## Confidence

- **High Confidence:** The superiority of few-shot prompting over zero-shot (accuracy increase from 0.62 to 0.82) and the effectiveness of LLMs compared to traditional BERT models are well-supported by the experimental results
- **Medium Confidence:** The optimal temperature setting of 0.8 and the general advantage of larger models (70B parameters) are demonstrated within this specific task but may not generalize to all scientific text classification scenarios
- **Low Confidence:** The reproducibility of results across different scientific taxonomies and the scalability of the approach for real-time applications remain uncertain without further validation

## Next Checks

1. **Hierarchical Depth Validation:** Test the model's performance on second and third-level taxonomy classification (subdomains and subjects) to determine if accuracy degrades significantly as classification granularity increases

2. **Cross-Taxonomy Transferability:** Apply the identical prompt and temperature configuration (few-shot + temp 0.8) to a different scientific taxonomy system (e.g., ACM Computing Classification System) to assess whether the optimization is specific to ORKG or represents a more general approach

3. **Resource-Constrained Evaluation:** Benchmark the smaller models (Phi 3.8B and Mistral Nemo 12B) with the same prompt strategy to quantify the accuracy-resource tradeoff and establish practical deployment thresholds for organizations with limited computational resources