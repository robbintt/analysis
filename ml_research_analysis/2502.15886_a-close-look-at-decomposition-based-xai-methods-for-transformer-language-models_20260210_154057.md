---
ver: rpa2
title: A Close Look at Decomposition-based XAI-Methods for Transformer Language Models
arxiv_id: '2502.15886'
source_url: https://arxiv.org/abs/2502.15886
tags:
- input
- language
- computational
- layer
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work conducts a comprehensive quantitative and qualitative
  analysis of state-of-the-art decomposition-based XAI methods (ALTI-Logit, LRP, and
  AttnLRP) for transformer language models. The authors close a gap in the literature
  by evaluating these methods side-by-side using a carefully constructed subject-verb
  agreement benchmark dataset with ground truth annotations.
---

# A Close Look at Decomposition-based XAI-Methods for Transformer Language Models

## Quick Facts
- **arXiv ID:** 2502.15886
- **Source URL:** https://arxiv.org/abs/2502.15886
- **Reference count:** 39
- **Primary result:** Comprehensive quantitative and qualitative analysis of ALTI-Logit, LRP, and AttnLRP on transformer LMs, showing significant performance variation across model families

## Executive Summary
This work conducts a comprehensive evaluation of decomposition-based XAI methods for transformer language models, focusing on ALTI-Logit, LRP, and AttnLRP. The authors address a critical gap in the literature by benchmarking these methods against a subject-verb agreement task with ground truth annotations. They extend ALTI-Logit to the Llama model family and propose a fast implementation strategy using modified Gradient×Input. The study reveals significant performance differences across model families, with AttnLRP excelling on BERT and Llama models while ALTI-Logit performs best on GPT-2. The work also demonstrates computational speedups using their LRPx toolbox compared to existing implementations.

## Method Summary
The paper evaluates decomposition-based XAI methods (ALTI-Logit, LRP, AttnLRP) on transformer language models using a subject-verb agreement benchmark. The evaluation uses the Goldberg (2019) SVA dataset with Spacy-generated ground truth annotations for subject identification. The authors extend ALTI-Logit to Llama models and propose a modified Gradient×Input implementation strategy for LRP and AttnLRP using tensor detachment. Four metrics are used: Pointing Game top-2 (PG2), Mean Reciprocal Rank (MRR), Relevance Mass Accuracy (RMA), and Per-Token Accuracy (PTA). The implementation is validated through mathematical proofs of equivalence and computational benchmarks.

## Key Results
- AttnLRP outperforms ALTI-Logit on BERT and Llama-3 models, while ALTI-Logit excels on GPT-2 models
- The modified Gradient×Input implementation (LRPx) achieves ~1.5x speedup compared to existing toolboxes
- Relevance conservation properties hold across all methods, with relevances summing to the prediction logit difference
- Performance varies significantly across model families, suggesting architectural differences impact explanation quality

## Why This Works (Mechanism)

### Mechanism 1: Logit Conservation via Layer-wise Redistribution
Decomposition-based methods provide faithful explanations by enforcing that the sum of relevance scores across input tokens equals the model's prediction logit difference. The model's output logit is treated as a conserved quantity that is propagated backward through network layers, with total relevance redistributed among input neurons at each layer. Core assumption: The prediction logic can be linearly or locally approximated by redistributing contributions based on activation magnitudes and weights.

### Mechanism 2: Implementation via Modified Gradient×Input (LRPx)
Complex relevance propagation rules can be computed using standard automatic differentiation by strategically detaching tensors during the forward pass. By detaching specific tensors (e.g., standard deviation in LayerNorm or denominators in product layers), the chain rule in standard backpropagation effectively computes the LRP rules without explicit manual implementation. Core assumption: The PyTorch autograd engine faithfully executes the chain rule over these modified tensors in a way mathematically equivalent to formal LRP definitions.

### Mechanism 3: Linearization of Non-Linear Transformer Layers
To enable decomposition, complex non-linearities in Transformers are treated as linear operations during the backward pass. Attention matrices are treated as constants, and LayerNorm operations are linearized by treating standard deviation as a constant. Core assumption: The information flow critical to the decision is captured by linear contributions rather than precise non-linear gradients or attention mechanism dependencies.

## Foundational Learning

- **Concept: Layer-wise Relevance Propagation (LRP)**
  - Why needed here: This is the mathematical backbone of the methods analyzed. Without understanding the conservation principle (Input Relevance ≈ Output Relevance), the comparison between methods is opaque.
  - Quick check question: If the output logit is 5.0 and we have 5 input tokens, does LRP guarantee the sum of token relevances is 5.0?

- **Concept: Residual Connections in Transformers**
  - Why needed here: ALTI-Logit relies heavily on tracing information flow through residual connections to aggregate token contributions across layers.
  - Quick check question: How does the "skip connection" in a Transformer block influence where the "relevance" flows during a backward decomposition pass?

- **Concept: Subject-Verb Agreement (SVA) Tasks**
  - Why needed here: This is the ground-truth mechanism used to evaluate the XAI methods. You must understand that a correct explanation should identify the "subject" noun as the cause for the verb's conjugation.
  - Quick check question: In the sentence "The keys to the cabinet are on the table," which noun determines the verb "are," and should therefore receive the highest relevance?

## Architecture Onboarding

- **Component map:** Goldberg dataset -> Spacy dependency parser -> HuggingFace models (BERT, GPT-2, Llama) -> LRPx implementation -> XAI methods (ALTI-Logit, LRP, AttnLRP) -> Evaluation metrics (PG2, MRR, RMA, PTA)

- **Critical path:**
  1. Modify Forward Pass: Implement LRPx logic by wrapping PyTorch layers to detach necessary tensors while preserving forward output values
  2. Compute Gradients: Perform standard backward pass from target logit (or logit difference)
  3. Extract Relevance: Multiply input/hidden activations by computed gradients (R = x · ∇x)

- **Design tradeoffs:**
  - ALTI-Logit vs. AttnLRP: ALTI-Logit is faster/more accurate on GPT-2 but truncates information flow at MLP blocks. AttnLRP is slower traditionally but captures deep feature mixing; superior on BERT/Llama-3.
  - Precision: bfloat16 is sufficient for large Llama models, avoiding need for double precision.

- **Failure signatures:**
  - High Relevance on Special Tokens: If [CLS], [SEP], or <begin_of_text> consistently receive higher relevance than the linguistic subject, the XAI method is failing to capture task logic.
  - Metric Discrepancy: If "Pointing Game" scores are high but "Per-Token Accuracy" is near random, the method identifies general area but fails at precise token-level attribution.

- **First 3 experiments:**
  1. Sanity Check Implementation: Verify LRPx implementation by confirming sum of input relevances equals output logit (Conservation Property)
  2. SVA Benchmarking: Run implemented XAI method on SVA dataset and compute RMA against Spacy-generated ground truth
  3. Speed Benchmark: Compare runtime of modified Gradient×Input approach (LRPx) against standard explicit LRP toolbox on A100 GPU

## Open Questions the Paper Calls Out

### Open Question 1
What are the root causes for the significant variance in explanation quality across different model families? The paper observes different performance patterns but cannot explain why ALTI-Logit excels on GPT-2 while AttnLRP excels on BERT and Llama-3. A systematic ablation study isolating specific architectural differences (normalization placement, attention mechanisms, activation functions) could identify which changes flip the optimal method preference.

### Open Question 2
Can decomposition-based attribution methods outperform gradient-based methods for downstream model intervention tasks such as unbiasing, pruning, or localizing specific behaviors? The paper suggests decomposition-based approaches "might perform even better" than gradient-based methods for controlling model behaviors because of their higher token-level accuracy. Experiments using relevances to guide model pruning or causal interventions would test this hypothesis.

### Open Question 3
How do decomposition-based XAI methods perform on emerging non-Transformer language model architectures, such as State Space Models (SSMs) or xLSTMs? The evaluation approach can be extended to more recent architectures, but the mathematical proofs and empirical evaluations are tailored specifically to Transformers. Extending LRPx to SSMs (e.g., Mamba) would test whether conservation properties and accuracy hold on similar subject-verb agreement tasks.

## Limitations
- Evaluation restricted to subject-verb agreement tasks, which may not capture full linguistic complexity
- Ground truth annotations rely on Spacy's dependency parser, which has limitations with complex syntactic structures
- Computational efficiency claims based on A100 GPU benchmarks may not translate to other hardware configurations
- Extension of ALTI-Logit to Llama models described conceptually but not fully detailed in code

## Confidence

**High Confidence:**
- Mathematical proof of equivalence between modified Gradient×Input and formal LRP rules
- Relative performance rankings of XAI methods across model families
- Computational speedup claims for LRPx implementation
- Observation that AttnLRP performs better on BERT/Llama while ALTI-Logit excels on GPT-2

**Medium Confidence:**
- Generalization of SVA performance to other linguistic tasks
- Sufficiency of bfloat16 precision for Llama-3.2-3B models
- Claim that modified Gradient×Input is "first to support LRP and AttnLRP out-of-the-box"

**Low Confidence:**
- Robustness of ground truth extraction for complex syntactic structures
- Exact numerical equivalence of LRPx implementation across all edge cases
- Claim that ALTI-Logit's truncation at MLP blocks is primary cause of inferior performance on BERT/Llama

## Next Checks

1. **Edge Case Numerical Stability:** Test LRPx implementation on sentences with extreme token embeddings (very large or small values) and verify that the relevance conservation property holds. Specifically, check cases where LayerNorm standard deviation approaches zero or where softmax outputs become numerically unstable. This validates the core assumption that the modified Gradient×Input approach maintains mathematical equivalence across all input ranges.

2. **Cross-Linguistic Generalization:** Apply the evaluation framework to a multilingual subject-verb agreement dataset (e.g., Universal Dependencies treebanks) to verify whether observed method performance rankings hold across languages with different syntactic structures. This tests whether the SVA benchmark's findings generalize beyond English Wikipedia text.

3. **Alternative Ground Truth Validation:** Recompute evaluation metrics using an alternative dependency parser (e.g., Stanford CoreNLP or AllenNLP) and compare results. Calculate inter-annotator agreement between parsers on a sample of 100 sentences to quantify uncertainty introduced by ground truth extraction process. This validates whether observed performance differences between XAI methods are robust to variations in ground truth annotation.