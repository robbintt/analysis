---
ver: rpa2
title: 'CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits
  of Large Language Models'
arxiv_id: '2505.13559'
source_url: https://arxiv.org/abs/2505.13559
tags:
- llms
- en-zh
- en-ta
- en-ms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CS-Sum, the first benchmark for code-switching
  dialogue summarization across Mandarin-English, Tamil-English, and Malay-English,
  featuring 900-1300 human-annotated dialogues per language pair. Evaluating ten large
  language models, including open-source and closed-source variants, across few-shot,
  translate-summarize, and fine-tuning (LoRA, QLoRA on synthetic data) settings, the
  authors find that while automated metrics are high, LLMs frequently make subtle
  errors that distort meaning.
---

# CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models

## Quick Facts
- **arXiv ID:** 2505.13559
- **Source URL:** https://arxiv.org/abs/2505.13559
- **Reference count:** 15
- **Primary result:** Large language models struggle with code-switching dialogue summarization, with error rates ranging from 15% to 87% across language pairs and error types, despite high automated metric scores.

## Executive Summary
This paper introduces CS-Sum, the first benchmark for code-switching dialogue summarization across Mandarin-English, Tamil-English, and Malay-English. Evaluating ten large language models, including open-source and closed-source variants, across few-shot, translate-summarize, and fine-tuning settings, the authors find that while automated metrics are high, LLMs frequently make subtle errors that distort meaning. Three error types are identified: Code-Switching Loss (ignoring non-English content), Meaning Shift from Poor Translation (misinterpreting CS segments), and Speaker Misattribution (incorrectly assigning dialogue). Error rates vary by language pair and model, with fine-tuning on synthetic data not reliably improving comprehension, highlighting the need for specialized training on real code-switched data.

## Method Summary
The CS-Sum benchmark comprises 900-1300 human-annotated dialogues per language pair (EN-ZH, EN-TA, EN-MS) translated from DialogSum and SAMSum test sets. Synthetic training data was generated using Gemini-2-flash to convert 19,014 English dialogues into code-switched versions. Ten LLMs were evaluated across few-shot (3 examples), translate-summarize (internal translation), and parameter-efficient fine-tuning (LoRA, QLoRA) settings. Automated metrics included ROUGE-L, BERTScore, SBERT-Cosine, Jaccard, and METEOR, supplemented by LLM-driven error analysis using GPT-4o prompts to identify Code-Switching Loss, Meaning Shift, and Speaker Misattribution errors.

## Key Results
- Closed-source models (GPT-4o, GPT-4o-mini) consistently outperform open-source models across all language pairs
- Automated metrics (BERTScore >0.88) systematically overestimate LLM performance on code-switched inputs
- EN-TA exhibits the highest error rates due to Tamil's morphological complexity and syntactic divergence from English
- Fine-tuning on synthetic data can amplify errors when distributional mismatch (KL divergence up to 2.48) exists between synthetic and human CS patterns

## Why This Works (Mechanism)

### Mechanism 1: Metric-Comprehension Disconnect
- Claim: Standard summarization metrics systematically overestimate LLM performance on code-switched inputs because they measure surface-level overlap rather than cross-lingual semantic fidelity
- Mechanism: Embedding-based metrics capture topical relevance but fail to penalize omissions from non-English spans; LLMs generate fluent English summaries that appear semantically plausible while discarding or misinterpreting CS content
- Core assumption: Embeddings trained predominantly on monolingual English text encode cross-lingual misattribution patterns as "similar" rather than "erroneous"
- Evidence anchors: Abstract states LLMs make subtle mistakes that alter complete meaning despite high scores; Gemma-2-9B achieves BERTScore 0.903 while misrepresenting speaker confirmation; corpus neighbors focus on benchmark creation rather than metric failure analysis
- Break condition: If metrics incorporated explicit cross-lingual alignment checks or factual consistency scoring tuned for CS discourse, the disconnect would diminish

### Mechanism 2: Synthetic Data Distribution Shift Amplifies Errors
- Claim: Fine-tuning on LLM-generated synthetic CS data can degrade comprehension when the synthetic distribution diverges from human CS patterns
- Mechanism: Models learn to replicate synthetic surface patterns rather than true CS understanding; distributional mismatch (measured by KL divergence) correlates with error rate increases post-adaptation
- Core assumption: The Mahalanobis distance between synthetic and human CS metrics predicts fine-tuning quality
- Evidence anchors: EN-ZH shows KL divergence 2.48 vs. ≤0.55 for EN-TA/EN-MS; SEA-LION-Gemma-2-9B CSL jumps from 53.73% to 83.94% after LoRA on synthetic data; synthetic data generation limits are acknowledged by authors
- Break condition: If synthetic data were filtered to match human CS metric distributions or generated with human-in-the-loop validation, error amplification could be reduced

### Mechanism 3: Language-Pair-Specific Morphological and Syntactic Load
- Claim: EN-TA exhibits higher error rates due to Tamil's morphological complexity and syntactic divergence from English, increasing token-level ambiguity and speaker attribution failures
- Mechanism: Tamil's agglutinative morphology and SOV structure create longer dependency chains; LLMs trained on predominantly English/SVO data struggle with span-level alignment, leading to CSL and SMA
- Core assumption: Morphological complexity directly correlates with CS summarization difficulty
- Evidence anchors: EN-TA shows consistently higher error rates across models; EN-TA has longest average CS utterance length (9.642 tokens) and Tamil dominates as matrix language; CS-Dialogue dataset (Mandarin-English only) does not provide comparative morphological data
- Break condition: If models were pre-trained on balanced CS corpora with explicit morphological alignment supervision, the language-pair gap would narrow

## Foundational Learning

- **Code-Switching Metrics (M-Index, I-Index, Burstiness, Span Entropy, Memory)**
  - Why needed here: Quantifying CS structure enables diagnosis of distributional mismatch between synthetic and human data
  - Quick check question: Can you explain why negative burstiness indicates regular alternation while positive values indicate clustering?

- **Parameter-Efficient Fine-Tuning (LoRA, QLoRA)**
  - Why needed here: The paper uses these methods to adapt models to CS-Sum-Syn; understanding rank (r), scaling factor (α), and learning rate tradeoffs is essential for reproduction
  - Quick check question: What is the practical impact of using bf16 with gradient checkpointing for memory-constrained training?

- **Error Taxonomy for CS Summarization (CSL, MST, SMA)**
  - Why needed here: The paper's core contribution is identifying these failure modes; they structure all qualitative analysis
  - Quick check question: Given a summary that correctly captures English content but omits a key decision made in Tamil, which error type applies?

## Architecture Onboarding

- **Component map**: CS dialogue (human-annotated or Gemini-2-flash synthetic) -> Model (open-source LLMs 2B-9B or GPT-4o) -> Settings (few-shot, translate-summarize, LoRA/QLoRA) -> Evaluation (automated metrics + LLM-driven error analysis)

- **Critical path**: 1. Reproduce Few-Shot baseline on EN-ZH, EN-TA, EN-MS test sets 2. Run error analysis using provided GPT-4o prompts (Appendix C) 3. Compare LoRA-finetuned model with and without Mahalanobis filtering (Appendix D)

- **Design tradeoffs**: Synthetic vs. human data (scale vs. distributional mismatch); metric selection (ROUGE-L sensitive to non-English output vs. BERTScore/SBERT-Cosine overestimate quality); filtering threshold (aggressive filtering benefits some architectures but degrades others)

- **Failure signatures**: CSL (summary ignores non-English spans), MST (hallucinated intent or misinterpreted CS phrases), SMA (swapped speaker roles), high BERTScore (>0.88) with CSL/MST/SMA >50% indicates metric-comprehension disconnect

- **First 3 experiments**: 1. Establish Few-Shot baseline for Gemma-2-9B across all three language pairs; compute both automated metrics and error rates 2. Fine-tune Gemma-2-9B with LoRA on CS-Sum-Syn; measure whether metric improvements correlate with error rate changes 3. Apply Mahalanobis filtering to EN-ZH training data; compare LoRA performance on filtered vs. unfiltered synthetic data for Mistral-7B and Qwen2.5-7B

## Open Questions the Paper Calls Out

- Can alternative data filtering strategies or thresholds beyond Mahalanobis distance improve generalization across diverse LLM architectures? (Appendix D states further investigation is required to determine if alternative filtering thresholds could yield better generalization across LLMs)

- Does the ability to summarize code-switched dialogues correlate with performance on other long-context NLP tasks? (Section 7 notes extending the benchmark to tasks like machine translation, dialogue generation, and question answering would provide broader understanding)

- Does fine-tuning on human-curated code-switched data eliminate the semantic error types (CSL, MST, SMA) found in synthetic fine-tuning? (Section 5 shows synthetic fine-tuning amplifies errors under distribution shift, and Section 7 states synthetic data does not capture real-world CS complexities)

## Limitations

- The synthetic data generation process may not accurately capture real-world CS patterns, particularly for morphologically complex languages like Tamil
- The human evaluation protocol for error classification relies on GPT-4o interpretation, introducing subjectivity
- Test sets containing 900-1300 dialogues per language pair may not capture full diversity of CS phenomena

## Confidence

- **High confidence**: Existence of code-switching loss as systematic error pattern, general superiority of closed-source models, observation that synthetic data fine-tuning does not reliably improve comprehension
- **Medium confidence**: Specific error rate percentages for individual models and language pairs, quantitative relationship between Mahalanobis distance and fine-tuning effectiveness, proposed error taxonomy's completeness
- **Low confidence**: Exact threshold at which automated metrics become misleading, generalizability of synthetic data generation limitations beyond Gemini-2-flash, assertion that no LLM currently achieves acceptable CS summarization performance

## Next Checks

1. **Cross-linguistic error analysis**: Apply CS-Sum framework to an additional language pair (e.g., Spanish-English or Hindi-English) to test whether observed error patterns generalize beyond the three studied pairs

2. **Human-grounded metric validation**: Conduct controlled human evaluation comparing BERTScore/ROUGE performance against direct comprehension questions about non-English content in summaries, establishing quantitative bounds on metric reliability

3. **Synthetic data quality audit**: Generate synthetic CS data using multiple LLMs (Claude, Gemini, Llama) and compare their Mahalanobis distances to human CS patterns, testing whether generation approach or synthetic nature itself drives distribution mismatch