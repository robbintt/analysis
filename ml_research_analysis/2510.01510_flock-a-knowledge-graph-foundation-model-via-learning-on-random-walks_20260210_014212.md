---
ver: rpa2
title: 'Flock: A Knowledge Graph Foundation Model via Learning on Random Walks'
arxiv_id: '2510.01510'
source_url: https://arxiv.org/abs/2510.01510
tags:
- graph
- prediction
- random
- relation
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of zero-shot link prediction
  in knowledge graphs, where models must generalize to unseen entities and relations.
  The key limitation of existing approaches is their reliance on deterministic node-relation
  equivariance, which forces structurally similar but semantically distinct relations
  to have identical representations, reducing expressiveness.
---

# Flock: A Knowledge Graph Foundation Model via Learning on Random Walks

## Quick Facts
- arXiv ID: 2510.01510
- Source URL: https://arxiv.org/abs/2510.01510
- Authors: Jinwoo Kim, Xingyue Huang, Krzysztof Olejniczak, Kyungbin Min, Michael Bronstein, Seunghoon Hong, İsmail İlkan Ceylan
- Reference count: 40
- Key outcome: FLOCK achieves state-of-the-art zero-shot link prediction on 54 knowledge graphs by learning transferable structural patterns through random walks and probabilistic node-relation equivariance

## Executive Summary
FLOCK addresses the challenge of zero-shot link prediction in knowledge graphs, where models must generalize to unseen entities and relations. The paper identifies that existing knowledge graph foundation models (KGFMs) suffer from deterministic node-relation equivariance, which forces structurally similar but semantically distinct relations to have identical representations, reducing expressiveness. To overcome this limitation, the authors introduce probabilistic node-relation equivariance, which ensures equivariance only in distribution while allowing principled randomization to break symmetries during inference. FLOCK leverages random walks to encode structural information, anonymizes nodes and relations to ensure generalization, and uses sequence models with learned pooling to aggregate representations.

## Method Summary
FLOCK is a knowledge graph foundation model that learns transferable representations through random walks over the graph structure. The model generates multiple random walks of configurable length, anonymizes nodes and relations during recording to strip identity while preserving structural roles, and processes these sequences through a Bi-GRU to produce state update proposals with confidence scores. A consensus protocol aggregates proposals from multiple walks using learned weighting, and the process iterates for several steps before making predictions. The key innovation is probabilistic node-relation equivariance, which relaxes deterministic symmetry constraints to allow the model to distinguish structurally identical but semantically different relations while maintaining transferability across graphs.

## Key Results
- Perfect 100% accuracy on PETALS diagnostic dataset where existing KGFMs achieve only 50%
- State-of-the-art performance on entity prediction across 54 diverse knowledge graphs
- State-of-the-art performance on relation prediction across 54 diverse knowledge graphs
- Demonstrates superior generalization to unseen entities and relations compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Node-Relation Equivariance
Relaxing deterministic equivariance to distribution-level equivariance enables models to distinguish structurally isomorphic but semantically distinct relations while preserving transferability. The model maintains equivariance in expectation across stochastic forward passes, but each individual forward pass produces different representations due to random walk sampling—allowing structurally identical relations to be distinguished during inference.

### Mechanism 2: Random Walk Anonymization for Transfer
Anonymizing node and relation identities while preserving structural roles enables zero-shot generalization to unseen entities and relations. The recording protocol assigns anonymized IDs (1, 2, 3... for nodes; α, β, γ... for relations) in order of first encounter during each walk, stripping identity while retaining relative position and local connectivity patterns.

### Mechanism 3: Multi-Walk Consensus with Confidence Weighting
Aggregating proposals from multiple random walks with learned confidence scores suppresses uninformative signals from dangling walk regions. Each walk produces update proposals with confidence scores; the consensus protocol computes softmax-normalized weighted averages, where low-confidence proposals (e.g., from walk endpoints in sparse regions) are automatically downweighted.

## Foundational Learning

- **Concept: Graph Isomorphism**
  - Why needed here: The paper's theoretical claims rest on isomorphism invariance—understanding when two KGs are "the same up to renaming" is essential for grasping why deterministic equivariance fails.
  - Quick check question: Given two KGs with identical edge patterns but different node labels (G₁: A–likes→B, B–likes→C; G₂: X–likes→Y, Y–likes→Z), would an isomorphism-invariant function return the same values for corresponding queries?

- **Concept: Equivariance vs Invariance**
  - Why needed here: The paper distinguishes deterministic equivariance (strict identity) from probabilistic equivariance (distributional identity). Understanding this distinction explains why FLOCK can generalize while being more expressive.
  - Quick check question: If a function f is equivariant to permutations and you apply permutation π to inputs, does f(π(x)) = π(f(x)) or f(π(x)) = f(x)?

- **Concept: Random Walk Cover Time**
  - Why needed here: The universality proof depends on walks eventually covering all edges. Understanding cover time bounds explains why longer walks and more samples improve performance.
  - Quick check question: In a graph with n nodes and m edges, how does the expected number of steps to visit all edges scale (roughly)?

## Architecture Onboarding

- **Component map:**
  Input: KG G = (V, E, R), query q = (h, r, ?)
  ↓
  Random Walk Sampler → n walks of length ℓ
  ↓
  Recording Protocol → anonymized sequences z (with current embeddings v, r)
  ↓
  Sequence Processor (Bi-GRU) → proposals (Δv, Δr) with confidences (a, b)
  ↓
  Consensus Protocol → aggregated updates per entity/relation
  ↓
  Residual Update: v^(i+1) = v^(i) + Δv, r^(i+1) = r^(i) + Δr
  ↓
  Repeat I iterations → Binary classifier head → prediction ŷ

- **Critical path:** Walk sampling → recording protocol → sequence processor → consensus. Errors in anonymization logic propagate silently; verify ID assignment is consistent within each walk but reset across walks.

- **Design tradeoffs:**
  - More walks (higher n) → better coverage, higher compute cost
  - Longer walks (higher ℓ) → capture longer-range dependencies, risk of visiting irrelevant regions
  - More ensemble passes (P) → reduced variance, inference slowdown
  - Test-time walk adaptation (Equation 4) helps size generalization but requires tuning the harmonic mean formula

- **Failure signatures:**
  - Random guessing on isomorphic-distinguishing tasks: Indicates deterministic equivariance is still being enforced (check if anonymization is properly resetting per forward pass)
  - Strong transductive but weak inductive performance: Suggests overfitting to training graph structure rather than learning transferable patterns
  - Performance degradation on large graphs: Likely insufficient walk coverage; increase n adaptively using Equation 4

- **First 3 experiments:**
  1. Reproduce PETALS diagnostic: Train FLOCK from scratch on PETALS instances; verify 100% accuracy vs 50% baseline. This validates the core expressivity claim.
  2. Ablate ensemble size: Run zero-shot inference with P = 1, 2, 4, 8, 16 on a held-out KG; plot MRR vs P to find the saturation point (paper shows plateau ~12).
  3. Test walk length sensitivity: Fix other hyperparameters, vary ℓ ∈ {32, 64, 128, 256} on medium-sized KG; observe if longer walks help or hurt (theoretical bound suggests ℓ > C_{n,m}/δ for coverage, but empirical optimum may differ).

## Open Questions the Paper Calls Out
None

## Limitations

- **Random Walk Coverage Requirements**: The theoretical universality proof assumes random walks will eventually visit all edges with high probability, but this requires careful parameter tuning and may not generalize well across diverse graph structures.
- **Computational Complexity**: The ensemble-based inference requiring multiple forward passes (P=8-12) introduces substantial computational overhead during both training and inference, potentially limiting scalability.
- **Expressivity vs Transferability Trade-off**: While probabilistic equivariance theoretically enables better expressivity, there's an inherent tension between learning graph-specific patterns and maintaining transfer capability.

## Confidence

**High Confidence** (supported by strong empirical evidence):
- FLOCK's ability to solve the PETALS diagnostic task where baselines fail
- Overall state-of-the-art performance across the 54 KG benchmark
- The multi-walk ensemble approach improving prediction quality

**Medium Confidence** (supported by theoretical arguments and partial evidence):
- The universality claim for isomorphism-invariant link functions
- The theoretical advantages of probabilistic over deterministic equivariance
- The recording protocol's ability to preserve sufficient structural information

**Low Confidence** (limited or indirect evidence):
- The effectiveness of the confidence-weighted consensus protocol (no direct ablation studies)
- The generalizability of the adaptive walk length formula across diverse graph domains
- The scalability of the approach to extremely large KGs (no experiments with graphs >1M edges)

## Next Checks

1. **Ablation Study on Walk Length Formula**: Systematically vary the harmonic mean parameter in Equation 4 across multiple graph families (scale-free, random, small-world) to validate its effectiveness in ensuring adequate edge coverage without excessive computation.

2. **Extreme-Scale KG Performance**: Evaluate FLOCK on knowledge graphs with >10M edges to identify performance bottlenecks and assess whether the current architecture scales efficiently or requires modifications for industrial-scale applications.

3. **Semantic vs Structural Ablation**: Design controlled experiments on synthetic KGs where certain relation types have identical structural patterns but different semantic meanings, then measure whether FLOCK can distinguish them while maintaining transfer capability on structurally-similar but semantically-distinct relations.