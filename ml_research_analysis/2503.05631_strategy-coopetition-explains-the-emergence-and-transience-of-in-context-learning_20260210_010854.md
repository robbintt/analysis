---
ver: rpa2
title: Strategy Coopetition Explains the Emergence and Transience of In-Context Learning
arxiv_id: '2503.05631'
source_url: https://arxiv.org/abs/2503.05631
tags:
- ciwl
- learning
- training
- layer
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the emergence and transience of in-context
  learning (ICL) in transformers. The authors reproduce the finding that ICL can disappear
  after long training times and discover that the asymptotic strategy is a hybrid
  approach they call "context-constrained in-weights learning" (CIWL).
---

# Strategy Coopetition Explains the Emergence and Transience of In-Context Learning

## Quick Facts
- arXiv ID: 2503.05631
- Source URL: https://arxiv.org/abs/2503.05631
- Authors: Aaditya K. Singh; Ted Moskovitz; Sara Dragutinovic; Felix Hill; Stephanie C. Y. Chan; Andrew M. Saxe
- Reference count: 40
- Primary result: ICL emerges transiently because it shares subcircuits with the asymptotic CIWL strategy, creating "strategy coopetition"

## Executive Summary
This paper investigates why in-context learning (ICL) emerges and then disappears during transformer training. Through mechanistic analysis of a 2-layer attention-only model on bursty image classification data, the authors discover that the asymptotic strategy is "context-constrained in-weights learning" (CIWL) implemented via skip-trigram-like attention in Layer 2. They find that ICL and CIWL share sub-circuits, leading to both cooperative and competitive interactions - a phenomenon they term "strategy coopetition." A minimal mathematical model captures these dynamics and identifies data properties that enable persistent ICL.

## Method Summary
The authors train a 2-layer attention-only transformer (d_model=64, 8 heads/layer) on Omniglot images augmented to 12,800 classes. Images are encoded by ImageNet-pretrained ResNet18 and paired with one-hot label tokens. "Bursty" sequences ensure the query exemplar's class appears in context. The model learns through cross-entropy loss, with evaluators tracking ICL (OOD labels), CIWL (correct label, wrong exemplar), and Flip (label swapped vs training). Mechanistic analysis examines attention patterns, and a mathematical toy model captures the observed dynamics.

## Key Results
- ICL emerges but is transient, disappearing after ~1.7x10^7 sequences as CIWL dominates
- CIWL is implemented via Layer 2 heads acting as skip-trigram-copiers that attend to correct labels regardless of exemplar pairing
- ICL and CIWL share Layer 2 subcircuits, enabling cooperative emergence but competitive dynamics once both are established
- Mathematical model captures the dynamics and identifies class diversity as key to enabling persistent ICL

## Why This Works (Mechanism)

### Mechanism 1: CIWL Skip-Trigram Implementation
- Claim: The asymptotic strategy (context-constrained in-weights learning) is implemented via Layer 2 heads acting as skip-trigram-copiers that attend to the correct label regardless of exemplar pairing.
- Mechanism: L2 heads compute attention from the query token to the label token that was paired with the query's class during training, then copy that label to the output via the value pathway. This creates a "… [label] … [query] → [label]" skip-trigram pattern.
- Core assumption: The mechanism requires K- and V-composition from Layer 1, even though skip-trigrams are implementable in a single layer (the paper does not fully explain why the 2-layer structure emerges).
- Evidence anchors:
  - [abstract] "CIWL is implemented through skip-trigram-like attention mechanisms in Layer 2"
  - [Section 4.1] "L2 heads attend to the correct label, regardless of what exemplar it is paired with in context… causally indicating that each L2 head is serving to copy input labels to the output"
  - [corpus] Weak direct corpus support; related work on induction heads (Olsson et al. 2022) provides foundational context but not CIWL-specific mechanisms
- Break condition: If L2 heads cannot reliably attend to the correct label token across varying context positions, or if the value-copying pathway is disrupted.

### Mechanism 2: Layer 1 Strategy Switch
- Claim: The transition from ICL dominance to CIWL dominance is driven by Layer 1 heads switching their attention behavior, while Layer 2 remains functionally stable.
- Mechanism: During ICL dominance, L1 heads act as previous-token heads (copying token information forward to enable induction). During CIWL dominance, L1 heads switch to self-attention, effectively mapping label tokens to prototypical embeddings. L2 weights remain largely unchanged after initial formation.
- Core assumption: Assumption: L1 self-attention implements a learned mapping from label tokens to semantic embeddings that L2 can then retrieve.
- Evidence anchors:
  - [Section 5.1] "The transition from ICL to CIWL is implemented by a transition in the role of L1 heads, which switch from previous-token-attention to attending-to-self"
  - [Fig 3b] Fixing L2 weights to end-of-training values reproduces original behavior at all points after initial phase change, indicating L2 stability
  - [corpus] Related work on task recognition vs. task learning (Lin & Lee 2024) discusses similar strategy switches but not this specific mechanism
- Break condition: If L1 cannot learn both attention patterns sequentially, or if the switch is blocked (e.g., by clamping previous-token attention).

### Mechanism 3: Strategy Coopetition via Shared L2 Subcircuits
- Claim: ICL emerges despite not being asymptotically preferred because it shares L2 subcircuits with CIWL, enabling faster development through cooperative gradient signals.
- Mechanism: Both ICL and CIWL use the same L2 induction heads but with different L1 computations. ICL emerges faster because: (1) it reduces loss on bursty data, (2) it lies on the learning path toward CIWL (shared L2), and (3) CIWL forms slowly enough that ICL can transiently dominate. Competition emerges in L1 (mutually exclusive attention patterns).
- Core assumption: The cooperative effect requires that CIWL is not fully formed when ICL begins to emerge; once CIWL solidifies, competitive dynamics dominate.
- Evidence anchors:
  - [Section 5.2] "When the Layer 2 subcircuits are provided, ICL emerges quickly… ICL actually emerges more easily when it is not the only viable strategy"
  - [Fig 4c] "Once CIWL has formed (later checkpoints), ICL does not re-emerge even when switching to bursty data"
  - [Section 5.3] L2 weights from mid-training CIWL-only runs (but not early or late) can enable ICL, showing a "Goldilocks" window of reusability
  - [corpus] Competition between strategies noted in Nguyen & Reddy 2024, Park et al. 2024; cooperative dynamics are novel to this work
- Break condition: If CIWL forms too quickly (low class diversity), ICL may never emerge; if strategies do not share subcircuits, cooperative emergence fails.

## Foundational Learning

- **Concept: Induction heads and previous-token attention**
  - Why needed here: The paper assumes familiarity with induction heads as the canonical ICL mechanism. Understanding that L1 previous-token heads feed L2 induction heads is essential for grasping why shared subcircuits enable coopetition.
  - Quick check question: Can you explain how a 2-layer induction circuit implements the pattern "[A][B] … [A] → [B]"?

- **Concept: K-, Q-, and V-composition**
  - Why needed here: The paper reports that CIWL requires K- and V-composition between layers (L2 depends on L1 for keys and values). Understanding composition is necessary to interpret the ablation results.
  - Quick check question: If ablating L1 outputs drops CIWL accuracy from 98.7% to 59.3% when preserving only queries in L2, what does this indicate about the composition type?

- **Concept: Skip-trigram circuits**
  - Why needed here: CIWL is implemented as a distributed skip-trigram mechanism. The paper references Elhage et al. (2021) as background on how single-layer transformers implement skip-trigrams.
  - Quick check question: How does a skip-trigram differ from a bigram in terms of the attention pattern required?

## Architecture Onboarding

- **Component map:**
  Embedding layer (ResNet18 pretrained image encoder + learned positional embeddings) -> Layer 1: 8 attention heads (provides K/V to L2) -> Layer 2: 8 attention heads (skip-trigram copiers, direct output) -> Unembedding layer (maps to 12,800 class labels)

- **Critical path:**
  1. Input tokens (exemplar-image embeddings + label tokens) → embedding layer
  2. L1 attention: during ICL phase, previous-token attention; during CIWL phase, self-attention on labels
  3. L2 attention: attends from query token to correct label token (using L1-computed K/V)
  4. L2 output → unembedding → class prediction

- **Design tradeoffs:**
  - Learned absolute positional embeddings required for 2L attention-only models to exhibit ICL emergence (sinusoidal or RoPE fail in this setting)
  - Attention-only chosen for mechanistic interpretability; adding MLPs introduces noise and resurgent ICL behavior
  - High class count (12,800) slows CIWL formation, allowing ICL to emerge transiently

- **Failure signatures:**
  - No ICL emergence: check if class count is too low or exemplars per class too few (CIWL forms too fast)
  - ICL never fades: check if context/query exemplars are exactly matched (makes ICL asymptotically preferred)
  - Training plateau: L2 subcircuits may be stuck; check if L1 is developing correctly

- **First 3 experiments:**
  1. **Reproduce transience curve**: Train the 2L attention-only model on bursty data with 12,800 classes, track ICL/CIWL/Flip evaluators over time. Confirm ICL peaks then fades as CIWL rises.
  2. **Clamp L2 weights**: Fix L2 weights to a mid-training checkpoint and continue training. Verify that ICL emergence timing changes based on which L2 checkpoint is used (testing coopetition hypothesis).
  3. **Vary class count**: Train with 1,600 vs 12,800 classes. Confirm that lower class count accelerates CIWL and reduces/eliminates transient ICL emergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does a brief divot occur in CIWL loss formation during training, as mirrored in both the toy model and transformer experiments?
- Basis in paper: [explicit] "We do not fully understand what leads to this brief divot" (Section 6); authors speculate it may relate to competition forces during ICL formation.
- Why unresolved: The mathematical toy model reproduces this phenomenon, but the mechanistic cause in actual transformers remains unclear.
- What evidence would resolve it: Ablation experiments isolating the competition term's timing, or causal interventions tracking gradient contributions from each mechanism during the divot period.

### Open Question 2
- Question: Why does K- and V-composition between layers emerge when a one-layer network could implement skip-trigram-copiers alone?
- Basis in paper: [explicit] "Given that a one-layer network could implement skip-trigram-copiers, why does this observed K- and V- composition between layers emerge?" (Section 4.2).
- Why unresolved: The functional benefit of this more complex two-layer architecture for CIWL is not explained.
- What evidence would resolve it: Comparing convergence speed, sample efficiency, or generalization between single-layer and two-layer architectures on the same task.

### Open Question 3
- Question: Do attention heads store class-specific skip-trigram features in superposition, analogous to sparse features in MLP layers?
- Basis in paper: [explicit] "We hope this heads-in-superposition hypothesis, and preliminary evidence supporting it, can motivate further work on understanding individual attention head function" (Appendix C.2).
- Why unresolved: Evidence is preliminary—non-additive head interactions were observed, but direct decomposition into sparse skip-trigram features has not been demonstrated.
- What evidence would resolve it: Applying sparse autoencoder techniques to attention head outputs to decompose them into interpretable skip-trigram features.

### Open Question 4
- Question: Does the matched-exemplar intervention for persistent ICL generalize to real language data and larger-scale models?
- Basis in paper: [inferred] from Section 7 showing the intervention works in larger models but only on synthetic data; authors note the setup uses "bursty" data reflecting real-world distributions but the exact exemplar-matching intervention has no direct language analogue.
- Why unresolved: The intervention requires exact matching between context and query exemplars, which may not translate directly to natural language contexts.
- What evidence would resolve it: Testing analogous interventions in language models, such as ensuring few-shot examples contain near-identical phrasing to the query.

## Limitations

- The mechanistic interpretations of attention patterns lack rigorous quantitative validation through causal interventions
- The claim that ICL emerges specifically because it's "on the path" to CIWL assumes a particular causal ordering not fully ruled out by ablation studies
- The highly simplified 2-layer architecture may not generalize to deeper, more complex architectures with MLPs

## Confidence

- **High confidence**: The empirical observation that ICL is transient and gives way to CIWL in the 2-layer attention-only setting; the basic mechanistic description of CIWL as skip-trigram copying; the mathematical model capturing the qualitative dynamics
- **Medium confidence**: The interpretation of "coopetition" as the mechanism driving ICL emergence; the claim that ICL emerges specifically because it's "on the path" to CIWL; the assertion that shared L2 subcircuits are necessary for cooperative emergence
- **Low confidence**: The generalizability of these dynamics to deeper architectures; the claim that ICL emerges despite not being optimal rather than simply being an intermediate stage; the precise quantitative relationship between class diversity and transition timing

## Next Checks

1. **Causal intervention study**: Perform systematic ablations where L2 weights are frozen at various checkpoints while training L1, measuring whether ICL can still emerge and whether the timing matches predictions from the coopetition hypothesis.

2. **Architecture scaling experiment**: Test whether the ICL→CIWL transition dynamics persist in 4-layer and 6-layer attention-only transformers, or whether deeper architectures fundamentally change the cooperative/competitive balance.

3. **Alternative data distribution test**: Evaluate the model on non-bursty data where query class is absent from context, measuring whether CIWL can still form and whether ICL remains transient or becomes permanent, to test the claim that ICL emerges specifically due to the path-dependence created by data properties.