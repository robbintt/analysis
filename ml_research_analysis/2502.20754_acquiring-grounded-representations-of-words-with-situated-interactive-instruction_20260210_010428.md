---
ver: rpa2
title: Acquiring Grounded Representations of Words with Situated Interactive Instruction
arxiv_id: '2502.20754'
source_url: https://arxiv.org/abs/2502.20754
tags:
- agent
- learning
- knowledge
- object
- perceptual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an interactive instruction-based approach for
  acquiring grounded representations of language. The agent learns perceptual, spatial,
  semantic, and procedural knowledge through mixed-initiative dialog with a human
  instructor while performing tasks.
---

# Acquiring Grounded Representations of Words with Situated Interactive Instruction

## Quick Facts
- arXiv ID: 2502.20754
- Source URL: https://arxiv.org/abs/2502.20754
- Authors: Shiwali Mohan; Aaron H. Mininger; James R. Kirk; John E. Laird
- Reference count: 3
- One-line primary result: Interactive instruction enables 93-100% accuracy learning of grounded words with 1-13 examples

## Executive Summary
This paper presents an interactive instruction-based approach for acquiring grounded representations of language. The agent learns perceptual, spatial, semantic, and procedural knowledge through mixed-initiative dialog with a human instructor while performing tasks. The agent uses Soar's cognitive architecture to integrate perception, action, and language processing, storing learned knowledge in semantic and episodic memories. Evaluation shows fast, incremental learning: the agent acquired 93-100% accuracy for nouns/adjectives (1-13 examples), prepositions (3.17 examples), and action verbs (1.26 examples) through interactive instruction with a table-top robot.

## Method Summary
The system uses Soar cognitive architecture with a Kinect sensor for perception, Link-Grammar parser for language, and a table-top robot arm for action. Learning occurs through mixed-initiative dialog where the agent detects knowledge gaps (impasses) and proactively queries the instructor. Knowledge is stored in semantic memory (word-to-symbol maps), episodic memory (task histories), and procedural memory (action rules). Spatial prepositions are grounded using logical compositions of primitive predicates, while action verbs are learned through retrospective projection and chunking from single demonstrations.

## Key Results
- Nouns/adjectives: 100% accuracy after 1-1.5 examples for color/size, 96% after 12.9 examples for shape
- Prepositions: 93.98% accuracy after 3.17 examples on average
- Verbs: 100% accuracy after 1.26 examples
- Response time under 2 seconds for all concepts

## Why This Works (Mechanism)

### Mechanism 1: Impasse-Driven Knowledge Acquisition
Interactive learning efficiency improves when the agent detects specific knowledge gaps and proactively queries an instructor. When the agent cannot ground a word or select an operator, the architecture enters an "impasse" substate that triggers a new interaction segment to query the human instructor for the missing definition. The result resolves the impasse and creates new semantic or procedural entries.

### Mechanism 2: Retrospective Projection for Procedural Learning
Agents acquire complex action sequences from single demonstrations by combining episodic memory retrieval with internal simulation. After instructor guidance, the agent retrieves the initial state from episodic memory, uses internal action models to simulate the instructed actions forward, and chunks this explanation into a permanent production rule.

### Mechanism 3: Compositional Spatial Symbol Grounding
Spatial prepositions generalize from few examples by mapping words to logical compositions of primitive spatial predicates rather than fixed coordinates. The Spatial Visual System computes low-level primitives, and the agent stores semantic compositions as logical structures of conjunctions and disjunctions of these primitives.

## Foundational Learning

- **Concept: Soar Impasses & Substates**
  - Why needed here: The entire learning trigger depends on the agent halting when knowledge is missing and creating a substate to handle the learning interaction
  - Quick check question: Can you explain what happens in Soar when no operator is selected or when multiple operators conflict?

- **Concept: Chunking (Explanation-Based Learning)**
  - Why needed here: This converts temporary reasoning in a substate into a permanent production rule, enabling "one-shot" learning of verbs
  - Quick check question: How does Chunking differ from memorization in terms of generalizing to new situations?

- **Concept: Semantic vs. Episodic Memory**
  - Why needed here: Semantic Memory stores facts while Episodic Memory captures the specific temporal context needed for procedural learning from demonstrations
  - Quick check question: Where would the system store the definition of "red" vs. the memory of the specific time the instructor taught "store"?

## Architecture Onboarding

- **Component map:** Link-Grammar Parser -> Kinect/Point Cloud -> SVS -> Working Memory -> Procedural Memory (rules) -> Semantic Memory (maps) -> Episodic Memory (history) -> Chunking/KNN -> Instructor Interface

- **Critical path:**
  1. Utterance "Pick up the red triangle" received
  2. LG-Soar generates syntactic structure
  3. Indexing retrieves "red" and "triangle" maps from Semantic Memory; SVS verifies object properties
  4. Procedural rule for "pick-up" fires
  5. Failure (if new word): Impasse -> Push Dialog Segment -> Query Instructor -> Store new map in Semantic Memory

- **Design tradeoffs:**
  - Fixed Primitives vs. Learned Features: Hand-coded spatial primitives allow fast learning but restrict system to concepts expressible by these primitives
  - Interactive vs. Batch: Mixed-initiative approach reduces data requirements but demands human availability

- **Failure signatures:**
  - Over-specific spatial rules from single examples
  - Perceptual aliasing when distinct concepts map to same perceptual symbol
  - Permanent corruption from instruction errors

- **First 3 experiments:**
  1. Noun/Adjective Loop: Verify KNN classifier updates for "orange" vs. "square"
  2. Spatial Preposition Composition: Train "left of" with 1 example, then 3 disjunctive examples
  3. Verb Retrospective Projection: Trigger "Move X" impasse, guide manually, inspect Procedural Memory for new rule

## Open Questions the Paper Calls Out

### Open Question 1
How can an agent detect and recover from instruction errors when a human instructor provides incorrect or misleading examples? The system currently assumes perfect instructor information and has no mechanisms for identifying bad examples or correcting learned knowledge.

### Open Question 2
When should an agent perform additional information-gathering actions versus asking a human instructor for clarification under perceptual uncertainty? The paper raises this as an open question about decision policies for self-directed exploration vs. human queries.

### Open Question 3
How can an agent learn from instructions about hypothetical situations or historical contexts not currently present in the environment? The system only processes instructions grounded in immediate perceptual context; hypothetical reasoning requires new mechanisms.

## Limitations
- Core mechanisms depend on Soar architecture specifics without implementation details
- Fixed spatial primitive set may not generalize beyond simple tabletop arrangements
- Dependence on compliant human instructors raises questions about real-world robustness
- Evaluation focuses on controlled scenarios with limited vocabulary

## Confidence

- **High confidence** in interactive instruction achieving high accuracy with few examples for concrete, perceptually simple concepts
- **Medium confidence** in generalizability to broader vocabulary or complex spatial relationships
- **Low confidence** in robustness to instructor errors, ambiguous instructions, or inference beyond provided examples

## Next Checks

1. **Spatial Primitive Coverage:** Systematically test prepositions requiring topological relationships to identify gaps in the primitive set and measure failure rates
2. **Single-Example Generalization:** Conduct ablation studies disabling each learning mechanism to isolate their contributions to "one-shot" verb learning
3. **Robustness to Instruction Noise:** Introduce deliberately incorrect instructor inputs during acquisition and measure permanent semantic corruption versus self-correction rates