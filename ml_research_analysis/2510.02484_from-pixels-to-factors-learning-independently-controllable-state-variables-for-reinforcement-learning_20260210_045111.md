---
ver: rpa2
title: 'From Pixels to Factors: Learning Independently Controllable State Variables
  for Reinforcement Learning'
arxiv_id: '2510.02484'
source_url: https://arxiv.org/abs/2510.02484
tags:
- learning
- factored
- state
- factors
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Action-Controllable Factorization (ACF),
  a contrastive learning method that learns a factored latent representation from
  pixel observations in reinforcement learning environments. The key insight is to
  exploit the fact that agent actions typically affect only a subset of state variables
  while others evolve naturally, enabling contrastive training that isolates independently
  controllable factors.
---

# From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.02484
- Source URL: https://arxiv.org/abs/2510.02484
- Reference count: 40
- Key outcome: ACF recovers ground-truth controllable factors from pixels on Taxi, FourRooms, and MiniGrid-DoorKey, outperforming baselines in factorization metrics

## Executive Summary
This paper introduces Action-Controllable Factorization (ACF), a contrastive learning method that learns a factored latent representation from pixel observations in reinforcement learning environments. The key insight is to exploit the fact that agent actions typically affect only a subset of state variables while others evolve naturally, enabling contrastive training that isolates independently controllable factors. ACF uses energy-based parameterization and contrastive objectives to align latent factors with ground-truth controllable state variables, leveraging action-induced discrepancies in next-state predictions against natural dynamics. Empirically, ACF recovers ground-truth controllable factors directly from pixel observations on Taxi, FourRooms, and MiniGrid-DoorKey environments, consistently outperforming baseline disentanglement algorithms like DMS and GCL in factorization metrics.

## Method Summary
ACF learns independently controllable state variables by contrasting action-driven transitions against natural dynamics through an energy-based framework. The method encodes pixel observations into latent factors, then learns per-factor transition dynamics as energy functions that take the current latent, action, and next latent as input. A no-op action provides a baseline for natural dynamics, and the ratio of action-driven to natural transition probabilities isolates the factor affected by each action. The learning objective combines contrastive forward prediction, inverse dynamics, ratio classification, and policy regularization losses to jointly train the encoder and energy models. The resulting factorization aligns with ground-truth controllable state variables, enabling efficient planning in factored MDPs.

## Key Results
- ACF consistently recovers ground-truth controllable factors from pixels on Taxi, FourRooms, and DoorKey environments
- Outperforms baseline disentanglement methods (DMS, GCL) on diagonal R² factorization metrics
- Ablation studies show all four loss components (forward, inverse, ratio, policy) are necessary for optimal performance
- Successfully handles state-dependent action effects (FourRooms orientation) that challenge global sparsity methods

## Why This Works (Mechanism)

### Mechanism 1
Contrasting action-driven transitions against natural dynamics isolates independently controllable factors. The ratio log r_a(x',x) = log[T(x'|x,a) / T(x'|x,a₀)] mathematically simplifies to depend only on the factor affected by action a, because the Jacobian determinants cancel and unaffected factors have identical distributions under both conditions. This converts a multi-factor identification problem into per-factor binary classification. Core assumption: The environment has a no-op action a₀ that reveals natural dynamics without intervention, and each controllable factor has at least one action whose effect differs sufficiently from natural evolution.

### Mechanism 2
Energy-based parameterization enforces factorized transition structure while remaining learnable without reconstruction. Transitions are parameterized as T(z'|z,a) ∝ exp(∑ᵢ E_θ(z'ᵢ, a, z)), where each energy E_θ represents one factor's dynamics. The additive structure directly encodes factorization assumptions, and the energy formulation enables contrastive training via softmax classifiers without requiring pixel-level decoding. Core assumption: The ground-truth transition dynamics factorize as a product of per-factor transitions, and energy functions can approximate log-transition densities sufficiently well.

### Mechanism 3
Sparsity in learned score differences (energy gradients) guarantees identifiability up to permutation and invertible transforms. Theorem 3.1 shows that if ∂/∂z'_i[E(z'_i,a,z) - E(z'_i,a₀,z)] ≠ 0 for at most one factor per action, then the Jacobian between ground-truth and learned factors must be diagonal (after permutation). The L_r loss implicitly promotes this by training classifiers to distinguish each action's unique effect. Core assumption: The solution found by training is sparse; identifiability is conditional on optimization finding such solutions, not guaranteed by architecture alone.

## Foundational Learning

- **Factored MDPs and Dynamic Bayesian Networks**
  - Why needed here: ACF assumes ground-truth state factorizes with sparse dependencies; understanding DBN structure clarifies what "independently controllable" means formally
  - Quick check question: Given a 2D navigation task where actions are "move forward" and "rotate," how many factors are needed and what are their parents in the DBN?

- **Contrastive Learning (InfoNCE)**
  - Why needed here: L_fwd uses InfoNCE to maximize mutual information between z and z' without reconstruction; this is the core representation learning mechanism
  - Quick check question: In a batch of N samples with one positive pair, what does the InfoNCE loss optimize and what happens as N→∞?

- **Nonlinear ICA with Auxiliary Variables**
  - Why needed here: ACF is framed as nonlinear ICA where actions are auxiliary variables providing identifiability conditions; this explains why unsupervised disentanglement alone is insufficient
  - Quick check question: Why does standard VAE-based disentanglement fail to guarantee identification, and how does action-conditioning change this?

## Architecture Onboarding

- **Component map**: Input pixels → positional embeddings → residual CNN blocks → flatten → MLP → latent z (tanh) → per-factor energy MLPs (z'_k, a, z) → scalar energies → losses → backprop

- **Critical path**: Sample transitions (x, a, x') → encode to (z, z') with noise → compute per-factor energies for all actions → calculate ratio classifiers (diagonal minus no-op energies) → aggregate all four losses with coefficients → backprop through encoder and energy models

- **Design tradeoffs**:
  - Latent dimension K: Must match or exceed true number of controllable factors; underestimation loses factors, overestimation may introduce redundancy
  - Coefficient weights (β_r, β_fwd, β_inv, β_π): Heavily domain-dependent; ablation shows removing any component degrades performance but sensitivity varies
  - Noise injection: Applied to latent codes but amount not specified; too little may cause overfitting, too much may blur factor boundaries

- **Failure signatures**:
  - High off-diagonal R²: Factors remain entangled; check if no-op action truly has zero effect on controllable factors
  - DoorKey-specific: Non-controllable factors (door position) not identified; expected behavior, indicates ACF cannot discover uncontrollable state variables
  - DMS outperforming ACF on uncontrollable factors: Expected since DMS optimizes state-dependency sparsity directly
  - Training instability: Energy values growing unbounded; check gradient clipping and learning rate scheduling

- **First 3 experiments**:
  1. Grid2D validation: Train on simple 2D grid with 5 actions, verify latent traversals show clean x/y factorization with R² > 0.9 on diagonal
  2. Ablation sweep: Run all five ablation variants on DoorKey; expect "no fwd" to be most damaging based on Table 1
  3. Hyperparameter sensitivity: Vary β_r from 1× to 100× baseline while fixing other coefficients; identify collapse threshold where ratio loss dominates

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the ACF framework be extended to identify relevant state variables that are not one-step controllable or are entirely uncontrollable? The current method relies on contrasting action-driven dynamics against natural dynamics to isolate factors. Variables that do not change in response to agent actions lack the necessary contrastive signal to be identified.

- **Open Question 2**: Does the factorized representation learned by ACF translate into improved sample efficiency and performance in downstream reinforcement learning tasks? While the paper establishes that factored MDPs allow for "exponential gains" in planning, the evaluation is restricted to factorization metrics rather than measuring actual utility in a full RL loop.

- **Open Question 3**: Can active exploration strategies be integrated with ACF to refine factorization by targeting state-action pairs that better isolate specific latent variables? The current method learns from a dataset of transitions but does not actively seek out the specific state contexts required to satisfy the "sufficiently different" condition for all factors.

## Limitations

- Theoretical identifiability conditions rely on strong assumptions (e.g., 1-sparse Jacobian, existence of effective no-op actions) that may not hold in complex environments
- Architectural details (exact residual block counts, noise injection parameters) are underspecified, affecting reproducibility
- Method's success depends on domain-specific hyperparameter tuning (coefficient weights), suggesting limited out-of-the-box applicability

## Confidence

- **High**: Experimental results showing ACF outperforms baselines on controllable factor recovery metrics in controlled domains
- **Medium**: Theoretical claims about energy-based factorization and contrastive learning mechanisms
- **Medium**: Ablation studies demonstrating necessity of all four loss components

## Next Checks

1. **Sparsity Analysis**: Verify that per-action energy gradients are truly 1-sparse in learned representations, confirming the core mechanism for disentanglement
2. **No-Op Action Robustness**: Test ACF performance when no perfect no-op action exists by introducing small natural dynamics even under "no-op" conditions
3. **Generalization Stress Test**: Evaluate ACF on a novel environment with coupled ground-truth factors to assess upper bounds on achievable disentanglement