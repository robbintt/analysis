---
ver: rpa2
title: Prompt-Aware Scheduling for Efficient Text-to-Image Inferencing System
arxiv_id: '2502.06798'
source_url: https://arxiv.org/abs/2502.06798
tags:
- system
- quality
- high
- prompts
- text-to-image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a prompt-aware scheduling system for high-throughput
  text-to-image inferencing, addressing the limitations of traditional accuracy scaling
  and model-switching approaches. The system runs a single diffusion model at multiple
  approximation levels (controlled by parameter K) on a fixed GPU cluster, using a
  novel optimization to allocate prompts to the most suitable K value based on predicted
  quality impact.
---

# Prompt-Aware Scheduling for Efficient Text-to-Image Inferencing System

## Quick Facts
- arXiv ID: 2502.06798
- Source URL: https://arxiv.org/abs/2502.06798
- Authors: Shubham Agarwal; Saud Iqbal; Subrata Mitra
- Reference count: 6
- Primary result: Achieves 10% higher quality (>90% vs <90-94% baselines), 40% higher throughput, and 10× lower SLO violations (<5%) compared to state-of-the-art systems

## Executive Summary
This work introduces a prompt-aware scheduling system for high-throughput text-to-image inferencing that addresses limitations of traditional accuracy scaling and model-switching approaches. The system runs a single diffusion model at multiple approximation levels (controlled by parameter K) on a fixed GPU cluster, using a novel optimization to allocate prompts to the most suitable K value based on predicted quality impact. It employs approximate caching to skip denoising steps while maintaining quality, and a load-aware route-and-batch strategy to maximize throughput under varying loads. Evaluated on SD-XL models with real-world workloads, the system demonstrates significant improvements in both quality and throughput metrics compared to existing systems like Clipper-HA, Clipper-HT, NIRVANA, and PROTEUS.

## Method Summary
The system introduces a novel prompt-aware scheduling approach that optimizes text-to-image inferencing by running a single diffusion model at multiple approximation levels (K values) on a fixed GPU cluster. The key innovation is a quality-aware routing mechanism that allocates prompts to the most appropriate K value based on predicted quality impact, combined with approximate caching to skip denoising steps while maintaining output quality. The load-aware route-and-batch strategy dynamically adjusts batch sizes and routing decisions based on system load to maximize throughput. The system was evaluated on SD-XL models using real-world workloads, demonstrating significant improvements over traditional approaches that either suffer from high latency violations or quality degradation.

## Key Results
- Achieves up to 10% higher quality (>90% vs <90-94% baselines)
- Demonstrates 40% higher throughput compared to state-of-the-art systems
- Reduces SLO violations by 10× (from 30-50% to <5%)
- Outperforms systems like Clipper-HA, Clipper-HT, NIRVANA, and PROTEUS in both quality and latency metrics

## Why This Works (Mechanism)
The system's effectiveness stems from its intelligent prompt-aware scheduling that optimizes resource allocation based on quality requirements rather than applying uniform approximation across all prompts. By using a single diffusion model at multiple approximation levels instead of switching between different models, the system maintains better quality control while achieving higher throughput. The approximate caching mechanism intelligently skips unnecessary denoising steps without compromising output quality, while the load-aware route-and-batch strategy ensures optimal resource utilization under varying workloads. This approach addresses the fundamental tradeoff between quality and throughput that plagues traditional text-to-image inferencing systems.

## Foundational Learning
- **Diffusion Model Approximation**: Understanding how different approximation levels (K values) affect image quality and computational cost - needed to balance quality vs. throughput tradeoffs, quick check: verify quality degradation curves across K values
- **Prompt Quality Prediction**: Methods for estimating the quality impact of different approximation levels on specific prompts - needed for intelligent routing decisions, quick check: test prediction accuracy on diverse prompt types
- **Approximate Caching in Diffusion**: Techniques for skipping denoising steps while maintaining output quality - needed to reduce computational overhead, quick check: measure quality loss when skipping specific steps
- **Load-Aware Batch Scheduling**: Dynamic adjustment of batch sizes based on system load and prompt characteristics - needed to maximize throughput under varying conditions, quick check: test batch size optimization under different load scenarios
- **Quality-Aware Routing**: Decision-making framework for allocating prompts to appropriate approximation levels - needed to ensure quality requirements are met while optimizing resources, quick check: validate routing decisions against ground truth quality metrics

## Architecture Onboarding

Component Map:
Prompt Input -> Quality Predictor -> Route-and-Batch Scheduler -> K-value Allocator -> Approximate Cache Manager -> Diffusion Model (multiple K levels) -> Output Quality Validator

Critical Path:
Prompt Input -> Quality Predictor -> Route-and-Batch Scheduler -> K-value Allocator -> Diffusion Model execution -> Output

Design Tradeoffs:
- Single model with multiple K values vs. multiple specialized models: Prioritizes quality consistency and reduces model switching overhead
- Approximate caching vs. full denoising: Trades minimal quality loss for significant computational savings
- Fixed GPU cluster vs. dynamic scaling: Emphasizes predictable performance and cost control over elastic scaling

Failure Signatures:
- Quality degradation below acceptable thresholds despite caching
- Route-and-batch scheduler misallocating prompts to inappropriate K values
- Approximate cache manager failing to maintain quality while skipping steps
- System overload when handling sudden spikes in prompt volume

First Experiments:
1. Baseline quality vs. K value characterization to establish quality degradation curves
2. Route-and-batch scheduler validation under controlled load variations
3. Approximate caching performance testing with quality preservation verification

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Limited evaluation to SD-XL models without testing generalization to other diffusion model architectures
- Lack of ablation studies showing individual contributions of each optimization component
- Absence of testing under different GPU cluster configurations and prompt complexity distributions
- Comparison metrics do not account for potential systematic quality differences across prompt types

## Confidence
- Performance claims vs baselines: High (supported by direct comparisons with established systems)
- Quality preservation mechanism: Medium (limited ablation studies, relies on approximate caching without detailed analysis)
- Generalization to other model architectures: Low (only tested on SD-XL)
- Real-world deployment feasibility: Medium (results based on controlled evaluations)

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of route-and-batch strategy, approximate caching, and K-value allocation to overall performance gains
2. Test the system on multiple diffusion model architectures (not just SD-XL) to verify generalizability of the prompt-aware scheduling approach
3. Evaluate performance under varying GPU cluster configurations and different prompt complexity distributions to assess robustness across deployment scenarios