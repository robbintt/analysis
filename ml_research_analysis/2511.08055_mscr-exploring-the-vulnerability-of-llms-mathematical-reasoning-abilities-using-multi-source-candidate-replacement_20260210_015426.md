---
ver: rpa2
title: 'MSCR: Exploring the Vulnerability of LLMs'' Mathematical Reasoning Abilities
  Using Multi-Source Candidate Replacement'
arxiv_id: '2511.08055'
source_url: https://arxiv.org/abs/2511.08055
tags:
- llms
- original
- attack
- response
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MSCR, an automated adversarial attack method
  based on multi-source candidate replacement, to systematically evaluate the vulnerability
  of LLMs in mathematical reasoning tasks. By combining cosine similarity in the embedding
  space of LLMs, the WordNet dictionary, and contextual predictions from a masked
  language model, MSCR generates high-quality semantically similar candidates and
  perturbs a single word in the input question.
---

# MSCR: Exploring the Vulnerability of LLMs' Mathematical Reasoning Abilities Using Multi-Source Candidate Replacement

## Quick Facts
- **arXiv ID:** 2511.08055
- **Source URL:** https://arxiv.org/abs/2511.08055
- **Reference count:** 40
- **Primary result:** Single-word perturbations reduce LLM math accuracy by up to 49.89% on GSM8K and 35.40% on MATH500

## Executive Summary
This paper introduces MSCR, an automated adversarial attack method that systematically evaluates LLM vulnerability in mathematical reasoning tasks. By perturbing a single word in math problems using candidates generated from embedding similarity, WordNet, and masked language model predictions, MSCR demonstrates that even slight semantic shifts can cause significant accuracy drops across 12 models on GSM8K and MATH500 benchmarks. The attack not only causes incorrect answers but also substantially increases response length, revealing efficiency bottlenecks in current LLMs' reasoning capabilities.

## Method Summary
MSCR generates adversarial candidates from three sources: (1) cosine similarity in the target model's embedding space, (2) WordNet synonyms ranked by cosine similarity, and (3) masked language model predictions from BERT. These candidates are filtered for grammatical validity and semantic similarity, then applied sequentially to each word in the input question. The attack succeeds when the perturbed input produces a different (incorrect) answer, verified by a secondary LLM. The method uses greedy word-by-word replacement and operates at temperature 0.6 with three-run averaging.

## Key Results
- Single-word perturbations reduce accuracy by up to 49.89% on GSM8K and 35.40% on MATH500
- Adversarial examples increase average response length by up to 2.14× the original
- Maximum response length collapse exceeds 10× in some models
- Perturbations work across both open-source and commercial models, demonstrating transfer

## Why This Works (Mechanism)

### Mechanism 1: Multi-Source Semantic Fusion
The system aggregates candidates from three distinct sources—cosine similarity, WordNet, and MLM predictions—to create semantically similar but structurally distinct replacements. This fusion ensures grammatical validity while disrupting the token patterns LLMs rely on for reasoning.

### Mechanism 2: Contextual Logic Drift via MLM
Unlike static dictionary lookups, the MLM predicts context-aware replacements that fit locally but alter logical relationships globally. This introduces subtle grammatical shifts that mislead the reasoning chain while passing perplexity checks.

### Mechanism 3: Inference Efficiency Collapse
Perturbations destabilize the Chain-of-Thought process, causing models to generate longer, more complex reasoning pathways. This increases computational cost and error probability, with response lengths sometimes exceeding twice the original.

## Foundational Learning

- **Concept: Adversarial Transferability** - Why needed: The paper shows attacks transfer from open-source to commercial models. Quick check: Why would a perturbation effective on a 7B model also fool GPT-4o?
- **Concept: Discrete Input Perturbation** - Why needed: Text is discrete, requiring proxy continuous representations (embeddings) for word replacement. Quick check: How does MSCR use cosine similarity to bridge continuous vector space and discrete word replacement?
- **Concept: Greedy Search Strategy** - Why needed: MSCR substitutes words "one by one" rather than optimizing the whole sentence. Quick check: What is the trade-off between greedy replacement and global sentence optimization?

## Architecture Onboarding

- **Component map:** Input Math problem → Candidate Generator (Embeddings/WordNet/MLM) → Filter (thresholds/morphology) → Attacker (sequential loop) → Evaluator (secondary LLM)
- **Critical path:** Candidate Filtering stage - if candidates are too dissimilar or too similar, the attack loop wastes queries
- **Design tradeoffs:** Semantic Preservation vs. Attack Success (balancing similarity and disruption), Query Efficiency (greedy vs. global optimization)
- **Failure signatures:** Semantic Drift (problem definition changes), Grammar Violation (nonsensical sentences), False Negative (longer response but correct answer)
- **First 3 experiments:**
  1. Replicate Single-Source Ablation: Compare attack success using only WordNet vs. only Embeddings
  2. Analyze Length Collapse: Plot response length distributions for successful vs. failed attacks
  3. Transfer Test: Generate examples on weak model and test on strong reasoning model

## Open Questions the Paper Calls Out
None

## Limitations
- The multi-source fusion superiority claim lacks rigorous ablation validation
- Secondary validation using qwen3-max-preview introduces potential false negative cascades
- Response length collapse metric lacks normalization across different model baselines

## Confidence
- **High confidence:** Single-word perturbations significantly degrade model accuracy (49.89% drop on GSM8K)
- **Medium confidence:** Response length increases indicating "reasoning confusion" is plausible but causal mechanism unclear
- **Low confidence:** MSCR's multi-source approach superiority requires more direct comparative evidence

## Next Checks
1. **Ablation study on candidate sources:** Systematically disable each candidate generation source and measure attack success rate
2. **Cross-model transfer analysis:** Generate examples on weakest model and test effectiveness on strongest model
3. **Human evaluation of semantic drift:** Have annotators rate semantic similarity between original and perturbed questions for 100 examples