---
ver: rpa2
title: 'MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization'
arxiv_id: '2510.21473'
source_url: https://arxiv.org/abs/2510.21473
tags:
- reward
- reasoning
- denoising
- learning
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes Multi-Reward Optimization (MRO) to address\
  \ the limited reasoning performance of diffusion language models (DLMs), which stems\
  \ from their failure to capture token correlations during denoising. MRO enhances\
  \ intra-sequence and inter-sequence token correlations by designing and optimizing\
  \ multiple rewards\u2014token verification, perplexity, and task quality\u2014using\
  \ test-time scaling, rejection sampling, and reinforcement learning."
---

# MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization

## Quick Facts
- arXiv ID: 2510.21473
- Source URL: https://arxiv.org/abs/2510.21473
- Authors: Chenglong Wang, Yang Gan, Hang Zhou, Chi Hu, Yongyu Mu, Kai Song, Murun Yang, Bei Li, Chunliang Zhang, Tongran Liu, Jingbo Zhu, Zhengtao Yu, Tong Xiao
- Reference count: 40
- Primary result: 3-point average accuracy improvement on reasoning benchmarks (GSM8K, MATH500, GPQA, Countdown, Sudoku)

## Executive Summary
This paper proposes Multi-Reward Optimization (MRO) to address the limited reasoning performance of diffusion language models (DLMs), which stems from their failure to capture token correlations during denoising. MRO enhances intra-sequence and inter-sequence token correlations by designing and optimizing multiple rewards—token verification, perplexity, and task quality—using test-time scaling, rejection sampling, and reinforcement learning. Theoretical analysis shows that a step-wise group reward optimization (SGRO) strategy reduces reward variance. Experiments on reasoning benchmarks demonstrate that MRO improves accuracy by up to 3 points on average and enables DLMs to generate high-quality reasoning paths with fewer denoising steps, approaching the performance of strong autoregressive models like Qwen2.5-7B.

## Method Summary
MRO introduces three reward components: token verification (Rtv) for intra-sequence correlation, perplexity (Rppl) for fluency, and task quality (Rq) for correctness. These rewards are optimized through test-time scaling with beam search, rejection sampling for fine-tuning, or reinforcement learning. SGRO groups denoising steps to reduce variance in reward computation. The approach is validated on LLaDA-8B-Instruct across five reasoning benchmarks, showing consistent improvements over baseline DLMs.

## Key Results
- 3-point average accuracy improvement across GSM8K, MATH500, GPQA, Countdown, and Sudoku benchmarks
- MRO enables DLMs to generate high-quality reasoning paths with fewer denoising steps
- Approaches performance of strong autoregressive models like Qwen2.5-7B on reasoning tasks
- SGRO reduces reward variance while preserving expected reward signals

## Why This Works (Mechanism)

### Mechanism 1: Token Correlation Enhancement via Multi-Reward Shaping
Optimizing DLMs with explicit token correlation rewards improves reasoning performance by reducing inconsistencies in chain-of-thought generation. The paper defines two correlation types—intra-sequence (within a single denoising step) and inter-sequence (across steps)—and designs rewards targeting each. The token verification reward (Rtv) re-enters predicted tokens to check mutual consistency, while perplexity reward (Rppl) ensures fluency. Task quality reward (Rq) provides delayed feedback on final output correctness. These are combined via potential-based reward shaping to maintain optimization properties while adding correlation signals absent from standard masked token prediction.

### Mechanism 2: Variance Reduction via Step-wise Group Reward Optimization (SGRO)
Grouping denoising steps for reward computation reduces variance while preserving the expected reward signal. Standard potential-based reward shaping adds variance through covariance terms between potential functions at adjacent timesteps. SGRO computes rewards over groups of w steps rather than per-step, reducing the frequency of potential function evaluations and weakening the correlation between distant potential terms. Theoretical analysis shows this reduces Var(R(s,a)) compared to step-by-step computation.

### Mechanism 3: Test-Time Scaling with Reward-Guided Beam Search
Using MRO rewards to guide beam search at inference time improves reasoning accuracy without parameter updates. At each denoising step, generate k candidate responses via temperature sampling, compute the combined reward for each, and select the highest-reward response. This allows the model to explore multiple reasoning paths while prioritizing those with stronger token correlations and task quality. Majority voting provides pseudo-ground truth for computing Rq when true answers are unavailable.

## Foundational Learning

- **Concept: Diffusion Language Models (DLMs) and Masked Denoising**
  - Why needed here: MRO operates on the specific structure of DLM denoising—the iterative prediction of masked tokens. Without understanding that DLMs generate tokens in parallel across steps (unlike AR models' sequential generation), the token correlation problem makes no sense.
  - Quick check question: Given a sequence with 50% tokens masked, how does a DLM denoise it differently from how an AR model would generate the same sequence?

- **Concept: Potential-Based Reward Shaping in Reinforcement Learning**
  - Why needed here: MRO combines multiple rewards using potential-based shaping to maintain policy invariance. The theoretical guarantees depend on understanding how shaping affects variance while preserving expected returns.
  - Quick check question: If you add F(s,s') = γΦ(s') - Φ(s) to a reward function, what property of the optimal policy remains unchanged, and what changes?

- **Concept: Markov Decision Process Formulation for Sequential Generation**
  - Why needed here: The paper models denoising as an MDP where state = (prompt, current tokens, timestep), action = predicted tokens, and policy = denoising distribution. This framing enables policy gradient methods for optimization.
  - Quick check question: In the DLM MDP formulation, what is the relationship between the discount factor λ and the temporal structure of the denoising process?

## Architecture Onboarding

- Component map:
```
Input: Prompt p0, masked response rT (fully masked)
↓
Denoising Loop (T steps):
  For each step t → t-1:
    └─ Predict masked tokens → rt-1
    └─ [MRO Branch] Compute rewards:
       ├─ Rtv: Token verification (re-mask & re-predict)
       ├─ Rppl: Perplexity via external AR model
       └─ [At t=0] Rq: Task quality (format + accuracy)
    └─ [SGRO] Accumulate rewards over group of w steps
↓
Output: Final response r0
```

- Critical path: The reward computation pipeline is the performance bottleneck. Token verification requires N additional forward passes per step (where N = number of masked tokens). The paper mitigates this via: (1) random token subsampling, (2) GPU parallelism, and (3) SGRO grouping.

- Design tradeoffs:
  - **Beam size k (test-time scaling)**: Larger k explores more paths but increases compute linearly. k=4 used in experiments.
  - **Group size w (SGRO)**: Larger w reduces variance but may lose fine-grained reward signals. Empirically, w=2-32 works; w=32 used for test-time scaling.
  - **Reward weights**: The paper doesn't explicitly weight Rtv, Rppl, Rq—they're summed directly. Tuning these could help for different task types.
  - **Perplexity model**: Uses external AR model (lmppl library) for Rppl. Choice of reference model affects reward calibration.

- Failure signatures:
  - **Low correlation rewards but high task accuracy**: Suggests reward-task misalignment; may need to recalibrate reward definitions.
  - **High variance in rewards across runs**: SGRO grouping may be too small, or temperature sampling too aggressive.
  - **Reasoning format violations (missing tags)**: Rq format reward not strong enough; consider increasing format reward weight.
  - **Fluent but incorrect reasoning**: Rppl dominates over Rq; may need to reweight task quality more heavily.
  - **Training instability with RL**: Gradient variance from sparse rewards; verify SGRO is correctly implemented and group size is appropriate.

- First 3 experiments:
  1. **Ablation: Individual reward contributions** - Train/run MRO with only Rtv, only Rppl, only Rq, and all combinations. The paper does this but replication on your target task is essential to understand which rewards matter most for your domain.
  2. **Sweep: Group size w impact on variance and performance** - Run SGRO with w ∈ {1, 2, 4, 8, 16, 32, 64} and measure both final accuracy and reward variance across seeds. Figure 7 provides a template; extend to your hardware and task to find the sweet spot.
  3. **Scaling: Beam size k vs. compute vs. accuracy tradeoff** - Systematically vary k ∈ {1, 2, 4, 8, 16} and measure accuracy gain per additional forward pass. This establishes the inference cost curve for your deployment constraints.

## Open Questions the Paper Calls Out
None

## Limitations

- **Reward-Reasoning Alignment**: While MRO shows consistent accuracy improvements, the correlation between individual reward components and actual reasoning quality is not fully established. The additive combination assumes reward orthogonality and additive contribution to final reasoning accuracy, which remains empirically unverified.

- **SGRO Variance Reduction Guarantees**: The theoretical analysis shows SGRO reduces variance compared to step-wise computation, but the magnitude of this reduction and its impact on final reasoning performance is unclear. The paper demonstrates variance reduction in isolation but doesn't establish the relationship between reward variance reduction and accuracy improvement.

- **Scalability to Larger Models and Tasks**: Experiments focus on LLaDA-8B-Instruct. The approach's effectiveness on larger models (70B+) or more complex reasoning tasks remains unknown. The token verification reward's computational cost scales with masked token count and model size, potentially creating bottlenecks.

## Confidence

**High Confidence (8-10/10):**
- Token correlation as reasoning bottleneck
- Multi-reward combination improves accuracy
- SGRO reduces variance

**Medium Confidence (5-7/10):**
- Individual reward contributions
- Computational efficiency claims

**Low Confidence (1-4/10):**
- Generalizability to new domains
- Long-form reasoning capability

## Next Checks

**Next Check 1: Reward Ablation with Task-Specific Weights**
Systematically evaluate MRO with individual rewards disabled and with task-specific reward weighting. For each reasoning benchmark, train and evaluate models with only Rtv, only Rppl, only Rq, and weighted combinations. Measure accuracy and analyze which reward components drive improvements for different reasoning task types.

**Next Check 2: SGRO Group Size Optimization Study**
Conduct a comprehensive sweep of group size w ∈ {1, 2, 4, 8, 16, 32, 64, 128} across all three optimization approaches and all reasoning benchmarks. For each configuration, measure final accuracy, reward variance across training runs, inference latency, and memory usage. Identify the optimal w for each task type and optimization approach.

**Next Check 3: Computational Cost-Benefit Analysis**
Measure the wall-clock time and FLOPs per reasoning step for MRO compared to baseline LLaDA and strong AR models across all benchmarks. Calculate the accuracy improvement per unit compute and FLOPs per reasoning step to validate whether MRO's accuracy gains justify its computational overhead.