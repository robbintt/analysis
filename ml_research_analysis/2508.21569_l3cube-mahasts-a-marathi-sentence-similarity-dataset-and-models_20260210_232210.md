---
ver: rpa2
title: 'L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models'
arxiv_id: '2508.21569'
source_url: https://arxiv.org/abs/2508.21569
tags:
- sentence
- dataset
- similarity
- marathi
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of high-quality annotated datasets
  for sentence similarity tasks in Marathi, a low-resource Indian language. To address
  this, the authors create MahaSTS, a human-annotated dataset of 16,860 Marathi sentence
  pairs with continuous similarity scores from 0 to 5, evenly distributed across six
  score buckets to ensure balanced supervision and reduce label bias.
---

# L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models

## Quick Facts
- arXiv ID: 2508.21569
- Source URL: https://arxiv.org/abs/2508.21569
- Reference count: 6
- Authors created a human-annotated Marathi sentence similarity dataset with 16,860 pairs and fine-tuned a Sentence-BERT model achieving Pearson correlation of 0.9600 and Spearman correlation of 0.9523 on the test set.

## Executive Summary
This paper addresses the critical gap in Marathi language resources for semantic textual similarity (STS) tasks. The authors introduce MahaSTS, a carefully curated dataset of 16,860 Marathi sentence pairs with continuous similarity scores ranging from 0 to 5, distributed across six balanced buckets to ensure robust model training. Building on this resource, they fine-tune MahaSBERT-STS-v2, a Sentence-BERT model that achieves state-of-the-art performance on Marathi STS tasks, significantly outperforming multilingual baselines including MuRIL, IndicBERT, and IndicSBERT. The work demonstrates the value of human-curated data and structured supervision for improving sentence similarity modeling in low-resource languages.

## Method Summary
The authors created MahaSTS by first generating candidate sentence pairs using an existing MahaSBERT-STS model, then having human annotators score these pairs on a continuous scale from 0 (completely dissimilar) to 5 (identical meaning). The dataset was structured into six balanced buckets with approximately 2,810 pairs each to reduce label bias. For model development, they fine-tuned MahaSBERT (pre-trained on IndicXNLI) using the MahaSTS dataset with mean pooling and CosineSimilarityLoss, training for 2 epochs with batch size 8 and learning rate 1e-5. The resulting MahaSBERT-STS-v2 model was evaluated against several baselines using Pearson and Spearman correlation metrics.

## Key Results
- MahaSBERT-STS-v2 achieves Pearson correlation of 0.9600 and Spearman correlation of 0.9523 on the MahaSTS test set
- The model outperforms all baselines including MahaBERT, MuRIL, IndicBERT, and IndicSBERT
- Mean pooling consistently outperforms CLS and MAX pooling strategies for Marathi sentence embeddings in STS tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Balanced bucketing across six similarity score ranges reduces label bias and stabilizes regression learning.
- Mechanism: By enforcing exactly 2,810 sentence pairs per bucket (scores 0, 0.1–1.0, 1.1–2.0, 2.1–3.0, 3.1–4.0, 4.1–5.0), the dataset prevents the model from overfitting to overrepresented similarity levels, which is common in scraped or naturally distributed data where middle-range scores dominate.
- Core assumption: Human annotations within each bucket are consistently applied; bucket boundaries align with perceptually meaningful semantic distinctions.
- Evidence anchors:
  - [abstract] "uniformly distributed across six score-based buckets spanning the full 0-5 range, thus reducing label bias and enhancing model stability"
  - [Section 3] "Each bucket contains exactly 2,810 sentence pairs and corresponds to a specific semantic similarity range"
  - [corpus] Weak direct evidence in neighbors; MahaParaphrase uses binary labels rather than continuous buckets, limiting cross-validation of this mechanism.
- Break condition: If downstream tasks require fine-grained distinctions within buckets (e.g., distinguishing 1.2 from 1.8), the bucketing may obscure subtle differences.

### Mechanism 2
- Claim: Sequential training—first on NLI, then on STS—yields better sentence representations than NLI alone.
- Mechanism: NLI pre-training (IndicXNLI) teaches the model entailment and contradiction patterns, which transfer to similarity scoring. Fine-tuning on MahaSTS then calibrates these representations for regression-based similarity rather than classification.
- Core assumption: NLI-to-STS transfer holds for Marathi similarly to English; the translation of NLI data to Indic languages preserves semantic relationships.
- Evidence anchors:
  - [Section 2] "Prior work by Joshi et al. (2023) has shown that sequential training, first on a Natural Language Inference (NLI) dataset followed by fine-tuning on an STS-style dataset, yields better performance than training on NLI alone"
  - [Section 4.1] "MahaSBERT... is a model trained on the IndicXNLI dataset using the MahaBERT model as the base model"
  - [corpus] Indirect support from MahaParaphrase paper showing BERT-based models benefit from task-specific fine-tuning on Marathi data.
- Break condition: If NLI annotations are machine-translated with errors, the pre-training may introduce artifacts rather than useful semantic priors.

### Mechanism 3
- Claim: Mean pooling outperforms CLS and MAX pooling for Marathi sentence embeddings in STS tasks.
- Mechanism: Mean pooling averages all token embeddings, capturing distributed semantic information across the sentence. CLS relies on a single token's representation, which may not fully encode sentence-level semantics, while MAX pooling emphasizes salient features but loses distributional nuance.
- Core assumption: Marathi sentence semantics are distributed across tokens similarly to English; no language-specific token weighting is required.
- Evidence anchors:
  - [Section 5, Table 4] MEAN pooling: Pearson 0.9600, Spearman 0.9523 vs. CLS: 0.958, 0.9503 vs. MAX: 0.9532, 0.9444
  - [Section 5] "Among these, the MEAN pooling strategy consistently yields the best performance"
  - [corpus] No direct pooling comparisons in neighbor papers; this finding is internal to this work.
- Break condition: For very long sentences (>20 tokens, which were filtered out), mean pooling may dilute signal from key phrases.

## Foundational Learning

- Concept: **Semantic Textual Similarity (STS) vs. Paraphrase Detection**
  - Why needed here: STS assigns continuous scores (0-5) capturing degrees of semantic overlap, while paraphrase detection is binary. Understanding this distinction clarifies why regression loss (CosineSimilarityLoss) is used rather than classification loss.
  - Quick check question: If two sentences share the same topic but differ in detail (e.g., "The train arrived late" vs. "The train was delayed by two hours"), should they score 0, 2, or 4?

- Concept: **Monolingual vs. Multilingual Models for Low-Resource Languages**
  - Why needed here: The paper shows MahaSBERT (monolingual-focused) outperforms MuRIL and IndicBERT (multilingual). This reflects that monolingual fine-tuning can specialize representations better than shared multilingual parameters when target-language data is sufficient.
  - Quick check question: Why might a multilingual model underperform on Marathi even if it was trained on 17 Indian languages?

- Concept: **Sentence-BERT Siamese Architecture**
  - Why needed here: SBERT uses siamese networks to produce fixed-size sentence embeddings that can be compared via cosine similarity. This is more efficient than cross-encoder BERT for pairwise comparison at scale.
  - Quick check question: For 10,000 sentences, how many pairwise comparisons are needed? Why is a cross-encoder impractical?

## Architecture Onboarding

- Component map: MahaBERT -> IndicXNLI pre-training -> MahaSBERT -> MahaSTS fine-tuning -> MahaSBERT-STS-v2
- Critical path:
  1. Start from `l3cube-pune/marathi-sentence-bert-nli` (pre-trained on IndicXNLI)
  2. Load MahaSTS dataset (14,328 train pairs, 1,692 test pairs, 840 validation pairs)
  3. Apply mean pooling to extract sentence embeddings
  4. Fine-tune with CosineSimilarityLoss, 2 epochs, batch size 8, learning rate 1e-5
  5. Evaluate using Pearson and Spearman correlation against human scores
- Design tradeoffs:
  - **Bucketing granularity**: 6 buckets balance label resolution with annotation consistency; finer buckets (e.g., 0.1 increments) would increase noise.
  - **Sentence length filtering**: 3-20 words excludes complex sentences but keeps examples tractable; may limit generalization to formal documents.
  - **Monolingual specialization**: MahaSBERT outperforms multilingual baselines but requires separate models per language, increasing deployment complexity.
- Failure signatures:
  - **High correlation but poor calibration**: Model ranks pairs correctly but outputs scores clustered around the mean (e.g., always predicting 2.5).
  - **Lexical overlap bias**: Model scores sentences high if they share words, even when meanings differ (noted in BERT-flow literature, Section 2).
  - **Out-of-domain degradation**: Model trained on short sentences may fail on long or formal Marathi text.
- First 3 experiments:
  1. **Reproduce baseline comparison**: Run MahaSBERT, MahaBERT, MuRIL, IndicBERT, and IndicSBERT on MahaSTS test set; verify Pearson/Spearman rankings match Table 3.
  2. **Pooling ablation**: Compare CLS, MEAN, and MAX pooling on the same model; confirm MEAN pooling advantage holds.
  3. **Bucket-wise error analysis**: For each bucket, compute mean absolute error between predicted and ground-truth scores; identify if certain similarity ranges are systematically over/underestimated.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of MahaSBERT-STS-v2 degrade significantly on complex or compound sentences compared to the short-to-moderate sentences currently in the dataset?
- Basis in paper: [explicit] The "Limitations" section states there is "limited generalization for longer or more complex sentences" and suggests this could be overcome by creating datasets with "sentences of varying lengths."
- Why unresolved: The current MahaSTS dataset was preprocessed to remove sentences longer than 20 words, focusing the evaluation on simpler syntactic structures.
- What evidence would resolve it: Evaluation results on a new test set containing Marathi compound and complex sentences with length > 20 words.

### Open Question 2
- Question: Does the use of an existing model (MahaSBERT-STS) to select candidate pairs for human annotation introduce a selection bias that limits the dataset's ability to correct model errors?
- Basis in paper: [inferred] Section 3.1 describes using MahaSBERT-STS embeddings to calculate cosine similarity and bucket sentences *before* human annotation.
- Why unresolved: Selecting candidates based on existing model predictions may filter out "hard negatives" or nuanced semantic distinctions that the base model failed to capture, potentially reinforcing existing model biases.
- What evidence would resolve it: A comparative analysis of model performance on a dataset constructed via random sampling versus the model-guided sampling used in this study.

### Open Question 3
- Question: Does the high correlation score (0.96 Pearson) on the MahaSTS test set translate to improved effectiveness in downstream applications like Retrieval Augmented Generation (RAG) or Information Retrieval?
- Basis in paper: [inferred] The Introduction lists RAG and information retrieval as key applications for STS, but the Experiments section only evaluates the model on the intrinsic MahaSTS test split.
- Why unresolved: High performance on a semantic similarity benchmark does not always guarantee superior performance in retrieval tasks where the distribution of queries and documents may differ.
- What evidence would resolve it: Benchmarking the fine-tuned model on a downstream retrieval task (e.g., document retrieval for Marathi) to measure Recall@k or MRR.

## Limitations

- The dataset focuses on short sentences (3-20 words), limiting generalization to longer or more complex Marathi text
- Human annotation process may introduce subtle biases based on annotators' backgrounds or interpretation of similarity scores
- The bucketing strategy, while effective for balanced supervision, may obscure fine-grained semantic distinctions within each range

## Confidence

**High Confidence**: The dataset creation methodology (16,860 pairs, 6 balanced buckets, 0-5 continuous scores) is clearly specified and reproducible. The fine-tuning procedure (MahaSBERT base, 2 epochs, CosineSimilarityLoss, MEAN pooling) is explicitly detailed with hyperparameters.

**Medium Confidence**: The superiority of MahaSBERT-STS-v2 over baselines (Pearson 0.9600, Spearman 0.9523) is well-supported by Table 3 comparisons, though the choice of baselines and evaluation metrics could be expanded. The effectiveness of sequential NLI-to-STS training is supported by prior work citation but not independently verified within this paper.

**Low Confidence**: The bucketing mechanism's effectiveness in reducing label bias is theoretically sound but lacks ablation studies removing the bucketing structure. The claim that mean pooling is universally superior for Marathi requires validation across different model architectures and task variations.

## Next Checks

1. **External Domain Generalization Test**: Evaluate MahaSBERT-STS-v2 on Marathi sentences from different domains (news articles, academic papers, social media) to assess whether the 3-20 word conversational training domain limits real-world applicability.

2. **Ablation of Bucketing Structure**: Retrain models using the same 16,860 pairs but with natural (unbalanced) score distributions to empirically test whether the bucketing mechanism provides measurable benefits over standard regression approaches.

3. **Pooling Strategy Robustness**: Conduct a comprehensive comparison of pooling strategies (CLS, MEAN, MAX, and weighted variants) across multiple Marathi sentence similarity datasets and model architectures to determine if mean pooling's advantage is consistent or dataset-specific.