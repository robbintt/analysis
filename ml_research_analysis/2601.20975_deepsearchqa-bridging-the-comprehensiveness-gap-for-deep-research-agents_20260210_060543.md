---
ver: rpa2
title: 'DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents'
arxiv_id: '2601.20975'
source_url: https://arxiv.org/abs/2601.20975
tags:
- arxiv
- answer
- research
- agent
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeepSearchQA, a benchmark for evaluating
  agents on complex, multi-step information-seeking tasks across 17 fields. Unlike
  traditional benchmarks focused on single-answer retrieval, DeepSearchQA requires
  agents to generate exhaustive answer lists, testing their ability to systematically
  collate fragmented information, resolve entity duplicates, and reason about stopping
  criteria in open-ended searches.
---

# DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents

## Quick Facts
- **arXiv ID:** 2601.20975
- **Source URL:** https://arxiv.org/abs/2601.20975
- **Reference count:** 10
- **Primary result:** Introduces a benchmark revealing current deep research agents struggle with exhaustive list generation, achieving high F1 scores but failing on precision-recall balance and entity resolution.

## Executive Summary
DeepSearchQA is a new benchmark designed to evaluate deep research agents on complex, multi-step information-seeking tasks across 17 domains. Unlike traditional benchmarks focused on single-answer retrieval, it requires agents to generate exhaustive answer lists, testing their ability to systematically collate fragmented information, resolve entity duplicates, and reason about stopping criteria. The benchmark includes 900 prompts with verifiable answer sets, structured as causal chains to stress long-horizon planning and context retention. State-of-the-art agents like Gemini Deep Research Agent and GPT-5 Pro High Reasoning achieve high F1 scores but struggle with precision-recall balance, often either under-retrieving or hedging with low-confidence answers. DeepSearchQA reveals critical headroom in current agent designs and highlights the need for improved exploration strategies, synthesis, and stopping criteria.

## Method Summary
DeepSearchQA evaluates agents on 900 multi-step, information-seeking tasks requiring exhaustive answer list generation. The benchmark uses outcome-based evaluation with F1-score as the primary metric, supplemented by categorical classification (Fully Correct, Fully Incorrect, Partially Correct, Correct with Extraneous Answers). An LLM-as-a-Judge (Gemini 2.5 Flash) performs zero-shot semantic equivalence evaluation against ground truth. Prompts are structured as causal chains, stressing long-horizon planning and context retention. The evaluation pipeline extracts answer sets from model responses and applies the judge to determine semantic equivalence, then computes precision, recall, and F1 per prompt.

## Key Results
- Top models achieve high F1 scores but show a ~15-point gap between F1 and Fully Correct rates ("Last Mile Problem")
- Agents exhibit both under-retrieval (premature stopping) and over-retrieval/hedging behaviors
- Smaller models (e.g., Gemini 2.5 Flash) show higher failure rates (~45% Fully Incorrect)
- Entity resolution and de-duplication remain significant challenges, with "Correct with Extraneous Answers" being a common failure mode

## Why This Works (Mechanism)
DeepSearchQA works by requiring agents to perform exhaustive list generation rather than single-answer retrieval, exposing weaknesses in current architectures. The benchmark's causal chain structure forces agents to maintain context across multiple steps and make decisions about search completeness. By providing verifiable answer sets and using semantic equivalence evaluation, it enables rigorous measurement of both precision and recall. The four-category classification system reveals specific failure modes, distinguishing between under-retrieval, over-retrieval, and complete search failures. This comprehensive approach exposes the "Last Mile Problem" where agents struggle with the final synthesis and verification stages.

## Foundational Learning

- Concept: **Causal Chain Reasoning**
  - Why needed here: To solve prompts where each step depends on the last (e.g., filtering a list, then checking attributes of the filtered items). Without this, an agent will attempt parallel or out-of-order searches and fail.
  - Quick check question: Given the query "Which cities in the state where LaMotte established a fort have Amtrak stations?", what is the first sub-question that must be answered?

- Concept: **Entity Resolution (De-duplication)**
  - Why needed here: To maintain high precision when collating lists from multiple sources that may refer to the same entity with different names or surface forms (e.g., "JPMorgan Chase" vs. "JPM").
  - Quick check question: If Source A lists "GM" and Source B lists "General Motors" as top employers, should the final list contain one or two entries? Why?

- Concept: **Epistemic Uncertainty and Stopping Criteria**
  - Why needed here: To decide when to stop searching. An agent must recognize when further searching is unlikely to yield new correct answers versus when it simply hasn't found them yet.
  - Quick check question: A search for "all companies founded in 1990 in a specific niche" returns 10 results after 5 pages. What factors should influence the decision to continue or stop?

## Architecture Onboarding

- **Component map:** Reasoning/Planning Module -> Tool Use/Execution Module -> Memory/Context Management Module -> Synthesis/Filtering Module
- **Critical path:** The agent's performance hinges on the **Reasoning -> Tool Use -> Synthesis** loop. The "Last Mile Problem" (a 15-point gap between F1 and Fully Correct rates in SOTA models) often fails in the final **Synthesis/Filtering** stage, where the agent must resolve entities and apply final constraints.
- **Design tradeoffs:**
  - **Exploration vs. Exploitation:** Agents must trade off broad, exploratory searches for high recall against deep verification of candidates for high precision. The paper shows top models still struggle with this balance.
  - **Outcome vs. Process Evaluation:** DeepSearchQA is purely outcome-based (F1 on final answer). This allows architectural flexibility but obscures *why* an agent failed (e.g., lucky guess vs. sound reasoning). Future work suggests adding process metrics.
  - **Cost vs. Performance:** SOTA performance requires significant test-time compute. The paper notes a "step-function drop" in utility for cheaper/smaller models, indicating a non-linear trade-off.
- **Failure signatures:**
  - **Under-retrieval (Premature Stopping):** High precision, low recall. Agent stops before finding all items, often due to weak exploration strategy. Associated with a "Fully Incorrect" or "Partially Correct" result.
  - **Hedging / Over-retrieval:** High recall, low precision. Agent includes speculative or hallucinated items ("Correct with Extraneous Answers") to avoid missing answers.
  - **Trajectory Divergence:** A complete search failure ("Fully Incorrect") where the agent pursues an entirely incorrect path, more common in smaller models (e.g., 45% failure rate for Gemini 2.5 Flash).
  - **Tool/Format Limitation:** Agent fails to process a required file format (e.g., an Excel spreadsheet), halting the process (Example from Table 5).
- **First 3 experiments:**
  1. **Establish a Baseline:** Evaluate a current strong agent on a sample of 50 DeepSearchQA prompts. Measure F1-score and categorize each failure into the four provided categories (Fully Correct, Fully Incorrect, Partially Correct, Correct with Extraneous). This will reveal the dominant failure mode.
  2. **Ablate on Prompt Complexity:** Select 10 "Linear Fact Retrieval" (easy) and 10 "Dependency Graph Retrieval" (hard) prompts from the benchmark (using the taxonomy in Table 2). Run the same agent. The delta in F1-score will quantify the impact of causal chain reasoning on performance.
  3. **Test a Stopping Heuristic:** Implement a simple stopping criterion for the agent (e.g., "stop after N consecutive searches yield no new entities matching the constraints"). Run an evaluation and compare the precision and recall components of the F1-score against the baseline run to see if this heuristics moves the agent towards or away from the optimal stopping point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agents develop dynamic stopping criteria to reliably distinguish between absence of evidence and evidence of absence during open-ended search?
- Basis in paper: [explicit] Section 5.3 states: "Perhaps the most critical capability is the ability to reason about search completeness. Agents must develop a stopping criterion to dynamically determine when the retrieved set is likely exhaustive."
- Why unresolved: Current agents exhibit both premature stopping (under-retrieval) and hedging behaviors (over-retrieval), with a ~15-point gap between F1-score and strict Fully Correct rates—the "Last Mile Problem."
- What evidence would resolve it: Novel agent architectures that maintain stable precision while achieving exhaustive recall, narrowing the F1-to-Fully-Correct gap below 5% on DeepSearchQA.

### Open Question 2
- Question: Can process-based metrics (trajectory analysis) effectively differentiate agents that reason correctly from those that succeed through inefficient or accidental means?
- Basis in paper: [explicit] Section 5.1 notes the black-box limitation: "In the absence of trajectory data, it is difficult to distinguish between an agent that reasoned correctly and one that arrived at the correct list through inefficient or accidental means." Section 5.2 proposes incorporating trajectory categorization.
- Why unresolved: DeepSearchQA uses purely outcome-based evaluation; no study has validated whether trajectory metrics correlate with robust, generalizable reasoning.
- What evidence would resolve it: Correlation analysis between trajectory efficiency metrics (query sequences, pages visited) and out-of-distribution generalization on held-out tasks.

### Open Question 3
- Question: What architectural mechanisms enable robust entity resolution and de-duplication across heterogeneous web sources with varying surface forms?
- Basis in paper: [explicit] Section 1.2 identifies entity resolution as one of three critical, under-evaluated capabilities: "failure to resolve entities leads to inflated lists and degraded precision, a common failure mode we observe in our experiments."
- Why unresolved: The paper observes this failure mode but does not propose or evaluate specific de-duplication mechanisms; current models still produce "Correct with Extraneous Answers" (10.30% for Gemini Deep Research Agent).
- What evidence would resolve it: Ablation studies comparing entity resolution modules on DeepSearchQA's set-answer tasks, measuring reductions in extraneous answers while maintaining recall.

## Limitations

- Ground-truth answer sets for all 900 prompts require access to external Kaggle resources, limiting full reproducibility
- The benchmark's focus on outcome-only evaluation means process-level failure modes are not directly observable
- LLM-as-a-judge evaluation introduces potential subjectivity, though the paper reports high agreement with human labels

## Confidence

- **High Confidence:** The benchmark's design is well-founded—multi-step, causal reasoning tasks are a known challenge for current agents, and the four-category failure analysis aligns with observed SOTA performance.
- **Medium Confidence:** The claim that DeepSearchQA is the first benchmark to stress exhaustive list generation with verifiable answer sets. While novel, similar multi-step reasoning benchmarks exist (e.g., HotpotQA, M3), and the paper doesn't exhaustively compare scope.
- **Medium Confidence:** The assertion that the 15-point "Last Mile Problem" is a fundamental gap in agent design. While clearly demonstrated, it could also reflect current model limitations rather than an unsolvable architectural challenge.

## Next Checks

1. **Replicate Failure Mode Distribution:** Run a current SOTA agent on a sample of DeepSearchQA prompts and categorize failures into the four provided categories to confirm the observed dominance of "Partially Correct" and "Fully Incorrect" outcomes.

2. **Isolate the Last Mile Problem:** Implement and test a simple entity resolution module (e.g., name normalization + fuzzy matching) on an agent's output to measure its impact on the gap between F1 and fully correct rates.

3. **Test Prompt Complexity Impact:** Using the benchmark's taxonomy, compare agent performance on "Linear Fact Retrieval" vs. "Dependency Graph Retrieval" prompts to quantify the effect of causal chain reasoning difficulty.