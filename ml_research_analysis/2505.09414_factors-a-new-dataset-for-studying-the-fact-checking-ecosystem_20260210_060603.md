---
ver: rpa2
title: 'FACTors: A New Dataset for Studying the Fact-checking Ecosystem'
arxiv_id: '2505.09414'
source_url: https://arxiv.org/abs/2505.09414
tags:
- fact-checking
- dataset
- organisations
- claims
- factors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FACTors, the first comprehensive fact-checking
  dataset at the ecosystem level, containing 118,112 claims from 117,993 reports published
  between 1995-2025 by 39 IFCN/EFCSN signatory organizations. The dataset addresses
  limitations of existing fact-checking datasets by providing long-term coverage,
  including real-world claims without aggregation of overlapping claims, and capturing
  metadata about fact-checkers.
---

# FACTors: A New Dataset for Studying the Fact-checking Ecosystem

## Quick Facts
- **arXiv ID**: 2505.09414
- **Source URL**: https://arxiv.org/abs/2505.09414
- **Reference count**: 40
- **Primary result**: Introduces FACTors, the first comprehensive fact-checking dataset at the ecosystem level with 118,112 claims from 117,993 reports (1995-2025) by 39 IFCN/EFCSN signatory organizations

## Executive Summary
This paper introduces FACTors, the first comprehensive fact-checking dataset at the ecosystem level, containing 118,112 claims from 117,993 reports published between 1995-2025 by 39 IFCN/EFCSN signatory organizations. The dataset addresses limitations of existing fact-checking datasets by providing long-term coverage, including real-world claims without aggregation of overlapping claims, and capturing metadata about fact-checkers. Key features include 7,327 overlapping claims corresponding to 2,977 unique claims, a Lucene index for efficient searching, and normalized verdicts mapped to six categories. The authors demonstrate the dataset's utility through three applications: ecosystem statistical analysis showing dramatic growth after 2010, political bias detection using a pre-trained BERT model revealing left-leaning tendencies, and credibility assessment assigning scores to organizations based on multiple factors. The dataset enables new research directions in automated fact-checking, ecosystem analysis, and credibility assessment, addressing critical gaps in current fact-checking resources.

## Method Summary
The authors constructed FACTors by scraping 117,993 reports from 39 IFCN/EFCSN signatory organizations using Scrapy and Playwright, prioritizing ClaimReview structured data. The dataset includes 118,112 fact-checks (including duplicates) spanning 1995-2025. Data cleaning involved language detection (English only), removal of claims under 10 characters, deduplication using SBERT cosine similarity threshold 0.95, and ISO 8601 date standardization. Verdict normalization combined manual mapping of 68 short verdicts (72,309 fact-checks), RoBERTa fine-tuning (84.9% accuracy), and manual correction of low-confidence predictions. Overlapping claims were identified using SBERT cosine similarity threshold 0.88, yielding 7,327 overlapping claims corresponding to 2,977 unique claims. The dataset provides a Lucene index for efficient searching and demonstrates three applications: ecosystem statistical analysis, political bias detection using politicalBiasBERT, and credibility assessment using a multi-factor ranking formula.

## Key Results
- FACTors contains 118,112 fact-checks from 117,993 reports (1995-2025) by 39 IFCN/EFCSN signatory organizations
- Identifies 7,327 overlapping claims corresponding to 2,977 unique claims using SBERT cosine similarity threshold 0.88
- Verdict normalization achieved 84.9% accuracy using RoBERTa fine-tuning on manually mapped 68 short verdicts
- Demonstrates ecosystem growth analysis, political bias detection (left-leaning tendencies), and credibility scoring for fact-checking organizations

## Why This Works (Mechanism)

### Mechanism 1
Preserving non-aggregated overlapping claims enables weighted verdict aggregation instead of simple majority voting. The dataset identifies 7,327 overlapping claims (2,977 unique claims) fact-checked by multiple organizations using SBERT cosine similarity (threshold 0.88). By keeping each organization's original verdict separate and normalizing to six categories, researchers can assign differentiated credibility weights to fact-checkers rather than treating them as equally reliable.

### Mechanism 2
Multi-factor credibility scoring differentiates fact-checking organizations beyond treating them as uniformly trustworthy. The paper computes credibility scores using Equation 1, combining six factors (experience, fact-check count, unique claim percentage, fact-checking rate, word count, author count) with political bias as a negative factor. Each factor is ranked across organizations, and scores aggregate weighted rank positions.

### Mechanism 3
Hybrid verdict normalization (manual mapping + RoBERTa fine-tuning + low-confidence correction) achieves sufficient accuracy for cross-organization analysis. The authors first manually mapped 68 unique short verdicts (covering 72,309 fact-checks from 33 organizations). They fine-tuned RoBERTa on this labeled subset (84.9% accuracy), applied it to remaining verdicts, then manually corrected 1,564 predictions with confidence below 0.5.

## Foundational Learning

### Concept: Sentence embeddings and semantic similarity thresholds
- Why needed here: The dataset uses SBERT embeddings with cosine similarity thresholds for both deduplication (0.95) and overlapping claim detection (0.88). Understanding that these thresholds trade off precision vs. recall is essential for interpreting the 7,327 overlapping claims.
- Quick check question: If you lower the overlapping claim threshold from 0.88 to 0.80, would you expect more false positives or more false negatives?

### Concept: IFCN/EFCSN signatory standards as quality filters
- Why needed here: The dataset restricts sources to 39 active IFCN/EFCSN signatories, which the paper presents as a proxy for adherence to fact-checking principles (non-partisanship, transparency, accountability). This is the key justification for ecosystem-level representativeness.
- Quick check question: Why might excluding non-signatory fact-checkers introduce selection bias into ecosystem analysis?

### Concept: Verdict normalization across heterogeneous rating scales
- Why needed here: Different organizations use different verdict vocabularies. The six-category normalization (true, partially true, false, misleading, unverifiable, other) is a design choice that collapses organizational diversity for cross-comparison.
- Quick check question: What information is potentially lost when mapping organization-specific verdicts to six standardized categories?

## Architecture Onboarding

### Component map
Data Collection Layer: Scrapy spiders + Playwright for dynamic pages → ClaimReview structured data (priority) → meta tags (fallback)
Preprocessing Layer: Language detection (langdetect) → deduplication (SBERT 0.95) → verdict normalization (manual + RoBERTa + correction)
Index Layer: Apache Lucene inverted index (version 8.11.0) with Pyserini Python interface
Analysis Layer: Statistical aggregation → politicalBiasBERT inference → credibility scoring (Equation 1)

### Critical path
1. Load Lucene index via Pyserini
2. Query by organization, date range, or claim text
3. Retrieve overlapping claims via shared claim_id
4. Apply normalized verdicts for cross-organization comparison
5. Compute credibility-weighted aggregation (if desired)

### Design tradeoffs
- **English-only**: Simplifies analysis but excludes 3 organizations with non-English primary content and limits global ecosystem coverage
- **Copyright compliance**: Full-text reports removed from public release; researchers must access original URLs for content analysis
- **Overlapping claim detection**: 0.88 threshold chosen for 95% precision on 1,000-sample annotation; authors acknowledge likely false positives and false negatives without manual verification
- **Verdict normalization model**: 84.9% accuracy accepted as sufficient; low-confidence predictions manually corrected

### Failure signatures
- **Anti-scraping blocks**: Reuters, Washington Post, Deutsche Welle could not be collected (3 of 42 target organizations)
- **Non-English contamination**: Some multilingual organizations included non-English reports on English pages (removed via langdetect)
- **Missing ClaimReview**: Organizations without structured data required fallback to meta tag extraction, potentially noisier claim/verdict pairs
- **Aggregated verdicts unusable for weighted voting**: If you need ground-truth credibility labels, the dataset provides only proxy scores (Equation 1 is a demonstration, not validated methodology)

### First 3 experiments
1. **Reproduction check**: Query the Lucene index for all overlapping claims (claim_id with count > 1), extract their normalized verdicts, and compute inter-annotator agreement across organizations using Cohen's kappa.
2. **Threshold sensitivity analysis**: Re-run overlapping claim detection with alternative SBERT thresholds (0.80, 0.85, 0.90) on a sample, manually annotate precision/recall, and compare to the paper's 0.88/95% precision claim.
3. **Credibility factor validation**: Using the 7,327 overlapping claims as a test set, evaluate whether organizations with higher credibility scores (Equation 1) are more likely to agree with majority verdicts on contested claims.

## Open Questions the Paper Calls Out

### Open Question 1
How can the impact coefficients ($c_j$) and negativity effects ($e_j$) in the credibility scoring formula be empirically optimized rather than assumed equal? The current scoring model treats all credibility factors (e.g., experience, political bias) as having equal importance, which may not reflect real-world influence on fact-checker reliability.

### Open Question 2
What is the precision and recall of the 0.88 cosine similarity threshold used for identifying overlapping claims? The threshold was determined via a limited random sample, leaving the actual error rate of the 7,327 identified overlapping claims in the full dataset unverified.

### Open Question 3
Can weighted voting games and non-classical logics be successfully applied to FACTors to generate more robust consensus on conflicting verdicts? While the dataset provides the necessary structure, the proposed theoretical frameworks have not yet been implemented or tested on the data.

## Limitations
- English-only coverage excludes 3 organizations and limits global ecosystem representation
- Verdict normalization accuracy of 84.9% introduces uncertainty in cross-organization comparisons
- Overlapping claim detection may contain false positives and negatives due to lack of manual verification
- Credibility scoring methodology uses arbitrary factor weights (all set to 1) and proxy metrics
- Exclusion of non-signatory fact-checkers may introduce selection bias into ecosystem analysis

## Confidence

**High confidence**: Dataset size (118,112 claims from 39 organizations), date coverage (1995-2025), Lucene index availability, basic statistical findings about ecosystem growth

**Medium confidence**: Verdict normalization methodology and accuracy, overlapping claim detection (threshold 0.88), political bias detection using politicalBiasBERT

**Low confidence**: Credibility scoring methodology and factor weights, weighted verdict aggregation for overlapping claims, conclusions about fact-checker reliability

## Next Checks
1. **Inter-annotator agreement**: Query overlapping claims from the Lucene index and compute Cohen's kappa for normalized verdicts across organizations to assess consistency in the dataset's core feature.

2. **Threshold sensitivity**: Test alternative SBERT thresholds (0.80, 0.85, 0.90) on a sample of overlapping claims, manually annotate precision/recall, and compare results to the paper's claimed 95% precision at threshold 0.88.

3. **Credibility factor validation**: Using the 7,327 overlapping claims as ground truth, evaluate whether organizations with higher credibility scores (Equation 1) more frequently agree with majority verdicts on contested claims, providing empirical support for the credibility ranking methodology.