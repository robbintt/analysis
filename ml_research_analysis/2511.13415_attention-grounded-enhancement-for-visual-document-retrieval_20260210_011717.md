---
ver: rpa2
title: Attention Grounded Enhancement for Visual Document Retrieval
arxiv_id: '2511.13415'
source_url: https://arxiv.org/abs/2511.13415
tags:
- attention
- retrieval
- document
- arxiv
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AGREE introduces a training framework that leverages attention
  maps from multimodal large language models as fine-grained supervision to enhance
  visual document retrieval. While existing methods use global relevance labels, they
  struggle with non-extractive queries due to lack of region-level matching clues.
---

# Attention Grounded Enhancement for Visual Document Retrieval

## Quick Facts
- arXiv ID: 2511.13415
- Source URL: https://arxiv.org/abs/2511.13415
- Authors: Wanqing Cui; Wei Huang; Yazhi Guo; Yibo Hu; Meiguang Jin; Junfeng Ma; Keping Bi
- Reference count: 40
- Primary result: AGREE improves visual document retrieval with attention supervision, achieving +7.03% nDCG@1 and +2.95% nDCG@5 on ViDoRe V2

## Executive Summary
AGREE introduces a training framework that leverages attention maps from multimodal large language models as fine-grained supervision to enhance visual document retrieval. While existing methods use global relevance labels, they struggle with non-extractive queries due to lack of region-level matching clues. AGREE addresses this by extracting query-conditioned attention from an MLLM, downsampling it to align with the retriever's patch layout, and training the retriever with both global contrastive loss and local alignment loss between patch similarity scores and the attention. This dual supervision teaches the retriever not only if a document is relevant, but which regions support the match. Experiments on the challenging ViDoRe V2 benchmark show AGREE improves nDCG@1 from 54.81% to 61.84% (+7.03%) and nDCG@5 from 58.59% to 61.54% (+2.95%) over the strong baseline ColQwen2.5.

## Method Summary
The AGREE framework trains visual document retrievers using attention maps from multimodal large language models as region-level supervision. The approach extracts query-conditioned attention from an MLLM, downsamples it to match the retriever's patch layout, and applies dual supervision during training: a global contrastive loss for overall relevance and a local alignment loss between patch similarity scores and attention maps. This enables the retriever to learn not just whether a document is relevant, but which specific regions support the match. The framework is evaluated on the ViDoRe V2 benchmark, demonstrating significant improvements in retrieval performance for non-extractive queries that require understanding document regions beyond simple keyword matching.

## Key Results
- AGREE improves nDCG@1 from 54.81% to 61.84% (+7.03%) on ViDoRe V2
- AGREE improves nDCG@5 from 58.59% to 61.54% (+2.95%) on ViDoRe V2
- The method demonstrates ability to capture both explicit and implicit matches beyond surface-level keyword matching

## Why This Works (Mechanism)
AGREE works by providing region-level supervision through attention maps from MLLMs, addressing the limitation of global relevance labels that fail to guide retrievers on which document regions support matching. The attention maps extracted from MLLMs conditioned on queries provide fine-grained clues about region importance, which are aligned with the retriever's patch layout through downsampling. The dual supervision approach combines global contrastive loss with local alignment loss, teaching the retriever both overall relevance and specific region matching. This allows the model to learn rationale-aware retrieval rather than relying solely on surface-level features or keyword matching.

## Foundational Learning
- **Multimodal Large Language Models (MLLMs)**: AI models that process both visual and textual inputs to generate responses; needed because they provide sophisticated attention mechanisms that can identify relevant document regions for given queries; quick check: verify attention maps capture semantically relevant regions
- **Visual Document Retrieval**: The task of finding relevant documents from a collection based on visual and textual content; needed as the core problem AGREE addresses; quick check: ensure retrieval metrics (nDCG) are properly computed
- **Attention Mechanisms**: Neural network components that weigh the importance of different input elements; needed to extract region-level relevance information from MLLMs; quick check: validate attention map quality and alignment with patches
- **Contrastive Learning**: Training approach that brings similar items closer and pushes dissimilar items apart in embedding space; needed for global relevance supervision; quick check: verify contrastive loss implementation
- **Patch-based Document Representation**: Dividing documents into patches for processing and matching; needed to align with MLLM attention maps; quick check: ensure patch alignment with attention map downsampling is correct
- **Non-extractive Queries**: Queries that cannot be answered by simply extracting text from documents; needed to understand the challenging scenarios AGREE addresses; quick check: analyze query types in evaluation dataset

## Architecture Onboarding

Component Map: MLLM Attention Extractor -> Patch Alignment Module -> Retriever Training (Global Contrastive + Local Alignment Loss) -> Visual Document Retrieval

Critical Path: Query -> MLLM -> Attention Map Extraction -> Downsampling -> Patch Alignment -> Dual Loss Training -> Enhanced Retriever

Design Tradeoffs:
- Using MLLM attention provides rich region-level supervision but introduces computational overhead and potential bias from the MLLM's own limitations
- Dual supervision (global + local) improves learning but increases training complexity compared to single-loss approaches
- Patch-based representation enables alignment with attention maps but may lose fine-grained spatial information

Failure Signatures:
- Poor performance on non-extractive queries indicates failure to capture region-level relevance
- Large gap between nDCG@1 and nDCG@5 suggests the model struggles with consistent ranking beyond top results
- Computational inefficiency during training due to attention map extraction and processing

First Experiments:
1. Train retriever with only global contrastive loss (no attention supervision) to establish baseline performance
2. Test retriever with attention supervision on extractive queries to verify general effectiveness
3. Evaluate attention map quality by comparing with human-annotated region relevance on sample documents

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is confined to a single dataset (ViDoRe V2), raising questions about generalizability across different visual document retrieval tasks and domains
- The improvement in nDCG@5 (+2.95%) is more modest compared to nDCG@1 (+7.03%), suggesting the method may be less consistent across broader ranking positions
- Reliance on attention maps from an MLLM introduces potential confounding factors regarding the quality and reliability of these attention signals as ground truth for region-level relevance

## Confidence

**High confidence**: The technical implementation of attention-based supervision and the experimental improvements on ViDoRe V2 are well-documented and reproducible

**Medium confidence**: The claim that AGREE captures both explicit and implicit matches is supported by qualitative analysis but lacks rigorous quantitative validation across diverse query types

**Low confidence**: The assertion that attention maps provide reliable region-level supervision for training retrievers, given potential biases and limitations in MLLM attention mechanisms

## Next Checks

1. Evaluate AGREE on additional visual document retrieval benchmarks (e.g., RVL-CDIP, FUNSD) to assess cross-domain generalization
2. Conduct ablation studies removing the attention supervision component to quantify its specific contribution versus the global contrastive loss
3. Perform human evaluation studies comparing retrieved document regions with human-annotated relevance regions to validate the quality of attention-based supervision