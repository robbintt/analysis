---
ver: rpa2
title: Fully tensorial approach to hypercomplex neural networks
arxiv_id: '2407.00449'
source_url: https://arxiv.org/abs/2407.00449
tags:
- algebra
- neural
- tensor
- hypercomplex-valued
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A tensor-based formulation for hypercomplex-valued neural networks
  is introduced, where algebra multiplication is represented as a rank-three tensor.
  This enables universal implementation of hypercomplex dense and convolutional layers
  using standard tensor contractions, permutations, and reshaping operations, generalizing
  existing quaternion and 4-dimensional algebra constructions.
---

# Fully tensorial approach to hypercomplex neural networks

## Quick Facts
- arXiv ID: 2407.00449
- Source URL: https://arxiv.org/abs/2407.00449
- Reference count: 35
- Primary result: Tensor-based hypercomplex neural networks enable universal implementation of arbitrary finite-dimensional algebra operations using standard tensor contractions, generalizing beyond fixed-dimension quaternion formulations.

## Executive Summary
This paper introduces a fully tensorial framework for hypercomplex neural networks that represents algebra multiplication as a rank-three structure tensor, enabling universal implementation of hypercomplex dense and convolutional layers using standard tensor operations. The approach generalizes existing quaternion-based methods to arbitrary finite-dimensional algebras, providing dimension-independent neural network architectures compatible with optimized deep learning libraries. The framework includes a tensor version of the universal approximation theorem for single-layer hypercomplex perceptrons under non-degeneracy assumptions, and supports natural encoding of multi-component data into algebra elements.

## Method Summary
The method encodes algebra multiplication via a rank-three structure tensor $A^k_{ij}$, where tensor contractions with learnable kernels $K$ transform hypercomplex operations into standard linear algebra operations. Dense layers contract $A$ with $K$ and reshape to form effective weight matrices, while convolutional layers decompose into independent real-valued convolutions per algebra component after structure tensor processing. The framework supports arbitrary finite-dimensional algebras and includes a theoretical foundation showing single-layer hypercomplex perceptrons are universal approximators under non-degeneracy conditions. Implementation is provided through the Hypercomplex Keras library.

## Key Results
- Universal approximation theorem holds for single-layer hypercomplex perceptrons with non-degenerate algebras
- Tensor formulation enables arbitrary finite-dimensional algebra support beyond fixed-dimension quaternion approaches
- Implementation compatible with standard deep learning libraries through tensor contractions and reshaping operations

## Why This Works (Mechanism)

### Mechanism 1: Algebra Multiplication via Structure Tensors
The paper proposes representing algebraic multiplication using a rank-three structure tensor $A$, where the product of two elements is computed via tensor contraction. This transforms hypercomplex operations into standard linear algebra compatible with deep learning libraries. The core assumption is that algebra multiplication is bilinear and the algebra is finite-dimensional.

### Mechanism 2: Component-wise Separation for Convolution
Hypercomplex convolution is decomposed into independent real-valued convolutions followed by concatenation. The algebra multiplication tensor $A$ is contracted with the kernel $K$ first, creating real-valued kernels for each algebra component that can be processed by optimized convolution routines.

### Mechanism 3: Universal Approximation via Non-Degeneracy
A single-layer hypercomplex perceptron is a universal approximator for continuous functions over compact subsets when the algebra is non-degenerate. The proof reduces to classical real-valued UAT by projecting hypercomplex functions onto real components and leveraging the Riesz representation theorem.

## Foundational Learning

- **Structure Constants / Multiplication Tensors**: Core representation of algebra multiplication as rank-3 tensor $A^k_{ij}$. Quick check: Given a 2D algebra with basis $\{e_0, e_1\}$ where $e_1 \cdot e_1 = e_0$, what are the non-zero entries of $A$?
- **Tensor Contractions (Einstein Summation)**: Implementation uses contraction $C$ and permutation $\tau$ operations. Quick check: In $W \leftarrow C_{1,0}(A, K)$, are resulting dimensions larger, smaller, or same rank as $A$?
- **Non-Degeneracy in Algebras**: Critical property for universal approximation theorem. Quick check: Does the algebra of $2 \times 2$ real matrices satisfy the non-degeneracy condition?

## Architecture Onboarding

- **Component map**: Inputs (X, A) -> HyperDense: contract A with K -> reshape -> matrix multiply with X -> bias/activation
- **Critical path**: Correct definition of multiplication tensor $A$ is single point of failure; incorrect $A$ breaks algebraic logic
- **Design tradeoffs**: Increasing algebra dimension increases computation quadratically for dense layers, linearly for convolutions; adds memory overhead
- **Failure signatures**: Dimension mismatch in reshape/permute, loss instability with degenerate algebras, slow convergence with conflicting algebra structure
- **First 3 experiments**:
  1. Implement 2D complex algebra using rank-3 tensor and verify reproduces standard complex network on XOR task
  2. Benchmark HyperConv2D with varying algebra dimensions (al=2,4,8) against standard Conv2D with equivalent parameters
  3. Train single HyperDense layer with non-degenerate vs degenerate algebra on regression task targeting known hypercomplex function

## Open Questions the Paper Calls Out

### Open Question 1
Does the Universal Approximation Theorem hold for degenerate algebras within the fully tensorial framework? Section 5 explicitly states the guarantee is not covered for degenerate algebras. The proof relies on non-degeneracy for Riesz representation. Evidence would require theoretical extension or counter-example.

### Open Question 2
Can the framework support learnable algebra parameters while maintaining computational efficiency? The current method uses fixed multiplication tensor $A$. Evidence would require showing backpropagation through $A$ remains stable when treated as learnable.

### Open Question 3
How does algebraic structure choice quantitatively impact representational capacity and inductive bias? Section 3.3 states expressivity depends on both activation and algebraic structure, but offers no selection methodology. Evidence would require comparative analysis mapping algebraic axioms to performance metrics.

## Limitations
- Theoretical guarantees only apply to non-degenerate algebras, with no complete characterization of which algebras satisfy this property
- Computational complexity scaling with algebra dimension not systematically quantified across different layer types
- Implementation details for Hypercomplex Keras library referenced but not fully specified, creating reproducibility gaps

## Confidence

- **High Confidence**: Tensorial formulation of algebra multiplication is mathematically sound
- **Medium Confidence**: Convolution decomposition approach is correct but efficiency claims need empirical validation
- **Medium Confidence**: Practical implementation through Hypercomplex Keras appears viable but lacks detailed specification

## Next Checks

1. Systematically identify which finite-dimensional algebras satisfy the non-degeneracy condition required for universal approximation theorem
2. Quantify computational overhead of tensor contraction and permutation operations across varying algebra dimensions (al=2,4,8,16) for both dense and convolutional layers
3. Reproduce basic functionality of HyperDense and HyperConv layers using standard deep learning framework operations without relying on Hypercomplex Keras library