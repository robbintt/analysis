---
ver: rpa2
title: 'OntoMetric: An Ontology-Guided Framework for Automated ESG Knowledge Graph
  Construction'
arxiv_id: '2512.01289'
source_url: https://arxiv.org/abs/2512.01289
tags:
- extraction
- knowledge
- metric
- validation
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OntoMetric is an ontology-guided framework for automatically constructing
  ESG metric knowledge graphs from unstructured regulatory documents. The system addresses
  the challenge that ESG metric knowledge, though inherently structured, remains embedded
  in narrative regulatory text and is rarely available as an explicit, machine-actionable
  artifact.
---

# OntoMetric: An Ontology-Guided Framework for Automated ESG Knowledge Graph Construction

## Quick Facts
- arXiv ID: 2512.01289
- Source URL: https://arxiv.org/abs/2512.01289
- Reference count: 40
- OntoMetric achieves 65–90% semantic accuracy and over 80% schema compliance on ESG metric extraction from regulatory documents.

## Executive Summary
OntoMetric is an ontology-guided framework for automatically constructing ESG Metric Knowledge Graphs (ESGMKG) from unstructured regulatory documents. The system addresses the challenge that ESG metric knowledge, though inherently structured, remains embedded in narrative regulatory text and is rarely available as an explicit, machine-actionable artifact. OntoMetric operationalizes the ESGMKG ontology as a first-class constraint embedded directly into extraction, combining structure-aware segmentation, ontology-constrained LLM extraction enriched with semantic fields, and two-phase validation (semantic verification plus rule-based schema checking) while preserving segment-level and page-level provenance for auditability. Evaluated on five major ESG standards (SASB Commercial Banks, SASB Semiconductors, IFRS S2, AASB S2, TCFD), OntoMetric achieved 65–90% semantic accuracy and over 80% schema compliance, compared to 3–10% for baseline unconstrained extraction, at approximately $0.01–0.02 per validated entity and a 48× efficiency improvement.

## Method Summary
OntoMetric operationalizes the ESGMKG ontology as a first-class constraint embedded directly into extraction, combining structure-aware segmentation, ontology-constrained LLM extraction enriched with semantic fields, and two-phase validation (semantic verification plus rule-based schema checking) while preserving segment-level and page-level provenance for auditability. The framework processes five ESG regulatory standards through a three-stage pipeline: (1) Structure-aware segmentation that uses TOC-based PDF splitting into JSON segments, (2) Extraction using Claude Sonnet 4.5 with a 9-component prompt enforcing ESGMKG schema constraints and ID generation rules, and (3) Two-phase validation filtering that combines LLM-based semantic verification with 6 rule-based validators (VR001–VR006) to ensure both semantic validity and schema compliance.

## Key Results
- Semantic accuracy of 65–90% on five ESG regulatory standards compared to 3–10% for baseline unconstrained extraction
- Schema compliance exceeding 80% through ontology-constrained extraction versus baseline performance
- Cost efficiency of $0.01–0.02 per validated entity with 48× improvement over baseline approaches

## Why This Works (Mechanism)
OntoMetric works by embedding the ESGMKG ontology directly into the extraction process as a first-class constraint, rather than treating it as a post-hoc validation step. This ontology-first approach ensures that extraction is guided by the expected structure from the outset, with the LLM generating only valid entities and relationships defined in the schema. The two-phase validation system provides both semantic verification (ensuring extracted content makes sense in context) and rule-based schema checking (ensuring structural compliance), while the preservation of provenance metadata enables auditability and debugging.

## Foundational Learning
- **ESGMKG Ontology**: The five-entity, five-relationship schema that defines valid ESG metric knowledge structures. *Why needed*: Provides the semantic foundation that guides extraction and validation. *Quick check*: Verify the ontology schema includes Industry, ReportingFramework, Category, Metric, and Model entities with their relationships.
- **Ontology-Constrained Extraction**: Embedding schema constraints directly into the LLM prompt to guide generation. *Why needed*: Prevents the model from hallucinating invalid entities or relationships. *Quick check*: Confirm the prompt explicitly lists allowed entity types and relationship predicates.
- **Two-Phase Validation**: Combining LLM-based semantic verification with rule-based schema validation. *Why needed*: Catches both semantic inconsistencies and structural violations. *Quick check*: Test that both Phase 1 and Phase 2 validators are applied to extracted triples.
- **Provenance Preservation**: Maintaining segment-level and page-level metadata throughout extraction. *Why needed*: Enables auditability and debugging of extraction failures. *Quick check*: Verify JSON outputs include document, page, and segment identifiers.
- **Structure-Aware Segmentation**: Using TOC-based PDF splitting to create coherent extraction contexts. *Why needed*: Provides the LLM with focused, relevant text segments rather than entire documents. *Quick check*: Confirm segmentation produces JSON segments with appropriate page ranges.
- **Cost Efficiency Optimization**: Balancing extraction quality with API usage costs. *Why needed*: Makes large-scale ESGKG construction economically viable. *Quick check*: Monitor API token usage and extraction success rates to optimize cost per entity.

## Architecture Onboarding

**Component Map**: PDF Documents -> Structure-aware Segmentation -> LLM Extraction (Claude Sonnet 4.5) -> Two-Phase Validation -> ESGMKG Output

**Critical Path**: The extraction pipeline's critical path is: PDF segmentation → LLM extraction with ontology constraints → two-phase validation (semantic + schema). Each stage depends on successful completion of the previous stage, with validation failures causing entity rejection.

**Design Tradeoffs**: The framework trades computational overhead (two-phase validation) for higher precision, and LLM API costs for automated extraction versus manual annotation. The choice of Claude Sonnet 4.5 balances capability with cost, while the 9-component prompt design prioritizes schema compliance over extraction flexibility.

**Failure Signatures**: 
- Low Schema Compliance (<80%): Indicates extraction prompt or schema constraint issues
- High Cost Waste Ratio (>35%): Suggests poor segmentation causing LLM hallucinations
- JSON parsing errors: Indicates extraction prompt not properly enforcing output schema
- Semantic accuracy below 65%: May indicate ontology mismatch with source documents

**3 First Experiments**:
1. Run Algorithm 1 on a single PDF to verify TOC-based segmentation produces coherent JSON segments with proper provenance metadata
2. Execute the LLM extraction pipeline on one segmented document using the 9-component prompt to verify valid JSON output conforming to ESGMKG schema
3. Apply the two-phase validation (VR001–VR006) to extracted triples to confirm >80% schema compliance and identify validation failure patterns

## Open Questions the Paper Calls Out

**Open Question 1**: Can the segmentation mechanism be effectively extended to process documents with implicit or inconsistent section boundaries that lack table-of-contents structures? The authors state that extending the mechanism to documents with "implicit, inconsistent, or weakly signalled section boundaries is a natural next step," but the current framework relies strictly on TOC-guided boundaries, limiting its applicability to regulatory documents with explicit structural markers. Successful extraction results from a corpus of unstructured or semi-structured documents where segmentation is performed without TOC metadata would resolve this.

**Open Question 2**: Does the reliance on the same class of LLM for both extraction and semantic validation introduce systematic biases or circularity? The paper notes that semantic validation "relies on the same class of large language model used for extraction, introducing potential circularity in which systematic model biases may go undetected." A model might consistently fail to detect its own hallucinations or semantic errors during the self-verification phase. Comparative evaluation where semantic verification is performed by a different model family or human experts would identify discrepancies with the current LLM-based validation.

**Open Question 3**: Can the ontology schema and pipeline be generalized to capture broader ESG concepts such as risks, opportunities, and temporal aspects? The authors acknowledge the current system "omitting other ESG concepts such as risks, opportunities, targets, governance structures, and temporal aspects." The existing ESGMKG ontology is limited to five core entity types, restricting the knowledge graph's ability to represent the full complexity of ESG standards. An extended ontology and extraction prompts that successfully instantiate these additional entity types with high semantic accuracy would resolve this.

**Open Question 4**: How does the framework perform on corporate reports, financial filings, and non-English regulatory sources? The evaluation "does not yet cover corporate reports, financial filings, or non-English sources, which may exhibit different structural and linguistic characteristics." The empirical validation was confined to five English-language regulatory standards, leaving the pipeline's robustness across diverse document types unproven. Evaluation metrics (Semantic Accuracy, Schema Compliance) derived from applying OntoMetric to corporate sustainability reports and non-English regulatory documents would resolve this.

## Limitations
- Evaluation relies on manual ground-truth creation, introducing potential subjectivity in semantic accuracy measurements
- Claims about superiority over "previous frameworks" are unsupported by comparative data against other published ESG extraction systems
- The baseline comparison lacks transparency in implementation details, making independent verification difficult

## Confidence

**High Confidence**: The three-stage pipeline architecture is clearly specified and reproducible. The ESGMKG ontology schema and rule-based validators (VR001–VR006) are fully defined. Cost and efficiency metrics are straightforward calculations from reported data.

**Medium Confidence**: Semantic accuracy and schema compliance percentages are plausible given the methodology but depend on the quality of manual annotations, which are not publicly available. The comparison to the unconstrained baseline lacks transparency in implementation details.

**Low Confidence**: Claims about superiority over "previous frameworks" are unsupported by comparative data against other published ESG extraction systems.

## Next Checks

1. **Schema Compliance Audit**: Re-run the validation phase on a subset of extracted triples using only the VR001–VR006 rule-based validators to verify the reported >80% compliance rate independently of semantic judgments.

2. **Segmentation Impact Analysis**: Systematically vary the TOC detection threshold in Algorithm 1 and measure resulting changes in semantic accuracy to quantify the impact of Stage 1 segmentation quality on downstream extraction performance.

3. **Cost Benchmarking**: Track actual API usage costs (prompts, tokens, retries) during reproduction to validate the reported $0.01–0.02 per entity range and identify potential cost optimization opportunities in the current pipeline configuration.