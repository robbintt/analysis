---
ver: rpa2
title: Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes
arxiv_id: '2505.23619'
source_url: https://arxiv.org/abs/2505.23619
tags:
- detection
- deepfake
- gaussian
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting audio deepfake detection
  systems to new, unseen text-to-speech (TTS) models with minimal data. The proposed
  method, ADD-GP, combines a deep embedding model (XLS-R) with a Gaussian Process
  (GP) classifier to enable few-shot adaptation.
---

# Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes

## Quick Facts
- **arXiv ID:** 2505.23619
- **Source URL:** https://arxiv.org/abs/2505.23619
- **Reference count:** 0
- **Primary result:** ADD-GP outperforms SSL-AASIST and CD-ADD in few-shot deepfake detection, achieving lower error rates with as few as 5-10 samples.

## Executive Summary
This paper addresses the challenge of adapting audio deepfake detection systems to new, unseen text-to-speech (TTS) models with minimal data. The proposed method, ADD-GP, combines a deep embedding model (XLS-R) with a Gaussian Process (GP) classifier to enable few-shot adaptation. The GP framework allows for non-parametric learning and provides uncertainty estimates, making it well-suited for adapting to new TTS models. The authors introduce a benchmark dataset, LibriFake, for evaluating few-shot and personalized deepfake detection. Results show that ADD-GP significantly outperforms existing methods like SSL-AASIST and CD-ADD in detecting deepfakes from unseen TTS models, achieving lower error rates with as few as 5-10 samples. Additionally, the personalized version of ADD-GP demonstrates strong performance even with a single sample, highlighting its robustness and adaptability. The method also provides well-calibrated uncertainty estimates, which is valuable for practical deployment. Overall, ADD-GP offers an effective solution for adapting deepfake detection to evolving TTS technologies.

## Method Summary
The method employs Deep Kernel Learning (DKL) with a Gaussian Process classifier. Audio is first processed by XLS-R (a self-supervised speech model) into embeddings. An RBF kernel is applied to these embeddings, and the GP classifier uses these kernel similarities to predict real/fake labels. The kernel parameters (including the feature extractor weights) are trained on known TTS data via marginal likelihood optimization. For few-shot adaptation to a new TTS, the method simply adds the new samples to the GP's support set and recomputes predictionsâ€”no gradient updates required. A MixPro augmentation scheme generates interpolated embeddings to expand the few-shot dataset. The approach also supports personalized detection by adapting to both the speaker and TTS generator.

## Key Results
- ADD-GP achieves significantly lower EER than SSL-AASIST and CD-ADD when adapting to unseen TTS models with 5-10 samples
- The method provides well-calibrated uncertainty estimates, unlike baseline deep learning approaches
- Personalized ADD-GP maintains strong performance even with a single adaptation sample
- The MixPro augmentation further improves performance in few-shot scenarios

## Why This Works (Mechanism)

### Mechanism 1: Non-Parametric Adaptation via Kernel Expansion
The GP classifier is non-parametric, so adapting to a new TTS model involves simply expanding the kernel covariance matrix with the new samples. This avoids gradient-based updates and catastrophic forgetting.

### Mechanism 2: Deep Kernel Learning (DKL) for Semantic Similarity
XLS-R transforms audio into embeddings where the RBF kernel can effectively measure similarity. Training the kernel parameters ensures the embedding space clusters real and fake samples appropriately.

### Mechanism 3: Uncertainty-Calibrated Detection
GPs provide full posterior distributions, including variance estimates. This allows the system to identify ambiguous samples where it's uncertain, rather than making overconfident predictions.

## Foundational Learning

- **Concept: Gaussian Process Classification**
  - **Why needed here:** The paper replaces standard classifiers with GPs to enable non-parametric adaptation and uncertainty estimation.
  - **Quick check question:** How does the prediction of a GP change if a new data point is added to the training set, compared to a standard Multi-Layer Perceptron (MLP)?

- **Concept: Deep Kernel Learning (DKL)**
  - **Why needed here:** DKL bridges deep learning for audio and Bayesian non-parametrics by using a neural network to transform inputs before applying a kernel.
  - **Quick check question:** In Eq. 4, what happens to the kernel output if the neural network $g_\theta$ produces identical embeddings for two different audio files?

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The paper frames few-shot adaptation as a solution to the problem where fine-tuning on new TTS models degrades performance on old ones.
  - **Quick check question:** Why does the "non-parametric" nature of the GP help avoid catastrophic forgetting better than the weight updates in "SSL-AASIST-FT"?

## Architecture Onboarding

- **Component map:** XLS-R (front-end) -> RBF Kernel (similarity measure) -> Dirichlet-based GP Classifier (back-end)
- **Critical path:** Pre-train kernel parameters on known TTS data using Log Marginal Likelihood, reserve 1000 training examples for GP basis, adapt to new TTS by adding few-shot examples to the basis and recomputing GP predictions
- **Design tradeoffs:** Scalability vs. Accuracy (cubic GP complexity limits support set size), Frozen vs. Trainable Backbone (most XLS-R frozen for speed)
- **Failure signatures:** High EER on OOD TTS (poor kernel generalization), Overconfidence (incorrect high-confidence predictions)
- **First 3 experiments:** 1) Baseline Verification: reproduce ID vs OOD TTS gap, 2) Ablation on Support Set: vary reserved examples, 3) Calibration Check: reproduce reliability diagram

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the requirement for speaker-specific deepfake samples in personalized detection be relaxed by utilizing generic deepfake samples?
- **Basis in paper:** The authors note the personalized detector is "limited by the fact that it requires deepfake samples specifically for the target speaker rather than relying on generic deepfake samples."
- **Why unresolved:** It is unclear if generic samples can provide sufficient signal for the GP classifier to distinguish speaker-specific real/fake boundaries.
- **What evidence would resolve it:** An ablation study comparing performance when adapting the personalized detector with speaker-specific samples versus generic samples from the target TTS.

### Open Question 2
- **Question:** How does ADD-GP performance and latency trade-off when scaling the support set beyond 1,000 examples?
- **Basis in paper:** The method fixes the training subset for the GP to 1,000 examples, likely due to the cubic computational complexity of standard Gaussian Processes.
- **Why unresolved:** Standard GPs struggle with large datasets; it is unknown if maintaining a larger "memory" of attacks significantly boosts few-shot robustness.
- **What evidence would resolve it:** Benchmarks using sparse GP approximations (e.g., inducing points) on support sets exceeding 10,000 samples.

### Open Question 3
- **Question:** Does the Deep Kernel Learning approach generalize to other speech embedding models?
- **Basis in paper:** The implementation specifically utilizes XLS-R without testing the framework's compatibility with other SSL models like HuBERT or WavLM.
- **Why unresolved:** It is uncertain if the kernel learning benefits are specific to the XLS-R feature geometry or if they apply broadly to other self-supervised representations.
- **What evidence would resolve it:** Cross-model experiments applying the ADD-GP framework to HuBERT or WavLM embeddings.

## Limitations

- Scalability concerns due to cubic GP complexity with support set size
- Performance depends on XLS-R's ability to generalize embeddings across diverse TTS architectures
- Requires at least some adaptation samples from the target TTS model
- May struggle if new TTS generates audio outside XLS-R's learned embedding distribution

## Confidence

- **High confidence:** The GP-based few-shot adaptation mechanism works effectively on the LibriFake benchmark
- **Medium confidence:** The XLS-R feature extractor generalizes sufficiently across different TTS models
- **Medium confidence:** The uncertainty estimates from the GP framework are well-calibrated and practically useful

## Next Checks

1. **OOD Generalization Stress Test:** Systematically vary the degree of similarity between seen and unseen TTS models to determine the limits of the kernel's generalization ability
2. **Kernel Hyperparameter Sensitivity:** Conduct an ablation study on the RBF kernel's length scale and output scale, measuring how these parameters affect both detection accuracy and uncertainty calibration
3. **Scalability Benchmark:** Evaluate the GP's computational complexity and memory usage as the number of support points increases beyond 1000, and test if approximate GP methods can maintain accuracy while improving efficiency