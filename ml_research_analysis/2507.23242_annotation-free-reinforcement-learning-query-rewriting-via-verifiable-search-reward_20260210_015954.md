---
ver: rpa2
title: Annotation-Free Reinforcement Learning Query Rewriting via Verifiable Search
  Reward
arxiv_id: '2507.23242'
source_url: https://arxiv.org/abs/2507.23242
tags:
- query
- rl-qr
- retrieval
- rewriting
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RL-QR introduces an annotation-free reinforcement learning framework
  for query rewriting in Retrieval-Augmented Generation systems. It synthesizes index-aligned
  queries and uses verifiable search rewards derived from NDCG scores, eliminating
  the need for human-annotated data.
---

# Annotation-Free Reinforcement Learning Query Rewriting via Verifiable Search Reward

## Quick Facts
- arXiv ID: 2507.23242
- Source URL: https://arxiv.org/abs/2507.23242
- Reference count: 9
- RL framework achieves up to 3.9× improvement on lexical retrievers and 3.5× on semantic retrievers for visual documents

## Executive Summary
RL-QR introduces an annotation-free reinforcement learning framework for query rewriting in Retrieval-Augmented Generation systems. It synthesizes index-aligned queries and uses verifiable search rewards derived from NDCG scores, eliminating the need for human-annotated data. The method trains query rewriters to adapt queries to the representation space of specific retrievers, enabling universal applicability across text and multi-modal unstructured documents. Experiments show substantial gains on MS MARCO v2.1 and MTEB VIDORE V2 benchmarks, with consistent 5%–10% improvements across datasets.

## Method Summary
RL-QR operates in two stages: first generating index-aligned synthetic queries using a large LLM to create document-requiring questions, then training a query rewriter via GRPO reinforcement learning with NDCG-based verifiable rewards. The framework eliminates human annotation requirements by procedurally generating training queries directly from target corpora, where synthesized queries are designed to require specific documents for answering. During training, the rewriter policy receives rewards based on the NDCG score of the target document's rank position after retrieval, combined with format penalties to encourage concise, search-intent-focused outputs. The approach trains one specialized rewriter per retriever/index pair rather than attempting universal generalization.

## Key Results
- 3.9× improvement on lexical retrievers and 3.5× on semantic retrievers for visual documents (MTEB VIDORE V2)
- Consistent 5%–10% improvements on MS MARCO v2.1 benchmark
- Effectively "unlearns" chat-oriented behaviors, focusing on retrieval intent rather than conversational responses

## Why This Works (Mechanism)

### Mechanism 1: Index-Aligned Ground Truth Generation
RL-QR eliminates human-labeled relevance data by procedurally generating training queries directly from the target corpus using an LLM to synthesize questions requiring specific documents. This guarantees positive pairs without manual annotation, though synthetic queries must approximate real user query distributions to avoid overfitting to artificial patterns.

### Mechanism 2: Verifiable Search Reward via NDCG
The framework optimizes directly for retrieval metrics by executing rewritten queries against the actual index and using NDCG scores as verifiable scalar rewards. This creates a tighter feedback loop than proxy reward models, though it assumes retriever stability during training and requires the reward signal to be differentiable or estimable via GRPO.

### Mechanism 3: Unlearning Conversational Priors
Standard LLMs misalign with retrieval tasks by treating queries as chat prompts; RL-QR suppresses this behavior through reward structure and penalties that favor keyword density and search intent over conversational fluency. The model learns to toggle between generative and retrieval modes without catastrophic forgetting.

## Foundational Learning

**Concept: GRPO (Group Relative Policy Optimization)**
- Why needed: Specific RL algorithm used that computes advantages based on group outputs rather than absolute value functions
- Quick check: How does computing advantages based on a group of rollouts differ from using a dedicated value network?

**Concept: NDCG (Normalized Discounted Cumulative Gain)**
- Why needed: Objective function that heavily penalizes retrieving correct documents at low ranks
- Quick check: Why is NDCG preferred over simple Recall for training a rewriter in RAG context?

**Concept: Index Representation Spaces (Lexical vs. Semantic)**
- Why needed: Rewriter adapts queries to retriever-specific representation spaces
- Quick check: What structural change would you expect when moving a query from semantic to lexical retriever?

## Architecture Onboarding

**Component map:** Synthesizer (LLM + Prompt) -> Policy (Rewriter) -> Environment (Retriever + Index) -> Reward Module (NDCG + Penalty)

**Critical path:**
1. Data Gen: Offline generation of index-aligned queries
2. Rollout: Policy rewrites query q → q'
3. Search: Retriever retrieves top-k docs for q'
4. Score: Check if target doc in top-k; compute NDCG
5. Update: GRPO step using reward

**Design tradeoffs:**
- Per-index specialization maximizes performance but increases maintenance overhead
- Retrieval overhead is the training bottleneck

**Failure signatures:**
- Verbose Regression: Model outputs long explanations (increase λ₂ penalty)
- Keyword Stuffing: Nonsensical term repetition (may need semantic consistency check)

**First 3 experiments:**
1. Baseline Alignment: Raw Query vs. Vanilla LLM vs. RL-QR on hold-out set
2. Ablation on Rewards: Remove penalty to observe verbose query generation
3. Cross-Retriever Generalization: Train on Lexical, test on Semantic (and vice versa)

## Open Questions the Paper Calls Out
- Extending framework to multi-turn conversational retrieval contexts
- Integrating downstream generation quality as a reward signal
- Generalizing a single rewriter across diverse retriever architectures without specific fine-tuning

## Limitations
- Hyperparameter sensitivity with undisclosed λ₁, λ₂, and GRPO rollout values
- Generalizability constraints limited to specific retriever types (BM25, ColBERT-v2, CoCa-CLIP)
- Synthetic query quality dependence on LLM-generated queries accurately representing real user behavior

## Confidence
- **High Confidence**: Annotation elimination methodology, verifiable search reward mechanics, unlearning conversational priors
- **Medium Confidence**: Performance improvement claims (3.9×, 3.5×) depending on undisclosed hyperparameters
- **Low Confidence**: "Universal" applicability claims across all unstructured document types

## Next Checks
1. Ablation Study on Hyperparameters: Systematically vary λ₁, λ₂, and rollout counts to establish sensitivity
2. Cross-Retriever Generalization Test: Train on one retriever type, evaluate zero-shot on structurally different retrievers
3. Real vs. Synthetic Query Distribution Analysis: Compare LLM-synthesized queries against actual user query logs from production systems