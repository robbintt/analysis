---
ver: rpa2
title: 'NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between Lexical
  Systems and Language Use'
arxiv_id: '2509.22479'
source_url: https://arxiv.org/abs/2509.22479
tags:
- context
- agents
- color
- training
- informativeness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NeLLCom-Lex is a neural-agent framework that simulates lexical
  semantic change by grounding agents in a real language and systematically manipulating
  their communicative needs. Using a color naming task, agents first learn human-like
  color labels through supervised learning, then adapt their naming behavior through
  reinforcement learning in context-sensitive communication games.
---

# NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between Lexical Systems and Language Use

## Quick Facts
- arXiv ID: 2509.22479
- Source URL: https://arxiv.org/abs/2509.22479
- Reference count: 16
- Primary result: Neural agents trained with context access develop pragmatic naming behavior matching human color lexicon properties

## Executive Summary
NeLLCom-Lex is a neural-agent framework that simulates lexical semantic change by grounding agents in a real language and systematically manipulating their communicative needs. Using a color naming task, agents first learn human-like color labels through supervised learning, then adapt their naming behavior through reinforcement learning in context-sensitive communication games. Experiments show that agents trained with context access develop pragmatic naming behavior, using more informative words in harder contexts, and achieve human-like system-level informativeness (IL=2.6 vs. 2.78 for humans) and lexical diversity (34.5 word types vs. 49 for humans). When exposed to different communicative needs during training, agents adapt their lexicons accordingly, with those trained on difficult contexts showing stronger pragmatic adaptation.

## Method Summary
The framework implements a two-phase pipeline where neural agents learn to name colors in referential communication games. In the supervised learning phase, agents learn to predict human color labels given target colors and distractors. In the reinforcement learning phase, agents jointly optimize speaker and listener policies using REINFORCE with rewards based on communication success. The speaker encodes three colors (target + two distractors) using parallel feedforward networks, concatenates representations, and produces a single word label. The listener encodes three colors (shuffled) and the word embedding to identify the target. Context manipulation is achieved by zeroing distractor embeddings to create context-unaware conditions. The CIELAB color space defines context difficulty through perceptual distances (far/split/close), with context-aware training enabling pragmatic word selection based on discriminability.

## Key Results
- Context-aware agents develop pragmatic naming behavior (β(E_ctx) significantly negative) while context-unaware agents show no pragmatic adaptation
- Sequential SL+RL training produces human-like lexicon properties (IL=2.60, |W|=34.5) vs. SL alone (IL=3.23, |W|=12.7)
- Agents trained on difficult contexts show stronger pragmatic sensitivity (β(E_ctx)=−0.005 for AllClose vs. −0.003 for AllFar)

## Why This Works (Mechanism)

### Mechanism 1: Context Exposure Enables Pragmatic Word Selection
- **Claim:** Agents trained with access to distractor context develop pragmatic naming behavior, using more informative words in harder contexts.
- **Mechanism:** When speakers encode distractors alongside targets, the feedforward network learns to modulate word choice based on contextual discriminability; reinforcement learning then refines this through communicative success feedback.
- **Core assumption:** The learned mapping generalizes from training context distributions to novel test contexts.
- **Evidence anchors:** β(E_ctx) is significantly negative only in context-aware conditions (SL+RL+: −0.002, p<0.001); context-unaware conditions show β≈0.

### Mechanism 2: Two-Phase Learning Produces Human-Like Lexicon Properties
- **Claim:** Sequential supervised learning followed by reinforcement learning yields lexicons with mid-level informativeness and higher lexical diversity, closer to human patterns.
- **Mechanism:** SL anchors agents to existing human lexical conventions; RL then optimizes for communication success, encouraging richer vocabulary use to handle varied contexts efficiently.
- **Core assumption:** The reward signal (listener prediction accuracy) is sufficient to shape system-level lexical properties without explicit regularization for diversity.
- **Evidence anchors:** SL+RL+ yields IL=2.60 and |W|=34.5 (closest to human IL=2.78, |W|=49); SL alone yields IL=3.23, |W|=12.7.

### Mechanism 3: Difficult Context Exposure Strengthens Pragmatic Sensitivity
- **Claim:** Agents trained predominantly on close (difficult) contexts develop stronger context-dependent word informativeness modulation.
- **Mechanism:** Frequent exposure to fine-grained discrimination tasks forces agents to learn more nuanced mappings between context difficulty and word specificity.
- **Core assumption:** The training context distribution directly shapes the strength of pragmatic adaptation, not just vocabulary size.
- **Evidence anchors:** AllClose condition shows β(E_ctx)=−0.005; AllFar shows β(E_ctx)=−0.003 (both p<.001, but AllClose steeper).

## Foundational Learning

- **Referential Communication Games**
  - Why needed here: The entire framework builds on dyadic referential games where a speaker conveys a target among distractors to a listener.
  - Quick check question: Can you explain why communication accuracy depends on both speaker word choice and listener interpretation?

- **CIELAB Color Space**
  - Why needed here: Color inputs are represented as 3D perceptual vectors; distances in this space define context difficulty (far/split/close).
  - Quick check question: Why is a perceptually uniform color space necessary for defining context ease?

- **Pragmatic Adaptation**
  - Why needed here: The core hypothesis is that humans and agents modulate word informativeness based on contextual discriminability.
  - Quick check question: What would "green" vs. "sage" choice tell you about the referential context?

## Architecture Onboarding

- **Component map:**
  - Speaker: 3 parallel FNN encoders (target + 2 distractors) → concat → FNN → linear classifier → softmax over vocabulary
  - Listener: 3 parallel FNN encoders (shuffled colors) + word embedding → dot product similarity → softmax over 3 positions
  - Context manipulation: Zeroed distractor embeddings = context-unaware; normal encoding = context-aware

- **Critical path:**
  1. SL phase: Train speaker to predict human labels given color triplets; train listener to identify target given label and shuffled triplet
  2. RL phase: Joint optimization via REINFORCE with reward = log p(correct position | speaker's word)
  3. Evaluation: Measure accuracy, pragmatic adaptation (β of Iw ~ Ectx), system-level informativeness (IL), lexical diversity (|W|)

- **Design tradeoffs:**
  - Context-aware SL improves human label prediction (79% vs. 75%) but alone yields poor lexicon (|W|=12.7)
  - RL increases lexical diversity and pragmatic behavior but introduces semantic drift (DL increases)
  - Single-symbol utterances simplify analysis but limit expressivity compared to multi-word RSA models

- **Failure signatures:**
  - Context-unaware training: β(E_ctx) ≈ 0, no pragmatic adaptation, overly informative lexicon (IL>5)
  - SL-only: Low lexical diversity (<15 word types) despite pragmatic behavior
  - AllFar RL training: Reduced pragmatic sensitivity, near-uniform lexicon across context types

- **First 3 experiments:**
  1. Replicate SL+RL+ vs. SL−RL− comparison on TEST_gen,distH to confirm context access effect on β(E_ctx) and IL
  2. Ablate RL duration (epoch 0 vs. 30) to measure lexicon enrichment trajectory
  3. Vary training context distribution (AllClose/AllFar) and evaluate on balanced test set to confirm pragmatic sensitivity modulation

## Open Questions the Paper Calls Out
1. How does varying the entropy regularization parameter during RL training influence agents' pragmatic naming behavior and the shaping of their emergent lexicons?
2. Can NeLLCom-Lex successfully simulate specific instances of lexical semantic change such as meaning narrowing and broadening?
3. How do findings generalize beyond the constrained color triplet domain to more complex semantic domains with richer contextual variability?

## Limitations
- The framework is validated only on a specific color naming task in CIELAB space, limiting generalizability to more complex semantic domains.
- The core mechanism of context-aware pragmatic adaptation is primarily supported within this framework, with limited external validation against prior work.
- While human data provides benchmarks, the comparison is limited to system-level metrics without validating individual naming behavior patterns.

## Confidence
- **High**: Context-aware agents develop pragmatic naming behavior (β(E_ctx) significantly negative) and SL+RL+ yields human-like lexicon properties
- **Medium**: Difficult context exposure strengthens pragmatic sensitivity (novel finding without external validation)
- **Low**: Broader claim that neural agents can "study language evolution mechanisms" remains speculative

## Next Checks
1. Test the framework on a different semantic domain (e.g., shape or texture naming) to verify if context-aware pragmatic adaptation generalizes beyond color
2. Conduct a controlled experiment comparing individual agent naming patterns to human speakers in the same contexts to validate beyond system-level metrics
3. Systematically vary speaker architecture (e.g., add attention, use transformers) and listener architecture to test robustness of pragmatic adaptation to design choices