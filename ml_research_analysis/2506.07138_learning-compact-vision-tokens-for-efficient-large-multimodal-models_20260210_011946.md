---
ver: rpa2
title: Learning Compact Vision Tokens for Efficient Large Multimodal Models
arxiv_id: '2506.07138'
source_url: https://arxiv.org/abs/2506.07138
tags:
- vision
- tokens
- token
- llav
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational challenges of large multimodal
  models (LMMs) caused by high-cost large language models (LLMs) and the quadratic
  complexity of processing long vision token sequences. The authors propose a Spatial
  Token Fusion (STF) method that fuses spatial-adjacent vision tokens into compact
  representations, reducing the vision token sequence length while preserving information.
---

# Learning Compact Vision Tokens for Efficient Large Multimodal Models

## Quick Facts
- arXiv ID: 2506.07138
- Source URL: https://arxiv.org/abs/2506.07138
- Authors: Hao Tang; Chengchao Shen
- Reference count: 40
- Vision token sequence length reduced to 25% while maintaining performance

## Executive Summary
This paper addresses the computational challenges of large multimodal models (LMMs) caused by high-cost large language models (LLMs) and the quadratic complexity of processing long vision token sequences. The authors propose a Spatial Token Fusion (STF) method that fuses spatial-adjacent vision tokens into compact representations, reducing the vision token sequence length while preserving information. Additionally, they introduce a Multi-Block Token Fusion (MBTF) module to incorporate multi-granularity features from different layers of the vision encoder. The combined approach significantly improves inference efficiency without sacrificing multimodal reasoning capabilities. Experimental results demonstrate that their method based on LLaVA-1.5 achieves comparable or even superior performance to the baseline on 8 popular vision-language benchmarks while using only 25% of the vision tokens.

## Method Summary
The authors propose two complementary token fusion modules to reduce vision token sequence length in LMMs. The Multi-Block Token Fusion (MBTF) extracts features from 8 evenly-spaced blocks of the ViT-L/14 vision encoder, concatenates them along channels, and applies 1×1 convolutions to produce a unified representation. The Spatial Token Fusion (STF) then applies 2×2 convolutions with stride 2 to spatially fuse adjacent tokens, followed by additional 1×1 convolutions. The method is trained in two stages: pretraining with CC-595K dataset to learn token fusion patterns, then finetuning on LLaVA-Instruct-158K with the full LLM. The vision encoder remains frozen throughout training, while only the fusion modules and LLM parameters are updated.

## Key Results
- Achieves comparable or superior performance to LLaVA-1.5 baseline on 8 vision-language benchmarks
- Reduces vision token sequence length to 25% of original
- Maintains performance on challenging benchmarks like TextVQA and VisWiz
- Demonstrates significant TFLOPs reduction while preserving multimodal reasoning capabilities

## Why This Works (Mechanism)
The method works by addressing two key bottlenecks in LMMs: the quadratic complexity of self-attention on long token sequences and the computational cost of processing high-resolution vision tokens. MBTF captures multi-scale visual information by combining features from different encoder layers, providing richer representations than single-layer extraction. STF reduces spatial redundancy by fusing adjacent tokens while preserving essential visual information through learned convolution operations. The two-stage training allows the model to first learn effective token fusion patterns on large-scale pretraining data before fine-tuning for instruction following.

## Foundational Learning
- **Vision Transformer architecture**: Understanding ViT block structure and feature extraction at different layers. Needed to implement MBTF module that extracts features from specific blocks. Quick check: Verify feature shapes match expected {3,6,9,12,15,18,21,24} block outputs.
- **Token fusion operations**: Spatial and channel-wise fusion through convolution operations. Needed to implement both MBTF and STF modules. Quick check: Confirm STF reduces 24×24 tokens to 12×12 with correct channel dimensions.
- **Two-stage training**: Separate pretraining and finetuning phases with different learning rates and parameter updates. Needed to reproduce the training procedure. Quick check: Verify only fusion modules update during pretraining, full model during finetuning.
- **Multimodal reasoning**: How vision tokens integrate with LLM through cross-attention. Needed to understand performance implications. Quick check: Ensure token reduction doesn't break cross-attention compatibility.
- **Evaluation benchmarks**: GQA, ScienceQA, VQAv2, VisWiz, TextVQA, POPE, MMBench, MMBench-CN. Needed to validate results. Quick check: Run inference on at least 3 benchmarks to verify baseline comparability.

## Architecture Onboarding

**Component Map:** Input image → ViT-L/14 → MBTF (8 block features) → STF (spatial fusion) → LLM (cross-attention) → Output

**Critical Path:** Image → Vision Encoder → MBTF → STF → LLM → Response
The vision encoder processes the image and is frozen throughout training. MBTF extracts multi-scale features from specified blocks, concatenates them, and reduces channels. STF spatially fuses tokens to reduce sequence length. The LLM processes the fused tokens through cross-attention.

**Design Tradeoffs:** The authors chose 8 evenly-spaced blocks for MBTF to balance computational cost with multi-scale representation. STF uses 2×2 kernels to reduce tokens by 4× while maintaining spatial relationships. The two-stage training trades off some efficiency for better fusion learning.

**Failure Signatures:** Performance degradation on TextVQA and VisWiz indicates loss of fine-grained visual details. Shape mismatches in STF suggest incorrect implementation of spatial reduction. Significant performance drops on any benchmark suggest the fusion modules are not preserving critical information.

**3 First Experiments:**
1. Verify STF spatial reduction: Input 336×336 image produces 24×24 tokens, STF with k=2 produces 12×12 tokens with correct channel dimensions.
2. Test MBTF feature extraction: Extract features from blocks {3,6,9,12,15,18,21,24}, verify shapes and channel concatenation.
3. Run single benchmark: Evaluate on VQAv2 to confirm basic functionality before full evaluation suite.

## Open Questions the Paper Calls Out
None

## Limitations
- Initialization method for new convolution layers in MBTF/STF modules is unspecified
- Computational cost measurements may not reflect real-world inference efficiency due to hardware-specific assumptions
- Method validated only on LLaVA-1.5 with ViT-L/14, limiting generalizability to other LMM architectures

## Confidence
**High confidence** in core technical contribution - spatial token fusion approach and architectural details are clearly specified with sufficient reproducibility.

**Medium confidence** in quantitative results - benchmark performance claims are plausible given established LLaVA-1.5 baseline, but independent validation would strengthen claims.

**Low confidence** in generalizability - method tested only on specific LLaVA-1.5 configuration without validation on alternative architectures or vision backbones.

## Next Checks
1. **Initialization sensitivity analysis**: Reproduce with different initialization schemes (Kaiming vs. Xavier) for MBTF/STF layers and measure performance variance across 8 benchmarks.
2. **Cross-architecture generalization**: Apply MBTF+STF to SigLIP-based LMM variant to test if 25% token reduction and performance preservation generalize.
3. **Computational cost validation**: Measure actual wall-clock inference time and memory usage on 8× RTX A6000 for baseline vs. proposed method across varying batch sizes.