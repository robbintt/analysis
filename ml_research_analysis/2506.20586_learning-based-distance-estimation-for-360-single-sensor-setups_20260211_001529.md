---
ver: rpa2
title: "Learning-Based Distance Estimation for 360\xB0 Single-Sensor Setups"
arxiv_id: '2506.20586'
source_url: https://arxiv.org/abs/2506.20586
tags:
- distance
- estimation
- object
- depth
- camera
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a learning-based method for monocular distance\
  \ estimation using a single 360\xB0 fisheye camera. The approach integrates a distance\
  \ estimation head into the YOLOv7 object detection framework, enabling joint object\
  \ detection and distance prediction without requiring explicit geometric calibration."
---

# Learning-Based Distance Estimation for 360° Single-Sensor Setups

## Quick Facts
- arXiv ID: 2506.20586
- Source URL: https://arxiv.org/abs/2506.20586
- Authors: Yitong Quan; Benjamin Kiefer; Martin Messmer; Andreas Zell
- Reference count: 19
- Primary result: Learning-based monocular distance estimation method that significantly outperforms geometric baselines and other learning approaches on 360° fisheye data

## Executive Summary
This paper presents a novel learning-based approach for monocular distance estimation using a single 360° fisheye camera. The method extends the YOLOv7 object detection framework by adding a dedicated distance regression head, enabling joint object detection and distance prediction without requiring explicit geometric calibration. The approach achieves real-time performance at 45 FPS while demonstrating superior accuracy compared to both geometric baselines and other learning-based methods across three diverse datasets. The model shows particular robustness to camera pose variations and dynamic environments, making it suitable for resource-constrained robotics and surveillance applications.

## Method Summary
The method modifies the YOLOv7 architecture by adding a distance estimation head that outputs one normalized distance value per anchor box. The model is trained end-to-end with a custom multi-task loss that balances object detection (classification and bounding box regression) with distance estimation. The approach uses linear normalization of distance values to the [0,1] range and applies fisheye-specific data augmentation while disabling geometric transforms that would corrupt the distortion profile. The distance targets are computed as linearly normalized values based on the maximum distance in the dataset, with a loss weight of λ_dist = 1/3.

## Key Results
- Significantly outperforms geometric baselines and other learning-based approaches in absolute and confidence-weighted distance errors
- Achieves real-time performance at 45 FPS on NVIDIA Orin AGX
- Demonstrates robustness to camera pose variations and dynamic environments
- Shows performance degradation at long ranges (>100m) and in high-dynamic conditions

## Why This Works (Mechanism)

### Mechanism 1: Implicit Distortion Mapping via End-to-End Regression
The model bypasses explicit geometric calibration by learning a direct mapping from raw fisheye pixel intensities to metric distance. By adding a dedicated distance regression head to YOLOv7, the network simultaneously learns object boundaries and non-linear distortion characteristics of the 360° lens. The convolutional layers encode varying spatial resolution and stretching inherent to fisheye projection, mapping these features to scalar distance values without intermediate ray-tracing steps. This works because training data sufficiently covers the spatial distortion field, allowing interpolation for unseen objects at similar pixel locations. Performance degrades significantly if the camera lens is replaced or housing shifts relative to sensor without retraining.

### Mechanism 2: Robustness to Dynamic Extrinsic Variations
The learning-based approach maintains estimation accuracy under camera pose changes (e.g., pitch/roll) that would invalidate geometric trigonometric methods. While geometric methods rely on fixed extrinsic parameters (camera height h and tilt angle θ), the network learns statistical correlations between visual features and distance. In dynamic environments, the network relies on features invariant to minor pose shifts or implicitly corrects for them based on scene context rather than rigid triangulation. The dataset must include enough variance in camera pose for the model to learn robust features rather than overfitting to static conditions.

### Mechanism 3: Object-Level Distance Concentration
Regressing a single distance value per detected object (per anchor) is more efficient and robust than generating dense pixel-wise depth maps. General depth models predict depth for every pixel, which is computationally expensive and susceptible to errors in textureless regions. By tying distance prediction directly to object detection anchors, the model optimizes specifically for the object's location, averaging or attending to the most relevant features within the bounding box while discarding background noise. This assumes correct object detection and that distance is a meaningful singular value for that object.

## Foundational Learning

- **Concept: Fisheye Projection Geometry**
  - Why needed here: To understand why standard perspective models fail and why raw input preserves spatial relationships that equirectangular projection might distort
  - Quick check question: Does the model learn distance from the angle of incident ray (implied by pixel radius) or from scene context features?

- **Concept: Multi-Task Loss Balancing**
  - Why needed here: The system optimizes for classification, bounding box regression, and distance simultaneously; understanding trade-offs (λ weights) is crucial for training stability
  - Quick check question: If distance loss dominates, how might object detection precision (mAP) be affected?

- **Concept: Logarithmic vs. Linear Normalization**
  - Why needed here: Distance values vary by orders of magnitude (0.5m to 300m); normalizing to neural network output range (0-1) is required for stable gradient descent
  - Quick check question: Why might logarithmic normalization be preferred if error tolerance is relative (percentage) rather than absolute (meters)?

## Architecture Onboarding

- **Component map:** Image Input → YOLOv7 Backbone → Feature Aggregation (PANet) → Multi-Head Output (Objectness/Class, Bounding Box, Distance) → Denormalization → Final Metric Distance
- **Critical path:** Image Input → YOLOv7 Backbone → Distance Head Output → Denormalization (Inverse Linear/Log transform) → Final Metric Distance
- **Design tradeoffs:**
  - Input Format: Raw fisheye preserves distortion cues for better distance estimation but yields lower detection mAP; equirectangular provides higher detection mAP but worse distance estimation
  - Speed vs. Accuracy: Lightweight at 45 FPS compared to foundation models at 10 FPS, but lacks generalization of large-scale pre-training
- **Failure signatures:** Long-range drift shows significant error increase for objects >100m; format mismatch training on equirectangular increases distance error ~2x compared to raw fisheye
- **First 3 experiments:**
  1. Implement principal point shift test (1-5 pixel) on validation set to verify model brittleness to calibration drift
  2. Compare Linear vs. Logarithmic normalization on high-variance dataset to determine if penalizing near-field errors improves safety-critical performance
  3. Train two instances (raw fisheye vs. equirectangular) to empirically determine deployment priority between detection accuracy and distance estimation

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating explicit geometric priors into deep learning architecture improve generalization compared to current purely data-driven approach? The conclusion states future work will explore hybrid models integrating geometric priors into deep learning architectures to improve generalization. This remains unresolved because the current method learns mappings directly from raw inputs without explicit geometric constraints, potentially limiting physical consistency in entirely unseen environments. Evidence would come from comparative study showing models with geometric constraints outperform Y7-D on cross-dataset evaluations or out-of-distribution scenes.

### Open Question 2
To what extent does training on larger datasets with highly varied dynamic scenes mitigate performance gap between steady and high-dynamic motion? The conclusion suggests expanding training datasets with more varied dynamic scenes will enhance model robustness for practical deployment. This is unresolved because BOAT360 results show performance drop (Abs. Err 18.59m → 32.16m) moving from steady to high-dynamic conditions, indicating current training data may not cover full variance of platform dynamics. Evidence would come from re-training on expanded dataset rich in acceleration and turning maneuvers, followed by evaluation showing reduced error divergence between steady and dynamic test sets.

### Open Question 3
How can architecture be modified to reduce significant performance degradation observed in long-range distance estimation (>100m)? Section V-B-2 notes absolute error grows with increasing distance bins on BOAT360 dataset, with errors reaching ~115m in 200-300m range. This remains unresolved because current feature extraction or normalization strategy (linear scaling) appears insufficient for precise estimation at extreme ranges. Evidence would come from introducing range-specific loss weights, multi-scale feature extraction, or logarithmic regression heads demonstrating statistically significant error reductions in 100m+ distance bins.

## Limitations
- Dataset availability: ULM360 and BOAT360 datasets are not publicly released, limiting full reproduction and validation
- Hyperparameter transparency: Exact optimizer configuration, learning rate schedule, and specific value of d_max for normalization are not fully specified
- Transfer robustness: Performance on unseen environments or camera configurations outside training distribution remains untested

## Confidence
- High confidence in core claim that learning-based approach outperforms geometric baselines and other learning methods on tested datasets
- Medium confidence in claimed robustness to camera pose variations (limited supporting evidence in corpus)
- Low confidence in generalization claims to radically different environments or camera setups without additional validation

## Next Checks
1. Obtain or reconstruct ULM360 and BOAT360 datasets to verify performance claims across all three reported domains
2. Evaluate trained model on independent 360° fisheye dataset to assess true generalization beyond reported test sets
3. Systematically vary camera height and mounting angle beyond training distributions to identify breaking points for claimed pose robustness