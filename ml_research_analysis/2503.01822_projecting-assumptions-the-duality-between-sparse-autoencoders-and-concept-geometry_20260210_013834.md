---
ver: rpa2
title: 'Projecting Assumptions: The Duality Between Sparse Autoencoders and Concept
  Geometry'
arxiv_id: '2503.01822'
source_url: https://arxiv.org/abs/2503.01822
tags:
- concepts
- saes
- data
- spade
- relu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Projecting Assumptions: The Duality Between Sparse Autoencoders and Concept Geometry

## Quick Facts
- **arXiv ID**: 2503.01822
- **Source URL**: https://arxiv.org/abs/2503.01822
- **Reference count**: 40
- **Primary result**: Sparse Autoencoder architectures impose structural assumptions about concept geometry that determine which concepts they can successfully recover monosemantically

## Executive Summary
This paper establishes a formal duality between Sparse Autoencoder (SAE) architectures and the geometric structure of concepts in neural network representations. The authors show that different SAE encoders (ReLU, JumpReLU, TopK, SpaDE) impose distinct structural constraints on the sparse dictionary learning problem through their projection nonlinearities, which in turn shape which concepts can be discovered. This framework unifies sparse dictionary learning theory with SAE interpretability research, revealing that architecture design choices fundamentally determine what concepts can be recovered monosemantically.

## Method Summary
The authors formalize SAEs as bilevel optimization problems where the inner optimization (encoder) constrains sparse codes to a specific projection set, and the outer optimization (dictionary learning) can only find solutions within this constrained space. They analyze four SAE architectures by their projection sets: ReLU projects onto the positive orthant (assuming linear separability), TopK projects onto K-sparse subspaces (assuming angular separability with uniform dimensionality), and SpaDE projects onto the probability simplex via Sparsemax (allowing adaptive sparsity). Experiments on synthetic datasets with controlled concept geometry demonstrate that each architecture succeeds or fails based on whether concept structure matches its implicit assumptions.

## Key Results
- ReLU and JumpReLU SAEs achieve F1 ≤ 0.5 on nonlinearly separable concepts due to half-space receptive fields
- TopK SAEs show high normalized MSE (>20%) when K < concept intrinsic dimensionality, failing on heterogeneous concepts
- SpaDE consistently achieves F1 = 1.0 across both linearly and nonlinearly separable concepts by adaptively matching sparsity to concept dimensionality
- The duality framework correctly predicts architecture performance based on concept geometry

## Why This Works (Mechanism)

### Mechanism 1: Bilevel Optimization Constrains Discoverable Concepts
SAE architectures impose structural constraints on the sparse dictionary learning problem through their encoder projection sets. The encoder solves an inner optimization that constrains sparse codes to a specific set S, and the outer optimization can only find solutions within this constrained space. Different projection sets yield different discoverable concept structures, requiring receptive fields to match concept structure in data for monosemantic recovery.

### Mechanism 2: Projection Nonlinearity Determines Receptive Field Geometry
The encoder nonlinearity defines receptive field shapes that create implicit assumptions about concept separability and dimensionality. ReLU/JumpReLU project onto the positive orthant, creating half-space receptive fields that assume linear separability. TopK projects onto K-sparse subspaces, creating hyperpyramid receptive fields that assume angular separability plus uniform dimensionality across concepts.

### Mechanism 3: Adaptive Sparsity via Probability Simplex Projection
SpaDE enables adaptive sparsity per input by projecting onto the probability simplex using Sparsemax. This allows different concepts to activate different numbers of latents, capturing heterogeneous concepts. The architecture implicitly assumes concepts are distance-separated in activation space and that Euclidean distances can meaningfully disentangle concepts.

## Foundational Learning

- **Sparse Dictionary Learning**: Core framework SAEs approximate—data = Dz with sparsity penalty on z. Why needed: Explains why overcomplete dictionaries require iterative solvers rather than pseudo-inverse due to non-convexity and sparsity constraints. Quick check: Can you explain why overcomplete dictionaries (s > d) require iterative solvers rather than pseudo-inverse?

- **Projection Nonlinearities**: Unifies ReLU, TopK, JumpReLU as orthogonal projections onto constraint sets. Why needed: Provides the mathematical foundation for understanding how different encoders constrain discoverable concepts. Quick check: What constraint set does ReLU project onto? What geometric shape is the receptive field?

- **Receptive Fields**: Formalizes the duality between encoder structure and concept geometry. Why needed: Connects SAE architecture to the geometric properties of concepts they can recover. Quick check: For a TopK SAE with K=3, why are receptive fields hyperpyramids through the origin?

## Architecture Onboarding

**Component map:**
Input x ∈ R^d → Encoder: z = g(W^T x + b) [g = projection nonlinearity] → Latent z ∈ R^s → Decoder: x̂ = Dz + b_d

**Critical path:**
1. Identify concept geometry in your data (linear vs. nonlinear separability; uniform vs. heterogeneous dimensionality)
2. Match SAE architecture to geometry via Table 2/3 (e.g., heterogeneous → avoid TopK; nonlinear → avoid ReLU/JumpReLU)
3. Train with appropriate regularizer (L1 for ReLU/JumpReLU, distance-weighted L1 for SpaDE)

**Design tradeoffs:**
- ReLU/JumpReLU: Simple, fast, handles heterogeneity; fails on nonlinearly separable concepts
- TopK: Fixed sparsity simplifies tuning; fails on heterogeneous concepts
- SpaDE: Captures both properties; higher compute (distance matrix); assumes distance-separation

**Failure signatures:**
- Low F1 scores on specific concepts → mismatch between SAE assumption and concept geometry
- High latent co-occurrence across concepts → receptive fields overlapping multiple concepts
- Dead latents → over-aggressive sparsity or poor initialization

**First 3 experiments:**
1. **Separability test**: Create 2D Gaussian clusters at different radii (Fig. 5). Verify ReLU/JumpReLU fail on small-radius (nonlinearly separable) clusters; SpaDE succeeds.
2. **Heterogeneity test**: Create 128D clusters with dimensions [6, 14, 30, 62, 126] (Fig. 6). Verify TopK's MSE spikes when K < dimension; SpaDE tracks y=x for sparsity vs. dimensionality.
3. **Real activation test**: Train SAEs on GPT-2 residual stream; compute per-concept F1 scores and latent co-occurrence matrices to identify which architecture matches your data's geometry.

## Open Questions the Paper Calls Out

### Open Question 1
What additional geometric properties of concepts—beyond nonlinear separability and heterogeneity—are important for SAE architecture design? The paper identifies only two properties; the full taxonomy of concept geometry remains unexplored. Future work may explore additional geometric structure of concepts in neural networks to design better SAEs.

### Open Question 2
How does SpaDE fail when concepts are not well-separated by Euclidean distance in representation space? The paper notes SpaDE "implicitly assumes that Euclidean distances are useful in concept space—concepts are distance-separated—and distances can be used to disentangle concepts," but no experiments test SpaDE on concepts with non-Euclidean separability.

### Open Question 3
How do the duality and receptive field analysis extend to overlapping or co-occurring concepts rather than mutually exclusive ones? The paper focuses on mutually exclusive concepts, noting that while arguments about SAE assumptions hold for overlapping concepts, the expected co-occurrence structure may differ in such cases.

## Limitations
- The duality framework is primarily validated on synthetic datasets with controlled concept geometry rather than real-world representations
- SpaDE requires distance-separated concepts, which may not hold in actual model representations where concepts overlap or are arranged in complex manifolds
- Limited direct evidence on receptive field geometry assumptions from the broader corpus; most neighboring work focuses on SAE applications rather than architectural constraints

## Confidence
- **High**: Bilevel optimization framework correctly models SAE architecture constraints
- **Medium**: Projection nonlinearity determines receptive field geometry and concept separability assumptions
- **Medium**: SpaDE successfully handles both nonlinear separability and heterogeneous dimensionality

## Next Checks
1. **Concept geometry analysis**: Before selecting SAE architecture, perform spectral clustering on your data to identify concept separability patterns and intrinsic dimensionality distributions
2. **Architectural stress test**: Create synthetic datasets mixing linear and nonlinearly separable concepts; verify each SAE architecture's F1 scores match theoretical predictions
3. **Real-world geometry mapping**: Apply the duality framework to analyze actual model representations (e.g., GPT-2 activations) to identify which geometric assumptions are violated and why certain concepts fail monosemantic recovery