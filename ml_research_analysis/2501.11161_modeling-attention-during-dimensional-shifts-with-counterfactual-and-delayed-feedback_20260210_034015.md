---
ver: rpa2
title: Modeling Attention during Dimensional Shifts with Counterfactual and Delayed
  Feedback
arxiv_id: '2501.11161'
source_url: https://arxiv.org/abs/2501.11161
tags:
- feedback
- learning
- shift
- human
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares two methods for modeling human attention during
  dimensional shifts in contextual bandit tasks with immediate, delayed, and counterfactual
  feedback. One method uses reward prediction errors (RPE) to iteratively update attention
  weights, while the other calculates mutual information over a memory of past experiences.
---

# Modeling Attention during Dimensional Shifts with Counterfactual and Delayed Feedback

## Quick Facts
- arXiv ID: 2501.11161
- Source URL: https://arxiv.org/abs/2501.11161
- Reference count: 10
- Compares mutual information-based attention modeling with reward prediction error approaches in contextual bandit tasks

## Executive Summary
This study investigates how humans allocate attention during dimensional shifts in decision-making tasks by comparing two computational modeling approaches. The research focuses on contextual bandit tasks where participants must adapt to changing reward structures, either staying within the same dimension (intra-dimensional shifts) or switching to new dimensions (extra-dimensional shifts). The study evaluates both immediate, delayed, and counterfactual feedback conditions to understand how different information availability affects attention allocation.

The core contribution lies in demonstrating that information-theoretic approaches based on mutual information better capture human-like attention patterns compared to traditional reward prediction error methods. This finding has implications for understanding human decision-making processes and developing more accurate computational models of cognitive attention mechanisms.

## Method Summary
The study employs computational modeling to simulate human attention during dimensional shifts in contextual bandit tasks. Two modeling approaches are compared: one using reward prediction errors (RPE) to update attention weights iteratively, and another using mutual information calculated over a memory of past experiences (WIBL method). The models are tested across three feedback conditions - immediate, delayed, and counterfactual - to examine how different information availability affects performance. Simulations measure initial performance after dimensional shifts and asymptotic learning rates to capture both short-term and long-term adaptation patterns.

## Key Results
- WIBL method shows worse initial performance after extra-dimensional shifts but superior asymptotic performance compared to RPE-based approaches
- WIBL demonstrates more pronounced differences between intra-dimensional and extra-dimensional shifts under delayed and counterfactual feedback conditions
- Information-theoretic metrics (mutual information) better predict human attention patterns than reward prediction errors in dimensional shift tasks

## Why This Works (Mechanism)
The mutual information approach captures how attention should optimally shift based on the statistical relationship between observed features and outcomes, rather than relying solely on prediction errors. This allows the model to maintain more stable attention weights during transitions, leading to more human-like adaptation patterns where initial performance suffers but long-term learning improves.

## Foundational Learning
1. **Contextual Bandit Tasks** - Decision-making frameworks where actions yield rewards based on context; needed to create controlled environments for studying attention shifts; quick check: tasks have discrete contexts and actions with stochastic rewards
2. **Dimensional Shifts** - Changes in which task dimensions are relevant for determining rewards; needed to test attention flexibility; quick check: shifts can be intra-dimensional (same dimension) or extra-dimensional (new dimension)
3. **Reward Prediction Errors** - Differences between expected and actual rewards; traditional metric for updating decision values; quick check: calculated as reward minus predicted value
4. **Mutual Information** - Information-theoretic measure of dependence between variables; used to quantify relevance of features; quick check: calculated from joint and marginal probability distributions
5. **Counterfactual Feedback** - Information about outcomes of unchosen options; provides richer learning signals; quick check: reveals opportunity costs of decisions
6. **Delayed Feedback** - Time lag between action and outcome information; tests memory and credit assignment; quick check: introduces temporal dependency in learning

## Architecture Onboarding

**Component Map:**
Memory Buffer -> Mutual Information Calculator -> Attention Weight Updater -> Decision Policy -> Action Selection

**Critical Path:**
Feature observations are stored in memory buffer, mutual information is calculated between features and outcomes, attention weights are updated based on this information, which then influences the decision policy for action selection.

**Design Tradeoffs:**
- WIBL trades immediate performance for better long-term adaptation by maintaining more stable attention weights
- Information-theoretic approach requires more computational resources but captures richer statistical relationships
- Memory-based methods can handle delayed and counterfactual feedback better than purely online RPE methods

**Failure Signatures:**
- If memory buffer is too small, mutual information estimates become unreliable
- Excessive focus on mutual information may miss important local prediction error signals
- Delayed feedback conditions may cause attention to lag behind optimal shifts

**First 3 Experiments:**
1. Compare initial performance drops after extra-dimensional shifts between WIBL and RPE models
2. Measure asymptotic performance differences across all feedback conditions
3. Test sensitivity to memory buffer size and decay parameters

## Open Questions the Paper Calls Out
None

## Limitations
- Findings are entirely simulation-based without human behavioral validation
- Claims about human-like behavior have medium confidence without empirical testing
- Limited generalizability beyond dimensional shift tasks in contextual bandits

## Confidence
- WIBL better captures human-like behavior: Medium confidence (no human validation)
- Information-theoretic metrics superior to RPEs: Medium confidence (theoretical but unproven)
- WIBL shows more pronounced ID/ED differences: Medium-High confidence (simulation supported)

## Next Checks
1. Conduct human behavioral experiments using the same task paradigms to compare actual human performance against both WIBL and RPE model predictions
2. Test the models across diverse decision-making tasks beyond dimensional shift contexts to assess generalizability
3. Implement ablation studies removing key components of the WIBL method to identify which aspects drive its superior performance