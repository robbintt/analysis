---
ver: rpa2
title: 'VL-RouterBench: A Benchmark for Vision-Language Model Routing'
arxiv_id: '2512.23562'
source_url: https://arxiv.org/abs/2512.23562
tags:
- cost
- routing
- router
- accuracy
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VL-RouterBench is the first systematic, reproducible benchmark
  for routing vision-language models (VLMs). It constructs quality and cost matrices
  from inference logs across 14 datasets, 30,540 samples, and 17 models, yielding
  519,180 sample-model pairs.
---

# VL-RouterBench: A Benchmark for Vision-Language Model Routing

## Quick Facts
- arXiv ID: 2512.23562
- Source URL: https://arxiv.org/abs/2512.23562
- Reference count: 40
- Primary result: First systematic benchmark for routing vision-language models (VLMs)

## Executive Summary
VL-RouterBench introduces the first systematic, reproducible benchmark for routing vision-language models. The benchmark constructs quality and cost matrices from inference logs across 14 datasets, 30,540 samples, and 17 models, yielding 519,180 sample-model pairs. The evaluation protocol jointly measures average accuracy, average cost, and throughput, and introduces a ranking score based on the harmonic mean of normalized cost and accuracy to enable comparison across configurations. Experiments on 10 routing methods show that learned routers significantly outperform any single model at comparable or lower cost, while still lagging behind the ideal Oracle, indicating substantial room for improvement in router architecture. The benchmark is open-sourced to promote reproducibility and practical deployment in multimodal routing research.

## Method Summary
VL-RouterBench creates quality and cost matrices from inference logs by evaluating 17 VLMs on 30,540 samples across 14 datasets. The quality matrix Y stores binary correctness values while the cost matrix C stores inference costs for each sample-model pair. The benchmark evaluates routing methods by computing average accuracy, average cost, and throughput metrics. A ranking score based on the harmonic mean of normalized cost and accuracy enables fair comparison across different model configurations. Router training uses soft labels derived from Lagrangian relaxation of multi-objective optimization, allowing controllable trade-offs between accuracy and cost through the λ parameter.

## Key Results
- Learned routers significantly outperform any single model at comparable or lower cost
- Learned routers still lag behind the ideal Oracle, indicating room for improvement
- Soft label training with accuracy-cost-aware weighting improves routing decisions
- Normalize-Concat fusion method outperforms other fusion approaches in accuracy-cost trade-off

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Routing between multiple VLMs can achieve better accuracy-cost trade-offs than any single VLM.
- Mechanism: A learned router maps each multimodal query (image + text) to a probability distribution over candidate VLMs, selecting the model that balances correctness probability and inference cost for that specific input.
- Core assumption: Different VLMs have heterogeneous strengths across different query types and difficulty levels, creating exploitable routing opportunities.
- Evidence anchors:
  - [abstract] "learned routers significantly outperform any single model at comparable or lower cost"
  - [section 5.1] "The large Oracle–single-model performance gap indicates that routing potentially can substantially improve VLM cost-effectiveness"
  - [corpus] ECVL-ROUTER paper confirms scenario-aware routing benefits for VLMs with varying requirements
- Break condition: If candidate VLMs have near-identical performance-cost profiles across all query types, routing gains diminish toward zero.

### Mechanism 2
- Claim: Accuracy-cost-aware soft labels enable controllable trade-offs during router training.
- Mechanism: The training objective derives from Lagrangian relaxation of multi-objective optimization, producing soft labels t_i^(λ)(j) that concentrate probability on correct models weighted by exp(-λ·C_i,j), where λ controls cost sensitivity.
- Core assumption: The optimization landscape admits smooth solutions where probability mass can be meaningfully distributed among multiple correct models.
- Evidence anchors:
  - [section 3.3] Equations 4-5 define the soft label strategy with mathematical derivation in Appendix C
  - [section 4.3] "We consider λ={0,10,100,1000,10000,+∞} in the experiments to control the trade-off"
  - [corpus] No direct corpus evidence for this specific mechanism; related routing work (RouterDC, ZOOTER) uses different training strategies
- Break condition: If most samples have only one correct model (sparse quality matrix), soft labels degenerate toward hard labels.

### Mechanism 3
- Claim: Simple multimodal fusion (normalized concatenation of frozen text + visual embeddings) provides sufficient discriminative signal for routing.
- Mechanism: Feature-level routers extract embeddings from frozen encoders (e.g., BGE-M3 for text, SigLIP-L-16 for images), normalize and concatenate them, then train a lightweight classifier (MLP) to predict model selection.
- Core assumption: Routing decisions require coarse semantic discrimination rather than fine-grained multimodal reasoning.
- Evidence anchors:
  - [section 5.3] "Normalize-Concat yields the best accuracy and cost trade-off and is adopted as the default fusion method"
  - [Table 3] Normalize-Concat achieves 74.05 Rank Score vs 70.97 (Only-Text) and 71.82 (Only-Image)
  - [corpus] No corpus evidence specifically validating this fusion approach for routing
- Break condition: If routing requires understanding complex cross-modal relationships (e.g., spatial reasoning, document layout), simple concatenation may underperform end-to-end multimodal encoders.

## Foundational Learning

- **Multi-objective optimization with Lagrangian relaxation**
  - Why needed here: The routing objective jointly minimizes performance risk (incorrect predictions) and expected cost, requiring understanding how λ trades off these competing goals.
  - Quick check question: If λ=0, what does the router optimize? If λ→∞? (Answers: accuracy only; cost only among correct models)

- **Vision-language model heterogeneity**
  - Why needed here: Router effectiveness depends on candidate VLMs having meaningfully different accuracy-cost profiles across task types (General, STEM, Charts/OCR).
  - Quick check question: Why would two VLMs with identical accuracy on all samples but different costs still enable routing gains? (Answer: Router can preferentially select cheaper model when both are correct)

- **Soft label training with KL divergence**
  - Why needed here: Router training uses soft targets derived from quality-cost matrices rather than hard model selections, requiring understanding of distribution matching.
  - Quick check question: How does the soft label cross-entropy loss (Eq. 6-7) differ from standard classification loss? (Answer: Targets are distributions over correct+cheap models, not single class indices)

## Architecture Onboarding

- **Component map:** VLMEvalKit logs → quality matrix Y + cost matrix C → router training (encoders → fusion → classifier) → probability distribution over models

- **Critical path:**
  1. Ensure quality matrix has sufficient routable samples (multiple correct models per sample)
  2. Select λ based on deployment cost budget (higher λ = cheaper but potentially less accurate)
  3. Choose router architecture: feature-level (faster, modular) vs end-to-end (better accuracy-cost trade-off per Table 2)

- **Design tradeoffs:**
  - Feature-level vs End-to-end: Feature-level has higher throughput (Table 2: Linear 150.74 vs RouterDC 6.31 K tokens/s) but end-to-end achieves better Rank Scores
  - Encoder dimensionality: Higher-dimensional embeddings (BGE-M3 + SigLIP-L-16) improve routing but increase feature extraction latency
  - Fusion complexity: Normalize-Concat outperforms learned fusion (GMU, MLB) in Rank Score, suggesting simplicity is preferable

- **Failure signatures:**
  - Router always selects same model: Quality matrix may have one dominant model, or λ too extreme
  - Accuracy drops sharply as cost decreases: Candidate pool lacks cost-efficient correct alternatives
  - Large gap to Oracle but good single-model accuracy: Router architecture not capturing discriminative features; try different encoders or fusion

- **First 3 experiments:**
  1. Reproduce baseline comparison: Train MLP router with BGE-M3 + SigLIP-L-16, normalized concatenation, λ=100; verify Rank Score ~73-74 per Table 3
  2. Ablate λ sensitivity: Sweep λ ∈ {0, 10, 100, 1000, 10000, ∞} and plot accuracy-cost Pareto frontier; confirm frontier shape matches Figure 4
  3. Test generalization: Train on one task group (e.g., General), evaluate on held-out groups (STEM, Charts/OCR); assess cross-task routing capability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can router architectures be optimized to close the performance gap with the Oracle through finer visual cues and better modeling of textual structure?
- **Basis in paper:** [explicit] The authors conclude there is "considerable room for improvement in router architecture through finer visual cues and modeling of textual structure" (Abstract, Conclusion).
- **Why unresolved:** While learned routers (e.g., RouterDC) outperform single models, they still exhibit a "clear gap" to the ideal Oracle upper bound, indicating current architectures fail to capture all discriminative routing signals.
- **What evidence would resolve it:** Novel architectures that integrate fine-grained visual features and structural text modeling, demonstrating a significantly higher Rank Score and a Pareto frontier shifted closer to the Oracle on the benchmark.

### Open Question 2
- **Question:** How do routing performance and optimal strategies change when extending the benchmark to multi-image inputs and subjective or preference-based evaluations?
- **Basis in paper:** [explicit] The Limitations section identifies "Single Image Input" as a constraint that excludes complex scenarios like multi-image tasks. The Conclusion adds that future work involves "datasets with subjective or preference-based evaluation."
- **Why unresolved:** The current benchmark design strictly enforces single-image prompts with rule-based correctness, failing to cover real-world multimodal workflows requiring complex visual fusion or human-aligned judgment.
- **What evidence would resolve it:** An extension of the VL-RouterBench pipeline to multi-image datasets and preference-based rewards, showing that routers can maintain accuracy-cost trade-offs in these more complex settings.

### Open Question 3
- **Question:** To what extent does the performance of current VLM routers rely on the clean, curated nature of benchmark data versus noisy, real-world inputs?
- **Basis in paper:** [explicit] Section 7 (Limitations) notes the benchmark "does not explicitly stress-test routers under noisy or imperfect inputs (e.g., typos, paraphrasing, or spurious visual artifacts)."
- **Why unresolved:** The evaluation currently relies on "standardized logs with clean single-image prompts," potentially overestimating robustness for real-world deployments where query quality is highly variable.
- **What evidence would resolve it:** A robustness evaluation of the top-performing routers (e.g., RouterDC, MLP) on perturbed inputs (typos, noise), demonstrating stable accuracy and cost metrics compared to the clean baseline.

## Limitations

- The benchmark's reliance on dollar-based cost metrics may not capture deployment-specific costs like memory constraints or latency requirements
- The quality matrix construction depends on a fixed VLM set, potentially limiting generalization to new models
- The benchmark does not explicitly stress-test routers under noisy or imperfect inputs (e.g., typos, paraphrasing, or spurious visual artifacts)

## Confidence

- **High confidence**: VL-RouterBench successfully constructs a reproducible routing benchmark with measurable quality and cost matrices
- **Medium confidence**: Learned routers significantly outperform single models while lagging behind Oracle
- **Medium confidence**: Soft label training with accuracy-cost-aware weighting improves routing decisions

## Next Checks

1. **Cross-dataset generalization**: Train routers on combinations of 10 datasets and evaluate on the remaining 4 to measure performance degradation when dataset distributions shift.
2. **Model addition robustness**: Add 2-3 new VLMs to the candidate pool and retrain routers to verify whether routing gains persist or diminish as model heterogeneity changes.
3. **Deployment cost alignment**: Replace dollar costs with alternative metrics (e.g., latency, memory footprint) and verify whether routing conclusions remain consistent under different cost definitions.