---
ver: rpa2
title: 'CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation
  Learning'
arxiv_id: '2506.17818'
source_url: https://arxiv.org/abs/2506.17818
tags:
- music
- learning
- pre-training
- task
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CultureMERT addresses the limited cross-cultural effectiveness
  of music foundation models by developing a culturally adapted model through continual
  pre-training. The study introduces a two-stage CPT strategy with learning rate re-warming
  and staged adaptation to stabilize training and enable effective cross-cultural
  adaptation under computational constraints.
---

# CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning

## Quick Facts
- arXiv ID: 2506.17818
- Source URL: https://arxiv.org/abs/2506.17818
- Authors: Angelos-Nikolaos Kanatas; Charilaos Papaioannou; Alexandros Potamianos
- Reference count: 0
- Primary result: 4.9% average improvement in ROC-AUC and AP across non-Western music auto-tagging tasks

## Executive Summary
CultureMERT addresses the limited cross-cultural effectiveness of music foundation models by developing a culturally adapted model through continual pre-training. The study introduces a two-stage CPT strategy with learning rate re-warming and staged adaptation to stabilize training and enable effective cross-cultural adaptation under computational constraints. Training on 650 hours of multi-cultural data spanning Greek, Turkish, and Indian music traditions, CultureMERT achieves significant performance gains on non-Western music tasks while maintaining Western benchmarks.

## Method Summary
CultureMERT employs a two-stage continual pre-training strategy designed to work within computational constraints while enabling effective cross-cultural adaptation. The approach involves learning rate re-warming and staged adaptation techniques to stabilize training on the limited 650-hour multi-cultural dataset. The study compares multi-cultural CPT against single-culture adaptations and task arithmetic approaches, where adapted models are merged in weight space. The experimental framework evaluates performance across Western and non-Western music auto-tagging tasks using ROC-AUC and average precision metrics.

## Key Results
- CultureMERT achieves an average improvement of 4.9% in ROC-AUC and AP across non-Western music auto-tagging tasks
- Task arithmetic performs comparably to multi-cultural CPT on non-Western tasks and outperforms both pre-trained and multi-culturally adapted models on Western benchmarks
- Carnatic-adapted models show the most consistent generalization across cultural boundaries

## Why This Works (Mechanism)
CultureMERT's effectiveness stems from its two-stage continual pre-training strategy that enables stable adaptation to culturally diverse musical patterns within computational constraints. The learning rate re-warming technique helps overcome catastrophic forgetting by allowing the model to readjust to new cultural patterns without losing previously learned representations. The staged adaptation approach gradually introduces cultural diversity, enabling the model to build upon existing knowledge while incorporating new musical traditions. This incremental learning process is particularly effective given the limited 650-hour training dataset compared to the original 1.1 million hours used for MusicLM foundation model training.

## Foundational Learning
- **Continual Pre-Training (CPT)**: Sequential training on new data while preserving learned representations
  - *Why needed*: Foundation models trained on Western music struggle with non-Western traditions
  - *Quick check*: Compare performance on Western vs non-Western tasks before/after CPT

- **Learning Rate Re-warming**: Resetting learning rate to initial values during CPT
  - *Why needed*: Prevents model collapse when adapting to new cultural patterns
  - *Quick check*: Monitor training stability and convergence speed

- **Task Arithmetic**: Merging adapted models in weight space
  - *Why needed*: Alternative to multi-cultural CPT for computational efficiency
  - *Quick check*: Validate merged model performance against individual adaptations

## Architecture Onboarding
- **Component Map**: Foundation Model -> Two-Stage CPT -> Cultural Adaptation -> Evaluation
- **Critical Path**: CPT training → Cultural validation → Task arithmetic merging → Performance benchmarking
- **Design Tradeoffs**: Limited data (650h) vs computational efficiency vs cross-cultural performance
- **Failure Signatures**: Performance degradation on Western tasks, training instability, poor generalization across cultures
- **First Experiments**:
  1. Baseline evaluation of pre-trained model on Western/non-Western tasks
  2. Single-culture CPT adaptation and transfer testing
  3. Multi-cultural CPT training with learning rate re-warming

## Open Questions the Paper Calls Out
None

## Limitations
- Computational constraints limit training to 650 hours versus 1.1M hours for original foundation model
- Single-task evaluation framework may not generalize to other music understanding tasks
- Cultural representation balance within the dataset is not specified

## Confidence
- **High Confidence**: Multi-cultural CPT improves non-Western music auto-tagging while maintaining Western performance (4.9% average improvement)
- **Medium Confidence**: Task arithmetic matches or exceeds multi-cultural CPT performance on both Western and non-Western tasks
- **Medium Confidence**: Token-level similarity correlates with cross-cultural transfer effectiveness

## Next Checks
1. Scale-up validation: Replicate experiments with progressively larger training datasets (2x, 5x, 10x current 650 hours)
2. Cross-task generalization: Evaluate on additional tasks beyond auto-tagging including genre classification and similarity retrieval
3. Long-term stability analysis: Track task arithmetic stability across multiple weight-space merging operations