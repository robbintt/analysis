---
ver: rpa2
title: An Exploration of Higher Education Course Evaluation by Large Language Models
arxiv_id: '2411.02455'
source_url: https://arxiv.org/abs/2411.02455
tags:
- course
- evaluation
- llms
- learning
- education
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigates the use of large language models (LLMs)\
  \ for automated course evaluation in higher education, addressing the limitations\
  \ of traditional methods such as subjectivity, high labor costs, and limited scalability.\
  \ Three LLMs\u2014GPT-4o, Kimi, and a fine-tuned Llama (Llama-UKP)\u2014were evaluated\
  \ at both micro (classroom discussion analysis) and macro (holistic course review)\
  \ levels using real course data from China."
---

# An Exploration of Higher Education Course Evaluation by Large Language Models

## Quick Facts
- arXiv ID: 2411.02455
- Source URL: https://arxiv.org/abs/2411.02455
- Reference count: 0
- Primary result: Fine-tuned Llama-UKP achieves Pearson correlation of 0.754 and Spearman correlation of 0.843 with human expert scores, outperforming baseline LLMs in automated course evaluation

## Executive Summary
This study investigates the application of large language models (LLMs) to automate higher education course evaluation, addressing the limitations of traditional methods that suffer from subjectivity, high labor costs, and limited scalability. The researchers evaluate three LLMs—GPT-4o, Kimi, and a fine-tuned Llama model (Llama-UKP)—at both micro-level (classroom discussion analysis) and macro-level (holistic course review) evaluations using real course data from China. The fine-tuned Llama-UKP demonstrates superior performance with stronger correlations to human expert scores and more balanced score distributions, highlighting the effectiveness of combining fine-tuning with prompt engineering for this domain-specific task.

## Method Summary
The study evaluates three LLMs for automated course evaluation using 100 course packages from a Chinese university, containing course information, objectives, teaching plans, assessment designs, and development history. The evaluation framework employs 5 first-level and 15 second-level indicators for a 100-point total score. Three models are tested: GPT-4o (micro-level only), Kimi, and Llama-UKP (a fine-tuned Llama variant). Llama-UKP undergoes fine-tuning on historical course packages with human scores as labels, incorporating manual annotations that highlight relevant text segments per indicator. Prompt engineering includes role definition, chain-of-thought workflow, and explicit output formatting. Evaluation metrics include Pearson and Spearman correlation with human expert scores, score distribution variance, excellence rate, and Bland-Altman agreement analysis.

## Key Results
- Llama-UKP achieves Pearson correlation of 0.754 and Spearman correlation of 0.843 with human expert scores, significantly outperforming Kimi's correlations of 0.516 and 0.523
- Llama-UKP demonstrates a more balanced score distribution with an excellence rate of 27% compared to Kimi's 73%, indicating reduced leniency bias
- The fine-tuned model shows stronger consistency with human judgments and produces more differentiated scores across the evaluation spectrum

## Why This Works (Mechanism)
The fine-tuned Llama-UKP model outperforms general-purpose LLMs in course evaluation by leveraging domain-specific training data that captures the nuanced requirements of educational assessment. The fine-tuning process allows the model to internalize the relationship between textual evidence and evaluation criteria, while prompt engineering provides structured guidance for consistent application of scoring rubrics. This combination enables the model to better interpret educational materials and apply appropriate weightings across the 15 evaluation indicators, resulting in evaluations that align more closely with human expert judgments.

## Foundational Learning
- **Course evaluation indicators**: 15 specific criteria organized under 5 broader categories that form the basis for scoring; needed to structure the