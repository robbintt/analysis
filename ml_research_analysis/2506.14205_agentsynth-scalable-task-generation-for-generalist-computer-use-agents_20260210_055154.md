---
ver: rpa2
title: 'AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents'
arxiv_id: '2506.14205'
source_url: https://arxiv.org/abs/2506.14205
tags:
- task
- tasks
- uni00000013
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgentSynth, a scalable pipeline for synthesizing
  high-quality computer-use tasks and trajectory datasets for generalist agents. By
  leveraging information asymmetry, AgentSynth constructs simple subtasks that are
  easy to generate but challenging when composed into long-horizon tasks.
---

# AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents

## Quick Facts
- arXiv ID: 2506.14205
- Source URL: https://arxiv.org/abs/2506.14205
- Reference count: 40
- Agents drop from 18% to 4% success across difficulty levels 1 to 6

## Executive Summary
AgentSynth introduces a scalable pipeline for synthesizing high-quality computer-use tasks and trajectory datasets for generalist agents. By leveraging information asymmetry, it constructs subtasks that are simple to generate but challenging when composed into long-horizon tasks. The pipeline uses six LLM-based agents to propose, execute, verify, revise, and summarize tasks, achieving controllable difficulty through subtask chaining. Experiments demonstrate the benchmark's discriminative power, with state-of-the-art agents showing sharp performance decline across difficulty levels.

## Method Summary
AgentSynth employs a six-agent pipeline to generate computer-use tasks in the OSWorld environment. Starting with a sampled persona, a task proposer generates an initial subtask based on current desktop screenshots and history. The task executor runs the subtask using a two-stage approach: GPT-4.1 plans high-level actions, which are then grounded to pixel coordinates by a computer-use model. A verifier checks completion, a reviser adjusts incomplete tasks, and follow-up proposers generate subsequent subtasks. This process repeats up to six times, with a summarizer composing the first k subtasks into a composite task at difficulty level k. The approach exploits information asymmetry—tasks are easy to generate incrementally but hard to solve when presented as unified goals.

## Key Results
- State-of-the-art LLM agents drop from 18% to 4% success across difficulty levels 1 to 6
- Pipeline achieves $0.6 cost per trajectory, orders of magnitude cheaper than human annotations
- Generates over 6,000 diverse and realistic tasks with 94% persona relevance in human evaluation

## Why This Works (Mechanism)

### Mechanism 1: Information Asymmetry Between Generation and Execution
The pipeline generates tasks step-by-step with state context, making them reliable to synthesize but substantially harder to solve when presented as unified goals. During generation, each subtask receives current screenshots and prior history, allowing grounding in achievable states. At test time, agents must infer entire action sequences from initial states alone, without incremental scaffolding.

### Mechanism 2: Persona-Guided Task Diversity and Coherence
Sampling personas from a large hub induces combinatorial diversity while maintaining task coherence. Each trajectory begins with a randomly sampled persona, and all agents condition on this persona throughout subtask generation, ensuring realistic workflows rather than random action sequences.

### Mechanism 3: Two-Stage Execution Separating Planning From Visual Grounding
Decoupling high-level action planning from pixel-level grounding improves trajectory reliability. GPT-4.1 outputs natural language action descriptions from task, screenshot, and history, which are passed to the computer-use-preview model for precise coordinate generation. This leverages GPT-4.1's reasoning strength and the computer-use model's grounding accuracy.

## Foundational Learning

- Concept: ReAct-style agent architecture (reasoning interleaved with acting)
  - Why needed here: Task executor uses ReAct prompting to maintain thought-action history across steps
  - Quick check question: Can you explain how a ReAct agent differs from a pure chain-of-thought or pure action-posing agent?

- Concept: Visual grounding in pixel-space GUI environments
  - Why needed here: Agents must map natural language actions to (x, y) coordinates on 1920×1080 screens
  - Quick check question: What makes GUI grounding harder than web-element grounding (e.g., by DOM selectors)?

- Concept: Long-horizon task composition and state tracking
  - Why needed here: Level 6 tasks require 40-60 steps; agents must maintain context across application switches
  - Quick check question: What failure modes emerge when agents lose state awareness over long trajectories?

## Architecture Onboarding

- Component map: Persona Sampling → Task Proposer → Task Executor → Task Verifier → Task Reviser → Follow-up Task Proposer → Task Summarizer

- Critical path:
  1. Sample persona → Task Proposer generates initial subtask
  2. Task Executor runs subtask (max 10 steps)
  3. Task Verifier checks completion; if partial, Task Reviser adjusts description
  4. Follow-up Proposer generates next subtask (repeat up to n times)
  5. Task Summarizer composes first k subtasks into difficulty-level-k task

- Design tradeoffs:
  - Subtask count (n): Higher n increases difficulty but also execution cost and failure risk. Paper uses n=6.
  - Verification granularity: Full screenshot review is accurate but expensive; WebJudge-style filtering reduces tokens but may miss edge cases.
  - Model selection: GPT-4.1 is robust but costly; model choice effects on task properties remain open.

- Failure signatures:
  - Inaccurate mouse clicks (correct element identified, wrong coordinates)
  - Repetitive actions without adaptation (agent loops on failed clicks)
  - State tracking loss (agent re-does completed steps)
  - Poor screenshot interpretation (misidentifies popups, ads as task-relevant UI)

- First 3 experiments:
  1. Run pipeline on single persona with n=3 subtasks; inspect verifier accuracy manually against ground truth.
  2. Evaluate baseline agent (GPT-4.1 without computer-use-preview grounder) on difficulty levels 1-3 to quantify two-stage executor benefit.
  3. Ablate persona conditioning: generate tasks with and without personas, measure diversity metrics (unique action sequences, software coverage).

## Open Questions the Paper Calls Out

### Open Question 1
How does choice of underlying LLM in generation pipeline influence diversity, complexity, and realism of synthesized tasks? The authors note this remains open, as they used GPT-4.1 exclusively for robustness. Evidence would require comparative studies with various frontier and open-source models followed by statistical comparison of generated task distributions and human evaluations.

### Open Question 2
Can agents effectively improve visual grounding and long-term planning through curriculum learning using AgentSynth difficulty levels? The paper focuses on using generated data as benchmark rather than training set, so training dynamics remain untested. Evidence would require fine-tuning experiments starting from difficulty level 1 up to 6, followed by evaluation on held-out real-world tasks.

### Open Question 3
How can intrinsic task complexity be systematically disentangled from task novelty or "lack of exposure" in the generated benchmark? Current difficulty definition ties primarily to subtask count rather than cognitive load or reasoning difficulty. Evidence would require refined evaluation protocol scoring tasks based on reasoning depth or visual grounding difficulty independently of step count.

## Limitations
- Generalization to human-created tasks untested due to synthetic evaluation focus
- Difficulty gap may narrow if future agents learn to simulate incremental reasoning
- Two-stage executor benefits lack extensive ablation studies or error analysis

## Confidence
- High confidence: Information asymmetry mechanism is clearly articulated and empirically supported by sharp decline in success rates across difficulty levels
- Medium confidence: Two-stage execution design is logically sound but lacks extensive ablation studies or error analysis
- Low confidence: Generalization to real-world tasks, robustness across model updates, and true cost-effectiveness are asserted but not thoroughly validated

## Next Checks
1. Generalization Test: Run AgentSynth tasks through human evaluation and real-agent execution to measure fidelity of synthetic tasks to actual usability and agent performance.
2. Robustness Ablation: Compare two-stage executor against single-stage variants on common task set, conduct error analysis to quantify and localize failure modes.
3. Scaling Sensitivity: Vary persona diversity, subtask count, and model parameters to assess robustness of task difficulty and diversity metrics, identify breaking points or diminishing returns.