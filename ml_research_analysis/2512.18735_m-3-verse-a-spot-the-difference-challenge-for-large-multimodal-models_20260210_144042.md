---
ver: rpa2
title: '$M^3-Verse$: A "Spot the Difference" Challenge for Large Multimodal Models'
arxiv_id: '2512.18735'
source_url: https://arxiv.org/abs/2512.18735
tags:
- state
- object
- room
- video
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "M3-Verse is a new benchmark designed to evaluate large multimodal\
  \ models\u2019 ability to understand state changes in paired videos. It contains\
  \ 270 scenes with 2,932 questions across 50+ task types, testing four core capabilities:\
  \ spatial understanding, temporal understanding, attribute recognition, and reasoning."
---

# $M^3-Verse$: A "Spot the Difference" Challenge for Large Multimodal Models

## Quick Facts
- arXiv ID: 2512.18735
- Source URL: https://arxiv.org/abs/2512.18735
- Reference count: 40
- 16 state-of-the-art LMMs achieve only ~45% accuracy on this benchmark, compared to human performance of 89.75%

## Executive Summary
M3-Verse is a new benchmark designed to evaluate large multimodal models' ability to understand state changes in paired videos. It contains 270 scenes with 2,932 questions across 50+ task types, testing four core capabilities: spatial understanding, temporal understanding, attribute recognition, and reasoning. Evaluations of 16 state-of-the-art LMMs show significant limitations in tracking state transitions, with even top models achieving only around 45% accuracy compared to human performance of 89.75%. The proposed Hierarchical Captioning and Text-based Reasoning (HCTR) method achieves significant performance improvements, especially in inter-state reasoning tasks. The benchmark reveals that larger model size does not guarantee better performance and that hallucination remains a major challenge for current models.

## Method Summary
The M3-Verse benchmark presents paired videos showing state changes in egocentric environments, requiring models to identify differences between "before" and "after" states. The core innovation is the Hierarchical Captioning and Text-based Reasoning (HCTR) method, which transforms video understanding into a text-based task through a three-stage process: 1) segmenting videos into clips, 2) generating dense captions for each clip, and 3) summarizing captions into a cohesive narrative before reasoning. This approach forces explicit timeline serialization and shifts the burden of comparison to the Language Model, which handles textual logic more reliably than end-to-end multimodal reasoning.

## Key Results
- Top-performing model (InternVL3.5-4B) achieves 44.79% accuracy on test set
- Human performance significantly exceeds all models at 89.75% accuracy
- HCTR method improves Inter-State reasoning accuracy by 7-9 points over direct LMM approaches
- 4B parameter models can outperform 8B and even 235B parameter models, suggesting instruction tuning quality matters more than raw size

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting video understanding into a text-based reasoning task via HCTR mitigates temporal conflation errors in LMMs
- **Mechanism:** By segmenting videos into clips, generating dense captions, and summarizing them into a cohesive narrative before reasoning, the model is forced to explicitly serialize the timeline. This reduces the cognitive load on the visual encoder to maintain "perfect memory" of long sequences and shifts the burden of comparison to the Language Model, which handles textual logic more reliably
- **Core assumption:** The visual encoder is capable of accurate local perception (describing a short clip), and the text-only LLM possesses superior logical comparison capabilities compared to the end-to-end multimodal reasoning pathway
- **Evidence anchors:** [abstract]: The proposed HCTR method achieves significant performance improvements, especially in inter-state reasoning tasks. [Section 5.1]: This method transforms the multimodal reasoning problem into a text-based task through a three-stage process... forcing the model to explicitly serialize the timeline of events into a textual narrative

### Mechanism 2
- **Claim:** Smaller, instruction-tuned models (e.g., 4B parameters) can outperform larger models (e.g., 8B or 235B) on fine-grained state tracking
- **Mechanism:** Fine-grained spatial and state understanding relies heavily on the quality of instruction tuning and visual grounding rather than raw capacity. Larger models may over-rely on priors or hallucinations, whereas well-tuned smaller models might follow stricter visual grounding protocols
- **Core assumption:** The evaluation accurately reflects "understanding" rather than just retrieval, and the smaller models are not simply overfitting to the specific benchmark format
- **Evidence anchors:** [Section 4.2]: Qwen3-VL-4B-Instruct emerges as the top-performing open-source model, even surpassing the massive Qwen3-VL-235B... suggests that architectural design and instruction tuning may be more critical. [Section 4.2]: The 4B versions... clearly outperform their respective 8B counterparts

### Mechanism 3
- **Claim:** Testing "inter-state" changes (comparing two videos) exposes distinct failure modes compared to "intra-state" (single video) analysis, specifically highlighting hallucination and temporal agnosia
- **Mechanism:** Standard benchmarks often allow models to answer using spatial priors (e.g., "toasters are usually on counters"). By requiring a comparison of two distinct states of the same scene, the model must override these priors and rely strictly on observed visual evidence (e.g., the toaster moved to the table)
- **Core assumption:** The model can maintain a stable representation of Scene State A while processing Scene State B without catastrophic forgetting or feature blending
- **Evidence anchors:** [Section 1]: This two-state structure transcends simple recognition, requiring models to perform a fine-grained semantic comparison and infer the transformations. [Section 4.2]: Intra-state tasks pose a more significant challenge by resembling a "needle in a haystack" problem... [while inter-state requires] comparing the 'before' and 'after'

## Foundational Learning

- **Concept: Egocentric Video Understanding**
  - **Why needed here:** The M3-Verse benchmark uses egocentric (first-person) videos from an agent exploring a room. Standard third-person video understanding models often fail here due to camera motion and lack of stable global frames
  - **Quick check question:** Can you identify the difference between camera ego-motion (the agent moving) and object motion (an object moving) in a video stream?

- **Concept: State Space Representation**
  - **Why needed here:** The core task is "spot the difference," which requires constructing a representation of "State A" and "State B" and computing the delta. You must understand how to map visual frames to discrete state vectors
  - **Quick check question:** If an object is partially obscured in "State A" but visible in "State B," how should a system represent this change without falsely reporting a "new object"?

- **Concept: Hallucination Detection**
  - **Why needed here:** The benchmark includes specific "hallucination-type" questions where "No correct option is listed" is the answer. Systems must learn to distinguish between confident visual extraction and guessing based on language priors
  - **Quick check question:** If asked "What color is the invisible elephant?" and the model answers "Gray," is this a visual error or a hallucination error?

## Architecture Onboarding

- **Component map:** Frame Sampler -> [Branch 1: Direct LMM] OR [Branch 2: HCTR (Clip Captioner -> Summarizer -> Text QA)]
- **Critical path:** The Visual-to-Text Serialization in HCTR. The system relies on the captioner accurately describing the object states. If the captioner misses a detail (e.g., "open drawer" vs "closed drawer") in the clip phase, the reasoning engine cannot recover the correct answer
- **Design tradeoffs:**
  - End-to-End vs. HCTR: End-to-End (Direct LMM) is faster and simpler but suffers from hallucination (~12% performance drop on hallucination questions). HCTR is computationally heavier (requires multiple inference passes) but significantly boosts Inter-State reasoning accuracy
  - Frame Count: Increasing frames (up to 1000) improves scores but linearly increases inference cost and memory usage
- **Failure signatures:**
  - Temporal Conflation: Answering that an object was "always there" when it was moved between states
  - Instruction Misalignment: Generating open-ended text when a multiple-choice letter is required (common in reasoning-enhanced models like WeThink)
  - Hallucination Loop: Insisting on an answer (e.g., "Blue pillow") even when the "No correct option" choice is valid, due to strong language priors
- **First 3 experiments:**
  1. Baseline Verification: Run InternVL3.5-4B on the full M3-Verse test set using standard end-to-end inference to establish a baseline accuracy (expected ~35-38%)
  2. HCTR Integration: Implement the HCTR pipeline on the same model. Compare "Inter-State" accuracy specifically to validate the ~7-9 point gain claimed in the text
  3. Frame Sensitivity Ablation: Test the model with 60 frames vs. 200 frames per video to quantify the "needle in a haystack" difficulty and determine the optimal latency/accuracy tradeoff for your deployment constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural innovations beyond raw parameter scaling can improve LMM performance on multi-state visual understanding tasks, given that larger models do not consistently outperform smaller ones?
- Basis in paper: [explicit] The authors state "larger model size does not consistently lead to improved performance on M3-Verse" and suggest that "architectural design and instruction tuning may be more critical than raw parameter count"
- Why unresolved: The paper identifies the non-monotonic scaling phenomenon but does not propose or test specific architectural alternatives that could systematically address it
- What evidence would resolve it: A systematic study comparing different architectural modifications (e.g., temporal attention mechanisms, state-difference modules) while controlling for model size and training data

### Open Question 2
- Question: How can LMMs be made more robust to hallucination when answering questions about visual state changes in paired videos?
- Basis in paper: [explicit] The authors report "a performance drop of 12.32 points on hallucination-centric questions relative to their fact-based counterparts" and identify this as "a primary contributor to the modest overall performance scores"
- Why unresolved: The HCTR method improves overall performance but does not specifically target hallucination reduction, and the paper does not analyze which types of hallucinations are most common
- What evidence would resolve it: Fine-grained error analysis categorizing hallucination types, combined with interventions (e.g., contrastive training, grounding mechanisms) that specifically target each category

### Open Question 3
- Question: To what extent do findings from the simulation-based M3-Verse benchmark generalize to real-world multi-state visual understanding scenarios?
- Basis in paper: [inferred] The benchmark is built entirely on the AI2-THOR simulator and ProcTHOR-10k dataset, with no real-world validation. The 270 scenes are all indoor environments with controlled state changes
- Why unresolved: Simulators inevitably have domain gaps with real-world complexity (lighting, texture variation, occlusion patterns, noise) that could affect model behavior
- What evidence would resolve it: Evaluation of the same models on a paired-video benchmark constructed from real-world footage (e.g., home surveillance cameras, robotics deployment logs)

### Open Question 4
- Question: Why do many models exhibit stronger performance on inter-state tasks (requiring comparison across states) than on intra-state tasks (requiring perception within a single state)?
- Basis in paper: [explicit] The authors observe that "many models exhibit stronger performance on inter-state tasks than on intra-state tasks" and hypothesize that intra-state tasks pose a "needle in a haystack" challenge requiring accurate localization
- Why unresolved: The hypothesis is plausible but untested; it remains unclear whether the issue is localization difficulty, attention distribution, or another factor
- What evidence would resolve it: A controlled study varying object density and scene complexity in intra-state tasks, coupled with attention visualization to diagnose where models fail

## Limitations
- Benchmark is entirely simulation-based using AI2-THOR and ProcTHOR-10k, lacking real-world validation
- HCTR method implementation details remain underspecified (captioner consistency, summarization approach, prompt engineering)
- Hallucination measurement lacks precise operational definition and distinction between true hallucinations vs reasonable but unmatched answers
- Instruction tuning quality assessment lacks comparative analysis of actual instruction datasets and training procedures

## Confidence
- **High Confidence**: The benchmark construction methodology (270 scenes, 2,932 questions, 50+ task types) and baseline evaluations are well-documented and reproducible. The core observation that current LMMs struggle with state change detection is robustly supported
- **Medium Confidence**: The HCTR method's effectiveness, while demonstrated, relies on some assumptions about caption quality and text reasoning capabilities that weren't exhaustively validated. The performance improvements are significant but the exact contribution of each HCTR component remains unclear
- **Medium Confidence**: The inverted scaling relationship (4B outperforming larger models) is interesting but could be influenced by specific benchmark characteristics rather than representing a general principle. More diverse evaluation scenarios would strengthen this claim

## Next Checks
1. **Cross-Domain Robustness Test**: Evaluate the top-performing models on non-egocentric video datasets (e.g., surveillance footage, movie clips, or action recognition datasets) to assess whether the M3-Verse-specific tuning generalizes beyond the egocentric domain
2. **HCTR Component Ablation**: Systematically disable each HCTR component (caption summarization, caption generation quality control, or text reasoning stage) to quantify the individual contribution of each step and identify whether certain components are critical bottlenecks
3. **Temporal Resolution Sensitivity Analysis**: Conduct a controlled experiment varying frame sampling rates (e.g., 30fps vs 5fps vs 1fps) to determine the minimum temporal resolution required for accurate state change detection and whether current models' failures stem from insufficient temporal granularity rather than fundamental understanding limitations