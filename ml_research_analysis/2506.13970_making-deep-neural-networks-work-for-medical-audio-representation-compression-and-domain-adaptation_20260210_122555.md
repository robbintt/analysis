---
ver: rpa2
title: 'Making deep neural networks work for medical audio: representation, compression
  and domain adaptation'
arxiv_id: '2506.13970'
source_url: https://arxiv.org/abs/2506.13970
tags:
- domain
- audio
- infant
- page
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis addresses key technical challenges in applying machine
  learning to medical audio analysis, focusing on infant cry sounds for health condition
  detection. It introduces neural transfer learning to leverage adult speech datasets
  for building robust infant cry models, achieving high accuracy in detecting perinatal
  asphyxia.
---

# Making deep neural networks work for medical audio: representation, compression and domain adaptation

## Quick Facts
- arXiv ID: 2506.13970
- Source URL: https://arxiv.org/abs/2506.13970
- Authors: Charles C Onu
- Reference count: 0
- Primary result: Transfer learning from adult speech datasets achieves high accuracy in detecting perinatal asphyxia from infant cry sounds

## Executive Summary
This thesis tackles core technical challenges in applying deep learning to medical audio, focusing on infant cry analysis for health condition detection. It introduces neural transfer learning from adult speech datasets to build robust infant cry models, achieving high accuracy in detecting perinatal asphyxia. An end-to-end tensor decomposition method compresses recurrent neural networks for lightweight, portable deployment on resource-constrained devices. The work also addresses dataset bias and domain shift through domain adaptation techniques adapted from computer vision, improving cross-domain generalization. A novel large-scale infant cry dataset (CryCeleb) is released to support research in cry-based recognition tasks. These contributions collectively advance scalable, accurate, and accessible AI-driven tools for affordable healthcare.

## Method Summary
The work develops a multi-pronged approach to medical audio analysis. Transfer learning leverages pre-trained models from adult speech datasets (e.g., word recognition) to bootstrap infant cry classification, with fine-tuning on limited cry data. Self-supervised learning (SSL) pre-trains audio encoders (e.g., CNN14) on large generic audio corpora (VGGSound) with contrastive loss, optionally adapted to unlabeled infant cries, before supervised fine-tuning. Domain adaptation methods—including kernel regularization (HAFN/SAFN), entropy minimization, domain-adversarial training, adaptive batch normalization, and target noise injection—are applied to improve cross-hospital generalization. For model compression, an end-to-end tensor-train (TT) decomposition is used to factorize RNN weights across gates, enabling up to 300x compression with minimal accuracy loss. The CryCeleb dataset (6.5h, 786 infants) supports verification tasks.

## Key Results
- Transfer learning from adult speech tasks (especially word recognition) effectively bridges data gaps for infant cry classification, achieving high AUC/UAR on perinatal asphyxia detection.
- End-to-end tensor-train decomposition compresses RNNs up to 300x while preserving task performance, enabling lightweight deployment on resource-constrained devices.
- Domain adaptation techniques significantly improve cross-hospital generalization, reducing performance degradation due to domain shift in medical audio.
- CryCeleb dataset release provides a large-scale resource for infant cry speaker verification and recognition research.

## Why This Works (Mechanism)
The thesis addresses data scarcity and domain shift in medical audio by leveraging transfer learning from adult speech, where large labeled datasets exist. Pre-training on speech tasks (e.g., word recognition) provides a strong feature representation that transfers well to infant cry classification. SSL further enhances representation learning by exploiting unlabeled audio data. Tensor-train decomposition compresses models efficiently by exploiting weight redundancy in RNNs, enabling deployment on edge devices. Domain adaptation methods mitigate performance degradation when models encounter data from new hospitals or recording conditions, crucial for real-world clinical deployment.

## Foundational Learning
- **Infant cry acoustic features**: Temporal and spectral patterns unique to infant vocalizations, useful for detecting health conditions like asphyxia. Needed to understand what the models learn from. Quick check: Visualize cry spectrograms and compare with healthy vs. pathological cries.
- **Transfer learning in audio**: Pre-training on source tasks (e.g., speech) and fine-tuning on target tasks (e.g., cry classification) to overcome data scarcity. Needed to bootstrap models with limited infant cry data. Quick check: Compare fine-tuning vs. training from scratch on cry classification.
- **Self-supervised learning (SSL)**: Learning representations from unlabeled data using contrastive or reconstruction objectives. Needed to leverage large unlabeled audio corpora. Quick check: Evaluate SSL-pretrained models vs. supervised baselines on downstream tasks.
- **Domain adaptation**: Techniques (e.g., adversarial training, kernel regularization) to align source and target domain distributions. Needed to handle hospital-specific biases. Quick check: Measure performance drop when training and testing on different hospitals.
- **Tensor-train decomposition**: Factorizing weight matrices into low-rank tensors for model compression. Needed to reduce model size for edge deployment. Quick check: Measure accuracy vs. compression ratio trade-off.
- **Cross-entropy loss**: Standard supervised loss for classification tasks. Needed to train models on labeled data. Quick check: Monitor training loss and accuracy curves.

## Architecture Onboarding
- **Component map**: Adult speech dataset → Pre-trained CNN/RNN → Fine-tuning on infant cries → Cry classification/verification
- **Critical path**: Pre-training (adult speech/SSL) → Domain adaptation (if needed) → Fine-tuning (cry data) → Compression (TT-RNN) → Deployment
- **Design tradeoffs**: Transfer learning vs. training from scratch (data efficiency vs. task specificity), SSL vs. supervised pre-training (unlabeled data use vs. labeled accuracy), domain adaptation methods (alignment quality vs. complexity), compression level vs. accuracy (deployment vs. performance).
- **Failure signatures**: Poor transfer learning (domain mismatch), overfitting (small labeled data), domain shift (hospital bias), excessive compression (accuracy loss).
- **First experiments**:
  1. Pre-train a small CNN (e.g., res8) on Speech Commands dataset, fine-tune on CryCeleb for cry classification; report AUC.
  2. Apply domain adaptation (e.g., entropy minimization) to improve cross-hospital generalization on multi-hospital cry data.
  3. Compress a trained RNN using tensor-train decomposition (TT-rank=2) and measure accuracy-robustness tradeoff.

## Open Questions the Paper Calls Out
- **Open Question 1**: Does incorporating video or physiological measurements (e.g., heart rate) improve the performance and robustness of cry-based health diagnostics compared to audio-only models? The thesis suggests exploring multimodal learning by incorporating other data types, such as video or physiological measurements, as a promising direction. The current work focuses exclusively on audio signal analysis; the potential synergistic effects of combining modalities remain untested. A comparative study evaluating the proposed audio models against multimodal models using the Ubenwa dataset augmented with video and vital sign data would resolve this.
- **Open Question 2**: Can the proposed end-to-end tensor decomposition compression methods be effectively integrated with non-recurrent architectures, such as convolutional or transformer-based models? The integration of tensor-train RNNs with non-recurrent architectures, such as convolutional or transformer-based models, remains an open avenue for exploration. The compression technique was validated specifically on RNNs (LSTM/GRU), and its interaction with the attention mechanisms in transformers is unknown. Experiments applying the tensorization process to a transformer-based audio encoder and measuring the trade-off between compression rate and task performance would resolve this.
- **Open Question 3**: Can the methodologies developed for infant cry analysis be successfully generalized to other types of medical audio, such as heart or lung sounds? The thesis states that methodologies presented can be applied to other types of medical audio, such as heart and lung sounds, to expand utility. The techniques were tailored to the specific acoustic characteristics of infant cries; their robustness to the spectral differences of cardio-respiratory sounds is not demonstrated. Applying the transfer learning and domain adaptation pipelines to standard heart and lung sound datasets to benchmark performance against existing baselines would resolve this.

## Limitations
- Dependence on restricted-access clinical databases (Chillanto, Ubenwa) prevents full independent verification of results.
- Incomplete specification of hyperparameters for domain adaptation methods and preprocessing details limits reproducibility.
- Lack of large-scale clinical trials to validate real-world performance and regulatory considerations for medical device deployment.

## Confidence
- Transfer learning results: Medium-High (supported by published evaluations)
- Domain adaptation claims: Medium (fewer quantitative comparisons)
- Clinical validation: Low (limited access to proprietary databases)
- Compression results: Medium-High (methodologically sound, but hardware-specific results not fully detailed)

## Next Checks
1. Reproduce transfer learning results using publicly available datasets (Speech Commands for pre-training, CryCeleb for fine-tuning) and compare against baseline models.
2. Implement and evaluate domain adaptation techniques on multi-hospital datasets to quantify cross-domain performance improvements.
3. Validate tensor-train compression approach on a small RNN for infant cry classification, measuring accuracy-robustness tradeoffs at different compression levels.