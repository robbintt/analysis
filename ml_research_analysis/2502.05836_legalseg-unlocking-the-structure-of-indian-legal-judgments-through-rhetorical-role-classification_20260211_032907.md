---
ver: rpa2
title: 'LegalSeg: Unlocking the Structure of Indian Legal Judgments Through Rhetorical
  Role Classification'
arxiv_id: '2502.05836'
source_url: https://arxiv.org/abs/2502.05836
tags:
- legal
- rhetorical
- role
- sentence
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of semantic segmentation of legal
  documents through rhetorical role classification, focusing on Indian legal judgments.
  The authors introduce LegalSeg, the largest annotated dataset for this task, comprising
  over 7,000 documents and 1.4 million sentences labeled with 7 rhetorical roles.
---

# LegalSeg: Unlocking the Structure of Indian Legal Judgments Through Rhetorical Role Classification

## Quick Facts
- arXiv ID: 2502.05836
- Source URL: https://arxiv.org/abs/2502.05836
- Reference count: 40
- Primary result: LegalSeg dataset of 7,120 annotated legal documents; Hierarchical BiLSTM-CRF achieves F1-score of 0.77

## Executive Summary
This paper introduces LegalSeg, the largest annotated dataset for rhetorical role classification in legal documents, comprising over 7,000 Indian Supreme Court judgments and 1.4 million sentences labeled with 7 rhetorical roles. The authors evaluate multiple state-of-the-art models including Hierarchical BiLSTM-CRF, TransformerOverInLegalBERT, Graph Neural Networks, and Role-Aware Transformers. Results demonstrate that models incorporating broader context, structural relationships, and sequential sentence information significantly outperform sentence-level approaches, with the Hierarchical BiLSTM-CRF model achieving the highest performance. The work also explores the limitations of large language models for this specialized task.

## Method Summary
The LegalSeg dataset contains 7,120 Indian Supreme Court judgments annotated with 7 rhetorical roles: Facts, Issue, Arguments of Petitioner (AoP), Arguments of Respondent (AoR), Reasoning, Decision, and None. Sentences are segmented using SpaCy and mapped to rhetorical roles. Multiple models are evaluated: Hierarchical BiLSTM-CRF with sent2vec embeddings, TransformerOverInLegalBERT, GNN-based approaches, Role-Aware Transformers, and RhetoricLLaMA. The Hierarchical BiLSTM-CRF model uses sentence embeddings from sent2vec (trained on Indian Supreme Court judgments), processes them through a BiLSTM, and applies a CRF layer for structured prediction. The dataset is split 70-20-10 for training, validation, and testing.

## Key Results
- LegalSeg dataset contains 7,120 documents and 1,487,149 sentences with 7 rhetorical role labels
- Hierarchical BiLSTM-CRF achieves highest performance with F1-score of 0.77
- Context and structural information significantly improve performance over sentence-only models
- RhetoricLLaMA underperforms with F1-score of 0.09, indicating need for domain-specific adaptation
- Severe class imbalance exists (None: 48.5%, Facts: 16.4%, Reasoning: 20.4%, Issue: 1.2%, Decision: 2.1%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly modeling label transition probabilities improves rhetorical role classification by enforcing structural coherence.
- Mechanism: The CRF layer learns which role sequences are plausible (e.g., Issue → Arguments → Reasoning → Decision) and penalizes invalid transitions, reducing incoherent predictions that sentence-level classifiers produce.
- Core assumption: Legal judgments follow predictable rhetorical sequences that can be captured as transition probabilities.
- Evidence anchors:
  - [abstract] "models incorporating broader context, structural relationships, and sequential sentence information outperform those relying solely on sentence-level features"
  - [section 7.1] "The CRF layer explicitly models label transitions, refining predictions by enforcing structural coherence... labels in legal documents follow a structured sequence"
  - [corpus] Related work on rhetorical role labeling (MARRO, Structured Definitions) similarly emphasizes sequential context, though causal claims about CRF superiority remain domain-specific.
- Break condition: If documents have highly non-linear rhetorical structures (e.g., frequent flashbacks, nested arguments), transition constraints may over-constrain predictions.

### Mechanism 2
- Claim: Providing preceding sentence context improves classification by disambiguating role boundaries.
- Mechanism: Concatenating previous sentences with the current sentence gives the model access to discourse continuity signals (e.g., "submitted that" continuing from petitioner context indicates Arguments).
- Core assumption: Rhetorical roles exhibit local coherence where adjacent sentences tend to share or transition smoothly between roles.
- Evidence anchors:
  - [abstract] "conducted experiments using surrounding context and predicted or actual labels of neighboring sentences to assess their impact"
  - [section 5.4] InLegalBERT(i-2, i-1, i) achieves best performance among variants (F1=0.58 vs 0.49 for sentence-only)
  - [corpus] Limited direct corpus validation; neighboring papers focus on role labeling but don't isolate context window effects causally.
- Break condition: If sentences are overly long or documents have abrupt topic shifts, context concatenation introduces noise rather than signal.

### Mechanism 3
- Claim: Hierarchical architectures capture document-level position signals that sentence encoders miss.
- Mechanism: ToInLegalBERT adds sentence-level positional encodings atop token-level representations, enabling the model to learn that certain roles (e.g., Facts) appear earlier and others (e.g., Decision) appear later in documents.
- Core assumption: Rhetorical roles correlate with document position in systematic ways.
- Evidence anchors:
  - [section 5.1] "This method incorporates both the local context of sentences and their position in the document"
  - [section 7.1] "ToInLegalBERT, which integrates sentence-level positional encodings and hierarchical structuring, performs better than standard BERT-based models"
  - [corpus] Position-aware legal document processing appears in related work but without controlled ablation studies confirming causality.
- Break condition: If position-role correlations are weak (e.g., Facts appearing throughout long judgments), positional features add minimal value.

## Foundational Learning

- **Conditional Random Fields (CRFs) for sequence labeling**
  - Why needed here: CRFs model dependencies between adjacent labels, crucial for coherent rhetorical role sequences.
  - Quick check question: Can you explain why a CRF layer would prevent predicting "Decision" immediately after "Facts"?

- **Hierarchical document representations**
  - Why needed here: Legal documents have multi-level structure (tokens → sentences → documents) that flat models cannot capture.
  - Quick check question: What information does a sentence-level encoder add that a token-level encoder cannot provide?

- **Context window strategies for discourse tasks**
  - Why needed here: Rhetorical roles depend on surrounding discourse, not just sentence content.
  - Quick check question: Why might adding the next sentence (i+1) not help as much as adding the previous sentence (i-1)?

## Architecture Onboarding

- Component map: Tokenization -> Sentence embedding -> Context aggregation -> CRF decoding -> Label assignment
- Critical path: Tokenization → Sentence embedding → Context aggregation → CRF decoding → Label assignment. The BiLSTM-CRF path is the highest-performing and should be the baseline.
- Design tradeoffs:
  - BiLSTM-CRF vs. Transformer: BiLSTM-CRF better captures sequential transitions; transformers better capture long-range attention but require more data.
  - Context window size: Larger windows add information but increase noise and compute; i-2 to i was optimal here.
  - True vs. predicted previous labels: Training with predicted labels improves robustness at inference (exposure to noise).
- Failure signatures:
  - Confusion between Facts and Reasoning: Overlapping language patterns (see confusion matrix, Figure 3).
  - Class imbalance: "None" and "Facts" overpredicted; "Issue" and "Decision" underpredicted.
  - LLM underperformance: RhetoricLLaMA (F1=0.09) failed despite instruction-tuning, suggesting specialized adaptation is required.
- First 3 experiments:
  1. Baseline replication: Implement Hierarchical BiLSTM-CRF with sent2vec embeddings; target F1 ≈ 0.77.
  2. Context ablation: Compare InLegalBERT(i), InLegalBERT(i-1,i), and InLegalBERT(i-2,i-1,i) to quantify context contribution.
  3. Class imbalance mitigation: Apply class-weighted loss or oversampling for minority labels (Issue, Decision); measure per-class F1 improvements.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does explicitly modeling label uncertainty improve classification performance when using predicted labels as context for subsequent sentences?
  - Basis in paper: Section 7.3 notes that models using predicted labels sometimes outperform those using true labels and states, "Further research is needed to analyze whether explicitly modeling label uncertainty could further enhance performance."
  - Why unresolved: The authors observed this counter-intuitive phenomenon but did not implement mechanisms to quantify or utilize the uncertainty of the predicted labels provided to the model during inference.
  - What evidence would resolve it: An ablation study comparing the current Hierarchical BiLSTM-CRF against variants utilizing uncertainty-aware training (e.g., label smoothing or Bayesian neural networks) to handle noisy context labels.

- **Open Question 2**: What specific domain-specific pre-training strategies or instruction sets are required to make open-source LLMs competitive with smaller, fine-tuned models for structured legal prediction?
  - Basis in paper: Section 7.4 concludes that "for highly specialized domains such as legal NLP, additional domain-specific pre-training and refined instruction sets are necessary," as RhetoricLLaMA lagged behind BiLSTM-CRF.
  - Why unresolved: The paper's exploration was limited to standard instruction tuning; the authors did not determine the specific architectural or data refinements needed to bridge the performance gap with simpler models.
  - What evidence would resolve it: Experiments evaluating LLMs (e.g., LLaMA) after undergoing continued pre-training on raw legal corpora versus standard generic corpora, measured against the Hierarchical BiLSTM-CRF benchmark.

- **Open Question 3**: Can models trained on the Indian LegalSeg dataset be effectively adapted to other jurisdictions or legal systems using transfer learning?
  - Basis in paper: The Limitations section states: "Future research [should] look into cross-jurisdictional generalization using transfer learning techniques," as the dataset is specialized to the Indian judiciary.
  - Why unresolved: The study focused exclusively on Indian legal judgments, and the models were not evaluated on documents from different legal systems (e.g., Common Law vs. Civil Law).
  - What evidence would resolve it: Zero-shot or few-shot evaluation results of the trained LegalSeg models on rhetorical role datasets from other countries (e.g., the Italian or Brazilian corpora mentioned in the related work).

## Limitations

- Severe class imbalance in the dataset (None: 48.5%, Facts: 16.4%, Reasoning: 20.4%, Issue: 1.2%, Decision: 2.1%) may skew performance metrics and model behavior
- Limited generalizability to other legal domains and jurisdictions beyond Indian Supreme Court judgments
- Insufficient analysis of per-class performance for minority labels critical to legal document understanding
- Unclear optimization of context window parameters and their impact across different document types

## Confidence

- **High confidence**: Hierarchical BiLSTM-CRF architecture achieving F1=0.77 on LegalSeg dataset
- **Medium confidence**: Superiority of models incorporating broader context and structural relationships
- **Low confidence**: RhetoricLLaMA failure (F1=0.09) as evidence that general LLMs cannot perform this task

## Next Checks

1. **Per-class performance audit**: Compute and report F1 scores for each rhetorical role separately, with particular attention to minority classes (Issue, Decision). If these remain below 0.5, investigate whether the task requires class-balancing techniques or whether these labels are inherently ambiguous in the corpus.

2. **Context contribution isolation**: Design an ablation study that systematically varies context window size (i-1, i-2, i-3) and context source (previous sentences vs. next sentences) across documents of varying lengths. This would quantify the marginal benefit of each additional context sentence and identify when context becomes noise.

3. **Cross-domain robustness test**: Apply the best-performing model (Hierarchical BiLSTM-CRF) to a small validation set from a different legal domain (e.g., Indian contracts or US court opinions). Measure performance drop to assess whether the model has learned domain-general rhetorical structures or overfit to Indian judgment prose.