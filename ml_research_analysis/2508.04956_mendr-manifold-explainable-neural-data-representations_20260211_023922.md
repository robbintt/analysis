---
ver: rpa2
title: 'MENDR: Manifold Explainable Neural Data Representations'
arxiv_id: '2508.04956'
source_url: https://arxiv.org/abs/2508.04956
tags:
- patch
- subject
- embeddings
- wavelet
- manifold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MENDR is the first Riemannian EEG foundation model that learns
  generalized representations of EEG signals on the SPD manifold using wavelet-based
  embeddings and a novel Manifold Attention transformer architecture. By decomposing
  EEG into five frequency bands, applying a graph neural network for spatial harmonization,
  and learning interpretable SPD matrix embeddings, MENDR achieves near state-of-the-art
  performance on clinical EEG tasks with significantly fewer parameters (1.8M vs 4-46M
  in competing models).
---

# MENDR: Manifold Explainable Neural Data Representations

## Quick Facts
- **arXiv ID:** 2508.04956
- **Source URL:** https://arxiv.org/abs/2508.04956
- **Reference count:** 40
- **Primary result:** First Riemannian EEG foundation model achieving near state-of-the-art performance with 10x fewer parameters (1.8M vs 4-46M) using SPD manifold embeddings and wavelet decomposition.

## Executive Summary
MENDR introduces the first Riemannian EEG foundation model that learns generalized representations on the symmetric positive definite (SPD) manifold using wavelet-based embeddings and a novel Manifold Attention transformer architecture. By decomposing EEG into five frequency bands, applying a graph neural network for spatial harmonization, and learning interpretable SPD matrix embeddings, MENDR achieves near state-of-the-art performance on clinical EEG tasks while requiring significantly fewer parameters than competing models. The model demonstrates efficient, interpretable, and clinically applicable EEG analysis with strong generalization to datasets with fewer channels.

## Method Summary
MENDR is a 3-phase pretrained EEG foundation model that processes 19-channel EEG signals through discrete wavelet packet transform (DWPT) decomposition into five frequency bands (δ, θ, α, β, γ). Each band is processed through a graph neural network spatial harmonizer, convolutional encoder, and subject ID injection (decoder side only) before projection onto the SPD manifold via sample covariance matrices. The model employs a novel Manifold Attention transformer with BiMap layers constrained to the Stiefel manifold, using Log-Euclidean metrics instead of Euclidean operations. Pretraining involves autoencoder reconstruction, wavelet LOO contrastive learning, and masked autoencoding on LEM eigenvalues, followed by fine-tuning on downstream clinical tasks.

## Key Results
- Achieves near state-of-the-art performance on clinical EEG tasks with only 1.8M parameters (vs 4-46M in competing models)
- Demonstrates strong generalization to datasets with fewer channels (6-channel ISRUC task)
- Reconstructs signals accurately and visualizes embeddings as interpretable ellipsoids
- Shows 10x parameter efficiency compared to Euclidean baseline models

## Why This Works (Mechanism)

### Mechanism 1: Riemannian SPD Manifold Embeddings Enable Parameter-Efficient Representations
Constraining EEG patch embeddings to symmetric positive definite (SPD) matrices on the Riemannian manifold yields comparable performance to larger Euclidean models with ~10x fewer parameters. The geometric constraint acts as strong inductive bias, reducing the hypothesis space while maintaining representational capacity.

### Mechanism 2: Wavelet Band Decomposition Preserves Neurophysiological Interpretability
Decomposing EEG into physiologically meaningful frequency bands (δ, θ, α, β, γ) before embedding enables both efficient multi-scale learning and interpretable representations. The Leave-One-Out (LOO) contrastive loss forces bands to learn mutually predictive relationships, capturing cross-frequency coupling known in neuroscience.

### Mechanism 3: GNN Spatial Harmonization Enables Cross-Dataset Generalization
Using graph neural networks with electrode position-based adjacency to harmonize all datasets to a canonical 19-channel layout enables robust handling of varying electrode configurations. Channel dropout during pretraining forces the GNN to learn spline interpolation for missing channels.

## Foundational Learning

- **Symmetric Positive Definite Matrices and Riemannian Geometry:** Why needed here - the entire embedding space and all transformer operations are defined on the SPD manifold, not Euclidean space. Quick check: Given a 19×19 covariance matrix, can you explain why adding a small ε to the diagonal before and after trace normalization guarantees positive definiteness?

- **Discrete Wavelet Packet Transform (DWPT):** Why needed here - the input representation is not raw EEG but wavelet coefficients across 5 frequency bands. Quick check: If you resample EEG to 200Hz instead of 128Hz, which frequency band boundaries would change and why?

- **Manifold Attention (MAtt) and Stiefel Manifold Optimization:** Why needed here - standard attention uses matrix multiplication; MAtt uses BiMap layers where weights W must remain on the Stiefel manifold. Quick check: In BiMap layer `A_new = W @ A @ W^T`, what constraint must W satisfy and how does backpropagation differ from standard linear layers?

## Architecture Onboarding

- **Component map:** Raw EEG (C×T) → [DWPT] → 5 frequency bands → [Patchify] → 2-second patches per band → [GNN Spatial Harmonizer] → 19-channel harmonized patches → [Conv Encoder + SE Block] → Wavelet features → [Subject ID injection, decoder side only] → Subject-augmented features → [SPD Projection via Sample Covariance] → 19×19 SPD matrices → [Optional: Bilinear dim reduction to 6×6 for Large model] → [Manifold Transformer with MAtt attention] → Contextualized embeddings → [Log-Euclidean Mean across bands] → Combined embedding → [Downstream head] → Task prediction

- **Critical path:** Numerical stability in SPD operations - Eigenvalue degeneracy will crash SVD gradients. Channel dropout during pretraining - Without this, the GNN spatial harmonizer won't learn interpolation. Subject ID only to decoder - This is the key to subject-agnostic encoder embeddings.

- **Design tradeoffs:** Tiny vs. Large contextualizer - Tiny keeps 19×19 SPD matrices throughout; Large reduces to 6×6 via bilinear layer. Fixed 19-channel layout - Enforces consistency but requires imputation for datasets with fewer channels. 2-second patches - Longer than typical 1-second patches, necessary because wavelet decomposition reduces temporal resolution.

- **Failure signatures:** Exploding gradients during manifold attention - Check eigenvalue spread in SPD matrices. Poor reconstruction loss - If autoencoder doesn't converge, wavelet embeddings don't preserve signal information. Degraded performance on few-channel datasets - GNN harmonizer may be hallucinating channels.

- **First 3 experiments:** 1) Reconstruction sanity check - Train only the autoencoder, verify wavelet coefficient reconstruction loss converges. 2) Tiny model on TUAB with frozen encoder - Fine-tune only contextualizer and downstream head. 3) Ablate GNN harmonizer - Replace with simple zero-padding or linear interpolation, test on ISRUC.

## Open Questions the Paper Calls Out

- Can the Riemannian manifold transformer architecture be refined to scale effectively to larger dimensions without succumbing to numerical instability?
- Does integrating multi-headed attention into the Manifold Attention (MAtt) module improve the model's representational capacity for complex EEG patterns?
- Is the lower-dimensional SPD manifold inherently insufficient for specific tasks like sleep staging, or is the underperformance on low-channel datasets due to suboptimal harmonization?

## Limitations

- The claimed 10x parameter efficiency may not hold when accounting for training time, as SVD operations in BiMap layers are O(n³)
- The GNN spatial harmonizer may struggle with datasets having very sparse or unusual electrode configurations
- The model's performance on tasks requiring fine temporal resolution may be limited by the 2-second patch size needed for wavelet decomposition

## Confidence

- **High confidence:** Manifold transformer architecture works as described; SPD embedding pipeline is numerically stable; subject-agnostic encoder design enables generalization
- **Medium confidence:** GNN spatial harmonizer successfully learns spline interpolation for missing channels; Log-Euclidean Mean across frequency bands produces discriminative combined embeddings
- **Low confidence:** LOO contrastive loss meaningfully captures cross-frequency coupling for downstream tasks; claimed 10x parameter efficiency holds when accounting for training time

## Next Checks

1. **Training-time efficiency validation:** Measure and compare wall-clock training time per epoch for MENDR vs. Euclidean baselines with similar parameter counts to verify practical efficiency.

2. **Numerical stability audit:** Implement automated checks during training to verify SPD matrix eigenvalues remain well-conditioned and monitor SVD gradient magnitudes with adaptive clipping.

3. **Channel dropout ablation study:** Train a version of MENDR without channel dropout during autoencoder pretraining and test on ISRUC (6 channels) to determine if the GNN interpolation is load-bearing.