---
ver: rpa2
title: 'PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team
  Environments'
arxiv_id: '2509.06235'
source_url: https://arxiv.org/abs/2509.06235
tags:
- action
- cause
- effect
- team
- opponent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PillagerBench introduces a novel benchmark for evaluating LLM-based\
  \ multi-agent systems in competitive team-vs-team scenarios within Minecraft, addressing\
  \ the gap in existing benchmarks that focus primarily on cooperative tasks. The\
  \ framework features two competitive scenarios\u2014Mushroom War and Dash & Dine\u2014\
  that test task allocation and strategic adaptation under dynamic, adversarial conditions."
---

# PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments

## Quick Facts
- arXiv ID: 2509.06235
- Source URL: https://arxiv.org/abs/2509.06235
- Reference count: 36
- Key outcome: Novel benchmark for evaluating LLM-based multi-agent systems in competitive team-vs-team Minecraft scenarios

## Executive Summary
PillagerBench introduces a novel benchmark for evaluating LLM-based multi-agent systems in competitive team-vs-team scenarios within Minecraft, addressing the gap in existing benchmarks that focus primarily on cooperative tasks. The framework features two competitive scenarios—Mushroom War and Dash & Dine—that test task allocation and strategic adaptation under dynamic, adversarial conditions. To tackle these challenges, the authors propose TactiCrafter, an LLM-based multi-agent system with a Tactics Module for strategy generation, a Causal Model for learning game mechanics, and an Opponent Model for inferring enemy strategies. Experimental results show TactiCrafter outperforms both random and Chain-of-Thought baselines across multiple metrics, including points scored (13.05), sabotage (1.55), points difference (-1.16), and win rate (46%). The benchmark and code are open-sourced to foster further research in competitive multi-agent AI.

## Method Summary
PillagerBench is a competitive multi-agent benchmark built on Minecraft using Mineflayer API and Docker containers. The framework evaluates LLM-based agents in two team-vs-team scenarios: Mushroom War (harvest mushrooms while sabotaging opponents) and Dash & Dine (farm crops, craft food items, submit for points). TactiCrafter, the proposed system, consists of four modules: Tactics Module (generates/updates human-readable tactics), Causal Model (builds causal graph from inventory observations), Opponent Model (infers enemy strategies from chat), and Base Agents (execute tactics via iterative LLM prompting). Agents play 2-minute episodes with tactics updated between games using opponent history. Performance is measured across points scored, sabotage actions, points difference, and win rate.

## Key Results
- TactiCrafter achieves 13.05 average points, 1.55 sabotage actions, -1.16 points difference, and 46% win rate across scenarios
- Ablation studies show each module contributes significantly to performance, with GPT-4o providing optimal results
- Opponent modeling enables strategy adaptation, with higher win rates when facing familiar opponents versus novel ones
- Self-play causes overfitting, degrading performance against built-in opponents in Dash & Dine scenario

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical tactic generation enables coordinated multi-agent behavior under time pressure
- Mechanism: The Tactics Module generates human-readable, conditional action policies (≤6 line items) referencing agents by name in third person. Tactics are generated once pre-game (zero-shot CoT) and updated between episodes (few-shot CoT) using opponent tactics and prior events. This separation allows Base Agents to execute tactics in parallel without inter-agent communication overhead during gameplay.
- Core assumption: Natural language tactics can be consistently interpreted and executed by independent agents without real-time coordination failures.
- Evidence anchors:
  - [abstract]: "facilitates teamwork through human-readable tactics"
  - [Section IV-A]: Tactics are "a list of conditional actions between players... where players are referenced by name in third person"
  - [corpus]: Parallelized Planning-Acting paper addresses similar coordination challenges in LLM-based MAS
- Break condition: When agents misinterpret tactics or when tactics become stale due to rapid opponent adaptation within a single episode

### Mechanism 2
- Claim: Inventory-based causal discovery accumulates actionable game knowledge across episodes
- Mechanism: The Causal Model infers cause-effect relationships from chat messages and inventory states. When agents broadcast actions and inventory changes are observed, the LLM extracts 3-tuples (action, causes, effects). These accumulate into a causal graph that informs subsequent tactic generation. Deduplication reduces event logs by 28%, improving token efficiency.
- Core assumption: Inventory state changes reliably indicate causal relationships, and LLM can correctly infer causation from correlation in observed sequences.
- Evidence anchors:
  - [abstract]: "learns causal dependencies"
  - [Section IV-B]: "causal graph G is formatted... as a 3-tuple: an action, a list of its causes, and a list of its effects"
  - [corpus]: Weak direct corpus support; related work Adam [25] uses intervention-based causal discovery but TactiCrafter removes this for efficiency
- Break condition: When causal dependencies involve world state (block states, entity behaviors, spatial-temporal factors) invisible to inventory-only observation

### Mechanism 3
- Claim: Opponent modeling enables strategy adaptation that generalizes partially to novel opponents
- Mechanism: The Opponent Model extracts opposing team's chat messages and infers their tactics via few-shot CoT. Updated opponent tactics O inform the Tactics Module's counter-strategy. Evidence shows TactiCrafter achieves higher win rate (0.47 vs 0.38) and points difference (+1.53 vs -2.84) when facing the same opponent after 5 episodes versus a different opponent.
- Core assumption: Opponents reveal actionable strategic intent through observable chat messages, and counter-tactics transfer across opponent behavior patterns.
- Evidence anchors:
  - [abstract]: "adapts to opponent strategies"
  - [Table VII]: "Avg Same" shows D=+1.53, W=0.47 vs "Avg Different" D=-2.84, W=0.38
  - [corpus]: HLSMAC and Pokemon Tournament papers address high-level strategic adaptation in competitive settings
- Break condition: When opponents withhold chat or use deceptive signaling; when self-play causes overfitting to own strategy patterns

## Foundational Learning

- Concept: **Causal Graphical Models (DAGs)**
  - Why needed here: The Causal Model represents game mechanics as directed edges between variables (items, actions). Understanding that X→Y means "X causes Y" is essential for interpreting why the system builds this graph and where it fails (e.g., missing block-state dependencies).
  - Quick check question: Given observations [agent had wheat, agent crafted, agent now has bread], what causal edge should be added?

- Concept: **Zero-Sum Game Theory (Nash Equilibrium, Minimax)**
  - Why needed here: PillagerBench models team-vs-team as zero-sum (UA = PA - PB). Tactics generation implicitly seeks strategies robust to opponent responses. Understanding minimax helps interpret why opponent modeling matters.
  - Quick check question: If your team scores 10 points and opponent scores 7, what is your utility? What if opponent changes strategy to score 12?

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: All four modules use CoT (zero-shot for generation, few-shot for updates). The tactics, causal relations, opponent inferences, and action code all depend on LLMs reasoning step-by-step before outputting structured results.
  - Quick check question: What is the difference between zero-shot and few-shot CoT, and which does the Tactics Module use for initial vs. updated tactics?

## Architecture Onboarding

- Component map:
  PillagerBench Core -> Benchmark Module (YAML configs via Hydra, Docker containers) -> Spawns Minecraft Server + Mineflayer processes (one per agent)
  PillagerBench Core -> Bridge Module (Express.js) -> Translates between LLM outputs and Mineflayer API calls
  PillagerBench Core -> Environment Module -> State observations (chat, inventory, nearby blocks/entities)
  TactiCrafter -> Tactics Module -> generates/updates tactics T
  TactiCrafter -> Causal Model -> maintains causal graph G
  TactiCrafter -> Opponent Model -> maintains opponent tactics hypothesis O
  TactiCrafter -> Base Agents (2×) -> execute tactics, generate action code, self-critique

- Critical path:
  1. Pre-game: Tactics Module generates initial T from scenario description + initial G
  2. Game: Base Agents iteratively (generate code → execute → critique) until episode ends; log events
  3. Post-game: Causal Model updates G from event logs; Opponent Model updates O from opponent chat; Tactics Module updates T using G, O, and H (history)
  4. Persist: Multi-agent system object retains G, O, and T across episodes for continuous learning

- Design tradeoffs:
  - **Causal discovery without interventions**: Faster than Adam [25] but may miss non-inventory dependencies (block states, entity behaviors)
  - **Parallel Base Agents**: No real-time inter-agent communication; relies on shared tactics for coordination
  - **Event deduplication**: Saves 28% tokens but may discard repeated-failure diagnostic signals
  - **2-minute episodes**: Forces time-pressure decisions but limits complexity of discoverable causal chains

- Failure signatures:
  - **Stale tactics**: Win rate drops when opponent changes strategy mid-episode (tactics only update between episodes)
  - **Missing causal edges**: Actions fail due to unmodeled prerequisites (e.g., "need bucket for milk" not in graph)
  - **Opponent overfitting**: Self-play performance improves but generalization to built-in opponents degrades (Dash & Dine: +3 → -5.2 points difference over 20 self-play episodes)
  - **Execution errors in loops**: Infinite action loops generate duplicate event logs; agent stalls while LLM regenerates code

- First 3 experiments:
  1. **Baseline sanity check**: Run Random and CoT baselines against `do nothing` opponent in Mushroom War. Confirm Random ≈0 win rate, CoT shows non-trivial points. Verifies benchmark installation.
  2. **Module ablation**: Run TactiCrafter full vs. without Causal Model vs. without Opponent Model on both scenarios. Compare P, S, D, W metrics. Expect: without causal → lower D; without opponent → lower S.
  3. **Adaptation test**: Train TactiCrafter for 5 episodes against `aggressive` opponent in Mushroom War, then evaluate episode 6 against `aggressive` vs. `passive`. Measure points difference to confirm specialization effect (Table VII pattern).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agents maintain adaptability while preventing detrimental overspecialization during self-play?
- Basis in paper: [explicit] Section V.D states, "This raises an important question: how can we maintain adaptability while preventing detrimental overspecialization?"
- Why unresolved: Self-play causes TactiCrafter to overfit to its own strategies, resulting in worse performance when facing built-in opponents with different strategies.
- What evidence would resolve it: A method that successfully decouples general world knowledge from opponent-specific specialization, maintaining win rates against novel opponents after self-play training.

### Open Question 2
- Question: How can causal discovery methods be extended to incorporate spatial reasoning and temporal environmental states?
- Basis in paper: [inferred] Section V.E notes the current Causal Model relies solely on inventory states, failing to represent block states, entity behaviors, or spatial-temporal dependencies.
- Why unresolved: The current text-based causal graph cannot model complex dynamics like crop growth cycles or entity locations, limiting the agent's ability to reason about the full game state.
- What evidence would resolve it: A modified causal architecture that integrates spatial coordinates and time-steps into the graph, improving planning accuracy in the Dash & Dine scenario.

### Open Question 3
- Question: What is the optimal trade-off between LLM reasoning depth and real-time action execution speed?
- Basis in paper: [inferred] Section V.D and Table V highlight that reasoning models (e.g., o3-mini) suffer from long response times ($T_{resp}$), causing agents to idle during generation in real-time environments.
- Why unresolved: While deeper reasoning might yield better strategies, the incurred latency reduces the number of executable actions within the 2-minute time limit, creating a performance trade-off that remains unoptimized.
- What evidence would resolve it: A study measuring the Pareto frontier of response latency versus reasoning quality, identifying the optimal model configuration for maximizing points per second.

## Limitations

- **Prompt Template Dependency**: The benchmark's validity critically depends on the exact LLM prompts (p_a through p_h), which are referenced but not disclosed in the paper. Different prompt engineering could significantly alter agent performance, making fair comparison difficult without access to these templates.
- **Causal Discovery Scope**: The Causal Model only infers dependencies from inventory states and chat messages, potentially missing crucial game mechanics involving block states, entity behaviors, and spatial-temporal factors that cannot be observed through this limited view.
- **Self-Play Overfitting**: While Table VII shows TactiCrafter improves against the same opponent over 5 episodes, it demonstrates negative transfer when facing new opponents, with Dash & Dine performance degrading by 5.2 points after 20 self-play episodes.

## Confidence

- **High Confidence**: The benchmark framework structure (Mushroom War and Dash & Dine scenarios), the four-module architecture of TactiCrafter, and the reported baseline performance differences (CoT vs. Random) are well-supported by experimental results and code availability.
- **Medium Confidence**: The causal discovery mechanism's effectiveness and the opponent modeling's generalization capabilities are supported by ablation studies and Table VII, but the limited scope of observable variables and potential for overfitting create uncertainty about real-world applicability.
- **Low Confidence**: The exact quantitative impact of individual modules (especially Opponent Model in Dash & Dine) and the scalability of the approach to more complex scenarios remain unclear without access to full experimental details and prompt templates.

## Next Checks

1. **Prompt Template Verification**: Request and test the exact prompt templates (p_a through p_h) from the authors to verify that reproduced results match reported metrics within 10% tolerance. This is critical since prompt engineering significantly influences LLM agent performance.

2. **Causal Model Completeness Test**: Design a controlled experiment in Mushroom War where inventory-only observations are insufficient (e.g., requiring block placement knowledge). Measure whether TactiCrafter fails to discover necessary causal dependencies that a human would recognize, quantifying the gap between observed and complete causal knowledge.

3. **Opponent Generalization Stress Test**: Run TactiCrafter for 10 episodes against a single opponent type, then test against all opponent types. Compare performance degradation against the "Avg Different" baseline from Table VII to determine if the system exhibits catastrophic forgetting or maintains strategic flexibility.