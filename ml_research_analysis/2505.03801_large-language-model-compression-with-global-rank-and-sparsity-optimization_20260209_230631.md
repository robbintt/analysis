---
ver: rpa2
title: Large Language Model Compression with Global Rank and Sparsity Optimization
arxiv_id: '2505.03801'
source_url: https://arxiv.org/abs/2505.03801
tags:
- pruning
- sparse
- low-rank
- rpca
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of compressing large language
  models (LLMs) by combining low-rank and sparse approximations. The method, called
  CAP, uses Robust Principal Component Analysis (RPCA) to decompose weight matrices
  into low-rank and sparse components, then applies a probabilistic global optimization
  technique based on Bernoulli sampling and policy gradient to jointly identify important
  parameters in both components.
---

# Large Language Model Compression with Global Rank and Sparsity Optimization

## Quick Facts
- arXiv ID: 2505.03801
- Source URL: https://arxiv.org/abs/2505.03801
- Reference count: 23
- Method outperforms state-of-the-art sparsification techniques at 50% compression on LLaMA models

## Executive Summary
This paper introduces CAP, a novel approach for compressing large language models that combines low-rank and sparse approximations through Robust Principal Component Analysis (RPCA). The method decomposes weight matrices into low-rank and sparse components, then uses a probabilistic global optimization technique to jointly identify important parameters in both components. CAP automatically detects layer-wise redundancy and manages interactions between sparse and low-rank parts without requiring manual thresholds or extensive fine-tuning. Experiments on LLaMA and LLaMA-2 models demonstrate superior performance compared to state-of-the-art sparsification methods while reducing computational overhead compared to iterative pruning approaches.

## Method Summary
CAP employs a two-stage compression framework. First, it uses Robust Principal Component Analysis (RPCA) to decompose each weight matrix into low-rank and sparse components, capturing global and local redundancy respectively. Second, it applies a probabilistic global optimization technique based on Bernoulli sampling and policy gradient to jointly identify important parameters across both components. This global approach eliminates the need for iterative pruning and manual threshold selection, automatically adapting to the specific redundancy patterns in each layer. The method operates at 50% compression ratios and achieves better perplexity and zero-shot accuracy compared to existing techniques like HASSLE-free and Pangu-Σ.

## Key Results
- Achieves superior perplexity and zero-shot accuracy compared to state-of-the-art sparsification methods at 50% compression
- Outperforms HASSLE-free and Pangu-Σ on LLaMA and LLaMA-2 models with consistent improvements across evaluation tasks
- Reduces computational overhead compared to iterative pruning methods by avoiding multiple training cycles

## Why This Works (Mechanism)
The method works by decomposing weight matrices into complementary components that capture different types of redundancy. RPCA separates global redundancy (captured by the low-rank component) from local redundancy (captured by the sparse component). The global optimization then jointly prunes parameters from both components using a probabilistic approach, allowing the method to automatically discover the optimal trade-off between sparsity and low-rank structure for each layer. This eliminates the need for manual threshold tuning and iterative pruning cycles that characterize traditional methods.

## Foundational Learning

**Robust Principal Component Analysis (RPCA)**: Decomposes a matrix into low-rank and sparse components. Why needed: Separates global from local redundancy patterns in weight matrices. Quick check: Verify decomposition recovers original matrix when combined.

**Policy Gradient Optimization**: A reinforcement learning technique for optimizing parameterized policies. Why needed: Enables global optimization across low-rank and sparse components simultaneously. Quick check: Confirm gradient estimates converge during training.

**Bernoulli Sampling**: Probabilistic sampling technique for parameter selection. Why needed: Provides differentiable approximation for discrete pruning decisions. Quick check: Verify sampling probabilities match expected distributions.

**Layer-wise Redundancy Detection**: Automatic identification of redundant parameters in neural network layers. Why needed: Adapts compression strategy to specific layer characteristics without manual intervention. Quick check: Compare redundancy patterns across different model layers.

## Architecture Onboarding

**Component Map**: Input Model → RPCA Decomposition → Joint Parameter Selection → Compressed Model

**Critical Path**: The global optimization phase that jointly prunes parameters from both low-rank and sparse components is the critical path, as it determines the final compressed model architecture and performance.

**Design Tradeoffs**: 
- Rank selection (k=1) vs model fidelity: Lower ranks provide better compression but may lose important information
- Sparsity level (0.4) vs accuracy: Higher sparsity reduces computational cost but may hurt performance
- Global vs local optimization: Global approach avoids iterative cycles but may be computationally expensive

**Failure Signatures**: 
- Poor RPCA decomposition indicates the matrix structure doesn't cleanly separate into low-rank and sparse components
- Suboptimal parameter selection suggests the global optimization hasn't converged properly
- Performance degradation beyond expected levels may indicate insufficient rank or excessive sparsity

**First Experiments**:
1. Verify RPCA decomposition recovers original weight matrices when combining low-rank and sparse components
2. Test global optimization convergence on synthetic matrices with known redundancy patterns
3. Compare compressed model performance against baseline iterative pruning methods on small transformer blocks

## Open Questions the Paper Calls Out
None

## Limitations
- Limited comparison with recent pruning methods like SparseGPT and DSTAR that could challenge the claimed performance advantages
- Computational overhead of global optimization with policy gradient not quantified against iterative pruning methods
- Fixed parameter choices (rank k=1, sparsity 0.4) without sensitivity analysis or ablation studies
- Evaluation focused on perplexity and zero-shot accuracy without detailed per-task performance breakdowns

## Confidence

**High confidence**: The technical framework combining RPCA with probabilistic global optimization is mathematically sound and the core algorithmic approach is clearly articulated

**Medium confidence**: The reported performance improvements over specific baselines (HASSLE-free, Pangu-Σ) at 50% compression are credible given the methodology, though the magnitude of improvement relative to newer methods is uncertain

**Low confidence**: Claims about computational efficiency compared to iterative pruning lack quantitative validation, and the generalization to different model scales and architectures remains unproven

## Next Checks
1. Conduct comprehensive ablation study comparing CAP against all major baselines (including SparseGPT and DSTAR) across multiple sparsity levels (0.2, 0.4, 0.6) and different model architectures (LLaMA-2 7B, 13B, 70B) to verify consistent performance advantages

2. Measure and report actual training time and computational overhead of global optimization phase compared to standard iterative pruning methods, including wall-clock time and GPU hours for both compression and inference phases

3. Perform robustness testing on models with corrupted or noisy weight matrices to evaluate CAP's performance when RPCA decomposition doesn't cleanly separate components, and assess sensitivity to parameter choices (rank k, sparsity threshold) through systematic parameter sweeps