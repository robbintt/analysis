---
ver: rpa2
title: Developing and Utilizing a Large-Scale Cantonese Dataset for Multi-Tasking
  in Large Language Models
arxiv_id: '2503.03702'
source_url: https://arxiv.org/abs/2503.03702
tags:
- cantonese
- data
- language
- shot
- yuetung-7b
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work constructs a high-quality Cantonese corpus exceeding
  2 billion tokens, collected from diverse sources and rigorously processed through
  language filtering, quality filtering, content filtering, and deduplication. The
  corpus is used to train a Cantonese language model (YueTung) via pre-training and
  supervised fine-tuning (SFT) on curated Cantonese tasks.
---

# Developing and Utilizing a Large-Scale Cantonese Dataset for Multi-Tasking in Large Language Models

## Quick Facts
- arXiv ID: 2503.03702
- Source URL: https://arxiv.org/abs/2503.03702
- Reference count: 29
- Constructed a 2B+ token Cantonese corpus and trained YueTung-7b to achieve SOTA on Cantonese benchmarks with cross-lingual transfer to English and Standard Chinese

## Executive Summary
This work addresses the scarcity of high-quality Cantonese data for training large language models by constructing YueData, a 2+ billion token corpus sourced from Hong Kong-specific online forums, news outlets, and web crawls. The corpus undergoes rigorous multi-stage filtering to ensure language purity, content quality, and absence of toxic material. Using this corpus, the authors pre-train and fine-tune a Qwen-2.5-7B base model (YueTung), achieving state-of-the-art performance on four Cantonese benchmarks. Notably, the model also demonstrates improved performance on mainstream English and Standard Chinese tasks, suggesting effective cross-lingual knowledge transfer.

## Method Summary
The methodology centers on constructing YueData through multi-source collection (LIHKG, OpenRice, Apple Daily, Common Crawl, etc.) followed by sequential filtering: language identification (Fast-Langid) to isolate Cantonese text, quality filtering using heuristic rules from Gopher (symbol-to-word ratio, word count limits, repeated n-grams), content filtering with Jigsaw toxicity classifiers and PII masking, and deduplication using MinHash + LSH with 0.5-0.6 thresholds. The filtered corpus (~2B tokens pre-training, ~1.3B tokens SFT) is used to train Qwen-2.5-7B with modified AdamW (reduced β2 for noisy data) and LoRA for efficiency. Supervised fine-tuning employs curated Cantonese tasks and translated Chinese data, with evaluation on Yue-Benchmarks and cross-lingual transfer to English/Chinese tasks.

## Key Results
- YueTung achieves SOTA performance on four Cantonese benchmarks (Yue-TruthfulQA, Yue-GSM8K, Yue-ARC-C, Yue-MMLU)
- Model demonstrates strong proficiency in context understanding, reasoning, and knowledge retrieval
- Cross-lingual transfer yields improved performance on mainstream English and Standard Chinese benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage data filtering improves corpus quality for low-resource language training.
- Mechanism: Sequential filtering removes non-Cantonese text, low-quality content, toxic material, and duplicates using Fast-Langid, heuristic rules, Jigsaw classifiers, and MinHash+LSH.
- Core assumption: Heuristic thresholds from Gopher transfer to Cantonese text characteristics.
- Evidence anchors: Corpus construction section describes rigorous processing through four filtering stages; low-resource NMT work confirms Cantonese differs significantly from standard Chinese.
- Break condition: Aggressive filtering may over-exclude valid colloquial Cantonese, especially code-switched content.

### Mechanism 2
- Claim: Modified AdamW with reduced β2 accelerates convergence on noisy low-resource corpora.
- Mechanism: Lowering β2 increases optimizer sensitivity to recent gradients, allowing faster adaptation when noisy entries appear in pre-training data.
- Core assumption: Noisy entries are distributed such that recent gradients provide cleaner signal than accumulated history.
- Evidence anchors: Paper states decreased β2 parameter for noisy data; training completed in ~3 weeks on 8x A100-80G.
- Break condition: If noise is systematic rather than random, reduced β2 may amplify biased gradients.

### Mechanism 3
- Claim: High-quality Cantonese pre-training induces cross-lingual transfer to related languages.
- Mechanism: Shared character sets, code-switching patterns, and transferable reasoning/knowledge representations enable performance gains on non-Cantonese benchmarks.
- Core assumption: Linguistic overlap and shared knowledge domains between Cantonese, Mandarin, and English enable positive transfer.
- Evidence anchors: Abstract notes improved performance on mainstream language tasks; YueTung-7b achieves 92.63%/94.49% on CMMLU outperforming Qwen-2.5-72b and GPT-4.
- Break condition: Transfer may not generalize to languages without character overlap or code-switching prevalence.

## Foundational Learning

- Concept: **MinHash + LSH for deduplication**
  - Why needed here: Cantonese data spans multiple sources with high redundancy. MinHash approximates Jaccard similarity; LSH enables sub-linear nearest-neighbor search.
  - Quick check question: Given documents A and B with MinHash signatures, how do you estimate their Jaccard similarity?

- Concept: **Language identification (FastText-based)**
  - Why needed here: Cantonese text often mixes with Mandarin and English; Fast-Langid distinguishes zh-yue from zh-hant and other languages.
  - Quick check question: Why might a FastText-based LID model struggle with code-switched Cantonese-English text?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Full fine-tuning of 7B model is computationally expensive; LoRA injects trainable low-rank matrices, freezing base weights.
  - Quick check question: If LoRA rank is 8 and hidden dimension is 4096, how many trainable parameters does a single LoRA adapter add?

## Architecture Onboarding

- Component map:
Raw Sources (LIHKG, OpenRice, Wikipedia, Common Crawl, etc.)
    → Language Filter (Fast-Langid)
    → Quality Filter (heuristic rules)
    → Content Filter (Jigsaw classifiers + PII regex)
    → Deduplication (MinHash + LSH)
    → YueData (~2B tokens pre-training, ~1.3B tokens SFT)
    → Qwen-2.5-7b base
    → Pre-training (modified AdamW, LoRA)
    → SFT (curated Cantonese tasks + translated Chinese data)
    → YueTung-7b

- Critical path: Data processing pipeline (especially language filtering and deduplication) determines corpus quality; training succeeds or fails on data quality. SFT translation quality from Chinese sources is a secondary bottleneck.

- Design tradeoffs:
  - Deduplication threshold 0.6 vs 0.5: Higher threshold retains more data but includes near-duplicates; paper chose 0.6 for Common Crawl (44% removal vs 78% at 0.5).
  - Heuristic vs model-based quality filtering: Paper uses Gopher rules instead of LLaMA-based classifiers to avoid over-filtering.
  - Translation vs native Cantonese SFT data: Translation from Chinese provides scale but may introduce artifacts; paper uses Llama-3.1-70b for translation with secondary validation.

- Failure signatures:
  - Model outputs Mandarin characters/phrases → language filtering failed or insufficient Cantonese pre-training signal.
  - Toxic or NSFW outputs → content filtering thresholds too permissive (paper uses 1e-2 and 1e-4).
  - Performance degradation on Standard Chinese/English → excessive Cantonese specialization, possible catastrophic forgetting.

- First 3 experiments:
  1. Validate language filtering: Sample 1000 documents from each source, manually verify Cantonese purity; compute zh-yue vs zh-hant confusion rate.
  2. Ablate deduplication threshold: Train with 0.5 vs 0.6 threshold on held-out subset; measure benchmark delta.
  3. Test cross-lingual transfer: Zero-shot evaluate YueTung on CantoNLU benchmark tasks (not used in training) to assess generalization.

## Open Questions the Paper Calls Out
- **Question**: To what extent does the predominance of Hong Kong-specific sources in YueData affect the model's ability to generalize to other Cantonese dialects, such as those spoken in Guangzhou or among the Malaysian diaspora?
- **Question**: By what specific mechanism does training on the YueData corpus lead to improved performance on mainstream English and Standard Chinese benchmarks, rather than resulting in catastrophic forgetting or performance degradation?
- **Question**: Can the integration of rigorous Automatic Speech Recognition (ASR) transcriptions, which were excluded due to quality and fidelity concerns, significantly enhance the model's proficiency in spoken colloquial Cantonese?

## Limitations
- The corpus predominantly comprises text from Hong Kong-specific sources, potentially limiting generalizability to other Cantonese dialects.
- Automated translation from Chinese to Cantonese for SFT data may introduce artifacts that don't capture authentic colloquial usage.
- Cross-lingual transfer improvements lack ablation studies to confirm causation versus base model architecture effects.

## Confidence
- **High Confidence**: Corpus construction methodology is well-specified and reproducible; SOTA claims on Cantonese benchmarks are supported by direct comparisons.
- **Medium Confidence**: Cross-lingual transfer claims are supported by benchmark results but lack rigorous ablation to confirm causation.
- **Low Confidence**: Real-world applicability remains unproven without user studies or deployment validation beyond synthetic benchmarks.

## Next Checks
1. **Ablation Study on β2 Modification**: Train two identical models with standard AdamW (β2=0.999) versus modified AdamW (β2<0.999) on the same Cantonese corpus subset to isolate optimizer impact.
2. **Cross-Lingual Transfer Isolation**: Fine-tune base Qwen-2.5-7B directly on Chinese SFT data without Cantonese pre-training, then compare performance on English/Chinese benchmarks.
3. **Real-World Deployment Evaluation**: Deploy YueTung in controlled user study with native Cantonese speakers completing practical tasks to validate real-world utility beyond benchmark performance.