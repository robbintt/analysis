---
ver: rpa2
title: 'AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling
  for Edge DNN Training'
arxiv_id: '2508.16647'
source_url: https://arxiv.org/abs/2508.16647
tags:
- dataset
- training
- adapsne
- accuracy
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of training deep neural networks
  (DNNs) on resource-constrained edge devices, where conventional training methods
  impose prohibitive computational overhead due to the large scale of datasets. The
  authors propose AdapSNE, a DNN-free dataset sampling framework that leverages Fireworks
  Algorithm (FWA)-optimized t-SNE for dimensionality reduction and entropy-guided
  optimization to ensure uniform sampling, thereby addressing issues of outliers and
  representative bias in the state-of-the-art NMS method.
---

# AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling for Edge DNN Training

## Quick Facts
- arXiv ID: 2508.16647
- Source URL: https://arxiv.org/abs/2508.16647
- Reference count: 40
- Key outcome: AdapSNE achieves 4.4%–14.4% accuracy improvements on image datasets and 2.1%–4.1% on LLM benchmarks compared to state-of-the-art methods.

## Executive Summary
AdapSNE addresses the challenge of training deep neural networks on resource-constrained edge devices by developing a DNN-free dataset sampling framework. The method combines Fireworks Algorithm-optimized t-SNE for dimensionality reduction with entropy-guided optimization to ensure uniform sampling, addressing outliers and representative bias issues in existing methods. A custom hardware accelerator with dataflow optimization and time-multiplexing is designed to minimize energy and area overhead, achieving significant accuracy improvements across various datasets and keeping ratios.

## Method Summary
AdapSNE proposes a three-component approach for efficient edge DNN training. First, it replaces binary search with Fireworks Algorithm (FWA) for t-SNE perplexity optimization, addressing the non-monotonic nature of the perplexity error function. Second, it implements an entropy-guided feedback loop that maximizes the Shannon entropy of the low-dimensional grid distribution to ensure uniform sampling and mitigate representative bias. Third, it designs a custom hardware accelerator with tailored dataflow and time-multiplexing to efficiently execute the iterative FWA and entropy algorithms, reducing computational overhead on edge devices.

## Key Results
- Outperforms both DNN-based (DQAS) and DNN-free (NMS) methods by 4.4%–14.4% on image datasets
- Achieves 2.1%–4.1% improvements on LLM benchmarks across varying dataset scales
- Maintains accuracy gains across different keeping ratios (10%, 20%, 30%)
- Hardware accelerator reduces energy consumption while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1: Outlier Suppression via Non-Monotonic Search
The Fireworks Algorithm (FWA) addresses the fundamental limitation of binary search in t-SNE perplexity optimization. Binary search assumes the perplexity error function is monotonic, but the authors prove it is inherently non-monotonic with derivatives that change signs. This causes binary search to converge to incorrect bandwidth parameters (σi), projecting data points as outliers. FWA, as a population-based global optimizer, explores the search space through "explosions" and "mutations," effectively navigating the non-monotonic landscape to find true optimal σi values.

### Mechanism 2: Uniform Sampling via Entropy Feedback
AdapSNE implements a closed-loop control system using Shannon entropy to ensure representative sampling. Previous methods selected target perplexity empirically, often leading to uneven data density (clumping) in the low-dimensional space. The system calculates grid-based entropy of the projected distribution. If entropy is low (indicating clumping), it adjusts the target perplexity (Πt) using Newton's method and recomputes the projection. This forces samples to spread out across all regions of the manifold, ensuring comprehensive coverage of the data structure.

### Mechanism 3: Energy Reduction via Time-Multiplexed Dataflow
The custom hardware accelerator minimizes energy and area overhead through direct hardware mapping of FWA operations. The design eliminates instruction fetch overhead by mapping Explosion, Mutation, and Selection operations directly into hardware blocks (SPK RAM, MUT RAM, Evaluate Module). For entropy calculation, time-multiplexing reuses arithmetic units across different grid cells, reducing silicon area while maintaining throughput. This architecture avoids the inefficiencies of general-purpose CPUs for the iterative, random nature of FWA operations.

## Foundational Learning

- **Concept:** **t-SNE (t-Distributed Stochastic Neighbor Embedding)**
  - Why needed here: Core algorithm being fixed; understanding how perplexity defines "neighbors" explains why incorrect perplexity causes outliers
  - Quick check question: How does the perplexity parameter change the effective number of neighbors considered when calculating conditional probabilities?

- **Concept:** **Evolutionary Computation (Fireworks Algorithm)**
  - Why needed here: FWA is the proposed solution to the search problem; distinguishes "Explosion" (local search) from "Mutation" (global exploration)
  - Quick check question: In FWA, does a "good" firework generate more sparks or fewer sparks, and do they travel a larger or smaller amplitude?

- **Concept:** **Shannon Entropy**
  - Why needed here: Optimization metric; maximizing entropy corresponds to maximizing uncertainty/uniformity
  - Quick check question: If one grid cell contains 99% of the data points, is the entropy high or low?

## Architecture Onboarding

- **Component map:** FWA Engine (RAND -> SPK RAM -> MUT RAM -> Evaluate Module) -> t-SNE projection -> Entropy Engine (Reconfigurable Unit -> Shared RAM) -> Control Logic (adjusts Πt)

- **Critical path:** The Perplexity Search Loop. FWA must find σi for every data point xi:
  1. Initialize fireworks for σ
  2. Generate sparks (candidates) and mutate
  3. Evaluate fitness (|Ri(σi) - Πt|)
  4. Select best σ*
  5. If entropy is low, update Πt and restart FWA search

- **Design tradeoffs:**
  - Parallelism vs. Area: 4-way parallel computation for entropy with time-multiplexing elsewhere; increasing parallelism reduces latency but exponentially increases area
  - Accuracy vs. Iterations: Threshold α=80% sets "evenness" bar; higher α means better sampling but more re-computation loops (energy)

- **Failure signatures:**
  - Empty Sampling: Accuracy degrades despite FWA; check entropy threshold (H0); if too high, loop may never converge or forced uniformity distorts data structure
  - Stuck Search: FWA returns same σ repeatedly; check RAND module seeding or Explosion Amplitude constraints

- **First 3 experiments:**
  1. Validate Non-monotonicity: Run binary search vs. FWA on synthetic perplexity error function to observe convergence failures
  2. Hardware Profiling: Synthesize Verilog RTL to measure power/area breakdown of SPK RAM vs. Evaluate Module
  3. Ablation on Entropy: Run AdapSNE with entropy feedback disabled vs. enabled to quantify accuracy delta on CIFAR-10

## Open Questions the Paper Calls Out

### Open Question 1
How sensitive is AdapSNE performance to the entropy threshold factor (α=80%) when applied to datasets with non-uniform underlying class distributions? The authors select this threshold based on a single external citation rather than adaptive calculation or ablation study. Evidence would require varying α (0.5 to 0.95) on imbalanced datasets to observe impact on accuracy and convergence speed.

### Open Question 2
Does Fireworks Algorithm provide sufficient theoretical convergence guarantee for the perplexity error function compared to other non-monotonic optimization methods? While the paper proves non-monotonicity, it doesn't rigorously prove FWA is optimal, only that it outperforms empirically. Evidence would include theoretical analysis of FWA's convergence rate on this specific landscape or comparison against other global optimization heuristics.

### Open Question 3
Can the hardware accelerator maintain efficiency if dimensionality reduction target requires grid size g exceeding on-chip memory limits? The design demonstrates results on specific datasets, but scalability analysis against increasing grid sizes (64×64 vs 256×256 vs 1024×1024) is needed to verify linear or sub-linear scaling of energy consumption.

## Limitations
- Critical hyperparameters (FWA population size, spark count, explosion amplitude, max iterations) are unspecified
- Grid dimensions for entropy calculation are undefined, affecting threshold calculation
- Initial step size for perplexity updates in adaptive loop is missing
- Hardware efficiency claims depend on unspecified dataset characteristics and grid sizes

## Confidence
- **High Confidence:** Core architectural insight (FWA for non-monotonic search, entropy feedback loop) is well-explained and theoretically sound
- **Medium Confidence:** Hardware acceleration design is detailed in dataflow and modules, but performance gains depend heavily on unspecified hyperparameters
- **Low Confidence:** Exact numerical results cannot be independently verified without missing hyperparameters and grid specifications

## Next Checks
1. Validate Non-monotonicity: Implement binary search vs. FWA on synthetic perplexity error function to demonstrate convergence failure empirically
2. Ablation Study on Entropy: Reproduce AdapSNE with entropy feedback loop disabled vs. enabled to isolate accuracy contribution on CIFAR-10
3. Hardware Synthesis: Synthesize provided Verilog RTL for FWA and Entropy engines to measure actual power, area, and latency breakdown