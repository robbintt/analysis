---
ver: rpa2
title: 'Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous
  Driving VQA from Peru'
arxiv_id: '2503.07587'
source_url: https://arxiv.org/abs/2503.07587
tags:
- questions
- each
- driving
- vlms
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the Robusto-1 dataset to evaluate the cognitive
  alignment of vision-language models (VLMs) and humans on out-of-distribution driving
  scenarios from Peru. Using Visual Question Answering (VQA) and Representational
  Similarity Analysis (RSA), the researchers compared human and machine responses
  to 105 questions across 7 video clips.
---

# Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous Driving VQA from Peru

## Quick Facts
- **arXiv ID:** 2503.07587
- **Source URL:** https://arxiv.org/abs/2503.07587
- **Reference count:** 40
- **Primary result:** VLMs exhibit high internal similarity regardless of question type, while humans diverge significantly on counterfactual and hypothetical questions.

## Executive Summary
This study introduces the Robusto-1 dataset to evaluate the cognitive alignment of vision-language models (VLMs) and humans on out-of-distribution driving scenarios from Peru. Using Visual Question Answering (VQA) and Representational Similarity Analysis (RSA), the researchers compared human and machine responses to 105 questions across 7 video clips. Results show that VLMs exhibit high internal similarity regardless of question type, while humans diverge significantly on counterfactual and hypothetical questions. Cross-system analysis reveals mild alignment between VLMs and humans, particularly in multiple-choice questions, but notable differences in open-ended and complex reasoning tasks. The findings highlight the limitations of current VLMs in handling nuanced, real-world driving scenarios and underscore the need for further research into their cognitive alignment with human drivers.

## Method Summary
The researchers evaluated cognitive alignment between humans and VLMs using the Robusto-1 dataset of Peruvian dashcam videos. They designed three blocks of 105 questions (Variable, Multiple Choice, Counterfactual/Hypothetical) and collected responses from 9 human subjects and 6 VLMs. Responses were embedded using sentence-transformers, Gramian matrices were computed, and RSA correlations were calculated between all system pairs. The analysis included L2 distance-to-median metrics and PCA visualizations to compare response patterns across different question types and systems.

## Key Results
- VLMs show high internal similarity (r ≈ 0.8-0.9) regardless of question type or architecture
- Humans diverge significantly on counterfactual/hypothetical questions compared to variable and multiple-choice questions
- Cross-system alignment is highest for multiple-choice questions, with mild correlation between humans and VLMs
- PCA visualizations show apparent overlap but RSA reveals low actual alignment for complex reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
Representational Similarity Analysis (RSA) enables dimensionality-agnostic comparison of human and VLM cognitive responses. RSA constructs Gramian matrices from embedded answer vectors, then correlates upper triangular portions across systems. This bypasses incompatibility in raw feature dimensions (e.g., human answer length vs. VLM internal representations) by comparing response topographies rather than direct outputs. Core assumption: Sentence embeddings preserve semantic similarity well enough that correlation of Gramian upper triangles reflects meaningful cognitive alignment.

### Mechanism 2
Question type modulates human-VLM alignment, with constrained formats yielding higher agreement. Multiple-choice questions have low answer-space dimensionality (Yes/No, 1-10 scales, predefined intervals), reducing variance in valid responses. Open-ended counterfactual/hypothetical questions require complex reasoning with no ground truth, exposing deeper cognitive divergence. Core assumption: The observed alignment differences reflect genuine cognitive processing gaps rather than artifacts of embedding or response format.

### Mechanism 3
VLMs exhibit convergent response patterns regardless of architecture or training origin, while humans show individual divergence on complex reasoning. The paper hypothesizes that "the curse of training with the same large pool of data: The Internet" causes VLMs to develop similar representational structures despite different architectures. Humans, conversely, bring diverse experiential and cultural priors to counterfactual reasoning. Core assumption: High VLM inter-model correlation indicates shared training data and/or Transformer architecture effects, not necessarily superior cognitive alignment.

## Foundational Learning

- **Representational Similarity Analysis (RSA)**
  - Why needed here: This paper's core methodology; understanding how to compare systems with incompatible internal dimensions is essential to interpreting results.
  - Quick check question: Given two systems with feature dimensions 100 and 1000 exposed to the same 50 images, how would you compute their similarity using RSA?

- **Counterfactual vs. Hypothetical Reasoning**
  - Why needed here: Block 3 questions explicitly test these; understanding the distinction (counterfactual = conditioned on known outcome, hypothetical = future-focused) clarifies why they probe different cognitive capacities.
  - Quick check question: "What would have caused this crash?" vs. "What would you do if a pedestrian appeared?" — which is counterfactual and which is hypothetical?

- **Out-of-Distribution (OOD) Generalization**
  - Why needed here: The entire dataset is designed around OOD scenarios (Peruvian driving conditions rarely seen in training data); evaluating VLM robustness requires understanding why OOD matters.
  - Quick check question: Why might a VLM trained on US/European dashcam data fail on "taxis that are tuk-tuks" or "street dogs as common obstacles"?

## Architecture Onboarding

- **Component map**: Video frames extraction -> Frame rate adaptation per model -> Prompt formatting per API -> VLM inference -> Response curation -> Sentence embedding -> Gramian computation -> RSA correlation + distance-to-median + PCA visualization

- **Critical path**: 1) Frame extraction (handle varying model constraints: Pixtral max 6 frames, Llama max 3 frames, others ~50 at 10fps) 2) Prompt formatting per model API (binary video for CogVLM, base64 frames for Pixtral/Gemini, etc.) 3) Response curation (31.75% of VLM responses required formatting fixes; 1.44% discarded) 4) Embedding and RSA computation (tested with all-mpnet, paraphrase-mpnet, e5-large for robustness)

- **Design tradeoffs**: Frame rate vs. model compatibility (higher fps provides more temporal info but some models can't process it); Pooled vs. single responses (paper pools multiple VLM responses per question; single responses show similar patterns but more variance); Language (questions in English despite Peruvian participants, all confirmed fluent; future work should test language-provenance interactions)

- **Failure signatures**: VLMs produce format-incompatible answers (e.g., "Option: 2 to 4" when valid ranges are "1-3, 4-6"); DeepSeek-V3 only performs OCR on images, not true visual understanding (confirmed via filename-manipulation test); PCA can be misleading: Block 3 shows apparent overlap in 2D projection but RSA reveals low actual alignment

- **First 3 experiments**: 1) Replicate RSA on the 7 held-out videos with a single new VLM (e.g., GPT-4V) to verify convergence pattern holds; expect high correlation with existing VLM cluster 2) Add English-speaking non-Peruvian participants to test whether human divergence on Block 3 is cultural or universal; if cultural, Peru-specific experience drives counterfactual reasoning 3) Expand to all 200 videos with Block 2 questions only (lowest computational cost, highest human-VLM alignment) to establish baseline alignment distribution before tackling complex reasoning

## Open Questions the Paper Calls Out

### Open Question 1
Does behavioral alignment in Visual Question Answering (VQA) responses correlate with alignment in internal representations, such as comparing human eye-movements or brain activations to VLM attention maps? Basis: The Discussion section states that future research "would imply comparing not only outputs from humans answering surveys... but also advanced behaviour & internal representations (e.g., comparing the eye-movements or brain activations... with the attention maps... of a VLM)." Why unresolved: The current study relied solely on behavioral text outputs (sentence embeddings) and did not measure or compare the internal cognitive states or attention mechanisms of the subjects.

### Open Question 2
Is the high internal similarity observed across different VLMs primarily caused by the homogeneity of their training data (the "curse of the same large pool of data")? Basis: The authors explicitly ask, "Why do most VLMs behave so similarly...?" and hypothesize that "it has to do with the curse of training with the same large pool of data: The Internet." Why unresolved: While the paper observes that VLMs converge regardless of architecture or manufacturer, it does not isolate training data provenance as the independent variable to prove this causality.

### Open Question 3
How does the interaction between participant language fluency and data provenance (e.g., US vs. Peru) affect the cognitive alignment scores between humans and VLMs? Basis: The Supplementary Material notes that a "future study will include English-speaking participants... and show them a mixture of data from both people driving in the United States and Peru to study the interaction of language fluency and dashcam data provenance." Why unresolved: The current study involved Peruvian participants answering questions in English, which confounds cultural driving norms with non-native language processing capabilities.

## Limitations
- Small sample size: Only 9 human subjects and 7 held-out videos limit statistical power
- Sentence embedding limitations: May inadequately capture semantic nuances in counterfactual reasoning
- OOD focus: Dataset's Peruvian-specific focus may constrain generalizability to other real-world scenarios

## Confidence
- **High Confidence**: VLMs show high internal similarity (r ≈ 0.8-0.9) regardless of question type is well-supported by consistent RSA correlations across all tested models
- **Medium Confidence**: Humans diverge significantly on counterfactual questions is supported but could benefit from larger sample sizes to confirm statistical significance
- **Medium Confidence**: Mild alignment between VLMs and humans on multiple-choice questions is robust but may reflect format constraints rather than genuine cognitive alignment

## Next Checks
1. **Expand Human Sample Size**: Test the same 7 videos with 30-50 Peruvian participants to determine if the observed human divergence pattern is statistically significant and representative
2. **Cross-Cultural Human Testing**: Add 10-15 English-speaking participants from non-Peruvian backgrounds to isolate whether human divergence on Block 3 stems from cultural experience or universal cognitive complexity
3. **Single VLM Replication**: Apply the full RSA pipeline to one additional VLM (e.g., GPT-4V) on the 7 held-out videos to verify whether the high VLM convergence pattern persists across fundamentally different architectures