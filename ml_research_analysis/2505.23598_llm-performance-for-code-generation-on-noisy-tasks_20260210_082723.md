---
ver: rpa2
title: LLM Performance for Code Generation on Noisy Tasks
arxiv_id: '2505.23598'
source_url: https://arxiv.org/abs/2505.23598
tags:
- performance
- llms
- tasks
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how well large language models can solve
  tasks when the text is severely obfuscated, even to the point of being unintelligible
  to humans. The study uses three datasets (LeetCode, NewLeetCode, and MATH) and applies
  noise methods like typos, deletions, and truncation to create increasingly obfuscated
  versions of each task.
---

# LLM Performance for Code Generation on Noisy Tasks

## Quick Facts
- **arXiv ID:** 2505.23598
- **Source URL:** https://arxiv.org/abs/2505.23598
- **Reference count:** 38
- **Key outcome:** All tested LLMs solved heavily obfuscated tasks via pattern matching when training data overlap existed, revealing dataset contamination risks.

## Executive Summary
This paper investigates how well large language models can solve tasks when text is severely obfuscated, even to the point of being unintelligible to humans. Using three datasets (LeetCode, NewLeetCode, and MATH) and applying noise methods like typos, deletions, and truncation, the study finds that all models could solve heavily obfuscated tasks if the task structure matched problems seen during training. The key finding is that models often respond with solutions to similar-looking problems from training data rather than genuinely reasoning about the obfuscated input, a behavior termed "eager pattern matching." This indicates memorization and overfitting, raising concerns for benchmarking and real-world deployment.

## Method Summary
The study uses 20 questions each from OldLC, NewLC, and MATH datasets, applying three obfuscation methods (Truncation, Typos, Deletions) at 10 augmentation levels (0.1-1.0). Five LLMs (Claude 3.5 Haiku, DeepSeek V3, Gemini 2.0 Flash, Llama 3.3, GPT-4o-mini) are accessed via OpenRouter and queried with prompts for coding or math tasks. Outputs are evaluated via automated test cases or exact answer matching, and performance decay curves are computed to identify contamination. The primary metric is the augmentation level causing 50% performance decay.

## Key Results
- All models could solve heavily obfuscated tasks when training data overlap existed, especially on the older LeetCode dataset.
- Performance dropped sharply on newer datasets (NewLeetCode, MATH) as obfuscation increased, indicating lack of pattern matching.
- The concept of "eager pattern matching" reveals memorization and overfitting, undermining reliability and interpretability of LLMs in real-world applications.

## Why This Works (Mechanism)
The study leverages the tendency of LLMs to rely on pattern matching and memorization when faced with obfuscated inputs, especially when similar tasks exist in their training data. This behavior is more pronounced in older datasets that are more likely to have been included in pretraining, leading to high accuracy even under severe obfuscation. The method exploits this by measuring how quickly performance degrades as noise increases, with slower decay suggesting contamination.

## Foundational Learning
- **Dataset contamination detection**: Why needed—ensures benchmarking validity; Quick check—compare decay curves across known clean and suspected contaminated datasets.
- **Pattern matching vs. reasoning**: Why needed—distinguishes genuine problem-solving from memorization; Quick check—test model on adversarial examples with similar structure but different semantics.
- **Obfuscation methods**: Why needed—create controlled noise to probe model robustness; Quick check—verify augmentation levels produce intended noise without breaking task structure.

## Architecture Onboarding
- **Component map**: Datasets (OldLC, NewLC, MATH) -> Obfuscation pipeline (Truncation, Typos, Deletions) -> LLM API calls (OpenRouter) -> Evaluation (code tests / answer matching) -> Decay analysis.
- **Critical path**: Obfuscation generation → LLM inference → Output evaluation → Performance decay calculation.
- **Design tradeoffs**: Using only 20 tasks per dataset limits statistical power but allows fine-grained decay analysis; sandboxing blocks malicious code but may reject valid solutions.
- **Failure signatures**: Non-deterministic LLM outputs cause variable decay curves; API/model version drift affects reproducibility.
- **First experiments**: 1) Re-run obfuscation with varied nlpaug parameters; 2) Expand task sample size to test generality; 3) Conduct controlled experiment with known train/test splits.

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- The study uses a small sample size (20 tasks per dataset), which may not capture broader model behaviors.
- Exact prompt templates, nlpaug parameters, and sandbox configurations are not fully specified, affecting reproducibility.
- The claim that performance decay reliably indicates contamination is plausible but not proven across varied datasets or task types.

## Confidence
- Performance decay as contamination signal: Medium
- Eager pattern matching as widespread phenomenon: Medium
- Sandbox and adversarial testing robustness: Low

## Next Checks
1. Re-run the obfuscation pipeline with varied nlpaug parameters and prompt structures to test sensitivity of decay curves.
2. Expand the task sample size and include additional domains beyond coding and math to test generality of contamination detection.
3. Conduct a controlled experiment with known train/test splits to quantify false positive/negative rates for the contamination detection method.