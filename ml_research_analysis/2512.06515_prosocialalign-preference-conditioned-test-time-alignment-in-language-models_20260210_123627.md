---
ver: rpa2
title: 'ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models'
arxiv_id: '2512.06515'
source_url: https://arxiv.org/abs/2512.06515
tags:
- urlhttps
- response
- preference
- safety
- harm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of aligning large language models
  (LLMs) with human values in emotionally charged or high-stakes scenarios, where
  refusal-only approaches can alienate users and naive compliance can amplify harm.
  The proposed method, ProSocialAlign, casts safety as lexicographic constrained generation:
  first applying hard constraints to eliminate harmful continuations, then optimizing
  for prosocial quality within the safe set.'
---

# ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models

## Quick Facts
- arXiv ID: 2512.06515
- Source URL: https://arxiv.org/abs/2512.06515
- Authors: Somnath Banerjee; Sayan Layek; Sayantan Adak; Mykola Pechenizkiy; Animesh Mukherjee; Rima Hazra
- Reference count: 40
- Primary result: State-of-the-art performance on safety benchmarks, reducing unsafe leakage and boosting alignment to human values

## Executive Summary
ProSocialAlign addresses the challenge of aligning large language models with human values in emotionally charged or high-stakes scenarios where traditional refusal-only approaches can alienate users. The method casts safety as lexicographic constrained generation, first applying hard constraints to eliminate harmful continuations, then optimizing for prosocial quality within the safe set. It combines directional regulation (subtracting a learned "harm vector" in parameter space) with preference-aware autoregressive reward modeling trained jointly across five human-centered attributes—empathy, sensitivity, non-judgmental stance, truthfulness, and helpfulness—using gradient conflict resolution.

The proposed approach demonstrates state-of-the-art performance across five safety benchmarks, achieving the highest Mean Inner Product (MIP) scores and outperforming baselines by over 20% in winrate comparisons. ProSocialAlign enables test-time alignment without retraining the base model, providing users with granular control over which prosocial attributes to prioritize during generation while maintaining safety constraints.

## Method Summary
ProSocialAlign operates through three main components: Directional Regulation (DiReg) creates a harm-mitigated base model by subtracting a learned "harm vector" from the original parameters; a Preference-Conditioned Autoregressive Reward Model (PV-ARM) is trained jointly across five attributes with gradient conflict resolution; and Guided Inference (GUI-Gen) combines the harm-mitigated base with the reward model during decoding. The method uses PBLoRA to condition the reward model on preference vectors, enabling test-time control over attribute emphasis. At inference, token probabilities are modified by combining the safe base distribution with the preference-conditioned reward distribution, allowing generation that is both safe and aligned to specified prosocial attributes.

## Key Results
- Achieved highest Mean Inner Product (MIP) scores across all tested datasets
- Outperformed baselines by over 20% in GPT-4o winrate comparisons
- Demonstrated significant reduction in Attack Success Rate (ASR) across safety benchmarks
- Showed minimal capability degradation while substantially improving alignment to human values

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If a "harm direction" exists in parameter space as a sparse, linearly accessible vector, then subtracting it from the base model parameters reduces harmful output probability while preserving other capabilities.
- **Mechanism:** Fine-tune a copy of the base model on harmful question-response pairs, compute the parameter difference, sparsify to top-m components by magnitude, then create a harm-mitigated model via parameter subtraction.
- **Core assumption:** Harmful behavior concentrates in a small, identifiable subset of parameter directions that are semi-orthogonal to general language capabilities.
- **Evidence anchors:** The abstract mentions "directional regulation, a harm-mitigation mechanism that subtracts a learned 'harm vector' in parameter space" and section 3.2 formalizes the sparse vector extraction.
- **Break condition:** If harmful behaviors are distributed diffusely across parameters rather than concentrated, the sparse subtraction degrades capabilities without meaningfully reducing harm.

### Mechanism 2
- **Claim:** If gradient conflicts between prosocial attributes are projected away during training, then a single reward model can jointly encode all attributes while remaining controllable via preference vectors at inference time.
- **Mechanism:** For each attribute, compute per-objective gradient. When gradients conflict (dot product < 0), project the conflicting gradient to remove the opposing component. Aggregate deconflicted gradients for the update. The PBLoRA architecture conditions on preference vector through learned transformations.
- **Core assumption:** Gradient conflicts are the primary obstacle to joint multi-attribute learning, and projection preserves sufficient signal for each objective.
- **Evidence anchors:** The abstract references "preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution" and section 3.3.1 implements the projection step.
- **Break condition:** If attributes are fundamentally irreconcilable, projection produces a compromised policy that satisfies no objective well.

### Mechanism 3
- **Claim:** If token-level reward guidance is applied to a harm-corrected base model, then generation stays within the safe set while optimizing for user-specified attribute preferences.
- **Mechanism:** At each decoding step, combine distributions by multiplying the harm-mitigated base model distribution with the reward model distribution raised to a power. This steers generation toward preferred attributes while maintaining safety constraints.
- **Core assumption:** The product of a safe-tilted base distribution and a preference-conditioned reward distribution yields coherent, safe, and aligned outputs.
- **Evidence anchors:** The abstract states it "steers generation toward safe, empathetic, and value-aligned responses without retraining the base model" and section 3.3.2 formalizes the guided generation objective.
- **Break condition:** If the reward model over-weights certain attributes, outputs become incoherent or fail the underlying safety constraint despite the preprocessing.

## Foundational Learning

- **Concept: Task arithmetic / parameter-space editing**
  - **Why needed here:** The DiReg mechanism assumes you can compute meaningful task vectors (harm direction) and manipulate them via linear algebra. Without this, the core safety intervention makes no sense.
  - **Quick check question:** Given models θ_base and θ_finetuned, what does θ_diff = θ_finetuned - θ_base represent, and when would adding θ_diff to a third model transfer the fine-tuned behavior?

- **Concept: Multi-objective optimization with gradient conflict resolution**
  - **Why needed here:** Training PV-ARM across five attributes requires resolving situations where improving empathy hurts truthfulness. Gradient projection is the specific technique used.
  - **Quick check question:** Two loss gradients g_1 and g_2 point in opposite directions. Describe one method to update parameters that doesn't simply average them (which would cancel signal).

- **Concept: Autoregressive reward models and test-time alignment**
  - **Why needed here:** Unlike standard reward models that score complete responses, ARMs assign token-level rewards, enabling fine-grained guidance during decoding without retraining the base model.
  - **Quick check question:** In Eq. 4, why does the ARM sum log-probabilities token-by-token rather than scoring the full response? What capability does this enable at inference time?

## Architecture Onboarding

- **Component map:** Base Model -> DiReg (harm vector subtraction) -> GUI-Gen (combined generation) with Reward Model + PBLoRA providing preference-conditioned guidance
- **Critical path:** 1) Construct harmful dataset and compute harm vector 2) Train PV-ARM with gradient surgery on 5-attribute preference dataset 3) At inference, provide preference vector and generate via modified decoding
- **Design tradeoffs:**
  - Sparsity level m in DiReg: Higher m removes more harm but risks capability degradation
  - β in GUI-Gen: Controls reward model influence (low = safe but unaligned, high = risk of incoherence)
  - PBLoRA rank split: Shared vs. preference-conditioned capacity (paper uses fixed values)
  - Attribute selection: Five attributes are psychologically motivated but not exhaustively validated
- **Failure signatures:**
  - Over-refusal: Model refuses benign queries (DiReg too aggressive or reward model biased toward safety)
  - Attribute collapse: Outputs optimize one attribute at expense of others despite balanced preference vector
  - Reward hacking: Outputs achieve high ARM scores but are semantically empty or repetitive
  - Category-specific failure: Good performance on most benchmarks but poor on specific categories (e.g., self-harm)
- **First 3 experiments:**
  1. Ablate DiReg: Run GUI-Gen with base model instead of harm-mitigated model to isolate DiReg's contribution
  2. Single-attribute control: Fix preference vector to one-hot values to verify PBLoRA conditioning and attribute isolation
  3. Gradient conflict visualization: Log pairwise cosine similarities between attribute gradients during training to identify conflict patterns

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several significant uncertainties emerge from the methodology and results:

1. **Synthetic label reliability:** How does the reliance on synthetic preference labels generated by distinct teacher LLMs for the training dataset impact the robustness of the alignment compared to human annotations?
2. **Scalability to larger models:** Does the "harm vector" subtraction method scale effectively to frontier-class models (70B+ parameters), or does the linear separation assumption fail in higher-dimensional representation spaces?
3. **Computational overhead:** What is the computational latency overhead of performing test-time alignment with the PV-ARM compared to standard decoding methods?

## Limitations

- The method's core claims hinge on the existence of a sparse, linearly separable "harm vector" in parameter space, which may not hold across different model families or scales
- The five-attribute formulation may face fundamental incompatibilities that gradient conflict resolution cannot fully address
- Empirical results rely heavily on preference models rather than human judgment for evaluation, creating potential evaluation bias

## Confidence

- **High confidence:** The GUI-Gen inference mechanism (combining safe base distribution with preference-conditioned rewards) is technically sound and well-grounded in controlled decoding literature
- **Medium confidence:** The gradient conflict resolution approach is theoretically valid, but its sufficiency for five-way attribute balancing hasn't been rigorously validated
- **Medium confidence:** Empirical results on benchmark datasets are compelling, but evaluation relies heavily on preference models rather than human judgment

## Next Checks

1. **Single-attribute isolation test:** Generate responses using one-hot preference vectors and measure whether outputs optimize the target attribute without degrading others—this validates the controllability of PBLoRA conditioning
2. **DiReg ablation study:** Compare performance with and without the harm vector subtraction on ASR and MIP to isolate DiReg's contribution beyond what the reward model provides
3. **Cross-dataset generalization:** Test ProSocialAlign on held-out safety datasets (not used in training or fine-tuning) to assess whether the five-attribute formulation transfers to novel harm categories