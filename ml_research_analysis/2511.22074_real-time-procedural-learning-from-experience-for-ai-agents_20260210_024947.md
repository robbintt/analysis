---
ver: rpa2
title: Real-Time Procedural Learning From Experience for AI Agents
arxiv_id: '2511.22074'
source_url: https://arxiv.org/abs/2511.22074
tags:
- memory
- agents
- arxiv
- procedural
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PRAXIS, a state-dependent memory mechanism
  that enables AI agents to learn procedures from experience in real time. The method
  stores action-outcome traces indexed by both environmental and internal agent states,
  then retrieves relevant memories during decision-making.
---

# Real-Time Procedural Learning From AI Agents

## Quick Facts
- **arXiv ID**: 2511.22074
- **Source URL**: https://arxiv.org/abs/2511.22074
- **Reference count**: 23
- **Primary result**: State-indexed procedural memory improves agent accuracy, reliability, and efficiency in stateful environments

## Executive Summary
This paper introduces PRAXIS, a state-dependent memory mechanism that enables AI agents to learn procedures from experience in real time. The method stores action-outcome traces indexed by both environmental and internal agent states, then retrieves relevant memories during decision-making. Evaluated on the REAL web browsing benchmark, PRAXIS improved task completion accuracy from 40.3% to 44.1% across multiple VLM backbones, increased reliability from 74.5% to 79.0%, and reduced average steps-to-completion from 25.2 to 20.2. Performance scaled with retrieval breadth, suggesting generalizable procedural priors.

## Method Summary
PRAXIS implements a real-time procedural learning system where successful action traces are stored as tuples of pre-action environment state, internal goal state, action taken, and post-action environment state. During execution, the system computes composite similarity scores using environmental state overlap (IoU x length overlap) and internal state embedding cosine similarity to retrieve top-k relevant memories. These retrieved exemplars are injected into the action selection context as demonstrations, biasing the VLM's output toward previously successful trajectories. The memory bank grows incrementally through experience, enabling continuous learning without retraining.

## Key Results
- Task completion accuracy improved from 40.3% to 44.1% across multiple VLM backbones
- Reliability increased from 74.5% to 79.0% (success rate consistency over 5 runs)
- Steps-to-completion reduced from 25.2 to 20.2, indicating more efficient trajectories
- Performance scaled with retrieval breadth k, plateauing at higher values

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Retrieving state-matched procedural traces biases action selection toward previously successful trajectories, reducing variance in agent behavior.
- **Mechanism**: The system computes a composite similarity score between the current state and stored memories using (1) Intersection-over-Union on environmental state descriptions, (2) length-normalized overlap, and (3) embedding cosine similarity on internal states/directives. Top-k memories exceeding threshold τ are injected into the action selection context as exemplars.
- **Core assumption**: Visual/textual state descriptions capture enough invariance to make past experiences relevant to new situations with similar states.
- **Evidence anchors**:
  - [abstract] "retrieves relevant memories during decision-making" by "jointly matching environmental and internal states"
  - [section 3.2] Algorithm 1 formally defines s_env = v_i · l_i where v_i = IoU(M_env_i, Q_env) and l_i = LengthOverlap
  - [corpus] ProcMEM (arXiv:2602.01869) similarly proposes experience reuse via non-parametric retrieval, suggesting convergent validation of the general approach
- **Break condition**: If state descriptions are too sparse or noisy, IoU matching may retrieve irrelevant memories, degrading performance. The ablation (Fig. 3) shows slight within-step decreases, indicating local context crowding.

### Mechanism 2
- **Claim**: Procedural memory provides reusable local state-to-action priors that generalize across similar tasks in similar environments.
- **Mechanism**: Memories are indexed by both environmental state and internal goal state. When facing a new task with a recognizable substate, previously learned procedures transfer without re-deriving solutions from scratch.
- **Core assumption**: Procedures decompose into reusable sub-procedures anchored to recognizable states rather than being monolithic task-specific chains.
- **Evidence anchors**:
  - [section 4.1] Best-of-5 accuracy improved from 53.7% to 55.7%, suggesting "reusable priors that improve agentic behavior on complex tasks"
  - [section 4.4] Performance scaled with retrieval breadth k, plateauing at higher values—interpreted as "generalizable context"
  - [corpus] Experience-Evolving Multi-Turn Tool-Use Agent (arXiv:2512.07287) explicitly addresses hybrid episodic-procedural memory for context transfer, supporting the conceptual validity
- **Break condition**: If tasks share no common substates, or if environment changes invalidate state descriptors, transfer fails. The paper only claims "preliminary generalization to unseen tasks in similar environments."

### Mechanism 3
- **Claim**: Injecting retrieved exemplars reduces stochastic variance in VLM outputs by constraining the action space.
- **Mechanism**: Retrieved memories appear in the procedural memory section of the action selection node's context. This provides concrete demonstrations that anchor the VLM's sampling distribution.
- **Core assumption**: The VLM backbone is capable of in-context learning from retrieved examples when they are sufficiently relevant.
- **Evidence anchors**:
  - [section 4.2] Reliability improved from 74.5% to 79.0%, attributed to "biasing their decisions towards previously successful trajectories"
  - [section 4.3] Steps-to-completion reduced from 25.2 to 20.2, indicating exemplars steer agents "along both correct and more direct trajectories"
  - [corpus] Weak direct corpus evidence for variance reduction specifically; related work (Reflexion, Self-Refine) focuses on reflection rather than state-indexed retrieval
- **Break condition**: If retrieved memories conflict with current optimal behavior (e.g., environment has changed), they may introduce systematic errors rather than reduce variance.

## Foundational Learning

- **State-dependent memory (psychology)**
  - Why needed here: The paper explicitly grounds its design in Tulving's encoding specificity principle and Bower's mood-memory research (citations [3], [14]). Understanding this helps explain why joint environment+internal state matching matters.
  - Quick check question: Can you explain why matching both environmental context and internal goal state at retrieval time should improve recall relevance compared to matching either alone?

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: PRAXIS extends RAG principles from factual retrieval to procedural retrieval. The paper positions itself relative to RAG in Section 2, contrasting factual memory with action-policy learning.
  - Quick check question: How does retrieving procedural traces differ from retrieving documents in standard RAG? What additional indexing dimensions are required?

- **Agentic action selection architecture**
  - Why needed here: The method modifies a specific "node" in Altrina's architecture. Understanding agent scaffolding and action selection loops is prerequisite to knowing where to inject retrieved memories.
  - Quick check question: In a node-based agent architecture, why is the action selection node the appropriate place to inject procedural memory context rather than perception or execution nodes?

## Architecture Onboarding

- **Component map**:
  - Memory Store -> State Encoder -> Retrieval Module -> Action Selection Node -> Memory Writer -> Environment

- **Critical path**:
  1. Agent observes current state → State Encoder produces Q_env, Q_int
  2. Retrieval Module queries Memory Store → returns indices R matching similarity criteria
  3. Retrieved memories formatted as exemplars → injected into Action Selection Node context
  4. VLM generates next action → executed in environment
  5. Memory Writer records (M_env-pre, M_int, action, M_env-post) tuple
  6. Loop continues

- **Design tradeoffs**:
  - Retrieval breadth k: Higher k improves performance (Fig. 3) but increases context length and latency; paper shows plateau behavior
  - Similarity threshold τ: Higher τ improves precision but may return empty results in novel states
  - State encoding granularity: Richer encodings improve retrieval quality but increase compute; paper notes current implementation uses "basic visual and DOM feature overlap"
  - Memory scope: Paper uses agent's own trajectories; could also incorporate human demonstrations (mentioned as compatible)

- **Failure signatures**:
  - Empty retrieval: τ too high or no similar states → agent falls back to baseline behavior
  - Context crowding: Too many low-relevance memories → slight performance dips (observed in ablation)
  - Stale memories: Environment changes invalidate stored procedures → retrieved exemplars mislead action selection
  - State description mismatch: Encoder fails to capture task-relevant features → irrelevant retrieval

- **First 3 experiments**:
  1. **Baseline replication**: Run agent on REAL benchmark without procedural memory to establish baseline accuracy, then enable PRAXIS with k=5, τ=0.3 to verify ~4% accuracy improvement matches paper claims
  2. **Retrieval breadth ablation**: Sweep k ∈ {1, 3, 5, 7, 10} on a held-out subset of tasks to reproduce Fig. 3 plateau pattern and identify optimal k for your compute budget
  3. **State encoding sensitivity**: Replace IoU-based state matching with pure embedding similarity to test whether the composite scoring function is necessary, measuring both accuracy and retrieval relevance (manual inspection of retrieved memories for 20-30 queries)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can richer state encoders improve retrieval quality and invariance to superficial changes compared to the current IoU-based visual and DOM feature overlap?
- Basis in paper: [explicit] "Our proof-of-concept implementation... uses basic visual and DOM feature overlap along with simple similarity metrics. A richer encoder can improve both retrieval quality and invariance to superficial changes."
- Why unresolved: The current implementation is described as "proof-of-concept" with basic similarity metrics; the paper does not explore learned or semantic encoders.
- What evidence would resolve it: Ablation experiments comparing IoU-based retrieval against learned embedding encoders on the same benchmark, measuring accuracy and robustness to UI variations.

### Open Question 2
- Question: Does adaptive retrieval accounting for real-time factors (uncertainty, compute budget) outperform fixed state-similarity heuristics?
- Basis in paper: [explicit] "Rather than a fixed retrieval based on state similarity heuristics, the retrieval mechanism can account for real-time factors such as uncertainty and compute budget."
- Why unresolved: The paper uses fixed Algorithm 1 with static k and τ; no experiments on adaptive or iterative retrieval are reported.
- What evidence would resolve it: Comparative evaluation of adaptive retrieval strategies (e.g., uncertainty-triggered iterative retrieval) against the baseline on REAL, measuring task success and latency.

### Open Question 3
- Question: Does state-dependent procedural memory transfer effectively beyond web browsing to general computer-use environments?
- Basis in paper: [explicit] "State-dependent memory is conceptually agnostic to the environment, and the same idea can be naturally extended to general cases of agentic computer use."
- Why unresolved: Experiments are restricted to the REAL web benchmark; no empirical results on OS-level or desktop application tasks are provided.
- What evidence would resolve it: Evaluation of PRAXIS on non-web agentic benchmarks (e.g., OSWorld or Mind2Web) reporting accuracy, reliability, and efficiency gains.

### Open Question 4
- Question: How does performance scale with memory bank size, and does retrieval latency become a bottleneck in long-running deployments?
- Basis in paper: [inferred] The paper shows scaling with retrieval breadth k but does not study scaling with the number of stored memories n or retrieval time complexity.
- Why unresolved: Longitudinal deployment would accumulate large memory banks; the current nearest-neighbor approach may face computational limits.
- What evidence would resolve it: Scaling experiments varying n (e.g., 10^2–10^5 memories) and measuring both task performance and retrieval latency, with potential approximations (e.g., ANN indices).

## Limitations

- **Proprietary Architecture Dependence**: The paper relies on "Altrina," a closed-source agent architecture, creating significant reproduction barriers for verifying claims independently.
- **State Representation Gap**: Without access to the actual method for generating compressed textual environmental states, implementation fidelity cannot be verified, and the IoU-based similarity metric implementation remains ambiguous.
- **Limited Generalization Scope**: Claims about "preliminary generalization to unseen tasks in similar environments" lack systematic testing on truly novel tasks or environment variations.

## Confidence

- **Accuracy Improvement Claims (40.3% → 44.1%)**: Medium confidence - the relative improvement is modest and methodology appears sound, but proprietary architecture dependence limits independent verification
- **Reliability and Efficiency Claims**: Medium confidence - improvements follow logically from the mechanism but depend entirely on faithful implementation of unspecified architectural components
- **Generalizability Claims**: Low confidence - the paper explicitly acknowledges limitations but doesn't demonstrate performance degradation on truly novel tasks or provide systematic state-space coverage analysis

## Next Checks

1. **Architecture Portability Test**: Implement PRAXIS in an open-source web agent (e.g., BrowserGym + GPT-4o) and measure whether similar accuracy gains materialize without the Altrina architecture, isolating the memory mechanism's independent contribution.

2. **State Encoding Ablation**: Systematically vary state representation methods (raw DOM vs accessibility tree vs visual embeddings) and measure retrieval relevance quality through human evaluation of 50+ retrieved memory exemplars across diverse tasks.

3. **Generalization Boundary Test**: Create a test suite with progressively dissimilar tasks/environment states and measure performance decay, identifying the actual scope limits of procedural memory transfer rather than relying on author claims about "similar environments."