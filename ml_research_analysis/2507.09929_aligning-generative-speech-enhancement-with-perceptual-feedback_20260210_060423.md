---
ver: rpa2
title: Aligning Generative Speech Enhancement with Perceptual Feedback
arxiv_id: '2507.09929'
source_url: https://arxiv.org/abs/2507.09929
tags:
- speech
- enhancement
- perceptual
- arxiv
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GSEPF, the first perceptually aligned LM-based
  speech enhancement method. It uses Direct Preference Optimization (DPO) guided by
  a neural MOS predictor (UTMOS) to steer enhancement toward human-preferred outputs.
---

# Aligning Generative Speech Enhancement with Perceptual Feedback

## Quick Facts
- arXiv ID: 2507.09929
- Source URL: https://arxiv.org/abs/2507.09929
- Reference count: 0
- Introduces GSEPF, the first perceptually aligned LM-based speech enhancement method using DPO with neural MOS prediction

## Executive Summary
This paper introduces GSEPF, the first perceptually aligned LM-based speech enhancement method using Direct Preference Optimization (DPO) guided by a neural MOS predictor (UTMOS) to steer enhancement toward human-preferred outputs. The approach generates preference pairs by sampling candidate acoustic sequences from a reference model and ranking them with UTMOS, then trains a target model to maximize the likelihood of preferred sequences. On the Deep Noise Suppression Challenge 2020 test sets, GSEPF achieves up to 56% relative gains in UTMOS and strong improvements in NISQA and DNSMOS while maintaining speaker similarity.

## Method Summary
GSEPF employs a two-stage decoder-only language model architecture: an N2S model that converts noisy waveforms to clean semantic tokens, followed by an S2S model that generates enhanced acoustic tokens. The reference S2S model is pre-trained with cross-entropy loss, then fine-tuned using DPO with UTMOS as the reward proxy. Preference pairs are constructed by sampling N=32 candidate sequences from the reference model's top-K logits, decoding to waveforms, scoring with UTMOS, and selecting top-Z and bottom-Z as preferred and rejected sets respectively. The target model is trained with combined CE+DPO loss to maintain speaker identity while improving perceptual quality.

## Key Results
- Achieves up to 56% relative improvement in UTMOS on DNS 2020 test sets
- Strong gains in NISQA and DNSMOS perceptual quality metrics
- Maintains speaker similarity (SECS) better than DPO-only approach
- Shows better subjective naturalness in listening tests

## Why This Works (Mechanism)

### Mechanism 1
DPO shifts optimization from token-level likelihood toward perceptually preferred outputs by contrastively maximizing the margin between preferred and rejected acoustic sequences. Given context tokens y, the target model πθ is trained to increase relative log-probability of preferred sequences A+ over rejected A− compared to a frozen reference model πref, per LDPO = −E[log σ(β · log(πθ(A+|y)/πref(A+|y)) − β · log(πθ(A−|y)/πref(A−|y)))]. The core assumption is that UTMOS scores serve as a valid proxy for human auditory preference that generalizes beyond its training distribution.

### Mechanism 2
Sampling diverse candidates from top-K logits and ranking by UTMOS creates preference pairs that encode perceptual distinctions absent from token-level CE training. From Logitsref(Â) with top-K filtering, sample N=32 candidate acoustic sequences; decode to waveform; score with UTMOS; assign top-Z as A+, bottom-Z as A−. The core assumption is that the reference model πref is good enough that sampled candidates span a perceptually meaningful quality range.

### Mechanism 3
Combining CE loss with DPO preserves speaker identity while improving perceptual quality, as CE provides an anchoring effect against over-optimization. Loverall = LCE + LDPO with no scaling; CE maintains token-level grounding while DPO shifts distribution toward perceptual preference. The core assumption is that loss magnitudes are close enough that joint optimization is stable without weighting.

## Foundational Learning

- **Direct Preference Optimization (DPO)**
  - Why needed: Core training paradigm replacing reward-model RL with contrastive preference pairs
  - Quick check: Given a reference model πref and preferred sequence A+, write the DPO loss term for target πθ

- **Neural MOS Prediction (UTMOS/DNSMOS/NISQA)**
  - Why needed: These provide reference-free perceptual scores used as reward proxies
  - Quick check: Explain why DNSMOS is preferred over PESQ for LM-based SE evaluation

- **Autoregressive Language Modeling for Audio Tokens**
  - Why needed: The S2S LM generates acoustic tokens autoregressively; DPO operates on sequence probabilities
  - Quick check: Compute pθ(A|y) as a product of conditional probabilities for a 3-token sequence

## Architecture Onboarding

- **Component map**: WavLM → K-means → N2S LM → S2S LM → SimCodec → enhanced waveform
- **Critical path**: Pre-train N2S LM and S2S LM (πref) with CE loss until DNSMOS saturates → For each training batch: sample N candidates from πref, decode, score with UTMOS, construct Z preference pairs → Compute LCE and LDPO under teacher forcing; update πθ for 400 steps (β=0.1, N=32, Z=4, K=50)
- **Design tradeoffs**: Z=1 vs Z=4: More pairs don't improve quality but add compute; Z=1 sufficient; Ground-truth as A+: Fails because redundant with CE direction; CE+DPO vs DPO-only: CE anchors speaker similarity but slightly tempers perceptual gains; N candidates: Larger N improves ranking quality but increases decode+UTMOS cost
- **Failure signatures**: SECS degradation with DPO-only → CE anchor missing; add LCE; No improvement over baseline → check that preference pairs have meaningful MOS separation; Training instability → verify πref is frozen; reduce β if gradients explode
- **First 3 experiments**: 1) Reproduce GenSE* baseline with CE-only training; confirm DNSMOS saturation at ~44k steps; 2) Run preference pair ablation: compare Z=1 (Ground-truth), Z=1 (sampled), Z=4 on DNSMOS/UTMOS/SECS; 3) Validate CE+DPO vs DPO-only: confirm SECS anchoring effect on both w/ and w/o Reverb test partitions

## Open Questions the Paper Calls Out
- How can the framework be extended to explicitly preserve or control speaker identity? The current results show a drop in Speaker Embedding Cosine Similarity (SECS) on non-reverberant speech when using DPO, indicating a trade-off between perceptual quality and identity preservation.
- Can the DPO-based alignment enable fine-grained controllability over enhancement characteristics? The current system is trained as a binary classifier based on general quality, lacking mechanisms to guide specific attributes like noise type suppression or tonal modification.
- Does the reliance on UTMOS as a proxy lead to "reward hacking" where the model optimizes for proxy artifacts rather than true human auditory perception? While cross-metric gains were found, the specific divergence between the proxy's decision boundary and human preference is not analyzed.

## Limitations
- Reliance on UTMOS as proxy for human preference may lead to metric overfitting rather than true perceptual gains
- Preference sampling assumes reference model generates candidates spanning meaningful quality ranges without validation
- Limited ablation studies on β values, preference pair quality, and cross-domain generalization
- Subjective test results lack statistical significance testing and detailed preference distributions

## Confidence
- **DPO improves perceptual metrics**: High for DNSMOS/UTMOS/NISQA improvements, Medium for true perceptual quality gains
- **CE+DPO preserves speaker similarity**: Medium-High for SECS maintenance, Low-Medium for understanding tradeoff dynamics
- **New paradigm for SE**: Medium-High as methodological innovation, Low for practical superiority
- **Better subjective naturalness**: Low due to insufficient statistical and methodological detail

## Next Checks
1. **Preference Pair Quality Analysis**: For 100 randomly selected training samples, compute UTMOS score variance between A+ and A− sets. Calculate percentage where A+ mean > A− mean by >0.1 MOS points to validate meaningful perceptual distinctions.
2. **Cross-Domain Perceptual Generalization**: Evaluate GSEPF on held-out noise type (e.g., babble speech) not in training data. Compare DNSMOS/UTMOS improvements against baseline to test generalization beyond training distribution.
3. **Statistical Significance of Subjective Tests**: Re-run listening tests with 30+ participants per condition, report preference percentages with confidence intervals, and perform paired t-tests or binomial tests to determine if "better naturalness" claim achieves p < 0.05 significance.