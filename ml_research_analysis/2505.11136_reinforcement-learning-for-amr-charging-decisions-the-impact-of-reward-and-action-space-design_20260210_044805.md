---
ver: rpa2
title: 'Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and
  Action Space Design'
arxiv_id: '2505.11136'
source_url: https://arxiv.org/abs/2505.11136
tags:
- charging
- battery
- reward
- amrs
- strategies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends an open-source warehouse simulation framework
  to model battery management for autonomous mobile robots (AMRs) in large-scale block
  stacking warehouses. The authors propose a reinforcement learning (RL) design that
  jointly optimizes charging decisions and charging durations using a Proximal Policy
  Optimization agent.
---

# Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design

## Quick Facts
- arXiv ID: 2505.11136
- Source URL: https://arxiv.org/abs/2505.11136
- Reference count: 37
- This paper extends an open-source warehouse simulation framework to model battery management for autonomous mobile robots (AMRs) in large-scale block stacking warehouses, achieving an average service time of 581.85 seconds with RL-based charging strategies.

## Executive Summary
This paper extends an open-source warehouse simulation framework to model battery management for autonomous mobile robots (AMRs) in large-scale block stacking warehouses. The authors propose a reinforcement learning (RL) design that jointly optimizes charging decisions and charging durations using a Proximal Policy Optimization agent. They evaluate different reward functions, action spaces, and the use of an interrupt heuristic, comparing their RL-based strategies against several heuristic baselines on a large real-world dataset. Results show that RL-based charging strategies can outperform heuristic approaches, with the best configuration achieving an average service time of 581.85 seconds when interruption is enabled, outperforming the best heuristic by 20.27 seconds. The study highlights the importance of reward function design and action space configuration in achieving stable learning and strong performance.

## Method Summary
The authors extend the SLAPStack simulation framework to model battery management for AMRs in block stacking warehouses. The RL agent jointly optimizes charging decisions (when to charge) and charging durations (how long to charge) using Proximal Policy Optimization (PPO) from stable-baselines3. The action space includes discrete charging durations, with masking to prevent invalid actions based on battery level. Four configurations are tested: Basic1 (unshaped reward, full action space), Basic2 (unshaped reward, binary action space), LightShaped (shaped reward, binary action space), and FullyShaped (shaped reward, binary action space with interrupt heuristic). Training uses a rollout buffer of 2048, minibatch size of 64, and 4M steps with evaluation every 200K steps. The best model is selected based on periodic evaluation over training weeks.

## Key Results
- RL-based charging strategies can outperform heuristic approaches, with the best configuration achieving an average service time of 581.85 seconds when interruption is enabled.
- The FullyShaped configuration with binary action space and interrupt heuristic achieved the best performance, outperforming the best heuristic by 20.27 seconds.
- Reward shaping and action space design significantly impact learning stability and final performance, with shaped rewards and constrained action spaces leading to more stable training.
- The interrupt heuristic improves performance but may lead to overfitting, as the FullyShaped model performed significantly worse without it.

## Why This Works (Mechanism)
The RL agent learns to balance the trade-off between keeping AMRs available for order fulfillment and avoiding unnecessary charging that could block charging stations. By jointly optimizing charging decisions and durations, the agent can adapt to varying order patterns and battery consumption rates. The shaped reward functions guide the agent toward charging strategies that minimize service time while respecting queue constraints. The binary action space simplifies the decision problem, leading to more stable learning compared to the full action space. The interrupt heuristic allows the agent to adapt charging durations based on real-time demand, further improving performance.

## Foundational Learning
- **Reinforcement Learning**: Why needed: To optimize complex, sequential decision-making under uncertainty. Quick check: Can the agent learn a policy that outperforms simple heuristics?
- **Proximal Policy Optimization (PPO)**: Why needed: Stable, sample-efficient RL algorithm suitable for continuous control tasks. Quick check: Does PPO converge to a good policy within the given training steps?
- **Reward Shaping**: Why needed: To guide the agent toward desirable behaviors and improve learning stability. Quick check: Does the shaped reward lead to better performance than the unshaped reward?
- **Action Masking**: Why needed: To prevent the agent from selecting invalid actions that violate battery level constraints. Quick check: Does action masking improve learning stability and prevent catastrophic failures?
- **Simulation-Based RL**: Why needed: To train and evaluate RL policies in a safe, controlled environment before deploying on real robots. Quick check: Does the simulation accurately capture the key dynamics of the real warehouse system?

## Architecture Onboarding

**Component Map**
SLAPStack (warehouse simulation) -> PPO Agent (stable-baselines3) -> Action Space (discrete charging durations) -> Reward Function (shaped/unshaped) -> Interrupt Heuristic (optional)

**Critical Path**
1. SLAPStack generates observations and rewards based on AMR and warehouse state
2. PPO agent selects action (charge/not charge, charging duration) based on current state
3. SLAPStack applies action, updates AMR and warehouse state, generates next observation and reward
4. PPO agent learns from the experience to improve its policy

**Design Tradeoffs**
- **Action Space**: Full action space allows more granular control but may lead to unstable learning; binary action space simplifies decision-making but may miss optimal charging durations
- **Reward Shaping**: Shaped rewards guide the agent toward desirable behaviors but may introduce bias; unshaped rewards are simpler but may lead to slower learning
- **Interrupt Heuristic**: Improves performance by adapting charging durations but may lead to overfitting and poor generalization

**Failure Signatures**
- Unstable training: High entropy loss, reward degradation, policy collapse
- Overfitting to Interrupt: Significant performance degradation when Interrupt is disabled
- Queue constraint violations: Maximum retrieval or delivery queue exceeds limits

**First Experiments**
1. Implement and train the FullyShaped configuration with default stable-baselines3 PPO hyperparameters, monitor training stability, and verify if the reported best service time (581.85s) is achievable.
2. Conduct ablation studies by disabling the Interrupt heuristic and comparing performance degradation to assess overfitting.
3. Test alternative reward function formulations and action spaces (e.g., LightShaped, A_full) to confirm the reported trade-offs in stability and performance.

## Open Questions the Paper Calls Out
1. How does the inclusion of non-linear charging behavior and battery degradation impact the convergence speed and stability of the proposed PPO configurations?
2. Can systematic reward shaping approaches prevent the overfitting observed in domain-informed configurations (like FullyShaped) when domain heuristics (like Interrupt) are removed?
3. In what specific warehouse scenarios does battery management cease to be the primary performance bottleneck compared to layout design or order sequencing?
4. How sensitive are the reported trade-offs between optimization potential and learning stability to the specific choice of PPO hyperparameters?

## Limitations
- Several key implementation details are missing, including exact PPO hyperparameters, reward function formulations, normalization constants, and random seeds, which could lead to different learning dynamics and final performance.
- The study focuses on a specific warehouse configuration and dataset, limiting generalizability to other settings.
- The reliance on a specific heuristic for charging station selection (nearest available) may constrain the potential benefits of RL-based charging decisions.

## Confidence
- High: The overall experimental framework, metrics, and general RL design choices are clearly specified and reproducible.
- Medium: The qualitative findings about the importance of reward shaping and action space design are supported by the results, but exact quantitative comparisons depend on unreported implementation details.
- Low: Claims about specific performance improvements (e.g., 20.27s reduction) are hard to verify without complete hyperparameter and reward function specifications.

## Next Checks
1. Implement and train the FullyShaped configuration with default stable-baselines3 PPO hyperparameters, monitor training stability, and verify if the reported best service time (581.85s) is achievable.
2. Conduct ablation studies by disabling the Interrupt heuristic and comparing performance degradation to assess overfitting.
3. Test alternative reward function formulations and action spaces (e.g., LightShaped, A_full) to confirm the reported trade-offs in stability and performance.