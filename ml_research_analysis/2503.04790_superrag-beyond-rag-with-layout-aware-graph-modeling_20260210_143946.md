---
ver: rpa2
title: 'SuperRAG: Beyond RAG with Layout-Aware Graph Modeling'
arxiv_id: '2503.04790'
source_url: https://arxiv.org/abs/2503.04790
tags:
- graph
- table
- retrieval
- page
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a layout-aware graph modeling approach for
  multimodal retrieval-augmented generation (RAG). Unlike traditional RAG methods
  that handle flat text chunks, the proposed method uses a graph structure to capture
  relationships between text, tables, and figures while preserving document layout
  and structure.
---

# SuperRAG: Beyond RAG with Layout-Aware Graph Modeling

## Quick Facts
- **arXiv ID:** 2503.04790
- **Source URL:** https://arxiv.org/abs/2503.04790
- **Reference count:** 29
- **Primary result:** Layout-aware graph modeling improves multimodal RAG accuracy by 7.3% on DOCBENCH and 9% on SPIQA Test-C

## Executive Summary
This paper introduces a layout-aware graph modeling approach for multimodal retrieval-augmented generation (RAG) that preserves document structure and relationships between text, tables, and figures. Unlike traditional RAG methods that handle flat text chunks, SuperRAG constructs a property graph capturing hierarchical relationships and layout information. The system uses a hybrid retrieval pipeline combining vector search, BM25, and graph traversal with a self-reflective multimodal expansion layer. Experimental results on DOCBENCH and SPIQA benchmarks show significant performance improvements over state-of-the-art RAG baselines, demonstrating superior handling of complex multimodal queries across various document types.

## Method Summary
SuperRAG processes documents through a parsing layer using an in-house parser fine-tuned on DocLayNet and 5,773 in-house pages, combined with Azure Document Intelligence for section detection. The parsed output is transformed into a property graph (LAGM) with nodes for documents, pages, sections, tables, diagrams, and chunks connected via IS_UNDER and HAS_NEXT relationships. The retrieval pipeline employs hybrid search (Milvus vector similarity + ElasticSearch BM25) plus graph traversal using LLM-generated Cypher queries or heuristics, with a self-reflection layer that conditionally expands retrieval to include tables and diagrams based on query intent. Retrieved contexts are re-ranked and passed to GPT-4o for final answer generation.

## Key Results
- Layout-aware approach achieves 7.3% accuracy improvement on DOCBENCH over non-layout-aware methods
- 9% accuracy gain on SPIQA Test-C with multimodal queries
- Outperforms state-of-the-art RAG baselines including GPT-4, Claude-3, and KimiChat
- Ablation shows self-reflection layer adds 4.1% improvement on DOCBENCH and 4.1% on SPIQA Test-C

## Why This Works (Mechanism)

### Mechanism 1: Layout-Preserving Graph Structure
The system constructs a property graph where nodes (Company → Document → Page → Section/SectionChunk/Table/Diagram) are connected via typed relationships (IS_UNDER, HAS_NEXT). This preserves hierarchical navigation paths—enabling retrieval to leverage structural context rather than treating content as isolated fragments. For example, finding which section contains a specific table becomes a graph traversal rather than a heuristic guess.

### Mechanism 2: Multi-Retriever Fusion with Graph Traversal
The pipeline runs three retrieval strategies: (1) Milvus vector similarity search, (2) ElasticSearch BM25 for keyword matching, and (3) Neo4J graph traversal via LLM-generated Cypher queries or heuristic ToC/table/diagram extraction. Retrieved candidates are merged, cross-page context is consolidated via graph edges, and a re-ranker selects top-k contexts before LLM generation.

### Mechanism 3: Self-Reflective Multimodal Expansion
After initial retrieval, a self-reflection module (LLM-based) evaluates whether the query requires visual/tabular information (e.g., mentions of charts, numerical comparisons). If yes, the pipeline retrieves associated Table or Diagram nodes via graph edges; if no, it skips expansion, avoiding noise from irrelevant images/tables.

## Foundational Learning

- **Concept: Property Graphs and Graph Databases (Neo4j, Cypher)**
  - Why needed here: LAGM represents documents as property graphs with typed nodes and relationships; retrieval uses Cypher queries generated by LLMs or heuristics.
  - Quick check question: Given nodes Document, Section, Table with IS_UNDER edges, write a Cypher query to find all Tables under a Section with header containing "Results".

- **Concept: Hybrid Retrieval (Vector + BM25)**
  - Why needed here: SuperRAG combines Milvus vector search with ElasticSearch BM25. Understanding embedding-based semantic similarity vs. term-frequency ranking is essential for debugging retrieval failures.
  - Quick check question: Why might BM25 outperform vector search for a query asking "What is the exact value of α in Equation 3?"

- **Concept: Multimodal Document Parsing (Layout Analysis, OCR, Table Structure Recognition)**
  - Why needed here: The in-house parser performs DLA, reading order detection, table structure recognition, and figure classification using models like DETR. Parsing failures propagate directly to graph quality.
  - Quick check question: If a borderless table is misclassified as prose text, how would this affect downstream graph construction and retrieval?

## Architecture Onboarding

- **Component map:**
  - Parsing Layer: In-house parser + Azure DI → JSON/Markdown with layout labels, tables, diagrams, reading order
  - Graph Construction: Parsed output → LAGM property graph in Neo4j with IS_UNDER/HAS_NEXT edges + KNN/has_stem augmentation
  - Indexing: Milvus (embeddings), ElasticSearch (BM25), Neo4j (graph traversal)
  - Retrieval: Hybrid (vector + BM25) + graph traversal (LLM Cypher or heuristics) + self-reflection for multimodal expansion
  - Re-Ranking: Cohere Reranker → top-k contexts
  - Generation: GPT-4o receives query + contexts → answer

- **Critical path:**
  1. Accurate parsing (DLA, table recognition) → correct graph structure
  2. Proper IS_UNDER/HAS_NEXT relationships → navigable retrieval
  3. Self-reflection correctly classifying intent → appropriate expansion
  4. Re-ranking quality → final context relevance

- **Design tradeoffs:**
  - In-house parser offers domain fine-tuning (DocLayNet + 5,773 pages) but requires maintenance; Azure DI supplements ToC detection but may not handle all layouts
  - LLM-generated Cypher is flexible but adds latency/cost; heuristics are faster but less adaptive
  - Self-reflection reduces noise but risks missing implicit multimodal needs

- **Failure signatures:**
  - Misclassified layout elements → missing Table nodes → retrieval fails on table queries
  - Incorrect IS_UNDER edges → disconnected sections → empty traversal results
  - Reflection misclassification → critical figures omitted or irrelevant tables included

- **First 3 experiments:**
  1. Validate parsing quality on domain documents using NID/TEDS metrics (replicate Table 1 comparisons)
  2. Ablate retrieval components: hybrid → +graph → +ToC → +expansion → +reflection (replicate Table 5)
  3. Stress-test self-reflection with explicit vs. implicit multimodal queries; measure false negatives/positives

## Open Questions the Paper Calls Out
- How can the retrieval pipeline be refined to successfully locate specific visual components (e.g., individual figures) that were missed in the output observation analysis?
- What mitigation strategies are effective for maintaining performance when the layout-aware graph modeling is applied to documents with noisy layouts or significant structural variations?
- What are the specific computational overheads and latency costs associated with the complex graph construction and LLM-based traversal in high-throughput environments?

## Limitations
- Proprietary parsing pipeline (in-house DLA model on 5,773 pages, reading order model on 5,010 images) limits reproducibility and may not generalize to different document layouts
- Self-reflection mechanism for multimodal expansion lacks specification of decision criteria, making its reliability uncertain
- Direct comparisons to concurrent graph-RAG approaches (KET-RAG, E²GraphRAG) are absent

## Confidence
- **High confidence**: Layout-aware graph modeling improves over flat text chunking (supported by 7.3% DOCBENCH gain and 9% SPIQA gain; corroborated by LAD-RAG)
- **Medium confidence**: Self-reflection layer adds value (shows 4.1% DOCBENCH improvement in ablation; limited corpus support)
- **Medium confidence**: Multi-retriever fusion superiority (combination beats individual methods; corroborated by KET-RAG/E²GraphRAG, but no direct ablation shown)

## Next Checks
1. Ablate the self-reflection component on SPIQA Test-C with explicit multimodal queries to measure false negative rate for figure/table retrieval
2. Compare LAGM graph construction quality against E²GraphRAG using standard document structure metrics (precision/recall of node/edge extraction)
3. Test the proprietary parser on out-of-domain documents to establish generalizability boundaries and failure rates