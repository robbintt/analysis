---
ver: rpa2
title: Human-like fleeting memory improves language learning but impairs reading time
  prediction in transformer language models
arxiv_id: '2508.05803'
source_url: https://arxiv.org/abs/2508.05803
tags:
- memory
- language
- fleeting
- reading
- decay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether human-like fleeting memory benefits
  language learning in transformer models. The authors introduce a "fleeting memory
  transformer" that adds a power-law memory decay to self-attention, simulating human
  forgetting.
---

# Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models

## Quick Facts
- arXiv ID: 2508.05803
- Source URL: https://arxiv.org/abs/2508.05803
- Reference count: 32
- Adding human-like power-law memory decay to transformers improves language learning but makes models worse at predicting human reading times.

## Executive Summary
This paper investigates whether adding human-like fleeting memory to transformer language models benefits learning. The authors introduce a "fleeting memory transformer" that applies power-law decay to self-attention, simulating human forgetting, and combine it with an "echoic memory buffer" that perfectly retains the first 3-7 words. Training these models on developmentally realistic BabyLM datasets (10M and 100M tokens), they find that fleeting memory consistently improves language modeling performance and BLiMP accuracy for syntactic knowledge - but only when combined with the echoic buffer. Surprisingly, these same models perform worse at predicting human reading times, with the impairment concentrated on low-frequency words. This dissociation cannot be explained by prior accounts of why better language models sometimes fit reading times worse. The results support the idea that memory limitations can benefit language learning in neural networks, while also revealing that architectural constraints making models more "human-like" don't necessarily make them better at predicting human behavior.

## Method Summary
The authors modify GPT-2 transformers by adding a power-law memory decay function B(d) = 1 - ((d-E+1)/(n-E))^(1/e^α) to self-attention weights, where d is token distance and E is the echoic buffer size. The models are trained on BabyLM datasets (10M and 100M tokens) containing CHILDES, OpenSubtitles, and children's stories. They evaluate language modeling performance via validation loss, syntactic knowledge via BLiMP accuracy across 12 phenomena, and human behavioral alignment via reading time prediction on Natural Stories (self-paced reading) and Dundee (eye-tracking) corpora using ∆ Log-Likelihood metrics.

## Key Results
- Fleeting memory with echoic buffer improves validation loss by ~0.006 and BLiMP accuracy by 1-3% at developmental scales
- Benefits are concentrated in local-structure phenomena (Subject-Verb Agreement, Anaphor Agreement, Argument Structure) but not lexical knowledge (Irregular Forms)
- Better language models with fleeting memory predict reading times worse, with impairment concentrated on low-frequency words
- The echoic buffer (retaining first 3-7 words perfectly) is necessary - naive decay without buffer degrades performance and causes spelling errors

## Why This Works (Mechanism)

### Mechanism 1: Memory Decay as Statistical Prior for Local Dependencies
- Claim: Power-law decay encodes the inductive bias that linguistic dependencies are predominantly local, improving learning efficiency in data-limited regimes.
- Mechanism: The retention function attenuates attention to distant tokens, concentrating representational capacity on statistically more probable local relationships. This reduces the hypothesis space the model must search.
- Core assumption: Linguistic dependencies follow a locality distribution where most informative relationships occur within a short window.
- Evidence anchors:
  - [section] Discussion states: "memory decay encodes the prior that dependencies in language are predominantly local"
  - [section] BLiMP improvements concentrated in local-structure phenomena (Subject-Verb Agreement, Anaphor Agreement, Argument Structure) while Irregular Forms (lexical knowledge) showed no change
  - [corpus] Related work on lossy-context surprisal (Futrell et al.) supports locality benefits for processing
- Break condition: Effect likely reverses at billion-token scales where models can learn long-range patterns without architectural guidance, or on text types with genuine long-range dependencies (code, academic papers).

### Mechanism 2: Abstraction Pressure Through Information Forgetting
- Claim: Rapid decay of exact wordforms creates pressure to discover compressive abstractions (chunking, syntactic structure) rather than memorizing surface statistics.
- Mechanism: When verbatim memory is unavailable, the learner must encode relationships at a higher level of representation to preserve predictive information across time.
- Core assumption: Surface-level statistical regularities between distant words are less generalizable than abstract structural patterns.
- Evidence anchors:
  - [section] Discussion: "because exact wordforms decay rapidly, the learner must discover higher-level abstractions to preserve information"
  - [section] BLiMP syntactic evaluation improved (structural knowledge) while lexical tasks (Irregular Forms) did not
  - [corpus] Limited direct evidence; mechanism is inferred rather than directly tested
- Break condition: Assumption: If the task primarily rewards surface pattern matching rather than structural generalization, abstraction pressure may not confer benefits.

### Mechanism 3: Echoic Buffer Prevents Disruption of Critical Local Processing
- Claim: Perfect retention of initial 3-7 tokens is necessary to preserve within-word and immediately-local dependencies that would otherwise be corrupted by decay.
- Mechanism: The buffer acts as a protected region where subword token relationships and adjacent-word syntax remain intact, while decay operates beyond this window.
- Core assumption: Critical low-level processing (morphology, immediate syntactic agreement) operates within ~5 tokens.
- Evidence anchors:
  - [section] Results: Naive decay (no buffer) caused spelling errors—"even the most local (within-word, across-token) dependencies were disrupted"
  - [section] Figure 3D shows significant loss improvement only when buffer is present
  - [corpus] Not directly addressed in related work
- Break condition: If buffer size is set too small (<3 tokens), local dependencies break; if too large, the inductive bias benefits diminish.

## Foundational Learning

- Concept: Power-law retention functions in cognitive modeling
  - Why needed here: The decay function B(d) uses power-law rather than exponential decay, motivated by psychological memory models (Donkin & Nosofsky, 2012). Understanding this helps interpret α parameter choices.
  - Quick check question: Can you explain why power-law decay might capture human memory better than linear decay for language?

- Concept: Surprisal theory and reading time prediction
  - Why needed here: Reading time evaluation assumes human processing difficulty correlates with negative log probability (surprisal). The paper's central paradox—a better LM predicting reading times worse—requires understanding this link.
  - Quick check question: What is the assumed relationship between model surprisal and human reading times?

- Concept: Developmentally realistic training (BabyLM regime)
  - Why needed here: All effects are demonstrated at 10-100M token scales (child-like exposure). Results may not transfer to web-scale pretraining where memorization dominates.
  - Quick check question: Why might the fleeting memory benefit disappear at billion-token training scales?

## Architecture Onboarding

- Component map:
  - Standard GPT-2 backbone (6 layers, 6 heads, 384 hidden dim, 8K vocab)
  - Fleeting Memory Layer: Bias matrix B applied post-softmax, element-wise with attention weights
  - Echoic Buffer: Hard-coded retention=1 region for first E tokens
  - Power-Law Decay: B(d) = 1 - ((d-E+1)/(n-E))^(1/e^α) for d ≥ E

- Critical path:
  1. Compute standard attention scores via QK^T
  2. Apply causal mask (future tokens)
  3. Apply softmax
  4. **Key insertion point:** Element-wise multiply with pre-computed retention matrix B
  5. Multiply by V and proceed

- Design tradeoffs:
  - Decay strength (α) vs. echoic buffer size (E): Strong decay + adequate buffer = best results; either alone fails
  - Fixed vs. learnable decay: Authors use fixed, parameter-free decay for interpretability; ALiBi (Press et al.) uses learnable head-specific biases but sacrifices cognitive interpretability
  - Data regime: Benefits proven only for 10-100M tokens; assumption that effects reverse at scale

- Failure signatures:
  - Spelling errors / subword corruption → Echoic buffer too small or missing
  - No BLiMP improvement despite lower loss → Decay too weak (α too low)
  - Degraded long-context tasks → Decay too aggressive for task requirements
  - Training instability → Retention matrix not properly normalized

- First 3 experiments:
  1. **Reproduction test:** Train paired models (identical seeds) with/without fleeting memory (α=3, E=10) on 10M BabyLM; verify validation loss delta ≈ -0.006
  2. **Buffer ablation:** Systematically vary E ∈ {0, 3, 5, 10, 15} with fixed decay; confirm naive (E=0) degrades performance while E∈{5,10} improves it
  3. **Reading time evaluation:** Extract surprisal from both model types on Natural Stories corpus; fit mixed-effects regression and verify fleeting memory shows lower ΔLL despite better language modeling

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does fleeting memory improve learning solely by encoding a statistical bias for local dependencies, or does it qualitatively drive the discovery of higher-level abstractions?
- **Basis in paper:** [explicit] The authors state in the Discussion that "our current findings are not sufficient to conclude whether fleeting memory operates only through statistical biasing or (also) drives qualitatively different learning."
- **Why unresolved:** The current evaluation relies on behavioral metrics (BLiMP scores) which capture linguistic knowledge but do not reveal the internal representational changes.
- **What evidence would resolve it:** Mechanistic analyses of layer-wise abstraction, attention head specialization, or the temporal dynamics of feature emergence during training.

### Open Question 2
- **Question:** Do the observed benefits of memory limitations vanish or reverse when applied to contemporary pretraining scales (billions to trillions of tokens)?
- **Basis in paper:** [explicit] The authors hypothesize that "The benefits we observe would likely vanish or reverse at contemporary pretraining scales, where models train on billions to trillions of tokens."
- **Why unresolved:** The experiments were restricted to developmentally realistic, data-limited regimes (10M and 100M tokens) to model human acquisition.
- **What evidence would resolve it:** Training transformer models with and without fleeting memory on web-scale datasets (e.g., The Pile) to observe if the performance advantage persists.

### Open Question 3
- **Question:** Can a content-sensitive retention function better approximate human memory than the fixed, distance-based decay used in this study?
- **Basis in paper:** [explicit] The authors note that "human forgetting is content-sensitive and non-monotonic" and suggest "content-dependent retention functions" as a specific future extension (Discussion).
- **Why unresolved:** The current implementation utilizes a fixed, parameter-free power-law decay based strictly on token distance, ignoring the semantic nature of the retained information.
- **What evidence would resolve it:** Implementing retention functions that selectively preserve high-information or unpredictable words while allowing predictable ones to fade faster, and comparing performance against the distance-based model.

## Limitations

- **Data regime specificity**: Benefits demonstrated only at developmental scales (10-100M tokens); effects likely reverse at billion-token pretraining scales
- **Mechanism isolation**: Unclear whether benefits come from decay encoding locality bias, abstraction pressure from forgetting, or buffer protecting local processing
- **Behavioral prediction dissociation**: The finding that better models predict reading times worse is surprising but not fully explained; may be specific to fleeting memory or general property of better models

## Confidence

**High confidence**: Fleeting memory with echoic buffer improves language modeling loss and BLiMP syntactic accuracy at developmental scales. Directly demonstrated through paired comparisons across 10 seeds with consistent effect sizes.

**Medium confidence**: Memory decay encodes locality bias that benefits learning efficiency. Supported by BLiMP improvements concentrating on local-structure phenomena, but mechanism is inferred rather than directly tested.

**Low confidence**: The abstraction pressure mechanism and its specific role in syntactic generalization. Primarily theoretical reasoning without direct experimental validation separating decay from buffer effects.

## Next Checks

1. **Scale extrapolation test**: Train fleeting memory models at web-scale (1B+ tokens) to empirically verify whether benefits disappear as predicted, and whether reading time dissociation persists or resolves at scale.

2. **Mechanism decomposition**: Design controlled experiments isolating three proposed mechanisms - test (a) power-law decay alone without buffer, (b) buffer alone without decay, and (c) different decay shapes (exponential vs power-law) to determine which components drive benefits.

3. **Behavioral prediction triangulation**: Evaluate whether other architectural modifications that improve language modeling (different attention biases, dropout patterns) also show the same dissociation from reading time prediction, to determine if this is a general property of better models or specific to fleeting memory.