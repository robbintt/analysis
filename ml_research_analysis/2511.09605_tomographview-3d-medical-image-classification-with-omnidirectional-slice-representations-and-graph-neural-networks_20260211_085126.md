---
ver: rpa2
title: 'TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice
  Representations and Graph Neural Networks'
arxiv_id: '2511.09605'
source_url: https://arxiv.org/abs/2511.09605
tags:
- slicing
- performance
- medical
- graph
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TomoGraphView improves 3D medical image classification by combining
  omnidirectional volume slicing with spherical graph-based feature aggregation. Traditional
  slicing along axial, coronal, and sagittal planes often misses structures not aligned
  with these views.
---

# TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice Representations and Graph Neural Networks

## Quick Facts
- arXiv ID: 2511.09605
- Source URL: https://arxiv.org/abs/2511.09605
- Reference count: 40
- Key result: AUROC improved from 0.7701 to 0.8372 on oncology datasets

## Executive Summary
TomoGraphView addresses the limitation of traditional 2.5D slicing in medical image classification by introducing omnidirectional slicing combined with spherical graph neural networks. The method samples slices from uniformly distributed points on a sphere enclosing the volume, capturing both canonical and non-canonical perspectives of anatomical structures. By encoding these slices with a frozen DINOv2 model and aggregating them through a graph neural network on a spherical mesh, TomoGraphView achieves superior performance compared to traditional 2.5D approaches and large-scale pretrained 3D models across six oncology datasets.

## Method Summary
The framework samples omnidirectional slices from uniformly distributed points on a sphere enclosing the input volume, rather than limiting analysis to axial, coronal, and sagittal planes. Each slice is encoded using a frozen DINOv2 vision transformer, producing feature representations. These features are then aggregated using a graph neural network operating on a spherical mesh, where nodes represent slice locations and edges encode spatial relationships. This approach preserves spatial relationships while capturing information from multiple viewing angles, enabling the model to detect structures not aligned with traditional viewing planes.

## Key Results
- Omnidirectional slicing improved AUROC from 0.7701 to 0.8154 compared to 2.5D slicing
- Full TomoGraphView framework further improved AUROC to 0.8372
- Outperformed large-scale pretrained 3D models on six oncology datasets
- Slice-wise aggregation baselines achieved lower performance than the spherical graph approach

## Why This Works (Mechanism)
Traditional medical image classification relies on fixed anatomical planes (axial, coronal, sagittal) that may miss critical structures not aligned with these views. Omnidirectional slicing overcomes this limitation by sampling from all directions, ensuring comprehensive coverage of the volume. The spherical graph neural network preserves the geometric relationships between slices while allowing information to flow across different viewing angles. Using a frozen pretrained encoder like DINOv2 provides rich visual features without requiring extensive training on medical data, while the graph architecture adapts these features to the specific spatial configuration of each case.

## Foundational Learning
- **Spherical Sampling**: Uniformly distributing points on a sphere to capture multiple viewing angles; needed because anatomical structures may not align with standard planes
- **Graph Neural Networks**: Message-passing between nodes on a spherical mesh to aggregate spatial information; needed to maintain geometric relationships while combining information from different slices
- **Frozen Pretrained Encoders**: Using DINOv2 features without fine-tuning; needed to leverage large-scale visual knowledge without requiring medical domain adaptation
- **Slice-wise Feature Extraction**: Processing individual 2D slices independently before aggregation; needed to maintain computational efficiency while handling volumetric data
- **Spherical Mesh Construction**: Creating graph structure from 3D coordinates on a sphere; needed to preserve spatial topology in the aggregation process

## Architecture Onboarding

**Component Map**: Volume -> Spherical Sampling -> 2D Slice Extraction -> DINOv2 Encoding -> Spherical Graph Construction -> GNN Aggregation -> Classification

**Critical Path**: The spherical sampling and GNN aggregation steps are critical, as they enable the omnidirectional perspective and preserve spatial relationships that drive performance improvements.

**Design Tradeoffs**: The frozen DINOv2 backbone provides strong visual features but prevents domain-specific adaptation. Omnidirectional slicing increases computational cost but captures more comprehensive information. The spherical graph approach maintains spatial relationships but requires careful mesh construction.

**Failure Signatures**: Performance degradation occurs when anatomical structures are extremely small relative to the volume, when the spherical sampling density is insufficient for fine-grained structures, or when the graph neural network fails to properly aggregate features across viewing angles.

**First Experiments**:
1. Evaluate AUROC improvement from 2.5D to omnidirectional slicing on a single oncology dataset
2. Compare frozen vs fine-tuned DINOv2 backbone performance on the same task
3. Test different spherical mesh resolutions to identify optimal trade-off between accuracy and computation

## Open Questions the Paper Calls Out
**Open Question 1**: Can the slice selection mechanism be effectively replaced by a bounding-box-based or detection-guided approach to remove the dependency on pixel-wise segmentation masks? The current implementation requires masks to identify the "largest lesion area" for slice alignment, which limits applicability in scenarios where only coarse localization is available.

**Open Question 2**: How can multimodal large language models (MLLMs) be effectively integrated to automate the localization of regions of interest within the TomoGraphView pipeline? The framework currently operates as a classification model on pre-cropped volumes and lacks an internal mechanism for automatic target localization or detection.

**Open Question 3**: Does unfreezing the 2D vision encoder (DINOv2) for end-to-end fine-tuning yield significant performance improvements over the frozen backbone approach? The performance gap between TomoGraphView and 3D models might narrow further if the 2D features were specifically adapted to the medical domain rather than used as generic frozen features.

## Limitations
- Computational challenges when scaling to larger volumes due to multiple slice processing
- Assumes uniform distribution of points on the sphere, which may not optimally capture anisotropic anatomical structures
- Frozen feature extractor prevents domain-specific fine-tuning for rare pathologies
- Current dependency on pixel-wise segmentation masks for slice alignment limits practical deployment

## Confidence
**High confidence**: AUROC improvements from 0.7701 to 0.8372 are statistically significant and robust across six oncology datasets. The omnidirectional slicing methodology and spherical graph aggregation are well-defined and reproducible.

**Medium confidence**: The claim that TomoGraphView outperforms large-scale pretrained 3D models is based on comparison with specific baselines rather than a comprehensive evaluation across multiple 3D architectures. Computational efficiency claims need independent validation.

**Low confidence**: Generalization of omnidirectional slicing benefits to non-oncology medical imaging domains remains untested. Framework's performance on extremely small or sparse volumes has not been established.

## Next Checks
1. Benchmark TomoGraphView against a broader range of 3D CNN architectures (e.g., Swin3D, UNETR) on the same oncology datasets to verify relative performance claims.

2. Evaluate memory and computational requirements scaling with input volume size across different spherical mesh resolutions to establish practical deployment limits.

3. Test omnidirectional slicing and graph aggregation on non-oncology domains (cardiac imaging, neurology) to assess cross-domain generalization and identify anatomical structures where the approach excels or fails.