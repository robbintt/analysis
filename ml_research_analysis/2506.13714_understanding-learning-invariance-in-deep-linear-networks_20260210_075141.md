---
ver: rpa2
title: Understanding Learning Invariance in Deep Linear Networks
arxiv_id: '2506.13714'
source_url: https://arxiv.org/abs/2506.13714
tags:
- linear
- data
- networks
- learning
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares three approaches for achieving invariance
  in deep linear networks: data augmentation, regularization, and hard-wiring. The
  authors analyze the optimization landscapes of these approaches in the context of
  mean squared error regression with rank-bounded linear maps.'
---

# Understanding Learning Invariance in Deep Linear Networks

## Quick Facts
- arXiv ID: 2506.13714
- Source URL: https://arxiv.org/abs/2506.13714
- Reference count: 40
- Primary result: Three approaches to invariance in deep linear networks—data augmentation, regularization, and hard-wiring—produce identical critical points and global optima in function space under specific conditions.

## Executive Summary
This paper provides a theoretical analysis of three approaches for achieving invariance in deep linear networks: data augmentation, explicit regularization, and hard-wiring constraints into the architecture. The authors prove that when trained with mean squared error loss and using unitary group representations, data augmentation and hard-wiring produce identical optimization landscapes with the same critical points (saddles and global optimum). Regularization creates a continuous path in function space that converges to the hard-wired solution as the penalty parameter increases. The global optima for all three approaches are proven to be equivalent, corresponding to the rank-constrained projection onto the invariant subspace. Experimental results on MNIST classification validate these theoretical findings.

## Method Summary
The authors analyze three approaches for achieving invariance in deep linear networks with rank constraints. For data augmentation, they train on all group-transformed versions of the data; for hard-wiring, they constrain the network to invariant functions; for regularization, they add a penalty term for invariance violation. The theoretical analysis focuses on characterizing critical points and global optima in function space, using determinantal varieties and projection operators. Experiments use a two-layer linear network with 5 hidden units on downsampled MNIST (1000 training samples, 9 classes excluding digit 9). Three variants are trained: (1) data augmentation with full C4 group orbit, (2) hard-wired invariant model with constraint matrix B, and (3) regularized model with penalty λ||WG||². All are trained with Adam for 2000 epochs using MSE loss.

## Key Results
- Data augmentation and hard-wiring produce identical critical points in function space (only saddles and global optimum)
- Regularization creates a continuous path converging to the hard-wired solution as λ→∞
- All three approaches share the same global optimum corresponding to rank-constrained projection onto invariant subspace
- Experiments on MNIST show data augmentation and hard-wiring achieve similar performance, while regularization with appropriate λ also achieves comparable results

## Why This Works (Mechanism)

### Mechanism 1: Critical Point Equivalence Between Data Augmentation and Hard-wiring
When a deep linear network is trained with full data augmentation, the optimization landscape in function space is constrained such that all critical points are identical to those obtained by directly constraining the function space to invariant maps. This holds under unitary representations and rank-bounded function spaces.

### Mechanism 2: Regularization Path Convergence
Explicit regularization on invariance violation creates a continuous path in function space that converges to the hard-wired solution as the penalty increases. The regularization term λ||WG||² modifies the loss landscape smoothly, with the global optimum approaching the constrained invariant solution as λ → ∞.

### Mechanism 3: Global Optimum Identity via Low-Rank Invariant Projection
All three approaches share the same global optimum when approximating a target matrix under rank constraints. This is achieved by projecting the target matrix to the invariant subspace (left null space of the constraint operator) then taking the rank-r approximation.

## Foundational Learning

- **Concept**: Group equivariance and invariance (Definition 2.1-2.2)
  - Why needed here: Understanding the constraint WG=0 and why unitary representations matter.
  - Quick check question: Can you explain why Wρ(g) = W implies W(Id - ρ(g)) = 0 for cyclic groups?

- **Concept**: Determinantal varieties and rank constraints
  - Why needed here: The function space of deep linear networks with bottlenecks is non-convex due to rank constraints.
  - Quick check question: Why is Mr = {W : rank(W) ≤ r} non-convex when r < min(d0, dL)?

- **Concept**: SVD-based low-rank approximation (Eckart-Young-Mirsky)
  - Why needed here: The proofs rely on projecting and truncating SVD components.
  - Quick check question: Given a matrix A, what is its best rank-r approximation in Frobenius norm?

## Architecture Onboarding

- **Component map**: Input -> Data matrix X with group action ρ(g) -> Constraint operator G = Id - ρ(g) -> Function space (Rank-bounded matrices Mr intersected with invariant subspace) -> Three optimization heads: (1) constrained space, (2) data-augmented loss, (3) regularized loss

- **Critical path**:
  1. Compute the constraint matrix G from the group representation
  2. For hard-wiring: Project to invariant subspace, then solve low-rank approximation
  3. For data augmentation: Apply all group actions to training data, solve equivalent problem
  4. For regularization: Solve modified problem, track path as λ varies

- **Design tradeoffs**:
  - Data augmentation: No architecture changes, but increases effective sample size and parameter count; higher compute
  - Hard-wiring: Minimal parameters, fastest training; requires explicit constraint implementation
  - Regularization: Intermediate compute; introduces additional critical points; requires λ tuning

- **Failure signatures**:
  - Non-unitary representations: Data augmentation may not yield invariant global optima
  - Cross-entropy loss: Critical point structure not guaranteed to be invariant
  - Rank constraint violation: If r ≥ nullity(G), rank constraint becomes non-binding

- **First 3 experiments**:
  1. Replicate MNIST rotation invariance: Train linear network with bottleneck (r=5) using all three methods; verify convergence to same test accuracy
  2. Ablate λ: Sweep regularization strength; plot regularization path continuity and deviation from hard-wired solution
  3. Stress test non-unitary representation: Replace rotation with a non-orthogonal transformation; compare data augmentation vs hard-wiring global optima

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the "double descent" pattern observed in the evolution of the non-invariant component ∥W⊥∥F during training with data augmentation and regularization?
- Basis in paper: [explicit] Section 4.3 states: "Our conjecture is that the loss may also be decomposed into two parts, one controlling the error from invariance, and the other one controlling the error from the target... Further research needs to be done to investigate this both theoretically and empirically."
- Why unresolved: The authors only provide an intuitive conjecture without formal analysis or proof of this phenomenon.
- What evidence would resolve it: A theoretical analysis of the loss decomposition and gradient dynamics that formally predicts the double descent pattern, along with controlled experiments varying the relative strengths of invariance error vs. target error.

### Open Question 2
- Question: Do the theoretical results about critical points and global optima extend to networks trained with cross-entropy loss rather than mean squared error?
- Basis in paper: [explicit] Appendix A.11 states: "Our theoretical results only support the scenario for mean squared loss. Thus, when trained with cross entropy, we cannot say whether all the critical points are invariant or not. Future work can be done to investigate the critical points when trained with cross entropy loss."
- Why unresolved: The theoretical analysis relies on properties specific to MSE loss, and cross-entropy experiments show different dynamics for ∥W⊥∥F.
- What evidence would resolve it: A characterization of critical points in function space for cross-entropy loss, potentially showing whether invariant solutions remain global optima.

### Open Question 3
- Question: Can the equivalence between data augmentation and hard-wired invariance be extended to nonlinear networks with arbitrary activation functions?
- Basis in paper: [explicit] Section 6 states: "It would be interesting to investigate this phenomenon theoretically." referring to the empirical observation that "nonlinear networks may still learn invariance via data augmentation when trained with enough data."
- Why unresolved: The theoretical results rely on linear network structure and rank-bounded linear maps; nonlinear activations introduce non-convexities not captured by the current analysis.
- What evidence would resolve it: Proofs characterizing critical points in nonlinear networks with data augmentation, or counterexamples showing where invariance is not learned.

## Limitations

- Theoretical results critically depend on unitary group representations and mean squared error loss, with limited applicability to non-unitary transformations or cross-entropy losses
- Analysis is restricted to rank-bounded linear maps, limiting direct applicability to nonlinear deep networks with arbitrary architectures
- Experimental validation uses simplified MNIST task with only 1000 training samples and two-layer linear architecture, which may not capture challenges in more complex settings

## Confidence

- **High Confidence**: The critical point equivalence results (Mechanism 1) are rigorously proven with clear mathematical foundations in determinantal varieties and constrained optimization theory.
- **Medium Confidence**: The regularization path convergence (Mechanism 2) relies on technical assumptions about full rank and distinct singular values that may not always hold in practice.
- **Medium Confidence**: The global optimum identity claim (Mechanism 3) is well-supported theoretically but depends on the rank constraint being active and the target matrix having appropriate structure.

## Next Checks

1. **Non-unitary Representation Test**: Implement a non-orthogonal transformation group (e.g., shear transformation) and verify whether data augmentation and hard-wiring still converge to identical global optima, as predicted by Theorem 3.9's assumptions.

2. **Cross-entropy Loss Extension**: Extend the theoretical analysis to cross-entropy loss and empirically test whether critical point structures remain equivalent across the three approaches when applied to classification tasks.

3. **Deep Nonlinear Network Validation**: Test whether the theoretical insights about invariance learning transfer to deeper nonlinear architectures by implementing invariant bottleneck layers in a multi-layer network and comparing optimization dynamics across approaches.