---
ver: rpa2
title: Non-Uniform Noise-to-Signal Ratio in the REINFORCE Policy-Gradient Estimator
arxiv_id: '2602.01460'
source_url: https://arxiv.org/abs/2602.01460
tags:
- policy
- estimator
- reinforce
- ratio
- policy-gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the noise-to-signal ratio (NSR) of the
  REINFORCE policy-gradient estimator in reinforcement learning. The NSR is defined
  as the estimator variance normalized by the squared norm of the true gradient, and
  serves as a measure of how informative a stochastic gradient is relative to its
  mean.
---

# Non-Uniform Noise-to-Signal Ratio in the REINFORCE Policy-Gradient Estimator

## Quick Facts
- arXiv ID: 2602.01460
- Source URL: https://arxiv.org/abs/2602.01460
- Reference count: 40
- Primary result: Shows REINFORCE estimator NSR can grow unbounded near optimal policies, explaining training instability.

## Executive Summary
This paper analyzes the noise-to-signal ratio (NSR) of the REINFORCE policy-gradient estimator, showing that NSR can increase sharply and even blow up as policies approach optimality in structured RL problems. The authors provide exact closed-form expressions for NSR in one-step linear-quadratic systems, an exact numerical procedure for multi-step linear systems, and extend the analysis to polynomial systems. For general nonlinear dynamics, they derive an upper bound on the variance. The analysis reveals that NSR scales inversely with policy covariance as policies become more deterministic, scales with initial-state covariance as distributions broaden, and can grow exponentially with horizon when closed-loop dynamics are unstable. Through experiments on linear and polynomial systems, the paper demonstrates that NSR typically increases as policies approach optimality, explaining observed training instability and policy collapse in practice.

## Method Summary
The paper computes the noise-to-signal ratio (NSR) for the REINFORCE estimator in finite-horizon LQG and polynomial systems. The method uses Gaussian policies π(a|s)=N(Ks,Σ) and computes closed-form NSR via Isserlis theorem for one-step systems (Theorem 3) and lifted dynamics for multi-step systems (Theorem 5). The authors analyze how NSR scales with policy covariance, initial-state covariance, and horizon, providing exact expressions and numerical procedures. Experiments compare GD, SGD, and Adam optimizers on double integrator systems, tracking NSR and objective J(K,Σ) along optimization trajectories.

## Key Results
- NSR grows unbounded as policy covariance Σ → 0 near optimality, causing variance to dominate signal
- NSR scales with initial-state covariance tr(Σ₀) as distributions become broader
- For unstable closed-loop dynamics (ρ(F) > 1), variance grows exponentially with horizon T
- NSR typically increases as policies approach optimality, explaining training instability and policy collapse

## Why This Works (Mechanism)

### Mechanism 1: Deterministic Policy Gradient Explosion
NSR grows unbounded as policy covariance Σ → 0 near optimality, destabilizing SGD. The variance term E₁,ℓ = Θ(σ⁴₀/σ²) dominates when σ → 0, causing variance to blow up while the true gradient remains Θ(σ⁴₀), yielding NSR = Θ(σ₀²/σ²). This occurs because optimal policies are deterministic (Σ = 0), and gradient descent pushes toward this limit.

### Mechanism 2: Initial-State Uncertainty Amplification
Broader initial-state distributions increase NSR proportionally to tr(Σ₀). Terms like IS_Σ₀(In, M_ss, M_ss) scale with initial-state covariance; the signal (true gradient norm) does not grow as fast as variance when Σ₀ increases. This happens because the variance computation explicitly depends on Σ₀ through the lifted state map.

### Mechanism 3: Horizon-Coupled Instability Cascade
When closed-loop spectral radius ρ(F) > 1, variance grows exponentially with horizon T. The lifted state map F_S has spectral norm ∥F_S∥₂² = O(ρ(F)²ᵀ) when ρ(F) > 1, amplifying variance through lifted dynamics. This causes the REINFORCE estimator to become increasingly noisy as the planning horizon increases.

## Foundational Learning

- **Noise-to-Signal Ratio (NSR)**: Core diagnostic metric that determines whether SGD can converge; NSR > 1 implies gradient noise dominates signal. Quick check: For your current policy, can you estimate NSR via Monte Carlo rollouts of the gradient estimator?

- **Score Function / Likelihood Ratio Gradient**: REINFORCE uses ∇θ log π(a|s) = Σ⁻¹(at - Kst)s_t^⊤; understanding this is essential for variance analysis. Quick check: What is the variance of the score function when action noise σ → 0?

- **Isserlis/Wick Theorem for Gaussian Moments**: Enables exact closed-form variance computation via IS_Ω(A₁,...,Aₖ) shorthand. Quick check: Given x ~ N(0, Ω), what is E[(x^⊤Ax)(x^⊤Bx)]?

## Architecture Onboarding

- **Component map**: REINFORCE estimator → bG_θ = Σₜ ∇θ log π(a_t|s_t) · R(τ) → Policy parameters (K, Σ) → Lifted dynamics F_S, F_E, K_S, K_E → NSR computation

- **Critical path**: Initialize policy (K₀, Σ₀) with Σ₀ ≻ 0 for exploration → Sample trajectories → Compute REINFORCE estimator → Monitor NSR alongside objective → Converge toward K* while preventing Σ → 0 too quickly

- **Design tradeoffs**: Entropy regularization vs. optimality (maintaining σ > 0 keeps NSR bounded but may sacrifice return); Batch size vs. compute (larger batches reduce noise but scale poorly with NSR blowup); Horizon reduction vs. task coverage (discounting reduces exponential variance but may miss long-horizon structure)

- **Failure signatures**: Oscillating objective near optimum (Figure 2: high NSR causes gradient noise to dominate); Policy collapse (Figure 4: single bad gradient step drives log-std → -∞, NSR → ∞); Divergence in unstable systems (Figure 3: exponential variance growth for ρ(F) > 1)

- **First 3 experiments**: NSR landscape mapping (compute NSR analytically/numerically across (K, σ) grid); Optimizer comparison on double-integrator (run GD, SGD, Adam with identical initialization; plot trajectories overlaid on NSR heatmap); Horizon sensitivity test (vary T from 5 to 50 for systems with ρ(F) < 1, = 1, > 1; measure variance growth rate)

## Open Questions the Paper Calls Out

- Can variance-reduction techniques (e.g., baselines, actor-critic methods) fundamentally alter the NSR scaling behavior near optima, or do they merely provide constant-factor improvements? The paper states extending analysis beyond REINFORCE to estimators with baselines could make conclusions broader, but this remains uncharacterized.

- Does adaptive covariance or entropy control provide asymptotic stability guarantees in high-NSR regimes, or does it merely delay instability? The authors propose studying how NSR interacts with exploration may lead to more robust algorithms, but whether entropy-regularized schemes can provably avoid NSR blow-up is unknown.

- What are the precise convergence conditions for stochastic policy gradient when NSR grows unbounded near the optimum? The paper shows NSR can grow unbounded but does not establish whether SGD/Adam can still converge under such conditions, nor what step-size or batch-size schedules might suffice.

## Limitations

- Empirical scope focuses on linear-quadratic and low-dimensional polynomial systems; scaling to high-dimensional or non-Gaussian settings remains unclear
- Does not validate findings on standard RL benchmarks (Atari, MuJoCo) where closed-loop stability and horizon effects may differ
- NSR computation for multi-step systems requires O(n⁶) matrix operations in worst case, limiting practicality for large state spaces

## Confidence

- **High confidence**: NSR blowup as Σ → 0 (Theorem 3, Corollary 6), variance scaling with initial-state covariance (Theorem 3), and exponential variance growth for ρ(F) > 1 (Theorem 8)
- **Medium confidence**: Multi-step NSR computation via lifted dynamics (Theorem 5), polynomial system extensions, and practical implications for training instability
- **Low confidence**: NSR behavior in general nonlinear dynamics (Theorem 9 bound is loose), and effectiveness of proposed NSR-aware training heuristics without empirical validation

## Next Checks

1. **NSR landscape mapping**: For a chosen LQG system, compute NSR analytically across a grid of (K, σ) values and identify high-NSR regions where optimization becomes unstable

2. **Optimizer comparison**: Replicate Figure 2 by running GD, SGD, and Adam on the double integrator, plotting trajectories over the NSR landscape to observe how different optimizers navigate high-variance regions

3. **Horizon sensitivity test**: For systems with ρ(F) < 1, = 1, and > 1, measure variance growth as T increases from 5 to 50 to verify exponential scaling predicted by Theorem 8