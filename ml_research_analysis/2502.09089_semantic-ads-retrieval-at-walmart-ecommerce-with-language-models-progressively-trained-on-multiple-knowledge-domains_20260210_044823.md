---
ver: rpa2
title: Semantic Ads Retrieval at Walmart eCommerce with Language Models Progressively
  Trained on Multiple Knowledge Domains
arxiv_id: '2502.09089'
source_url: https://arxiv.org/abs/2502.09089
tags:
- search
- walmart
- retrieval
- embedding
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving semantic ad retrieval
  in e-commerce search, where matching user queries with relevant sponsored products
  is difficult due to language ambiguity, sparse data, and large volumes. To tackle
  this, the authors propose a two-stage progressive training framework using a DistilBERT-based
  two-tower Siamese network.
---

# Semantic Ads Retrieval at Walmart eCommerce with Language Models Progressively Trained on Multiple Knowledge Domains

## Quick Facts
- **arXiv ID:** 2502.09089
- **Source URL:** https://arxiv.org/abs/2502.09089
- **Reference count:** 22
- **Primary result:** Two-stage progressive training framework achieves up to 16% improvement in NDCG and 4% reduction in irrelevant ads rate over baseline DSSM model.

## Executive Summary
This paper addresses the challenge of semantic ad retrieval in e-commerce search by proposing a two-stage progressive training framework using a DistilBERT-based two-tower Siamese network. The model is first pretrained on product category labels to ground the embedding space, then fine-tuned using diverse datasets including natural language inference data, search logs, and ad interactions. A human-in-the-loop mechanism dynamically adjusts training weights to improve relevance. The approach demonstrates significant improvements in both offline metrics and online A/B testing, achieving higher click-through rates, cost efficiency, and ad revenue across multiple placements.

## Method Summary
The approach uses a two-stage progressive training framework with a DistilBERT two-tower Siamese network. Stage 1 involves pre-training on product category labels using cross-entropy loss to create a semantically grounded embedding space. Stage 2 fine-tunes the model using diverse datasets (SNLI, MultiNLI, search logs, ad interactions) with cosine similarity objectives and hard negative mining. The training incorporates a human-in-the-loop mechanism that dynamically adjusts domain sampling weights based on nDCG scores, allowing the model to focus more on underperforming domains. The system uses 384-dimensional embeddings and employs triplet loss for e-commerce-specific data.

## Key Results
- Up to 16% improvement in NDCG@20 over baseline DSSM model
- 4% reduction in irrelevant ads rate (IAR@20)
- Significant gains in click-through rates, cost efficiency, and ad revenue confirmed through online A/B testing
- Model successfully deployed in production with stable and scalable performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-training on domain-specific taxonomy labels grounds the embedding space before fine-tuning for semantic similarity.
- **Mechanism:** The model first learns to map text to discrete product categories, creating a vector space where semantic proximity aligns with taxonomic proximity, reducing search space for subsequent Siamese fine-tuning.
- **Core assumption:** Walmart's categorical hierarchy accurately reflects user mental models of product similarity, and distinct tokens in product titles are sufficient predictors of these categories.
- **Evidence anchors:** Abstract states approach enhances model's understanding of Walmart product semantics through category pre-training; Section 2.1 describes multi-class classification tasks leveraging product category data.
- **Break condition:** If the product taxonomy is noisy, inconsistent, or too coarse, the model may learn rigid boundaries that fail to capture nuanced user intent.

### Mechanism 2
- **Claim:** Incorporating "hard negatives" forces the model to resolve fine-grained semantic ambiguities that random negatives miss.
- **Mechanism:** Hard negatives (items frequently shown but rarely clicked) share many lexical and semantic features with the query but differ in user intent, tightening the decision boundary in the embedding space.
- **Core assumption:** Lack of clicks on displayed items is a reliable proxy for irrelevance rather than a result of pricing, visual unattractiveness, or position bias.
- **Evidence anchors:** Section 2.2 defines hard negatives as items often displayed yet seldom clicked; Section 4.2 shows 3.6% average drop in NDCG when not using hard negative training datasets.
- **Break condition:** If position bias is strong, items in lower positions might be incorrectly labeled as "hard negatives," teaching the model to falsely penalize relevant but poorly ranked products.

### Mechanism 3
- **Claim:** Dynamically adjusting sampling probability of different knowledge domains based on human evaluation scores prevents overfitting to "easy" domains.
- **Mechanism:** Uses a sigmoid-like function to inversely weight domain sampling; domains with high nDCG scores receive lower weights, forcing more training on underperforming domains.
- **Core assumption:** nDCG score from human-labeled data is sufficient for determining optimal training distribution, and training on weaker domains improves them without catastrophic forgetting of stronger domains.
- **Evidence anchors:** Section 2.1 describes recalibration of training and sampling weights based on human-labeled data assessment; inverse relationship allows de-prioritizing domains where performance is already satisfactory.
- **Break condition:** If human-labeled evaluation set is not representative of true traffic distribution, dynamic weighting could oscillate or prioritize irrelevant domain slices.

## Foundational Learning

- **Concept: Two-Tower (Siamese) Architecture**
  - **Why needed here:** In retrieval settings, cross-encoders are computationally intractable at scale; Two-Tower architecture allows independent encoding for fast Approximate Nearest Neighbor search.
  - **Quick check question:** Why is interaction between query and product tokens limited to cosine similarity of final embeddings rather than cross-attention layers?

- **Concept: Asymmetric Language Structures**
  - **Why needed here:** Users type short, ambiguous queries while product titles are long and keyword-stuffed; the model must bridge this information gap.
  - **Quick check question:** How does the model ensure short query vectors effectively retrieve long, detailed product vectors without losing specific intent?

- **Concept: Transfer Learning & Domain Adaptation**
  - **Why needed here:** Generic BERT lacks commercial context; pre-training on specific corpus adapts the model to the target domain.
  - **Quick check question:** Why is DistilBERT chosen over full BERT model for this architecture?

## Architecture Onboarding

- **Component map:** DistilBERT backbone -> [Stage 1: Category Pre-training] -> [Stage 2: Siamese Fine-tuning with Progressive Fusion] -> [Embedding Pipeline: Airflow -> Spark Clusters -> Validation -> Vespa] -> [Online Serving: Orchestrator -> Retrieval Service -> Vespa -> Reranker/Sanity Check]

- **Critical path:** The Hard Negative Data Labeling and Progressive Fusion training loop; if hard negatives are mislabeled or domain weights are miscalibrated, the vector space geometry will be distorted, leading to poor retrieval regardless of backend infrastructure.

- **Design tradeoffs:**
  - Latency vs. Nuance: Uses DistilBERT (384-dim embeddings) to meet strict latency requirements, potentially sacrificing deeper semantic understanding of larger models
  - Freshness vs. Stability: Reliance on pre-computed query embeddings creates lag in adapting to sudden shifts in user search intent

- **Failure signatures:**
  - Semantic Drift: Ads for irrelevant but semantically similar items appear (e.g., "Apple" fruit retrieving "Apple" phone accessories)
  - Cold Start Collapse: New products with no click history may have generic embeddings that never surface
  - High IAR: If easy domain overpowers hard domain during fusion, model may prioritize grammatical correctness over commercial intent

- **First 3 experiments:**
  1. Hard Negative Ablation: Train with only random negatives vs. proposed hard negatives, measure NDCG@20 delta (expected: ~3.6% drop)
  2. Category Pre-training Validation: Compare model fine-tuned from scratch vs. initialized with Category Pre-training weights
  3. Domain Weight Sensitivity: Manually fix domain weights to test if human-in-the-loop adjustment converges to better optimum than static weights

## Open Questions the Paper Calls Out

The paper identifies several future directions, including incorporating more dynamic, real-time models to overcome the current reliance on pre-computed query embeddings, which may restrict the full spectrum of semantic search capabilities. The authors also suggest exploring how to better handle the cold-start problem for new products with limited interaction data, and investigating ways to improve the model's understanding of temporal trends and seasonal variations in user search behavior.

## Limitations

- Training hyperparameters (learning rates, batch sizes, optimizer choices, regularization parameters) and hard negative mining thresholds are not specified, preventing exact replication
- The dynamic human-in-the-loop weighting mechanism's superiority over static weighting approaches is asserted but not rigorously demonstrated through ablation studies
- Reliance on pseudo-labels from click data assumes click-through rate is a reliable relevance signal, which may not hold under position bias or when ads are displayed below the fold

## Confidence

- **High Confidence:** The core architectural approach (DistilBERT two-tower Siamese network with progressive multi-domain training) is sound and well-motivated by the asymmetric nature of query-product language in e-commerce
- **Medium Confidence:** The claimed performance improvements are plausible given the methodology, but lack of exact hyperparameter specifications prevents full verification
- **Low Confidence:** The dynamic human-in-the-loop weighting mechanism's superiority over static weighting is asserted but not rigorously demonstrated

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary learning rates, batch sizes, and margin parameter Î± in triplet loss to determine if reported performance gains are robust to hyperparameter choices

2. **Hard Negative Mining Validation:** Implement exact hard negative mining procedure and conduct ablation study comparing models trained with random negatives, hard negatives, and mixture to verify claimed ~3.6% NDCG drop

3. **Dynamic vs. Static Weighting Comparison:** Implement baseline model with static domain weights and compare convergence trajectory and final performance against dynamic human-in-the-loop approach to quantify actual benefit of adaptive weighting mechanism