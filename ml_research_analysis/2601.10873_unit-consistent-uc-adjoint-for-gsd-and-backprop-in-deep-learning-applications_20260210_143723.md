---
ver: rpa2
title: Unit-Consistent (UC) Adjoint for GSD and Backprop in Deep Learning Applications
arxiv_id: '2601.10873'
source_url: https://arxiv.org/abs/2601.10873
tags:
- diagonal
- gauge
- canonical
- adjoint
- normalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unit-consistent (UC) adjoint framework
  to address gauge symmetry in deep networks with positively homogeneous nonlinearities
  like ReLU. The method replaces the Euclidean transpose in backpropagation with a
  UC adjoint that respects diagonal scaling symmetries, thereby eliminating parameterization
  dependence in optimization.
---

# Unit-Consistent (UC) Adjoint for GSD and Backprop in Deep Learning Applications

## Quick Facts
- arXiv ID: 2601.10873
- Source URL: https://arxiv.org/abs/2601.10873
- Reference count: 31
- One-line result: UC adjoint eliminates parameterization dependence in ReLU networks by respecting diagonal gauge symmetries

## Executive Summary
This paper introduces a unit-consistent (UC) adjoint framework to address gauge symmetry in deep networks with positively homogeneous nonlinearities like ReLU. The method replaces the Euclidean transpose in backpropagation with a UC adjoint that respects diagonal scaling symmetries, thereby eliminating parameterization dependence in optimization. The UC adjoint is defined via a canonical decomposition (e.g., RZ scaling), and updates are performed in canonical coordinates to ensure equivariance under diagonal gauge transformations. The framework is extended to biases, convolutions, residual connections, and optimizer states, maintaining global consistency.

## Method Summary
The UC framework replaces standard backpropagation with a gauge-equivariant update rule. For a weight matrix W with canonical decomposition W = DW'E (where D, E are positive diagonal matrices and W' is the canonical form), the UC adjoint is W*_UC = E⁻¹W^T D⁻¹ = (W')^T. Gradient descent is performed in canonical coordinates using the preconditioned update W+ = W - ηD²GE², which transforms covariantly under diagonal rescaling. The method extends to biases (using output scaling D), convolutions (flattening kernels), residual connections (forcing scale matching), and optimizer states (maintaining velocity/moments in canonical coordinates).

## Key Results
- Achieves strong performance in large-scale image recognition without normalization layers
- Eliminates parameterization dependence through gauge-equivariant optimization
- Demonstrates that normalization-by-geometry can replace normalization-by-architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing Euclidean transpose with UC adjoint eliminates parameterization dependence in ReLU networks
- Mechanism: UC adjoint computed via canonical decomposition W = DW'E, yielding W*_UC = (W')^T that transforms covariantly under diagonal rescaling
- Core assumption: Networks use positively homogeneous nonlinearities (ReLU, leaky-ReLU) enabling diagonal gauge freedom
- Evidence anchors: Definition 3.1 formally defines W*_UC := E⁻¹W^T D⁻¹ and shows it simplifies to (W')^T
- Break condition: Non-homogeneous nonlinearities (sigmoid, softmax, tanh) prevent diagonal gauge symmetry

### Mechanism 2
- Claim: Gradient descent in canonical coordinates yields equivariant updates
- Mechanism: Preconditioned update W+ = W - ηD²GE² transforms as Ŵ+ = S_out W+ S_in⁻¹ under gauge transformation Ŵ = S_out W S_in⁻¹
- Core assumption: Consistent canonical decomposition D, E computed from W at each step
- Evidence anchors: Section 3.2-3.3 derives update rule and proves equivariance: Ũ = S_out(D²GE²)S_in⁻¹
- Break condition: Inconsistent canonical decomposition across layers or time steps

### Mechanism 3
- Claim: UC optimization replaces normalization layers by achieving scale stability through geometry
- Mechanism: Normalization acts as data-dependent gauge-fixing; UC achieves intrinsic scale insensitivity via optimization geometry
- Core assumption: Primary benefit of normalization is scale control, not implicit regularization
- Evidence anchors: Empirically achieves strong performance without normalization layers; section 5.1 lists BN downsides
- Break condition: Normalization provides benefits beyond scale control (e.g., implicit regularization, noise injection)

## Foundational Learning

- Concept: Gauge symmetry / Parameterization invariance
  - Why needed here: ReLU networks have massive gauge freedom—many parameterizations implement identical functions
  - Quick check question: Given W and Ŵ = S_out W S_in⁻¹, do they implement the same network function in bias-free ReLU MLP? (Answer: Yes)

- Concept: Positive homogeneity
  - Why needed here: Property σ(αu) = ασ(u) for α > 0 creates diagonal rescaling symmetry
  - Quick check question: Which of ReLU, sigmoid, tanh, max-pooling are positively homogeneous? (Answer: ReLU, max-pooling)

- Concept: Adjoint operators
  - Why needed here: Backpropagation propagates gradients via adjoint of forward operator
  - Quick check question: For y = Wx with units [Seconds] = W × [Meters], what units should ∇_x L have if ∇_y L has units [1/Seconds]? (Answer: [1/Meters])

## Architecture Onboarding

- Component map:
  Linear layers (Dense/Conv) -> Apply canonical decomposition W = DW'E, store D, E per layer
  Biases -> Use same output scaling D from associated weight matrix
  Convolutions -> Flatten kernel to matrix, apply same rule, broadcast D²/E² across spatial dimensions
  Residual connections -> Force scale matching S_skip = S_residual = S_output
  Optimizer state (Momentum/Adam) -> Maintain velocity/moments in canonical coordinates V' = D⁻¹VE⁻¹
  Skip connections / Concatenation -> Block-diagonal scale composition

- Critical path:
  1. Implement RZ canonical scaling for arbitrary matrices (computational primitive)
  2. Replace W^T with W*_UC in backward pass for all linear layers
  3. Apply preconditioned update W+ = W - ηD²GE² instead of W+ = W - ηG
  4. Extend to biases using per-layer D²
  5. Modify optimizer state updates to operate in canonical coordinates

- Design tradeoffs:
  - RZ scaling algorithm choice affects O(mn) computation per layer per step
  - Applying UC globally vs. locally around normalization layers
  - Exact equivariance requires consistent canonical decomposition

- Failure signatures:
  - Training instability persists → canonical decomposition may be inconsistent
  - Performance degrades with small batches → verify optimizer state in canonical coordinates
  - Gradient explosion/vanishing → check D, E are positive and finite
  - Mismatch between train/inference → UC should eliminate this if replacing BN

- First 3 experiments:
  1. Train small MLP (3-4 layers) on MNIST/CIFAR-10 with standard SGD vs. UC-GSD, no normalization layers
  2. Take trained model W, apply random diagonal rescaling Ŵ = S_out W S_in⁻¹, verify UC-GSD produces gauge-equivalent updates
  3. Train ResNet-style architecture on ImageNet subset with (a) BatchNorm + SGD, (b) No normalization + SGD, (c) No normalization + UC-GSD

## Open Questions the Paper Calls Out

- Question: Does UC-GSD maintain convergence speed and stability with adaptive optimizers like Adam?
  - Basis: Section 4.4 and Appendix C.9 provide theoretical derivation for maintaining optimizer states in canonical coordinates
  - Why unresolved: Empirical benchmarks comparing UC-Adam against standard Adam not presented
  - What evidence would resolve it: Large-scale benchmarks (e.g., ImageNet) comparing convergence of UC-Adam vs standard Adam

- Question: Can UC adjoint framework extend to networks with non-homogeneous components like Softmax attention?
  - Basis: Appendix C.8 states Softmax, Sigmoid, and Tanh "do not satisfy positive homogeneity"
  - Why unresolved: Method relies on positive homogeneity to define valid gauge group
  - What evidence would resolve it: Theoretical modification of adjoint for attention layers or empirical results in Transformer architectures

- Question: Does removing normalization layers eliminate beneficial implicit regularization from BatchNorm?
  - Basis: Appendix D.3 notes normalization layers may provide "accidental benefits in the form of, e.g., implicit noise injection"
  - Why unresolved: While addressing geometric conditioning problem, doesn't analyze if stochasticity contributes to generalization
  - What evidence would resolve it: Generalization gap analysis comparing UC-GSD vs UC-GSD with explicit noise injection vs BatchNorm

## Limitations

- Exact RZ canonical scaling algorithm not specified, requiring reverse-engineering or alternative methods
- Lack of detailed empirical results with specific architectures, hyperparameters, and comprehensive benchmarks
- No experimental validation for adaptive optimizers (Adam) or non-homogeneous components (Transformers)

## Confidence

- High Confidence: Theoretical framework for UC adjoint definition and gauge-equivariant updates is mathematically sound
- Medium Confidence: Claim that normalization layers primarily serve scale-stabilization function that can be replaced by UC optimization
- Low Confidence: Empirical performance claims on large-scale image recognition without normalization layers due to insufficient details

## Next Checks

1. **Gauge Equivariance Test**: Apply random diagonal rescaling to trained model and verify UC-GSD produces gauge-equivalent updates while standard SGD does not
2. **Minimal Architecture Experiment**: Train small MLP (3-4 layers) on MNIST/CIFAR-10 comparing standard SGD vs UC-GSD with no normalization layers
3. **Normalization Ablation Study**: Train ResNet-style architecture on ImageNet subset comparing (a) BatchNorm + SGD, (b) No normalization + SGD, (c) No normalization + UC-GSD