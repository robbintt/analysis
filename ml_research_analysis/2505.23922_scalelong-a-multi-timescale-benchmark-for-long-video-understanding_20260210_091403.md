---
ver: rpa2
title: 'ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding'
arxiv_id: '2505.23922'
source_url: https://arxiv.org/abs/2505.23922
tags:
- video
- question
- temporal
- questions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ScaleLong introduces a multi-timescale benchmark for long-video\
  \ understanding by embedding questions targeting four hierarchical temporal scales\u2014\
  clip (seconds), shot (tens of seconds), event (minutes), and story (hours)\u2014\
  all within the same video content. This within-content design enables direct comparison\
  \ of model performance across timescales on identical videos, addressing limitations\
  \ of existing benchmarks that scatter scale-specific questions across different\
  \ videos."
---

# ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding

## Quick Facts
- arXiv ID: 2505.23922
- Source URL: https://arxiv.org/abs/2505.23922
- Reference count: 27
- Multi-timescale benchmark reveals U-shaped performance curve across 23 MLLMs

## Executive Summary
ScaleLong introduces a multi-timescale benchmark for long-video understanding by embedding questions targeting four hierarchical temporal scales—clip (seconds), shot (tens of seconds), event (minutes), and story (hours)—all within the same video content. This within-content design enables direct comparison of model performance across timescales on identical videos, addressing limitations of existing benchmarks that scatter scale-specific questions across different videos. ScaleLong includes 269 long videos (avg. 86 min) from 5 main categories and 36 sub-categories, with 4–8 carefully designed questions per video, ensuring at least one question for each timescale. Evaluations of 23 multimodal large language models (MLLMs) reveal a U-shaped performance curve, with higher accuracy at the shortest (clip) and longest (story) timescales and a dip at intermediate levels. Ablation studies show that increased visual token capacity consistently enhances reasoning across all timescales.

## Method Summary
ScaleLong evaluates MLLMs on long-video understanding through multiple-choice QA targeting four hierarchical temporal scales (Clip/Shot/Event/Story) within identical video content. The benchmark uses 269 videos averaging 86 minutes, with 4–8 questions per video covering five task types. Each question has 4 options (1 correct + 3 distractors from 10 types). Evaluation tests frame counts (4–256) at 240p resolution, with token allocation experiments across different frame/resolution combinations. 23 MLLMs were evaluated, revealing timescale-stratified performance patterns and sensitivity to visual token scaling.

## Key Results
- U-shaped performance curve: Models achieve 71.5% accuracy on Clip and 69.0% on Story tasks, but drop to 62.8% on Shot and 68.0% on Event tasks
- Visual token scaling consistently improves performance across all timescales, with Clip tasks showing most substantial gains (~9% accuracy increase from 4→128 frames)
- Missing information distractors cause highest error rates (53% for Gemini 2.5 Pro), followed by spatial replacement errors (46.6%)
- Optimal frame allocation varies by timescale: 32×240p optimal for Clip; 16×360p better for Story tasks

## Why This Works (Mechanism)

### Mechanism 1: Within-Content Multi-Timescale Disentanglement
- Claim: Embedding questions at all four temporal scales within the same video isolates temporal reasoning ability from content-specific variance.
- Mechanism: By holding video content constant while varying question timescale, the design eliminates confounds where model performance differences could be attributed to content difficulty rather than temporal granularity. This enables direct comparison of Clip→Shot→Event→Story performance on identical narratives.
- Core assumption: Temporal reasoning capabilities are scale-dependent and can be measured independently of semantic content familiarity.
- Evidence anchors:
  - [abstract] "within-content multi-timescale questioning design enables direct comparison of model performance across timescales on identical videos"
  - [Section 2.1] "This 'within-content' embedding of questions targeting multiple, distinct temporal scales... effectively decouples the assessment of temporal understanding of specific video content"
  - [corpus] Related benchmarks (LEMON, HLV-1K) also target long-video understanding but do not explicitly isolate timescales within identical content
- Break condition: If questions at different timescales require fundamentally different reasoning types (not just temporal span), the comparison becomes confounded.

### Mechanism 2: Visual Token Scaling for Temporal Coverage
- Claim: Increasing visual token count—primarily through frame count expansion—systematically improves multi-timescale reasoning.
- Mechanism: More frames provide denser temporal sampling, capturing transient details needed for Clip-level tasks while maintaining sufficient context for longer scales. The paper shows Clip accuracy gains of ~9% (43.5%→54.5%) when increasing from 4→128 frames, versus ~2% from resolution increases.
- Core assumption: Visual tokens encode temporal information linearly; redundancy at longer timescales does not severely degrade performance.
- Evidence anchors:
  - [abstract] "ablation studies show that increased visual token capacity consistently enhances reasoning across all timescales"
  - [Section 4.3.1] "accuracy of all models tends to improve as the number of input frames increases... Clip level exhibits the most substantial gain"
  - [corpus] No direct corpus contradiction; related work does not systematically test token allocation
- Break condition: If context length limits or attention degradation cause severe information loss at high frame counts, scaling returns diminish (observed at 128 frames for Event-level tasks).

### Mechanism 3: U-Shaped Temporal Performance Curve
- Claim: MLLMs exhibit higher accuracy at extreme timescales (Clip, Story) with a performance dip at intermediate levels (Shot, Event).
- Mechanism: Clip tasks rely on localized visual pattern recognition (well-suited to vision encoders). Story tasks leverage language model priors about narrative structure. Intermediate scales require integrating visual dynamics across shots without strong linguistic scaffolding—exposing a capability gap.
- Core assumption: The U-shape reflects architectural limitations, not dataset artifacts.
- Evidence anchors:
  - [abstract] "reveals a U-shaped performance curve, with higher accuracy at the shortest (clip) and longest (story) timescales and a dip at intermediate levels"
  - [Section 4.2] "Gemini 2.5 Pro achieves 71.5% accuracy on Clip and 69.0% on Story, yet drops to 62.8% on Shot and 68.0% on Event"
  - [corpus] Insufficient external validation; related benchmarks do not report timescale-stratified results
- Break condition: If intermediate-scale questions are systematically harder by design (not due to model limitations), the U-shape is a dataset artifact.

## Foundational Learning

- Concept: Temporal granularity hierarchy (Clip→Shot→Event→Story)
  - Why needed here: The entire benchmark taxonomy depends on understanding that video understanding decomposes into qualitatively different temporal reasoning modes
  - Quick check question: Can you explain why a 3-second action recognition task differs fundamentally from a 60-minute narrative summarization task?

- Concept: Visual token budget allocation (frames × resolution)
  - Why needed here: Ablation results show optimal allocation varies by timescale; engineers must understand this tradeoff
  - Quick check question: Given a fixed token budget of 8K visual tokens, how would you allocate between 256 low-res frames vs. 32 high-res frames for Clip vs. Story tasks?

- Concept: Within-subject experimental design
  - Why needed here: ScaleLong's core innovation is controlling for content variance; understanding why this matters is essential for interpreting results
  - Quick check question: Why does comparing model performance across Video-MME (different videos per task) vs. ScaleLong (same video, different timescales) yield different conclusions about temporal capabilities?

## Architecture Onboarding

- Component map:
  Video corpus -> Question annotations -> Evaluation protocol -> Model inputs

- Critical path:
  1. Video selection → 2. Full-video viewing by annotators → 3. Hierarchical question design (2 per timescale) → 4. Distractor generation (10 types) → 5. Two-round QC (correctness, then confound removal) → 6. Model evaluation at fixed resolution/frame settings

- Design tradeoffs:
  - Frame count vs. resolution: Paper shows 32×240p optimal for Clip; 16×360p better for Story. No single setting dominates.
  - Annotation depth vs. scale: 8 questions/video balances coverage against annotation cost. Fewer questions would reduce statistical power per timescale.
  - Distractor plausibility: Overly obvious distractors inflate accuracy; overly subtle ones create annotation noise.

- Failure signatures:
  - Missing information errors (53% for Gemini 2.5 Pro): Models accept claims lacking evidential support
  - Spatial replacement errors (46.6%): Models fail to verify spatial relationships across frames
  - Counting task collapse (20-30% lower than Object Recognition): Quantitative reasoning remains brittle

- First 3 experiments:
  1. Replicate U-shape verification on a held-out 50-video subset to confirm pattern generalizes
  2. Ablate frame count at fixed 240p for a single model family (e.g., Qwen2.5-VL 7B/32B/72B) to isolate scaling effects from architecture
  3. Error analysis by distractor type on the worst-performing timescale (Shot or Event) to identify whether failures cluster around specific reasoning gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms underlie the consistent U-shaped performance curve across all 23 evaluated MLLMs, where accuracy peaks at Clip and Story timescales but dips at intermediate Shot and Event levels?
- Basis in paper: [explicit] The authors repeatedly observe this U-shaped trend but do not explain its cause, stating only that "current MLLMs often excel at processing either highly localized visual details or overarching narrative structures, yet face challenges with intermediate temporal contexts."
- Why unresolved: The paper identifies the pattern but does not investigate whether the dip stems from token allocation, attention mechanisms, training data distribution, or other architectural factors.
- What evidence would resolve it: Ablation studies varying attention window sizes, hierarchical memory mechanisms, or training curricula targeting mid-range temporal reasoning would clarify causal factors.

### Open Question 2
- Question: How can MLLMs be improved to better handle "missing information" and "spatial replacement" distractors, which exhibit the highest error rates (53% and 46.6% for Gemini 2.5 Pro)?
- Basis in paper: [explicit] The authors identify these two distractor types as pervasive failure modes and state "future work [needs] to incorporate mechanisms...that explicitly verify evidential completeness and model multi-view spatial configurations."
- Why unresolved: The paper diagnoses the problem through error analysis but does not propose or test interventions.
- What evidence would resolve it: Training with explicit evidence-verification objectives or spatial-reasoning augmentation, followed by re-evaluation on ScaleLong, would demonstrate whether targeted improvements address these specific weaknesses.

### Open Question 3
- Question: Do the timescale-specific performance patterns generalize beyond multiple-choice QA to open-ended generation and retrieval tasks?
- Basis in paper: [inferred] ScaleLong exclusively uses multiple-choice format; the authors acknowledge no evaluation of free-form responses, temporal grounding, or retrieval, limiting claims about general multi-timescale understanding.
- Why unresolved: The benchmark design constrains assessment to discrimination among options, which may not reflect real-world video understanding demands.
- What evidence would resolve it: Extending ScaleLong with open-ended questions or temporal grounding tasks, then re-evaluating models, would test whether the U-shaped pattern persists across task formats.

## Limitations
- Frame sampling strategy not explicitly specified, which could affect temporal coverage measurements
- Annotation process relies on human judgment, introducing potential bias in question design and distractor generation
- U-shaped performance curve generalizability needs external validation on additional MLLM families

## Confidence
- **High Confidence**: Within-content multi-timescale design effectively isolates temporal reasoning from content-specific variance
- **Medium Confidence**: Visual token scaling reliably improves multi-timescale reasoning (with diminishing returns at high frame counts)
- **Medium Confidence**: U-shaped temporal performance curve reflects architectural limitations rather than dataset artifacts

## Next Checks
1. External Generalization Test: Evaluate U-shaped performance curve on a held-out 50-video subset using a different MLLM family (e.g., LLaVA or Mini-Gemini) to verify pattern holds beyond initial model set

2. Frame Sampling Validation: Compare performance when using uniform frame sampling versus model-specific strategies (e.g., keyframes for Story-level tasks) to determine if sampling affects measured temporal reasoning capabilities

3. Distractor Analysis Replication: Conduct error analysis by distractor type on worst-performing timescale (Shot or Event) using different annotator group to confirm failure modes cluster around specific reasoning gaps rather than annotation bias