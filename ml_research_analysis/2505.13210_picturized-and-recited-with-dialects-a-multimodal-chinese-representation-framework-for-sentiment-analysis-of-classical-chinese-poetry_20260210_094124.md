---
ver: rpa2
title: 'Picturized and Recited with Dialects: A Multimodal Chinese Representation
  Framework for Sentiment Analysis of Classical Chinese Poetry'
arxiv_id: '2505.13210'
source_url: https://arxiv.org/abs/2505.13210
tags:
- chinese
- features
- audio
- poetry
- classical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multimodal framework for sentiment analysis
  of classical Chinese poetry by incorporating phonetic, visual, and textual features.
  The approach extracts audio features from Mandarin and four dialects to capture
  rhythmic information, generates visual features using text-to-image models for imagery
  representation, and combines these with modern Chinese translations of classical
  text using large language models.
---

# Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry

## Quick Facts
- arXiv ID: 2505.13210
- Source URL: https://arxiv.org/abs/2505.13210
- Reference count: 12
- Primary result: 64.85% accuracy on FSPC and 87.32% macro F1 on CCPD, outperforming state-of-the-art by at least 2.51% and 1.63% respectively

## Executive Summary
This paper introduces a multimodal framework for sentiment analysis of classical Chinese poetry by incorporating phonetic, visual, and textual features. The approach extracts audio features from Mandarin and four dialects to capture rhythmic information, generates visual features using text-to-image models for imagery representation, and combines these with modern Chinese translations of classical text using large language models. The method employs a two-stage training strategy with multimodal contrastive representation learning to fuse the different modalities. Experiments on two public datasets show the framework outperforms state-of-the-art methods, achieving at least 2.51% improvement in accuracy and 1.63% in macro F1.

## Method Summary
The framework extracts audio features from poetry sentences in Mandarin and four Chinese dialects (Cantonese, Wu, Minnan, Chaozhou) using OpenSMILE, generates visual features from text-to-image diffusion models, and processes classical Chinese text through modern Chinese translation and GuwenBERT. A two-stage training strategy first aligns the three modalities using contrastive learning, then concatenates features for sentiment classification. The dialect audio features are fused with Mandarin through cross-attention to emphasize shared phonetic content while suppressing speaker-specific noise.

## Key Results
- Achieves 64.85% accuracy on FSPC (5-class single-label sentiment classification) and 87.32% macro F1 on CCPD (12-class multi-label classification)
- Outperforms state-of-the-art methods by at least 2.51% in accuracy and 1.63% in macro F1
- Dialect-specific audio features improve classification accuracy for poems from corresponding regions (4.44% improvement in Shandong, 8.11% in Guangdong)
- Sentence-level visual features from diffusion model latents outperform real image features retrieved for individual words

## Why This Works (Mechanism)

### Mechanism 1: Dialect-Specific Phonetic Feature Alignment
The framework extracts per-character audio features from Mandarin and dialects, then uses cross-attention to align features while suppressing speaker-specific noise. This captures sentiment-relevant rhythmic information that may be closer to ancient pronunciations in certain dialects.

### Mechanism 2: Latent Visual Representation via Text-to-Image Generation
Instead of retrieving real images for individual imagery words, the framework extracts intermediate latent features from a text-to-image diffusion model. These latent representations preserve high-level text-image correspondence while avoiding pixel-level noise and better capturing sentence-level imagery.

### Mechanism 3: Two-Stage Multimodal Contrastive Alignment Before Fusion
The framework first aligns audio, visual, and text features in a shared space using contrastive learning, then concatenates them for classification. This prevents the pre-trained text encoder from dominating the randomly initialized audio/visual encoders.

## Foundational Learning

- **Concept: Cross-attention for modality fusion**
  - Why needed: Aligns Mandarin and dialect audio features, emphasizing shared phonetic content while suppressing speaker-specific noise
  - Quick check: Given two sequences F_M and F_C from different sources, can you write the cross-attention operation that queries one and keys/values the other?

- **Concept: Contrastive learning (InfoNCE / CLIP-style)**
  - Why needed: Aligns audio, visual, and text features in a shared space before classification
  - Quick check: If you have a batch of 4 samples with paired features, how many positive and negative pairs are evaluated in the contrastive objective?

- **Concept: Latent diffusion models (U-Net latent space)**
  - Why needed: Visual features are extracted from the U-Net latent tensor before decoding
  - Quick check: In Stable Diffusion, what is the role of the U-Net, and why might intermediate latents be more useful than the final decoded image for feature extraction?

## Architecture Onboarding

- **Component map**: Classical Chinese sentence S → LLM translation → GuwenBERT → F_text; Audio clips (Mandarin+4 dialects) → OpenSMILE + Transformer → cross-attention → F_audio; Sentence S → Taiyi diffusion model → latent tensor → CNN → F_vision; Contrastive pre-training on all three modalities; Concatenation → classifier

- **Critical path**: Prepare audio clip datasets for Mandarin + 4 dialects; Generate translations S_trans via GLM-4; Extract latent visual features I_latent from Taiyi; Run Stage 1 contrastive pre-training on all three modality encoders; Run Stage 2 joint training with concatenated features on sentiment classification

- **Design tradeoffs**: Real images vs. generated latents (real are easier but isolate words; generated capture sentence-level but depend on diffusion quality); More dialects vs. noise (enriches coverage but increases variability); Freeze vs. fine-tune visual encoder (reduces compute but may limit adaptation)

- **Failure signatures**: Audio modality collapse (features dominated by noise, cross-attention ignores them); Visual modality degradation (Taiyi produces irrelevant latents, +vision hurts); Imbalanced modality contributions (text dominates, ablating audio/vision yields minimal drop)

- **First 3 experiments**: 1) Modality ablation on FSPC (trans only, trans+audio, trans+audio+vision, trans+audio+vision+dialect); 2) Dialect regional analysis (model-mandarin vs model-cantonese on poems grouped by poet province); 3) Generated vs. retrieved visual features (replace Taiyi latents with real image features from Baidu)

## Open Questions the Paper Calls Out
- To what extent can specialized ancient pronunciation reconstruction methods improve sentiment analysis performance compared to using modern dialect proxies?
- How can the framework be adapted to effectively fuse phonetic features from more than two dialects simultaneously without introducing excessive noise?
- Can the sentence-level tri-modal fusion approach proposed for classical poetry generalize effectively to general Chinese representation tasks?
- Does the concatenation of isolated character audio clips fail to capture the prosodic contours (natural pauses and intonation) necessary for full rhythmic understanding?

## Limitations
- Dialect audio data quality is uncertain due to unspecified speaker demographics and recording conditions across sources
- Diffusion model domain alignment is assumed but not validated, as Taiyi was trained on modern Chinese not classical poetry
- Ablation studies lack specificity about whether cross-attention dialect fusion or contrastive pre-training are individually necessary

## Confidence
- **High confidence**: Two-stage training strategy improves performance over single-stage training, supported by ablation experiments
- **Medium confidence**: Dialect-specific audio features improve regional classification accuracy, but relies on untested assumptions about ancient phonetic preservation
- **Low confidence**: Superiority of latent visual features over real images demonstrated on one dataset, but underlying assumption about diffusion model alignment is not directly validated

## Next Checks
1. Analyze audio feature distributions across dialect speakers to quantify potential noise introduced by inconsistent recording sources
2. Fine-tune the Taiyi model on a small corpus of classical Chinese poetry images and re-evaluate visual feature quality for sentiment analysis
3. Train models with and without dialect cross-attention on a regional subset of FSPC to isolate the contribution of dialect fusion from other audio features