---
ver: rpa2
title: 'Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language
  Models'
arxiv_id: '2510.17620'
source_url: https://arxiv.org/abs/2510.17620
tags:
- unlearning
- contextual
- utility
- context
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language model unlearning methods effectively remove targeted\
  \ knowledge but degrade the model\u2019s ability to use that knowledge when provided\
  \ in context, harming contextual utility. This work introduces context-aware unlearning,\
  \ which augments existing objectives with a KL-divergence term that aligns contextual\
  \ responses with the original model, preserving contextual utility."
---

# Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models

## Quick Facts
- **arXiv ID:** 2510.17620
- **Source URL:** https://arxiv.org/abs/2510.17620
- **Reference count:** 15
- **Primary result:** Context-aware unlearning restores contextual QA performance to near-perfect levels without sacrificing forgetting effectiveness or overall model utility.

## Executive Summary
This paper addresses a critical limitation in existing LLM unlearning methods: while they effectively remove targeted knowledge, they also degrade the model's ability to use that same knowledge when provided in context. The authors introduce context-aware unlearning, which augments traditional unlearning objectives with a KL-divergence term that aligns contextual responses with the original model. This approach preserves contextual utility while maintaining strong forgetting effectiveness, solving a practical gap for real-world applications where unlearned models must still process retrieved information from external sources.

## Method Summary
The paper proposes a context-aware unlearning framework that extends standard unlearning objectives by adding a KL-divergence regularization term. This term minimizes the divergence between the unlearned model's output distribution and the original model's distribution specifically when ground-truth answers are provided in the context. The composite objective includes three components: a forget term that drives the probability of answers to zero without context, a retain term that maintains overall model utility, and the novel context term that preserves the model's ability to process contextual evidence. This design allows the model to learn the conditional behavior of "I do not know this, but I can see it is written here."

## Key Results
- Restores contextual QA performance to near-perfect levels (LLM-Judge ≥ 0.95) on Gemma-2B-IT and Qwen3-8B
- Maintains strong forgetting effectiveness without degrading overall model utility
- Demonstrates robustness across context variants including paraphrased and reasoning contexts

## Why This Works (Mechanism)

### Mechanism 1: Representation Suppression Ripple
Standard unlearning methods like Gradient Ascent or RMU maximize loss or perturb hidden activations on the forget set. This optimization doesn't just delete parametric memory—it corrupts the representations of forgotten tokens. When these tokens reappear in prompts as retrieved context, the model's internal processing is degraded, leading to hallucination or gibberish. The unlearning penalty affects shared processing pathways used for both recalling memory and grounding on context.

### Mechanism 2: Distributional Anchoring via KL-Divergence
The paper introduces a context term that minimizes KL-divergence between the unlearned model and the frozen original model when ground-truth answers are provided in context. This forces the unlearned model to maintain the "reasoning pathways" to process context, counteracting the suppression from the forget term. The original model serves as a reference for optimal contextual processing.

### Mechanism 3: Conditional Behavior Decoupling
The composite objective simultaneously optimizes for forget-set loss and context-set alignment, decoupling "knowledge recall" from "knowledge usage." The forget term drives answer probability to zero when no context is given, while the context term drives it back up when context is present. This allows the model to learn conditional behavior: "I do not know this, but I can see it is written here."

## Foundational Learning

- **Concept: KL-Divergence Regularization**
  - **Why needed here:** The paper uses KL-divergence to anchor the unlearned model's behavior to a "gold standard" (the original model) for specific inputs. Understanding this is key to grasping why the model doesn't just re-learn the forbidden fact.
  - **Quick check question:** Why is KL-divergence preferred over Mean Squared Error (MSE) when aligning output distributions of language models?

- **Concept: In-Context Learning (ICL) vs. Parametric Memory**
  - **Why needed here:** The core problem is the destruction of ICL capabilities while trying to modify Parametric Memory. You must distinguish between the model "knowing" a fact and "seeing" a fact.
  - **Quick check question:** If a model has "unlearned" the capital of France, should it be able to answer "What is the capital of France?" if the prompt explicitly says "The capital of France is Paris"?

- **Concept: Catastrophic Forgetting vs. Targeted Unlearning**
  - **Why needed here:** Standard training suffers from catastrophic forgetting of old tasks when learning new ones. Unlearning attempts a controlled version of this but risks destroying the model's fundamental ability to process certain linguistic patterns.
  - **Quick check question:** How does the "retain set" in unlearning differ from the training set in standard fine-tuning?

## Architecture Onboarding

- **Component map:** Input Layers (Tokenizer + Embedding) -> Reference Model (Frozen Original) -> Unlearning Target (Active Model) -> Objective Function (Forget Loss + Retain Loss + Context Term)
- **Critical path:** The implementation of the Context Term requires constructing a context set where questions from the forget set are paired with ground-truth answers in the prompt context. During backprop, the KL term calculates divergence between the Active Model and Reference Model only on these context-augmented samples.
- **Design tradeoffs:**
  - Efficiency: Requires maintaining two models in memory during training, doubling VRAM usage
  - Sensitivity: Claims to be insensitive to λc, but context set format (verbatim vs. paraphrased) may impact robustness
- **Failure signatures:**
  - "The Stubborn Model": Direct QA scores remain high; forget term weight is too low
  - "The Broken Reader": Contextual QA scores remain low; KL term is failing or context format is too distinct
  - "The Gibberish Generator": Model outputs nonsense on retain set; unlearning has collapsed representation space
- **First 3 experiments:**
  1. Baseline Verification: Implement RMU or NPO on Gemma-2B-IT with TOFU dataset, measure ROUGE-L on forget set with vs without context
  2. Ablation on Context Format: Implement Context-Aware term with verbatim context, then test on paraphrased context to verify generalization
  3. Hyperparameter Sensitivity: Grid search on λc (e.g., [0.1, 0.5, 1.0, 2.0]) to verify robustness, plot Direct QA vs Contextual QA

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness relies heavily on quality and coverage of context set, which may not generalize to open-ended or ambiguous queries
- KL-divergence term assumes original model's contextual processing is optimal, which may not hold for all tasks or domains
- Experiments focus on simple fact-based questions, leaving uncertainty about performance on complex reasoning or multi-hop queries

## Confidence
- **High Confidence:** KL-divergence regularization mechanism is well-grounded and experimentally validated
- **Medium Confidence:** Claim of robustness to context format variations is supported but needs more extensive testing
- **Medium Confidence:** Claim of insensitivity to λc is plausible but based on limited hyperparameter sweeps

## Next Checks
1. **Generalization to Complex Queries:** Test on multi-hop reasoning tasks or open-ended questions requiring synthesis rather than direct lookup
2. **Robustness to Context Quality:** Evaluate performance when context set contains noisy, incomplete, or contradictory information
3. **Privacy Analysis:** Investigate whether context set inadvertently leaks information about forget set through adversarial testing or formal privacy guarantees