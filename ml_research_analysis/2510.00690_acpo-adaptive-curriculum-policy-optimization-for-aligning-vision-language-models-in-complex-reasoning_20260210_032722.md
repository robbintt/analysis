---
ver: rpa2
title: 'ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language
  Models in Complex Reasoning'
arxiv_id: '2510.00690'
source_url: https://arxiv.org/abs/2510.00690
tags:
- acpo
- policy
- arxiv
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning large-scale vision-language
  models (VLMs) for complex reasoning tasks using reinforcement learning. Existing
  methods like GRPO and DAPO rely on static training schedules and fixed clipping
  thresholds, leading to training instability and suboptimal performance.
---

# ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning

## Quick Facts
- **arXiv ID**: 2510.00690
- **Source URL**: https://arxiv.org/abs/2510.00690
- **Reference count**: 40
- **Primary result**: Achieves 49.90% average accuracy across 7 benchmarks, surpassing DAPO/PAPO by ~3-4%

## Executive Summary
ACPO addresses training instability in aligning VLMs for complex reasoning by introducing a dynamic curriculum that transitions from on-policy exploration to off-policy exploitation, and an Advantage-Aware Adaptive Clipping mechanism that replaces fixed PPO bounds with dynamic, advantage-modulated bounds. The framework consistently outperforms strong baselines across seven challenging multimodal reasoning benchmarks, achieving state-of-the-art performance with improved convergence and stability.

## Method Summary
ACPO combines two key innovations: a dynamic curriculum that schedules training epochs with increasing off-policy sample reuse (K(t) = max(1, ⌈N·t/T⌉)), and Advantage-Aware Adaptive Clipping that modulates PPO clipping bounds based on advantage estimates (ε_high(Â) = ε₀^high + δ·Ã). The framework uses strategic gating to filter samples where Σ I(R(o_i) > τ) ≤ N_max, enabling efficient off-policy training while maintaining quality. The method was evaluated on seven benchmarks using Qwen2.5-VL base models.

## Key Results
- Achieves 49.90% average accuracy across seven benchmarks, surpassing DAPO and PAPO by ~3-4%
- Outperforms strong baselines including GRPO and DAPO in both accuracy and training stability
- Ablation study shows AAAC contributes 1.16% performance improvement when removed
- Demonstrates accelerated convergence and superior training stability via reward/clip-ratio curves

## Why This Works (Mechanism)
ACPO addresses two core challenges in VLM alignment: training instability from static clipping thresholds and inefficiency from pure on-policy training. The dynamic curriculum enables smooth transition from stable exploration to efficient exploitation, while AAAC adapts clipping bounds to advantage magnitudes, preventing both under-update (high-advantage samples) and over-update (low-advantage samples). Strategic gating ensures off-policy samples maintain quality while increasing training efficiency.

## Foundational Learning
- **PPO Clipping Mechanism**: Constrains policy updates to prevent destructive large steps; needed for stable RL training; quick check: verify KL penalty or clipping is applied in policy update
- **Advantage Estimation**: Measures relative value of actions; needed to prioritize updates; quick check: confirm advantages are computed as Q(s,a) - V(s)
- **Curriculum Learning**: Schedules difficulty progression; needed to balance exploration/exploitation; quick check: verify sample reuse increases over training
- **Off-Policy Training**: Reuses past experiences; needed for efficiency; quick check: confirm multiple epochs over stored samples
- **Strategic Gating**: Filters samples by reward criteria; needed to maintain off-policy quality; quick check: verify sample filtering rate is reasonable
- **DeepSpeed ZeRO Optimization**: Enables large-scale training; needed for memory efficiency; quick check: confirm model parallelism is configured

## Architecture Onboarding
- **Component Map**: Base VLM → PPO Policy Update ← Advantage Estimation ← Sample Collection ← Strategic Gating → Curriculum Scheduler
- **Critical Path**: Sample collection → advantage estimation → AAAC clipping → policy update → curriculum scheduling
- **Design Tradeoffs**: Fixed vs adaptive clipping (stability vs responsiveness), on-policy vs off-policy (purity vs efficiency), gating threshold (quality vs quantity)
- **Failure Signatures**: High clip ratios (>0.3) indicate over-aggressive updates; reward collapse suggests poor gating threshold; training instability suggests δ too large
- **First Experiments**: 1) Verify AAAC with fixed vs adaptive clipping on a small benchmark; 2) Test curriculum transition timing with different N values; 3) Validate gating mechanism with varying τ thresholds

## Open Questions the Paper Calls Out
None

## Limitations
- Single base model family (Qwen2.5-VL) limits generalizability claims
- No statistical significance testing for performance improvements
- Curriculum difficulty categorization mechanism lacks detailed specification
- Critical hyperparameters (gating threshold τ, N_max, base clipping bounds) are not disclosed

## Confidence
- **High confidence**: AAAC mechanism specification is complete and implementable
- **Medium confidence**: Performance improvements are reproducible with correct hyperparameters
- **Low confidence**: Training stability claims are difficult to verify without complete hyperparameter set

## Next Checks
1. Implement AAAC with multiple δ values (0.03, 0.05, 0.1) and verify δ=0.05 achieves optimal stability
2. Test strategic gating with different τ values (0.3, 0.5, 0.7) to identify optimal sample retention rate
3. Compare ACPO against GRPO baseline using identical data and base models to isolate mechanism contributions