---
ver: rpa2
title: Learning to Rank with Top-$K$ Fairness
arxiv_id: '2509.18067'
source_url: https://arxiv.org/abs/2509.18067
tags:
- fairness
- ranking
- top-k
- exposure
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a learning-to-rank framework that addresses
  top-K fairness by optimizing both ranking quality and exposure disparity at training
  time. The method introduces a top-K exposure disparity metric extending classic
  group exposure disparity, and transforms the non-differentiable top-K selection
  into a differentiable objective function.
---

# Learning to Rank with Top-$K$ Fairness

## Quick Facts
- **arXiv ID:** 2509.18067
- **Source URL:** https://arxiv.org/abs/2509.18067
- **Reference count:** 22
- **One-line primary result:** Framework optimizes ranking quality and top-K exposure disparity, outperforming existing methods in balancing NDCG with fairness across benchmark datasets.

## Executive Summary
This paper proposes a learning-to-rank framework that addresses top-K fairness by optimizing both ranking quality and exposure disparity at training time. The method introduces a top-K exposure disparity metric extending classic group exposure disparity, and transforms the non-differentiable top-K selection into a differentiable objective function. Efficient stochastic optimization algorithms are developed to achieve high accuracy and sufficient fairness. Extensive experiments on benchmark datasets demonstrate that the proposed method consistently outperforms existing methods in balancing NDCG ranking performance with fairness across various top-K lengths, achieving both high accuracy and reduced exposure disparities between protected groups.

## Method Summary
The method optimizes a joint objective combining NDCG ranking quality with top-K exposure disparity fairness. It uses a NeuMF backbone and introduces a differentiable top-K selection mechanism via bilevel optimization. The approach constrains fairness calculations to top-K positions using a smooth surrogate for the top-K indicator, enabling end-to-end training with convergence guarantees. Stochastic optimization with moving average estimators reduces gradient variance in compositional loss functions.

## Key Results
- Consistently outperforms existing methods in balancing NDCG performance with fairness across various top-K lengths
- Achieves both high ranking accuracy and reduced exposure disparities between protected groups
- Demonstrates superior performance on MovieLens-20M and Netflix-20M benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining fairness to top-K positions more effectively addresses real-world allocation scenarios than whole-list fairness.
- Mechanism: The method restricts the exposure disparity calculation to only items ranked within top-K, using a smooth surrogate œà(¬∑) to approximate the top-K indicator I(x_i ‚àà S_K^q). Exposure weights items by their position using e(w, x, S) = exp(h(x;w))/Œ£exp(h(x';w)), but only for items exceeding the learned threshold Œª_q(w).
- Core assumption: Decision-makers primarily act on top-K results; fairness beyond K has negligible real-world impact.
- Evidence anchors:
  - [abstract]: "decision-makers often prioritize only the top-K ranked items, while the ranking beyond top-K becomes less relevant"
  - [Page 5, Eq. 9]: Top-K fairness condition explicitly weights by œà(h(x;w) - Œª_q(w))¬∑e(w,x,S)
  - [corpus]: Weak direct support; related work on top-K ranking optimization (K-order Ranking Preference Optimization) exists but focuses on relevance, not fairness.

### Mechanism 2
- Claim: Differentiable top-K selection via bilevel optimization enables end-to-end training with convergence guarantees.
- Mechanism: The threshold Œª_q(w) is defined implicitly as the solution to a lower-level optimization (Eq. 10), identifying the (K+1)-th largest score. The non-smooth original is approximated by a strongly convex, smooth surrogate G_q(w,Œª) (Eq. 13) using soft-plus functions. Gradients flow through Œª via implicit differentiation: ‚àáŒª_q(w) = -‚àá¬≤_{Œª,w}G_q ¬∑ (‚àá¬≤_Œª G_q)^{-1}.
- Core assumption: The smoothing parameters œÑ‚ÇÅ, œÑ‚ÇÇ sufficiently approximate the true top-K boundary without excessive bias.
- Evidence anchors:
  - [Page 5-6, Eq. 10-13]: Full derivation of Œª_q(w) and its smooth approximation
  - [Page 8, Theorem 4.2]: Convergence to Œµ-stationary solution in O(1/Œµ‚Å¥) iterations
  - [corpus]: K-SONG (Qiu et al., 2022) uses similar bilevel optimization for top-K NDCG but without fairness.

### Mechanism 3
- Claim: Moving average estimators reduce gradient variance in compositional loss functions, enabling stable large-scale optimization.
- Mechanism: Both ranking loss L(w) and fairness loss U(w) are finite-sum compositional functions (outer function depending on inner function outputs). The algorithm maintains moving averages u_{q,i} for inner function estimates (Eq. 17, 21) and momentum on gradients (Eq. 23). This decouples the need to compute full expectations at each iteration.
- Core assumption: Averaging parameters Œ≥‚ÇÄ...Œ≥‚ÇÖ balance bias-variance tradeoff appropriately for the data distribution.
- Evidence anchors:
  - [Page 6-8, Eqs. 17-23]: Full stochastic approximation construction for both losses
  - [Page 11, Figure 2]: Ablation shows lower Œ≥ (stronger averaging) improves fairness-accuracy tradeoff
  - [corpus]: Standard technique in compositional optimization; no direct corpus contradiction.

## Foundational Learning

- Concept: **Bilevel Optimization**
  - Why needed here: The top-K threshold Œª_q(w) is defined as an optimization problem nested within the main loss minimization. Understanding implicit differentiation is required to see why gradients flow through Œª.
  - Quick check question: Can you explain why ‚àá_w Œª_q(w) is non-zero even though Œª doesn't appear explicitly in the outer loss?

- Concept: **Compositional Stochastic Optimization**
  - Why needed here: Both L(w) and U(w) have structure f(g(w)) where g is an expectation. Standard SGD is biased here; moving average estimators correct this.
  - Quick check question: Why doesn't sampling a mini-batch directly give an unbiased gradient estimate for f(ùîº[g(w)])?

- Concept: **Exposure-based Fairness in Ranking**
  - Why needed here: The fairness metric isn't about outcomes but about attention/visibility allocation. Position bias is modeled via exponential exposure weights.
  - Quick check question: In Eq. 5, why does higher score lead to higher exposure, and how does this relate to reciprocal rank?

## Architecture Onboarding

- Component map:
Input: (query q, items S_q, relevance Y_q, group labels S_q^a, S_q^b)
      ‚Üì
Scoring Network h_q(x; w)  ‚Üê NeuMF or similar
      ‚Üì
Soft Top-K Selector ‚Üê Œª_q(w) from lower-level opt
      ‚Üì
Loss Computation:
  ‚îú‚îÄ‚îÄ NDCG Loss L_q(w) ‚Üê position-weighted relevance
  ‚îî‚îÄ‚îÄ Fairness Loss U_K_q(w, Œª_q(w)) ‚Üê exposure disparity
      ‚Üì
Gradient Estimation:
  ‚îú‚îÄ‚îÄ Moving averages u_{q,i}, u_{q,a}, u_{q,b}, u_q
  ‚îú‚îÄ‚îÄ Œª estimator Œª_{q,t}, s_{q,t}, v_{q,t}
  ‚îî‚îÄ‚îÄ Momentum gradient z_t
      ‚Üì
Parameter Update: w ‚Üê w - Œ∑‚ÇÅ z_t

- Critical path:
  1. Correct Œª_q(w) estimation (must track K+1-th largest score)
  2. Accurate exposure computation for each group
  3. Stable moving average updates (Œ≥ tuning is sensitive)

- Design tradeoffs:
  - **K selection**: Smaller K focuses fairness more sharply but may exclude relevant items from consideration
  - **C parameter (Eq. 12)**: Higher C prioritizes fairness over NDCG; no single optimal value‚Äîrequires Pareto frontier analysis
  - **œÑ‚ÇÅ, œÑ‚ÇÇ smoothing**: Larger values improve differentiability but blur top-K boundary
  - **Batch size |B_q|**: Must sample enough minority items per batch to estimate group exposure reliably

- Failure signatures:
  - **Œª_q(w) collapse**: If Œª diverges or oscillates, top-K selection becomes noise; check s_{q,t} stability
  - **Zero minority in batch**: If B_q^a is empty, gradient for fairness term is undefined; enforce minimum sampling
  - **NDCG crashes with high C**: Fairness constraint too strong for data; reduce C or check if minority items have systematically lower relevance
  - **Training divergence**: Moving averages may lag; try reducing learning rates Œ∑‚ÇÄ, Œ∑‚ÇÅ or increasing Œ≥ values

- First 3 experiments:
  1. **Sanity check**: Set C=0 (color-blind); verify KSO-RED matches K-SONG baseline in NDCG‚Äîthis validates the optimization machinery.
  2. **Hyperparameter sweep**: Fix K=100, vary C ‚àà {0, 10¬≤, 10‚Å¥, 10‚Å∂} and Œ≥ ‚àà {0.2, 0.6, 1.0}; plot NDCG vs. fairness loss (MAE) curve. Expect Pareto frontier.
  3. **Ablation on Œª estimation**: Replace learned Œª_q(w) with ground-truth K-th percentile of scores at each iteration; measure gap in convergence speed and final fairness. Large gap indicates smoothing approximation error dominates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be generalized to handle multiple protected groups rather than just binary group membership?
- Basis in paper: [explicit] The authors state in Section 6 that the "current formulation assumes binary protected attributes" and that extending to multi-group settings requires modifications to the fairness constraints.
- Why unresolved: The mathematical formulation (Eq. 9) and the resulting stochastic gradients are derived specifically for a disparity between a single minority ($S_a$) and majority ($S_b$) group.
- What evidence would resolve it: A modified objective function that aggregates disparities across $M > 2$ groups and empirical results on datasets with intersectional protected attributes.

### Open Question 2
- Question: Can adaptive methods be integrated to eliminate the need for extensive manual tuning of the algorithm's hyperparameters?
- Basis in paper: [explicit] Section 6 lists the requirement to tune averaging parameters ($\gamma$), learning rates ($\eta$), and smoothing parameters ($\tau$) as a limitation, suggesting adaptive selection as future work.
- Why unresolved: The current algorithm requires manually searching for values (e.g., $\gamma \in \{0.2, 0.6, 1.0\}$) to balance stability and convergence speed.
- What evidence would resolve it: A self-tuning variant of the KSO-RED algorithm that dynamically adjusts $\gamma$ or $\tau$ while maintaining the theoretical convergence guarantees.

### Open Question 3
- Question: Can the differentiable top-K selection mechanism be applied to optimize fairness metrics beyond exposure disparity?
- Basis in paper: [explicit] Section 6 notes the current focus on "exposure parity" and suggests "exploring additional fairness metrics beyond exposure disparity" to broaden applicability.
- Why unresolved: The current differentiable loss function (Eq. 11) is tightly coupled with the definition of exposure based on ranking scores.
- What evidence would resolve it: The derivation of surrogate gradients for alternative metrics (e.g., demographic parity or calibration) within the top-K bilevel optimization framework.

## Limitations
- The smoothing parameters œÑ‚ÇÅ, œÑ‚ÇÇ for top-K boundary approximation are not explicitly specified, leaving performance sensitive to heuristic tuning
- The method's effectiveness depends on sufficient minority group representation in training batches, which may not hold for highly imbalanced datasets
- Theoretical convergence guarantees assume strong convexity and smoothness conditions that may not hold exactly in practice

## Confidence
- **High confidence**: The mechanism of differentiable top-K selection via bilevel optimization (Mechanism 2) and the exposure-based fairness formulation are well-grounded
- **Medium confidence**: The moving average gradient estimators reduce variance effectively (Mechanism 3), though sensitivity to Œ≥ parameters requires empirical validation
- **Low confidence**: The claim that top-K fairness is universally more impactful than whole-list fairness (Mechanism 1) lacks sufficient empirical validation across diverse ranking use cases

## Next Checks
1. **Ablation study**: Compare learned Œª_q(w) against ground-truth K-th percentile scores to quantify approximation error impact on fairness convergence
2. **Robustness test**: Evaluate performance on datasets with varying minority group sizes (1%, 5%, 10%) to identify minimum representation thresholds
3. **Pareto analysis**: Systematically sweep C parameter values and plot NDCG vs. fairness trade-off curves to verify claimed superior balance across the frontier