---
ver: rpa2
title: 'TruePose: Human-Parsing-guided Attention Diffusion for Full-ID Preserving
  Pose Transfer'
arxiv_id: '2502.03426'
source_url: https://arxiv.org/abs/2502.03426
tags:
- image
- attention
- source
- diffusion
- pose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of preserving both facial and
  clothing details in pose-guided person image synthesis (PGPIS), where diffusion-based
  methods often struggle to maintain clothing patterns during pose transformation.
  The authors propose a human-parsing-guided attention diffusion approach that introduces
  a human-parsing-aware Siamese network with three key components: dual identical
  UNets (SourceNet and TargetNet), a human-parsing-guided fusion attention (HPFA)
  module, and a CLIP-guided attention alignment (CAA) module.'
---

# TruePose: Human-Parsing-guided Attention Diffusion for Full-ID Preserving Pose Transfer

## Quick Facts
- arXiv ID: 2502.03426
- Source URL: https://arxiv.org/abs/2502.03426
- Reference count: 40
- Primary result: Achieves LPIPS 0.151 on DeepFashion, outperforming 13 baselines

## Executive Summary
TruePose addresses the challenge of preserving facial and clothing details in pose-guided person image synthesis, where diffusion-based methods often struggle with clothing patterns during pose transformation. The authors propose a human-parsing-guided attention diffusion approach using a Siamese dual-UNet architecture with three key components: SourceNet for source image embedding extraction, TargetNet for diffusion denoising, and two novel modules—HPFA (Human-Parsing-guided Fusion Attention) and CAA (CLIP-guided Attention Alignment). Experiments on DeepFashion and WPose datasets demonstrate significant improvements over 13 baselines, achieving LPIPS of 0.151, SSIM of 0.744, and PSNR of 17.627, while preserving complex clothing patterns including text and textures even under substantial pose differences.

## Method Summary
TruePose introduces a Siamese dual-UNet architecture based on Stable Diffusion v1.5, where SourceNet and TargetNet share identical architecture and initial weights. The method incorporates two key attention modules: HPFA uses binary parsing masks to reweight attention scores on body, clothing, and facial regions, while CAA refines attention using CLIP embeddings for underrepresented regions. The model is trained with classifier-free guidance (30% dropout) on DeepFashion dataset, using DDIM sampling with 35 steps at inference. The approach specifically addresses the limitation that pre-trained encoders pay less attention to clothing regions, as identified through statistical analysis.

## Key Results
- Achieves LPIPS 0.151 (vs. 0.163 for previous best) on DeepFashion 512×352
- Maintains SSIM of 0.744 and PSNR of 17.627
- Preserves complex clothing patterns including text and textures under substantial pose differences
- Outperforms 13 baselines including PoseWarp, PDA-Net, PIDM, and CFLD
- Demonstrates superior performance on WPose in-the-wild dataset

## Why This Works (Mechanism)

### Mechanism 1: Human-parsing-guided fusion attention (HPFA)
HPFA improves clothing pattern preservation by explicitly directing model attention to body, clothing, and facial regions during diffusion using binary parsing masks. The mask weight matrix M' applies higher weights (1+δ) to high-attention masked regions and lower weights (σ) to unmasked regions, focusing cross-attention on semantically relevant areas while suppressing background interference. This addresses the limitation that pre-trained encoders pay less attention to clothes (only 21.5% and 37.3% respectively vs. 47.1% for body).

### Mechanism 2: Siamese dual-UNet architecture
The dual-UNet architecture (SourceNet + TargetNet) produces better-aligned source embeddings than pre-trained encoders by sharing identical architecture and initial weights. This natural alignment addresses the limitation that pre-trained encoders like CLIP, DINOv2, and Swin-B pay less attention to clothes during diffusion denoising. The Siamese approach maintains feature alignment during training when source-target pose discrepancy is manageable.

### Mechanism 3: CLIP-guided attention alignment (CAA)
CAA adaptively enhances underrepresented regions by cross-attending refined embeddings with CLIP features from low-attention parsing regions. The module identifies K regions with lowest average attention scores from HPFA output, extracts CLIP embeddings for these regions, and performs cross-attention to boost features in locally weak areas. This addresses the issue that some areas, particularly the face, remain underrepresented in HPFA output.

## Foundational Learning

- **Stable Diffusion Latent Space & Denoising Process**: Essential for understanding the modified SD v1.5 architecture and how zt = √α̅t·z0 + √(1-α̅t)·ε (Eq. 1) and classifier-free guidance (Eq. 11) integrate with HPFA/CAA. Quick check: Can you explain why the paper uses η=30% dropout for classifier-free training?

- **Cross-Attention in Diffusion UNets**: Prerequisite for implementing Equations 7-9 and understanding how both HPFA and CAA inject conditions via cross-attention mechanisms. Quick check: How does mask-weighted attention (M' in Eq. 6) differ from standard self-attention?

- **Human Parsing Segmentation**: Critical for understanding how parsing maps Hs define M regions and the limitations of parsing categories (arm, leg, dress, skirt). Quick check: What happens if the parsing model fails to segment a clothing item (e.g., patterned dress merges with background)?

## Architecture Onboarding

- Component map: Source Image (Is) → VAE Encoder → zs → SourceNet → F' (L layer embeddings) → HPFA: M' reweighting → Cross-attention with Fl → CLIP(Ri) → Low-score region selection → CAA: Cross-attention with F̃ → Target Pose (pτ) → PoseEncoder → TargetNet (with HPFA+CAA inserted) → DDIM sampling (35 steps) → VAE Decoder → Iτ

- Critical path: SourceNet embedding extraction → HPFA mask-weighted attention → CAA CLIP refinement → TargetNet denoising. If any stage fails, clothing patterns degrade.

- Design tradeoffs:
  - HPFA σ/δ values (0.3/0.6): Higher values risk overfitting/distortion; lower values lose attention guidance
  - K=2 for CAA: Selecting more regions increases compute but may over-enhance; K=2 balances refinement vs. stability
  - Siamese weight sharing: Maintains alignment but doubles UNet parameters; alternative is frozen pre-trained encoder (cheaper but worse alignment)

- Failure signatures:
  - Clothing patterns blurry/distorted → Check HPFA mask quality; parsing may have failed
  - Face details lost → CAA may not be selecting face region; verify K selection
  - Background artifacts → σ value too high; reduces to 0.2
  - Pose misalignment → PoseEncoder (4 conv layers) may need adjustment for extreme poses

- First 3 experiments:
  1. Reproduce ablation baseline B1: Train with CLIP-only source encoding (no SourceNet/HPFA/CAA) on DeepFashion 256×176 subset to verify LPIPS ~0.27 range
  2. HPFA-only variant: Ablate CAA (set K=0) to isolate HPFA contribution; expect LPIPS ~0.187 per Table 2 B3
  3. Parsing robustness test: Run on 20 images with unusual clothing (hoodies, capes) where parsing may fail; visualize attention maps

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The paper does not specify exact UNet layers where HPFA and CAA modules are inserted, which could affect performance
- The CLIP variant (ViT-B/32 vs ViT-L/14) and region cropping strategy for parsing categories remain unspecified
- The K=2 selection for CAA is empirically chosen without sensitivity analysis

## Confidence
- High confidence: The core mechanism of parsing-guided attention (HPFA) is well-supported by the ablation study showing LPIPS improvement from 0.187 to 0.151
- Medium confidence: The claim that Siamese architecture provides better alignment than pre-trained encoders is supported by ablation B1 but lacks comparative analysis with other fine-tuned encoders
- Low confidence: The CLIP-guided attention alignment's contribution (CAA) is validated only through one ablation (B3), and its interaction with parsing failures is not examined

## Next Checks
1. Test parsing robustness on 50 images with unusual clothing items (hoodies, capes, complex patterns) and visualize attention maps to identify failure modes
2. Perform K sensitivity analysis for CAA (K=1, 2, 3, 4) on the same dataset to quantify trade-off between refinement and over-enhancement
3. Implement a variant replacing SourceNet with frozen pre-trained CLIP encoder to verify the claimed alignment advantage of the Siamese architecture