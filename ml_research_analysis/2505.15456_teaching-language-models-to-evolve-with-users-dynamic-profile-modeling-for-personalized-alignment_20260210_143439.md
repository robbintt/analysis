---
ver: rpa2
title: 'Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for
  Personalized Alignment'
arxiv_id: '2505.15456'
source_url: https://arxiv.org/abs/2505.15456
tags:
- uni00000013
- user
- uni00000051
- uni00000048
- profile
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RLPA, a reinforcement learning framework that
  dynamically infers and updates user profiles through dialogue to achieve personalized
  alignment in language models. It addresses limitations of static methods like prompt-based
  and offline optimization, which struggle with cold-start scenarios and long-term
  personalization.
---

# Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment

## Quick Facts
- arXiv ID: 2505.15456
- Source URL: https://arxiv.org/abs/2505.15456
- Reference count: 40
- Introduces RLPA framework for dynamic user profile inference in personalized alignment

## Executive Summary
This paper addresses the challenge of personalized alignment in language models by introducing RLPA (Reinforcement Learning for Personalized Alignment), a framework that dynamically infers and updates user profiles through dialogue. Unlike static methods like prompt-based or offline optimization, RLPA employs a dual-level reward structure—Profile Reward for accurate user profile construction and Response Reward for generating profile-aligned responses—guided by interaction with a simulated user. Fine-tuning Qwen-2.5-3B-Instruct with RLPA produces Qwen-RLPA, which significantly outperforms baselines and commercial models in personalization quality while maintaining efficiency.

## Method Summary
The method formulates personalized dialogue as a Markov Decision Process where the model must infer and maintain a user profile while generating aligned responses. Training uses PPO fine-tuning on Qwen-2.5-3B-Instruct with a simulated user (GPT-4o-mini) that reveals profile information gradually. The dual reward structure combines Profile Reward (F1 matching between inferred and golden profiles) with Response Reward (binary gate across five quality dimensions). The model is trained on the ALOE dataset converted to structured slot-value profiles, with training running for 80,000 steps on 8x A100 GPUs.

## Key Results
- Qwen-RLPA achieves 73.38 average alignment score, significantly outperforming SFT (45.54) and DPO (66.19) baselines
- Surpasses commercial models Claude-3.5 and GPT-4o in personalization quality metrics
- Demonstrates robust performance in handling conflicting preferences and sustaining long-term coherence across 50+ turns
- Maintains efficient inference while delivering superior personalization compared to reasoning-focused LLMs

## Why This Works (Mechanism)

### Mechanism 1: Dual-Level Reward Synergy
- Combining profile inference supervision with response quality evaluation creates mutually reinforcing learning signals. The Profile Reward uses slot-wise F1 matching while Response Reward applies a strict binary gate across five quality dimensions. Together they optimize both "understanding user" and "responding appropriately" objectives simultaneously.
- **Core assumption**: Accurate profile inference causally improves response personalization.
- **Evidence anchors**: Ablation shows removing either component drops performance significantly (Profile-only: 45.54 avg vs full: 73.38; Response-only: 66.19 with instability).
- **Break condition**: If profile inference and response generation compete for model capacity rather than reinforce each other.

### Mechanism 2: Simulated User with Incremental Disclosure
- Training against a simulated user that reveals profile information gradually forces the model to learn incremental profile refinement. The user simulator is prompted to avoid revealing >15% of profile content per turn while maintaining behavioral consistency.
- **Core assumption**: The simulated user's behavior distribution sufficiently approximates real user interaction patterns.
- **Evidence anchors**: Human evaluation selects GPT-4o-mini simulator based on coherence, stability, proactivity, and persona-fit metrics (avg 4.04/5).
- **Break condition**: If the simulated user's disclosure patterns diverge significantly from real users, the model may overfit to simulation artifacts.

### Mechanism 3: Turn-Level Reward Shaping in MDP Formulation
- Providing combined rewards at every dialogue turn enables faster credit assignment for both profile accuracy and response quality. The MDP formulation treats each turn as a decision point where the model outputs both a response and an inferred profile.
- **Core assumption**: Short-term reward optimization does not conflict with long-term personalization goals.
- **Evidence anchors**: Profile scores increase steadily from 9.26 at turn 1 to 52.54 at turn 50 in 70-turn evaluation, suggesting sustained learning.
- **Break condition**: If the model learns to optimize short-term rewards at the expense of long-term profile coherence.

## Foundational Learning

- **Concept: Markov Decision Process (MDP)**
  - **Why needed here**: The paper formulates personalized dialogue as an MDP (state=history, action=response, transition=next user utterance, reward=dual-level). Understanding MDPs is essential to grasp why RL is applicable and how credit assignment works across turns.
  - **Quick check question**: Can you explain why treating dialogue as an MDP differs from treating it as supervised sequence prediction? What does the "transition function" represent in this context?

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here**: The paper uses PPO for policy optimization. PPO's clipping mechanism prevents destabilizing policy updates during RL training—critical when rewards come from non-differentiable LLM-based evaluators.
  - **Quick check question**: What problem does PPO's clipping term solve? Why would standard policy gradient be risky when the reward model is itself an LLM?

- **Concept: Slot-Filling / Structured Profile Representation**
  - **Why needed here**: Profile Reward relies on slot-value matching (e.g., Age, Interests, Occupation). Understanding structured representation enables you to debug profile inference errors and modify the schema for new domains.
  - **Quick check question**: If you wanted to add a new profile slot (e.g., "Communication Style"), what changes would be needed in the Profile Reward calculation?

## Architecture Onboarding

- **Component map**: Policy Model (Qwen-2.5-3B-Instruct → Qwen-RLPA) -> Simulated User (GPT-4o-mini) -> Profile Reward Model -> Response Reward Model (GPT-4o-mini) -> PPO Optimizer (OpenRLHF + vLLM)
- **Critical path**: Policy receives dialogue history → outputs structured profile + response → Simulated user generates next utterance conditioned on golden profile → Profile Reward calculates F1 between inferred and golden profiles → Response Reward evaluates response quality (binary) → Combined reward feeds PPO update → Loop repeats for up to 10 turns per episode
- **Design tradeoffs**: Simulated vs Real Users (simulation enables scalable training but risks distribution shift), Slot-based vs Free-form Profiles (slots enable structured supervision but limit expressiveness), Binary vs Scalar Response Reward (strict binary gate ensures quality but may slow training convergence)
- **Failure signatures**: Profile drift (inferred profile becomes inconsistent across turns), Reward hacking (model learns to game reward model artifacts), Simulator overfitting (model performs well with GPT-4o-mini simulator but fails with real users), Cold-start collapse (early-turn alignment scores near zero)
- **First 3 experiments**: 1) Reproduce ablation (Table 2) training with Profile-only, Response-only, and full rewards, 2) Stress-test simulator dependency replacing GPT-4o-mini with DeepSeek-V3 and measure alignment score degradation, 3) Long-term coherence probe running 50+ turn dialogues and plotting profile accuracy over time

## Open Questions the Paper Calls Out
- **Open Question 1**: How can RLPA be extended to handle multi-user, multi-session, or cross-domain personalization? The framework currently assumes a single-user interaction thread.
- **Open Question 2**: What are the theoretical convergence properties and long-term alignment dynamics of the RLPA framework? The "theoretical understanding of long-term alignment dynamics and convergence properties remains underexplored."
- **Open Question 3**: Does the performance of RLPA transfer to real-world human users given it was trained exclusively on a simulated user model (GPT-4o-mini)? The distribution of simulated responses may not perfectly match the complexity of real human interactions.

## Limitations
- The simulator-dependent nature creates significant generalization risks—performance with real users may deviate substantially from GPT-4o-mini evaluation results
- The binary response reward gate may create sparse learning signals that could slow convergence or cause suboptimal policy updates
- Long-term profile coherence beyond 50 turns remains untested, with potential for profile drift or contradiction accumulation

## Confidence
- **High**: Dual-reward architecture effectively improves personalization metrics on controlled benchmarks (A/B tests on Profile-only vs Response-only confirm synergistic effects)
- **Medium**: Simulated user quality sufficient for meaningful training (human evaluation shows GPT-4o-mini simulator scores 4.04/5, but real-user deployment may reveal gaps)
- **Low**: Commercial model comparisons (Claude-3.5/GPT-4o) generalize to production settings (leaderboard-style evaluation on curated datasets may not reflect real-world personalization complexity)

## Next Checks
1. **Real-User Validation**: Deploy RLPA with 50+ actual users across 10+ turns and compare profile accuracy/coherence against GPT-4o-mini simulator predictions
2. **Cross-Simulator Generalization**: Train with GPT-4o-mini simulator but test with DeepSeek-V3 simulator (or vice versa) to quantify simulator dependency
3. **Long-Horizon Stress Test**: Extend dialogue to 100+ turns with the same user profile to detect profile drift, contradiction accumulation, or response quality degradation