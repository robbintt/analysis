---
ver: rpa2
title: 'SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models'
arxiv_id: '2504.02883'
source_url: https://arxiv.org/abs/2504.02883
tags:
- unlearning
- retain
- forget
- task
- split
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper summarizes SemEval-2025 Task 4 on unlearning sensitive
  content from Large Language Models (LLMs). The task featured three subtasks: unlearning
  long-form synthetic creative documents, short-form synthetic biographies containing
  personally identifiable information (PII), and real documents sampled from the target
  model''s training dataset.'
---

# SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models

## Quick Facts
- arXiv ID: 2504.02883
- Source URL: https://arxiv.org/abs/2504.02883
- Reference count: 40
- Top team achieved 0.706 final score on 7B model

## Executive Summary
SemEval-2025 Task 4 challenged participants to remove sensitive content from LLMs through unlearning. The task featured three subtasks: unlearning long-form synthetic creative documents, short-form synthetic biographies containing PII, and real documents sampled from the target model's training dataset. Twelve distinct metrics evaluated both regurgitation (sentence completion) and knowledge (question-answer pairs) for each subtask. While top teams achieved near-perfect performance on forget sets, substantial degradation occurred on retain sets, indicating over-unlearning. The best final score achieved was 0.706 for the 7B model, with notable utility drop from 0.494 to 0.443 MMLU score.

## Method Summary
The challenge evaluated unlearning methods on OLMo-7B and 1B parameter models fine-tuned on forget (sensitive) and retain (non-sensitive) datasets. Participants could use any unlearning algorithm, with baseline methods including Gradient Ascent, Gradient Difference, KL Regularization, and NPO. The winning team used Gradient Difference with LoRA adapters on transformer projection layers, carefully sampling chunks of the forget set mixed with a large, resampled retain set. Teams submitted code for reproducibility, and evaluation occurred on private test sets using three axes: Task Aggregate (harmonic mean of 12 memorization metrics), MIA Score (loss-based membership inference), and MMLU Utility (general capability preservation).

## Key Results
- AILS-NTUA achieved the highest final score of 0.706 on the 7B model using Gradient Difference with LoRA adapters
- Substantial over-unlearning observed, with MMLU utility dropping from 0.494 to 0.443 for the top submission
- Task 2 (PII unlearning) was relatively easier than Tasks 1 and 3, with near-perfect scores for top teams
- Model merging strategy by ZJUKLAB achieved highest Task Aggregate (0.944) but poor MIA score (0.048)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient Difference with LoRA and strategic data chunking achieves effective unlearning while preserving utility
- Mechanism: Gradient ascent on forget set reduces likelihood of generating sensitive content; gradient descent on retain set (resampled at higher volume) preserves general knowledge. LoRA adapters restrict updates to low-rank subspace, enabling more iterations within compute budget without catastrophic interference.
- Core assumption: Sensitive information localizes to modifiable parameters without affecting unrelated capabilities; retain set coverage generalizes to held-out utility tasks.
- Evidence anchors:
  - [abstract]: Top team achieved 0.706 (7B) and 0.688 (1B) final scores
  - [section 4, Table 5]: "AILS-NTUA: Iterative unlearning on carefully sampled chunks of forget set, mixed with a larger volume of retain set"
  - [corpus]: AILS-NTUA paper (arXiv:2503.02443) confirms "parameter-efficient, gradient-based unlearning using low-rank (LoRA) adaptation"
- Break condition: If forget/retain distributions overlap significantly, gradient conflict causes both under-unlearning and utility degradation simultaneously.

### Mechanism 2
- Claim: Multi-metric evaluation (Task Aggregate + MIA + MMLU) exposes hidden trade-offs invisible to single-metric optimization
- Mechanism: ROUGE-L regurgitation and knowledge accuracy measure surface-level forgetting. Membership inference attack (MIA) AUC detects whether forget-set examples remain statistically distinguishable in loss space. MMLU ensures general capabilities survive. Harmonic/arithmetic means penalize asymmetric failure modes.
- Core assumption: Low regurgitation does not guarantee parameter-level erasure; MIA AUC ≈ 0.5 indicates indistinguishability from non-members.
- Evidence anchors:
  - [section 2.5.2]: "MIA Score = 1 − 2 · |mia_auc − 0.5|"
  - [section 4.1, Table 3]: ZJUKLAB achieved 0.944 Task Aggregate but 0.048 MIA Score, revealing incomplete unlearning
  - [corpus]: Weak/missing—neighbor papers focus on methods, not evaluation framework validation
- Break condition: If unlearning method modifies loss landscape in ways MIA doesn't capture (e.g., output noise injection), MIA AUC may be misleadingly low while information persists.

### Mechanism 3
- Claim: Model merging balances under/over-unlearning by interpolating between differently-tuned checkpoints
- Mechanism: Train two models with distinct hyperparameters (aggressive vs. conservative unlearning). Merge their weights to average out extremes—one model retains too much, the other destroys utility; merged model lands in workable middle ground.
- Core assumption: Unlearning dynamics are roughly linear in weight space; optimal operating point lies between two trained configurations.
- Evidence anchors:
  - [section 4, Table 5]: "ZJUKLAB: Two distinct NPO+KL+GD trained models are merged to balance under/over-unlearning"
  - [section 4.1]: ZJUKLAB achieved highest Task Aggregate (0.944) but poor MIA (0.048), suggesting merge improved one axis but not both
  - [corpus]: ZJUKLAB paper (arXiv:2503.21088) confirms "Unlearning via Model Merging" as core strategy
- Break condition: If unlearning causes nonlinear catastrophic forgetting (e.g., mode collapse), linear interpolation may not recover utility.

## Foundational Learning

- **Gradient Ascent vs. Descent Dynamics**
  - Why needed here: Understanding that reversing loss sign is unbounded and can cause divergence, while retaining descent on retain set stabilizes updates.
  - Quick check question: Why does pure gradient ascent on forget set risk model collapse, but gradient difference doesn't?

- **KL Divergence as Distributional Constraint**
  - Why needed here: KL regularization prevents unlearned model from drifting too far from reference, trading off forgetting strength for stability.
  - Quick check question: If KL(θ, θ_ref) is weighted too heavily, what happens to forget-set erasure?

- **MIA AUC Interpretation**
  - Why needed here: AUC = 0.5 means random chance—ideal for unlearning; AUC > 0.5 means model still "remembers" forget set in its loss patterns.
  - Quick check question: A model achieves 0.0 regurgitation but MIA AUC = 0.8. Is unlearning successful?

## Architecture Onboarding

- **Component map**:
  - Target model (OLMo-7B/1B, pre-fine-tuned on forget+retain sets)
  - Unlearning loop: forget set → gradient ascent / NPO loss; retain set → gradient descent / KL constraint
  - LoRA adapters (optional): attached to transformer projection layers
  - Evaluation: 12 task metrics → harmonic mean; MIA classifier; MMLU probe

- **Critical path**:
  1. Load pre-fine-tuned OLMo checkpoint
  2. Initialize unlearning objective (GD / NPO / custom)
  3. Chunk forget set, oversample retain set
  4. Train for N epochs with early stopping on retain performance
  5. Run all 3 evaluation axes before submission

- **Design tradeoffs**:
  - LoRA vs. full fine-tuning: LoRA enables more epochs within time limit but may under-unlearn deeply embedded knowledge
  - Aggressive vs. conservative unlearning: Aggressive improves forget metrics but risks MMLU threshold failure (0.371 floor for 7B)
  - MIA vs. Task Aggregate: ZJUKLAB showed optimizing one can harm the other

- **Failure signatures**:
  - Over-unlearning: Retain set scores drop significantly, MMLU falls below threshold
  - Under-unlearning: Forget set regurgitation > 0.2 or MIA AUC > 0.6
  - Model instability: Garbage token generation on specific prompts (observed even in top submission)
  - Timeout: Code not DeepSpeed-compatible; manual fixes required

- **First 3 experiments**:
  1. Replicate AILS-NTUA baseline: GD with LoRA on projection layers, 4:1 retain:forget sampling ratio, 5 epochs
  2. Ablate retain set volume: Test 2:1, 4:1, 8:1 ratios; measure Task Aggregate vs. MMLU trade-off curve
  3. Add MIA monitoring during training: Track loss-based AUC on held-out forget subset each epoch to detect when unlearning plateaus vs. over-unlearning begins

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can unlearning methods achieve both high task aggregate scores and high MIA scores while preserving full model utility?
- Basis in paper: [explicit] The authors observe "a trade-off between MIA and the Task Aggregate scores" and note that even the top team experienced notable MMLU utility drop (0.494 to 0.443) and generated "garbage tokens with specific prompts."
- Why unresolved: No participating team simultaneously maximized all three metrics; ZJUKLAB achieved high Task Aggregate (0.944) but very low MIA (0.048), while AILS-NTUA balanced metrics but still showed degradation.
- What evidence would resolve it: A unified method achieving >0.90 on Task Aggregate, >0.85 on MIA score, and maintaining MMLU within 5% of the original model.

### Open Question 2
- Question: How do unlearning methods scale to larger LLMs (e.g., 70B+ parameters) in terms of effectiveness and computational efficiency?
- Basis in paper: [explicit] "In future work we may expand on this challenge by inviting a subset of teams to onboard to specialized compute platforms to motivate further research on unlearning larger models."
- Why unresolved: The challenge was limited to 1B and 7B parameter models due to participants' compute constraints; scalability to larger models remains untested.
- What evidence would resolve it: Benchmark results on 70B+ models showing comparable forgetting-retention trade-offs with manageable compute requirements.

### Open Question 3
- Question: What evaluation metrics can reliably compare unlearned LLMs to gold-standard retrained models without prohibitive computational cost?
- Basis in paper: [explicit] "Outside LLMs, unlearning literature typically uses some form of statistical hypothesis testing between the model posteriors from the unlearned and the retrained model candidates. However, this is not feasible for LLMs since the model would have to be trained from ground up."
- Why unresolved: Full retraining for LLMs is computationally infeasible, so current metrics (memorization rates, MIA, MMLU) serve as imperfect proxies.
- What evidence would resolve it: A proxy metric that correlates strongly with retrained model behavior across diverse unlearning scenarios, validated on smaller models where retraining is feasible.

### Open Question 4
- Question: Do unlearning methods generalize across different content types (creative text, PII, factual knowledge) and unlearning tasks (copyright removal vs. capability erasure)?
- Basis in paper: [inferred] Task 2 (synthetic PII) appeared "relatively easier" than Tasks 1 and 3, with near-perfect scores for top teams; the authors mention "Unlearning of sensitive information or a class of model capabilities" as a future direction.
- Why unresolved: Current methods show uneven performance across tasks, and unlearning model capabilities (e.g., coding) rather than specific content is unexplored in this benchmark.
- What evidence would resolve it: A single unlearning method achieving >0.95 Task Aggregate across all three tasks and demonstrating effectiveness on capability-based unlearning (e.g., disabling code generation while preserving natural language abilities).

## Limitations

- Evaluation framework may not capture all forms of information persistence, as MIA AUC could miss certain unlearning-resistant patterns
- Private test set composition remains unknown, limiting reproducibility of final Task Aggregate scores
- Winning team's specific LoRA configuration (rank, alpha, target modules) and exact sampling strategies remain underspecified

## Confidence

- **High confidence**: Challenge structure, baseline algorithms, and observation of over-unlearning are well-supported
- **Medium confidence**: Winning team's specific implementation details and model merging strategy lack full transparency
- **Low confidence**: MIA AUC as complete measure of parameter-level erasure and evaluation framework's comprehensive effectiveness

## Next Checks

1. **MIA Sensitivity Analysis**: Implement controlled unlearning experiments where sensitive information is deliberately preserved through different mechanisms (parameter modification vs. output noise vs. distributional shift). Measure whether MIA AUC consistently detects all forms of information persistence, or whether it misses certain unlearning-resistant patterns.

2. **Parameter-Level Analysis of Top Methods**: Apply the winning AILS-NTUA approach (GD with LoRA) to a simplified task with known sensitive content. After unlearning, perform targeted analysis of parameter changes in the specified projection layers versus other transformer components. Compare parameter distance metrics with MIA scores and regurgitation rates to validate whether MIA AUC truly reflects parameter-level erasure.

3. **Generalization of Model Merging**: Replicate ZJUKLAB's merging strategy on a synthetic benchmark where the optimal interpolation ratio is known a priori. Systematically vary the merging weights between aggressive and conservative unlearning checkpoints, measuring the resulting trade-off curve between forget-set erasure and retain-set utility. Determine whether the observed balance was due to the specific hyperparameter combination or a generalizable property of model merging for unlearning.