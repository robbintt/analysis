---
ver: rpa2
title: 'Everything is Plausible: Investigating the Impact of LLM Rationales on Human
  Notions of Plausibility'
arxiv_id: '2510.08091'
source_url: https://arxiv.org/abs/2510.08091
tags:
- rating
- rationale
- human
- plausibility
- rationales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We show that LLM-generated rationales can significantly influence\
  \ human plausibility judgments of commonsense reasoning answers, with PRO rationales\
  \ increasing and CON rationales decreasing mean ratings. Humans and LLMs alike exhibit\
  \ shifts in ratings when rationales are presented, but differ in their response\
  \ patterns\u2014humans are less swayed overall, especially for gold answers, while\
  \ LLMs show larger changes."
---

# Everything is Plausible: Investigating the Impact of LLM Rationales on Human Notions of Plausibility

## Quick Facts
- **arXiv ID**: 2510.08091
- **Source URL**: https://arxiv.org/abs/2510.08091
- **Reference count**: 40
- **Primary result**: LLM-generated rationales significantly shift human plausibility judgments, with PRO rationales increasing and CON rationales decreasing mean ratings.

## Executive Summary
This paper investigates how LLM-generated rationales influence human and LLM plausibility judgments in commonsense reasoning tasks. The study finds that PRO rationales increase plausibility ratings for distractor answers while CON rationales decrease ratings for gold-label answers. Humans and LLMs both exhibit rating shifts when rationales are presented, though humans are less swayed overall. A key finding is that initial plausibility ratings anchor subsequent changes—higher initial ratings lead to smaller shifts. The paper raises concerns about the persuasive potential of LLM-generated rationales and their influence on human judgment in commonsense domains.

## Method Summary
The study sampled 100 questions (50 from SIQA, 50 from CQA) with 200 (question, answer) pairs (gold + 1 distractor each). GPT-4o was selected to generate 2-sentence PRO and CON rationales for each pair. Human annotators (US-based, English primary) rated each item under four conditions (NO, PRO, CON, PRO+CON) on a 5-point Likert scale. 17 LLMs also rated items using zero-shot prompting. The analysis computed mean ratings, rating changes (Δ), and used Chi-squared tests for distribution shifts and OLS regression to predict rating changes based on initial ratings and rationale types.

## Key Results
- LLM-generated rationales systematically shift plausibility judgments in the direction of the argument (PRO increases, CON decreases)
- Initial plausibility ratings anchor subsequent rating changes—higher initial ratings lead to smaller shifts
- CON rationales exert stronger influence than PRO rationales on both human and LLM judgments
- PRO+CON rationales produce mixed results, generally pushing ratings toward the middle of the scale
- Humans are less swayed by rationales than LLMs, especially for gold answers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM-generated rationales systematically shift plausibility judgments in the direction of the argument.
- **Mechanism**: Rationales provide contextual scenarios that expand or constrain the judge's mental model of possible circumstances. PRO rationales highlight conditions where the answer becomes more likely; CON rationales highlight conditions where it becomes less likely. The judge incorporates these scenarios into their assessment, adjusting the weighted probability.
- **Core assumption**: Judges interpret rationales as plausible hypotheticals rather than detecting them as manufactured arguments.
- **Evidence anchors**:
  - "We observe increases and decreases in mean human plausibility ratings in the presence of LLM-generated PRO and CON rationales, respectively"
  - Table 3 shows mean rating changes: PRO rationales increase distractor ratings (+0.47 SIQA, +0.45 CQA); CON rationales decrease gold ratings (-1.39 SIQA, -1.08 CQA)
- **Break condition**: If judges are explicitly told rationales are AI-generated counterarguments rather than genuine explanations, or if rationales contain logical fallacies that judges detect, the effect may diminish or reverse.

### Mechanism 2
- **Claim**: Initial plausibility ratings anchor subsequent rating changes—higher initial ratings lead to smaller shifts.
- **Mechanism**: Judges form an initial probability estimate before seeing rationales. This estimate serves as a reference point. When confronted with disconfirming evidence (rationales), adjustment from the anchor is insufficient—the classic anchoring bias. High-confidence initial judgments resist downward revision more than low-confidence judgments resist upward revision.
- **Core assumption**: The anchoring operates at the level of probability estimation rather than argument evaluation.
- **Evidence anchors**:
  - OLS regression shows "NO Rationale Plausibility Rating is a statistically significant predictor (p < 0.05) of the rating change" with "consistently negative coefficients"
  - "The anchoring effect is more pronounced for distractors than for gold-label answers across all groups"
- **Break condition**: If judges are forced to fully re-evaluate without access to their initial rating, or if the rationale directly contradicts core commonsense knowledge, anchoring may weaken.

### Mechanism 3
- **Claim**: CON rationales exert stronger influence than PRO rationales.
- **Mechanism**: Negative information carries greater epistemic weight in uncertain reasoning—a negativity bias in plausibility assessment. A single counterexample can definitively falsify, while supporting evidence only incrementally confirms. Judges treat CON rationales as identifying fatal flaws; PRO rationales are treated as "at best, this might be possible."
- **Core assumption**: This asymmetry reflects human reasoning tendencies.
- **Evidence anchors**:
  - "CON rationales exert a stronger effect than PRO rationales, with large negative coefficients" while "PRO rationales do increase plausibility, but the magnitude of their coefficients is smaller"
  - "CON rationales have a stronger impact than PRO rationales"
- **Break condition**: If CON rationales require implausible assumptions (e.g., "the glass won't break if dropped on the moon"), judges may discount them.

## Foundational Learning

- **Concept: Graded plausibility vs. binary truth**
  - Why needed here: Commonsense reasoning operates on a continuum (1-5 Likert scale), not definitive true/false. Understanding that "correct" answers are merely "most plausible given typical scenarios" is essential for interpreting why rationales can shift judgments.
  - Quick check question: If an answer is rated "3 - Plausible" rather than "5 - Very Likely," what does that imply about the judge's mental model of possible scenarios?

- **Concept: Defeasible reasoning**
  - Why needed here: The paper's framework relies on judgments being revisable when new information (rationales) is introduced. Defeasible inference allows conclusions to be withdrawn when counterarguments emerge—this is the cognitive mechanism rationales exploit.
  - Quick check question: Why can a PRO rationale for "the glass bounces" be valid even if "the glass breaks" is typically more plausible?

- **Concept: Anchoring bias in sequential judgment**
  - Why needed here: The regression analysis assumes initial ratings constrain later adjustments. Without understanding anchoring, you might expect rationales to produce uniform shifts independent of baseline, which the data contradict.
  - Quick check question: If a judge initially rates something "5 - Very Likely," why might a CON rationale produce a smaller absolute change than if they had rated it "3 - Plausible"?

## Architecture Onboarding

- **Component map**:
  - **Rationale Generator** (GPT-4o) → **Plausibility Rating Collector** → **Rating Aggregator** → **Regression Analyzer**

- **Critical path**:
  1. Sample (q, a) pairs from SIQA/CQA (gold + one distractor per question)
  2. Generate PRO and CON rationales using GPT-4o (temperature=1, max 2 sentences)
  3. Collect 5 human ratings per (q, a, rationale-type) condition (NO, PRO, CON, PRO+CON)
  4. Prompt 17 LLMs for ratings under identical conditions (temperature=0 for OpenAI, 0.1 for others)
  5. Compute mean ratings and changes; run Chi-squared tests for distribution shifts; run OLS regression for predictors

- **Design tradeoffs**:
  - **Single rationale generator (GPT-4o)**: Controls for model-specific style but limits generalizability—findings may not extend to other generators.
  - **2-sentence rationale limit**: Ensures brevity for annotation feasibility but may truncate more nuanced arguments.
  - **Separate annotators per rationale-type**: Prevents within-subject comparison but avoids carryover effects.
  - **US-based annotators only**: Controls for cultural variation in commonsense but limits cross-cultural validity.

- **Failure signatures**:
  - Rationales showing no significant distribution shift (Chi-squared p > 0.05) indicate the generator failed to produce persuasive arguments.
  - High variance in human ratings (low inter-annotator agreement) suggests ambiguous or subjective items.
  - PRO rationales decreasing gold-label ratings signals "underselling"—the rationale frame conflicts with the judge's prior.

- **First 3 experiments**:
  1. **Baseline replication**: Run the NO-rationale condition on a fresh sample to verify anchor ratings match before introducing rationales.
  2. **Rationale quality ablation**: Replace GPT-4o rationales with random sentences, weak arguments, or human-written arguments to test whether effect size correlates with argument quality.
  3. **Self-preference test**: Compare OpenAI models rating GPT-4o rationales vs. LLaMA rationales to isolate whether observed sensitivity is due to argument strength or source recognition.

## Open Questions the Paper Calls Out
- Why do PRO rationales cause human plausibility ratings to *decrease* for gold answers, while increasing them for distractors?
- To what extent do different LLM architectures vary in their persuasive impact on human and LLM plausibility judgments?
- How does the impact of LLM-generated rationales vary across languages, cultures, and domains beyond commonsense reasoning?
- Why do CON rationales exert a stronger influence than PRO rationales on both human and LLM plausibility judgments?

## Limitations
- Study uses a single rationale generator (GPT-4o), limiting generalizability to other models
- Human participants were exclusively US-based, not capturing cultural variation in commonsense reasoning
- 2-sentence constraint on rationales may truncate more nuanced arguments
- Paper does not investigate whether participants recognize rationales as AI-generated
- Analysis relies on plausibility ratings rather than direct measurement of reasoning processes

## Confidence

**High Confidence**: The anchoring effect finding (initial ratings predict smaller shifts for higher-rated items) is robust across both human and LLM data, supported by clear statistical evidence and consistent with established cognitive psychology literature.

**Medium Confidence**: The asymmetry finding (CON rationales exert stronger influence than PRO rationales) is supported by regression coefficients but lacks direct corpus evidence for this specific phenomenon in LLM-rationale contexts.

**Medium Confidence**: The core finding that LLM rationales shift human plausibility judgments is well-supported for distractors but shows concerning patterns for gold answers (PRO rationales sometimes decrease gold ratings, suggesting "underselling").

## Next Checks
1. **Cross-generator validation**: Repeat the study using LLaMA-3.1 and Claude-3.5 for rationale generation to test whether GPT-4o-specific style or reasoning patterns drive the observed effects.

2. **Transparency manipulation**: Add a condition where human participants are explicitly told rationales are AI-generated counterarguments, then compare persuasion effects to the original design to test whether awareness moderates the effect.

3. **Self-preference control**: Compare OpenAI model sensitivity to GPT-4o rationales versus LLaMA rationales to isolate whether observed shifts reflect genuine argument strength or model-specific preference patterns.