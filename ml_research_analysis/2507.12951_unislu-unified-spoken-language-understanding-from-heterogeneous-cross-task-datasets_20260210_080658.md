---
ver: rpa2
title: 'UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task
  Datasets'
arxiv_id: '2507.12951'
source_url: https://arxiv.org/abs/2507.12951
tags:
- tasks
- unified
- unislu
- task
- spoken
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Spoken Language Understanding
  (SLU), which encompasses tasks like Automatic Speech Recognition (ASR), spoken Named
  Entity Recognition (NER), and spoken Sentiment Analysis (SA). Existing methods typically
  use separate models for each task, limiting cross-task interactions and failing
  to fully leverage heterogeneous datasets.
---

# UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets

## Quick Facts
- **arXiv ID:** 2507.12951
- **Source URL:** https://arxiv.org/abs/2507.12951
- **Reference count:** 18
- **Primary result:** Unified generative framework for ASR, spoken NER, and SA that achieves superior SLUE SCORE compared to separate models and strong baselines

## Executive Summary
UniSLU addresses the challenge of Spoken Language Understanding (SLU) by proposing a unified generative framework that jointly models Automatic Speech Recognition (ASR), spoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA) within a single architecture. The key innovation is a unified representation that formulates these heterogeneous tasks into a consistent format, enabling effective use of diverse datasets. By employing a dynamic loss mechanism to balance task-specific training and integrating with large language models, UniSLU achieves superior overall SLUE SCORE compared to separate models while maintaining consistent improvements across all three tasks.

## Method Summary
UniSLU proposes a unified generative framework that jointly models ASR, NER, and SA within a single Whisper-based architecture. The core innovation is a unified representation format that converts heterogeneous tasks into a consistent generative format using special control tokens. The framework employs a dynamic weighted loss mechanism to balance task-specific training, where task weights are determined by the ratio of each task's output length to the total sequence length. The model fine-tunes pre-trained Whisper models (small or medium variants) on combined datasets, with full encoder and decoder fine-tuning for smaller models and frozen encoders for medium models. Experiments validate the approach on SLUE-VoxPopuli and SLUE-VoxCeleb datasets, demonstrating superior performance across all three SLU tasks.

## Key Results
- UniSLU achieves superior overall SLUE SCORE compared to separate task-specific models and strong baselines
- The unified model shows consistent improvements across ASR, NER, and SA tasks on SLUE-VoxPopuli and SLUE-VoxCeleb datasets
- UniSLU demonstrates better performance on samples with varying sequence lengths, indicating improved robustness compared to separate models
- The framework successfully leverages heterogeneous datasets while maintaining competitive performance on individual tasks

## Why This Works (Mechanism)
UniSLU works by unifying three heterogeneous SLU tasks into a single generative framework through a consistent representation format. The unified format "[ASR Transcript][T/L][NER/SA][Output]" allows the model to leverage cross-task interactions and shared representations. The dynamic loss mechanism balances task-specific training by weighting each task's loss according to its output length relative to the total sequence length, preventing task imbalance during training on heterogeneous datasets. By fine-tuning a pre-trained Whisper model with this unified approach, the framework benefits from both the strong ASR capabilities of the base model and the cross-task synergies enabled by joint training.

## Foundational Learning
- **Unified Representation Format:** The consistent template "[ASR][T/L][Task][Output]" needed to enable joint training across heterogeneous tasks. Quick check: Verify all three task types can be converted to this format without loss of information.
- **Dynamic Weighted Loss:** Task weights based on output length ratios (W_task = Len_ASR/Len_total) needed to balance training across tasks with different output lengths. Quick check: Confirm loss weights sum to 1 and vary appropriately with sequence composition.
- **Whisper Fine-tuning Strategy:** Full encoder+decoder fine-tuning for small models vs. frozen encoder for medium models needed to optimize performance across model capacities. Quick check: Compare WER/Micro-F1/Macro-F1 between fine-tuning strategies on validation sets.

## Architecture Onboarding
- **Component Map:** Pre-trained Whisper -> Unified Format Converter -> Dynamic Weighted Loss -> Fine-tuning Loop -> Evaluation
- **Critical Path:** Audio input → Whisper encoder → Unified format generation → Token prediction → Task-specific output parsing
- **Design Tradeoffs:** Full fine-tuning vs. frozen encoder (performance vs. parameter efficiency), unified vs. separate models (cross-task benefits vs. task specialization)
- **Failure Signatures:** ASR degradation when LLM decoder lacks sufficient pretraining, NER performance drop on long sequences (>30 words), task imbalance when dynamic loss is misconfigured
- **First Experiments:**
  1. Implement unified format conversion and verify correct tokenization for all three task types
  2. Test dynamic weighted loss on sample sequences to ensure proper task balancing
  3. Conduct ablation comparing full fine-tuning versus partial freezing on Whisper-small

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the unified framework be extended to additional SLU tasks (e.g., speaker diarization, intent detection, slot filling) while maintaining cross-task benefits? The paper states future work will extend to a broader range of tasks, languages, and application domains.
- **Open Question 2:** How can ASR performance degradation be mitigated when integrating LLM decoders while preserving the NER and SA gains? Section 4.4 notes ASR performance slightly declines due to limited LLM pretraining compared to Whisper's original decoder.
- **Open Question 3:** How does UniSLU perform on multilingual and code-switched speech data? The paper acknowledges the work focuses primarily on English data, with future work extending to languages.
- **Open Question 4:** Is the proposed unified representation format optimal compared to alternative sequential or parallel formulations? The paper proposes one specific template but does not ablate against alternative representations.

## Limitations
- **Code/Model Availability:** No released code or models at time of writing, creating significant barrier to independent verification
- **Dataset Scope:** Experiments limited to English-only datasets (SLUE-VoxPopuli, SLUE-VoxCeleb), leaving multilingual generalization untested
- **Implementation Details:** Exact data mixing strategy for heterogeneous datasets during training and inference protocol for control tokens remain unspecified

## Confidence
- **High Confidence:** The unified format approach and its theoretical benefits for cross-task learning
- **Medium Confidence:** The effectiveness of dynamic weighted loss in balancing heterogeneous tasks, based on reported experimental improvements
- **Low Confidence:** Practical deployment considerations and real-world performance without code release

## Next Checks
1. Implement the unified format conversion pipeline and verify correct tokenization with Whisper's vocabulary for all three task types
2. Recreate the dynamic weighted loss function and validate its behavior on sample sequences of varying lengths to ensure proper task balancing
3. Conduct ablation studies comparing full fine-tuning versus partial freezing on Whisper-small to quantify performance trade-offs reported in the paper