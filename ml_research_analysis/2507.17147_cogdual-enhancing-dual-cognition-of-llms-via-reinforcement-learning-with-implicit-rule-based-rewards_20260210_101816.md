---
ver: rpa2
title: 'CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with
  Implicit Rule-Based Rewards'
arxiv_id: '2507.17147'
source_url: https://arxiv.org/abs/2507.17147
tags:
- character
- reasoning
- cognitive
- cogdual
- dual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes CogDual, a dual cognitive framework for Role-Playing
  Language Agents (RPLAs) that incorporates situational and self-awareness to improve
  contextual understanding and character consistency. The method employs a cognize-then-respond
  paradigm and enhances the model through reinforcement learning using two implicit
  reward schemes: Inference-Conditioned Likelihood Gain (ICLG) and Latent Semantic
  Alignment (LSA).'
---

# CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards

## Quick Facts
- arXiv ID: 2507.17147
- Source URL: https://arxiv.org/abs/2507.17147
- Reference count: 20
- Up to 9.24% average improvement over baselines on CoSER benchmark

## Executive Summary
CogDual introduces a dual cognitive framework for Role-Playing Language Agents that explicitly models the transition from external perception to internal reflection before response generation. The method employs a cognize-then-respond paradigm with situational awareness (environmental perception + others' behavior/emotion/intent analysis) feeding into self-awareness (memory activation + self-emotion + self-intention + internal strategy). Evaluated on the CoSER benchmark and generalization tasks (Cross-MR, LifeChoice), CogDual achieves up to 9.24% average improvement over baselines, demonstrating superior performance in storyline consistency, anthropomorphism, and character fidelity across multiple open-source and closed-source LLMs.

## Method Summary
CogDual uses a two-stage training approach: first, supervised fine-tuning (SFT) on structured dual-cognition trajectories generated by GPT-4o using a cognize-then-respond template; second, reinforcement learning with GRPO using two implicit rewards - ICLG (measuring likelihood gain from conditioning on generated cognition) and LSA (measuring semantic similarity in frozen reference model's latent space). The model optimizes for both causally coherent reasoning and character-consistent semantic fidelity through a weighted combination of these rewards.

## Key Results
- 9.24% average improvement over baselines on CoSER benchmark
- Best Storyline Consistency (59.10) and Storyline Quality (72.42) with pure ICLG
- Highest Anthropomorphism (47.63) with pure LSA
- Superior performance across both open-source (Llama3.1-8B, Qwen2.5-7B) and closed-source LLMs

## Why This Works (Mechanism)

### Mechanism 1
Dual cognitive reasoning improves role-playing by explicitly modeling the transition from external perception to internal reflection before response generation. Situational Awareness (environmental perception + others' behavior/emotion/intent analysis) feeds into Self-Awareness (memory activation + self-emotion + self-intention + internal strategy), creating a cognize-then-respond pattern that grounds responses in both context and character psychology.

### Mechanism 2
ICLG reward promotes causally coherent reasoning by measuring likelihood gain from conditioning on generated cognition. RICLG = (πθ(dgolden | x ⊕ c) / πθ(dgolden | x))^(1/|dgolden|) quantifies how much the model's own cognitive trace c improves the likelihood of generating the reference response.

### Mechanism 3
LSA reward maintains semantic fidelity to character behavior while allowing expressive diversity. RLSA = cos(fref(x, dgolden), fref(x, d̂)) computes cosine similarity between mean-pooled hidden states from a frozen reference model, rewarding semantic proximity without requiring lexical matching.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Needed because standard RLHF requires training a separate reward model; GRPO enables optimization directly from implicit rule-based rewards by comparing trajectories within minibatches. Quick check: Can you explain why GRPO's advantage estimation (normalizing rewards within a batch) helps with the high-variance reward signals common in open-ended text generation?

- **Cognitive Psychology Foundations (Grice, Clark & Brennan, Tomasello)**: Needed because the dual cognition design draws from theories of human communication where individuals interpret environmental and social cues through mental representations before action. Quick check: How does the flow from Situational Awareness to Self-Awareness in CogDual map onto Tomasello's observation that "individuals interpret environmental and social cues through mental representations, which guide intentional actions"?

- **Likelihood-Based Reasoning Rewards**: Needed because ICLG requires understanding how conditioning on intermediate reasoning affects output probabilities. Quick check: Given RICLG = (πθ(dgolden | x ⊕ c) / πθ(dgolden | x))^(1/|dgolden|), what does a value >1.0 indicate about the relationship between the reasoning trace c and the target response?

## Architecture Onboarding

### Component Map
Input (Profile, Scene, Characters, History) -> [Stage 1: SFT] -> Dual Cognition Generator -> Response -> [Stage 2: RL with GRPO] -> Policy Model -> Generate (cognition, response) -> ICLG Reward <- Frozen Policy (likelihood comparison), LSA Reward <- Frozen SFT Model (hidden state similarity) -> Combined Reward -> GRPO Update -> Improved Policy

### Critical Path
1. SFT initialization is essential—RL without it is unstable
2. Reward weight tuning (λICLG=0.7, λLSA=0.3) significantly affects which metrics improve
3. Data quality filtering (cognitive field checks + GPT-4o verification) determines reasoning trajectory quality

### Design Tradeoffs
- Pure ICLG maximizes narrative coherence but may reduce expressive diversity
- Pure LSA maximizes anthropomorphism but may reduce storyline consistency
- Hybrid (0.7/0.3) provides best balance on average metrics but may not be optimal for specific task types

### Failure Signatures
- If Storyline Quality drops significantly, ICLG weight may be too low
- If responses become repetitive or formulaic, LSA weight may be too high
- If reasoning traces are incoherent or disconnected from responses, SFT data quality may be insufficient

### First 3 Experiments
1. **Ablate dual cognition components**: Train variants without situational awareness, without self-awareness, and without both. Compare on CoSER metrics to quantify each component's contribution.
2. **Reward weight sweep**: Run RL with λICLG ∈ {0.0, 0.3, 0.5, 0.7, 1.0} and measure impact on Storyline Consistency vs. Anthropomorphism to find task-optimal balance.
3. **Evaluator robustness check**: Evaluate trained models with multiple judges (GPT-4o, DeepSeek-v3, Gemini-2.0-Flash) to verify improvements are not evaluator-specific.

## Open Questions the Paper Calls Out

### Open Question 1
Does the CogDual reinforcement learning strategy scale effectively to models significantly larger than 8B parameters? The authors state in the Limitations section that due to computational constraints, they have not evaluated the effectiveness of their reinforcement learning approach on larger-scale models such as Llama3.1-70B-Instruct.

### Open Question 2
Can the dual cognitive reasoning paradigm transfer effectively to non-English role-playing contexts? The authors note that their current experiments are conducted solely on English datasets, and the model's adaptability to non-English contexts, such as Chinese role-playing scenarios, remains unexplored.

### Open Question 3
Does integrating an explicit retrieval mechanism into the Self-Awareness module improve performance over implicit context extraction? The authors identify a limitation where they rely on the model to extract previously mentioned memory fragments without incorporating an explicit retrieval mechanism to access character-specific memory.

## Limitations
- Dataset generality concerns due to domain specificity of CoSER (primarily narrative/role-play scenarios)
- Evaluator reliability issues with GPT-4o as sole primary judge for subjective metrics
- Unclear component independence - paper does not rigorously ablate situational and self-awareness separately

## Confidence

- **High Confidence**: Dual cognition architecture and formal definitions of ICLG/LSA rewards are well-specified and theoretically grounded
- **Medium Confidence**: Superiority of 0.7/0.3 reward weight combination is demonstrated but may vary by task type
- **Low Confidence**: Claims about generalizability to non-narrative tasks and necessity of dual cognition in all RP contexts are speculative

## Next Checks
1. **Component Ablation on Out-of-Domain Tasks**: Test CogDual on non-narrative tasks (customer service dialogue, code generation with personas) and ablate situational/self-awareness to quantify each component's contribution beyond CoSER.
2. **Reward Model Robustness**: Replace the frozen GPT-4o reference model in LSA with a different SFT-frozen model and measure changes in anthropomorphism and storyline consistency to assess sensitivity to semantic priors.
3. **Human Evaluation on Complex Narratives**: Conduct blinded human studies on a subset of CoSER examples, comparing CogDual to baselines on nuanced metrics like emotional depth, plot coherence, and character voice consistency.