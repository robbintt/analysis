---
ver: rpa2
title: 'HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents
  in Hierarchical Rule Application'
arxiv_id: '2510.19631'
source_url: https://arxiv.org/abs/2510.19631
tags:
- uni00000008
- agents
- uni00000003
- product
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HSCodeComp is the first benchmark to evaluate deep search agents
  in hierarchical rule application, focusing on predicting 10-digit Harmonized System
  Codes for products with realistic, noisy descriptions. Built from real e-commerce
  data, it comprises 632 products spanning 27 HS chapters, with expert-annotated codes.
---

# HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application

## Quick Facts
- arXiv ID: 2510.19631
- Source URL: https://arxiv.org/abs/2510.19631
- Reference count: 40
- Best agent achieves only 46.8% 10-digit accuracy, far below human experts at 95.0%

## Executive Summary
HSCodeComp is the first benchmark to evaluate deep search agents in hierarchical rule application, focusing on predicting 10-digit Harmonized System Codes for products with realistic, noisy descriptions. Built from real e-commerce data, it comprises 632 products spanning 27 HS chapters, with expert-annotated codes. The benchmark presents significant challenges due to complex tariff rules with vague boundaries and implicit logic. Extensive experiments with state-of-the-art LLMs and agents show a substantial performance gap: the best agent achieves only 46.8% 10-digit accuracy, far below human experts at 95.0%.

## Method Summary
HSCodeComp evaluates deep search agents' ability to predict 10-digit HS codes from noisy e-commerce product descriptions. The dataset contains 632 products with expert-annotated codes, including titles, attributes, images, categories, prices, and URLs. Agents use a framework like SmolAgents with a strong LLM backbone and tools including web search and tariff rule retrieval. The best setup uses SmolAgents + GPT-5 + product images + standard web search, outputting codes in boxed format. Agents are instructed not to use human-written decision rules, as these actually decrease performance.

## Key Results
- Best agent (SmolAgent + GPT-5) achieves 46.8% 10-digit accuracy vs. human experts at 95.0%
- Agents struggle with hierarchical reasoning and fail to effectively leverage human-written decision rules
- Test-time scaling methods like majority voting and self-reflection fail to improve performance
- Vision-enabled agents show consistent improvements by capturing material composition and completeness
- Tool-based retrieval significantly reduces hallucination and outdated knowledge errors compared to internal knowledge alone

## Why This Works (Mechanism)

### Mechanism 1
Tool-based information retrieval reduces hallucination and outdated knowledge errors compared to internal knowledge alone. External search tools provide grounded access to authoritative databases (CROSS, tariff rules), enabling verification against current official sources rather than relying on potentially stale or incorrect memorized information. Core assumption: Search tools return relevant, accurate information from authoritative databases. Evidence: agents significantly reduce hallucination, outdated and other errors through effective tool utilization, compared with LLMs; SmolAgent reduces outdated and hallucination failures by 56 corrected samples compared to GPT-5 alone. Break condition: When search queries are poorly formulated or databases return irrelevant/revoked precedents.

### Mechanism 2
Reducing pre-tool reasoning depth improves accuracy for hierarchical classification tasks. Minimal reasoning before tool-calling prevents early commitment to incorrect classification paths; errors in initial reasoning propagate through subsequent tool calls, reducing their effectiveness. Core assumption: Tools can provide actionable information without extensive query pre-processing. Evidence: Table 5 demonstrates that reducing reasoning depth improves accuracy, with No-Think nearly matching SmolAgent; prioritizing tool utilization over reasoning yields better results for such complex domain-specific tasks. Break condition: When tasks require sophisticated reasoning to formulate effective search queries.

### Mechanism 3
Multi-modal product information captures critical attributes missing from text descriptions. Product images reveal material composition, surface features, structural details, and completeness that text descriptions omit or misrepresent. Core assumption: Vision models accurately extract relevant product features from images. Evidence: Table 4 show that most baselines achieve consistent improvements when product images can be accessed; Table 10 Shows vision enabling correct material identification (natural silk vs. polyester) and completeness assessment (parts vs. whole articles). Break condition: When images are low-quality, show different products, or visual features are ambiguous.

## Foundational Learning

- **Hierarchical taxonomy with exception clauses**: HSCode prediction requires navigating a 10-digit hierarchy where rules contain cross-references and exclusions (e.g., "excluding articles of heading 8539"). Quick check: Given a product description, can you identify which exception clauses might apply before committing to a classification path?

- **Essential character determination (GRI 3)**: When products are classifiable under multiple headings, classification depends on identifying the "essential character" through material, function, or use. Quick check: For a medical alert bracelet with decorative elements, would you classify by form (jewelry) or function (medical device)?

- **Noisy attribute extraction vs. marketing language**: Real e-commerce titles contain subjective terms ("Aesthetic," "INS-style") that can mislead classification. Quick check: From "20PCS Pink Nitrile Gloves...Material: PVC," which signal should be trusted for material classification?

## Architecture Onboarding

- **Component map**: Input processor -> Tool layer (web search, CROSS database, tariff rules) -> Reasoning engine (hierarchical rule application) -> Output validator
- **Critical path**: 1) Extract structured features from noisy product data (ignore marketing language) 2) Query CROSS for precedent cases—if found, validate; if not, proceed to rule application 3) Apply hierarchical tariff rules level-by-level (2→4→6→8→10 digits) 4) Check exception clauses at each level before committing
- **Design tradeoffs**: No-Think > Medium-Think > Overthink for this task (Table 5); Web page visits disabled by default—search snippets are more focused (Section 5.2); Decision rules currently hurt performance for most agents—agents fail to apply them effectively (Table 3)
- **Failure signatures**: Premature decisions (search queries embed wrong assumptions), Information misprocessing (prioritizing title over explicit material attributes), Wrong rule application (applying form-based rules over function-based rules), Outdated codes (using memorized codes that have been restructured)
- **First 3 experiments**: 1) Backbone comparison: Run SmolAgent with GPT-5 vs. Gemini-2.5-Pro vs. Claude-4-Sonnet to identify which LLM handles tool-calling most effectively 2) Reasoning depth ablation: Compare "No-Think" vs. "Overthink" configurations on 50 products to quantify error propagation 3) Vision ablation: Test text-only vs. vision-enabled on products where material is visible but not explicitly stated in attributes

## Open Questions the Paper Calls Out

### Open Question 1
What novel test-time scaling strategies are required to improve performance on hierarchical rule applications where majority voting and self-reflection fail? Basis: Section 6.5 explicitly states that standard test-time scaling methods (majority voting and self-reflection) fail to improve or even decrease performance. Why unresolved: The paper demonstrates that simply increasing inference budget via standard sampling or critique mechanisms does not solve the reasoning gaps in this specific domain. What evidence would resolve it: Development of a new scaling method (e.g., verification-based search or specialized sampling) that yields statistically significant gains on the HSCodeComp benchmark.

### Open Question 2
Why do explicit human-written decision rules degrade performance in leading agent architectures, and how can agents be improved to leverage them? Basis: Table 3 and Section 5.2 show that providing agents with human-written decision rules (w/ DR) decreases accuracy for SmolAgent and WebSailor. The authors explicitly note agents "struggle at applying human-written decision rules." Why unresolved: It is counter-intuitive that providing high-level expert principles hurts performance; the specific mechanism (e.g., context window dilution vs. instruction following failure) is identified but not solved. What evidence would resolve it: An architectural or prompting intervention that utilizes decision rules to successfully increase 10-digit accuracy above the no-rule baseline.

### Open Question 3
How can agent systems be optimized to balance internal reasoning depth against tool-utilization frequency to avoid the "overthinking" penalty? Basis: Section 6.1 reveals that "No-Think" (direct tool calling) outperforms "Overthink" (deep pre-tool reasoning) by 5-7%. The authors infer that "prioritizing tool utilization over reasoning yields better results," but the optimal balance remains an open design problem. Why unresolved: The paper establishes a trade-off but does not propose a dynamic mechanism to decide when to stop reasoning and start searching. What evidence would resolve it: A dynamic agent policy that outperforms the static "No-Think" baseline by effectively determining when deep reasoning is actually necessary.

## Limitations

- Benchmark relies on Chinese eWTP tariff rules rather than globally harmonized schedules, limiting cross-jurisdictional applicability
- Exact failure modes remain unclear—agents make different errors than humans, particularly in handling exception clauses and hierarchical reasoning
- Choice to exclude human-written decision rules raises questions about whether agents are failing to apply existing expert knowledge or whether rules need reformulation for machine interpretability

## Confidence

- **High Confidence**: The benchmark construction methodology and dataset characteristics are well-documented and reproducible. The significant performance gap between humans and agents is clearly demonstrated across multiple metrics.
- **Medium Confidence**: The mechanisms explaining agent failures (overthinking, information misprocessing, premature decisions) are supported by ablation studies but could benefit from deeper error analysis. The effectiveness of tool-based retrieval over internal knowledge is demonstrated but not extensively compared against other retrieval approaches.
- **Low Confidence**: The generalizability of findings to other hierarchical classification domains and different tariff systems remains untested. The specific reasons why human-written rules decrease agent performance require further investigation.

## Next Checks

1. **Cross-jurisdictional validation**: Test the benchmark with HS codes from different national tariff schedules (US HTS, EU TARIC) to assess generalizability of the performance gap and failure patterns.

2. **Rule reformulation study**: Systematically reformulate human-written decision rules into agent-friendly formats (structured prompts, intermediate reasoning steps) to determine if the performance drop is due to rule format or agent reasoning limitations.

3. **Error type decomposition**: Conduct detailed error analysis categorizing agent failures by type (exception handling, hierarchical reasoning, attribute extraction) and compare these distributions against human expert errors to identify specific architectural requirements for closing the gap.