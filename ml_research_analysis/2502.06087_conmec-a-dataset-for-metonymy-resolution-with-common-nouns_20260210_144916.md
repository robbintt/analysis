---
ver: rpa2
title: 'ConMeC: A Dataset for Metonymy Resolution with Common Nouns'
arxiv_id: '2502.06087'
source_url: https://arxiv.org/abs/2502.06087
tags:
- word
- metonymy
- metonymic
- sentence
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses metonymy resolution with common nouns, a challenging
  aspect of natural language understanding. The authors introduce ConMeC, a new dataset
  of 6,000 sentences with human annotations indicating whether target common nouns
  are used metonymically.
---

# ConMeC: A Dataset for Metonymy Resolution with Common Nouns

## Quick Facts
- **arXiv ID:** 2502.06087
- **Source URL:** https://arxiv.org/abs/2502.06087
- **Reference count:** 40
- **Primary result:** LLM-based chain-of-thought prompting achieves 76.5 macro-F1 on metonymy classification, comparable to supervised BERT's 82.9

## Executive Summary
This work introduces ConMeC, a dataset of 6,000 Wikipedia sentences with human annotations for metonymy resolution with common nouns. The authors propose a two-step LLM pipeline that first categorizes target words into semantic types, then applies category-specific chain-of-thought reasoning to detect metonymy. The approach achieves performance comparable to a supervised BERT model on well-defined categories, though both methods struggle with semantically ambiguous cases like CAUSER and POSSESSED. The study highlights the challenges of metonymy resolution and provides a new benchmark for future research.

## Method Summary
The authors introduce a 2-step pipeline for metonymy resolution: first categorizing target words into semantic types (CONTAINER, PRODUCER, PRODUCT, LOCATION, or GENERAL), then applying category-specific chain-of-thought prompts to determine if metonymy is present. For LLM approaches, this uses self-consistency with majority voting. As a baseline, they fine-tune bert-base-uncased with 5-fold cross-validation using the [CLS] token representation. The ConMeC dataset contains 6,000 Wikipedia sentences across six metonymy categories, with 1,715 metonymic and 4,285 literal instances.

## Key Results
- BERT baseline achieves 82.9 macro-F1, outperforming GPT-4o (77.9) and Llama-70B-CoT-2S-SC (76.5)
- LLM 2-step pipeline performs competitively on well-defined categories (CONTAINER, PRODUCER, PRODUCT, LOCATION) with macro-F1 ~75-84%
- Both approaches show dramatic performance drops on CAUSER and POSSESSED categories (F1 ~40-50), indicating challenges with semantically ambiguous metonymy types

## Why This Works (Mechanism)
The 2-step approach works by first constraining the reasoning space through semantic categorization, then applying targeted reasoning patterns. By separating the categorization and reasoning tasks, the method reduces the cognitive load on the LLM and provides structured context for the chain-of-thought reasoning. The category-specific prompts guide the model to look for particular semantic relationships (containment, production, causation, etc.) rather than requiring general metonymy detection, which is more challenging for language models.

## Foundational Learning
- **Metonymy vs. Metaphor**: Metonymy involves substituting a word with something closely associated (e.g., "The White House announced" for the administration), while metaphor involves comparing unlike things. Understanding this distinction is crucial because metonymy resolution requires identifying semantic relationships rather than figurative comparisons.
- **Semantic Categories**: The six metonymy types (CONTAINER, PRODUCER, PRODUCT, LOCATION, CAUSER, POSSESSED) represent different semantic relationships. Each category requires different reasoning patterns to identify the metonymic usage.
- **Chain-of-Thought Prompting**: This technique breaks down complex reasoning tasks into intermediate steps, improving model performance on tasks requiring multi-step inference. It's essential for metonymy resolution because it allows the model to explicitly reason through the semantic relationships.
- **Self-Consistency**: Using multiple reasoning paths and selecting the majority answer helps reduce variance in LLM predictions. This is particularly important for metonymy detection where different reasoning paths might lead to different conclusions.
- **Zero-shot vs. Supervised Learning**: LLM approaches operate without task-specific training, while BERT is fine-tuned on labeled data. This distinction is important for understanding the different performance characteristics and generalization capabilities.

## Architecture Onboarding

**Component Map**: Input Sentence -> Category Categorizer -> Category-Specific CoT Prompt -> Metonymy Classifier -> Output

**Critical Path**: The two-step pipeline is the critical path: categorization must succeed before category-specific reasoning can be applied effectively. Errors in the first step propagate to the second.

**Design Tradeoffs**: The 2-step approach trades simplicity for accuracy by adding a categorization step. While this improves performance on well-defined categories, it introduces complexity and potential errors for ambiguous cases where semantic categorization is difficult.

**Failure Signatures**: The most significant failure occurs when CAUSER and POSSESSED categories are misclassified in step 1, leading to inappropriate reasoning patterns. Additionally, LLMs may over-predict metonymy when content is explicitly mentioned but the reasoning chain incorrectly identifies a metonymic relationship.

**First Experiments**:
1. Test category categorization accuracy on held-out CAUSER and POSSESSED instances to identify systematic errors
2. Compare 2-step pipeline performance against single-step CoT prompting to validate the benefit of categorization
3. Evaluate BERT performance using leave-one-category-out cross-validation to verify the reported dramatic performance drops

## Open Questions the Paper Calls Out
- **Downstream Application Impact**: The authors have not evaluated how their metonymy resolution system affects performance on downstream NLP benchmarks like Information Extraction or Named Entity Recognition. This remains an open question about the practical utility of their approach.
- **Generalization to Novel Categories**: The current methodology relies on category-specific prompts for six fixed types. The authors want to extend their work to study a diverse range of metonymic expressions beyond these predefined categories without manual prompt engineering.
- **Improving Ambiguous Category Handling**: The 2-step pipeline shows poor performance on CAUSER and POSSESSED categories due to their lack of well-defined semantic types. The authors seek methods to refine the categorization process for these semantically ambiguous cases.

## Limitations
- Performance degrades significantly on semantically ambiguous categories (CAUSER, POSSESSED) where semantic categorization is difficult
- Dataset validation relies on single human annotations without inter-annotator agreement metrics
- Experimental validation is limited to Wikipedia text, potentially limiting generalizability to other domains

## Confidence
- **High confidence**: Dataset creation methodology and basic statistics are well-documented and reproducible
- **Medium confidence**: LLM 2-step approach is conceptually sound but lacks specification of random seeds and exact majority vote counts
- **Low confidence**: Claims about superiority are undermined by ambiguity in handling CAUSER and POSSESSED categories

## Next Checks
1. Implement leave-one-category-out cross-validation for BERT to verify the dramatic performance drop (from ~75 F1 to ~30-40) when testing on held-out categories
2. Audit categorization accuracy of step-1 specifically on CAUSER and POSSESSED target words to identify systematic errors
3. Obtain or create a subset of 100-200 ConMeC instances with multiple human annotations to calculate inter-annotator agreement metrics