---
ver: rpa2
title: Context Parametrization with Compositional Adapters
arxiv_id: '2509.22158'
source_url: https://arxiv.org/abs/2509.22158
tags:
- context
- adapter
- adapters
- generator
- contexts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenges of efficiency and instability
  in adapting large language models (LLMs) to tasks requiring multiple demonstrations
  or retrieved passages. It introduces COMPAS, a teacher-student framework that translates
  contexts into compositional adapters, which can be algebraically combined.
---

# Context Parametrization with Compositional Adapters

## Quick Facts
- arXiv ID: 2509.22158
- Source URL: https://arxiv.org/abs/2509.22158
- Reference count: 40
- This work introduces COMPAS, a teacher-student framework that translates contexts into compositional adapters, enabling algebraic combination and improved efficiency in adapting large language models to tasks with multiple demonstrations.

## Executive Summary
This paper addresses the inefficiencies and instability of in-context learning when adapting large language models (LLMs) to tasks requiring multiple demonstrations or retrieved passages. The proposed method, COMPAS, maps each support context into a LoRA adapter and trains these adapters to approximate the behavior of context concatenation in input space. By enabling algebraic composition of adapters, COMPAS allows for more efficient and stable adaptation, especially as the number of contexts scales. Empirically, COMPAS outperforms both in-context learning and prior generator-based methods on multiple-choice and extractive question answering tasks, while also demonstrating robustness to noisy or contradictory contexts and significant speedups in inference.

## Method Summary
COMPAS is a teacher-student framework that translates contexts into compositional adapters. Each support context is mapped to a LoRA adapter, and these adapters are trained to approximate the behavior of concatenating contexts in input space. The core innovation is the ability to algebraically combine these adapters, allowing for efficient and stable adaptation of LLMs to tasks with multiple demonstrations. This approach decouples inference cost from context length, providing significant speedups and improved robustness compared to traditional in-context learning methods.

## Key Results
- COMPAS consistently outperforms in-context learning and prior generator-based methods on multiple-choice and extractive question answering tasks.
- The method shows improved robustness to noisy or contradictory contexts.
- COMPAS achieves significant speedups in inference by decoupling cost from context length.

## Why This Works (Mechanism)
The key mechanism behind COMPAS is the translation of contexts into compositional adapters, which can be algebraically combined. This allows the model to approximate the behavior of context concatenation in input space without the need to process all contexts at once. By training adapters to mimic the effects of context concatenation, COMPAS achieves more efficient and stable adaptation, especially as the number of contexts increases. The algebraic composition of adapters also enables the model to handle noisy or contradictory contexts more robustly.

## Foundational Learning
- **In-context learning**: Understanding how LLMs adapt to new tasks using demonstrations within the prompt. *Why needed*: Forms the baseline that COMPAS aims to improve upon. *Quick check*: Verify that the model can perform tasks using only prompt-based demonstrations.
- **LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning method that adds low-rank matrices to the model's weights. *Why needed*: Enables efficient adaptation of LLMs without full fine-tuning. *Quick check*: Confirm that LoRA adapters can be composed and combined effectively.
- **Adapter composition**: The ability to combine multiple adapters algebraically to achieve the desired behavior. *Why needed*: Core to COMPAS's efficiency and scalability. *Quick check*: Test the algebraic combination of adapters on simple tasks.

## Architecture Onboarding

### Component Map
Teacher Model -> Context-to-Adapter Mapping -> LoRA Adapters -> Algebraic Composition -> Adapted Model

### Critical Path
The critical path involves mapping contexts to LoRA adapters, training these adapters to approximate context concatenation, and then composing the adapters algebraically to adapt the model. This path ensures that the model can efficiently handle multiple contexts without the need for extensive prompt engineering or full fine-tuning.

### Design Tradeoffs
- **Efficiency vs. Fidelity**: Using LoRA adapters trades some fidelity for significant efficiency gains. This is acceptable given the performance improvements observed.
- **Scalability vs. Complexity**: The algebraic composition of adapters allows for scalability but introduces complexity in training and composition.
- **Robustness vs. Flexibility**: The method is robust to noisy contexts but may be less flexible in handling highly diverse or adversarial inputs.

### Failure Signatures
- **Poor Performance on Novel Tasks**: If the adapters are not well-trained, the model may struggle with tasks that require novel reasoning or context handling.
- **Inefficient Composition**: If the algebraic composition of adapters is not optimized, it may lead to increased inference times or reduced accuracy.
- **Overfitting to Training Contexts**: The model may overfit to the specific contexts used during training, leading to poor generalization to new contexts.

### 3 First Experiments
1. **Basic QA Task**: Test COMPAS on a simple question-answering task with a small number of contexts to verify basic functionality.
2. **Scalability Test**: Evaluate the method's performance as the number of contexts increases, comparing it to in-context learning and other baselines.
3. **Robustness Test**: Introduce noisy or contradictory contexts to assess the model's ability to handle such scenarios effectively.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation is limited to question-answering and multiple-choice tasks, with unclear generalization to other downstream applications.
- The assumption of additive context composition may not hold for complex reasoning tasks.
- The computational overhead of training and storing adapters for large context collections is not fully addressed.

## Confidence
- **Context-to-adapter mapping efficacy**: Medium confidence. Strong performance on evaluated tasks, but limited to specific QA and MC formats.
- **Robustness to noisy or contradictory contexts**: Low confidence. Evaluation is narrow and does not cover adversarial or semantically irrelevant contexts.
- **Inference speed and scalability**: Low confidence. Gains are reported but not rigorously benchmarked across varying context counts and model scales.

## Next Checks
1. Test COMPAS on open-ended generation tasks (e.g., summarization, dialogue) to assess generalization beyond QA.
2. Evaluate robustness to a broader spectrum of noisy contexts, including adversarial inputs and semantically irrelevant passages.
3. Benchmark inference efficiency and memory overhead with a larger number of contexts and bigger models (e.g., LLaMA-65B or GPT-4 class).