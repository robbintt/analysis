---
ver: rpa2
title: 'Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic
  Policies'
arxiv_id: '2508.18507'
source_url: https://arxiv.org/abs/2508.18507
tags:
- planning
- problems
- value
- pddl
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an LM-based planner, LMPLAN, that generates
  Python programs as value functions and sound policies for PDDL planning. The approach
  outperforms state-of-the-art planners and recent LM approaches on competition benchmarks
  within fixed time and memory constraints.
---

# Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies

## Quick Facts
- arXiv ID: 2508.18507
- Source URL: https://arxiv.org/abs/2508.18507
- Reference count: 26
- Key outcome: LM-based planner LMPLAN outperforms state-of-the-art planners on competition benchmarks while generating sound policies without external verifiers, with surprising performance gains using meaningless symbols over natural language

## Executive Summary
This paper introduces LMPLAN, an approach that uses language models to generate Python programs serving as generalized policies for PDDL planning domains. The method prompts LMs with domain descriptions, example problems, and code templates to synthesize either value functions or policies. These programs are then used to guide search algorithms or directly execute plans. The approach achieves state-of-the-art coverage on IPC benchmarks while maintaining soundness guarantees without external verification.

The work challenges conventional assumptions about LM reasoning by demonstrating that meaningless symbolic representations sometimes outperform natural language encodings for planning tasks. This finding suggests that LMs may rely more on syntactic patterns than semantic understanding when solving planning problems, raising important questions about the nature of LLM reasoning capabilities.

## Method Summary
LMPLAN synthesizes Python programs from LMs that implement either value functions or policies for PDDL domains. The approach constructs prompts containing domain descriptions, training problems, and example code, then uses the LM to generate candidate programs. These programs are validated on training problems and the best one selected. For execution, policies can be used directly with soundness wrappers, value functions guide GBFS search, or both approaches are combined in a dual-queue search architecture. The method maintains soundness guarantees by checking action applicability at each step.

## Key Results
- LMPLAN achieves 1302 solved instances across 10 IPC domains, outperforming state-of-the-art planner LAMA (1221 instances) within 30s/2GB constraints
- Policy-only execution (π_LM) solves 397 instances but is 100× slower than optimal on Transport domain, indicating soundness without optimality
- Surprisingly, symbolic representations (meaningless predicates) sometimes outperform natural language versions, with V_sym achieving 61 coverage versus V_sem's 59 on the same domains
- Dual-queue search (π_LM ⊕ V_LM) improves coverage over value-function-only search in 8 of 10 domains, demonstrating the complementary value of policy guidance

## Why This Works (Mechanism)

### Mechanism 1: Program Synthesis as Generalized Policies
The LM synthesizes Python programs that implement domain-general policies by learning syntactic patterns from example domains. Given task instructions, PDDL domain/problem files, and an example Python policy class, the LM outputs a domain-specific policy class. This in-context learning approach allows the LM to understand PDDL syntax and Python class structure without explicit solution hints. The core assumption is that the LM can generalize from the Gripper example to target domains based on structural patterns alone.

### Mechanism 2: Soundness via Action Applicability Filtering
Soundness is guaranteed by wrapping the LM-generated policy with a check that ensures predicted actions are applicable in the current state. If the policy suggests an invalid action, a random applicable action is selected instead. This approach maintains soundness relative to the PDDL domain model without requiring external verifiers. The assumption is that the PDDL domain model correctly specifies applicable actions for each state.

### Mechanism 3: Dual-Queue Search Combining Policy and Value Function
The approach extends GBFS with two queues: one for heuristic-guided states and one for policy-predicted states. This architecture alternates between queues, treating the policy as a preferred operator source while maintaining soundness and completeness. The policy provides useful guidance often enough to improve search coverage, with evidence showing 8 out of 10 domains benefit from adding policy-generated states to the search.

### Mechanism 4: Symbolic Processing Without Semantic Reliance (Tentative)
The paper observes that meaningless symbolic representations sometimes outperform natural language versions, suggesting LMs may rely more on structural reasoning than semantic understanding. However, this mechanism is not well-established - the performance difference is inconsistent across domains and policy types, with no clear predictive model for when symbolic encoding helps.

## Foundational Learning

- Concept: PDDL (Planning Domain Definition Language)
  - Why needed here: Essential for understanding domain/problem separation, predicates, actions, and preconditions/effects that structure the prompts and generated programs
  - Quick check question: Given a simple blocks-world domain, can you identify the predicates, action schemas, and write a valid PDDL problem file?

- Concept: Heuristic Search (GBFS, A*)
  - Why needed here: Critical for understanding how the LM-generated value functions guide search and why different search strategies offer tradeoffs between speed and completeness
  - Quick check question: Explain why GBFS is sound and complete for finite state spaces but not guaranteed optimal; what makes a heuristic "safe"?

- Concept: Soundness vs. Completeness
  - Why needed here: The paper explicitly trades completeness for speed while maintaining soundness; understanding this distinction is critical for interpreting results and choosing configurations
  - Quick check question: A planner returns only valid plans but may fail to find existing plans - is it sound, complete, both, or neither?

- Concept: Generalized Planning
  - Why needed here: The core problem is synthesizing programs that solve families of problems, not single instances; this framing distinguishes the work from classical planning
  - Quick check question: How does the training/test split in GP differ from standard ML, and why is test-time generalization harder?

## Architecture Onboarding

- Component map: Prompt Construction -> LM API -> Python Program (V_LM or π_LM) -> Validation Module -> Program Instantiation Module -> Planner Infrastructure

- Critical path:
  1. Choose domain and prepare PDDL files (domain + 2 training problems minimum)
  2. Construct prompt with 4 components (Section 3.1)
  3. Call LM API (10× for validation selection)
  4. Validate on training problems; select best program
  5. Deploy selected program on test problems via appropriate execution mode

- Design tradeoffs:
  - Policy-only (π_LM): Fast, sound, not complete - best for domains where LM learns correct strategy
  - Value-function-only (V_LM): Sound and complete, slower - better coverage on hard domains
  - Dual-queue (π_LM ⊕ V_LM): Sound, complete, improved coverage - adds complexity
  - Portfolio (π_LM ⊗ V_LM): Highest coverage - requires validation infrastructure to choose per-domain

- Failure signatures:
  - Policy returns poor-quality plans (e.g., Transport domain: 100× worse than LAMA) - indicates policy learns correct goal-reaching but not optimality
  - Policy fails completely on specific domains (Childsnack, Floortile) - suggests LM didn't learn domain structure from examples
  - Symbolic encoding drastically hurts policy performance (Blocksworld π_sem: 90 → π_sym: 1) - semantic names critical for some domains

- First 3 experiments:
  1. Replicate Gripper domain results: Run LMPlan with policy-only mode on Gripper validation problems; verify >90% coverage and soundness (all returned plans valid)
  2. Ablate prompt components: Remove example Gripper files from prompt; measure coverage drop to quantify in-context learning contribution
  3. Test symbolic encoding on a single domain: Run both V_sem and V_sym on Ferry domain; analyze whether improvement (59→61 coverage) is statistically significant or noise

## Open Questions the Paper Calls Out
The paper explicitly states that the mechanism behind symbolic encoding performance benefits requires further investigation, as the findings are inconsistent across domains and policy types. The authors acknowledge this as a tentative observation that challenges conventional assumptions about LLM reasoning.

## Limitations
- The mechanism behind symbolic encoding performance benefits remains unexplained, with inconsistent domain-level effects suggesting this is not a generalizable phenomenon
- Limited ablation studies prevent definitive attribution of performance gains to specific prompt components versus LM capabilities
- The reliance on a fixed two-training-problem minimum may create artificial generalization constraints not representative of real-world scenarios

## Confidence

- **High confidence**: Soundness guarantees via action applicability filtering, dual-queue architecture improving coverage, and GP formulation correctness
- **Medium confidence**: Program synthesis effectiveness across domains, though with clear failure cases in Childsnack and Floortile
- **Low confidence**: Symbolic encoding hypothesis and its inconsistent domain-level effects

## Next Checks

1. Conduct systematic ablation studies removing individual prompt components to quantify their contribution to performance
2. Test symbolic encoding hypothesis across a broader domain distribution with statistical significance analysis
3. Evaluate LMPlan's performance when increasing training problems from 2 to 5-10 to assess generalization bounds