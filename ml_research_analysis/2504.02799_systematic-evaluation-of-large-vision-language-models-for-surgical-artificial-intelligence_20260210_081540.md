---
ver: rpa2
title: Systematic Evaluation of Large Vision-Language Models for Surgical Artificial
  Intelligence
arxiv_id: '2504.02799'
source_url: https://arxiv.org/abs/2504.02799
tags:
- frame
- train
- surgical
- image
- gallbladder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates 11 large vision-language models
  (VLMs) across 17 surgical AI tasks spanning 13 datasets, including laparoscopic,
  robotic, and open procedures. Proprietary models like GPT-4o and Gemini demonstrated
  strong performance in surgical scene comprehension and progression understanding,
  with GPT-4o achieving up to 0.52 F1 score on error recognition and 0.33 F1 on phase
  recognition.
---

# Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence

## Quick Facts
- arXiv ID: 2504.02799
- Source URL: https://arxiv.org/abs/2504.02799
- Reference count: 40
- Benchmark of 11 VLMs on 17 surgical tasks across 13 datasets

## Executive Summary
This study systematically evaluates large vision-language models (VLMs) across 17 surgical AI tasks spanning 13 datasets including laparoscopic, robotic, and open procedures. Proprietary models like GPT-4o and Gemini demonstrated strong performance in surgical scene comprehension and progression understanding, with GPT-4o achieving up to 0.52 F1 score on error recognition and 0.33 F1 on phase recognition. In-context learning improved performance up to 3-fold for certain tasks. However, all models struggled with spatial and temporal reasoning tasks, with gesture recognition F1 scores remaining below 0.1. Surgery-specific models like SurgVLP excelled at tasks requiring domain expertise, particularly in critical view of safety assessment, achieving up to 0.46 F1 score.

## Method Summary
The study benchmarked 11 VLMs across 17 surgical visual understanding tasks using 13 datasets (11 public, 2 private). Tasks spanned scene comprehension, progression understanding, and safety/performance assessment. Evaluation used zero-shot and few-shot (1/3/5 examples) settings with F1 score as the primary metric. Proprietary models were accessed via API while open models used HuggingFace weights. Video inputs were sampled at 32-70 frames depending on context length. Few-shot examples were drawn from training splits per task. Prompts followed a standardized structure with JSON output parsing for autoregressive models and similarity scoring for contrastive models.

## Key Results
- GPT-4o achieved up to 0.52 F1 score on error recognition and 0.33 F1 on phase recognition
- In-context learning improved performance up to 3-fold for certain tasks
- All models struggled with spatial and temporal reasoning, with gesture recognition F1 scores below 0.1
- SurgVLP achieved up to 0.46 F1 score on critical view safety assessment
- 5-shot VLM results approached or exceeded task-specific models trained on thousands of labeled images

## Why This Works (Mechanism)
The study demonstrates that VLMs can effectively transfer general visual reasoning capabilities to surgical domains through prompt engineering and in-context learning. Proprietary models leverage extensive pretraining on diverse data to achieve strong performance on surgical scene comprehension and workflow understanding. Surgery-specific models like SurgVLP incorporate domain knowledge through specialized pretraining, excelling at safety-critical assessments. The few-shot learning capability allows VLMs to adapt to surgical tasks with minimal labeled examples, though this benefit is task-dependent and can degrade for spatial reasoning tasks.

## Foundational Learning
1. **Few-shot learning adaptation** - Why needed: Enables VLMs to perform surgical tasks with minimal labeled examples. Quick check: Measure F1 improvement from 0 to 5 examples per task.
2. **Prompt engineering for surgical contexts** - Why needed: Specialized prompts guide VLMs to interpret surgical imagery correctly. Quick check: Test multiple prompt formats on critical view safety assessment.
3. **Frame sampling strategies** - Why needed: Optimal temporal resolution affects video task performance. Quick check: Vary frame count (16, 32, 64) for error detection tasks.
4. **Zero-shot evaluation methodology** - Why needed: Establishes baseline performance without task-specific adaptation. Quick check: Compare zero-shot F1 scores across all models.
5. **Metric selection for surgical tasks** - Why needed: Appropriate metrics capture task-specific performance characteristics. Quick check: Verify F1, mAP, and mIoU calculations match task requirements.

## Architecture Onboarding

**Component Map**: API access -> Model inference -> Prompt processing -> Output parsing -> Metric calculation

**Critical Path**: Task definition -> Dataset preparation -> Model selection -> Prompt engineering -> Evaluation execution -> Result analysis

**Design Tradeoffs**: 
- Generalist models offer strong scene comprehension but struggle with surgical-specific tasks
- Surgery-specific models excel at domain tasks but lack general reasoning capabilities
- Few-shot learning provides adaptation but can degrade spatial reasoning performance

**Failure Signatures**:
- PaliGemma produces identical bounding boxes across inputs
- Qwen generates identical predictions regardless of input variation
- GPT-4o API exceptions on error detection tasks without clear causes
- Few-shot examples for detection causing performance to drop to near zero

**First Experiments**:
1. Test PaliGemma with varied prompt structures to identify cause of repeated bounding boxes
2. Compare frame sampling densities (16, 32, 64 frames) on video-based task performance
3. Evaluate SurgVLP weights once obtained to verify critical view safety assessment performance

## Open Questions the Paper Calls Out

**Open Question 1**: Does pre-training on large-scale unlabeled surgical data effectively combine the general reasoning of models like GPT-4o with the domain-specific performance of models like SurgVLP? The authors suggest this hybrid approach could capture domain-specific knowledge without requiring expert annotations, but this combination hasn't been tested. Evidence would come from benchmarks of VLMs pre-trained on both general internet-scale data and large-scale unlabeled surgical video logs.

**Open Question 2**: Can future video-language model architectures overcome current limitations in temporal reasoning to make surgical training applications viable? Current VLMs struggle significantly with video-based tasks requiring temporal context, such as gesture recognition (F1 < 0.1). Next-generation VLMs with advanced temporal encoders could enable surgical training and real-time guidance applications.

**Open Question 3**: How can prompt engineering or model alignment prevent the degradation of spatial reasoning performance observed during in-context learning for detection tasks? The authors observed that providing few-shot examples for tool detection caused performance to drop to nearly zero, suggesting fundamental misalignment in how autoregressive models interpret spatial coordinate examples.

## Limitations
- Access restrictions to SurgVLP weights and Med-Gemini create gaps in comparative analysis
- API-based evaluation for proprietary models introduces potential variability in output formatting
- Frame sampling strategies differ across tasks but impact on performance remains unexplored
- Prompt engineering sensitivity wasn't systematically addressed across all models

## Confidence
- **High confidence**: VLMs' strong performance on surgical scene comprehension and progression understanding tasks
- **Medium confidence**: Few-shot learning improvements and comparative performance against task-specific models
- **Medium confidence**: Surgery-specific model superiority due to limited model access

## Next Checks
1. Request and test SurgVLP weights to verify reported performance on critical view safety assessment
2. Conduct controlled experiments varying frame sampling density for video tasks to establish optimal temporal resolution
3. Perform systematic prompt engineering ablation to quantify sensitivity to prompt structure across all VLMs