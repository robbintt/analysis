---
ver: rpa2
title: 'BengaliSent140: A Large-Scale Bengali Binary Sentiment Dataset for Hate and
  Non-Hate Speech Classification'
arxiv_id: '2601.20129'
source_url: https://arxiv.org/abs/2601.20129
tags:
- bengali
- hate
- sentiment
- dataset
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BengaliSent140, a large-scale Bengali binary
  sentiment dataset created by merging seven heterogeneous Bengali text corpora into
  a unified benchmark. The dataset is designed to address the scarcity of large, diverse,
  and consistently annotated resources for Bengali hate and non-hate speech classification.
---

# BengaliSent140: A Large-Scale Bengali Binary Sentiment Dataset for Hate and Non-Hate Speech Classification

## Quick Facts
- arXiv ID: 2601.20129
- Source URL: https://arxiv.org/abs/2601.20129
- Authors: Akif Islam; Sujan Kumar Roy; Md. Ekramul Hamid
- Reference count: 18
- Primary result: Created 139,792-sample Bengali hate speech dataset with 0.91 BERT baseline accuracy

## Executive Summary
This paper introduces BengaliSent140, a large-scale Bengali binary sentiment dataset created by merging seven heterogeneous Bengali text corpora into a unified benchmark. The dataset is designed to address the scarcity of large, diverse, and consistently annotated resources for Bengali hate and non-hate speech classification. To ensure uniformity, diverse annotation schemes from the source datasets are harmonized into a binary formulation with two classes: Hate (1) and Not Hate (0). The resulting dataset contains 139,792 unique text samples, with 68,548 labeled as hate and 71,244 as not-hate, yielding a balanced class distribution. Multiple text representations—including raw, normalized, and lemmatized versions—are provided to facilitate systematic preprocessing analysis. Baseline experiments using classical machine learning, deep learning, and transfer learning models demonstrate the dataset's effectiveness, with the best-performing model (BERT) achieving a test accuracy of 0.91. BengaliSent140 offers broad linguistic and contextual coverage, making it a valuable resource for training and benchmarking deep learning models for Bengali sentiment and hate speech detection. The dataset is publicly available for research use.

## Method Summary
BengaliSent140 was constructed by merging seven heterogeneous Bengali text corpora into a unified dataset. The primary innovation lies in harmonizing diverse annotation schemes from the source datasets into a consistent binary formulation (Hate/Not Hate). The dataset provides three text representations: raw, normalized, and lemmatized versions, enabling systematic analysis of preprocessing effects. The resulting corpus contains 139,792 unique samples with balanced class distribution (68,548 hate vs 71,244 not-hate). Baseline experiments were conducted using classical machine learning, deep learning, and transfer learning approaches, with BERT achieving the highest performance at 0.91 test accuracy.

## Key Results
- Dataset size: 139,792 unique Bengali text samples
- Class balance: 68,548 hate samples (49.0%) and 71,244 not-hate samples (51.0%)
- Multiple representations: Raw, normalized, and lemmatized text versions provided
- Best baseline: BERT model achieving 0.91 test accuracy
- Public availability: Dataset released for research use

## Why This Works (Mechanism)
BengaliSent140 addresses the critical shortage of large-scale, consistently annotated Bengali text resources for hate speech detection. By harmonizing diverse annotation schemes into a unified binary classification framework, the dataset enables systematic model training and evaluation. The inclusion of multiple text representations (raw, normalized, lemmatized) allows researchers to analyze the impact of preprocessing on model performance. The balanced class distribution and substantial dataset size provide sufficient training data for deep learning models to learn robust hate speech detection patterns across diverse linguistic contexts.

## Foundational Learning
- Bengali language processing fundamentals - needed for understanding linguistic challenges in preprocessing and model training; quick check: familiarity with Bengali script and morphological structure
- Text normalization techniques - needed to understand the impact of preprocessing on model performance; quick check: ability to explain common normalization operations
- Hate speech detection principles - needed to contextualize the dataset's application; quick check: knowledge of key challenges in hate speech classification
- Binary classification metrics - needed to interpret model performance; quick check: understanding of precision, recall, and F1-score limitations
- Transfer learning in NLP - needed to understand BERT's effectiveness; quick check: familiarity with pre-trained language models
- Dataset harmonization methodology - needed to assess data quality and annotation consistency; quick check: understanding of label merging strategies

## Architecture Onboarding

**Component map:** Raw text -> Preprocessing (normalize/lemmatize) -> Feature extraction (classical ML, deep learning, transfer learning) -> Classification -> Evaluation

**Critical path:** Data preparation → Model training → Evaluation → Error analysis

**Design tradeoffs:** The dataset provides multiple preprocessing levels but requires users to manage the trade-off between raw linguistic information and standardized representations. Classical ML models offer interpretability but lower performance compared to deep learning approaches.

**Failure signatures:** High accuracy with poor class-specific metrics may indicate dataset artifacts rather than genuine model capability. Unbalanced error patterns across hate speech subtypes suggest annotation inconsistencies or model bias.

**First experiments:**
1. Compare model performance across raw, normalized, and lemmatized text versions to identify optimal preprocessing strategy
2. Evaluate class-specific metrics (precision, recall, F1) for hate and not-hate categories to assess model fairness
3. Conduct ablation studies removing different source datasets to identify their contribution to overall performance

## Open Questions the Paper Calls Out
None

## Limitations
- Limited transparency regarding specific source datasets and their original annotation schemes
- Harmonization methodology not detailed, raising questions about annotation consistency
- Reported BERT accuracy of 0.91 appears unusually high for hate speech detection without supporting class-specific metrics
- No inter-annotator agreement scores or annotation guidelines provided

## Confidence
- Dataset creation and characteristics: Medium confidence - Dataset size and structure are clearly specified, but source details and harmonization methodology lack transparency
- Baseline experimental results: Low confidence - The reported accuracy seems unusually high without supporting metrics or detailed methodology
- Dataset utility for research: Medium confidence - The dataset appears valuable for Bengali NLP, but quality concerns limit immediate applicability

## Next Checks
1. Conduct a thorough error analysis on the BERT baseline predictions to identify whether high accuracy stems from genuine pattern learning or dataset artifacts, focusing on false positives and false negatives across different hate speech subtypes

2. Replicate the baseline experiments using the released dataset with standardized train/validation/test splits and compare results across multiple random seeds to assess result stability

3. Perform linguistic analysis of a stratified sample of hate-labeled instances to verify annotation quality, checking for consistency in what constitutes hate speech across different cultural and contextual variations in Bengali language use