---
ver: rpa2
title: Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of
  Hierarchical Reasoning Models
arxiv_id: '2601.10679'
source_url: https://arxiv.org/abs/2601.10679
tags:
- reasoning
- fixed
- arxiv
- latent
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the reasoning patterns of Hierarchical Reasoning
  Models (HRM) to understand their strengths and failure modes. The authors identify
  three surprising facts: (1) HRM can fail on extremely simple puzzles due to violation
  of the fixed point property - it sometimes continues updating its latent state even
  after finding the correct answer; (2) HRM exhibits "grokking" dynamics where the
  answer doesn''t improve uniformly but instead has a critical reasoning step that
  suddenly makes the answer correct; (3) Multiple fixed points exist in the latent
  space, and HRM can get trapped at incorrect ones.'
---

# Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models

## Quick Facts
- arXiv ID: 2601.10679
- Source URL: https://arxiv.org/abs/2601.10679
- Reference count: 17
- Primary result: HRM accuracy on Sudoku-Extreme improves from 54.5% to 96.9% using data augmentation, input perturbation, and model bootstrapping

## Executive Summary
This paper provides a mechanistic analysis of Hierarchical Reasoning Models (HRM), revealing that these models appear to "guess" rather than "reason" in the traditional sense. Through detailed analysis of HRM's behavior on Sudoku puzzles, the authors identify three key failure modes: instability on simple puzzles due to fixed point violations, plateau-then-grokking dynamics, and convergence to spurious fixed points. Based on these insights, they propose three scaling strategies—data augmentation, input perturbation, and model bootstrapping—that together form Augmented HRM, achieving 96.9% accuracy on extremely difficult Sudoku puzzles compared to 54.5% for baseline HRM.

## Method Summary
The authors analyze HRM's reasoning process by examining its behavior on Sudoku-Extreme puzzles. They train HRM with one-step gradients and ACT halting, then identify failure modes through latent space trajectory analysis. To address these issues, they implement three augmentation strategies: (1) data mixing with partially-revealed puzzles to restore fixed point stability, (2) input perturbation via token relabeling to provide diverse guess attempts, and (3) model bootstrapping using checkpoint ensembles for majority voting. The augmented approach is evaluated against baseline HRM and variants like Tiny Recursive Model.

## Key Results
- HRM can fail on extremely simple puzzles due to fixed point instability, continuing to update even after finding correct answers
- HRM exhibits "grokking" dynamics with extended plateaus followed by sudden accuracy jumps, contradicting gradual refinement
- Multiple fixed points exist in latent space, with HRM sometimes getting trapped in spurious attractors
- Augmented HRM achieves 96.9% accuracy on Sudoku-Extreme versus 54.5% for baseline HRM
- Individual augmentation strategies provide complementary gains: relabeling (18.7%), checkpoints (10.2%), and data mixing (5.4%)

## Why This Works (Mechanism)

### Mechanism 1: Fixed Point Restoration via Difficulty-Calibrated Data
- **Claim:** Training exclusively on hard puzzles prevents HRM from learning stability around solutions; mixing in simpler puzzles restores the fixed point property.
- **Mechanism:** One-step gradient training isolates each segment, forcing independent problem-solving without curriculum. Adding partially-revealed puzzles provides explicit supervision for maintaining solutions once found.
- **Core assumption:** Fixed point acquisition is a learned skill requiring diverse difficulty exposure, not an emergent property.
- **Evidence anchors:**
  - [Section 3.2]: "One-step gradient, combined with Sudoku-Extreme, means only the noisiest data (extremely hard puzzles) and the clean data (solutions) are seen."
  - [Section 3.3]: Data mixing eliminates drift after finding correct answers and improves accuracy from 54.5% to 59.9%.
- **Break condition:** If your task lacks natural difficulty gradations (e.g., binary classification), this mechanism may not transfer directly.

### Mechanism 2: Escaping Spurious Fixed Points via Perturbation Diversity
- **Claim:** Spurious fixed points act as local minima in the latent reasoning landscape; perturbation in input or parameter spaces increases the probability of encountering the true fixed point.
- **Mechanism:** Multiple semantically-equivalent inputs (via token relabeling, row/column swaps) or multiple model checkpoints provide diverse starting trajectories. Majority voting filters out trajectories trapped in wrong basins.
- **Core assumption:** The true fixed point has a larger basin of attraction across the ensemble; wrong fixed points are more idiosyncratic.
- **Evidence anchors:**
  - [Section 4.5.1]: Relabeling transformation alone provides 18.7% accuracy improvement.
  - [Section 5.1]: Visualized "rival attractors" show clear separation basins between true and spurious fixed points.
  - [Section 4.5.2]: Adjacent checkpoints improve accuracy by 10.2% despite being strongly correlated.
- **Break condition:** If input transformations are not semantic-preserving, or if model checkpoints are too correlated, diversity benefits diminish.

### Mechanism 3: Recursion as Guess-Scaling, Not Incremental Refinement
- **Claim:** HRM's outer loop does not perform gradual refinement; instead, it scales the number of "guess" attempts at finding a valid latent state.
- **Mechanism:** Loss curves show extended plateaus followed by sudden drops ("grokking"), contradicting monotonic improvement. Most segments hover near spurious fixed points; success depends on randomly escaping toward the true basin.
- **Core assumption:** The segment update function F does not implement a reliable optimization trajectory; it more closely resembles stochastic search.
- **Evidence anchors:**
  - [Section 4.2]: "The error reduction process is by no means 'gradual'... the loss enters a lengthy plateau before suddenly dropping to zero."
  - [Section 4.6]: "HRM is perfectly capable of getting the solution in very few steps, given the correct latent state. However, it does not really strategize its search."
- **Break condition:** If future architectures add explicit search or planning modules, this "guessing" characterization may no longer apply.

## Foundational Learning

- **Fixed Point Theory in Iterative Systems**
  - Why needed: Understanding z* = F(z*, x̃; θ) is essential for grasping why one-step gradients are theoretically justified—and why their failure matters.
  - Quick check question: Can you explain why a fixed point that is not stable (Jacobian eigenvalues > 1) would break the gradient approximation in Equation (6)?

- **Attractor Dynamics and Basin of Attraction**
  - Why needed: The "rival attractors" analysis assumes intuition about basins, separation boundaries, and local minima in high-dimensional spaces.
  - Quick check question: If you initialize z₀ at two nearby points and they converge to different fixed points, what does that imply about the boundary structure?

- **Deep Supervision with Detached Gradients**
  - Why needed: The one-step gradient technique detaches zᵢ at each segment, breaking credit assignment across segments. Understanding this clarifies why curriculum must be explicit.
  - Quick check question: Why would full BPTT be prohibitively expensive for T segments with deep supervision, and what does detaching sacrifice?

## Architecture Onboarding

- **Component map:** Input Network f_I -> Segment F (N=2 high-level, T=6 low-level steps) -> Output Network f_O, with ACT mechanism deciding halt vs. continue
- **Critical path:**
  1. z₀ initialized deterministically
  2. Each segment runs N×T internal updates (f_L at every step, f_H every T steps)
  3. After segment completion, ACT decides whether to halt
  4. If continue, z_{i+1} ← z_N^H; repeat
  5. At halt, f_O produces final output
- **Design tradeoffs:**
  - One-step gradient vs. full BPTT: Saves O(T) compute but breaks temporal credit assignment; requires fixed point property to justify
  - Hierarchy (f_L/f_H) vs. flat recurrence: Ablation shows hierarchy not critical for performance; simplifies to single recurrence for analysis
  - ACT vs. fixed depth: Adaptive halting saves compute on easy problems but adds halting-error failure mode
- **Failure signatures:**
  - Instability on easy inputs: Model updates correct answers to wrong ones; indicates fixed point property not acquired
  - Plateau-then-grokking: Loss flat for many segments then sudden drop; suggests guessing dynamics, not refinement
  - Convergence to wrong fixed points: High conflict count E(ŷ) but model halts; trapped in spurious attractor
- **First 3 experiments:**
  1. Probe fixed point property: Run trained HRM on puzzles with 1-5 masked cells; track whether correct outputs remain stable across segments. If not, implement data mixing augmentation.
  2. Visualize latent trajectories: PCA-project z₀ → z_T for success/failure samples; classify into the four modes (trivial/non-trivial success/failure). Quantify spurious fixed point frequency.
  3. Ablate perturbation strategies: Measure accuracy gains from (a) relabeling alone, (b) checkpoint ensemble alone, (c) combined. Verify that gains are complementary, not redundant.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Hierarchical Reasoning Model (HRM) explicitly implement an optimization algorithm to minimize the heuristic error metric (count of conflicts) defined in the paper?
- Basis in paper: [explicit] Section 5.2 states, "It would be premature to say that HRM actually implements an optimization algorithm on this quantity [E]. This, however, opens up a new perspective to understand HRMs, worth investigating in future work."
- Why unresolved: The authors observe that spurious fixed points appear as local minima of the heuristic metric $E$, but they have not established a causal link or proven that the model's segment dynamics actively minimize this specific function.
- What evidence would resolve it: A theoretical or empirical demonstration that the gradient of the model's latent state update correlates strongly with the gradient of the metric $E$ during the reasoning process.

### Open Question 2
- Question: Do the identified failure modes (fixed point violation, grokking dynamics, and spurious attractors) generalize to other recursive reasoning models or large language models?
- Basis in paper: [explicit] Section 6 states, "While our experiments focus on HRM, the lens naturally extends to any recursive [model]. We conjecture that the qualitative taxonomy we provide will serve as a common vocabulary for the emerging class of recursive reasoners."
- Why unresolved: The mechanistic analysis and trajectory mapping were conducted exclusively on the HRM architecture using the Sudoku-Extreme dataset; the universality of these dynamics remains a hypothesis.
- What evidence would resolve it: Replicating the latent space trajectory analysis and fixed point inspection on other recursive architectures or Chain-of-Thought models to see if similar "guessing" behavior and attractors exist.

### Open Question 3
- Question: Are the spurious fixed points and "getting lost" behavior specific to the combinatorial constraints of Sudoku, or do they manifest in reasoning tasks with different structural properties?
- Basis in paper: [inferred] The paper defines the error metric $E$ and the "spurious fixed points" based on the specific constraint rules of Sudoku (row, column, and box conflicts). The analysis relies entirely on Sudoku-Extreme, leaving the behavior on other reasoning domains unexplored.
- Why unresolved: The authors do not provide mechanistic evidence of these failure modes in other domains (e.g., mathematical reasoning or pathfinding), where "conflicts" might not be as discretely defined.
- What evidence would resolve it: Applying the Augmented HRM and the mechanistic analysis to non-Sudoku reasoning benchmarks (e.g., maze-solving or math problems) to determine if spurious attractors persist.

## Limitations

- Analysis is conducted primarily on Sudoku-Extreme, a domain with discrete, well-defined rules and clear success metrics, raising questions about generalizability to continuous or open-ended reasoning tasks
- The perturbation-based ensemble methods rely on semantic-preserving transformations, but their effectiveness depends on these transformations truly sampling independent basins of attraction rather than correlated trajectories
- The "guessing" characterization may not apply to reasoning tasks where solution spaces are not discrete attractors

## Confidence

**High Confidence**: The fixed point instability on simple puzzles is empirically verified and directly observable through the one-step gradient training mechanism. The plateau-then-grokking behavior is well-documented in the loss curves and represents a fundamental departure from gradual refinement models.

**Medium Confidence**: The "guessing" characterization of HRM's recursive process is supported by multiple evidence lines but remains interpretive. While the dynamics suggest search-like behavior rather than optimization, the exact nature of what occurs in the latent space during each segment update could vary with architectural changes.

**Low Confidence**: The scalability of these findings to other reasoning domains is speculative. The specific mechanisms identified (fixed point restoration via difficulty mixing, perturbation diversity for escaping spurious attractors) may not transfer directly to tasks without natural difficulty gradations or semantic-preserving transformations.

## Next Checks

1. **Cross-Domain Fixed Point Analysis**: Apply the same stability testing (correct answers remaining stable across segments) to HRM on non-Sudoku reasoning tasks like logical deduction puzzles or mathematical proof generation. This would validate whether fixed point instability is a universal HRM failure mode or Sudoku-specific.

2. **Basin Volume Quantification**: Instead of just visualizing rival attractors, measure the relative volumes of basins for true versus spurious fixed points using Monte Carlo sampling from random initializations. This would provide quantitative support for the perturbation diversity mechanism.

3. **Gradual Refinement Ablation**: Implement a modified HRM with explicit search mechanisms (e.g., beam search over latent states, or attention-based refinement between segments) and compare its dynamics to baseline HRM. This would test whether the "guessing" characterization holds even when refinement capabilities are added.