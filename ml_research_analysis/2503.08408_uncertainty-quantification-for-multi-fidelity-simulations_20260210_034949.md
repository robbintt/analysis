---
ver: rpa2
title: Uncertainty Quantification for Multi-fidelity Simulations
arxiv_id: '2503.08408'
source_url: https://arxiv.org/abs/2503.08408
tags:
- neural
- data
- network
- deep
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The research addresses the challenge of uncertainty quantification
  in computational fluid dynamics by integrating high-fidelity and low-fidelity simulations.
  It uses Nektar++ for high-fidelity data and XFOIL for low-fidelity data to model
  aerodynamic properties like lift and drag coefficients.
---

# Uncertainty Quantification for Multi-fidelity Simulations

## Quick Facts
- arXiv ID: 2503.08408
- Source URL: https://arxiv.org/abs/2503.08408
- Reference count: 0
- One-line primary result: A multi-fidelity deep neural network (MF-DNN) approach significantly reduces computational cost while maintaining accuracy for uncertainty quantification in high-dimensional CFD problems.

## Executive Summary
This research addresses the computational challenge of uncertainty quantification (UQ) in computational fluid dynamics by integrating high-fidelity (HF) and low-fidelity (LF) simulations. The study develops a Multi-Fidelity Deep Neural Network (MF-DNN) that combines data from Nektar++ (HF) and XFOIL (LF) to model aerodynamic properties like lift and drag coefficients. By leveraging the broad coverage of LF data to guide the surrogate model while using sparse HF data to correct systematic errors, the MF-DNN reduces reliance on expensive high-fidelity simulations. The approach demonstrates superior performance compared to traditional methods like Co-kriging, particularly in high-dimensional settings (up to 100D), maintaining precision while significantly lowering computational costs.

## Method Summary
The methodology employs a two-stage sequential training approach for the MF-DNN. First, a low-fidelity deep neural network (LF-DNN) is trained on abundant LF data to learn the baseline system response. Second, a correction deep neural network is trained on sparse HF data, taking both input variables and LF predictions as inputs to learn the mapping from LF to HF behavior. The model uses ReLU activation and is optimized using ADAM followed by L-BFGS. The approach is validated across multiple benchmark functions (1D, 32D, 100D) with both uniform and Gaussian input uncertainties, demonstrating the model's capability in predicting probability density distributions and statistical moments. Testing compares performance against traditional methods like Co-kriging and Radial Basis Function models in accuracy and efficiency.

## Key Results
- MF-DNN outperforms Co-kriging and RBF models in accuracy and efficiency for high-dimensional problems
- The method achieves accurate UQ predictions while significantly reducing computational costs
- Testing demonstrates capability across 1D, 32D, and 100D functions with uniform and Gaussian input uncertainties
- The approach shows potential applications in aerospace engineering, including electric aircraft design for sustainability goals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining HF and LF data sources reduces the number of expensive simulations needed for surrogate model accuracy.
- **Mechanism:** The multi-fidelity paradigm leverages cheap LF data's broad coverage to guide the surrogate model, requiring only sparse HF data points to correct systematic errors, creating a model that approximates HF behavior with lower computational cost.
- **Core assumption:** LF data exhibits a sufficiently strong correlation (linear or non-linear) with HF data, allowing a correction term to bridge the fidelity gap.
- **Evidence anchors:** Abstract demonstrates superior accuracy using higher polynomial distribution; section 1.1, 1.3 explicitly defines HF/LF data and frames the problem as minimizing reliance on HF data for UQ; related work on multi-fidelity methods confirms leveraging correlated data of varying fidelity.
- **Break condition:** Mechanism fails if LF model is fundamentally uncorrelated with HF physics, or if available HF data is too sparse or located in uninformative regions of the design space.

### Mechanism 2
- **Claim:** MF-DNN with a single correction subnetwork can effectively approximate both linear and non-linear correlations between LF and HF data, outperforming Co-kriging in high-dimensional settings.
- **Mechanism:** MF-DNN trained in two sequential phases: first LF-DNN on abundant LF data to learn baseline response, then correction DNN on sparse HF data taking both input variables and frozen LF-DNN predictions to learn the mapping y_HF â‰ˆ F(y_LF, X). ReLU activation allows single subnetwork to adaptively capture complex, non-linear correction functions.
- **Core assumption:** Universal function approximation capability of neural networks is sufficient to learn complex, high-dimensional mapping between LF predictions and HF truths, and sequential training strategy prevents catastrophic interference.
- **Evidence anchors:** Section 2.6 integrates tasks within single sub-network allowing adaptive identification of linear/non-linear correlations; section 3.6.2, 3.6.4 benchmark results show MF-DNN with ReLU achieving low MSE on non-linear and 100-dimensional problems where Co-kriging and RBF fail; related works discuss hierarchical learning scenarios aligning with sequential training concept.
- **Break condition:** Mechanism degrades if LF model is very poor predictor, if HF dataset is too small to constrain correction network (overfitting), or if problem dimensionality overwhelms network's capacity despite optimization.

### Mechanism 3
- **Claim:** Co-kriging with adaptive sampling provides mathematically grounded baseline for UQ in low dimensions but faces fundamental memory and computational limits in high-dimensional (>32D) spaces.
- **Mechanism:** Co-kriging extends Kriging by modeling HF data as scaled LF Gaussian process plus difference process. Adaptive sampling uses model's prediction uncertainty to select new HF sample locations, iteratively reducing global uncertainty. Effective for small, smooth problems.
- **Core assumption:** Problem is low-dimensional, correlation structures are well-represented by Gaussian processes, and covariance matrices remain invertible and tractable.
- **Evidence anchors:** Sections 1.2.3, 2.4-2.5 describe Co-kriging formulation and use of adaptive sampling for UQ; section 3.6.3, abstract explicitly states Co-Kriging exhibited limitations addressing 32-Dimension problems due to memory capacity limitations for storage and manipulation; related works discuss limitations of traditional Gaussian process-based methods for expensive high-dimensional problems.
- **Break condition:** Mechanism fails when dimensionality causes covariance matrix size to exceed available memory, or when required number of samples for statistical significance in d-dimensional space becomes prohibitive.

## Foundational Learning

- **Concept: Spectral/hp Element Methods**
  - **Why needed here:** To understand how high-fidelity data (Nektar++ simulations) is generated with high accuracy and its computational cost.
  - **Quick check question:** Can you explain why increasing the polynomial order (p-refinement) in a spectral/hp element method improves accuracy but also increases computational cost?

- **Concept: Gaussian Process (Kriging) Basics**
  - **Why needed here:** To grasp foundation of Co-kriging, which models data as realizations of stochastic process with covariance function, enabling both prediction and uncertainty estimation.
  - **Quick check question:** In a Kriging model, what does the length-scale hyperparameter in the covariance function typically control?

- **Concept: Supervised Neural Network Training**
  - **Why needed here:** To comprehend MF-DNN training process including loss functions (MSE), optimizers (ADAM, L-BFGS), regularization (L2), and role of activation functions (ReLU).
  - **Quick check question:** What is the purpose of the loss function in training a neural network, and why is Mean Squared Error (MSE) suitable for regression tasks?

## Architecture Onboarding

- **Component map:** Data Generators (Nektar++ for HF, XFOIL for LF) -> Initial Fusion & UQ (Co-kriging with adaptive sampling) -> Advanced Surrogate (MF-DNN with LF-DNN and Correction DNN) -> UQ Engine (Monte Carlo sampling using trained MF-DNN for propagation)
- **Critical path:** For new engineering application: (1) Generate initial sparse HF data and dense LF data across input domain. (2) Use Co-kriging to perform initial data fusion and identify regions of high uncertainty. (3) Design and train MF-DNN: first LF-DNN, then correction DNN. (4) Validate MF-DNN against held-out HF test set. (5) Employ MF-DNN for computationally cheap UQ tasks (e.g., aleatory uncertainty propagation).
- **Design tradeoffs:** Key tradeoff between initial setup cost and final model capability - Co-kriging is simpler to implement but limited in dimension and scalability, while MF-DNN requires more complex engineering (architecture search, training) but offers superior performance in high dimensions and for complex correlations. Another tradeoff is interpretability vs. performance - Gaussian process models provide built-in uncertainty estimates, while MF-DNN's uncertainty must be assessed via external methods like Monte Carlo or ensemble techniques.
- **Failure signatures:**
  - Co-kriging Failure: `MemoryError` during covariance matrix inversion for problems with >~30-40 dimensions or too many LF points.
  - MF-DNN Overfitting: Very low training loss but poor predictive accuracy on validation data, often visible as large gap between training and validation MSE curves.
  - Poor Correlation: If MF-DNN correction network fails to reduce error, it may indicate LF model captures little relevant physics (check LF vs. HF scatter plots).
- **First 3 experiments:**
  1. **Validate Fidelity Gap:** Generate small HF/LF dataset on known analytic function (e.g., 1D Forrester function). Train both Co-kriging and MF-DNN. Compare their predictions and MSE to verify implementation of multi-fidelity concept.
  2. **Assess Scalability:** Use scalable benchmark (e.g., 32D or 100D functions from paper). Measure and compare training time, memory usage, and prediction error for Co-kriging vs. MF-DNN. Observe where Co-kriging fails.
  3. **Perform UQ with MF-DNN:** Train MF-DNN on physical CFD dataset (like NACA0012 case). Define input uncertainties (e.g., uniform angle of attack variation) and use trained MF-DNN within Monte Carlo loop to propagate uncertainty and obtain probability density function of lift coefficient.

## Open Questions the Paper Calls Out
None

## Limitations
- The method's effectiveness depends heavily on the quality of the low-fidelity model and the representativeness of the high-fidelity dataset
- Sequential training approach lacks extensive ablation studies on architectural choices (number of layers, neuron counts) and rigorous comparison against other neural network architectures for the same task
- The approach requires careful tuning of hyperparameters through Bayesian Optimization, though specific implementation details are not provided

## Confidence
- **Multi-fidelity DNN can effectively replace Co-kriging for UQ in high dimensions:** Medium. Supported by benchmark results, but dependent on problem specifics.
- **Sequential training (LF-DNN then Correction DNN) is an effective strategy:** Medium. Novel approach, but lacks extensive comparative analysis with alternative training schemes.
- **The method significantly reduces computational cost for UQ:** High. This is a direct consequence of replacing expensive HF simulations with cheaper surrogate, which is fundamental premise of multi-fidelity approach.

## Next Checks
1. **Reproduce the 1D Forrester function benchmark:** Generate LF and HF datasets as specified, train both Co-kriging and MF-DNN, and compare their MSE and UQ predictions (e.g., PDF of output) to validate core multi-fidelity concept and implementation.
2. **Stress-test the MF-DNN scalability:** Implement 32D or 100D benchmark functions. Measure and compare memory usage and training time of Co-kriging versus MF-DNN to directly observe scalability advantage claimed by paper.
3. **Validate on different physical system:** Apply MF-DNN methodology to different CFD problem or non-CFD multi-fidelity scenario (e.g., structural mechanics with analytical and numerical solvers) to assess generalizability beyond NACA0012 airfoil case.