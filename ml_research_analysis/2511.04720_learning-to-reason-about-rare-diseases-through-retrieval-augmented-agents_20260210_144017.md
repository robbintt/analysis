---
ver: rpa2
title: Learning to reason about rare diseases through retrieval-augmented agents
arxiv_id: '2511.04720'
source_url: https://arxiv.org/abs/2511.04720
tags:
- reasoning
- diagnostic
- radar
- system
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RADAR (Retrieval-Augmented Diagnostic Reasoning Agents) is a framework
  for rare disease detection in brain MRI that combines multi-agent collaboration
  with retrieval-augmented generation. The system uses specialized agents to generate
  diagnostic hypotheses, retrieve relevant medical evidence from external sources
  like Radiopaedia, and synthesize evidence-grounded diagnoses.
---

# Learning to reason about rare diseases through retrieval-augmented agents

## Quick Facts
- arXiv ID: 2511.04720
- Source URL: https://arxiv.org/abs/2511.04720
- Reference count: 0
- Key outcome: RADAR achieves up to 10.2% improvement in Top-5 accuracy for rare brain disease diagnosis compared to non-retrieval baselines

## Executive Summary
RADAR is a framework for rare disease detection in brain MRI that combines multi-agent collaboration with retrieval-augmented generation. The system uses specialized agents to generate diagnostic hypotheses, retrieve relevant medical evidence from external sources like Radiopaedia, and synthesize evidence-grounded diagnoses. On the NOVA dataset of 280 rare brain diseases, RADAR achieves up to 10.2% improvement in Top-5 accuracy compared to non-retrieval baselines, with strongest gains for open-source models like DeepSeek. The framework improves both diagnostic accuracy and interpretability by explicitly linking model decisions to retrieved medical literature.

## Method Summary
RADAR employs a three-agent system for rare disease diagnosis: an initial doctor agent generates 10 diagnostic candidates using high temperature sampling, a RAG agent retrieves relevant medical literature from Radiopaedia based on generated queries, and a final doctor agent synthesizes an evidence-grounded diagnosis. The retrieval pipeline uses all-MiniLM-L6-v2 embeddings indexed with FAISS for efficient similarity search over chunked medical documents. The framework processes radiologist-provided image captions and clinical history to output a primary diagnosis plus four differentials, achieving significant improvements over non-retrieval baselines while maintaining interpretability through traceable reasoning.

## Key Results
- RADAR achieves up to 10.2% improvement in Top-5 accuracy compared to non-retrieval baselines on the NOVA dataset
- Best performance (54.40% Top-1, 75.05% Top-5 accuracy) was achieved using GPT-4o, matching or exceeding human radiologist performance
- Open-source models like DeepSeek showed the strongest gains from retrieval augmentation, suggesting compensation for weaker parametric knowledge

## Why This Works (Mechanism)

### Mechanism 1: External Knowledge Injection for Low-Prevalence Conditions
Retrieving domain-specific medical literature at inference time compensates for insufficient training data on rare pathologies. The RAG agent queries Radiopaedia using keyword-based search, retrieves 10 documents per keyword (5 articles + 5 cases), chunks them into overlapping segments, embeds via all-MiniLM-L6-v2, and indexes with FAISS for cosine similarity retrieval. This external evidence is then fed to the final doctor agent. The core assumption is that Radiopaedia contains diagnostically relevant information for the 280 rare diseases in NOVA, and semantic similarity search retrieves useful chunks.

### Mechanism 2: Temperature-Calibrated Agent Specialization
Assigning different temperature settings to specialized agents balances exploratory hypothesis generation against factual grounding. The initial doctor agent uses high temperature/top-p for diagnostic diversity (generating 10 candidates); RAG answer generation uses low temperature to stay faithful to retrieved content; final doctor agent uses mid-range temperature for balanced reasoning. The core assumption is that higher temperature produces broader differential diagnoses without excessive hallucination; lower temperature constrains outputs to retrieved evidence.

### Mechanism 3: Explicit Evidence Conditioning Reduces Hallucination
Conditioning the final diagnosis on retrieved literature provides interpretable, traceable outputs and mitigates hallucinations. The final doctor agent receives the union of (image caption, clinical data, retrieved evidence chunks, candidate diagnoses) and must synthesize a primary diagnosis plus four differentials that reference the retrieved material. The core assumption is that LLMs can integrate conflicting signals (initial hypothesis vs. retrieved evidence) and update diagnoses when evidence contradicts priors.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: RADAR's core innovation is dynamically retrieving medical literature at inference time rather than relying solely on parametric knowledge.
  - Quick check question: Can you explain why dense retrieval (embedding similarity) might outperform keyword search for rare disease queries?

- Concept: **Multi-Agent Orchestration**
  - Why needed here: RADAR separates hypothesis generation, evidence retrieval, and final synthesis into distinct agents with different configurations.
  - Quick check question: What would happen if all three agents used the same temperature setting?

- Concept: **FAISS Vector Indexing**
  - Why needed here: Efficient similarity search over chunked medical documents enables real-time retrieval during diagnostic reasoning.
  - Quick check question: Why chunk documents into overlapping segments rather than embedding whole articles?

## Architecture Onboarding

- Component map: Initial Doctor Agent -> RAG Agent -> Final Doctor Agent
- Critical path: Image caption + clinical data → Initial Doctor (hypotheses) → RAG Agent (queries → retrieval → answers) → Final Doctor (evidence-grounded diagnosis)
- Design tradeoffs:
  - High vs. low temperature: exploration vs. faithfulness to evidence
  - Top-k=5 chunks: retrieval breadth vs. noise injection
  - Radiopaedia-only: domain relevance vs. knowledge base coverage gaps
- Failure signatures:
  - Top-1 accuracy significantly lower than Top-5: initial hypotheses lack precision
  - Open-source models gain more from RADAR than closed models (observed): suggests retrieval compensates for weaker parametric knowledge
  - No improvement over single-agent: retrieval returning irrelevant chunks
- First 3 experiments:
  1. Baseline comparison: Run single-agent, collaborative, and challenger setups without retrieval to quantify RADAR's marginal contribution
  2. Ablation by temperature: Vary temperature settings for each agent to validate the calibration hypothesis
  3. Retrieval source ablation: Test with vs. without Radiopaedia access to measure knowledge base dependency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RADAR be extended to perform direct visual reasoning on MRI images without relying on radiologist-provided captions as an intermediate step?
- Basis in paper: [explicit] "Nevertheless, RADAR still relies on radiologist-provided captions. Bridging this gap from textual reasoning to direct visual understanding remains an open challenge."
- Why unresolved: The current framework requires expert-written captions describing imaging findings, limiting full automation and introducing potential subjectivity from the captioning radiologist.
- What evidence would resolve it: Integration of a vision encoder that directly processes MRI images and feeds visual features into the reasoning pipeline, evaluated on the NOVA dataset with comparable or superior accuracy to the caption-based approach.

### Open Question 2
- Question: How does RADAR generalize to other medical imaging modalities (e.g., CT, PET) and anatomical regions beyond brain MRI?
- Basis in paper: [inferred] The evaluation is restricted to brain MRI and the NOVA dataset; no experiments assess cross-modal or cross-anatomical transfer.
- Why unresolved: Rare diseases affect all organ systems, and the retrieval corpus (Radiopaedia) covers multiple modalities, but the framework's effectiveness outside neuroimaging is untested.
- What evidence would resolve it: Evaluation on benchmark datasets spanning other modalities and body regions, demonstrating consistent accuracy improvements over non-retrieval baselines.

## Limitations
- The framework relies heavily on the quality and coverage of the Radiopaedia knowledge base for effective retrieval-augmented reasoning
- The study uses a single dataset (NOVA) with specific characteristics, limiting generalizability to other imaging modalities or clinical contexts
- Exact temperature settings for each agent stage remain unspecified, and the effectiveness depends on the retrieval pipeline returning relevant medical literature

## Confidence
- High confidence: The retrieval-augmented framework improves Top-5 accuracy for rare disease detection compared to non-retrieval baselines, with quantitative results (up to 10.2% improvement) and ablation studies supporting this claim
- Medium confidence: The mechanism of temperature-calibrated agent specialization is theoretically sound but lacks empirical validation of specific temperature values and their impact on performance
- Medium confidence: The evidence-grounded diagnosis approach reduces hallucination based on qualitative examples, though systematic hallucination measurement is not provided

## Next Checks
1. **Retrieval quality assessment**: Measure the relevance of retrieved chunks from Radiopaedia for a random sample of cases to quantify the actual utility of external knowledge injection
2. **Temperature sensitivity analysis**: Systematically vary temperature settings across the three agent stages to identify optimal configurations and validate the specialization hypothesis
3. **Knowledge base coverage evaluation**: Test RADAR's performance when Radiopaedia lacks coverage for specific rare diseases to understand the limits of external knowledge dependency