---
ver: rpa2
title: Accelerating Transformer Inference and Training with 2:4 Activation Sparsity
arxiv_id: '2503.16672'
source_url: https://arxiv.org/abs/2503.16672
tags:
- sparsity
- sparse
- training
- inference
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to accelerate both inference and training
  of large language models (LLMs) using 2:4 activation sparsity, leveraging the intrinsic
  sparsity found in Squared-ReLU activations. The approach enables up to 1.3x faster
  Feed-Forward Networks (FFNs) in both forward and backward passes without accuracy
  loss.
---

# Accelerating Transformer Inference and Training with 2:4 Activation Sparsity

## Quick Facts
- arXiv ID: 2503.16672
- Source URL: https://arxiv.org/abs/2503.16672
- Reference count: 3
- Primary result: 1.3x faster FFNs with 2:4 activation sparsity, <2% accuracy loss

## Executive Summary
This paper presents a method to accelerate both inference and training of large language models using 2:4 activation sparsity in Squared-ReLU Feed-Forward Networks. By exploiting the intrinsic sparsity that emerges in Squared-ReLU activations during training (85-98% sparsity), the approach enables hardware-accelerated sparse matrix multiplication on NVIDIA TensorCores, achieving up to 1.3x speedup in FFN layers. The method includes specialized optimizations for the backward pass, including a 95/5 split-GEMM approach and token permutation, to maintain training stability while accelerating 4 of 6 matrix multiplications in the FFN.

## Method Summary
The method replaces SwiGLU activations with Squared-ReLU in FFN layers, then applies 2:4 structured sparsity to the activations. During the forward pass, dense matrix multiplication produces Y₁, which undergoes ReLU² activation followed by 2:4 sparsification. The sparse activations Y₂ are then multiplied by W₂ using hardware-accelerated sparse GEMM. For the backward pass, features are split into 95% sparse (using 2:4 GEMM) and 5% dense (using regular GEMM) based on per-feature sparsity, with token permutation applied to break correlations. A warm-up period of 1k iterations training dense precedes enabling sparsity.

## Key Results
- 1.3x faster Feed-Forward Networks in both forward and backward passes
- <2% accuracy degradation versus dense baseline (perplexity ~2.65 vs 2.652)
- 1.5-1.7x speedup for 2:4 FP8 matrix multiplication on H100 GPUs
- 85-98% activation sparsity emerges during training without explicit regularization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Squared-ReLU activations naturally develop high sparsity (84-98%) during training, enabling 2:4 sparsity exploitation with minimal value dropping.
- Mechanism: Squared-ReLU = max(0, x)² maps all negative values to zero. At initialization with centered inputs, this yields ~50% sparsity. During training, sparsity increases substantially (the paper does not provide a theoretical explanation for this emergence). The high sparsity means fewer values conflict with the 2:4 constraint when sparsifying.
- Core assumption: The observed sparsity emergence generalizes across model scales and datasets beyond the 1.5B/7B models tested.
- Evidence anchors:
  - [abstract] "we exploit the intrinsic sparsity found in Squared-ReLU activations"
  - [section 1, page 2] "we observe that the sparsity level rapidly increases during training, reaching 85-98% depending on the layer (Figure 1)"
  - [corpus] Related work (La RoSA, Amber Pruner) confirms activation sparsity is viable for LLM acceleration, but typically requires additional training or recovery procedures.

### Mechanism 2
- Claim: 2:4 sparsity enables hardware-accelerated matrix multiplication via NVIDIA TensorCores, achieving up to 1.3x FFN speedup.
- Mechanism: Modern GPUs have specialized units that accelerate 2:4 sparse tensors (at most 2 non-zeros per 4 consecutive values). Theoretically 2x speedup is possible; practically, 1.5-1.7x is observed for FP8 on H100. The method accelerates the second FFN matrix multiply (Y₂W₂) where Y₂ = ReLU²(X₁W₁) is sparse.
- Core assumption: The target deployment hardware supports 2:4 sparse TensorCore operations (Ampere architecture and later).
- Evidence anchors:
  - [abstract] "achieves up to 1.3x faster Feed Forward Networks (FFNs) in both the forwards and backwards pass"
  - [section 1, page 2] "in practice we see 1.5x-1.7x speedups for 2:4 FP8 matrix multiplication on H100"
  - [corpus] Corpus evidence on 2:4 hardware mechanisms is sparse; most related work focuses on weight sparsity rather than activation sparsity.

### Mechanism 3
- Claim: Split-GEMM with feature-wise partitioning and token permutation maintains training stability while accelerating backward passes.
- Mechanism: The backward pass requires feature-wise sparsity (not token-wise) due to GPU constraints on reduction dimensions. Two optimizations: (1) split features into 95% sparse (2:4 GEMM) + 5% dense (regular GEMM) based on column-level sparsity; (2) apply fixed permutation to tokens to break correlations where non-zero values cluster consecutively. The forward sparsification mask is reused to ensure dropped values don't reappear.
- Core assumption: ~5% of features are insufficiently sparse (<80%) and cannot be sparsified without accuracy loss.
- Evidence anchors:
  - [section 2, page 3-4] "Some individual features are not sufficiently sparse (e.g. 20% sparsity only) and are unable to be sparsified"
  - [table 1, page 4] Ablation shows "2:4 - no permuting rows" plateaus early at perplexity 2.919 vs 2.652 baseline; "2:4 - no sparsify y1 in BW pass" diverges.
  - [corpus] No direct corpus precedent for the split-GEMM + permutation approach.

## Foundational Learning

- **Concept: 2:4 (N:M) Sparsity Pattern**
  - Why needed here: This is the only sparsity format NVIDIA TensorCores accelerate. Understanding it is prerequisite to comprehending why 50% sparsity minimum is required and why the constraint is "at most 2 non-zeros per 4 consecutive values."
  - Quick check question: Given a 8-element vector [0.1, 0.0, 0.3, 0.2, 0.0, 0.0, 0.5, 0.4], is it 2:4 sparse? (Answer: No—elements 5-8 have three non-zeros in the second block of 4.)

- **Concept: Squared-ReLU vs SwiGLU Activation Functions**
  - Why needed here: The paper replaces SwiGLU with Squared-ReLU specifically for its sparsity properties. Understanding both is necessary to evaluate the tradeoff.
  - Quick check question: What value does SwiGLU produce for input x=-1.0 with β=1, versus Squared-ReLU? (Answer: SwiGLU ≈ -0.27, Squared-ReLU = 0. SwiGLU rarely produces exact zeros.)

- **Concept: Forward vs Backward Pass Sparsity Requirements**
  - Why needed here: The paper accelerates 4 of 6 matrix multiplications in the FFN. NVIDIA GPUs only accelerate sparsity along the reduction dimension, which differs between forward (token-wise) and backward (feature-wise) passes.
  - Quick check question: In computing ∂W₂ = Y₂ᵀ × ∂y₃, which dimension is reduced and which tensor must be sparse? (Answer: Feature dimension is reduced; Y₂ must be feature-wise sparse.)

## Architecture Onboarding

- **Component map:**
  Input X → Dense FP8 GEMM: XW₁ → Y₁ → Fused Kernel: ReLU² + 2:4 Sparsification + FP8 Quantize → Y₂ (sparse) → 2:4 Sparse FP8 GEMM: Y₂W₂ → Output

  Backward: Split into 95% features (2:4 sparse GEMM) + 5% features (dense GEMM)
            Token permutation fused into add/quantize ops before/after FFN

- **Critical path:** The three-kernel forward pass: (1) dense matmul W₁, (2) fused activation+sparsify+quantize, (3) 2:4 sparse matmul W₂. The sparsification kernel is the critical enabler—it must produce valid 2:4 sparse output in FP8 format with minimal dropped values.

- **Design tradeoffs:**
  - Squared-ReLU vs SwiGLU: Swaps activation function for sparsity; paper claims no accuracy loss but requires validation for your use case.
  - 5% dense GEMM overhead: Split approach adds complexity but preserves accuracy vs naive feature-wise sparsification.
  - Token permutation: Adds overhead but breaks feature correlations critical for backward pass stability.

- **Failure signatures:**
  - Early training divergence (step ~42k in ablation): Caused by sparsifying forward pass without corresponding backward pass handling.
  - Plateauing at high perplexity (2.919): Caused by omitting token permutation—correlated features can't be sparsified.
  - Accuracy degradation vs dense baseline: Likely caused by high value-drop rate (>2%); check sparsity levels are 85%+.

- **First 3 experiments:**
  1. Validate Squared-ReLU baseline: Train a small model (e.g., 100M params) with SwiGLU vs Squared-ReLU on your dataset to confirm no accuracy degradation before adding sparsity.
  2. Measure intrinsic sparsity: Log activation sparsity per layer during training to verify 85%+ sparsity emerges; if not, investigate input distributions or activation scaling.
  3. Kernel microbenchmark: Isolate the fused sparsification kernel and 2:4 GEMM to measure actual speedup vs dense baseline on your target hardware before full integration.

## Open Questions the Paper Calls Out
None

## Limitations
- Sparsity mechanism depends on unexplained emergence of 85-98% sparsity in Squared-ReLU, raising generalizability concerns
- Hardware acceleration limited to NVIDIA Ampere+ GPUs with 2:4 sparse TensorCore support
- Backward optimizations lack theoretical justification for 95/5 split and specific permutation parameters

## Confidence
**High confidence**: Hardware acceleration claims (1.5-1.7x speedup) and accuracy degradation (<2%) are well-supported by benchmarks.

**Medium confidence**: Intrinsic sparsity emergence (85-98%) is empirically observed but lacks theoretical explanation for why sparsity increases during training.

**Low confidence**: Generalizability across model scales and datasets is weakly supported, with experiments limited to 1.5B/7B models on DCLM dataset.

## Next Checks
1. **Sparsity emergence validation**: Instrument a small model to log activation sparsity per layer throughout training. If sparsity levels remain below 75% after several epochs, investigate whether input normalization, activation scaling, or initialization parameters need adjustment.

2. **Hardware compatibility check**: Before integration, verify your target GPU supports 2:4 sparse TensorCores by querying the hardware capabilities (compute capability >= 8.0) and measuring baseline dense vs sparse GEMM performance on a small matrix to confirm the expected 1.5-1.7x speedup range.

3. **Feature sparsity distribution analysis**: Profile the per-feature sparsity levels across all FFN layers to determine the actual percentage of features requiring the dense fallback. If >10% of features are insufficiently sparse, investigate whether different threshold criteria or additional preprocessing could reduce this overhead.