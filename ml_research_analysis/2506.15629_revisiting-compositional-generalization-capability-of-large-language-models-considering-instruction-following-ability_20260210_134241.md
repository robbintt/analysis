---
ver: rpa2
title: Revisiting Compositional Generalization Capability of Large Language Models
  Considering Instruction Following Ability
arxiv_id: '2506.15629'
source_url: https://arxiv.org/abs/2506.15629
tags:
- llms
- order
- linguistics
- computational
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces Ordered CommonGen, a benchmark to evaluate
  both compositional generalization and instruction-following abilities in large language
  models (LLMs). Unlike traditional CommonGen tasks that focus only on concept coverage,
  Ordered CommonGen requires models to generate sentences that include all given concepts
  in a specified order.
---

# Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability

## Quick Facts
- arXiv ID: 2506.15629
- Source URL: https://arxiv.org/abs/2506.15629
- Reference count: 40
- Key outcome: Introduces Ordered CommonGen benchmark revealing LLMs understand but frequently fail to follow specified concept order instructions

## Executive Summary
This study introduces Ordered CommonGen, a benchmark that evaluates both compositional generalization and instruction-following abilities in large language models (LLMs). The benchmark extends traditional CommonGen tasks by requiring models to generate sentences that include all given concepts in a specified order. Through evaluation of 36 LLMs, the research reveals that while models generally understand the intent of order instructions, they often fail to generate sentences that precisely follow the specified concept order, with the best model achieving only about 75% ordered coverage. The study identifies significant limitations in both compositional generalization and instruction-following capabilities of LLMs, particularly for verb-heavy concept sets and when natural frequency priors in training data conflict with explicit order instructions.

## Method Summary
The study constructs Ordered CommonGen by permuting 192 CommonGen-lite concept sets (4 concepts each) to generate 4,608 ordered concept sets, then applies 6 FLAN-derived instruction templates modified with "in the specified order" phrase. Evaluation uses zero-shot inference across 36 LLMs with greedy decoding (open models) or temperature=0 (proprietary models), calculating three coverage metrics, two similarity metrics, two diversity metrics, and perplexity via GPT2-XL. The methodology deliberately avoids reference-based metrics due to the unavailability of human references for all permutations.

## Key Results
- Best-performing model achieves only ~75% ordered coverage, revealing significant instruction-following limitations
- Models frequently generate identical sentences across different concept order permutations, ignoring order instructions
- Verb-heavy concept sets (VVVV patterns) show dramatically lower coverage (~37%) compared to noun-heavy sets (NNNN ~91%)
- Instruction-tuned models outperform base models due to enhanced instruction-following capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit order instructions improve but do not guarantee ordered concept generation
- Mechanism: Adding "in the specified order" activates instruction-following pathways that modulate token generation preferences, but frequency-based associations from training data often override these constraints
- Core assumption: Instruction-tuned models have separate pathways for general text generation and instruction constraint satisfaction
- Evidence anchors:
  - Abstract: "while LLMs generally understand the intent of instructions, biases toward specific concept order patterns often lead to low-diversity outputs"
  - Section 4: "specifying the usage order of concepts in the prompt through Ordered CommonGen improves coverage rates for most LLMs"
  - Corpus: Neighbor paper confirms instruction-following evaluation is an active research area

### Mechanism 2
- Claim: Training data frequency patterns dominate over explicit instructions in ambiguous cases
- Mechanism: Models encode frequency-based associations that create "natural" concept sequences; these associations compete with instruction-driven constraints, with frequency priors often winning
- Core assumption: LLMs have implicit ordering preferences derived from corpus statistics that operate independently of explicit task instructions
- Evidence anchors:
  - Section 4: "LLMs tend to reorder concepts into sequences they consider most natural, overriding instructions to follow the specified order"
  - Section 4, Finding 5: "LLMs sometimes generate identical sentences even when the concepts are shuffled"
  - Corpus: Weak/no direct corpus evidence on this specific mechanism

### Mechanism 3
- Claim: Compositional generalization capability varies systematically with linguistic structure patterns
- Mechanism: Models handle noun-heavy concept sets better than verb-heavy ones; NNNN patterns achieve highest coverage while VVVV patterns show highest variance and lowest base coverage
- Core assumption: Compositional generalization is not uniform across semantic/syntactic categories and depends on how models process different word classes during generation
- Evidence anchors:
  - Section 5.1: "the NNNN pattern (noun-only) achieves the highest Concepts Coverage... the VVVV pattern (verb-only) results in outputs where all concepts are included in only approximately 37% of cases"
  - Section 5.1: "the VVVV pattern exhibits the highest variance in Concepts Coverage"
  - Corpus: "Towards Compositional Generalization of LLMs via Skill Taxonomy Guided Data Synthesis" addresses compositional generalization bottlenecks

## Foundational Learning

- Concept: Generative Commonsense Reasoning (GCR)
  - Why needed here: This is the base task that Ordered CommonGen extends; understanding that GCR requires generating natural sentences containing all given concepts is prerequisite to understanding why adding order constraints creates a harder dual-evaluation task
  - Quick check question: Given concepts {dog, frisbee, throw, catch}, can you explain why "The dog catches the frisbee" would pass traditional CommonGen but fail Ordered CommonGen if the specified order was [throw, dog, frisbee, catch]?

- Concept: Instruction-Following vs. In-Context Learning
  - Why needed here: The paper deliberately tests zero-shot settings to isolate "inductive reasoning ability" rather than in-context learning; distinguishing these helps understand why one-shot priming experiments produced different results than the main zero-shot evaluation
  - Quick check question: If a model generates correct ordered sentences after seeing one example, does that demonstrate compositional generalization or in-context learning? What would distinguish these?

- Concept: Coverage Metrics (w/o order vs. w/ order vs. Ordered Rate)
  - Why needed here: The paper introduces a three-metric evaluation framework that separates concept inclusion from concept ordering; understanding this distinction is essential for interpreting the 75% ceiling result and why "w/o order" scores are higher than "w/ order" scores
  - Quick check question: A model achieves 95% coverage w/o order and 50% coverage w/ order. What does this gap tell you about the model's capabilities?

## Architecture Onboarding

- Component map:
  - Concept Set Permutation Engine -> Instruction Template Library -> Evaluation Pipeline -> LLM Interface Layer

- Critical path:
  1. Load CommonGen-lite concept sets → permute → generate prompts
  2. Collect outputs from target LLMs across all 27,648 instances (6 templates × 4,608 concept sets)
  3. Lemmatize outputs using spaCy EN_CORE_WEB_LG
  4. Calculate ordered coverage by checking if concepts appear in specified sequence
  5. Compute pairwise similarity across 24 permutations per original concept set

- Design tradeoffs:
  - Zero-shot vs. few-shot evaluation: Chose zero-shot to measure "pure inductive ability"; one-shot experiments showed priming helps but reduces diversity
  - Reference-free vs. reference-based metrics: Deliberately avoided BLEU/ROUGE against human references because (a) CommonGen test data unavailable, (b) human labels don't exist for all permutations, and (c) reference-based metrics have known limitations in generation tasks
  - Multi-template averaging vs. single template: Averaging across 6 templates accounts for prompt variance but increases compute cost 6×

- Failure signatures:
  - Identical output across permutations: Indicates model is ignoring order instructions entirely; check Diverse Rate metric
  - High w/o order coverage but low w/ order coverage: Model includes concepts but reorders them; suggests natural sequence bias is overriding instructions
  - Verb-heavy concept set failures: NNNN patterns succeed at ~91% coverage, VVVV at ~37%; verb composition is a known weak point
  - Template-dependent performance variance: If one template works much better than others, the model may be template-sensitive rather than genuinely instruction-compliant

- First 3 experiments:
  1. Baseline coverage gap measurement: Run your target LLM on Ordered CommonGen zero-shot; calculate the gap between coverage w/o order and coverage w/ order; this gap quantifies instruction-following limitation
  2. Permutation sensitivity test: For a single concept set, check if all 24 permutations produce different outputs; a Diverse Rate below 0.8 indicates the model is collapsing to a single "natural" ordering regardless of input
  3. Template variance analysis: Run evaluation with each of the 6 templates separately; high variance across templates indicates prompt engineering sensitivity rather than robust instruction-following capability

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the Ordered CommonGen evaluation framework generalize to languages with rigid word orders or different syntactic structures, such as Korean or Japanese?
  - Basis in paper: Section 7 (Limitations) notes the study focused only on English, which "limits the generalizability of our findings to other languages, such as Korean."
  - Why unresolved: The difficulty of adhering to a specified concept order may vary drastically depending on the target language's syntactic flexibility (e.g., SOV vs. SVO).
  - What evidence would resolve it: Adapting the benchmark for multilingual datasets like Korean CommonGen and comparing the "Ordered Rate" scores against English baselines.

- **Open Question 2**: How does model performance degrade when increasing the size of the concept set beyond four concepts?
  - Basis in paper: Section 7 states that while increasing the number of concepts could provide insights, the "exponential growth in permutations makes such evaluations highly challenging," leaving it for future work.
  - Why unresolved: It is unclear if the ~75% ceiling on ordered coverage holds, improves, or collapses as the cognitive load of tracking 5+ concepts increases.
  - What evidence would resolve it: Implementing efficient sampling strategies to evaluate permutations for concept sets of size 5 and 6, measuring the drop in ordered coverage.

- **Open Question 3**: Can the "specified order" constraint framework be effectively transferred to creative and visual domains, such as video generation or poetry?
  - Basis in paper: Section 6 proposes extending the core idea to tasks like "poem or story composition" or the "vision domain, e.g., manga or anime," to verify adherence to intended developments.
  - Why unresolved: The paper establishes the difficulty of ordered composition in text sentences but has not validated if this instruction-following capability transfers to temporal or structural planning in other modalities.
  - What evidence would resolve it: Constructing a multimodal benchmark where models must generate image sequences or video clips adhering to a specific narrative order of input concepts.

## Limitations
- The evaluation framework relies on lemmatization-based concept matching, which may not capture semantic equivalence between inflected forms and may struggle with multi-word concepts or proper nouns
- Zero-shot evaluation design limits generalizability to few-shot or fine-tuned scenarios where instruction-following performance might differ substantially
- The benchmark construction assumes all 24 permutations of a 4-concept set are equally valid test cases, but some permutations may be semantically nonsensical

## Confidence

**High Confidence**: The finding that instruction-tuned models achieve better performance on Ordered CommonGen than base models due to their instruction-following capabilities. This is well-supported by the systematic comparison across 36 LLMs and the observed correlation between instruction-following scores and ordered concept coverage.

**Medium Confidence**: The claim that verb-heavy concept sets (VVVV patterns) present significantly greater compositional generalization challenges than noun-heavy sets (NNNN patterns). While the empirical results show this pattern clearly, the underlying reasons for this difference require further investigation into the models' compositional mechanisms.

**Medium Confidence**: The assertion that models tend to generate identical sentences across different concept order permutations, indicating failure to follow instructions. This finding is robust across multiple models and templates, but the exact threshold for what constitutes "identical" behavior could be debated.

## Next Checks

1. **Cross-linguistic validation**: Test whether the observed patterns in instruction-following and compositional generalization hold when evaluating multilingual LLMs on multilingual versions of Ordered CommonGen, particularly focusing on languages with different syntactic structures.

2. **Temporal and causal relationship testing**: Design concept sets that specifically require temporal or causal ordering (e.g., {wake, brush, dress, eat}) to test whether models can distinguish between different types of ordering constraints beyond simple sequential arrangement.

3. **Template sensitivity analysis**: Conduct ablation studies on the instruction templates to determine whether specific linguistic formulations (e.g., "in the specified order" vs. "following this exact sequence") produce significantly different performance outcomes, and whether this sensitivity varies across different model families.