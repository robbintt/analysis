---
ver: rpa2
title: Evaluating Online Moderation Via LLM-Powered Counterfactual Simulations
arxiv_id: '2511.07204'
source_url: https://arxiv.org/abs/2511.07204
tags:
- very
- high
- user
- moderation
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COSMOS, a LLM-powered simulator for evaluating
  content moderation strategies on online social networks. It uses LLM-based agents
  with socio-demographic and psychological profiles to generate parallel factual and
  counterfactual simulations of OSN conversations.
---

# Evaluating Online Moderation Via LLM-Powered Counterfactual Simulations
## Quick Facts
- arXiv ID: 2511.07204
- Source URL: https://arxiv.org/abs/2511.07204
- Reference count: 23
- Primary result: LLM-based simulator demonstrates toxicity propagation and shows personalized moderation outperforms generic approaches

## Executive Summary
This paper introduces COSMOS, a LLM-powered simulator for evaluating content moderation strategies on online social networks. It uses LLM-based agents with socio-demographic and psychological profiles to generate parallel factual and counterfactual simulations of OSN conversations. The counterfactual version applies moderation interventions while keeping other variables constant. Experiments demonstrate that agents exhibit believable and consistent toxic behavior, that toxicity propagates through social contagion, and that personalized moderation strategies outperform one-size-fits-all approaches. The simulator provides a controllable, cost-effective method for testing moderation effectiveness without real-world data collection.

## Method Summary
COSMOS employs LLM-based agents with assigned socio-demographic and psychological profiles to simulate social network conversations. The system runs parallel simulations: one factual (no intervention) and one counterfactual (with moderation applied). By keeping all variables constant except the intervention, the framework isolates the effects of different moderation strategies. The agents generate interactions that exhibit consistent toxic behavior patterns, allowing researchers to study how toxicity propagates through networks and how various moderation approaches affect outcomes.

## Key Results
- Agents consistently exhibit believable toxic behavior patterns that align with their assigned profiles
- Toxicity propagates through simulated networks via social contagion mechanisms
- Personalized moderation strategies demonstrate superior effectiveness compared to generic, one-size-fits-all approaches

## Why This Works (Mechanism)
The simulator leverages LLMs' ability to generate contextually appropriate text while incorporating agent profiles that guide behavior. By maintaining parallel simulations with identical starting conditions except for moderation intervention, the system creates controlled counterfactual scenarios. This design enables isolation of intervention effects while preserving realistic social dynamics. The LLM agents' ability to maintain consistent personas across interactions allows for credible modeling of how individual characteristics influence participation in toxic conversations and responsiveness to moderation.

## Foundational Learning
- **Socio-demographic profiling**: Understanding how demographic factors (age, location, education) influence online behavior - needed to create realistic agent diversity, quick check: verify profile assignments produce distinct behavioral patterns
- **Psychological trait modeling**: Incorporating personality dimensions like agreeableness, openness, and conscientiousness - needed to predict individual responses to moderation, quick check: test whether agents with low agreeableness show higher baseline toxicity
- **Social contagion dynamics**: Modeling how behaviors spread through network connections - needed to capture realistic propagation patterns, quick check: measure correlation between network proximity and shared behavioral traits
- **Counterfactual simulation design**: Creating parallel scenarios with controlled variable manipulation - needed to isolate intervention effects, quick check: verify that only intervention differs between factual and counterfactual runs
- **Moderation intervention modeling**: Simulating different approaches (content filtering, user warnings, temporary bans) - needed to compare strategy effectiveness, quick check: ensure interventions are applied consistently across scenarios

## Architecture Onboarding
- **Component map**: User Profiles -> LLM Agents -> Conversation Simulator -> Moderation Module -> Outcome Analyzer
- **Critical path**: Profile assignment → Agent interaction generation → Moderation application → Behavior measurement
- **Design tradeoffs**: High fidelity to human behavior versus computational efficiency; rich psychological profiles versus manageable complexity; realistic toxic behavior generation versus ethical concerns about simulating harmful content
- **Failure signatures**: Agents producing unrealistic or inconsistent behavior; moderation effects that don't scale proportionally with intervention strength; lack of propagation patterns despite social connections
- **First experiments**: 1) Baseline toxicity measurement across agent profiles, 2) Network density impact on propagation speed, 3) Moderation timing effects on intervention success rates

## Open Questions the Paper Calls Out
None

## Limitations
- Simulation fidelity to real-world social dynamics remains uncertain, particularly regarding complex human motivations and context-dependent communication
- Psychological and demographic profiles use simplified representations that may not capture full user heterogeneity
- Effectiveness of moderation interventions in simulation may not translate directly to real-world implementation due to contextual nuances and language subtleties

## Confidence
- High confidence in simulator's ability to generate consistent toxic behavior patterns and demonstrate toxicity propagation
- Medium confidence in comparative effectiveness of personalized versus one-size-fits-all moderation strategies
- Lower confidence in real-world applicability of findings due to limitations in LLM-based social simulation

## Next Checks
1. Test simulator predictions against real-world moderation data from actual social networks to assess predictive validity
2. Conduct ablation studies removing specific agent profile features to determine which psychological and demographic factors most influence toxicity propagation
3. Implement multi-stage validation where simulated interventions are tested in controlled real-world pilot programs before full deployment