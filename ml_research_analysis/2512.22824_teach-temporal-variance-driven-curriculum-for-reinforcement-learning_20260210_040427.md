---
ver: rpa2
title: 'TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning'
arxiv_id: '2512.22824'
source_url: https://arxiv.org/abs/2512.22824
tags:
- u1d461
- u1d70b
- u1d703
- u1d454
- u1d460
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses sample inefficiency in multi-goal reinforcement
  learning by proposing TEACH, a temporal variance-driven curriculum learning method.
  The approach leverages the temporal variance of Q-values to dynamically prioritize
  goals where policy learning is most active, rather than relying on static heuristics
  or noisy value estimates.
---

# TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.22824
- Source URL: https://arxiv.org/abs/2512.22824
- Reference count: 40
- The paper proposes a temporal variance-driven curriculum learning method that uses Q-value variance to dynamically prioritize goals where policy learning is most active

## Executive Summary
TEACH addresses sample inefficiency in multi-goal reinforcement learning by introducing a temporal variance-driven curriculum learning approach. The method leverages the temporal variance of Q-values to identify goals where policy learning is most active, creating a dynamic curriculum that focuses on goals at the skill frontier. Unlike static heuristics or methods relying on noisy value estimates, TEACH uses recent Q-value histories to quantify learning progress through a policy confidence score, enabling more effective goal prioritization across diverse robotic manipulation and maze navigation tasks.

## Method Summary
TEACH introduces a novel curriculum learning framework that uses temporal variance of Q-values to dynamically prioritize goals during multi-goal reinforcement learning. The approach establishes a theoretical connection between Q-value variance and policy evolution, demonstrating that high variance indicates significant policy divergence. A policy confidence score derived from Q-values quantifies learning progress, and the curriculum is designed based on the temporal variance of this score over recent timesteps. This strategy mitigates the impact of noisy value estimates and focuses learning on goals where the policy is evolving most rapidly, improving sample efficiency across 11 diverse tasks.

## Key Results
- TEACH consistently outperforms state-of-the-art curriculum learning and goal-selection methods
- Demonstrated improved sample efficiency across 11 diverse robotic manipulation and maze navigation tasks
- Shows robust performance by focusing learning on goals at the skill frontier

## Why This Works (Mechanism)
TEACH works by identifying the frontier of policy learning through temporal Q-value variance analysis. The core insight is that when a policy is actively learning and evolving on a particular goal, the Q-values for that goal will exhibit higher temporal variance. This variance serves as a signal that the policy is still exploring and improving its behavior for that goal, rather than having converged to a suboptimal solution. By prioritizing goals with high temporal variance, TEACH ensures that learning resources are allocated to areas where the most significant policy improvements are occurring, rather than wasting samples on goals where the policy has already plateaued.

## Foundational Learning

**Temporal variance analysis**: Understanding how to measure and interpret the variance of Q-values over time is crucial for identifying active learning regions. This is needed to distinguish between goals where the policy is still evolving versus those where it has converged. Quick check: Verify that temporal variance calculations use appropriate window sizes to capture meaningful policy changes without excessive noise.

**Multi-goal RL framework**: The ability to manage and prioritize multiple goals simultaneously is fundamental to the approach. This is needed because TEACH must evaluate learning progress across all goals to construct an effective curriculum. Quick check: Confirm that goal selection mechanisms can handle the dynamic prioritization based on temporal variance signals.

**Policy confidence quantification**: Converting Q-value variance into actionable confidence scores requires understanding the relationship between value estimates and policy quality. This is needed to create a reliable metric for measuring learning progress. Quick check: Validate that confidence scores correlate with actual policy improvement rates across different goal difficulties.

## Architecture Onboarding

**Component map**: Q-value estimator -> Temporal variance calculator -> Policy confidence scorer -> Goal prioritization module -> Curriculum scheduler -> Environment interaction loop

**Critical path**: The temporal variance calculation depends on recent Q-value history, which requires maintaining rolling windows of value estimates. The policy confidence scorer transforms variance into prioritization weights, feeding into the curriculum scheduler that determines which goals to train on next. This path is critical because any breakdown in variance estimation or confidence scoring directly impacts curriculum quality.

**Design tradeoffs**: The method trades computational overhead of maintaining Q-value histories against improved sample efficiency. Window size for temporal variance represents a key hyperparameter balancing responsiveness to policy changes against noise sensitivity. The approach also trades off between exploiting known good goals versus exploring uncertain ones based on variance signals.

**Failure signatures**: Low variance across all goals indicates the policy has converged or training is stuck in local optima. Extremely high variance may suggest unstable learning or exploration issues. If confidence scores fail to correlate with actual performance improvements, the variance-based prioritization may be misaligned with true learning progress.

**First experiments**: 1) Verify temporal variance detection on a simple environment with known policy evolution patterns 2) Test sensitivity of curriculum quality to window size parameters across different environment dynamics 3) Compare variance-based prioritization against random and performance-based curricula on a toy multi-goal task

## Open Questions the Paper Calls Out

None

## Limitations

The method's effectiveness relies on sufficient exploration during early training phases, which may break down in sparse-reward scenarios where initial value estimates are highly unreliable. The temporal variance-based approach may not generalize well to environments with longer temporal dependencies or non-stationary dynamics where recent Q-value histories may not capture meaningful policy evolution. The experimental validation, while demonstrating strong performance, tested against a relatively narrow set of baseline methods, limiting the breadth of comparative claims.

## Confidence

Temporal variance as policy learning indicator: Medium
Sample efficiency improvements across diverse tasks: Medium
Robustness in sparse-reward scenarios: Low
Generalization to environments with long temporal dependencies: Low

## Next Checks

1. Test TEACH's performance on environments with highly sparse rewards and long time horizons to evaluate robustness when initial value estimates are unreliable
2. Compare temporal variance-based curriculum learning against methods that incorporate both temporal and spatial state visitation variance
3. Evaluate the sensitivity of curriculum performance to the window size parameter used for computing temporal variance in different environment dynamics