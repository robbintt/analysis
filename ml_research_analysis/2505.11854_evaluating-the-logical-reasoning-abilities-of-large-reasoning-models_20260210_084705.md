---
ver: rpa2
title: Evaluating the Logical Reasoning Abilities of Large Reasoning Models
arxiv_id: '2505.11854'
source_url: https://arxiv.org/abs/2505.11854
tags:
- reasoning
- logical
- language
- performance
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces LogiEval, a comprehensive logical reasoning
  benchmark for evaluating large reasoning models, covering deductive, inductive,
  abductive, and analogical reasoning across 10 task formats. The benchmark addresses
  the gap in assessing fundamental logical reasoning independent of domain knowledge.
---

# Evaluating the Logical Reasoning Abilities of Large Reasoning Models

## Quick Facts
- arXiv ID: 2505.11854
- Source URL: https://arxiv.org/abs/2505.11854
- Reference count: 14
- Large reasoning models excel at 4-choice argument analysis (surpassing human performance) but struggle with structured deductive reasoning tasks like syllogisms, with performance ranging from 73.74% to 89.90% across models.

## Executive Summary
This work introduces LogiEval, a comprehensive logical reasoning benchmark for evaluating large reasoning models across deductive, inductive, abductive, and analogical reasoning. The benchmark addresses the gap in assessing fundamental logical reasoning independent of domain knowledge, covering 10 task formats with 6,235 problems. The study reveals that modern reasoning models excel at 4-choice argument analysis but struggle with structured deductive reasoning tasks like syllogisms. A novel screening paradigm identifies LogiEval-Hard, a subset where all models show consistent failures, demonstrating persistent reasoning bottlenecks across model scales. The benchmark reveals inverse difficulty correlations where models perform well on problems humans find challenging yet fail unexpectedly on mid-difficulty items.

## Method Summary
The study evaluates logical reasoning across four types (deductive, inductive, abductive, analogical) and 10 task formats using the LogiEval benchmark containing 6,235 instances from human examinations. Models are evaluated via official APIs with temperature=0.7 and 16k token limits, using minimal prompt templates and regex-based answer extraction. For LogiEval-Hard creation, Qwen3-30B-A3B runs 3 trials per problem, selecting examples with majority-wrong consensus. The evaluation reports accuracy by reasoning type, task format, and option count, comparing performance on the full set versus the hard subset.

## Key Results
- Modern reasoning models achieve 95%+ accuracy on essential part identification but only 50-55% on 5-option syllogism questions
- Grok3-Think and OpenAI o4-mini perform near-random (51.86% and 54.85%) on 5-option syllogism questions while achieving high accuracy on other formats
- LogiEval-Hard subset shows all models averaging 37.97% accuracy, confirming persistent reasoning bottlenecks across model scales

## Why This Works (Mechanism)

### Mechanism 1: Small-Model Screening for Cross-Scale Failure Prediction
- Claim: If a compact reasoning model (e.g., 3B active parameters) consistently fails on a problem across multiple attempts, that problem will likely challenge much larger state-of-the-art models.
- Mechanism: Reasoning difficulties rooted in logical structure—rather than parametric capacity—manifest consistently across model scales. Small models serve as diagnostic probes for identifying fundamental architectural bottlenecks that scaling alone cannot resolve.
- Core assumption: The reasoning failures are structural/cognitive rather than capacity-limited, and thus persist across scales.
- Evidence anchors:
  - [abstract]: "a novel screening paradigm where small-model failures (Qwen3-30B-A3B) reliably predict difficulties for larger models"
  - [Section 4.3]: "82.3% of small-model failures simultaneously perplex this 32B-parameter state-of-the-art reasoner"
  - [corpus]: Weak direct evidence; related work LR²Bench and SokoBench evaluate long-chain reasoning but do not test cross-scale prediction.

### Mechanism 2: Non-Monotonic Difficulty Mapping Between Human and Model Cognition
- Claim: Model accuracy does not correlate monotonically with human difficulty; models may solve problems humans find hard while failing on problems humans find moderately difficult.
- Mechanism: LLMs trained on long CoT data may develop pattern-matching shortcuts for problems with distinctive surface features (common in "hard" exam questions) while lacking robust formal inference procedures for deceptively simple structures.
- Core assumption: Human difficulty rankings reflect cognitive load and working memory demands, while model failures reflect missing inference primitives or spurious correlations.
- Evidence anchors:
  - [abstract]: "inverse difficulty correlations where models perform well on problems humans find challenging yet fail unexpectedly on mid-difficulty items"
  - [Section 4.2]: "LLMs develop non-human reasoning strategies that excel on extreme difficulties but exhibit brittleness on specific problem types"
  - [corpus]: No direct corroboration; MME-Reasoning and related benchmarks evaluate multimodal reasoning but do not systematically compare human-model difficulty alignment.

### Mechanism 3: Task-Format Sensitivity in Reasoning Transfer
- Claim: Performance on logical reasoning tasks varies substantially by format (e.g., syllogism vs. argument analysis), even within the same reasoning type (deductive), suggesting format-specific optimization rather than general reasoning capability.
- Mechanism: Models may leverage superficial cues (option count, context length, linguistic patterns) rather than executing formal inference chains, leading to high variance across structurally similar tasks.
- Core assumption: Current long-CoT training produces format-bound heuristics rather than abstract logical operators.
- Evidence anchors:
  - [Section 3.2]: "Grok3-Think (51.86%) and OpenAI o4-mini (54.85%) performing near-random on 5-option [syllogism] questions, while DeepSeek-R1 achieved 73.38%"
  - [Section 3.2]: "all models achieved 95%+ accuracy on essential part identification... suggesting either an inherent strength in component-based reasoning or that these tasks rely on predictable pattern recognition"
  - [corpus]: LogicBench (cited in paper) and related work show similar format sensitivity but do not isolate causal factors.

## Foundational Learning

- Concept: **Deductive vs. Abductive vs. Inductive vs. Analogical Reasoning**
  - Why needed here: LogiEval explicitly evaluates all four types; understanding the distinctions is prerequisite to interpreting per-type performance gaps.
  - Quick check question: Given "All birds fly. Penguins are birds. Therefore penguins fly."—is this deductive, inductive, or abductive? (Answer: Deductive; applies general rule to specific case.)

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The evaluated models are "post-trained on long CoT data with reinforcement learning"; understanding CoT explains why models may succeed on multi-step tasks but fail single-step formal inference.
  - Quick check question: What is the difference between zero-shot prompting and chain-of-thought prompting? (Answer: CoT elicits intermediate reasoning steps before final answer.)

- Concept: **Benchmark Contamination and Saturation**
  - Why needed here: The paper notes existing benchmarks show "saturation" (models cluster >81%), motivating LogiEval-Hard; contamination awareness is critical for designing valid evaluations.
  - Quick check question: Why might a model achieve 99% on a benchmark yet fail real-world tasks? (Answer: Potential data contamination during training; model memorized examples rather than learning transferable skills.)

## Architecture Onboarding

- Component map: LogiEval full set (6,235 problems) -> LogiEval-Hard (1,617 problems) -> Small-model screening pipeline -> Evaluation harness

- Critical path:
  1. Run Qwen3-30B-A3B (or similar compact model) on full LogiEval with 3 trials per problem
  2. Identify problems with ≥2 incorrect responses -> LogiEval-Hard candidate set
  3. Evaluate target large reasoning models on both full set and LogiEval-Hard
  4. Compare per-type, per-format accuracy; flag inverse-difficulty cases

- Design tradeoffs:
  - Bilingual (English/Chinese) preserves linguistic nuance but complicates cross-lingual comparison
  - Exam-sourced questions ensure ecological validity but may carry copyright/contamination risks
  - Small-model screening reduces annotation cost but assumes structural failure transfer (may miss capacity-limited failures)

- Failure signatures:
  - Near-random accuracy on syllogisms (50–55% on 5-option) despite high performance on argument analysis -> formal logic deficit
  - 0% accuracy on specific mid-difficulty problems where humans achieve 30–50% -> inverse difficulty signature
  - High variance across formats within same reasoning type (e.g., deductive: 70% syllogism vs. 90% logical sequence) -> format-specific heuristics

- First 3 experiments:
  1. **Baseline profiling**: Run target model on full LogiEval; report accuracy by reasoning type, task format, and option count to establish capability distribution.
  2. **LogiEval-Hard validation**: Evaluate on LogiEval-Hard subset; if accuracy <40% with consistent failure patterns, confirm small-model screening transfer holds for your model class.
  3. **Inverse difficulty audit**: Identify problems where model accuracy inversely correlates with human accuracy; manually inspect 10–20 cases to determine if failures stem from missing inference primitives or spurious pattern reliance.

## Open Questions the Paper Calls Out

- **Question**: Does performance on text-only logical reasoning benchmarks translate effectively to multi-modal reasoning scenarios?
  - Basis in paper: [explicit] The authors explicitly state in the Limitations section that "Text-only evaluation excludes multi-modal reasoning challenges" and suggest "Future work should expand to multi-modal tasks."
  - Why unresolved: LogiEval currently relies solely on textual input, leaving the reasoning capabilities involving visual or auditory logical cues unassessed.
  - What evidence would resolve it: Evaluation of state-of-the-art models on a multi-modal extension of LogiEval to determine if text-only proficiency correlates with performance on visual logic puzzles (e.g., diagrammatic syllogisms).

- **Question**: To what extent does final-answer accuracy reflect the validity of the internal reasoning process in large reasoning models?
  - Basis in paper: [explicit] The Limitations section notes that "Accuracy metrics overlook reasoning validity and explanation robustness" and calls for the development of "process-aware evaluation metrics."
  - Why unresolved: High accuracy scores may mask underlying reasoning fallacies or "lucky" guesses, as current evaluation relies on exact string matching against gold labels.
  - What evidence would resolve it: A study correlating answer accuracy with automated formal verification of the generated chain-of-thought steps to check for logical consistency.

- **Question**: What specific training interventions can resolve the "inverse difficulty correlation" where models fail on mid-difficulty items?
  - Basis in paper: [inferred] The paper highlights an "inverse difficulty relationship where models perform well on problems humans find challenging yet fail unexpectedly on mid-difficulty items," suggesting current training methods result in brittle, non-monotonic reasoning strategies.
  - Why unresolved: Current scaling laws and RL post-training do not guarantee consistent generalization across difficulty levels, leading to unpredictable failure modes.
  - What evidence would resolve it: Demonstration of a training paradigm that produces a monotonic relationship between model confidence/accuracy and human-defined problem difficulty ratings.

## Limitations

- Text-only evaluation excludes multi-modal reasoning challenges that may require visual or auditory logical inference
- Accuracy metrics overlook reasoning validity and explanation robustness, potentially masking logical fallacies behind correct answers
- Small-model screening mechanism assumes structural reasoning failures transfer across scales, but this may not hold for capacity-limited failures

## Confidence

- **High confidence**: Format-specific performance variance and LogiEval-Hard screening mechanism validation
- **Medium confidence**: Cross-scale failure prediction claims (requires additional empirical validation)
- **Low confidence**: Claims about inverse difficulty mapping between human and model cognition (limited evidence)

## Next Checks

1. Test whether LogiEval-Hard subset maintains its difficulty signature when evaluated with models substantially larger than Qwen3-30B-A3B
2. Conduct controlled contamination analysis by comparing performance on LogiEval examples with/without matching training data overlap
3. Perform ablation study on prompt templates to verify format-specific performance differences aren't artifacts of prompting style