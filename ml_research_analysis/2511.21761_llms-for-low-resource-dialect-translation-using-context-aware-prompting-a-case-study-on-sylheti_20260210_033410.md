---
ver: rpa2
title: 'LLMs for Low-Resource Dialect Translation Using Context-Aware Prompting: A
  Case Study on Sylheti'
arxiv_id: '2511.21761'
source_url: https://arxiv.org/abs/2511.21761
tags:
- sylheti
- bangla
- translation
- sylheti-cap
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates Large Language Models (LLMs)
  for translating between Bangla and Sylheti, a low-resource dialect of Bangla. Five
  advanced LLMs (GPT-4.1, LLaMA 4, Grok 3, DeepSeek V3.2, and Gemini 2.5 Flash) were
  tested in both translation directions, revealing that LLMs struggle with dialect-specific
  vocabulary and morphology.
---

# LLMs for Low-Resource Dialect Translation Using Context-Aware Prompting: A Case Study on Sylheti

## Quick Facts
- arXiv ID: 2511.21761
- Source URL: https://arxiv.org/abs/2511.21761
- Reference count: 25
- This study shows that a context-aware prompting framework (Sylheti-CAP) significantly improves LLM translation quality for low-resource dialect pairs.

## Executive Summary
This paper tackles the challenge of translating between Bangla and Sylheti, a low-resource dialect with distinct vocabulary and morphology. The authors systematically evaluate five advanced LLMs (GPT-4.1, LLaMA 4, Grok 3, DeepSeek V3.2, and Gemini 2.5 Flash) and find they struggle with dialect-specific features due to pre-training data imbalance. To address this, they propose Sylheti-CAP, a context-aware prompting framework that embeds linguistic rules, a bilingual dictionary of 2,260 core vocabulary items and idioms, and authenticity checks directly into prompts. Extensive experiments demonstrate consistent improvements across models and metrics (BLEU, METEOR, ChrF), outperforming zero-shot, few-shot, and chain-of-thought strategies. Human evaluations and MQM analysis confirm that Sylheti-CAP reduces hallucinations, mistranslations, and awkward phrasing, establishing it as a scalable solution for dialectal and low-resource machine translation.

## Method Summary
The study evaluates five LLMs using four prompting strategies: Zero-Shot, Few-Shot (5 exemplars), Chain-of-Thought (CoT), and the proposed Sylheti-CAP framework. Sylheti-CAP integrates a linguistic rulebook (12 rules covering pronouns, copulas, verbs, negation, imperatives, passive voice, and classifiers), a bilingual dictionary of 2,260 word pairs derived from three parallel corpora, and authenticity meta-instructions guiding the model to prioritize fluency and natural spoken style. All models were run with temperature=1. The framework relies entirely on in-context learning without fine-tuning. The Vashantor corpus (2,500 parallel sentences) was used, with a 375-sentence test set. Translation quality was assessed using automatic metrics (BLEU-1, METEOR, ChrF) and human evaluation (Good/Fair/Poor ratings and MQM scores).

## Key Results
- Sylheti-CAP consistently outperforms zero-shot, few-shot, and chain-of-thought prompting across all five LLMs and all evaluation metrics.
- Translation into Sylheti (Bangla→Sylheti) is harder than into Bangla (Sylheti→Bangla), with 1.3-1.4x higher BLEU scores in the latter direction.
- Human evaluation and MQM analysis confirm that Sylheti-CAP reduces hallucinations, mistranslations, and awkward phrasing compared to baseline strategies.

## Why This Works (Mechanism)

### Mechanism 1: Structured Linguistic Knowledge Injection via Prompting
Embedding explicit grammatical rules, morphological transformations, and syntactic constraints directly into prompts compensates for LLMs' lack of dialect-specific pretraining data. The Sylheti-CAP framework encodes a "linguistic rulebook" (12 rules covering pronouns, copulas, verb conjugations, negation, imperatives, passive voice, classifiers) into the prompt. This provides the model with structured knowledge it cannot retrieve from pre-training alone. The core assumption is that LLMs can reliably follow explicit linguistic instructions when they are structured and specific, even for language variants not well-represented in training data. Evidence includes the abstract's description of the three-step framework, the specific rules listed in Step 1, and related work on dictionary-augmented approaches for other dialects. Break condition: If the linguistic rules are internally inconsistent, conflict with model's priors too strongly, or exceed context window limits, the model may ignore or misapply rules.

### Mechanism 2: Vocabulary Grounding via In-Context Dictionary
Providing a bilingual dictionary of dialect-specific vocabulary within the prompt reduces lexical hallucinations and normalization to standard language forms. A curated lexicon of 2,260 word pairs (derived from three parallel corpora) is embedded in the prompt. This anchors the model to correct dialect-specific lexical choices rather than defaulting to standard Bangla equivalents or hallucinating. The core assumption is that the model can perform accurate lexical lookup-like behavior from in-context examples even without parametric knowledge of the words. Evidence includes the abstract's mention of the 2,260-word dictionary, the dictionary construction process in Section 3, Table 4 showing correct Sylheti vocabulary usage with Sylheti-CAP versus zero-shot outputs, and similar gains reported in dictionary-augmented fine-tuning for other dialects. Break condition: If vocabulary coverage is insufficient (<80% of test tokens), or if dictionary entries are ambiguous/outdated, the model may revert to standard language forms or produce inconsistent translations.

### Mechanism 3: Authenticity Constraints Reduce Dialect Normalization Bias
Explicit instructions to prioritize "natural spoken Sylheti" over "formal Bangla" combined with authenticity checks reduce the model's tendency to normalize toward the higher-resource standard language. The prompt includes meta-instructions about tone, authenticity, and avoidance of literal/standard forms. This counteracts the pre-training bias toward standard Bangla, which is substantially more represented in training corpora. The core assumption is that LLMs have a configurable "register" or "style" that can be steered away from dominant training patterns via explicit instruction. Evidence includes Step 3's meta-instructions, Section 4.1's explanation of pre-training data imbalance, and Figure 4's MQM analysis showing reduced "awkward phrasing" penalties with Sylheti-CAP. Break condition: If the authenticity instructions are too vague or conflict with other prompt elements, the model may produce inconsistent style mixing or over-correction.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** Sylheti-CAP relies entirely on ICL—no fine-tuning occurs. Understanding how LLMs learn from prompt examples is essential for diagnosing why dictionary-in-prompt works.
  - **Quick check question:** Can you explain why a model might follow a 2,260-word dictionary in-context but fail to learn those mappings from pre-training?

- **Concept: Translation Directionality Asymmetry**
  - **Why needed here:** The paper reports 1.3-1.4x higher BLEU for Sylheti→Bangla vs. Bangla→Sylheti. Understanding why generation into low-resource targets is harder is critical for interpreting results.
  - **Quick check question:** Why might a model translate *from* a dialect better than *into* it, even with identical prompting?

- **Concept: MQM (Multidimensional Quality Metrics)**
  - **Why needed here:** The paper uses MQM for human evaluation beyond automatic metrics. MQM categories (mistranslation, omission, awkward phrasing) reveal specific failure modes that BLEU cannot.
  - **Quick check question:** If BLEU improves but MQM "awkward phrasing" increases, what might be happening?

## Architecture Onboarding

- **Component map:** Input Sentence (Bangla) -> [Step 1] Linguistic Rulebook (12 rules: pronouns, verbs, negation, syntax) -> [Step 2] Dictionary Lookup (2,260 word pairs + idioms) -> [Step 3] Authenticity Check (fluency/naturalness directives) -> Output Sentence (Sylheti)
- **Critical path:** The dictionary coverage is the bottleneck. If the source sentence contains words not in the 2,260-pair dictionary, the model must rely on rule-based inference or risk normalization to standard Bangla.
- **Design tradeoffs:**
  - Prompt length vs. coverage: Full dictionary exceeds most context windows; truncated dictionaries sacrifice lexical coverage.
  - Rule specificity vs. generalization: Overly specific rules may not transfer to unseen constructions; overly general rules may not constrain output enough.
  - No fine-tuning required (pro) but also no permanent knowledge gain (con)—each query must carry full context cost.
- **Failure signatures:**
  - Lexical fallback: Output contains standard Bangla words (e.g., "টাকা" instead of "ফইশা") → dictionary missing or not retrieved.
  - Morphological normalization: Verbs retain standard conjugations → verb transformation rules not applied.
  - Hallucination loops: Nonsensical repetitions or invented words → conflicting rules or insufficient grounding.
- **First 3 experiments:**
  1. **Ablate dictionary size:** Test with 500, 1,000, 2,260, and 3,000 word pairs to find coverage-accuracy inflection point.
  2. **Cross-dialect transfer:** Apply Sylheti-CAP template to Chittagonian (another Bangla dialect with different morphology) using same rule structure but new dictionary—measure transferability.
  3. **Rule priority testing:** Remove one rule category at a time (e.g., only pronouns, only verbs) to quantify contribution of each rule type to BLEU/MQM scores.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does Sylheti-CAP generalize effectively to other Bangla dialects or unrelated low-resource languages?
- **Basis in paper:** [explicit] The conclusion states the method is "paving the way for improved translation quality across other Bangla dialects and underrepresented languages" (Page 9).
- **Why unresolved:** This study is restricted to Sylheti; different dialects may require distinct rule structures that behave differently when injected into the prompt.
- **What evidence would resolve it:** Applying the Sylheti-CAP framework to dialects like Chittagonian or a typologically distinct low-resource language and comparing the metric gains over baselines.

### Open Question 2
- **Question:** Can incorporating fine-tuning or word embeddings provide more stable improvements than prompting alone?
- **Basis in paper:** [explicit] The authors note the framework "relies solely on prompting without model fine-tuning" and suggest embeddings could offer "a more stable and permanent improvement" (Page 9).
- **Why unresolved:** In-context knowledge is ephemeral; it is unknown if parameter updates would internalize Sylheti morphology more effectively than prompt injection.
- **What evidence would resolve it:** A comparative experiment evaluating a LoRA-fine-tuned model against the Sylheti-CAP prompting strategy on the same held-out test set.

### Open Question 3
- **Question:** To what extent does increasing the dictionary size improve translation quality before encountering prompt capacity limits?
- **Basis in paper:** [explicit] Limitations mention the current dictionary contains only "2,260 word pairs" and suggest expanding it would "enhance translation quality" (Page 9).
- **Why unresolved:** While more vocabulary is assumed to help, increasing prompt length with larger dictionaries may eventually degrade model attention or reasoning (context window saturation).
- **What evidence would resolve it:** An ablation study varying dictionary sizes (e.g., 2k, 5k, 10k entries) to observe the performance curve relative to prompt token length.

## Limitations
- The study relies entirely on in-context learning without fine-tuning, making the approach sensitive to prompt construction quality and context window constraints. The 2,260-word dictionary may not achieve sufficient coverage for all test sentences.
- The human evaluation methodology lacks explicit details on rater qualifications, inter-annotator agreement scores, and potential cultural or linguistic biases in the evaluation process.
- Translation directionality asymmetry (1.3-1.4x higher BLEU for Sylheti→Bangla) suggests inherent limitations in generating into low-resource dialects that the prompt-based approach may not fully address.

## Confidence
- **High Confidence:** The effectiveness of context-aware prompting (Sylheti-CAP) compared to zero-shot and few-shot baselines is well-supported by consistent improvements across multiple metrics (BLEU, METEOR, ChrF) and all five tested models.
- **Medium Confidence:** The superiority of Sylheti-CAP over chain-of-thought prompting is demonstrated, but the paper does not explore alternative reasoning strategies or more sophisticated prompting techniques that might achieve similar results.
- **Low Confidence:** The claim that Sylheti-CAP represents a "scalable solution" for dialectal and low-resource translation lacks validation beyond the Bangla-Sylheti language pair.

## Next Checks
1. **Dictionary Coverage Analysis:** Conduct a systematic evaluation of the 2,260-word dictionary coverage on the test set, identifying the percentage of source tokens covered versus normalized or hallucinated. Perform ablation studies with dictionaries of varying sizes (500, 1,000, 3,000 words) to determine the optimal coverage-accuracy tradeoff and identify coverage gaps that lead to performance degradation.
2. **Cross-Dialect Transferability:** Apply the Sylheti-CAP framework structure (12 rules + dictionary + authenticity check) to another Bangla dialect (e.g., Chittagonian or Noakhali) with distinct morphological features. Measure whether the rule template transfers effectively or requires dialect-specific modification, and quantify the impact of linguistic similarity on prompting effectiveness across dialects.
3. **Fine-Tuning Comparison:** Implement a lightweight fine-tuning approach using the same 2,260-word dictionary and rule set as supervision signals. Compare the computational cost (time, parameters, inference latency) and translation quality of fine-tuned versus prompted models on the same test set, controlling for model size and training data. This would validate whether the prompting approach offers genuine efficiency advantages over parametric adaptation.