---
ver: rpa2
title: Near-Exponential Savings for Mean Estimation with Active Learning
arxiv_id: '2511.05736'
source_url: https://arxiv.org/abs/2511.05736
tags:
- label
- algorithm
- mean
- partibandits
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of efficiently estimating the mean
  of a k-class random variable when labels are limited but informative covariates
  are available. The core challenge is that the optimal stratification of the data
  is unknown, and poor stratification can lead to inefficient or biased estimates.
---

# Near-Exponential Savings for Mean Estimation with Active Learning

## Quick Facts
- arXiv ID: 2511.05736
- Source URL: https://arxiv.org/abs/2511.05736
- Reference count: 40
- Primary result: Achieves near-exponentially faster error rates than classical methods for mean estimation with limited labels when covariates are predictive

## Executive Summary
This paper addresses the challenge of estimating the mean of a k-class random variable when only limited labels are available but informative covariates exist. The key insight is that optimal stratification of data for mean estimation depends on unknown structure, and poor stratification leads to inefficient estimates. The proposed method, PartiBandits, combines active learning with bandit-style exploration to learn both the optimal partition and the mean estimates within each stratum.

The method demonstrates theoretical convergence rates that are near-exponentially faster than classical approaches when covariates are predictive, bridging previously separate research strands of disagreement-based active learning and UCB bandit methods. Empirical results on synthetic and real-world electronic health record data validate the approach's effectiveness in realistic small-sample regimes.

## Method Summary
PartiBandits is a two-stage active learning algorithm that first learns an optimal partition of the data space to minimize within-stratum variance, then applies stratified sampling to estimate means. The partition learning stage uses a disagreement-based active learning subroutine to identify boundaries between classes efficiently, while the estimation stage employs a UCB-style stratified sampling method (WarmStart-UCB) to aggregate estimates from each learned stratum.

The algorithm's theoretical guarantees show error rates of Õ(ν + exp(c·(-N/log(N)))/N), where ν represents the risk of the Bayes-optimal classifier. This near-exponential improvement over classical methods depends critically on the informativeness of covariates. The approach connects UCB bandit methods with disagreement-based active learning, creating a unified framework for efficient mean estimation with limited labels.

## Key Results
- Achieves error rate of Õ(ν + exp(c·(-N/log(N)))/N), near-exponentially faster than classical methods when covariates are predictive
- Theoretical convergence rates are shown to be minimax optimal in classical settings
- Partibandits outperforms simple random sampling in empirical evaluations using synthetic data and real-world electronic health records
- Establishes theoretical connections between UCB and disagreement-based approaches in active learning

## Why This Works (Mechanism)
The algorithm leverages the structure in covariates to learn an optimal partition of the data space, then uses stratified sampling within each partition to achieve more efficient mean estimates. By actively selecting which examples to label based on disagreement in predictions, it focuses labeling effort on the most informative regions of the data space. The combination of partition learning and stratified estimation allows the method to adapt to the underlying data structure rather than relying on uniform sampling.

## Foundational Learning
- **Disagreement-based active learning**: Needed to efficiently identify boundaries between classes with minimal labels; quick check: examine the disagreement coefficient in the data
- **UCB bandit methods**: Required for balancing exploration and exploitation in the stratified sampling stage; quick check: verify confidence bounds tighten appropriately with sample size
- **Stratified sampling theory**: Fundamental for understanding variance reduction through optimal partitioning; quick check: calculate within-stratum variances for different partitions
- **Minimax optimality**: Framework for establishing theoretical convergence rates; quick check: compare achieved rates against known lower bounds

## Architecture Onboarding

**Component map**: Data -> Partition Learning (Disagreement Oracle) -> Stratified Sampling (WarmStart-UCB) -> Mean Estimate

**Critical path**: The partition learning stage is the bottleneck, as it determines the quality of subsequent stratified sampling. The algorithm requires two passes through the data: first to learn the partition, second to estimate means within each stratum.

**Design tradeoffs**: The method trades computational complexity in the partition learning stage for reduced labeling requirements and improved estimation accuracy. The two-stage approach requires sufficient memory to store intermediate results but enables more efficient use of limited labels.

**Failure signatures**: Poor performance occurs when covariates are not predictive of the outcome (high ν), when the active learning oracle fails to identify true boundaries, or when the number of strata becomes too large relative to available labels.

**First experiments**: 1) Test on synthetic data with known partition structure to verify theoretical guarantees; 2) Vary the Bayes-optimal classifier risk ν to quantify impact on near-exponential savings; 3) Compare against uniform sampling baselines across different data distributions.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance degrades significantly when covariates are not predictive of the outcome (high Bayes-optimal classifier risk ν)
- Theoretical guarantees assume access to an active learning oracle with specific properties that may not hold in practice
- Limited empirical evaluation on relatively small-scale datasets; scalability to large-scale problems untested
- Two-stage approach requires two passes over data, potentially problematic in streaming or memory-constrained settings

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical convergence rates and optimality | High |
| Near-exponential savings claim | Medium (depends on problem structure) |
| Empirical performance on tested datasets | Medium (small sample sizes, limited domains) |
| Practical applicability in large-scale settings | Low |

## Next Checks
1. Test the algorithm on larger-scale datasets with millions of examples to evaluate scalability and wall-clock performance
2. Conduct ablation studies varying the Bayes-optimal classifier risk ν to quantify the impact on the near-exponential savings
3. Implement and evaluate the algorithm in a streaming data setting where the two-stage approach must be adapted