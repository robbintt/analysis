---
ver: rpa2
title: 'CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided
  Neural Architecture Search'
arxiv_id: '2509.26037'
source_url: https://arxiv.org/abs/2509.26037
tags:
- search
- architectures
- architecture
- performance
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoLLM-NAS, a novel collaborative framework
  that integrates Large Language Models (LLMs) with two-stage Neural Architecture
  Search (NAS). The framework employs two complementary LLMs - a stateful Navigator
  LLM for adaptive search strategy generation and a stateless Generator LLM for synthesizing
  high-quality architectures - coordinated by a dedicated Coordinator module.
---

# CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided Neural Architecture Search

## Quick Facts
- arXiv ID: 2509.26037
- Source URL: https://arxiv.org/abs/2509.26037
- Authors: Zhe Li; Zhiwei Lin; Yongtao Wang
- Reference count: 40
- One-line primary result: CoLLM-NAS integrates two complementary LLMs with a Coordinator module to achieve state-of-the-art NAS results with reduced search costs across multiple two-stage NAS methods.

## Executive Summary
CoLLM-NAS introduces a novel framework that leverages Large Language Models (LLMs) to enhance Neural Architecture Search (NAS) efficiency and performance. The framework uses a stateful Navigator LLM for adaptive search strategy generation and a stateless Generator LLM for synthesizing high-quality architectures, coordinated by a dedicated Coordinator module. This collaborative approach enables CoLLM-NAS to exploit both the LLMs' inherent knowledge and iterative feedback from search trajectories, consistently outperforming existing NAS methods and conventional search algorithms across diverse search spaces.

## Method Summary
CoLLM-NAS integrates two complementary LLMs with a Coordinator module to guide the search process. The Navigator LLM adapts search strategies based on historical trajectories, while the Generator LLM synthesizes architectures leveraging its knowledge of neural patterns. The Coordinator orchestrates their collaboration, ensuring coherent and efficient exploration. This two-stage approach is compatible with existing NAS methods like OFA, SPOS, and AutoFormer, and achieves state-of-the-art results on ImageNet and NAS-Bench-201 while significantly reducing search costs.

## Key Results
- CoLLM-NAS outperforms existing NAS methods and conventional search algorithms on ImageNet and NAS-Bench-201.
- Achieves new state-of-the-art results across multiple two-stage NAS methods (OFA, SPOS, AutoFormer).
- Consistently enhances performance and efficiency, significantly reducing search costs compared to baselines.

## Why This Works (Mechanism)
CoLLM-NAS works by leveraging the complementary strengths of two specialized LLMs within a coordinated search framework. The Navigator LLM uses stateful reasoning to adaptively guide search strategies based on historical performance and trajectories, enabling intelligent exploration of the architecture space. The Generator LLM, being stateless, synthesizes high-quality architectures by drawing on its broad pretraining knowledge of neural patterns and design principles. The Coordinator module ensures that both LLMs work in harmony, balancing exploration and exploitation, and integrating iterative feedback to refine the search process. This synergy allows CoLLM-NAS to surpass the limitations of traditional gradient-based or heuristic NAS methods, achieving superior efficiency and performance.

## Foundational Learning
- **Neural Architecture Search (NAS)**: The automated process of discovering optimal neural network architectures; needed to understand the problem CoLLM-NAS addresses, and quick check is familiarity with NAS benchmarks and search spaces.
- **Large Language Models (LLMs)**: Pretrained models capable of understanding and generating human-like text; required to grasp how LLMs contribute knowledge and reasoning to NAS, and quick check is understanding of LLM capabilities and limitations.
- **Coordinator Module**: A central orchestrator that manages interactions between specialized components; essential for understanding how CoLLM-NAS ensures coherent collaboration, and quick check is recognizing the role of coordination in multi-agent systems.
- **Stateful vs Stateless LLMs**: Distinction between models that retain context across interactions (stateful) and those that do not (stateless); needed to appreciate the design choice in CoLLM-NAS, and quick check is the ability to identify when each type is advantageous.
- **Iterative Feedback**: The process of refining strategies based on past performance; critical for understanding how CoLLM-NAS improves over time, and quick check is recognizing the role of feedback loops in optimization.
- **Search Strategy Adaptation**: The dynamic adjustment of search methods based on ongoing results; required to see how CoLLM-NAS maintains efficiency and effectiveness, and quick check is familiarity with adaptive optimization techniques.

## Architecture Onboarding
- **Component Map**: Navigator LLM -> Coordinator -> Generator LLM -> NAS Engine
- **Critical Path**: Navigator LLM generates adaptive search strategy → Coordinator evaluates and refines → Generator LLM synthesizes architectures → NAS Engine evaluates and feeds back → Navigator LLM updates strategy
- **Design Tradeoffs**: Using two complementary LLMs balances adaptability and knowledge breadth, but increases coordination complexity; Coordinator ensures coherence but may become a bottleneck if not scalable.
- **Failure Signatures**: Poor coordination leads to conflicting guidance; overreliance on LLM pretraining knowledge may cause overfitting to common patterns; lack of adaptation in novel search spaces reduces effectiveness.
- **First Experiments**:
  1. Validate individual contributions of Navigator and Generator LLMs via ablation.
  2. Test Coordinator module's decision-making on a simplified search space.
  3. Benchmark CoLLM-NAS against a baseline NAS method on a standard dataset.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to specific two-stage NAS methods (OFA, SPOS, AutoFormer) and benchmarks, raising questions about generalizability.
- Reliance on state-of-the-art LLMs may hinder reproducibility and risk overfitting to common architecture patterns.
- Performance improvements not extensively validated on out-of-distribution tasks or under resource constraints.

## Confidence
- **High confidence** in the core innovation of using two complementary LLMs for adaptive and generative NAS tasks, supported by consistent performance gains across multiple baselines.
- **Medium confidence** in the claimed state-of-the-art results, given the limited scope of benchmarks and search spaces evaluated.
- **Low confidence** in the scalability and adaptability of the framework to highly novel or specialized domains, due to the lack of extensive cross-domain validation.

## Next Checks
1. Evaluate CoLLM-NAS on out-of-distribution search spaces and real-world, non-standard datasets to test generalization and robustness.
2. Conduct ablation studies to isolate the contributions of the Navigator and Generator LLMs, and assess the impact of different LLM choices on search efficiency and architecture quality.
3. Perform resource-constrained experiments to quantify search cost reductions and practical deployment benefits compared to existing NAS methods.