---
ver: rpa2
title: 'TokAlign: Efficient Vocabulary Adaptation via Token Alignment'
arxiv_id: '2506.03523'
source_url: https://arxiv.org/abs/2506.03523
tags:
- uni00000013
- uni00000011
- language
- uni00000014
- vocabulary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting large language models
  (LLMs) to new domains or languages by proposing TokAlign, an efficient method for
  vocabulary adaptation. TokAlign aligns token IDs between source and target vocabularies
  using token-token co-occurrence matrices, enabling the replacement of an LLM's vocabulary
  without requiring additional training of a hypernetwork.
---

# TokAlign: Efficient Vocabulary Adaptation via Token Alignment

## Quick Facts
- arXiv ID: 2506.03523
- Source URL: https://arxiv.org/abs/2506.03523
- Reference count: 34
- This paper proposes TokAlign, an efficient method for vocabulary adaptation that aligns token IDs between source and target vocabularies using token-token co-occurrence matrices, enabling vocabulary replacement without hypernetwork training.

## Executive Summary
TokAlign addresses the challenge of adapting large language models to new domains or languages by aligning token vocabularies without requiring additional training of a hypernetwork. The method leverages token-token co-occurrence matrices to learn cross-tokenizer alignments, enabling effective cross-lingual knowledge transfer and token-level distillation between models. Experimental results demonstrate significant improvements in multilingual text compression rates and vocabulary initialization, reducing perplexity from 3.4e2 to 1.2e2 after initialization, while achieving 4.4% better performance than sentence-level distillation.

## Method Summary
TokAlign aligns token IDs between source and target vocabularies using GloVe-trained token embeddings derived from token-token co-occurrence matrices. The method tokenizes a multilingual corpus with both tokenizers, trains GloVe embeddings (300-dim) on each tokenized version, and aligns tokens via cosine similarity. Direct parameter transfer occurs for shared tokens, while non-overlapping tokens are initialized from their most similar source tokens. A two-stage tuning approach is employed: Stage 1 fine-tunes only embedding and lm_head parameters (50% of steps), while Stage 2 unlocks full model fine-tuning (remaining 50%). The method achieves 98% of vanilla performance with as few as 5k fine-tuning steps.

## Key Results
- Reduces perplexity from 3.4e2 to 1.2e2 after vocabulary initialization
- Enables 4.4% better performance than sentence-level distillation through token-level distillation
- Restores 98% of vanilla performance with as few as 5k fine-tuning steps
- Achieves >90% vocabulary coverage for most tested tokenizers with 1B token corpus

## Why This Works (Mechanism)

### Mechanism 1: Global Co-occurrence Preserves Semantic Structure Across Tokenizers
Different tokenizers applied to the same corpus produce token sequences that, despite differing IDs, encode similar semantic relationships through token-token co-occurrence patterns. GloVe factorizes the global co-occurrence matrix to produce 300-dimensional token embeddings that capture statistical regularities relatively stable regardless of how the vocabulary is segmented. Token semantics emerge from contextual distribution rather than surface form alone.

### Mechanism 2: Cosine Similarity Creates Valid Token Mapping Without Bilingual Dictionaries
Pairwise cosine similarity between GloVe embeddings from two tokenizers produces an effective one-to-one alignment matrix for parameter initialization. For each target token, the most similar source token is retrieved by cosine similarity. Shared tokens bypass alignment and map directly. The resulting matrix drives embedding and lm_head parameter transfer, with proximity in embedding space correlating with functional equivalence in the downstream model.

### Mechanism 3: Progressive Two-Stage Tuning Prevents Catastrophic Interference
Freezing internal layers while adapting only embedding and lm_head parameters in the first stage stabilizes training under new vocabularies. Stage 1 (50% of steps) tunes only vocabulary-related parameters, treating them as adapters. Stage 2 (50%) unlocks full model fine-tuning once the embedding space has adjusted. Well-trained internal representations are robust to input distribution shift if the embedding adapter is calibrated first.

## Foundational Learning

- **Distributional Semantics (GloVe mechanism)**: Why needed here - TokAlign assumes co-occurrence statistics encode semantics; understanding why helps diagnose when alignment will fail. Quick check: Can you explain why "king" and "queen" might cluster near each other in a GloVe space trained on Wikipedia, but not in a space trained solely on Python code?

- **Subword Tokenization (BPE/SentencePiece)**: Why needed here - The method transfers between vocabularies built via different segmentation strategies; understanding subword granularity clarifies why direct string matching is insufficient. Quick check: How does a BPE vocabulary handle the word "unhappiness" differently than a character-level tokenizer?

- **Knowledge Distillation (Token-level vs. Sentence-level)**: Why needed here - TokAlign enables token-level distillation across models by unifying vocabularies; knowing the difference explains the 4.4% gain. Quick check: Why would matching teacher logits at each token position transfer more fine-grained knowledge than matching only final sentence outputs?

## Architecture Onboarding

- **Component map**: Corpus sampler -> Tokenizer pair (source/target) -> GloVe trainer -> Alignment matrix (cosine similarity) -> Embedding/lm_head initializer -> Two-stage trainer (Stage 1: embed+head only; Stage 2: full model)

- **Critical path**: 1. Tokenize corpus with both tokenizers → parallel token ID sequences; 2. Train GloVe on each tokenized version → two embedding matrices; 3. Compute pairwise cosine similarity → alignment matrix; 4. Initialize target embeddings/lm_head from source via alignment; 5. Stage 1 fine-tune (embed+head, 2.5k steps typical); 6. Stage 2 full fine-tune (2.5k steps, restore ~98% vanilla performance)

- **Design tradeoffs**: Corpus size vs. coverage (1B tokens achieves >90% coverage); Vocabulary overlap vs. alignment quality (higher overlap accelerates convergence); Two-stage vs. direct tuning (two-stage more stable at higher learning rates but adds complexity)

- **Failure signatures**: Loss spike at step 0 (alignment matrix noisy, increase GloVe corpus); Perplexity stalls above 2.0 (learning rate too high, reduce to 2e-5); English performance degrades after multilingual transfer (too much LAT on low-resource languages, increase English proportion); Token-level distillation underperforms sentence-level (vocabularies not fully aligned, verify BLEU-1 > 40%)

- **First 3 experiments**: 1. Baseline alignment check - train GloVe on 100M tokens, align Pythia→Qwen2, measure BLEU-1 and BERTScore, confirm negative correlation with initial training loss; 2. Ablate two-stage tuning - compare Stage 1→2 vs. direct full tuning on Pythia→Gemma at learning rate 8e-5, log loss curves and final perplexity; 3. Cross-lingual transfer sanity check - replace Pythia tokenizer with Qwen2, run 2k LAT steps on CulturaX subset (5 languages), evaluate perplexity reduction vs. Focus baseline

## Open Questions the Paper Calls Out

- Can TokAlign be effectively extended to replace the vocabulary of multi-modal models by aligning different modal tokens? The authors state this extension is possible but untested beyond text-based LLMs.

- Can incorporating meta-learning into the two-stage tuning process significantly speed up convergence? The authors propose this as a future direction to accelerate the current 5k-step restoration process.

- How robust is TokAlign when the original pre-training corpus distribution is unavailable and must be reconstructed via vocabulary inference? The method hasn't been tested on models with closed training data like Mistral using only proxy reconstructed corpora.

## Limitations

- Vocabulary overlap sensitivity - performance degrades significantly with low vocabulary overlap (e.g., 6.23% for Gemma vs. Pythia), though convergence is still possible
- Corpus dependency - effectiveness depends on GloVe corpus capturing needed semantic relationships; insufficient or domain-mismatched data may cause failure
- Two-stage tuning complexity - while providing stability, it adds hyperparameter complexity and the full hyperparameter space hasn't been explored

## Confidence

**High confidence:** 
- GloVe co-occurrence embeddings work for cross-tokenizer alignment
- Token-level distillation across vocabularies is feasible with 4.4% improvement
- Multilingual vocabulary adaptation reduces perplexity from 3.4e2 to 1.2e2

**Medium confidence:**
- 5k-step restoration claim holds across tested model scales and tokenizer pairs
- Two-stage tuning is universally superior to direct fine-tuning
- Method generalizes to tokenizers beyond those tested

**Low confidence:**
- Performance claims in low-resource language settings are robust to dataset variations
- Method works equally well for specialized domains without corpus adaptation
- Computational efficiency claims scale linearly with model size

## Next Checks

1. **Extreme vocabulary divergence test:** Apply TokAlign to tokenizers with <1% overlap (character-level vs. BPE) and measure convergence behavior, tracking loss curves, final perplexity, and the relationship between vocabulary overlap and steps required to restore 95% of vanilla performance.

2. **Corpus size sensitivity analysis:** Train GloVe on progressively smaller corpora (100M, 500M, 1B, 2B tokens) and measure alignment quality and downstream performance for the same tokenizer pair, identifying the minimum corpus size that maintains <10% degradation in final perplexity.

3. **Ablation of two-stage tuning components:** Compare Stage 1→2, direct full fine-tuning, three-stage tuning, and embedding-only fine-tuning followed by random lm_head initialization using identical learning rates and step counts, measuring both convergence speed and final perplexity.