---
ver: rpa2
title: 'Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI'
arxiv_id: '2508.08270'
source_url: https://arxiv.org/abs/2508.08270
tags:
- medical
- data
- training
- doctor
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Doctor Sun, a bilingual (Chinese-English)
  multimodal large language model specialized for biomedical AI. The authors address
  the limitations of existing multimodal biomedical models that rely on general-purpose
  LLMs and struggle with domain-specific medical knowledge and the text-image relationship.
---

# Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI

## Quick Facts
- **arXiv ID**: 2508.08270
- **Source URL**: https://arxiv.org/abs/2508.08270
- **Reference count**: 40
- **Primary result**: Bilingual (Chinese-English) multimodal LLM achieving superior zero-shot performance on medical VQA and QA benchmarks, outperforming state-of-the-art models like RadFM and LLaVA-Med.

## Executive Summary
Doctor Sun is a bilingual (Chinese-English) multimodal large language model designed to overcome the limitations of existing biomedical AI systems that struggle with domain-specific medical knowledge and text-image relationships. The model integrates a pre-trained vision encoder with a medical LLM using a two-stage training strategy: feature alignment and instruction tuning, applied to a curated bilingual medical multimodal dataset. Experiments show Doctor Sun achieves superior zero-shot performance across multiple medical benchmarks, excelling in both visual question answering and text-based question answering tasks. The model's performance, combined with its reduced parameter count, makes it a promising tool for practical biomedical applications, with the model, code, and dataset publicly available for further research.

## Method Summary
Doctor Sun employs a two-stage training strategy to integrate a CLIP vision encoder with a medical-specialized LLM (Llama-3.1-8B). First, the LLM backbone undergoes two-phase fine-tuning: initial medical data SFT followed by mixed general-medical data fine-tuning to prevent catastrophic forgetting. Second, multimodal training involves (1) feature alignment where a linear projector maps image features to LLM space using frozen components, and (2) instruction tuning where LoRA adapters update the vision encoder and the LLM is fully fine-tuned on VQA pairs with a 1:0.5 domain-to-generic data ratio.

## Key Results
- Doctor Sun outperforms state-of-the-art models like RadFM, LLaVA-Med, and LLaVA-Med-v1.5-mistral-7b on zero-shot medical VQA and QA benchmarks.
- The model achieves superior performance while using only 64% of the parameters compared to RadFM, demonstrating efficiency.
- Ablation studies show that using LoRA adapters to adapt the vision encoder to medical features improves VQA performance by approximately 26% over keeping it frozen.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating domain-specific knowledge into the visual encoder, rather than keeping it frozen, improves medical Visual Question Answering (VQA) performance.
- **Mechanism:** The authors apply LoRA (Low-Rank Adaptation) adapters to the CLIP vision encoder during the instruction-tuning phase. This allows the encoder to update its weights specifically for medical feature extraction (e.g., pathology slides) without losing the general visual priors from pre-training, bridging the gap between generic visual features and medical semantics.
- **Core assumption:** Assumes that the pre-trained general vision features are insufficient for fine-grained medical diagnosis and that a simple linear projector cannot fully bridge this semantic gap.
- **Evidence anchors:**
  - [abstract] Mentions "integrates a pre-trained vision encoder... focusing on feature alignment and instruction tuning."
  - [section: Model Architecture] States "LORA adapters are added to the CLIP vision encoder to enable the vision model to learn more fine-grained medical image features."
  - [corpus] Weak relevance; neighbor papers focus on multimodal reasoning datasets rather than adapter-based architectural tuning.
- **Break condition:** If the LoRA rank is too low to capture complex medical features, or if the medical dataset is too small/noisy, causing the adapter to overfit or diverge.

### Mechanism 2
- **Claim:** A two-stage training strategy (Feature Alignment $\to$ Instruction Tuning) is necessary to establish a functional cross-modal connection before optimizing for diagnostic reasoning.
- **Mechanism:** In Stage 1 (Feature Alignment), the model trains a linear projector to map image embeddings to the LLM's text space using image-caption pairs (e.g., PMC-OA), keeping backbones frozen. In Stage 2 (Instruction Tuning), the model trains on VQA tasks with unfrozen components (using LoRA) to optimize for dialogue and diagnosis.
- **Core assumption:** Assumes that training the projector and the diagnostic reasoning simultaneously leads to training instability or suboptimal convergence.
- **Evidence anchors:**
  - [abstract] "conducts two-stage training... focusing on feature alignment and instruction tuning."
  - [section: Biomedical Image-Text Feature Alignment] Describes the auto-regressive loss function used to align visual and text embeddings before instruction tuning.
- **Break condition:** If the alignment dataset (image-caption pairs) has a different distribution than the instruction dataset (VQA pairs), the projector may fail to map relevant visual features required for the downstream questions.

### Mechanism 3
- **Claim:** Mixing general-purpose data with domain-specific data during training preserves general reasoning capabilities (preventing catastrophic forgetting) while maintaining high medical accuracy.
- **Mechanism:** The authors utilize a "Two-stage Hybrid Fine-tuning" approach for the LLM backbone and maintain a specific ratio of general-to-medical data (e.g., 1:0.5) in multimodal instruction tuning. This prevents the model from over-fitting to medical jargon and losing the ability to process natural language instructions effectively.
- **Core assumption:** Assumes that domain-specific fine-tuning degrades general reasoning abilities (catastrophic forgetting), which are essential for complex diagnostic logic.
- **Evidence anchors:**
  - [section: Two-stage Hybrid Fine-tuning] "Fine-tuning LLMs solely with medical data introduces a high risk of catastrophic forgetting."
  - [section: Experiments on General and Specialized Abilities] Shows that models trained only on domain data (E1-V1) suffer in general benchmarks (MMbench), while mixed ratios improve stability.
- **Break condition:** If the ratio of general data is too high, it dilutes the domain-specific knowledge; if too low, the model loses instruction-following capabilities.

## Foundational Learning

- **Concept: Vision-Language Projectors (e.g., Linear Layers/MLPs)**
  - **Why needed here:** Doctor Sun relies on a trainable linear layer to bridge the dimensionality gap between the CLIP vision encoder and the Medical LLM.
  - **Quick check question:** How does a trainable linear layer transform the dimension and semantic space of image tokens to match the LLM's input requirements?

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The paper explicitly addresses this by mixing general data with medical data to ensure the model doesn't lose its ability to interact naturally while learning medicine.
  - **Quick check question:** What happens to a neural network's performance on Task A (general reasoning) if it is trained exclusively on Task B (medical diagnosis) without mitigation strategies?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) / LoRA**
  - **Why needed here:** The authors use LoRA on the vision encoder to adapt it to medical images without the computational cost of full fine-tuning.
  - **Quick check question:** How does LoRA modify weight updates (using low-rank matrices) to reduce memory usage compared to full fine-tuning?

## Architecture Onboarding

- **Component map:**
  - **Input:** Medical Image + Text Query.
  - **Vision Encoder:** CLIP (ViT-L/14) + **LoRA Adapters** (Trainable).
  - **Projector:** Single Linear Layer (Trainable).
  - **Language Backbone:** Llama-3.1-8B (Medical-specialized via SFT/DPO) + **Full Fine-tuning**.
  - **Output:** Generated Text Answer.

- **Critical path:**
  1.  **Data Prep:** Curate SunMed-VL (Bilingual medical VQA/Captioning).
  2.  **Backbone Prep:** Train Medical LLM (Llama-3.1) using Two-stage Hybrid Fine-tuning (Medical $\to$ Mixed).
  3.  **Multimodal Stage 1:** Freeze LLM & Vision; Train Projector on Image-Text pairs (Alignment).
  4.  **Multimodal Stage 2:** Unfreeze Vision (LoRA) & LLM; Train on VQA pairs (Instruction Tuning).

- **Design tradeoffs:**
  - **Frozen vs. Adapted Vision:** The paper argues for unfreezing the vision encoder (DocS-M) over keeping it frozen (DocS-FV). While frozen is safer, adapted vision improves VQA performance by $\sim$26% (Section: Results) by learning medical textures.
  - **Parameter Count vs. Performance:** Doctor Sun achieves better results than RadFM with only 64% of the parameters, favoring efficiency.

- **Failure signatures:**
  - **Low Recall/Empty Answers:** Sign of "Catastrophic Forgetting" or poor instruction tuning—model reverts to medical knowledge but fails to answer the specific prompt (seen in E1-V1 ablation).
  - **Visual Hallucination:** If Feature Alignment (Stage 1) is skipped or under-trained, the LLM ignores image features.
  - **Language Confusion:** If bilingual data (SunMed-VL) is not cleaned (Section: Data Pre-processing), the model may generate mixed-language nonsense.

- **First 3 experiments:**
  1.  **Vision Encoder Ablation:** Train two models—one with frozen CLIP, one with LoRA-adapted CLIP—on VQA-Rad to reproduce the 26% performance gap cited in the paper.
  2.  **Data Mixing Ratio Test:** Run a sweep on the "General vs. Medical" data ratio (e.g., 1:0, 1:0.2, 1:0.5) during instruction tuning to observe the trade-off between MMbench (General) and VQA-Rad (Medical) scores.
  3.  **Backbone Validation:** Swap the Medical LLM backbone for a generic Llama-3.1 model to verify the contribution of the domain-specific pre-training (expect a ~4% drop as per DocS-GL results).

## Open Questions the Paper Calls Out
- **Question:** Does the optimal ratio of domain-specific to generic data identified for Doctor Sun transfer effectively to non-biomedical specialized domains?
  - **Basis in paper:** [explicit] The conclusion states that future work involves testing the impact of integrating domain-specific and generic data on multimodal models in "other specialized domains" to generalize the findings.
  - **Why unresolved:** The current experimental scope is strictly limited to the biomedical field, leaving the universality of the training strategy unproven.
  - **What evidence would resolve it:** Applying the same data mixing ratios to models in fields like law or engineering and observing if the balance between general reasoning and domain expertise holds.

- **Question:** What is Doctor Sun's reliability and error rate when deployed in real-world clinical workflows compared to its benchmark performance?
  - **Basis in paper:** [explicit] The discussion explicitly notes that the model "has not been evaluated in a real-world environment" and is not yet suitable for direct clinical application despite high benchmark scores.
  - **Why unresolved:** Standardized benchmarks like VQA-Rad may not capture the noise, ambiguity, and complexity of actual patient cases and hospital systems.
  - **What evidence would resolve it:** Prospective studies or clinical trials where the model assists physicians with actual case data, measuring diagnostic accuracy and user trust.

- **Question:** How does the performance trade-off between using a general-purpose vision encoder with adapters versus a fully domain-pretrained vision encoder evolve with larger model scales?
  - **Basis in paper:** [inferred] The paper uses a CLIP encoder with LoRA adapters to bridge the domain gap, acknowledging that general encoders struggle with medical features, but does not compare this against a fully medical-pretrained vision model at scale.
  - **Why unresolved:** It is unclear if adapting a general encoder is sufficient as model size increases, or if a specialized pre-trained vision backbone becomes necessary to maintain feature alignment efficiency.
  - **What evidence would resolve it:** Ablation studies comparing the current adapter approach against a vision encoder trained from scratch on medical data within a larger parameter regime.

## Limitations
- **Real-world deployment uncertainty:** The model's reliability and error rates in actual clinical workflows remain untested despite strong benchmark performance.
- **Generalizability constraint:** The optimal training strategies (data mixing, adapter tuning) are validated only within the biomedical domain, with no evidence for other specialized fields.
- **Reproducibility barrier:** Key hyperparameters for training stages (learning rates, batch sizes, LoRA configuration) are not specified, hindering exact replication.

## Confidence
- **High Confidence:** The two-stage training strategy (Feature Alignment → Instruction Tuning) is well-motivated and produces measurable performance improvements. The catastrophic forgetting mitigation through data mixing is theoretically sound and empirically validated.
- **Medium Confidence:** The specific performance gains over competing models (RadFM, LLaVA-Med) are impressive but difficult to fully verify without hyperparameter details. The claim that LoRA-adapted vision encoders outperform frozen ones is supported by ablation studies, but the magnitude of improvement may depend on implementation specifics.
- **Low Confidence:** The generalizability of Doctor Sun to real-world clinical deployment scenarios is not addressed. The model's performance on rare conditions, edge cases, or in different healthcare systems remains unknown.

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Systematically vary the data mixing ratio (general:medical) and LoRA rank during instruction tuning to quantify the trade-off between general reasoning and medical specialization, confirming the reported optimal ratio of 1:0.5.

2. **Vision Encoder Ablation Replication:** Train two identical Doctor Sun variants differing only in whether the CLIP vision encoder is frozen or uses LoRA adapters, then evaluate zero-shot on VQA-Rad to independently verify the claimed 26% performance gap.

3. **Backbone Dependency Test:** Replace the medical-specialized LLM backbone with a generic Llama-3.1 model while keeping all other components identical, then measure the degradation in medical VQA performance to isolate the contribution of domain-specific pre-training.