---
ver: rpa2
title: Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction
arxiv_id: '2511.00537'
source_url: https://arxiv.org/abs/2511.00537
tags:
- sentiment
- cisea-mrfe
- across
- classification
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses sentiment analysis using deep learning and
  pre-trained language models, which often underperform in scenarios involving nuanced
  emotional cues, domain shifts, and imbalanced sentiment distributions. The proposed
  CISEA-MRFE framework integrates Contextual Instruction (CI), Semantic Enhancement
  Augmentation (SEA), and Multi-Refined Feature Extraction (MRFE) to overcome these
  challenges.
---

# Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction

## Quick Facts
- **arXiv ID**: 2511.00537
- **Source URL**: https://arxiv.org/abs/2511.00537
- **Reference count**: 40
- **Primary result**: Proposed CISEA-MRFE framework achieves up to 4.6% accuracy improvement on IMDb, 6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon

## Executive Summary
This paper addresses sentiment analysis using deep learning and pre-trained language models, which often underperform in scenarios involving nuanced emotional cues, domain shifts, and imbalanced sentiment distributions. The proposed CISEA-MRFE framework integrates Contextual Instruction (CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature Extraction (MRFE) to overcome these challenges. CI injects domain-aware directives to guide sentiment disambiguation, SEA improves robustness through sentiment-consistent paraphrastic augmentation, and MRFE combines a Scale-Adaptive Depthwise Encoder (SADE) for multi-scale feature specialization with an Emotion Evaluator Context Encoder (EECE) for affect-aware sequence modeling. Experimental results on four benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb, 6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon.

## Method Summary
The CISEA-MRFE framework integrates three core components: Contextual Instruction (CI) that prepends domain-aware directives to guide sentiment disambiguation, Semantic Enhancement Augmentation (SEA) that generates sentiment-preserving paraphrases using T5 for training robustness, and Multi-Refined Feature Extraction (MRFE) combining Scale-Adaptive Depthwise Encoder (SADE) for multi-scale local pattern extraction with Emotion Evaluator Context Encoder (EECE) for affect-aware sequence modeling. The SADE module uses depthwise convolutions with kernels [1,3,5,7] for parameter-efficient feature extraction, while EECE employs BiLSTM with attention and emotion gating to capture long-range affective dependencies. SEA operates offline during training, augmenting data with T5-generated paraphrases validated for sentiment consistency.

## Key Results
- CISEA-MRFE achieves 97.8% accuracy on Twitter, representing a 30.3% relative improvement over BERT baseline (67.5%)
- Accuracy improvements across all four benchmark datasets: 4.6% on IMDb, 6.5% on Yelp, 4.1% on Amazon
- Ablation studies confirm CI contributes 8.1% performance gain on Twitter, SADE contributes 2.5% on Yelp, and EECE contributes 1.9% on IMDb
- T5-small provides 38% speed improvement over T5-large with only 2.3% accuracy trade-off in SEA augmentation

## Why This Works (Mechanism)

### Mechanism 1: Contextual Instruction for Disambiguation
Prepending domain-aware directives improves sentiment classification by guiding the encoder toward sentiment-relevant cues. Instruction templates prime BERT's attention to focus on emotion-bearing tokens and domain context before feature extraction, reducing ambiguity in expressions that require contextual grounding.

### Mechanism 2: Sentiment-Preserving Paraphrastic Augmentation
T5-based paraphrasing increases semantic diversity while maintaining polarity, improving generalization to varied linguistic patterns. SEA masks adjectives/nouns, generates top-k paraphrase candidates, applies instruction templates for stylistic clarity, and validates sentiment consistency through a classifier-filter loop.

### Mechanism 3: Multi-Scale Local + Emotion-Aware Global Feature Fusion
Combining depthwise convolutions for n-gram patterns with BiLSTM-based emotion evaluation captures both localized sentiment cues and long-range affective dependencies. SADE applies grouped depthwise convolutions with kernels [1,3,5,7] to extract multi-scale local patterns, while EECE uses BiLSTM with bidirectional context, attention pooling, and emotion projection with negation-aware modulation.

## Foundational Learning

- **Concept: Prompt/Instruction-Based Learning**
  - Why