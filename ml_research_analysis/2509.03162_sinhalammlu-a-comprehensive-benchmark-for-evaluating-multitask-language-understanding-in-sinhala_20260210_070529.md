---
ver: rpa2
title: 'SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language Understanding
  in Sinhala'
arxiv_id: '2509.03162'
source_url: https://arxiv.org/abs/2509.03162
tags:
- language
- questions
- b-chat
- qwen2
- llama-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SinhalaMMLU, the first multiple-choice question-answering
  benchmark designed for Sinhala, a low-resource language. The dataset contains over
  7,000 questions across six domains and 30 subjects, aligned with the Sri Lankan
  national curriculum and including culturally grounded content.
---

# SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language Understanding in Sinhala

## Quick Facts
- arXiv ID: 2509.03162
- Source URL: https://arxiv.org/abs/2509.03162
- Reference count: 40
- Top models achieve 67% (Claude 3.5 Sonnet) and 62% (GPT-4o) accuracy on 7,044 Sinhala multiple-choice questions

## Executive Summary
This paper introduces SinhalaMMLU, the first comprehensive multiple-choice question-answering benchmark for Sinhala, a low-resource language. The dataset contains over 7,000 questions across six domains and 30 subjects aligned with the Sri Lankan national curriculum, including culturally grounded content. Evaluations of 26 large language models show that while top models like Claude 3.5 Sonnet and GPT-4o achieve the highest average accuracies, overall performance remains limited, especially in culturally rich domains such as Humanities. Results highlight substantial room for improvement in adapting LLMs to low-resource and culturally specific contexts.

## Method Summary
The benchmark was constructed by scraping questions from the Sri Lankan government's e-Thaksalawa platform, manually annotating them, and removing duplicates with cosine similarity >95%. The dataset includes 7,044 questions across 30 subjects in six domains, with native Sinhala content and a subset of translated GlobalMMLU questions for comparison. Evaluation used zero-shot and three-shot prompting with subject-inclusive prompts in Sinhala. Open models used probability-based answer selection with 4-bit quantization, while closed APIs used regex extraction on first tokens.

## Key Results
- Claude 3.5 Sonnet achieved highest average accuracy at 67%, followed by GPT-4o at 62%
- Models struggle significantly in culturally rich domains, with Claude 3.5 Sonnet dropping 28.22 percentage points on cultural subset
- Native STEM questions rated 97.3% naturalness vs. 71.07% for translated questions
- No significant improvement from few-shot prompting on instruction-tuned models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Culturally grounded questions expose a performance gap that scales inconsistently with model size.
- **Mechanism:** LLMs trained predominantly on global or anglocentric corpora develop weaker representations for region-specific knowledge (e.g., Sri Lankan history, Buddhist civilization, indigenous dance). When evaluation requires this knowledge, model confidence and accuracy diverge more sharply than on fact-based, standardized subjects.
- **Core assumption:** Cultural knowledge requires exposure to native-language sources, not just multilingual transfer.
- **Evidence anchors:**
  - [abstract]: "models struggle in culturally rich domains such as the Humanities, revealing substantial room for improvement"
  - [section 8, Table 9]: Claude 3.5 Sonnet drops from 67.65% to 39.43% on culturally grounded subset (-28.22 points)
  - [corpus]: Cetvel benchmark for Turkish similarly combines "culturally relevant content" with task diversity
- **Break condition:** If models were trained on large-scale native Sinhala web corpora with cultural coverage, this gap would likely narrow.

### Mechanism 2
- **Claim:** Translated benchmarks misrepresent both linguistic naturalness and technical terminology accuracy.
- **Mechanism:** Machine translation maps surface forms but fails to preserve curriculum-aligned technical vocabulary (e.g., "plasmolysis" transliterated vs. proper Sinhala term). This introduces distribution shift between training and evaluation.
- **Core assumption:** Technical terms have standardized native equivalents that differ from transliterations.
- **Evidence anchors:**
  - [abstract]: "many rely on automatic translation, which can introduce errors and misrepresent the original cultural context"
  - [section 7, Table 8]: Native STEM questions rated 97.3% naturalness vs. 71.07% for translated GlobalMMLU-si
  - [corpus]: Evaluating Turkish benchmarks (arXiv:2504.09714) similarly questions "linguistic and cultural suitability" of translated datasets
- **Break condition:** If translation quality were verified by domain experts with terminology glossaries, this degradation would reduce.

### Mechanism 3
- **Claim:** Few-shot prompting provides marginal gains on instruction-tuned models for low-resource languages.
- **Mechanism:** Instruction-tuned models optimize for dialogue patterns rather than in-context learning on unfamiliar scripts. Without strong prior representations, additional examples don't shift the decision boundary meaningfully.
- **Core assumption:** The few-shot examples are sufficiently representative and correctly formatted.
- **Evidence anchors:**
  - [section 5, Table 4]: Qwen2.5-72B-Chat improves only 0.06% from 0-shot to 3-shot; LLaMA-3.1-70B-Chat degrades
  - [corpus]: Weak/no direct corpus evidence on few-shot for Sinhala specifically
- **Break condition:** If few-shot examples were curriculum-matched and culturally aligned, gains might be larger—but this is untested.

## Foundational Learning

- **Concept: Low-resource language evaluation**
  - **Why needed here:** Sinhala has limited NLP tooling and training data; benchmark design must account for tokenizer inefficiency and script complexity.
  - **Quick check question:** Can you explain why a 4-bit quantized model might underperform on Sinhala compared to English?

- **Concept: Cultural grounding vs. factual knowledge**
  - **Why needed here:** The paper distinguishes subjects requiring rote facts (Citizenship Education: 77.55%) from those requiring cultural context (Humanities: 66.15% for Claude).
  - **Quick check question:** Why would "Buddhist civilization" questions be harder than "Geography" for a model trained mostly on English web text?

- **Concept: Multiple-choice evaluation limitations**
  - **Why needed here:** SinhalaMMLU excludes open-ended, multimodal, and mathematical reasoning; results don't generalize to generative tasks.
  - **Quick check question:** What types of student ability would be missed by an MCQ-only benchmark?

## Architecture Onboarding

- **Component map:** e-Thaksalawa (government platform) → OCR/scraping → manual annotation → deduplication (cosine >95% removal) → Hugging Face dataset
- **Critical path:** Load dataset from Hugging Face → apply 4-bit NF4 quantization for open models → run evaluation with subject-inclusive prompts → log accuracy by domain, difficulty, and cultural subset
- **Design tradeoffs:**
  - **Hard questions (5 options) vs. standard (4 options):** Reducing options improves accuracy (+6.02 for Claude) but doesn't fully offset difficulty
  - **Native vs. translated content:** Native ensures naturalness but limits scalability; translation scales but risks terminology errors
  - **Zero-shot vs. few-shot:** Zero-shot is simpler; few-shot shows inconsistent gains on instruction-tuned models
- **Failure signatures:**
  - Sub-30% average accuracy → model likely lacks Sinhala tokenization or pretraining exposure
  - Large gap between Humanities and Social Science → cultural knowledge bottleneck
  - Degraded performance on negation/suboption questions → reasoning limitation, not just language
- **First 3 experiments:**
  1. **Baseline evaluation:** Run Qwen2.5-7B-Chat and LLaMA-3.1-70B-Chat on full SinhalaMMLU with subject-inclusive prompts; log domain-wise accuracy
  2. **Cultural subset analysis:** Isolate the 1,608 culturally grounded questions; compare Claude 3.5 Sonnet vs. GPT-4o vs. best open model
  3. **Option reduction ablation:** Re-evaluate hard questions with 4 options (randomly remove one distractor); measure accuracy gain and confidence-accuracy correlation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLMs be effectively adapted to bridge the performance gap between general knowledge and culturally grounded knowledge in low-resource languages like Sinhala?
- **Basis in paper:** [explicit] The authors state: "In particular, models struggle in culturally rich domains such as the Humanities, revealing substantial room for improvement in adapting LLMs to low-resource and culturally specific contexts." Claude 3.5 Sonnet and GPT-4o showed drops of 28.22 and 23.83 percentage points respectively on the cultural subset.
- **Why unresolved:** Current adaptation techniques (instruction tuning, scaling) do not adequately transfer to culturally specific content; the paper only identifies the gap without proposing solutions.
- **What evidence would resolve it:** Training or fine-tuning interventions specifically targeting cultural knowledge that significantly narrow the performance gap between general and cultural question subsets.

### Open Question 2
- **Question:** What evaluation methods beyond multiple-choice accuracy can capture LLM capabilities on generative and multimodal tasks in low-resource languages?
- **Basis in paper:** [explicit] The authors acknowledge: "SinhalaMMLU is limited to text-based multiple-choice questions... Extending the benchmark to include images, tables, diagrams, audio and essay-style questions remains a valuable direction for future work."
- **Why unresolved:** The benchmark deliberately excluded multimodal content and open-ended responses, leaving generative capabilities unevaluated.
- **What evidence would resolve it:** A follow-up benchmark or evaluation framework assessing open-ended generation, multimodal understanding, and reasoning in Sinhala with demonstrated correlations to real-world task performance.

### Open Question 3
- **Question:** Do LLMs perform comparably to human students on SinhalaMMLU, and what are the specific failure modes that differentiate model from human performance?
- **Basis in paper:** [explicit] The authors note: "Human evaluation was not conducted in this study due to practical limitations. Evaluating subject-specific questions requires age-appropriate annotators with domain expertise."
- **Why unresolved:** Only model-to-model comparisons were conducted; no human baseline establishes whether current performance levels are acceptable or identifies specific weaknesses relative to human test-takers.
- **What evidence would resolve it:** Controlled human evaluation using actual Sri Lankan students at appropriate grade levels with error analysis comparing model and human failure patterns across subjects and question types.

## Limitations
- Benchmark covers only multiple-choice questions, excluding open-ended, multimodal, and mathematical reasoning tasks
- Translation quality verification lacks detailed error analysis of terminology failures
- Methodological transparency issues with unspecified random seeds, temperature settings, and regex patterns

## Confidence

**High confidence:** The core finding that current LLMs struggle with Sinhala cultural knowledge is well-supported. The 28.22-point drop in Claude 3.5 Sonnet's performance on culturally grounded questions (67.65% to 39.43%) provides clear empirical evidence that cultural grounding exposes genuine capability gaps.

**Medium confidence:** The claim that translated benchmarks misrepresent linguistic naturalness and technical terminology is plausible given the naturalness rating differences, but lacks detailed error analysis of specific terminology failures. The mechanism is reasonable but not fully validated.

**Low confidence:** The assertion that few-shot prompting provides marginal gains on instruction-tuned models is weakly supported, as the paper only reports one model degrading (LLaMA-3.1-70B-Chat) and minimal improvements for others. Without systematic analysis of why gains are limited, this claim remains speculative.

## Next Checks

1. **Statistical significance testing:** Re-run evaluations with fixed random seeds and compute 95% confidence intervals for model accuracies. Apply paired t-tests between Claude 3.5 Sonnet and GPT-4o across domains to determine if performance differences are statistically meaningful.

2. **Error analysis on translated subset:** Manually examine 50 randomly selected questions from the GlobalMMLU-si subset to identify specific terminology errors and assess their correlation with model accuracy degradation. This would validate whether translation quality directly impacts performance.

3. **Expanded benchmark coverage:** Develop a supplementary evaluation set including open-ended questions, multimodal inputs, and mathematical reasoning tasks in Sinhala. Test whether models that perform well on MCQ also demonstrate competence in these broader capabilities, or if MCQ performance overestimates general language understanding.