---
ver: rpa2
title: 'Mid-Training of Large Language Models: A Survey'
arxiv_id: '2510.06826'
source_url: https://arxiv.org/abs/2510.06826
tags:
- arxiv
- data
- training
- preprint
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first comprehensive survey of mid-training
  in large language models (LLMs). The authors introduce a taxonomy spanning three
  domains: data distribution refinement, learning rate scheduling, and long-context
  extension.'
---

# Mid-Training of Large Language Models: A Survey

## Quick Facts
- **arXiv ID**: 2510.06826
- **Source URL**: https://arxiv.org/abs/2510.06826
- **Reference count**: 40
- **Primary result**: First comprehensive survey of mid-training as a distinct stage between pre-training and fine-tuning, showing performance gains through data refinement, learning rate scheduling, and context extension.

## Executive Summary
This paper presents the first comprehensive survey of mid-training in large language models (LLMs). The authors introduce a taxonomy spanning three domains: data distribution refinement, learning rate scheduling, and long-context extension. They show that mid-training is a critical intermediate stage between pre-training and fine-tuning, where models shift from memorization to abstraction by refining data quality, optimizing learning rates, and extending context length. The survey includes practical insights, evaluation benchmarks, and reported performance gains across models. The authors also identify open challenges and propose future research directions. Overall, mid-training emerges as a key strategy for improving LLM performance, efficiency, and adaptability.

## Method Summary
Mid-training involves continuing training a pre-trained LLM with refined data distributions, optimized learning rate schedules, and/or extended context lengths. The method requires constructing a mid-training dataset (100B-1T tokens) mixing high-quality educational data, code/math content, and synthetic instruction data. A learning rate scheduler (WSD or cosine decay) is configured to decay from stable pre-training values to near-zero over the target token budget. For context extension, RoPE scaling methods like ABF or YaRN are applied while training on progressively longer sequences with mixed short and long data to prevent degradation.

## Key Results
- Mid-training improves reasoning benchmarks (GSM8K, MATH) by 5-15% through data quality refinement
- Learning rate annealing with WSD schedules shows steeper late-stage decay and better checkpoint reusability than cosine
- Context extension via ABF+YaRN scaling enables 4K→32K token window expansion with measurable LongBench gains
- Data distribution shifts from general web to curated corpora reinforce compositional reasoning skills
- WSD schedulers outperform cosine for continual training scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Mid-training shifts models from memorization toward abstraction by improving gradient signal quality.
- **Mechanism**: High-quality data induces greater gradient variance (higher gradient noise scale), helping models escape sharp minima associated with overfitting to noisy pre-training data. Lower learning rates during annealing suppress gradient noise, stabilizing convergence near favorable optima.
- **Core assumption**: Models that have acquired broad features from web-scale data benefit from refined signals rather than more tokens.
- **Evidence anchors**:
  - [abstract] "Its effectiveness can be explained through gradient noise scale, the information bottleneck, and curriculum learning, which together promote generalization and abstraction."
  - [section II] "Higher-quality data tends to induce greater gradient variance, yielding a higher GNS, whereas redundant or noisy data reduces diversity."
  - [corpus] Related work on RL-guided mid-training suggests bidirectional optimization may further enhance signal quality (weak evidence—emerging area).
- **Break condition**: If gradient variance metrics plateau despite data quality improvements, this mechanism may be saturating.

### Mechanism 2
- **Claim**: Learning rate annealing with refined data compresses representations to task-relevant features.
- **Mechanism**: The information bottleneck principle suggests that during annealing, models progressively attenuate spurious correlations while retaining predictive structures. High-quality supervision provides lower-entropy guidance, facilitating identification of semantically meaningful patterns.
- **Core assumption**: Pre-training representations contain noisy features that can be selectively compressed.
- **Evidence anchors**:
  - [section II] "During the learning rate annealing phase of training, the model progressively reduces reliance on noisy or redundant features."
  - [section I] "Lowering the learning rate mitigates gradient variance and stabilizes convergence near favorable minima."
  - [corpus] No direct corpus support for IB mechanism in mid-training (limited evidence).
- **Break condition**: If downstream task performance degrades after annealing, the compression may be too aggressive.

### Mechanism 3
- **Claim**: Staged data distribution shifts reinforce complex reasoning through curriculum-style progression.
- **Mechanism**: Early broad pre-training establishes generalization; subsequent shifts toward curated corpora (STEM, code, reasoning) reinforce underrepresented compositional skills. This aligns with curriculum learning—gradual introduction of harder examples optimizes learning efficiency.
- **Core assumption**: Models can acquire compositional skills more efficiently when exposed to them after foundational capabilities are established.
- **Evidence anchors**:
  - [section II] "Early pre-training exposes the model to diverse and noisy corpora for generalization, after which the data distribution can be gradually shifted toward more challenging and informative examples."
  - [section III.C] "SmolLM2 demonstrates consistent gains by replacing low-signal web text with structured educational, math, and coding data."
  - [corpus] "Large-Scale Diverse Synthesis for Mid-Training" validates that synthesized QA data improves knowledge-intensive tasks.
- **Break condition**: If domain-specific benchmarks don't improve despite data upweighting, the curriculum may be misaligned with model capacity.

## Foundational Learning

- **Concept: Rotary Position Embeddings (RoPE)**
  - **Why needed here**: Long-context extension methods (PI, NTK, YaRN, LongRoPE) all modify RoPE frequency mappings. Understanding RoPE's relative-position encoding is prerequisite to grasping why frequency remapping enables context extension.
  - **Quick check question**: Can you explain why RoPE attention scores depend only on relative position (m−n)?

- **Concept: Learning Rate Scheduling Dynamics**
  - **Why needed here**: Mid-training relies on annealing strategies (cosine, linear, WSD) with specific decay timing. Understanding warmup, decay shapes, and their interaction with batch size is essential for reproducing results.
  - **Quick check question**: Why might WSD schedulers outperform cosine for continual training scenarios?

- **Concept: Gradient Noise Scale and Batch Size Scaling**
  - **Why needed here**: The paper links GNS to optimization stability and data quality. Understanding how batch size affects gradient variance helps interpret why certain schedules work at specific scales.
  - **Quick check question**: How does increasing batch size affect optimal learning rate, and why does this matter for mid-training stability?

## Architecture Onboarding

- **Component map**: Pre-trained checkpoint → Mid-training pipeline: Data refinement layer (Quality filtering, Domain upweighting, Synthetic augmentation) -> Learning rate scheduler (Warmup, Stable phase/decay, Annealing decay) -> Context extension (optional) (Frequency remapping, Temperature scaling, Progressive stages)

- **Critical path**:
  1. Start from converged pre-training checkpoint (not early checkpoint)
  2. Introduce refined data mix while maintaining ~80% distribution continuity to prevent catastrophic forgetting
  3. Begin annealing phase with 10-20% of total tokens, using WSD or cosine decay
  4. If extending context: apply ABF (base θ ~10⁶ for 128K) + YaRN temperature scaling
  5. Validate on both short-context benchmarks (prevent degradation) and target long-context tasks

- **Design tradeoffs**:
  - **WSD vs. Cosine**: WSD enables efficient checkpoint reuse and flexible token budgets; cosine requires predetermined total steps but may be more stable for fixed budgets.
  - **Synthetic data ratio**: Higher synthetic content (Phi-4: ~50%) improves reasoning but may reduce factual grounding—balance with filtered web data.
  - **Context extension aggressiveness**: Fast extension (fewer stages, larger jumps) saves compute but risks perplexity spikes; staged extension (4K→32K→128K) is safer.

- **Failure signatures**:
  - Loss spikes during context extension → frequency remapping too aggressive; reduce temperature scaling or add intermediate stage.
  - Short-context benchmark degradation → long-data ratio too high; rebalance to 30-40% long, 60-70% short.
  - Catastrophic forgetting → distribution shift too abrupt; ensure 80%+ data overlap with pre-training corpus.
  - Annealing shows no loss reduction → data quality insufficient; audit filtering pipeline or increase curated data fraction.

- **First 3 experiments**:
  1. **Ablate data quality**: Train with filtered vs. unfiltered web data during annealing only (hold LR schedule constant). Measure GSM8K and MMLU deltas. Expect 5-15% gains if mechanism holds.
  2. **Compare LR schedules**: Run WSD vs. cosine decay for identical token budget (1T tokens). Track loss curves, convergence speed, and checkpoint reusability. WSD should show steeper late-stage decay.
  3. **Validate context extension recipe**: Apply ABF (θ=500K) + YaRN scaling to extend 4K model to 32K. Train on 75B tokens with 40% long data. Measure LongBench performance vs. short-context baseline degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal placement and proportion of instruction-style data during mid-training to balance reasoning gains against potential overfitting or diminishing returns?
- **Basis**: [inferred] Section III.C notes that while instruction data aids reasoning, "its optimal placement in the training pipeline remains unresolved," with conflicting reports on benefits versus diminishing returns.
- **Why unresolved**: Different models report divergent outcomes; early introduction (MiniCPM) is effective for some, while others (DeepSeek) suggest late additions yield diminishing returns comparable to SFT.
- **Evidence**: Ablation studies tracking reasoning benchmarks (e.g., GSM8K) versus general perplexity when injecting instruction data at varying stages (early, mid, late) and ratios within the mid-training mix.

### Open Question 2
- **Question**: How can learning rate scheduler designs be unified under a theoretical framework that accounts for conflicting empirical evidence regarding decay strategies (e.g., linear vs. cosine) across different model scales?
- **Basis**: [explicit] Section VII calls for future work to "unify these disparate empirical findings under a theoretical framework," noting that Section IV highlights conflicting evidence on decay strategy impacts.
- **Why unresolved**: Current designs are largely heuristic; Kaplan et al. suggest scheduler insensitivity under large LR budgets, while Hoffmann et al. provide evidence of significant variance, creating a lack of consensus.
- **Evidence**: The derivation of universal scaling laws for scheduler hyperparameters that predict optimal decay shapes (cosine, linear, WSD) based on model dimension and compute budget.

### Open Question 3
- **Question**: How can long-context extension methods evolve from handcrafted frequency policies (e.g., PI, YaRN) to general, learnable position functions that ensure robustness to out-of-distribution (OOD) lengths?
- **Basis**: [explicit] Section VII states that future progress hinges on "fixing position-embedding OOD" and pushing "toward general, learnable position functions" rather than relying on RoPE tweaks.
- **Why unresolved**: Current state-of-the-art methods (PI, NTK, YaRN) rely on fixed interpolation rules or hyperparameters (α, β) that may not generalize robustly to ultra-long contexts (e.g., >1M tokens) or unseen sequence structures.
- **Evidence**: The development of a trainable position encoding mechanism that outperforms handcrafted frequency remapping on benchmarks (e.g., Ruler) extending significantly beyond the training context window.

## Limitations
- Mid-training boundaries remain fluid with overlap between pre-training, mid-training, and fine-tuning stages
- Performance gains are highly model- and data-dependent with no universal recipe emerging
- Theoretical claims linking gradient noise scale and information bottleneck to mid-training gains lack direct empirical validation

## Confidence
- **High**: Data distribution refinement improves reasoning and coding benchmarks; context extension via RoPE scaling is technically sound and reproducible
- **Medium**: Learning rate annealing benefits are well-documented but optimal schedules vary significantly across models and scales
- **Low**: Theoretical claims linking gradient noise scale and information bottleneck to mid-training gains lack direct empirical validation in most cited works

## Next Checks
1. **Ablate data quality**: Train with filtered vs. unfiltered web data during annealing only (hold LR schedule constant). Measure GSM8K and MMLU deltas. Expect 5-15% gains if mechanism holds.
2. **Compare LR schedules**: Run WSD vs. cosine decay for identical token budget (1T tokens). Track loss curves, convergence speed, and checkpoint reusability. WSD should show steeper late-stage decay.
3. **Validate context extension recipe**: Apply ABF (θ=500K) + YaRN scaling to extend 4K model to 32K. Train on 75B tokens with 40% long data. Measure LongBench performance vs. short-context baseline degradation.