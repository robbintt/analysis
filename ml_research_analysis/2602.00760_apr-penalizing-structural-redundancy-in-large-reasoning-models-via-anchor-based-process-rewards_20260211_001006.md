---
ver: rpa2
title: 'APR: Penalizing Structural Redundancy in Large Reasoning Models via Anchor-based
  Process Rewards'
arxiv_id: '2602.00760'
source_url: https://arxiv.org/abs/2602.00760
tags:
- reasoning
- length
- redundancy
- arxiv
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Anchor-based Process Reward (APR), a method\
  \ to reduce structural redundancy in Large Reasoning Models (LRMs) by identifying\
  \ and penalizing the Answer-Stable Tail (AST)\u2014the repetitive verification phase\
  \ after the final answer stabilizes. APR uses anchor localization to precisely target\
  \ this redundancy, unlike coarse global length penalties."
---

# APR: Penalizing Structural Redundancy in Large Reasoning Models via Anchor-based Process Rewards

## Quick Facts
- arXiv ID: 2602.00760
- Source URL: https://arxiv.org/abs/2602.00760
- Reference count: 32
- Primary result: Achieves Pareto frontier of accuracy-efficiency across five mathematical reasoning datasets, reducing generation length by up to 56.7% while improving accuracy by up to 16.3%

## Executive Summary
This paper introduces Anchor-based Process Reward (APR), a method to reduce structural redundancy in Large Reasoning Models (LRMs) by identifying and penalizing the Answer-Stable Tail (AST)—the repetitive verification phase after the final answer stabilizes. APR uses anchor localization to precisely target this redundancy, unlike coarse global length penalties. Combined with the Direct Alignment Policy Optimization (DAPO) algorithm, APR achieves the Pareto frontier of accuracy-efficiency across five mathematical reasoning datasets, reducing generation length by up to 56.7% while improving accuracy by up to 16.3%. It also requires fewer computational resources during RL training, demonstrating that dense, accurate process reward signals enhance training efficiency.

## Method Summary
APR introduces a novel process reward that targets structural redundancy in LRM reasoning processes. The method first identifies an "anchor" - the first sentence where the model's answer stabilizes - using either rule-based or model-based localization. The Answer-Stable Tail (AST) is defined as all tokens after this anchor position. APR then applies a length penalty exclusively to AST tokens: `R_APR(y) = I(ŷ = y*) · (1 - β · L_AST(y, ŷ))`. This is combined with DAPO, a policy optimization algorithm that dynamically filters training groups to preserve penalty sensitivity. The approach achieves state-of-the-art accuracy-efficiency trade-offs on five mathematical reasoning datasets while requiring fewer training steps than baselines.

## Key Results
- APR achieves the Pareto frontier of accuracy-efficiency across five mathematical reasoning datasets
- Generation length reduced by up to 56.7% while accuracy improved by up to 16.3%
- APR-1.5B achieves comparable performance to L1-Qwen-1.5B-Max with 250 vs. 820 training steps
- Answer stability rate of 95.78% enables reliable AST identification

## Why This Works (Mechanism)

### Mechanism 1: Structural Segmentation via Reasoning Anchor
- **Claim:** The reasoning process can be split into information-dense derivation (pre-anchor) and information-sparse repetition (post-anchor), enabling targeted optimization.
- **Mechanism:** The anchor localizes the first sentence where the model's answer stabilizes. Everything after this position is defined as the Answer-Stable Tail (AST). The reward function applies a length penalty exclusively to AST tokens: `R_APR(y) = I(ŷ = y*) · (1 - β · L_AST(y, ŷ))`
- **Core assumption:** The first complete answer rarely changes (95.78% consistency observed on MATH500), so post-answer tokens contribute negligible marginal information.
- **Evidence anchors:**
  - [Section 3.1]: Defines anchor as `t_anc(y, y_ref) = end(s_k*)` where answer first appears
  - [Section 3.3, Observation 2]: "the consistency rate is 95.78% regardless of difficulty"
  - [corpus]: "Reconsidering Overthinking" paper similarly decomposes redundancy but distinguishes internal vs. external forms; APR's structural approach is more granular
- **Break condition:** If models frequently revise answers after initial formulation, AST-based penalty would suppress useful correction behavior.

### Mechanism 2: Dense Process Signals Improve Sample Efficiency
- **Claim:** Localizing rewards to specific segments provides stronger gradient signal than global length penalties.
- **Mechanism:** Global penalties (e.g., `R_len(y) = I(ŷ = y*) - β · L(y)`) indiscriminately penalize all tokens. APR's segment-aware penalty creates dense feedback that directly targets redundancy without discouraging necessary derivation steps.
- **Core assumption:** The pre-anchor phase contains necessary reasoning; penalizing it would harm accuracy.
- **Evidence anchors:**
  - [Section 5.2]: APR-1.5B achieves comparable performance to L1-Qwen-1.5B-Max with 250 vs. 820 training steps
  - [Section 5.2]: "We attribute this high training efficiency to the dense and accurate feedback signals provided by the process reward"
  - [corpus]: Corpus papers on reasoning faithfulness (e.g., "Measuring the Faithfulness of Thinking Drafts") do not directly address reward density; mechanism lacks external validation
- **Break condition:** If dense rewards overfit to training distribution, generalization to out-of-distribution problems would degrade.

### Mechanism 3: DAPO Preserves Penalty Sensitivity via Dynamic Sampling
- **Claim:** DAPO's dynamic sampling avoids the normalization degeneracy that neutralizes penalty coefficients in GRPO.
- **Mechanism:** GRPO normalizes advantages within groups (`Â_i = (R_i - μ_R) / σ_R`). In "Correct-but-Verbose" homogeneous groups, this reduces to `Â_i ≈ -(L_i - μ_L) / σ_L`, making the signal approximately independent of β. DAPO filters such groups, preserving β's effect.
- **Core assumption:** Maintaining β sensitivity is critical for calibrating the accuracy-efficiency trade-off.
- **Evidence anchors:**
  - [Section 4.3]: Mathematical derivation showing GRPO degeneracy in homogeneous groups
  - [Section 5.3, Ablation]: "GRPO requires 250 steps to match the performance of DAPO at 50 steps"
  - [corpus]: No corpus papers directly compare DAPO vs. GRPO for length penalties; this is an isolated claim
- **Break condition:** If dynamic sampling excessively filters batches, training throughput could suffer without quality gains.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here:** APR operates within RLVR, where rewards are computed from deterministic verification (e.g., answer correctness) rather than learned reward models.
  - **Quick check question:** Can you explain why RLVR enables process-level reward shaping without training a separate reward model?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** GRPO is the baseline algorithm; understanding its normalization mechanics is prerequisite to grasping why DAPO helps.
  - **Quick check question:** In GRPO, what happens to the advantage estimate when all samples in a group receive the same outcome reward?

- **Concept: Chain-of-Thought (CoT) Scaling**
  - **Why needed here:** Overthinking emerges from test-time scaling that encourages longer CoT; APR addresses the side-effect while preserving reasoning capability.
  - **Quick check question:** How does test-time scaling differ from model-parameter scaling in terms of computational cost distribution?

## Architecture Onboarding

- **Component map:** Anchor Locator -> AST Calculator -> Reward Shaper -> DAPO Sampler -> Policy Optimizer
- **Critical path:** 1. Rollout generation -> 2. Answer extraction -> 3. Anchor localization -> 4. AST length computation -> 5. Reward assignment -> 6. DAPO filtering -> 7. Policy update
- **Design tradeoffs:**
  - Rule-based vs. model-based localization: Rule-based is more robust for long contexts; model-based handles complex formats but risks "lost-in-the-middle" hallucination (80.1% acceptable error rate)
  - Penalty coefficient β: Higher values aggressively compress length but risk accuracy drop; paper uses β ∈ {2e-4, 5e-4} based on redundancy analysis
- **Failure signatures:**
  - Truncation without answer: If model learns to truncate early to avoid penalty, accuracy collapses
  - Anchor mislocalization: Late anchor detection → under-penalized redundancy; early detection → over-penalized useful reasoning
  - Homogeneous batch filtering starvation: If DAPO filters too aggressively, training stalls
- **First 3 experiments:**
  1. **Validate anchor localization accuracy:** Manually annotate 100 samples; compare rule-based vs. model-based against ground truth; confirm >65% exact match
  2. **Ablate reward design:** Train with global length penalty vs. APR on same data; compare accuracy-length Pareto curves
  3. **Stress-test DAPO vs. GRPO:** Run matched training with both algorithms; log β sensitivity (correlation between β value and achieved length reduction)

## Open Questions the Paper Calls Out
None

## Limitations
- Claims about DAPO's superiority over GRPO for length penalties remain weakly supported with no external literature validation
- Anchor localization relies on answer stability (95.78% consistency) that may not hold across all reasoning domains or model scales
- Performance improvements were achieved with LRM variants of Qwen and DeepSeek-Coder, leaving generalizability to other architectures open

## Confidence

- **High confidence:** The empirical Pareto frontier results showing accuracy-efficiency trade-offs; the observed answer stability enabling AST identification; the basic reward shaping mechanism
- **Medium confidence:** The mechanism by which dense process rewards improve sample efficiency; the claim that DAPO preserves penalty sensitivity better than GRPO; the generalizability across reasoning domains
- **Low confidence:** The theoretical necessity of DAPO over GRPO for this specific task; the robustness of anchor localization across diverse problem types; the absence of accuracy degradation from aggressive length penalties

## Next Checks
1. **Cross-architecture validation:** Replicate the full APR+DAPO pipeline on a different LRM architecture (e.g., OpenAI's o1 or Claude-3-Opus) to verify that the accuracy-efficiency gains transfer beyond Qwen/DeepSeek models.

2. **Answer stability stress test:** Systematically vary problem difficulty and format to measure answer stability rates below the reported 95.78%. Determine the threshold at which APR's AST penalty begins suppressing useful answer revisions rather than mere redundancy.

3. **Algorithmic ablation with GRPO variants:** Implement and compare against GRPO variants that use alternative normalization schemes (e.g., group-wise standardization vs. global normalization) to isolate whether DAPO's advantages stem from dynamic sampling or the specific normalization approach.