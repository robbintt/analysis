---
ver: rpa2
title: '''Rich Dad, Poor Lad'': How do Large Language Models Contextualize Socioeconomic
  Factors in College Admission ?'
arxiv_id: '2509.16400'
source_url: https://arxiv.org/abs/2509.16400
tags:
- quintile
- tier
- llms
- admit
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models (LLMs) consistently favor low-SES applicants\
  \ in college admissions, even when controlling for academic performance, and Chain-of-Thought\
  \ prompting amplifies this tendency by explicitly invoking SES as compensatory justification.\
  \ Across 4 models and 30,000 synthetic profiles, LLMs admitted 4\u20138\xD7 more\
  \ low-SES applicants in top SES quintiles and showed higher decision volatility\
  \ for disadvantaged students under deliberative reasoning."
---

# 'Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?

## Quick Facts
- arXiv ID: 2509.16400
- Source URL: https://arxiv.org/abs/2509.16400
- Reference count: 40
- Primary result: Large language models consistently favor low-SES applicants in college admissions, even when controlling for academic performance

## Executive Summary
This study systematically examines how large language models contextualize socioeconomic status (SES) in college admissions decisions using a dual-process audit framework (DPAF). The research reveals that LLMs demonstrate a consistent bias favoring low-SES applicants across both fast, automatic reasoning (System 1) and slow, deliberative reasoning (System 2) modes. Chain-of-Thought prompting amplifies this tendency by explicitly invoking SES factors as compensatory justification for weaker academic credentials. The study introduces a novel framework for auditing LLM decision-making in high-stakes domains and demonstrates that SES features (particularly fee waiver and first-generation status) remain top predictors of admission outcomes even after controlling for academic performance and institutional selectivity.

## Method Summary
The study uses 30,000 semi-synthetic applicant profiles grounded in Common App correlations, featuring academic metrics (GPA, SAT), SES indicators (first-gen status, fee waiver eligibility), and demographic proxies (ZIP code, school type). Four open-source LLMs (Qwen2 7B, Mistral 7B v0.3, Gemma2 9B, Llama 3.1 7B) were evaluated across 60 U.S. institutions categorized by selectivity tiers. The dual-process framework compares System 1 (decision-only) and System 2 (Chain-of-Thought with JSON explanations) prompting modes, with 5 million prompts total. Statistical analysis employs mixed-effects logistic regression with random intercepts for institution, prompt variant, and attribute ordering. GPT-4o-mini tags explanations with support/penalize/null labels for each feature to analyze reasoning patterns.

## Key Results
- LLMs admitted 4–8× more low-SES applicants in top SES quintiles, with consistent bias across both System 1 and System 2 reasoning modes
- Chain-of-Thought prompting amplifies SES consideration, with higher decision volatility (flip rates) for disadvantaged students under deliberative reasoning
- Direct SES markers (fee waiver, first-gen) show stronger admission effects (OR = 1.86–5.87) than indirect proxies (ZIP code OR = 1.03–1.08)
- Model sensitivity to institutional selectivity varies substantially, with Gemma showing strong alignment while Mistral remains permissive across tiers

## Why This Works (Mechanism)

### Mechanism 1: Chain-of-Thought SES Amplification
- Claim: COT prompting increases LLMs' use of SES factors as compensatory justification for low-performing applicants
- Evidence: System 2 amplifies SES tendency by explicitly invoking SES as compensatory justification; low-SES applicants show higher flip rates (3.89% vs 2.67%) in COT mode
- Core assumption: COT outputs reflect actual reasoning processes rather than post-hoc rationalization (paper cautions this may not hold)

### Mechanism 2: Direct vs. Indirect SES Signal Asymmetry
- Claim: Direct hardship markers produce stronger admission effects than indirect proxies
- Evidence: Fee waiver and first-gen odds 1.86 to 5.87 times higher than ZIP code (OR = 1.03–1.08); models cite first-gen (66.8%) and fee-waiver (43.9%) far more than ZIP (5.1%) or school type (10.6%)
- Core assumption: Training data contains stronger associations between direct hardship labels and equity-oriented language

### Mechanism 3: Institutional Selectivity Sensitivity
- Claim: LLMs adjust admission thresholds based on stated institutional selectivity, but magnitude varies by model
- Evidence: Gemma shows strongest alignment with real-world selectivity bands; Mistral admits over 40% even in Tier 1; selectivity specification shifts Gemma's admit rate to 33.3%
- Core assumption: Models have internalized reliable associations between institutional prestige and appropriate admit rates

## Foundational Learning

- Concept: Dual-process theory (System 1 vs. System 2 cognition)
  - Why needed here: The entire DPAF framework depends on distinguishing fast, automatic decisions from slow, deliberative reasoning
  - Quick check: If you run the same profile through both prompting modes and get identical decisions, what does this tell you about the model's reasoning?

- Concept: Mixed-effects logistic regression
  - Why needed here: Used to isolate SES variable effects while controlling for institutional clustering, prompt variants, and attribute ordering
  - Quick check: Why include random intercepts for institution rather than just fixed effects for selectivity tier?

- Concept: Chain-of-Thought faithfulness problem
  - Why needed here: The paper warns COT explanations may not reflect true model reasoning—critical for interpreting System 2 results
  - Quick check: If a model admits a low-SES applicant and cites "resilience" in its explanation, how would you verify this actually drove the decision?

## Architecture Onboarding

- Component map: Synthetic data generator -> System 1 prompter -> System 2 prompter -> Tagging pipeline -> Statistical analyzer
- Critical path: Data generation → System 1 experiments → System 2 experiments → Explanation tagging → Mixed-effects modeling → Flip rate analysis → DPAF synthesis
- Design tradeoffs: Synthetic vs. real profiles (controlled experimentation vs. real-world complexity); 7-9B parameter models (compute constraints vs. behavior generalization)
- Failure signatures: Llama's near-zero admit rates (safe non-compliance alignment); Mistral's insensitivity to selectivity; high flip rates for low-SES applicants (unstable reasoning)
- First 3 experiments: 1) Replicate System 1 analysis to establish baseline SES sensitivity; 2) Run System 2 on 10% subsample to measure reasoning volatility; 3) Apply tagging pipeline to validate explanation analysis

## Open Questions the Paper Calls Out

- How does inclusion of qualitative application components (personal statements, essays) influence LLM decisions compared to structured profiles? The study notes future research should incorporate essays to complete analysis.
- Do LLMs implicitly adjust academic benchmarks (GPA/SAT thresholds) based on applicant's SES during reasoning? The paper suggests investigating if models shift internal benchmarks across tiers and SES quintiles.
- To what extent does DPAF generalize to other high-stakes domains like healthcare or criminal justice? The framework is claimed adaptable beyond education, but validation is needed.
- Does increasing model parameter scale or using proprietary models alter observed equity-oriented bias? The study limits scope to 7-9B parameter models, acknowledging potential behavioral differences.

## Limitations

- Synthetic profiles may not fully capture real-world admission dynamics and complexity of actual applications
- Reliance on GPT-4o-mini for explanation annotation introduces model-in/model-out dependencies
- High decision volatility for low-SES applicants under COT prompting may indicate unstable rather than compensatory reasoning
- Study limited to 7-9B parameter open-source models, potentially limiting generalizability to larger or proprietary models

## Confidence

- **High confidence**: LLMs favor low-SES applicants consistently across both System 1 and System 2 modes
- **Medium confidence**: Chain-of-Thought amplification mechanism (paper cautions COT may reflect rationalization)
- **Medium confidence**: Selectivity sensitivity patterns (substantial variation across models)

## Next Checks

1. Conduct human validation on stratified sample (n=200) of LLM-generated explanations to verify GPT-4o-mini tagging accuracy and alignment with holistic review standards
2. Test model behavior on small set of real Common App profiles to identify systematic gaps between synthetic and actual applicant data distributions
3. Run ablation studies removing individual SES features to quantify their specific contribution to admission likelihood and validate mixed-effects model coefficients