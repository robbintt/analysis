---
ver: rpa2
title: 'BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection in
  Language Model Pretraining'
arxiv_id: '2510.06048'
source_url: https://arxiv.org/abs/2510.06048
tags:
- data
- training
- proxy
- selection
- bilevel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BLISS, a lightweight bilevel influence scoring
  method for data selection in language model pretraining. The method operates from
  scratch without relying on external pretrained models and explicitly accounts for
  the long-term impact of selected data by training a proxy model to convergence.
---

# BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection in Language Model Pretraining

## Quick Facts
- **arXiv ID:** 2510.06048
- **Source URL:** https://arxiv.org/abs/2510.06048
- **Reference count:** 34
- **Primary result:** 1.7× speedup in pretraining LLMs while maintaining downstream accuracy through lightweight bilevel data selection

## Executive Summary
BLISS introduces a novel bilevel optimization framework for data selection in LLM pretraining that operates from scratch without relying on external pretrained models. The method trains a lightweight proxy model to convergence while simultaneously learning to score training samples based on their long-term influence on downstream performance. By formulating data selection as a bilevel problem and incorporating KL divergence alignment between proxy and target models, BLISS achieves 1.7× speedup in reaching state-of-the-art performance while consistently outperforming baselines across multiple downstream tasks.

## Method Summary
BLISS formulates data selection as a bilevel optimization problem where a score model learns to assign importance weights to training samples. The lower-level objective trains a proxy model on weighted training loss until convergence, while the upper-level objective optimizes the score model to maximize validation performance of the converged proxy. The method incorporates KL divergence distillation to align proxy model outputs with the target LLM, ensuring learned data preferences transfer across model scales. BLISS uses softmax reparameterization to create differentiable data weighting and operates without external pretrained models by training everything from scratch.

## Key Results
- Achieves 1.7× speedup in reaching same performance as state-of-the-art methods under 1B model setting
- Consistently outperforms baselines across multiple downstream tasks including LAMBADA, HellaSwag, and ARC
- Reduces computational cost by 80% in initial pretraining phase while maintaining downstream accuracy

## Why This Works (Mechanism)

### Mechanism 1: Bilevel Optimization for Long-term Influence Estimation
The bilevel formulation captures cumulative training impact when models are trained to convergence, unlike single-step influence methods. The lower-level solves for proxy model convergence given data weights, while the upper-level propagates validation signal through the entire training trajectory via hypergradients. This ensures selected data provides long-term benefit rather than short-term gains.

### Mechanism 2: Proxy-LLM Alignment via KL Divergence Distillation
KL divergence loss aligns proxy model output distributions with the target LLM, ensuring data preferences learned by the proxy transfer to the larger model. This distillation transfers the LLM's current data preferences without expensive gradient updates to the LLM itself, with ablation showing +1.4% average accuracy improvement.

### Mechanism 3: Softmax Reparameterization for Differentiable Data Weighting
Softmax normalization of raw scores creates sharp distinctions between high/low-value samples while maintaining gradient flow. The softmax amplifies differences: high-scoring samples get disproportionately larger weights. Ablation shows +1.1% improvement over naive weighting, indicating the reparameterization captures meaningful importance distinctions.

## Foundational Learning

- **Concept: Bilevel Optimization / Hypergradients**
  - Why needed here: Core algorithmic framework; you must understand how gradients flow through an optimization process itself, not just through a network.
  - Quick check question: Given lower-level solution θ*ₚ = argmin g(θₚ, θₛ), how does the upper-level gradient ∇θₛf(θ*ₚ(θₛ)) depend on the lower-level Hessian?

- **Concept: Influence Functions for Data Valuation**
  - Why needed here: BLISS builds on influence estimation literature; understanding how individual training samples affect model behavior clarifies what the score model learns.
  - Quick check question: Why does the paper contrast "single-step" influence (MATES) with "convergence" influence (BLISS)?

- **Concept: Knowledge Distillation**
  - Why needed here: KL divergence term distills LLM behavior into proxy; essential for understanding why proxy preferences transfer.
  - Quick check question: If the LLM has high validation loss early in training, what risk does distillation introduce to data selection?

## Architecture Onboarding

- **Component map:** Training Data → Score Model → Influence Scores → Top-K Selection → Proxy Model → Validation Data → Upper-level Loss → Hypergradient → Score Model Update

- **Critical path:** Warmup → Bilevel optimization (3000 steps/round) → Score inference on full D_tr → Select top 20% → Pretrain LLM for 10k steps → Repeat

- **Design tradeoffs:** Proxy size: Paper shows 31M works for 410M LLM; larger proxy (160M) doesn't improve results. Bilevel steps: 3k steps sufficient; more steps don't improve convergence. Memory: HVP computation increases peak memory vs. MATES (49GB vs. 36GB for 410M setting).

- **Failure signatures:** Validation loss increases after initial decrease (normal—KL term dominates in phase 2), proxy and LLM preferences diverge (check KL divergence curve), no improvement over random selection (likely validation set mismatch).

- **First 3 experiments:**
  1. Reproduce 410M setting with random baseline: Train Pythia-410M on randomly sampled C4 for 5 rounds (10k steps each). Measure downstream accuracy on 9 tasks.
  2. Ablate KL divergence: Run BLISS without the γ·D_KL term. Compare training loss curves and downstream accuracy.
  3. Validate proxy fidelity: Train domain reweighting with proxy vs. full LLM. Compare learned domain weights.

## Open Questions the Paper Calls Out

### Open Question 1
Can the peak memory usage incurred by the Hessian-Vector-Product (HVP) calculation in the bilevel optimization be reduced to facilitate scaling to models significantly larger than 2.8B parameters? The conclusion states, "the bilevel optimization in our method may incur higher peak memory usage, which remains an important direction for future work."

### Open Question 2
Does the fidelity of the lightweight proxy model degrade when selecting data for pretraining target models with orders of magnitude more parameters (e.g., 70B+)? The paper validates scaling from 1B to 2.8B, but the gap between the proxy (160M) and much larger foundation models is substantially wider.

### Open Question 3
How can the dependency on a specific validation dataset (D_val) be mitigated to prevent the data selection from overfitting to a limited domain distribution? The ablation study demonstrates that different validation sets yield different downstream performance profiles.

## Limitations
- Memory overhead from HVP computation limits scaling to very large models (requires ~49GB for 410M target)
- Dependency on specific validation dataset creates risk of overfitting to narrow domain
- Performance assumes proxy-LLM alignment via KL divergence is sufficient for preference transfer

## Confidence
- **Speedup claim (1.7×):** Medium confidence - validated only on C4 dataset with LLaMA/Pythia models
- **No external models requirement:** High confidence - method operates from scratch but assumes KL alignment transfers preferences
- **Long-term influence capture:** Medium confidence - no direct comparison against single-step methods on same data

## Next Checks
1. **Ablation on data selection criteria:** Run BLISS without the KL divergence term and with uniform weighting to isolate the contribution of bilevel optimization vs. alignment vs. reweighting.

2. **Generalization to different datasets:** Apply BLISS to a non-web text corpus (e.g., biomedical or code) and measure if the 1.7× speedup holds, testing the claim of being "from scratch."

3. **Robustness to proxy model size:** Systematically vary proxy model size (e.g., 10M, 100M, 500M parameters) and measure impact on downstream accuracy to validate the claim that small proxies are sufficient.