---
ver: rpa2
title: 'My Answer Is NOT ''Fair'': Mitigating Social Bias in Vision-Language Models
  via Fair and Biased Residuals'
arxiv_id: '2505.23798'
source_url: https://arxiv.org/abs/2505.23798
tags:
- bias
- ours
- social
- fair
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses social bias in large vision-language models
  (VLMs), focusing on their generative responses and confidence levels. It introduces
  a new evaluation method using multiple-choice selection tasks on the PAIRS and SocialCounterfactuals
  datasets, with carefully designed prompts to elicit models' opinions on gender and
  race.
---

# My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals

## Quick Facts
- **arXiv ID**: 2505.23798
- **Source URL**: https://arxiv.org/abs/2505.23798
- **Reference count**: 40
- **Primary result**: Introduces post-hoc bias mitigation for VLMs using orthogonal projection of fair and biased residuals

## Executive Summary
This paper addresses social bias in large vision-language models (VLMs) by introducing a novel post-hoc mitigation method that decomposes residuals into fair and biased components and uses orthogonal projection to amplify fairness-associated residuals while ablating bias-associated ones. The approach is training-free and model-agnostic, designed to improve both the fairness of generated responses and the calibration of confidence levels. The study introduces a new evaluation method using multiple-choice selection tasks on the PAIRS and SocialCounterfactuals datasets, revealing that state-of-the-art VLMs like LLaVA-NeXT-13B and Qwen2.5-VL-32B exhibit significant bias and miscalibrated confidence. Experimental results demonstrate substantial improvements in fairness scores and KL divergence, outperforming competing training strategies while maintaining performance on other tasks like VQA.

## Method Summary
The method introduces a novel approach to bias mitigation by extracting fair and biased residual vectors from each layer of VLMs and applying orthogonal projection matrices to amplify fairness-associated residuals while ablating bias-associated ones. The core insight is that fairness levels fluctuate across hidden layers, with some residuals contributing to bias. By decomposing these residuals and selectively amplifying or suppressing them, the method achieves post-hoc bias mitigation without requiring additional training. The approach uses multiple-choice evaluation tasks on the PAIRS and SocialCounterfactuals datasets with carefully designed prompts to elicit models' opinions on gender and race. The method is described as training-free and model-agnostic, making it broadly applicable across different VLM architectures.

## Key Results
- Fairness scores increased significantly (e.g., from 0.68 to 0.84 on LLaVA-NeXT-13B for gender)
- KL divergence decreased substantially (e.g., from 0.209 to 0.096), indicating fairer responses and more reliable confidence
- Method outperforms competing training-based strategies while maintaining performance on other tasks like VQA

## Why This Works (Mechanism)
The method works by exploiting the observation that fairness levels fluctuate across hidden layers in VLMs, with certain residuals contributing to bias while others promote fairness. By decomposing these residuals into fair and biased components and using orthogonal projection matrices, the approach can selectively amplify fairness-associated residuals while suppressing bias-associated ones. This post-hoc manipulation allows for bias mitigation without requiring model retraining or architectural changes. The orthogonal projection ensures that the fair and biased components remain separated, preventing interference between the two. The effectiveness stems from the ability to identify and manipulate the specific residual components that drive biased behavior in the model's intermediate representations.

## Foundational Learning

**Residual Decomposition**: Understanding how to separate model residuals into fair and biased components is crucial for the method's success. This requires knowledge of how VLMs process information through their layers and how biases manifest in these intermediate representations.

*Why needed*: The entire approach depends on being able to identify and separate the components that contribute to fairness versus bias in the model's outputs.

*Quick check*: Can the decomposition algorithm consistently identify the same fair and biased components across different model runs and inputs?

**Orthogonal Projection**: The method uses orthogonal projection matrices to manipulate the decomposed residuals. This mathematical technique allows for selective amplification and suppression of specific components while maintaining their separation.

*Why needed*: Orthogonal projection ensures that fairness and bias components remain distinct and don't interfere with each other during the amplification/suppression process.

*Quick check*: Do the projected components remain truly orthogonal after multiple applications of the projection matrices?

**Multiple-Choice Evaluation**: The study introduces a new evaluation framework using multiple-choice tasks rather than open-ended generation to assess bias more systematically.

*Why needed*: Multiple-choice evaluation provides more controlled and comparable measurements of bias across different models and conditions.

*Quick check*: Does the multiple-choice framework capture the same types and degrees of bias as open-ended generation tasks?

## Architecture Onboarding

**Component map**: Input image/text -> VLM encoder/decoder layers (with residual extraction) -> Orthogonal projection matrices -> Fair/bias component separation -> Output generation

**Critical path**: The core processing path involves passing inputs through VLM layers, extracting residuals at each layer, applying orthogonal projection to separate fair and biased components, then using the modified residuals for final output generation.

**Design tradeoffs**: The method trades computational overhead (additional projection operations) for bias mitigation benefits. It also prioritizes post-hoc modification over training-based approaches, sacrificing potential performance gains from end-to-end optimization.

**Failure signatures**: Potential failures include: residual decomposition that doesn't cleanly separate fair and biased components, projection matrices that don't maintain orthogonality, or over-amplification of fairness components leading to unnatural outputs.

**3 first experiments**: 1) Validate residual decomposition effectiveness on known biased/FAIR datasets, 2) Test orthogonal projection stability across multiple VLM layers, 3) Measure fairness improvements on a held-out dataset with different prompt structures.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Multiple-choice evaluation may not fully capture bias in open-ended generation scenarios
- Method's effectiveness depends on the assumption that fairness and bias can be cleanly separated into orthogonal components
- Limited testing scope across different VLM architectures and tasks raises questions about broader applicability

## Confidence
- **Core methodology**: Medium - innovative approach with reasonable experimental design
- **Reported improvements**: Medium - demonstrated through appropriate metrics but limited testing scope
- **Model-agnostic claim**: Medium - experiments focus on a limited set of models
- **Long-term effectiveness**: Low - no longitudinal studies conducted
- **Broader applicability**: Low - limited validation across diverse contexts and tasks

## Next Checks
1. Test the method's effectiveness on open-ended generation tasks rather than just multiple-choice selection
2. Validate the approach across a broader range of VLM architectures including smaller and differently-structured models
3. Conduct longitudinal studies to assess the stability of fairness improvements over extended usage periods