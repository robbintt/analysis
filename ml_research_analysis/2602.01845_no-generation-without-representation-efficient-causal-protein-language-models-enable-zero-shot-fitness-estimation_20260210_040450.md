---
ver: rpa2
title: 'No Generation without Representation: Efficient Causal Protein Language Models
  Enable Zero-Shot Fitness Estimation'
arxiv_id: '2602.01845'
source_url: https://arxiv.org/abs/2602.01845
tags:
- proust
- protein
- language
- https
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Proust is a 309M-parameter causal protein language model that achieves
  masked language model (MLM) level performance on protein fitness prediction while
  retaining generation capabilities. It uses architectural innovations adapted from
  large language models, including grouped-query attention with shared K/V projections,
  cross-layer value residuals, and depthwise causal convolutions.
---

# No Generation without Representation: Efficient Causal Protein Language Models Enable Zero-Shot Fitness Estimation

## Quick Facts
- arXiv ID: 2602.01845
- Source URL: https://arxiv.org/abs/2602.01845
- Authors: Furkan Eris
- Reference count: 40
- Key result: 309M-parameter causal protein language model achieves MLM-level fitness prediction while retaining generation capabilities

## Executive Summary
Proust is a 309M-parameter causal protein language model that achieves masked language model (MLM) level performance on protein fitness prediction while retaining generation capabilities. It uses architectural innovations adapted from large language models, including grouped-query attention with shared K/V projections, cross-layer value residuals, and depthwise causal convolutions. Trained on 33B tokens in 40 B200 GPU-hours, Proust achieves Spearman ρ=0.390 on ProteinGym substitutions, competitive with MLMs requiring 50–200× more compute, and sets state-of-the-art performance on indels. It also approaches structure-aware methods on viral fitness benchmarks using sequence alone.

## Method Summary
Proust uses a 309M parameter causal transformer architecture with 24 layers, hidden dimension 1024, and 16 query heads sharing 2 KV heads. Key innovations include grouped-query attention with shared K/V projections (GQA-S2), cross-layer value residuals, depthwise causal convolutions (Canon layers) at three positions in each block, and a hybrid position encoding with 96 NoPE and 32 RoPE dimensions. The model is trained with a dual-optimizer setup: Muon optimizer (with Polar Express orthogonalization) for attention and FFN layers, and AdamW for embeddings and output layers. Training uses 33B tokens from diverse protein datasets including UniRef50, metagenomic sources, and viral sequences, with 40 B200 GPU-hours of compute.

## Key Results
- Achieves Spearman ρ=0.390 on ProteinGym substitutions, competitive with MLMs requiring 50–200× more compute
- Sets state-of-the-art performance on indels with ρ=0.521
- Approaches structure-aware methods on viral fitness benchmarks using sequence alone
- Demonstrates per-position entropy variance predicts retrieval augmentation effectiveness (ρ=-0.40 correlation)

## Why This Works (Mechanism)

### Mechanism 1: GQA-S2 with Shared K/V Projections
Sharing K and V projections with hybrid position encoding achieves efficient attention without sacrificing fitness prediction quality. The head dimension is split into 96 NoPE dimensions and 32 RoPE dimensions, with inverse RoPE applied to recover relative position encoding in the output. This design assumes content matching and position matching are partially separable tasks that benefit from dedicated capacity.

### Mechanism 2: Depthwise Causal Convolutions (Canon Layers)
Canon layers enable single-layer induction heads and improve local pattern recognition without attention overhead. The learned 1D convolution with kernel size 4 over hidden states, combined with key offset, allows a query at position t to match bigram patterns in a single layer rather than requiring two attention layers.

### Mechanism 3: Per-Position Entropy Variance for Retrieval Guidance
The model computes entropy at each position via logit lens, with entropy standard deviation predicting retrieval augmentation effectiveness. When entropy std is low, external homologs provide useful signal everywhere; when high, the model already knows which positions matter and retrieval risks overwriting correct predictions.

## Foundational Learning

- **Causal vs Masked Language Models**: Understanding the bidirectional vs left-to-right context distinction is essential since Proust's core claim is that CLMs can match MLM fitness prediction while retaining generation. Quick check: Given "MKTLLV", can a causal model use "LLV" when predicting "T"?
- **Grouped-Query Attention (GQA)**: Proust uses 16 query heads sharing 2 KV heads (8:1 ratio). Understanding how KV cache scales with group count is necessary for memory planning. Quick check: For sequence length L with 16 heads and 2 KV groups, what is KV cache size per layer vs standard MHA?
- **Logit Lens Interpretation**: The entropy-based retrieval heuristic depends on projecting intermediate hidden states to vocabulary space. Understanding this tool is necessary to extend the interpretability analysis. Quick check: If layer 6 has near-zero prediction accuracy via logit lens, what does that suggest about what the layer is computing?

## Architecture Onboarding

- **Component map**: Input → Embedding → Post-embed Norm → [Block 1-24: Canon-A → Attention (GQA-S2 + VO-RoPE) → Canon-C → FFN (w/ Canon-D)] → Value residual from V_0 mixed at each layer → Output Head → Logits

- **Critical path**: The attention mechanism (GQA-S2 + VO-RoPE) is the highest-complexity component. Verify that inverse RoPE is applied correctly to the 32-dim portion of attention output. The Canon layers are simple but placement matters: Canon-A + key offset enables single-layer induction.

- **Design tradeoffs**: 19% MFU vs 29% without Canon layers (naive implementation overhead; recoverable with fused kernels); 309M parameters keeps inference fast but may lag on stability assays vs structure-aware models like SaProt; single-sequence inference is 10-60s faster than retrieval-augmented methods but sacrifices ~0.04 Spearman ρ.

- **Failure signatures**: Tryptophan (W) predicted at >5% of positions → likely missing inverse logit lens or output normalization issue; attention concentrated on local positions (<30% long-range) → key offset may not be applied or RoPE/NoPE split misconfigured; retrieval always helps or always hurts → entropy std threshold may be miscalibrated for your protein distribution.

- **First 3 experiments**: 
  1. Score wild-type vs single-mutant variants on 5 ProteinGym assays; expect |Δ log-likelihood| to correlate with experimental fitness.
  2. Train a 50M proxy with and without Canon-D; compare convergence speed and perplexity to isolate local pattern contribution.
  3. On held-out assay set, compute entropy std for each protein and compare retrieval-augmented vs single-sequence performance; verify ρ ≈ -0.40 correlation.

## Open Questions the Paper Calls Out

- Does scaling Proust to 1–3B parameters close the performance gap with larger masked language models and retrieval-augmented baselines?
- Does pretraining on concatenated homologs improve Proust's retrieval-augmented performance compared to single-sequence pretraining?
- What is the optimal warmup schedule for the Newton-Schulz-based Muon optimizer in protein language models?
- Can fused CUDA kernel implementations for Canon layers recover the MFU lost in the naive implementation?

## Limitations
- Architectural innovations (GQA-S2, Canon layers) lack direct corpus validation for proteins
- Entropy-based retrieval heuristic shows moderate correlation (ρ=-0.40) but requires further validation across diverse protein families
- 309M parameter model may underperform on stability assays where structure-aware models like SaProt excel

## Confidence
- **High confidence**: Base performance claims (Spearman ρ=0.390 on ProteinGym substitutions, ρ=0.521 on indels, state-of-the-art on indels)
- **Medium confidence**: Architecture-specific claims (GQA-S2 efficiency, Canon layer benefits, entropy-retrieval correlation)
- **Medium confidence**: Training efficiency claims (40 B200 GPU-hours for 33B tokens, 19% MFU)

## Next Checks
1. Train a 50M parameter proxy with and without Canon layers to isolate the contribution of local pattern recognition versus long-range attention
2. On held-out protein fitness assays, compute per-position entropy variance and verify the negative correlation (ρ ≈ -0.40) between entropy std and retrieval augmentation effectiveness
3. Implement GQA-S2 with shared K/V projections and VO-RoPE, then test whether removing inverse RoPE degrades fitness prediction on a subset of ProteinGym assays