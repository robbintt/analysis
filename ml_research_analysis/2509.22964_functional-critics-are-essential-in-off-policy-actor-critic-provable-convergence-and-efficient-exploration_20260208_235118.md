---
ver: rpa2
title: 'Functional Critics Are Essential in Off-Policy Actor-Critic: Provable Convergence
  and Efficient Exploration'
arxiv_id: '2509.22964'
source_url: https://arxiv.org/abs/2509.22964
tags:
- functional
- critic
- off-policy
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper revisits the use of functional critics (policy-conditioned
  value functions) in off-policy actor-critic methods. The authors identify two critical
  aspects that make functional critics necessary rather than optional: (1) they enable
  the first provable convergence result for an off-policy target-based AC algorithm
  under linear function approximation by addressing the deadly triad instability and
  moving target problem, and (2) they are essential for approximating posterior sampling-based
  exploration in model-free RL.'
---

# Functional Critics Are Essential in Off-Policy Actor-Critic: Provable Convergence and Efficient Exploration

## Quick Facts
- arXiv ID: 2509.22964
- Source URL: https://arxiv.org/abs/2509.22964
- Authors: Qinxun Bai; Yuxuan Han; Wei Xu; Zhengyuan Zhou
- Reference count: 35
- Key outcome: Functional critics enable provable convergence for off-policy actor-critic methods and tractable exact off-policy gradient estimation.

## Executive Summary
This paper identifies functional critics—policy-conditioned value functions Q(s,a,θ)—as essential for both theoretical and practical advances in off-policy actor-critic methods. The authors show that functional critics resolve the deadly triad instability by decoupling moving target problems from off-policy learning, enabling the first convergence proof for target-based TD under linear function approximation. Empirically, a minimal actor-critic architecture with functional critics achieves competitive performance on DeepMind Control Suite tasks without relying on standard heuristics like entropy regularization or minimum target values.

## Method Summary
The proposed method uses an ensemble of deterministic actors paired with functional critics. Each functional critic estimates Q(πθ,s,a;ξ) using a transformer-based actor encoder that processes the policy through trainable evaluation samples, a state-action encoder, and a joint encoder. The algorithm performs exact off-policy policy gradient updates using ∇θQ̂(πθ,s,a;ξ), eliminating the need for emphasis weight estimation. Training uses a 3:1 critic-to-actor update ratio, target networks with polyak averaging, and parallel environments for data collection.

## Key Results
- First provable convergence result for off-policy target-based actor-critic under linear function approximation
- Functional critics enable exact off-policy policy gradient estimation without emphasis weights
- Competitive performance with RLPD on DeepMind Control Suite tasks
- Method benefits more from parallel environments compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1: Deadly Triad and Moving Target Resolution Through Policy-Conditioning
Functional critics enable convergent off-policy TD learning by conditioning the value function on policy parameters Q(s,a,θ). This creates a global functional that generalizes across policy space, decoupling the moving target problem from the deadly triad. The functional representation allows target-based TD updates to converge to λ-regularized LSTD solutions without requiring gradient-TD or emphatic-TD methods.

### Mechanism 2: Tractable Exact Off-Policy Gradient via Differentiable Policy Input
The functional critic Q̂(πθ,s,a;ξ) is explicitly differentiable with respect to θ through its policy input, making the second term of the off-policy gradient tractable via ∇θQ̂(πθ,s,a;ξ). This eliminates the need for estimating emphasis weights mθ(s) that account for distribution mismatch, introducing stability by removing additional estimation steps.

### Mechanism 3: Posterior Sampling Approximation via Ensemble of Functional Curves
An ensemble of functional critics enables Thompson sampling-style exploration by approximating a distribution over policy-value mappings. Each functional critic estimates a complete "policy → value" mapping, and the ensemble serves as a particle-based approximation of the posterior over these mappings, enabling posterior sampling in model-free RL.

## Foundational Learning

### Concept: Deadly Triad in Off-Policy RL
- Why needed: Understanding why standard TD-learning diverges under off-policy + function approximation + bootstrapping is essential for appreciating why functional critics enable convergence.
- Quick check: Given a fixed behavior policy μ and target policy π ≠ μ, explain why semi-gradient TD updates with function approximation can diverge, and how target networks change the update dynamics.

### Concept: Off-Policy Policy Gradient with Emphasis Weights
- Why needed: The paper's contribution relies on showing that functional critics eliminate the need for estimating emphasis weights mθ(s) in exact off-policy gradients.
- Quick check: Derive the off-policy policy gradient formula with emphasis weights and identify where the distribution mismatch correction appears.

### Concept: Thompson Sampling and Posterior Sampling RL
- Why needed: The exploration mechanism claims functional critics enable model-free approximation of PSRL; understanding PSRL's structure is necessary.
- Quick check: Explain why maintaining a posterior over MDPs induces a distribution over optimal policies, and why standard critic ensembles do not replicate this in model-free settings.

## Architecture Onboarding

### Component map:
- Actor Encoder (E_act): Transformer-based; processes policy πθ through trainable evaluation samples {ζ_i} to produce policy embedding
- State-Action Encoder (E_sa): MLP; processes (s,a) pairs independently of policy
- Joint Encoder (E_joint): MLP; concatenates actor encoding and state-action encoding, outputs Q̂(πθ,s,a)
- Target Network: Delayed copy of E_sa and E_joint (polyak averaging); shares E_act with online critic
- Actor Ensemble: n deterministic policies; each paired with one functional critic throughout training
- Rollout Selection: At episode start, randomly sample actor index id from {1,...,n} for data collection

### Critical path:
1. Sample transition batch B_C from replay buffer
2. For each critic i: compute TD targets using target functional critic across all actors in ensemble (A_t × B_C)
3. Update functional critic parameters by minimizing MSE loss
4. Update target critics via polyak averaging
5. Sample transition batch B_A; for each actor i, compute exact off-policy gradient using paired critic Q̂^(i) and update actor

### Design tradeoffs:
- Shared vs. separate actor encoders for targets: Paper shares E_act to reduce parameters but sacrifices target independence
- Deterministic vs. stochastic actors: Paper uses deterministic actors with ensemble for exploration, avoiding entropy tuning
- UTD ratio: Paper uses 2-3 critic updates per actor update (vs. 10+ in RLPD)
- Evaluation sample set size: {ζ_i} size (512) trades off actor encoding quality vs. computation

### Failure signatures:
- Critic divergence: If TD loss grows unbounded, check regularization factor λ and target update rate τ
- Poor actor generalization: If actors collapse to similar policies despite ensemble, critic may not be learning distinct value curves
- Stall at 1:1 UTD: Paper reports instability at equal update frequencies
- Exploration failure: If performance plateaus early, the posterior sampling approximation may be insufficient

### First 3 experiments:
1. **Convergence diagnostic on simple MDP**: Implement functional critic with linear approximation on a 5-state chain MDP with known linear features. Verify ξ_t converges to w*_λ(θ_t) as theory predicts.
2. **Ablation: actor encoder architecture**: Test transformer-based E_act vs. simple MLP that takes flattened actor parameters directly. Measure gradient variance ∇θQ̂ and final performance.
3. **Ensemble size sweep**: Run with n ∈ {2, 5, 10, 20} critics/actors on hopper-hop. Plot performance vs. ensemble size and critic variance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a more expressive actor encoder architecture enable stable training at a 1:1 actor-critic update ratio?
- Basis in paper: The authors state they "have not achieved stable training at 1:1 actor-critic update ratio," attributing this instability to the "relatively simple actor encoder design" used for efficiency.
- Why unresolved: The current implementation requires a critic-to-actor update ratio of 2 or 3 to maintain training stability, limiting the speed of policy improvement.

### Open Question 2
- Question: Do the convergence guarantees established for linear functional approximation extend to the non-linear neural network setting?
- Basis in paper: The theoretical analysis is explicitly restricted to linear functional approximation, whereas the practical algorithm utilizes deep neural networks.
- Why unresolved: The proof relies on linear dynamics and specific feature map properties that do not strictly hold for over-parameterized neural networks.

### Open Question 3
- Question: How does the error in the learned feature map (ε in Assumption 1) quantitatively impact the gradient bias b(θt) and final performance?
- Basis in paper: Theorem 3.2 shows convergence to a neighborhood dependent on a bias term b(θt), which arises from the linear approximation error ε of the feature map φ(·).
- Why unresolved: While Assumption 1 assumes a known feature map for analysis, the practical implementation must learn this map, potentially introducing unaccounted variance or bias.

## Limitations

- The convergence proof scope is limited to linear function approximation with known feature maps, while practical implementation uses non-linear neural networks
- The exploration mechanism through functional critic ensembles lacks empirical validation beyond relative performance comparisons
- The framework's generalization to environments with discrete state transitions, long horizons, or sparse rewards is unproven

## Confidence

- **High confidence**: The functional critic architecture resolves the exact off-policy gradient estimation problem by making ∇θQ̂(πθ,s,a;ξ) tractable
- **Medium confidence**: The convergence proof under linear function approximation provides theoretical grounding, but practical neural network instantiation may not fully capture theoretical assumptions
- **Medium confidence**: Empirical performance claims are supported by results on DeepMind Control Suite, but ablation studies and hyperparameter sensitivity are limited

## Next Validation Checks

1. **Feature map approximation experiment**: Implement the linear function approximation version on a simple MDP (5-state chain) with known linear features. Measure whether ξ_t converges to w*_λ(θ_t) as predicted by Theorem 3.2, and compare against standard TD with target network divergence.

2. **Actor encoder architecture ablation**: Compare the transformer-based E_act against a simpler MLP that directly processes flattened actor parameters. Measure gradient variance ∇θQ̂ and final performance on DMC tasks to quantify the value of the transformer architecture.

3. **Ensemble diversity analysis**: For a fixed actor policy, measure the variance across functional critics in the ensemble during training. Track how this variance evolves with ensemble size and whether it correlates with exploration performance.