---
ver: rpa2
title: Efficient LLM Safety Evaluation through Multi-Agent Debate
arxiv_id: '2511.06396'
source_url: https://arxiv.org/abs/2511.06396
tags:
- judge
- safety
- multi-agent
- debate
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of scalable and cost-effective\
  \ safety evaluation of large language models (LLMs) against jailbreak attacks. The\
  \ authors propose a multi-agent judging framework that uses structured debate among\
  \ role-specific agents\u2014critic, defender, and judge\u2014powered by small language\
  \ models (SLMs)."
---

# Efficient LLM Safety Evaluation through Multi-Agent Debate

## Quick Facts
- arXiv ID: 2511.06396
- Source URL: https://arxiv.org/abs/2511.06396
- Reference count: 35
- Multi-agent debate framework achieves 43% cost reduction while maintaining GPT-4o-level accuracy on jailbreak evaluation

## Executive Summary
This paper introduces a novel approach to evaluating large language model safety against jailbreak attacks using a multi-agent debate framework. The system employs small language models (SLMs) in structured debate roles - critic, defender, and judge - to assess whether adversarial prompts successfully bypass safety guardrails. By constructing the HAJailBench benchmark of 12,000 human-annotated adversarial interactions, the authors demonstrate that their SLM-based approach achieves agreement levels comparable to GPT-4o judges while reducing inference costs by approximately 43%.

## Method Summary
The proposed framework uses role-specific agents to simulate structured debate for jailbreak detection. The critic agent generates attack arguments, the defender agent presents safety counterarguments, and the judge agent synthesizes these positions to render a final safety verdict. This multi-round debate process leverages SLMs rather than larger models, with ablation studies identifying three debate rounds as optimal for balancing accuracy and efficiency. The HAJailBench benchmark provides human-annotated ground truth across diverse attack methods and target models, enabling rigorous evaluation of the debate framework's effectiveness.

## Key Results
- SLM-based judging framework achieves agreement comparable to GPT-4o judges on HAJailBench benchmark
- 43% reduction in inference costs compared to using larger language models
- Three rounds of debate identified as optimal balance between accuracy and efficiency
- Framework demonstrates effective semantic capture of jailbreak attack nuances through structured debate

## Why This Works (Mechanism)
The framework leverages structured debate to compensate for SLM capacity limitations. By decomposing the judgment task into specialized roles - critic, defender, and judge - the system creates a deliberative process that surfaces nuanced perspectives on jailbreak attempts. The iterative debate rounds allow agents to refine arguments and counterarguments, with the judge synthesizing information across rounds to reach more informed safety determinations than single-pass evaluation would provide.

## Foundational Learning
- **Multi-agent debate architecture** - Three specialized roles (critic, defender, judge) enable decomposition of complex safety judgments into manageable subtasks, necessary because SLMs cannot process jailbreak evaluations in a single pass.
- **Role-specific prompt engineering** - Each agent receives carefully crafted instructions tailored to their function, critical for ensuring coherent debate progression and meaningful safety judgments.
- **Human-annotated benchmark construction** - HAJailBench provides ground truth for training and evaluation, required because existing jailbreak datasets lack consistent safety labeling standards.
- **Ablation-based optimization** - Systematic testing of debate round counts identifies efficiency-accuracy trade-offs, essential for deployment in cost-sensitive evaluation pipelines.
- **Cost-performance trade-off analysis** - Quantitative comparison between SLM and LLM judges validates the framework's practical viability, needed to justify adoption over established evaluation methods.

## Architecture Onboarding
- **Component map:** User Query -> Critic Agent -> Defender Agent -> Judge Agent -> Safety Verdict (repeated for 3 rounds)
- **Critical path:** Input prompt → Debate rounds (Critic→Defender→Judge) → Aggregated judgment
- **Design tradeoffs:** SLMs vs. LLMs (cost vs. capability), debate rounds (accuracy vs. latency), human annotation vs. automated labeling (quality vs. scalability)
- **Failure signatures:** Inconsistent judge verdicts across debate rounds, critic/defender role confusion, excessive debate length without substantive content evolution
- **First experiments:** 1) Single-round debate baseline comparison, 2) SLM judge accuracy vs. GPT-4o on HAJailBench, 3) Debate round count ablation study

## Open Questions the Paper Calls Out
None

## Limitations
- Framework generalizability to diverse real-world deployment contexts remains uncertain
- Human-annotated benchmark may not fully capture evolving jailbreak technique landscape
- Long-term effectiveness against rapidly evolving attack methodologies unproven

## Confidence
- **High Confidence:** Cost reduction claims and ablation findings are well-supported by empirical results
- **Medium Confidence:** Comparable agreement to GPT-4o judges requires context about quality thresholds
- **Low Confidence:** Framework scalability to production environments with different safety requirements untested

## Next Checks
1. Conduct temporal robustness testing by evaluating framework performance against newly discovered jailbreak techniques
2. Implement cross-dataset validation using alternative safety benchmarks to verify SLM judge performance
3. Perform deployment simulation testing in realistic operational environments to assess practical effectiveness