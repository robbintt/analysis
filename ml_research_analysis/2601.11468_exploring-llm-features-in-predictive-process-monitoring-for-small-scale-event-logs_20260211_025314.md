---
ver: rpa2
title: Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs
arxiv_id: '2601.11468'
source_url: https://arxiv.org/abs/2601.11468
tags:
- process
- traces
- time
- prediction
- total
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Large Language Models (LLMs) can
  effectively perform predictive process monitoring in data-scarce settings. The authors
  extend their prior work by evaluating LLM generality, semantic exploitation, and
  reasoning strategies across multiple KPIs.
---

# Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs

## Quick Facts
- arXiv ID: 2601.11468
- Source URL: https://arxiv.org/abs/2601.11468
- Reference count: 38
- Primary result: LLMs outperform state-of-the-art benchmarks in data-scarce PPM settings using semantic priors and reasoning patterns

## Executive Summary
This paper investigates whether Large Language Models can effectively perform predictive process monitoring when training data is severely limited. The authors extend prior work by evaluating LLM generality, semantic exploitation, and reasoning strategies across multiple KPIs. Experiments on three real-world event logs with only 100 training traces show that non-hashed LLM prompts achieve competitive or superior performance to state-of-the-art benchmarks, with MAE values outperforming CatBoost and PGTNet in most cases. Hashing activity/attribute names significantly degrades LLM performance, confirming semantic knowledge usage. Analysis of 150 reasoning traces reveals that LLMs employ diverse reasoning patterns (β-learners) and outperform these strategies individually, indicating sophisticated reasoning beyond simple replication.

## Method Summary
The study evaluates LLM performance on predictive process monitoring tasks using three real-world event logs (Bpi12, Bac, Hospital) with only 100 training traces per experiment. The approach uses Gemini 2.5 Flash without fine-tuning, relying on In-Context Learning through carefully structured prompts. The framework employs sequential encoding of traces, injecting them into a 7-part prompt template with task instructions. The LLM generates predictions along with reasoning traces, which are analyzed to identify distinct reasoning patterns (β-learners). Experiments are repeated 20 times for statistical validity, comparing MAE and F1 scores against traditional benchmarks like CatBoost and PGTNet.

## Key Results
- Non-hashed LLM prompts achieve MAE values that outperform CatBoost and PGTNet in most cases across three datasets
- Hashing activity/attribute names significantly degrades LLM performance, confirming semantic knowledge usage
- Analysis of 150 reasoning traces reveals that LLMs employ diverse reasoning patterns (β-learners) and outperform these strategies individually

## Why This Works (Mechanism)

### Mechanism 1: Semantic Prior Injection via Activity Labels
The LLM improves predictions by anchoring statistical patterns to the semantic meaning of activity names, rather than treating them as arbitrary tokens. Standard models require hundreds of examples to learn that "Validation" implies a decision step. An LLM approaches the task with pre-trained "embodied knowledge." When input activity names are hashed, the model loses this semantic anchor and must rely purely on shallow pattern matching, causing performance to degrade. The mechanism fails if activity names are adversarial (e.g., "Activity_A", "Activity_B") or hashed, reducing the LLM to a less effective sequence modeler.

### Mechanism 2: Dynamic Ensemble of β-Learners
The LLM does not execute a single deterministic algorithm but dynamically selects or combines distinct predictive strategies (β-learners) based on the specific input trace. Through Chain-of-Thought prompting, the model generates intermediate reasoning that reveals it mimics known heuristics (e.g., k-NN, path prediction). The paper suggests the LLM acts as a meta-learner, outperforming any single static heuristic by flexibly applying the one best suited to the trace's current state. The mechanism fails if the prompt suppresses reasoning output or if the trace is unlike any training example, causing the model to default to a hallucinated guess.

### Mechanism 3: In-Context Trace Correlation
Beyond semantic priors, the LLM extracts specific correlation rules (e.g., "High Loan Amount → Longer Duration") strictly from the limited examples provided in the prompt window. The prompt includes 100 completed traces, and the LLM uses attention mechanisms to weigh the relevance of these examples against the running trace. It identifies local correlations that contradict or refine its general world knowledge. The mechanism fails if the number of provided traces exceeds the model's effective context length, or if the training traces are noisy/outliers, causing the extracted correlations to be spurious.

## Foundational Learning

- **Concept: Event Logs & Traces**
  - Why needed here: The input data structure is not a standard time series but a sequence of events (activities + timestamps). Understanding the difference between global attributes (case-level) and local attributes (event-level) is critical for the encoding step.
  - Quick check question: Can you explain why the authors chose to drop "local attributes" but keep "global attributes" in the prompt?

- **Concept: In-Context Learning (ICL)**
  - Why needed here: The model is not "trained" in the traditional sense (no weight updates). It learns to predict purely by conditioning on the examples provided in the prompt.
  - Quick check question: If you swap the order of the example traces in the prompt, would you expect the prediction to change significantly? (Hint: See Section 4.1 on context length/position bias).

- **Concept: β-Learners (Reasoning Distillation)**
  - Why needed here: This is the interpretability layer. To trust the LLM, the authors reverse-engineer its logic into human-readable models (like "Average of similar cases" or "Path prediction").
  - Quick check question: If the LLM's reasoning matches a "Path Prediction" β-learner, but the actual prediction is much worse than a simple k-NN, what might be happening?

## Architecture Onboarding

- **Component map:** Trace Encoder (ρ_seq) -> Prompt Assembler -> LLM (Gemini 2.5 Flash) -> Response Parser
- **Critical path:** The Prompt Assembler is the highest-leverage component. Errors here (e.g., confusing total_time with duration, or omitting the reasoning header) cause immediate failure or hallucination.
- **Design tradeoffs:**
  - *Sequential vs. Aggregated Encoding:* Sequential (ρ_seq) preserves order but consumes more tokens. Aggregated is token-cheap but loses temporal structure.
  - *Precision vs. Token Limit:* The authors dropped local attributes to avoid hitting context limits and "diluting" the model's attention.
- **Failure signatures:**
  - **Semantic Leakage:** If hashed prompts perform as well as non-hashed, the LLM is likely ignoring the training data and relying solely on priors (or the hash function was reversible).
  - **Outlier Regression:** If the LLM predicts extreme values (e.g., 70,000 mins) where benchmarks predict 10,000, check if it is overweighting a single outlier in the 100 training traces.
- **First 3 experiments:**
  1. **Sanity Check (Hashed vs. Non-Hashed):** Run the framework on the Bpi12 dataset with activity names hashed. Verify that performance degrades significantly (MAE should rise from ~6500 to ~9200) to confirm semantic dependence.
  2. **Beta-Learner Validation:** Manually implement the knn_act_mean β-learner described in Section 5.4.2. Compare its MAE against the LLM. The LLM should strictly outperform it.
  3. **Trace Limit Stress Test:** Reduce training traces from 100 to 20. Observe if the LLM maintains advantage over CatBoost, which should crash faster due to lack of training data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the identified reasoning patterns (β-learners) be operationalized to generate actionable recommendations in a Prescriptive Process Analytics framework?
- Basis in paper: [explicit] The conclusion states future work will "extend this work toward a Prescriptive Process Analytics framework, enabling not only predictions but also actionable recommendations."
- Why unresolved: The current framework successfully predicts outcomes (Total Time, Activity Occurrence) and explains the reasoning, but does not suggest interventions to alter those outcomes.
- Evidence: A framework that maps LLM-identified reasoning traces to specific process modifications, validated by an improvement in future KPIs.

### Open Question 2
- Question: How does the inclusion of local trace attributes impact prediction accuracy and context window efficiency?
- Basis in paper: [inferred] Section 4.1 states that local attributes were intentionally excluded to reduce input size and adhere to Context Length constraints.
- Why unresolved: It is unclear if the exclusion of local attributes resulted in a loss of critical predictive signals, or if compression techniques could allow for their inclusion.
- Evidence: Comparative experiments using context-compression methods (e.g., summarization) to include local attributes without exceeding token limits.

### Open Question 3
- Question: At what volume of training data do traditional fine-tuned models consistently surpass few-shot LLM performance?
- Basis in paper: [inferred] The study is limited to "data-scarce settings" (specifically 100 traces), leaving the scaling behavior relative to benchmarks unexplored.
- Why unresolved: The "tipping point" where the overhead of fine-tuning becomes superior to the semantic leverage of LLMs remains undefined.
- Evidence: A sensitivity analysis varying training set sizes logarithmically (e.g., 100, 500, 1000, full log) to identify the performance crossover point.

### Open Question 4
- Question: How robust are the specific β-learner reasoning strategies across different LLM architectures?
- Basis in paper: [explicit] The conclusion notes that future work will focus on "exploring new LLMs, given the rapid evolution of the field."
- Why unresolved: The findings rely on a single model (Gemini 2.5 Flash); it is uncertain if other architectures (e.g., Llama, GPT) would utilize the same β-learners or hallucinate invalid reasoning patterns.
- Evidence: Cross-model benchmarking using the same 150 analyzed traces to verify the reproducibility of reasoning patterns.

## Limitations

- Results depend on the assumption that LLM reasoning output reflects genuine decision logic rather than post-hoc rationalization
- Semantic mechanism depends on domain-relevant activity labels existing in the LLM's pretraining corpus, which is not empirically verified
- The β-learner analysis is novel and internally consistent but lacks external validation against other interpretable AI methods

## Confidence

- **High Confidence:** The empirical finding that hashing activity names degrades performance (supported by quantitative MAE increases and statistical tests)
- **Medium Confidence:** The claim of superior performance over state-of-the-art benchmarks (results are strong but depend on specific log characteristics and private Hospital data)
- **Medium Confidence:** The interpretation of β-learner diversity and ensemble behavior (logical but relies on the fidelity of extracted reasoning)
- **Low Confidence:** The exact nature of semantic priors the LLM exploits (the pretraining corpus is unknown and the mechanism is inferred)

## Next Checks

1. **Semantic Prior Verification:** Conduct a controlled experiment using adversarial activity names (e.g., "Step_1", "Step_2") to test whether performance collapses to baseline levels, confirming dependence on meaningful semantics.

2. **β-Learner Ground Truth:** Implement the top 3 β-learners as standalone models on the same data. Verify that the LLM's predictions align with the best individual β-learner and that its MAE is strictly better.

3. **Cross-Domain Generalization:** Apply the framework to a new, publicly available event log from a different domain (e.g., manufacturing or IT operations). Test whether the LLM maintains its advantage over traditional models when activity semantics differ significantly from the training corpus.