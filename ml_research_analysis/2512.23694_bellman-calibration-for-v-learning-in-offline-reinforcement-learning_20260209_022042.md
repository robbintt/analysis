---
ver: rpa2
title: Bellman Calibration for V-Learning in Offline Reinforcement Learning
arxiv_id: '2512.23694'
source_url: https://arxiv.org/abs/2512.23694
tags:
- calibration
- bellman
- value
- learning
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Iterated Bellman Calibration, a simple post-hoc
  procedure for calibrating off-policy value predictions in infinite-horizon MDPs.
  The method adapts classical histogram and isotonic calibration to the dynamic, counterfactual
  setting of reinforcement learning by repeatedly regressing fitted Bellman targets
  onto a model's predictions, using a doubly robust pseudo-outcome to handle off-policy
  data.
---

# Bellman Calibration for V-Learning in Offline Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2512.23694
- **Source URL**: https://arxiv.org/abs/2512.23694
- **Reference count**: 40
- **Primary result**: Iterated Bellman Calibration is a post-hoc procedure that calibrates off-policy value predictions in infinite-horizon MDPs, providing finite-sample guarantees for both calibration and prediction error without requiring Bellman completeness or realizability assumptions.

## Executive Summary
This paper introduces Iterated Bellman Calibration, a simple post-hoc procedure for calibrating off-policy value predictions in infinite-horizon MDPs. The method adapts classical histogram and isotonic calibration to the dynamic, counterfactual setting of reinforcement learning by repeatedly regressing fitted Bellman targets onto a model's predictions, using a doubly robust pseudo-outcome to handle off-policy data. The approach requires no Bellman completeness or realizability assumptions and provides finite-sample guarantees for both calibration and prediction error. The method can be applied to any value estimator, including those produced by fitted value and Q iteration.

## Method Summary
The calibration method works by first fitting a base value estimator v̂ on training data, then using a held-out calibration dataset to iteratively learn a transformation θ that maps v̂ predictions to better-calibrated values. Each iteration computes doubly robust Bellman targets and fits a one-dimensional calibrator (histogram or isotonic) that minimizes the squared difference between targets and transformed predictions. The key insight is that restricting the calibrator to functions of the scalar predicted value creates automatic Bellman completeness, ensuring the fixed point exists and is reachable. The algorithm requires nuisance estimators for importance weights, rewards, and transitions to construct the doubly robust targets.

## Key Results
- Iterated Bellman Calibration consistently improves estimation error across different model classes and sample sizes, with 10-15% relative error reduction for neural network estimators
- The method provides finite-sample guarantees for both calibration and prediction error, with statistical error scaling as Õ(√(B/n)) and bias decaying as γ^K
- Histogram and hybrid isotonic-histogram calibration methods show the most robust performance, particularly for under-trained or misspecified models

## Why This Works (Mechanism)

### Mechanism 1: One-Dimensional Fitted Value Iteration
Restricting the calibration function class to transformations of the scalar predicted value creates automatic Bellman completeness. The algorithm learns θ: R→R such that v̂_cal = θ ∘ v̂. Since the coarsened Bellman operator P_{π,v̂} averages P_π over bins of v̂, it maps any step function of v̂ to another step function of v̂. This ensures the fixed point v̂₀, exists and is reachable.

### Mechanism 2: Doubly Robust Off-Policy Correction
The doubly robust pseudo-outcome provides unbiased Bellman targets if either importance weights or the transition/reward model is correct. The target T̂_π(v)(S,A,R,S') = (πq̂_v)(S) + ŵ_π(A|S)[R + γv(S') - q̂_v(S,A)] uses a control variate: when q̂_v = q_v (true Q-function), the bracketed term has mean zero, removing dependence on ŵ_π accuracy.

### Mechanism 3: Iterative Refinement with Finite-Sample Guarantees
Iterating K ≍ log(n) times suffices for convergence because each iteration is a γ-contraction toward the coarsened fixed point. Each calibration step reduces distance to v̂₀,B by factor γ. The statistical error from histogram/isotonic regression (Õ(√(B/n))) does not compound across iterations—it remains a one-step regression error.

## Foundational Learning

- **Bellman Equation and Fixed Points**: Why needed here - the entire calibration framework assumes understanding that v₀ = T_π(v₀) is a fixed-point equation, and calibration enforces a "coarsened" version of this. Quick check: Can you explain why the true value function is the unique fixed point of the Bellman operator?

- **Importance Sampling and Off-Policy Correction**: Why needed here - the doubly robust target requires understanding why w_π = π/b₀ corrects for distribution mismatch, and why high variance is problematic. Quick check: If behavior policy b₀ never takes action a in state s but target policy π does, what happens to w_π(a|s)?

- **Isotonic Regression and Histogram Binning**: Why needed here - the calibration algorithms use classical nonparametric regression tools adapted to the Bellman setting. Quick check: Why might isotonic regression be preferred to histogram binning when the base predictor is already well-calibrated in some regions?

## Architecture Onboarding

- **Component map**: Offline Data → Base Value Estimator v̂ (any method) → Calibration Data C_n (held out) → Nuisance Estimators {ŵ_π, r̂, P̂} → Iterated Bellman Calibration (K iterations) → Calibrated Predictor v̂^(K)

- **Critical path**: The doubly robust target computation (Step 3 in Algorithm 1) is the only step requiring nuisance models. If w_π is known exactly (e.g., logged bandit data), set r̂=0, P̂=0 to simplify.

- **Design tradeoffs**: Histogram vs. Isotonic - histogram requires choosing B (bias-variance tradeoff); isotonic is tuning-free but lacks estimation error guarantees in the paper's current analysis. Hybrid (Algorithm 2) recommended: uses isotonic to learn adaptive bins, then histogram for subsequent iterations.

- **Failure signatures**: Calibration increases error - base predictor likely has negative correlation with true values, or nuisance estimates are severely biased. Calibration helps early iterations but plateaus - finite-iteration error dominated by statistical error; increase sample size or reduce B. Large variance across runs - importance weights have high variance; improve ŵ_π estimation or use stronger transition model.

- **First 3 experiments**: 1) Sanity check on known MDP: Construct a tabular MDP where v₀ is computable exactly. Train a neural network v̂ on partial data, apply calibration, verify RMSE decreases and calibration error approaches zero. 2) Ablation on nuisance quality: Compare calibration with (a) perfect w_π, zero model; (b) zero w_π, perfect model; (c) both estimated. 3) Bin count sensitivity: For histogram calibration, sweep B ∈ {10, 50, 100, 500} and plot calibration error vs. estimation error.

## Open Questions the Paper Calls Out

- **Finite-sample estimation error for isotonic calibration**: Can finite-sample estimation error guarantees be established for iterated isotonic Bellman calibration without relying on a fixed partition? The paper states that extending the analysis to obtain an analogue of Theorem 4 for isotonic calibration is challenging and left for future work.

- **Target distribution calibration**: Can density-ratio estimators for stationary distributions be integrated into the calibration loss to explicitly minimize error under the target policy's stationary distribution? The paper suggests explicitly targeting this remaining distribution mismatch by calibrating under distributions closer to the target policy's stationary distribution.

- **Strong Bellman Calibration**: Can "Strong Bellman Calibration" (calibration with respect to the true Bellman target) be achieved efficiently without requiring accurate estimation of the full Q-function or discounted occupancy ratio? The paper defines strong calibration as a stricter notion but states it is generally unattainable.

## Limitations
- The theoretical analysis relies on distributional assumptions (stationary coverage, bounded densities) that may not hold in practice
- The doubly robust target construction requires accurate nuisance estimates, but practical guidance on nuisance model selection is minimal
- The finite-sample bounds hide dependence on problem-specific constants (overlap κ_B, calibration error ε_B) that could be large in real applications

## Confidence

- **High confidence**: Calibration framework logic, doubly robust target construction, general mechanism of iterated refinement
- **Medium confidence**: Statistical error bounds and their practical implications, sample size requirements
- **Low confidence**: Performance guarantees under model misspecification, choice of calibration iterations K

## Next Checks

1. **Break condition testing**: Systematically vary the correlation between v̂ and v₀ (e.g., add noise, train on limited data) to identify when calibration degrades to noise averaging
2. **Nuisance sensitivity analysis**: Compare calibration performance across (a) perfect importance weights, (b) perfect transition/reward model, (c) both estimated, to verify double robustness in practice
3. **Scaling verification**: Test the B ∝ n^(1/3) scaling by sweeping bin counts across different sample sizes and measuring the bias-variance tradeoff in calibration error