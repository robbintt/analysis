---
ver: rpa2
title: AI Agent for Source Finding by SoFiA-2 for SKA-SDC2
arxiv_id: '2512.00769'
source_url: https://arxiv.org/abs/2512.00769
tags:
- agent
- training
- data
- parameters
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes an AI agent based on Soft Actor-Critic (SAC)
  to automatically optimize parameters for the SoFiA-2 source-finding algorithm. The
  agent interacts with simulated HI spectral cubes from the SKA-SDC2 dataset, using
  the SDC2 score as a reward signal to iteratively adjust key parameters (detection
  threshold, reliability threshold, scale kernel, and minimum SNR).
---

# AI Agent for Source Finding by SoFiA-2 for SKA-SDC2

## Quick Facts
- arXiv ID: 2512.00769
- Source URL: https://arxiv.org/abs/2512.00769
- Reference count: 6
- An SAC-based AI agent automatically optimizes SoFiA-2 source-finding parameters, achieving higher SDC2 scores than benchmark methods in fewer evaluation steps.

## Executive Summary
This work introduces an AI agent using the Soft Actor-Critic (SAC) algorithm to automatically optimize parameters for the SoFiA-2 source-finding algorithm on HI spectral cubes from the SKA-SDC2 dataset. The agent iteratively adjusts four key parameters—detection threshold, reliability threshold, scale kernel, and minimum SNR—using the SDC2 score as a reward signal. After training on simulated data, the agent identifies parameter configurations that outperform the benchmark set by Team SoFiA, achieving superior performance in only 100 evaluation steps while reducing computational time.

## Method Summary
The method employs a Soft Actor-Critic (SAC) agent implemented via `stable-baselines3` with `MlpPolicy` to optimize four continuous parameters for SoFiA-2. The agent interacts with a custom `gymnasium` environment that executes SoFiA-2 on SKA-SDC2 HI spectral cubes, computes the SDC2 Score against ground truth catalogs, and returns a normalized Score Ratio as the reward. Training uses three specific patches (0.0625 deg² each) processed simultaneously over 10,000 total steps, with a complex piecewise reward function based on SR exponential scaling, improvement bonuses, and penalties. The agent's learned parameters are validated on both development and large development datasets, demonstrating improved performance and generalization.

## Key Results
- The SAC agent achieves higher SDC2 scores than the benchmark set by Team SoFiA.
- Optimal parameter configurations are identified within only 100 evaluation steps.
- Feature importance analysis confirms the detection threshold is the most influential parameter.

## Why This Works (Mechanism)
The SAC agent leverages reinforcement learning to efficiently explore the parameter space of SoFiA-2, using the SDC2 score as a reward signal to guide optimization. By iteratively adjusting parameters based on performance feedback, the agent identifies configurations that maximize source detection accuracy while minimizing false positives. The use of multiple training patches simultaneously reduces variance and improves generalization across different sky regions.

## Foundational Learning
- **Soft Actor-Critic (SAC):** A model-free RL algorithm that optimizes both policy and value functions for continuous action spaces. *Why needed:* Enables efficient exploration and optimization of continuous SoFiA-2 parameters. *Quick check:* Verify SAC implementation uses correct hyperparameters and reward shaping.
- **HI Spectral Cubes:** 3D data cubes representing hydrogen line observations in frequency and spatial dimensions. *Why needed:* Input data for source finding; understanding structure aids parameter tuning. *Quick check:* Confirm data preprocessing matches SKA-SDC2 specifications.
- **SDC2 Scoring:** Metric combining source matching and property consistency between detected and true sources. *Why needed:* Provides quantitative reward signal for agent training. *Quick check:* Validate scorer implementation against official SKA-SDC2 tools.

## Architecture Onboarding

**Component Map:** Environment (SoFiA-2 execution + SDC2 Scorer) <- SAC Agent (MlpPolicy) <- Gymnasium Wrapper

**Critical Path:** Parameter initialization → SoFiA-2 execution → Source catalog generation → SDC2 scoring → Reward computation → Agent update → Parameter adjustment

**Design Tradeoffs:** Uses SAC for continuous action space optimization, trading computational complexity for precise parameter control. Employs multiple training patches to balance exploration and generalization.

**Failure Signatures:** Score fails to stabilize or overfits to specific patches; high computational cost due to sequential SoFiA-2 execution.

**First Experiments:**
1. Test agent convergence with default SB3 SAC parameters on a single training patch.
2. Validate SDC2 scorer implementation by comparing scores with official benchmark tools.
3. Perform ablation study on reward function components to assess impact on learning stability.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains may be sensitive to specific training patch selection and parameter initialization.
- Exact neural network architecture details for the SAC policy are not provided, requiring assumptions about default settings.
- Computational cost remains high due to sequential SoFiA-2 execution, though reduced compared to exhaustive search.

## Confidence

**High confidence:** The core methodology (SAC agent for parameter optimization) is sound and the reported performance improvements on benchmark datasets are likely reproducible given access to the data and SoFiA-2.

**Medium confidence:** The reported convergence within 100 evaluation steps and the specific parameter importance ranking depend heavily on implementation details and training patch selection, which may vary.

**Low confidence:** The exact contribution of the reward shaping (Eq 8) versus the agent's exploration strategy to the performance gain is unclear without ablation studies.

## Next Checks
1. **Architecture verification:** Test the impact of varying hidden layer sizes in the MlpPolicy on convergence speed and final SR.
2. **Reward sensitivity analysis:** Perform an ablation study to quantify the effect of each reward component (improvement bonus, penalty, exponential scaling) on learning stability and final performance.
3. **Generalization test:** Validate the agent's performance on additional, unseen sky patches from the SKA-SDC2 dataset to confirm robustness beyond the reported training and test patches.