---
ver: rpa2
title: Rethinking the Potential of Multimodality in Collaborative Problem Solving
  Diagnosis with Large Language Models
arxiv_id: '2504.15093'
source_url: https://arxiv.org/abs/2504.15093
tags:
- https
- data
- learning
- indicators
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the use of multimodal data and large language
  models (LLMs) for automated detection of collaborative problem-solving (CPS) indicators
  in secondary school students. Traditional machine learning models using TF-IDF and
  transformer-based models using BERT were compared for detecting CPS classes using
  audio data.
---

# Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models

## Quick Facts
- arXiv ID: 2504.15093
- Source URL: https://arxiv.org/abs/2504.15093
- Reference count: 40
- Primary result: Transformer-based models (BERT) outperform traditional ML (RF TF-IDF) for CPS indicator detection; multimodal AudiBERT improves social-cognitive CPS diagnosis but not affective states

## Executive Summary
This study investigates the use of multimodal data and large language models for automated detection of collaborative problem-solving (CPS) indicators in secondary school students. Traditional machine learning models using TF-IDF and transformer-based models using BERT were compared for detecting CPS classes using audio data. Multimodal variants were also explored by incorporating audio features into both traditional and transformer-based models. Results showed that transformer-based models outperformed traditional models in detecting CPS classes. While including audio features did not improve traditional models, multimodal transformer-based models (AudiBERT) demonstrated improved performance for diagnosing social-cognitive CPS classes compared to unimodal transformer-based models. The study highlights the potential of multimodality and LLMs for CPS diagnosis but emphasizes that their effectiveness depends on the specific CPS indicators and dataset characteristics.

## Method Summary
The study compares four models for CPS indicator detection: RF TF-IDF (baseline), RF TF-IDF+A (with audio features), BERT, and AudiBERT (multimodal fusion). Data comes from 78 secondary students (26 triads) solving math problems, with 8692 utterances labeled into social-cognitive (12 classes) and affective (3 classes) dimensions. Audio recordings are transcribed via ASR with manual correction, and NER masking is applied. Models are evaluated using weighted precision, recall, and F1 scores with row-normalized confusion matrices. BERT uses bert-base-uncased with AdamW optimization, while AudiBERT employs late fusion of BERT text embeddings and Wav2Vec2.0 audio embeddings through separate BiLSTM and self-attention layers.

## Key Results
- Transformer-based models (BERT) outperformed traditional ML models (RF TF-IDF) for CPS indicator detection
- Multimodal AudiBERT improved performance for social-cognitive CPS indicators (sense-making, solution formulation) compared to unimodal BERT
- Including audio features in traditional models (RF TF-IDF+A) degraded performance due to noise
- Multimodal benefit was heterogeneous across CPS classes, with no improvement for affect dimension

## Why This Works (Mechanism)

### Mechanism 1
Transformer-based models outperform traditional ML for CPS indicator detection because contextual embeddings capture semantic dependencies that sparse representations miss. BERT's bidirectional attention learns word relationships within utterances, enabling detection of nuanced CPS behaviors that depend on how words relate contextually rather than just keyword presence. Core assumption: CPS indicators have learnable semantic patterns that manifest in word sequences and dependencies. Evidence anchors: Both unimodal and multimodal transformer-based models outperformed traditional models in detecting CPS classes. Transformer-based models alleviate this issue by providing a wider range of true positive classifications, enabling prediction of classes that are sparse in the dataset. Break condition: When training instances fall below ~20 per class.

### Mechanism 2
AudiBERT's cross-modal fusion improves social-cognitive CPS detection by capturing acoustic-prosodic cues that complement text semantics. Separate BiLSTM processing of BERT text embeddings and Wav2Vec2.0 audio embeddings, followed by self-attention and concatenation, allows the model to leverage vocal stress patterns, hesitation, and prosody alongside word meaning. Core assumption: Social-cognitive CPS behaviors have both semantic content and acoustic manifestations. Evidence anchors: AudiBERT involving an audio-textual dual self-attentive framework that captures interdependence between different parts of speech in audio and textual form. AudiBERT was able to improve the classification of 'SS1: Sense-making', 'SS3: Formulating a solution', 'SS5: Reaching a solution', 'SS6: Maintaining roles and responsibilities'... Break condition: When labels are semantically-coded, acoustic signal may not align with class boundaries.

### Mechanism 3
Multimodal benefit is heterogeneous across CPS classes—improvement depends on label-acoustic alignment and class complexity. Certain CPS indicators involve cognitive-affective states with acoustic correlates, while others are text-determined. Class distribution also matters: sparse classes benefit less from complex models. Core assumption: Ground-truth labels reflect observable behaviors that may or may not have acoustic signatures. Evidence anchors: Their value is limited to certain types of CPS indicators, affected by the complexity of the labels, and dependent on the composition of indicators in the dataset. Human-coded CPS labels were based on semantic rather than emotional analysis. This consequently can make the labelling more nuanced and aligned with text-based representations. Break condition: Adding audio features to traditional models decreased performance—noise exceeded signal when model couldn't learn cross-modal relationships.

## Foundational Learning

- **Contextual vs. sparse text representations**
  - Why needed here: Understanding why BERT beats TF-IDF requires knowing how attention captures word dependencies that bag-of-words ignores.
  - Quick check question: Why would "Let's try another approach" and "Another approach won't work" have similar TF-IDF vectors but different meanings in CPS detection?

- **Late fusion vs. early fusion in multimodal learning**
  - Why needed here: AudiBERT uses late fusion (concatenating post-attention embeddings); understanding this explains why it succeeded where RF TF-IDF+A (early feature concatenation) failed.
  - Quick check question: What advantage does fusing learned representations have over concatenating raw features before learning?

- **Class imbalance and stratified evaluation**
  - Why needed here: The dataset has severe imbalance (AS3: 8 test samples vs. SC1: 318); weighted F1 is necessary because accuracy would be dominated by majority classes.
  - Quick check question: If 90% of utterances are "SS2: Building shared understanding," why would a model achieving 90% accuracy be useless?

## Architecture Onboarding

- **Component map**:
  - Input: Audio files (16kHz) → ASR (ins8.ai) → transcription → manual correction → NER masking
  - Text branch: BertTokenizer → bert-base-uncased → BiLSTM → self-attention → text embedding
  - Audio branch: Wav2Vec2.0 → CNN embeddings → BiLSTM → self-attention → audio embedding
  - Fusion: Concatenate [text_emb; audio_emb] → classifier head
  - Training: AdamW (lr=2×10⁻⁵, ε=1×10⁻⁸), epoch selection via validation F1 + loss balance

- **Critical path**:
  1. Fine-tune unimodal BERT first (80/20 train/val split, stratified)
  2. Initialize AudiBERT text encoder with fine-tuned BERT weights (transfer learning)
  3. Train AudiBERT with same fine-tuning protocol
  4. Evaluate on held-out test set using weighted precision/recall/F1

- **Design tradeoffs**:
  - Semantic labels limit acoustic benefit: If coders labeled based on word meaning, audio adds noise for affect classes
  - Domain pre-training not tested: Math-specific BERT initialization mentioned but not implemented
  - Single-task vs. multi-task: Social-cognitive and affective trained separately; multi-task could share representations

- **Failure signatures**:
  - Model predicts only majority classes (SS2, SC1, SC2) → insufficient representation learning
  - Multimodal F1 < unimodal F1 → acoustic features introduce noise without discriminative signal
  - Near-zero true positives for sparse classes → need data augmentation or class-weighted loss

- **First 3 experiments**:
  1. **Baseline replication**: Compare BERT vs. RF TF-IDF on your data; confirm transformer advantage holds for your CPS framework
  2. **Per-class multimodal analysis**: Run AudiBERT vs. BERT; compute per-class F1 deltas to identify which indicators benefit from audio
  3. **Modality ablation**: Test text-only BERT, audio-only Wav2Vec2.0 classifier, and fused AudiBERT to quantify each modality's contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Ground truth label quality: Human-coded semantic labels rather than emotion-based coding may explain limited acoustic benefit for affective states
- Dataset size and class imbalance: Several CPS classes have only 8-30 samples, causing high variance in performance estimates
- Implementation details missing: Critical hyperparameters for RF, BiLSTM architecture, and self-attention mechanism specifications are not provided

## Confidence
**High confidence**: Transformer-based models outperform traditional ML; multimodal fusion improves social-cognitive indicators but not affective states; class imbalance significantly impacts predictions.

**Medium confidence**: AudiBERT's cross-modal fusion mechanism provides benefits through late-fusion; heterogeneous multimodal benefit across CPS classes; transfer learning from fine-tuned BERT to AudiBERT provides advantages.

**Low confidence**: Exact architectural details of BiLSTM and self-attention components; whether improvements generalize to different CPS frameworks or domains; impact of NER masking on performance.

## Next Checks
1. **Per-class ablation study**: Run AudiBERT with text-only, audio-only, and fused variants on your dataset; compute per-class F1 differences to identify which CPS indicators genuinely benefit from acoustic features versus those where text suffices.

2. **Class-balanced evaluation**: Resample or use stratified evaluation to ensure minority classes receive adequate representation in validation; compare weighted vs. macro metrics to detect hidden performance gaps.

3. **Acoustic feature analysis**: Train an audio-only classifier (Wav2Vec2.0) on the same data; if it performs poorly, this confirms that semantic coding limits acoustic utility and suggests focusing multimodal efforts on emotion-coded datasets.