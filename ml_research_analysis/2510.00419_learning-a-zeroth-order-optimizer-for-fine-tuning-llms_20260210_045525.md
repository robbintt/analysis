---
ver: rpa2
title: Learning a Zeroth-Order Optimizer for Fine-Tuning LLMs
arxiv_id: '2510.00419'
source_url: https://arxiv.org/abs/2510.00419
tags:
- learning
- fine-tuner
- loss
- memory
- mezo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZO Fine-tuner, a learning-based zeroth-order
  optimizer for fine-tuning large language models (LLMs) that learns adaptive per-block
  perturbation variances instead of using static sampling strategies. Motivated by
  the observation that foundation models and their derivatives are widely adopted
  in practice, the method is designed to be trained once per LLM and reused across
  diverse downstream tasks, enabling efficient "train once, reuse widely" deployment.
---

# Learning a Zeroth-Order Optimizer for Fine-Tuning LLMs

## Quick Facts
- arXiv ID: 2510.00419
- Source URL: https://arxiv.org/abs/2510.00419
- Reference count: 35
- Introduces ZO Fine-tuner, a learning-based zeroth-order optimizer that achieves 2.5% average accuracy improvement over prior methods

## Executive Summary
This paper presents ZO Fine-tuner, a novel learning-based zeroth-order optimizer designed specifically for fine-tuning large language models. The method learns adaptive per-block perturbation variances using lightweight neural networks, exploiting the block-diagonal Hessian structure of LLMs. By training once per model and reusing across diverse downstream tasks, ZO Fine-tuner enables efficient "train once, reuse widely" deployment with minimal memory overhead (less than 2MB for OPT-30B).

## Method Summary
ZO Fine-tuner introduces a learning-based approach to zeroth-order optimization by replacing static perturbation sampling strategies with adaptive per-block variance learning. The method leverages the approximately block-diagonal Hessian structure of LLMs, using lightweight neural networks to learn shared perturbation variances for each parameter block. This design allows the optimizer to be trained once per LLM architecture and reused across multiple downstream tasks, achieving memory efficiency while maintaining competitive performance. The learned variances adapt to the local geometry of the loss landscape, potentially improving convergence compared to traditional zeroth-order methods.

## Key Results
- Achieves 2.5% average accuracy improvement over prior zeroth-order baselines
- Outperforms prior methods in 82.1% of task-model combinations
- Maintains memory overhead under 2MB for OPT-30B (compared to 60GB model size)
- Memory usage comparable to inference, not training

## Why This Works (Mechanism)
ZO Fine-tuner works by learning adaptive perturbation variances that better match the local geometry of the loss landscape. Traditional zeroth-order methods use static sampling strategies that may not account for varying sensitivity across different parameter blocks. By exploiting the block-diagonal Hessian structure of LLMs, the method can learn block-specific perturbation scales that improve gradient estimation quality. The lightweight neural networks used for variance learning have minimal parameters, enabling efficient adaptation without significant computational overhead.

## Foundational Learning

### Zeroth-Order Optimization
- Why needed: Provides gradient estimation without backpropagation, crucial when gradients are unavailable or expensive
- Quick check: Verify that the method maintains zeroth-order characteristics while improving gradient quality

### Block-Diagonal Hessian Structure
- Why needed: Enables parameter grouping for efficient variance learning without exploding memory costs
- Quick check: Confirm that block-diagonal approximation captures sufficient model structure

### Transfer Learning for Optimization
- Why needed: Allows optimizer to be trained once and reused across tasks, reducing per-task optimization overhead
- Quick check: Evaluate performance degradation when applying learned variances to substantially different tasks

## Architecture Onboarding

### Component Map
Model Blocks -> Variance Networks -> Perturbation Generator -> Zeroth-Order Gradient Estimation -> Parameter Update

### Critical Path
The critical path involves learning perturbation variances through neural networks, generating parameter perturbations based on learned variances, estimating gradients from function evaluations, and updating parameters accordingly.

### Design Tradeoffs
- Memory vs. Expressiveness: Lightweight networks keep memory overhead minimal but may limit variance learning capacity
- Transfer vs. Specialization: Training once enables task-agnostic deployment but may sacrifice task-specific optimization
- Static vs. Dynamic: Learned variances provide adaptation but require initial training overhead

### Failure Signatures
- Poor transfer performance indicates learned variances are too task-specific
- Memory overhead exceeding predictions suggests network architecture issues
- Accuracy degradation compared to static methods implies variance learning is ineffective

### First Experiments to Run
1. Baseline comparison: Static vs. learned variance performance on a single task
2. Memory profiling: Verify claimed <2MB overhead for OPT-30B
3. Transfer test: Apply learned variances to out-of-distribution tasks

## Open Questions the Paper Calls Out

## Limitations
- Limited evaluation scope to four LLM architectures and seven datasets may restrict generalizability
- Statistical significance of 2.5% average improvement across different task-model combinations not explicitly reported
- Computational overhead characterization (wall-clock time, energy consumption) not thoroughly addressed

## Confidence

**Major Claim Confidence Labels:**
- ZO Fine-tuner's adaptive variance learning improves accuracy over static methods: **High** (supported by comprehensive experimental results)
- Train once, reuse widely approach maintains effectiveness across diverse tasks: **Medium** (empirically demonstrated but limited task diversity)
- Memory overhead remains negligible compared to model size: **High** (theoretical analysis and empirical measurements provided)

## Next Checks
1. Evaluate transfer performance on out-of-distribution tasks (e.g., code generation, mathematical reasoning) to assess the limits of the "train once, reuse widely" claim.
2. Conduct ablation studies isolating the contribution of learned perturbation variances from other methodological choices.
3. Measure wall-clock training time and energy consumption compared to both first-order and zeroth-order baselines across different hardware platforms.