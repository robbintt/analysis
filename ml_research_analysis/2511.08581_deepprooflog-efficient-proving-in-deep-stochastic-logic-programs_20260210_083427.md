---
ver: rpa2
title: 'DeepProofLog: Efficient Proving in Deep Stochastic Logic Programs'
arxiv_id: '2511.08581'
source_url: https://arxiv.org/abs/2511.08581
tags:
- learning
- nesy
- neural
- logic
- dprl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepProofLog (DPrL) introduces a neurosymbolic AI framework that
  addresses the scalability challenges in probabilistic logic reasoning. Unlike prior
  approaches that rely on possible world semantics, DPrL leverages Stochastic Logic
  Programs with derivation-based semantics and maps the proving process to a Markov
  Decision Process.
---

# DeepProofLog: Efficient Proving in Deep Stochastic Logic Programs

## Quick Facts
- arXiv ID: 2511.08581
- Source URL: https://arxiv.org/abs/2511.08581
- Reference count: 40
- Authors: Ying Jiao; Rodrigo Castellano Ontiveros; Luc De Raedt; Marco Gori; Francesco Giannini; Michelangelo Diligenti; Giuseppe Marra
- Primary result: State-of-the-art performance on MNIST addition and knowledge graph completion with interpretable proofs

## Executive Summary
DeepProofLog (DPrL) introduces a neurosymbolic AI framework that addresses the scalability challenges in probabilistic logic reasoning. Unlike prior approaches that rely on possible world semantics, DPrL leverages Stochastic Logic Programs with derivation-based semantics and maps the proving process to a Markov Decision Process. This allows efficient neural-guided inference and learning using reinforcement learning and dynamic programming techniques. Experiments on MNIST addition and knowledge graph completion show that DPrL achieves state-of-the-art performance, scales to larger problems, and provides interpretable proofs for each prediction, outperforming existing exact and approximate neurosymbolic systems in both accuracy and training efficiency.

## Method Summary
DPrL parameterizes all derivation steps with neural networks, allowing efficient neural guidance over the proving system. It establishes a formal mapping between the resolution process in Stochastic Logic Programs and Markov Decision Processes, enabling the application of dynamic programming and reinforcement learning. The framework uses a goal-conditioned neural parameterization where the policy selects clauses based on compatibility scores between the current goal and candidate next goals. This approach enables learning by aligning the maximization of the MDP expected return with the minimization of the neurosymbolic loss function, allowing for both exact inference via dynamic programming and approximate inference via reinforcement learning.

## Key Results
- Achieves 100% accuracy on MNIST addition task for N=4 using Dynamic Programming, scaling linearly with problem size
- Outperforms DeepStochLog on MNIST addition with N=10, achieving 96.9% accuracy versus 85.5%
- Demonstrates state-of-the-art performance on knowledge graph completion with MRR of 0.285 and Hits@10 of 48.1 on WN18RR dataset

## Why This Works (Mechanism)

### Mechanism 1: Goal-Conditioned Neural Parameterization
Parameterizing the resolution process with neural networks conditioned on the entire current goal enables more flexible, context-aware reasoning than prior stochastic logic approaches. DPrL computes a compatibility score between the current entire goal and candidate next goals by embedding the goal (aggregating atom embeddings) and comparing it with next-step possibilities. This assumes that a continuous embedding space can effectively capture the logical structure and semantic relevance of sub-goals to guide proof search.

### Mechanism 2: Resolution-to-MDP Mapping
Mapping the logical derivation process to a Markov Decision Process allows the application of scalable optimization techniques to otherwise intractable proof searches. The resolution process is re-framed as a trajectory search where states are logical goals, actions are clause selections/unifications, and transitions are deterministic. This reduces the problem of finding a proof to finding an optimal policy that maximizes expected reward (proving positive queries).

### Mechanism 3: Alignment of Inference and RL Objectives
The system achieves learning by aligning the maximization of the MDP expected return with the minimization of the neurosymbolic loss function (cross-entropy/linear loss). A reward of +1 is given for proving a positive query and -1 for a negative one, and the paper proves that maximizing the expected discounted return under this reward scheme is mathematically equivalent to minimizing a linear loss on the query success probability.

## Foundational Learning

- **Concept: Stochastic Logic Programs (SLPs)**
  - Why needed here: DPrL is a deep extension of SLPs. You must understand that SLPs define probability distributions over derivations (proof paths) rather than possible worlds (standard ProbLog).
  - Quick check question: Does the system calculate the probability of a query by summing the weights of all worlds where it is true, or by summing the probabilities of the derivation paths that prove it? (Answer: Derivation paths).

- **Concept: SLD Resolution**
  - Why needed here: The MDP states and actions are built directly on Selective Linear Definite (SLD) resolution steps. Understanding "goals," "clauses," "unification," and the "leftmost atom" selection rule is required to define the action space.
  - Quick check question: In SLD resolution, which atom in the current goal is typically selected for resolution? (Answer: The leftmost atom).

- **Concept: Policy Gradient (PPO/REINFORCE)**
  - Why needed here: For large state spaces, DPrL relies on approximate inference via Reinforcement Learning. You need to understand how a neural policy selects actions (clauses) and how the value function estimates expected returns.
  - Quick check question: In the context of DPrL, what constitutes an "action" for the RL agent? (Answer: Selecting a clause and applying a unification/substitution).

## Architecture Onboarding

- **Component map:** Labeled queries + Knowledge Base -> Neural Policy -> Environment (Logic Program Interpreter) -> Solver (DP or RL) -> Output predictions with proofs
- **Critical path:** Encoding: raw logical goal converted into dense vector e_G -> Action Masking: environment finds all clauses unifiable with leftmost atom -> Selection: Policy computes probabilities for valid actions -> Transition: action sampled, leading to new sub-goal -> Update: gradients computed via DP or RL policy gradient
- **Design tradeoffs:** Exact vs. Approximate: Use DP for small domains (MNIST addition, N < 100) for perfect accuracy, use RL (PPO) for large Knowledge Graphs where state space is infinite/continuous; Expressiveness vs. Tractability: restriction to leftmost atom resolution preserves completeness but may be less efficient than global selection heuristics
- **Failure signatures:** Policy Collapse (RL mode): agent repeatedly selects same subset of clauses, failing to explore required proof paths; Grounding Explosion (DP mode): if logic program is not recursive or depth too high, state space grows exponentially, causing timeouts; Uninformative Embeddings: if aggregation function is too simple, distinct complex goals map to similar vectors, confusing policy
- **First 3 experiments:**
  1. Sanity Check (MNIST Addition N=4): Run DPrL in DP mode, verify neural classifier learns to distinguish digits solely from sum supervision
  2. Scalability Limit (MNIST Addition N > 20): Switch to Policy Gradient mode, compare convergence speed against DeepStochLog
  3. Interpretability (Knowledge Graph): Run inference on known test query in Family dataset, extract and verify generated proof tree corresponds to valid logical rules

## Open Questions the Paper Calls Out
- How can the DeepProofLog framework be extended to support flexible atom selection during resolution rather than restricting selection to the leftmost atom?
- Can the derivation-based MDP mapping in DeepProofLog be effectively adapted to support possible world semantics, such as those found in Markov Logic Networks?
- Can advanced reinforcement learning techniques overcome the convergence failure observed in the Policy Gradient variant for highly complex combinatorial tasks?

## Limitations
- The paper assumes recursive logic programs and leftmost-atom resolution without exploring alternatives
- DP mode is only viable for small N, while RL mode performance on complex KG queries is not thoroughly analyzed
- Interpretability benefits are demonstrated but not quantitatively measured

## Confidence
- **High**: Core claims about MDP formulation, reward alignment proof, and experimental results on MNIST addition
- **Medium**: KG results due to limited detail on negative sampling and policy implementation
- **Low**: Claims about scalability beyond tested ranges and performance in extreme combinatorial settings

## Next Checks
1. Reproduce the MNIST DP results with varying N to identify the exact scalability threshold where RL becomes necessary
2. Implement the PPO agent with the RotatE prior and test on a small subset of WN18RR to verify the sparse reward signal can find valid proofs
3. Extract and validate proof trees from the Family dataset to confirm the interpretability claims with concrete examples of logical inference chains