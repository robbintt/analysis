---
ver: rpa2
title: 'InfiR : Crafting Effective Small Language Models and Multimodal Small Language
  Models in Reasoning'
arxiv_id: '2502.11573'
source_url: https://arxiv.org/abs/2502.11573
tags:
- data
- reasoning
- language
- arxiv
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfiR introduces a novel training pipeline to enhance reasoning
  capabilities in small language models (SLMs) and multimodal small language models
  (MSLMs). The approach involves curated data collection and cleaning for pre-training,
  followed by instruction tuning and multimodal fine-tuning.
---

# InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning

## Quick Facts
- arXiv ID: 2502.11573
- Source URL: https://arxiv.org/abs/2502.11573
- Reference count: 10
- Primary result: InfiR achieves state-of-the-art reasoning performance at 1B scale, with 2.26x and 1.33x improvements over Llama3.2-1B

## Executive Summary
InfiR introduces a novel training pipeline to enhance reasoning capabilities in small language models (SLMs) and multimodal small language models (MSLMs). The approach involves curated data collection and cleaning for pre-training, followed by instruction tuning and multimodal fine-tuning. The resulting models, InfiR-1B-Base and InfiR-1B-Instruct, achieve state-of-the-art performance at the 1B parameter scale, with reasoning-related average score improvements of 2.26x and 1.33x over Llama3.2-1B-Base and Llama3.2-1B-Instruct, respectively. Additionally, the InfiR-VL-1.6B model demonstrates a 28% accuracy increment compared to the best SOTA among small models in the Android World scenario. The research advances practical, efficient AI systems capable of performing reasoning tasks while reducing barriers to adoption and addressing privacy concerns through model size reduction.

## Method Summary
InfiR employs a three-stage training pipeline: pre-training, annealing, and instruction tuning. The base model uses Llama-3.2-1B architecture trained on ~900B tokens from diverse sources including Common Crawl, The Stack v2, and OpenWebMath. This is followed by annealing with ~40B high-quality tokens to refine model capabilities. The instruction tuning phase uses datasets like Infinity-Instruct and ScaleQuest, augmented with synthetic data generated from Qwen-2.5-32B-Instruct. For the multimodal variant, InfiR-VL-1.6B, a SigLip-So400m encoder with MLP projector is employed, trained through curriculum learning that progressively unfreezes components from the LLM backbone to the full model.

## Key Results
- InfiR-1B-Base achieves 2.26x improvement in reasoning-related average scores compared to Llama3.2-1B-Base
- InfiR-1B-Instruct demonstrates 1.33x improvement over Llama3.2-1B-Instruct in reasoning tasks
- InfiR-VL-1.6B shows 28% accuracy improvement on Android World benchmark compared to SOTA small models
- Models are optimized for edge deployment, addressing privacy concerns and reducing adoption barriers

## Why This Works (Mechanism)
The InfiR pipeline works by systematically enhancing reasoning capabilities through three complementary stages. Pre-training establishes broad knowledge foundations across diverse domains. Annealing refines the model with high-quality, reasoning-focused data to improve logical coherence. Instruction tuning with synthetic data generation and rejection sampling further specializes the model for complex reasoning tasks. For multimodal reasoning, the curriculum learning approach ensures stable training by gradually integrating visual understanding capabilities while maintaining strong language reasoning foundations.

## Foundational Learning
- **Curriculum Learning**: Training strategy that starts with simpler tasks and progressively increases complexity. Why needed: Enables stable training of complex multimodal models by preventing catastrophic forgetting. Quick check: Verify that each training stage shows improved performance over the previous stage.
- **Synthetic Data Generation**: Creating training data using larger models to augment instruction tuning. Why needed: Provides diverse, high-quality examples for reasoning tasks beyond available human-annotated data. Quick check: Compare model performance with and without synthetic data augmentation.
- **Multimodal Integration**: Combining language and vision models through shared representations. Why needed: Enables reasoning across text and visual inputs for comprehensive understanding. Quick check: Test model performance on pure text versus multimodal tasks to assess integration quality.
- **Long Chain-of-Thought Training**: Training models with extended reasoning chains to improve logical coherence. Why needed: Helps models develop better reasoning capabilities for complex problem-solving. Quick check: Evaluate performance on reasoning tasks with varying complexity levels.
- **Data Curation with FastText**: Using text classification models to filter and select relevant training data. Why needed: Ensures high-quality, domain-specific data for efficient model training. Quick check: Measure performance improvements when using curated versus raw datasets.
- **Rejection Sampling**: Filtering synthetic data based on quality metrics before training. Why needed: Prevents low-quality examples from degrading model performance during fine-tuning. Quick check: Compare model performance using different rejection thresholds.

## Architecture Onboarding

**Component Map**: Pre-training (900B tokens) -> Annealing (40B tokens) -> SFT (synthetic + curated) -> MSLM Fine-tuning (ViT integration)

**Critical Path**: The three-stage training pipeline represents the critical path, with each stage building upon the previous one. The progression from general pre-training to reasoning-focused annealing to instruction-tuned specialization ensures comprehensive capability development.

**Design Tradeoffs**: The paper prioritizes reasoning capabilities over general knowledge breadth, which may limit performance on non-reasoning benchmarks. The choice of 1B parameter scale balances efficiency with capability, making the model suitable for edge deployment but potentially limiting maximum performance compared to larger models.

**Failure Signatures**: 
- Gradient norm overflow during pre-training when mixing diverse data domains
- Catastrophic forgetting during multimodal fine-tuning if curriculum learning is not followed
- Overfitting during instruction tuning with synthetic data if rejection sampling thresholds are too lenient

**Three First Experiments**:
1. Train base model with different data mixing ratios to identify optimal composition for reasoning performance
2. Evaluate impact of synthetic data quantity on instruction tuning effectiveness using various rejection sampling thresholds
3. Test curriculum learning effectiveness by comparing full unfreeze versus staged unfreeze approaches in MSLM training

## Open Questions the Paper Calls Out
- The method's generalization ability in real-world scenarios requires further exploration, as current evaluation relies heavily on academic benchmarks
- The effectiveness of integrating synthetic data into pre-training without causing distribution gaps remains an open challenge
- The saturation point for Long Chain-of-Thought data scaling when fine-tuning 1B-parameter models has not been established

## Limitations
- Exact data mixing ratios for pre-training and annealing datasets are not specified, creating reproducibility challenges
- The prompts used for synthetic data generation and "Instruction Evolution" are not detailed in the paper
- Implementation details of the model-based classifier for math quality scoring remain unclear
- Statistical significance of the reported 2.26x and 1.33x improvements is not established

## Confidence
- **High confidence**: Architecture specification (Llama-3.2-1B base) and training pipeline structure with clear hyper-parameters
- **Medium confidence**: Benchmark results on standard reasoning tasks (MMLU, GSM8K, MATH) where methodology is explicit
- **Medium confidence**: MSLM performance claims on Android World, though evaluation methodology could benefit from additional clarity

## Next Checks
1. Reconstruct exact data mixing ratios by training ablation models with different proportions of the listed 900B pre-training sources
2. Implement and evaluate the synthetic data generation pipeline using Qwen-2.5-32B-Instruct with various rejection sampling thresholds
3. Conduct statistical significance testing on benchmark improvements by running multiple evaluation seeds and calculating confidence intervals for the reported 2.26x and 1.33x performance gains