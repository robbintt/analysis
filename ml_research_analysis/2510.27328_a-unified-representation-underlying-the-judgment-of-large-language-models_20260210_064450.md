---
ver: rpa2
title: A Unified Representation Underlying the Judgment of Large Language Models
arxiv_id: '2510.27328'
source_url: https://arxiv.org/abs/2510.27328
tags:
- answer
- think
- number
- wealth
- numbers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper reveals that diverse evaluative judgments in large language
  models (LLMs) are computed along a single dominant dimension, the Valence-Assent
  Axis (VAA). This axis unifies subjective preferences ("what is good") and factual
  endorsement ("what is true").
---

# A Unified Representation Underlying the Judgment of Large Language Models

## Quick Facts
- arXiv ID: 2510.27328
- Source URL: https://arxiv.org/abs/2510.27328
- Authors: Yi-Long Lu; Jiajun Song; Wei Wang
- Reference count: 40
- The paper reveals that diverse evaluative judgments in large language models (LLMs) are computed along a single dominant dimension, the Valence-Assent Axis (VAA).

## Executive Summary
This paper investigates how large language models (LLMs) make evaluative judgments across diverse domains including subjective preferences, factual endorsements, and moral assessments. The authors identify a single dominant dimension called the Valence-Assent Axis (VAA) that unifies these seemingly disparate forms of judgment. The VAA functions as a control signal that steers reasoning processes to construct rationales consistent with its evaluative state, even when this comes at the cost of factual accuracy. This leads to a phenomenon termed "subordination of reasoning," where models generate coherent but factually flawed justifications.

## Method Summary
The researchers conducted systematic experiments across eight different LLMs of varying architectures and scales. They measured evaluative judgments across multiple domains including subjective preferences, factual endorsements, and moral assessments. Through correlation analysis and dimensional reduction techniques, they identified the VAA as the primary axis along which these judgments varied. The team then examined how this axis influences reasoning processes by analyzing the rationales generated by models when operating at different points along the VAA.

## Key Results
- The Valence-Assent Axis (VAA) unifies subjective preferences ("what is good") and factual endorsement ("what is true") across diverse evaluative judgments
- The VAA functions as a control signal that steers reasoning to construct rationales consistent with its evaluative state
- This mechanism leads to "subordination of reasoning," where models generate coherent but factually flawed justifications
- The effect was demonstrated across eight LLMs of varying architectures and scales, suggesting the VAA is a general mechanism for goal-directed argumentation

## Why This Works (Mechanism)
The VAA represents a unified internal representation that captures the model's overall evaluative stance. When a model operates at a particular point along this axis, it uses this state to guide subsequent reasoning and justification generation. The mechanism works by having the VAA influence attention patterns and token selection during the generation process, effectively biasing the model toward constructing arguments that align with its current evaluative position. This creates a coherent narrative that may sacrifice factual accuracy when the VAA's demands conflict with ground truth.

## Foundational Learning
- **Dimensional reduction techniques**: Needed to identify the dominant axis in high-dimensional judgment space; quick check involves applying PCA to judgment data
- **Correlation analysis**: Required to establish relationships between different types of evaluative judgments; quick check involves computing Pearson correlation coefficients
- **Control signal theory**: Understanding how internal states can guide reasoning processes; quick check involves testing whether manipulating VAA affects subsequent outputs
- **Goal-directed reasoning**: Framework for understanding how models construct rationales to support desired conclusions; quick check involves prompting models to justify opposing positions
- **Trade-off between coherence and accuracy**: Recognizing that models may prioritize narrative consistency over factual correctness; quick check involves comparing outputs when accuracy is explicitly prioritized

## Architecture Onboarding

**Component Map**
Input -> VAA computation -> Reasoning steering -> Output generation

**Critical Path**
The VAA computation occurs early in the reasoning process and continuously influences token generation throughout the critical path, creating a feedback loop between evaluative state and output construction.

**Design Tradeoffs**
The unified VAA representation enables efficient computation of diverse judgments but creates vulnerability to coherence-over-accuracy trade-offs. Alternative designs might maintain separate representations for different judgment types, potentially at the cost of computational efficiency.

**Failure Signatures**
- Coherent but factually incorrect outputs
- Inability to simultaneously maintain high coherence and high accuracy
- Systematic bias toward generating justifications that align with current evaluative state
- Resistance to factual corrections when they conflict with established narrative

**First 3 Experiments**
1. Measure VAA strength across different model families and sizes
2. Test whether explicit accuracy prompts can override VAA influence
3. Examine how VAA states evolve during multi-step reasoning chains

## Open Questions the Paper Calls Out
None

## Limitations
- The sample size of eight LLMs, while diverse, remains relatively small compared to the full range of available models
- The study focuses primarily on English-language reasoning tasks, raising questions about cross-linguistic generalization
- The analysis of "subordination of reasoning" is based on observed patterns rather than controlled experimental manipulation
- Causality between VAA states and factual accuracy compromises cannot be definitively established from observational data

## Confidence
- **High Confidence**: The existence of a dominant evaluative dimension (VAA) that correlates with both subjective preferences and factual endorsement across multiple LLMs
- **Medium Confidence**: The interpretation that VAA functions as a control signal for goal-directed reasoning and the characterization of "subordination of reasoning" as a systematic phenomenon
- **Low Confidence**: The claim that VAA is a general mechanism across all LLM architectures and the extent to which the subordination effect generalizes to real-world applications

## Next Checks
1. Conduct cross-linguistic validation by testing the VAA hypothesis across models trained on different language corpora and evaluating whether the same evaluative dimension emerges
2. Design controlled experiments where models are explicitly prompted to prioritize either factual accuracy or coherence, measuring whether VAA states predictably shift and whether factual accuracy consistently decreases in service of coherence
3. Test whether targeted interventions (such as adversarial prompting or fine-tuning) can successfully decouple subjective preferences from factual endorsement along the VAA, providing causal evidence for the mechanism's function