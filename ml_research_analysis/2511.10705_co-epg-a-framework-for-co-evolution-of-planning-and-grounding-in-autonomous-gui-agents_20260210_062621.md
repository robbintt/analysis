---
ver: rpa2
title: 'Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous
  GUI Agents'
arxiv_id: '2511.10705'
source_url: https://arxiv.org/abs/2511.10705
tags:
- arxiv
- grounding
- training
- planning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Co-EPG, a self-iterative training framework
  for co-evolution of planning and grounding in autonomous GUI agents. The core idea
  is to establish an iterative positive feedback loop where the planning model explores
  strategies under grounding-based reward guidance via GRPO, generating diverse data
  to optimize the grounding model, which in turn provides more effective rewards for
  the planning model.
---

# Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents

## Quick Facts
- arXiv ID: 2511.10705
- Source URL: https://arxiv.org/abs/2511.10705
- Reference count: 22
- Co-EPG achieves 58.4% average Step Success Rate on Multimodal-Mind2Web and 83.1% average Step Accuracy on AndroidControl after three iterations without external data.

## Executive Summary
This paper introduces Co-EPG, a self-iterative training framework that enables autonomous GUI agents to improve planning and grounding capabilities simultaneously through a positive feedback loop. The core innovation is establishing a closed-loop system where the planning model explores strategies under reward guidance from the grounding model via GRPO, generating diverse data to optimize the grounding model, which in turn provides more effective rewards for the planning model. A confidence-based dynamic reward ensemble mechanism (C-DREM) reduces reward noise by aggregating multiple grounding models with confidence-based weighting. Experiments on Multimodal-Mind2Web and AndroidControl benchmarks demonstrate consistent improvement across three iterations, achieving state-of-the-art performance without requiring external data.

## Method Summary
Co-EPG operates through a three-iteration self-evolution loop. First, both planning (π) and grounding (ϕ) models are fine-tuned on initial benchmark data using Qwen2.5-VL backbones. The planning model generates action plans and action specifications, while the grounding model predicts target coordinates. During GRPO training, the planning model explores strategies using rewards computed by C-DREM—an ensemble of three grounding models with confidence-weighted aggregation. The planning model's successful plans are distilled to enhance the grounding model through SFT. In subsequent iterations, updated models join Planner and Verifier pools to generate diverse, high-purity training data. The process repeats for three iterations, with each cycle showing measurable improvements in both plan quality and grounding accuracy.

## Key Results
- Co-EPG achieves 58.4% average Step Success Rate on Multimodal-Mind2Web and 83.1% average Step Accuracy on AndroidControl
- Data purity improves from 79.42% to 88.26% across three iterations
- Plan diversity increases from 2.25 to 6.11 plans per task
- C-DREM with confidence weighting outperforms single reward model by 1.91% absolute on Step Success Rate

## Why This Works (Mechanism)

### Mechanism 1: Positive Feedback Loop via GRPO-Guided Co-Evolution
The closed-loop co-evolution between planning and grounding models enables continuous self-improvement without external data, driven by reciprocal optimization signals. The planning model generates action plans under reward guidance from the grounding model via GRPO; improved plans create diverse training data for the grounding model, which then provides higher-quality rewards for subsequent planning training. Plans serve as the semantic bridge between models.

### Mechanism 2: C-DREM Reduces Reward Variance via Confidence-Weighted Ensembling
Aggregating multiple grounding models with dynamic confidence-weighted rewards reduces reward noise and accelerates GRPO convergence. C-DREM computes rewards as weighted sums across N grounding models using normalized token log-likelihood as confidence scores, with weights determined by softmax(σ_j · c_j).

### Mechanism 3: Iterative Data Enhancement Improves Purity and Diversity
Self-evolution improves both data purity (plan executability) and diversity (plan variety) across iterations, enabling generalization gains. Higher-purity, more diverse training data yields better cross-domain generalization as verified plans expand the training corpus.

## Foundational Learning

- **Partially Observable Markov Decision Process (POMDP)**: GUI tasks are formulated as POMDPs where agents receive partial observations (screenshots, HTML) but must act in the full state space. Why needed: Models the fundamental uncertainty in GUI navigation. Quick check: Why is a GUI environment modeled as partially observable rather than fully observable?

- **Group Relative Policy Optimization (GRPO)**: GRPO is the core RL algorithm for planning model training, normalizing rewards across rollout groups to compute advantages. Why needed: Provides stable policy updates with reduced variance. Quick check: How does GRPO's group-based advantage computation differ from standard PPO's advantage estimation?

- **Reward Model Ensembling for RLHF**: C-DREM constructs a composite reward model from grounding model outputs; understanding reward noise sources is critical for debugging. Why needed: Addresses reward variance and instability in RL training. Quick check: Why might a single reward model produce high-variance gradients during policy optimization?

## Architecture Onboarding

- **Component map**: Planning Model π (Qwen2.5-VL-7B) -> outputs plan text + action type + action value -> Grounding Model ϕ (Qwen2.5-VL-7B) -> outputs target coordinates -> C-DREM Ensemble (ϕ_k + Qwen2.5-VL-72B + Qwen2.5-VL-32B) -> outputs weighted reward r_i -> GRPO Trainer -> updates π_k → π'_k -> Data Verifier Φ -> produces D_{k+1}

- **Critical path**:
  1. SFT both models on initial dataset D_0 (verified plans from Planner/Verifier pools)
  2. Generate G=7 rollouts per group with current π_k
  3. Compute rewards via C-DREM (plan accuracy from grounding + type/value exact/F1 match)
  4. Apply GRPO: A_i = (r_i - mean) / std, update policy
  5. Distill high-quality data from π'_k to enhance ϕ_k via SFT
  6. Update Planner/Verifier pools; repeat for 3 iterations

- **Design tradeoffs**:
  - Prior weight ratio (1:1:2): Prioritizes trained grounding model for production relevance; 1:1:3 shows diminishing returns
  - Rollout group size G=7: Larger G improves advantage estimation but linearly increases inference cost
  - Iteration count: Gains observed through iteration 3; beyond this is untested

- **Failure signatures**:
  - Vague plans ("the button" vs "the 'Submit' button in top-right") indicate planning model hasn't learned grounding-friendly descriptions
  - Coordinates consistently outside bounding boxes suggest grounding-planning misalignment
  - Oscillating rewards without convergence indicates reward noise overwhelming learning signal
  - Data purity plateauing or declining signals error propagation between iterations

- **First 3 experiments**:
  1. Reproduce C-DREM vs single-reward-model ablation on a held-out validation split; log reward variance and convergence speed
  2. Sweep prior weight ratios (1:1:1, 1:1:1.5, 1:1:2, 1:1:3) on iteration 1 to verify optimal weighting generalizes beyond the paper's benchmarks
  3. Analyze plan diversity: For 50 fixed tasks, count distinct valid plans from π_1 vs π_3; correlate diversity gains with cross-domain Step SR improvements

## Open Questions the Paper Calls Out
The paper explicitly states that applying Co-EPG with other data synthesis techniques could unlock even greater data potential, suggesting future work combining the framework with external data generation methods.

## Limitations
- Performance relies entirely on benchmark-provided data, leaving open questions about truly unseen GUI environments
- C-DREM ensemble effectiveness hinges on confidence scores correlating with accuracy, which needs validation across diverse UI contexts
- Planner/Verifier pool composition remains underspecified, making exact reproduction difficult
- Error propagation between iterations is not explicitly tested, raising concerns about early suboptimal plans corrupting later training

## Confidence
- Mechanism 1 (Positive feedback loop): **High** - Well-specified algorithm with clear GRPO training procedure and measurable purity/diversity improvements
- Mechanism 2 (C-DREM): **Medium** - Ablation shows benefit over single reward model, but confidence-weighting effectiveness needs more diverse UI context validation
- Mechanism 3 (Data enhancement): **Medium** - Purity and diversity metrics are reported, but relationship to cross-domain generalization requires further testing

## Next Checks
1. **Error propagation analysis**: Run iteration 3 on corrupted training data (10-30% artificially bad plans) to quantify how early errors compound through the feedback loop
2. **Cross-domain transfer test**: Evaluate the final Co-EPG model on a held-out domain (e.g., mobile apps vs web interfaces) not represented in either benchmark to assess true generalization
3. **Confidence calibration study**: On a diverse set of 100 UI screenshots, compute correlation between C-DREM confidence scores and actual grounding accuracy across different element types to validate the weighting mechanism's reliability