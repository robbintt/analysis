---
ver: rpa2
title: 'Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language
  Model Fine-Tuning'
arxiv_id: '2510.21885'
source_url: https://arxiv.org/abs/2510.21885
tags:
- safety
- sampling
- data
- random
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  language model safety alignment during fine-tuning. The authors propose a behavior-aware
  sampling framework that selects safety examples based on instruction-response behavior
  (e.g., refusal versus compliance) and semantic diversity across harm categories.
---

# Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning

## Quick Facts
- arXiv ID: 2510.21885
- Source URL: https://arxiv.org/abs/2510.21885
- Reference count: 19
- Primary result: Behavior-aware sampling achieves up to 41% harm reduction with 0.05% additional data

## Executive Summary
This paper addresses catastrophic forgetting in language model safety alignment during fine-tuning. The authors propose a behavior-aware sampling framework that selects safety examples based on instruction-response behavior (e.g., refusal versus compliance) and semantic diversity across harm categories. Their method achieves significant safety improvements with minimal additional training data, demonstrating that targeted data selection can improve safety efficiency during fine-tuning.

## Method Summary
The authors propose a behavior-aware sampling framework that selects safety examples based on two complementary factors: instruction-response behavior (e.g., refusal versus compliance) and semantic diversity across harm categories. The framework evaluates four sampling strategies: stratified sampling (SSS), prototypical sampling (PSS), cosine similarity sampling (Cossim), and their behavioral variants (SSS-B, PSS-B, Cossim-B). They apply these methods to augment base fine-tuning datasets with safety examples, using LoRA for parameter-efficient training.

## Key Results
- SSS-B achieves 41% reduction in harmfulness with only 0.05% additional training data
- Attack success rate reduced by up to 9% compared to random sampling baseline
- T1 (refusal) examples provide disproportionately effective safety signals
- Semantic diversity across harm categories improves generalization to adversarial inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Refusal-type (T1) examples provide a disproportionately effective safety signal compared to compliant or mixed behavior types.
- Mechanism: T1 examples pair harmful instructions with explicit refusals, creating a clear input-output pattern that reinforces recognition of harmful intent and safe abstention behavior.
- Evidence: SSS-B yields harmfulness score of 1.19±0.14 vs 1.48±0.11 from Random (19.6% improvement); Figure 2 shows T1-based subsets consistently outperform others.

### Mechanism 2
- Claim: Semantic diversity across harm categories improves generalization to unseen adversarial inputs.
- Mechanism: Stratified sampling across predefined harm categories ensures the fine-tuning process encounters diverse harmful intent patterns.
- Evidence: SSS reduces harmfulness to 1.32±0.06 vs 1.48±0.11 with Random (11% improvement); SALAD-Bench shows SSS achieves ASR of 23.40%±1.08 vs 27.39%±3.67 with Random.

### Mechanism 3
- Claim: Combining T1 behavioral filtering with categorical diversity produces synergistic safety gains.
- Mechanism: T1 filtering ensures clear refusal signals while categorical diversity ensures coverage breadth across harm space.
- Evidence: SSS-B and Cossim-B perform similarly and outperform single-dimension approaches; 49.8% harm reduction at 150 samples; 25.6% reduction in ASR relative to Random.

## Foundational Learning

- Concept: **Catastrophic Forgetting**
  - Why needed: The paper addresses preserving safety alignment during fine-tuning; understanding that neural networks can overwrite previously learned behaviors is prerequisite.
  - Quick check: Can you explain why fine-tuning on benign data might degrade safety behaviors, even when the new data contains no harmful content?

- Concept: **Supervised Fine-Tuning (SFT) with LoRA**
  - Why needed: The experimental setup uses Low-Rank Adaptation for efficient fine-tuning; understanding parameter-efficient training helps interpret why small data additions can have outsized effects.
  - Quick check: What is the difference between full fine-tuning and LoRA, and why might LoRA change how safety signals are integrated?

- Concept: **Behavioral Taxonomy for Safety (T1-T4)**
  - Why needed: The paper's core contribution depends on classifying examples by instruction-response behavior; without this framework, the sampling strategies cannot be implemented.
  - Quick check: Given a harmful instruction and a compliant-but-safe response, which behavior type (T1-T4) would this exemplify, and why might it be less effective than a refusal response?

## Architecture Onboarding

- Component map: Safety Dataset Pool -> Behavior Classifier -> Category Labeler -> Embedding Encoder -> Sampling Module -> Augmentation Pipeline -> Evaluation Suite

- Critical path:
  1. Filter safety pool to T1-type examples using behavior classifier
  2. Assign harm categories to filtered examples using LLM labeling
  3. Compute embeddings and category centroids or prepare stratified sampling
  4. Sample k examples per category using chosen method
  5. Augment base dataset with sampled subset
  6. Fine-tune with LoRA (3 epochs, batch size 128)
  7. Evaluate on safety benchmarks and helpfulness metrics

- Design tradeoffs:
  - Deterministic vs. Stochastic: PSS/Cossim are reproducible but may miss edge cases; SSS introduces variance but often achieves stronger low-data performance
  - Sample size vs. Over-rejection: Larger samples improve safety but increase over-rejection; optimal range appears to be 150-350 samples
  - Centroid vs. Stratified: Centroid-based (PSS) selects prototypical examples; stratified (SSS) ensures uniform coverage—neither dominates across all sizes

- Failure signatures:
  - Insufficient safety gain: Random sampling baseline not outperformed → check behavior classifier accuracy
  - High over-rejection (>20% on XSTest): Safety sample size too large → reduce to 150-250 range
  - High variance across runs: Using SSS at small sample sizes → switch to deterministic PSS or increase trials
  - Category imbalance: Some harm types underprotected → verify category labeling quality

- First 3 experiments:
  1. Reproduce T1 vs. other behavior types: Fine-tune with 50 examples each of T1, T2, T3, T4 separately; measure harmfulness on BeaverTails-Evaluation.
  2. Compare SSS-B vs. Random at 150 samples: Measure harmfulness score and ASR on SALAD-Bench.
  3. Measure over-rejection curve: Run SSS-B at sample sizes [50, 100, 150, 250, 500, 1000]; evaluate on XSTest for over-rejection rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying mechanism that makes refusal-type (T1) behavioral examples particularly effective for safety alignment compared to other response types?
- Basis: The Future Work section states that "the underlying mechanism remains unverified" regarding why T1 examples provide a strong safety signal.
- Why unresolved: The paper empirically demonstrates effectiveness but does not analyze internal model states or gradients.
- Resolution: A gradient-level analysis or causal intervention study identifying how T1 examples alter model representations.

### Open Question 2
- Question: How does gradient interference or compatibility between safety examples and fine-tuning tasks influence the preservation of alignment?
- Basis: The authors explicitly call for "a formal study of gradient interference or compatibility" to understand features beyond just refusal behaviors.
- Why unresolved: The current study focuses on input-level features but does not analyze optimization dynamics during fine-tuning.
- Resolution: Experiments measuring gradient alignment or conflict between safety data and downstream task data during training.

### Open Question 3
- Question: Can dynamic or adaptive sampling strategies that adjust based on real-time model behavior outperform static, one-shot sampling methods?
- Basis: The Future Work section notes "future work could explore dynamic or adaptive strategies that adjust to model behavior during training."
- Why unresolved: The proposed framework selects all safety samples before training begins.
- Resolution: A comparative study where the safety dataset is updated iteratively during training based on real-time safety metrics.

## Limitations
- Dataset composition bias: Evaluation relies heavily on BeaverTails-derived datasets which may not represent real-world adversarial distributions.
- Behavior classifier dependency: Effectiveness depends on WildGuard's accuracy; errors could lead to suboptimal example selection.
- Taxonomy coverage gaps: The 14-category harm taxonomy may not capture emerging harm types or culturally-specific harmful content.

## Confidence
- High confidence: The claim that T1 examples outperform other behavior types in safety preservation is well-supported by multiple experiments.
- Medium confidence: The synergistic effect of combining behavioral filtering with categorical diversity is demonstrated but requires careful interpretation.
- Medium confidence: The efficiency claim (41% harm reduction with 0.05% additional data) is technically accurate but represents an optimal case.

## Next Checks
1. Cross-dataset generalization test: Evaluate the same sampling strategies on distinct safety benchmark (e.g., RealToxicityPrompts or AdvBench).
2. Behavior classifier ablation: Compare sampling results when using alternative behavior classifiers (e.g., GPT-4, Claude).
3. Long-term stability assessment: Fine-tune a model using SSS-B, then perform additional fine-tuning on benign data after 1-3 months to measure safety gain persistence.