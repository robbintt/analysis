---
ver: rpa2
title: Provably Learning Attention with Queries
arxiv_id: '2601.16873'
source_url: https://arxiv.org/abs/2601.16873
tags:
- attention
- queries
- learning
- single-head
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the problem of learning the parameters of Transformer-based
  sequence models from black-box value queries, where an adversary can adaptively
  query any sequence of vectors and observe real-valued outputs. The authors focus
  on single-head attention models and one-layer Transformers, providing the first
  provable guarantees for exact parameter recovery in this setting.
---

# Provably Learning Attention with Queries

## Quick Facts
- **arXiv ID:** 2601.16873
- **Source URL:** https://arxiv.org/abs/2601.16873
- **Reference count:** 5
- **Primary result:** First provable guarantees for exact parameter recovery of single-head attention from black-box value queries using O(d²) queries

## Executive Summary
This work establishes the first provable guarantees for learning parameters of single-head attention models from black-box value queries. The authors demonstrate that exact parameter recovery is possible using O(d²) queries by exploiting the softmax-to-sigmoid reduction for length-two sequences. They extend this to the low-rank regime (where rank(W*) ≤ r ≪ d) using O(rd) queries via compressed sensing, and analyze robustness to noisy oracle access. The paper also proves that multi-head attention parameters are not identifiable from value queries in general, requiring additional structural assumptions.

## Method Summary
The method learns parameters of a single-head softmax attention regressor f_{W,v}(X) = α(X,W)ᵀ(Xv) from black-box value queries. First, recover v* using d one-row queries [e_iᵀ] since softmax weight equals 1 for single tokens. Then, for each column j, construct d two-row probes X = [(u_ℓ + e_j)ᵀ; e_jᵀ] where softmax reduces to σ(u_ℓᵀwⱼ), extract the attention weight α from the oracle output, invert σ to obtain linear equations u_ℓᵀwⱼ, and solve the resulting linear system. For low-rank W*, use random probe pairs and nuclear norm minimization instead of column-wise recovery.

## Key Results
- Exact recovery of single-head attention parameters using O(d²) queries via softmax-to-sigmoid reduction
- Low-rank recovery with O(rd) queries using compressed sensing when rank(W*) ≤ r ≪ d
- Robustness to noisy oracles with polynomial query complexity under margin conditions
- Impossibility of multi-head attention parameter identification from value queries without structural assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Length-2 queries reduce softmax attention to an invertible sigmoid function, enabling exact parameter recovery.
- Mechanism: For input X = [(u+eⱼ)ᵀ; eⱼᵀ], the attention weight becomes α = σ(uᵀwⱼ). From oracle output y = vⱼ* + α(uᵀv*), if uᵀv* ≠ 0, we recover α = (y - vⱼ*)/(uᵀv*), then invert: uᵀwⱼ = σ⁻¹(α).
- Core assumption: v* ≠ 0 (otherwise W* is not identifiable)
- Break condition: If v* = 0 or uᵀv* = 0 for all probes, the attention weight cannot be isolated

### Mechanism 2
- Claim: Each query yields one linear equation; d linearly independent probes per column recover W* completely.
- Mechanism: After recovering v*, construct d probes with Z = [u₁ᵀ; ...; u_dᵀ] invertible and u_ℓᵀv* ≠ 0. Solve Zwⱼ = t where t_ℓ = σ⁻¹((y_ℓ - vⱼ*)/(u_ℓᵀv*)).
- Core assumption: Probe vectors are linearly independent and avoid null space of v*
- Break condition: If probes are linearly dependent or all satisfy uᵀv* = 0, the system is underdetermined

### Mechanism 3
- Claim: Low-rank structure enables O(rd) query recovery via compressed sensing.
- Mechanism: Random probe pairs (a_k, b_k) ~ N(0, I_d) create rank-one measurements t_k = ⟨a_k b_kᵀ, W*⟩. With m = Cr(2d) measurements, solve nuclear norm minimization min ||W||_* s.t. ⟨a_k b_kᵀ, W⟩ = t_k.
- Core assumption: rank(W*) ≤ r, v* ≠ 0, and measurement operator satisfies RUB property
- Break condition: If W* is not low-rank, nuclear norm minimization may not recover exact parameters

## Foundational Learning

- Concept: **Softmax-to-sigmoid reduction**
  - Why needed here: The entire extraction hinges on recognizing that 2-element softmax equals sigmoid of the difference
  - Quick check question: Given scores s₁ = 3 and s₂ = 1, compute the first softmax weight and verify it equals σ(s₁-s₂)

- Concept: **Sigmoid invertibility and numerical stability**
  - Why needed here: σ⁻¹(p) = log(p/(1-p)) is unbounded near p ∈ {0, 1}, requiring clipping in noisy settings
  - Quick check question: Compute σ⁻¹(0.99) and σ⁻¹(0.01). How does the magnitude change as you approach the boundaries?

- Concept: **Nuclear norm minimization for low-rank matrix recovery**
  - Why needed here: The low-rank regime uses convex relaxation via nuclear norm as a proxy for rank minimization
  - Quick check question: For a matrix W ∈ ℝ^{d×d} with rank r, what is the minimum number of rank-one measurements needed for recovery (up to constants)?

## Architecture Onboarding

- Component map: ATT_d model -> f_{W,v}(X) = α(X,W)ᵀ(Xv) where α = softmax([x₁ᵀWx_N, ..., x_NᵀWx_N])
- Parameters: W* ∈ ℝ^{d×d} (query-key interaction), v* ∈ ℝ^d (value projection + output head)
- Oracle: VQ(X) returns f_{W*,v*}(X); learner can query any X ∈ ℝ^{N×d}
- Query types: (1) length-1 queries to isolate v*, (2) length-2 queries to reduce softmax → sigmoid

- Critical path:
  1. Query [e_iᵀ] for i ∈ [d] → recover v* directly (softmax weight = 1)
  2. For each column j ∈ [d], construct d probes X = [(u_ℓ + e_j)ᵀ; e_jᵀ] → get α = (y - v_j*)/(u_ℓᵀv*)
  3. Compute t_ℓ = σ⁻¹(α), solve Zw_j = t → recover column j of W*
  4. (Low-rank variant) Use random probes + nuclear norm minimization instead of column-wise linear solves

- Design tradeoffs:
  - Exact vs. approximate oracle: Exact recovery requires σ⁻¹ to be applied to noiseless α; approximate oracle requires clipping and margin assumptions (|v*_i| ≥ μ > 0) to bound Lipschitz constant
  - O(d²) vs. O(rd) queries: Column-wise method is deterministic and simple; low-rank method is randomized but efficient when r ≪ d
  - Single-head vs. multi-head: Proposition 7.1 proves multi-head is not identifiable—different (W, v) can produce identical functions without structural assumptions

- Failure signatures:
  - v* = 0 → all queries return 0; W* unidentifiable
  - Probe u satisfies uᵀv* = 0 → division by zero in α recovery
  - Noise + no margin (|v*_i| ≈ 0 for some i) → σ⁻¹ amplifies noise unboundedly
  - Multi-head architecture with shared W matrices across heads → parameter-function map is many-to-one

- First 3 experiments:
  1. **Sanity check on small d**: Set d = 4, generate random (W*, v*) with ||v*||₂ = 1 and min_i |v*_i| ≥ 0.1. Implement column-wise extraction using Gaussian probes. Verify ||Ŵ - W*||_F < 1e-10 and ||v̂ - v*||₂ < 1e-10
  2. **Noise robustness test**: Add Gaussian noise η ~ N(0, σ²) to oracle outputs. Plot parameter error vs. noise level σ for margin μ ∈ {0.01, 0.1, 0.5}. Confirm error scales as O(σ/(μ·min_entry))
  3. **Low-rank regime validation**: Generate W* with rank r ∈ {2, 4, 8} in d = 64 dimensions. Compare query counts: column-wise (O(d²)) vs. compressed sensing (O(rd)). Measure recovery success rate over 20 random seeds

## Open Questions the Paper Calls Out

- **Question:** Can we characterize specific structural assumptions (beyond the single-head case) under which multi-head attention parameters are identifiable from value queries?
- **Basis in paper:** Section 7 states that "guarantees analogous to the single-head setting are impossible without additional structural assumptions" and suggests identifying such assumptions as a direction for future work
- **Why unresolved:** The paper proves general non-identifiability for multi-head attention but notes that identifiability holds if parameters lie in mutually orthogonal subspaces
- **What evidence would resolve it:** A theoretical characterization of conditions (e.g., rank constraints, orthogonality) that guarantee unique parameter recovery, or an algorithm that exploits such structure

- **Question:** Is it possible to efficiently learn a functionally equivalent model or verify functional equivalence for multi-head attention when exact parameter recovery is impossible?
- **Basis in paper:** Section 7 asks if it is possible to "learn a function that closely approximates the target even if the parameters are different" and questions the decidability of checking if two parameter sets represent the same function
- **Why unresolved:** The paper demonstrates that distinct parameters can induce the same input-output map, but does not provide methods to navigate this redundancy to learn an equivalent function
- **What evidence would resolve it:** An algorithm that recovers parameters guaranteed to approximate the target function within ε, or a proof of decidability for the equivalence problem

- **Question:** Can efficient algorithms for learning biased ReLU networks from value queries be adapted to the unbiased case required for the one-layer Transformer reduction?
- **Basis in paper:** Section 4.1 states, "we conjecture that with some modifications, their algorithm [for biased ReLU networks] can also be applied in this scenario"
- **Why unresolved:** The paper's result for one-layer Transformers is conditional on the existence of an algorithm for unbiased ReLU networks, which is not yet proven
- **What evidence would resolve it:** A formal proof or algorithm demonstrating that value-query learnability extends to two-layer ReLU networks without bias terms

## Limitations
- Multi-head attention parameters are not identifiable from value queries in general (Proposition 7.1)
- Low-rank recovery requires nuclear norm minimization with compressed sensing, practical performance depends on solver choice
- Noise robustness requires margin condition (|v*_i| ≥ μ > 0) to prevent unbounded error amplification

## Confidence
- **High confidence**: The softmax-to-sigmoid reduction mechanism and O(d²) exact recovery for single-head attention - this follows from elementary linear algebra and is rigorously proven in Theorem 4.1
- **Medium confidence**: The O(rd) low-rank recovery via nuclear norm minimization - while the compressed sensing framework is well-established, practical performance depends on implementation details not fully specified
- **Medium confidence**: Noise robustness claims - the clipping and margin-based error bounds are sound, but real-world performance may vary with noise characteristics and solver tolerances

## Next Checks
1. Implement the exact O(d²) recovery algorithm on small synthetic instances (d ≤ 8) and verify that recovered parameters match ground truth to numerical precision
2. Test the low-rank recovery algorithm with varying ranks (r ∈ {2,4,8} in d=64 dimensions) and compare query complexity against the column-wise method across 20 random seeds
3. Conduct noise robustness experiments with varying noise levels σ and margin conditions μ, plotting parameter error against (σ/μ) to verify the predicted O(σ/(μ·min_entry)) scaling