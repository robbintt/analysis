---
ver: rpa2
title: Data-Efficient RLVR via Off-Policy Influence Guidance
arxiv_id: '2510.26491'
source_url: https://arxiv.org/abs/2510.26491
tags:
- training
- data
- influence
- gradient
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CROPI, a curriculum-based reinforcement learning
  framework that leverages influence functions for efficient data selection in RLVR.
  The method addresses the computational challenges of online influence estimation
  by introducing an off-policy gradient estimator that uses pre-collected offline
  trajectories and employs sparse random projection to handle high-dimensional gradients.
---

# Data-Efficient RLVR via Off-Policy Influence Guidance

## Quick Facts
- arXiv ID: 2510.26491
- Source URL: https://arxiv.org/abs/2510.26491
- Reference count: 40
- Primary result: Achieves up to 2.66× step-level acceleration using only 10% of training data per stage compared to full-dataset training

## Executive Summary
This paper introduces CROPI, a curriculum-based reinforcement learning framework that leverages influence functions for efficient data selection in RLVR. The method addresses the computational challenges of online influence estimation by introducing an off-policy gradient estimator that uses pre-collected offline trajectories and employs sparse random projection to handle high-dimensional gradients. CROPI iteratively selects the most influential training data for each stage of RL training based on these influence estimates. Experiments on models ranging from 1.5B to 7B parameters demonstrate significant improvements in training efficiency while maintaining strong generalization capabilities.

## Method Summary
CROPI is an iterative curriculum-based RLVR framework that uses influence functions to select the most impactful training data. It pre-collects offline trajectories from a base model, then uses an off-policy gradient estimator to compute influence scores for each training prompt relative to a validation set. These gradients are compressed using sparse random projection before ranking via cosine similarity. The method iteratively selects the top α% of prompts (typically 10%) for each training phase, which runs for E=200 steps using GRPO. The process repeats for multiple phases, with selection performed at the start of each phase using the current model checkpoint.

## Key Results
- Achieves up to 2.66× step-level acceleration compared to full-dataset training
- Maintains strong performance using only 10% of data per training stage
- Shows robust generalization to both targeted and untargeted tasks
- Sparse random projection with 10% sparsity significantly outperforms dense projection for rank preservation

## Why This Works (Mechanism)

### Mechanism 1: Off-Policy Gradient Alignment
Selecting data based on cosine similarity of off-policy gradients aligns training updates with the optimization direction needed to improve validation performance. The Practical Off-Policy Influence (POPI) estimator computes gradients using pre-collected trajectories from the behavior policy β, calculating similarity between training prompt gradients and validation set gradients. This works because the KL-term in the RL objective constrains the current policy to remain close to the base model, making the off-policy gradient a valid approximation.

### Mechanism 2: Sparse Random Projection for Noise Filtering
Randomly dropping gradient dimensions before projection improves influence ranking fidelity by filtering numerical noise. High-dimensional gradients contain noise (especially in fp16), and standard projection can amplify this noise. By selecting a sparse subset first, the method improves signal-to-noise ratio in the projected gradient's inner product. Empirical results show 10% sparsity significantly outperforms full gradients for rank preservation.

### Mechanism 3: Iterative Curriculum Selection
Iteratively selecting the top-k most influential prompts creates a dynamic curriculum that focuses on the "learning frontier." CROPI segments training into phases, re-evaluating influence of all prompts against the current checkpoint at each phase start. This ensures training on data most relevant to fixing current errors on the validation set. As training progresses, selected data becomes semantically more specific and difficult.

## Foundational Learning

- **Concept:** Influence Functions (Data Attribution)
  - **Why needed:** The theoretical engine of CROPI; influence measures how a training point changes validation loss via gradient dot product
  - **Quick check:** If a training gradient is orthogonal to the validation gradient, what is the influence score? (Answer: 0)

- **Concept:** Importance Sampling & Off-Policy RL
  - **Why needed:** Uses trajectories from old policy (base model) to estimate gradients for current policy; requires understanding probability ratio ρ_θ for distribution correction
  - **Quick check:** Why can't we use raw rewards from base model's trajectories directly to train new model without correction? (Answer: Distribution mismatch/Static vs. Dynamic policy)

- **Concept:** Random Projection (Johnson-Lindenstrauss Lemma)
  - **Why needed:** Compresses billions of gradient parameters into smaller space for efficient influence computation
  - **Quick check:** Does random projection preserve exact dot product value or just relative distances/rankings with high probability? (Answer: Relative distances/rankings)

## Architecture Onboarding

- **Component map:** Rollout Buffer -> Gradient Engine -> Sparse Projector -> Selector -> Trainer (GRPO)
- **Critical path:** The Selection Phase between training phases requires forward/backward pass for every training prompt against current checkpoint; this is main computational bottleneck
- **Design tradeoffs:**
  - Selection Ratio (α): Low α maximizes speedup but risks dropping diverse data; paper uses 0.1
  - Sparse Ratio: Trading gradient information for numerical stability; paper finds 0.1 optimal, 1.0 fails
  - Validation Size: Larger sets improve selection robustness but increase computation cost
- **Failure signatures:**
  - Selection Collapse: Top-k prompts become identical/near-duplicates, reducing diversity
  - Stale Gradients: If KL divergence spikes, off-policy gradients become uncorrelated with true gradient
  - Precision Loss: Standard dense projection drops rank preservation to random (13%)
- **First 3 experiments:**
  1. Reproduction (1.5B Model): Run CROPI on Qwen2.5-1.5B with GSM8K/MATH; verify 2.66x speedup claim
  2. Sparsity Ablation: Plot "Precision@10%" vs "Sparse Ratio" to confirm sparser gradients (0.1) yield better rank preservation than full (1.0)
  3. Influence vs. Difficulty: Compare CROPI selection vs "Learnability" (pass rate) baseline to validate gradient alignment over heuristic difficulty

## Open Questions the Paper Calls Out

### Open Question 1
Can a rigorous theoretical analysis be established to quantify the error, bias, and variance of the off-policy gradient estimator compared to on-policy methods?
- Basis: Authors explicitly state they "do not provide a theoretical analysis of the associated errors, bias, or variance; addressing these aspects is left for future research"
- Why unresolved: Current work relies on empirical validation without formal guarantees for approximation quality
- Evidence needed: Derivation bounding error of off-policy estimator under varying KL-constraints

### Open Question 2
Can the zero-gradient issue for fully correct or incorrect prompts be resolved by incorporating rollouts from a replay buffer or smaller proxy model?
- Basis: Limitations section notes using only base model trajectories results in zero gradients for some prompts and suggests investigating "reusing rollouts collected during the training process (e.g., via a replay buffer)"
- Why unresolved: Current implementation discards prompts with 0% or 100% pass rates, potentially ignoring useful hard negatives or easy positives
- Evidence needed: Experiments showing non-zero gradient extraction and performance improvements with dynamic replay buffer

### Open Question 3
Does random dropout prior to projection fundamentally improve rank preservation by filtering numerical noise in float16 operations?
- Basis: Section 6.1 and Appendix G observe counter-intuitive improvement with sparse projection and hypothesize it mitigates numerical noise, but state they "leave further exploration of this area to future work"
- Why unresolved: Phenomenon observed empirically (precision increases from 13% to 80%) but exact mechanism not proven
- Evidence needed: Ablation studies using higher precision formats (float32/bfloat16) to see if sparsity benefit persists or vanishes

## Limitations
- Efficacy contingent on maintaining sufficiently small KL divergence between current policy and base model; threshold not quantified
- Sparsity-before-projection trick presented as novel contribution but underlying mechanism (why dropping random dimensions improves signal-to-noise ratio) not rigorously explained
- Iterative curriculum selection introduces computational overhead for selection phase; detailed cost-benefit analysis vs full-dataset baseline missing

## Confidence

- **High Confidence:** Core mathematical formulation of POPI estimator and use of sparse random projection are sound; experimental results demonstrating significant speedup (2.66x) and improved accuracy are robust
- **Medium Confidence:** Empirical findings regarding sparsity-before-projection trick and optimal sparse ratio are reliable but theoretical understanding of why this works is limited
- **Low Confidence:** Long-term stability and generalization when applied to very different domains or significantly larger models not explored; sensitivity to validation set size and composition unclear

## Next Checks
1. **KL Divergence Monitoring:** Instrument CROPI implementation to log KL divergence between current policy and base model at each selection phase; plot against performance to validate low KL divergence assumption
2. **Selection Diversity Analysis:** Analyze diversity of selected prompts across phases; compute average pairwise cosine similarity or number of unique problem types in top-k; investigate if selection collapse occurs and its impact
3. **Ablation on Validation Set Size:** Run experiments with varying validation set sizes (10, 50, 100 examples) to quantify trade-off between selection quality and computational cost; assess if method remains effective with very small validation sets