---
ver: rpa2
title: Multiple Object Stitching for Unsupervised Representation Learning
arxiv_id: '2506.07364'
source_url: https://arxiv.org/abs/2506.07364
tags:
- uni00000013
- object
- multi-object
- learning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Multiple Object Stitching (MOS), a method
  to improve unsupervised representation learning for multi-object images. The core
  idea is to synthesize multi-object images by stitching together single-object images,
  thereby creating object-level correspondences without human annotations.
---

# Multiple Object Stitching for Unsupervised Representation Learning

## Quick Facts
- **arXiv ID:** 2506.07364
- **Source URL:** https://arxiv.org/abs/2506.07364
- **Reference count:** 40
- **Primary result:** State-of-the-art unsupervised representation learning for multi-object images using synthetic stitching to create deterministic object correspondences

## Executive Summary
This paper addresses the "semantics inconsistency issue" in unsupervised contrastive learning on multi-object images, where random crops can capture different objects leading to false positive pairs. The authors propose Multiple Object Stitching (MOS), which synthesizes multi-object images by deterministically stitching together single-object images, creating known object-level correspondences without human annotations. MOS employs three dedicated contrastive objectives—Multiple-to-Single, Multiple-to-Multiple, and Single-to-Single—to model representations across different object scales and scenarios. Experimental results demonstrate state-of-the-art performance on both single-object (ImageNet, CIFAR) and multi-object (COCO) datasets, significantly outperforming existing methods in downstream tasks like object detection and instance segmentation.

## Method Summary
MOS constructs multi-object images by stitching single-object images in a grid pattern, where the objects and their spatial arrangement are predetermined. The method employs three contrastive losses: Multiple-to-Single ($L_{m2s}$) forces the global stitched image representation to encode features of individual constituent objects, Multiple-to-Multiple ($L_{m2m}$) contrasts different stitched views, and Single-to-Single ($L_{s2s}$) applies standard contrastive learning on unmodified natural images to bridge the domain gap. The approach uses ViT-S/16 or ViT-B/16 as backbone with a base-momentum encoder setup, and training involves a data pipeline that generates stitched views alongside natural views with appropriate index mapping for the multiple contrastive objectives.

## Key Results
- Achieves state-of-the-art unsupervised representation learning performance on ImageNet-1K, CIFAR10, and CIFAR100
- Demonstrates significant improvements on COCO downstream tasks including object detection and instance segmentation
- Ablation studies confirm the necessity of all three contrastive losses, with $L_{m2s}$ providing ~9% linear accuracy gain on CIFAR100
- Performance continues to improve on ImageNet even when loss values do not consistently decrease during training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constructing multi-object images by stitching single-object samples creates deterministic object-level correspondences, potentially resolving the "semantics inconsistency" found in natural multi-object scenes.
- **Mechanism:** Standard contrastive learning on natural multi-object images suffers when random crops capture different objects (false positives). By synthetically stitching images, the ground-truth composition is known a priori. This allows the model to map a "whole" (stitched image) to its "parts" (constituent single-object images) accurately.
- **Core assumption:** The model can generalize from the synthetic distribution of stitched images to the natural distribution of multi-object scenes without overfitting to stitching artifacts.
- **Evidence anchors:** [Abstract] "...construct the multi-object images by stitching... where the objects... are predetermined." [Section 1] "...alleviate the above semantics inconsistency issue... by combining off-the-shelf single object centric images..."

### Mechanism 2
- **Claim:** A composite-to-component contrastive objective ($L_{m2s}$) forces the global image representation to encode features of individual constituent objects.
- **Mechanism:** The Multiple-to-Single loss ($L_{m2s}$) contrasts the representation of the stitched image against the representations of the single-object images contained within it. This prevents the encoder from collapsing the image into a single global "instance" code and encourages the preservation of local features.
- **Core assumption:** The representation capacity of the encoder is sufficient to hold multiple object concepts simultaneously without catastrophic interference.
- **Evidence anchors:** [Section 3.2] "The contrast between multi-object image and the corresponding single object views guides the model to discriminate each object in the image." [Table 4] Ablation shows $L_{m2s}$ provides significant accuracy gains.

### Mechanism 3
- **Claim:** A Single-to-Single objective ($L_{s2s}$) is required to bridge the domain gap between artificial stitched images and natural images.
- **Mechanism:** Stitched images contain unnatural boundaries. The $L_{s2s}$ loss applies standard contrastive learning on unmodified natural images. This stabilizes the training and ensures the representation remains useful for real-world data, counteracting the distribution shift caused by the synthetic stitching.
- **Core assumption:** The benefits of $L_{m2s}$ (object awareness) can be transferred to the representation space used for natural images via shared weights.
- **Evidence anchors:** [Section 3.2] "...alleviate the potential domain gap produced by the domain gap between the stitched images and the natural ones." [Table 4] Removing $L_{s2s}$ drops performance.

## Foundational Learning

- **Concept:** **Instance Discrimination (Contrastive Learning)**
  - **Why needed here:** The method builds upon MoCo-v3/SimCLR. You must understand how positive/negative pairs are formed via augmentation to grasp why random crops in multi-object scenes break the "positive pair" assumption.
  - **Quick check question:** Why does a random crop from a multi-object image potentially create a "false positive" pair in standard contrastive learning?

- **Concept:** **Vision Transformer (ViT) Inductive Bias**
  - **Why needed here:** The paper explicitly uses ViT, claiming it is "less sensitive to artificiality produced by the boundary." Understanding patch-based processing helps explain why stitching works better here than in CNNs.
  - **Quick check question:** How does the global self-attention mechanism in ViT differ from the local receptive field of CNNs when processing the artificial boundaries of stitched images?

- **Concept:** **Multi-view Consistency**
  - **Why needed here:** The core mechanism relies on enforcing consistency between a "global" view (stitched) and "local" views (single objects).
  - **Quick check question:** In the MOS framework, what constitutes a "positive pair" for the Multiple-to-Single loss?

## Architecture Onboarding

- **Component map:** Input Batch -> Stitching Module (creates $r^2$ composite views) -> Base Encoder + Momentum Encoder -> 3-layer MLP Projector + 2-layer MLP Predictor -> Three Contrastive Losses ($L_{m2s}$, $L_{m2m}$, $L_{s2s}$)

- **Critical path:**
  1. Input Batch ($x$) $\to$ Split into $x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}$
  2. Stitch $x^{(1)}, x^{(2)} \to$ Multi-object views $I^{(1)}, I^{(2)}$
  3. Encode $I^{(1)}$ (Base) and $I^{(2)}$ (Momentum) for Multi-to-Multi loss
  4. Encode $I^{(1)}$ (Base) and $x^{(3)}$ (Momentum) for Multi-to-Single loss
  5. Encode $x^{(3)}$ (Base) and $x^{(4)}$ (Momentum) for Single-to-Single loss

- **Design tradeoffs:**
  - **Stitching Grid Size ($r$):** Larger $r$ increases object density but reduces per-object resolution
  - **Scale Factor ($s$):** Creates scale variation. The paper finds $\{1, 2\}$ is optimal; larger ranges $\{1, 2, 3\}$ degrade performance (Table 5)
  - **Architecture Choice:** CNNs are not recommended as they are sensitive to stitching artifacts; ViT is the preferred backbone

- **Failure signatures:**
  - **Divergence:** Training fails to converge if using *only* the Multiple-to-Multiple loss ($L_{m2m}$) without the Single-to-Single ($L_{s2s}$) or Multiple-to-Single ($L_{m2s}$) terms (Table 4)
  - **Boundary Artifacts:** If the encoder overfits to the stitching boundaries, downstream performance on natural images (like COCO detection) will drop relative to the baseline

- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run MOS on CIFAR100 with only $L_{s2s}$ vs. $L_{s2s} + L_{m2s}$ to verify the performance gain from the stitching objective (expected ~9% linear accuracy gain)
  2. **Scale Sensitivity:** Vary the scale factor sampling range $\{1\}$ vs. $\{1, 2\}$ on ImageNet-1K to confirm the multi-scale benefit (expected improvement in COCO AP)
  3. **Attention Visualization:** Visualize the [CLS] attention map on a stitched image to confirm the model attends to the correct constituent objects rather than the stitching boundaries

## Open Questions the Paper Calls Out

- **Open Question 1:** Why does representation quality continue to improve on ImageNet even when the loss values do not consistently decrease during training?
  - **Basis in paper:** [explicit] Appendix E notes that while loss curves do not consistently drop, performance sustains improvement, prompting the authors to "further explore the insight behind this result in future work."
  - **Why unresolved:** The disconnect between the optimization trajectory of the loss functions ($L_{m2s}$, $L_{m2m}$, $L_{s2s}$) and the linear evaluation accuracy is currently unexplained.
  - **What evidence would resolve it:** A theoretical analysis of the gradient direction or an empirical study correlating specific loss fluctuations with the learning of distinct semantic features.

- **Open Question 2:** Can the multiple object stitching strategy be effectively generalized to temporal or non-visual modalities?
  - **Basis in paper:** [explicit] The conclusion states, "In future work, we plan to apply our method on more modalities, e.g. audio and video, to model more complicated scenario representations."
  - **Why unresolved:** The concept of "stitching" is spatially defined for images; it is unclear how to define or stitch "objects" in 1D audio signals or maintain temporal consistency in video without creating unnatural artifacts.
  - **What evidence would resolve it:** Adapting the stitching algorithm for video volumes or spectrograms and demonstrating performance gains in corresponding unsupervised benchmarks.

- **Open Question 3:** Is the method compatible with Convolutional Neural Networks (CNNs) given their sensitivity to boundary artifacts?
  - **Basis in paper:** [inferred] The introduction explicitly selects Vision Transformers (ViT) because they are "less sensitive to artificiality produced by the boundary of image stitching," implying this is a barrier for CNNs.
  - **Why unresolved:** The paper does not demonstrate if the artificial boundaries in the synthesized images would degrade the local feature learning of CNN-based architectures.
  - **What evidence would resolve it:** Experiments applying MOS to ResNet backbones, potentially alongside boundary-aware padding or masking techniques, to observe if performance degrades.

## Limitations
- Performance gains on COCO remain below supervised baselines, suggesting incomplete capture of natural multi-object scene semantics
- Method's scalability to scenes with more than four objects or datasets with very different object statistics is not addressed
- Reliance on synthetic stitching may limit generalizability to highly complex, naturally occurring multi-object scenes with significant occlusion and interaction

## Confidence
- **High:** The core mechanism of using stitching to create deterministic object correspondences and the necessity of the three-loss framework (particularly the role of $L_{m2s}$ and $L_{s2s}$ in preventing collapse and bridging the domain gap) are well-supported by ablation studies
- **Medium:** The claim that the model learns a "multi-object representation" is supported by downstream detection results, but a deeper analysis of what the representation actually encodes would strengthen this
- **Low:** The scalability of the method to scenes with more than four objects or to datasets with very different object statistics is not addressed and remains an open question

## Next Checks
1. **Occlusion Test:** Evaluate MOS on a dataset like COCO-Stuff or a synthetic dataset with high occlusion rates to test if the stitching-based pre-training can generalize to scenes where objects partially hide each other
2. **Architectural Transfer:** Re-implement MOS using a CNN backbone (e.g., ResNet) with a patch-based projection layer to test if the performance drop reported in the paper is consistent and to identify the minimal architectural requirements for the method to work
3. **Representation Probing:** Use a linear probe to measure the model's accuracy on a "single-object vs. multi-object" classification task using the frozen MOS representation, to directly quantify if the model has learned to distinguish between individual and composite instances