---
ver: rpa2
title: Maximum Score Routing For Mixture-of-Experts
arxiv_id: '2508.12801'
source_url: https://arxiv.org/abs/2508.12801
tags:
- routing
- wang
- zhang
- arxiv
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Maximum Score Routing (MaxScore) addresses inefficiencies in mixture-of-experts
  (MoE) routing caused by token dropping under capacity constraints and poor load
  balancing without them. The method models routing as a minimum-cost maximum-flow
  problem and integrates a SoftTopk operator to improve expert selection beyond traditional
  softmax-based approaches.
---

# Maximum Score Routing For Mixture-of-Experts

## Quick Facts
- arXiv ID: 2508.12801
- Source URL: https://arxiv.org/abs/2508.12801
- Authors: Bowen Dong; Yilong Fan; Yutao Sun; Zhenyu Li; Tengyu Pan; Xun Zhou; Jianyong Wang
- Reference count: 4
- Primary result: MaxScore achieves lower training loss (2.62 vs 2.65 for GShard) and higher evaluation scores (43.44% average accuracy vs 42.11% for GShard) at equivalent FLOPs

## Executive Summary
Maximum Score Routing (MaxScore) addresses inefficiencies in mixture-of-experts (MoE) routing caused by token dropping under capacity constraints and poor load balancing without them. The method models routing as a minimum-cost maximum-flow problem and integrates a SoftTopk operator to improve expert selection beyond traditional softmax-based approaches. MaxScore achieves lower training loss and higher evaluation scores at equivalent FLOPs while effectively eliminating token dropping and achieving near-perfect load balancing across model scales and sparsity levels.

## Method Summary
MaxScore reformulates MoE routing as a minimum-cost maximum-flow problem in a bipartite network where tokens and experts are nodes connected by edges weighted by negative affinity scores. A super source connects to tokens (capacity = k) and experts connect to a super sink (capacity = c), ensuring maximum total affinity flow respects all constraints simultaneously. The method integrates a SoftTopk operator that redistributes affinity scores across top-k experts more evenly than softmax, with temperature decaying from t0=4 to 1 during training. The combined approach eliminates token dropping while achieving near-perfect load balancing (mean ratio 0.9996 vs 0.8237 for GShard).

## Key Results
- Training loss: 2.62 vs 2.65 for GShard at equivalent FLOPs
- Evaluation scores: 43.44% average accuracy vs 42.11% for GShard
- Load balancing: Mean ratio 0.9996 vs 0.8237 for GShard
- Token dropping: Effectively eliminated (0%) vs 35% for second expert in GShard

## Why This Works (Mechanism)

### Mechanism 1
Modeling MoE routing as a minimum-cost maximum-flow problem eliminates token dropping while preserving capacity constraints. Tokens and experts become nodes in a bipartite flow network. A super source connects to tokens (capacity = k), experts connect to a super sink (capacity = c), and token-expert edges carry unit capacity with cost = -affinity. The algorithm finds maximum total affinity flow that respects all constraints simultaneously, rather than greedy sequential assignment. Core assumption: The optimal routing exists within the feasible solution space where each token-expert pair matches at most once. Evidence: Section 3.2 describes the flow network formulation with edge properties in Table 2. Break condition: If the flow network solution becomes computationally prohibitive at very large scales, the quality-speed tradeoff may favor approximate methods that reintroduce dropping.

### Mechanism 2
The SoftTopk operator redistributes affinity scores across top-k experts more evenly than Softmax, improving gradient signal quality for non-top-1 experts. SoftTopk incrementally builds scores by adding a temperature-scaled Softmax to non-top-k entries only, decaying temperature t from t0 to 0 during training. This creates a smoother approximation to ArgTopk than Softmax's winner-takes-all dynamics. Core assumption: The disproportionate top-1 affinity scores under Softmax harm learning when tokens are rerouted away from saturated experts. Evidence: Section 3.2, Eq. 6 shows the full SoftTopk formulation, and Figure 2(b) vs 6(b) demonstrates improved affinity distribution. Break condition: If temperature decay schedule is poorly tuned, the operator may either collapse to near-Softmax behavior or introduce instability.

### Mechanism 3
The combination of network flow modeling and SoftTopk produces superadditive gains—joint improvement exceeds the sum of individual contributions. SoftTopk provides meaningful affinity scores for all k experts (enabling gradient flow), while network flow ensures those scores translate to globally optimal, constraint-satisfying assignments. Without either component, the system fails: Softmax + flow suffers from uninformative rerouting gradients; SoftTopk + greedy routing cannot escape local assignment traps. Core assumption: The two components address independent failure modes that interact constructively. Evidence: Table 4 shows GShard-S alone improves to 42.55%, GShard-M to 42.16%, but combined GShard-SM reaches 43.44%. Break condition: If either component is removed or degraded, performance may regress below baseline GShard levels.

## Foundational Learning

- **Top-k MoE routing with capacity constraints**: Why needed here: The entire MaxScore method is designed to fix failures (token dropping, load imbalance) inherent in constrained top-k routing. Understanding what GShard does—selecting top-k experts per token, dropping overflow tokens when expert capacity is exceeded—is prerequisite to understanding why flow-based routing helps. Quick check: Given 8 tokens and 4 experts with k=2 and capacity c=3 per expert, how many tokens can be assigned before dropping occurs?

- **Minimum-cost maximum-flow formulation**: Why needed here: MaxScore reformulates routing as a flow problem. You need to understand how bipartite graphs, source/sink nodes, capacity constraints, and cost-weighted edges combine to find optimal assignments. Quick check: In a flow network, if edge (token_i, expert_j) has cost -0.7 and capacity 1, what does maximizing flow while minimizing total cost mean for this edge?

- **Differentiable sparse operators (Softmax vs SoftTopk)**: Why needed here: The SoftTopk operator is central to MaxScore's improvement. Understanding why Softmax over-emphasizes top-1 (exponential normalization) and how SoftTopk differs (incremental masking with temperature decay) is essential for implementation and tuning. Quick check: Why does Softmax([0.8, 0.1, 0.05, 0.05]) produce a more skewed distribution than SoftTopk with appropriate temperature?

## Architecture Onboarding

- Component map:
```
Input tokens (batch of n)
    ↓
Router weight Wg → affinity matrix A (n × e)
    ↓
SoftTopk operator → modified affinities A'
    ↓
[Stage 1] Top-1 assignment → mask, reduce expert capacities
    ↓
[Stage 2] Residual routing via Sinkhorn on remaining capacity
    ↓
Final assignment matrix P (binary, constraint-satisfying)
    ↓
Expert computations → weighted aggregation
```

- Critical path:
  1. Affinity computation via SoftTopk (not Softmax)—temperature schedule is critical
  2. Two-stage routing: top-1 first (low drop rate), then flow-based residual
  3. Load balance auxiliary loss (λ=0.01 for most models, 0.001 for DeepSeek-style)

- Design tradeoffs:
  - SPFA (exact flow solution) vs. two-stage Sinkhorn approximation: SPFA is higher quality but sequential and slow; two-stage is parallelizable with minor quality loss
  - Temperature initialization t0: Higher values (t0=4→1 decay) work better than constant or aggressive decay
  - Capacity factor: MaxScore uses cf=1.0 (exact capacity) since flow guarantees no dropping, unlike GShard which needs cf>1 for safety margin

- Failure signatures:
  - Token dropping > 0%: Flow solver not converging or capacity miscalculated
  - Load ratio variance > 0.1: Auxiliary loss weight too low or SoftTopk temperature stuck
  - Training loss plateau above GShard baseline: SoftTopk not activating (t stuck at 0) or flow edge costs inverted

- First 3 experiments:
  1. **Baseline replication**: Train GShard MoE (k=2, e=16) on C4 subset for 10k steps, verify token drop rate matches paper (~35% for second expert). Log load ratio distribution.
  2. **SoftTopk ablation**: Replace Softmax with SoftTopk (t0=4→1 decay over 10B tokens) keeping greedy routing. Compare training loss curve against baseline—should see early improvement.
  3. **Full MaxScore integration**: Enable two-stage flow routing with SoftTopk. Measure: (a) token drop rate should be 0%, (b) mean load ratio should be ~0.999, (c) training loss at 65B tokens should reach ~2.62. If any metric fails, isolate by disabling one component.

## Open Questions the Paper Calls Out

### Open Question 1
Does MaxScore maintain its performance advantages and load balancing efficiency when scaled to state-of-the-art model sizes (e.g., >10B parameters)?
Basis in paper: The authors explicitly state in the Conclusion and Limitations sections that "Future work will focus on evaluating the method at larger model scales" and "our experiments are restricted to smaller-scale models."
Why unresolved: The provided experiments were limited to Base, Large, and XL sizes (up to 3.2B total parameters) due to computational constraints.
What evidence would resolve it: Benchmark results and training loss comparisons against baselines like GShard or DeepSeek on models with significantly larger parameter counts.

### Open Question 2
How does MaxScore generalize to diverse tasks beyond standard NLP benchmarks, such as multimodal or code generation tasks?
Basis in paper: The conclusion identifies evaluating "across more diverse benchmarks to validate its generality and robustness" as a specific direction for future work.
Why unresolved: The current evaluation is restricted to a standard suite of text-based tasks (e.g., ARC, BoolQ, HellaSwag).
What evidence would resolve it: Evaluation results on datasets outside the C4/standard NLP domain, such as coding benchmarks or multimodal understanding tasks.

### Open Question 3
What is the optimal trade-off between routing quality and speed for MaxScore when $k > 2$?
Basis in paper: The methodology section notes that for top-$k$ routing where $k > 2$, "a trade-off needs to be made between quality (SPFA) and speed (Iter)," but the paper focuses primarily on top-2 routing.
Why unresolved: Algorithm 1 optimizes specifically for top-2 routing; the behavior and optimal algorithmic configuration for higher values of $k$ remain empirically unexplored.
What evidence would resolve it: A comparative analysis of training latency and model quality for MaxScore using SPFA versus the iterative strategy when $k=4$ or $k=6$.

## Limitations
- Computational cost: Maximum flow solvers like SPFA are sequential and have high computational complexity, potentially limiting scalability to very large models
- Evaluation scope: Experiments restricted to smaller-scale models (up to 3.2B parameters) and standard NLP benchmarks
- Algorithmic trade-offs: For $k > 2$, a quality-speed trade-off exists between exact SPFA and iterative approximation methods

## Confidence
High confidence in core methodology and experimental results based on:
- Clear problem formulation addressing known MoE routing failures
- Detailed algorithmic description with mathematical formulations
- Reproducible experimental setup with standard benchmarks
- Consistent improvements across multiple model scales and sparsity levels

Medium confidence in scalability claims pending:
- Larger-scale model evaluations beyond 3.2B parameters
- Cross-domain generalization beyond standard NLP tasks
- Optimal routing strategies for $k > 2$ scenarios

## Next Checks
1. Implement baseline GShard MoE routing and verify token dropping rates (~35% for second expert) on C4 subset
2. Replace Softmax with SoftTopk operator using temperature decay from 4→1 and measure training loss improvement
3. Enable two-stage flow routing with SoftTopk and verify: (a) 0% token dropping, (b) load ratio ~0.999, (c) training loss reaches ~2.62 at 65B tokens