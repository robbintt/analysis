---
ver: rpa2
title: 'Multi-Trajectory Physics-Informed Neural Networks for HJB Equations with Hard-Zero
  Terminal Inventory: Optimal Execution on Synthetic & SPY Data'
arxiv_id: '2512.12708'
source_url: https://arxiv.org/abs/2512.12708
tags:
- inventory
- terminal
- mt-pinn
- loss
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MT-PINNs enforce hard-zero terminal inventory in optimal execution\
  \ via a rollout-based trajectory loss, using BPTT to propagate a terminal penalty\
  \ through simulated trajectories. A lightweight \u03BB-curriculum stabilizes training\
  \ as the state expands from risk-neutral to risk-averse HJB."
---

# Multi-Trajectory Physics-Informed Neural Networks for HJB Equations with Hard-Zero Terminal Inventory: Optimal Execution on Synthetic & SPY Data

## Quick Facts
- arXiv ID: 2512.12708
- Source URL: https://arxiv.org/abs/2512.12708
- Reference count: 40
- Primary result: MT-PINNs achieve near-zero terminal inventory (|XT|≤0.05) up to 60% of the time vs 5.5% for vanilla PINNs on synthetic data, and match TWAP exposure on SPY while outperforming in falling markets.

## Executive Summary
This paper introduces Multi-Trajectory Physics-Informed Neural Networks (MT-PINNs) for solving Hamilton-Jacobi-Bellman (HJB) equations with hard-zero terminal inventory constraints in optimal execution problems. The method combines a rollout-based trajectory loss with backpropagation-through-time (BPTT) to directly enforce terminal inventory constraints, along with a λ-curriculum that stabilizes training by progressively expanding from risk-neutral to risk-averse states. On synthetic data, MT-PINNs achieve dramatically better terminal constraint satisfaction and align closely with closed-form controls compared to vanilla PINNs. On SPY intraday data, the method matches TWAP in low-risk regimes while reducing exposure and improving performance in falling markets.

## Method Summary
MT-PINNs train a neural network to approximate the value function Γ(τ,X,S) in the Gatheral-Schied optimal execution framework, where τ is time-to-close, X is inventory, and S is price. The method uses a composite loss function combining HJB PDE residuals, a trajectory loss computed via forward Euler rollouts and BPTT, and internal conditions including symmetry and terminal constraints. A key innovation is the λ-curriculum that starts training from the risk-neutral case (λ=0, 1D state) and progressively warms up to the full risk-averse problem (λ>0, 2D state) through staged training. The trajectory loss directly enforces hard-zero terminal inventory by penalizing |XT| at the end of simulated trajectories, providing stronger constraint satisfaction than soft penalties used in vanilla PINNs.

## Key Results
- Terminal inventory constraint satisfaction: MT-PINNs achieve p(|XT|≤0.05) up to 60% vs 5.5% for vanilla PINNs on synthetic data
- Synthetic benchmark: MT-PINNs closely align with closed-form controls for Gatheral-Schied model with mean errors in value (~2.8%), inventory (~5.6%), and control (~4.1%)
- SPY backtest: λ=0 matches TWAP exposure (~0.334) and cost (~-6.35 bps); higher λ reduces exposure and outperforms TWAP in falling markets
- Stable risk-cost frontier: Clear trade-off between inventory risk and execution cost across λ values

## Why This Works (Mechanism)

### Mechanism 1: Rollout-Based Trajectory Loss with BPTT
Simulating forward trajectories under the network-implied control and backpropagating a terminal inventory penalty through time enforces hard-zero constraints more effectively than soft boundary penalties. For each batch of initial states and horizons, the network computes v* = (1/2)∂Γ/∂X via autodiff, rolls out inventory dynamics via forward Euler, and accumulates a composite terminal penalty. BPTT propagates the penalty backward through all steps via adjoint sensitivities.

### Mechanism 2: Lambda-Curriculum Stabilization
Training from λ=0 (risk-neutral, 1D state) to λ>0 (risk-averse, 2D state) via staged warm-starts stabilizes learning and reduces PDE residuals. The curriculum begins with the reduced HJB where Γ is price-invariant, yielding simpler optimization. Parameters are warm-started into progressively higher λ stages, avoiding the singularity near τ→0 in the full problem.

### Mechanism 3: Composite Loss with Adaptive DWA-Style Weighting
Balancing PDE residuals, trajectory loss, internal conditions, and symmetry constraints via adaptive weight rebalancing prevents any single term from dominating. Weights are updated via EMA-smoothed loss ratios with inverse power-law scaling and clipping, ensuring stable gradient magnitudes across terms.

## Foundational Learning

- **Concept**: Hamilton-Jacobi-Bellman (HJB) Equations
  - Why needed: The value function Γ satisfies a nonlinear PDE derived from dynamic programming; understanding reduced forms (risk-neutral vs. risk-averse) is prerequisite to implementing the PDE residual loss.
  - Quick check: Can you derive why v* = (1/2)∂Γ/∂X minimizes the Hamiltonian for the Gatheral-Schied cost functional?

- **Concept**: Backpropagation Through Time (BPTT)
  - Why needed: The trajectory loss requires unrolling dynamics over N_dt steps and computing ∂L_traj/∂θ through the chain of state transitions.
  - Quick check: Given x_{k+1} = f_θ(x_k), write the adjoint recursion for λ_k = ∂L/∂x_k when L depends only on x_N.

- **Concept**: Terminal Singularity in Optimal Execution
  - Why needed: The terminal condition Γ(0, X, S) = 0 if X=0, +∞ otherwise creates a non-smooth boundary; vanilla PINNs fail to enforce this, motivating the trajectory loss.
  - Quick check: Why does a soft quadratic penalty Γ(0, X) ≈ cX² fail to concentrate terminal inventory near zero in practice?

## Architecture Onboarding

- **Component map**: Input (τ, X) for λ=0; (τ, X, S) for λ>0 → Normalized MLP with tanh → Scalar Γ_θ with derivatives → v* = (1/2)∂Γ/∂X → Euler rollouts → Composite loss with adaptive weights
- **Critical path**: 1) Implement MLP forward pass with autodiff-compatible activation (tanh) 2) Implement HJB residual for both regimes 3) Implement Euler rollout loop with v* queries 4) Wrap rollout in stop_gradient-safe BPTT 5) Add DWA weight update scheduler
- **Design tradeoffs**: Smaller network width (32³) sufficient for MT-PINN due to trajectory loss guidance; larger (500³) needed for vanilla PINN but underperforms. Trajectory batch size P×J: larger batches improve terminal constraint satisfaction but increase memory O(N_dt × P).
- **Failure signatures**: Non-zero terminal inventory with high variance (|X_T| > 0.5): indicates trajectory loss weight too low or BPTT not propagating effectively. Exploding PDE residual near τ→0: network failing to capture singularity.
- **First 3 experiments**: 1) Replicate synthetic benchmark: verify |X_T| ≤ 0.05 probability ≥0.60 vs. vanilla PINN ~0.055 2) Ablate trajectory loss: train with L_PDE + L_term only; confirm terminal inventory degradation 3) SPY backtest with λ∈{0, 0.05, 0.1}: verify λ=0 matches TWAP exposure and cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can order-book-derived state variables be successfully integrated into the MT-PINN framework to model market microstructure and liquidity constraints?
- Basis: The paper states "Incorporating order-book-derived state variables would be a natural extension toward more realistic liquidity modeling."
- Why unresolved: Current model abstracts away microstructure features like bid-ask dynamics and order-flow imbalance.
- What evidence would resolve it: Demonstrating stable control training on a modified HJB including LOB state variables, resulting in policies that adjust for realized liquidity.

### Open Question 2
- Question: Does the MT-PINN framework scale computationally to multi-asset execution problems involving cross-impact?
- Basis: Experiments are "single-asset" and trajectory loss has complexity O(J × N_dt × P).
- Why unresolved: Memory requirements of BPTT for trajectory rollouts may become prohibitive when state dimensionality increases for portfolios.
- What evidence would resolve it: Successful application of MT-PINN to a basket of assets with cross-impact terms, showing manageable training time and memory usage.

### Open Question 3
- Question: How does the MT-PINN control policy perform under realistic market frictions such as nonlinear temporary impact and transaction fees?
- Basis: Authors list "fees, nonlinear temporary impact" and frictionless execution as specific limitations.
- Why unresolved: Gatheral-Schied model assumes linear impact; real-world deviations could invalidate learned risk-cost frontier.
- What evidence would resolve it: Backtest comparison showing method's robustness when square-root impact laws and exchange fees are applied.

## Limitations
- The BPTT-based trajectory loss relies on accurate discretization of inventory dynamics; sensitivity to discretization error remains untested.
- DWA-style adaptive weighting details are underspecified, leaving room for implementation variance.
- The λ-curriculum's effectiveness is demonstrated empirically but lacks theoretical justification for stage progression rates.

## Confidence
- **High confidence**: Terminal inventory constraint satisfaction and closed-form alignment in synthetic tests are directly measurable and reproducible.
- **Medium confidence**: Risk-cost frontier behavior in SPY data is observational and dependent on specific market conditions during Feb 2025.
- **Medium confidence**: Curriculum stabilization is supported by training stability metrics but lacks ablation studies isolating curriculum vs. other factors.

## Next Checks
1. Perform discretization sensitivity analysis: Vary N_dt and Δt in trajectory rollouts to quantify impact on terminal inventory concentration and PDE residuals.
2. Ablate DWA weighting: Replace adaptive weights with fixed values to isolate contribution of dynamic rebalancing to constraint satisfaction.
3. Stress-test curriculum: Train MT-PINN directly at λ>0 without warm-start to measure degradation in terminal constraint enforcement and training stability.