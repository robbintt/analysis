---
ver: rpa2
title: 'KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering
  in Multi-Turn Dialogues'
arxiv_id: '2509.21856'
source_url: https://arxiv.org/abs/2509.21856
tags:
- answer
- question
- multi-turn
- qwen-2
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KnowMT-Bench, the first benchmark for Multi-Turn
  Long-Form Question Answering (MT-LFQA), addressing the gap in evaluating conversational
  factual capabilities in knowledge-intensive domains. The benchmark employs a dynamic
  evaluation setting where models generate their own multi-turn dialogue histories,
  and uses a human-validated automated pipeline based on Natural Language Inference
  to assess factual capability and information delivery efficiency.
---

# KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues

## Quick Facts
- **arXiv ID:** 2509.21856
- **Source URL:** https://arxiv.org/abs/2509.21856
- **Reference count:** 40
- **Primary result:** Multi-turn dialogues degrade LLM factual performance due to contextual noise from self-generated histories; RAG effectively mitigates this degradation.

## Executive Summary
This paper introduces KnowMT-Bench, the first benchmark specifically designed to evaluate Multi-Turn Long-Form Question Answering (MT-LFQA) factuality and efficiency. Unlike static single-turn benchmarks, KnowMT-Bench employs a dynamic evaluation setting where models generate their own multi-turn dialogue histories before answering a final knowledge-intensive question. The benchmark reveals that multi-turn contexts significantly degrade model performance compared to single-turn settings, primarily due to the accumulation of contextual noise from self-generated histories. Experiments show that retrieval-augmented generation (RAG) can effectively mitigate this degradation, with per-turn retrieval ("Rounds" strategy) proving most effective. The findings highlight the inadequacy of current single-turn evaluations and underscore the need for robust frameworks like KnowMT-Bench to guide future research in enhancing conversational factual capabilities of LLMs.

## Method Summary
KnowMT-Bench evaluates MT-LFQA through a dynamic process where models recursively generate dialogue turns based on their own previous outputs, creating self-generated histories that condition the final answer. The evaluation pipeline uses Qwen2.5-32B-Instruct to decompose long-form answers into atomic factual statements, then employs Qwen2.5-14B-Instruct as an NLI evaluator to check each statement's entailment or contradiction against ground truth. Factuality is measured through Factual F1 ($S_f$) and Hallucination F1 ($S_h$), while efficiency is assessed via tokens per correct fact ($D_f$) and tokens per hallucinated fact ($D_h$). The benchmark includes 801 instances across Medicine, Finance, and Law domains, with RAG mitigation strategies tested using Qwen3 embedding and reranker models.

## Key Results
- Multi-turn contexts cause significant factual degradation across all tested models, with some models (Llama-3.1-8B) experiencing up to 30% drops in Factual F1 compared to single-turn performance
- Factual degradation is primarily attributable to contextual noise from self-generated histories rather than conversation length itself
- Retrieval-augmented generation (RAG) with per-turn retrieval ("Rounds" strategy) effectively mitigates factual degradation, even reversing decline in some cases
- Models exhibit uniform verbosity behavior with strong positive correlation ($R^2 = 0.82$) between token cost of correct and incorrect facts

## Why This Works (Mechanism)

### Mechanism 1: Noise-Induced Factual Decay in Self-Generated History
Factual degradation in multi-turn dialogues is primarily caused by the accumulation of contextual noise in the model's self-generated history. As models generate responses for earlier turns, they introduce irrelevant or low-quality information into the context window. When conditioning on this noisy history for the final turn, the model's attention mechanism becomes diluted, leading to higher rates of hallucination and factual omission compared to a pristine single-turn setting.

### Mechanism 2: Uniform Verbosity Trade-off (Efficiency Decay)
Models exhibit "uniform verbosity" behavior where they cannot dynamically adjust conciseness based on information density. As conversation grows, models generate longer, more repetitive explanations rather than dense factual statements. This creates a linear decay in information delivery efficiency, with strong positive correlation between tokens per correct fact and tokens per hallucinated fact.

### Mechanism 3: RAG as Contextual Grounding (Noise Mitigation)
Retrieval-Augmented Generation mitigates factual decay by providing a "grounding anchor" that overrides the noise of self-generated history. By retrieving relevant chunks at each turn, the model shifts attention from potentially noisy dialogue history to high-fidelity retrieved context, effectively resetting the context window's signal-to-noise ratio for every generation step.

## Foundational Learning

- **Concept: Dynamic vs. Static Evaluation** - Why needed: This is the central distinction of the benchmark. Unlike standard benchmarks that use fixed contexts, this paper evaluates "Dynamic" contexts where the *model* creates its own history. Quick check: Does the evaluation measure the model's ability to answer the final question given *human-written* history or *self-generated* history?

- **Concept: Atomic Fact Decomposition** - Why needed: The paper's evaluation relies on breaking long-form answers into "atomic statements" to check for entailment/contradiction. Quick check: How does the evaluation pipeline handle a long paragraph of text before comparing it to the ground truth?

- **Concept: Signal-to-Noise Ratio in Context Windows** - Why needed: The paper frames multi-turn dialogue as a problem of "noise accumulation." Foundational knowledge of how attention mechanisms handle long contexts is required to interpret results. Quick check: According to the paper, why does adding more turns degrade performance even if the "reasoning" isn't complex?

## Architecture Onboarding

- **Component map:** Data Engine (curates single-turn pairs â†’ expands into multi-turn sequences) -> Dialogue Simulator (generates recursive responses) -> Decomposer (Qwen2.5-32B breaks answers into atomic claims) -> NLI Evaluator (Qwen2.5-14B checks claims vs. ground truth)

- **Critical path:** The "Rounds" RAG Strategy. The paper identifies this as the most robust mitigation. New engineers should focus on how retrieval is triggered *at every turn* using the *current query*, contrasting it with the "Last" or "All" strategies which proved inferior.

- **Design tradeoffs:** Dynamic contexts are more realistic but introduce self-generated noise that makes evaluation unstable. The authors accept this instability to measure real-world robustness. Medium-sized models (Qwen2.5-14B) were chosen as NLI evaluators over larger models, hypothesizing they follow "more direct and consistent reasoning paths" for constrained judgment tasks.

- **Failure signatures:** Factuality Collapse (sudden drop in $S_f$ when moving to multi-turn), Verbosity Spiral (high $D_f$ score indicating rambling), Negative Context Influence (context tokens having net *negative* importance on final answer).

- **First 3 experiments:** 1) Baseline Degradation Test: Compare single-turn vs. multi-turn performance to quantify "Contextual Noise Tax." 2) Diagnostic "Replace" Experiment: Have weaker models condition on history from stronger models to test noise hypothesis. 3) RAG Mitigation (Rounds): Implement per-turn retrieval to measure if factuality recovers to single-turn baseline.

## Open Questions the Paper Calls Out

### Open Question 1
How can model architectures or training objectives be designed to decouple the token cost of correct facts ($D_f$) from the token cost of hallucinated facts ($D_h$) in multi-turn dialogues? The authors identify a strong positive correlation ($R^2 = 0.82$) between these efficiency metrics and explicitly state: "a key direction for future research is to design models or strategies that can break this trade-off." Evidence would be a model achieving significant reduction in tokens per correct fact while maintaining or increasing tokens per hallucinated fact.

### Open Question 2
What dynamic intervention strategies can effectively filter contextual noise from dialogue history without sacrificing model reliability or useful context? Section 5.3 shows simple prompting interventions improve factuality but degrade reliability, leading authors to conclude simple prompting "may be too blunt an instrument." Evidence would be a method yielding improvement in both Factual F1 ($S_f$) and Hallucination F1 ($S_h$) simultaneously.

### Open Question 3
What specific characteristics of domain-specific finetuning determine its efficacy in suppressing noise accumulation during multi-turn dialogues? Section 5.1 presents conflicting results where HuatuoGPT (medical) successfully suppresses degradation while Fin-R1 (financial) fails to show consistent improvement. Evidence would be an ablation study identifying training variables (dataset noise levels, reasoning depth) that correlate with improved multi-turn performance.

## Limitations
- The generalizability of the "noise hypothesis" beyond tested domains and models is uncertain
- The evaluation pipeline's reliance on Qwen models as both decomposer and NLI judge introduces potential bias
- The paper doesn't conclusively prove self-generated factual quality (vs. context length or attention limitations) is the primary cause of degradation

## Confidence

- **High confidence**: Observation that multi-turn contexts degrade performance relative to single-turn baselines is robust and well-supported
- **Medium confidence**: Attribution of degradation specifically to "contextual noise" from self-generated history is plausible but could benefit from additional ablation studies
- **Medium confidence**: Mechanism explaining efficiency decay through uniform verbosity is internally consistent but lacks direct empirical validation

## Next Checks

1. **History Source Ablation**: Run the diagnostic experiment where weaker models condition on history from stronger models versus their own history. Measure the differential in factuality ($S_f$) to directly test the noise hypothesis.

2. **Context Length Control**: Isolate the effect of context length from self-generated noise by providing identical-length histories but varying their source (human-written vs. model-generated). This would validate whether length alone explains degradation.

3. **Human Evaluation Validation**: Select 50 random instances and have human experts validate the NLI pipeline's factual judgments. Calculate precision and recall of the automated system against ground truth to establish confidence bounds on reported metrics.