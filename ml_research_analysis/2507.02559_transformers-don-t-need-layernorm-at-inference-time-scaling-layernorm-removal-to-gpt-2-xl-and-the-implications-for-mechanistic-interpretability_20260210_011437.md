---
ver: rpa2
title: 'Transformers Don''t Need LayerNorm at Inference Time: Scaling LayerNorm Removal
  to GPT-2 XL and the Implications for Mechanistic Interpretability'
arxiv_id: '2507.02559'
source_url: https://arxiv.org/abs/2507.02559
tags:
- gpt-2
- ln-free
- loss
- neurons
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LayerNorm (LN) is widely believed to be essential for transformer-based
  language models, but its exact role at inference time remains unclear. LN also hinders
  mechanistic interpretability by introducing nonlinearities and increasing component
  interactions.
---

# Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability

## Quick Facts
- arXiv ID: 2507.02559
- Source URL: https://arxiv.org/abs/2507.02559
- Reference count: 40
- Primary result: LayerNorm can be entirely removed from GPT-2 models by replacing it with a linear transformation and fine-tuning, achieving cross-entropy losses within 0.03-0.1 of original models

## Executive Summary
LayerNorm (LN) is widely believed to be essential for transformer-based language models, but this work demonstrates it can be entirely removed from GPT-2 models ranging from 124M to 1.5B parameters. By replacing LN with a linear transformation and fine-tuning on a small fraction of training data, the LN-free models achieve cross-entropy losses within 0.03-0.1 of their original counterparts. The removal also enables more precise mechanistic interpretability, with Direct Logit Attribution errors dropping from ~50% to 0% without LN.

## Method Summary
The authors remove LayerNorm by replacing each LN block with a fixed linear transformation (FakeLN) and fine-tuning sequentially. The process involves replacing LN with (x-μ)/σ_avg ⊙ γ + β where σ_avg is the batch-averaged standard deviation computed at removal time. The removal follows a specific order (LN_MLP → LN_qk → LN_v → LN_f) with gaps between each stage, plus an auxiliary loss regularizing per-position standard deviation variance to stabilize fine-tuning.

## Key Results
- LN layers can be removed from every GPT-2 model (Small, Medium, Large, XL) with only a small increase in validation loss
- Cross-entropy loss increases are bounded: +0.03-0.1 for all models
- GPT-2 XL LN-free achieves 2.5052 vs 2.4799 vanilla (+0.0253)
- DLA NMAE drops from 49.07% to 0.00% in LN-free models
- Confidence neurons are inactive in LN-free models, supporting their role in calibration through LN's nonlinear effects

## Why This Works (Mechanism)

### Mechanism 1
Replacing LayerNorm with a fixed linear transformation preserves most model capability if done sequentially during fine-tuning. The standard deviation term σ in LayerNorm is replaced with σ_avg, converting the non-linear LN(x) = (x−μ)/σ ⊙ γ + β into FakeLN(x) = (x−μ)/σ_avg ⊙ γ + β. Sequential removal allows the model to adapt incrementally rather than experiencing catastrophic disruption. Removing all LN blocks simultaneously irreparably breaks the model's performance.

### Mechanism 2
An auxiliary loss regularizing per-position standard deviation variance stabilizes fine-tuning during LN removal. The auxiliary loss L_aux = λ · E[(σ_b,s − σ̂)²] encourages the model to maintain uniform residual stream norms across token positions, compensating for removing LN's per-position normalization. Applied only at LN_f since it's a global bottleneck.

### Mechanism 3
LN removal eliminates the discrepancy between Direct Logit Attribution (DLA) and Direct Effect (DE), but does not improve attribution patching accuracy. DLA approximates DE by linearizing LN. When LN is removed, DLA = DE exactly (0% NMAE vs. ~50% with LN). However, attribution patching uses first-order Taylor expansion; its errors stem from other transformer nonlinearities (softmax, GELU), not just LN.

## Foundational Learning

- **Layer Normalization mechanics**: Understanding what's being removed—the per-position normalization, learnable γ/β, and the non-linear division by σ. Quick check: Can you explain why LN cannot be folded into adjacent layers at inference time the way BatchNorm can?

- **Residual stream architecture**: LN operates on the residual stream in GPT-2; understanding information flow is essential for knowing where each LN type sits and what components it affects. Quick check: In GPT-2, which LN blocks normalize the query/key path versus the value path?

- **Direct Effect vs. direct logit attribution**: The paper's central interpretability claim hinges on why these differ in standard models and converge in LN-free models. Quick check: Why does the cached LN scale approximation in DLA introduce error compared to the true Direct Effect?

## Architecture Onboarding

- **Component map**: LN_qk (query/key) → LN_v (value) → LN_MLP (MLP input) → LN_f (final unembedding), each replaced with FakeLN

- **Critical path**: Standard fine-tuning warmup → Sequential removal: LN_MLP (layer 0→L) → LN_qk (all layers) → LN_v (all layers) → LN_f → Post-removal fine-tuning to convergence

- **Design tradeoffs**: Smaller gaps between removals → faster but riskier (instability, exploding gradients); Larger gaps → more stable but higher compute cost; LN_v removal is most unstable; always requires gaps

- **Failure signatures**: Exploding gradients during LN_v removal; Loss spikes to irrecoverably high values; Overconfidence on out-of-distribution tokens (e.g., control characters)

- **First 3 experiments**: 1) Replicate LN removal on GPT-2 Small using provided code (300 steps, ~1.5 GPU hours); 2) Compare DLA vs. DE on original vs. LN-free models using 100 random sequences; 3) Ablate top-3 confidence neurons and measure CE loss change in both model variants

## Open Questions the Paper Calls Out

- **What specific nonlinearities in the transformer architecture, beyond LayerNorm, limit the accuracy of attribution patching?**: The authors state "attribution patching's limitations likely arise from other nonlinearities in the transformer architecture" after finding no improvement in LN-free models.

- **What mechanism does LayerNorm provide that creates a small but persistent performance benefit which cannot be compensated by additional fine-tuning?**: "the gap remains approximately constant throughout fine-tuning, suggesting that LN contributes a small but persistent performance benefit that cannot be compensated by additional fine-tuning."

- **What mechanisms beyond confidence neurons contribute to overconfidence in LN-free models?**: Cumulative confidence neuron ablation in vanilla fine-tuned models yields matching entropy but still shows a 2% CE loss gap compared to LN-free models, "implying that the overconfidence in LN-free models is due to more complex mechanisms."

- **Can LN removal protocols scale to larger, more modern LLM architectures with comparable stability and performance?**: "In the future, we would like to expand our LN removal protocol to more recent models" and they note that "an early version of our protocol without auxiliary loss worked for GPT-2 Small, but did not scale to larger models."

## Limitations
- Results are demonstrated only on GPT-2 architecture and may not generalize to other transformer variants or architectures with different normalization schemes
- The broader claim about transformers generally not needing LN at inference time extrapolates beyond the GPT-2 results
- Interpretability improvements are limited since attribution patching accuracy doesn't improve, suggesting other nonlinearities (GELU, softmax) remain limiting factors

## Confidence
- **High confidence**: The LN removal methodology works as described for GPT-2 models; the cross-entropy loss increases are bounded (≤0.1); the 0% NMAE for DLA in LN-free models is a reproducible empirical finding
- **Medium confidence**: The auxiliary loss design is novel and appears effective, but the specific form and hyperparameter sensitivity suggest room for refinement
- **Low confidence**: The broader claim about transformers generally not needing LN at inference time; the discussion of confidence neuron deactivation is interesting but based on a single paper's findings

## Next Checks
1. Apply the LN removal protocol to a different transformer family (e.g., Llama-2 or BERT) and measure whether similar cross-entropy loss bounds (+0.03-0.10) and interpretability improvements (0% DLA NMAE) are achievable

2. Train models from scratch without LN (or with LN removed after initial training) to determine whether the auxiliary loss alone can stabilize training, or if LN serves irreplaceable training-time functions

3. Remove the auxiliary loss entirely and attempt LN removal on GPT-2 Small with extended fine-tuning schedules to measure whether the sequential protocol alone can achieve acceptable performance