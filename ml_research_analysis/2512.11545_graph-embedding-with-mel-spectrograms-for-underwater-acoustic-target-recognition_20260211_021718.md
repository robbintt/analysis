---
ver: rpa2
title: Graph Embedding with Mel-spectrograms for Underwater Acoustic Target Recognition
arxiv_id: '2512.11545'
source_url: https://arxiv.org/abs/2512.11545
tags:
- graph
- acoustic
- recognition
- underwater
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of underwater acoustic target
  recognition (UATR), which is complicated by the non-stationary, non-Gaussian, and
  nonlinear characteristics of underwater acoustic signals. To overcome the limitations
  of existing Euclidean-space-based models, the authors propose the UATR-GTransformer,
  a non-Euclidean deep learning model that integrates Transformer architectures with
  graph neural networks (GNNs).
---

# Graph Embedding with Mel-spectrograms for Underwater Acoustic Target Recognition

## Quick Facts
- arXiv ID: 2512.11545
- Source URL: https://arxiv.org/abs/2512.11545
- Reference count: 40
- UATR-GTransformer achieves state-of-the-art accuracy (~0.83) on ShipsEar and DeepShip underwater acoustic datasets.

## Executive Summary
This paper tackles underwater acoustic target recognition (UATR), addressing the challenge of non-stationary, non-Gaussian, and nonlinear underwater acoustic signals. Existing Euclidean-space-based models struggle with this complexity, so the authors propose UATR-GTransformer, a non-Euclidean deep learning model that integrates Transformer architectures with graph neural networks (GNNs). The model processes Mel-spectrograms by partitioning them into overlapping patches, using a Transformer Encoder to capture global dependencies, and then employing a GNN to model local neighborhood relationships. Experiments on two widely used benchmark datasets (ShipsEar and DeepShip) demonstrate that the UATR-GTransformer achieves state-of-the-art performance, with overall accuracy of 0.832 on ShipsEar and 0.827 on DeepShip. The model also provides interpretability through attention and graph visualization, highlighting its potential for ocean engineering applications.

## Method Summary
The UATR-GTransformer addresses underwater acoustic target recognition by processing Mel-spectrograms through a hybrid architecture combining Transformers and GNNs. The input Mel-spectrograms are first partitioned into overlapping patches using a 5-layer convolutional stem. A standard Transformer Encoder then captures global dependencies across these patches. The resulting feature map is converted into a graph, where nodes represent patches and edges are determined by k-nearest neighbors (k ∈ [2,8]). A Max-Relative Graph Convolution layer processes these graph structures, capturing local neighborhood relationships, followed by a feed-forward network. The model is trained using Adam optimizer with specific learning rates and batch sizes for each dataset, and employs SpecAugment for regularization.

## Key Results
- UATR-GTransformer achieves state-of-the-art performance on ShipsEar (OA: 0.832) and DeepShip (OA: 0.827) datasets.
- The model provides interpretability through attention and graph visualization, highlighting its potential for ocean engineering applications.
- Ablation studies confirm the necessity of both Transformer and GNN components for optimal performance.

## Why This Works (Mechanism)
The UATR-GTransformer leverages the strengths of both Transformers and GNNs to address the unique challenges of underwater acoustic signals. Transformers capture long-range global dependencies in the Mel-spectrogram patches, while GNNs model local neighborhood relationships and non-Euclidean structures inherent in the data. This hybrid approach allows the model to effectively learn both global patterns and local interactions, which are crucial for distinguishing between different underwater acoustic targets. The use of Mel-spectrograms as input further enhances the model's ability to capture relevant frequency information, which is essential for accurate target recognition.

## Foundational Learning

### Mel-spectrogram Processing
- **Why needed:** Transforms raw audio into a 2D representation that highlights frequency content over time, making it easier for deep learning models to identify patterns.
- **Quick check:** Verify that the Mel-spectrogram has 128 Mel bins and 512 time steps, as specified in the paper.

### Transformer Encoder
- **Why needed:** Captures global dependencies and long-range relationships in the Mel-spectrogram patches, which are crucial for understanding the overall structure of the acoustic signal.
- **Quick check:** Ensure the Transformer Encoder has 8 heads and a dimension of 96, as specified in the paper.

### Graph Neural Networks (GNNs)
- **Why needed:** Models local neighborhood relationships and non-Euclidean structures in the data, which are inherent in underwater acoustic signals.
- **Quick check:** Verify that the graph construction uses k-nearest neighbors (k ∈ [2,8]) and that the Max-Relative Graph Convolution is implemented correctly.

## Architecture Onboarding

### Component Map
Mel-spectrogram -> Patchify (5 conv layers) -> Transformer Encoder -> Graph Construction (KNN) -> Max-Relative Graph Convolution -> FFN -> Classification

### Critical Path
The critical path for the UATR-GTransformer is the sequential processing of the Mel-spectrogram through the patchify layers, Transformer Encoder, graph construction, Max-Relative Graph Convolution, and FFN. This path ensures that both global and local features are captured and processed effectively.

### Design Tradeoffs
The use of a hybrid Transformer-GNN architecture provides a balance between capturing global dependencies and modeling local neighborhood relationships. However, this approach may introduce computational overhead, particularly in the graph construction and Max-Relative Graph Convolution steps. The choice of k-nearest neighbors (k ∈ [2,8]) for graph construction is a tradeoff between capturing sufficient local information and maintaining computational efficiency.

### Failure Signatures
- **Performance Collapse (OA < 0.6):** Indicates issues with patch embedding dimensions or graph construction.
- **Overfitting on ShipsEar:** Suggests the need for stronger regularization or augmentation.
- **Slow Convergence:** May indicate the need for learning rate warmup or adjustments to the optimizer settings.

### 3 First Experiments
1. **Data Pipeline Validation:** Recreate the 70/15/15% time-based splits for both ShipsEar and DeepShip datasets and verify the Mel-spectrogram processing.
2. **Model Implementation Check:** Implement the UATR-GTransformer architecture and verify that each component (patchify, Transformer Encoder, graph construction, Max-Relative Graph Convolution, FFN) is correctly integrated.
3. **Training Configuration Test:** Train the model using the specified learning rates, batch sizes, and augmentation parameters, and compare the results to the reported performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the UATR-GTransformer maintain high recognition accuracy when deployed in diverse, unseen ocean environments?
- **Basis in paper:** The Conclusion states, "the model’s generalization ability to unseen sea areas and conditions requires further validation."
- **Why unresolved:** The experiments were restricted to only two publicly available datasets (ShipsEar and DeepShip), which may not fully represent the variability of real-world ocean acoustic channels.
- **What evidence would resolve it:** Performance metrics (OA, F1) evaluated on a new dataset containing data from distinct geographic locations or varying channel conditions.

### Open Question 2
- **Question:** Can the architecture be optimized to reduce the computational overhead of similarity calculations and MHSA for real-time applications?
- **Basis in paper:** The Conclusion notes, "computational complexity... is relatively high... which may restrict its real-time applicability."
- **Why unresolved:** The current implementation has a lower FPS (52.65) and higher prediction time compared to lightweight CNNs, hindering deployment on resource-constrained hardware.
- **What evidence would resolve it:** A modified architecture achieving a significant reduction in GFLOPs and latency (ms) without a substantial drop in accuracy.

### Open Question 3
- **Question:** How can graph feature quantification techniques be developed to identify the specific critical frequency bands influencing the model's decisions?
- **Basis in paper:** The Conclusion mentions, "it does not yet provide detailed insights into the most critical frequency bands."
- **Why unresolved:** While the model visualizes graph connections, it lacks a quantitative mechanism to pinpoint exactly which frequency components drive specific class distinctions.
- **What evidence would resolve it:** A quantitative study identifying specific frequency bands (e.g., via node importance weights) that correlate with known physical characteristics of target classes.

## Limitations
- The reproducibility of UATR-GTransformer is significantly hindered by the absence of public training/validation/test splits for both benchmark datasets.
- The Max-Relative Graph Convolution implementation depends on citation [41], and without access to the specific code or weight initialization strategies for the positional encoding, exact replication is challenging.
- The model's computational complexity may restrict its real-time applicability on resource-constrained hardware.

## Confidence
- **High Confidence:** The overall architectural design (patchify + Transformer + GNN) and the reported performance on public datasets (OA ~0.83) are credible, as the method is well-documented and the datasets are accessible.
- **Medium Confidence:** The specific training configurations (learning rates, batch sizes, augmentation parameters) are detailed, but the lack of data split reproducibility and implementation details for key components (graph convolution, positional encoding) introduces variability in achieving the exact reported results.
- **Low Confidence:** Without code or exact split information, achieving identical performance metrics (e.g., OA of 0.832 on ShipsEar) is uncertain.

## Next Checks
1. **Data Split Validation:** Recreate the 70/15/15% time-based splits for both ShipsEar and DeepShip datasets using the paper's methodology. Train the model and compare OA to the reported ~0.83.
2. **Graph Convolution Implementation:** Implement the Max-Relative Graph Convolution as described in citation [41]. Verify that the edge direction (e.g., $x_j - x_i$ vs. $x_i - x_j$) and aggregation function match the paper's intent.
3. **Positional Encoding Initialization:** Experiment with different initialization strategies for the learnable 2D positional encoding ($PE \in \mathbb{R}^{32 \times 8}$) and assess its impact on model convergence and performance.