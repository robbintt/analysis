---
ver: rpa2
title: Contributions to the Decision Theoretic Foundations of Machine Learning and
  Robust Statistics under Weakly Structured Information
arxiv_id: '2501.10195'
source_url: https://arxiv.org/abs/2501.10195
tags:
- decision
- contribution
- data
- preference
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis explores decision-theoretic foundations for machine
  learning and robust statistics under weakly structured information. It addresses
  the challenge of making decisions under uncertainty when preference and uncertainty
  information is incomplete or imprecise.
---

# Contributions to the Decision Theoretic Foundations of Machine Learning and Robust Statistics under Weakly Structured Information

## Quick Facts
- **arXiv ID:** 2501.10195
- **Source URL:** https://arxiv.org/abs/2501.10195
- **Reference count:** 40
- **Primary result:** Decision-theoretic framework using preference systems and credal sets enables robust comparison of classifiers and random variables under imprecise preferences and uncertainty.

## Executive Summary
This thesis develops decision-theoretic foundations for machine learning and robust statistics when information is incomplete or imprecise. It addresses the challenge of making decisions under uncertainty by modeling partial preferences as preference systems and imprecise probabilities as credal sets. The core contribution is a generalized stochastic dominance (GSD) relation that allows for robust comparison of random variables and classifiers without requiring complete cardinal utility or precise probability information. The work provides statistical tests for GSD and applies these methods to real-world problems including multi-dimensional poverty measurement, medical diagnosis, and credit risk assessment, demonstrating how decision theory can handle weakly structured information to produce more robust analyses.

## Method Summary
The method models partial preferences using preference systems (combining ordinal relations R₁ with cardinal intensity relations R₂) and imprecise probabilities using credal sets. For classifier comparison, it establishes a GSD relation where classifier X dominates Y if expected performance under all compatible utilities and probabilities satisfies the dominance condition. This is operationalized through linear programming, checking constraints across the convex sets of valid utilities and probabilities. Regularization (parameter δ) prevents overfitting when testing dominance statistically on finite samples. The framework handles mixed-scale data without aggregating metrics, instead producing a partial order that identifies undominated classifiers.

## Key Results
- Generalized Stochastic Dominance (GSD) relation enables robust comparison of classifiers under mixed-scale metrics without requiring aggregation
- Regularized statistical tests for GSD control Type I error while maintaining sensitivity to true dominance relationships
- Preference system framework successfully handles real-world problems including poverty measurement, medical diagnosis, and credit risk assessment
- Computational methods using linear programming make GSD feasible for practical applications across benchmark suites

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modeling incomplete preferences as preference systems and imprecise probabilities as credal sets enables robust decision-making without single-point estimates.
- **Mechanism:** Replaces single utility function with constrained set U_A defined by ordinal relation R₁ and cardinal intensity relation R₂; replaces single probability with convex set M; evaluates acts based on worst-case across these sets.
- **Core assumption:** Decision-maker can reliably express partial intensity information and probability bounds, and these constraints are consistent.
- **Evidence anchors:** Abstract states core method involves preference systems and credal sets; Section A.1.3 defines preference system structure; corpus references weak information handling.

### Mechanism 2
- **Claim:** GSD enables multi-criteria benchmarking respecting mixed-scale data without averaging metrics into single scores.
- **Mechanism:** Instead of comparing scalar expected values, GSD establishes partial order: Act X dominates Y if E_π(u∘X) ≥ E_π(u∘Y) for all compatible utilities u and probabilities π.
- **Core assumption:** Partial order (incomparability) is acceptable outcome when data insufficient to determine strict winner.
- **Evidence anchors:** Abstract mentions GSD allows robust comparison; Section B.2 proposes regularized GSD-relation for classifier comparison; corpus highlights balancing robust stats with learning efficiency.

### Mechanism 3
- **Claim:** Regularization of preference systems (parameter δ) prevents overfitting when statistically testing dominance relations on finite samples.
- **Mechanism:** Introduces granularity parameter δ which enforces minimum threshold for utility differences, filtering out extreme representations from sampling noise.
- **Core assumption:** Empirical sample deviates from true population, manifesting as spurious preferences in tail of utility representation set.
- **Evidence anchors:** Section B.1.2 discusses regularization counteracting conservatism; Section C.1.2 proposes regularized test statistic; corpus lacks specific link for this technique.

## Foundational Learning

- **Concept: Preference Systems (Partial Orders & Intensity)**
  - **Why needed here:** Standard decision theory assumes full cardinal utility; this relaxes to "weakly structured information" where you know A > B and maybe (A-B) > (C-D) but not exact values.
  - **Quick check question:** Can you distinguish between ordinal relation R₁ (A is better than B) and cardinal relation R₂ (difference between A and B is greater than C and D)?

- **Concept: Credal Sets (Imprecise Probabilities)**
  - **Why needed here:** Moves beyond single probability distributions; understanding convex sets of probabilities required to grasp modeling "uncertainty about uncertainty."
  - **Quick check question:** If you only know event S has probability between 0.2 and 0.4, can you define the set of valid probability measures (the credal set)?

- **Concept: Stochastic Dominance**
  - **Why needed here:** GSD is core comparison tool; need to understand classical First-Order Stochastic Dominance (comparing CDFs) to appreciate generalization to multiple criteria and imprecise probabilities.
  - **Quick check question:** Does random variable X stochastically dominate Y if CDF of X is always below (or equal to) that of Y?

## Architecture Onboarding

- **Component map:** Raw data (benchmark results) → Consequence Space A → Preference System [A, R₁, R₂] → Credal Set M → Linear Programming Solver → GSD-Relation (Partial Order) → Choice Set / GSD-Front

- **Critical path:**
  1. Define consequence space (e.g., vector of performance metrics)
  2. Elicit or define Preference System (R₁ for ordinal rankings, R₂ for intensity)
  3. Define Credal Set M (constraints on data probabilities)
  4. Operationalize via Linear Optimization: Set up LP to check if act dominated across all extreme points of M and U_A
  5. Apply Regularization (δ) if working with empirical/statistical samples

- **Design tradeoffs:**
  - Robustness vs. Decidability: Larger credal set increases robustness but increases likelihood of "incomparability"
  - Complexity vs. Information: Including partial cardinal info (R₂) increases computational complexity but improves discriminatory power over pure ordinal methods

- **Failure signatures:**
  - Empty Choice Set: Constraints too loose; everything incomparable. Fix: Tighten credal set or increase regularization δ
  - Inconsistent System: No utility function represents user input. Fix: Check R₁/R₂ consistency (Definition 2)
  - Conservative Testing: Permutation tests fail to reject null even when trends exist. Fix: Adjust regularization δ or use dynamic test variants

- **First 3 experiments:**
  1. Simple Classifier Comparison: Take 2 classifiers and 2 metrics (1 ordinal, 1 cardinal) from UCI dataset; implement LP check for GSD to verify dominance without aggregating scores
  2. Sensitivity Analysis (δ): Run statistical test on synthetic data; vary regularization parameter δ to observe trade-off between Type I error control and test sensitivity
  3. Robustness to Noise: Simulate contaminated sample (non-i.i.d. data); compare standard permutation tests against robustified "least favorable pair" test to verify error bound maintenance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can empirical choice functions (ecfs) be defined for common decision criteria such that they are strongly consistent statistical estimators for true choice sets, avoiding need for Closed World or Small World Assumptions?
- **Basis in paper:** Page 21, Section A.3 proposes developing "empirical decision theory" relying only on observable act-consequence pairs rather than specified states of nature
- **Why unresolved:** Represents proposed shift in theoretical foundation of decision theory to handle unspecified state spaces beyond standard model
- **What evidence would resolve it:** Formal proof defining ecfs for criteria like GSD and demonstrating strong consistency under i.i.d. sampling

### Open Question 2
- **Question:** Do robust methods for Pseudo-Label Selection (PLS) based on credal sets and Γ-minimax criterion, evaluated via nested Laplace approximation, outperform state-of-the-art methods in experimental studies?
- **Basis in paper:** Page 34, Section B.3 states experimental studies were previously lacking due to computational difficulties, but new approximation techniques now allow investigation
- **Why unresolved:** Theoretical framework proposed in Contribution 7 but specific robustification using credal sets lacked experimental validation
- **What evidence would resolve it:** Experimental results comparing accuracy of this robust PLS method against traditional baselines on real-world datasets

### Open Question 3
- **Question:** How can statistical inference—specifically parameter estimation, hypothesis testing, and regression analysis—be developed for poset-valued random variables based on depth functions?
- **Basis in paper:** Page 42, Section C.3 identifies lack of inferential methods for poset-valued data as gap, listing estimation and testing as "next natural steps"
- **Why unresolved:** While depth-based models for partial orders exist (Contribution 9), statistical tools to perform inference on these models have not been developed
- **What evidence would resolve it:** Formulation of unbiased estimators for model parameters and derivation of test statistics for comparing distributions of partial orders

### Open Question 4
- **Question:** Can computational complexity of linear programs used to check in-sample Generalized Stochastic Dominance (GSD) be significantly reduced for preference systems with only one cardinal dimension?
- **Basis in paper:** Page 43, Section C.3 discusses high computational cost of LPs for in-sample GSD and notes Proposition 8 suggests "drastic reduction" possible
- **Why unresolved:** Current LP formulations computationally intensive for large datasets, limiting scalability of proposed robust statistical comparisons
- **What evidence would resolve it:** Modified algorithm or closed-form solution exploiting structure of single-cardinal-dimension preference systems to lower time complexity

## Limitations
- Framework relies heavily on assumption that preference systems and credal sets can be reliably elicited from experts or derived from data
- Regularization parameter δ lacks systematic method for selection, potentially leading to overly conservative or liberal conclusions
- Computational complexity of solving linear programs for all classifier pairs across many permutations may limit scalability to very large benchmark suites

## Confidence
- **High Confidence:** Foundational definitions of preference systems, credal sets, and mathematical framework for GSD are clearly specified and logically sound within decision-theoretic literature; mechanism for transforming dominance checks into linear programming problems is well-established
- **Medium Confidence:** Effectiveness of regularization parameter δ and its impact on statistical tests is described, but lack of detailed procedure for selecting δ or sensitivity analysis results introduces uncertainty about practical application
- **Low Confidence:** Specific details of experimental implementation including exact form of credal set constraints and concrete values of regularization parameter δ employed are not provided, making it difficult to fully assess reproducibility and generalizability

## Next Checks
1. **Inconsistency Detection:** Implement pre-processing step to check for logical inconsistencies in given preference system (e.g., cyclical preferences) before running dominance analysis
2. **Regularization Sensitivity Analysis:** Systematically vary regularization parameter δ across grid of values (0.0, 0.1, 0.2, 0.3, 0.4, 0.5) and observe effect on statistical test's Type I error rate and ability to identify true dominance relationships
3. **Scalability Benchmark:** Measure computational time required to perform LP-based dominance checks for all pairs of classifiers across growing number of datasets and permutations to identify practical limits and inform optimizations