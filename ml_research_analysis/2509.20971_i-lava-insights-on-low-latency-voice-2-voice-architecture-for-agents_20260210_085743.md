---
ver: rpa2
title: 'i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents'
arxiv_id: '2509.20971'
source_url: https://arxiv.org/abs/2509.20971
tags:
- latency
- audio
- iterations
- speech
- component
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work presents i-LAVA, a low-latency voice-to-voice system\
  \ optimized for real-time conversational applications. The key contribution is optimizing\
  \ the TTS component of an end-to-end pipeline (ASR \u2192 LLM \u2192 TTS) by reducing\
  \ the number of Residual Vector Quantization (RVQ) iterations in the CSM-1B model."
---

# i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents

## Quick Facts
- arXiv ID: 2509.20971
- Source URL: https://arxiv.org/abs/2509.20971
- Reference count: 12
- One-line primary result: Optimizing CSM-1B TTS with reduced RVQ iterations achieves real-time voice generation (RTF < 1) while maintaining acceptable audio quality.

## Executive Summary
This work presents i-LAVA, a low-latency voice-to-voice system optimized for real-time conversational applications. The key contribution is optimizing the TTS component of an end-to-end pipeline (ASR → LLM → TTS) by reducing the number of Residual Vector Quantization (RVQ) iterations in the CSM-1B model. This trade-off reduces latency while maintaining acceptable audio quality. Experiments show that reducing RVQ iterations from 32 to 16 decreases first-chunk latency significantly (e.g., from 1382ms to 641ms on GPU) and improves the Real-Time Factor to below 1, enabling real-time voice generation. Streaming the LLM response further reduces overall latency. The optimized architecture achieves stable streaming with average inter-chunk latency close to two-thirds of chunk length, ensuring a responsive user experience.

## Method Summary
The method involves optimizing a cascaded voice-to-voice pipeline by focusing on the TTS bottleneck. The approach reduces Residual Vector Quantization iterations in the CSM-1B decoder, applies torch.compile to speed up inference, and implements streaming of LLM tokens to parallelize generation and synthesis. The system uses Whisper v3-large-turbo for chunked ASR, gpt-4o-mini for LLM with 10-token streaming, and CSM-1B with configurable RVQ iterations and Mimi tokenizer. Key optimizations include hardware-specific tuning (GPU vs CPU) and cold-start mitigation through warmup generations.

## Key Results
- Reducing RVQ iterations from 32 to 16 decreases GPU first-chunk latency from 1382ms to 641ms while maintaining acceptable audio quality (SNR drops from 33.1dB to 7.2dB).
- Streaming LLM tokens to TTS reduces end-to-end latency from 3.33s (one-shot) to 1.28s, with LLM contribution dropping from 2.15s to 0.15s.
- The optimized system achieves Real-Time Factor below 1 on GPU, enabling real-time voice generation, with average inter-chunk latency close to two-thirds of chunk length.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing RVQ iterations in the CSM-1B TTS decoder proportionally decreases first-chunk latency at the cost of audio quality.
- Mechanism: Residual Vector Quantization is inherently sequential—each iteration quantizes the residual error from the previous stage. Fewer iterations means fewer sequential decoder passes before audio output begins.
- Core assumption: The downstream use case (e.g., telephone-quality support) tolerates reduced Signal-to-Noise Ratio without breaking user immersion.
- Evidence anchors:
  - [abstract] "reducing RVQ iterations from 32 to 16 decreases first-chunk latency significantly (e.g., from 1382ms to 641ms on GPU)"
  - [section III-B] Table IV shows GPU first-chunk latency dropping from 1381.9ms (32 iterations) to 640.9ms (16 iterations); SNR drops from 33.1dB to 7.2dB
  - [corpus] Related work (arXiv:2508.04721) confirms cascaded V2V systems remain latency-bound by TTS, validating optimization focus on this component
- Break condition: If SNR drops below the perceptual threshold for the target channel (e.g., <5dB for telephony), user comprehension degrades and the trade-off is no longer viable.

### Mechanism 2
- Claim: Streaming LLM tokens to TTS (every 10 tokens) reduces end-to-end latency by parallelizing generation and synthesis.
- Mechanism: Instead of waiting for complete LLM response, TTS begins synthesizing the first audio chunk while the LLM continues generating subsequent tokens. This overlaps compute rather than serializing it.
- Core assumption: The TTS model can produce coherent prosody from incomplete sentence prefixes without requiring full context.
- Evidence anchors:
  - [abstract] "Streaming the LLM response further reduces overall latency"
  - [section III-A] Table I shows total GPU latency dropping from 3.33s (one-shot) to 1.28s (streaming); LLM contribution drops from 2.15s to 0.15s
  - [corpus] PredGen (arXiv:2506.15556) demonstrates similar gains via input-time speculation for LLM-driven TTS
- Break condition: If LLM token generation stalls or produces disfluencies requiring backtracking, streamed TTS may produce unusable audio requiring regeneration.

### Mechanism 3
- Claim: Maintaining inter-chunk latency below chunk duration ensures non-empty audio buffer and seamless playback.
- Mechanism: When average inter-chunk latency is less than chunk length (~2/3 per the paper), the audio queue remains populated. The consumer (playback) is slower than the producer (TTS).
- Core assumption: Network jitter and playback buffer overhead are bounded and predictable.
- Evidence anchors:
  - [abstract] "average inter-chunk latency close to two-thirds of chunk length, ensuring a responsive user experience"
  - [section III-C] GPU streaming shows 685ms inter-chunk latency vs 1600ms average chunk size; CPU shows 1191ms vs 1504ms (borderline)
  - [corpus] Voila (arXiv:2505.02707) emphasizes continuous listening and chunked response as core to real-time interaction
- Break condition: If compute contention (e.g., GPU memory pressure) spikes inter-chunk latency above chunk duration, buffer underruns cause audible gaps.

## Foundational Learning

- Concept: **Residual Vector Quantization (RVQ)**
  - Why needed here: RVQ is the bottleneck in CSM-1B's decoder; understanding it is prerequisite to reasoning about the latency/quality trade-off.
  - Quick check question: If you halve RVQ iterations from 32 to 16, would you expect SNR to improve, degrade, or stay the same? Why?

- Concept: **Real-Time Factor (RTF)**
  - Why needed here: RTF < 1 is the threshold for real-time generation; this metric determines if the system can stream without prebuffering.
  - Quick check question: An RTF of 0.5 means generating 1 second of audio takes how long?

- Concept: **Voice Activity Detection (VAD) End-of-Speech Window**
  - Why needed here: The 1.5s silence window determines when ASR finalizes; this sets the floor for user-perceived responsiveness.
  - Quick check question: If ASR processing takes 0.5s and is absorbed into the VAD silence window, what is the *effective* ASR latency?

## Architecture Onboarding

- Component map:
  - Silero-VAD (VAD with 1.5s silence threshold) -> Whisper v3-large-turbo (chunked ASR) -> gpt-4o-mini (LLM with 10-token streaming) -> CSM-1B (TTS with configurable RVQ iterations) -> Playback queue

- Critical path:
  1. User speaks → VAD detects speech segments
  2. After 1.5s silence, VAD triggers ASR finalization
  3. ASR transcript + context → LLM (streaming enabled)
  4. LLM tokens (every 10) → TTS (CSM-1B with 16 RVQ iterations on GPU)
  5. TTS emits audio chunks → playback queue

- Design tradeoffs:
  - **RVQ iterations vs audio quality**: 16 iterations = 641ms first-chunk / 7.2dB SNR; 32 = 1382ms / 33.1dB SNR
  - **Mimi codebook configuration**: Using full 32 codebooks with padded RVQ output adds natural pauses but elongates speech; may not suit time-critical agents
  - **LLM streaming vs coherence**: Streaming reduces latency 2.6x but requires TTS robustness to partial sentences
  - **GPU vs CPU deployment**: GPU achieves RTF <1 with 32 iterations; CPU requires dropping to 16

- Failure signatures:
  - **Buffer underrun**: Inter-chunk latency exceeds chunk duration → audible gaps (observed on CPU with 16 RVQ)
  - **Cold start penalty**: First 2 CSM generations are slow; production systems must pre-warm
  - **Quality collapse**: SNR variability (observed range -2.2dB to 26.5dB with padding strategies) indicates unstable outputs at low RVQ

- First 3 experiments:
  1. **Baseline profiling**: Measure first-chunk latency, RTF, and SNR on your target hardware (GPU/CPU) at 32, 24, 20, 16 RVQ iterations using the same benchmark sentence from the paper.
  2. **Streaming integration test**: Enable LLM streaming (10-token chunks) and measure end-to-end latency vs one-shot; verify the ~2.6x speedup holds in your stack.
  3. **Quality threshold calibration**: Run MOS-like human evaluation or WADA-SNR on outputs at each RVQ level to determine the minimum acceptable quality for your use case before committing to 16-iteration deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generating custom CUDA kernels for the sequential Residual Vector Quantization (RVQ) process minimize CPU-GPU handoff overhead?
- Basis in paper: [explicit] The authors propose optimizing the sequential Python RVQ code by generating CUDA kernels to run natively on GPU.
- Why unresolved: Current RVQ implementation involves sequential computations causing CPU bottlenecks that limit GPU utilization.
- What evidence would resolve it: Latency benchmarks comparing current Python execution against a custom CUDA-optimized implementation.

### Open Question 2
- Question: Does implementing streaming ASR allow processing time to be fully absorbed within the Voice Activity Detector's (VAD) silence window?
- Basis in paper: [explicit] The paper suggests streaming ASR to process audio immediately upon VAD detection rather than waiting for full audio collection.
- Why unresolved: The current non-streaming approach contributes distinct latency; simultaneous processing is hypothesized to mask this delay effectively.
- What evidence would resolve it: End-to-end profiling showing ASR processing completes within the 1.5-second silence window, resulting in near-zero effective latency.

### Open Question 3
- Question: Can replacing the Llama 3.2 backbone and decoder with more efficient models further reduce TTS latency without degrading audio quality?
- Basis in paper: [explicit] The Future Work section suggests experimenting with efficient model replacements for the native CSM components.
- Why unresolved: Current optimizations focus on RVQ iterations, leaving the underlying model architecture as a potential speed bottleneck.
- What evidence would resolve it: Comparative metrics showing reduced RTF and first-chunk latency using alternative backbone architectures while maintaining SNR.

## Limitations

- The reported gains are highly specific to the CSM-1B architecture and the particular RVQ implementation, limiting generalizability to other TTS models.
- Achieving inter-chunk latency below chunk duration on CPU requires aggressive RVQ reduction, which may introduce quality variability as observed in the SNR range.
- The assertion that the system is "optimized for real-time" is qualified: while RTF < 1 on GPU with 32 RVQ iterations, real-world deployment may still require compromises on audio quality or hardware investment.

## Confidence

- **High Confidence**: RVQ iteration reduction directly decreases first-chunk latency (quantitative, hardware-measured).
- **Medium Confidence**: Streaming LLM to TTS reduces end-to-end latency by ~2.6x (demonstrated, but dependent on stable token generation).
- **Medium Confidence**: Inter-chunk latency < chunk duration ensures non-empty buffer (supported by measurements, but hardware-sensitive).

## Next Checks

1. Benchmark RTF and inter-chunk latency on your target hardware (GPU/CPU) with 16 vs 32 RVQ iterations; verify <1 RTF and non-empty buffer for your chunk size.
2. Evaluate audio quality degradation at 16 RVQ iterations via human MOS or WADA-SNR; confirm acceptability for your use case.
3. Test streaming LLM integration with your chosen model and measure end-to-end latency gains; validate robustness to partial-sentence TTS.