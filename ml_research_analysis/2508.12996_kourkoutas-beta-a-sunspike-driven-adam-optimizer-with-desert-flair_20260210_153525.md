---
ver: rpa2
title: 'Kourkoutas-Beta: A Sunspike-Driven Adam Optimizer with Desert Flair'
arxiv_id: '2508.12996'
source_url: https://arxiv.org/abs/2508.12996
tags:
- adam
- loss
- none
- seed
- params
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Kourkoutas-Beta replaces Adam\u2019s fixed second-moment discount\
  \ with a layer-wise dynamic value driven by a bounded sunspike ratio, which compares\
  \ current pooled gradient norms to their exponential moving average. Large spikes\
  \ lower the discount toward a minimum value to enable faster adaptation, while calm\
  \ phases keep it near a maximum for stability."
---

# Kourkoutas-Beta: A Sunspike-Driven Adam Optimizer with Desert Flair

## Quick Facts
- **arXiv ID**: 2508.12996
- **Source URL**: https://arxiv.org/abs/2508.12996
- **Reference count**: 40
- **Primary result**: Dynamic second-moment discount (beta2) driven by gradient spikes improves stability and final loss versus fixed-beta2 Adam across PDE surrogates, PINNs, and character-level LMs.

## Executive Summary
Kourkoutas-Beta modifies Adam's second-moment exponential moving average by replacing the fixed beta2 with a layer-wise dynamic value driven by a "sunspike" ratio. This ratio compares current pooled gradient norms to their exponential moving average, lowering beta2 during spikes for faster adaptation and raising it during calm phases for stability. The method includes optional extensions like leaky-AMSGrad and trust-region clipping, and matches Adam exactly when dynamic beta2 is disabled. Across four testbeds, Kourkoutas-Beta improves stability and final loss compared to fixed-beta2 Adam, notably reducing bits-per-character by about 38% on small-enwik8 versus Adam-0.95 over 10 seeds while maintaining runtime parity.

## Method Summary
Kourkoutas-Beta replaces Adam's fixed second-moment discount (beta2) with a dynamic value per parameter bucket, computed via a sunspike ratio that compares current gradient norms to their exponential moving average. Large spikes lower beta2 toward a minimum for rapid adaptation, while calm phases keep it near a maximum for stability. The method optionally includes leaky-AMSGrad, trust-region clipping, and adaptive tiny terms, and matches Adam exactly when dynamic beta2 and all options are disabled. It uses Apple's MLX framework and is designed for scenarios where gradient spiking degrades fixed-beta2 Adam performance.

## Key Results
- On small-enwik8 (30 MB slice), Kourkoutas-Beta reduces final bits-per-character by about 38% versus Adam-0.95 and about 58% versus Adam-0.999 over 10 seeds.
- Dramatically smaller variance compared to fixed-beta2 Adam while maintaining runtime parity.
- Improves stability and final loss across four testbeds: Transformer PDE surrogate, 3D PINN, synthetic sequence toy with rare triggers, and character-level Transformer.
- Preserves Adam-style convergence guarantees under standard assumptions while improving robustness under spiky gradients.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Lowering beta2 during gradient spikes prevents optimizer lag behind rapid shifts in gradient scale.
- **Mechanism:** Sunspike ratio sun = raw/(1+raw) where raw = ||g_t||/(r_t + epsilon). When raw >> 1, sun approaches 1, driving beta2 toward beta2,min, shortening the second-moment memory window for rapid adaptation.
- **Core assumption:** Spiky gradients represent meaningful loss landscape shifts rather than pure noise.
- **Evidence anchors:** Abstract states large spikes lower discount for faster adaptation; section 6.1.6 shows Adam-999 stalls requiring 10^3 steps to forget pre-transition scale while Kourkoutas-beta adapts rapidly.
- **Break condition:** If spikes are purely noise (e.g., label errors), shortening memory might amplify noise and destabilize training.

### Mechanism 2
- **Claim:** Restoring high beta2 during calm phases preserves Adam's stability and smoothing properties.
- **Mechanism:** When gradient norms align with EMA (raw ≈ 0), sun approaches 0, pushing beta2 toward beta2,max, restoring long-memory behavior for noise smoothing.
- **Core assumption:** Training alternates between spiky phases requiring agility and calm phases benefiting from stability.
- **Evidence anchors:** Abstract mentions calm phases keep discount near maximum for stability; section 6.1.5 shows Kourkoutas-beta is more consistent than high-memory Adam-999.
- **Break condition:** If EMA coefficient alpha is poorly tuned, optimizer might misclassify calm phases as spiky or vice versa.

### Mechanism 3
- **Claim:** Layer-wise adaptation handles heterogeneous gradient scales across the network.
- **Mechanism:** User-defined layer_key_fn (e.g., grouping by parameter path or shape) computes distinct sunspike ratios and beta2 values for different parameter groups.
- **Core assumption:** Different network parts experience gradient spikes at different times or magnitudes.
- **Evidence anchors:** Section 2.1 describes parameter buckets and typical choices like module path or tensor shape; section 3 discusses coarser buckets yielding smoother signals versus finer buckets localizing adaptation.
- **Break condition:** If bucketing is too granular, pooled gradient norms might be statistically insignificant, leading to erratic beta2 dynamics.

## Foundational Learning

- **Concept: Exponential Moving Average (EMA) in Adam**
  - **Why needed here:** Beta2 acts as decay rate on second moment v_t. High beta2 (0.999) implies long memory (~1000 steps), low beta2 implies short memory. Without this, sunspike mechanism is opaque.
  - **Quick check question:** "If I set beta2 to 0.5 instead of 0.999, approximately how many recent steps influence current second-moment estimate?" (Answer: ~2 steps).

- **Concept: Gradient Spikes / Heavy-Tailed Noise**
  - **Why needed here:** Optimizer designed for spiky regimes (PDEs, rare triggers). Distinguish stationary noise (Gaussian) from non-stationary bursts to understand why dynamic window preferable to fixed one.
  - **Quick check question:** "Why might fixed long memory (beta2=0.999) be dangerous when gradient magnitude suddenly increases by 100x?" (Answer: Denominator v_t lags, causing massive effective step sizes).

- **Concept: Bias Correction**
  - **Why needed here:** Standard Adam corrects initialization bias of m_t and v_t using 1-beta^t. Kourkoutas-beta introduces dynamic beta2, complicating correction (modes: none, beta2max, exact). Understanding standard correction vital to grasp why exact mode requires tracking product of beta2 values.
  - **Quick check question:** "In standard Adam, why is bias correction critical during first few hundred steps?" (Answer: Because v_t initialized at 0, making denominator artificially small).

## Architecture Onboarding

- **Component map:** Gradients g_t and Parameters theta -> Pooling Layer (computes L2 norms per bucket ||g^(ell)_t||) -> Sunspike Logic (maintains EMA r_t, computes bounded ratio sun in [0,1)) -> Beta Scheduler (maps sun -> beta2,t) -> Adam Core (standard Adam update rules m_t, v_t using dynamic beta2,t) -> Optional Extensions (leaky-AMSGrad, Trust-region clipping, Adaptive tiny terms).

- **Critical path:** Sunspike Logic -> Beta Scheduler. This is the novel contribution. Engineer must verify raw is correctly normalized by EMA r_t and squashed to [0,1) for stability.

- **Design tradeoffs:**
  - **Bucket Granularity:** Fine-grained (per-module) buckets offer precise adaptation but incur higher overhead and lower statistical significance for norms. Coarse buckets (e.g., p.shape) are faster and smoother but less adaptive.
  - **Bias Correction:** Exact mode is theoretically robust but requires log-space accumulation to avoid underflow at long horizons. Beta2max mode is conservative approximation.
  - **Safety features:** Trust-region clipping improves robustness (critical in PINNs) but adds complexity to update rule.

- **Failure signatures:**
  - **High Variance/Instability:** If beta2,min is too low and spikes are frequent, optimizer might behave like SGD with unstable learning rates.
  - **Stagnation:** If beta2,max is too low, optimizer fails to smooth noise in calm phases.
  - **Numerical Underflow:** In exact bias correction mode over long training runs (>100k steps) if log-space accumulation isn't used.

- **First 3 experiments:**
  1. **Sanity Check (Control):** Run quadratic_bowl.py or logistic_regression.py with beta2,min = beta2,max and verify exact numerical equivalence with standard MLX Adam.
  2. **Toy Spike Test:** Run rare_trigger_toy.py (Testbed C). Visualize sunspike ratio sun over time; confirm it spikes when rare trigger appears and decays otherwise.
  3. **Real-World Stress Test:** Train Transformer on PDE surrogate or small-enwik8. Compare loss curves of Kourkoutas-beta against Adam(0.999) and Adam(0.95). Look specifically for "jitter" in Adam(0.999) that is smoothed by Kourkoutas-beta.

## Open Questions the Paper Calls Out
- How does Kourkoutas-Beta compare against other modern adaptive optimizers designed for non-stationary or spiky gradients, such as AdaBelief, RAdam, or Yogi?
- Does the method retain its stability and convergence advantages when scaled to Large Language Model (LLM) pretraining or Mixture-of-Experts (MoE) architectures?
- Can the optimizer's performance be further improved by implementing dynamic first-moment adaptation (dynamic beta1) alongside existing second-moment modulation?
- Is the observed optimal range for sunspike EMA coefficient (alpha ≈ 0.93) universal across diverse domains, or does it require tuning for different gradient regimes?

## Limitations
- Efficacy depends critically on correctly identifying meaningful gradient spikes versus noise, which may fail in datasets with high intrinsic variance or labeling errors.
- Reliance on Apple's MLX framework limits reproducibility to Apple Silicon hardware, creating significant barrier for broader validation.
- Layer-wise adaptation via bucketing introduces uncertainty about optimal granularity, which is problem-dependent and may not generalize.

## Confidence
- **High Confidence:** Core mechanism of dynamic beta2 modulation via sunspike ratio is sound and internally consistent with clear mathematical derivation and implementation details.
- **Medium Confidence:** Empirical improvements on small-enwik8 (38% BPC reduction) are compelling but based on single dataset slice and 10 seeds, requiring further validation for generalization.
- **Low Confidence:** Claim that method "preserves Adam-style convergence guarantees under standard assumptions" is asserted but not rigorously proven, needing formal analysis of dynamic beta2 interaction with Adam's convergence theory.

## Next Checks
1. **Noise Robustness Test:** Run Kourkoutas-beta on synthetic dataset with injected label noise. Measure whether optimizer's dynamic beta2 amplifies noise-induced spikes or remains stable. Compare final loss and variance to standard Adam.
2. **Hardware Portability:** Attempt to reimplement sunspike mechanism in PyTorch/CUDA. Validate that core dynamic beta2 logic produces identical numerical results to MLX version on simple task (e.g., logistic regression).
3. **Bucketing Sensitivity Analysis:** Systematically vary layer_key_fn (e.g., per-module vs. per-shape vs. global) on PDE surrogate task. Quantify tradeoff between adaptation granularity and statistical stability of pooled gradient norms. Identify optimal bucketing strategy for heterogeneous networks.