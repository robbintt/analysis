---
ver: rpa2
title: Unreflected Use of Tabular Data Repositories Can Undermine Research Quality
arxiv_id: '2503.09159'
source_url: https://arxiv.org/abs/2503.09159
tags:
- datasets
- data
- benchmark
- performance
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Unreflected use of tabular data repositories can undermine research
  quality. Data repositories are widely used for tabular data research, but their
  datasets are often used without critical reflection, leading to reduced research
  quality.
---

# Unreflected Use of Tabular Data Repositories Can Undermine Research Quality

## Quick Facts
- arXiv ID: 2503.09159
- Source URL: https://arxiv.org/abs/2503.09159
- Reference count: 34
- Primary result: Unreflected use of tabular data repositories leads to inappropriate validation, weak baselines, and missing preprocessing guidance, undermining research quality.

## Executive Summary
This paper demonstrates how unreflected use of tabular data repositories undermines research quality through three primary issues: inappropriate validation strategies (e.g., holdout instead of cross-validation), missing strong baseline performance, and insufficient preprocessing guidance. Using examples from OpenML and recent studies, the authors show these issues lead to biased model comparisons, underestimated performance, and misleading conclusions. They recommend avoiding default validation strategies, including strong baselines, and carefully inspecting datasets for preprocessing needs. To address these issues systematically, they propose repositories provide expert-reviewed default evaluation tasks with preprocessing protocols, validation strategies, and strong baseline performance.

## Method Summary
The paper evaluates tabular benchmarks (TabZilla-hard with 36 classification datasets, Grinsztajn et al. with 45 datasets) using LightGBM, XGBoost, MLP, and TabM with 100 HPO trials per model (20 random + 80 TPE via Optuna). They compare holdout validation vs 5-fold cross-validation, use 15 random seeds for Grinsztajn, and apply early stopping with patience=200 (trees) or 5 (NN). Preprocessing variants are tested, including date feature handling. Performance is measured via logloss for classification and MSE/RMSE for regression, aggregated through average rank and normalized metrics.

## Key Results
- Single holdout validation underestimates model performance and increases variance compared to 5-fold cross-validation
- Weak baseline performance due to insufficient HPO budgets leads to "fluctuating state-of-the-art" results
- Target leakage and suboptimal preprocessing distort model comparisons when datasets are used without reflection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Utilizing single holdout validation for model selection tends to underestimate model performance and increases variance in rankings compared to cross-validation.
- Mechanism: The paper suggests that holdout validation is prone to overfitting the hyperparameters to a specific validation split. By switching to 5-fold cross-validation (5CV), the evaluation averages performance across multiple splits, which the authors demonstrate consistently improves generalization estimates.
- Core assumption: The performance gains observed are not solely due to increased computational budget but the reduction in split-specific bias.
- Evidence anchors:
  - [abstract]: Mentions "inappropriate validation strategies (e.g., using holdout validation instead of cross-validation)" as a key issue.
  - [Page 3, Section 2.1]: Figure 1 shows 5CV consistently improves performance over holdout across the TabZilla benchmark.
  - [corpus]: Weak or missing direct evidence in the provided corpus neighbors regarding holdout vs. cross-validation bias specifically in tabular repositories.
- Break condition: If a dataset is so large that a single holdout set contains sufficient samples to represent the distribution statistically, the performance gap between holdout and 5CV likely diminishes.

### Mechanism 2
- Claim: The absence of strong, standardized baselines in repositories leads to "fluctuating state-of-the-art" results where improvements are claimed over weak baselines rather than genuine advancements.
- Mechanism: The paper demonstrates that prior benchmarks (specifically TabZilla) used restrictive Hyperparameter Optimization (HPO) budgets for baselines. When these baselines are tuned properly (e.g., 100 trials with 5CV), they often outperform the "novel" models that were compared against the weaker versions.
- Core assumption: Assumption: The improvements shown in the re-evaluation are due to better tuning protocols rather than fundamental changes to the baseline model architectures.
- Evidence anchors:
  - [abstract]: Highlights "missing strong baseline performance for each dataset."
  - [Page 5, Section 2.2]: Table 1 shows that a better-tuned MLP outperforms all 18 models from the original TabZilla benchmark.
  - [corpus]: Weak or missing; corpus neighbors focus on LLM applications rather than benchmarking methodology standards.
- Break condition: If a repository enforces a strict, compute-equitable HPO budget for all uploaded results, the "weak baseline" issue is mitigated.

### Mechanism 3
- Claim: Unreflected usage of dataset versions without preprocessing guidance leads to target leakage or suboptimal feature representation, distorting model comparisons.
- Mechanism: Datasets often contain "forbidden features" (e.g., IDs, alternative targets) or raw features (e.g., dates) that require engineering. The paper shows that using preprocessed versions blindly can introduce leaks (inflating performance), while failing to engineer features (like date differences) can artificially depress performance for specific model types.
- Core assumption: The "leaks" identified (e.g., total amount predicting tip) are universally invalid features for predictive modeling tasks.
- Evidence anchors:
  - [abstract]: Notes "insufficient preprocessing guidance" as a primary issue.
  - [Page 7, Section 2.3]: Figure 3 illustrates how resolving target leaks drastically lowers performance, and Figure 4 shows how simple preprocessing changes model rankings.
  - [corpus]: Weak or missing; corpus focuses on representation learning rather than data hygiene or leakage detection.
- Break condition: If a user manually inspects feature semantics and constructs a theoretically sound preprocessing pipeline independent of the repository's default, this mechanism is bypassed.

## Foundational Learning

- Concept: **Cross-Validation vs. Holdout Strategies**
  - Why needed here: To understand why the paper advocates for 5CV over the default single-split strategies often found in repositories.
  - Quick check question: Why might a model selected on a single holdout set perform poorly on unseen test data compared to one selected via 5-fold cross-validation?

- Concept: **Target Leakage (Data Leakage)**
  - Why needed here: To identify the "forbidden features" discussed in Section 2.3 that invalidate benchmark results.
  - Quick check question: If a dataset aims to predict customer churn, would including a feature titled "Cancellation_Date" constitute a target leak?

- Concept: **Hyperparameter Optimization (HPO) Budgets**
  - Why needed here: To grasp the argument that "weak baselines" are often a result of insufficient tuning (low trial counts or time limits) rather than poor model architecture.
  - Quick check question: How does limiting the number of HPO trials for a baseline model bias a comparison against a novel model that was tuned more extensively?

## Architecture Onboarding

- Component map:
  - Repository Layer: Stores Raw Data, Processed Versions, and Task Definitions (Target + Metrics)
  - Evaluation Schema: Proposed "Default Task" containing: (1) Preprocessing Protocol, (2) Validation Protocol (e.g., 5CV), (3) Estimation Protocol (Train/Test splits)
  - Baseline Service: A subsystem to store and display the "Strong Baseline Performance" (e.g., LightGBM with 100 HPO trials) for every dataset task

- Critical path:
  1. Ingestion: Curator uploads raw data
  2. Validation: Curator defines "Default Task" with explicit preprocessing and validation strategy
  3. Calibration: A strong baseline (e.g., XGBoost/LightGBM) is run with full HPO to establish a reference score
  4. Consumption: User downloads the "Default Task" and compares their model against the calibrated baseline

- Design tradeoffs:
  - Flexibility vs. Standardization: The paper suggests repositories currently prioritize flexibility (open upload), leading to "unreflected use." The proposed architecture adds rigid "Default Tasks" to enforce rigor, potentially at the cost of experimentation speed
  - Raw vs. Processed: Storing only raw data requires users to have high preprocessing skill; storing processed data risks propagating "leaks" or "suboptimal transforms." The paper argues for a linked architecture where processed versions explicitly reference raw versions

- Failure signatures:
  - Unrealistic Performance: Models achieving near-perfect scores (e.g., R2 > 0.98) often indicate target leakage (Section 2.3)
  - High Variance: Large performance gaps between validation and test scores indicate overfitting to the validation split (Section 2.1)
  - Baseline Inversion: If a simple, well-tuned LightGBM model outperforms complex deep learning models cited as "SOTA" on a repository task, the original benchmark likely suffered from weak baselines (Section 2.2)

- First 3 experiments:
  1. Validation Audit: Take a dataset currently using holdout validation. Retrain the model using 5-fold cross-validation. Compare the variance and mean performance to quantify the "Holdout Bias."
  2. Leak Detection Sweep: Scan high-performing datasets in the repository for features that are deterministic of the target (e.g., high linear correlation or categorical overlap). Remove them and re-run the "Strong Baseline" to check for performance collapse.
  3. Baseline Calibration: Run a standard LightGBM model with 100 HPO trials and no time limit on a specific dataset. Compare this score against the top entries in the repository to see if the current "State-of-the-Art" is actually under-tuned.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated procedures be developed to select the optimal validation strategy for a specific tabular dataset?
- Basis in paper: [explicit] Section 2.1 states that "further research is needed to determine the appropriate validation strategy for different datasets... and automated procedures could be beneficial."
- Why unresolved: No standard exists for choosing between holdout and cross-validation, leading to the widespread use of biased defaults like single holdout splits.
- What evidence would resolve it: A meta-learning framework or heuristic that dynamically selects a validation protocol (e.g., based on data size or variance) that minimizes the risk of overfitting compared to fixed defaults.

### Open Question 2
- Question: What is the most effective governance model for defining and updating "default evaluation tasks" in open repositories?
- Basis in paper: [explicit] The authors ask "Who should define default tasks?" in Section 3, noting that manual expert review is tedious for open-upload communities.
- Why unresolved: There is a conflict between the scalability of open repositories and the need for rigid, expert-reviewed standards to ensure research quality.
- What evidence would resolve it: A successful pilot implementation where a repository uses a hybrid community-expert curation pipeline to maintain benchmark integrity without creating bottlenecks.

### Open Question 3
- Question: How can data repositories structure metadata to explicitly distinguish raw data from preprocessed versions and prevent target leakage?
- Basis in paper: [inferred] Section 2.3 highlights that OpenML's versioning system misleads users and lacks processing history, causing errors like target leakage.
- Why unresolved: Repositories currently treat dataset versions as static files rather than pipeline outputs, making it difficult to trace feature provenance or detect inappropriate feature inclusion.
- What evidence would resolve it: A new schema implementation that links raw datasets to specific preprocessed variants via documented transformation scripts, successfully flagging leakage risks to users.

## Limitations

- Reproducibility constraints: The paper lacks specific random seeds and does not provide a public codebase, making exact numerical replication difficult.
- Scope of evidence: While the paper demonstrates issues using specific tabular benchmarks, the generalizability to all tabular data repositories and ML tasks remains untested.
- Bias assumptions: The claim that 5CV consistently outperforms holdout assumes performance gains are due to reduced split-specific bias rather than increased computational budget.

## Confidence

- High confidence: The existence of target leakage and weak baselines in real datasets is well-supported by the empirical examples shown in Sections 2.2 and 2.3.
- Medium confidence: The claim that holdout validation underestimates model performance and increases variance is plausible but lacks direct corpus support; the mechanism is theoretically sound but requires broader validation.
- Low confidence: The assertion that these issues are widespread across all tabular repositories is extrapolated from a limited set of benchmarks without systematic survey data.

## Next Checks

1. **Validation protocol audit**: Systematically compare holdout vs. 5-fold CV across a larger, diverse set of tabular datasets to quantify the frequency and magnitude of performance gaps.
2. **Leak detection protocol**: Develop and apply an automated pipeline to scan high-performing models in repositories for deterministic or near-deterministic features predicting the target, then re-evaluate baselines post-removal.
3. **Baseline calibration sweep**: Run a standardized LightGBM baseline with 100 HPO trials across multiple datasets in a repository and compare its performance against all existing submissions to identify "weak baseline" cases.