---
ver: rpa2
title: 'PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus'
arxiv_id: '2505.20323'
source_url: https://arxiv.org/abs/2505.20323
tags:
- event
- time
- events
- case
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PMOA-TTS is a large-scale corpus of 124,699 clinical case reports
  from PubMed Open Access, each annotated with structured textual timelines of (event,
  time) pairs using large language models. The corpus includes over 5.6 million timestamped
  clinical events alongside extracted demographics and diagnoses.
---

# PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus

## Quick Facts
- arXiv ID: 2505.20323
- Source URL: https://arxiv.org/abs/2505.20323
- Reference count: 40
- Primary result: Large-scale corpus of 124,699 clinical case reports with structured textual timelines of (event, time) pairs

## Executive Summary
PMOA-TTS is a corpus of 124,699 clinical case reports from PubMed Open Access, each annotated with structured textual timelines of (event, time) pairs using large language models. The corpus includes over 5.6 million timestamped clinical events alongside extracted demographics and diagnoses. Technical validation using a clinician-curated gold set showed event match rates up to 84.7% and temporal concordance (c-index) up to 0.891 across different LLM models. The dataset enables research on timeline extraction, temporal reasoning, and survival modeling from narrative text and is publicly available.

## Method Summary
The PMOA-TTS corpus is created through a two-stage filtering process: first, regex filtering followed by a Llama 3.3 70B Instruct classifier to identify single-patient case reports. Timeline extraction is performed using LLM prompting (Llama 3.3 70B, DeepSeek-R1) to identify reference times and convert relative temporal expressions into numeric hour offsets. Events are extracted as structured (event, time) tuples and validated against a gold standard using PubMedBERT embeddings and recursive best-match alignment. The corpus supports downstream tasks including survival analysis using LLM-derived embeddings as covariates.

## Key Results
- 124,699 clinical case reports from PubMed Open Access with structured timelines
- Over 5.6 million timestamped clinical events extracted
- Event match rates up to 84.7% and temporal concordance up to 0.891 in technical validation
- LLM embeddings outperform traditional methods for survival prediction (c-index 0.822)

## Why This Works (Mechanism)

### Mechanism 1: Relative Temporal Anchoring via LLM Prompting
Large Language Models can resolve ambiguous, relative temporal expressions in clinical text into structured, normalized timestamps better than regex-based approaches. The system prompts LLMs using few-shot examples to identify a reference time (admission, t=0) and convert relative phrases into numeric hour offsets, preserving narrative order without requiring clock-time.

### Mechanism 2: Semantic Event Matching via Vector Space Alignment
Semantic matching using embeddings allows flexible evaluation of event extraction where exact lexical overlap is impossible. Instead of exact string matching, the validation pipeline embeds both LLM-output and gold-standard events using PubMedBERT and uses a "recursive best-match" strategy to find the closest semantic pairs.

### Mechanism 3: Retaining Prognostic Signal in Textual Time Series
Textual time series derived from case reports encode sufficient signal for predictive tasks like survival analysis. By treating the sequence of (event, time) tuples as a text document, the system uses language model embeddings as covariates for survival models, bypassing the need for structured EHR codes.

## Foundational Learning

- **Temporal Normalization**: Why needed - Clinical narratives rarely use absolute timestamps; Quick check - If a patient is admitted on Monday and the report says "symptoms started 3 days ago," is the timestamp -72 or +72?
- **Recursive Best-Match Alignment**: Why needed - Standard classification metrics fail when predicted events differ from gold standard; Quick check - Why does this algorithm favor "one-to-one" matching, and how does it handle if the LLM hallucinates 10 extra events?
- **Concordance Index (C-index)**: Why needed - Primary metric for temporal ordering and survival analysis; Quick check - If Model A predicts event 1 at t=5 and event 2 at t=10 (actual: t=5, t=10), and Model B swaps them (t=10, t=5), which has a higher C-index?

## Architecture Onboarding

- **Component map**: Ingest: PMOA → Regex Filter → Llama 3.3 (Single-Patient Classifier) → Extract: LLM Annotator → Textual Time Series (JSON) → Validate: Gold Set → PubMedBERT Embedder → Recursive Matcher → Metrics
- **Critical path**: The Prompt Design. If the prompt fails to enforce the "admission = 0" rule or separation of conjunctive events, the entire temporal structure collapses.
- **Design tradeoffs**: Open vs. Closed Weights (Llama/DeepSeek vs. GPT-5/O3), Heuristic vs. LAPJV Matching
- **Failure signatures**: Unit Confusion, Granularity Drift, Reference Drift
- **First 3 experiments**: 1) Prompt Ablation: Run extraction with "Zero-shot" vs. "Interval+Type" prompts to quantify C-index drop; 2) Threshold Sensitivity: Vary cosine distance threshold (0.1 to 0.3) to see if looser matching inflates Match Rate; 3) Survival Reproduction: Train Logistic Regression on TF-IDF vs. LLM embeddings to verify signal claim

## Open Questions the Paper Calls Out

### Open Question 1
How does the LLM-based extraction pipeline perform when applied to real-world Electronic Health Record (EHR) clinical notes compared to published case reports? The current technical validation relies entirely on PubMed Open Access case reports, which are structured narrative summaries, rather than the heterogeneous, telegraphic text often found in routine EHR progress notes.

### Open Question 2
Does normalizing extracted event text spans to standard clinical ontologies (e.g., UMLS) improve performance on downstream tasks? The current dataset retains minimally edited text spans to preserve clinical nuance, but this prevents the explicit mapping of events to structured codes which may be necessary for certain predictive models.

### Open Question 3
Can the textual time series representation be extended to accurately capture complex temporal relations, such as event durations and overlapping intervals? The current schema enforces a simplification where events are point estimates, potentially losing granular temporal information essential for process mining or causal inference.

## Limitations
- Temporal granularity degrades significantly for events more than 2 weeks from admission, limiting utility for chronic disease trajectories
- No explicit hallucination detection mechanism; semantic matching may inadvertently reward fabricated events
- Derived exclusively from case reports, creating potential selection bias not representative of routine clinical practice

## Confidence
- High Confidence (85%+): Technical implementation details are well-specified and reproducible
- Medium Confidence (65-85%): Event extraction accuracy based on 200-case gold standard may vary across specialties
- Low Confidence (50-65%): Generalizability to real-world clinical data (EHRs) is uncertain

## Next Checks
1. Systematically evaluate event extraction accuracy across different temporal distances from admission to quantify degradation pattern
2. Manually review random sample of 100 LLM-extracted timelines to estimate false positive rate and characterize hallucination types
3. Apply extraction pipeline to small set of non-case-report clinical documents to assess transferability outside case report domain