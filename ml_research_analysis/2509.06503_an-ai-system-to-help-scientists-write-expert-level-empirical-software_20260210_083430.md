---
ver: rpa2
title: An AI system to help scientists write expert-level empirical software
arxiv_id: '2509.06503'
source_url: https://arxiv.org/abs/2509.06503
tags:
- search
- tree
- methods
- system
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an AI system that automatically creates expert-level
  scientific software to maximize measurable quality scores, addressing the bottleneck
  of manual software development in computational experiments. The system uses a large
  language model and tree search to explore and integrate complex research ideas from
  external sources, enabling exhaustive and intelligent navigation of vast solution
  spaces.
---

# An AI system to help scientists write expert-level empirical software

## Quick Facts
- **arXiv ID:** 2509.06503
- **Source URL:** https://arxiv.org/abs/2509.06503
- **Reference count:** 40
- **Primary result:** An AI system that automatically creates expert-level scientific software, achieving superhuman performance across diverse domains including genomics, epidemiology, and numerical analysis

## Executive Summary
This paper presents an AI system that automatically creates expert-level scientific software by combining large language models with tree search algorithms. The system treats software development as a scorable task, iteratively generating, executing, and optimizing code to maximize measurable quality metrics. Applied across multiple scientific domains, it has discovered novel methods that outperform state-of-the-art human-developed approaches, including 40 new single-cell RNA sequencing methods and COVID-19 forecasting models that surpassed CDC benchmarks.

## Method Summary
The system uses a PUCT tree search algorithm to explore the space of possible software solutions. It starts with an initial code candidate and iteratively generates new versions through an LLM (Gemini), executes them in a sandbox environment to obtain scores, and updates the search tree accordingly. The key innovation is the integration of external research knowledge—summaries of scientific papers are injected into the LLM prompt to guide the generation of scientifically valid code. The search balances exploitation of high-scoring code with exploration of new variants, allowing it to systematically navigate vast solution spaces that would be impossible to explore through random sampling.

## Key Results
- Discovered 40 novel single-cell RNA sequencing batch integration methods that outperformed all existing human-developed methods on public leaderboards
- Generated 14 epidemiological models that surpassed the CDC ensemble for COVID-19 hospitalization forecasting
- Produced top-tier software for geospatial analysis, neural activity prediction, time series forecasting, and numerical integration
- Demonstrated superhuman performance by systematically combining and optimizing existing approaches across diverse scientific domains

## Why This Works (Mechanism)

### Mechanism 1
If the system combines a Large Language Model (LLM) with a Tree Search (TS) algorithm, it may systematically navigate the solution space more effectively than random sampling or one-shot generation. The system utilizes a Predictor + Upper Confidence bound applied to Trees (PUCT) algorithm. Instead of random mutations typical of genetic programming, the LLM acts as a semantic "mutator," rewriting code based on previous iterations. The search balances exploitation (refining high-scoring code) and exploration (trying new, unvisited code structures).

### Mechanism 2
If expert-level research ideas are injected into the prompt, the system is more likely to discover high-quality solutions because it restricts the search to scientifically valid regions of the code space. The system augments the standard "code mutation" loop with external knowledge (e.g., summaries of scientific papers, textbooks, or specialized "Deep Research" agent outputs). This guides the LLM to implement specific algorithms rather than guessing from scratch.

### Mechanism 3
If the system treats software creation as a "scorable task" with rapid feedback, it can autonomously perform the iterative debugging and optimization loop that usually requires human intuition. The system executes generated code in a sandbox to obtain a scalar score. This score serves as the error signal for the Tree Search. By repeatedly generating, executing, and scoring, the system "hill climbs" towards better metrics, effectively automating the trial-and-error phase of empirical software development.

## Foundational Learning

- **Concept: Tree Search Algorithms (PUCT/MCTS)**
  - **Why needed here:** The paper relies on a modified PUCT algorithm to decide which code candidates to mutate next. Understanding the balance between exploration (trying new things) and exploitation (improving known good solutions) is critical to understanding why this system outperforms random sampling.
  - **Quick check question:** How does the PUCT formula in this paper differ from standard AlphaZero applications regarding node selection? (Hint: It samples from the whole set vs. recursing from root).

- **Concept: Prompt Engineering & Context Injection**
  - **Why needed here:** The system's performance heavily depends on injecting "research ideas" into the LLM prompt. You must understand how to format scientific summaries or method descriptions so an LLM can translate them into executable code.
  - **Quick check question:** What specific components must be included in the prompt to replicate a scientific method from a paper summary? (Hint: Method description + data loading + evaluation metrics).

- **Concept: Empirical Software & Scorable Tasks**
  - **Why needed here:** The paper defines its domain as "empirical software"—code written to maximize a measurable score. This distinguishes it from general software engineering; the system doesn't "understand" the science, it optimizes the number.
  - **Quick check question:** Can the system write a device driver? Why or why not? (Hint: No, because device drivers lack a simple, rapid, scalar "quality metric" for hill climbing).

## Architecture Onboarding

- **Component map:** Prompt Constructor -> LLM Rewriter (Gemini) -> Sandbox Executor -> Tree Search Controller (PUCT) -> Idea Injector

- **Critical path:** Selection: PUCT selects a node from the tree based on high reward potential or low visit counts. Generation: The LLM rewrites the code of the selected node based on the prompt. Execution: The new code runs in the sandbox; errors are caught, scores are calculated. Backpropagation: The score updates the statistics of the selected node and its ancestors.

- **Design tradeoffs:**
  - Generality vs. Optimization: The system is general (works on integrals, genomics, forecasting) but may be less sample-efficient than a specialized AutoML tool for a specific model type.
  - Compute Cost: Running thousands of LLM calls and sandbox executions is expensive compared to single-shot generation.
  - Safety: The code runs in a "sandbox," but complex dependencies (e.g., importing obscure libraries) can still cause environment failures.

- **Failure signatures:**
  - Code Collapse: The tree fills with "syntax error" nodes (-inf score) because the LLM fails to produce valid Python for a specific library version.
  - Local Optima: The score plateaus early because the PUCT constant is too low (exploiting too much) or the LLM runs out of "ideas" to perturb the code significantly.
  - Gaming: The code achieves a high score by finding a bug in the evaluation metric (e.g., hardcoding the test set) rather than solving the scientific problem.

- **First 3 experiments:**
  1. Baseline Check (Kaggle): Run the system on a Kaggle Playground regression task with "No Advice." Confirm if the score improves over 100 nodes compared to a single LLM call.
  2. Injection Ablation: Run the same task twice: once with a generic prompt, once with a prompt including a specific "Expert Advice" snippet (e.g., "Use XGBoost"). Compare the rate of score improvement.
  3. Search Space Verification: Pick a "recombination" task (e.g., Method A + Method B). Verify if the resulting code actually contains distinct elements from both parents by inspecting the generated script, rather than just checking the score.

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating synaptic-level structural connectomes into the system enable biophysically-grounded models that outperform current top-performing data-driven video models on the ZAPBench neural activity prediction task? The authors state, "The forthcoming synaptic-level structural reconstruction... provides a unique opportunity to develop such models by integrating anatomical wiring diagrams." This is unresolved because the current "functional connectome" approach using a latent autoencoder did not outperform the top video models, and the true structural data was unavailable during the study.

### Open Question 2
How can the Tree Search objective function be modified to explicitly optimize for model interpretability alongside predictive performance? The authors identify interpretability as a "key future direction" alongside biophysical information, noting that current top solutions rely on complex, opaque architectures like U-Nets and Transformers. This is unresolved because the current system maximizes a "quality metric" (e.g., accuracy, error), which often favors complex black-box ensembles over transparent, explainable code.

### Open Question 3
What algorithmic modifications are required to apply the Tree Search system efficiently to large-scale benchmarks that were excluded due to computational constraints? In the GIFT-Eval benchmark results, the authors note they "excluded the five largest [datasets] due to computational constraints." This is unresolved because the current Tree Search implementation was unable to handle the computational load of the largest datasets within feasible time limits, creating a scalability ceiling.

## Limitations
- The system's performance depends heavily on the availability of reliable, well-defined scalar metrics for each scientific task
- The computational cost of running thousands of LLM calls and sandbox executions may limit practical deployment
- While demonstrating superhuman performance in benchmarks, it remains unclear how it would perform on tasks requiring deep domain expertise beyond what can be captured in brief research summaries

## Confidence

- **High confidence:** The core mechanism of combining LLM with tree search for code generation is well-established, and the empirical results on standardized benchmarks (scRNA-seq, COVID forecasting) are reproducible and clearly presented. The system architecture and evaluation methodology are sound.

- **Medium confidence:** The claims about "expert-level" performance rely on comparisons with published results, which may not represent the full state-of-the-art. Some results depend on specific dataset splits and metric definitions that may not generalize.

- **Low confidence:** The paper does not provide sufficient detail about hyperparameter tuning, sandbox constraints, or the exact prompt engineering techniques used to achieve optimal results. The reproducibility of the most impressive results would require access to the same external knowledge base and evaluation infrastructure.

## Next Checks

1. **Metric robustness test:** Apply the system to a task with multiple competing quality metrics (e.g., Pareto optimization for predictive accuracy vs. interpretability) to verify it can balance trade-offs rather than simply maximizing a single score.

2. **Knowledge injection ablation:** Systematically vary the quality and specificity of injected research ideas across multiple domains to quantify how much of the performance gain comes from the search algorithm versus the external knowledge component.

3. **Long-term generalization test:** After the system achieves high scores on a validation set, evaluate the generated methods on completely unseen datasets or real-world scenarios to assess whether it has learned robust scientific principles or simply overfit to the evaluation metric.