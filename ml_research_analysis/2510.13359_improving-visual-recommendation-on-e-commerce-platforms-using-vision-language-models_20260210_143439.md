---
ver: rpa2
title: Improving Visual Recommendation on E-commerce Platforms Using Vision-Language
  Models
arxiv_id: '2510.13359'
source_url: https://arxiv.org/abs/2510.13359
tags:
- image
- mercari
- product
- visual
- siglip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrated the effectiveness of using a fine-tuned
  vision-language model (SigLIP) for visual similarity-based product recommendations
  on Mercari, a major C2C marketplace. The model was trained on one million product
  image-title pairs and achieved a 9.1% improvement in nDCG@5 and 15.7% improvement
  in precision@1 compared to a traditional CNN-based baseline in offline evaluation.
---

# Improving Visual Recommendation on E-commerce Platforms Using Vision-Language Models

## Quick Facts
- arXiv ID: 2510.13359
- Source URL: https://arxiv.org/abs/2510.13359
- Reference count: 19
- Primary result: SigLIP model achieved 9.1% nDCG@5 and 15.7% precision@1 improvements over CNN baseline, with 50% CTR and 14% conversion increases in live A/B test

## Executive Summary
This study demonstrates the effectiveness of fine-tuned vision-language models for visual similarity-based product recommendations on Mercari's C2C marketplace. The researchers trained a SigLIP model on one million product image-title pairs, achieving significant improvements over traditional CNN-based approaches in both offline metrics and live A/B testing. The results validate the potential of VLMs to enhance product discovery and user engagement on large-scale e-commerce platforms.

## Method Summary
The researchers implemented a vision-language model-based recommendation system using SigLIP architecture trained on one million product image-title pairs from Mercari's dataset. The model was fine-tuned to learn visual-semantic embeddings that capture product similarities beyond traditional image features. Offline evaluation compared the VLM approach against a CNN baseline using precision@1 and nDCG@5 metrics, followed by a live A/B test measuring click-through and conversion rates. The training process focused on optimizing the model's ability to understand product relationships through combined visual and textual information.

## Key Results
- Achieved 9.1% improvement in nDCG@5 and 15.7% improvement in precision@1 compared to CNN baseline in offline evaluation
- Live A/B test showed 50% increase in click-through rate and 14% increase in conversion rate
- SigLIP-based system demonstrated strong real-world performance in enhancing product discovery

## Why This Works (Mechanism)
The vision-language model leverages both visual and textual information to create richer product representations, capturing semantic relationships that traditional image-only models miss. By learning from image-title pairs, the model understands product attributes, styles, and contextual relationships that drive user preferences. The fine-tuning process adapts pre-trained VLMs to e-commerce-specific product characteristics, enabling more accurate similarity matching and personalized recommendations.

## Foundational Learning
- **Vision-Language Models**: Why needed - to capture both visual and semantic product relationships; Quick check - validate embeddings capture both appearance and functional attributes
- **Fine-tuning Strategy**: Why needed - adapt pre-trained models to specific e-commerce domain; Quick check - monitor domain-specific metric improvements during training
- **Visual-Semantic Embeddings**: Why needed - create unified representation space for similarity matching; Quick check - verify embedding distances correlate with user preferences

## Architecture Onboarding

Component Map:
Image Encoder -> Text Encoder -> Joint Embedding Space -> Similarity Matching -> Recommendation Engine

Critical Path:
Image and title pairs enter encoders simultaneously, joint embeddings are created, similarity scores are computed, and top-k recommendations are generated for user interfaces.

Design Tradeoffs:
The model prioritizes semantic understanding over pure visual similarity, trading computational efficiency for richer representations. Fine-tuning on domain-specific data improves accuracy but requires significant labeled examples. The choice of SigLIP balances model capacity with inference speed requirements for real-time recommendations.

Failure Signatures:
Poor recommendations when product titles are missing or uninformative, suggesting the model relies heavily on textual context. Suboptimal performance on niche product categories with limited training data. Potential overfitting to Mercari's specific product distribution rather than general e-commerce patterns.

Three First Experiments:
1. Test model performance on product pairs with minimal textual descriptions to isolate visual-only capabilities
2. Evaluate cross-category recommendation accuracy to assess generalization beyond training distribution
3. Measure latency impact of VLM inference compared to CNN baseline under production load

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Results validated only on Mercari platform, limiting generalizability to other e-commerce environments
- Offline evaluation metrics (precision@1, nDCG@5) may not fully capture long-term engagement impacts
- No analysis of potential biases in recommendations across different product categories or seller demographics

## Confidence
- Offline metrics improvement (9.1% nDCG@5, 15.7% precision@1): High
- A/B test results (50% CTR increase, 14% conversion increase): High
- Scalability to larger platforms with different user behaviors: Medium

## Next Checks
1. Test SigLIP model across multiple e-commerce platforms with diverse product categories and user demographics to assess generalizability
2. Conduct longer-term A/B tests (minimum 3 months) to evaluate impact on user retention, repeat purchases, and customer lifetime value
3. Perform bias and fairness audits using diverse product and seller datasets to identify potential systematic exclusions or misrepresentations in recommendations