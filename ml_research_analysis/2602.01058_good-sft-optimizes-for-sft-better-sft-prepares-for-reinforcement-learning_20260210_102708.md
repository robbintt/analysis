---
ver: rpa2
title: Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning
arxiv_id: '2602.01058'
source_url: https://arxiv.org/abs/2602.01058
tags:
- offline
- learning
- online
- pass
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PEAR improves RL readiness by reweighting SFT losses with suffix-based
  importance sampling. Standard SFT and related variants optimize for offline accuracy
  alone, yet experiments show such gains can reverse after identical RL training.
---

# Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning

## Quick Facts
- arXiv ID: 2602.01058
- Source URL: https://arxiv.org/abs/2602.01058
- Reference count: 40
- Primary result: PEAR reweights SFT losses to improve RL readiness and post-RL reasoning accuracy

## Executive Summary
PEAR (Policy-Enhanced Alignment for RL Readiness) is a method that improves the alignment between supervised fine-tuning (SFT) and subsequent reinforcement learning (RL) by reweighting SFT losses with suffix-based importance sampling. Unlike standard SFT, which optimizes for offline accuracy, PEAR emphasizes trajectories that remain plausible under the target policy, leading to better RL readiness and higher post-RL performance on math and logic tasks.

## Method Summary
PEAR applies a likelihood-ratio weighting to SFT training, using sequence-level, suffix-based token-level, or block-level importance sampling to prioritize offline trajectories that align with the target policy. It can also incorporate negative examples. By doing so, it addresses the mismatch between SFT's offline accuracy focus and RL's optimization goals, improving post-RL reasoning accuracy across multiple model families and datasets.

## Key Results
- PEAR consistently improves post-RL pass@1 and pass@8 scores across Qwen and DeepSeek models.
- Up to 14.6% absolute gain on AIME-2025 after RL training.
- PEAR incurs minimal computational overhead compared to standard SFT.

## Why This Works (Mechanism)
PEAR works by reweighting SFT losses to emphasize trajectories that remain plausible under the target policy, bridging the gap between offline accuracy and RL optimization. By using suffix-based importance sampling, it focuses on the most informative parts of sequences, which are more likely to generalize during RL.

## Foundational Learning
- **Importance Sampling**: Used to prioritize more informative samples during training; needed for efficient learning from imbalanced datasets.
  - Quick check: Verify that the sampling distribution matches the target policy distribution.
- **Likelihood Ratio Weighting**: Adjusts the contribution of each sample based on its probability under the target policy; needed to align offline and online objectives.
  - Quick check: Ensure that weights are normalized and do not explode during training.
- **Suffix-Based Token-Level Weighting**: Focuses on the latter parts of sequences, which are more indicative of reasoning quality; needed to capture long-range dependencies.
  - Quick check: Compare performance with and without suffix weighting to isolate its effect.

## Architecture Onboarding
- **Component Map**: SFT Dataset -> PEAR Reweighting -> RL Training -> Evaluation
- **Critical Path**: PEAR reweighting must be applied before RL training to ensure readiness.
- **Design Tradeoffs**: Balancing importance sampling granularity (sequence vs. token vs. block) vs. computational cost.
- **Failure Signatures**: If PEAR is not applied, RL may fail to improve or even degrade performance.
- **First Experiments**:
  1. Compare post-RL performance with and without PEAR on a small math dataset.
  2. Test different importance sampling granularities to find the optimal balance.
  3. Evaluate the impact of including negative examples in the SFT dataset.

## Open Questions the Paper Calls Out
- The degree to which PEAR's improvements transfer to domains outside math and logic puzzles.
- The computational overhead of importance sampling for large datasets.
- The method's effectiveness when a reliable target policy is not available.
- Long-term robustness and whether gains persist after extended RL training.

## Limitations
- PEAR's effectiveness is primarily demonstrated on math and logic puzzles, with unclear generalizability to other domains.
- The method requires a reliable target policy for importance sampling, which may not always be available.
- Long-term robustness and scalability to larger datasets are not fully explored.

## Confidence
- Confidence in PEAR consistently improving post-RL reasoning accuracy: High
- Confidence in the claim that standard SFT can reverse gains after RL: Medium
- Confidence in PEAR's scalability and applicability to other domains: Low

## Next Checks
1. Test PEAR's effectiveness on non-mathematical reasoning tasks, such as code generation or general QA, to assess cross-domain robustness.
2. Conduct long-term RL training to determine if PEAR's advantages persist or decay over time.
3. Evaluate the computational overhead and scalability of PEAR on datasets an order of magnitude larger than those used in the current experiments.